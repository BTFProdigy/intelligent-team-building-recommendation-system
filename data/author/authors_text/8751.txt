Robust Sub-Sentential Alignment of Phrase-Structure Trees
Declan Groves
School of Computing
Dublin City University
Dublin 9, Ireland
dgroves@computing.dcu.ie
Mary Hearne
School of Computing
Dublin City University
Dublin 9, Ireland
mhearne@computing.dcu.ie
Andy Way
School of Computing
Dublin City University
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
Data-Oriented Translation (DOT), based on Data-
Oriented Parsing (DOP), is a language-independent
MT engine which exploits parsed, aligned bitexts
to produce very high quality translations. How-
ever, data acquisition constitutes a serious bottleneck
as DOT requires parsed sentences aligned at both
sentential and sub-structural levels. Manual sub-
structural alignment is time-consuming, error-prone
and requires considerable knowledge of both source
and target languages and how they are related. Au-
tomating this process is essential in order to carry out
the large-scale translation experiments necessary to
assess the full potential of DOT.
We present a novel algorithm which automatically in-
duces sub-structural alignments between context-free
phrase structure trees in a fast and consistent fash-
ion requiring little or no knowledge of the language
pair. We present results from a number of experi-
ments which indicate that our method provides a se-
rious alternative to manual alignment.
1 Introduction
Approaches to Machine Translation (MT) using
Data-Oriented Parsing (DOP: (Bod, 1998; Bod et
al., 2003)) require ?source,target? tree fragments
aligned at sentential and sub-sentential levels. In
previous approaches to Data-Oriented Translation
(DOT: (Poutsma, 2000; Hearne and Way, 2003)),
such fragments were produced manually. This is
time-consuming, error-prone, and requires consid-
erable expertise of both source and target languages
as well as how they are related. The obvious solu-
tion, therefore, is to automate the process of sub-
sentential alignment. However, while there are
many approaches to sentential alignment e.g. (Kay
and Ro?scheisen, 1993; Gale & Church, 1993), no
methods exist for aligning non-isomorphic phrase-
structure (PS) tree fragments at sub-sentential level
for use in MT. (Matsumoto et al, 1993) align
?source,target? dependency trees, with a view to re-
solve parsing ambiguities, but their approach can-
not deal with complex or compound sentences.
Other researchers (Imamura, 2001) also use phrase-
alignment in parsing but in DOT the translation
fragments are already in the form of parse-trees.
(Eisner, 2003) outlines a computationally expensive
structural manipulation tool which he has used for
intra-lingual translation but has yet to apply to inter-
lingual translation. (Gildea, 2003) performs tree-to-
tree alignment, but treats it as part of a generative
statistical translation model, rather than a seperate
task. The method of (Ding et al, 2003) can cope
with a limited amount of non-isomorphism, but the
algorithm is only suitable for use with dependency
trees.
We develop a novel algorithm which automati-
cally aligns translationally equivalent tree fragments
in a fast and consistent fashion, and which requires
little or no knowledge of the language pair. Our ap-
proach is similar to that of (Menezes and Richard-
son, 2003), who use a best-first approach to align
dependency-type tree structures.
We conduct a number of experiments on the
English-French section of the Xerox HomeCentre
corpus. Using the manual alignment of (Hearne
and Way, 2003) as a ?gold standard?, we show that
our algorithm identifies sub-structural translational
equivalences with 73.7% precision and 67.84% re-
call. Furthermore, we replicate previous DOT ex-
periments performed using manually aligned data.
However, we use data aligned by our novel al-
gorithm and evaluate the output translations. We
demonstrate that while coverage decreases by 10%,
the translations output are of comparable quality.
These results indicate that our automatic alignment
algorithm provides a serious alternative to manual
alignment.
The remainder of this paper is organised as fol-
lows: in section 2, we discuss related research in
more detail, while in section 3, we provide an over-
iew of DOT. We present our algorithm in section 4,
and in section 5 describe the experiments conducted
together with the results obtained. Finally, we con-
clude and provide avenues for further research.
2 Related Research
Several approaches to sub-structural alignment of
tree representations have been proposed.
(Matsumoto et al, 1993) and (Imamura, 2001)
focus on using alignments to help resolve pars-
ing ambiguities. As we wish to develop an align-
ment process for use in MT rather than parsing, this
makes their approaches unsuitable for our use.
(Eisner, 2003) presents a tree-mapping method
for use on dependency trees which he claims can be
adapted for use with PS trees. He uses dynamic pro-
gramming to break tree pairs into pairs of aligned
elementary trees, similar to DOT. However, he aims
to estimate a translation model from unaligned data,
whereas we wish to align our data off-line. Cur-
rently, he has used his algorithm to perform intra-
lingual translation but has yet to develop and apply
real models to inter-lingual MT.
(Gildea, 2003) outlines an algorithm for use in
syntax-based statistical models of MT, applying a
statistical TSG with probabilities parameterized to
generate the target tree conditioned on the structure
of the source tree. His approach is unsuitable for
DOT as it involves altering the shape of trees in or-
der to impose isomorphism and the algorithm does
not always generate a complete target tree structure.
However, unlike (Gildea, 2003), we treat the prob-
lem of alignment as a seperate task rather than as
part of a generative translation model.
(Ding et al, 2003) and (Menezes and Richard-
son, 2003) also present approaches to the alignment
of tree structures. Both deal with dependency struc-
tures rather than PS trees. (Ding et al, 2003) out-
line an algorithm to extract word-level alignments
using structural information taken from parallel de-
pendency trees. They fix the nodes of tree pairs
based on word alignments deduced statistically and
then proceed by partitioning the tree into treelet
pairs with the fixed nodes as their roots. Their al-
gorithm relies on the fact that, in dependency trees,
subtrees are headed by words rather than syntactic
labels, making it unsuitable for our use.
(Menezes and Richardson, 2003) employ a best-
first strategy and use a small alignment grammar
to extract transfer mappings from bilingual corpora
for use in translation. They use a bilingual dictio-
nary and statistical techniques to supply translation
pair candidates and to identify multi-word terms.
Lexical correspondences are established using the
lexicon of 98,000 translation pairs and a deriva-
tional morphology component to match other lexi-
cal items. Nodes are then aligned using these lexical
correspondences along with structural information.
Our algorithm uses a similar methodology. How-
ever, (Menezes and Richardson, 2003) use logical
forms, which constitute a variation of dependency
trees that normalize both the lexical and syntactic
form of examples, whereas we align PS trees.
Although the methods outlined above have
achieved promising results, only the approach of
(Menezes and Richardson, 2003) seems relevant
to our goal, even though they deal with abstract
dependency-type structures rather than PS trees.
3 Data-Oriented Translation
Data-Oriented Translation (DOT) (Poutsma, 2000;
Hearne and Way, 2003), which is based on Data-
Oriented Parsing (DOP) (Bod, 1998; Bod et al,
2003), comprises a context-rich, experience-based
approach to translation, where new translations
are derived with reference to grammatical analy-
ses of previous translations. DOT exploits bilin-
gual treebanks comprising linguistic representations
of previously seen translation pairs, as well as ex-
plicit links which map the translational equivalences
present within these pairs at sub-sentential level ?
an example of such a linked translation pair can be
seen in Figure 1(a). Analyses and translations of
the input are produced simultaneously by combin-
ing source and target language fragment pairs de-
rived from the treebank trees.
3.1 Fragmentation
The tree fragment pairs used in Tree-DOT are
called subtree pairs and are extracted from bilingual
aligned treebank trees. The two decomposition op-
erators, which are similar to those used in Tree-DOP
but are refined to take the translational links into ac-
count, are as follows:
? the root operator which takes any pair of linked
nodes in a tree pair to be the roots of a subtree pair
and deletes all nodes except these new roots and all
nodes dominated by them;
? the frontier operator which selects a (possibly
empty) set of linked node pairs in the newly cre-
ated subtree pairs, excluding the roots, and deletes
all subtree pairs dominated by these nodes.
Allowing the root operator to select the root nodes
of the original treebank tree pair and then the fron-
tier operator to select an empty set of node pairs
ensures that the original treebank tree pair is al-
ways included in the fragment base ? in Figure 1,
fragment (a) exactly matches the original treebank
tree pair from which fragments (a) ? (f) were de-
rived. Fragments (b) and (f) were also derived by
allowing the frontier operator to select the empty
set; the root operation selected node pairs <A,N>
and <NPadj,NPdet> respectively. Fragments (c),
(d) and (e) were derived by selecting all further pos-
sible combinations of node pairs by root and fron-
tier.
(a) (b)
VPv
V NPadj
clearing A N
paper jams
NPpp
N PP
resolution P NPdet
de D NPap
les N N
incidents papier
A
paper
N
papier
(c) (d)
VPv
V NPadj
clearing A N
jams
NPpp
N PP
resolution P NPdet
de D NPap
les N N
incidents
NPadj
A N
jams
NPdet
D NPap
les N N
incidents
(e) (f)
VPv
V NPadj
clearing
NPpp
N PP
resolution P NPdet
de
NPadj
A N
paper jams
NPdet
D NPap
les N N
incidents papier
Figure 1: DOT fragments generated via root and frontier
3.2 Translation
The DOT composition operator is defined as fol-
lows. The composition of tree pairs <s1,t1> and
<s2,t2> (<s1,t1> ? <s2,t2>) is only possible if
? the leftmost non-terminal frontier node of s1 is of
the same syntactic category (e.g. S, NP, VP) as the
root node of s2, and
? the leftmost non-terminal frontier node of s1?s
linked counterpart in t1 is of the same syntactic cat-
egory as the root node of t2.
The resulting tree pair consists of a copy of s1 where
s2 has been inserted at the leftmost frontier node and
a copy of t1 where t2 has been inserted at the node
linked to s1?s leftmost frontier node, as illustrated in
Figure 2.
The DOT probability of a translation derivation is
the joint probability of choosing each of the subtree
pairs involved in that derivation. The probability of
selecting a subtree pair is its number of occurrences
in the corpus divided by the number of pairs in the
corpus with the same root nodes as it:
P (< es, et >) =
|<es,et>|
?
<us,ut>:r(<us,ut>)=r(<es,et>) |<us,ut>|
The probability of a derivation in DOT is the prod-
uct of the probabilities of the subtree pairs involved
ROOT
VPv PERIOD
V N .
click
LISTITEM
VPverb PERIOD
V PP .
cliquez P N
sur
?
N
Save
N
Enregistrer
=
ROOT
VPv PERIOD
V N .
click Save
LISTITEM
VPverb PERIOD
V PP .
cliquez P N
sur Enregistrer
Figure 2: The DOT composition operation
in building that derivation. Thus, the probability of
derivation <s1,t1> ? ... ? <sn,tn> is given by
P (< s1, t1 > ?...? < sn, tn >) =
?
i
P (< si, ti >)
Again, a translation can be generated by many dif-
ferent derivations, so the probability of a transla-
tion ws ??wt is the sum of the probabilities of its
derivations:
P (< ws, wt >) =
?
<tsi ,tti> yields <ws,wt>
P (< tsi , tti >)
Selection of the most probable translation via Monte
Carlo sampling involves taking a random sample of
derivations and outputting the most frequently oc-
curring translation in the sample.
4 Our Algorithm
The operation of a DOT system is dependent on the
availability of bilingual treebanks aligned at senten-
tial and sub-sentential level. Our novel algorithm
attempts to fully automate sub-sentential alignment
using an approach inspired by that of (Menezes and
Richardson, 2003). The algorithm takes as input a
pair of ?source,target? PS trees and outputs a map-
ping between the nodes of the tree pair.
As with the majority of previous approaches, the
algorithm starts by finding lexical correspondences
between the source and target trees. Our lexicon
is built automatically using a previously developed
word aligner based on the k-vec aligner as outlined
by (Fung & Church, 1994). This lexical aligner uses
a combination of automatically extracted cognate
information, mutual information and probabilistic
measures to obtain one-to-one lexical correspon-
dences between the source and target strings. Dur-
ing lexical alignment, function words are excluded
because, as they are the most common words in a
language, they tend to co-occur frequently with the
content words they precede. This can lead to the
incorrect alignment of content words with function
words.
The algorithm then proceeds from the aligned
lexical terminal nodes in a bottom-up fashion, us-
ing a mixture of node label matching and structural
information to perform language-independent link-
ing between all ?source,target? node pairs within the
trees. As with (Menezes and Richardson, 2003),
it uses a best-first approach. After each step, new
linked node pairs are added to the current list of
linked nodes. The links made between the nodes
are fixed, thus restricting the freedom of alignment
for the remaining unaligned nodes in the tree pair.
The methods of the algorithm are applied to each
new linked node pair in turn until no new node pairs
can be added. The algorithm consists of five main
methods which are performed on each linked node
pair in the list:
Verb + Object Align (Figure 3): We have a linked source-
target node pair ?s,t?. s and t are both verbs, are the
leftmost children in their respective trees, both have VP
parent nodes and they have the same number of siblings
which have similar syntactic labels. We align the corre-
sponding siblings of s and t. This aligns the objects of
the source verb with the equivalent objects of the target
verb. We also align the parents of s and t.
S
PRON VPaux
you MODAL VPv
can V NPadj
scan A N
entire pages
S[dec]
PRON VPverb
vous MODAL VPverb[main]
pouvez V NPdet
nume?riser D NPap
des N A
pages entie`res
Figure 3: Verb + Object Align: the dashed lines represent
the links made by Verb + Object Align when the current linked
node pair is ?MODAL,MODAL?.
Parent Align (Figure 4): We have a current linked source-
target node pair ?s,t? with unlinked parents pars and part
respectively. All the sister nodes of s are aligned with
sister nodes of t. We link pars and part. If s and t each
have one unlinked sister, but the remaining sisters of s are
aligned with sister nodes of t, link the unlinked sisters and
link pars with part.
NP/VP Align (Figure 5): We have a linked source-target
node pair ?s,t? and s and t are both nouns. Traverse up
the source tree to find the topmost NP node nps dominat-
ing s and traverse up the target tree to find the topmost
Nmod
A NP
color print head
NPap
NP N
te?te d?impression couleur
Figure 4: Parent Align: The dashed lines are the links made
by Parent Align, when ?color,couleur? is the current linked node
pair.
target NP node npt dominating t. We link nps and npt.
We then traverse down from nps and npt to the leftmost
leaf nodes ( ls and lt) in the source and target subtrees
rooted at nps and npt. If ls and lt have similar labels,
we link them. This helps to preserve the scope of noun-
phrase modifiers. If s and t are both verbs, we perform a
similar method, this time linking the topmost VP nodes
in the source and target trees.
NP
D NPadj
a A N
pending document
NPdet
D NPpp
un N PP
document P NPpp
en N PP
attenteP N
de impression
Figure 5: NP Align: the dashed lines represent the links made
by NP Align when the current linked node pair is ?N,N?.
Child Align (Figure 6): This method is similar to that of Par-
ent Align. We have a current linked source-target node
pair ?s,t?. Each node has the same number of children
and these children have similar node labels. We link their
corresponding children.
S
NP VPcop
PRON N Vcop NP
is PRON NPadj
your imagination your A N
only limitation
S
NPdet VPcop
D N Vcop NPdet
est D NPap
votre imagination votre A N
seule limite
Figure 6: Child Align: the dashed lines represent the links
made by Child Align when the current linked node pair is ?S,S?.
Subtree Align: We have a linked source-target node pair ?s,t?.
If the subtrees rooted at s and at t are fully isomorphic, we
link the corresponding nodes within the subtrees. This
accounts for the fact that trees may not be completely
isomorphic from their roots but may be isomorphic at
subtree level.1
1Originally we used a method isomophic which checked for
Once lexical correspondences have been estab-
lished, the methods outlined above use structural in-
formation to align the ?source,target? nodes. The
comparison of ?source,target? node labels during
alignment ensures that sub-structures with corre-
sponding syntactic categories are aligned. If the
algorithm fails to find any alignments between the
source and target tree pairs, due to the absence
of initial lexical correspondences, we align the
?source,target? root nodes.
5 Experiments and results
Previous DOT experiments (Hearne and Way, 2003)
were carried out on a subset of the HomeCentre
corpus consisting of 605 English-French sentence
pairs from Xerox documentation parsed into LFG
c(onstituent)- and f(unctional)-structure representa-
tions and aligned at sentence level. This bilingual
treebank constitutes a linguistically complex frag-
ment base containing many ?hard? translation ex-
amples, including cases of nominalisations, pas-
sivisation, complex coordination and combinations
thereof. Accordingly, the corpus would appear to
present a challenge to any MT system.
The insertion of the links denoting translational
equivalence for the set of tree pairs used in the pre-
vious experiments was performed manually. We
have applied our automatic sub-structural alignment
algorithm to this same set of 605 tree pairs and
evaluated performance using two distinct methods.
Firstly, we used the manual alignments as a ?gold
standard? against which we evaluated the output of
the alignment algorithm in terms of precision, recall
and f-score. The results of this evaluation are pre-
sented in Section 5.1. Secondly, we repeated the
DOT experiments described in (Hearne and Way,
2003) using the automatically generated alignments
in place of those determined manually. We evalu-
ated the output translations in terms of IBM Bleu
scores, precision, recall and f-score and present
these results in Section 5.2.
5.1 Evaluation of alignment quality
Using the manually aligned tree pairs as a ?gold
standard?, we evaluated the performance of each
of the five methods which constitute the alignment
algorithm both individually and in combination.
These evaluations are summarised in Figures 7 and
8 respectively.
The alignment process is always initialised by
finding word correspondences between the source
isomorphism from the roots downwards, assuming a root-root
correspondence. However, this significantly decreased the per-
formance of the aligner.
PRECISION RECALL F-SCORE
Lex 0.6800 0.3057 0.4212
Par 0.7471 0.4983 0.5978
NP/VP 0.7206 0.4879 0.5819
Child 0.7045 0.3856 0.4984
Verb + Object 0.6843 0.3191 0.4352
Figure 7: Individual evaluation of alignment methods
PRECISION RECALL F-SCORE
Par + Child 0.7525 0.5588 0.6414
Par + NP/VP 0.7373 0.6106 0.6680
Par + Child + NP/VP 0.7411 0.6587 0.6974
All 0.7430 0.6686 0.7039
All + Subtree 0.7370 0.6784 0.7064
Figure 8: Evaluation of combined alignment methods
and target trees, meaning that lexical alignment is
carried out regardless of which other method or
combination of methods is included. The low rate of
recall achieved by the lexical alignment process of
0.3057, shown in Figure 7, can be largely attributed
to the fact that it does not align function words. We
achieve high precision relative to recall ? as is gen-
erally preferred for automatic procedures ? indicat-
ing that the alignments induced are more likely to
be ?partial? than incorrect.
When evaluated individually, the Parent Align
method performs best, achieving an f-score of
0.5978. Overall, the highest f-score of 0.7064 is
achieved by using all methods, including the addi-
tional subtree method, in combination.
5.2 Evaluation of translation quality
In order to evaluate the impact of using automat-
ically generated alignments on translation quality,
we repeated the DOT experiments described in
(Hearne and Way, 2003) using these alignments in
place of manually determined translational equiva-
lences.
In order to ensure that differences in the results
achieved could be attributed solely to the differ-
ent sub-structural alignments imposed, we used pre-
cisely the same 8 training/test set splits as before,
where each training set contained 545 parsed sen-
tence pairs, each test set 60 sentences, and all words
occurring in the source side of the test set alo oc-
curred in the source side of the training set (but not
necessarily with the same lexical category). As be-
fore, all translations carried out were from English
into French and the number of samples taken during
the disambiguation process was limited to 5000.
Due to constraints on time and memory, data-
oriented language processing applications gener-
ally limit the size of the fragment base by exclud-
Bleu/Auto Bleu/Man F-Score/Aut. F-Score/Man
LD1 0.0605 0.2627 0.3558 0.5506
LD2 0.1902 0.3018 0.4867 0.5870
LD3 0.1983 0.3235 0.4957 0.6045
LD4 0.214 0.3235 0.5042 0.6069
Figure 9: Evaluation over all translations
ing larger fragments. In these experiments, we in-
creased the size of the fragment base incrementally
by initially allowing only fragments of link depth
(LD) 1 and then including those of LD 2, 3 and 4. 2
We evaluated the output translations in terms of
IBM Bleu scores using the NIST MT Evaluation
Toolkit3 and in terms of precision, recall and f-score
using the NYU General Text Matcher.4 We sum-
marise our results and reproduce and extend those
of (Hearne and Way, 2003)5 in Figures 9, 10 and
11.
Results over the full set of output translations,
summarised in Figure 9, show that using the man-
ually linked fragment base results in significantly
better overall performance at all link depths (LD1
- LD4) than using the automatic alignments. How-
ever, both metrics used assign score 0 in all in-
stances where no translation was output by the sys-
tem. The comparatively poor scores achieved us-
ing the automatically induced alignments reflect the
fact that these alignments give poorer coverage at all
depths than those determined manually (47.71% vs.
66.46% at depth 1, 56.39% vs. 67.92% at depths 2
- 4).
The results in Figure 10 include scores only
where a translation was produced. Here, transla-
tions produced using manual alignments score bet-
ter only at LD 1; better performance is achieved at
LD 2 - 4 using the automatically linked fragment
base. Again, this may ? at least in part ? be an issue
of coverage: many of the sentences for which only
the manually aligned fragment base produces trans-
lations are translationally complex and, therefore,
more likely to be only partially correct and achieve
poor scores.
Finally, we determined the subset of sentences
for which translations were produced both when
the manually aligned fragment bases were used and
2The link depth of a fragment pair is defined as greatest
number of steps taken which depart from a linked node to get
from the root node to any frontier nodes (Hearne and Way,
2003).
3http://www.nist.gov/speech/tests/mt/mt2001/resource/
4http://nlp.cs.nyu.edu/GTM/
5The Bleu scores shown here differ from those published in
(Hearne and Way, 2003) as a result of recent modifications to
the NIST MT Evaluation Kit.
Bleu/Auto Bleu/Man F-Score/Auto F-Score/Man
LD1 0.6118 0.6591 0.7900 0.8090
LD2 0.7519 0.7144 0.8751 0.8446
LD3 0.7790 0.7610 0.8887 0.8688
LD4 0.7940 0.7611 0.8930 0.8736
Figure 10: Evaluation over translations produced
Bleu/Auto Bleu/Man F-Score/Auto F-Score/Man
LD1 0.5945 0.6363 0.7918 0.7989
LD2 0.7293 0.7382 0.8823 0.8629
LD3 0.7700 0.7930 0.8938 0.8913
LD4 0.7815 0.7940 0.8964 0.8933
Figure 11: Evaluation of sentences translated by both
alignment methods
when the automatically linked ones were used. Fig-
ure 11 summarises the results achieved when eval-
uating only these translations. In terms of Bleu
scores, translations produced using manual align-
ments score slightly better at all depths. However,
as link depth increases the gap narrows consistently
and at depth 4 the difference in scores is reduced
to just 0.0125. In terms of f-scores, the translations
produced using automatic alignments actually score
better than those produced using manual alignments
at depths 2 - 4.
5.3 Discussion
Our first evaluation method (Section 5.1) is, per-
haps, the obvious one to use when evaluating align-
ment performance. However, the results of this
evaluation, which show best f-scores of 70%, pro-
vide no insight into the effect using these align-
ments has on translation accuracy. Evaluating these
alignments in context ? by using them in the DOT
system for which they were intended ? gives us a
true picture of their worth. Crucially, in Section 5.2
we showed that using automatic rather than manual
alignments results in translations of extremely high
quality, comparable to those produced using manual
alignments.
In many cases, translations produced using au-
tomatic alignments contain fewer errors involving
local syntactic phenomena than those produced us-
ing manual alignment. This suggests that, as links
between function words are infrequent in the au-
tomatic alignments, we achieve better modelling
of phenomena such as determiner-noun agreement
because the determiner fragments do not gener-
ally occur without context. For example, there are
relatively few instances of ?D?the? aligned with
?D?le/la/l?/les? in the automatic alignment com-
pared to the manual alignment.
On the other hand, we achieve 10% less coverage
when translating using the automatic alignments.
The automatic alignments are less likely to identify
non-local phenomena such as long-distance depen-
dencies. Consequently, the sentences only trans-
lated when using manual alignments are generally
longer and more complex than those translated by
both. While a degree of trade-off between coverage
and accuracy is to be expected, we would like to
increase coverage while maintaining or improving
translation quality. Improvements to lexical align-
ment should prove valuable in this regard. While
we expect translation quality to improve as depth
increases, experiments using the automatical align-
ment show disproportionately poor performance at
depth 1. The majority of links in the depth 1 frag-
ment base are inserted using the lexical aligner, in-
dicating that these are less than satisfactory. We ex-
pect improvements to the lexical aligner to signifi-
cantly improve the overall performance of the align-
ment algorithm and, consequently, the quality of the
translations produced. Lexical alignment is crucial
in identifying complex phenomena such as long dis-
tance dependencies. Using machine-readable bilin-
gual dictionaries or, alternatively, manually estab-
lished word-alignments to intiate the automatic sub-
structural alignment algorithm may provide more
accurate results.
6 Conclusions and future work
We have presented an automatic algorithm which
aligns bilingual context-free phrase-structure trees
at sub-structural level and applied this algorithm to
a subset of the English-French section of the Home-
Centre corpus. We have outlined detailed eval-
uations of our algorithm. They show that while
translation coverage was 10% lower using the au-
tomatically aligned data, the quality of the trans-
lations produced is comparable to the quality of
those produced using manual alignments. While
DOT systems produce very high quality transla-
tions in reasonable time, resource acquisition re-
mains an issue. Manual sub-structural alignment is
time-consuming, error-prone and requires consider-
able linguistic expertise. Our alignment method, on
the other hand, is efficient, consistent and language-
independent, constituting a viable alternative to
manual sub-structural alignment; thus solving the
data acquisition problem.
We intend to apply our automatic alignment
methodology to the full English-French section of
the HomeCentre corpus, as well as the English-
German and French-German sections, and perform
experiments to further validate the the language-
independent nature of both our alignment algorithm
and the data-oriented approach to translation. We
also plan to automatically parse existing bitexts,
thus creating further resources for use with our
DOT system and, together with our aligner, en-
abling much larger-scale DOT-based translation ex-
periments than have been performed to date.
7 Aknowledgements
The work presented in this paper is partly supported
by an IRCSET 6 PhD Fellowship Award.
References
Rens Bod. 1998. Beyond Grammar: An Experience-
Based Theory of Language. CSLI, Stanford, CA.
Rens Bod, Remko Scha and Khalil Sima?an. (eds.) 2003.
Data-Oriented Parsing. CSLI, Stanford CA.
Yuan Ding, Dan Gildea and Martha Palmer. 2003. An
Algorithm for Word-Level Alignment of Parallel De-
pendency Trees. MT Summit IX. New Orleans, LO.,
pp.95?101.
Jason Eisner. 2003. Learning Non-Isomorphic Tree
Mappings for Machine Translation. In Proceedings of
the 41st COLING, Sapporo, Japan.
Pascale Fung & Ken W. Church. 1994. K-vec: A New
Approach for Aligning Parallel Texts. In Proceedings
of COLING 94, Kyoto, Japan, pp.1096-1102.
William A. Gale & Ken W. Church. 1993. A program
for aligning sentences in bilingual corpora. Computa-
tional Linguistics 19(1):75?102.
Daniel Gildea. 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proceedings of the 41st ACL.
Sapporo, Japan, pp.80?87.
Mary Hearne and Andy Way. 2003. Seeing the Wood
for the Trees: Data-Oriented Translation. MT Summit
IX. New Orleans, LO., pp.165?172.
Kenji Imamura. 2001. Hierarchical Phrase Alignment
Harmonized With Parsing. In Proceedings of the
Sixth Natural Language Processing Pacific Rim Sym-
posium. Tokyo, Japan, pp.377?384.
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics
19(1):121?142.
Yuji Matsumoto, Ishimoto Hiroyuki and Takehito Ut-
suro. 1993. Structural Matching of Parallel Texts. In
Proceedings of the 31st ACL. Columbus, OH., pp.23?
30.
Arul Menezes and Stephen D. Richardson. 2003. A
Best-First Alignment Algorithm for Automatic Ex-
traction of Transfer Mappings from Bilingual Cor-
pora. In M. Carl & A. Way (eds.) Recent Advances
in Example-Based Machine Translation. Kluwer
Academic Publishers, Dordrecht, The Netherlands,
pp.421?442.
Arjen Poutsma. 2000. Data-Oriented Translation. In
18th COLING, Saarbru?cken, Germany, pp.635?641.
6http://www.ircset.ie
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1105?1112
Manchester, August 2008
Automatic Generation of Parallel Treebanks
Ventsislav Zhechev
NCLT, School of Computing
Dublin City University
Dublin, Ireland
vzhechev@computing.dcu.ie
Andy Way
NCLT, School of Computing
Dublin City University
Dublin, Ireland
away@computing.dcu.ie
Abstract
The need for syntactically annotated data 
for use in natural language processing has 
increased dramatically in recent years. This 
is true especially for parallel treebanks, of 
which very few exist. The ones that exist 
are mainly hand-crafted and too small for 
reliable use in data-oriented applications. 
In this paper we introduce a novel platform 
for fast and robust automatic generation of 
parallel treebanks. The software we have 
developed based on this platform has been 
shown to handle large data sets. We also 
present evaluation results demonstrating 
the quality of the derived treebanks and 
discuss some possible modifications and 
improvements that can lead to even better 
results. We expect the presented platform 
to help boost research in the field of data-
oriented machine translation and lead to 
advancements in other fields where paral-
lel treebanks can be employed.
1 Introduction
In recent years much effort has been made to make 
use of syntactic information in statistical machine 
translation (MT) systems (Hearne and Way, 2006, 
Nesson et al, 2006). This has led to increased in-
terest in the development of parallel treebanks as 
the source for such syntactic data. They consist 
of a parallel corpus, both sides of which have 
been parsed and aligned at the sub-tree level.
So far parallel treebanks have been created 
manually or semi-automatically. This has proven 
to be a laborious and time-consuming task that is 
prone to errors and inconsistencies (Samuelsson 
and Volk, 2007). Because of this, only a few paral-
lel treebanks exist and none are of sufficient size for 
productive use in any statistical MT application.
In this paper we present a novel platform for 
the automatic generation of parallel treebanks 
from parallel corpora and discuss several meth-
ods for the evaluation of the results. We discuss 
algorithms both for cases in which monolingual 
parsers exist for both languages and for cases in 
which such parsers are not available. The parallel 
treebanks created with the methods described in 
this paper can be used by different statistical MT 
applications and for translation studies.
We start in section 2 by introducing the tech-
niques for automatic generation of parallel tree-
banks. The evaluation methods and results are 
introduced in section 3 and in section 4 we give 
suggestions for possible improvements to the 
generation technology and to the evaluation algo-
rithms. Finally, in section 5 we present existing 
parallel treebanks and conclude in section 6.
2 Automatic Generation of
Parallel Treebanks
In this section we introduce a method for the auto-
matic generation of parallel treebanks from paral-
lel corpora. The only tool that is required besides 
the software presented in this paper is a word 
alignment tool. Such tools exist and some are freely 
available (eg. GIZA++ (Och and Ney, 2003)). If 
monolingual phrase-structure parsers1  or at least 
POS taggers exist for both languages, their use for 
pre-processing the data is highly recommended.
In all cases, a word alignment tool is used to 
first obtain word-alignment probabilities for the 
? 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.
1 Henceforth, we will use ?parser? to mean ?monolingual phrase-structure parser?, unless stated otherwise.
1105
parallel corpus in question for both language di-
rections. We will start with the description of the 
case in which parsers are available for both lan-
guages, as this is the core of the system. The 
parsers are used to parse both sides of the paral-
lel corpus. The resulting parsed data and word-
alignment probability tables are then used as the 
input to a sub-tree alignment algorithm that in-
troduces links between nodes in corresponding 
trees according to their translational equivalence 
scores. The output of the sub-tree aligner is the 
desired parallel treebank.
If there is no parser available for at least one 
of the languages, the parallel corpus ? together 
with the word-alignment tables ? is fed directly 
to a modified version of the sub-tree aligner. In 
this modification of the alignment algorithm, all 
possible binary phrase-structure trees are hy-
pothesised for each sentence in a sentence pair. 
Afterwards ? during the induction of alignments 
? only those tree nodes are left intact that take 
part in the alignments or are necessary for the 
production of connected trees. Thus, the output is 
again a parallel treebank with unambiguous 
phrase-structure trees for each language side.
In the present version of our software, if a 
parser or a POS tagger exists only for one of the 
languages in the parallel corpus you want to 
work with, they cannot be made use of. With the 
tree-to-tree and string-to-string modules in place, 
it is a minor task to add a tree-to-string and 
string-to-tree modules that will allow for the 
maximum utilisation of any available resources. 
We plan to start the development and evaluation 
of these new modules shortly.
We will now look at the currently available 
alignment algorithms in greater detail, starting 
with the tree-to-tree alignment and then moving 
on to the string-to-string case.
2.1 Tree-to-Tree Alignment
First, the tree-to-tree aligner has to follow certain 
principles to fit in the framework described above:
? Independence with respect to language pair, 
constituent-labelling scheme and POS tag 
set. Any language-dependence would re-
quire human input to adjust the aligner to a 
new language pair.
? Preservation of the original tree structures. 
We regard these structures as accurate en-
codings of the languages, and any change to 
them might distort the encoded information.
? Dependence on a minimal number of external 
resources, so that the aligner can be used even 
for languages with few available resources.
? The word-level alignments should be guided 
by links higher up the trees, where more 
context information is available.
These principles guarantee the usability of the 
algorithm for any language pair in many different 
contexts. Additionally, there are a few well-
formedness criteria that have to be followed to 
enforce feasible alignments:
? A node in a tree may only be linked once.
? Descendants / ancestors of a source linked 
node may only be linked to descendants / 
ancestors of its target linked counterpart.
Links produced according to these criteria en-
code enough information to allow the inference 
of complex translational patterns from a parallel 
treebank, including some idiosyncratic transla-
tional divergences, as discussed in (Hearne et al, 
2007). In what follows, a hypothesised alignment 
is regarded as incompatible with the existing 
alignments if it violates any of these criteria.
The sub-tree aligner operates on a per sentence-
pair basis and each sentence-pair is processed in 
two stages. First, for each possible hypothetical 
link between two nodes, a translational equiva-
lence score is calculated. Only the links for 
which a nonzero score is calculated are stored for 
further processing. Unary productions from the 
original trees, if available, are collapsed to single 
nodes, preserving all labels. Thus the aligner will 
consider a single node ? instead of several 
nodes ? for the same lexical span. This does not 
reduce the power of the aligner, as the transla-
tional equivalence scores are based on the sur-
face strings and not on the tree structures.
During the second stage, the optimal combina-
tion of links is selected from among the available 
nonzero links. The selection can be performed 
using either a greedy search, or a full search for 
the best combination.
Translational Equivalence
Given a tree pair S, T and a hypothesis s, t, we 
first compute the strings in (1), where si?six and 
tj?tjy denote the terminal sequences dominated 
by s and t respectively, and S1?Sm and T1?Tn 
denote the terminal sequences dominated by S and 
T. Here, inside are the strings that represent the 
spans of the nodes being linked and outside are the 
strings that lay outside the spans of those nodes.
1106
(1)
inside outside
sl = si?six sl = S1?si  1six + 1?Sm
tl = t j?t jy tl = T1?t j  1t jy + 1?Tn
(2)  s, t =  sl tl  tl sl  sl tl  tl sl
(3)  x y = P xi y jj
y
yi
x
The score for the given hypothesis s, t  is 
computed using (2) and (3). According to the 
formula in (3), the word-alignment probabilities 
are used to get an average vote by the source to-
kens for each target token. Then the product of 
the votes for the target words gives the alignment 
probability for the two strings. The final transla-
tional equivalence score is the product of the 
alignment probabilities for the inside and outside 
strings in both language directions as in (2).
Greedy-Search Algorithm
The greedy-search algorithm is very simple. The 
set of nonzero-scoring links is processed itera-
tively by linking the highest-scoring hypothesis 
at each iteration and discarding all hypotheses 
that are incompatible with it until the set is empty.
Problems arise when there happen to be several 
hypotheses that share the same highest score. There 
are two distinct cases that can be observed here: 
these top-scoring hypotheses may or may not repre-
sent incompatible links. If all such hypotheses are 
compatible, they are all linked at the same time and 
all remaining unprocessed hypotheses that are 
incompatible with any of those links are discarded. 
In case even one among the top-scoring hypothe-
ses is incompatible with the others, these hypothe-
ses are skipped and processed at a later stage.
The sub-tree aligner can be built to use one of 
two possible skipping strategies, which we will 
call skip1  and skip2. According to the skip1 strat-
egy, hypotheses are simply skipped until a score 
is reached, for which only one hypothesis exists. 
This hypothesis is then linked and the selection 
algorithm continues as usual.
The skip2 strategy is more complex, in that we 
also keep track of which nodes take part in the 
skipped hypotheses. Then, when a candidate for 
linking is found, it is only linked if it does not 
include any of these nodes. The motivation be-
hind this strategy is that a situation may occur in 
which a low-scoring hypothesis for a given con-
stituent is selected in the same iteration as 
higher-scoring hypotheses for the same constitu-
ent were skipped, thereby preventing one of the 
competing higher-scoring hypotheses from being 
selected and resulting in an undesired link.
Regardless of whether skip1 or skip2 is used, 
sometimes a situation occurs in which the only 
hypotheses remaining unprocessed are equally 
likely candidates for linking according to the se-
lection strategy. In such ambiguous cases our 
decision is not to link anything, rather than make 
a decision that might be wrong.
During initial testing of the aligner we found 
that often lexical links would get higher scores 
than the non-lexical links,2  which sometimes re-
sulted in poor lexical links preventing the selec-
tion of bona fide non-lexical ones. To address 
this issue, an extension to the selection algorithm 
was developed, which we call span1. When en-
abled, this extension results in the set of nonzero 
hypotheses being split in two subsets: one con-
taining all hypotheses for lexical links, and one 
containing the hypotheses for non-lexical links. 
Links are then first selected from the second sub-
set, and only when it is exhausted does the selec-
tion continue with the lexical one. This division 
does not affect the discarding of incompatible 
links after linking; incompatible links are dis-
carded in whichever set they are found.
Full-Search Algorithm
This is a backtracking recursive algorithm that 
enumerates all possible combinations of non-
crossing links. All maximal combinations3  found 
during the search are stored for further process-
ing. After the search is complete, the probability 
mass of each combination is calculated by sum-
ming the translational equivalence scores for all 
the links in the combination. The maximal com-
bination of non-crossing links that has the high-
est probability mass is selected as the best align-
ment for the sentence pair.
Often, there are several distinct maximal combi-
nations that share the highest probability mass; for 
longer sentences this number can rise to several 
hundred. The disambiguation strategy that we cur-
rently employ is to take the largest common subset 
of all maximal combinations. Another strategy 
would be to output all possible combinations and 
mark them as relating to the same sentence pair, 
thus leaving the disambiguation to the applica-
tion that uses the resulting parallel treebank.
2 lexical are such links, for which at least one of the linked nodes spans over only one word. All other links are non-lexical.
3 A maximal combination of non-crossing links is a combination of links for which any newly added link would be 
incompatible with at least one of the links already in the combination.
1107
2.2 String-to-String Alignment
The string-to-string aligner can accept as its in-
put plain or POS-tagged data. For a pair of sen-
tences, all possible binary trees are first con-
structed for each sentence. All nodes in these 
trees have the same label (X) and are then used as 
available link targets. In the case of POS-tagged 
data, the pre-terminal nodes receive the POS tags 
as labels. Here it is obvious that the number of 
links will be much higher than for the sub-tree 
alignment case, so the string-to-string aligner 
will operate much more slowly.
After all link-hypothesis scores have been cal-
culated, the string-to-string aligner continues 
with the selection of links in the same manner as 
the sub-tree aligner, with one extension; after a 
link has been selected ? besides all incompati-
ble links ? all binary trees that do not include 
the linked nodes are discarded with any nonzero 
hypotheses attached to them. In this way, only 
those binary trees that are compatible with the 
selected links remain after the linking process.
In an additional step for the string-to-string 
aligner, all non-linked nodes (except for the root 
nodes) are discarded, thus allowing for the construc-
tion of unambiguous n-ary trees for the source and 
target sentences. If necessary, non-linked nodes are 
left intact to provide supporting structure in the trees.
3 Evaluation and Results
The quality of a parallel treebank depends directly 
on the quality of the sub-tree alignments that it 
contains. Because of this, we use the evaluation 
results mainly as a metric for the improvements 
in the sub-tree aligner during development. Of 
course, the evaluation presented in this section 
also presents an insight into the usability of the 
parallel treebanks produced using our method.
For the evaluation of the aligner, a battery of in-
trinsic and extrinsic tests was developed. As a refer-
ence for the tests, a hand-crafted parallel treebank 
was used (HomeCentre (Hearne and Way, 2006)). 
This treebank consists of 810 English?French sen-
tence pairs. As discussed in section 5, we are not 
aware of an existing parallel treebank besides the 
HomeCentre that can be used directly for cross 
evaluation and comparison to versions automati-
cally generated using the sub-tree aligner.
The word-alignment probabilities required by 
our system were obtained by running the Moses 
decoder4 (Koehn et al, 2007) on the plain sentences 
from the HomeCentre in both language directions.
We will first describe the intrinsic testing and 
then go into the details of the extrinsic evaluation.
3.1 Intrinsic Evaluation
The intrinsic evaluation is performed by compar-
ing the links induced by the automatic aligner to 
the manually annotated links in the HomeCentre 
treebank. This evaluation can only be performed 
for the result of the tree-to-tree alignment, as the 
string-to-string alignment produces different 
trees. The metrics used for the comparison are 
precision and recall for all alignments and lexical 
and non-lexical alignments alone. The results of 
the evaluation are shown in Table 1.5
all links lexical links non-lexical links
Configura-
tions
skip1
skip2
skip1_span1
skip2_span1
preci-
sion
recall preci-
sion
recall preci-
sion
recall
61,29% 77,46% 51,06% 79,99% 80,75% 75,69%
61,54% 77,50% 51,29% 80,03% 80,75% 75,70%
61,56% 78,44% 51,53% 80,51% 78,67% 77,22%
61,79% 78,49% 51,76% 80,60% 78,73% 77,22%
Table 1. Intrinsic evaluation results
Looking first to the all links column, it is imme-
diately apparent that recall is significantly higher 
than precision for all configurations. In fact, all 
aligner variations consistently induce on average 
two more links than exist in the manual version. 
Considering the lexical links and non-lexical links 
columns, apparently the bulk of the automatically 
induced links that do not occur in the manual an-
notation are at the lexical level, as attested by the 
low precision at the lexical level and balanced 
precision and recall at the non-lexical level.
If the manual alignments in the HomeCentre 
are regarded as a gold standard, it would seem 
that fewer lexical links should be produced, 
while the quality of the non-lexical links needs 
improvement. We will try to judge whether this 
is really the case using the extrinsic evaluation 
techniques described below.
3.2 Extrinsic Evaluation
For extrinsic evaluation, we trained and tested a 
DOT system (Hearne and Way, 2006) using the 
manually aligned HomeCentre treebank and 
evaluated the output translations to acquire base-
line scores. We then trained the system on the 
automatically generated treebank and repeated 
4 We found that using the Moses word-alignment probabilities yielded better results than those output directly by GIZA++.
5 Throughout the paper we use boldface to highlight the best results and italics for the worst.
1108
the same tests, such that the only difference 
across runs are the alignments.
For testing, we used the six English?French 
training / test splits for the HomeCentre used in 
(Hearne and Way, 2006). Each test set contains 80 
test sentences and each training set contains 730 
tree pairs. We evaluated the translation output 
using three automatic evaluation metrics: BLEU 
(Papineni et al, 2002), NIST (Doddington, 2002) 
and METEOR (Banerjee and Lavie, 2005). We 
averaged the results over the six splits. We also 
measured test-data coverage of the translation sys-
tem, i.e. the percentage of test sentences for which 
full trees were generated during translation.
We performed this evaluation using both the 
tree-to-tree algorithm and the string-to-string 
algorithm, employing greedy-search selection. 
For the latter case we extracted POS-tagged sen-
tences from the HomeCentre and used them as 
input for the aligner. The results for the tree-to-
tree case are presented in Table 2 and for the 
string-to-string case in Table 3.
Configurations BLEU NIST METEOR Coverage
manual
skip1
skip2
skip1_span1
skip2_span1
0,5222 6,8931 71,8531% 68,5417%
0,5236 6,8412 72,2485% 72,0833%
0,5233 6,8617 72,2847% 71,8750%
0,5296 6,8570 72,9833% 72,0833%
0,5334 6,9210 72,9736% 71,8750%
Table 2. Tree-to-tree extrinsic evaluation
Let us first look at the results from the tree-to-
tree aligner. Overall, the scores obtained when 
using the manual alignments are very competitive 
with those derived using the manually aligned 
data. In fact, NIST is the only metric for which the 
performance is below the baseline. An important 
observation is that the coverage of the translation 
system is up to 3.5% higher when using the auto-
matic alignments. Another observation is that skip2 
leads to better performance on the NIST metric 
over skip1, but the results from the other metrics are 
not so conclusive. The use of span1 leads to better 
translation scores. Ths results seem to point at 
the skip1_span1 and skip2_span1 configurations 
as the best-suited for further development.
Unexpectedly, the results of the extrinsic 
evaluation do not strictly follow the trends found 
in the intrinsic evaluation. Further analysis of the 
data revealed that direct comparison of the man-
ual and automatic alignments is not appropriate, 
especially regarding the lexical alignments. The 
manual alignments were produced with the aim 
of maximising precision, but the coverage-based 
automatic alignments lead to higher translation 
scores. This is the result of having many fewer 
manual word-alignments than automatic ones, as 
the low precision scores in the intrinsic evalua-
tion show. From this we conclude that the im-
provement of the automatic aligner should not be 
aimed at better matching the manual alignments, 
but rather at improving the quality of the transla-
tions produced using the automatic alignments.
Configurations BLEU NIST METEOR Coverage
manual
skip1
skip2
skip1_span1
skip2_span1
0,5222 6,8931 71,8531% 68,5417%
0,4939 6,6321 72,5192% 92,5000%
0,4886 6,5777 72,8241% 92,2917%
0,4661 6,3090 73,1017% 92,2917%
0,4683 6,3353 73,2828% 92,2917%
Table 3. String-to-string extrinsic evaluation
If we now look at the evaluation of the string-
to-string aligner, we see quite peculiar results. 
There is more than 20% increase in coverage 
compared to the tree-to-tree aligner, but the only 
other metric that sees improvement ? albeit 
modest ? is METEOR. It is also the only metric 
that follows the trends observed in the tree-to-
tree evaluation results. Not only are the results 
for the BLEU and NIST metrics lower, but they 
also seem to follow reversed trends. It is unclear 
what the reason for such an outcome is, and fur-
ther investigation ? including on other data sets 
? is needed. Still, as far as the METEOR metric 
is concerned, the use of the string-to-string algo-
rithm for the generation of parallel treebanks 
seems to be warranted.
The results obtained from the intrinsic and ex-
trinsic evaluations show that the methods de-
scribed in this paper produce high quality parallel 
treebanks. Using the automatically generated tree-
banks, a DOT system produces results with simi-
lar translation quality and better coverage com-
pared to its performance using manually aligned 
data. This makes our methods a good alternative 
to the manual construction of parallel treebanks.
3.3 Using the Full-Search Algorithm
as an Evaluation Metric
The full-search selection algorithm is combinato-
rial in nature and for sentence pairs with more 
than 100 nonzero link hypotheses its time re-
quirements become prohibitive. Still, this algo-
rithm can be used in its current form for devel-
opment purposes.
It is reasonable to ask whether the greedy-search 
algorithm produces the best set of alignments for 
a given sentence pair. It could be that it picks a 
local maximum differing greatly from the absolute 
maximal set of alignments, thus producing either 
low quality links or a small number of links.
1109
The full-search selection algorithm can be used 
to test the performance of the greedy search, as it by 
definition produces the best available set of align-
ments. We decided to use the rate of coincidence 
between the alignments induced using both selec-
tion algorithms as a metric for the quality of the 
links derived using the greedy search: the higher the 
number of cases in which the greedy-search algo-
rithm matches the result of the full-search algo-
rithm, the better the quality of the greedy search.
We ran this coincidence evaluation for all four 
configurations of the aligner. The results are pre-
sented in Table 4. It should be noted that 30 sen-
tence pairs from the HomeCentre could not be 
handled by the full-search algorithm within a 
reasonable timeframe and were skipped.
all links lexical links non-lexical links
Configura-
tions
skip1
skip2
skip1_span1
skip2_span1
preci-
sion
recall preci-
sion
recall preci-
sion
recall
98,71% 99,18% 98,36% 99,14% 99,57% 99,21%
99,23% 99,21% 99,06% 99,17% 99,57% 99,23%
95.78% 97,00% 95.92% 96,33% 95.19% 99,21%
96,27% 97,09% 96,58% 96,44% 95,25% 99,21%
Table 4. Evaluation against full-search results
The outcome of this test seems to be unex-
pected and a little disconcerting in view of the 
results obtained from the extrinsic evaluation. It 
does not seem reasonable that the configurations 
including span1 should obtain scores that are 
relatively much worse than the scores for the 
other configurations, when we saw them perform 
better at the extrinsic evaluation tests.
The reason for this discrepancy might not be 
obvious, but it is fairly simple and lies in the na-
ture of the span1 extension. As discussed in sec-
tion 2.1, span1 introduces a separation in the in-
duction of lexical and non-lexical links. The full-
search algorithm, however, derives the maximal 
link set from a common pool of all nonzero 
alignment hypotheses. This suggests that an ex-
tension to the full-search algorithm similar to 
span1 should be developed to allow for the 
evaluation of configurations using this feature.
Nevertheless, this evaluation shows some very 
important results. Besides the fact that configura-
tions using skip2 perform slightly better than 
those using skip1, we see that the greedy search 
comes very close to the best maximal link set. 
Our tests show that in over 95% of the cases the 
greedy search finds the best maximal link set 
available for the particular sentence pair.
The results are very encouraging and show 
that the fast greedy-search algorithm produces 
the desired results and there is no need to use the 
prohibitively slow full-search algorithm, except 
for comparison purposes.
4 A Review of Possible Enhancements
Here, we discuss possible avenues for the im-
provement of the quality of the parallel treebanks 
produced using the methods presented in this paper.
As already stated in section 3, the quality of a 
parallel treebank is to be judged by the quality of 
the induced sub-tree alignments. Thus, all effort 
should be directed at producing better alignments. 
There are two possible ways to address this: one 
option is to work on improving the alignment 
algorithm, and the other option is to improve the 
scoring mechanism used by the aligner.
Improvements to the alignment algorithm can 
be evaluated against the full-search selection al-
gorithm. The evaluation results from section 3.3 
suggest, however, that the margin for improve-
ment here is very small. Thus, we do not expect any 
improvements here to bring serious boosts in over-
all performance. Nevertheless, we plan to investi-
gate one possible modification to the greedy search.
It can be argued that each newly induced link in 
a sentence pair should affect the decisions regard-
ing which links to select further in the alignment 
process for this sentence pair. This can be simulated 
to a certain extent by the introduction of a simple 
re-scoring module to the aligner. Each time a new 
link has been selected, this module will be used to 
recalculate the scores of the remaining links, con-
sidering the restrictions on the possible word-level 
alignments introduced by this link, e.g. that words 
within the spans of the nodes being linked cannot 
be aligned to words outside those spans.
The effects of changes to the scoring mecha-
nism used can only be evaluated using extrinsic 
methods, as such changes also influence the op-
eration of the full-search selection. On this front, 
we plan to investigate a maximum-entropy-based 
scoring mechanism. We expect such a mecha-
nism to better encode mathematically the de-
pendence of the translational equivalence scores 
on the word-alignment probabilities.
Besides the improvements to the sub-tree 
aligner, we plan to extend the whole generation 
framework with two additional modules: for 
string-to-tree and tree-to-string alignment. This 
would allow for better utilisation of all available 
resources for the derivation of a parallel treebank 
from a parallel corpus.
We also plan to perform large-scale extrinsic 
evaluation experiments. Though the evaluation re-
sults presented in section 3 are very promising, they 
1110
were performed on a very small set of data. (John 
Tinsley (p.c.) reports successfully deriving a paral-
lel treebank with over 700 000 sentence-pairs using 
our software.) Further experiments on larger data 
sets ? from different languages, as well as from 
different domains ? should help better understand 
the real qualities of the methods presented here.
5 Existing Parallel Treebanks
In this section we look at several attempts at the 
creation of parallel treebanks besides the Home-
Centre treebank presented earlier.
Closest to the material presented in this paper 
comes the parallel treebank presented in (Sam-
uelsson and Volk, 2006). This manually created 
treebank aligns three languages ? German, Eng-
lish and Swedish ? consisting of over 1000 sen-
tences from each language. The main difference 
compared to our method is that they allow many-
to-many lexical alignments and one-to-many non-
lexical alignments. The authors also allow unary 
productions in the trees, which, as stated in section 
2.1, does not provide any additional useful infor-
mation. Another difference is that they deepen the 
original German and Swedish trees before align-
ment, rather than preserve their original form.
A further attempt to align phrase-structure 
trees is presented in (Uibo et al, 2005). The 
authors develop a rule-based method for aligning 
Estonian and German sentences. The parallel 
treebank consist of over 500 sentences, but in the 
version presented only NPs are aligned.
In (Han et al, 2002) the authors claim to have built 
a Korean?English parallel treebank with over 5000 
phrase-structure tree pairs, but at the time of writing 
we were unable to find details about this treebank.
Although the Prague Czech?English Depend-
ency Treebank (PCEDT (mejrek et al, 2004)) can 
be used as a parallel treebank, it is not such per se. 
The authors do not use phrase-structure trees. In-
stead, tectogrammatical dependency structures are 
used (Hajiov?, 2000). Either a word alignment 
tool like GIZA++ or a probabilistic electronic dic-
tionary (supplied with the treebank) can be used to 
automatically align the dependency structures. The 
presented version contains over 21000 sentence 
pairs that can be aligned. Because of its nature, this 
treebank can only be used by MT systems that em-
ploy tectogrammatical dependency structures.
We are also aware of the existence of the LinES 
(Ahrenberg, 2007), CroCo (Hansen-Schirra et al, 
2006) and FuSe (Cyrus, 2006) parallel corpora. 
Although it seems possible to use them as parallel 
treebanks, they have been designed to serve as 
resources for the study of translational phenomena 
and it does not appear that they can be used effec-
tively for other natural language processing tasks.
An attempt to develop an automatic tree-to-
tree aligner is described in (Groves et al, 2004). 
The authors present a promising rule-based sys-
tem. Further testing, however, has shown that the 
rules are only applicable to a particular treebank 
and language pair. This means that the set of 
rules has to be adjusted for each particular case.
Thus, the methods presented in this paper are 
the only available ones that can be used to pro-
duce a sufficiently large parallel treebank appro-
priate for use by state-of-the-art statistical MT 
applications (eg. DOT (Hearne and Way, 2006)).6
6 Conclusions
We have presented a novel platform for the fast 
and robust automatic generation of parallel tree-
banks. The algorithms described are completely 
language-pair-independent and require a minimal 
number of resources; besides a parallel corpus, a 
word alignment tool is the only extra software 
required. If available, POS taggers or monolin-
gual phrase-structure parsers can be used to pre-
process the data. Certain extensions to the cur-
rent software are planned that will assure the op-
timal use of any available resources.
A series of evaluations have shown promising 
results. The quality of the automatically generated 
parallel treebanks is very high, even improving on 
a manually created treebank on certain metrics. 
We plan to carry out extensive large-scale testing 
on a range of language pairs, which we expect to 
corroborate the results reported in this paper. The 
planned improvements to the algorithms discussed 
in section 4 are expected to further increase the 
quality of the generated parallel treebanks.
Currently existing treebanks are small and re-
quire extensive human resources to be created and 
extended, which has limited their use for data-
oriented tasks. The platform presented in this pa-
per provides a means to circumvent these prob-
lems by allowing for the fast automatic genera-
tion of very large parallel treebanks with very little 
human effort, thus overcoming this hurdle for 
research in tree-based machine translation.
6 An alternative methodology is described in (Lavie et al, to appear), but this work was not available at the time of writing.
1111
Acknowledgements
We would like to thank Khalil Sima'an, Mary 
Hearne and John Tinsley for many insightful dis-
cussions. This work was generously supported by 
Science Foundation Ireland (grant no. 05/RF/
CMS064) and the Irish Centre for High-End 
Computing (http://www.ichec.ie).
References
Ahrenberg, Lars. 2007. LinES: An English-Swedish 
Parallel Treebank. In Proceedings of the 16th 
Nordic Conference of Computational Linguistics 
(NODALIDA ?07), pp. 270?274. Tartu, Estonia.
Banerjee, Satanjeev and Alon Lavie. 2005. 
METEOR: An Automatic Metric for MT 
Evaluation with Improved Correlation with 
Human Judgements. In Proceedings of the 
Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization 
at the 43rd Annual Meeting of the Association 
for Computational Linguistics (ACL ?05), 
pp. 65?72. Ann Arbor, MI.
mejrek, Martin, Jan Cu
?n, Ji
? Havelka, Jan 
Haji and Vladislav Kubo	. 2004. Prague 
Czech-English Dependency Treebank: Syntac-
tically Annotated Resources for Machine 
Translation. In Proceedings of the 4th Interna-
tional Conference on Language Resources and 
Evaluation (LREC?04). Lisbon, Portugal.
Cyrus, Lea. 2006. Building a resource for studying 
translation shifts. In Proceedings of the 5th Con-
ference of Language Resources and Evaluation 
(LREC ?06), pp.1240?1245. Genoa, Italy.
Doddington, George. 2002. Automatic Evaluation 
of Machine Translation Quality Using N-Gram 
Co-Occurrence Statistics. In Proceedings of the 
ARPA Workshop on Human Language Technol-
ogy, pp. 128?132. San-Diego, CA.
Groves, Declan, Mary Hearne and Andy Way. 
2004. Robust Sub-Sentential Alignment of 
Phrase-Structure Trees. In Proceedings of the 
20th International Conference on Computa-
tional Linguistics (CoLing?04), pp. 1072?1078. 
Geneva, Switzerland: COLING.
Hajiov?, Eva. 2000. Dependency-Based Underlying-
Structure Tagging of a Very Large Czech Corpus. 
TAL (Special Issue Grammaires de D?pendance / 
Dependency Grammars), 41 (1): 47?66.
Han, Chung-hye, Na-Rare Han, Eon-Suk Ko and 
Martha Palmer. 2002. Development and 
Evaluation of a Korean Treebank and its Appli-
cation to NLP. In Proceedings of the 3rd Inter-
national Conference on Language Resources 
and Evaluation (LREC  ?02), pp. 1635?1642. 
Las Palmas, Canary Islands, Spain.
Hansen-Schirra, Silvia, Stella Neumann and Mi-
haela Vela. 2006. Multi-dimensional Annota-
tion and Alignment in an English-German 
Translation Corpus. In Proceedings of the 
workshop on Multi-dimensional Markup in 
Natural Language Processing (NLPXML ?06), 
pp. 35?42. Trento, Italy.
Hearne, Mary and Andy Way. 2006. Disambigua-
tion Strategies for Data-Oriented Translation. 
In Proceedings of the 11th Conference of the 
European Association for Machine Translation 
(EAMT?06), pp. 59?68. Oslo, Norway.
Hearne, Mary, John Tinsley, Ventsislav Zhechev and 
Andy Way. 2007. Capturing Translational Diver-
gences with a Statistical Tree-to-Tree Aligner. In 
Proceedings of the 11th International Conference 
on Theoretical and Methodological Issues in Ma-
chine Translation (TMI ?07), pp. 85?94. Sk?vde, 
Sweden: Sk?vde University Studies in Informatics.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Mo-
ran, Richard Zens, Chris Dyer, Ond
ej Bojar, Al-
exandra Constantin and Evan Herbst. 2007. 
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the Demo 
and Poster Sessions of the 45th Annual Meeting of 
the Association for Computational Linguistics 
(ACL ?07), pp. 177?180. Prague, Czech Republic.
Lavie, Alon, Alok Parlikar and Vamshi Ambati. to 
appear. Syntax-driven Learning of Sub-sentential 
Translation Equivalents and Translation Rules 
from Parsed Parallel Corpora. In Proceedings of 
the 2nd Workshop on Syntax and Structure in Sta-
tistical Translation (SSST?08). Columbus, OH.
Nesson, Rebecca, Stuart M. Shieber and Alexander 
Rush. 2006. Induction of Probabilistic Synchro-
nous Tree-Insertion Grammars for Machine 
Translation. In Proceedings of the 7th Conference 
of the Association for Machine Translation in the 
Americas (AMTA?06), pp. 128?137. Boston, MA.
Och, Franz Josef and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29 (1): 19?51.
Papineni, Kishore, Salim Roukos, Todd Ward 
and Wei-Jing Zhu. 2002. BLEU: A Method for 
Automatic Evaluation of Machine Translation. 
In Proceedings of the 40th Annual Meeting of 
the Association of Computational Linguistics 
(ACL ?02), pp. 311?318. Philadelphia, PA.
Samuelsson, Yvonne and Martin Volk. 2006. 
Phrase Alignment in Parallel Treebanks. In 
Proceedings of the 5th Workshop on Treebanks 
and Linguistic Theories (TLT ?06), pp. 91?102. 
Prague, Czech Republic.
Samuelsson, Yvonne and Martin Volk. 2007. 
Alignment Tools for Parallel Treebanks. In 
Proceedings of the GLDV Fr?hjahrstaggung. 
T?bingen, Germany.
Uibo, Heli, Krista Liin and Martin Volk. 2005. Phrase 
alignment of Estonian-German parallel treebanks. 
Paper presented at Workshop ?Exploiting parallel 
corpora in up to 20 languages?, Arona, Italy.
1112
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 371?380,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Accuracy-Based Scoring for DOT: Towards Direct Error Minimization for
Data-Oriented Translation
Daniel Galron
CIMS
New York University
galron@cs.nyu.edu
Sergio Penkale, Andy Way
CNGL
Dublin City University
{spenkale,away}
@computing.dcu.ie
I. Dan Melamed
AT&T Shannon Laboratory
{lastname}
@research.att.com
Abstract
In this work we present a novel technique
to rescore fragments in the Data-Oriented
Translation model based on their contri-
bution to translation accuracy. We de-
scribe three new rescoring methods, and
present the initial results of a pilot experi-
ment on a small subset of the Europarl cor-
pus. This work is a proof-of-concept, and
is the first step in directly optimizing trans-
lation decisions solely on the hypothesized
accuracy of potential translations resulting
from those decisions.
1 Introduction
The Data-Oriented Translation (DOT) (Poutsma,
2000) model is a tree-structured translation model,
in which linked subtree fragments extracted from
a parsed bitext are composed to cover a source-
language sentence to be translated. Each linked
fragment pair consists of a source-language side
and a target-language side, similar to (Wu, 1997).
Translating a new sentence involves composing
the linked fragments into derivations so that a
new source-language sentence is covered by the
source tree fragments of the linked pairs, where
the yields of the target-side derivations are the can-
didate translations. Derivations are scored accord-
ing to their likelihood, and the translation is se-
lected from the derivation pair with the highest
score. However, we have no reason to believe that
maximizing likelihood is the best way to maxi-
mize translation accuracy ? likelihood and accu-
racy do not necessarily correlate well.
We can frame the problem as a search problem,
where we are searching a space of derivations for
the one that yields the highest scoring translation.
By putting weights on the derivations in the search
space, we wish to point the decoder in the direc-
tion of the optimal translation. Since we want
the decoder to find the translation with the high-
est evaluation score, we would want to score the
derivations with weights that correlate well with
the particular evaluation measure in mind.
Much of the work in the MT literature has
focused on the scoring of translation decisions
made. (Yamada and Knight, 2001) follow (Brown
et al, 1993) in using the noisy channel model,
by decomposing the translation decisions mod-
eled by the translation model into different types,
and inducing probability distributions via max-
imum likelihood estimation over each decision
type. This model is then decoded as described
in (Yamada and Knight, 2002). This type of ap-
proach is also followed in (Galley et al, 2006).
There has been some previous work on
accuracy-driven training techniques for SMT, such
as MERT (Och, 2003) and the Simplex Armijo
Downhill method (Zhao and Chen, 2009), which
tune the parameters in a linear combination of var-
ious phrase scores according to a held-out tun-
ing set. While this does tune the relative weights
of the scores to maximize the accuracy of candi-
dates in the tuning set, the scores themselves in the
linear combination are not necessarily correlated
with the accuracy of the translation. Tillmann and
Zhang (2006) present a procedure to directly opti-
mize the global scoring function used by a phrase-
based decoder on the accuracy of the translations.
Similarly to MERT, Tillmann and Zhang estimate
the parameters of a weight vector on a linear com-
bination of (binary) features using a global objec-
tive function correlated with BLEU (Papineni et
al., 2002).
In this work, we prototype some methods for
moving directly towards incorporating a measure
of the translation quality of each fragment used,
bringing DOT more into the mainstream of cur-
rent SMT research. In Section 2 we describe
probability-based DOT fragment scoring. In Sec-
tion 3 we describe our rescoring setup and the
371
(a)
S
NP VP
V
likes
NP
S
NP VP
V
pla??t
PP
P
a`
NP
(b)
NP
John
NP
John
(c)
S
NP
John
VP
V
likes
NP
S
NP VP
V
pla??t
PP
P
a`
NP
John
(d)
NP
Mary
NP
Mary
Figure 1: Example DOT Fragments.
three rescoring methods. In Section 4, we describe
our experiments. In Section 5 we compare the
results of rescoring the fragments with the three
methods. In Section 6 we discuss some of the
decisions that are affected by our rescoring meth-
ods. Finally, we discuss the next steps in training
the DOT system by optimizing over a translation
accuracy-based objective function in Section 7.
2 DOT Scoring
As described in previous work (Poutsma, 2000;
Hearne and Way, 2003), DOT scores translations
according to the probabilities of the derivations,
which are in turn computed from the relative fre-
quencies of linked tree fragments in a parallel tree-
bank. Linked fragment pairs are conditionally in-
dependent, so the score of a derivation is the prod-
uct of the probabilities of all the linked fragments
used. To find the probability of a translation,
DOT marginalizes over the scores of all deriva-
tions yielding the translation.
From a parallel treebank aligned at the sub-
sentential level, we extract all possible linked frag-
ment pairs by first selecting all linked pairs of
nodes in the treebank to be the roots of a new sub-
tree pair, and then selecting a (possibly empty) set
of linked node pairs that are descendants of the
newly selected fragment roots and deleting all sub-
tree pairs dominated by these nodes. Leaves of
fragments can either be terminals, or non-terminal
frontier nodes where we can compose other frag-
ments (c.f. (Eisner, 2003)). We give example DOT
fragment pairs in Figure 1.
Given two subtree pairs ?s
1
, t
1
? and ?s
2
, t
2
?,
we can compose them using the DOT composi-
tion operator ? if the leftmost non-terminal fron-
tier node of s
1
is equal to the root node of s
2
,
and the leftmost non-terminal frontier node of s
1
?s
linked counterpart in t
1
is equal to the root node
of t
2
. The resulting tree pair consists of a copy
of s
1
where s
2
has been inserted at the leftmost
frontier node, and a copy of t
1
where t
2
has been
inserted at the node linked to s
1
?s leftmost frontier
node (Hearne and Way, 2003).
In Figure 1, fragment pair (a) is a fragment with
two open substitution sites. If we compose this
fragment pair with fragment pair (b), the source
side composition must take place on the leftmost
non-terminal frontier node (the leftmost NP). On
the target side we compose on the frontier linked
to the leftmost source side non-terminal frontier.
The result is fragment pair (c). If we now com-
pose the resulting fragment pair with fragment pair
(d), we obtain a fragment pair with no open sub-
stitution sites whose source-side yield is John likes
Mary and whose target-side yield is Mary pla??t a`
John. Note that there are two different derivations
using the fragment pairs in Figure 1 that result in
the same fragment pair, namely (a) ? (b) ? (d), and
(c) ? (d).
For a given linked fragment pair ?d
s
, d
t
?, the
probability assigned to it is
P (?d
s
, d
t
?) =
|?d
s
, d
t
?|
?
r(u
s
)=r(d
s
)?r(u
t
)=r(d
t
)
|?u
s
, u
t
?|
(1)
where |?d
s
, d
t
?| is the number of times the frag-
ment pair ?d
s
, d
t
? is found in the bitext, and r(d)
is the root nonterminal of d. Essentially, the prob-
ability assigned to the fragment pair is the relative
frequency of the fragment pair to the pair of non-
terminals that root the fragments.
Then, with the assumption that DOT fragments
are conditionally independent, the probability of a
derivation is
P (d) = P (?d
s
, d
t
?
1
? . . . ? ?d
s
, d
t
?
N
)
=
?
i
P (?d
s
, d
t
?
i
) (2)
In the original DOT formulation, DOT disam-
biguated translations according to their probabil-
ities. Since a translation can have many possible
derivations, to obtain the probability of a transla-
tion it is necessary to marginalize over the distinct
derivations yielding a translation. The probabil-
ity of a translation w
t
of a source sentence w
s
, is
372
given by (3):
P (w
s
, w
t
) =
?
d?D
P (d
?w
s
,w
t
?
) (3)
and the translation is chosen so as to maximize (4):
w?
t
= argmax
w
t
P (w
s
, w
t
) (4)
Hearne and Way (2006) examined alternative dis-
ambiguation strategies. They found that rather
than disambiguating on the translation probability,
the translation quality would improve by disam-
biguating on the derivation probability, as in (5):
w?
t
= argmax
d
P (d) (5)
Our analysis suggest that this is because many
derivations with very low probabilities generate
the same, poor translation. When applying Equa-
tion (3) to marginalize over those derivations, the
resulting score is higher for the poor translation
than a better translation with fewer derivations but
where the derivations had higher likelihood.
Using the DOT model directly is difficult ?
the number of fragments extracted from a paral-
lel treebank is exponential in the size of the tree-
bank. Therefore we use the Goodman reduction
of DOT (Hearne, 2005) to create an isomorphic
PCFG representation of the DOT model that is lin-
ear in the size of the treebank. The idea behind the
Goodman reduction is that rather than storing frag-
ments in the grammar and translating via compo-
sition, we simultaneously build up the fragments
using the PCFG reduction and compose them to-
gether. To perform the reduction, we first relabel
the two linked nodes (X, Y) with the new label
X=Y. We then label each node in the parallel tree-
bank with a unique Goodman index. Each binary-
branching node and its two children can be inter-
nal or root/frontier. We add rules to the grammar
reflecting the role that each node can take, keeping
unaligned nodes as fragment-internal nodes. So in
the case where a node and both of its children are
aligned, we commit 8 rules into the grammar, as
follows:
LHS ? RHS1 RHS2 LHS+a ? RHS1 RHS2
LHS ? RHS1+b RHS2 LHS+a ? RHS1+b RHS2
LHS ? RHS1 RHS2+c LHS+a ? RHS1 RHS2+c
LHS ? RHS1+b RHS+c LHS+a ? RHS1+b RHS2+c
A category label which ends in a ?+? symbol fol-
lowed by a Goodman index is fragment-internal
and all other nodes are either fragment roots or
S=S
1
N=N
3
John
VP
2
V
4
likes
N=N
5
Mary
S=S
1
N=N
4
Mary
VP
2
V
5
pla??t
PP
3
P
6
a`
N=N
7
John
Source PCFG Target PCFG
S=S? N=N VP+2 0.5 S=S? N=N VP+2 0.5
S=S? N=N+3 VP+2 0.5 S=S? N=N+4 VP+2 0.5
S=S+1? N=N VP+2 0.5 S=S+1? N=N VP+2 0.5
S=S+1? N=N+3 VP+2 0.5 S=S+1? N=N+4 VP+2 0.5
N=N? John 0.5 N=N?Mary 0.5
N=N+3? John 1 N=N+4?Mary 1
VP+2? V+4 N=N 0.5 VP+2? V+5 PP+3 1
VP+2? V+4 N=N+5 0.5 V+5? pla??t 1
V+4? likes 1 PP+3? P+6 N=N 0.5
N=N?Mary 0.5 PP+3? P+6 N=N+7 0.5
N=N+5?Mary 1 P+6? a` 1
N=N? John 0.5
N=N+7? John 1
Figure 2: A parallel tree and its corresponding Goodman re-
duction.
frontier nodes. A fragment pair, then, is a pair of
subtrees in which the root does not have an index,
all internal nodes have indices, and all the leaves
are either terminals or un-indexed nodes. We give
an example Goodman reduction in Figure 2.
While we store the source grammar and the tar-
get grammar separately, we also keep track of the
correspondence between source and target Good-
man indices and can easily identify the alignments
according to the Goodman indices. Probabilities
for the PCFG rules are computed monolingually
as in the standard Goodman reduction for DOP
(Goodman, 1996). In decoding with the Goodman
reduction, we first find the n-best parses on the
source side, and for each source fragment, we con-
struct the k-best fragments on the target side. We
finally compute the bilingual derivation probabil-
ities by multiplying the source and target deriva-
tion probabilities by the target fragment relative
frequencies conditioned on the source fragment.
There are a few problems with a likelihood-
based scoring scheme. First, it is not clear that
if a fragment is more likely to be seen in training
data then it is more likely to be used in a correct
translation of an unseen sentence. In our analysis
of the candidate translations of the DOT system,
we observed that frequently, the highest-likelihood
candidate translation output by the system was not
the highest-accuracy candidate inferred. An addi-
tional problem is that, as described in (Johnson,
2002), the relative frequency estimator for DOP
373
(and by extension, DOT) is known to be biased
and inconsistent.
3 Accuracy-Based Fragment Scoring
In our work, we wish to incorporate a measure
of fragment accuracy into the scoring. To do so,
we reformulate the scoring of DOT as log-linear
rather than probabilistic, in order to incorporate
non-likelihood features into the derivation scores.
For all tree fragment pairs ?d
s
, d
t
?, let
l(?d
s
, d
t
?) = log(p(?d
s
, d
t
?)) (6)
The general form of a rescored tree fragment will
be
s(?d
s
, d
t
?) = ?
0
l(?d
s
, d
t
?) +
k
?
i=1
?
i
f
i
(?d
s
, d
t
?)
(7)
where each ?
i
is the weight of that term in the fi-
nal score, and each f
i
(d) is a feature. In this work,
we only consider f
1
(d), an accuracy-based score,
although in future work we will consider a wide
variety of features in the scoring function, includ-
ing combinations of the different scoring schemes
described below, binary lexical features, binary
source-side syntactic features, and local target side
features. The score of a derivation is now given by
(8):
s(d) = s(?d
s
, d
t
?
1
? . . . ? ?d
s
, d
t
?
N
)
=
?
i
s(?d
s
, d
t
?
i
) (8)
In order to disambiguate between candidate
translations, we follow (Hearne and Way, 2006)
by using Equation (5).
3.1 Structured Fragment Rescoring
In all our approaches, we rescore fragments ac-
cording to their contribution to the accuracy of
a translation. We would like to give fragments
that contribute to good translations relatively high
scores, and give fragments that contribute to bad
translations relatively low scores, so that during
decoding fragments that are known to contribute to
good translations would be chosen over those that
are known to contribute to bad translations. Fur-
thermore, we would like to score each fragment in
a derivation independently, since bad translations
may contain good fragments, and vice-versa.
In practice, it is infeasible to rescore only those
fragments seen during the rescoring process, due
to the Goodman reduction for DOT. If we were to
properly rescore each fragment, a new rule would
need to be added to the grammar for each rule ap-
pearing in the fragment. Since the number of frag-
ments is exponential, this would lead to a substan-
tial increase in grammar size. Instead, we rescore
the individual rules in the fragments, by evenly di-
viding the total amount of scoring mass among the
rules of the particular fragment, and then assigning
them the average of the rule scores over all frag-
ments in which they appear. That is for each rule
r in a fragment f consisting of c
f
(r) rules with
score ?(f), the score of the rule is given as:
s(r) =
?
f :r?f
?(f)/c
f
(r)
|f |
(11)
This has the further advantage that we are al-
lowing fragments that were unseen during tuning
to be rescored according to previously seen frag-
ment substructures.
To implement this scheme, we select a set of or-
acle translations for each sentence in the tuning
data by evaluating all the candidate translations
against the gold standard translation using the F-
score (Turian et al, 2003), and selecting those
with the highest F
1
-measure, with exponent 1. We
use GTM, rather than BLEU, because BLEU is
not known to work well on a per-sentence level
(Lavie et al, 2004) as needed for oracle selection.
We then compare all the target-side fragments in-
ferred in the translation process for each candidate
translation against the fragments that yielded the
oracles. There are two relevant parts of the frag-
ments ? the internal yields (i.e. the terminal leaves
of the fragment) and the substitution sites (i.e. the
frontiers where other fragments attach). We score
the fragments rooted at the substitution sites sepa-
rately from the parent fragment. We can uniquely
identify the set of fragments that can be rooted at
substitution sites by determining the span of the
linked source-side derivation.
To compare two fragments, we define an edit
distance between them. For a given fragment d,
let r(d) be the root of the fragment, let r(d) ?
rhs1 be the left subtree of r(d), and let r(d) ?
rhs2 be the right subtree. The difference between
a candidate fragment d
c
and an oracle fragment
d
gs
is given by the equations in Table 1.
These equations define a minimum edit dis-
tance between two fragment trees, allowing sub-
fragment order inversion, insertion, and deletion
374
?(d
c
, d
gs
) =
(
0 if d
c
= d
gs
1 if d
c
6= d
gs
Base case: d
c
and d
gs
are unary subtrees or substitution sites (9)
?(d
c
, d
gs
) = min
8
>
>
>
>
<
>
>
>
>
:
?(d
c
? rhs1, d
gs
? rhs1) + ?(d
c
? rhs2, d
gs
? rhs2),
?(d
c
? rhs2, d
gs
? rhs1) + ?(d
c
? rhs1, d
gs
? rhs2) + 1,
?(d
c
, d
gs
? rhs1) + |y(d
gs
? rhs2)|,
?(d
c
, d
gs
? rhs2) + |y(d
gs
? rhs1)|,
?(d
c
? rhs1, d
gs
) + |y(d
c
? rhs2)|,
?(d
c
? rhs2, d
gs
) + |y(d
c
? rhs1)|
(10)
Table 1: The recursive relation defining the fragment difference between two fragments.
(a) A
B
b
C
c
(b) A
C
c
B
b
(c) D
A
B
b
F
f
E
e
Figure 3: Comparing trees (a) and (b) with our distance met-
ric yields a value of 1. The difference between trees (a) and
(c) is 2, and for trees (b) and (c) the distance is 3.
as edit operations. For example, the only dif-
ference between trees (a) and (b) in Figure 3 is
that their children have been inverted. To com-
pare these trees using our distance metric, we first
compute the first argument of the min function in
Equation (10), directly comparing the structure of
each immediate subtree. We then compute the sec-
ond argument, obtaining the cost of performing an
inversion, and finally compute the remaining argu-
ments, assessing the cost of allowing each tree to
be a direct subtree of the other. The result of this
computation is 1, representing the inversion oper-
ation required to transform tree (a) into tree (b).
If we compare trees (a) and (c) in Figure 3, we
obtain a value of 2, given that the minimum opera-
tions required to transform tree (a) into tree (c) are
inserting an additional subtree at the top level and
then substituting the subtree rooted by C for the
subtree rooted by F. If we compare tree (b) with
tree (c) then the distance is 3, since we are now
required to also replace the subtree rooted by C by
the one rooted by B.
Since it is not efficient to compute the differ-
ences directly, we utilize common substructures
and derive a dynamic programming implementa-
tion of the recursion. We compare each fragment
against the set of oracle fragments for the same
source span, and select the lowest cost as the score,
assigning the candidate the negative difference be-
tween it and the oracle fragment it is most similar
to, as in (12):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
??(d
t
, d
o
t
) (12)
In practice, given the Goodman reduction for
DOT, we divide the fragment score by the number
of rules in the fragment, and assign the average of
those scores for each rule instance across all frag-
ments rescored.
3.2 Normalized Structured Fragment
Rescoring
In the structured fragment rescoring scheme, the
scores that the fragments are assigned are the un-
normalized edit distances between the two frag-
ments. It may be better to normalize the fragment
scores, rather than using the minimum number of
tree transformations to convert one fragment into
the other. We would expect that when compar-
ing larger fragments, on average there would be
more transformations needed to change one into
the other than when comparing small fragments.
However in the previous scheme, small fragments
would have higher scores than large fragments,
since fewer differences would be observed. The
normalized score is given in (13):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
log(1 ? ?(d
t
, d
o
t
)/
max(|d
t
|, |d
o
t
|))
(13)
Essentially, we are normalizing the edit distance
by the maximum edit distance possible, namely
the size of the largest fragment of the two being
compared.
3.3 Fragment Surface Rescoring
The disadvantage of the minimum tree fragment
edit approach is that it explicitly takes the internal
375
syntactic structure of the fragment into account.
In comparing two fragments, they may have the
same (or very similar) surface yields, but differ-
ent internal structures. The previous approach
would penalize the candidate fragment, even if its
yield is quite close to the oracle. In this rescor-
ing method, we extract the leaves of the candi-
date and oracle fragments, representing the substi-
tution sites by the source span which their frag-
ments cover. We then compare them using the
Damerau-Levenshtein distance ?
dl
(d
c
, d
gs
) (Dam-
erau, 1964) between the two fragment yields, and
score them as in (14):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
??
dl
(d
t
, d
o
t
) (14)
In Equation (14) we are selecting the maximal
score for ?d
s
, d
t
? from its comparison to all the
possible corresponding oracle fragments. In this
way, we are choosing to score ?d
s
, d
t
? against the
oracle fragment it is closest to.
4 Experiments
For our pilot experiments, we tested all the rescor-
ing methods in the previous section on Spanish-to-
English translation against the relative-frequency
baseline. We randomly selected 10,000 sentences
from the Europarl corpus (Koehn, 2005), and
parsed and aligned the bitext as described in (Tins-
ley et al, 2009). From the parallel treebank, we
extracted a Goodman reduction DOT grammar, as
described in (Hearne, 2005), although on an order
of magnitude greater amount of training data. Un-
like (Bod, 2007), we did not use the unsupervised
version of DOT, and did not attempt to scale up
our amount of training data to his levels, although
in ongoing work we are optimizing our system to
be able to handle that amount of training data. To
perform the rescoring, we randomly chose an ad-
ditional 30K sentence pairs from the Spanish-to-
English bitext. We rescored the grammar by trans-
lating the source side of the 10K training sentence
pairs and 10K of the additional sentences, and us-
ing the methods in Section 3 to score the frag-
ments derived in the translation process. We then
performed the same experiment translating the full
40K-sentence set. Rules in the grammar that were
not used during tuning were rescored using a de-
fault score defined to be the median of all scores
observed.
Our system performs translation by first obtain-
ing the n-best parses for the source sentences and
BLEU NIST F-SCORE
Baseline 8.78 3.582 38.21
2-8 4-6 5-5 6-4 8-2
BLEU SFR 10.30 10.31 10.32 10.27 10.08
NSFR 8.31 9.37 9.53 9.66 9.90
FSR 10.19 10.25 10.18 10.19 9.93
NIST SFR 3.792 3.805 3.808 3.800 3.781
NSFR 3.431 3.638 3.661 3.693 3.722
FSR 3.784 3.799 3.792 3.795 3.764
F-SCORE SFR 40.92 40.82 40.86 40.84 40.78
NSFR 37.53 39.50 39.93 40.38 40.78
FSR 40.83 40.85 40.87 40.91 40.67
Table 2: Results on test set. Rescoring on 20K sentences.
SFR stands for Structured Fragment Rescoring, NSFR for
Normalized SFR and FSR for Fragment Surface Rescoring.
system-i-j represents the corresponding system with ?
0
= i
and ?
1
= j. Underlined results are statistically significantly
better than the baseline at p = 0.01.
BLEU NIST F-SCORE
Baseline 8.78 3.582 38.21
2-8 4-6 5-5 6-4 8-2
BLEU SFR 10.59 10.58 10.41 10.38 10.08
NSFR 8.61 9.71 9.90 9.96 9.93
FSR 10.49 10.48 10.35 10.38 10.06
NIST SFR 3.841 3.835 3.810 3.807 3.785
NSFR 3.515 3.694 3.713 3.734 3.727
FSR 3.834 3.833 3.820 3.816 3.784
F-SCORE SFR 41.12 40.99 40.86 40.88 40.75
NSFR 38.16 40.39 40.69 40.90 40.75
FSR 41.03 41.02 41.01 40.98 40.72
Table 3: Results on test set. Rescoring on 40K sentences. Un-
derlined are statistically significantly better than the baseline
at p = 0.01.
then computing the k-best bilingual derivations for
each source parse. In our experiments we used
beams of n = 10, 000 and k = 5. We also ex-
perimented with different values of ?
0
and ?
1
in
Equation (7). We set these parameters manually,
although in future work we will automatically tune
them, perhaps using a MERT-like algorithm.
We tested our rescored grammars on a set of
2,000 randomly chosen Europarl sentences, and
used a set of 200 randomly chosen sentences as
a development test set. 1
5 Results
Translation quality results can be found in Tables
2 and 3. In these tables, columns labeled i-j in-
dicate that the corresponding system was trained
using parameters ?
0
= i and ?
1
= j in Equa-
tion 7. Statistical significance tests for NIST and
BLEU were performed using Bootstrap Resam-
pling (Koehn, 2004).
1All sentences, including the ones used for training, were
limited to a length of at most 20 words.
376
BLEU NIST F-SCORE
Baseline 10.82 3.493 42.31
2-8 4-6 5-5 6-4 8-2
BLEU SFR 11.34 12.12 11.94 11.97 11.78
NSFR 9.68 10.99 11.38 11.63 11.30
FSR 11.40 11.49 11.72 11.91 11.72
NIST SFR 3.653 3.727 3.723 3.708 3.694
NSFR 3.376 3.530 3.554 3.616 3.572
FSR 3.655 3.675 3.698 3.701 3.675
F-SCORE SFR 44.84 45.47 45.36 45.33 45.08
NSFR 41.44 43.38 44.18 44.79 44.26
FSR 44.68 44.91 45.15 45.19 44.82
Table 4: Results on development test set. Rescoring on 40K
sentences.
As Table 2 indicates, all three rescoring meth-
ods significantly outperform the relative frequency
baseline. The unnormalized structured fragment
rescoring method performed the best, with the
largest improvement of 1.5 BLEU points, a 17.5%
relative improvement. We note that the BLEU
scores for both the baseline and the experiments
are low. This is to be expected, because the gram-
mar is extracted from a very small bitext espe-
cially when the heterogeneity of the Europarl cor-
pus is considered. In our analysis, only 32.5 per-
cent of the test sentences had a complete source-
side parse, meaning that a lot of structural infor-
mation is lost contributing to arbitrary target-side
ordering. In these experiments we did not use an
additional language model. DOT (and many other
syntax-based SMT systems) essentially have the
target language model encoded within the trans-
lation model, since the inferences derived dur-
ing translations link source structures to target
structures, so in principle, no additional language
model should be necessary. Furthermore, we only
evaluate against a single reference, which also
contributes to the lowering of absolute scores. To
provide a sanity check against a state-of-the-art
system, we trained the Moses phrase-based MT
system (Koehn et al, 2007) using our training
corpus, using no language model and using uni-
form feature weights, to provide a fair comparison
against our baseline. We used this system to de-
code our development test set, and as a result we
obtained a BLEU score of 10.72, which is compa-
rable to the score obtained by our baseline on the
same set.
When we scale up to tuning on 40,000 sen-
tences we see an improvement in BLEU scores as
well, as shown in Table 3. When tuning on 40K
sentences, we observe an increase of 1.81 BLEU
points on the best-performing system, which is a
20.6% improvement over the baseline. We note
that rescoring on 20K sentences rescores approxi-
mately 275,000 rules out of 655,000 in the gram-
mar, whereas rescoring on 40K sentences rescores
approximately 280,000.
To analyze the benefits of the rescored gram-
mar, we set aside a separate development set that
we decoded with the grammar trained on 40K sen-
tences. The results are presented in Table 4. The
analysis is presented in Section 6.
Interestingly, there is a large difference between
the normalized and unnormalized versions of the
SFR scoring scheme. Our analysis suggests that
the differences are mostly due to numerical issues,
namely the difference in magnitude between the
NSFR scores and the likelihood scores in the linear
combination, and the default value assigned when
the NSFR score was zero. In ongoing work, we
are working to address these issues.
For most configurations the difference between
SFR and FSR was not statistically significant at
p = 0.05. Our analysis indicated that surface dif-
ferences tended to co-occur with structural differ-
ences. We hypothesize that as we scale up to larger
and more ambiguous grammars, the system will
infer more derivations with the same yields, ren-
dering a larger difference between the quality of
the two scoring mechanisms.
6 Discussion
To analyze the advantages and disadvantages of
our approach over the baseline, we closely ex-
amined and compared the derivations made on
the devset translation by the SFR-scored gram-
mar and the likelihood-scored grammar. Although
the BLEU scores are rather low, there were sev-
eral sentences in which the SFR-scored grammar
showed a marked improvement over the baseline.
We observed two types of improvements.
The first is where the rescored grammar gave
us translations that, while still generally bad, were
closer to the gold standard than the baseline trans-
lation. For example, the Spanish sentence ?Y en
tercer lugar , esta? el problema de la aplicacio?n uni-
forme del Derecho comunitario .? translates into
the gold standard ?Thirdly , we have the problem
of the uniform application of Community law .?
The baseline grammar translates the sentence as
?on third place , Transport and Tourism . I are
the problems of the implementation standardised
is the EU law .? with a GTM F-Score of 0.378,
377
sn=NP+67600 ?1.97/?5.66
NP+67608
the rapporteur
sp=PP+67601
s=IN ?0.48/?0.37
in
sn=SBAR+165198 ?1.39/?1.90
nc=TO+165203
to
dn=VP 0/?0.49
make
sn=NP+36950 ?5.89/?5.09
NP+36952
the rapporteur
sp=PP+36951 ?4.28/?3.81
s=IN ?0.48/?0.37
in
sn=NP+36953
dn=DT 0/?0.58
both
nc=NNS ?1.03/?0.81
questions
Figure 4: Target side of the highest-scoring translations for a sentence, according to the baseline system (left) and the SFR
system (right). Boxed nodes are substitution sites. Scores in superscripts denote the score of the sub-derivation according to
the baseline and to the SFR system.
and the rescored grammar outputs the translation
?to there in the third place , I are the problem of
the implementation standardised is the Commu-
nity law .?, with an F-Score of 0.5. While many of
the fragments in the derivations that yielded these
two translations differ, the ones we would like to
focus on are the fragments that yield the transla-
tion of ?comunitario?. The grammar contains sev-
eral competing unary fragment pairs for ?comuni-
taro?. In the baseline grammar, the pair (aq=NNP
? comunitario, aq=NNP ? EU) has a score
of ?0.693147, whereas the pair (aq=NNP ?
comunitario, aq=NNP? Community) has a
score of ?1.38629. In the rescored grammar how-
ever, (aq=NNP ? comunitario, aq=NNP ?
EU) has a score of -0.762973, whereas (aq=NNP
? comunitario, aq=NNP ? Community)
has a score of -0.74399. In effect, the rescoring
scheme rescored the word alignment itself. This
suggests that in future work, it may be possible
to integrate a word aligner or fragment aligner di-
rectly into the MT training method.
The other improvement was where the baseline
and the SFR-scored grammar output translations
of roughly the same quality according to the eval-
uation measure, yet in terms of human evaluation,
the SFR translation was much better than the base-
line translation. For instance, our devset contained
the Spanish sentence ?Estoy de acuerdo con el po-
nente en dos cuestiones .? The baseline transla-
tion given is ?I agree with the rapporteur in to
make .?, and the SFR-scored translation given is
?I agree with the rapporteur in both questions .?.
While both translations have the same GTM score
against the gold standard ?I agree with the rap-
porteur on two issues .?, clearly, the second one
is of far higher quality than the first. As we can
see in Figure 4, the derivation over the substring
?in both questions? gets a higher score than ?in
to make? when translated with the rescored gram-
mar. In the baseline, ?en dos cuestiones? is not
translated as a whole unit ? rather, the derivation of
?el ponente en dos cuestiones? is decomposed into
four subderivations, yielding ?el? ?ponente? ?en?
?dos cuestiones?, where each of those is translated
separately, into ??? ?the rapporteur? ?in? and ?to
make?. The SFR-scored grammar, however, out-
puts a different bilingual derivation. The source
is decomposed into five sub-derivations, one for
each word, and each word is translated separately.
Then, the rescored target fragments set the proper
target-side word order and select the target-side
words that maximize the score of the subderiva-
tion covering the source span. We note that in this
example, the score of translating ?dos? to ?make?
was higher than the score of translating ?dos? to
?both?. However, the higher level target frag-
ment that composed the translation of ?dos? to-
gether with the translation of ?cuestiones? yielded
a higher score when composing ?both questions?
rather than ?to make?.
7 Conclusions and Future Work
The results presented above indicate that aug-
menting the scoring mechanism with an accuracy-
based measure is a promising direction for transla-
tion quality improvement. It gives us a statistically
significant improvement over the baseline, and our
analysis has indicated that the system is indeed
making better decisions, moving us a step closer
towards the goal of making translation decisions
based on the hypothesis of the resulting transla-
378
tion?s accuracy.
Now that we have demonstrated that translation
quality can be improved by incorporating a mea-
sure of fragment quality into the scoring scheme,
our immediate next step is to optimize our sys-
tem so that we can scale up to significantly larger
training and tuning sets, and determine whether
the improvements we have noted carry over when
the likelihood is computed from more data. Af-
terwards, we will implement a training scheme
to maximize an accuracy-based objective func-
tion, for instance, by minimizing the difference
between the scores of the highest-scoring deriva-
tion and the oracle derivations, in effect maximiz-
ing the score of the highest-scoring translation.
The rescoring method presented in this paper
need not be limited to DOT. Fragments can be
thought of as analogous to phrases in Phrase-
Based SMT systems ? we could implement a sim-
ilar rescoring system for phrase-based systems,
where we generate several candidate translations
for source sentences in a tuning set, and score each
phrase used against the phrases used in a set of or-
acles. More broadly, we could potentially take any
statistical MT system, and compare the features
of all candidates generated against those of oracle
translations, and score those that are closer to the
oracle higher than those further away.
Finally, by explicitly framing the translation
problem as a search problem, where we are di-
vorcing the inferences in the search space (i.e.
the model) from the path we take to find the op-
timal inference according to some criterion (i.e.
the scoring scheme), we can remove some of the
variability when comparing two models or scoring
mechanisms (Lopez, 2009).
Acknowledgements
This work is supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142). We would like to
thank the anonymous reviewers for their helpful
comments and suggestions.
References
R. Bod. 2007. Unsupervised syntax-based ma-
chine translation: The contribution of discontiguous
phrases. In Proceedings of the 11th Machine Trans-
lation Summit, pages 51?57, Copenhagen, Den-
mark.
P. F. Brown, S. Della Pietra, V. Della Pietra, and
R. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
F. J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7(3):171?176.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Companion Volume,
pages 205?208, Sapporo.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thayer. 2006. Scalable in-
ference and training of context-rich syntactic trans-
lation models. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 961?968, Sydney, Aus-
tralia.
J. Goodman. 1996. Efficient algorithms for parsing the
DOP model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 143?152, Philadelphia, PA.
M. Hearne and A. Way. 2003. Seeing the wood for the
trees: Data-oriented translation. In Proceedings of
the Ninth Machine Translation Summit, pages 165?
172, New Orleans, LA.
M. Hearne and A. Way. 2006. Disambiguation strate-
gies for data-oriented translation. In Proceedings of
the 11th Conference of the European Association for
Machine Translation, pages 59?68, Oslo, Norway.
M. Hearne. 2005. Data-Oriented Models of Parsing
and Translation. Ph.D. thesis, Dublin City Univer-
sity, Dublin, Ireland.
M. Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Linguistics,
28(1):71?76, March.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics, demonstation session, pages
177?180, Prague, Czech Republic.
P. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
379
the Conference on Empirical Methods in Natural
Language Processing, pages 388?395, Barcelona,
Spain.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
A. Lavie, K. Sagae, and S. Jayaraman. 2004. The sig-
nificance of recall in automatic metrics for MT eval-
uation. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Ameri-
cas, pages 134?143, Washington, DC.
A. Lopez. 2009. Translation as weighted deduction. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 532?540,
Athens, Greece.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 311?318, Philadelphia, PA.
A. Poutsma. 2000. Data-oriented translation. In The
18th International Conference on Computational
Linguistics, pages 635?641, Saarbru?cken, Germany.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical MT. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 721?728, Sydney, Australia.
J. Tinsley, M. Hearne, and A. Way. 2009. Parallel tree-
banks in phrase-based statistical machine transla-
tion. In Proceedings of the Tenth International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing), pages 318?331, Mex-
ico City, Mexico.
J. Turian, L. Shen, and I. D. Melamed. 2003. Eval-
uation of machine translation and its evaluation. In
Proceedings of the Ninth Machine Translation Sum-
mit, pages 386?393, New Orleans, LA.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based
statistical translation model. In Proceedings of
39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France.
K. Yamada and K. Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of 40th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 303?310, Philadelphia, PA.
B. Zhao and S. Chen. 2009. A simplex armijo
downhill algorithm for optimizing statistical ma-
chine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Companion Volume: Short Papers, pages 21?
24, Boulder, Colorado.
380
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182?1191,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Syntactified Direct Translation Model with Linear-time Decoding
Hany Hassan
Cairo TDC
IBM
Cairo, Egypt
hanyh@eg.ibm.com
Khalil Sima?an
Language and Computation
University of Amsterdam
Amsterdam, The Netherlands
k.simaan@uva.nl
Andy Way
School of Computing
Dublin City University
Dublin, Ireland
away@computing.dcu.ie
Abstract
Recent syntactic extensions of statisti-
cal translation models work with a syn-
chronous context-free or tree-substitution
grammar extracted from an automatically
parsed parallel corpus. The decoders ac-
companying these extensions typically ex-
ceed quadratic time complexity.
This paper extends the Direct Transla-
tion Model 2 (DTM2) with syntax while
maintaining linear-time decoding. We
employ a linear-time parsing algorithm
based on an eager, incremental interpre-
tation of Combinatory Categorial Gram-
mar (CCG). As every input word is pro-
cessed, the local parsing decisions resolve
ambiguity eagerly, by selecting a single
supertag?operator pair for extending the
dependency parse incrementally. Along-
side translation features extracted from
the derived parse tree, we explore syn-
tactic features extracted from the incre-
mental derivation process. Our empiri-
cal experiments show that our model sig-
nificantly outperforms the state-of-the art
DTM2 system.
1 Introduction
Syntactic structure is gradually showing itself to
constitute a promising enrichment of state-of-the-
art Statistical Machine Translation (SMT) models.
However, it would appear that the decoding algo-
rithms are bearing the brunt of this improvement in
terms of time and space complexity. Most recent
extensions work with a synchronous context-free
or tree-substitution grammar extracted from an au-
tomatically parsed parallel corpus. While attrac-
tive in many ways, the decoders that are needed
for these types of grammars usually have time
and space complexities that are far beyond linear.
Leaving pruning aside, there is a genuine ques-
tion as to whether syntactic structure necessarily
implies more complex decoding algorithms. This
paper shows that this need not necessarily be the
case.
In this paper we extend the Direct Translation
Model (DTM2) (Ittycheriah and Roukos, 2007)
with target language syntax while maintaining
linear-time decoding. With this extension we
make three novel contributions to SMT. Our first
contribution is to define a linear-time syntactic
parser that works as incrementally as standard
SMT decoders (Tillmann and Ney, 2003; Koehn,
2004a). At every word position in the target lan-
guage string, this parser spans at most a single
parse-state to augment the translation states in
the decoder. The parse state summarizes previ-
ous parsing decisions and imposes constraints on
the set of valid future extensions such that a well-
formed sequence of parse states unambiguously
defines a dependency structure. This approach
is based on an incremental interpretation of the
mechanisms of Combinatory Categorial Grammar
(CCG) (Steedman, 2000).
Our second contribution lies in extending the
DMT2 model with a novel set of syntactically-
oriented feature functions. Crucially, these feature
functions concern the derived (partial) dependency
structure as well as local aspects of the derivation
process, including such information as the CCG
lexical categories (supertag), the CCG operators
and the intermediate parse states. This accom-
plishment is interesting both from a linguistic and
technical point of view.
Our third contribution is the extension of the
standard phrase-based decoder with the syntactic
structure and definition of new grammar-specific
pruning techniques that control the size of the
search space. Interestingly, because it is eager,
the incremental parser used in this work is hard
pushed to perform at a parsing level close to state-
1182
of-the-art cubic-time parsers. Nevertheless, the
parsing information it provides allows for signif-
icant improvement in translation quality.
We test the new model, called the Dependency-
based Direct Translation Model (DDTM), on stan-
dard Arabic?English translation tasks used in the
community, including LDC and GALE data. We
show that our DDTM system provides significant
improvements in BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006) scores over the already
extremely competitive DTM2 system. We also
provide results of manual, qualitative analysis of
the system output to provide insight into the quan-
titative results.
This paper is organized as follows. Section 2
reviews the related work. Section 3 discusses the
DTM2 baseline model. Section 4 presents the gen-
eral workings of the incremental CCG parser lay-
ing the foundations for integrating it into DTM2.
Section 5 details our own DDTM, the dependency-
based extension of the DTM2 model. Section 6
reports on extensive experiments and their results.
Section 7 provides translation output to shed fur-
ther detailed insight into the characteristics of the
systems. Finally, Section 8 concludes, and dis-
cusses future work.
2 Related Work
In (Marcu et al, 2006), it is demonstrated that
?syntactified? target language phrases can im-
prove translation quality for Chinese?English. A
stochastic, top-down transduction process is em-
ployed that assigns a joint probability to a source
sentence and each of its alternative syntactified
translations; this is done by specifying a rewrit-
ing process of the target parse-tree into a source
sentence.
Likewise, the model in (Zollmann and Venu-
gopal, 2006) extends (Chiang, 2005) by augment-
ing the hierarchical phrases with syntactic cate-
gories derived from parsing the target side of a
parallel corpus. They use an existing parser to
parse the target side of the parallel corpus in or-
der to extract a syntactically motivated, bilingual
synchronous grammar as in (Chiang, 2005).
The above-mentioned approaches for incor-
porating syntax into Phrase-based SMT (Marcu
et al, 2006; Zollmann and Venugopal, 2006)
share common drawbacks. Firstly, they are
based on syntactic phrase-structure parse trees
incorporated into a Synchronous CFG or Tree-
Substitution Grammar, which makes for a diffi-
cult match with non-constituent phrases that are
common within Phrase-based SMT. These ap-
proaches usually resort to ad hoc solutions to
enrich the non-constituent phrases with syntactic
structures. Secondly, they deploy chart-based de-
coders with a high computational cost compared
with the phrase-based beam search decoders, e.g.,
(Tillmann and Ney, 2003; Koehn, 2004a). Thirdly,
due to the large parse space, some of the pro-
posed approaches are forced to employ small lan-
guage models compared to what is usually used
in phrase-based systems. To circumvent these
computational limitations, various pruning tech-
niques are usually needed, e.g., (Huang and Chi-
ang, 2007).
Other recent approaches, e.g., (Birch et al,
2007; Hassan et al, 2007; Hassan et al, 2008a)
incorporate a linear-time supertagger into SMT to
take the role of a syntactic language model along-
side the standard language model. While these ap-
proaches share with our work the use of lexical-
ized grammars, they never seek to build a full de-
pendency tree or employ syntactic features in or-
der to directly influence the reordering probabili-
ties in the decoder. In the current work, we ex-
pand our previous work in (Hassan et al, 2007;
Hassan et al, 2008a) to introduce the capabilities
of building a full dependency structure and em-
ploying syntactic features to influence the decod-
ing process.
Recently, (Shen et al, 2008) introduced an ap-
proach for incorporating a dependency-based lan-
guage model into SMT. They proposed to extract
String-to-Dependency trees from the parallel cor-
pus. As the dependency trees are not constituents
by nature, they handle non-constituent phrases as
well. While this work is in the same general
direction as our work, namely aiming at incor-
porating dependency parsing into SMT, there re-
main three major differences. Firstly, (Shen et al,
2008) resorted to heuristics to extract the String-
to-Dependency trees, whereas our approach em-
ploys the well formalized CCG grammatical the-
ory. Secondly, their decoder works bottom-up
and uses a chart parser with a limited language
model capability (3-grams), while we build on the
efficient, linear-time decoder commonly used in
phrase-based SMT. Thirdly, (Shen et al, 2008)
deploys the dependency language model to aug-
ment the lexical language model probability be-
1183
tween two head words but never seek a full de-
pendency graph. In contrast, our approach inte-
grates an incremental parsing capability, that pro-
duces the partial dependency structures incremen-
tally while decoding, and thus provides for better
guidance for the search of the decoder for more
grammatical output. To the best of our knowledge,
our approach is the first to incorporate incremental
dependency parsing capabilities into SMT while
maintaining the linear-time and -space decoder.
3 Baseline: Direct Translation Model 2
The Direct Translation Model (DTM) (Papineni
et al, 1997) employs the a posteriori conditional
distribution P (T |S) of a target sentence T given
a source sentence S, instead of the common in-
version into P (S|T ) based on the source chan-
nel approach (Brown et al, 1990). DTM2, in-
troduced in (Ittycheriah and Roukos, 2007), ex-
presses the phrase-based translation task in a uni-
fied log-linear probabilistic framework consisting
of three components: (i) a prior conditional dis-
tribution P
0
(.|S), (ii) a number of feature func-
tions ?
i
() that capture the translation and language
model effects, and (iii) the weights of the features
?
i
that are estimated under MaxEnt (Berger et al,
1996), as in (1):
P (T |S) =
P
0
(T, J |S)
Z
exp
?
i
?
i
?
i
(T, J, S) (1)
Here J is the skip reordering factor for the phrase
pair captured by ?
i
() and represents the jump from
the previous source word, and Z is the per source
sentence normalization term. The prior probabil-
ity P
0
is the prior distribution for the phrase prob-
ability which is estimated using the phrase nor-
malized counts commonly used in conventional
Phrase?based SMT systems, e.g., (Koehn et al,
2003).
DTM2 differs from other Phrase?based SMT
models in that it extracts from a word-aligned par-
allel corpus only a non-redundant set of minimal
phrases in the sense that no two phrases overlap
with each other.
Baseline DTM2 Features: The baseline em-
ploys the following five types of features (beside
the language model):
? Lexical Micro Features examining source
and target words of the phrases,
? Lexical Context Features encoding the
source and target phrase context (i.e. previ-
ous and next source and previous target),
? Source Morphological Features encoding
morphological and segmentation characteris-
tics of source words.
? Part-of-Speech Features encoding source and
target POS tags as well as the POS tags of the
surrounding contexts of phrases.
The DTM2 approach based on MaxEnt provides
a flexible framework for incorporating other avail-
able feature types as we demonstrate below.
DTM2 Decoder: The decoder for the baseline is
a beam search decoder similar to decoders used in
standard phrase-based log-linear systems such as
(Tillmann and Ney, 2003) and (Koehn, 2004a).
The main difference between the DTM2 decoder
and the standard Phrase?based SMT decoders is
that DTM2 deploys Maximum Entropy probabilis-
tic models to obtain the translation costs and var-
ious feature costs by deploying the features de-
scribed above in a discriminative MaxEnt fashion.
In the rest of this paper we adopt the DTM2 for-
malization of translation as a discriminative task,
and we describe the CCG-based incremental de-
pendency parser that we use for extending the
DTM2 decoder, and then list a new set of syntac-
tic dependency feature functions that extend the
DTM2 feature set. We also discuss pruning and
other details of the approach.
4 The Incremental Dependency Parser
As it processes an input sentence left-to-right
word-by-word, the incremental dependency model
builds?for each prefix of the input sentence?a
partial parse that is a subgraph of the partial parse
that it builds for a longer prefix. The dependency
graph is constructed incrementally, in that the sub-
graph constructed at a preceding step is never al-
tered or revised in any later steps. The following
schematic view in (2) exhibits the general work-
ings of this parser:
S
0
o
1
w
1
,st
1
//
S
1
o
2
w
2
,st
2
//
S
2
S
i
o
i
w
i
,st
i
//
S
i+1
S
n
(2)
The syntactic process is represented by a sequence
of transitions between adjacent syntactic states S
i
.
1184
A transition from state S
i?1
to S
i
scans the cur-
rent word w
i
and stochastically selects a com-
plex lexical descriptor/category st
i
and an oper-
ator o
i
given the local context in the transition se-
quence. The syntactic state S
i
summarizes all the
syntactic information about fragments that have
already been processed and registers the syntac-
tic arguments which are to be expected next. Only
an impoverished deterministic procedure (called a
?State-Realizer?) is needed in order to compose a
state S
i
with the previous states S
0
. . . S
i?1
in or-
der to obtain a fully connected intermediate depen-
dency structure at every position in the input.
To implement the incremental parsing scheme
described above we use the parser described in
(Hassan et al, 2008b; Hassan et al, 2009), which
is based on Combinatory Categorial Grammar
(CCG) (Steedman, 2000). We only briefly de-
scribe this parser as its full description is beyond
the scope of this paper. The notions of a supertag
as a lexical category and the process of supertag-
ging are both crucial here (Bangalore and Joshi,
1999). Fortunately, CCG specifies the desired kind
of lexical categories (supertags) st
i
for every word
and a small set of combinatory operators o
i
that
combine the supertag st
i
with a previous parse
state S
i?1
into the next parse state S
i
. In terms
of CCG representations, the parse state is a CCG
composite category which specifies either a func-
tor and the arguments it expects to the right of the
current word, or is itself an argument for a functor
that will follow it to the right. At the first word in
the sentence, the parse state consists solely of the
supertag of that word.
Attacks rocked Riyadh
S
0
NP (S\NP)/NP NP
> NOP
S
1
: NP
> TRFC
S
2
: S/NP
> FA
S
3
: S
Figure 1: A sentence and possible supertag-,
operator- and state-sequences. NOP: No Oper-
ation; TRFC: Type Raise-Forward Composition;
FA: Forward Application. The CCG operators
used show that Attacks and Riyadh are both
dependents of rocked.
Figure 1 exhibits an example of the workings of
this parser. Practically speaking, after POS tag-
ging the input sentence, the parser employs two
components:
? A Supertag-Operator Tagger which proposes
a supertag?operator pair for the current word,
? A deterministic State-Realizer, which real-
izes the current state by applying the current
operator to the previous state and the current
supertag.
The Supertag-Operator Tagger is a probablistic
component while the State-Realizer is a determin-
istic component. The generative model underlying
this component concerns the probability P (W,S)
of a word sequence W = wn
1
and a parse-state
sequence S = Sn
1
, with associated supertag se-
quence ST = stn
1
and operator sequence O = on
1
,
which represents a possible derivation. Note that
given the choice of supertags st
i
and operator o
i
,
the state S
i
is calculated deterministically by the
State-Realizer.
A generative version of this model is described
in (3):
P (W,S) =
n
?
i=1
Word Predictor
? ?? ?
P (w
i
|W
i?1
S
i?1
)
.
Supertagger
? ?? ?
P (st
i
|W
i
) .
Operator Tagger
? ?? ?
P (o
i
|W
i
, S
i?1
, ST
i
) (3)
In (3):
? P (W,S) represents the product of the pro-
duction probabilities at each parse-state and
is similar to the structured language model
representation introduced in (Chelba, 2000).
? P (w
i
|W
i?1
S
i?1
) is the probability of w
i
given the previous sequence of words W
i?1
and the previous sequence of states S
i?1
,
? P (st
i
|W
i
): is the supertag st
i
probability
given the word sequence W
i
up to the cur-
rent position. Basically, this represents a se-
quence tagger (a ?supertagger?).
? P (o
i
|W
i
, S
i?1
, ST
i
) represents the probabil-
ity of the operator o
i
given the previous
words, supertags and state sequences up to
the current position. This represents a CCG
operator tagger.
The different local conditional components (for
every i) in (3) are estimated as discriminative
MaxEnt submodels trained on a corpus of incre-
mental CCG derivations. This corpus was ex-
tracted from the CCGbank (Hockenmaier, 2003)
1185
by transforming every normal form derivation into
strictly left-to-right CCG derivations, with the
CCG operators only slightly redesigned to allow
incrementality while still satisfying the dependen-
cies in the CCGbank (cf. (Hassan et al, 2008b;
Hassan et al, 2009)).
As mentioned before, the State-Realizer is a
deterministic function. Starting at the first word
with (obviously) a null previous state, the realizer
performs the following deterministic steps for
each word in turn: (i) set the current supertag
and operator to those of the current word; (ii) at
the current state, apply the current operator to the
previous state and current supertag; (iii) add edges
to the dependency graphs between words that are
linked as CCG arguments; and (iv) if not at the
end of the sentence, set the previous state to the
current one, then set the current word to the next
one, and iterate from (i).
It is worth noting that the proposed dependency
parser is deterministic in the sense that it maintains
only one parse state per word. This characteris-
tic is crucial for its incorporation into a large-scale
SMT system to avoid explosion of the translation
space during decoding.
5 Dependency-based DTM (DDTM)
In this section we extend the DTM2 model with
incremental target dependency-based syntax. We
call the resulting model the Dependency-based Di-
rect Translation Model (DDTM). This extension
takes place by (i) extracting syntactically enriched
minimal phrase pairs, (ii) including a new set of
syntactic feature functions among the exponen-
tial model features, and (iii) adapting the decoder
for dealing with syntax, including various pruning
strategies and enhancements. Next we describe
each extension in turn.
5.1 Phrase Table: Incremental Syntax
The target-side sentences in the word-aligned par-
allel corpus used for training are parsed using
the incremental dependency parser described in
section 4. This results in a word-aligned par-
allel corpus where the words of the target sen-
tences are tagged with supertags and operators.
From this corpus we extract the set of minimal
phrase pairs using the method described in (Itty-
cheriah and Roukos, 2007), extracting along with
every target phrase the associated sequences of su-
pertags and operators. As shown in (4), a source
phrase s
1
, . . . , s
n
translates into a target phrase
w
1
, . . . , w
m
where every word w
i
is labeled with
a supertag st
i
, and a possible parsing operator o
i
appearing with it in the parsed parallel corpus:
s
1
...s
n
//
[w
1
, st
1
, o
1
]...[w
m
, st
m
, o
m
] (4)
Hence, our phrase table associates with every
target phrase an incremental parsing subgraph.
These subgraphs along with their probabilities
represent our phrase table augmented with incre-
mental dependency parsing structure.
This representation turns the complicated prob-
lem of MT with incremental parsing into a sequen-
tial classification problem in which the classifier
deploys various features from the source sentence
and the candidate target translations to specify a
sequence of decisions that finally results in an out-
put target string along with its associated depen-
dency graph. The classification decisions are per-
formed in sequence step-by-step while traversing
the input string to provide decisions on possible
words, supertags, operators and states. A beam
search decoder simultaneously decides which se-
quence is the most probable.
5.2 DDTM Features
The exponential model and the MaxEnt frame-
work used in DTM2 and DDTM enabled us to ex-
plore the utility of incremental syntactic parsing
within a rich feature space. In our DDTM sys-
tem, we implemented a set of features alongside
the baseline DTM2 features that were discussed in
Section 3. The features described here encode all
the probabilistic components in (3) within a log
linear interpretation along with some more empir-
ically intuitive features.
? Supertag-Word features: these features ex-
amine the target phrase words with their as-
sociated supertags and is related to the Su-
pertagger component in (3).
? Supertag sequence features: these features
encode n-gram supertags (equivalent to the n-
gram supertags Language Model). This fea-
ture is related to the supertagger component
as well.
? Supertag-Operator features: these features
encode supertags and associated operators
which is related to the Operator Tagger com-
ponent in (3).
1186
? Supertag-State features: these features regis-
ter state and supertag co-occurrences.
? State sequence features: these features en-
code n-gram state features and are equiva-
lent to an n-gram Language Model over parse
state sequences which is related to the multi-
plication in (3).
? Word-State sequence features: these fea-
tures encode words and states co-occurrences
which is related to the Word Predictor com-
ponent in (3).
The exponential model and the MaxEnt frame-
work used in DTM2 and DDTM enable us to ex-
plore the utility of incremental syntactic parsing
with the use of minimal phrases within a rich fea-
ture space.
5.3 DDTM Decoder
In order to support incremental dependency pars-
ing, we extend the DTM2 decoder in three ways:
firstly, by constructing the syntactic states during
decoding; secondly, by extending the hypothesis
structures to incorporate the syntactic states and
the partial dependency derivations; and thirdly, by
modifying the pruning strategy to handle the large
search space.
At decoding time, each hypothesis state is as-
sociated with a parse-state which is constructed
while decoding using the incremental parsing ap-
proach introduced in ((Hassan et al, 2008b; Has-
san et al, 2009)). The previous state, the se-
quences of supertags and CCG incremental opera-
tors are deployed in a deterministic manner to re-
alize the parse-states as well as the intermediate
dependency graphs between words.
Figure 2 shows the DDTM decoder while de-
coding a sentence with the English translation ?At-
tacks rocked Riyadh?. Each hypothesis is asso-
ciated with a parse-state S
i
and a partial depen-
dency graph (shown for some states only). More-
over, each transition is associated with an opera-
tor o that combines the previous state and the cur-
rent supertag st to construct the next state S
i
. The
decoder starts from a null state S
1
and then pro-
ceeds with a possible expansion with the word ?at-
tacks?, supertag NP and operator NOP to pro-
duce the next hypothesis with state S
2
and cate-
gory NP . Further expansion for that path with the
verb ?rocked?, supertag ?(S\NP )/NP and oper-
ator TRFC will produce the state S
5
with cat-
egory S/NP . The partial dependency graph for
state S
5
is shown above the state where a depen-
dency relation between the two words is estab-
lished. Furthermore, another expansion with the
word ?Riyadh?, supertag NP and operator FA
produces state S
7
with category S and a completed
dependency graph as shown above the state. An-
other path which spans the states S
1
, S
3
, S
6
and
S
8
ends with a state category S/NP and a partial
dependency graph as shown under state S
8
where
the dependency graph is still missing its object
(e.g. ?Riyadh attacks rocked the Saudi Govt.?).
The addition of parse-states may result in a very
large search space due to the fact that the same
phrase/word may have many possible supertags
and many possible operators. Moreover, the same
word sequences may have many parse-state se-
quences and, therefore, many hypotheses that rep-
resent the same word sequence. The search space
is definitely larger than the baseline search space.
We adopt the following three pruning heuristics to
limit the search space.
5.3.1 Grammatical Pruning
Any hypothesis which does not constitute a valid
parse-state is discarded, i.e. if the previous parse-
state and the current supertag sequence cannot
construct a valid state using the associated oper-
ator sequence, then the expansion is discarded.
Therefore, this pruning strategy maintains only
fully connected graphs and discards any partially
connected graphs that might result during the de-
coding process.
As shown in Figure 2, the expansion from state
S
1
to state S
4
(with the dotted line) is pruned and
not expanded further because the proposed expan-
sion is the verb ?attacks?, supertag (S\NP )/NP
and operator TRFC . Since the previous state is
NULL, it cannot be combined with the verb using
the TRFC operator. This would produce an un-
defined state and thus the hypothesis is discarded.
5.3.2 Supertags and Operators Threshold
We limit the supertag and operator variants per tar-
get phrase to a predefined number of alternatives.
We tuned this on the MT03 DevSet for the best
accuracy while maintaining a manageable search
space. The supertags limit was set to four alterna-
tives while the operators limit was set to three.
As shown in Figure 2, each word can have many
alternatives with different supertags. In this exam-
ple the word ?attacks? has two forms, namely a
1187
e:
a : --------
P:1
S1:NULL
e: attacks
a: *----
P:=.162
ST=NP
S2=NP
e: attacks
a: *-------
P:=.092
ST=(S\NP)/NP
S4= UNDEF
O:TRFC
e: Riyadh
a: -*------
P:=.142
ST=NP/NP
S3=NP/NP
e: rocked
a: --*--
P:=.083
ST=(S\NP)/NP
S5=S/NP
O:NOP
O:NOP
O:TRFC
e: rocked
a: --*------
P:=.01
ST=(S\NP)/NP
S8=S/NP
attacks
attacks rocked
e: Riyadh
a: --*--
P:=.04
ST=NP
S7=S
O:FC
attacks rocked Riyadh
e: attacks
a: *-------
P:=.07
ST=NP
S6=NP
O:TRFC
Riyadh attacks rocked
O:FA
Figure 2: DDTM Decoder: each hypothesis has a parse state and a partial dependency structure.
noun and a verb, with different supertags and op-
erators. The proposed thresholds limit the possible
alternatives to a reasonable number.
5.3.3 Merging Hypotheses
Standard Phrase?based SMT decoders merge
translation hypotheses if they cover the same
source words and share the same n-gram lan-
guage model history. Similarly, DDTM decoder
merges translation hypotheses if they cover the
same source words, share the same n-gram lan-
guage model history and share the same parse-
state history. This helps in reducing the search
space by merging paths that will not constitute a
part of the best path.
6 Experiments
We conducted experiments on an Arabic-to-
English translation task using LDC parallel data
and GALE parallel data. We used the UN paral-
lel corpus and LDC news corpus together with the
GALE parallel corpus, totaling 7.8M parallel sen-
tences. The 5-gram Language Model was trained
on the English Gigaword Corpus and the English
part of the parallel corpus. Our baseline system is
similar to the system described in (Ittycheriah and
Roukos, 2007). We report results on NIST MT05
and NIST MT06 evaluations test sets using BLEU
and TER as automatic evaluation metrics.
To train the DDTM model, we use the incre-
mental parser introduced in (Hassan et al, 2008b;
Hassan et al, 2009) to parse the target side of the
parallel training data. Each sentence is associated
with supertag, operator and parse-state sequences.
We then train models with different feature sets.
Results: We compared the baseline DTM2 (It-
tycheriah and Roukos, 2007) with our DDTM sys-
tem with the features listed above. We examine
the effect of all features on system performance.
In this set of experiments we used LDC parallel
data only which is composed of 3.7M sentences
and the results are reported on MT05 test set. Each
of the examined systems deploys DTM2 features
in addition to a number of newly added syntactic
features. The systems examined are:
? DTM2: Direct Translation model 2 baseline.
? D-SW: DTM2 + Supertag-Word features.
? D-SLM: DTM2 + Supertag-Word and su-
pertag n-gram features.
? D-SO: DTM2+ Supertag-Operator features.
? D-SS : DTM2 + supertags and states features
with parse-state construction.
? D-WS : DTM2 + words and states features
with parse-state construction.
? D-STLM: DTM2 + state n-gram features
with parse-state construction.
? DDTM: fully fledged system with all fea-
tures that proved useful above which are:
Supertag-Word features, supertag n-gram
1188
features, supertags and states features and
state n-gram features .
System BLEU Score on MT05
DTM2-Baseline 52.24
D-SW 52.28
D-SLM 52.29
D-SO 52.01
D-SS 52.39
D-WS 52.03
D-STLM 52.53
DDTM 52.61
Table 1: DDTM Results with various features.
As shown in Table 1, the DTM baseline system
demonstrates a very high BLEU score, unsurpris-
ingly given its top-ranked performance in two re-
cent major MT evaluation campaigns. Among the
features we tried, supertags and n-gram supertags
systems (D-SW and D-SLM systems) give slight
yet statistically insignificant improvements. On
the other hand, the states n-gram sequence features
(D-SS and DDTM systems) give small yet statis-
tically significant improvements (as calculated via
bootstrap resampling (Koehn, 2004b)). The D-WS
system shows a small degradation in performance,
probably due to the fact that the states-words inter-
actions are quite sparse and could not be estimated
with good evidence. Similarly, the D-SO system
shows a small degradation in performance. When
we investigated the features types, we found out
that all features that deploy the operators had bad
effect on the model. We think this is due to the fact
that the operator set is a small set with high evi-
dence in many training instances such that it has
low discriminative power on it is own. However,
it implicitly helps in producing the state sequence
which proved useful.
System DTM2-Baseline DDTM
MT05 (BLEU) 55.28 55.66
MT05 (TER) 38.79 38.48
MT06 (BLEU) 43.56 43.91
MT06 (TER) 49.08 48.65
Table 2: DDTM Results on MT05 and MT06.
We examined a combination of the best fea-
tures in our DDTM system on a larger training
data comprising 7.8M sentences from both NIST
and GALE parallel corpora. Table 2 shows the
results on both MT05 and MT06 test sets. As
shown, DDTM significantly outperforms the state-
of-the-art baseline system. It is worth noting that
DDTM outperforms this baseline even when very
large amounts of training data are used. Despite
the fact that the actual scores are not so different,
we found that the baseline translation output and
the DDTM translation outout are significantly dif-
ferent. We measured this by calculating the TER
between the baseline translation and the DDTM
translation for the MT05 test set, and found this
to be 25.9%. This large difference has not been
realized by the BLEU or TER scores in compari-
son to the baseline. We believe that this is due to
the fact that most changes that match the syntac-
tic constraints do not bring about the best match
where the automatic evaluation metrics are con-
cerned. Accordingly, in the next section we de-
scribe the outcome of a detailed manual analysis
of the output translations.
7 Manual Analysis of Results
Although the BLEU score does not mark a large
improvement by the dependency-based system
over the baseline system, human inspection of the
data gives us important insights into the pros and
cons of the dependency-based model. We ana-
lyzed a randomly selected set of 100 sentences
from the MT05 test set. In this sample, the base-
line and the DDTM system perform similarly in
68% of the sentences. The outputs of both system
are similar though not identical. In these cases,
the systems may choose equivalent paraphrases.
However, the translations using syntactic struc-
tures are rather similar. It is worth noting that the
DDTM system tends to produce more concise sys-
ntactic structures which may lead to less BLUE
score due to penalizing the translation length al-
though the translation might be equivelent to the
baseline if not better.
In 28% of the sentences, the DDTM system pro-
duces remarkably better translations. The exam-
ples here illustrate the behaviour of the baseline
and the DDTM systems which can be observed
consistently throughout the test set. We only high-
light some of the examples for illustration pur-
poses. DDTM manages to insert verbs which are
deleted by any standard phrase-based SMT sys-
tem. DDTM prefers to deploy verbs since they
have complex and more detailed syntactic struc-
tures which give better and more likely state se-
1189
quences. Furthermore, the DDTM system avoids
longer noun phrases and instead uses some prepo-
sitions in-between. Again, this is probably due to
the fact that like verbs, prepositions have a com-
plex syntactic description that give rise to more
likely state sequences.
On the other hand, the baseline produced better
translation in 8% of the analysis sample. We ob-
served that the baseline is doing better mainly in
two cases. The first when the produced translation
is very poor and producing poor sysntatctic struc-
ture due to out of vocabularies or hard to trans-
late sentences. The second case is with sentences
with long noun phrases, in such cases the DDTM
system prefres to introduce verbs or prepositions
in the middle of long noun phrase and thus the
baseline would produce better translations. This
is maybe due to the fact that noun phrases have
relatively simple structure in CCG such that it did
not help in constructing long noun phrases.
Source: ??Q???  ZAJ
.
? Yg A

?Qk
.


HA??j
	
?? ??
	
X Y?K
.
?
	
?
	
k?
Reference: He then underwent medical examinations by a po-
lice doctor .
Baseline: He was subjected after that tests conducted by doc-
tors of the police .
DDTM: Then he underwent tests conducted by doctors of the
police .
Source: 	?



KPA J


?
.
	
?A??j
.
? ?J


?  ZA??
	
?AK


Q ?
	
Q? Y

??
	
?




J
	
j
	
j
	
??
Reference: Riyadh was rocked tonight by two car bomb at-
tacks..
Baseline: Riyadh rocked today night attacks by two booby -
trapped cars.
DDTM: Attacks rocked Riyadh today evening in two car
bombs.
Figure 3: DDTM provides better syntactic struc-
ture with more concise translations.
Figure 3 shows two examples where DDTM
provides better and more concise syntactic struc-
ture. As we can see, there is not much agree-
ment between the reference and the proposed
translation. However, longer translations enhance
the possibility of picking more common n-gram
matches via the BLEU score and so increases the
chance of better scores. This well-known bias
does not favour the more concise output derived
by our DDTM system, of course.
8 Conclusion and Future Work
In this paper, we presented a novel model of de-
pendency phrase-based SMT which integrates in-
cremental dependency parsing into the transla-
tion model while retaining the linear decoding as-
sumed in conventional Phrase?based SMT sys-
tems. To the best of our knowledge, this model
constitutes the first effective attempt at integrating
a linear-time dependency parser that builds a con-
nected tree incrementally into SMT systems with
linear-time decoding. Crucially, it turns out that
incremental dependency parsing based on lexical-
ized grammars such as CCG and LTAG can pro-
vide valuable incremental parsing information to
the decoder even if their output is imperfect. We
believe this robustness in the face of imperfect
parser output to be a property of the probabilistic
formulation and statistical estimation used in the
Direct Translation Model. A noteworthy aspect of
our proposed approach is that it integrates features
from the derivation process as well as the derived
tree. We think that this is possible due to the im-
portance of the notion of a derivation in linguistic
frameworks such as CCG and LTAG.
Future work will attempt further extensions of
our DDTM system to allow for the exploitation
of long-range aspects of the dependency struc-
ture. We will work on expanding the features
set of DDTM system to leverage features from
the constructed dependency structure itself. Fi-
nally, we will work on enabling the deployment
of source side dependency structures to influence
the construction of the target dependency structure
based on a bilingually enabled dependency pars-
ing mechanism using the discriminative modeling
capabilities.
Acknowledgments
We would like to thank Salim Roukos, IBM TJ
Watson Research Center, for fruitful, insightful
discussions and for his support during this work.
We also would like to thank Abe Ittycheriah, IBM
TJ Watson Research Center, for providing the
DTM2 baseline and for his support during de-
veloping this system. Finally, we would like to
thank the anonymous reviewers for their helpful
and constructive comments.
1190
References
Bangalore, S. and Joshi, A. (1999). ?Supertagging: An
Approach to Almost Parsing?, Computational Lin-
guistics 25(2):237?265, 1999.
Berger, A. and Della Pietra, S. and Della Pietra, V.J.
(1996). Maximum Entropy Approach to Natural
Language Processing Computational Linguistics,
22(1): 39?71, 1996.
Birch, A., Osborne, M. and Koehn, P. (2007). CCG Su-
pertags in Factored Statistical Machine Translation.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, ACL-07, pp.9?16, 2007.
Brown, P., Cocke,J., Della Pietra, S., Jelinek, F., Della
Pietra, V.J. Lafferty, R. Mercer and Roossin, P. ?A
Statistical Approach to Machine Translation? Com-
putational Linguistics 16(2):79?85, 1990.
Chelba, C. (2000). Exploiting Syntactic Structure for
Natural Language Modeling. PhD thesis, Johns
Hopkins University, Baltimore, MD.
Chiang, D. (2005). A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL05), pp.263?270, Ann Arbor,
MI.
Hassan, H., Sima?an, K., and Way, A.. (2009). Lex-
icalized Semi-Incremental Dependency Parsing. In
Proceedings of RANLP 2009, the International Con-
ference on Recent Advances in Natural Language
Processing, Borovets, Bulgaria (to appear).
Hassan, H., Sima?an, K., and Way, A. (2008a). Syntac-
tically Lexicalized Phrase-Based Statistical Transla-
tion. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 6(7):1260?1273.
Hassan, H., Sima?an, K., and Way, A.. (2008b). A Syn-
tactic Language Model Based on Incremental CCG
Parsing. In Proceedings IEEE Workshop on Spoken
Language Technology (SLT) 2008, Goa, India.
Hassan, H., Sima?an, K., and Way, A. (2007). Inte-
grating Supertags into Phrase-based Statistical Ma-
chine Translation. In Proceedings of the ACL-2007,
Prague, Czech Republic, pp.288?295, 2007.
Hockenmaier, J. (2003). Data and Models for Statisti-
cal Parsing with Combinatory Categorial Grammar,
Ph.D Thesis, University of Edinburgh, UK, 2003.
Huang, L. and Chiang, D. (2007). Forest Rescoring:
Faster Decoding with Integrated Language Models.
In Proceedings of the ACL-2007, Prague, Czech Re-
public, 2007.
Ittycheriah, A. and Roukos, S. (2007). Direct trans-
lation model 2. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pp.57?64,
Rochester, NY.
Koehn, P. (2004a). Pharaoh: A Beam Search De-
coder for phrase-based Statistical Machine Transla-
tion Models. Machine Translation: From Real Users
to Research. In Proceedings of 6th Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA 2004, pp.115?124, Washington, DC.
Koehn, P. (2004b). Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pp.388?395,
Edmonton, AB, Canada.
Koehn, P. Och, F.J. and Marcu, D. (2003). Statisti-
cal phrase-based translation. In Proceedings of the
Joint Human Language Technology Conference and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2003), pp.127?133, Edmonton, AL,
Canada.
Marcu, D., Wang, W., Echihabi, A., and Knight, K.
(2006). SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2006),
pp.44?52, Sydney, Australia.
Papineni, K., Roukos, S., and Ward, T. (1997).
Feature-Based Language Understanding. In Pro-
ceedings of 5th European Conference on Speech
Communication and Technology EUROSPEECH
?97 , pp.1435?1438, Rhodes, Greece.
Papineni, K., Roukos, S., Ward, T. and Zhu, W-J.
(2002). BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?02), pp.311?318, Philadelphia, PA.
Shen, L., Xu, J., and Weischedel, R. (2008). A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL-08: HLT, pp.577?585, Columbus,
OH.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L. and
Makhoul, J. (2006) A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA 2006:
Proceedings of the 7th Conference of the Association
for Machine Translation in the Americas, pp.223?
231, Cambridege, MA.
Steedman, M. (2000). The Syntactic Process. MIT
Press, Cambridge, MA.
Tillmann, C. and Ney, H. (2003). Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133.
Zollmann, A. and Venugopal, A. (2006). Syntax aug-
mented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL, pp.138?141, New York,
NY.
1191
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 549?557,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Bilingually Motivated Domain-Adapted Word Segmentation
for Statistical Machine Translation
Yanjun Ma Andy Way
National Centre for Language Technology
School of Computing
Dublin City University
Dublin 9, Ireland
{yma, away}@computing.dcu.ie
Abstract
We introduce a word segmentation ap-
proach to languages where word bound-
aries are not orthographically marked,
with application to Phrase-Based Statis-
tical Machine Translation (PB-SMT). In-
stead of using manually segmented mono-
lingual domain-specific corpora to train
segmenters, we make use of bilingual cor-
pora and statistical word alignment tech-
niques. First of all, our approach is
adapted for the specific translation task at
hand by taking the corresponding source
(target) language into account. Secondly,
this approach does not rely on manu-
ally segmented training data so that it
can be automatically adapted for differ-
ent domains. We evaluate the perfor-
mance of our segmentation approach on
PB-SMT tasks from two domains and
demonstrate that our approach scores con-
sistently among the best results across dif-
ferent data conditions.
1 Introduction
State-of-the-art Statistical Machine Translation
(SMT) requires a certain amount of bilingual cor-
pora as training data in order to achieve compet-
itive results. The only assumption of most cur-
rent statistical models (Brown et al, 1993; Vogel
et al, 1996; Deng and Byrne, 2005) is that the
aligned sentences in such corpora should be seg-
mented into sequences of tokens that are meant to
be words. Therefore, for languages where word
boundaries are not orthographically marked, tools
which segment a sentence into words are required.
However, this segmentation is normally performed
as a preprocessing step using various word seg-
menters. Moreover, most of these segmenters are
usually trained on a manually segmented domain-
specific corpus, which is not adapted for the spe-
cific translation task at hand given that the manual
segmentation is performed in a monolingual con-
text. Consequently, such segmenters cannot pro-
duce consistently good results when used across
different domains.
A substantial amount of research has been car-
ried out to address the problems of word segmen-
tation. However, most research focuses on com-
bining various segmenters either in SMT training
or decoding (Dyer et al, 2008; Zhang et al, 2008).
One important yet often neglected fact is that the
optimal segmentation of the source (target) lan-
guage is dependent on the target (source) language
itself, its domain and its genre. Segmentation con-
sidered to be ?good? from a monolingual point
of view may be unadapted for training alignment
models or PB-SMT decoding (Ma et al, 2007).
The resulting segmentation will consequently in-
fluence the performance of an SMT system.
In this paper, we propose a bilingually moti-
vated automatically domain-adapted approach for
SMT. We utilise a small bilingual corpus with
the relevant language segmented into basic writ-
ing units (e.g. characters for Chinese or kana for
Japanese). Our approach consists of using the
output from an existing statistical word aligner
to obtain a set of candidate ?words?. We evalu-
ate the reliability of these candidates using sim-
ple metrics based on co-occurrence frequencies,
similar to those used in associative approaches to
word alignment (Melamed, 2000). We then mod-
ify the segmentation of the respective sentences
in the parallel corpus according to these candi-
date words; these modified sentences are then
given back to the word aligner, which produces
new alignments. We evaluate the validity of our
approach by measuring the influence of the seg-
mentation process on Chinese-to-English Machine
Translation (MT) tasks in two different domains.
The remainder of this paper is organised as fol-
549
lows. In section 2, we study the influence of
word segmentation on PB-SMT across different
domains. Section 3 describes the working mecha-
nism of our bilingually motivated word segmenta-
tion approach. In section 4, we illustrate the adap-
tation of our decoder to this segmentation scheme.
The experiments conducted in two different do-
mains are reported in Section 5 and 6. We discuss
related work in section 7. Section 8 concludes and
gives avenues for future work.
2 The Influence of Word Segmentation
on SMT: A Pilot Investigation
The monolingual word segmentation step in tra-
ditional SMT systems has a substantial impact on
the performance of such systems. A considerable
amount of recent research has focused on the in-
fluence of word segmentation on SMT (Ma et al,
2007; Chang et al, 2008; Zhang et al, 2008);
however, most explorations focused on the impact
of various segmentation guidelines and the mech-
anisms of the segmenters themselves. A current
research interest concerns consistency of perfor-
mance across different domains. From our ex-
periments, we show that monolingual segmenters
cannot produce consistently good results when ap-
plied to a new domain.
Our pilot investigation into the influence of
word segmentation on SMT involves three off-
the-shelf Chinese word segmenters including
ICTCLAS (ICT) Olympic version1, LDC seg-
menter2 and Stanford segmenter version 2006-05-
113. Both ICTCLAS and Stanford segmenters
utilise machine learning techniques, with Hidden
Markov Models for ICT (Zhang et al, 2003) and
conditional random fields for the Stanford seg-
menter (Tseng et al, 2005). Both segmenta-
tion models were trained on news domain data
with named entity recognition functionality. The
LDC segmenter is dictionary-based with word fre-
quency information to help disambiguation, both
of which are collected from data in the news do-
main. We used Chinese character-based and man-
ual segmentations as contrastive segmentations.
The experiments were carried out on a range of
data sizes from news and dialogue domains using
a state-of-the-art Phrase-Based SMT (PB-SMT)
1http://ictclas.org/index.html
2http://www.ldc.upenn.edu/Projects/
Chinese
3http://nlp.stanford.edu/software/
segmenter.shtml
system?Moses (Koehn et al, 2007). The perfor-
mance of PB-SMT system is measured with BLEU
score (Papineni et al, 2002).
We firstly measure the influence of word seg-
mentation on in-domain data with respect to the
three above mentioned segmenters, namely UN
data from the NIST 2006 evaluation campaign. As
can be seen from Table 1, using monolingual seg-
menters achieves consistently better SMT perfor-
mance than character-based segmentation (CS) on
different data sizes, which means character-based
segmentation is not good enough for this domain
where the vocabulary tends to be large. We can
also observe that the ICT and Stanford segmenter
consistently outperform the LDC segmenter. Even
using 3M sentence pairs for training, the differ-
ences between them are still statistically signifi-
cant (p < 0.05) using approximate randomisation
(Noreen, 1989) for significance testing.
40K 160K 640K 3M
CS 8.33 12.47 14.40 17.80
ICT 10.17 14.85 17.20 20.50
LDC 9.37 13.88 15.86 19.59
Stanford 10.45 15.26 16.94 20.64
Table 1: Word segmentation on NIST data sets
However, when tested on out-of-domain data,
i.e. IWSLT data in the dialogue domain, the re-
sults seem to be more difficult to predict. We
trained the system on different sizes of data and
evaluated the system on two test sets: IWSLT
2006 and 2007. From Table 2, we can see that on
the IWSLT 2006 test sets, LDC achieves consis-
tently good results and the Stanford segmenter is
the worst.4 Furthermore, the character-based seg-
mentation also achieves competitive results. On
IWSLT 2007, all monolingual segmenters outper-
form character-based segmentation and the LDC
segmenter is only slightly better than the other seg-
menters.
From the experiments reported above, we
can reach the following conclusions. First of
all, character-based segmentation cannot achieve
state-of-the-art results in most experimental SMT
settings. This also motivates the necessity to
work on better segmentation strategies. Second,
monolingual segmenters cannot achieve consis-
4Interestingly, the developers themselves also note the
sensitivity of the Stanford segmenter and incorporate exter-
nal lexical information to address such problems (Chang et
al., 2008).
550
40K 160K
IWSLT06 CS 19.31 23.06
Manual 19.94 -
ICT 20.34 23.36
LDC 20.37 24.34
Stanford 18.25 21.40
IWSLT07 CS 29.59 30.25
Manual 33.85 -
ICT 31.18 33.38
LDC 31.74 33.44
Stanford 30.97 33.41
Table 2: Word segmentation on IWSLT data sets
tently good results when used in another domain.
In the following sections, we propose a bilingually
motivated segmentation approach which can be
automatically derived from a small representative
data set and the experiments show that we can con-
sistently obtain state-of-the-art results in different
domains.
3 Bilingually Motivated Word
Segmentation
3.1 Notation
While in this paper, we focus on Chinese?English,
the method proposed is applicable to other lan-
guage pairs. The notation, however, assumes
Chinese?English MT. Given a Chinese sentence
cJ1 consisting of J characters {c1, . . . , cJ} and
an English sentence eI1 consisting of I words
{e1, . . . , eI}, AC?E will denote a Chinese-to-
English word alignment between cJ1 and eI1. Since
we are primarily interested in 1-to-n alignments,
AC?E can be represented as a set of pairs ai =
?Ci, ei? denoting a link between one single En-
glish word ei and a few Chinese characters Ci.The
set Ci is empty if the word ei is not aligned to any
character in cJ1 .
3.2 Candidate Extraction
In the following, we assume the availability of an
automatic word aligner that can output alignments
AC?E for any sentence pair (cJ1 , eI1) in a paral-
lel corpus. We also assume that AC?E contain
1-to-n alignments. Our method for Chinese word
segmentation is as follows: whenever a single En-
glish word is aligned with several consecutive Chi-
nese characters, they are considered candidates for
grouping. Formally, given an alignment AC?E
between cJ1 and eI1, if ai = ?Ci, ei? ? AC?E ,
with Ci = {ci1 , . . . , cim} and ?k ? J1,m ? 1K,
ik+1 ? ik = 1, then the alignment ai between ei
and the sequence of words Ci is considered a can-
didate word. Some examples of such 1-to-n align-
ments between Chinese and English we can derive
automatically are displayed in Figure 1.5
Figure 1: Example of 1-to-n word alignments be-
tween English words and Chinese characters
3.3 Candidate Reliability Estimation
Of course, the process described above is error-
prone, especially on a small amount of training
data. If we want to change the input segmentation
to give to the word aligner, we need to make sure
that we are not making harmful modifications. We
thus additionally evaluate the reliability of the can-
didates we extract and filter them before inclusion
in our bilingual dictionary. To perform this filter-
ing, we use two simple statistical measures. In the
following, ai = ?Ci, ei? denotes a candidate.
The first measure we consider is co-occurrence
frequency (COOC(Ci, ei)), i.e. the number of
times Ci and ei co-occur in the bilingual corpus.
This very simple measure is frequently used in as-
sociative approaches (Melamed, 2000). The sec-
ond measure is the alignment confidence (Ma et
al., 2007), defined as
AC(ai) =
C(ai)
COOC(Ci, ei)
,
where C(ai) denotes the number of alignments
proposed by the word aligner that are identical to
ai. In other words, AC(ai) measures how often
the aligner aligns Ci and ei when they co-occur.
We also impose that |Ci | ? k, where k is a fixed
integer that may depend on the language pair (be-
tween 3 and 5 in practice). The rationale behind
this is that it is very rare to get reliable alignments
between one word and k consecutive words when
k is high.
5While in this paper we are primarily concerned with lan-
guages where the word boundaries are not orthographically
marked, this approach, however, can also be applied to lan-
guages marked with word boundaries to construct bilingually
motivated ?words?.
551
The candidates are included in our bilingual dic-
tionary if and only if their measures are above
some fixed thresholds tCOOC and tAC , which al-
low for the control of the size of the dictionary and
the quality of its contents. Some other measures
(including the Dice coefficient) could be consid-
ered; however, it has to be noted that we are more
interested here in the filtering than in the discov-
ery of alignments per se, since our method builds
upon an existing aligner. Moreover, we will see
that even these simple measures can lead to an im-
provement in the alignment process in an MT con-
text.
3.4 Bootstrapped word segmentation
Once the candidates are extracted, we perform
word segmentation using the bilingual dictionar-
ies constructed using the method described above;
this provides us with an updated training corpus,
in which some character sequences have been re-
placed by a single token. This update is totally
naive: if an entry ai = ?Ci, ei? is present in the
dictionary and matches one sentence pair (cJ1 , eI1)
(i.e. Ci and ei are respectively contained in cJ1 and
eI1), then we replace the sequence of characters Ci
with a single token which becomes a new lexical
unit.6 Note that this replacement occurs even if
no alignment was found between Ci and ei for the
pair (cJ1 , eI1). This is motivated by the fact that the
filtering described above is quite conservative; we
trust the entry ai to be correct.
This process can be applied several times: once
we have grouped some characters together, they
become the new basic unit to consider, and we can
re-run the same method to get additional group-
ings. However, we have not seen in practice much
benefit from running it more than twice (few new
candidates are extracted after two iterations).
4 Word Lattice Decoding
4.1 Word Lattices
In the decoding stage, the various segmentation
alternatives can be encoded into a compact rep-
resentation of word lattices. A word lattice G =
?V,E? is a directed acyclic graph that formally is
a weighted finite state automaton. In the case of
word segmentation, each edge is a candidate word
associated with its weights. A straightforward es-
6In case of overlap between several groups of words to
replace, we select the one with the highest confidence (ac-
cording to tAC).
timation of the weights is to distribute the proba-
bility mass for each node uniformly to each out-
going edge. The single node having no outgoing
edges is designated the ?end node?. An example
of word lattices for a Chinese sentence is shown in
Figure 2.
4.2 Word Lattice Generation
Previous research on generating word lattices re-
lies on multiple monolingual segmenters (Xu et
al., 2005; Dyer et al, 2008). One advantage of
our approach is that the bilingually motivated seg-
mentation process facilitates word lattice genera-
tion without relying on other segmenters. As de-
scribed in section 3.4, the update of the training
corpus based on the constructed bilingual dictio-
nary requires that the sentence pair meets the bilin-
gual constraints. Such a segmentation process in
the training stage facilitates the utilisation of word
lattice decoding.
4.3 Phrase-Based Word Lattice Decoding
Given a Chinese input sentence cJ1 consisting of J
characters, the traditional approach is to determine
the best word segmentation and perform decoding
afterwards. In such a case, we first seek a single
best segmentation:
f?K1 = arg max
fK1 ,K
{Pr(fK1 |cJ1 )}
Then in the decoding stage, we seek:
e?I1 = arg max
eI1,I
{Pr(eI1|f?K1 )}
In such a scenario, some segmentations which are
potentially optimal for the translation may be lost.
This motivates the need for word lattice decoding.
The search process can be rewritten as:
e?I1 = arg max
eI1,I
{max
fK1 ,K
Pr(eI1, f
K
1 |cJ1 )}
= arg max
eI1,I
{max
fK1 ,K
Pr(eI1)Pr(f
K
1 |eI1, cJ1 )}
= arg max
eI1,I
{max
fK1 ,K
Pr(eI1)Pr(f
K
1 |eI1)Pr(fK1 |cJ1 )}
Given the fact that the number of segmentations
fK1 grows exponentially with respect to the num-
ber of characters K , it is impractical to firstly enu-
merate all possible fK1 and then to decode. How-
ever, it is possible to enumerate all the alternative
segmentations for a substring of cJ1 , making the
utilisation of word lattices tractable in PB-SMT.
552
Figure 2: Example of a word lattice
5 Experimental Setting
5.1 Evaluation
The intrinsic quality of word segmentation is nor-
mally evaluated against a manually segmented
gold-standard corpus using F-score. While this
approach can give a direct evaluation of the qual-
ity of the word segmentation, it is faced with sev-
eral limitations. First of all, it is really difficult to
build a reliable and objective gold-standard given
the fact that there is only 70% agreement between
native speakers on this task (Sproat et al, 1996).
Second, an increase in F-score does not necessar-
ily imply an improvement in translation quality. It
has been shown that F-score has a very weak cor-
relation with SMT translation quality in terms of
BLEU score (Zhang et al, 2008). Consequently,
we chose to extrinsically evaluate the performance
of our approach via the Chinese?English transla-
tion task, i.e. we measure the influence of the
segmentation process on the final translation out-
put. The quality of the translation output is mainly
evaluated using BLEU, with NIST (Doddington,
2002) and METEOR (Banerjee and Lavie, 2005)
as complementary metrics.
5.2 Data
The data we used in our experiments are from
two different domains, namely news and travel di-
alogues. For the news domain, we trained our
system using a portion of UN data for NIST
2006 evaluation campaign. The system was de-
veloped on LDC Multiple-Translation Chinese
(MTC) Corpus and tested on MTC part 2, which
was also used as a test set for NIST 2002 evalua-
tion campaign.
For the dialogue data, we used the Chinese?
English datasets provided within the IWSLT 2007
evaluation campaign. Specifically, we used the
standard training data, to which we added devset1
and devset2. Devset4 was used to tune the param-
eters and the performance of the system was tested
on both IWSLT 2006 and 2007 test sets. We used
both test sets because they are quite different in
terms of sentence length and vocabulary size. To
test the scalability of our approach, we used HIT
corpus provided within IWSLT 2008 evaluation
campaign. The various statistics for the corpora
are shown in Table 3.
5.3 Baseline System
We conducted experiments using different seg-
menters with a standard log-linear PB-SMT
model: GIZA++ implementation of IBM word
alignment model 4 (Och and Ney, 2003), the
refinement and phrase-extraction heuristics de-
scribed in (Koehn et al, 2003), minimum-error-
rate training (Och, 2003), a 5-gram language
model with Kneser-Ney smoothing trained with
SRILM (Stolcke, 2002) on the English side of the
training data, and Moses (Koehn et al, 2007; Dyer
et al, 2008) to translate both single best segmen-
tation and word lattices.
6 Experiments
6.1 Results
The initial word alignments are obtained using
the baseline configuration described above by seg-
menting the Chinese sentences into characters.
From these we build a bilingual 1-to-n dictionary,
and the training corpus is updated by grouping the
characters in the dictionaries into a single word,
using the method presented in section 3.4. As pre-
viously mentioned, this process can be repeated
several times. We then extract aligned phrases us-
ing the same procedure as for the baseline sys-
tem; the only difference is the basic unit we are
considering. Once the phrases are extracted, we
perform the estimation of weights for the fea-
tures of the log-linear model. We then use a
simple dictionary-based maximum matching algo-
rithm to obtain a single-best segmentation for the
Chinese sentences in the development set so that
553
Train Dev. Eval.
Zh En Zh En Zh En
Dialogue Sentences 40,958 489 (7 ref.) 489 (6 ref.)/489 (7 ref.)
Running words 488,303 385,065 8,141 46,904 8,793/4,377 51,500/23,181
Vocabulary size 2,742 9,718 835 1,786 936/772 2,016/1,339
News Sentences 40,000 993 (9 ref.) 878 (4 ref.)
Running words 1,412,395 956,023 41,466 267,222 38,700 105,530
Vocabulary size 6057 20,068 1,983 10,665 1,907 7,388
Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En)
minimum-error-rate training can be performed.7
Finally, in the decoding stage, we use the same
segmentation algorithm to obtain the single-best
segmentation on the test set, and word lattices can
also be generated using the bilingual dictionary.
The various parameters of the method (k, tCOOC ,
tAC , cf. section 3) were optimised on the develop-
ment set. One iteration of character grouping on
the NIST task was found to be enough; the optimal
set of values was found to be k = 3, tAC = 0.0
and tCOOC = 0, meaning that all the entries in the
bilingually dictionary are kept. On IWSLT data,
we found that two iterations of character grouping
were needed: the optimal set of values was found
to be k = 3, tAC = 0.3, tCOOC = 8 for the first
iteration, and tAC = 0.2, tCOOC = 15 for the
second.
As can be seen from Table 4, our bilingually
motivated segmenter (BS) achieved statistically
significantly better results than character-based
segmentation when enhanced with word lattice de-
coding.8 Compared to the best in-domain seg-
menter, namely the Stanford segmenter on this
particular task, our approach is inferior accord-
ing to BLEU and NIST. We firstly attribute this
to the small amount of training data, from which
a high quality bilingual dictionary cannot be ob-
tained due to data sparseness problems. We also
attribute this to the vast amount of named entity
terms in the test sets, which is extremely difficult
for our approach.9 We expect to see better re-
sults when a larger amount of data is used and the
segmenter is enhanced with a named entity recog-
niser. On IWSLT data (cf. Tables 5 and 6), our
7In order to save computational time, we used the same
set of parameters obtained above to decode both the single-
best segmentation and the word lattice.
8Note the BLEU scores are particularly low due to the
number of references used (4 references), in addition to the
small amount of training data available.
9As we previously point out, both ICT and Stanford seg-
menters are equipped with named entity recognition func-
tionality. This may risk causing data sparseness problems on
small training data. However, this is beneficial in the transla-
tion process compared to character-based segmentation.
approach yielded a consistently good performance
on both translation tasks compared to the best in-
domain segmenter?the LDC segmenter. More-
over, the good performance is confirmed by all
three evaluation measures.
BLEU NIST METEOR
CS 8.43 4.6272 0.3778
Stanford 10.45 5.0675 0.3699
BS-SingleBest 7.98 4.4374 0.3510
BS-WordLattice 9.04 4.6667 0.3834
Table 4: BS on NIST task
BLEU NIST METEOR
CS 0.1931 6.1816 0.4998
LDC 0.2037 6.2089 0.4984
BS-SingleBest 0.1865 5.7816 0.4602
BS-WordLattice 0.2041 6.2874 0.5124
Table 5: BS on IWSLT 2006 task
BLEU NIST METEOR
CS 0.2959 6.1216 0.5216
LDC 0.3174 6.2464 0.5403
BS-SingleBest 0.3023 6.0476 0.5125
BS-WordLattice 0.3171 6.3518 0.5603
Table 6: BS on IWSLT 2007 task
6.2 Parameter Search Graph
The reliability estimation process is computation-
ally intensive. However, this can be easily paral-
lelised. From our experiments, we observed that
the translation results are very sensitive to the pa-
rameters and this search process is essential to
achieve good results. Figure 3 is the search graph
on the IWSLT data set in the first iteration step.
From this graph, we can see that filtering of the
bilingual dictionary is essential in order to achieve
better performance.
554
Figure 3: The search graph on development set of
IWSLT task
6.3 Vocabulary Size
Our bilingually motivated segmentation approach
has to overcome another challenge in order to
produce competitive results, i.e. data sparseness.
Given that our segmentation is based on bilingual
dictionaries, the segmentation process can signif-
icantly increase the size of the vocabulary, which
could potentially lead to a data sparseness prob-
lem when the size of the training data is small. Ta-
bles 7 and 8 list the statistics of the Chinese side
of the training data, including the total vocabulary
(Voc), number of character vocabulary (Char.voc)
in Voc, and the running words (Run.words) when
different word segmentations were used. From Ta-
ble 7, we can see that our approach suffered from
data sparseness on the NIST task, i.e. a large
vocabulary was generated, of which a consider-
able amount of characters still remain as separate
words. On the IWSLT task, since the dictionary
generation process is more conservative, we main-
tained a reasonable vocabulary size, which con-
tributed to the final good performance.
Voc. Char.voc Run. Words
CS 6,057 6,057 1,412,395
ICT 16,775 1,703 870,181
LDC 16,100 2,106 881,861
Stanford 22,433 1,701 880,301
BS 18,111 2,803 927,182
Table 7: Vocabulary size of NIST task (40K)
6.4 Scalability
The experimental results reported above are based
on a small training corpus containing roughly
40,000 sentence pairs. We are particularly inter-
ested in the performance of our segmentation ap-
Voc. Char.voc Run. Words
CS 2,742 2,742 488,303
ICT 11,441 1,629 358,504
LDC 9,293 1,963 364,253
Stanford 18,676 981 348,251
BS 3,828 2,740 402,845
Table 8: Vocabulary size of IWSLT task (40K)
proach when it is scaled up to larger amounts of
data. Given that the optimisation of the bilingual
dictionary is computationally intensive, it is im-
practical to directly extract candidate words and
estimate their reliability. As an alternative, we can
use the obtained bilingual dictionary optimised on
the small corpus to perform segmentation on the
larger corpus. We expect competitive results when
the small corpus is a representative sample of the
larger corpus and large enough to produce reliable
bilingual dictionaries without suffering severely
from data sparseness.
As we can see from Table 9, our segmenta-
tion approach achieved consistent results on both
IWSLT 2006 and 2007 test sets. On the NIST task
(cf. Table 10), our approach outperforms the basic
character-based segmentation; however, it is still
inferior compared to the other in-domain mono-
lingual segmenters due to the low quality of the
bilingual dictionary induced (cf. section 6.1).
IWSLT06 IWSLT07
CS 23.06 30.25
ICT 23.36 33.38
LDC 24.34 33.44
Stanford 21.40 33.41
BS-SingleBest 22.45 30.76
BS-WordLattice 24.18 32.99
Table 9: Scale-up to 160K on IWSLT data sets
160K 640K
CS 12.47 14.40
ICT 14.85 17.20
LDC 13.88 15.86
Stanford 15.26 16.94
BS-SingleBest 12.58 14.11
BS-WordLattice 13.74 15.33
Table 10: Scalability of BS on NIST task
555
6.5 Using different word aligners
The above experiments rely on GIZA++ to per-
form word alignment. We next show that our ap-
proach is not dependent on the word aligner given
that we have a conservative reliability estimation
procedure. Table 11 shows the results obtained on
the IWSLT data set using the MTTK alignment
tool (Deng and Byrne, 2005; Deng and Byrne,
2006).
IWSLT06 IWSLT07
CS 21.04 31.41
ICT 20.48 31.11
LDC 20.79 30.51
Stanford 17.84 29.35
BS-SingleBest 19.22 29.75
BS-WordLattice 21.76 31.75
Table 11: BS on IWSLT data sets using MTTK
7 Related Work
(Xu et al, 2004) were the first to question the use
of word segmentation in SMT and showed that the
segmentation proposed by word alignments can be
used in SMT to achieve competitive results com-
pared to using monolingual segmenters. Our ap-
proach differs from theirs in two aspects. Firstly,
(Xu et al, 2004) use word aligners to reconstruct
a (monolingual) Chinese dictionary and reuse this
dictionary to segment Chinese sentences as other
monolingual segmenters. Our approach features
the use of a bilingual dictionary and conducts a
different segmentation. In addition, we add a pro-
cess which optimises the bilingual dictionary ac-
cording to translation quality. (Ma et al, 2007)
proposed an approach to improve word alignment
by optimising the segmentation of both source and
target languages. However, the reported experi-
ments still rely on some monolingual segmenters
and the issue of scalability is not addressed. Our
research focuses on avoiding the use of monolin-
gual segmenters in order to improve the robustness
of segmenters across different domains.
(Xu et al, 2005) were the first to propose the
use of word lattice decoding in PB-SMT, in order
to address the problems of segmentation. (Dyer
et al, 2008) extended this approach to hierarchi-
cal SMT systems and other language pairs. How-
ever, both of these methods require some mono-
lingual segmentation in order to generate word lat-
tices. Our approach facilitates word lattice gener-
ation given that our segmentation is driven by the
bilingual dictionary.
8 Conclusions and Future Work
In this paper, we introduced a bilingually moti-
vated word segmentation approach for SMT. The
assumption behind this motivation is that the lan-
guage to be segmented can be tokenised into ba-
sic writing units. Firstly, we extract 1-to-n word
alignments using statistical word aligners to con-
struct a bilingual dictionary in which each entry
indicates a correspondence between one English
word and n Chinese characters. This dictionary is
then filtered using a few simple association mea-
sures and the final bilingual dictionary is deployed
for word segmentation. To overcome the segmen-
tation problem in the decoding stage, we deployed
word lattice decoding.
We evaluated our approach on translation tasks
from two different domains and demonstrate that
our approach is (i) not as sensitive as monolingual
segmenters, and (ii) that the SMT system using
our word segmentation can achieve state-of-the-art
performance. Moreover, our approach can easily
be scaled up to larger data sets and achieves com-
petitive results if the small data used is a represen-
tative sample.
As for future work, firstly we plan to integrate
some named entity recognisers into our approach.
We also plan to try our approach in more do-
mains and on other language pairs (e.g. Japanese?
English). Finally, we intend to explore the corre-
lation between vocabulary size and the amount of
training data needed in order to achieve good re-
sults using our approach.
Acknowledgments
This work is supported by Science Foundation Ire-
land (O5/IN/1732) and the Irish Centre for High-
End Computing.10 We would like to thank the re-
viewers for their insightful comments.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, MI.
10http://www.ichec.ie/
556
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224?232, Columbus, OH.
Yonggang Deng and William Byrne. 2005. HMM
word and phrase alignment for statistical machine
translation. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
169?176, Vancouver, BC, Canada.
Yonggang Deng and William Byrne. 2006. MTTK:
An alignment toolkit for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, pages 265?268,
New York City, NY.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1012?1020, Colum-
bus, OH.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 48?54, Edmonton, AL,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 304?
311, Prague, Czech Republic.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA.
Richard W Sproat, Chilin Shih, William Gale, and
Nancy Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377?404.
Andrea Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904, Denver, CO.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 168?
171, Jeju Island, Republic of Korea.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th International
Conference on Computational Linguistics, pages
836?841, Copenhagen, Denmark.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical
machine translation? In ACL SIGHAN Workshop
2004, pages 122?128, Barcelona, Spain.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings
of the International Workshop on Spoken Language
Translation, pages 141?147, Pittsburgh, PA.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun
Liu. 2003. HHMM-based Chinese lexical ana-
lyzer ICTCLAS. In Proceedings of Second SIGHAN
Workshop on Chinese Language Processing, pages
184?187, Sappora, Japan.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 216?223, Columbus, OH.
557
c? 2003 Association for Computational Linguistics
wEBMT: Developing and Validating an
Example-Based Machine Translation
System Using the World Wide Web
Andy Way? Nano Gough?
Dublin City University Dublin City University
We have developed an example-based machine translation (EBMT) system that uses the World
Wide Web for two different purposes: First, we populate the system?s memory with translations
gathered from rule-based MT systems located on the Web. The source strings input to these
systems were extracted automatically from an extremely small subset of the rule types in the Penn-
II Treebank. In subsequent stages, the ?source, target? translation pairs obtained are automatically
transformed into a series of resources that render the translation process more successful. Despite
the fact that the output from on-line MT systems is often faulty, we demonstrate in a number
of experiments that when used to seed the memories of an EBMT system, they can in fact prove
useful in generating translations of high quality in a robust fashion. In addition, we demonstrate
the relative gain of EBMT in comparison to on-line systems. Second, despite the perception that
the documents available on the Web are of questionable quality, we demonstrate in contrast that
such resources are extremely useful in automatically postediting translation candidates proposed
by our system.
1. Introduction
In quite a short space of time, translation memory (TM) systems have become a very
useful tool in the translator?s armory. TM systems store a set of ?source, target? trans-
lation pairs in their databases. If a new input string cannot be found exactly in the
translation database, a search is conducted for close (or ?fuzzy?) matches of the input
string, and these are retrieved together with their translations for the translator to
manipulate into the final, output translation. From this description, it should be clear
that TM systems do not translate: Indeed, some researchers consider them to be little
more than a search-and-replace engine, albeit a rather sophisticated one (Macklovitch
and Russell 2000).
We can illustrate this with respect to the TM entries in (1), taken from the Canadian
Hansards:
(1) a. While most were critical, some contributions were thoughtful and
constructive =? La plupart ont formule? des critiques, mais certains ont
fait des observations re?fle?chies et constructives.
b. Others were plain meanspirited and some contained errors of fact =?
D?autres discours comportaient des propos mesquins et me?me des
erreurs de fait.
? School of Computing, Dublin 9, Ireland. E-mail: away@computing.dcu.ie
? School of Computing, Dublin 9, Ireland. E-mail: ngough@computing.dcu.ie
422
Computational Linguistics Volume 29, Number 3
Consider the new source string in (2):
(2) While most were critical, some contributions were plain meanspirited.
Despite the fact that this new input in (2) is extremely close to the source strings
in the TM entries in (1), no TM system containing just these translation pairs in its
database would be able to translate (2); the best they could do would be to identify
one or both of the two source sentences in the TM in (1) as fuzzy matches and display
these, together with their French translations. The translator would then manipulate
the target strings in the TM into the final translation (3):
(3) La plupart ont formule? des critiques, mais certains ont fait des
observations mesquines.
An alternative translation that might be derived from the TM entries in (1) is that in
(4):
(4) La plupart ont formule? des critiques, mais certains comportaient des
observations mesquines.
At all stages in the translation process, therefore, the translators themselves are the
integral figures: They are free to accept or reject any suggested matches, they construct
the translations, and they may or may not use any translations proposed by the TM
system to formulate the translations in the target document. Finally, they are free to
insert the translations produced into the TM itself as they see fit: that is, either (3) or
(4) could be inserted into the TM with the source string (2), or some other translation,
if that were preferred.
A prerequisite for TM (and example-based machine translation [EBMT]) applica-
tions is a parallel corpus aligned at sentential level. Such a corpus may be presented
to translators en bloc, or translators may help construct it themselves. Here too the
translator maintains a large degree of autonomy: Using a tool such as Trados WinAlign,
for example, he or she may manually overwrite some of the aligner?s decisions by
linking ?source, target? sentence pairs using the graphical interface provided.
Nevertheless, TM systems are currently falling far short of their potential, given
the limitation that the smallest accessible translation units are ?source, target? strings
aligned only at sentential level. Consider the fuzzy matching operation, for instance:
Translators are able to set a fuzzy match threshold below which no translation pairs
are proposed by the TM system. If this threshold is set too low, then potentially useful
translation pairs will be presented along with a lot of noise, thereby risking that this
useful translation information will be obscured (high recall, low precision); if it is set
too low, then good matches will be presented, but potentially useful matches will
not be (low recall, high precision). We noted above that faced with the new input in
(2), a TM system might be able to present the translator with the fuzzy matches in
(1). However, if a translator were to set the level of fuzzy matching at 80% (a not
unreasonable level), then neither of the translation pairs in (1) would be deemed to
be a suitably good fuzzy match, as only 7/9 (77%) of the words in (1a) match those
in (2) exactly, and only 3/9 (33%) of the words in (1b) match those in (2) exactly.
Indeed, setting an appropriate fuzzy match level is such a difficult problem that some
translators switch off this option and use the TM only to find exact matches.
If subsentential alignment could be integrated into the TM databases, more useful
fragments could be put at the disposal of the translator. If we could fragment the
423
Way and Gough wEBMT
sententially aligned TM examples in (1) so that subsentential chunks were displayed
to the user, then the chance of finding exact matches or good fuzzy matches would
increase considerably. This is currently beyond the scope of TM systems.
In contrast, EBMT systems have overcome this constraint by storing subsentential
translational correspondences in addition to the sententially aligned pairs from which
they are derived. As a consequence, where a TM system can only propose a number
of close-scoring matches in its database for the translator to adapt into the final trans-
lation, an EBMT system can produce translations itself by automatically combining
chunks from different translation examples stored in its memories.
In Section 2, we describe how we automatically obtain a hierarchy of lexical re-
sources that are used sequentially by our EBMT system, wEBMT, to translate new
input. The primary resource gathered is a ?phrasal lexicon,? constructed by extracting
over 200,000 phrases from the Penn Treebank and having them translated into French
by three Web-based machine translation (MT) systems.
Each set of translations is stored separately, and for each set the ?marker hypoth-
esis? (Green 1979) is used to segment the phrasal lexicon into a ?marker lexicon.? The
marker hypothesis is a universal psycholinguistic constraint which states that natural
languages are ?marked? for complex syntactic structure at surface form by a closed
set of specific lexemes and morphemes. That is, a basic phrase-level segmentation of
an input sentence can be achieved by exploiting a closed list of known marker words
to signal the start and end of each segment.
Consider the following example, selected at random from the Wall Street Journal
section of the Penn-II Treebank:
(5) The Dearborn, Mich., energy company stopped paying a dividend in the
third quarter of 1984 because of troubles at its Midland nuclear plant.
Here we see that three noun phrases start with determiners and one with a possessive
pronoun. The sets of determiners and possessive pronouns are both very small. Fur-
thermore, there are four prepositional phrases, and the set of prepositions is similarly
small. A further assumption that could be made is that all words that end with -ed are
verbs, such as stopped in (5). The marker hypothesis is arguably universal in presum-
ing that concepts and structures like these have similar morphological or structural
marking in all languages.
The marker hypothesis has been used for a number of different language-related
tasks, including
? language learning (Green 1979; Mori and Moeser 1983; Morgan, Meier,
and Newport 1989)
? monolingual grammar induction (Juola 1998)
? grammar optimization (Juola 1994)
? insights into universal grammar (Juola 1998)
? machine translation (Juola 1994, 1997; Veale and Way 1997; Gough, Way,
and Hearne 2002)
With respect to translation, a potential problem in using the marker hypothesis is that
some languages do not have marker words such as articles, for instance. Green?s (1979)
work showed that artificial languages, both with and without specific marker words,
may be learned more accurately and quickly if such psycholinguistic cues exist. The
424
Computational Linguistics Volume 29, Number 3
research of Mori and Moeser (1983) showed a similar effect due to case marking on
pseudowords in such artificial languages, and Morgan, Meier, and Newport (1989)
demonstrated that languages that do not permit pronouns as substitutes for phrases
also provide evidence in favor of the marker hypothesis. Juola?s (1994, 1998) work
on grammar optimization and induction shows that context-free grammars can be
converted to ?marker-normal form.? However, marker-normal form grammars cannot
capture the sorts of regularities demonstrated for languages that do not have a one-
to-one mapping between a terminal symbol and a word. Nevertheless, Juola (1998,
page 23) observes that ?a slightly more general mapping, where two adjacent termi-
nal symbols can be merged into a single lexical item (for example, a word and its
case-marking), can capture this sort of result quite handily.? Work using the marker
hypothesis for MT adapts this monolingual mapping for pairs of languages: It is rea-
sonably straightforward to map an English determiner-noun sequence onto a Japanese
noun?case marker segment, once one has identified the sets of marker tags in the lan-
guages to be translated.
Following construction of the marker lexicon, the ?source, target? chunks are gen-
eralized further using a methodology based on Block (2000) to permit a limited form
of insertion in the translation process. As a byproduct of the chosen methodology,
we also derive a standard ?word-level? translation lexicon. These various resources
render the set of original translation pairs far more useful in deriving translations of
previously unseen input.
In Section 3, we describe in detail the segmentation process, together with the
procedure whereby target chunks are combined to produce candidate translations. In
Section 4, we report initially on two experiments in which we test different versions
of our EBMT system against test sets of NPs and sentences. We then conduct a set of
further experiments which show that using the resources developed from more than
one on-line MT system may improve both translation coverage and quality. Further-
more, seeding the system databases with more fragments improves translation quality.
In addition, we calculate the net gain of our EBMT system by comparing translation
quality against that of the three on-line MT systems. Finally, we comment on the
relative strengths and weaknesses of the three on-line MT systems used.
Like most EBMT systems, our approach suffers from the problem of ?boundary
friction?: where chunks from different translation examples are recombined, the quality
of the resulting translations may be compromised. Assume that the aligned examples
in (6) are located in the system database:
(6) a. You can attach a phone to the connector =? Vous pouvez re?lier un
te?le?phone au connecteur.
b. Connect only the keyboard and a mouse =? Connectez uniquement
le clavier et une souris.
Let us now confront the EBMT system with the new input string in (7):
(7) You can attach a mouse to the connector.
This could be correctly translated by the EBMT system by isolating the useful trans-
lation fragments in (8):
(8) a. You can attach =? Vous pouvez re?lier (from (6a))
b. a mouse =? une souris (from (6b))
c. to the connector =? au connecteur (from (6a))
425
Way and Gough wEBMT
Recombining the French chunks gives us the correct translation in (9):
(9) Vous pouvez re?lier une souris au connecteur.
However, a number of mistranslations could also ensue, including those in (10):
(10) a. *Vous pouvez re?lier un souris au connecteur.
b. *Vous pouvez re?lier un souris au le connecteur.
The mistranslation (10a) could be formed via the set of inferences in (11):
(11) You can attach a =? Vous pouvez re?lier un (from (6a))
mouse =? souris (from (6b))
to the connector =? au connecteur (from (6a))
The mistranslation (10b) could be formed via the set of inferences in (12):
(12) You can attach a =? Vous pouvez re?lier un (from (6a))
mouse =? souris (from (6b))
to =? au (from (6a))
the =? le (from (6b))
connector =? connecteur (from (6a))
It is clear, therefore, that unless the process by which the original ?source, target?
sentence pairs are fragmented is well defined and strictly controlled, chunks may
be combined from different contexts that result in agreement errors such as those in
(10).1 Depending on the input string, our wEBMT system may generate thousands
of candidate translations, including many mistranslations like those in (10). A major
advantage of MT systems based on probabilities is that output translations can be
ranked (and pruned, if required): One would hope that such systems would rank
good translations such as that in (9) more highly than poor ones such as those in (10).
We demonstrate that in almost all experiments, our EBMT system consistently ranks
the ?best? translation in the top 10 output translations, and always in the top 1% of
the translations generated.
In order to minimize errors of boundary friction, in Section 5 we develop a novel,
post hoc procedure via the World Wide Web to validate and, if necessary, correct
translations prior to their being output to the user.2 Finally we conclude and point to
areas of future research.
1 Note also that with respect to the translations given in (3) and (4), the translator interacting with the
TM has used his or her translation knowledge to avoid a problem of boundary friction: Given the TM
entries in (1), the translation of plain meanspirited would appear to be mesquins. This is correct in this
context, as it co-occurs with a masculine plural noun propos. In translating (2), however, observations is a
feminine plural noun, so the adjective mesquines is inserted to maintain agreement throughout the NP.
If the translation pair ?plain meanspirited, mesquines? were not found in the system?s memories, then
only the mistranslation observations mesquins could be produced by an EBMT system.
2 One of the areas of boundary friction that we use our post hoc validation procedure to correct is that of
subject-verb agreement. Note that with examples such as (18), this is not usually (such) a problem for
marker-based approaches to MT as we face here, as verbs are contained within (part of) the same
chunk as their subject NPs. However, given that we translate phrases rather than sentences, it is a
considerable problem for our approach, yet one that we overcome satisfactorily. In further work, if we
were to store the translations of the VPs with their dummy subject NPs in a sentential lexicon and
derive all marker lexicons from this database, the problem of subject-verb agreement would be largely
overcome.
426
Computational Linguistics Volume 29, Number 3
2. Deriving Translation Resources from Web-Based MT Systems
All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection
of Carl and Way (2003), are premised on the availability of subsentential alignments
derived from the input bitext. There is a wealth of literature on trying to establish sub-
sentential translations from a bilingual corpus.3 Kay and Ro?scheisen (1993) attempt to
extract a bilingual dictionary using a hybrid method of sentence and word alignment
on the assumption that the ?source, target? words have a similar distribution. Fung
and McKeown (1997) attempt to translate technical terms using word relation matrices,
although the resource from which such relations are derived is a pair of nonparallel
corpora. Somers (1998) replicates the work of Fung and McKeown with different lan-
guage pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis
(1998) use a tagged parallel corpus to extract translationally equivalent English-Greek
clauses on the basis of word occurrence and co-occurrence probabilities. The respec-
tive lengths of the putative alignments in terms of characters is also an important
factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken
languages, the relative lack of linguistic tools and resources has forced developers of
word alignment tools for such languages to use shallow processing and basic statis-
tical approaches to word linking. Accordingly, they generate lexical correspondences
by means of co-occurrence measures and string similarity metrics.
More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has
been used successfully in a number of areas:
? Learnability (Zernik and Dyer 1987)
? Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996)
? Speech generation (Rayner and Carter 1997)
? Localization (Scha?ler 1996)
More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at
a subsentential level, while Carl, Way, and Scha?ler (2002) and Scha?ler, Way, and Carl
(2003, pages 108?109) describe how phrasal lexicons might come to occupy a central
place in a future hybrid integrated translation environment. This, they suggest, may
result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are
on the whole wary of MT technology, but once subsentential alignment is enabled,
translators will become aware of the benefits to be gained from ?source, target? phrasal
segments, and from there they suggest that ?it is a reasonably short step to enabling
an automated solution via the recombination element of EBMT systems such as those
described in [Carl and Way 2003].?
In this section, we describe how the memory of our EBMT system is seeded with
a set of translations obtained from Web-based MT systems. From this initial resource,
we subsequently derive a number of different databases that together allow many
new input sentences to be translated that it would not be possible to translate in
other systems. First, the phrasal lexicon is segmented using the marker hypothesis to
produce a marker lexicon. This is then generalized, following a methodology based on
Block (2000), to generate the ?generalized marker lexicon.? Finally, as a result of the
3 We refer the interested reader to the excellent and comprehensive bibliography on parallel text
processing available at http://www.up.univ-mrs.fr/?veronis/biblios/ptp.htm.
427
Way and Gough wEBMT
methodology chosen, we automatically derive a fourth resource, namely, a ?word-level
lexicon.?
2.1 The Phrasal Lexicon
Our phrasal lexicon was built by selecting a set of 218,697 English noun phrases and
verb phrases from the Penn Treebank. We identified all rule types occurring 1,000
or more times and eliminated those that were not relevant (e.g., rules dealing only
with numbers). Where rules contained just a single nonterminal on their right-hand
side, only those rules whose left-hand side was VP were retained in order to ensure
that we could handle intransitive verbs. In total, 59 rule types out of a total of over
29,000 (i.e., just 0.002% of the rules in Penn-II) were used in creating the various lexical
resources. For each of these 59 rule types, the tokens corresponding to the rule right-
hand sides were extracted. These extracted English phrases were then translated using
three on-line MT systems:
? SDL International?s Enterprise Translation Server4 (system A)
? Reverso by Softissimo5 (system B)
? Logomedia6 (system C)
Translating the NPs via these MT systems was reasonably straightforward. We
report in Section 4 on the quality of the French NPs produced, and in Section 5 we
discuss experiments designed to discover whether our EBMT system could improve
on any mistranslations obtained. Translating the VPs involved a little more thought:
In the main, on-line MT systems such as these work far better when they translate
sentences. In order to obtain finite verb forms rather than the default infinitival forms,
we provided dummy subjects. Initially these were third-person plural pronouns, which
caused similar verb forms to be created. This obviously biases the EBMT system more
in favor of third-person plural sentences. Nevertheless, using the WWW-based post hoc
evaluation methodology proposed in Section 5, we were still able to obtain reasonable
translations for non-third-person-plural sentences too. In a subsequent experiment,
we seed the databases of wEBMT with third-person singular verb forms by providing
third-person singular pronouns as the dummy subjects, and in a final experiment
we combine all third-person fragments (both singular and plural) into the system?s
memories and compare results on the same test set.
The on-line MT systems were selected purely because they enable batch translation
of large quantities of text. In our experience, the most efficient way to translate large
amounts of data via on-line MT systems is to send each document as an HTML page
with the phrases to be translated encoded as an ordered list. We automatically tagged
the English phrases with HTML codes and input them into each translation system
using the Unix wget function, which takes a URL as input and writes the corresponding
HTML document to a file. If the URL takes the form of a query, then the document
retrieved is the result of the query, namely, the translated Web page. Once this is
obtained, it is a simple process to retrieve the French translations and associate them
with their English source equivalents.
4 http://www.freetranslation.com
5 http://trans.voila.fr
6 http://www.logomedia.net
428
Computational Linguistics Volume 29, Number 3
2.2 The Marker Lexicons
Given that the marker hypothesis is arguably universal, it is clear that benefits may
accrue by using it to facilitate subsentential alignment of ?source, target? chunks. Juola
(1994, 1997) conducts some small experiments using his METLA system to show the
viability of this approach for English ?? French and English ?? Urdu. For the English
?? French language pair, Juola gives results of 61% correct translation when the
system is tested on the training corpus, and 36% accuracy when it is evaluated with
test data. For English ?? Urdu, Juola (1997, page 213) notes that ?the system learned
the original training corpus . . . perfectly and could reproduce it without errors?; that
is, it scored 100% accuracy when tested against the training corpus. On novel test
sentences, he gives results of 72% correct translation. In their Gaijin system, Veale and
Way (1997) give a result of 63% accurate translations obtained for English ?? German
on a test set of 791 sentences from CorelDRAW manuals.
As in METLA and Gaijin, we exploit lists of known marker words for each language
to indicate the start and end of segments. For English, our source language, we use
the sets of marker words in (13):
(13)
<DET> {the, a, an, those, these, . . . }
<PREP> {in, on, out, with, from, to, under, . . . }
<QUANT> {all, some, few, many, . . . }
<CONJ> {and, or, . . . }
<POSS> {my, your, our,. . . }
<PRON> {I, you, he, she, it,. . . }
A similar set (14) was produced for French, the target language in our wEBMT system:
(14)
<DET> {le, la, l?, les, ce, ces, ceux, cet, . . . }
<PREP> {dans, sur, avec, de, a`, sous, . . . }
<QUANT> {tous, tout, toutes, certain, quelques, beaucoup, . . . }
<CONJ> {et, ou, . . . }
<POSS> {mon, ma, mes, ton, ta, tes, notre, nos, . . . }
<PRON> {je, j?, tu, il, elle, . . . }
In a preprocessing stage, the aligned ?source, target? pairs in the phrasal lexicon are
traversed word by word, and whenever any such marker word is encountered, a new
chunk is begun, with the first word labeled with its marker category (<DET>, <PREP>,
etc.). The example in (15) illustrates the results of running the marker hypothesis over
the source phrase all uses of asbestos:
(15)
<QUANT> all uses
<PREP> of asbestos
In addition, we impose a further constraint that each chunk must also contain at least
one non?marker word, so that the phrase out in the cold will be viewed as one segment
(labeled with <PREP>), rather than split into still smaller chunks.
For each ?English, FrenchX? pair, where X is one of the sets of translations derived
from the three separate MT on-line systems (see above), we derive separate marker
lexicons for each of the 218,697 source phrases and target translations. This gives
429
Way and Gough wEBMT
us a total of 656,091 ?source, target? translation pairs (including many repetitions, of
course). Given that English and French have essentially the same word order, these
marker lexicons are predicated on the na??ve yet effective assumption that marker-
headed chunks in the source S map sequentially to their target equivalents T; that is,
chunkS1 ?? chunkT1, chunkS2 ?? chunkT2, . . .chunkSn ?? chunkTn, subject to their
marker categories matching, where possible. Using the previous example of all uses of
asbestos, this gives us the marker chunks in (16):
(16)
<QUANT> all uses : tous usages
<PREP> of asbestos : d? asbeste
Sometimes the number of marker chunks in the two languages differs, with respect to
both the marker categories and the number of chunks obtained. Consider the example
in (17):
(17) The man looks at the woman =? L?homme regarde la femme.
Once the marker hypothesis is applied to (17), it would be marked up as in (18):
(18) <DET> The man looks <PREP> at <DET> the woman =?
<DET> L? homme regarde <DET> la femme.
That is, the English verb subcategorizes for a PP complement which in this case con-
tains two marker words, whereas the French verb regarder is a straightforward tran-
sitive verb. It may appear, therefore, that there are three chunks in the English string
and only two on the French side, but this is not the case: The restriction that each
segment must contain at least one non?marker word ensures that we have just two
marker chunks for the English string in (18). However, it remains the case that the
chunks are tagged differently; we obtain the marker chunks in (19):
(19) English:
<DET> The man looks
<PREP> at <DET> the woman
French:
<DET> L? homme regarde
<DET> la femme
Our alignment method would therefore align the first English chunk with the first
French chunk, as their marker categories match. Note, of course, that this contains a
translation error: regarde translates not as looks but rather as looks at. Errors such as this
will adversely affect translation quality, but as we report in Section 4, good-quality
translations are obtainable on the whole. The second pair in (19), however, cannot
be mapped straightforwardly onto one another, as the marker categories differ. Nev-
ertheless, our algorithm would align ?<DET> the woman? with ?<DET> la femme,? as
their marker categories match. This ensures that as many potentially useful translation
fragments are generated as possible.
This na??ve alignment procedure works well between (broadly) similar languages
such as English and French, but there are cases even between quite closely related
languages in which the procedure breaks down. In order to increase translation quality
430
Computational Linguistics Volume 29, Number 3
still further, the mapping function needs to be improved to account for examples such
as (20):
(20) The man likes the woman =? La femme pla??t a` l?homme.
The like =? plaire case is an argument-switching (or relation-changing) example, in
that the subject in English becomes the indirect object in French, and the English object
translates as the French subject. If we were to apply the marker hypothesis to (20), we
would derive (21):
(21) <DET> The man likes <DET> the woman =?
<DET> La femme pla??t <PREP> a` <DET> l? homme.
That is, without recourse to a lexicon or information about the relative distribution of
words and their translations, we would derive the marker chunks in (22):
(22) a. <DET> The man likes =? <DET> La femme pla??t
b. <DET> the woman =? <DET> l? homme
Of course, both alignments are wrong. However, our alignment method correctly aligns
?source, target? segments in approximately 80% of cases. We calculate this as an ap-
proximation by testing all translations of marker chunks to see whether these French
chunks appear anywhere on the Web: If so, we assume that the translations obtained
by the online MT systems are correct. For 39,895 such translations, 75.2% of those
produced by system A appear on the Web, with 81.7% of those generated by system
B and 81.5% of those produced by system C also appearing on the Web. Note that
this gives us only an approximation of the correctness of our alignments, as we are
testing whether the French translations are ?good French? rather than whether the
alignments in which they appear are actually correct.
Correcting misalignments such as those in (22) is a topic for further research.
Adding a bilingual lexicon (our word-level lexicon, for example) and incorporating the
constraints contained therein into the marker-based alignment process would prevent
chunks such as those in (22) from being generated, and we conjecture that translation
quality would improve accordingly.
Given marker chunks such as those in (16), we are able to extract automatically a
further bilingual dictionary, the word-level lexicon. We take advantage of the assump-
tion that where a chunk contains just one non?marker word in both source and target,
these words are translations of each other. Where a marker-headed pair contains just
two words, as in (16), for instance, we can extract the word-level translations in (23):
(23)
<QUANT> all : tous
<PREP> of : d?
<LEX> uses : usages
<LEX> asbestos : asbeste
That is, using the marker hypothesis method of segmentation, smaller aligned seg-
ments can be extracted from the phrasal lexicon without recourse to any detailed
parsing techniques or complex co-ocurrence measures.
431
Way and Gough wEBMT
Juola (1994, 1997) assumes that words ending in -ed are verbs. However, given
that verbs are not a closed class, in our approach we do not mark chunks beginning
with a verb with any marker category. Instead, we take advantage of the fact that the
initial phrasal chunks correspond to rule right-hand sides. That is, for a rule in the
Penn Treebank VP ?? VBG, NP, PP, we are certain (if the annotators have done their
job correctly) that the first word in each of the strings corresponding to this right-hand
side is a VBG, that is, a present participle. Given this information, in such cases we tag
such words with the <LEX> tag. Taking expanding the board to 14 members ?? augmente
le conseil a` 14 membres as an example, we extract the chunks in (24):
(24)
<DET> the board : le conseil
<DET> the : le
<PREP> to <QUANT> 14 members : a` 14 membres
<QUANT> 14 members : 14 membres
<LEX> expanding : augmente
<LEX> board : conseil
<PREP> to : a`
<LEX> members : membres
We ignore here the trivially true lexical chunk ?<QUANT> 14 : 14.?
In a final processing stage, we generalize over the marker lexicon following a
process found in Block (2000). In Block?s approach, word alignments are assigned
probabilities by means of a statistical word alignment tool. In a subsequent stage,
chunk pairs are extracted, which are then generalized to produce a set of translation
templates for each ?source, target? segment.
Block distinguishes chunks from ?patterns,? as we do: His chunks are similar to
our marker chunks, and his patterns are similar to our generalized marker chunks.
Once chunks are derived from ?source, target? alignments, patterns are computed from
the derived chunks by means of the following algorithm: ?for each pair of chunk pairs
??CS1, CT1?, ?CS2, CT2??, if CS1 is a substring in CS2 and CT1 is a substring in CT2, then
?PS, PT? is a pattern pair where PS equals CS2 with CS1 replaced by a variable V and
PT equals CT2 with CT1 replaced by V? (Block 2000, pages 414?415). Block then gives
an example that shows how patterns are derived. Assume the chunk pairs in (25):
(25) ? [das], [which] ?
? [ist], [is] ?
? [was], [what] ?
? [Sie], [you] ?
? [wollten], [wanted] ?
? [das ist], [which is] ?
? [das ist was], [which is what] ?
? [das ist was Sie], [which is what you] ?
? [das ist was Sie wollten], [which is what you wanted] ?
432
Computational Linguistics Volume 29, Number 3
Using the algorithm described above, the patterns in (26) are derived from the chunks
in (25):
(26) ? [V ist], [V is] ?
? [das V], [which V] ?
? [das V was], [which V what] ?
...
? [V ist was Sie], [V is what you] ?
...
? [das ist was V wollten], [which is what V wanted] ?
...
Of course, many other researchers also try to extract generalized templates. Kaji,
Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and
replace such equivalents with variables to generate a set of translation patterns. Watan-
abe (1993) combines lexical and dependency mappings to form his generalizations.
Other similar approaches include those of Cicekli and Gu?venir (1996), McTait and
Trujillo (1999), Carl (1999), and Brown (2000), inter alia.
In our system, in some cases the smallest chunk obtainable via the marker-based
segmentation process may be something like (27):
(27) <DET> the good man : le bon homme
In such cases, if our system were confronted with a good man, it would not be able
to translate such a phrase, assuming this to be missing from the marker lexicon. Ac-
cordingly, we convert examples such as (27) into their generalized equivalents, as in
(28):
(28) <DET> good man : bon homme
That is, where Block (2000) substitutes variables for various words in his templates,
we replace certain lexical items with their marker tag. Given that examples such as
??<DET> a : un? are likely to exist in the word-level lexicon, they may be inserted at
the point indicated by the marker tag to form the correct translation un bon homme. We
thus cluster on marker words to improve the coverage of our system (see Section 5
for results that show exactly how clustering on marker words helps); others (notably
Brown [2000, 2003]) use clustering techniques to determine equivalence classes of
individual words that can occur in the same context, and in so doing derive translation
templates from individual translation examples.
2.3 Summary
In sum, we automatically create four knowledge sources:
? the original ?source,target? phrasal translation pairs
? the marker lexicon (cf. (16))
? the generalized marker lexicon (cf. (28))
? the word-level lexicon (cf. (24))
433
Way and Gough wEBMT
When matching the input to the corpus, we search for chunks in the order given
here, that is, from specific examples (those containing more context) to generic (those
containing less context). We give in (29) an example of how a particular sentence from
our test set is translated via these different knowledge sources:
(29) Input:
A major concern for the parent company is what advertisers are paying
per page.
Chunks found in marker lexicon:
for the parent company : pour la socie?te? me`re
what advertisers are paying per page : quels annonceurs paient per page
Chunk found in generalized marker lexicon:
<DET> major concern : inquie?tude majeure
Words found in word-level lexicon:
<DET> a : une
<LEX> is : est
Given the fragments shown in (29), a translation can now be derived. First, the
word pair ?<DET> a : une? is inserted into the generalized template ?<DET> major
concern: inquie?tude majeure? to begin the translation process; the next chunk, ?for
the parent company : pour la socie?te? me`re,? is retrieved from the marker lexicon; the
missing word pair ?<LEX> is : est? is retrieved from the word-level lexicon; and finally,
the marker chunk ?what advertisers are paying per page : quels annonceurs paient
per page? is appended to produce the translation in (30):
(30) Une inquie?tude majeure pour la socie?te? me`re est quels annonceurs
paient per page.
Of course, this ?translation? is not without problems: There is a poor (in this instance)
translation of what as quels, and a nontranslation of per. There is little our system can
do about errors such as these made by the on-line MT systems. Nevertheless, (29)
illustrates how the various knowledge sources play a part in determining the final
translation in our system.
Note that none of these aligned resources would be possible in a TM system. The
problem of segmentation is not an inconsiderable one in all EBMT systems, but we
(and others) have found that using the marker hypothesis can greatly facilitate such a
process. We shall show in subsequent sections that because such knowledge sources
are derived automatically from the original translations obtained via Web-based MT
systems, the translations obtained in our EBMT process are largely of high quality, are
ranked highly in the set of output translation candidates, and may be generated in
almost all cases?all this despite the fact that the original translations obtained via the
Web contain many errors, and that the source phrases to be translated were selected
from a mere fraction of the rule types in the Penn-II Treebank.
3. Retrieving Chunks and Producing Translations
In Section 4, we report on a number of experiments using the resources obtained in
the previous section to translate two test sets of data, one a set of NPs and the other
434
Computational Linguistics Volume 29, Number 3
a set of sentences. Although we are primarily interested in translating sentences, we
translate NPs for two reasons: (1) to assure ourselves that we are in fact translating
nominal chunks correctly, and (2) to see whether our methodology can actually correct
any NPs mistranslated by the three on-line MT systems. In this section, we describe
the processes involved in retrieving appropriate chunks and forming translations for
NPs only (these being fewer in number than for sentences, of course).
3.1 Segmentation of the Input
In many cases, a 100% match for a given NP cannot be found in the phrasal lexicon.
In order to try and process the NP in a compositional manner, it is segmented into
smaller chunks, and the system then attempts to locate these chunks individually and
to retrieve their relevant translation(s) from the various lexicons described above. We
use an n-gram-based segmentation method. Initially, we located all possible bigrams,
trigrams and so on within the input string and then searched for these within the
relevant knowledge sources.
However, many of these n-grams cannot be found by our system, given that new
chunks are placed in the marker lexicon when a marker word is found in a sentence.
Taking the NP the total at risk a year as an example, chunks such as the total at risk a
or at risk a cannot be located, as new chunks would be formed at each marker word
(assuming the adjacent word is a non?marker word), so the best that could be expected
here might be to find the chunks in (31):
(31) <DET> the total, <PREP> at risk, <DET> a year
The respective translations of these chunks would then be recombined to form the
target string. In a recent addition to our work, we have eliminated certain n-grams
(such as those that end in a marker word, for instance) from the search process, as
these would never be found given our chosen method of segmentation.
3.2 Retrieving Translation Chunks
We use translations retrieved from the three different on-line MT systems specified
above (see Section 2.1). These translations are further broken down using the marker
hypothesis to provide us with an additional three knowledge sources A?, B?, and
C?, a marker lexicon, generalized marker lexicon and word-level lexicon derived from
chunks produced by each system. These knowledge sources can be combined in several
different ways. We have produced translations using
? information from a single source: A/A?, B/B?, or C/C?, that is, a phrasal
lexicon and set of marker lexicons derived from translations produced
by each on-line system
? information from pairs of sources: A/A? and B/B?, A/A? and C/C? or
B/B? and C/C?, that is, phrasal and marker lexicons derived from
translations produced by two different on-line systems
? information from all available knowledge sources: A/A? and B/B? and
C/C?, that is, phrasal and marker lexicons derived from translations
produced by all three on-line systems
The objective here is to see how much translation coverage and quality are improved
by using chunks derived from multiple sources. Assuming that the English strings are
435
Way and Gough wEBMT
not translated in exactly the same manner by the three on-line MT systems means that
more knowledge sources could be combined in attempting to translate the new input
contained in the test sets of noun phrases and sentences. Results from experiments
conducted using multiple knowledge sources are given in Section 4.2.
3.3 Calculation of Weights
Each time a source language (SL) chunk is submitted for translation, the appropriate
target language (TL) chunks are retrieved and returned with a weight attached. We
use a maximum of six knowledge sources:
? Stage 1: Three sets of translations (A, B, and C) are retrieved using each
of the three on-line MT systems.
? Stage 2: Three sets of translations (A?, B?, and C?) acquired by breaking
down the translations retrieved in Stage 1 using the marker hypothesis
to form the marker lexicon, the generalized marker lexicon, and the
word-level lexicon.
Within each knowledge source, each translation is weighted according to the formula
in (32):
(32) weight = number of occurrences of the proposed translationtotal number of translations produced for SL phrase
For the SL phrase the house, assuming that la maison is found eight times and le
domicile is found twice, then P(la maison | the house) = 8/10 and P(le domicile | the
house) = 2/10. Note that since each SL phrase will only have one proposed translation
within each of the knowledge sources acquired at Stage 1, these translations will
always have a weight of 1.
If we wish to consider only those translations produced using a single MT system
(e.g., A and A?), we add the weights of translations found in both knowledge sources
and divide the weights of all proposed translations by two. For the SL phrase the
house, assuming P(la maison | the house) = 5/10 in knowledge source A and P(la
maison | the house) = 8/10 in A?, then P(la maison | the house) = 13/20 over both
knowledge sources. Similarly, if we wish to consider translations produced by all three
MT systems, then we add the weights of common translations and divide the weights
of all proposed translations by six.
When translated phrases have been retrieved for each chunk of the input string,
they must then be combined to produce an output string. In order to calculate a
ranking for each TL sentence produced, we multiply the weights of each chunk used
in its construction. Note that this ensures that greater importance is attributed to longer
chunks, as is usual in most EBMT systems (cf. Sato and Nagao 1990; Veale and Way
1997; Carl 1999).7
As an example, consider the translation into French of the house collapsed. Assume
the conditional probabilities in (33):
7 Note that approaches that prefer the greatest context to be taken into account are not limited to EBMT.
Research in the area of data-oriented parsing (cf. Bod, Scha, and Sima?an, 2003) also shows that unless
the corpus is inherently biased, derivations constructed using the smallest number of subtrees have a
higher probability than those built with a larger number of smaller subtrees.
436
Computational Linguistics Volume 29, Number 3
(33) a. P(la maison | the house) = 8/10
b. P(le domicile | the house) = 2/10
c. P(s?e?croula | collapsed) = 1/7
d. P(s?effondra | collapsed) = 6/7
Given the weights in (33), the four translations in (34) can be produced, each with
an associated probability:
(34) a. P(la maison s?e?croula | the house collapsed) = 810 .
1
7 =
8
70
b. P(le domicile s?e?croula | the house collapsed) = 210 .
1
7 =
2
70
c. P(la maison s?effondra | the house collapsed) = 810 .
6
7 =
48
70
d. P(le domicile s?effondra | the house collapsed) = 210 .
6
7 =
12
70
Where different derivations result in the same TL string, their weights are summed
and the duplicate strings are removed.
The examples in (33) and (34) are reasonably straightforward if we assume, as
here, that the chunks in (35) exist in the system databases shown:
(35) Marker lexicon:
<DET> the house : la maison
<DET> the house : le domicile
Word-level lexicon:
<LEX> collapsed : s?e?croula
<LEX> collapsed : s?effondra
If the input string were instead a house collapsed, and the NP a house were absent from
the marker lexicon, then a translation could be formed via the chunks in (36):
(36) Generalized marker lexicon:
<DET> house : maison
<DET> house : domicile
Word-level lexicon:
<LEX> collapsed : s?e?croula
<LEX> collapsed : s?effondra
<DET> a : un
<DET> a : une
Given the aligned segments in (36), the correct translations (37) would be built:
(37) a. Une maison s?e?croula.
b. Une maison s?effondra.
c. Un domicile s?e?croula.
d. Un domicile s?effondra.
However, in addition, the mistranslations in (38) would be constructed:
(38) a. *Un maison s?e?croula.
437
Way and Gough wEBMT
b. *Un maison s?effondra.
c. *Une domicile s?e?croula.
d. *Une domicile s?effondra.
These mistranslations are all caused by boundary friction.
Each of the translations in (37) and (38) would be output with an associated weight
and ranked by the system. We would like to incorporate into our model a procedure
whereby translation chunks extracted from the phrasal and marker lexicons are more
highly regarded than those constructed by inserting words from the word-level lexicon
into generalized marker chunks. That is, we want to allocate a larger portion of the
probability space to the phrasal and marker lexicons than to the generalized or word-
level lexicons. We have yet to import such a constraint into our model, but we plan
to do so in the near future using the weighted majority algorithm (Littlestone and
Warmuth 1992).
4. Experiments and System Evaluation
We report here on a number of experiments using test sets of 200 sentences and 500
noun phrases. Some typical examples from the two test sets are given in (39):
(39) Noun phrases:
? the heavy use of management fees last year
? an increase through issues of new shares and convertible bonds
? a space-based defense shield for official acts by the congressman
Sentences:
? The bright red one interferes with the genes that are responsible for
collecting pollen.
? A more recent novel permitted the new basket product.
? The area with the museums and the charities is under something of a
cloud.
? Reducing the supply of goods as commissions to middlemen permitted
a chaotic sex life.
The test sets were created automatically from words contained in at least one of the
systems? knowledge bases, with the proviso that the strings corresponding to the 59
rule types we extracted from the Penn Treebank reflected the frequency bias of these
rule types, as far as possible. That is, we wanted to ensure that strings corresponding
to a rule type that was (approximately) twice as frequent as some other rule type
occurred (approximately) twice as often in the test sets. Finally, we ensured that strings
corresponding to all 59 rule types were present in the sentence test set.
The experiments were designed to evaluate the coverage and translation quality
of different versions of our EBMT system. We contrast the results obtained when the
memories of our system are seeded with source strings and their translations derived
from
? each of the three individual on-line MT systems (A, B, and C)
? each pair of on-line MT systems (AB, AC, and BC)
? all three on-line MT systems (ABC).
438
Computational Linguistics Volume 29, Number 3
We also compare and contrast the results obtained when the memories of our sys-
tem are seeded with source strings and their translations derived using third-person
singular, third-person plural, and both third-person singular and third-person plural
dummy subjects.
Both sets of experiments are designed to test whether coverage and translation
quality improve when more ?source, target? fragments are taken into account. With
respect to quality, the translations output (for sentences, using chunks derived with
third-person plural dummy subjects) were scored according to the following scale by
two native speakers of French with excellent English:
? 1: Contains major syntactic errors and is unintelligible
? 2: Contains minor syntactic errors and is intelligible
? 3: Contains no syntactic errors and is intelligible
This scale is used to measure the impact on translation quality, both for sentences
and NPs, of using multiple knowledge sources. Although we are primarily interested
in the translation of sentences, we use the NP test set to see whether we are in fact
translating nominal chunks correctly, and also to investigate whether our methodology
can actually correct any NPs mistranslated by the three on-line MT systems. We discuss
this further in Section 5.
We also measure the ability of our system to rank the ?best? translation (as deter-
mined by our human experts) highly in the set of output translations. Statistical MT
systems such as wEBMT may derive many different translations for a particular input,
each of which is output with a confidence weighting. We are keen to ensure that if
our system is able to produce high-quality translations, these are ranked as highly
as possible: We do not consider it feasible for a human to have to sift through many
hundreds or thousands of translations in order to determine the ?correct? one.
In a further experiment, we translate the test set of sentences via the three on-
line MT systems and test to see whether our system wEBMT can improve on these
translations. In so doing we calculate the ?net gain? of performing example-based MT
compared to using on-line MT systems.
Finally, we offer some thoughts on the relative merits of the three on-line MT
systems used in our research. Although this was not the primary focus of our research,
it turned out that we were able, as a direct consequence of our methodology, to evaluate
the on-line MT systems chosen.
4.1 Experiments Using Single Knowledge Sources
Here we report on experiments in which the two test sets are tackled by our system
when its memory is seeded with translations obtained by the individual on-line MT
systems specified in Section 2.1. A parameter that is altered in the experiment on
the sentential test set is the nature of the dummy subjects used to gather the initial
translation fragments: third-person singular, third-person plural, and both third-person
singular and third-person plural.
4.1.1 Sentences. The test set comprised 200 sentences, with an average sentence length
of 8.5 words (minimum 3 words, maximum 18). The input strings were segmented by
applying the n-gram segmentation approach outlined in Section 3.
439
Way and Gough wEBMT
Experiment 1: Third-Person Plural Subjects
As far as coverage is concerned, our system wEBMT translated 184 (92%) of the sen-
tences using chunks derived from Systems A and C, and using chunks from system B,
our system managed to translate 180 sentences (90%). The same 16 sentences were not
translated by any of the systems owing to their failure to locate one or more words in
the sentence within the word-level lexicon. Recall that despite the fact that all words
in the test set were seen by the system in the training phase, only those content (i.e.,
non-marker) words that occur in bigram marker chunks are inserted into the word-
level lexicon (cf. (23)). In cases such as these, in which one or more words cannot be
translated by our system, partial translations such as those in (40) are output:
(40) A little girl misplaced a full page =? Une petite fille misplaced une
pleine page.
That is, although misplaced was present in the system?s database, it was not present in
the correct context. That is, it appeared in the phrasal lexicon, as shown in (41):
(41) were misplaced =? ont e?te? e?gare?s
The form required in (40) is a simple past-tense verb, but misplaced appears in (41) only
as a passive participle. The word were in (41) is not a marker word, so this fragment
cannot be broken down any further by our segmentation method.8 In such cases we
output the partial translation with source equivalents for any untranslated words, as
shown in (40).
Ninety-six (48%) of the sentences were translated by combining fragments con-
tained in the original phrasal lexicon or the marker lexicon, 56 (28%) of the translations
were obtained by locating single content (i.e., non-marker) words in the word-level
lexicon and inserting these into the translation at the appropriate position, and 32
(16%) were produced by inserting marker words into generalized templates.
Table 1 shows the results obtained from our human evaluators? ratings of the
translations produced by our system when it was populated with fragments derived
from one of the individual on-line MT systems. Evaluators rated more than one-third
of translations as intelligible and without syntactic errors (score 3), with over 85% of
translations deemed intelligible (scores 2 and 3) for all systems. Unintelligble transla-
tions (score 1) ranged from 14% for chunks derived from SDL to just 4.4% for trans-
lations formed from knowledge sources created from Logomedia. These initial results
provide some evidence in favor of the hypothesis that Logomedia might be the best
system. Although such evaluation is not a primary focus of our work at the outset,
our methodology provides as a spin-off an evaluation of the three MT systems. We
discuss this further in Section 4.5.
When the system cannot produce a translation for a particular input, the main
reason is an absent word in the word-level lexicon. Adding more lexical entries would
improve translation coverage and would also affect translation quality (possibly ad-
versely, in some cases). We plan to measure the impact of a larger lexicon in future
work. Low-quality translations are almost invariably caused by inappropriate verb
forms in the word-level dictionary: For the experiments carried out in which all verbs
8 Of course, even if it could be, we would be able to derive only the mistranslation Une petite fille e?gare?s
une pleine page, assuming there to be no other relevant fragments in the system?s databases.
440
Computational Linguistics Volume 29, Number 3
Table 1
Translation quality for sentences: Chunks
derived from individual on-line MT
systems, third-person plural dummy
subjects.
System Score 1 Score 2 Score 3
A 14.2% 51.2% 34.6%
B 8.9% 54.7% 36.4%
C 4.4% 59.1% 36.5%
were third-person plural, any NP with a third-person singular subject in the test set
would be accompanied by a third-person plural verb in the translation. A similar effect
is seen where the databases of wEBMT were seeded with third-person singular verbs,
of course. However, we should expect an improvement in translation quality when
both sets of verb forms are included in the memories of the system (see Experiment
2).
Table 2 shows where the ?best? translation, as defined by a human expert, was
ranked among the of translations output by our system. In over 65% of cases, the
system itself had ranked the ?best? translation first, and the ?best? translation was
never located outside the top five ranked translations. This is remarkable given that
over 2,000 translations are output for certain source sentences.
Experiment 2: Seeding the Databases with More Examples
The results for the previous experiment were obtained when the databases were seeded
with third-person plural dummy subjects. We ran two variations on this experiment:
(1) we tested the system by seeding its memories with third-person singular dummy
subjects, and (2) We tested the system by seeding its memories with both third-person
singular and third-person plural dummy subjects.
Figure 1 shows that translation quality improves when the system databases are
seeded with more translation pairs. We can see that the system does slightly better
when it uses third-person plural chunks compared to when it uses their singular coun-
terparts. When third-person singular dummy subjects are inserted in order to derive
the initial translation fragments inserted into our system?s memories, the number of
translations rated 3 for quality deteriorates by about 5% for systems B and C and by
about 3% for system A. Given a larger number of third-person plural NP subjects in
our test set, this was to be expected.
However, a considerable improvement in quality can be seen when fragments from
Table 2
Ranking of ?best? translation for sentences:
Chunks derived from individual on-line MT
systems, third-person plural dummy subjects.
System Ranked 1 Ranked 2?5
A 71.6% 28.4%
B 65.3% 34.7%
C 70.3% 29.7%
441
Way and Gough wEBMT
0
10
20
30
40
50
60
70
80
90
100
A B C
3rd p.s:3
3rd p.p:3
3rd p.p/s:3
3rd
p.p/s:3+2
Figure 1
Translation quality improves when system databases are seeded with more translation pairs:
Measuring % translation quality using fragments derived from single on-line MT systems.
Table 3
Ranking of ?best? translation for sentences: Chunks derived from
individual on-line MT systems, third-person singular and third-person
plural dummy subjects.
System Ranked 1 Ranked 2?5 Ranked 6?10 Ranked 10?20
A 65.2% 30.5% 0.0% 4.3%
B 60.8% 34.9% 0.0% 4.3%
C 64.1% 31.6% 0.0% 4.3%
both singular and plural forms are inserted into the system?s memories. Translations
produced from chunks derived from system A are rated 3 in 66.1% of cases, and this
rises to 67.9% for system B and 68% for system C. Regarding intelligibility (scores 2
and 3 for quality), system A scores 85.8%, system B scores 91.1%, and system C scores
95.6%. We consider these to be very reasonable results.
Table 3 summarizes the relative ranking of the ?best? translation when more trans-
lation pairs are used to seed the system?s memories. Now that both third-person sin-
gular and third-person plural dummy subjects are provided, we see that the number
of ?best? translations ranked first deteriorates, by about 6% for systems A and C and
by 4.5% for system B. In Table 2, we saw that the ?best? translation was in no case
ranked lower than fifth, but now that many more translations are output per test set
sentence, we sometimes have to search as low as 20th in order to find the ?best?
translation.
4.1.2 Experiment 3: Noun Phrases. The test set comprised 500 noun phrases, with an
average NP length of 6.14 words (minimum 3 words, maximum 12). The noun phrases
in the test set alo need to be fragmented using our n-gram segmentation method, as
442
Computational Linguistics Volume 29, Number 3
it is highly probable that they do not exist en bloc in the phrasal lexicon and therefore
need to be analyzed using smaller fragments in the system?s databases.
We give results for coverage and translation quality in Table 4. These results are
for NPs translated via chunks derived from the three individual on-line MT systems.
As with the sentence test set, fragments derived from systems A and C achieve the
broadest coverage, producing translations for 474 out of the 500 NPs; those obtained
from system B enable 463 of the 500 NPs to be translated.
As for quality, wEBMT clearly performs best when using translation fragments
derived from system C: 47.3% of these translations were awarded a quality score of 3,
more than 10% better than for chunks derived from system B. For system C, a total of
452 (96%) of the generated translations were deemed intelligible (scores 2 and 3), that
is, 31 (6.6%) more translations than with system B.
On average, about 54% of translations are formed by combining chunks from
the phrasal lexicon with those from the marker lexicon, about 9% are produced by
inserting marker words into the generalized templates, and about 37% are generated
by inserting single non?marker words from the word-level lexicon at the appropriate
locations in phrasal chunks. The major reason that translations fail to be produced in
6% of cases is the absence of a relevant generalized template. For example, the unseen
input her negative TV ads is generalized to (42):
(42) <POSS> negative TV ads
However, the nearest relevant generalized template found in the system?s memory is
(43):
(43) <DET> negative TV ads
That is, the template in (43) allows the insertion of any determiner, but no other
marker word. Deriving translation fragments from more examples would lead to an
improvement in coverage. Alternatively, for marker words that appear in the same
relative position, such as determiners and possessive pronouns, we could ?back off?
to a more general marker tag to allow mutual substitution of such words in a subse-
quent operation to enable translation of examples like these. This remains an area of
investigation in future work.
The results in Table 4 further substantiate our findings on the sentence test set,
namely, that system C may be the best of the three on-line MT systems used to populate
the memories of our EBMT system. We comment further on this in Section 4.5. In
addition, these figures provide strong evidence that our system can indeed translate
most noun phrases with which it is confronted and with more than reasonable quality.
Table 4
Translation coverage and quality for NPs: Chunks
derived from individual on-line MT systems.
System Coverage Quality
Score 1 Score 2 Score 3
A 94.8% 13.7% 52.5% 33.8%
B 92.6% 10.6% 52.3% 37.1%
C 94.8% 4.0% 48.7% 47.3%
443
Way and Gough wEBMT
Table 5
Ranking of ?best? translation for NPs: Chunks derived from
individual on-line MT systems.
System Ranked 1 Ranked 2 Ranked 3?5 Ranked 6?10
A 64.6% 9.1% 23.6% 2.7%
B 57.7% 15.6% 24.8% 1.9%
C 60.0% 7.6% 29.3% 3.1%
Table 6
Number of translations produced for the
NP a plan for reducing debt over 20 years.
System(s) Number of Translations
A 14
B 10
C 5
AB 108
AC 72
BC 42
ABC 224
The results obtained regarding the ranking of the ?best? translation appear in
Table 5. Our system ranks the ?best? translation first over 57% of the time, and in
over 96% of cases, it ranks it in the top five, and at worst in the top ten.
4.2 Experiments Using Multiple Knowledge Sources
If the three on-line MT systems translate the phrases extracted from the Penn-II Tree-
bank in different ways, then combining systems to obtain results for AB, AC, BC,
and ABC always involves an increase in the number of translations produced, both
for sentences and noun phrases. That is, if an input string receives a translation via
chunks derived from the individual on-line systems, when chunks are combined from
different systems, more translations will be output for that input string.
As an example, the number of translations produced by each system for the NP
a plan for reducing debt over 20 years is shown in Table 6. Whereas the greatest num-
ber of translations for this NP produced from chunks from any individual on-line
system is 14, when translation fragments from all three systems are merged (ABC),
224 translations are produced. Combining systems in this way means that all possible
combinations of chunks from the systems are produced: That is, the number of trans-
lations generated via AB is much larger than those derived from either A or B, as now
chunks from A and B may be combined to produce new translations that could not
be generated from the individual knowledge sources. As a further example, consider
the translation of the NP the total at risk a year. When fragments from systems A and
B are combined, the ?best? translation is comprised of the chunk combination AAB,
that is, the three-chunk combination in (44), with the first two chunks obtained from
system A, and the last from system B:
(44) [Athe total], [Aat risk], [Ba year]
That is, the translation of this NP improves when the performance of system AB is
444
Computational Linguistics Volume 29, Number 3
0
20
40
60
80
100
AB BC AC ABC
3rd p.s:3
3rd p.p:3
3rd p.s/p:3
3rd
p.s/p:3+2
Figure 2
Translation quality improves when system databases are seeded with more translation pairs
and when more knowledge sources are used: Measuring % translation quality using fragments
derived from combinations of on-line MT systems.
evaluated: Of course, if we consider (say) three-chunk combinations from either system
A or B, the only possibilities are AAA or BBB, respectively.
However, the number of translations produced by the system is less significant
than their quality. The ranking process outlined in Section 3 classifies the translations
produced with regard to their position as the ?best? translation. In the sections below,
we also discuss the issue of quality and show that it improves when more translation
fragments are taken into account. Furthermore, we show below that despite generating
more translations per input string, wEBMT still ranks the ?best? translation in the top
1% of all output translation candidates.
4.2.1 Experiment 4: Translating Sentences by Combining Fragments from Different
Systems. We saw in Experiment 1 that 16 strings in the test set were left untranslated
by systems A, B, and C individually. When knowledge sources are combined, these 16
strings remain untranslated. However, as Figure 2 shows, the translation quality im-
proves significantly. The best individual system performance was 36.5% scoring 3. This
rises to a best performance of 48.9% among pairs of systems combined and improves
still further to 50% when chunks from all three knowledge sources are combined.
Table 7 provides results regarding the relative location of the ?best? translation for
sentences. For all system combinations, the ?best? translation is to be found among
the top 10 ranked translations in all permutations of combinations of chunks, with
at least 54% ranked first. Despite a corresponding rise in the number of translations
produced per input sentence when all three knowledge sources are combined (ABC),
in over 97% of cases, the ?best? translation continues to be found in the top five output
candidates.
445
Way and Gough wEBMT
Table 7
Ranking of ?best? translation for sentences: Chunks
derived from combinations of on-line MT systems,
third-person plural dummy subjects.
System Ranked 1 Ranked 2?5 Ranked 6?10
AB 67.6% 31.1% 1.3%
AC 54.0% 46.0% 0.0%
BC 63.6% 35.1% 1.3%
ABC 62.2% 35.1% 2.7%
4.2.2 Experiment 5: Combining Fragments from Different Systems and Seeding the
Databases with More Examples. Figure 2 demonstrates that considerable improve-
ments in translation quality are achieved when the memory of wEBMT is seeded with
both third-person singular and third-person plural fragments. For the pairwise com-
binations, 78.7% of the translations derived from AB are rated 3 for quality, compared
to 80.4% of those derived from AC and BC. The results for ABC improve again, to
81.5%. Regarding intelligibility (scores 2 and 3 for quality), we can see from Figure 2
that near perfect results are obtained: AB scores 95.6%, and all other combinations
score 96.7%.
Table 8 shows the ranking of the ?best? translation when multiple knowledge
sources are employed and both third-person singular and third-person plural dummy
subjects are used to populate the system?s memories. The number of instances in which
the ?best? translation is ranked first by wEBMT deteriorates: by 24% for AB, by 15% for
AC, by 20% for BC, and by 27% for ABC. For all system combinations, Table 7 shows
that the ?best? translation was ranked no lower than 10th; for the system combinations
in Table 8, sometimes the ?correct? translation is ranked as low as 36th. As expected,
the worst ranking results are for system combination ABC, in which all system chunks
are combined for both third-person singular and third-person plural dummy subjects.
However, even here the ?best? translation is ranked in the top five in over 63% of
cases, and 72.6% of the time it is located among the top 10 ranked translations. For
this system configuration, the lowest we have to look to find the ?best? translation
is 36th. For that particular sentence (i.e., the one for which the ?best? translation is
ranked 36), over 4,000 possible translations are generated, so even here the ?best?
translation remains in the top 1% of translation candidates.
Table 8
Ranking of ?best? translation for sentences: Chunks derived from combinations of
on-line MT systems, third-person singular and third-person plural dummy
subjects.
System Ranked 1 Ranked 2?5 Ranked 6?10 Ranked 10?20 Ranked 20?40
AB 43.4% 32.6% 2.3% 13.0% 8.7%
AC 39.1% 34.8% 5.4% 12.0% 8.7%
BC 43.4% 31.7% 2.7% 13.5% 8.7%
ABC 35.2% 28.0% 9.4% 18.3% 9.1%
446
Computational Linguistics Volume 29, Number 3
Table 9
NPs: Coverage and quality improve when
fragments from different sources are included.
Knowledge Source Coverage Quality
Combination Percentage Score 3
A 94.8 33.8
B 92.6 37.1
C 94.8 47.3
AB 95.4 54.1
BC 95.6 64.0
AC 94.8 72.0
ABC 96.0 77.8
4.2.3 Experiment 6: Translating Noun Phrases by Combining Fragments from Dif-
ferent Systems.
As we did with sentences, we seeded our EBMT system with fragments derived from
the three different on-line MT systems and confronted it with the NP test set. Table 9
clearly shows that as more knowledge sources are added, translation quality improves
considerably. The worst-performing individual system scores 3 for quality in just over
a third of cases, but when all system chunks are combined, this rises to 77.8%. Note
also that, unlike with sentences, we see an increase in coverage when more knowledge
sources are used, from a low of 92.6% for system B to a high of 96% when all chunks
are combined. Many more improvements are seen when our post hoc validation and
correction methodology, described in Section 5, is used, but the merging of fragments
derived from different on-line systems also leads to an improvement in translation
quality. Consider the examples in (45):
(45) Input: an old story in common
System B: une vieille histoire dans commun
System Combination BC: une vieille histoire en commun
That is, the optional PP in common was mistranslated by system B as dans commun, but
when knowledge from system C is added to that of system B, the improved translation
en commun is generated.
We saw in Section 4.1.2 that when translating the NP test set, the ?best? transla-
tion, as adjudged by our human evaluators, was to be found no lower than tenth of
all translations output by our system. When knowledge sources are combined, it is
Table 10
Ranking of ?best? translation for NPs: Chunks derived from
more that one on-line MT system.
System Ranked 1 Ranked 2 Ranked 3?5 Ranked 6?10
AB 42.2% 13.8% 41.3% 2.7%
AC 62.1% 14.1% 21.3% 2.5%
BC 66.4% 11.4% 19.8% 2.4%
ABC 62.0% 17.5% 13.5% 7.0%
447
Way and Gough wEBMT
important to measure whether the ?best? translation is still highly ranked. The results
of such an assessment are summarized in Table 10. When the ?best? translation is
ranked first by the system, we see that the optimal combination of knowledge sources
is the pair BC, with 66.4%. This is an interesting result given that in Table 5, only 57.7%
of the NPs translated by system B were ranked first, with 60% of those produced by
system C ranked in first place. That is, we see a 6.4% improvement (31 NPs) when
fragments from systems B and C are combined. All other combinations cause the num-
ber of ?best? translations ranked first to deteriorate, as may be expected. When the
?best? translation is ranked either first or second by the system, the best combination
of fragments is that from system ABC, with 79.5% (376 NPs).
Importantly, for all combinations, the ?best? translation remains among the top 10
ranked translations. This is encouraging, as any translator using our system needs only
to examine a small subset of the translations produced to find the ?best? one. Indeed,
given the various results shown, we are confident that we could prune the number of
translations generated for presentation to the translator for selection of the ?best? one:
For the most part, this is the top ten translations for both NPs and sentences, but in
the worst case, we need present no more than the top 1% of the candidate translations.
4.3 Summary
The results presented in this section show that seeding the wEBMT system?s databases
with more fragments improves both coverage and translation quality. We do this addi-
tional seeding in two ways: (1) by combining fragments derived from different on-line
MT systems, and (2) by obtaining translations using both third-person singular and
third-person plural dummy subjects. The best combination of these parameters is to
use chunks derived from Logomedia with third-person plural dummy subjects pro-
vided. Nevertheless, despite the fact that Logomedia appears to be the best on-line MT
system, adding chunks from the other two on-line MT systems improves coverage
and translation quality. In sum, therefore, the best results are obtained when chunks
from all three on-line systems (combination ABC) are used and the wEBMT system?s
databases are seeded with translations from these systems for both third-person sin-
gular and third-person plural versions of sentences.
The disadvantage of using more knowledge sources, of course, is that many more
candidate translations are generated, which sometimes causes the ?best? translation
to appear lower in the ranked order of output translations. Nevertheless, the ?best?
translation is almost always to be found in the top 10 translations produced by wEBMT
and always in the top 1% of the candidate translations.
4.4 Relative Gain of EBMT
In order to try to calculate the relative gain of EBMT, we translated all 200 strings in the
sentence test set via the three on-line MT systems used elsewhere in our experiments.
Of course, the main advantage of using such Web-based systems is that they are
extremely robust: no matter what they are confronted with, they will always produce
some translation. With respect to coverage, therefore, the on-line systems currently
win out over wEBMT: the size of the lexicons available to the on-line systems means
that they will generate translations with all source words translated more often than
our system will. Nevertheless, the fact that our system outputs partial translations in
situations in which it encounters a word that it cannot translate demonstrates a certain
level of robustness in our system.
Where quality is concerned, however, wEBMT can improve on the translations
produced by the three on-line MT systems. We provided our human evaluators with
the source sentences from the test set, together with the translation generated by our
448
Computational Linguistics Volume 29, Number 3
wEBMT system using combinations of chunks derived from all three on-line systems
(ABC), and the translation obtained directly from the on-line systems themselves. The
pairs of translations (wEBMT and on-line MT system) were presented in a random
order. The evaluators were simply asked to state, for all three on-line MT systems,
which of the pair of translations they preferred: that from the on-line MT system or
that from wEBMT.
Translations produced by wEBMT using chunks derived from all three systems
were preferred to those from system A in 30/184 cases (16.3%) (we ignored the 16
cases for which our system could produce only partial translations); for system B,
our system?s translations were preferred in 8/180 cases (4.4%); and for system C,
our system was judged as producing better translations in 6/184 cases (3.3%). Some
examples in which our system offered improvements over the translations provided
by the on-line MT systems are shown in (46):
(46) Input: Her short term interest rates link the issues.
System A: Son lien a` court terme de taux d?intere?t les questions.
wEBMT ABC: Ses taux d?intere?t a` court terme lient les questions.
Input: The researchers air the shows.
System B: L?air de chercheurs les expositions.
wEBMT ABC: Les chercheurs ae?rent les expositions.
Input: A group hire lawyers to provide information about clients.
System C: Un avocats de la location du groupe fournir de l?informations
au sujet de clients.
wEBMT ABC: Un groupe embauche des avocats a` fournir de
l?informations au sujet de clients.
Regarding the first two translation pairs in (46), wEBMT has provided a finite verb
where the on-line MT systems have none. In addition, the translation of the subject
NPs is much improved. As for the final translation pair in (46), whereas Logomedia
managed to retrieve the correct translation of provide, this was not the main verb in
the English input string. wEBMT, on the other hand, does translate hire correctly as a
verb rather than a noun.
We consider three ways in which the net gain of EBMT may be calculated. First,
we assume it is equal to the number of translations produced by wEBMT that are
preferred by the human evaluator, minus those derived by the on-line MT systems
that are preferred, divided by the total number of translations. In fact, where both
wEBMT and the on-line systems produce a translation, those derived via wEBMT are
always preferred. Those translations produced by the on-line systems that are preferred
are those in which wEBMT was unable to generate a complete translation. This is quite
a harsh measure: As can be seen from the translations in (46), although the words in
the translations produced by the on-line systems are all French (but recall (29)?(30),
in which this was not the case), the translations themselves are poor. In some cases,
despite the fact that the translations derived via wEBMT may contain an untranslated
English word, the accompanying partial translations may in fact be deemed superior
to the ?complete? translations derived via the on-line systems.
Nevertheless, assuming that the on-line systems win out in these situations, the
net gain compared to system A is 14/200 (7%), whereas for systems B and C, we see
in effect a net loss: ?12/200 (?6%) for system B, and ?10/200 (?5%) for system C. If
we can obtain complete translations in those cases in which we currently encounter
449
Way and Gough wEBMT
an untranslatable word, we are confident that we can convert these net losses into
net gains. With respect to system A, we can assume that our net gain would increase
further.
However, we provide two other interpretations of the net gain of EBMT, calculated
using the formula in (47):
(47) Net Gain = Coverage Percentage + K(Translation Quality)
The term Translation Quality in (47) refers to the number of translations preferred by
the human evaluator, excluding cases in which one system failed to produce a trans-
lation, which is already factored into the equation under the term Coverage Percentage.
Where K=1, we view coverage and translation quality as equally important. If we
consider quality to be more important, we can increase K. We provide results for K=1
and K=2 in (48):
(48) Where K=1:
Net GainMT = 100
Net GainEBMT = 92 + 30 = 122 (compared to system A)
Net GainEBMT = 92 + 8 = 100 (compared to system B)
Net GainEBMT = 92 + 6 = 98 (compared to system C)
Where K=2:
Net GainMT = 100
Net GainEBMT = 92 + 60 = 152 (compared to system A)
Net GainEBMT = 92 + 16 = 108 (compared to system B)
Net GainEBMT = 92 + 12 = 104 (compared to system C)
That is, where K=1, wEBMT outperforms SDL by a factor of 22, whereas there is no gain
with respect to Reverso, and a slight loss compared to Logomedia. However, wEBMT
outperforms all three on-line systems when translation quality is viewed as twice as
important as coverage. This is a reasonable view, we feel, and our system shows a net
gain against all three on-line systems in this context. Although the coverage obtained
with the on-line systems is better, the improved translation quality obtained with
wEBMT ensures a net gain.
We expect to obtain more insightful results regarding the relative gain of EBMT
over on-line MT systems when automatic evaluation metrics (such as IBM?s Bleu, or
dynamic programming, or sentence- and word-error rates) have been obtained. This
is a priority in future work.
4.5 Evaluating Individual On-line MT Systems
The previous sections detail the results obtained when translation fragments derived
from the three individual on-line MT systems are used, together with various combi-
nations of knowledge sources. We provided results both for coverage and translation
quality. As we noted above, we were able, as a consequence of our chosen methodol-
ogy, to evaluate the on-line MT systems used.
Where sentences are concerned, we saw that coverage was approximately the same
for each individual system, and that combinations of multiple knowledge sources did
450
Computational Linguistics Volume 29, Number 3
not improve coverage. For NPs, however, coverage improved when more fragments
were considered: Whereas 474 NPs could be translated by both systems A (SDL) and C
(Logomedia), this number rose to 480 when all three knowledge sources were combined.
With respect to translation quality, whereas Logomedia and Reverso could hardly
be distinguished when it came to numbers of translations of sentences adjudged as
intelligible and syntactically correct, if we consider those translations considered un-
intelligible by our human evaluators, about twice as many translations produced by
chunks derived from Reverso were unintelligible compared to those produced by Lo-
gomedia. This would indicate that Logomedia may be better.
When fragments from combinations of systems are considered, we note that for
sentences, no improvement in coverage results, but quite significant improvements in
quality are seen. System C slightly outperformed system B in the individual face-off,
and we see that combinations that utilize chunks from system C outperform those
that do not: AC and BC both score 3 for quality in 80.4% of cases, compared to AB?s
78.7%, and ABC improves still further, to 81.5%. When the databases of wEBMT are
seeded with more chunks (using both singular and plural dummy subjects), system C
continues to outperform the other two systems.
When NPs are considered, system C considerably outperforms the other two sys-
tems, obtaining a score of 3 for quality in 10.2% more cases than its nearest challenger,
system B. When chunks from different systems are combined, again we see that com-
binations with chunks derived from system C outperform those that omit them: BC
and AC improve over AB by 10% and 18%, respectively, and ABC shows a further
increase.
Finally, in Section 4.4, we discussed the relative gain of using wEBMT over the
three on-line MT systems. Further evidence that Logomedia may be the best of the three
systems is provided by the fact that the relative gain compared to Logomedia was much
lower than with the other two systems.
It is worth considering why some combinations seem to work better than others.
For NPs, in cases where system A fails to produce the ?best? translation, this is often
due to incorrect word order. For example, in the NP cellular mobile lines for the workmen,
system A produces the translation cellulaire mobile lignes pour les ouvriers.9 It is also the
case that when system A fails to retrieve a translation for a particular chunk, it simply
proposes the English for that chunk as its translation. This is useful to some extent,
in that a default translation is produced (cf. the similar approach that we have taken
with respect to (40), for instance). However, in all of these cases this utility is lessened
considerably given that either system B or system C produces a better translation.
System B has the added advantage that it sometimes provides an alternative translation
in brackets. If no translation is available, the English is output. System C often produces
a correct translation of a verb where the other systems are lacking. It is probably
because of this aspect of translation that system C?s translations are preferred over
those of the other two systems.
5. Validation and Correction of Translations via the Web
A translation can be formed in our system only when the recombination of chunks
causes the input string to be matched exactly. Therefore, if all chunks cannot be re-
trieved, then no complete translation can be produced (cf. (40) and resultant discus-
9 Such a translation would be a candidate for post hoc validation via the Web (cf. Section 5), but the
correct translation lignes cellulaires mobiles pour les ouvriers is produced in any case by system C,
rendering this unnecessary.
451
Way and Gough wEBMT
sion). We have shown that when a translation cannot be produced by combining
phrasal chunks, translations can be formed by the insertion of single marker words
into generalized templates. This can be compared to the idea of ?hooks? (Somers,
McLean, and Jones 1994), where some context in which fragments have occurred is
maintained in the translation templates. The hooks indicate which words and POS
tags can occur in the immediate left and right context of a fragment, together with a
weight that reflects how often this context is found in a corpus. The ?best? translation,
therefore, is simply that which is output by the system with the highest score.
Consider the translation of the NP the personal computers. There are three possible
ways in which this may be segmented using the marker hypothesis, namely, the chunks
in (49):
(49) Phrasal lexicon: the personal computers
Marker lexicon: <DET> the personal computers
Generalized lexicon: <DET> personal computers
In our system, the only chunk retrieved is the generalized chunk in (49). The system
stores a list of marker words and their translations in the word-level lexicon. A weight
derived via the method in (32) is attached to each translation. The system searches
for marker words within the string and retrieves their translations.10 In this case, the
marker word in the string is the and its translation can be one of le, la, l?, or les,
depending on the context. The system simply attaches the translation with the highest
weight to the existing chunk ordinateurs personnels to produce the mistranslation in
(50):
(50) *la ordinateurs personnels
The problem of boundary friction is clearly visible here: We have inserted a feminine
singular determiner into a chunk that was generalized from a masculine plural NP.
However, rather than output this wrong translation directly, we use a post hoc val-
idation and (if required) correction process based on Grefenstette (1999). Grefenstette
shows that the Web can be used as a filter on translation quality simply by searching
for competing translation candidates and selecting the one that is found most often.
Rather than search for competing candidates, we select the ?best? translation and have
its morphological variants searched for on-line. In the example above, namely, the per-
sonal computers, we search for les ordinateurs personnels versus the wrong alternatives
le/la/l?ordinateurs personnels. Interestingly, using Lycos, and setting the search language
to French, the correct form les ordinateurs personnels is uniquely preferred over the other
alternatives, as it is found 2,454 times, whereas the others are not found at all. In this
case, this translation overrides the highest-ranked translation (50) and is output as the
final translation. In fact, in checking the translations obtained for NPs using system
combination ABC, we noted that 251 NPs out of the test set of 500 could be improved.
Of these 251, 207 (82.5%) were improved post hoc via the Web, with no improvement
for the remaining 43 cases. We consider this to be quite a significant result.
In addition to determiner-noun agreement, we use this methodology to check for
agreement between the head noun in the subject NP with the head verb in the main
10 Although this is not relevant for the example discussed here, if non?marker words remain untranslated
yet exist in the word-level marker lexicon, these too would be inserted at this stage.
452
Computational Linguistics Volume 29, Number 3
Table 11
Validating translations using AltaVista
n-Gram Searched For Number of Web Occurrences
empire sont 353
sont au 91,197
empire est 1,809
est au 217,820
clause VP. We extracted a list of all verbs in the Penn-II Treebank and obtained trans-
lations for all verb forms using the three on-line MT systems by inserting appropriate
third-person dummy subjects. We use the list of translated verbs to attempt to find the
main verb and identify the head noun as the rightmost non?marker word or the right-
most word before any other marker word in a nominal chunk. Having extracted the
noun and the verb from the mistranslation in this way, we then search for this bigram
on the Web and correct the verb if its morphological variant (third-person singular or
third-person plural form) is found more often than in the translation obtained by our
system.
To exemplify this procedure with a sentence from the test set, his empire is beyond
the reach of the president, system A produces the translations in (51):
(51) a. *son empire sont au dela` de la porte?e du pre?sident (ranked first, with
probability 0.614)
b. son empire est au dela` de la porte?e du pre?sident (ranked fifth, with
probability 0.028)
This shows that a correct translation such as (51b) may be ranked lower than an incor-
rect variant (51a) with considerably less probability. The higher ranking is accounted
for because the pair ?is beyond the reach of the president, sont au dela` de la porte?e du pre?sident?
is contained in the phrasal lexicon, whereas the pair ?is beyond the reach of the president,
est au dela` de la porte?e du pre?sident? does not appear.
Prior to outputting translations such as (51a), we search for the relevant n-grams
via the Web. For this example, using AltaVista, we obtained the results in Table 11.
With the counts for all other bigrams for the two translation candidates in (51) being
exactly the same, in order to evaluate which of these proposed target strings is the
?better? translation, one can simply add the occurrences found on the Web for all four
different bigrams and report their relative probabilities. This gives us the probabilities
in (52):
(52) a. #empire sont + #sont au = 91,550/311,179 = 0.294
b. #empire est + #est au = 219,629/311,179 = 0.706
That is, the string empire est au is about 2.4 times more likely than the string empire
sont au. However, the count for the second bigram in each example in (52) can of
course be discounted, as the juxtaposition of sont or est with au bears no relevance to
the correctness or otherwise of the translations in (51). The amended probabilities are,
therefore, those in (53):
(53) a. #empire sont = 353/2,162 = 0.163
b. #empire est = 1,809/2,162 = 0.837
453
Way and Gough wEBMT
Table 12
Using the Web to improve noun-verb agreement
Improvement No Improvement N-V Confusion Not on Web
System A: Enterprise Translation Server
58.6% 3.4% 17.3% 20.7%
System B: Reverso
62% 3.4% 17.3% 20.7%
System C: Logomedia
76% 3.4% 17.2% 3.4%
These figures accurately reflect the likelihood of the translations in (51). Given that
P(empire est) is about five times higher than P(empire sont), translation (51a) is rejected
in favor of (51b).
Having given examples of how post hoc validation works within both NPs and
sentences, we summarize in Table 12 the results obtained when we tested the 58 sen-
tences whose translations contained subject-verb agreement errors owing to boundary
friction. Improvements were seen for translations derived from each of the on-line MT
systems: from a minimum of 58.6% (34 translations) for system A to a maximum of
76% (44 translations) for system C. For each system, no improvement was found for
two translations, and for 10 translations, our methodology could not tell definitively
whether the word to be corrected was a noun or a verb, so no change was made.
Finally, in a small number of cases (between 2 and 12 translations), the target string
was not found on the Web, so again, no change was made.
In order to be as accurate and relevant as possible, statistical language (and trans-
lation) models should be derived from corpora that are as large as possible, represen-
tative, and of high quality. Although the Web is large, this post hoc validation process
shows that despite the inherent noise contained on the Web because of the heteroge-
neous nature of the documents contained therein, it remains a resource that is of great
use in evaluating translation candidates.
6. Conclusions and Further Work
We have presented an EBMT system based on the marker hypothesis that uses post
hoc validation and correction via the Web.11 Over 218,000 NPs and VPs were extracted
automatically from the Penn-II Treebank using just 59 of its 29,000 rule types. These
phrases were then translated automatically by three on-line MT systems. These trans-
lations gave rise to a number of automatically constructed linguistic resources: (1) the
original ?source,target? phrasal translation pairs, (2) the marker lexicon, (3) the gen-
11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system, seeded
with input from multiple translation systems, with a postvalidation process via the Web (amounting to
an n-gram target language model), in effect forms a multiengine MT system as described by Frederking
and Nirenburg (1994), Frederking et al (1994), and Hogan and Frederking (1998).
454
Computational Linguistics Volume 29, Number 3
eralized lexicon, and (4) the word-level lexicon. When the system is confronted with
new input, these knowledge sources are searched in turn for matching chunks, and
the target language chunks are combined to create translation candidates.
We presented a number of experiments that showed how the system fared when
confronted with NPs and sentences. For the test set of 500 NPs, we obtained trans-
lations in 96% of cases, with 77.8% of the 500 NPs being translated correctly. For
sentences, we obtained translations in 92% of cases, with a completely correct transla-
tion obtained 81.5% of the time. Translation quality improved both when chunks from
different on-line systems were used and when the system?s memories were seeded
with both third-person singular and third-person plural forms. For both NPs and sen-
tences, we obtained intelligible translations in over 96% of cases. In most cases, the
?best? translation was ranked in the top 10 translations output by the system and was
always ranked in the top 1% of translation candidates. This facilitates the task of any
translator interacting with our system who needs to search for the ?best? translation
among the alternatives provided.
We calculated the net gain of using wEBMT compared to the three on-line MT
systems. In some cases, an improvement of 50% was seen when EBMT was used.
As a consequence of the methodology chosen, we were able to perform a detailed
evaluation of the strengths and weaknesses of the three Web-based systems used in
our research, with Logomedia clearly outranking the other systems used. Neverthe-
less, adding chunks from the other two on-line MT systems improves both coverage
and translation quality. In sum, therefore, the best results are obtained when chunks
from all three on-line systems are used, and the system?s databases are seeded with
translations from these systems for both third-person singular and third-person plural
versions of sentences.
In addition, prior to the system?s outputting the best-ranked translation candidate,
morphological variants of certain components in the translation are searched for via
the Web in order to confirm it as the final output translation or to propose a corrected
alternative. Currently we validate our translations only with regard to subject head
noun?head verb agreement and determiner-noun agreement, but we plan to extend
this validation to cover more cases of boundary friction. We demonstrated that con-
siderable improvements can be made to the translations derived by the system by
submitting them to the Web for validation and correction.
A number of issues for further work present themselves. The decision to take only
those Penn-II rules occurring 1,000 or more times was completely arbitrary, and it
might be useful to include some strings corresponding to the less frequently occurring
structures in our database. Similarly, it would be a good idea to extend our word-level
lexicon by including more entries using rules in which the right-hand side contains a
single non-terminal.
Furthermore, the quality of the output was not taken into consideration when
selecting the on-line MT systems from which all our system resources are derived, so
that any results obtained may be further improved by selecting a ?better? MT system
that permits batch processing.
We could expect a significant improvement in the results obtained if we were to im-
port the original sentences and their translations into a sentential database. Although
we insert dummy subject pronouns to derive appropriate finite verb forms, we do not
maintain these translation pairs as a resource for subsequent consultation and retrieval.
Although the chance of finding an exact match at sentential level is very low, it will
increase as more sentence pairs are added to the database, especially if we restrict the
domain of applicability of (a version of) our system to a particular sublanguage area.
However, the major improvement that can be expected is in the segmentation process:
455
Way and Gough wEBMT
Given that verbs are not a closed class, any verb will be contained within (part of) a
chunk pertaining to its subject NP. That is, although subject-verb agreement poses a
considerable problem to our system given the choice of original input material, this
particular instance of boundary friction will disappear if we segment our translation
pairs at the sentential rather than at the phrasal level.
In addition, we want to evaluate our system further with respect to larger data sets.
Manual evaluation is costly, both in terms of time and effort required. Accordingly,
in future work we plan to use automatic evaluation methodologies such as sentence
error rate or word error rate. These are very harsh metrics: Consider the example in
(54), extracted from the Canadian Hansards:
(54) Again this was voted down by the Liberal majority =?
Malheureusement, encore une fois, la majorite? libe?rale l?a rejete?.
Automatic evaluation measures presuppose the existence of an ?oracle? (i.e., ?cor-
rect?) translation produced by a human, such as here. Translations derived by the
MT system to be evaluated are then compared against the human translation. In the
example in (54), the human has inserted malheureusement although there is no sign of
unfortunately in the English source. If the perfect translation Encore une fois, la majorite?
libe?rale l?a rejete? were produced by an MT system, therefore, it would be penalized, as
the human translation is always considered to be the ?correct? translation. We have
obtained a number of translation memories from two major computer companies, as
well as a large amount of monolingual data from the same domain, with which we
plan to test our system using automatic evaluation metrics in future work. This will
also enable us to test our EBMT methodology against other language pairs, which
may present the segmentation method employed with new challenges to overcome.
Finally, we plan to prioritize the lexical resources produced so that more weight-
ing would be given to translations derived from the phrasal and marker lexicons as
opposed to those derived via word insertion from the word level lexicon and the
generalized templates.
In sum, we have demonstrated that using a ?linguistics-lite? approach based on
the marker hypothesis, with a large number of phrases extracted automatically from
a very small number of the rules in the Penn Treebank, many new reusable linguistic
resources can be derived automatically that can be utilized in an EBMT system capable
of translating new input with quite reasonable rates of success. We have demonstrated
that a net gain may be achieved by using EBMT over on-line MT systems. We have
also shown that the Web can be used to validate and correct candidate translations
prior to their being output.
Acknowledgments
The authors wish to thank Mary Hearne for
helpful input in the initial stages of this
project. In addition, the insightful comments
provided by four anonymous reviewers
helped improve this article considerably. All
remaining errors are our own.
References
Ahrenberg, Lars, Mikael Andersson, and
Magnus Merkel. 2002. A system for
incremental and interactive word linking.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 485?490, Las
Palmas, Canary Islands, Spain.
Becker, Joseph. 1975. The phrasal lexicon. In
Proceedings of the International Workshop on
Theoretical Issues in Natural Language
Processing, pages 70?73, Cambridge, MA.
Block, Hans-Ulrich. 2000. Example-based
incremental synchronous interpretation.
In Wolfgang Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translation,
Springer Verlag, Berlin/Heidelberg/New
456
Computational Linguistics Volume 29, Number 3
York, pages 411?417.
Bod, Rens, Remko Scha, and Khalil Sima?an,
editors. 2003. Data-Oriented Parsing. CSLI
Publications, Stanford, CA.
Boutsis, Sotiris, and Stelios Piperidis. 1998.
Aligning clauses in parallel texts. In
Proceedings of the Third Conference on
Empirical Methods in Natural Language
Processing, pages 17?26, Granada, Spain.
Brown, Ralf. 2000. Automated
generalization of translation examples. In
Eighteenth International Conference on
Computational Linguistics: COLING 2000 in
Europe, pages 125?131, Saarbru?cken,
Germany.
Brown, Ralf. 2003. Clustered transfer-rule
induction for example-based translation.
In Michael Carl and Andy Way, editors,
Recent Advances in Example-Based Machine
Translation. Kluwer Academic, Dordrecht,
the Netherlands, pages 287?305.
Carl, Michael. 1999. Inducing translation
templates for example-based machine
translation. In Machine Translation Summit
VII, pages 250?258, Singapore.
Carl, Michael, and Andy Way, editors. 2003.
Recent Advances in Example-Based Machine
Translation. Kluwer Academic, Dordrecht,
the Netherlands.
Carl, Michael, Andy Way, and Reinhard
Scha?ler. 2002. Toward a hybrid integrated
translation environment. In Stephen
Richardson, editor, Machine Translation:
From Research to Real Users: Fifth Conference
of the Association for Machine Translation in
the Americas (AMTA-2002). Lecture Notes
in Artificial Intelligence 2499. Springer
Verlag, Berlin/Heidelberg, pages 11?20.
Cicekli, Ilyas, and Altay Gu?venir. 1996.
Learning translation rules from a
bilingual corpus. In Proceedings of the
Second International Conference on New
Methods in Language Processing, pages
90?97, Ankara, Turkey.
Frederking, Robert, and Sergei Nirenburg.
1994. Three heads are better than one. In
Proceedings of the Fourth Conference on
Applied Natural Language Processing
(ANLP-94), pages 95?100, Stuttgart,
Germany.
Frederking, Robert, Sergei Nirenburg, David
Farwell, Steven Helmreich, Eduard Hovy,
Kevin Knight, Stephen Beale, Constantin
Domashnev, Donna Attardo, Dean
Grannes, and Ralf Brown. 1994.
Integrating translations from multiple
sources with the Pangloss Mark III
machine translation system. In Proceedings
of the First Conference of the Association for
Machine Translation in the Americas, pages
73?80, Columbia, MD.
Fung, Pascale, and Kathleen McKeown.
1997. Finding terminology translations
from non-parallel corpora. In Proceedings
of the Fifth Annual Workshop on Very Large
Corpora, pages 192?202, Hong Kong.
Gough, Nano, Andy Way, and Mary
Hearne. 2002. Example-based machine
translation via the Web. In Stephen
Richardson, editor, Machine Translation:
From Research to Real Users: Fifth Conference
of the Association for Machine Translation in
the Americas (AMTA-2002) Lecture Notes
in Artificial Intelligence 2499. Springer
Verlag, Berlin/Heidelberg, pages 74?83.
Green, Thomas. 1979. The necessity of
syntax markers: Two experiments with
artificial languages. Journal of Verbal
Learning and Behavior, 18:481?496.
Grefenstette, Gregory. 1999. The World Wide
Web as a resource for example-based
machine translation tasks. In Proceedings of
the ASLIB Conference on Translating and the
Computer, volume 21, London.
Hogan, Christopher, and Robert E.
Frederking. 1998. An evaluation of the
multi-engine MT architecture. In Machine
Translation and the Information Soup:
Proceedings of the Third Conference of the
Association for Machine Translation in the
Americas (AMTA ?98), Lecture Notes in
Artificial Intelligence 1529. Springer
Verlag, Berlin/Heidelberg, pages 113?123.
Hovy, Edward. 1988. Generating language
with a phrasal lexicon. In David
McDonald and Leonard Bolc, editors,
Natural Language Generation Systems.
Springer Verlag, New York, pages
353?384.
Juola, Patrick. 1994. A psycholinguistic
approach to corpus-based machine
translation. In CSNLP 1994: Third
International Conference on the Cognitive
Science of Natural Language Processing,
Dublin.
Juola, Patrick. 1997. Corpus-based
acquisition of transfer functions using
psycholinguistic principles. In Daniel
Jones and Harold Somers, editors, New
Methods in Language Processing. UCL Press,
London, pages 207?218.
Juola, Patrick. 1998. On psycholinguistic
grammars. Grammars, 1(1):15?31.
Kaji, Hiroyuki, Takuya Kida, and Yuji
Morimoto. 1992. Learning translation
templates from bilingual text. In
Proceedings of the 15th [sic] International
Conference on Computational Linguistics
(COLING), pages 672?678, Nantes,
France.
Kay, Martin, and Martin Ro?scheisen. 1993.
Text-translation alignment. Computational
457
Way and Gough wEBMT
Linguistics, 19(1):121?142.
Littlestone, Nick, and Manfred Warmuth.
1992. The weighted majority algorithm.
Technical Report UCSC-CRL-91-28,
University of California, Santa Cruz.
Macklovitch, Elliott. 2000. Two types of
translation memory. In Proceedings of the
ASLIB Conference on Translating and the
Computer, volume 22, London.
Macklovitch, Elliott, and Graham Russell.
2000. What?s been forgotten in translation
memory. In Envisioning Machine Translation
in the Information Future: Proceedings of
Fourth Conference of the Association for
Machine Translation in the Americas
(AMTA-2000), pages 137?146, Cuernavaca,
Mexico.
McTait, Kevin, and Arturo Trujillo. 1999. A
language-neutral sparse-data algorithm
for extracting translation patterns. In
Proceedings of the Eighth International
Conference on Theoretical and Methodological
Issues in Machine Translation, pages 98?108,
Chester, England.
Milosavljevic, Maria, Adrian Tulloch, and
Robert Dale. 1996. Text generation in a
dynamic hypertext environment. In
Proceedings of the 19th Australasian
Computer Science Conference, pages
417?426, Melbourne, Australia.
Morgan, James, Richard Meier, and Elissa
Newport. 1989. Facilitating the acquisition
of syntax with cross-sentential cues to
phrase structure. Journal of Memory and
Language, 28:360?374.
Mori, Kazuo, and Shannon Moeser. 1983.
The role of syntax markers and semantic
referents in learning an artificial
language. Journal of Verbal Learning and
Verbal Behavior, 22:701?718.
Nagao, Makoto. 1984. A framework of a
mechanical translation between Japanese
and English by analogy principle. In
Alick Elithorn and Ranan Banerji, editors,
Artificial and Human Intelligence.
North-Holland, Amsterdam, pages
173?180.
Rayner, Manny, and David Carter. 1997.
Hybrid language processing in the
spoken language translator. In Proceedings
of the IEEE International Conference on
Acoustics, Speech and Signal Processing,
pages 107?110, Munich.
Sato, Satoshi, and Makoto Nagao. 1990.
Toward memory-based translation. In
COLING-90: Papers Presented to the 13th
International Conference on Computational
Linguistics, pages 247?252, Helsinki.
Scha?ler, Reinhard. 1996. Machine translation,
translation memories and the phrasal
lexicon: The localisation perspective. In
Proceedings of TKE-96: EAMT Workshop on
Machine Translation, pages 21?33, Vienna.
Scha?ler, Reinhard, Andy Way, and Michael
Carl. 2003. Example-based machine
translation in a controlled environment.
In Michael Carl and Andy Way, editors,
Recent Advances in Example-Based Machine
Translation, Kluwer Academic, Dordrecht,
the Netherlands, pages 83?114.
Simard, Michel, and Philippe Langlais. 2001.
Subsentential exploitation of translation
memories. In Machine Translation Summit
VIII, pages 335?339, Santiago de
Compostela, Spain.
Somers, Harold. 1998. Further experiments
in bilingual text alignment. International
Journal of Corpus Linguistics, 3:115?150.
Somers, Harold, Ian McLean, and Daniel
Jones. 1994. Experiments in multilingual
example-based generation. In CSNLP
1994: Third International Conference on the
Cognitive Science of Natural Language
Processing, Dublin.
Veale, Tony, and Andy Way. 1997. Gaijin: A
bootstrapping, template-driven approach
to example-based machine translation. In
International Conference, Recent Advances in
Natural Language Processing, pages
239?244, Tzigov Chark, Bulgaria.
Watanabe, Hideo. 1993. A method for
extracting translation patterns from
translation examples. In Proceedings of the
Fifth International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI ?93): MT in the Next
Generation, pages 292?301, Kyoto, Japan.
Zernik, Uri, and Michael Dyer. 1987. The
self-extending phrasal lexicon. Com-
putational Linguistics, 13(3?4):308?327.
Large-Scale Induction and Evaluation of
Lexical Resources from the Penn-II and
Penn-III Treebanks
Ruth O?Donovan?
Dublin City University
Michael Burke??
Dublin City University
Aoife Cahill?
Dublin City University
Josef van Genabith??
Dublin City University
Andy Way??
Dublin City University
We present a methodology for extracting subcategorization frames based on an automatic
lexical-functional grammar (LFG) f-structure annotation algorithm for the Penn-II and
Penn-III Treebanks. We extract syntactic-function-based subcategorization frames (LFG
semantic forms) and traditional CFG category-based subcategorization frames as well as
mixed function/category-based frames, with or without preposition information for obliques
and particle information for particle verbs. Our approach associates probabilities with frames
conditional on the lemma, distinguishes between active and passive frames, and fully
reflects the effects of long-distance dependencies in the source data structures. In contrast
to many other approaches, ours does not predefine the subcategorization frame types extracted,
learning them instead from the source data. Including particles and prepositions, we extract
21,005 lemma frame types for 4,362 verb lemmas, with a total of 577 frame types and an
average of 4.8 frame types per verb. We present a large-scale evaluation of the complete
set of forms extracted against the full COMLEX resource. To our knowledge, this is
the largest and most complete evaluation of subcategorization frames acquired automatically
for English.
1. Introduction
In modern syntactic theories (e.g., lexical-functional grammar [LFG] [Kaplan and
Bresnan 1982; Bresnan 2001; Dalrymple 2001], head-driven phrase structure gram-
mar [HPSG] [Pollard and Sag 1994], tree-adjoining grammar [TAG] [Joshi 1988], and
combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is
the central repository for much morphological, syntactic, and semantic information.
? National Centre for Language Technology, School of Computing, Dublin City University, Glasnevin,
Dublin 9, Ireland. E-mail: {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie.
? Centre for Advanced Studies, IBM, Dublin, Ireland.
Submission received: 19 March 2004; revised submission received: 18 December 2004; accepted for
publication: 2 March 2005.
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
Extensive lexical resources, therefore, are crucial in the construction of wide-coverage
computational systems based on such theories.
One important type of lexical information is the subcategorization requirements
of an entry (i.e., the arguments a predicate must take in order to form a grammatical
construction). Lexicons, including subcategorization details, were traditionally pro-
duced by hand. However, as the manual construction of lexical resources is time con-
suming, error prone, expensive, and rarely ever complete, it is often the case that the
limitations of NLP systems based on lexicalized approaches are due to bottlenecks in
the lexicon component. In addition, subcategorization requirements may vary across
linguistic domain or genre (Carroll and Rooth 1998). Manning (1993) argues that, aside
from missing domain-specific complementation trends, dictionaries produced by hand
will tend to lag behind real language use because of their static nature. Given these
facts, research on automating acquisition of dictionaries for lexically based NLP sys-
tems is a particularly important issue.
Aside from the extraction of theory-neutral subcategorization lexicons, there has
also been work in the automatic construction of lexical resources which comply
with the principles of particular linguistic theories such as LTAG, CCG, and HPSG
(Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004;
Nakanishi, Miyao, and Tsujii 2004). In this article we present an approach to auto-
mating the process of lexical acquisition for LFG (i.e., grammatical-function-based sys-
tems). However, our approach also generalizes to CFG category-based approaches. In
LFG, subcategorization requirements are enforced through semantic forms specifying
which grammatical functions are required by a particular predicate. Our approach is
based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and
Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III
Treebanks with LFG f-structures (Cahill et al 2002; Cahill, McCarthy, et al 2004). Our
technique requires a treebank annotated with LFG functional schemata. In the early
approach of van Genabith, Sadler, and Way (1999), this was provided by manually
annotating the rules extracted from the publicly available subset of the AP Treebank to
automatically produce corresponding f-structures. If the f-structures are of high qual-
ity, reliable LFG semantic forms can be generated quite simply by recursively reading
off the subcategorizable grammatical functions for each local PRED value at each level of
embedding in the f-structures. The work reported in van Genabith, Sadler, and Way
(1999) was small scale (100 trees) and proof of concept and required considerable
manual annotation work. It did not associate frames with probabilities, discriminate
between frames for active and passive constructions, properly reflect the effects of
long-distance dependencies (LDDs), or include CFG category information. In this
article we show how the extraction process can be scaled to the complete Wall
Street Journal (WSJ) section of the Penn-II Treebank, with about one million words
in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm
described in Cahill et al (2002) and Cahill, McCarthy, et al (2004). More recently
we have extended the extraction approach to the larger, domain-diverse Penn-III
Treebank. Aside from the parsed WSJ section, this version of the treebank contains
parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees)
taken from a variety of text genres.1 In addition to extracting grammatical-function-
1 For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ,
and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus
combined.
330
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
based subcategorization frames, we also include the syntactic categories of the predicate
and its subcategorized arguments, as well as additional details such as the prepositions
required by obliques and particles accompanying particle verbs. Our method discrim-
inates between active and passive frames, properly reflects LDDs in the source data
structures, assigns conditional probabilities to the semantic forms associated with each
predicate, and does not predefine the subcategorization frames extracted.
In Section 2 of this article, we briefly outline LFG, presenting typical lexical entries
and the encoding of subcategorization information. Section 3 reviews related work in
the area of automatic subcategorization frame extraction. Our methodology and its
implementation are presented in Section 4. In Section 5 we present results from the
extraction process. We evaluate the complete induced lexicon against the COMLEX
resource (Grishman, MacLeod, and Meyers 1994) and present the results in Section 6.
To our knowledge, this is by far the largest and most complete evaluation of subcat-
egorization frames automatically acquired for English. In Section 7, we examine the
coverage of our lexicon in regard to unseen data and the rate at which new lexical
entries are learned. Finally, in Section 8 we conclude and give suggestions for future
work.
2. Subcategorization in LFG
Lexical functional grammar (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a member of the family of constraint-based grammars. It posits minimally
two levels of syntactic representation:2 c(onstituent)-structure encodes details of sur-
face syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic
information about predicate?argument?modifier relations and certain morphosyntactic
properties such as tense, aspect, and case. C-structure takes the form of phrase structure
trees and is defined in terms of CFG rules and lexical entries. F-structure is pro-
duced from functional annotations on the nodes of the c-structure and implemented
in terms of recursive feature structures (attribute?value matrices). This is exemplified
by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using
the grammar in Figure 1, which results in the annotated c-structure and f-structure in
Figure 2.
The value of the PRED attribute in an f-structure is a semantic form ??gf1, gf2, . . . ,
gfn?, where ? is a lemma and gf a grammatical function. The semantic form provides
an argument list ?gf1,gf2, . . . ,gfn? specifying the governable grammatical functions (or
arguments) required by the predicate to form a grammatical construction. In Figure 1
the verb FOCUS requires a subject and an oblique object introduced by the preposition
on: FOCUS?(? SUBJ)(? OBLon)?. The argument list can be empty, as in the PRED value
for judge in Figure 1. According to Dalrymple (2001), LFG assumes the following uni-
versally available inventory of grammatical functions: SUBJ(ect), OBJ(ect), OBJ?, COMP,
XCOMP, OBL(ique)?, ADJ(unct), XADJ. OBJ? and OBL? represent families of grammatical
functions indexed by their semantic role, represented by the theta subscript. This list
of grammatical functions is divided into governable (subcategorizable) grammatical
functions (arguments) and nongovernable (nonsubcategorizable) grammatical func-
tions (modifiers/adjuncts), as summarized in Table 1.
2 LFGs may also involve morphological and semantic levels of representation.
331
Computational Linguistics Volume 31, Number 3
Figure 1
Sample LFG rules and lexical entries.
A number of languages allow the possibility of object functions in addition to the
primary OBJ, such as the second or indirect object in English. Oblique arguments are
realized as prepositional phrases in English. COMP, XCOMP, and XADJ are all clausal
functions which differ in the way in which they are controlled. A COMP is a closed
function which contains its own internal SUBJ:
The judge thinks [COMP that it will resume].
XCOMP and XADJ are open functions not requiring an internal SUBJ. The subject is
instead specified externally in the matrix phrase:
The judge wants [XCOMP to open an inquiry].
While many linguistic theories state subcategorization requirements in terms
of phrase structure (CFG categories), Dalrymple (2001) questions the viability and
universality of such an approach because of the variety of ways in which grammatical
functions may be realized at the language-specific constituent structure level. LFG
argues that subcategorization requirements are best stated at the f-structure level,
in functional rather than phrasal terms. This is because of the assumption that
abstract grammatical functions are primitive concepts as opposed to derivatives
332
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Figure 2
C- and f-structures for Penn Treebank sentence wsj 0267 72, The inquiry soon focused on the judge.
of phrase structural position. In LFG, the subcategorization requirements of a
particular predicate are expressed by its semantic form: FOCUS?(? SUBJ)(? OBLon)? in
Figure 1.
The subcategorization requirements expressed by semantic forms are enforced at
f-structure level through completeness and coherence well-formedness conditions on
f-structure (Kaplan and Bresnan 1982):
An f-structure is locally complete iff it contains all the governable grammatical
functions that its predicate governs. An f-structure is complete iff it and all its
subsidiary f-structures are locally complete. An f-structure is locally coherent iff
all the governable grammatical functions that it contains are governed by a
local predicate. An f-structure is coherent iff it and all its subsidiary f-structures
are locally coherent. (page 211)
Consider again the f-structure in Figure 2. The semantic form associated with
the verb focus is FOCUS?(? SUBJ)(? OBLon)?. The f-structure is locally complete, as it
contains the SUBJ and an OBL with the preposition on specified by the semantic
form. The f-structure also satisfies the coherence condition, as it does not contain
any governable grammatical functions other than the SUBJ and OBL required by the
local PRED.
333
Computational Linguistics Volume 31, Number 3
Table 1
Governable and nongovernable grammatical functions in LFG.
Governable GFs Nongovernable GFs
SUBJ ADJ
OBJ XADJ
XCOMP
COMP
OBJ?
OBL?
Because of the specific form of the LFG lexicon, our extraction approach differs in
interesting ways from that of previous lexical extraction experiments. This contrast is
made evident in Sections 3 and 4.
3. Related Work
The encoding of verb subcategorization properties is an essential step in the
construction of computational lexicons for tasks such as parsing, generation, and
machine translation. Creating such a resource by hand is time consuming and error
prone, requires considerable linguistic expertise, and is rarely if ever complete. In
addition, a hand-crafted lexicon cannot be easily adapted to specific domains or
account for linguistic change. Accordingly, many researchers have attempted to
construct lexicons automatically, especially for English. In this section, we discuss
approaches to CFG-based subcategorization frame extraction as well as attempts to
induce lexical resources which comply with specific linguistic theories or express
information in terms of more abstract predicate-argument relations. The evaluation of
these approaches is discussed in greater detail in Section 6, in which we compare our
results with those reported elsewhere in the literature.
We will divide more-general approaches to subcategorization frame acquisition
into two groups: those which extract information from raw text and those which
use preparsed and hand-corrected treebank data as their input. Typically in the
approaches based on raw text, a number of subcategorization patterns are predefined,
a set of verb subcategorization frame associations are hypothesized from the data,
and statistical methods are applied to reliably select hypotheses for the final lexicon.
Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators
of six predefined subcategorization frames. The frames do not include details of specific
prepositions. Brent used hypothesis testing on binomial frequency data to statistically
filter the induced frames. Ushioda et al (1993) run a finite-state NP parser on a
POS-tagged corpus to calculate the relative frequency of the same six subcategoriza-
tion verb classes. The experiment is limited by the fact that all prepositional phrases
are treated as adjuncts. Ushioda et al (1993) employ an additional statistical method
based on log-linear models and Bayes? theorem to filter the extra noise introduced by
the parser and were the first to induce relative frequencies for the extracted frames.
Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw
text through a stochastic tagger and a finite-state parser (which includes a set of
simple rules for subcategorization frame recognition) in order to extract verbs and
the constituents with which they co-occur. He assumes 19 different subcategorization
334
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
frame definitions, and the extracted frames include details of specific prepositions.
The extracted frames are noisy as a result of parser errors and so are filtered using
the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique
to approximately four million words of New York Times newswire, Manning acquired
4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames
per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames,
obtained by manually merging the classes exemplified in the COMLEX (MacLeod,
Grishman, and Meyers 1994) and ANLT (Boguraev et al 1987) dictionaries and adding
around 30 frames found by manual inspection. The frames incorporate control informa-
tion and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a
priori information about the probabilities of subcategorization frame membership and
use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering
phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining
more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998)
use a handwritten head-lexicalized, context-free grammar and a text corpus to
compute the probability of particular subcategorization patterns. The approach is
iterative with the aim of estimating the distribution of subcategorization frames
associated with a particular predicate. They perform a mapping between their frames
and those of the OALD, resulting in 15 frame types. These do not contain details of
specific prepositions.
More recently, a number of researchers have applied similar techniques to auto-
matically derive lexical resources for languages other than English. Schulte im Walde
(2002a, 2002b) uses a head-lexicalized probabilistic context-free grammar similar to
that of Caroll and Rooth (1998) to extract subcategorization frames from a large
German newspaper corpus from the 1990s. She predefines 38 distinct frame types,
which contain maximally three arguments each and are made up of a combination
of the following: nominative, dative, and accusative noun phrases; reflexive pro-
nouns; prepositional phrases; expletive es; subordinated nonfinite clauses; subordinated
finite clauses; and copula constructions. The frames may optionally contain details of
particular prepositional use. Unsupervised training is performed on a large German
newspaper corpus, and the resulting probabilistic grammar establishes the relevance of
different frame types to a specific lexical head. Because of computing time constraints,
Schulte im Walde limits sentence length for grammar training and parsing. Sentences
of length between 5 and 10 words were used to bootstrap the lexicalized grammar
model. For lexicalized training, sentences of length between 5 and 13 words were
used. The result is a subcategorization lexicon for over 14,000 German verbs. The
extensive evaluation carried out by Schulte im Walde will be discussed in greater detail
in Section 6.
Approaches using treebank-based data as a source for subcategorization infor-
mation, such as ours, do not predefine the frames to be extracted but rather learn them
from the data. Kinyon and Prolo (2002) describe a simple tool which uses fine-grained
rules to identify the arguments of verb occurrences in the Penn-II Treebank. This is
made possible by manual examination of more than 150 different sequences of syntactic
and functional tags in the treebank. Each of these sequences was categorized as a
modifier or argument. Arguments were then mapped to traditional syntactic functions.
For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic
function is subject. In general, argumenthood was preferred over adjuncthoood. As
Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to
say how effective their technique is. Sarkar and Zeman (2000) present an approach to
learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic
335
Computational Linguistics Volume 31, Number 3
1998). Czech is a language with a freer word order than English and so configurational
information cannot be relied upon. In a dependency tree, the set of all dependents
of the verb make up a so-called observed frame, whereas a subcategorization frame
contains a subset of the dependents in the observed frame. Finding subcategorization
frames involves filtering adjuncts from the observed frame. This is achieved using three
different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137
subcategorization frames from 19,126 sentences for 914 verbs (those which occurred
five times or more). Marinov and Hemming (2004) present preliminary work on the
automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank
(Simov, Popova, and Osenova 2002). In a similar way to that of Sarkar and Zeman
(2000), Marinov and Hemming?s system collects both arguments and adjuncts. It then
uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees
are annotated with HPSG-typed feature structure information and thus contain more
detail than the dependency trees. The work done for Bulgarian is small-scale, however,
as Marinov and Hemming are working with a preliminary version of the treebank with
580 sentences.
Work has been carried out on the extraction of formalism-specific lexical resources
from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are
fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component,
the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and
Vijay-Shanker (2000) explore a number of related approaches to the extraction of a
lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical
model for parsing. The extraction procedure utilizes a head percolation table as intro-
duced by Magerman (1995) in combination with a variation of Collins?s (1997) approach
to the differentiation between complement and adjunct. This results in the construction
of a set of lexically anchored elementary trees which make up the TAG in question.
The number of frame types extracted (i.e., an elementary tree without a specific lexical
anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for
the extraction of a TAG from the Penn Treebank. The extraction procedure consists
of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and
extended based on the approaches of Magerman (1994) and Collins (1997). Then the
elementary trees are read off in a quite straightforward manner. Finally any invalid
elementary trees produced as a result of annotation errors in the treebank are filtered out
using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged
from 3,014 to 6,099.
Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic
extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the
algorithm annotates the nodes with CCG categories in a top-down recursive manner.
The first step is to label each node as either a head, complement, or adjunct based
on the approaches of Magerman (1994) and Collins (1997). Each node is subsequently
assigned the relevant category based on its constituent type and surface configuration.
The algorithm handles ?like? coordination and exploits the traces used in the treebank
in order to interpret LDDs. Unlike our approach, those of Xia (1999) and Hockenmaier,
Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the
Penn-II trees.
Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004)
describe a methodology for acquiring an English HPSG from the Penn-II Treebank.
Manually defined heuristics are used to automatically annotate each tree in the treebank
with partially specified HPSG derivation trees: Head/argument/modifier distinctions
are made for each node in the tree based on Magerman (1994) and Collins (1997);
336
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
the whole tree is then converted to a binary tree; heuristics are applied to deal with
phenomena such as LDDs and coordination and to correct some errors in the tree-
bank, and finally an HPSG category is assigned to each node in the tree in accordance
with its CFG category. In the next phase of the process (externalization), HPSG lexical
entries are automatically extracted from the annotated trees through the application of
?inverse schemata.?
4. Methodology
The first step in the application of our methodology is the production of a tree-
bank annotated with LFG f-structure information. F-structures are attribute?value
structures which represent abstract syntactic information, approximating to ba-
sic predicate?argument?modifier structures. Most of the early work on automatic
f-structure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler,
van Genabith, and Way 2000) was applied only to small data sets (fewer than 200
sentences) and was largely proof of concept. However, more recent work (Cahill et al
2002; Cahill, McCarthy, et al 2004) has presented efforts in evolving and scaling up
annotation techniques to the Penn-II Treebank (Marcus et al 1994), containing more
than 1,000,000 words and 49,000 sentences.
We utilize the automatic annotation algorithm of Cahill et al (2002) and Cahill,
McCarthy, et al (2004) to derive a version of Penn-II in which each node in each
tree is annotated with LFG functional annotations in the form of attribute-value struc-
ture equations. The algorithm uses categorial, configurational, local head, and Penn-II
functional and trace information. The annotation procedure is dependent on locating
the head daughter, for which an amended version of Magerman (1994) is used. The
head is annotated with the LFG equation ?=?. Linguistic generalizations are provided
over the left (the prefix) and the right (suffix) context of the head for each syntactic
category occurring as the mother nodes of such heads. To give a simple example, the
rightmost NP to the left of a VP head under an S is likely to be the subject of the sen-
tence (? SUBJ =?), while the leftmost NP to the right of the V head of a VP is most
probably the verb?s object (? OBJ =?). Cahill, McCarthy, et al (2004) provide four
classes of annotation principles: one for noncoordinate configurations, one for coor-
dinate configurations, one for traces (long-distance dependencies), and a final ?catch
all and clean up? phase.
The satisfactory treatment of long-distance dependencies by the annotation algo-
rithm is imperative for the extraction of accurate semantic forms. The Penn Treebank
employs a rich arsenal of traces and empty productions (nodes which do not realize
any lexical material) to coindex displaced material with the position where it should
be interpreted semantically. The algorithm of Cahill, McCarthy, et al (2004) translates
the traces into corresponding reentrancies in the f-structure representation by treating
null constituents as full nodes and recording the traces in terms of index=i f-structure
annotations (Figure 3). Passive movement is captured and expressed at f-structure level
using a passive:+ annotation. Once a treebank tree is annotated with feature structure
equations by the annotation algorithm, the equations are collected, and a constraint
solver produces an f-structure.
In order to ensure the quality of the semantic forms extracted by our method, we
must first ensure the quality of the f-structure annotations. The results of two different
evaluations of the automatically generated f-structures are presented in Table 2. Both
use the evaluation software and triple encoding presented in Crouch et al (2002). The
first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures
337
Computational Linguistics Volume 31, Number 3
Figure 3
Use of reentrancy between TOPIC and COMP to capture long-distance dependency in Penn
Treebank sentence wsj 0008 2, Until Congress acts, the government hasn?t any authority to issue new
debt obligations of any kind, the Treasury said.
from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al (2004). For
the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.
There is, however, a risk of overfitting when evaluation is limited to a gold standard
of this size. More recently, Burke, Cahill, et al (2004a) carried out an evaluation of the
automatic annotation algorithm against the publicly available PARC 700 Dependency
Bank (King et al 2003), a set of 700 randomly selected sentences from Section 23
which have been parsed, converted to dependency format, and manually corrected
and extended by human validators. They report precision of over 88.5% and recall of
over 86% (Table 2). The PARC 700 Dependency Bank differs substantially from both
the DCU 105 f-structure bank and the automatically generated f-structures in regard to
Table 2
Results of f-structure evaluation.
DCU 105 PARC 700
Precision 96.52% 88.57%
Recall 96.62% 86.10%
F-score 96.57% 87.32%
338
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
the style of linguistic analysis, feature nomenclature, and feature geometry. Some, but
not all, of these differences are captured by automatic conversion software. A detailed
discussion of the issues inherent in this process and a full analysis of results is presented
in Burke, Cahill, et al (2004a). Results broken down by grammatical function for the
DCU 105 evaluation are presented in Table 3. OBL (prepositional phrase) arguments are
traditionally difficult to annotate reliably. The results show, however, that with respect
to obliques, the annotation algorithm, while slightly conservative (recall of 82%), is very
accurate: 96% of the time it annotates an oblique, the annotation is correct.
A high-quality set of f-structures having been produced, the semantic form ex-
traction methodology is applied. This is based on and substantially extends both the
granularity and coverage of an idea in van Genabith, Sadler, and Way (1999):
For each f-structure generated, for each level of embedding we determine the local
PRED value and collect the subcategorisable grammatical functions present at that level
of embedding. (page 72)
Consider the automatically generated f-structure in Figure 4 for tree wsj 0003 22
in the Penn-II and Penn-III Treebanks. It is crucial to note that in the automatically
generated f-structures the value of the PRED feature is a lemma and not a semantic
form. Exploiting the information contained in the f-structure and applying the
method described above, we recursively extract the following nonempty semantic
forms: impose([subj, obj, obl:on]), in([obj]), of([obj]), and on([obj]). In effect,
in both the approach of van Genabith, Sadler, and Way (1999) and our approach,
semantic forms are reverse-engineered from automatically generated f-structures
for treebank trees. The automatically induced semantic forms contain the following
subcategorizable syntactic functions:
SUBJ OBJ OBJ2 OBLprep OBL2 COMP XCOMP PART
PART is not a syntactic function in the strict sense, but we decided to capture the
relevant co-occurrence patterns of verbs and particles in the semantic forms. Just as
Table 3
Precision and recall on automatically generated f-structures by feature against the DCU 105.
Feature Precision Recall F-score
ADJUNCT 892/968 = 92 892/950 = 94 93
COMP 88/92 = 96 88/102 = 86 91
COORD 153/184 = 83 153/167 = 92 87
DET 265/267 = 99 265/269 = 99 99
OBJ 442/459 = 96 442/461 = 96 96
OBL 50/52 = 96 50/61 = 82 88
OBLAG 12/12 = 100 12/12 = 100 100
PASSIVE 76/79 = 96 76/80 = 95 96
RELMOD 46/48 = 96 46/50 = 92 94
SUBJ 396/412 = 96 396/414 = 96 96
TOPIC 13/13 = 100 13/13 = 100 100
TOPICREL 46/49 = 94 46/52 = 88 91
XCOMP 145/153 = 95 145/146 = 99 97
339
Computational Linguistics Volume 31, Number 3
Figure 4
Automatically generated f-structure and extracted semantic forms for the Penn-II Treebank
string wsj 0003 22, In July, the Environmental Protection Agency imposed a gradual ban on virtually
all uses of asbestos.
OBLprep includes the prepositional head of the PP, PART includes the actual particle
which occurs, for example, add([subj, obj, part:up]).
In the work presented here, we substantially extend and scale the approach of
van Genabith, Sadler, and Way (1999) in regard to coverage, granularity, and eval-
uation. First, we scale the approach to the full WSJ section of the Penn-II Treebank
and the parsed Brown corpus section of Penn-III, with a combined total of approx-
imately 75,000 trees. Van Genabith, Sadler, and Way (1999) was proof of concept on
100 trees. Second, in contrast to the approach of van Genabith, Sadler, and Way (1999)
(and many other approaches), our approach fully reflects long-distance dependencies,
indicated in terms of traces in the Penn-II and Penn-III Treebanks and correspond-
ing reentrancies at f-structure. Third, in addition to abstract syntactic-function-
based subcategorization frames, we also compute frames for syntactic function?CFG
category pairs, for both the verbal heads and their arguments, and also generate
340
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 4
Conflation of Penn Treebank tags.
Conflated Category Penn Treebank Category
JJ JJ
JJR
JJS
N NN
NNS
NNP
NNPS
PRP
RB RB
RBR
RBS
V VB
VBD
VBG
VBN
VBP
VBZ
MD
pure CFG-based subcategorization frames. Fourth, in contrast to the approach of
van Genabith, Sadler, and Way (1999) (and many other approaches), our method differ-
entiates between frames for active and passive constructions. Fifth, in contrast to that of
van Genabith, Sadler, and Way (1999), our method associates conditional probabilities
with frames. Sixth, we evaluate the complete set of semantic forms extracted (not
just a selection) against the manually constructed COMLEX (MacLeod, Grishman, and
Meyers 1994) resource.
In order to capture CFG-based categorial information, we add a CAT feature to
the f-structures automatically generated from the Penn-II and Penn-III Treebanks. Its
value is the syntactic category of the lexical item whose lemma gives rise to the PRED
value at that particular level of embedding. This makes it possible to classify words
and their semantic forms based on their syntactic category and reduces the risk of
inaccurate assignment of subcategorization frame frequencies due to POS ambiguity,
distinguishing, for example, between the nominal and verbal occurrences of the lemma
fight. With this, the output for the verb impose in Figure 4 is impose(v,[subj, obj,
obl:on]). For some of our experiments, we conflate the different verbal (and other)
tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further
extension, the extraction procedure reads off the syntactic category of the head of
each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]).3
In this way, our methodology is able to produce surface syntactic as well as abstract
functional subcategorization details. Dalrymple (2001) argues that there are cases,
albeit exceptional ones, in which constraints on syntactic category are an issue in
subcategorization. In contrast to much of the work reviewed in Section 3, which limits
itself to the extraction of surface syntactic subcategorization details, our system can
provide this information as well as details of grammatical function.
3 We do not associate syntactic categories with OBLs as they are always PPs.
341
Computational Linguistics Volume 31, Number 3
Another way in which we develop and extend the basic extraction algorithm
is to deal with passive voice and its effect on subcategorization behavior. Consider
Figure 5: Not taking into account that the example sentence is a passive construction,
the extraction algorithm extracts outlaw([subj]). This is incorrect, as outlaw is a tran-
sitive verb and therefore requires both a subject and an object to form a gram-
matical sentence in the active voice. To cope with this problem, the extraction al-
gorithm uses the feature-value pair passive:+, which appears in the f-structure at
the level of embedding of the verb in question, to mark that predicate as occurring
in the passive: outlaw([subj],p). The annotation algorithm?s accuracy in recognizing
passive constructions is reflected by the f-score of 96% reported in Table 3 for the
PASSIVE feature.
The syntactic functions COMP and XCOMP refer to clausal complements with
different predicate control patterns as described in Section 2. However, as it stands,
neither of these functions betrays anything about the syntactic nature of the constructs
in question. Many lexicons, both automatically acquired and manually created, are
more fine grained in their approaches to subcategorized clausal arguments, differ-
entiating, for example, between a that-clause and a to + infinitive clause (Ushioda
et al 1993). With only a slight modification, our system, along with the details
provided by the automatically generated f-structures, allows us to extract frames
with an equivalent level of detail. For example, to identify a that-clause, we use
Figure 5
Automatically generated f-structure for the Penn-II Treebank string wsj 0003 23. By 1997, almost
all remaining uses of cancer-causing asbestos will be outlawed.
342
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 5
Semantic forms for the verb accept.
Semantic form Occurrences Conditional probability
accept([subj, obj]) 122 0.813
accept ([subj]) 11 0.073
accept([subj, comp]) 5 0.033
accept([subj, obl:as]) 3 0.020
accept([subj, obj, obl:as]) 3 0.020
accept([subj, obj, obl:from]) 3 0.020
accept([subj, obj, obl:at]) 1 0.007
accept([subj, obj, obl:for]) 1 0.007
accept([subj, obj, xcomp]) 1 0.007
the feature-value pair that:+ at f-structure level to read off the following subcate-
gorization frame for the verb add: add([subj,comp(that)]). Using the feature-value pair
to inf:+, we can identify to + infinitive clauses, resulting in the following frame for
the verb want: want([subj,xcomp(to inf)]). We can also derive control information about
open complements. In Figure 5, the reentrant XCOMP subject is identical to the subject
of will in the matrix clause, which allows us to induce information about the nature
of the external control of the XCOMP (i.e., whether it is subject or object control).
In order to estimate the likelihood of the co-occurrence of a predicate with a partic-
ular argument list, we compute conditional probabilities for subcategorization frames
based on the number of token occurrences in the corpus:
P (ArgList|?) = count(??ArgList?)?n
i=1 count(??ArgListi?)
where ArgList1... ArgListn are the possible argument lists which can occur for ?. Be-
cause of variations in verbal subcategorization across domains, probabilities are also
useful for predicting the way in which verbs behave in certain contexts. In Section 6,
we use the conditional probabilities to filter possible error judgments by our system.
Tables 5?7 show, with varying levels of analysis, the attested semantic forms for the
verb accept with their associated conditional probabilities. The effect of differentiating
between the active and passive occurrences of verbs can be seen in the different con-
ditional probabilities associated with the intransitive frame ([subj]) of the verb accept
(shown in boldface type) in Tables 5 and 6.4 Table 7 shows the joint grammatical-
function/syntactic-category-based subcategorization frames.
5. Results
We extract semantic forms for 4,362 verb lemmas from Penn-III. Table 8 shows the
number of distinct semantic form types (i.e., lemma and argument list combination)
4 Given these, it is possible to condition frames on both lemma (?) and voice (v: active/passive):
P (ArgList|?, v) = count(??ArgList, v?)?n
i=1 count(??ArgListi, v?)
343
Computational Linguistics Volume 31, Number 3
Table 6
Semantic forms for the verb accept marked with p for passive use.
Semantic form Occurrences Conditional probability
accept([subj, obj]) 122 0.813
accept ([subj],p) 9 0.060
accept([subj, comp]) 5 0.033
accept([subj, obl:as],p) 3 0.020
accept([subj, obj, obl:as]) 3 0.020
accept([subj, obj, obl:from]) 3 0.020
accept ([subj]) 2 0.013
accept([subj, obj, obl:at]) 1 0.007
accept([subj, obj, obl:for]) 1 0.007
accept([subj, obj, xcomp]) 1 0.007
Table 7
Semantic forms for the verb accept including syntactic category for each grammatical function.
Semantic form Occurrences Conditional probability
accept([subj(n), obj(n)]) 116 0.773
accept([subj(n)]) 11 0.073
accept([subj(n), comp(that)]) 4 0.027
accept([subj(n), obj(n), obl:from]) 3 0.020
accept([subj(n), obl:as]) 3 0.020
Other 13 0.087
extracted. Discriminating obliques by associated preposition and recording particle
information, the algorithm finds a total of 21,005 semantic form types, 16,000 occurring
in active voice and 5,005 in passive voice. When the obliques are parameterized for
prepositions and particles are included for particle verbs, we find an average of 4.82
semantic form types per verb. Without the inclusion of details for individual preposi-
tions or particles, there was an average of 3.45 semantic form types per verb. Unlike
many of the researchers whose work is reviewed in Section 3, we do not predefine the
frames extracted by our system. Table 9 shows the numbers of distinct frame types
extracted from Penn-II, ignoring PRED values.5 We provide two columns of statistics,
one in which all oblique (PP) arguments are condensed into one OBL function and
all particle arguments are condensed into part, and the other in which we differen-
tiate among obl:to (e.g., give), obl:on (e.g., rely), obl:for (e.g., compensate), etc., and
likewise for particles. Collapsing obliques and particles into simple functions, we extract
38 frame types. Discriminating particles and obliques by preposition, we extract 577
frame types. Table 10 shows the same results for Penn-III, with 50 simple frame types
and 1,084 types when parameterized for prepositions and particles. We also show the
result of applying absolute thresholding techniques to the semantic forms induced.
Applying an absolute threshold of five occurrences, we still generate 162 frame types
5 To recap, if two verbs have the same subcategorization requirements (e.g., give([subj, obj, obj2]),
send([subj, obj, obj2])), then that frame [subj, obj, obj2] is counted only once.
344
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 8
Number of semantic form types for Penn-III.
Without prepositions and particles With prepositions and particles
Semantic form types 15,166 21,005
Active 11,038 16,000
Passive 4,128 5,005
Table 9
Number of frame types for verbs for Penn-II.
Without prepositions With prepositions
and particles and particles
Number of frame types 38 577
Number of singletons 1 243
Number occurring twice 1 84
Number occurring five or fewer times 7 415
Number occurring more than five times 31 162
from Penn-II and 221 from Penn-III. Briscoe and Carroll (1997), by comparison, employ
163 distinct predefined frames.
6. Evaluation
Most of the previous approaches discussed in Section 3 have been evaluated to
different degrees. In general, a small number of frequently occurring verbs is selected,
and the subcategorization frames extracted for these verbs (from some quantity of
unseen test data) are compared to a gold standard. The gold standard is either manually
custom-made based on the test data or adapted from an existing external resource
such as the OALD (Hornby 1980) or COMLEX (MacLeod, Grishman, and Meyers
1994). There are advantages and disadvantages to both types of gold standard. While
it is time-consuming to manually construct a custom-made standard, the resulting
standard has the advantage of containing only the subcategorization frames exhibited
in the test data. Using an existing externally produced resource is quicker, but the gold
Table 10
Number of frame types for verbs for Penn-III.
Without prepositions With prepositions
and particles and particles
Number of frame types 50 1,084
Number of singletons 6 544
Number occurring twice 2 147
Number occurring five or fewer times 12 863
Number occurring more than five times 38 221
345
Computational Linguistics Volume 31, Number 3
standard may contain many more frames than those which occur in the data from which
the test lexicon is induced or, indeed, may omit relevant correct frames contained in
the data. As a result, systems generally score better against custom-made, manually
established gold standards.
Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they
evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.
Their system recognizes 15 frames, and these do not contain details of subcategorized-
for prepositions. Still, to date this is the largest number of verbs used in any of the
evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000)
evaluate 914 Czech verbs against a custom-made gold standard and record a token
recall of 88%. However, their evaluation does not examine the extracted subcatego-
rization frames but rather the argument?adjunct distinctions posited by their sys-
tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b)
for German. She evaluates 3,000 German verbs with a token frequency between
10 and 2,000 against the Duden (Dudenredaktion 2001). We will refer to this work
and the methods and results presented by Schulte im Walde again in Sections 6.2
and 6.3.
We carried out a large-scale evaluation of our automatically induced lexicon (2,993
active verb lemmas for Penn-II and 3,529 for Penn-III, as well as 1,422 passive verb
lemmas from Penn-II) against the COMLEX resource. To our knowledge this is the most
extensive evaluation ever carried out for English lexical extraction. We conducted a
number of experiments on the subcategorization frames extracted from Penn-II and
Penn-III which are described and discussed in Sections 6.2, 6.3, and 6.4. Finding a
common format for the gold standard and induced lexical entries is a nontrivial task.
To ensure that we did not bias the evaluation in favor of either resource, we carried
out two different mappings for the frames from Penn-II and Penn-III: COMLEX-LFG
Mapping I and COMLEX-LFG Mapping II. For each mapping we carried out six basic
experiments (and two additional ones for COMLEX-LFG Mapping II) for the active
subcategorization frames extracted. Within each experiment, the following factors were
varied: level of prepositional phrase detail, level of particle detail, relative threshold
(1% or 5%), and incorporation of an expanded set of directional prepositions. Using
the second mapping we also evaluated the automatically extracted passive frames and
experimented with absolute thresholds. Direct comparison of subcategorization frame
acquisition systems is difficult because of variations in the number of frames extracted,
the number of test verbs, the gold standards used, the size of the test data, and the
level of detail in the subcategorization frames (e.g., whether they are parameterized
for specific prepositions). Therefore, in order to establish a baseline against which to
compare our results, following Schulte in Walde (2002b), we assigned the two most
frequent frame types (transitive and intransitive) by default to each verb and compared
this ?artificial? lexicon to the gold standard. The section concludes with a full discussion
of the reported results.
6.1 COMLEX
We evaluate our induced semantic forms against COMLEX (MacLeod, Grishman, and
Meyers 1994), a computational machine-readable lexicon containing syntactic infor-
mation for approximately 38,000 English headwords. Its creators paid particular
attention to the encoding of more detailed subcategorization information than is avail-
able in either the OALD or the LDOCE (Proctor 1978), both for verbs and for nouns
346
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Figure 6
Intersection between active-verb lemma types in COMLEX and the Penn-II-induced lexicon.
and adjectives which take complements (Grishman, MacLeod, and Meyers 1994). By
choosing to evaluate against COMLEX, we set our sights high: Our extracted semantic
forms are fine-grained, and COMLEX is considerably more detailed than the OALD
or LDOCE used for earlier evaluations. While our system can generate semantic forms
for any lemma (regardless of part of speech) which induces a PRED value, we have
thus far evaluated the automatic generation of subcategorization frames for verbs
only. COMLEX defines 138 distinct verb frame types without the inclusion of specific
prepositions or particles.
As COMLEX contains information other than subcategorization details, it was
necessary for us to extract the subcategorization frames associated with each verbal
lexicon entry. The following is a sample entry for the verb reimburse:
(VERB :ORTH ?reimburse? :SUBC ((NP-NP)
(NP-PP :PVAL (?for?))
(NP)))
Each entry is organized as a nested set of typed feature-value lists. The first symbol
(i.e., VERB) gives the part of speech. The value of the :ORTH feature is the base form
of the verb. Any entry with irregular morphological behavior will also include the
features :PLURAL, :PAST, and so on, with the relevant values. All verbs have a :SUBC
feature, and for our purposes, this is the most interesting feature. In the case of the
example above, the subcategorization values specify that reimburse can occur with two
object noun phrases (NP-NP), an object noun phrase followed by a prepositional phrase
headed by for (NP-PP :PVAL (?for?)) or just an object noun phrase (NP). (Note that the
details of the subject are not included in COMLEX frames.) What makes the COMLEX
resource particularly suitable for our evaluation is that each of the complement types
(NP-NP, NP-PP, and NP) which make up the value of the :SUBC feature is associated with
a formal frame definition which looks like the following:
(vp-frame np-np :cs ((np 2)(np 3))
:gs (:subject 1 :obj 2 :obj2 3)
:ex ?she asked him his name?)
The value of the :cs feature is the constituent structure of the subcategorization
frame, which lists the syntactic CF-PSG constituents in sequence (omitting the sub-
ject, again). The value of the :gs feature is the grammatical structure which indicates
the functional role played by each of the CF-PSG constituents. The elements of the
347
Computational Linguistics Volume 31, Number 3
Figure 7
Intersection between active-verb lemma types in COMLEX and the Penn-III-induced lexicon.
constituent structure are indexed, and these indices are referenced in the :gs field.
The index 1 always refers to the surface subject of the verb. This mapping between
constituent structure and functional structure makes the information contained in
COMLEX particularly suitable as an evaluation standard for the LFG semantic forms
which we induce.
We present the evaluation for the verbs which occur in an active context in the
treebank. COMLEX does not provide passive frames. For Penn-II, there are 2,993
verb lemmas (used actively) that both resources have in common. 2,669 verb lemmas
appear in COMLEX but not in the induced lexicon, and 416 verb lemmas (used actively)
appear in the induced lexicon but not in COMLEX (Figure 6). For Penn-III, COMLEX
and the induced lexicon share 3,529 verb lemmas (used actively). This is shown in
Figure 7. 6
6.2 COMLEX-LFG Mapping I and Penn-II
In order to carry out the evaluation, we have to find a common format for the expression
of subcategorization information between our induced LFG-style subcategorization
frames and those contained in COMLEX. The following are the common syntactic
functions: SUBJ, OBJ, OBJi, COMP, and PART. Unlike our system, COMLEX does not
distinguish an OBL from an OBJi, so we converted all the obliques in the induced frames
to OBJi. As in COMLEX, the value of i depends on the number of objects/obliques
already present in the semantic form. COMLEX does not differentiate between COMPs
and XCOMPs as our system does (control information is expressed in a different way:
see Section 6.3), so we conflate our two LFG categories to that of COMP. The process is
summarized in Table 11.
The manually constructed COMLEX entries provide a gold standard against which
we evaluate the automatically induced frames. We calculate the number of true pos-
itives (tps) (where our semantic forms and those from COMLEX are the same), the
number of false negatives ( fns) (those frames which appeared in COMLEX but were not
produced by our system), and the number of false positives ( fps) (those frames
6 Given these figures, one might begin to wonder about the value of automatic induction. First, COMLEX
does not rank frames by probabilities, which are essential in disambiguation. Second, the coverage of
COMLEX is not complete: 518 lemmas ?discovered? by the induction experiment are not listed in
COMLEX; see the error analysis in Section 6.5.
348
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 11
Mapping I: Merging of COMLEX and LFG syntactic functions.
Our syntactic functions COMLEX syntactic functions Merged function
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJi
OBL Obj3
OBL2 Obj4
COMP Comp COMP
XCOMP
PART Part PART
produced by our system which do not appear in COMLEX). We calculate precision,
recall, and F-score using the following standard equations:
recall =
tp
tp + fn
precision =
tp
tp + fp
f-score =
2 ? recall ? precision
recall + precision
We use the frequencies associated with each of our semantic forms in order to set
a relative threshold to filter the selection of semantic forms. For a threshold of 1% we
disregard any semantic forms with a conditional probability (i.e., given a lemma) of
less than or equal to 0.01. As some verbs occur less frequently than others, we think it
is important to use a relative rather than absolute threshold (as in Carroll and Rooth
[1998], for instance) in this way. We carried out the evaluation in a similar way to
Schulte im Walde?s (2002b) for German, the only experiment comparable in scale to
ours. Despite the obvious differences in approach and language, this allows us to make
some tentative comparisons between our respective results. The statistics shown in
Table 12 give the results of three different experiments with the relative threshold set
to 1%. As for all the results tables, the baseline statistics (simply assigning the most
frequent frames, in this case transitive and intransitive, to each lemma by default) are
in each case shown in the left column, and the results achieved by our induced lexicon
are presented in the right column. Distinguishing between complement and adjunct
prepositional phrases is a notoriously difficult aspect of automatic subcategorization
frame acquisition. For this reason, following the evaluation setup in Schulte im Walde
(2002b), the three experiments vary with respect to the amount of prepositional infor-
mation contained in the subcategorization frames.
Experiment 1. Here we excluded subcategorized prepositional-phrase arguments en-
tirely from the comparison. In a manner similar to that of Schulte im Walde (2002b), any
349
Computational Linguistics Volume 31, Number 3
Table 12
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).
Precision Recall F-score
Mapping I Baseline Induced Baseline Induced Baseline Induced
Experiment 1 66.1% 75.2% 65.8% 69.1% 66.0% 72.0%
Experiment 2 71.5% 65.5% 64.3% 63.1% 67.7% 64.3%
Experiment 3 64.7% 71.8% 11.9% 16.8% 20.1% 27.3%
frames containing an OBL were mapped to the same frame type minus that argument.
For example, the frame [subj,obl:for] becomes [subj]. Using a relative threshold of
1% (Table 12), our results (precision of 75.2%, recall of 69.1%, and F-score of 72.0%)
are remarkably similar to those of Schulte im Walde (2002b), who reports precision of
74.53%, recall of 69.74%, and an f-score of 72.05%.
Experiment 2. Here we include subcategorized prepositional phrase arguments but
only in their simplest form; that is, they were not parameterized for particular prepo-
sitions. For example, the frame [subj,obl:for] is rewritten as [subj,obl]. Using a
relative threshold of 1% (Table 12), our results (precision of 65.5%, recall of 63.1%, and
F-score of 64.3%) compare favorably to those of Schulte im Walde (2002b), who recorded
precision of 60.76%, recall of 63.91%, and an F-score of 62.30%.
Experiment 3. Here we used semantic forms which contain details of specific prepo-
sitions for any subcategorized prepositional phrase (e.g., [subj,obl:for]). Using a rela-
tive threshold of 1% (Table 12), our precision figure (71.8%) is quite high (in comparison
to 65.52% as recorded by Schulte im Walde [2002b]). However our recall (16.8%) is very
low (compared to the 50.83% that Schulte im Walde [2002b] reports). Consequently our
F-score (27.3%) is also low (Schulte im Walde [2002b] records an F-score of 57.24%). The
reason for this is discussed in Section 6.2.1.
The statistics in Table 13 are the result of the second experiment, in which the
relative threshold was increased to 5%. The effect of such an increase is obvious in
that precision goes up (by as much as 5%) for each of the three evaluations while
recall goes down (by as much as 5.5%). This is to be expected, as a greater threshold
means that there are fewer semantic forms associated with each verb in the induced
lexicon, but they are more likely to be correct because of their greater frequency of
occurrence. The conditional probabilities we associate with each semantic form together
with thresholding can be used to customize the induced lexicon to the task for which
it is required, that is, whether a very precise lexicon is preferred to one with broader
Table 13
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).
Precision Recall F-score
Mapping I Baseline Induced Baseline Induced Baseline Induced
Experiment 1 66.1% 80.2% 65.8% 63.6% 66.0% 70.9%
Experiment 2 71.5% 69.6% 64.3% 56.9% 67.7% 62.7%
Experiment 3 64.7% 76.7% 11.9% 13.9% 20.1% 23.5%
350
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
coverage. In Tables 12 and 13, the baseline is exceeded in all experiments with the
exception of Experiment 2. This can be attributed to Mapping I, in which OBLi becomes
OBJi (Table 11). Experiment 2 includes obliques without the specific preposition, mean-
ing that in this mapping, the frame [subj,obj:with] becomes [subj,obj]. Therefore,
the transitive baseline frame scores better than it should against the gold standard. A
more fine-grained LFG-COMLEX mapping in which this effect disappears is presented
in Section 6.3.
6.2.1 Directional Prepositions. Our recall statistic was particularly low in the case of
evaluation using details of prepositions (Experiment 3, Tables 12 and 13). This can be
accounted for by the fact that the creators of COMLEX have chosen to err on the side
of overgeneration in regard to the list of prepositions which may occur with a verb and
a subcategorization frame containing a prepositional phrase. This is particularly true
of directional prepositions. For COMLEX, a list of 31 directional prepositions (Table 14)
was prepared and assigned in its entirety by default to any verb which can potentially
appear with any directional preposition in order to save time and avoid the risk of
missing prepositions. Grishman, MacLeod, and Meyers (1994) acknowledge that this
can lead to a preposition list which is ?a little rich? for a particular verb, but this is
the approach they have chosen to take. In a subsequent experiment, we incorporated
this list of directional prepositions by default into our semantic form induction process
in the same way as the creators of COMLEX have done. Table 15 shows that doing
so results in a significant improvement in the recall statistic (45.1%), as would be
expected, with the new statistic being almost three times as good as the result re-
ported in Table 12 for Experiment 3 (16.8%). There is also an improvement in the
precision figure (from 71.8% to 86.9%). This is due to a substantial increase in the
number of true positives (from 5,612 to 14,675) compared with a stationary false pos-
itive figure (2,205 in both cases). The f-score increases from 27.3% to 59.4%.
6.3 COMLEX-LFG Mapping II and Penn-II
The COMLEX-LFG Mapping I presented above establishes a ?least common denomi-
nator? for the COMLEX and our LFG-inspired resources. More-fine-grained mappings
are possible: in order to ensure that the mapping from our semantic forms to the
COMLEX frames did not oversimplify the information in the automatically extracted
subcategorization frames, we conducted a further set of experiments in which we
converted the information in the COMLEX entries to the format of our extracted
semantic forms. We explicitly differentiated between OBLs and OBJs by automatically
Table 14
COMLEX directional prepositions.
about across along around
behind below beneath between
beyond by down from
in inside into off
on onto out out of
outside over past through
throughout to toward toward
up up to via
351
Computational Linguistics Volume 31, Number 3
Table 15
Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).
Mapping I Precision Recall F-score
Experiment 3 86.9% 45.1% 59.4%
deducing whether a COMLEX OBJi was coindexed with an NP or a PP. Furthermore, as
can be seen in the following example, COMLEX frame definitions contain details of the
control patterns of sentential complements, encoded using the :features attribute. This
allows for automatic discrimination between COMPs and XCOMPs.
(vp-frame to-inf-sc :cs (vp 2 :mood to-infinitive :subject 1)
:features (:control subject)
:gs (:subject 1 :comp 2)
:ex ?I wanted to come?)
The mapping is summarized in Table 16. The results of the subsequent evaluation are
presented in Tables 17 and 18. We have added Experiments 2a and 3a. These are the
same as Experiments 2 and 3, except that they additionally include the specific particle
with each PART function. While the recall figures in Tables 17 and 18 are slightly lower
than those in Tables 12 and 13, changing the mapping in this way results in an increase
in precision in each case (by as much as 11.6%). The results of the lexical evaluation
are consistently better than the baseline, in some cases by almost 16% (Experiment 2,
threshold 5%). Notice that in contrast to Tables 12 and 13, in the more-fine-grained
COMLEX-LFG Mapping II presented here, all experiments exceed the baseline.
6.3.1 Directional Prepositions. The recall figures for Experiments 3 and 3a in Table 17
(24.0% and 21.5%) and Table 18 (19.7% and 17.4%) drop in a similar fashion to the results
seen in Tables 12 and 13. For this reason, we again incorporated the list of 31 directional
prepositions (Table 14) by default and reran Experiments 3 and 3a for a threshold of
1%. The results are presented in Table 19. The effect was as expected: The recall scores
for the two experiments increased to 40.8% and 35.4% (from 24.0% and 22.5%), and the
F-scores increased to 54.4% and 49.7% (from 35.9% and 33.0%).
6.3.2 Passive Evaluation. Table 20 presents the results of evaluating the extracted pas-
sive semantic forms for 1,422 verb lemmas shared by the induced lexicon and COMLEX.
Table 16
Mapping II: Merging of COMLEX and LFG syntactic functions.
Our syntactic functions COMLEX syntactic functions Merged function
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJ2
OBL Obj3 OBL
OBL2 Obj4 OBL2
COMP Comp COMP
XCOMP Comp XCOMP
PART Part PART
352
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 17
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 72.1% 79.0% 58.5% 59.6% 64.6% 68.0%
Experiment 2 65.2% 77.1% 37.4% 50.4% 47.5% 61.0%
Experiment 2a 65.2% 76.4% 32.7% 44.5% 43.6% 56.3%
Experiment 3 65.2% 75.9% 15.2% 24.0% 24.7% 35.9%
Experiment 3a 65.2% 71.0% 13.6% 21.5% 22.5% 33.0%
Table 18
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 72.1% 83.5% 58.5% 54.7% 64.6% 66.1%
Experiment 2 65.2% 81.4% 37.4% 44.8% 47.5% 57.8%
Experiment 2a 65.2% 80.9% 32.7% 39.0% 43.6% 52.6%
Experiment 3 65.2% 75.9% 15.2% 19.7% 24.7% 31.3%
Experiment 3a 65.2% 75.5% 13.6% 17.4% 22.5% 28.3%
We applied lexical-redundancy rules (Kaplan and Bresnan 1982) to automatically con-
vert the active COMLEX frames to their passive counterparts: For example, subjects are
demoted to optional by oblique agents, and direct objects become subjects. The resulting
precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall
when prepositional details were included (from 54.7% to 29.3%).
Table 19
Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).
Mapping II Precision Recall F-score
Experiment 3 81.7% 40.8% 54.4%
Experiment 3a 83.1% 35.4% 49.7%
Table 20
Results of Penn-II evaluation of passive frames (relative threshold of 1%).
Passive Precision Recall F-score
Experiment 2 80.2% 54.7% 65.1%
Experiment 2a 79.7% 46.2% 58.5%
Experiment 3 72.6% 33.4% 45.8%
Experiment 3a 72.3% 29.3% 41.7%
353
Computational Linguistics Volume 31, Number 3
6.3.3 Absolute Thresholds. Many of the previous approaches discussed in Section 3 use
a limited number of verbs for evaluation, based on the verbs? absolute frequency in the
corpus. We carried out a similar experiment. Table 21 shows the results of Experiment
2 for all verbs, for the verb lemmas with an absolute frequency greater than 100, and
for verbs with a frequency greater than 200. The use of an absolute threshold results
in an increase in precision (from 77.1% to 82.3% and 81.7%), an increase in recall (from
50.4% to 60.8% to 58.7%), and an overall increase in F-score (from 61.0% to 69.9%
and 68.4%).
6.4 Penn-III (Mapping-II)
Recently we have applied our methodology to the Penn-III Treebank, a more balanced
corpus resource with a number of text genres. Penn-III consists of the WSJ section from
Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus
comprises 24,242 trees compiled from a variety of text genres including popular lore,
general fiction, science fiction, mystery and detective fiction, and humor. It has been
shown (Roland and Jurafsky 1998) that the subcategorization tendencies of verbs vary
across linguistic domains. Our aim, therefore, is to increase the scope of the induced
lexicon not only in terms of the verb lemmas for which there are entries, but also in
terms of the frames with which they co-occur. The f-structure annotation algorithm was
extended with only minor amendments to cover the parsed Brown corpus. The most
important of these was the way in which we distinguish between oblique and adjunct.
We noted in Section 4 that our method of assigning an oblique annotation in Penn-II
was precise, albeit conservative. Because of a change of annotation policy in Penn-III,
the -CLR tag (indicating a close relationship between a PP and the local syntactic head),
information which we had previously exploited, is no longer used. For Penn-III the
algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such
as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques.
In addition, the algorithm annotates as obliques PPs associated with -PUT (locative
complements of the verb put) or -DTV (second object in ditransitives) tags.
When evaluating the application of the lexical extraction system on Penn-III, we
carried out two sets of experiments, identical in each case to those described for Penn-II
in Section 6.3, including the use of relative (1% and 5%) rather than absolute thresholds.
For the first set of experiments we evaluated the lexicon induced from the parse-
annotated Brown corpus only. This evaluation was performed for 2,713 active-verb
lemmas using the more fine-grained Mapping-II. Tables 22 and 23 show that the results
generally exceed the baseline, in some cases by almost 10%, similar to those recorded
for Penn-II (Tables 17 and 18). While the precision is slightly lower than that re-
ported for the experiments in Tables 17 and 18, in particular for Experiments 2, 2a, 3,
Table 21
Penn-II evaluation of active frames against COMLEX using absolute thresholds (Experiment 2).
Threshold Precision Recall F-score
All 77.1% 50.4% 61.0%
Threshold 100 82.3% 60.8% 69.9%
Threshold 200 81.7% 58.7% 68.4%
354
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 22
Results of Penn-III active frames (Brown Corpus only) COMLEX comparison (relative threshold
of 1%).
Precision Recall F-Score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 73.2% 79.2% 60.1% 60.0% 66.0% 68.2%
Experiment 2 66.0% 70.5% 37.5% 50.5% 47.8% 58.9%
Experiment 2a 66.0% 71.3% 32.7% 44.5% 43.7% 54.8%
Experiment 3 66.0% 64.3% 15.2% 23.1% 24.8% 34.0%
Experiment 3a 66.0% 64.1% 13.5% 20.7% 22.4% 31.3%
and 3a, in which details of obliques are included, the recall in each of these experi-
ments is slightly higher than that recorded for Penn-II. We conjecture that the main
reason for this is that the amended approach to the annotation of obliques is slightly
less precise and conservative than the largely -CLR-tag-driven approach taken for
Penn-II. Consequently we record an increase in recall and a drop in precision. This
trend is repeated in the second set of experiments. In this instance, we combined the
lexicon extracted from the WSJ with that extracted from the parse-annotated Brown
corpus, and evaluated the resulting resource for 3,529 active-verb lemmas. The results
are shown in Tables 24 and 25. The results compare very positively against the baseline.
The precision scores are lower (by between 1.5% and 9.7%) than those reported for
Penn-II (Tables 17 and 18). There has however been a significant increase in recall (up to
8.7%) and an overall increase in F-score (by up to 4.4%).
6.5 Error Analysis and Discussion
The work presented in this section highlights a number of issues associated with the
evaluation of automatically induced subcategorization frames against an existing exter-
nal gold standard, in this case COMLEX. While this evaluation approach is arguably
less labor-intensive than the manual construction of a custom-made gold standard,
it does introduce a number of difficulties into the evaluation procedure. It is a
nontrivial task to convert both the gold standard and the induced resource to a common
Table 23
Results of Penn-III active frames (Brown corpus only) COMLEX comparison (relative threshold
of 5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 73.2% 82.7% 60.1% 56.4% 66.0% 67.0%
Experiment 2 66.0% 74.6% 37.5% 46.1% 47.8% 57.0%
Experiment 2a 66.0% 76.0% 32.7% 40.0% 43.7% 52.4%
Experiment 3 66.0% 69.2% 15.2% 18.7% 24.8% 29.5%
Experiment 3a 66.0% 69.0% 13.5% 16.6% 22.4% 26.7%
355
Computational Linguistics Volume 31, Number 3
Table 24
Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of
1%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 71.2% 77.4% 62.9% 66.2% 66.8% 71.4%
Experiment 2 64.5% 70.4% 40.0% 58.0% 49.3% 63.6%
Experiment 2a 64.5% 71.5% 35.1% 51.9% 45.5% 60.2%
Experiment 3 64.5% 66.2% 17.0% 27.4% 26.8% 38.8%
Experiment 3a 64.5% 66.0% 15.1% 24.8% 24.5% 36.0%
format in order to facilitate evaluation. In addition, as our results show, the choice
of common format and mapping to it can affect the results. In COMLEX-LFG Map-
ping I (Section 6.2), we found that mapping from the induced lexicon to COMLEX
resulted in higher recall scores than those achieved when we (effectively) reversed the
mapping (COMLEX-LFG Mapping II [Section 6.3]). The first mapping is essentially a
conflation of our more fine-grained LFG grammatical functions with the more generic
COMLEX functions, while the second mapping tries to maintain as many distinctions
as possible.
Another drawback to using an existing external gold standard such as COMLEX
to evaluate an automatically induced subcategorization lexicon is that the resources
are not necessarily constructed from the same source data. As noted above, it is well doc-
umented (Roland and Jurafsky 1998) that subcategorization frames (and their frequen-
cies) vary across domains. We have extracted frames from two sources (the WSJ and the
Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury
News, the Brown corpus, several literary works from the Library of America, scientific
abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely
to contain a greater variety of subcategorization frames than our induced lexicon. It is
also possible that because of human error, COMLEX contains subcategorization frames
the validity of which are in doubt, for example, the overgeneration of subcategorized-for
directional prepositional phrases. This is because the aim of the COMLEX project was to
construct as complete a set of subcategorization frames as possible, even for infrequent
verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure
Table 25
Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of
5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 71.2% 82.0% 62.9% 61.0% 66.8% 69.9%
Experiment 2 64.5% 74.3% 40.0% 53.5% 49.3% 62.2%
Experiment 2a 64.5% 76.4% 35.1% 45.1% 45.5% 56.7%
Experiment 3 64.5% 71.1% 17.0% 21.5% 26.8% 33.0%
Experiment 3a 64.5% 70.8% 15.1% 19.2% 24.5% 30.2%
356
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
which is bound to be less certain than the assignment of frames based entirely on exist-
ing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX
tend to demonstrate high precision but low recall. Briscoe and Carroll (1997) report
on manually analyzing an open-class vocabulary of 35,000 head words for predicate
subcategorization information and comparing the results against the subcategorization
details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has
an effect on both the precision and recall scores of our system against COMLEX. In order
to ascertain the effect of using COMLEX as a gold standard for our induced lexicon,
we carried out some more-detailed error analysis, the results of which are summarized
in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp)
across a range of active frame types containing prepositional and particle detail taken
from Penn-III and manually examined them in order to classify them as ?correct? or
?incorrect.? Of the 80 fps, 33 were manually judged to be legitimate subcategorization
frames. For example, as Table 26 shows, there are a number of correct transitive verbs
([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.
This examination was also useful in highlighting to us the frame types on which
the lexical extraction procedure was performing poorly, in our case, those containing
XCOMPs and those containing OBJ2S. Out of 80 fns, 14 were judged to be incorrect when
manually examined. These can be broken down as follows: one intransitive frame, three
ditransitive frames, three frames containing a COMP, and seven frames containing an
oblique were found to be invalid.
7. Lexical Accession Rates
In addition to evaluating the quality of our extracted semantic forms, we also examined
the rate at which they are induced. This can be expressed as a measure of the coverage
of the induced lexicon on new data. Following Hockenmaier, Bierner, and Baldridge
(2002), Xia (1999), and Miyao, Ninomiya, and Tsujii (2004), we extract a reference
lexicon from Sections 02?21 of the WSJ. We then compare this to a test lexicon from
Section 23. Table 27 shows the results of the evaluation of the coverage of an induced
lexicon for verbs only. There is a corresponding semantic form in the reference lexicon
for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not
appear in the reference lexicon. Within this group, we can distinguish between known
words, which have an entry in the reference lexicon, and unknown words, which do
not exist at all in the reference lexicon. In the same way we make the distinction
Table 26
Error analysis.
Frame type COMLEX: False negatives Induced: False positives
Correct Incorrect Correct Incorrect
[subj] 9 1 4 6
[subj, obj] 10 0 9 1
[subj, obj, obj2] 7 3 1 9
[.., xcomp, ..] 10 0 1 10
[.., comp, ..] 7 3 4 5
[.., obl, ..] 23 7 14 16
357
Computational Linguistics Volume 31, Number 3
Table 27
Coverage of induced lexicon (WSJ 02?21) on unseen data (WSJ 23) (verbs only).
Entries also in reference lexicon 89.89%
Entries not in reference lexicon 10.11%
Known words 7.85%
Known words, known frames 7.85%
Known words, unknown frames 0
Unknown words 2.32%
Unknown words, known frames 2.32%
Unknown words, unknown frames 0
between known frames and unknown frames. There are, therefore, four different cases
in which an entry may not appear in the reference lexicon. Table 27 shows that the
most common case is that of known verbs occurring with a different, although known,
subcategorization frame (7.85%).
The rate of accession may also be represented graphically. In Charniak (1996) and
Krotov et al (1998), it was observed that treebank grammars (CFGs extracted from
treebanks) are very large and grow with the size of the treebank. We were interested in
discovering whether the acquisition of lexical material from the same data displayed a
similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule
types from Penn-III (the WSJ and parse-annotated Brown corpus combined). Because
of the variation in the size of sections between the Brown and the WSJ, we plotted
accession against word count. The first part of the graph (up to 1,004,414 words)
Figure 8
Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty
frames) (WSJ followed by Brown).
358
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
represents the rate of accession from the WSJ, and the final 384,646 words are those
of the Brown corpus. The seven curves represent the following: The acquisition of
semantic form types (nonempty) for all syntactic categories with and without specific
preposition and particle information, the acquisition of semantic form types (non-
empty) for all verbs with and without specific preposition and particle information,
the number of lemmas associated with the extract semantic forms, and the acqui-
sition of CFG rule types. The curve representing the growth in the overall size of
the lexicon is similar in shape to that of the PCFG, while the rate of increase in
the number of verbal semantic forms (particularly when obliques and particles are
excluded) appears to slow more quickly. Figure 8 shows the effect of domain di-
versity from the Brown section in terms of increased growth rates for 1e+06 words
upward. Figure 9 depicts the same information, this time extracted from the Brown
section first followed by the WSJ. The curves are different, but similar trends are
represented. This time the effects of domain diversity for the Brown section are
discernible by comparing the absolute accession rate for the 0.4e+06 mark between
Figures 8 and 9.
Figure 10 shows the result when we abstract away from semantic forms (verb
frame combinations) to subcategorization frames and plot their rate of acces-
sion. The graph represents the growth rate of frame types for Penn-III (WSJ fol-
lowed by Brown and Brown followed by WSJ). The curve rises sharply initially
but gradually levels, practically flattening out, despite the increase in the number
of words. This reflects the information about Section 23 in Table 27, where we demon-
strate that although new verb frame combinations occur, all of the frame types in
Section 23 have been seen by the lexical extraction program in previous sections.
Figure 9
Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty
frames) (Brown followed by WSJ).
359
Computational Linguistics Volume 31, Number 3
Figure 10
Accession rates for frame types (without prepositions and particles) for Penn-III.
Figure 11 shows that including information about prepositions and particles in the
frames results in an accession rate which continues to grow, albeit ever more slowly,
with the increase in size of the extraction data. This emphasizes the advantage of our
approach, which extracts frames containing such information without the limitation
of predefinition.
Figure 11
Accession rates for frame types for Penn-III.
360
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
8. Conclusions and Further Work
We have presented an algorithm for the extraction of semantic forms (or subcatego-
rization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with
LFG f-structures. In contrast to many other approaches, ours does not predefine the sub-
categorization frames we extract. We have applied the algorithm to the WSJ sections of
Penn-II (50,000 trees) (O?Donovan et al 2004) and to the parse-annotated Brown corpus
of Penn-III (almost 25,000 additional trees). We extract syntactic-function-based subcat-
egorization frames (LFG semantic forms) and traditional CFG category-based frames, as
well as mixed-function-category-based frames. Unlike many other approaches to sub-
categorization frame extraction, our system properly reflects the effects of long-distance
dependencies. Also unlike many approaches, our method distinguishes between active
and passive frames. Finally, our system associates conditional probabilities with the
frames we extract. Making the distinction between the behavior of verbs in active and
passive contexts is particularly important for the accurate assignment of probabilities
to semantic forms. We carried out an extensive evaluation of the complete induced
lexicon against the full COMLEX resource. To our knowledge, this is the most extensive
qualitative evaluation of subcategorization extraction in English. The only evaluation of
a similar scale is that carried out by Schulte im Walde (2002b) for German. The results
reported here for Penn-II compare favorably against the baseline and, in fact, are an
improvement on those reported in O?Donovan et al (2004). The results for the larger,
more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15%
above the baseline. We believe our semantic forms are fine-grained, and by choosing
to evaluate against COMLEX, we set our sights high: COMLEX is considerably more
detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis
also revealed some interesting issues associated with using an external standard such as
COMLEX. In the future, we hope to evaluate the automatic annotations and extracted
lexicon against Propbank (Kingsbury and Palmer 2002).
Apart from the related approach of Miyao, Ninomiya, and Tsujii (2004), which
does not distinguish between argument and adjunct prepositional phrases, our
treebank and automatic f-structure annotation-based architecture for the automatic
acquisition of detailed subcategorization frames is quite unlike any of the architec-
tures presented in the literature. Subcategorization frames are reverse-engineered and
almost a byproduct of the automatic f-structure annotation algorithm. It is important
to realize that the induction of lexical resources is part of a larger project on the
acquisition of wide-coverage, robust, probabilistic, deep unification grammar resources
from treebanks Burke, Cahill, et al (2004b). We are already using the extracted seman-
tic forms in parsing new text with robust, wide-coverage probabilistic LFG grammar
approximations automatically acquired from the f-structure-annotated Penn-II tree-
bank, specifically in the resolution of LDDs, as described in Cahill, Burke, et al (2004).
We hope to be able to apply our lexical acquisition methodology beyond existing
parse-annotated corpora (Penn-II and Penn-III): New text is parsed by our probabilistic
LFG approximations into f-structures from which we can then extract further seman-
tic forms. The work reported here is part of the core components for bootstrapping
this approach.
In the shorter term, we intend to make the extracted subcategorization lexicons from
Penn-II and Penn-III available as a downloadable public-domain research resource.
We have also applied our more general unification grammar acquisition meth-
odology to the TIGER Treebank (Brants et al 2002) and Penn Chinese Treebank
(Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar
361
Computational Linguistics Volume 31, Number 3
approximations and lexical resources for German (Cahill et al 2003) and Chinese
(Burke, Lam, et al 2004). The lexical resources, however, have not yet been evaluated.
This, and much else, has to await further research.
Acknowledgments
The research reported here is partially
supported by Enterprise Ireland Basic
Research Grant SC/2001/186, an IRCSET
PhD fellowship award, and an IBM PhD
fellowship award. We are particularly
grateful to our anonymous reviewers, whose
insightful comments have helped to improve
this article considerably.
References
Ades, Anthony and Mark Steedman. 1982.
On the order of words. Linguistics and
Philosophy, 4(4):517? 558.
Boguraev, Branimir, Edward Briscoe,
John Carroll, David Carter, and
Claire Grover. 1987. The derivation of
a grammatically indexed lexicon from
the Longman Dictionary of Contemporary
English. In Proceedings of the 25th
Annual Meeting of the Association of
Computational Linguistics, pages 193?200,
Stanford, CA.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank. In
Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol, Bulgaria.
Brent, Michael. 1993. From grammar to
lexicon: Unsupervised learning of lexical
syntax. Computational Linguistics,
19(2):203?222.
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Briscoe, Edward. 2001. From dictionary to
corpus to self-organizing dictionary:
Learning valency associations in the face
of variation and change. In Proceedings of
Corpus Linguistics 2001, Lancaster, UK.
Briscoe, Edward and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
ACL Conference on Applied Natural
Language Processing, pages 356?363,
Washington, DC.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004a. Evaluation of an
automatic annotation algorithm against
the PARC 700 Dependency Bank. In
Proceedings of the Ninth International
Conference on LFG, pages 101?121,
Christchurch, New Zealand.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004b. Treebank-based
acquisition of wide-coverage, probabilistic
LFG resources: Project overview, results
and evaluation. In Proceedings of the
Workshop ?Beyond Shallow Analyses?
Formalisms and Statistical Modelling
for Deep Analyses? at the First International
Joint Conference on Natural Language
Processing (IJCNLP-04), Hainan
Island, China.
Burke, Michael, Olivia Lam, Rowena
Chan, Aoife Cahill, Ruth O?Donovan,
Adams Bodomo, Josef van Genabith,
and Andy Way. 2004. Treebank-based
acquisition of a Chinese lexical-functional
grammar. In Proceedings of the 18th
Pacific Asia Conference on Language,
Information and Computation,
pages 161?172, Tokyo.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings
of the 42nd Annual Meeting of the
Association of Computational Linguistics,
pages 320?327, Barcelona.
Cahill, Aoife, Martin Forst, Mairead
McCarthy, Ruth O?Donovan, Christian
Rohrer, Josef van Genabith, and Andy
Way. 2003. Treebank-based multilingual
unification-grammar development. In
Proceedings of the Workshop on Ideas and
Strategies for Multilingual Grammar
Development at the 15th ESS-LLI,
pages 17?24, Vienna.
Cahill, Aoife, Mairead McCarthy,
Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way.
2004. Evaluating automatic F-structure
annotation for the Penn-II Treebank.
Journal of Research on Language and
Computation, 2(4):523?547.
Cahill, Aoife, Mairead McCarthy, Josef van
Genabith, and Andy Way. 2002. Parsing
text with a PCFG derived from Penn-II
with an automatic F-structure annotation
procedure. In Proceedings of the Seventh
International Conference on LFG, edited by
Miriam Butt and Tracy Holloway King.
CSLI Publications, Stanford, CA,
pages 76?95.
362
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Carroll, Glenn and Mats Rooth. 1998. Valence
induction with a head-lexicalised PCFG. In
Proceedings of the Third Conference on
Empirical Methods in Natural Language
Processing, pages 36?45,
Granada, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In AAAI-96: Proceedings of the
Thirteenth National Conference on Artificial
Intelligence. MIT Press, Cambridge, MA,
pages 1031?1036.
Chen, John and K. Vijay-Shanker. 2000.
Automated extraction of TAGs from the
Penn Treebank. In Proceedings of the 38th
Annual Meeting of the Association of
Computational Linguistics, pages 65?76,
Hong Kong.
Collins, Michael. 1997. Three generative
lexicalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics,
pages 16?23, Madrid.
Crouch, Richard, Ron Kaplan, Tracy King,
and Stefan Riezler. 2002. A comparison
of evaluation metrics for a broad coverage
parser. In Proceedings of Workshop
?Beyond PARSEVAL? at Third International
Conference on Language Resources and
Evaluation, Las Palmas, Spain.
Dalrymple, Mary. 2001. Lexical Functional
Grammar. Volume 34 of Syntax and
Semantics. Academic Press, New York.
Dowty, David. 1982. Grammatical relations
and Montague grammar. In Pauline
Jacobson and Geoffrey Pullum, editors,
The Nature of Syntactic Representation.
Reidel, Dordrecht, The Netherlands,
pages 79?130.
Dudenredaktion, editor. 2001. DUDEN?Das
Stilworterbuch. [DUDEN?The Style
Dictionary]. Number 2 in Duden in zwo?lf
Banden [Duden in Twelve Volumes].
Dudenverlag, Mannheim, Germany.
Eckle, Judith. 1999. Linguistic Knowledge for
Automatic Lexicon Acquisition from German
Text Corpora. Ph.D. thesis, University of
Stuttgart, Germany.
Frank, Anette. 2000. Automatic F-structure
annotation of treebank trees. In Proceedings
of the Fifth International Conference on LFG,
Berkeley, CA, edited by Miriam Butt
and Tracy Holloway King. CSLI,
pages 139?160.
Grishman, Ralph, Catherine MacLeod, and
Adam Meyers. 1994. COMLEX syntax:
Building a computational lexicon. In
Proceedings of the 15th International
Conference on Computational Linguistics,
pages 268?272, Kyoto.
Hajic, Jan. 1998. Building a syntactically
annotated corpus: The Prague
Dependency Treebank. In Issues in Valency
and Meaning, edited by Eva Hajicova.
Karolinum, Prague, Czech Republic,
pages 106?132.
Hindle, Donald and Mats Rooth. 1993.
Ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. 2004. Extending the coverage of
a CCG system. Journal of Language and
Computation, 2(2):165?208.
Hornby, Albert, editor. 1980. Oxford Advanced
Learner?s Dictionary of Current English.
Oxford University Press, Oxford, UK.
Joshi, Aravind. 1988. Tree adjoining
grammars. In David Dowty, Lauri
Karttunen, and Arnold Zwicky, editors,
Natural Language Parsing. Cambridge
University Press, Cambridge,
pages 206?250.
Kaplan, Ronald and Joan Bresnan. 1982.
Lexical functional grammar: A formal
system for grammatical representation. In
Joan Bresnan, editor, The Mental
Representation of Grammatical Relations. MIT
Press, Cambridge, MA, pages 173?281.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and
Ronald Kaplan. 2003. The PARC 700
Dependency Bank. In Proceedings of the
Fourth International Workshop on
Linguistically Interpreted Corpora, Budapest.
Kingsbury, Paul and Martha Palmer. 2002.
From Treebank to PropBank. In Proceedings
of the Third International Conference on
Language Resources and Evaluation
(LREC-2002), Las Palmas, Spain.
Kinyon, Alexandra and Carlos Prolo. 2002.
Identifying verb arguments and their
syntactic function in the Penn Treebank. In
Proceedings of the Third LREC Conference,
pages 1982?1987, Las Palmas, Spain.
Korhonen, Anna. 2002. Subcategorization
acquisition. As Technical Report
UCAM-CL-TR-530, Computer Laboratory,
University of Cambridge, UK.
Krotov, Alexander, Mark Hepple, Robert
Gaizauskas, and Yorick Wilks. 1998.
Compacting the Penn Treebank grammar.
In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 669?703,
Montreal.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago.
363
Computational Linguistics Volume 31, Number 3
MacLeod, Catherine, Ralph Grishman, and
Adam Meyers. 1994. The Comlex Syntax
Project: The first year. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 669?703, Princeton.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Stanford University,
Stanford, CA.
Magerman, David. 1995. Statistical decision
tree models for parsing. In Proceedings of
the 33rd Annual Meeting for the Association
of Computational Linguistics, pages 276?283,
Cambridge, MA.
Manning, Christopher. 1993. Automatic
acquisition of a large subcategorisation
dictionary from corpora. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 235?242,
Columbus, OH.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Mark
Ferguson, Karen Katz, and Britta
Schasberger. 1994. The Penn Treebank:
Annotating predicate argument structure.
In Proceedings of the ARPA Human Language
Technology Workshop, Princeton.
Marinov, Svetoslav and Cecilia Hemming.
2004. Automatic Extraction of
Subcategorization Frames from the
Bulgarian Tree Bank. Unpublished
manuscript, Graduate School of Language
Technology, Go?teborg, Sweden.
Meyers, Adam, Catherine MacLeod, and
Ralph Grishman. 1996. Standardization of
the complement/adjunct distinction.
In Proceedings of the Seventh
EURALEX International Conference,
Go?teborg, Sweden.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2004. Corpus-oriented
grammar development for acquiring a
head-driven phrase structure grammar
from the Penn Treebank. In Proceedings of
the First International Joint Conference on
Natural Language Processing (IJCNLP-04),
pages 390?398, Hainan Island, China.
Nakanishi, Hiroko, Yusuke Miyao, and
Jun?ichi Tsujii. 2004. Using inverse
lexical rules to acquire a wide-coverage
lexicalized grammar. In Proceedings
of the Workshop ?Beyond Shallow
Analyses?Formalisms and Statistical
Modelling for Deep Analyses? at the First
International Joint Conference on Natural
Language Processing (IJCNLP-04), Hainan
Island, China.
O?Donovan, Ruth, Michael Burke,
Aoife Cahill, Josef van Genabith, and
Andy Way. 2004. Large-scale induction
and evaluation of lexical resources from
the Penn-II Treebank. In Proceedings
of the 42nd Annual Meeting of the
Association of Computational Linguistics,
pages 368?375, Barcelona.
Pollard, Carl and Ivan Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Proctor, Paul, editor. 1978. Longman
Dictionary of Contemporary English.
Longman, London.
Roland, Douglas and Daniel Jurafsky.
1998. How verb subcategorization
frequencies are affected by corpus
choice. In Proceedings of the 36th
Annual Meeting of the Association
for Computational Linguistics and
17th International Conference on
Computational Linguistics,
pages 1117?1121, Montreal.
Sadler, Louisa, Josef van Genabith,
and Andy Way. 2000. Automatic
F-structure annotation from the
AP Treebank. In Proceedings of the
Fifth International Conference on LFG,
Berkeley, CA, edited by Miriam
Butt and Tracy Holloway King. CSLI,
pages 226?243.
Sarkar, Anoop and Daniel Zeman. 2000.
Automatic extraction of subcategorization
frames for Czech. In Proceedings of the 19th
International Conference on Computational
Linguistics, pages 691?697, Saarbru?cken,
Germany.
Schulte im Walde, Sabine. 2002a. A
subcategorisation lexicon for German
verbs induced from a lexicalised PCFG. In
Proceedings of the Third LREC Conference,
pages 1351?1357, Las Palmas, Spain.
Schulte im Walde, Sabine. 2002b. Evaluating
verb subcategorisation frames learned by a
German statistical grammar against
manual definitions in the Duden
Dictionary. In Proceedings of the 10th
EURALEX International Congress,
pages 187?197, Copenhagen.
Simov, Kiril, Gergana Popova, and Petya
Osenova. 2002. HPSG-based syntactic
treebank of Bulgarian (BulTreeBank). In
Andrew Wilson, Paul Rayson, and Tony
McEnery, editors, A Rainbow of Corpora:
Corpus Linguistics and the Languages of the
World. Lincon-Europa, Munich,
pages 135?142.
Ushioda, Akira, David Evans, Ted Gibson,
and Alex Waibel. 1993. The Automatic
acquisition of frequencies of verb
subcategorization frames from tagged
364
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
corpora. In SIGLEX ACL Workshop on the
Acquisition of Lexical Knowledge from Text,
pages 95?106, Columbus, OH.
van Genabith, Josef, Louisa Sadler, and
Andy Way. 1999. Data-driven compilation
of LFG semantic forms. In EACL-99
Workshop on Linguistically Interpreted
Corpora (LINC-99), pages 69?76, Bergen,
Norway.
van Genabith, Josef, Andy Way, and Louisa
Sadler. 1999. Semi-automatic generation of
F-structures from Treebanks. In
Proceedings of the Fourth International
Conference on Lexical-Functional Grammar,
Manchester, UK. Available at
http://cslipublications.stanford.edu/.
Wauschkuhn, Oliver. 1999. Automatische
Extraktion von Verbvalenzen aus deutschen
Textkorpora [Automatic Extraction of Verb
Valence from German Text Corpora]. PhD
thesis, University of Stuttgart, Germany.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora.
In Fifth Natural Language Processing
Pacific Rim Symposium (NLPRS-99),
Beijing, China.
Xue, Nianwen, Fu-Dong Chiou, and Martha
Palmer. 2002. Building a large-scale
annotated Chinese corpus. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
Taipei, Taiwan.
365

Long-Distance Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van Genabith, Andy Way
National Centre for Language Technology and School of Computing,
Dublin City University, Dublin, Ireland
{acahill,mburke,rodonovan,josef,away}@computing.dcu.ie
Abstract
This paper shows how finite approximations of
long distance dependency (LDD) resolution can be
obtained automatically for wide-coverage, robust,
probabilistic Lexical-Functional Grammar (LFG)
resources acquired from treebanks. We extract LFG
subcategorisation frames and paths linking LDD
reentrancies from f-structures generated automati-
cally for the Penn-II treebank trees and use them
in an LDD resolution algorithm to parse new text.
Unlike (Collins, 1999; Johnson, 2002), in our ap-
proach resolution of LDDs is done at f-structure
(attribute-value structure representations of basic
predicate-argument or dependency structure) with-
out empty productions, traces and coindexation in
CFG parse trees. Currently our best automatically
induced grammars achieve 80.97% f-score for f-
structures parsing section 23 of the WSJ part of the
Penn-II treebank and evaluating against the DCU
1051 and 80.24% against the PARC 700 Depen-
dency Bank (King et al, 2003), performing at the
same or a slightly better level than state-of-the-art
hand-crafted grammars (Kaplan et al, 2004).
1 Introduction
The determination of syntactic structure is an im-
portant step in natural language processing as syn-
tactic structure strongly determines semantic inter-
pretation in the form of predicate-argument struc-
ture, dependency relations or logical form. For a
substantial number of linguistic phenomena such
as topicalisation, wh-movement in relative clauses
and interrogative sentences, however, there is an im-
portant difference between the location of the (sur-
face) realisation of linguistic material and the loca-
tion where this material should be interpreted se-
mantically. Resolution of such long-distance de-
pendencies (LDDs) is therefore crucial in the de-
termination of accurate predicate-argument struc-
1Manually constructed f-structures for 105 randomly se-
lected trees from Section 23 of the WSJ section of the Penn-II
Treebank
ture, deep dependency relations and the construc-
tion of proper meaning representations such as log-
ical forms (Johnson, 2002).
Modern unification/constraint-based grammars
such as LFG or HPSG capture deep linguistic infor-
mation including LDDs, predicate-argument struc-
ture, or logical form. Manually scaling rich uni-
fication grammars to naturally occurring free text,
however, is extremely time-consuming, expensive
and requires considerable linguistic and computa-
tional expertise. Few hand-crafted, deep unification
grammars have in fact achieved the coverage and
robustness required to parse a corpus of say the size
and complexity of the Penn treebank: (Riezler et
al., 2002) show how a deep, carefully hand-crafted
LFG is successfully scaled to parse the Penn-II tree-
bank (Marcus et al, 1994) with discriminative (log-
linear) parameter estimation techniques.
The last 20 years have seen continuously increas-
ing efforts in the construction of parse-annotated
corpora. Substantial treebanks2 are now available
for many languages (including English, Japanese,
Chinese, German, French, Czech, Turkish), others
are currently under construction (Arabic, Bulgarian)
or near completion (Spanish, Catalan). Treebanks
have been enormously influential in the develop-
ment of robust, state-of-the-art parsing technology:
grammars (or grammatical information) automat-
ically extracted from treebank resources provide
the backbone of many state-of-the-art probabilis-
tic parsing approaches (Charniak, 1996; Collins,
1999; Charniak, 1999; Hockenmaier, 2003; Klein
and Manning, 2003). Such approaches are attrac-
tive as they achieve robustness, coverage and per-
formance while incurring very low grammar devel-
opment cost. However, with few notable exceptions
(e.g. Collins? Model 3, (Johnson, 2002), (Hocken-
maier, 2003) ), treebank-based probabilistic parsers
return fairly simple ?surfacey? CFG trees, with-
out deep syntactic or semantic information. The
grammars used by such systems are sometimes re-
2Or dependency banks.
ferred to as ?half? (or ?shallow?) grammars (John-
son, 2002), i.e. they do not resolve LDDs but inter-
pret linguistic material purely locally where it oc-
curs in the tree.
Recently (Cahill et al, 2002) showed how
wide-coverage, probabilistic unification grammar
resources can be acquired automatically from f-
structure-annotated treebanks. Many second gen-
eration treebanks provide a certain amount of
deep syntactic or dependency information (e.g. in
the form of Penn-II functional tags and traces)
supporting the computation of representations of
deep linguistic information. Exploiting this in-
formation (Cahill et al, 2002) implement an
automatic LFG f-structure annotation algorithm
that associates nodes in treebank trees with f-
structure annotations in the form of attribute-value
structure equations representing abstract predicate-
argument structure/dependency relations. From the
f-structure annotated treebank they automatically
extract wide-coverage, robust, PCFG-based LFG
approximations that parse new text into trees and
f-structure representations.
The LFG approximations of (Cahill et al, 2002),
however, are only ?half? grammars, i.e. like most
of their probabilistic CFG cousins (Charniak, 1996;
Johnson, 1999; Klein and Manning, 2003) they do
not resolve LDDs but interpret linguistic material
purely locally where it occurs in the tree.
In this paper we show how finite approxima-
tions of long distance dependency resolution can be
obtained automatically for wide-coverage, robust,
probabilistic LFG resources automatically acquired
from treebanks. We extract LFG subcategorisation
frames and paths linking LDD reentrancies from
f-structures generated automatically for the Penn-
II treebank trees and use them in an LDD resolu-
tion algorithm to parse new text. Unlike (Collins,
1999; Johnson, 2002), in our approach LDDs are
resolved on the level of f-structure representation,
rather than in terms of empty productions and co-
indexation on parse trees. Currently we achieve f-
structure/dependency f-scores of 80.24 and 80.97
for parsing section 23 of the WSJ part of the Penn-
II treebank, evaluating against the PARC 700 and
DCU 105 respectively.
The paper is structured as follows: we give a
brief introduction to LFG. We outline the automatic
f-structure annotation algorithm, PCFG-based LFG
grammar approximations and parsing architectures
of (Cahill et al, 2002). We present our subcategori-
sation frame extraction and introduce the treebank-
based acquisition of finite approximations of LFG
functional uncertainty equations in terms of LDD
paths. We present the f-structure LDD resolution
algorithm, provide results and extensive evaluation.
We compare our method with previous work. Fi-
nally, we conclude.
2 Lexical Functional Grammar (LFG)
Lexical-Functional Grammar (Kaplan and Bres-
nan, 1982; Dalrymple, 2001) minimally involves
two levels of syntactic representation:3 c-structure
and f-structure. C(onstituent)-structure represents
the grouping of words and phrases into larger
constituents and is realised in terms of a CF-
PSG grammar. F(unctional)-structure represents
abstract syntactic functions such as SUBJ(ect),
OBJ(ect), OBL(ique), closed and open clausal
COMP/XCOMP(lement), ADJ(unct), APP(osition)
etc. and is implemented in terms of recursive feature
structures (attribute-value matrices). C-structure
captures surface grammatical configurations, f-
structure encodes abstract syntactic information
approximating to predicate-argument/dependency
structure or simple logical form (van Genabith
and Crouch, 1996). C- and f-structures are re-
lated in terms of functional annotations (constraints,
attribute-value equations) on c-structure rules (cf.
Figure 1).
S
NP VP
U.N. V NP
signs treaty
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
S ? NP VP
?SUBJ=? ?=?
VP ? V NP
?=? ?OBJ=?
NP ? U.N V ? signs
?PRED=U.N. ?PRED=sign
Figure 1: Simple LFG C- and F-Structure
Uparrows point to the f-structure associated with the
mother node, downarrows to that of the local node.
The equations are collected with arrows instanti-
ated to unique tree node identifiers, and a constraint
solver generates an f-structure.
3 Automatic F-Structure Annotation
The Penn-II treebank employs CFG trees with addi-
tional ?functional? node annotations (such as -LOC,
-TMP, -SBJ, -LGS, . . . ) as well as traces and coin-
dexation (to indicate LDDs) as basic data structures.
The f-structure annotation algorithm of (Cahill et
3LFGs may also involve morphological and semantic levels
of representation.
al., 2002) exploits configurational, categorial, Penn-
II ?functional?, local head and trace information
to annotate nodes with LFG feature-structure equa-
tions. A slightly adapted version of (Magerman,
1994)?s scheme automatically head-lexicalises the
Penn-II trees. This partitions local subtrees of depth
one (corresponding to CFG rules) into left and right
contexts (relative to head). The annotation algo-
rithm is modular with four components (Figure 2):
left-right (L-R) annotation principles (e.g. leftmost
NP to right of V head of VP type rule is likely to be
an object etc.); coordination annotation principles
(separating these out simplifies other components
of the algorithm); traces (translates traces and coin-
dexation in trees into corresponding reentrancies in
f-structure ( 1 in Figure 3)); catch all and clean-up.
Lexical information is provided via macros for POS
tag classes.
L/R Context ? Coordination ? Traces ? Catch-All
Figure 2: Annotation Algorithm
The f-structure annotations are passed to a con-
straint solver to produce f-structures. Annotation
is evaluated in terms of coverage and quality, sum-
marised in Table 1. Coverage is near complete with
99.82% of the 48K Penn-II sentences receiving a
single, connected f-structure. Annotation quality is
measured in terms of precision and recall (P&R)
against the DCU 105. The algorithm achieves an
F-score of 96.57% for full f-structures and 94.3%
for preds-only f-structures.4
S
S-TPC- 1
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
S
T- 1
?
?
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
1
SUBJ
[
SPEC the
PRED headline
]
PRED say
COMP 1
?
?
?
?
?
?
?
Figure 3: Penn-II style tree with LDD trace and cor-
responding reentrancy in f-structure
4Full f-structures measure all attribute-value pairs includ-
ing?minor? features such as person, number etc. The stricter
preds-only captures only paths ending in PRED:VALUE.
# frags # sent percent
0 85 0.176
1 48337 99.820
2 2 0.004
all preds
P 96.52 94.45
R 96.63 94.16
Table 1: F-structure annotation results for DCU 105
4 PCFG-Based LFG Approximations
Based on these resources (Cahill et al, 2002) de-
veloped two parsing architectures. Both generate
PCFG-based approximations of LFG grammars.
In the pipeline architecture a standard PCFG is
extracted from the ?raw? treebank to parse unseen
text. The resulting parse-trees are then annotated by
the automatic f-structure annotation algorithm and
resolved into f-structures.
In the integrated architecture the treebank
is first annotated with f-structure equations.
An annotated PCFG is then extracted where
each non-terminal symbol in the grammar
has been augmented with LFG f-equations:
NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?] . Nodes
followed by annotations are treated as a monadic
category for grammar extraction and parsing.
Post-parsing, equations are collected from parse
trees and resolved into f-structures.
Both architectures parse raw text into ?proto? f-
structures with LDDs unresolved resulting in in-
complete argument structures as in Figure 4.
S
S
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
SUBJ
[
SPEC the
PRED headline
]
PRED say
?
?
?
?
?
Figure 4: Shallow-Parser Output with Unresolved
LDD and Incomplete Argument Structure (cf. Fig-
ure 3)
5 LDDs and LFG FU-Equations
Theoretically, LDDs can span unbounded amounts
of intervening linguistic material as in
[U.N. signs treaty]1 the paper claimed . . . a source said []1.
In LFG, LDDs are resolved at the f-structure level,
obviating the need for empty productions and traces
in trees (Dalrymple, 2001), using functional uncer-
tainty (FU) equations. FUs are regular expressions
specifying paths in f-structure between a source
(where linguistic material is encountered) and a tar-
get (where linguistic material is interpreted seman-
tically). To account for the fronted sentential con-
stituents in Figures 3 and 4, an FU equation of the
form ? TOPIC = ? COMP* COMP would be required.
The equation states that the value of the TOPIC at-
tribute is token identical with the value of the final
COMP argument along a path through the immedi-
ately enclosing f-structure along zero or more COMP
attributes. This FU equation is annotated to the top-
icalised sentential constituent in the relevant CFG
rules as follows
S ? S NP VP
?TOPIC=? ?SUBJ=? ?=?
?TOPIC=?COMP*COMP
and generates the LDD-resolved proper f-structure
in Figure 3 for the traceless tree in Figure 4, as re-
quired.
In addition to FU equations, subcategorisation in-
formation is a crucial ingredient in LFG?s account
of LDDs. As an example, for a topicalised con-
stituent to be resolved as the argument of a local
predicate as specified by the FU equation, the local
predicate must (i) subcategorise for the argument in
question and (ii) the argument in question must not
be already filled. Subcategorisation requirements
are provided lexically in terms of semantic forms
(subcat lists) and coherence and completeness con-
ditions (all GFs specified must be present, and no
others may be present) on f-structure representa-
tions. Semantic forms specify which grammatical
functions (GFs) a predicate requires locally. For our
example in Figures 3 and 4, the relevant lexical en-
tries are:
V ? said ?PRED=say?? SUBJ, ? COMP?
V ? signs ?PRED=sign?? SUBJ, ? OBJ?
FU equations and subcategorisation requirements
together ensure that LDDs can only be resolved at
suitable f-structure locations.
6 Acquiring Lexical and LDD Resources
In order to model the LFG account of LDD resolu-
tion we require subcat frames (i.e. semantic forms)
and LDD resolution paths through f-structure. Tra-
ditionally, such resources were handcoded. Here we
show how they can be acquired from f-structure an-
notated treebank resources.
LFG distinguishes between governable (argu-
ments) and nongovernable (adjuncts) grammati-
cal functions (GFs). If the automatic f-structure
annotation algorithm outlined in Section 3 gen-
erates high quality f-structures, reliable seman-
tic forms can be extracted (reverse-engineered):
for each f-structure generated, for each level of
embedding we determine the local PRED value
and collect the governable, i.e. subcategoris-
able grammatical functions present at that level
of embedding. For the proper f-structure in
Figure 3 we obtain sign([subj,obj]) and
say([subj,comp]). We extract frames from
the full WSJ section of the Penn-II Treebank with
48K trees. Unlike many other approaches, our ex-
traction process does not predefine frames, fully
reflects LDDs in the source data-structures (cf.
Figure 3), discriminates between active and pas-
sive frames, computes GF, GF:CFG category pair-
as well as CFG category-based subcategorisation
frames and associates conditional probabilities with
frames. Given a lemma l and an argument list s, the
probability of s given l is estimated as:
P(s|l) := count(l, s)?n
i=1 count(l, si)
Table 2 summarises the results. We extract 3586
verb lemmas and 10969 unique verbal semantic
form types (lemma followed by non-empty argu-
ment list). Including prepositions associated with
the subcategorised OBLs and particles, this number
goes up to 14348. The number of unique frame
types (without lemma) is 38 without specific prepo-
sitions and particles, 577 with. F-structure anno-
tations allow us to distinguish passive and active
frames. Table 3 shows the most frequent seman-
tic forms for accept. Passive frames are marked
p. We carried out a comprehensive evaluation of
the automatically acquired verbal semantic forms
against the COMLEX Resource (Macleod et al,
1994) for the 2992 active verb lemmas that both re-
sources have in common. We report on the evalu-
ation of GF-based frames for the full frames with
complete prepositional and particle infomation. We
use relative conditional probability thresholds (1%
and 5%) to filter the selection of semantic forms
(Table 4). (O?Donovan et al, 2004) provide a more
detailed description of the extraction and evaluation
of semantic forms.
Without Prep/Part With Prep/Part
Lemmas 3586 3586
Sem. Forms 10969 14348
Frame Types 38 577
Active Frame Types 38 548
Passive Frame Types 21 177
Table 2: Verb Results
Semantic Form Occurrences Prob.
accept([obj,subj]) 122 0.813
accept([subj],p) 9 0.060
accept([comp,subj]) 5 0.033
accept([subj,obl:as],p) 3 0.020
accept([obj,subj,obl:as]) 3 0.020
accept([obj,subj,obl:from]) 3 0.020
accept([subj]) 2 0.013
accept([obj,subj,obl:at]) 1 0.007
accept([obj,subj,obl:for]) 1 0.007
accept([obj,subj,xcomp]) 1 0.007
Table 3: Semantic forms for the verb accept.
Threshold 1% Threshold 5%
P R F-Score P R F-Score
Exp. 73.7% 22.1% 34.0% 78.0% 18.3% 29.6%
Table 4: COMLEX Comparison
We further acquire finite approximations of FU-
equations. by extracting paths between co-indexed
material occurring in the automatically generated f-
structures from sections 02-21 of the Penn-II tree-
bank. We extract 26 unique TOPIC, 60 TOPIC-REL
and 13 FOCUS path types (with a total of 14,911 to-
ken occurrences), each with an associated probabil-
ity. We distinguish between two types of TOPIC-
REL paths, those that occur in wh-less constructions,
and all other types (c.f Table 5). Given a path p and
an LDD type t (either TOPIC, TOPIC-REL or FO-
CUS), the probability of p given t is estimated as:
P(p|t) := count(t, p)?n
i=1 count(t, pi)
In order to get a first measure of how well the ap-
proximation models the data, we compute the path
types in section 23 not covered by those extracted
from 02-21: 23/(02-21). There are 3 such path types
(Table 6), each occuring exactly once. Given that
the total number of path tokens in section 23 is 949,
the finite approximation extracted from 02-23 cov-
ers 99.69% of all LDD paths in section 23.
7 Resolving LDDs in F-Structure
Given a set of semantic forms s with probabilities
P(s|l) (where l is a lemma), a set of paths p with
P(p|t) (where t is either TOPIC, TOPIC-REL or FO-
CUS) and an f-structure f , the core of the algorithm
to resolve LDDs recursively traverses f to:
find TOPIC|TOPIC-REL|FOCUS:g pair; retrieve
TOPIC|TOPIC-REL|FOCUS paths; for each path p
with GF1 : . . . : GFn : GF, traverse f along GF1 : . . . :
GFn to sub-f-structure h; retrieve local PRED:l;
add GF:g to h iff
? GF is not present at h
wh-less TOPIC-REL # wh-less TOPIC-REL #
subj 5692 adjunct 1314
xcomp:adjunct 610 obj 364
xcomp:obj 291 xcomp:xcomp:adjunct 96
comp:subj 76 xcomp:subj 67
Table 5: Most frequent wh-less TOPIC-REL paths
02?21 23 23 /(02?21)
TOPIC 26 7 2
FOCUS 13 4 0
TOPIC-REL 60 22 1
Table 6: Number of path types extracted
? h together with GF is locally complete and co-
herent with respect to a semantic form s for l
rank resolution by P(s|l) ? P(p|t)
The algorithm supports multiple, interacting TOPIC,
TOPIC-REL and FOCUS LDDs. We use P(s|l) ?
P(p|t) to rank a solution, depending on how likely
the PRED takes semantic frame s, and how likely
the TOPIC, FOCUS or TOPIC-REL is resolved using
path p. The algorithm also supports resolution of
LDDs where no overt linguistic material introduces
a source TOPIC-REL function (e.g. in reduced rela-
tive clause constructions). We distinguish between
passive and active constructions, using the relevant
semantic frame type when resolving LDDs.
8 Experiments and Evaluation
We ran experiments with grammars in both the
pipeline and the integrated parsing architectures.
The first grammar is a basic PCFG, while A-PCFG
includes the f-structure annotations. We apply a
parent transformation to each grammar (Johnson,
1999) to give P-PCFG and PA-PCFG. We train
on sections 02-21 (grammar, lexical extraction and
LDD paths) of the Penn-II Treebank and test on sec-
tion 23. The only pre-processing of the trees that we
do is to remove empty nodes, and remove all Penn-
II functional tags in the integrated model. We evalu-
ate the parse trees using evalb. Following (Riezler et
al., 2002), we convert f-structures into dependency
triple format. Using their software we evaluate the
f-structure parser output against:
1. The DCU 105 (Cahill et al, 2002)
2. The full 2,416 f-structures automatically gen-
erated by the f-structure annotation algorithm
for the original Penn-II trees, in a CCG-style
(Hockenmaier, 2003) evaluation experiment
Pipeline Integrated
PCFG P-PCFG A-PCFG PA-PCFG
2416 Section 23 trees
# Parses 2416 2416 2416 2414
Lab. F-Score 75.83 80.80 79.17 81.32
Unlab. F-Score 78.28 82.70 81.49 83.28
DCU 105 F-Strs
All GFs F-Score (before LDD resolution) 79.82 79.24 81.12 81.20
All GFs F-Score (after LDD resolution) 83.79 84.59 86.30 87.04
Preds only F-Score (before LDD resolution) 70.00 71.57 73.45 74.61
Preds only F-Score (after LDD resolution) 73.78 77.43 78.76 80.97
2416 F-Strs
All GFs F-Score (before LDD resolution) 81.98 81.49 83.32 82.78
All GFs F-Score (after LDD resolution) 84.16 84.37 86.45 86.00
Preds only F-Score (before LDD resolution) 72.00 73.23 75.22 75.10
Preds only F-Score (after LDD resolution) 74.07 76.12 78.36 78.40
PARC 700 Dependency Bank
Subset of GFs following (Kaplan et al, 2004) 77.86 80.24 77.68 78.60
Table 7: Parser Evaluation
3. A subset of 560 dependency structures of the
PARC 700 Dependency Bank following (Ka-
plan et al, 2004)
The results are given in Table 7. The parent-
transformed grammars perform best in both archi-
tectures. In all cases, there is a marked improve-
ment (2.07-6.36%) in the f-structures after LDD res-
olution. We achieve between 73.78% and 80.97%
preds-only and 83.79% to 87.04% all GFs f-score,
depending on gold-standard. We achieve between
77.68% and 80.24% against the PARC 700 follow-
ing the experiments in (Kaplan et al, 2004). For
details on how we map the f-structures produced
by our parsers to a format similar to that of the
PARC 700 Dependency Bank, see (Burke et al,
2004). Table 8 shows the evaluation result broken
down by individual GF (preds-only) for the inte-
grated model PA-PCFG against the DCU 105. In
order to measure how many of the LDD reentran-
cies in the gold-standard f-structures are captured
correctly by our parsers, we developed evaluation
software for f-structure LDD reentrancies (similar
to Johnson?s (2002) evaluation to capture traces and
their antecedents in trees). Table 9 shows the results
with the integrated model achieving more than 76%
correct LDD reentrancies.
9 Related Work
(Collins, 1999)?s Model 3 is limited to wh-traces
in relative clauses (it doesn?t treat topicalisation,
focus etc.). Johnson?s (2002) work is closest to
ours in spirit. Like our approach he provides a fi-
nite approximation of LDDs. Unlike our approach,
however, he works with tree fragments in a post-
processing approach to add empty nodes and their
DEP. PRECISION RECALL F-SCORE
adjunct 717/903 = 79 717/947 = 76 78
app 14/15 = 93 14/19 = 74 82
comp 35/43 = 81 35/65 = 54 65
coord 109/143 = 76 109/161 = 68 72
det 253/264 = 96 253/269 = 94 95
focus 1/1 = 100 1/1 = 100 100
obj 387/445 = 87 387/461 = 84 85
obj2 0/1 = 0 0/2 = 0 0
obl 27/56 = 48 27/61 = 44 46
obl2 1/3 = 33 1/2 = 50 40
obl ag 5/11 = 45 5/12 = 42 43
poss 69/73 = 95 69/81 = 85 90
quant 40/55 = 73 40/52 = 77 75
relmod 26/38 = 68 26/50 = 52 59
subj 330/361 = 91 330/414 = 80 85
topic 12/12 = 100 12/13 = 92 96
topicrel 35/42 = 83 35/52 = 67 74
xcomp 139/160 = 87 139/146 = 95 91
OVERALL 83.78 78.35 80.97
Table 8: Preds-only results of PA-PCFG against the
DCU 105
antecedents to parse trees, while we present an ap-
proach to LDD resolution on the level of f-structure.
It seems that the f-structure-based approach is more
abstract (99 LDD path types against approximately
9,000 tree-fragment types in (Johnson, 2002)) and
fine-grained in its use of lexical information (sub-
cat frames). In contrast to Johnson?s approach, our
LDD resolution algorithm is not biased. It com-
putes all possible complete resolutions and order-
ranks them using LDD path and subcat frame prob-
abilities. It is difficult to provide a satisfactory com-
parison between the two methods, but we have car-
ried out an experiment that compares them at the
f-structure level. We take the output of Charniak?s
Pipeline Integrated
PCFG P-PCFG A-PCFG PA-PCFG
TOPIC
Precision (11/14) (12/13) (12/13) (12/12)
Recall (11/13) (12/13) (12/13) (12/13)
F-Score 0.81 0.92 0.92 0.96
FOCUS
Precision (0/1) (0/1) (0/1) (0/1)
Recall (0/1) (0/1) (0/1) (0/1)
F-Score 0 0 0 0
TOPIC-REL
Precision (20/34) (27/36) (34/42) (34/42)
Recall (20/52) (27/52) (34/52) (34/52)
F-Score 0.47 0.613 0.72 0.72
OVERALL 0.54 0.67 0.75 0.76
Table 9: LDD Evaluation on the DCU 105
Charniak -LDD res. +LDD res. (Johnson, 2002)
All GFs 80.86 86.65 85.16
Preds Only 74.63 80.97 79.75
Table 10: Comparison at f-structure level of LDD
resolution to (Johnson, 2002) on the DCU 105
parser (Charniak, 1999) and, using the pipeline
f-structure annotation model, evaluate against the
DCU 105, both before and after LDD resolution.
Using the software described in (Johnson, 2002) we
add empty nodes to the output of Charniak?s parser,
pass these trees to our automatic annotation algo-
rithm and evaluate against the DCU 105. The re-
sults are given in Table 10. Our method of resolv-
ing LDDs at f-structure level results in a preds-only
f-score of 80.97%. Using (Johnson, 2002)?s method
of adding empty nodes to the parse-trees results in
an f-score of 79.75%.
(Hockenmaier, 2003) provides CCG-based mod-
els of LDDs. Some of these involve extensive clean-
up of the underlying Penn-II treebank resource prior
to grammar extraction. In contrast, in our approach
we leave the treebank as is and only add (but never
correct) annotations. Earlier HPSG work (Tateisi
et al, 1998) is based on independently constructed
hand-crafted XTAG resources. In contrast, we ac-
quire our resources from treebanks and achieve sub-
stantially wider coverage.
Our approach provides wide-coverage, robust,
and ? with the addition of LDD resolution ? ?deep?
or ?full?, PCFG-based LFG approximations. Cru-
cially, we do not claim to provide fully adequate sta-
tistical models. It is well known (Abney, 1997) that
PCFG-type approximations to unification grammars
can yield inconsistent probability models due to
loss of probability mass: the parser successfully re-
turns the highest ranked parse tree but the constraint
solver cannot resolve the f-equations (generated in
the pipeline or ?hidden? in the integrated model)
and the probability mass associated with that tree is
lost. This case, however, is surprisingly rare for our
grammars: only 0.0018% (85 out of 48424) of the
original Penn-II trees (without FRAGs) fail to pro-
duce an f-structure due to inconsistent annotations
(Table 1), and for parsing section 23 with the in-
tegrated model (A-PCFG), only 9 sentences do not
receive a parse because no f-structure can be gen-
erated for the highest ranked tree (0.4%). Parsing
with the pipeline model, all sentences receive one
complete f-structure. Research on adequate prob-
ability models for unification grammars is impor-
tant. (Miyao et al, 2003) present a Penn-II tree-
bank based HPSG with log-linear probability mod-
els. They achieve coverage of 50.2% on section
23, as against 99% in our approach. (Riezler et
al., 2002; Kaplan et al, 2004) describe how a care-
fully hand-crafted LFG is scaled to the full Penn-II
treebank with log-linear based probability models.
They achieve 79% coverage (full parse) and 21%
fragement/skimmed parses. By the same measure,
full parse coverage is around 99% for our automat-
ically acquired PCFG-based LFG approximations.
Against the PARC 700, the hand-crafted LFG gram-
mar reported in (Kaplan et al, 2004) achieves an f-
score of 79.6%. For the same experiment, our best
automatically-induced grammar achieves an f-score
of 80.24%.
10 Conclusions
We presented and extensively evaluated a finite
approximation of LDD resolution in automati-
cally constructed, wide-coverage, robust, PCFG-
based LFG approximations, effectively turning the
?half?(or ?shallow?)-grammars presented in (Cahill
et al, 2002) into ?full? or ?deep? grammars. In
our approach, LDDs are resolved in f-structure, not
trees. The method achieves a preds-only f-score
of 80.97% for f-structures with the PA-PCFG in
the integrated architecture against the DCU 105
and 78.4% against the 2,416 automatically gener-
ated f-structures for the original Penn-II treebank
trees. Evaluating against the PARC 700 Depen-
dency Bank, the P-PCFG achieves an f-score of
80.24%, an overall improvement of approximately
0.6% on the result reported for the best hand-crafted
grammars in (Kaplan et al, 2004).
Acknowledgements
This research was funded by Enterprise Ireland Ba-
sic Research Grant SC/2001/186 and IRCSET.
References
S. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?
618.
M. Burke, A. Cahill, R. O?Donovan, J. van Gen-
abith, and A. Way 2004. The Evaluation of
an Automatic Annotation Algorithm against the
PARC 700 Dependency Bank. In Proceedings
of the Ninth International Conference on LFG,
Christchurch, New Zealand (to appear).
A. Cahill, M. McCarthy, J. van Genabith, and A.
Way. 2002. Parsing with PCFGs and Auto-
matic F-Structure Annotation. In Miriam Butt
and Tracy Holloway King, editors, Proceedings
of the Seventh International Conference on LFG,
pages 76?95. CSLI Publications, Stanford, CA.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2, pages 1031?1036.
E. Charniak. 1999. A Maximum-Entropy-Inspired
Parser. Technical Report CS-99-12, Brown Uni-
versity, Providence, RI.
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, PA.
M. Dalrymple. 2001. Lexical-Functional Gram-
mar. San Diego, CA; London Academic Press.
J. Hockenmaier. 2003. Parsing with Generative
models of Predicate-Argument Structure. In Pro-
ceedings of the 41st Annual Conference of the
Association for Computational Linguistics, pages
359?366, Sapporo, Japan.
M. Johnson. 1999. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
M. Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their
antecedents. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 136?143, Philadelphia, PA.
R. Kaplan and J. Bresnan. 1982. Lexical Func-
tional Grammar, a Formal System for Grammat-
ical Representation. In The Mental Representa-
tion of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
R. Kaplan, S. Riezler, T. H. King, J. T. Maxwell,
A. Vasserman, and R. Crouch. 2004. Speed and
accuracy in shallow and deep stochastic parsing.
In Proceedings of the Human Language Tech-
nology Conference and the 4th Annual Meeting
of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 97?
104, Boston, MA.
T.H. King, R. Crouch, S. Riezler, M. Dalrymple,
and R. Kaplan. 2003. The PARC700 dependency
bank. In Proceedings of the EACL03: 4th Inter-
national Workshop on Linguistically Interpreted
Corpora (LINC-03), pages 1?8, Budapest.
D. Klein and C. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL?02), pages 423?430, Sap-
poro, Japan.
C. Macleod, A. Meyers, and R. Grishman. 1994.
The COMLEX Syntax Project: The First Year.
In Proceedings of the ARPA Workshop on Human
Language Technology, pages 669-703, Princeton,
NJ.
D. Magerman. 1994. Natural Language Parsing as
Statistical Pattern Recognition. PhD thesis, Stan-
ford University, CA.
M. Marcus, G. Kim, M.A. Marcinkiewicz, R. Mac-
Intyre, A. Bies, M. Ferguson, K. Katz, and B.
Schasberger. 1994. The Penn Treebank: Anno-
tating Predicate Argument Structure. In Proceed-
ings of the ARPA Workshop on Human Language
Technology, pages 110?115, Princeton, NJ.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2003. Proba-
bilistic modeling of argument structures includ-
ing non-local dependencies. In Proceedings of
the Conference on Recent Advances in Natural
Language Processing (RANLP), pages 285?291,
Borovets, Bulgaria.
R. O?Donovan, M. Burke, A. Cahill, J. van Gen-
abith, and A. Way. 2004. Large-Scale Induc-
tion and Evaluation of Lexical Resources from
the Penn-II Treebank. In Proceedings of the 42nd
Annual Conference of the Association for Com-
putational Linguistics (ACL-04), Barcelona.
S. Riezler, T.H. King, R. Kaplan, R. Crouch,
J. T. Maxwell III, and M. Johnson. 2002. Pars-
ing the Wall Street Journal using a Lexical-
Functional Grammar and Discriminative Estima-
tion Techniques. In Proceedings of the 40th An-
nual Conference of the Association for Compu-
tational Linguistics (ACL-02), pages 271?278,
Philadelphia, PA.
Y. Tateisi, K. Torisawa, Y. Miyao, and J. Tsujii.
1998. Translating the XTAG English Grammar
to HPSG. In 4th International Workshop on Tree
Adjoining Grammars and Related Frameworks,
Philadelphia, PA, pages 172?175.
J. van Genabith and R. Crouch. 1996. Direct
and Underspecified Interpretations of LFG f-
Structures. In Proceedings of the 16th Interna-
tional Conference on Computational Linguistics
(COLING), pages 262?267, Copenhagen.
Large-Scale Induction and Evaluation of Lexical Resources from the
Penn-II Treebank
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef van Genabith, Andy Way
National Centre for Language Technology and School of Computing
Dublin City University
Glasnevin
Dublin 9
Ireland
{rodonovan,mburke,acahill,josef,away}@computing.dcu.ie
Abstract
In this paper we present a methodology for ex-
tracting subcategorisation frames based on an
automatic LFG f-structure annotation algorithm
for the Penn-II Treebank. We extract abstract
syntactic function-based subcategorisation frames
(LFG semantic forms), traditional CFG category-
based subcategorisation frames as well as mixed
function/category-based frames, with or without
preposition information for obliques and particle in-
formation for particle verbs. Our approach does
not predefine frames, associates probabilities with
frames conditional on the lemma, distinguishes be-
tween active and passive frames, and fully reflects
the effects of long-distance dependencies in the
source data structures. We extract 3586 verb lem-
mas, 14348 semantic form types (an average of 4
per lemma) with 577 frame types. We present a
large-scale evaluation of the complete set of forms
extracted against the full COMLEX resource.
1 Introduction
Lexical resources are crucial in the construction
of wide-coverage computational systems based on
modern syntactic theories (e.g. LFG, HPSG, CCG,
LTAG etc.). However, as manual construction of
such lexical resources is time-consuming, error-
prone, expensive and rarely ever complete, it is of-
ten the case that limitations of NLP systems based
on lexicalised approaches are due to bottlenecks in
the lexicon component.
Given this, research on automating lexical acqui-
sition for lexically-based NLP systems is a partic-
ularly important issue. In this paper we present an
approach to automating subcategorisation frame ac-
quisition for LFG (Kaplan and Bresnan, 1982) i.e.
grammatical function-based systems. LFG has two
levels of structural representation: c(onstituent)-
structure, and f(unctional)-structure. LFG differ-
entiates between governable (argument) and non-
governable (adjunct) grammatical functions. Sub-
categorisation requirements are enforced through
semantic forms specifying the governable grammat-
ical functions required by a particular predicate (e.g.
FOCUS?(? SUBJ)(? OBLon)?). Our approach is
based on earlier work on LFG semantic form extrac-
tion (van Genabith et al, 1999) and recent progress
in automatically annotating the Penn-II treebank
with LFG f-structures (Cahill et al, 2004b). De-
pending on the quality of the f-structures, reliable
LFG semantic forms can then be generated quite
simply by recursively reading off the subcategoris-
able grammatical functions for each local pred
value at each level of embedding in the f-structures.
The work reported in (van Genabith et al, 1999)
was small scale (100 trees), proof of concept and
required considerable manual annotation work. In
this paper we show how the extraction process can
be scaled to the complete Wall Street Journal (WSJ)
section of the Penn-II treebank, with about 1 mil-
lion words in 50,000 sentences, based on the au-
tomatic LFG f-structure annotation algorithm de-
scribed in (Cahill et al, 2004b). In addition to ex-
tracting grammatical function-based subcategorisa-
tion frames, we also include the syntactic categories
of the predicate and its subcategorised arguments,
as well as additional details such as the prepositions
required by obliques, and particles accompanying
particle verbs. Our method does not predefine the
frames to be extracted. In contrast to many other
approaches, it discriminates between active and pas-
sive frames, properly reflects long distance depen-
dencies and assigns conditional probabilities to the
semantic forms associated with each predicate.
Section 2 reviews related work in the area of
automatic subcategorisation frame extraction. Our
methodology and its implementation are presented
in Section 3. Section 4 presents the results of our
lexical extraction. In Section 5 we evaluate the
complete extracted lexicon against the COMLEX
resource (MacLeod et al, 1994). To our knowl-
edge, this is the largest evaluation of subcategorisa-
tion frames for English. In Section 6, we conclude
and give suggestions for future work.
2 Related Work
Creating a (subcategorisation) lexicon by hand is
time-consuming, error-prone, requires considerable
linguistic expertise and is rarely, if ever, complete.
In addition, a system incorporating a manually con-
structed lexicon cannot easily be adapted to specific
domains. Accordingly, many researchers have at-
tempted to construct lexicons automatically, espe-
cially for English.
(Brent, 1993) relies on local morphosyntactic
cues (such as the -ing suffix, except where such a
word follows a determiner or a preposition other
than to) in the untagged Brown Corpus as proba-
bilistic indicators of six different predefined subcat-
egorisation frames. The frames do not include de-
tails of specific prepositions. (Manning, 1993) ob-
serves that Brent?s recognition technique is a ?rather
simplistic and inadequate approach to verb detec-
tion, with a very high error rate?. Manning feeds
the output from a stochastic tagger into a finite state
parser, and applies statistical filtering to the parsing
results. He predefines 19 different subcategorisation
frames, including details of prepositions. Applying
this technique to approx. 4 million words of New
York Times newswire, Manning acquires 4900 sub-
categorisation frames for 3104 verbs, an average of
1.6 per verb. (Ushioda et al, 1993) run a finite state
NP parser on a POS-tagged corpus to calculate the
relative frequency of just six subcategorisation verb
classes. In addition, all prepositional phrases are
treated as adjuncts. For 1565 tokens of 33 selected
verbs, they report an accuracy rate of 83%.
(Briscoe and Carroll, 1997) observe that in the
work of (Brent, 1993), (Manning, 1993) and (Ush-
ioda et al, 1993), ?the maximum number of distinct
subcategorization classes recognized is sixteen, and
only Ushioda et al attempt to derive relative subcat-
egorization frequency for individual predicates?. In
contrast, the system of (Briscoe and Carroll, 1997)
distinguishes 163 verbal subcategorisation classes
by means of a statistical shallow parser, a classifier
of subcategorisation classes, and a priori estimates
of the probability that any verb will be a member
of those classes. More recent work by Korhonen
(2002) on the filtering phase of this approach has
improved results. Korhonen experiments with the
use of linguistic verb classes for obtaining more ac-
curate back-off estimates for use in hypothesis se-
lection. Using this extended approach, the average
results for 45 semantically classified test verbs eval-
uated against hand judgements are precision 87.1%
and recall 71.2%. By comparison, the average re-
sults for 30 verbs not classified semantically are pre-
cision 78.2% and recall 58.7%.
Carroll and Rooth (1998) use a hand-written
head-lexicalised context-free grammar and a text
corpus to compute the probability of particular sub-
categorisation scenarios. The extracted frames do
not contain details of prepositions.
More recently, a number of researchers have
applied similar techniques to derive resources for
other languages, especially German. One of these,
(Schulte im Walde, 2002), induces a computational
subcategorisation lexicon for over 14,000 German
verbs. Using sentences of limited length, she ex-
tracts 38 distinct frame types, which contain max-
imally three arguments each. The frames may op-
tionally contain details of particular prepositional
use. Her evaluation on over 3000 frequently occur-
ring verbs against the German dictionary Duden -
Das Stilwo?rterbuch is similar in scale to ours and is
discussed further in Section 5.
There has also been some work on extracting
subcategorisation details from the Penn Treebank.
(Kinyon and Prolo, 2002) introduce a tool which
uses fine-grained rules to identify the arguments,
including optional arguments, of each verb occur-
rence in the Penn Treebank, along with their syn-
tactic functions. They manually examined the 150+
possible sequences of tags, both functional and cat-
egorial, in Penn-II and determined whether the se-
quence in question denoted a modifier, argument or
optional argument. Arguments were then mapped
to traditional syntactic functions. As they do not in-
clude an evaluation, currently it is impossible to say
how effective this technique is.
(Xia et al, 2000) and (Chen and Vijay-Shanker,
2000) extract lexicalised TAGs from the Penn Tree-
bank. Both techniques implement variations on
the approaches of (Magerman, 1994) and (Collins,
1997) for the purpose of differentiating between
complement and adjunct. In the case of (Xia et al,
2000), invalid elementary trees produced as a result
of annotation errors in the treebank are filtered out
using linguistic heuristics.
(Hockenmaier et al, 2002) outline a method for
the automatic extraction of a large syntactic CCG
lexicon from Penn-II. For each tree, the algorithm
annotates the nodes with CCG categories in a top-
down recursive manner. In order to examine the
coverage of the extracted lexicon in a manner simi-
lar to (Xia et al, 2000), (Hockenmaier et al, 2002)
compared the reference lexicon acquired from Sec-
tions 02-21 with a test lexicon extracted from Sec-
tion 23 of the WSJ. It was found that the reference
CCG lexicon contained 95.09% of the entries in the
test lexicon, while 94.03% of the entries in the test
TAG lexicon also occurred in the reference lexicon.
Both approaches involve extensive correction and
clean-up of the treebank prior to lexical extraction.
3 Our Methodology
The first step in the application of our methodology
is the production of a treebank annotated with LFG
f-structure information. F-structures are feature
structures which represent abstract syntactic infor-
mation, approximating to basic predicate-argument-
modifier structures. We utilise the automatic anno-
tation algorithm of (Cahill et al, 2004b) to derive
a version of Penn-II where each node in each tree
is annotated with an LFG functional annotation (i.e.
an attribute value structure equation). Trees are tra-
versed top-down, and annotation is driven by cate-
gorial, basic configurational, trace and Penn-II func-
tional tag information in local subtrees of mostly
depth one (i.e. CFG rules). The annotation proce-
dure is dependent on locating the head daughter, for
which the scheme of (Magerman, 1994) with some
changes and amendments is used. The head is anno-
tated with the LFG equation ?=?. Linguistic gen-
eralisations are provided over the left (the prefix)
and the right (suffix) context of the head for each
syntactic category occurring as the mother node of
such heads. To give a simple example, the rightmost
NP to the left of a VP head under an S is likely to
be its subject (? SUBJ =?), while the leftmost NP
to the right of the V head of a VP is most proba-
bly its object (? OBJ =?). (Cahill et al, 2004b)
provide four sets of annotation principles, one for
non-coordinate configurations, one for coordinate
configurations, one for traces (long distance depen-
dencies) and a final ?catch all and clean up? phase.
Distinguishing between argument and adjunct is an
inherent step in the automatic assignment of func-
tional annotations.
The satisfactory treatment of long distance de-
pendencies by the annotation algorithm is impera-
tive for the extraction of accurate semantic forms.
The Penn Treebank employs a rich arsenal of traces
and empty productions (nodes which do not re-
alise any lexical material) to co-index displaced ma-
terial with the position where it should be inter-
preted semantically. The algorithm of (Cahill et
al., 2004b) translates the traces into corresponding
re-entrancies in the f-structure representation (Fig-
ure 1). Passive movement is also captured and ex-
pressed at f-structure level using a passive:+ an-
notation. Once a treebank tree is annotated with
feature structure equations by the annotation algo-
rithm, the equations are collected and passed to a
constraint solver which produces the f-structures.
In order to ensure the quality of the seman-
S
S-TPC- 1
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
S
T- 1
?
?
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
1
SUBJ
[
SPEC the
PRED headline
]
PRED say
COMP 1
?
?
?
?
?
?
?
Figure 1: Penn-II style tree with long distance depen-
dency trace and corresponding reentrancy in f-structure
tic forms extracted by our method, we must first
ensure the quality of the f-structure annotations.
(Cahill et al, 2004b) measure annotation quality
in terms of precision and recall against manually
constructed, gold-standard f-structures for 105 ran-
domly selected trees from section 23 of the WSJ
section of Penn-II. The algorithm currently achieves
an F-score of 96.3% for complete f-structures and
93.6% for preds-only f-structures.1
Our semantic form extraction methodology is
based on the procedure of (van Genabith et al,
1999): For each f-structure generated, for each
level of embedding we determine the local PRED
value and collect the subcategorisable grammat-
ical functions present at that level of embed-
ding. Consider the f-structure in Figure 1. From
this we recursively extract the following non-
empty semantic forms: say([subj,comp]),
sign([subj,obj]). In effect, in both (van
Genabith et al, 1999) and our approach seman-
tic forms are reverse engineered from automatically
generated f-structures for treebank trees. We ex-
tract the following subcategorisable syntactic func-
tions: SUBJ, OBJ, OBJ2, OBLprep, OBL2prep, COMP,
XCOMP and PART. Adjuncts (e.g. ADJ, APP etc)
are not included in the semantic forms. PART
is not a syntactic function in the strict sense but
we capture the relevant co-occurrence patterns of
verbs and particles in the semantic forms. Just
as OBL includes the prepositional head of the PP,
PART includes the actual particle which occurs e.g.
add([subj,obj,part:up]).
In the work presented here we substantially ex-
tend the approach of (van Genabith et al, 1999) as
1Preds-only measures only paths ending in PRED:VALUE so
features such as number, person etc are not included.
regards coverage, granularity and evaluation: First,
we scale the approach of (van Genabith et al, 1999)
which was proof of concept on 100 trees to the full
WSJ section of the Penn-II Treebank. Second, our
approach fully reflects long distance dependencies,
indicated in terms of traces in the Penn-II Tree-
bank and corresponding re-entrancies at f-structure.
Third, in addition to abstract syntactic function-
based subcategorisation frames we compute frames
for syntactic function-CFG category pairs, both for
the verbal heads and their arguments and also gen-
erate pure CFG-based subcat frames. Fourth, our
method differentiates between frames captured for
active or passive constructions. Fifth, our method
associates conditional probabilities with frames.
In contrast to much of the work reviewed in the
previous section, our system is able to produce sur-
face syntactic as well as abstract functional subcat-
egorisation details. To incorporate CFG details into
the extracted semantic forms, we add an extra fea-
ture to the generated f-structures, the value of which
is the syntactic category of the pred at each level
of embedding. Exploiting this information, the ex-
tracted semantic form for the verb sign looks as fol-
lows: sign(v,[subj(np),obj(np)]).
We have also extended the algorithm to deal with
passive voice and its effect on subcategorisation be-
haviour. Consider Figure 2: not taking voice into
account, the algorithm extracts an intransitive frame
outlaw([subj]) for the transitive outlaw. To
correct this, the extraction algorithm uses the fea-
ture value pair passive:+, which appears in the
f-structure at the level of embedding of the verb in
question, to mark that predicate as occurring in the
passive: outlaw([subj],p).
In order to estimate the likelihood of the cooc-
currence of a predicate with a particular argument
list, we compute conditional probabilities for sub-
categorisation frames based on the number of token
occurrences in the corpus. Given a lemma l and an
argument list s, the probability of s given l is esti-
mated as:
P(s|l) := count(l, s)?n
i=1 count(l, si)
We use thresholding to filter possible error judge-
ments by our system. Table 1 shows the attested
semantic forms for the verb accept with their as-
sociated conditional probabilities. Note that were
the distinction between active and passive not taken
into account, the intransitive occurrence of accept
would have been assigned an unmerited probability.
subj : spec : quant : pred : all
adjunct : 2 : pred : almost
adjunct : 3 : pred : remain
participle : pres
4 : obj : adjunct : 5 : pred : cancer-causing
pers : 3
pred : asbestos
num : sg
pform : of
pers : 3
pred : use
num : pl
passive : +
adjunct : 1 : obj : pred : 1997
pform : by
xcomp : subj : spec: quant : pred : all
adjunct : 2 : pred : almost
...
...
passive : +
xcomp : subj : spec: quant : pred : all
adjunct : 2 : pred : almost
...
...
passive : +
pred : outlaw
tense : past
pred : be
pred : will
modal : +
Figure 2: Automatically generated f-structure
for the string wsj 0003 23?By 1997, almost
all remaining uses of cancer-causing
asbestos will be outlawed.?
Semantic Form Frequency Probability
accept([subj,obj]) 122 0.813
- accept([subj],p) 9 0.060
accept([subj,comp]) 5 0.033
- accept([subj,obl:as],p) 3 0.020
accept([subj,obj,obl:as]) 3 0.020
accept([subj,obj,obl:from]) 3 0.020
- accept([subj]) 2 0.013
accept([subj,obj,obl:at]) 1 0.007
accept([subj,obj,obl:for]) 1 0.007
accept([subj,obj,xcomp]) 1 0.007
Table 1: Semantic Forms for the verb accept marked
with p for passive use.
4 Results
We extract non-empty semantic forms2 for 3586
verb lemmas and 10969 unique verbal semantic
form types (lemma followed by non-empty argu-
ment list). Including prepositions associated with
the OBLs and particles, this number rises to 14348,
an average of 4.0 per lemma (Table 2). The num-
ber of unique frame types (without lemma) is 38
without specific prepositions and particles, 577 with
(Table 3). F-structure annotations allow us to distin-
guish passive and active frames.
5 COMLEX Evaluation
We evaluated our induced (verbal) semantic forms
against COMLEX (MacLeod et al, 1994). COM-
2Frames with at least one subcategorised grammatical func-
tion.
Without Prep/Part With Prep/Part
Sem. Form Types 10969 14348
Active 8516 11367
Passive 2453 2981
Table 2: Number of Semantic Form Types
Without Prep/Part With Prep/Part
# Frame Types 38 577
# Singletons 1 243
# Twice Occurring 1 84
# Occurring max. 5 7 415
# Occurring > 5 31 162
Table 3: Number of Distinct Frames for Verbs (not in-
cluding syntactic category for grammatical function)
LEX defines 138 distinct verb frame types without
the inclusion of specific prepositions or particles.
The following is a sample entry for the verb
reimburse:
(VERB :ORTH ?reimburse? :SUBC ((NP-NP)
(NP-PP :PVAL (?for?))
(NP)))
Each verb has a :SUBC feature, specifying
its subcategorisation behaviour. For example,
reimburse can occur with two noun phrases
(NP-NP), a noun phrase and a prepositional phrase
headed by ?for? (NP-PP :PVAL (?for?)) or a single
noun phrase (NP). Note that the details of the subject
noun phrase are not included in COMLEX frames.
Each of the complement types which make up the
value of the :SUBC feature is associated with a for-
mal frame definition which looks as follows:
(vp-frame np-np :cs ((np 2)(np 3))
:gs (:subject 1 :obj 2 :obj2 3)
:ex ?she asked him his name?)
The value of the :cs feature is the constituent struc-
ture of the subcategorisation frame, which lists the
syntactic CF-PSG constituents in sequence. The
value of the :gs feature is the grammatical struc-
ture which indicates the functional role played by
each of the CF-PSG constituents. The elements of
the constituent structure are indexed, and referenced
in the :gs field. This mapping between constituent
structure and functional structure makes the infor-
mation contained in COMLEX suitable as an eval-
uation standard for the LFG semantic forms which
we induce.
5.1 COMLEX-LFG Mapping
We devised a common format for our induced se-
mantic forms and those contained in COMLEX.
This is summarised in Table 4. COMLEX does
not distinguish between obliques and objects so we
converted Obji to OBLi as required. In addition,
COMLEX does not explicitly differentiate between
COMPs and XCOMPs, but does encode control in-
formation for any Comps which occur, thus allow-
ing us to deduce the distinction automatically. The
manually constructed COMLEX entries provided us
with a gold standard against which we evaluated the
automatically induced frames for the 2992 (active)
verbs that both resources have in common.
LFG COMLEX Merged
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJ2
OBL Obj3 OBL
OBL2 Obj4 OBL2
COMP Comp COMP
XCOMP Comp XCOMP
PART Part PART
Table 4: COMLEX and LFG Syntactic Functions
We use the computed conditional probabilities to set
a threshold to filter the selection of semantic forms.
As some verbs occur less frequently than others we
felt it was important to use a relative rather than ab-
solute threshold. For a threshold of 1%, we disre-
gard any frames with a conditional probability of
less than or equal to 0.01. We carried out the evalu-
ation in a similar way to (Schulte im Walde, 2002).
The scale of our evaluation is comparable to hers.
This allows us to make tentative comparisons be-
tween our respective results. The figures shown in
Table 5 are the results of three different kinds of
evaluation with the threshold set to 1% and 5%. The
effect of the threshold increase is obvious in that
Precision goes up for each of the experiments while
Recall goes down.
For Exp 1, we excluded prepositional phrases en-
tirely from the comparison, i.e. assumed that PPs
were adjunct material (e.g. [subj,obl:for] becomes
[subj]). Our results are better for Precision than for
Recall compared to Schulte im Walde (op cit.), who
reports Precision of 74.53%, Recall of 69.74% and
an F-score of 72.05%.
Exp 2 includes prepositional phrases but not
parameterised for particular prepositions (e.g.
[subj,obl:for] becomes [subj,obl]). While our fig-
ures for Recall are again lower, our results for
Precision are considerably higher than those of
Schulte im Walde (op cit.) who recorded Preci-
sion of 60.76%, Recall of 63.91% and an F-score
of 62.30%.
For Exp. 3, we used semantic forms which con-
tained details of specific prepositions for any sub-
categorised prepositional phrase. Our Precision fig-
ures are again high (in comparison to 65.52% as
recorded by (Schulte im Walde, 2002)). However,
Threshold 1% Threshold 5%
P R F-Score P R F-Score
Exp. 1 79.0% 59.6% 68.0% 83.5% 54.7% 66.1%
Exp. 2 77.1% 50.4% 61.0% 81.4% 44.8% 57.8%
Exp. 2a 76.4% 44.5% 56.3% 80.9% 39.0% 52.6%
Exp. 3 73.7% 22.1% 34.0% 78.0% 18.3% 29.6%
Exp. 3a 73.3% 19.9% 31.3% 77.6% 16.2% 26.8%
Table 5: COMLEX Comparison
our Recall is very low (compared to the 50.83% that
Schulte im Walde (op cit.) reports). Consequently
our F-score is also low (Schulte im Walde (op cit.)
records an F-score of 57.24%). Experiments 2a and
3a are similar to Experiments 2 and 3 respectively
except they include the specific particle associated
with each PART.
5.1.1 Directional Prepositions
There are a number of possible reasons for our
low recall scores for Experiment 3 in Table 5. It
is a well-documented fact (Briscoe and Carroll,
1997) that subcategorisation frames (and their fre-
quencies) vary across domains. We have extracted
frames from one domain (the WSJ) whereas COM-
LEX was built using examples from the San Jose
Mercury News, the Brown Corpus, several literary
works from the Library of America, scientific ab-
stracts from the U.S. Department of Energy, and
the WSJ. For this reason, it is likely to contain
a greater variety of subcategorisation frames than
our induced lexicon. It is also possible that due
to human error COMLEX contains subcategorisa-
tion frames, the validity of which may be in doubt.
This is due to the fact that the aim of the COMLEX
project was to construct as complete a set of subcat-
egorisation frames as possible, even for infrequent
verbs. Lexicographers were allowed to extrapo-
late from the citations found, a procedure which
is bound to be less certain than the assignment of
frames based entirely on existing examples. Our re-
call figure was particularly low in the case of eval-
uation using details of prepositions (Experiment 3).
This can be accounted for by the fact that COMLEX
errs on the side of overgeneration when it comes to
preposition assignment. This is particularly true of
directional prepositions, a list of 31 of which has
been prepared and is assigned in its entirety by de-
fault to any verb which can potentially appear with
any directional preposition. In a subsequent exper-
iment, we incorporate this list of directional prepo-
sitions by default into our semantic form induction
process in the same way as the creators of COM-
LEX have done. Table 6 shows the results of this
experiment. As expected there is a significant im-
Precision Recall F-Score
Experiment 3 81.7% 40.8% 54.4%
Experiment 3a 83.1% 35.4% 49.7%
Table 6: COMLEX Comparison using p-dir(Threshold
of 1%)
Passive Precision Recall F-Score
Experiment 2 80.2% 54.7% 65.1%
Experiment 2a 79.7% 46.2% 58.5%
Experiment 3 72.6% 33.4% 45.8%
Experiment 3a 72.3% 29.3% 41.7%
Table 7: Passive evaluation (Threshold of 1%)
provement in the recall figure, being almost double
the figures reported in Table 5 for Experiments 3
and 3a.
5.1.2 Passive Evaluation
Table 7 presents the results of our evaluation of
the passive semantic forms we extract. It was
carried out for 1422 verbs which occur with pas-
sive frames and are shared by the induced lexicon
and COMLEX. As COMLEX does not provide ex-
plicit passive entries, we applied Lexical Redun-
dancy Rules (Kaplan and Bresnan, 1982) to auto-
matically convert the active COMLEX frames to
their passive counterparts. For example, the COM-
LEX entry see([subj,obj]) is converted to
see([subj]). The resulting precision is very
high, a slight increase on that for the active frames.
The recall score drops for passive frames (from
54.7% to 29.3%) in a similar way to that for active
frames when prepositional details are included.
5.2 Lexical Accession Rates
As well as evaluating the quality of our extracted
semantic forms, we also examine the rate at which
they are induced. (Charniak, 1996) and (Krotov et
al., 1998) observed that treebank grammars (CFGs
extracted from treebanks) are very large and grow
with the size of the treebank. We were interested in
discovering whether the acquisition of lexical mate-
rial on the same data displays a similar propensity.
Figure 3 displays the accession rates for the seman-
tic forms induced by our method for sections 0?24
of the WSJ section of the Penn-II treebank. When
we do not distinguish semantic forms by category,
all semantic forms together with those for verbs dis-
play smaller accession rates than for the PCFG.
We also examined the coverage of our system in
a similar way to (Hockenmaier et al, 2002). We ex-
tracted a verb-only reference lexicon from Sections
02-21 of the WSJ and subsequently compared this
to a test lexicon constructed in the same way from
 0
 5000
 10000
 15000
 20000
 25000
 0  5  10  15  20  25
N
o.
 o
f S
Fs
/R
ul
es
WSJ Section
All SF Frames
All Verbs
All SF Frames, no category
All Verbs, no category
PCFG
Figure 3: Accession Rates for Semantic Forms and CFG
Rules
Entries also in reference lexicon: 89.89%
Entries not in reference lexicon: 10.11%
Known words: 7.85%
- Known words, known frames: 7.85%
- Known words, unknown frames: -
Unknown words: 2.32%
- Unknown words, known frames: 2.32%
- Unknown words, unknown frames: -
Table 8: Coverage of induced lexicon on unseen
data (Verbs Only)
Section 23. Table 8 shows the results of this ex-
periment. 89.89% of the entries in the test lexicon
appeared in the reference lexicon.
6 Conclusions
We have presented an algorithm and its implementa-
tion for the extraction of semantic forms or subcate-
gorisation frames from the Penn-II Treebank, auto-
matically annotated with LFG f-structures. We have
substantially extended an earlier approach by (van
Genabith et al, 1999). The original approach was
small-scale and ?proof of concept?. We have scaled
our approach to the entire WSJ Sections of Penn-
II (50,000 trees). Our approach does not predefine
the subcategorisation frames we extract as many
other approaches do. We extract abstract syntac-
tic function-based subcategorisation frames (LFG
semantic forms), traditional CFG category-based
frames as well as mixed function-category based
frames. Unlike many other approaches to subcate-
gorisation frame extraction, our system properly re-
flects the effects of long distance dependencies and
distinguishes between active and passive frames.
Finally our system associates conditional probabil-
ities with the frames we extract. We carried out an
extensive evaluation of the complete induced lexi-
con (not just a sample) against the full COMLEX
resource. To our knowledge, this is the most exten-
sive qualitative evaluation of subcategorisation ex-
traction in English. The only evaluation of a similar
scale is that carried out by (Schulte im Walde, 2002)
for German. Our results compare well with hers.
We believe our semantic forms are fine-grained and
by choosing to evaluate against COMLEX we set
our sights high: COMLEX is considerably more
detailed than the OALD or LDOCE used for other
evaluations.
Currently work is under way to extend the cov-
erage of our acquired lexicons by applying our
methodology to the Penn-III treebank, a more bal-
anced corpus resource with a number of text gen-
res (in addition to the WSJ sections). It is impor-
tant to realise that the induction of lexical resources
is part of a larger project on the acquisition of
wide-coverage, robust, probabilistic, deep unifica-
tion grammar resources from treebanks. We are al-
ready using the extracted semantic forms in parsing
new text with robust, wide-coverage PCFG-based
LFG grammar approximations automatically ac-
quired from the f-structure annotated Penn-II tree-
bank (Cahill et al, 2004a). We hope to be able to
apply our lexical acquisition methodology beyond
existing parse-annotated corpora (Penn-II and Penn-
III): new text is parsed by our PCFG-based LFG ap-
proximations into f-structures from which we can
then extract further semantic forms. The work re-
ported here is part of the core component for boot-
strapping this approach.
As the extraction algorithm we presented derives
semantic forms at f-structure level, it is easily ap-
plied to other, even typologically different, lan-
guages. We have successfully ported our automatic
annotation algorithm to the TIGER Treebank, de-
spite German being a less configurational language
than English, and extracted wide-coverage, proba-
bilistic LFG grammar approximations and lexical
resources for German (Cahill et al, 2003). Cur-
rently, we are migrating the technique to Spanish,
which has freer word order than English and less
morphological marking than German. Preliminary
results have been very encouraging.
7 Acknowledgements
The research reported here is supported by Enter-
prise Ireland Basic Research Grant SC/2001/186
and an IRCSET PhD fellowship award.
References
M. Brent. 1993. From Grammar to Lexicon: Unsu-
pervised Learning of Lexical Syntax. Computa-
tional Linguistics, 19(2):203?222.
E. Briscoe and J. Carroll. 1997. Automatic Extrac-
tion of Subcategorization from Corpora. In Pro-
ceedings of the 5th ACL Conference on Applied
Natural Language Processing, pages 356?363,
Washington, DC.
A. Cahill, M. Forst, M. McCarthy, R. O?Donovan,
C. Rohrer, J. van Genabith, and A. Way.
2003. Treebank-Based Multilingual Unification-
Grammar Development. In Proceedings of the
Workshop on Ideas and Strategies for Multilin-
gual Grammar Development at the 15th ESSLLI,
pages 17?24, Vienna, Austria.
A. Cahill, M. Burke, R. O?Donovan, J. van Gen-
abith, and A. Way. 2004a. Long-Distance De-
pendency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approxima-
tions. In Proceedings of the 42nd Annual Con-
ference of the Association for Computational Lin-
guistics (ACL-04), Barcelona, Spain.
A. Cahill, M. McCarthy, M. Burke, R. O?Donovan,
J. van Genabith, and A. Way. 2004b. Evaluating
Automatic F-Structure Annotation for the Penn-
II Treebank. Journal of Research on Language
and Computation.
G. Carroll and M. Rooth. 1998. Valence Induc-
tion with a Head-Lexicalised PCFG. In Proceed-
ings of the 3rd Conference on Empirical Meth-
ods in Natural Language Processing, pages 36?
45, Granada, Spain.
E. Charniak. 1996. Tree-bank Grammars. In AAAI-
96: Proceedings of the Thirteenth National Con-
ference on Artificial Intelligence, MIT Press,
pages 1031?1036, Cambridge, MA.
J. Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proceedings of the 38th Annual Meeting of the
Association of Computational Linguistics, pages
65?76, Hong Kong.
M. Collins. 1997. Three generative lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 16?23.
J. Hockenmaier, G. Bierner, and J. Baldridge. 2002.
Extending the Coverage of a CCG System. Jour-
nal of Language and Computation, (2).
R. Kaplan and J. Bresnan. 1982. Lexical Func-
tional Grammar: A Formal System for Gram-
matical Representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical Re-
lations, pages 206?250. MIT Press, Cambridge,
MA, Mannheim, 8th Edition.
A. Kinyon and C. Prolo. 2002. Identifying Verb Ar-
guments and their Syntactic Function in the Penn
Treebank. In Proceedings of the 3rd LREC Con-
ference, pages 1982?1987, Las Palmas, Spain.
A. Korhonen. 2002. Subcategorization Acquisition.
PhD thesis published as Techical Report UCAM-
CL-TR-530, Computer Laboratory, University of
Cambridge, UK.
A. Krotov, M. Hepple, R. Gaizauskas, and Y. Wilks.
1998. Compacting the Penn Treebank Grammar.
In Proceedings of COLING-ACL?98, pages 669?
703, Montreal, Canada.
C. MacLeod, R. Grishman, and A. Meyers. 1994.
The Comlex Syntax Project: The First Year. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 669?703, Prince-
ton, NJ.
D. Magerman. 1994. Natural Language Parsing
as Statistical Pattern Recognition. PhD Thesis,
Stanford University, CA.
C. Manning. 1993. Automatic Acquisition of a
Large Subcategorisation Dictionary from Cor-
pora. In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguistics,
pages 235?242, Columbus, OH.
S. Schulte im Walde. 2002. Evaluating Verb Sub-
categorisation Frames learned by a German Sta-
tistical Grammar against Manual Definitions in
the Duden Dictionary. In Proceedings of the 10th
EURALEX International Congress, pages 187?
197, Copenhagen, Denmark.
A. Ushioda, D. Evans, T. Gibson, and A. Waibel.
1993. The Automatic Acquisition of Frequencies
of Verb Subcategorization Frames from Tagged
Corpora. In SIGLEX ACL Workshop on the Ac-
quisition of Lexical Knowledge from Text, pages
95?106, Columbus, OH.
J. van Genabith, A. Way, and L. Sadler. 1999. Data-
driven Compilation of LFG Semantic Forms. In
EACL-99 Workshop on Linguistically Interpreted
Corpora, pages 69?76, Bergen, Norway.
F. Xia, M. Palmer, and A. Joshi. 2000. A Uniform
Method of Grammar Extraction and its Applica-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2000), pages 53?62, Hong Kong.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288?295,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Supertagged Phrase-Based Statistical Machine Translation
Hany Hassan
School of Computing,
Dublin City University,
Dublin 9, Ireland
hhasan@computing.dcu.ie
Khalil Sima?an
Language and Computation,
University of Amsterdam,
Amsterdam, The Netherlands
simaan@science.uva.nl
Andy Way
School of Computing,
Dublin City University,
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
Until quite recently, extending Phrase-based
Statistical Machine Translation (PBSMT)
with syntactic structure caused system per-
formance to deteriorate. In this work we
show that incorporating lexical syntactic de-
scriptions in the form of supertags can yield
significantly better PBSMT systems. We de-
scribe a novel PBSMT model that integrates
supertags into the target language model
and the target side of the translation model.
Two kinds of supertags are employed: those
from Lexicalized Tree-Adjoining Grammar
and Combinatory Categorial Grammar. De-
spite the differences between these two ap-
proaches, the supertaggers give similar im-
provements. In addition to supertagging, we
also explore the utility of a surface global
grammaticality measure based on combina-
tory operators. We perform various experi-
ments on the Arabic to English NIST 2005
test set addressing issues such as sparseness,
scalability and the utility of system subcom-
ponents. Our best result (0.4688 BLEU)
improves by 6.1% relative to a state-of-the-
art PBSMT model, which compares very
favourably with the leading systems on the
NIST 2005 task.
1 Introduction
Within the field of Machine Translation, by far the
most dominant paradigm is Phrase-based Statistical
Machine Translation (PBSMT) (Koehn et al, 2003;
Tillmann &Xia, 2003). However, unlike in rule- and
example-based MT, it has proven difficult to date to
incorporate linguistic, syntactic knowledge in order
to improve translation quality. Only quite recently
have (Chiang, 2005) and (Marcu et al, 2006) shown
that incorporating some form of syntactic structure
could show improvements over a baseline PBSMT
system. While (Chiang, 2005) avails of structure
which is not linguistically motivated, (Marcu et al,
2006) employ syntactic structure to enrich the en-
tries in the phrase table.
In this paper we explore a novel approach towards
extending a standard PBSMT system with syntactic
descriptions: we inject lexical descriptions into both
the target side of the phrase translation table and the
target language model. Crucially, the kind of lexical
descriptions that we employ are those that are com-
monly devised within lexicon-driven approaches to
linguistic syntax, e.g. Lexicalized Tree-Adjoining
Grammar (Joshi & Schabes, 1992; Bangalore &
Joshi, 1999) and Combinary Categorial Grammar
(Steedman, 2000). In these linguistic approaches, it
is assumed that the grammar consists of a very rich
lexicon and a tiny, impoverished1 set of combina-
tory operators that assemble lexical entries together
into parse-trees. The lexical entries consist of syn-
tactic constructs (?supertags?) that describe informa-
tion such as the POS tag of the word, its subcatego-
rization information and the hierarchy of phrase cat-
egories that the word projects upwards. In this work
we employ the lexical entries but exchange the al-
gebraic combinatory operators with the more robust
1These operators neither carry nor presuppose further lin-
guistic knowledge beyond what the lexicon contains.
288
and efficient supertagging approach: like standard
taggers, supertaggers employ probabilities based on
local context and can be implemented using finite
state technology, e.g. Hidden Markov Models (Ban-
galore & Joshi, 1999).
There are currently two supertagging approaches
available: LTAG-based (Bangalore & Joshi, 1999)
and CCG-based (Clark & Curran, 2004). Both the
LTAG (Chen et al, 2006) and the CCG supertag
sets (Hockenmaier, 2003) were acquired from the
WSJ section of the Penn-II Treebank using hand-
built extraction rules. Here we test both the LTAG
and CCG supertaggers. We interpolate (log-linearly)
the supertagged components (language model and
phrase table) with the components of a standard
PBSMT system. Our experiments on the Arabic?
English NIST 2005 test suite show that each of the
supertagged systems significantly improves over the
baseline PBSMT system. Interestingly, combining
the two taggers together diminishes the benefits of
supertagging seen with the individual LTAG and
CCG systems. In this paper we discuss these and
other empirical issues.
The remainder of the paper is organised as fol-
lows: in section 2 we discuss the related work on en-
riching PBSMT with syntactic structure. In section
3, we describe the baseline PBSMT system which
our work extends. In section 4, we detail our ap-
proach. Section 5 describes the experiments carried
out, together with the results obtained. Section 6
concludes, and provides avenues for further work.
2 Related Work
Until very recently, the experience with adding syn-
tax to PBSMT systems was negative. For example,
(Koehn et al, 2003) demonstrated that adding syn-
tax actually harmed the quality of their SMT system.
Among the first to demonstrate improvement when
adding recursive structure was (Chiang, 2005), who
allows for hierarchical phrase probabilities that han-
dle a range of reordering phenomena in the correct
fashion. Chiang?s derived grammar does not rely on
any linguistic annotations or assumptions, so that the
?syntax? induced is not linguistically motivated.
Coming right up to date, (Marcu et al, 2006)
demonstrate that ?syntactified? target language
phrases can improve translation quality for Chinese?
English. They employ a stochastic, top-down trans-
duction process that assigns a joint probability to
a source sentence and each of its alternative trans-
lations when rewriting the target parse-tree into a
source sentence. The rewriting/transduction process
is driven by ?xRS rules?, each consisting of a pair
of a source phrase and a (possibly only partially)
lexicalized syntactified target phrase. In order to
extract xRS rules, the word-to-word alignment in-
duced from the parallel training corpus is used to
guide heuristic tree ?cutting? criteria.
While the research of (Marcu et al, 2006) has
much in common with the approach proposed here
(such as the syntactified target phrases), there re-
main a number of significant differences. Firstly,
rather than induce millions of xRS rules from par-
allel data, we extract phrase pairs in the standard
way (Och & Ney, 2003) and associate with each
phrase-pair a set of target language syntactic struc-
tures based on supertag sequences. Relative to using
arbitrary parse-chunks, the power of supertags lies
in the fact that they are, syntactically speaking, rich
lexical descriptions. A supertag can be assigned to
every word in a phrase. On the one hand, the cor-
rect sequence of supertags could be assembled to-
gether, using only impoverished combinatory opera-
tors, into a small set of constituents/parses (?almost?
a parse). On the other hand, because supertags are
lexical entries, they facilitate robust syntactic pro-
cessing (using Markov models, for instance) which
does not necessarily aim at building a fully con-
nected graph.
A second major difference with xRS rules is that
our supertag-enriched target phrases need not be
generalized into (xRS or any other) rules that work
with abstract categories. Finally, like POS tagging,
supertagging is more efficient than actual parsing or
tree transduction.
3 Baseline Phrase-Based SMT System
We present the baseline PBSMT model which we
extend with supertags in the next section. Our
baseline PBSMT model uses GIZA++2 to obtain
word-level alignments in both language directions.
The bidirectional word alignment is used to obtain
phrase translation pairs using heuristics presented in
2http://www.fjoch.com/GIZA++.html
289
(Och & Ney, 2003) and (Koehn et al, 2003), and the
Moses decoder was used for phrase extraction and
decoding.3
Let t and s be the target and source language
sentences respectively. Any (target or source) sen-
tence x will consist of two parts: a bag of elements
(words/phrases etc.) and an order over that bag. In
other words, x = ??x, Ox?, where ?x stands for the
bag of phrases that constitute x, andOx for the order
of the phrases as given in x (Ox can be implemented
as a function from a bag of tokens ?x to a set with a
finite number of positions). Hence, we may separate
order from content:
argmax
t
P (t|s) = argmax
t
P (s | t)P (t) (1)
= arg max
??t,Ot?
TM
? ?? ?
P (?s | ?t)
distortion
? ?? ?
P (Os | Ot)
LM
? ?? ?
Pw(t) (2)
Here, Pw(t) is the target language model, P (Os|Ot)
represents the conditional (order) linear distortion
probability, and P (?s|?t) stands for a probabilis-
tic translation model from target language bags of
phrases to source language bags of phrases using a
phrase translation table. As commonly done in PB-
SMT, we interpolate these models log-linearly (us-
ing different ?weights) together with a word penalty
weight which allows for control over the length of
the target sentence t:
arg max
??t,Ot?
P (?s | ?t) P (Os | Ot)
?o
Pw(t)
?lm exp|t|?w
For convenience of notation, the interpolation factor
for the bag of phrases translation model is shown in
formula (3) at the phrase level (but that does not en-
tail any difference). For a bag of phrases ?t consist-
ing of phrases ti, and bag ?s consisting of phrases
si, the phrase translation model is given by:
P (?s | ?t) =
Y
si
ti
P (si|ti)
P (si| ti) = Pph(si|ti)
?t1Pw(si|ti)
?t2Pr(ti|si)
?t3 (3)
where Pph and Pr are the phrase-translation proba-
bility and its reverse probability, and Pw is the lexi-
cal translation probability.
3http://www.statmt.org/moses/
4 Our Approach: Supertagged PBSMT
We extend the baseline model with lexical linguis-
tic representations (supertags) both in the language
model as well as in the phrase translation model. Be-
fore we describe how our model extends the base-
line, we shortly review the supertagging approaches
in Lexicalized Tree-Adjoining Grammar and Com-
binatory Categorial Grammar.
4.1 Supertags: Lexical Syntax
NP
D
The
NP
NP
N
purchase
NP
NP
N
price
S
NP VP
V
includes
NP
NP
N
taxes
Figure 1: An LTAG supertag sequence for the sen-
tence The purchase price includes taxes. The sub-
categorization information is most clearly available
in the verb includes which takes a subject NP to its
left and an object NP to its right.
Modern linguistic theory proposes that a syntactic
parser has access to an extensive lexicon of word-
structure pairs and a small, impoverished set of oper-
ations to manipulate and combine the lexical entries
into parses. Examples of formal instantiations of this
idea include CCG and LTAG. The lexical entries are
syntactic constructs (graphs) that specify informa-
tion such as POS tag, subcategorization/dependency
information and other syntactic constraints at the
level of agreement features. One important way of
portraying such lexical descriptions is via the su-
pertags devised in the LTAG and CCG frameworks
(Bangalore & Joshi, 1999; Clark & Curran, 2004).
A supertag (see Figure 1) represents a complex,
linguistic word category that encodes a syntactic
structure expressing a specific local behaviour of a
word, in terms of the arguments it takes (e.g. sub-
ject, object) and the syntactic environment in which
it appears. In fact, in LTAG a supertag is an elemen-
tary tree and in CCG it is a CCG lexical category.
Both descriptions can be viewed as closely related
functional descriptions.
The term ?supertagging? (Bangalore & Joshi,
1999) refers to tagging the words of a sentence, each
290
with a supertag. When well-formed, an ordered se-
quence of supertags can be viewed as a compact
representation of a small set of constituents/parses
that can be obtained by assembling the supertags
together using the appropriate combinatory opera-
tors (such as substitution and adjunction in LTAG
or function application and combination in CCG).
Akin to POS tagging, the process of supertagging
an input utterance proceeds with statistics that are
based on the probability of a word-supertag pair
given their Markovian or local context (Bangalore
& Joshi, 1999; Clark & Curran, 2004). This is the
main difference with full parsing: supertagging the
input utterance need not result in a fully connected
graph.
The LTAG-based supertagger of (Bangalore &
Joshi, 1999) is a standard HMM tagger and consists
of a (second-order) Markov language model over su-
pertags and a lexical model conditioning the proba-
bility of every word on its own supertag (just like
standard HMM-based POS taggers).
The CCG supertagger (Clark & Curran, 2004) is
based on log-linear probabilities that condition a su-
pertag on features representing its context. The CCG
supertagger does not constitute a language model
nor are the Maximum Entropy estimates directly in-
terpretable as such. In our model we employ the
CCG supertagger to obtain the best sequences of su-
pertags for a corpus of sentences from which we ob-
tain language model statistics. Besides the differ-
ence in probabilities and statistical estimates, these
two supertaggers differ in the way the supertags are
extracted from the Penn Treebank, cf. (Hocken-
maier, 2003; Chen et al, 2006). Both supertaggers
achieve a supertagging accuracy of 90?92%.
Three aspects make supertags attractive in the
context of SMT. Firstly, supertags are rich syntac-
tic constructs that exist for individual words and so
they are easy to integrate into SMT models that can
be based on any level of granularity, be it word-
or phrase-based. Secondly, supertags specify the
local syntactic constraints for a word, which res-
onates well with sequential (finite state) statistical
(e.g. Markov) models. Finally, because supertags
are rich lexical descriptions that represent under-
specification in parsing, it is possible to have some
of the benefits of full parsing without imposing the
strict connectedness requirements that it demands.
4.2 A Supertag-Based SMT model
We employ the aforementioned supertaggers to en-
rich the English side of the parallel training cor-
pus with a single supertag sequence per sentence.
Then we extract phrase-pairs together with the co-
occuring English supertag sequence from this cor-
pus via the same phrase extraction method used in
the baseline model. This way we directly extend
the baseline model described in section 3 with su-
pertags both in the phrase translation table and in
the language model. Next we define the probabilistic
model that accompanies this syntactic enrichment of
the baseline model.
Let ST represent a supertag sequence of the same
length as a target sentence t. Equation (2) changes
as follows:
argmax
t
?
ST
P (s | t, ST )PST (t, ST ) ?
arg max
?t,ST ?
TM w.sup.tags
? ?? ?
P (?s | ?t,ST )
distortion
? ?? ?
P (Os | Ot)
?o
LM w.sup.tags
? ?? ?
PST (t, ST )
word?penalty
? ?? ?
exp|t|?w
The approximations made in this formula are of two
kinds: the standard split into components and the
search for the most likely joint probability of a tar-
get hypothesis and a supertag sequence cooccuring
with the source sentence (a kind of Viterbi approach
to avoid the complex optimization involving the sum
over supertag sequences). The distortion and word
penalty models are the same as those used in the
baseline PBSMT model.
Supertagged Language Model The ?language
model? PST (t, ST ) is a supertagger assigning prob-
abilities to sequences of word?supertag pairs. The
language model is further smoothed by log-linear
interpolation with the baseline language model over
word sequences.
Supertags in Phrase Tables The supertagged
phrase translation probability consists of a combina-
tion of supertagged components analogous to their
counterparts in the baseline model (equation (3)),
i.e. it consists of P (s | t, ST ), its reverse and
a word-level probability. We smooth this proba-
bility by log-linear interpolation with the factored
291
John bought quickly sharesNNP_NN VBD_(S[dcl]\NP)/NP RB|(S\NP)\(S\NP) NNS_N
2 Violations
Figure 2: Example CCG operator violations: V = 2
and L = 3, and so the penalty factor is 1/3.
backoff version P (s | t)P (s | ST ), where we im-
port the baseline phrase table probability and ex-
ploit the probability of a source phrase given the tar-
get supertag sequence. A model in which we omit
P (s | ST ) turns out to be slightly less optimal than
this one.
As in most state-of-the-art PBSMT systems, we
use GIZA++ to obtain word-level alignments in both
language directions. The bidirectional word align-
ment is used to obtain lexical phrase translation pairs
using heuristics presented in (Och & Ney, 2003) and
(Koehn et al, 2003). Given the collected phrase
pairs, we estimate the phrase translation probability
distribution by relative frequency as follows:
P?ph(s|t) =
count(s, t)
?
s count(s, t)
For each extracted lexical phrase pair, we extract the
corresponding supertagged phrase pairs from the su-
pertagged target sequence in the training corpus (cf.
section 5). For each lexical phrase pair, there is
at least one corresponding supertagged phrase pair.
The probability of the supertagged phrase pair is es-
timated by relative frequency as follows:
Pst(s|t, st) =
count(s, t, st)
?
s count(s, t, st)
4.3 LMs with a Grammaticality Factor
The supertags usually encode dependency informa-
tion that could be used to construct an ?almost parse?
with the help of the CCG/LTAG composition oper-
ators. The n-gram language model over supertags
applies a kind of statistical ?compositionality check?
but due to smoothing effects this could mask cru-
cial violations of the compositionality operators of
the grammar formalism (CCG in this case). It is
interesting to observe the effect of integrating into
the language model a penalty imposed when formal
compostion operators are violated. We combine the
n-gram language model with a penalty factor that
measures the number of encountered combinatory
operator violations in a sequence of supertags (cf.
Figure 2). For a supertag sequence of length (L)
which has (V ) operator violations (as measured by
the CCG system), the language model P will be ad-
justed as P? = P ? (1 ? VL ). This is of course no
longer a simple smoothed maximum-likelihood es-
timate nor is it a true probability. Nevertheless, this
mechanism provides a simple, efficient integration
of a global compositionality (grammaticality) mea-
sure into the n-gram language model over supertags.
Decoder The decoder used in this work is Moses,
a log-linear decoder similar to Pharaoh (Koehn,
2004), modified to accommodate supertag phrase
probabilities and supertag language models.
5 Experiments
In this section we present a number of experiments
that demonstrate the effect of lexical syntax on trans-
lation quality. We carried out experiments on the
NIST open domain news translation task from Ara-
bic into English. We performed a number of ex-
periments to examine the effect of supertagging ap-
proaches (CCG or LTAG) with varying data sizes.
Data and Settings The experiments were con-
ducted for Arabic to English translation and tested
on the NIST 2005 evaluation set. The systems were
trained on the LDC Arabic?English parallel corpus;
we use the news part (130K sentences, about 5 mil-
lion words) to train systems with what we call the
small data set, and the news and a large part of
the UN data (2 million sentences, about 50 million
words) for experiments with large data sets.
The n-gram target language model was built us-
ing 250M words from the English GigaWord Cor-
pus using the SRILM toolkit.4 Taking 10% of the
English GigaWord Corpus used for building our tar-
get language model, the supertag-based target lan-
guage models were built from 25M words that were
supertagged. For the LTAG supertags experiments,
we used the LTAG English supertagger5 (Bangalore
4http://www.speech.sri.com/projects/srilm/
5http://www.cis.upenn.edu/?xtag/gramrelease.html
292
& Joshi, 1999) to tag the English part of the parallel
data and the supertag language model data. For the
CCG supertag experiments, we used the CCG su-
pertagger of (Clark & Curran, 2004) and the Edin-
burgh CCG tools6 to tag the English part of the par-
allel corpus as well as the CCG supertag language
model data.
The NIST MT03 test set is used for development,
particularly for optimizing the interpolation weights
using Minimum Error Rate training (Och, 2003).
Baseline System The baseline system is a state-
of-the-art PBSMT system as described in sec-
tion 3. We built two baseline systems with two
different-sized training sets: ?Base-SMALL? (5 mil-
lion words) and ?Base-LARGE? (50 million words)
as described above. Both systems use a trigram lan-
guage model built using 250 million words from
the English GigaWord Corpus. Table 1 presents the
BLEU scores (Papineni et al, 2002) of both systems
on the NIST 2005 MT Evaluation test set.
System BLEU Score
Base-SMALL 0.4008
Base-LARGE 0.4418
Table 1: Baseline systems? BLEU scores
5.1 Baseline vs. Supertags on Small Data Sets
We compared the translation quality of the baseline
systems with the LTAG and CCG supertags systems
(LTAG-SMALL and CCG-SMALL). The results are
System BLEU Score
Base-SMALL 0.4008
LTAG-SMALL 0.4205
CCG-SMALL 0.4174
Table 2: LTAG and CCG systems on small data
given in Table 2. All systems were trained on the
same parallel data. The LTAG supertag-based sys-
tem outperforms the baseline by 1.97 BLEU points
absolute (or 4.9% relative), while the CCG supertag-
based system scores 1.66 BLEU points over the
6http://groups.inf.ed.ac.uk/ccg/software.html
baseline (4.1% relative). These significant improve-
ments indicate that the rich information in supertags
helps select better translation candidates.
POS Tags vs. Supertags A supertag is a complex
tag that localizes the dependency and the syntax in-
formation from the context, whereas a normal POS
tag just describes the general syntactic category of
the word without further constraints. In this experi-
ment we compared the effect of using supertags and
POS tags on translation quality. As can be seen
System BLEU Score
Base-SMALL 0.4008
POS-SMALL 0.4073
LTAG-SMALL .0.4205
Table 3: Comparing the effect of supertags and POS
tags
in Table 3, while the POS tags help (0.65 BLEU
points, or 1.7% relative increase over the baseline),
they clearly underperform compared to the supertag
model (by 3.2%).
The Usefulness of a Supertagged LM In these
experiments we study the effect of the two added
feature (cost) functions: supertagged translation and
language models. We compare the baseline system
to the supertags system with the supertag phrase-
table probability but without the supertag LM. Ta-
ble 4 lists the baseline system (Base-SMALL), the
LTAG system without supertagged language model
(LTAG-TM-ONLY) and the LTAG-SMALL sys-
tem with both supertagged translation and language
models. The results presented in Table 4 indi-
System BLEU Score
Base-SMALL 0.4008
LTAG-TM-ONLY 0.4146
LTAG-SMALL .0.4205
Table 4: The effect of supertagged components
cate that the improvement is a shared contribution
between the supertagged translation and language
models: adding the LTAG TM improves BLEU
score by 1.38 points (3.4% relative) over the base-
line, with the LTAG LM improving BLEU score by
293
a further 0.59 points (a further 1.4% increase).
5.2 Scalability: Larger Training Corpora
Outperforming a PBSMT system on small amounts
of training data is less impressive than doing so on
really large sets. The issue here is scalability as well
as whether the PBSMT system is able to bridge the
performance gap with the supertagged system when
reasonably large sizes of training data are used. To
this end, we trained the systems on 2 million sen-
tences of parallel data, deploying LTAG supertags
and CCG supertags. Table 5 presents the compari-
son between these systems and the baseline trained
on the same data. The LTAG system improves by
1.17 BLEU points (2.6% relative), but the CCG sys-
tem gives an even larger increase: 1.91 BLEU points
(4.3% relative). While this is slightly lower than
the 4.9% relative improvement with the smaller data
sets, the sustained increase is probably due to ob-
serving more data with different supertag contexts,
which enables the model to select better target lan-
guage phrases.
System BLEU Score
Base-LARGE 0.4418
LTAG-LARGE 0.4535
CCG-LARGE 0.4609
Table 5: The effect of more training data
Adding a grammaticality factor As described in
section 4.3, we integrate an impoverished grammat-
icality factor based on two standard CCG combi-
nation operations, namely Forward and Backward
Application. Table 6 compares the results of the
baseline, the CCG with an n-gram LM-only system
(CCG-LARGE) and CCG-LARGE with this ?gram-
maticalized? LM system (CCG-LARGE-GRAM).
We see that bringing the grammaticality tests to
bear onto the supertagged system gives a further im-
provement of 0.79 BLEU points, a 1.7% relative
increase, culminating in an overall increase of 2.7
BLEU points, or a 6.1% relative improvement over
the baseline system.
5.3 Discussion
A natural question to ask is whether LTAG and CCG
supertags are playing similar (overlapping, or con-
System BLEU Score
Base-LARGE 0.4418
CCG-LARGE 0.4609
CCG-LARGE-GRAM 0.4688
Table 6: Comparing the effect of CCG-GRAM
flicting) roles in practice. Using an oracle to choose
the best output of the two systems gives a BLEU
score of 0.441, indicating that the combination pro-
vides significant room for improvement (cf. Ta-
ble 2). However, our efforts to build a system that
benefits from the combination using a simple log-
linear combination of the two models did not give
any significant performance change relative to the
baseline CCG system. Obviously, more informed
ways of combining the two could result in better per-
formance than a simple log-linear interpolation of
the components.
Figure 3 shows some example system output.
While the baseline system omits the verb giving ?the
authorities that it had...?, both the LTAG and CCG
found a formulation ?authorities reported that? with
a closer meaning to the reference translation ?The
authorities said that?. Omitting verbs turns out to
be a problem for the baseline system when trans-
lating the notorious verbless Arabic sentences (see
Figure 4). The supertagged systems have a more
grammatically strict language model than a standard
word-level Markov model, thereby exhibiting a pref-
erence (in the CCG system especially) for the inser-
tion of a verb with a similar meaning to that con-
tained in the reference sentence.
6 Conclusions
SMT practitioners have on the whole found it dif-
ficult to integrate syntax into their systems. In this
work, we have presented a novel model of PBSMT
which integrates supertags into the target language
model and the target side of the translation model.
Using LTAG supertags gives the best improve-
ment over a state-of-the-art PBSMT system for a
smaller data set, while CCG supertags work best on
a large 2 million-sentence pair training set. Adding
grammaticality factors based on algebraic composi-
tional operators gives the best result, namely 0.4688
BLEU, or a 6.1% relative increase over the baseline.
294
Reference: The authorities said he was allowed to contact family members by phone from the armored vehicle he was in.
Baseline: the authorities that it had allowed him to communicate by phone with his family of the armored car where
LTAG: authorities reported that it had allowed him to contact by telephone with his family of armored car where
CCG: authorities reported that it had enabled him to communicate by phone his family members of the armored car where
Figure 3: Sample output from different systems
Source: wmn AlmErwf An Al$Eb AlSyny mHb llslAm . Ref: It is well known that the Chinese people are peace loving .
Baseline: It is known that the Chinese people a peace-loving .
LTAG: It is known that the Chinese people a peace loving . CCG: It is known that the Chinese people are peace loving .
Figure 4: Verbless Arabic sentence and sample output from different systems
This result compares favourably with the best sys-
tems on the NIST 2005 Arabic?English task. We
expect more work on system integration to improve
results still further, and anticipate that similar in-
creases are to be seen for other language pairs.
Acknowledgements
We would like to thank Srinivas Bangalore and
the anonymous reviewers for useful comments on
earlier versions of this paper. This work is par-
tially funded by Science Foundation Ireland Princi-
pal Investigator Award 05/IN/1732, and Netherlands
Organization for Scientific Research (NWO) VIDI
Award.
References
S. Bangalore and A. Joshi, ?Supertagging: An Ap-
proach to Almost Parsing?, Computational Linguistics
25(2):237?265, 1999.
J. Chen, S. Bangalore, and K. Vijay-Shanker, ?Au-
tomated extraction of tree-adjoining grammars
from treebanks?. Natural Language Engineering,
12(3):251?299, 2006.
D. Chiang, ?A Hierarchical Phrase-Based Model for Sta-
tistical Machine Translation?, in Proceedings of ACL
2005, Ann Arbor, MI., pp.263?270, 2005.
S. Clark and J. Curran, ?The Importance of Supertagging
for Wide-Coverage CCG Parsing?, in Proceedings of
COLING-04, Geneva, Switzerland, pp.282?288, 2004.
J. Hockenmaier, Data and Models for Statistical Parsing
with Combinatory Categorial Grammar, PhD thesis,
University of Edinburgh, UK, 2003.
A. Joshi and Y. Schabes, ?Tree Adjoining Grammars and
Lexicalized Grammars? in M. Nivat and A. Podelski
(eds.) Tree Automata and Languages, Amsterdam, The
Netherlands: North-Holland, pp.409?431, 1992.
P. Koehn, ?Pharaoh: A Beam Search Decoder for phrase-
based Statistical Machine TranslationModels?, in Pro-
ceedings of AMTA-04, Berlin/Heidelberg, Germany:
Springer Verlag, pp.115?124, 2004.
P. Koehn, F. Och, and D. Marcu, ?Statistical Phrase-
Based Translation?, in Proceedings of HLT-NAACL
2003, Edmonton, Canada, pp.127?133, 2003.
D. Marcu, W. Wang, A. Echihabi and K. Knight, ?SPMT:
Statistical Machine Translation with Syntactified Tar-
get Language Phrases?, in Proceedings of EMNLP,
Sydney, Australia, pp.44?52, 2006.
D. Marcu andW.Wong, ?A Phrase-Based, Joint Probabil-
ity Model for Statistical Machine Translation?, in Pro-
ceedings of EMNLP, Philadelphia, PA., pp.133?139,
2002.
F. Och, ?Minimum Error Rate Training in Statistical Ma-
chine Translation?, in Proceedings of ACL 2003, Sap-
poro, Japan, pp.160?167, 2003.
F. Och and H. Ney, ?A Systematic Comparison of Var-
ious Statistical Alignment Models?, Computational
Linguistics 29:19?51, 2003.
K. Papineni, S. Roukos, T. Ward and W-J. Zhu, ?BLEU:
A Method for Automatic Evaluation of Machine
Translation?, in Proceedings of ACL 2002, Philadel-
phia, PA., pp.311?318, 2002.
L. Rabiner, ?A Tutorial on Hidden Markov Models and
Selected Applications in Speech Recognition?, in A.
Waibel & F-K. Lee (eds.) Readings in Speech Recog-
nition, San Mateo, CA.: Morgan Kaufmann, pp.267?
296, 1990.
M. Steedman, The Syntactic Process. Cambridge, MA:
The MIT Press, 2000.
C. Tillmann and F. Xia, ?A Phrase-based Unigram Model
for Statistical Machine Translation?, in Proceedings of
HLT-NAACL 2003, Edmonton, Canada. pp.106?108,
2003.
295
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304?311,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bootstrapping Word Alignment via Word Packing
Yanjun Ma, Nicolas Stroppa, Andy Way
School of Computing
Dublin City University
Glasnevin, Dublin 9, Ireland
{yma,nstroppa,away}@computing.dcu.ie
Abstract
We introduce a simple method to pack words
for statistical word alignment. Our goal is to
simplify the task of automatic word align-
ment by packing several consecutive words
together when we believe they correspond
to a single word in the opposite language.
This is done using the word aligner itself,
i.e. by bootstrapping on its output. We
evaluate the performance of our approach
on a Chinese-to-English machine translation
task, and report a 12.2% relative increase in
BLEU score over a state-of-the art phrase-
based SMT system.
1 Introduction
Automatic word alignment can be defined as the
problem of determining a translational correspon-
dence at word level given a parallel corpus of aligned
sentences. Most current statistical models (Brown
et al, 1993; Vogel et al, 1996; Deng and Byrne,
2005) treat the aligned sentences in the corpus as se-
quences of tokens that are meant to be words; the
goal of the alignment process is to find links be-
tween source and target words. Before applying
such aligners, we thus need to segment the sentences
into words ? a task which can be quite hard for lan-
guages such as Chinese for which word boundaries
are not orthographically marked. More importantly,
however, this segmentation is often performed in a
monolingual context, which makes the word align-
ment task more difficult since different languages
may realize the same concept using varying num-
bers of words (see e.g. (Wu, 1997)). Moreover, a
segmentation considered to be ?good? from a mono-
lingual point of view may be unadapted for training
alignment models.
Although some statistical alignment models al-
low for 1-to-n word alignments for those reasons,
they rarely question the monolingual tokenization
and the basic unit of the alignment process remains
the word. In this paper, we focus on 1-to-n align-
ments with the goal of simplifying the task of auto-
matic word aligners by packing several consecutive
words together when we believe they correspond to a
single word in the opposite language; by identifying
enough such cases, we reduce the number of 1-to-n
alignments, thus making the task of word alignment
both easier and more natural.
Our approach consists of using the output from
an existing statistical word aligner to obtain a set of
candidates for word packing. We evaluate the re-
liability of these candidates, using simple metrics
based on co-occurence frequencies, similar to those
used in associative approaches to word alignment
(Kitamura and Matsumoto, 1996; Melamed, 2000;
Tiedemann, 2003). We then modify the segmenta-
tion of the sentences in the parallel corpus accord-
ing to this packing of words; these modified sen-
tences are then given back to the word aligner, which
produces new alignments. We evaluate the validity
of our approach by measuring the influence of the
alignment process on a Chinese-to-English Machine
Translation (MT) task.
The remainder of this paper is organized as fol-
lows. In Section 2, we study the case of 1-to-
n word alignment. Section 3 introduces an auto-
matic method to pack together groups of consecutive
304
1: 0 1: 1 1: 2 1: 3 1:n (n > 3)
IWSLT Chinese?English 21.64 63.76 9.49 3.36 1.75
IWSLT English?Chinese 29.77 57.47 10.03 1.65 1.08
IWSLT Italian?English 13.71 72.87 9.77 3.23 0.42
IWSLT English?Italian 20.45 71.08 7.02 0.9 0.55
Europarl Dutch?English 24.71 67.04 5.35 1.4 1.5
Europarl English?Dutch 23.76 69.07 4.85 1.2 1.12
Table 1: Distribution of alignment types for different language pairs (%)
words based on the output from a word aligner. In
Section 4, the experimental setting is described. In
Section 5, we evaluate the influence of our method
on the alignment process on a Chinese to English
MT task, and experimental results are presented.
Section 6 concludes the paper and gives avenues for
future work.
2 The Case of 1-to-n Alignment
The same concept can be expressed in different lan-
guages using varying numbers of words; for exam-
ple, a single Chinese word may surface as a com-
pound or a collocation in English. This is fre-
quent for languages as different as Chinese and En-
glish. To quickly (and approximately) evaluate this
phenomenon, we trained the statistical IBM word-
alignment model 4 (Brown et al, 1993),1 using the
GIZA++ software (Och and Ney, 2003) for the fol-
lowing language pairs: Chinese?English, Italian?
English, and Dutch?English, using the IWSLT-2006
corpus (Takezawa et al, 2002; Paul, 2006) for the
first two language pairs, and the Europarl corpus
(Koehn, 2005) for the last one. These asymmet-
ric models produce 1-to-n alignments, with n ? 0,
in both directions. Here, it is important to mention
that the segmentation of sentences is performed to-
tally independently of the bilingual alignment pro-
cess, i.e. it is done in a monolingual context. For Eu-
ropean languages, we apply the maximum-entropy
based tokenizer of OpenNLP2; the Chinese sen-
tences were human segmented (Paul, 2006).
In Table 1, we report the frequencies of the dif-
ferent types of alignments for the various languages
and directions. As expected, the number of 1:n
1More specifically, we performed 5 iterations of Model 1, 5
iterations of HMM, 5 iterations of Model 3, and 5 iterations of
Model 4.
2http://opennlp.sourceforge.net/.
alignments with n 6= 1 is high for Chinese?English
(' 40%), and significantly higher than for the Eu-
ropean languages. The case of 1-to-n alignments is,
therefore, obviously an important issue when deal-
ing with Chinese?English word alignment.3
2.1 The Treatment of 1-to-n Alignments
Fertility-based models such as IBM models 3, 4, and
5 allow for alignments between one word and sev-
eral words (1-to-n or 1:n alignments in what fol-
lows), in particular for the reasons specified above.
They can be seen as extensions of the simpler IBM
models 1 and 2 (Brown et al, 1993). Similarly,
Deng and Byrne (2005) propose an HMM frame-
work capable of dealing with 1-to-n alignment,
which is an extension of the original model of (Vogel
et al, 1996).
However, these models rarely question the mono-
lingual tokenization, i.e. the basic unit of the align-
ment process is the word.4 One alternative to ex-
tending the expressivity of one model (and usually
its complexity) is to focus on the input representa-
tion; in particular, we argue that the alignment pro-
cess can benefit from a simplification of the input,
which consists of trying to reduce the number of
1-to-n alignments to consider. Note that the need
to consider segmentation and alignment at the same
time is also mentioned in (Tiedemann, 2003), and
related issues are reported in (Wu, 1997).
2.2 Notation
While in this paper, we focus on Chinese?English,
the method proposed is applicable to any language
3Note that a 1: 0 alignment may denote a failure to capture
a 1:n alignment with n > 1.
4Interestingly, this is actually even the case for approaches
that directly model alignments between phrases (Marcu and
Wong, 2002; Birch et al, 2006).
305
pair ? even for closely related languages, we ex-
pect improvements to be seen. The notation how-
ever assume Chinese?English MT. Given a Chi-
nese sentence cJ1 consisting of J words {c1, . . . , cJ}
and an English sentence eI1 consisting of I words
{e1, . . . , eI}, AC?E (resp. AE?C) will denote a
Chinese-to-English (resp. an English-to-Chinese)
word alignment between cJ1 and eI1. Since we are
primarily interested in 1-to-n alignments, AC?E
can be represented as a set of pairs aj = ?cj , Ej?
denoting a link between one single Chinese word
cj and a few English words Ej (and similarly for
AE?C). The set Ej is empty if the word cj is not
aligned to any word in eI1.
3 Automatic Word Repacking
Our approach consists of packing consecutive words
together when we believe they correspond to a sin-
gle word in the other language. This bilingually
motivated packing of words changes the basic unit
of the alignment process, and simplifies the task of
automatic word alignment. We thus minimize the
number of 1-to-n alignments in order to obtain more
comparable segmentations in the two languages. In
this section, we present an automatic method that
builds upon the output from an existing automatic
word aligner. More specifically, we (i) use a word
aligner to obtain 1-to-n alignments, (ii) extract can-
didates for word packing, (iii) estimate the reliability
of these candidates, (iv) replace the groups of words
to pack by a single token in the parallel corpus, and
(v) re-iterate the alignment process using the up-
dated corpus. The first three steps are performed
in both directions, and produce two bilingual dic-
tionaries (source-target and target-source) of groups
of words to pack.
3.1 Candidate Extraction
In the following, we assume the availability of an
automatic word aligner that can output alignments
AC?E and AE?C for any sentence pair (cJ1 , eI1)
in a parallel corpus. We also assume that AC?E
and AE?C contain 1:n alignments. Our method for
repacking words is very simple: whenever a single
word is aligned with several consecutive words, they
are considered candidates for repacking. Formally,
given an alignment AC?E between cJ1 and eI1, if
aj = ?cj , Ej? ? AC?E , with Ej = {ej1 , . . . , ejm}
and ?k ? J1,m? 1K, jk+1 ? jk = 1, then the align-
ment aj between cj and the sequence of words Ej
is considered a candidate for word repacking. The
same goes for AE?C . Some examples of such 1-
to-n alignments between Chinese and English (in
both directions) we can derive automatically are dis-
played in Figure 1.
????: white wine
????: department store
??: excuse me
??: call the police
?: cup of
??: have to
closest: ? ?
fifteen: ? ?
fine: ? ?
flight: ? ?? 
get: ? ?
here:  ? ??
Figure 1: Example of 1-to-n word alignments be-
tween Chinese and English
3.2 Candidate Reliability Estimation
Of course, the process described above is error-
prone and if we want to change the input to give to
the word aligner, we need to make sure that we are
not making harmful modifications.5 We thus addi-
tionally evaluate the reliability of the candidates we
extract and filter them before inclusion in our bilin-
gual dictionary. To perform this filtering, we use
two simple statistical measures. In the following,
aj = ?cj , Ej? denotes a candidate.
The first measure we consider is co-occurrence
frequency (COOC(cj , Ej)), i.e. the number of
times cj and Ej co-occur in the bilingual corpus.
This very simple measure is frequently used in as-
sociative approaches (Melamed, 1997; Tiedemann,
2003). The second measure is the alignment confi-
dence, defined as
AC(aj) =
C(aj)
COOC(cj , Ej)
,
where C(aj) denotes the number of alignments pro-
posed by the word aligner that are identical to aj .
In other words, AC(aj) measures how often the
5Consequently, if we compare our approach to the problem
of collocation identification, we may say that we are more in-
terested in precision than recall (Smadja et al, 1996). However,
note that our goal is not recognizing specific sequences of words
such as compounds or collocations; it is making (bilingually
motivated) changes that simplify the alignment process.
306
aligner aligns cj and Ej when they co-occur. We
also impose that |Ej | ? k, where k is a fixed inte-
ger that may depend on the language pair (between
3 and 5 in practice). The rationale behind this is that
it is very rare to get reliable alignment between one
word and k consecutive words when k is high.
The candidates are included in our bilingual dic-
tionary if and only if their measures are above some
fixed thresholds tcooc and tac, which allow for the
control of the size of the dictionary and the quality
of its contents. Some other measures (including the
Dice coefficient) could be considered; however, it
has to be noted that we are more interested here in
the filtering than in the discovery of alignment, since
our method builds upon an existing aligner. More-
over, we will see that even these simple measures
can lead to an improvement of the alignment pro-
cess in a MT context (cf. Section 5).
3.3 Bootstrapped Word Repacking
Once the candidates are extracted, we repack the
words in the bilingual dictionaries constructed using
the method described above; this provides us with
an updated training corpus, in which some word se-
quences have been replaced by a single token. This
update is totally naive: if an entry aj = ?cj , Ej? is
present in the dictionary and matches one sentence
pair (cJ1 , eI1) (i.e. cj and Ej are respectively con-
tained in cJ1 and eI1), then we replace the sequence
of words Ej with a single token which becomes a
new lexical unit.6 Note that this replacement occurs
even if no alignment was found between cj and Ej
for the pair (cJ1 , eI1). This is motivated by the fact
that the filtering described above is quite conserva-
tive; we trust the entry ai to be correct. This update
is performed in both directions. It is then possible to
run the word aligner using the updated (simplified)
parallel corpus, in order to get new alignments. By
performing a deterministic word packing, we avoid
the computation of the fertility parameters associ-
ated with fertility-based models.
Word packing can be applied several times: once
we have grouped some words together, they become
the new basic unit to consider, and we can re-run
the same method to get additional groupings. How-
6In case of overlap between several groups of words to re-
place, we select the one with highest confidence (according to
tac).
ever, we have not seen in practice much benefit from
running it more than twice (few new candidates are
extracted after two iterations).
It is also important to note that this process is
bilingually motivated and strongly depends on the
language pair. For example, white wine, excuse me,
call the police, and cup of (cf. Figure 1) translate re-
spectively as vin blanc, excusez-moi, appellez la po-
lice, and tasse de in French. Those groupings would
not be found for a language pair such as French?
English, which is consistent with the fact that they
are less useful for French?English than for Chinese?
English in a MT perspective.
3.4 Using Manually Developed Dictionaries
We wanted to compare this automatic approach to
manually developed resources. For this purpose,
we used a dictionary built by the MT group of
Harbin Institute of Technology, as a preprocessing
step to Chinese?English word alignment, and moti-
vated by several years of Chinese?English MT prac-
tice. Some examples extracted from this resource
are displayed in Figure 2.
?: whiti en 
??:?dp aw wr
??: aiim arw
??: ea str aw rs
?: pn nrra  pn
?: orrx pw
Figure 2: Examples of entries from the manually de-
veloped dictionary
4 Experimental Setting
4.1 Evaluation
The intrinsic quality of word alignment can be as-
sessed using the Alignment Error Rate (AER) met-
ric (Och and Ney, 2003), that compares a system?s
alignment output to a set of gold-standard align-
ment. While this method gives a direct evaluation of
the quality of word alignment, it is faced with sev-
eral limitations. First, it is really difficult to build
a reliable and objective gold-standard set, especially
for languages as different as Chinese and English.
Second, an increase in AER does not necessarily im-
ply an improvement in translation quality (Liang et
al., 2006) and vice-versa (Vilar et al, 2006). The
307
relationship between word alignments and their im-
pact on MT is also investigated in (Ayan and Dorr,
2006; Lopez and Resnik, 2006; Fraser and Marcu,
2006). Consequently, we chose to extrinsically eval-
uate the performance of our approach via the transla-
tion task, i.e. we measure the influence of the align-
ment process on the final translation output. The
quality of the translation output is evaluated using
BLEU (Papineni et al, 2002).
4.2 Data
The experiments were carried out using the
Chinese?English datasets provided within the
IWSLT 2006 evaluation campaign (Paul, 2006), ex-
tracted from the Basic Travel Expression Corpus
(BTEC) (Takezawa et al, 2002). This multilingual
speech corpus contains sentences similar to those
that are usually found in phrase-books for tourists
going abroad. Training was performed using the de-
fault training set, to which we added the sets de-
vset1, devset2, and devset3.7 The English side of
the test set was not available at the time we con-
ducted our experiments, so we split the development
set (devset 4) into two parts: one was kept for testing
(200 aligned sentences) with the rest (289 aligned
sentences) used for development purposes.
As a pre-processing step, the English sentences
were tokenized using the maximum-entropy based
tokenizer of the OpenNLP toolkit, and case infor-
mation was removed. For Chinese, the data pro-
vided were tokenized according to the output format
of ASR systems, and human-corrected (Paul, 2006).
Since segmentations are human-corrected, we are
sure that they are good from a monolingual point of
view. Table 2 contains the various corpus statistics.
4.3 Baseline
We use a standard log-linear phrase-based statistical
machine translation system as a baseline: GIZA++
implementation of IBM word alignment model 4
(Brown et al, 1993; Och and Ney, 2003),8 the re-
finement and phrase-extraction heuristics described
in (Koehn et al, 2003), minimum-error-rate training
7More specifically, we choose the first English reference
from the 7 references and the Chinese sentence to construct new
sentence pairs.
8Training is performed using the same number of iterations
as in Section 2.
Chinese English
Train Sentences 41,465
Running words 361,780 375,938
Vocabulary size 11,427 9,851
Dev. Sentences 289 (7 refs.)
Running words 3,350 26,223
Vocabulary size 897 1,331
Eval. Sentences 200 (7 refs.)
Running words 1,864 14,437
Vocabulary size 569 1,081
Table 2: Chinese?English corpus statistics
(Och, 2003) using Phramer (Olteanu et al, 2006),
a 3-gram language model with Kneser-Ney smooth-
ing trained with SRILM (Stolcke, 2002) on the En-
glish side of the training data and Pharaoh (Koehn,
2004) with default settings to decode. The log-linear
model is also based on standard features: condi-
tional probabilities and lexical smoothing of phrases
in both directions, and phrase penalty (Zens and
Ney, 2004).
5 Experimental Results
5.1 Results
The initial word alignments are obtained using the
baseline configuration described above. From these,
we build two bilingual 1-to-n dictionaries (one for
each direction), and the training corpus is updated
by repacking the words in the dictionaries, using the
method presented in Section 2. As previously men-
tioned, this process can be repeated several times; at
each step, we can also choose to exploit only one of
the two available dictionaries, if so desired. We then
extract aligned phrases using the same procedure as
for the baseline system; the only difference is the ba-
sic unit we are considering. Once the phrases are ex-
tracted, we perform the estimation of the features of
the log-linear model and unpack the grouped words
to recover the initial words. Finally, minimum-error-
rate training and decoding are performed.
The various parameters of the method (k, tcooc,
tac, cf. Section 2) have been optimized on the devel-
opment set. We found out that it was enough to per-
form two iterations of repacking: the optimal set of
values was found to be k = 3, tac = 0.5, tcooc = 20
for the first iteration, and tcooc = 10 for the second
308
BLEU[%]
Baseline 15.14
n=1. with C-E dict. 15.92
n=1. with E-C dict. 15.77
n=1. with both 16.59
n=2. with C-E dict. 16.99
n=2. with E-C dict. 16.59
n=2. with both 16.88
Table 3: Influence of word repacking on Chinese-to-
English MT
iteration, for both directions.9 In Table 3, we report
the results obtained on the test set, where n denotes
the iteration. We first considered the inclusion of
only the Chinese?English dictionary, then only the
English?Chinese dictionary, and then both.
After the first step, we can already see an im-
provement over the baseline when considering one
of the two dictionaries. When using both, we ob-
serve an increase of 1.45 BLEU points, which cor-
responds to a 9.6% relative increase. Moreover, we
can gain from performing another step. However,
the inclusion of the English?Chinese dictionary is
harmful in this case, probably because 1-to-n align-
ments are less frequent for this direction, and have
been captured during the first step. By including the
Chinese?English dictionary only, we can achieve an
increase of 1.85 absolute BLEU points (12.2% rela-
tive) over the initial baseline.10
Quality of the Dictionaries To assess the qual-
ity of the extraction procedure, we simply manu-
ally evaluated the ratio of incorrect entries in the
dictionaries. After one step of word packing, the
Chinese?English and the English?Chinese dictio-
naries respectively contain 7.4% and 13.5% incor-
rect entries. After two steps of packing, they only
contain 5.9% and 10.3% incorrect entries.
5.2 Alignment Types
Intuitively, the word alignments obtained after word
packing are more likely to be 1-to-1 than before. In-
9The parameters k, tac, and tcooc are optimized for each
step, and the alignment obtained using the best set of parameters
for a given step are used as input for the following step.
10Note that this setting (using both dictionaries for the first
step and only the Chinese dictionary for the second step) is also
the best setting on the development set.
deed, the word sequences in one language that usu-
ally align to one single word in the other language
have been grouped together to form one single to-
ken. Table 4 shows the detail of the distribution of
alignment types after one and two steps of automatic
repacking. In particular, we can observe that the 1: 1
1: 0 1: 1 1: 2 1: 3 1:n
(n > 3)
C-E Base. 21.64 63.76 9.49 3.36 1.75
n=1 19.69 69.43 6.32 2.79 1.78
n=2 19.67 71.57 4.87 2.12 1.76
E-C Base. 29.77 57.47 10.03 1.65 1.08
n=1 26.59 61.95 8.82 1.55 1.09
n=2 25.10 62.73 9.38 1.68 1.12
Table 4: Distribution of alignment types (%)
alignments are more frequent after the application
of repacking: the ratio of this type of alignment has
increased by 7.81% for Chinese?English and 5.26%
for English?Chinese.
5.3 Influence of Word Segmentation
To test the influence of the initial word segmenta-
tion on the process of word packing, we considered
an additional segmentation configuration, based on
an automatic segmenter combining rule-based and
statistical techniques (Zhao et al, 2001).
BLEU[%]
Original segmentation 15.14
Original segmentation + Word packing 16.99
Automatic segmentation 14.91
Automatic segmentation + Word packing 17.51
Table 5: Influence of Chinese segmentation
The results obtained are displayed in Table 5. As
expected, the automatic segmenter leads to slightly
lower results than the human-corrected segmenta-
tion. However, the proposed method seems to be
beneficial irrespective of the choice of segmentation.
Indeed, we can also observe an improvement in the
new setting: 2.6 points absolute increase in BLEU
(17.4% relative).11
11We could actually consider an extreme case, which would
consist of splitting the sentences into characters, i.e. each char-
acter would be blindly treated as one word. The segmentation
309
5.4 Exploiting Manually Developed Resources
We also compared our technique for automatic pack-
ing of words with the exploitation of manually
developed resources. More specifically, we used
a 1-to-n Chinese?English bilingual dictionary, de-
scribed in Section 3.4, and used it in place of the
automatically acquired dictionary. Words are thus
grouped according to this dictionary, and we then
apply the same word aligner as for previous experi-
ments. In this case, since we are not bootstrapping
from the output of a word aligner, this can actually
be seen as a pre-processing step prior to alignment.
These resources follow more or less the same for-
mat as the output of the word segmenter mentioned
in Section 5.1.2 (Zhao et al, 2001), so the experi-
ments are carried out using this segmentation.
BLEU[%]
Baseline 14.91
Automatic word packing 17.51
Packing with ?manual? dictionary 16.15
Table 6: Exploiting manually developed resources
The results obtained are displayed in Table 6.We
can observe that the use of the manually developed
dictionary provides us with an improvement in trans-
lation quality: 1.24 BLEU points absolute (8.3% rel-
ative). However, there does not seem to be a clear
gain when compared with the automatic method.
Even if those manual resources were extended, we
do not believe the improvement is sufficient enough
to justify this additional effort.
6 Conclusion and Future Work
In this paper, we have introduced a simple yet effec-
tive method to pack words together in order to give
a different and simplified input to automatic word
aligners. We use a bootstrap approach in which we
first extract 1-to-n word alignments using an exist-
ing word aligner, and then estimate the confidence
of those alignments to decide whether or not the n
words have to be grouped; if so, this group is con-
would thus be completely driven by the bilingual alignment pro-
cess (see also (Wu, 1997; Tiedemann, 2003) for related consid-
erations). In this case, our approach would be similar to the
approach of (Xu et al, 2004), except for the estimation of can-
didates.
sidered a new basic unit to consider. We can finally
re-apply the word aligner to the updated sentences.
We have evaluated the performance of our ap-
proach by measuring the influence of this process
on a Chinese-to-English MT task, based on the
IWSLT 2006 evaluation campaign. We report a
12.2% relative increase in BLEU score over a stan-
dard phrase-based SMT system. We have verified
that this process actually reduces the number of 1:n
alignments with n 6= 1, and that it is rather indepen-
dent from the (Chinese) segmentation strategy.
As for future work, we first plan to consider dif-
ferent confidence measures for the filtering of the
alignment candidates. We also want to bootstrap on
different word aligners; in particular, one possibility
is to use the flexible HMM word-to-phrase model of
Deng and Byrne (2005) in place of IBM model 4.
Finally, we would like to apply this method to other
corpora and language pairs.
Acknowledgment
This work is supported by Science Foundation Ire-
land (grant number OS/IN/1732). Prof. Tiejun Zhao
and Dr. Muyun Yang from the MT group of Harbin
Institute of Technology, and Yajuan Lv from the In-
stitute of Computing Technology, Chinese Academy
of Sciences, are kindly acknowledged for provid-
ing us with the Chinese segmenter and the manually
developed bilingual dictionary used in our experi-
ments.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond aer: An extensive analysis of word alignments
and their impact on mt. In Proceedings of COLING-
ACL 2006, pages 9?16, Sydney, Australia.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proceed-
ings of AMTA 2006, pages 10?18, Boston, MA.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Yonggang Deng and William Byrne. 2005. HMM word
and phrase alignment for statistical machine transla-
tion. In Proceedings of HLT-EMNLP 2005, pages
169?176, Vancouver, Canada.
310
Alexander Fraser and Daniel Marcu. 2006. Measuring
word alignment quality for statistical machine transla-
tion. Technical Report ISI-TR-616, ISI/University of
Southern California.
Mihoko Kitamura and Yuji Matsumoto. 1996. Auto-
matic extraction of word sequence correspondences in
parallel corpora. In Proceedings of the 4th Workshop
on Very Large Corpora, pages 79?87, Copenhagen,
Denmark.
Philip Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 48?54, Edmonton, Canada.
Philip Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004, pages 115?124,
Washington, District of Columbia.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL
2006, pages 104?111, New York, NY.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What?s the link?
In Proceedings of AMTA 2006, pages 90?99, Cam-
bridge, MA.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP 2002, pages 133?139,
Morristown, NJ.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of EMNLP 1997, pages 97?108, Somerset,
New Jersey.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL 2003,
pages 160?167, Sapporo, Japan.
Marian Olteanu, Chris Davis, Ionut Volosen, and Dan
Moldovan. 2006. Phramer - an open source statis-
tical phrase-based translator. In Proceedings of the
NAACL 2006 Workshop on Statistical Machine Trans-
lation, pages 146?149, New York, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002, pages 311?318, Philadelphia, PA.
Michael Paul. 2006. Overview of the IWSLT 2006 Eval-
uation Campaign. In Proceedings of IWSLT 2006,
pages 1?15, Kyoto, Japan.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Computa-
tional Linguistics, 22(1):1?38.
Andrea Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proceedings of LREC 2002,
pages 147?152, Las Palmas, Spain.
Jo?rg Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of EACL 2003, pages 339?346,
Budapest, Hungary.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
AER: Do we need to ?improve? our alignments? In
Proceedings of IWSLT 2006, pages 205?212, Kyoto,
Japan.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING 1996, pages 836?
841, Copenhagen, Denmark.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need chinese word segmentation for statistical
machine translation? In Proceedings of the Third
SIGHAN Workshop on Chinese Language Learning,
pages 122?128, Barcelona, Spain.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proceedings of HLT-NAACL 2004, pages 257?264,
Boston, MA.
Tiejun Zhao, Yajuan Lu?, and Hao Yu. 2001. Increas-
ing accuracy of chinese segmentation with strategy of
multi-step processing. Journal of Chinese Information
Processing, 15(1):13?18.
311
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 183?190,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Hybrid Example-Based SMT: the Best of Both Worlds?
Declan Groves
School of Computing
Dublin City University
Dublin 9, Ireland
dgroves@computing.dcu.ie
Andy Way
School of Computing
Dublin City University
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
(Way and Gough, 2005) provide an in-
depth comparison of their Example-Based
Machine Translation (EBMT) system with
a Statistical Machine Translation (SMT)
system constructed from freely available
tools. According to a wide variety of au-
tomatic evaluation metrics, they demon-
strated that their EBMT system outper-
formed the SMT system by a factor of two
to one.
Nevertheless, they did not test their EBMT
system against a phrase-based SMT sys-
tem. Obtaining their training and test
data for English?French, we carry out a
number of experiments using the Pharaoh
SMT Decoder. While better results are
seen when Pharaoh is seeded with Giza++
word- and phrase-based data compared to
EBMT sub-sentential alignments, in gen-
eral better results are obtained when com-
binations of this ?hybrid? data is used
to construct the translation and probabil-
ity models. While for the most part the
EBMT system of (Gough & Way, 2004b)
outperforms any flavour of the phrase-
based SMT systems constructed in our
experiments, combining the data sets au-
tomatically induced by both Giza++ and
their EBMT system leads to a hybrid sys-
tem which improves on the EBMT system
per se for French?English.
1 Introduction
(Way and Gough, 2005) provide what are to our
knowledge the first published results comparing
Example-Based and Statistical models of Machine
Translation (MT). Given that most MT research car-
ried out today is corpus-based, it is somewhat sur-
prising that until quite recently no qualitative re-
search existed on the relative performance of the two
approaches. This may be due to a number of factors:
the relative unavailability of EBMT systems, the
lack of participation of EBMT researchers in com-
petitive evaluations or the dominance in the MT re-
search community of the SMT approach?whenever
one paradigm finds favour with the clear majority of
MT practitioners, the assumption made by most of
the community is that this way of doing things is
clearly better than the alternatives.
Like (Way and Gough, 2005), we find this regret-
table: the only basis on which such views should
be allowed to permeate our field is following exten-
sive testing and evaluation. Nonetheless, given that
no EBMT systems are freely available, very few re-
search groups are in the position of being able to
carry out such work.
This paper extends the work of (Way and Gough,
2005) by testing EBMT against phrase-based mod-
els of SMT, rather than the word-based models used
in this previous work. In so doing, it provides a
more complete evaluation of the main question at
hand, namely whether an SMT system outperforms
an EBMT system on reasonably large training and
test sets.
We obtained the same training and test data used
183
in (Way and Gough, 2005), and evaluated a num-
ber of SMT systems which use the Pharaoh decoder1
against the Marker-Based EBMT system of (Gough
& Way, 2004b), for French?English and English?
French. We provide results using a range of au-
tomatic evaluation metrics: BLEU (Papineni et al,
2002), Precision and Recall (Turian et al, 2003), and
Word- and Sentence Error Rates. (Way and Gough,
2005) observe that EBMT tends to outperform a
word-based SMT model, and our experiments show
that a number of different phrase-based SMT sys-
tems still tend to fall short of the quality obtained
via EBMT for these evaluation metrics. However,
when Pharaoh is seeded with the data sets automati-
cally induced by both Giza++ and their EBMT sys-
tem, better results are seen for French?English than
for the EBMT system per se.
The remainder of the paper is constructed as fol-
lows. In section 2, we summarize the main ideas be-
hind typical models of SMT and EBMT, as well as
the EBMT system of (Gough & Way, 2004b) used in
our experiments. In section 3, we revisit the exper-
iments and results carried out by (Way and Gough,
2005). In section 4, we describe our extensions to
their work, and compare their findings to ours, and
in section 5, present a number of hybrid SMT mod-
els. Finally, we conclude and offer some thoughts
for future work in section 6, and in section 7 present
some further comments on the narrowing gap be-
tween EBMT and phrase-based SMT.
2 Example-Based and Statistical Models of
Translation
A sine qua non for both EBMT and SMT is a set of
sentences in one language aligned with their trans-
lations in another. Although similar in that both
models of translation automatically induce transla-
tion knowledge from this resource, there are signifi-
cant differences regarding both the type of informa-
tion learnt and how this is brought to bear in dealing
with new input.
2.1 EBMT
Given a new input string, EBMT models use three
separate processes in order to derive translations:
1http://www.isi.edu/licensed-sw/pharaoh/
1. Searching the source side of the bitext for
?close? matches and their translations;
2. Determining the sub-sentential translation links
in those retrieved examples;
3. Recombining relevant parts of the target trans-
lation links to derive the translation.
Searching for the best matches involves determin-
ing a similarity metric based on word occurrences
and part-of-speech labels, generalised templates and
bilingual dictionaries. The recombination process
depends on the nature of the examples used in
the first place, which may include aligning phrase-
structure (sub-)trees (Hearne & Way, 2003) or de-
pendency trees (Watanabe et al, 2003), or using
placeables (Brown, 1999) as indicators of chunk
boundaries.
Another method?and the one used in the EBMT
system used in our experiments?is to use a set
of closed-class words to segment aligned source
and target sentences and to derive an additional set
of lexical and phrasal resources. (Gough & Way,
2004b) base their work on the ?Marker Hypothe-
sis? (Green, 1979), a universal psycholinguistic con-
straint which posits that languages are ?marked?
for syntactic structure at surface level by a closed
set of specific lexemes and morphemes. In a pre-
processing stage, (Gough & Way, 2004b) use 7 sets
of marker words for English and French (e.g. de-
terminers, quantifiers, conjunctions etc.), which to-
gether with cognate matches and mutual information
scores are used to derive three new data sources: sets
of marker chunks, generalised templates and a lexi-
con.
In order to describe this in more detail, we revisit
an example from (Gough & Way, 2004a), namely:
(1) each layer has a layer number =?chaque
couche a un nombre de la couche
From the sentence pair in (1), the strings in (2)
are generated, where marker words are automati-
cally tagged with their marker categories:
184
(2) <QUANT> each layer has <DET> a
layer number =?<QUANT> chaque
couche a <DET> un nombre <PREP>
de la couche
Taking into account marker tag information (label,
and relative sentence position), and lexical similar-
ity, the marker chunks in (3) are automatically gen-
erated from the marker-tagged strings in (2):
(3) a. <QUANT> each layer has: <QUANT>
chaque couche a
b. <DET> a layer number: <DET> un
nombre de la couche
(3b) shows that n:m alignments are possible (the two
French marker chunks un nombre and de la couche
are absorbed into one following the lexical similari-
ties between layer and couche and number and nom-
bre, respectively) given the sub-sentential alignment
algorithm of (Gough & Way, 2004b).
By generalising over the marker lexicon, a set
of marker templates is produced by replacing the
marker word by its relevant tag. From the examples
in (3), the generalised templates in (4) are derived:
(4) a. <QUANT> layer has: <QUANT>
couche a
b. <DET> layer number: <DET> nombre
de la couche
These templates increase the robustness of the sys-
tem and make the matching process more flexible.
Now any marker word can be inserted after the rele-
vant tag if it appears with its translation in the lexi-
con, so that (say) the layer number can now be han-
dled by the generalised template in (4b) and insert-
ing a (or all) translation(s) for the in the system?s
lexicon.
2.2 Word- and Phrase-Based SMT
SMT systems require two large probability tables in
order to generate translations of new input:
1. a translation model induced from a large
amount of bilingual data;
2. a target language model induced from a(n even)
large(r) quantity of separate monolingual text.
Essentially, the translation model establishes the
set of target language words (and more recently,
phrases) which are most likely to be useful in trans-
lating the source string, while the language model
tries to assemble these words (and phrases) in the
most likely target word order. The language model
is trained by determining all bigram and/or trigram
frequency distributions occurring in the training
data, while the translation model takes into account
source and target word (and phrase) co-occurrence
frequencies, sentence lengths and the relative sen-
tence positions of source and target words.
Until quite recently, SMT models of translation
were based on the simple word alignment models
of (Brown et al, 1990). Nowadays, however, SMT
practitioners also get their systems to learn phrasal
as well as lexical alignments (e.g. (Koehn et al,
2003); (Och, 2003)). Unsurprisingly, the quality
obtained by today?s phrase-based SMT systems is
considerably better than that obtained by the poorer
word-based models.
3 Comparing EBMT and Word-Based
SMT
(Way and Gough, 2005) obtained a large translation
memory from Sun Microsystems containing 207,468
English?French sentence pairs, of which 3,939 sen-
tence pairs were randomly extracted as a test set,
with the remaining 203,529 sentences used as train-
ing data. The average sentence length for the En-
glish test set was 13.1 words and 15.2 words for the
corresponding French test set. The EBMT system
used was their Marker-based system as described in
section 2.1 above. In order to create the necessary
SMT language and translation models, they used:
? Giza++ (Och & Ney, 2003);2
? the CMU-Cambridge statistical toolkit;3
? the ISI ReWrite Decoder.4
Translation was performed from English?French
and French?English, and the resulting translations
were evaluated using a range of automatic metrics:
BLEU (Papineni et al, 2002), Precision and Recall
2http://www.isi.edu/?och/Giza++.html
3http://mi.eng.cam.ac.uk/?prc14/toolkit.html
4http://www.isi.edu/licensed-sw/rewrite-decoder/
185
(Turian et al, 2003), and Word- and Sentence Error
Rates. In order to see whether the amount of train-
ing data affected the (relative) performance of the
EBMT and SMT systems, (Way and Gough, 2005)
split the training data into three sets, of 50K (1.1M
words), 100K (2.4M words) and 203K (4.8M words)
sentence pairs (TS1?TS3 in what follows).
3.1 English?French Results
Table 1: Comparing the EBMT system of (Gough &
Way, 2004b) with a Word-Based SMT (WB-SMT) system for
English?French.
BLEU Prec. Recall WER SER
TS1 WB-SMT .2971 .6739 .5912 54.9 90.8
EBMT .3318 .6525 .6183 54.3 89.2
TS2 WB-SMT .3375 .6824 .5962 51.1 89.9
EBMT .4534 .7355 .6983 44.8 77.5
TS3 WB-SMT .3223 .6513 .5704 53.5 89.1
EBMT .4409 .6727 .6877 52.4 65.6
The results obtained by (Gough & Way, 2004b)
for English?French for their EBMT system and
word-based SMT (WB-SMT) are given in Table 1.
Essentially, all the automatic evaluation metrics bar
one (Precision) suggest that EBMT can outperform
SMT from English?French. Surprisingly, however,
apart from SER, all evaluation scores are higher us-
ing 100K sentence pairs as training data rather than
the full 203K sentences. It is generally assumed that
increasing the size of the training data for corpus-
based MT systems will improve the quality of the
output translations. (Way and Gough, 2005) observe
that while this dip in performance may be due to a
degree of over?fitting, they intend to carry out some
variance analysis on these results (e.g. performing
bootstrap-resampling on the test set (Koehn, 2004)),
or re-test with different sample test sets in order
to investigate whether the same phenomenon is ob-
served.
With respect to SER, however, for both SMT and
EBMT, the figures improve as more training data is
made available. However, the improvement is much
more significant for EBMT (20.6%) than for SMT
(0.1%). While the WER scores are much the same,
indicating that both systems are identifying reason-
able target vocabulary that should appear in the out-
put translation, the vast differences in SER using
TS3 indicate that a system containing essentially no
information about target syntax has very little hope
of arranging these target words in the right order.
On the contrary, even a system containing some ba-
sic knowledge of how phrases fit together such as
the Marker-based EBMT system of (Gough & Way,
2004b) will generate translations of far higher qual-
ity.
3.2 French?English Results
Table 2: Comparing the EBMT system of (Gough & Way,
2004b) with a WB-SMT system for French?English.
BLEU Prec. Recall WER SER
TS1 WB-SMT .3794 .7096 .7355 52.5 86.5
EBMT .2571 .5419 .6314 69.7 89.2
TS2 WB-SMT .3924 .7206 .7433 46.2 81.3
EBMT .4262 .6731 .7962 55.2 66.2
TS3 WB-SMT .4462 .7035 .7240 46.8 80.8
EBMT .4611 .6782 .7441 50.8 51.2
The results obtained by (Way and Gough, 2005)
for French?English translations are presented in Ta-
ble 2. Translating in this language direction is inher-
ently ?easier? than for English?French as far fewer
agreement errors and cases of boundary friction are
likely. Accordingly, all WB-SMT results in Table 2
are better than for the reverse direction, while for
EBMT, improved results are to be seen for BLEU,
Recall and SER.
While the majority of metrics obtained for
English?French indicate that EBMT outperforms
WB-SMT, the results for French?English are by no
means as conclusive. Of the 15 tests, WB-SMT out-
performs EBMT in nine.
4 Comparing EBMT and Phrase-Based
SMT
From the results in the previous sections for French?
English and for English?French, (Way and Gough,
2005) observe that EBMT outperforms WB-SMT in
the majority of tests. If we are to treat each of the
metrics as being equally significant, it can be said
that EBMT appears to outperform WB-SMT by a
factor of two to one. In fact, the only metric for
which EBMT seems to consistently underperform
is precision for French?English which, when we
examine WER, indicates that the EBMT system?s
knowledge of word correspondences is incomplete
and not as comprehensive as that of the WB-SMT
system.
186
However, it has been apparent for some time now
that phrase-based SMT outperforms previous sys-
tems using word-based models. The results obtained
by (Way and Gough, 2005) for SER also indicate
that if phrase-based SMT were used, then improve-
ments in translation quality ought to be seen.
Accordingly, in this section we describe a set
of experiments which extends the work of (Way
and Gough, 2005) by evaluating the Marker-based
EBMT system of (Gough & Way, 2004b) against a
phrase-based SMT system built using the following
components:
? Giza++, to extract the word-level correspon-
dences;
? The Giza++ word alignments are then refined
and used to extract phrasal alignments ((Och &
Ney, 2003); or (Koehn et al, 2003) for a more
recent implementation);
? Probabilities of the extracted phrases are calcu-
lated from relative frequencies;
? The resulting phrase translation table is passed
to the Pharaoh phrase-based SMT decoder
which along with SRI language modelling
toolkit5 performs translation.
4.1 English?French Results
Table 3: Seeding Pharaoh with Giza++ and EBMT sub-
sentential alignments for English?French.
BLEU Prec. Recall WER SER
TS3 GIZA-DATA .3753 .6598 .5879 58.5 86.82
EBMT-DATA .3643 .6661 .5759 61.33 87.99
We seeded the phrase-based SMT system con-
structed from the publicly available resources listed
above with the word- and phrase-alignments derived
via both Giza++ and the Marker-Based EBMT sys-
tem of (Gough & Way, 2004b). Using the full 203K
training set of (Gough & Way, 2004b), and testing
on their near 4K test set, the results are given in Ta-
ble 3. It is clear to see that the Giza++ alignments
obtain better scores than the EBMT sub-sentential
data. Before one considers the full impact of these
results, one should take into account that the size of
5http://www.speech.sri.com/projects/srilm/
the EBMT data set (word- and phrase-alignments)
is 403,317, while there are over four times as many
SMT sub-sentential alignments (1,732,715).
Comparing these results with those in Table 1,
we can see that for the same training-test data,
the phrase-based SMT system outperforms the WB-
SMT system on most metrics, considerably so with
respect to BLEU score (.3753 vs. .3223). WER,
however, is somewhat worse (.585 vs. .535), and
SER remains disappointingly high. Compared to
the EBMT system of (Gough & Way, 2004b), the
phrase-based SMT system still falls well short with
respect to BLEU score (.4409 for EBMT vs. .3573
for SMT), and again, notably for SER (.656 EBMT,
.868 SMT).
4.2 French?English Results
Table 4: Seeding Pharaoh with Giza++ and EBMT sub-
sentential alignments for French?English.
BLEU Prec. Recall WER SER
TS3 GIZA-DATA .4198 .6527 .7100 62.93 82.84
EBMT-DATA .3952 .6151 .6643 74.77 86.21
Again, the phrase-based SMT system was seeded
with the Giza++ and EBMT alignments, trained on
the full 203K training set, and tested on the 4K test
set. The results are given in Table 4. As for English?
French, the Giza++ alignments obtain better scores
than when the EBMT sub-sentential data is used.
Comparing these results with those in Table 2, we
see that the phrase-based SMT system actually does
worse than WB-SMT, which is an unexpected re-
sult6. As expected, therefore, the results for phrase-
based SMT here are worse still compared to EBMT.
5 Towards Hybridity: Merging SMT and
EBMT Alignments
We decided to experiment further by combining
parts of the EBMT sub-sentential alignments with
parts of the data induced by Giza++. In the follow-
ing sections, for both English?French and French?
English, we seed the Pharaoh phrase-based SMT
system with:
6The Pharaoh system is untuned, so as to provide an easily
replicable baseline for other similar research. It is quite possible
that with tuning the phrase-based SMT system will outperform
the word-based system.
187
1. the EBMT phrase-alignments with the Giza++
word-alignments;
2. all the EBMT and Giza++ sub-sentential align-
ments (both words and phrases).
5.1 Giza++ Words and EBMT Phrases
Here we seeded Pharaoh with the word-alignments
induced by Giza++ and the EBMT phrasal chunks
only (i.e. no Giza++ phrases and no EBMT lexical
alignments).
5.1.1 English?French Results
Table 5: Seeding Pharaoh with Giza++ word and EBMT
phrasal alignments for English?French.
BLEU Prec. Recall WER SER
TS3 .3962 .6773 .5913 59.32 85.43
Using the full 203K training set of (Gough &
Way, 2004b), and testing on their near 4K test set,
the results are given in Table 5. Comparing these
figures to those in Table 3, we can see that all au-
tomatic evaluation metrics improve with this hybrid
system configuration. Note that the data set size is
430,336, compared to 1.73M for the phrase-based
SMT system seeded solely with Giza++ alignments.
With respect to the EBMT system per se in Table 1,
these results remain slightly below those figures (ex-
cept for precision).
5.1.2 French?English Results
Table 6: Seeding Pharaoh with Giza++ word and EBMT
phrasal alignments for French?English.
BLEU Prec. Recall WER SER
TS3 .4265 .6424 .6918 68.05 83.40
Running the same experimental set up for the re-
verse language direction gives the results in Table 6.
While recall drops slightly, all the other metrics
show a slight increase compared to the performance
obtained when Pharaoh is seeded with Giza++ word-
and phrase-alignments (cf. Table 4).
5.2 Merging All Data
The following two experiments were carried out by
seeding Pharaoh with all the EBMT and Giza++
sub-sentential alignments, i.e. both words and
phrases.
5.2.1 English?French Results
Table 7: Seeding Pharaoh with all Giza++ and EBMT sub-
sentential alignments for English?French.
BLEU Prec. Recall WER SER
TS3 .4259 .7026 .6099 54.26 83.63
Inserting all Giza++ and EBMT data into
Pharaoh?s knowledge sources gives the results in Ta-
ble 7. These are considerably better than the scores
for the ?semi-hybrid? system described in section
5.1.1. This indicates that a phrase-based SMT sys-
tem is likely to perform better when EBMT word-
and phrase-alignments are used in the calculation of
the translation and target language probability mod-
els. Note, however, that the size of the data set in-
creases to over 2M items. Despite this, compared to
the results for the EBMT system of (Gough & Way,
2004b) shown in Table 1, these results for the ?fully
hybrid? SMT system still fall somewhat short (ex-
cept for Precision: .6727 vs. .7026).
5.2.2 French?English Results
Table 8: Seeding Pharaoh with all Giza++ and EBMT sub-
sentential alignments for French?English.
BLEU Prec. Recall WER SER
TS3 .4888 .6927 .7173 56.37 78.42
Carrying out a similar experiment for the reverse
language direction gives the results in Table 8. This
time this hybrid SMT system does outperform the
EBMT system of (Gough & Way, 2004b), with re-
spect to BLEU score (.4888 vs .4611) and Precision
(.6927 vs. 6782), but the EBMT system still wins
out where Recall, WER and SER are concerned. Re-
garding this latter, it seems that the correlation be-
tween low SER and high BLEU score is not as im-
portant as is claimed in (Way and Gough, 2005).
6 Conclusions
(Way and Gough, 2005) carried out a number of ex-
periments designed to test their large-scale Marker-
Based EBMT system described in (Gough & Way,
2004b) against a WB-SMT system constructed from
publicly available tools. While the results were a lit-
tle mixed, the EBMT system won out overall.
188
Nonetheless, WB-SMT has long been abandoned
in favour of phrase-based models. We extended
the work of (Way and Gough, 2005) by performing
a range of experiments using the Pharaoh phrase-
based decoder. Our main observations are as fol-
lows:
? Seeding Pharaoh with word- and phrase-
alignments induced via Giza++ generates bet-
ter results than if EBMT sub-sentential data is
used.
? Seeding Pharaoh with a ?hybrid? dataset of
Giza++ word alignments and EBMT phrases
improves over the baseline phrase-based SMT
system primed solely with Giza++ data. This
would appear to indicate that the quality of the
EBMT phrases is better than the SMT phrases,
and that SMT practitioners should use EBMT
phrasal data in the calculating of their language
and translation models, if available.
? Seeding Pharaoh with all data induced by
Giza++ and the EBMT system leads to the best-
performing hybrid SMT system: for English?
French, as well as EBMT phrasal data, EBMT
word alignments also contribute positively, but
the EBMT system per se still wins out (except
for Precision); for French?English, however,
our hybrid Example-Based SMT system out-
performs the EBMT system of (Gough & Way,
2004b) (cf. Table 9).
Table 9: Comparing the hybrid phrase-based SMT system us-
ing both the full Giza++ and full EBMT data against the EBMT
system of (Gough & Way, 2004b) for the full training set (TS3).
BLEU Prec. Recall WER SER
EN-FR HYBRID .2971 .6739 .5912 54.9 90.8
EBMT .3318 .6525 .6183 54.3 89.2
FR-EN HYBRID .2971 .6739 .5912 54.9 90.8
EBMT .3318 .6525 .6183 54.3 89.2
A number of avenues of further work remain open
to us. We would like to extend our investigations
into hybrid example-based statistical approaches to
machine translation by experiment with seeding the
Marker-Based system of (Gough & Way, 2004b)
with the SMT data, and combinations thereof with
the EBMT sub-sentential alignments, to investigate
the effect on translation quality. Given our find-
ings here, we are optimistic that ?hybrid statistical
EBMT? will outperform the baseline EBMT system,
and that our findings will prompt EBMT practition-
ers to augment their data resources with SMT align-
ments, something which to our knowledge is cur-
rently not done. In addition, we intend to continue
this line of research on different and larger data sets,
and for other language pairs.
7 Final Remarks
Finally, as (Way and Gough, 2005) observe, it is dif-
ficult to explain why to this day SMT practitioners
have not made full use of the large body of existing
work on EBMT, from (Nagao, 1984) to (Carl & Way,
2003) and beyond, which has contributed greatly to
the field of corpus-based MT.
From its very inception EBMT has made use of a
range of sub-sentential data ? both phrasal and lexi-
cal ? to perform translations whereas, until quite re-
cently, SMT models of translation were based on the
relatively simple word alignment models of (Brown
et al, 1990). With the advent of phrase-based SMT
systems the line between EBMT and SMT has be-
come significantly blurred, yet we are still unaware
of any papers on SMT which acknowledge their
debt to EBMT or which describe their approach as
?example?based?.
Despite it becoming increasingly difficulty to dis-
tinguish between EBMT and (phrase?based) SMT
models of translation, some differences still exist.
Rather than using models of syntax in a post hoc
fashion, as is the case with most SMT systems, an
EBMT model of translation builds in syntax at its
core. Given this, a phrase?based SMT system is
more likely to ?learn? chunks that an EBMT sys-
tem would not, as the system learns n-gram se-
quences rather than syntactically-motivated phrases
per se. Furthermore, our research here has demon-
strated quite clearly that if available, merging SMT
and EBMT data improves the quality of the result-
ing hybrid SMT system, as phrases extracted by both
methods that are more likely to function as syntac-
tic units (and therefore be more beneficial during
the translation process) are given a higher statistical
significance. Conversely, the probabilities of those
?less useful? SMT n-grams that are not also gener-
189
ated by the EBMT system are reduced. Essentially,
the EBMT data helps the SMT system to make the
best use of phrase alignments during translation.
Moreover, we see the fact that it is becoming in-
creasingly difficult to describe the differences be-
tween EBMT and SMT as a good thing, and that
as here, this convergence can lead to hybrid systems
capable of outperforming leading EBMT systems as
well as state-of-the-art phrase-based SMT.
We hope that the research presented here,
together with that begun by (Way and Gough,
2005), will lead to new areas of collaboration
between both sets of researchers, to the clear benefit
of the MT research community and the wider public.
Acknowledgements
We would like to thank Nano Gough for sup-
plying us with our EBMT training data. Thanks also
to three anonymous reviewers for their insightful
comments. The work presented in this paper is
partly supported by an IRCSET7 PhD Fellowship
Award.
References
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Fred Jelinek, Robert Mercer, and
Paul Roossin. 1990. A statistical approach to machine
translation Computational Linguistics 16:79?85.
Ralf Brown. 1999. Adding Linguistic Knowledge to a
Lexical Example-based Translation System. In In Pro-
ceedings of the 8th International Conference on The-
oretical and Methodological Issues in Machine Trans-
lation (TMI-99), Chester, England, pp.22?32.
Michael Carl and Andy Way (eds). 2003. Recent
Advances in Example-Based Machine Translation.
Kluwer, Dordrecht, The Netherlands.
Nano Gough and Andy Way. 2004. Example-Based Con-
trolled Translation. In Proceedings of the Ninth EAMT
Workshop, Valetta, Malta, pp.73?81.
Nano Gough and Andy Way. 2004. Robust Large-Scale
EBMT with Marker-Based Segmentation. In Pro-
ceedings of the Tenth Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-
04), Baltimore, MD., pp.95?104.
7http://www.ircset.ie
Thomas Green. 1979. The Necessity of Syntax Markers.
Two experiments with artificial languages. Journal of
Verbal Learning and Behavior 18:481?496.
Mary Hearne and Andy Way. 2003. Seeing the Wood for
the Trees: Data-Oriented Translation. In MT Summit
IX, New Orleans, LA., pp.165?172.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2004), Barcelona,
Spain, pp.388?395.
Philipp Koehn, Franz Och, and Dan Marcu. 2003. Sta-
tistical Phrase-Based Translation. Human Language
Technology Conference, (HLT-NAACL), Edmonton,
Canada, pp.48?54.
Makoto Nagao. 1984. A Framework of a Mechanical
Translation between Japanese and English by Analogy
Principle. In A. Elithorn and R. Banerji (eds.) Artifi-
cial and Human Intelligence, North-Holland, Amster-
dam, The Netherlands, pp.173?180.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
41stAnnual Meeting of the Association for Computa-
tional Linguistics (ACL-03), Sapporo, Japan, pp.160?
167.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics 29:19?51.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-02), Philadelphia, PA.,
pp.311?318.
Joseph Turian, Luke Shen and Dan Melamed. 2003.
Evaluation of Machine Translation and its Evaluation.
In MT Summit IX, New Orleans, LA., pp.386?393.
Hideo Watanabe, Sadao Kurohashi and Eiji Aramaki.
2003. Finding Translation Patterns from Paired Source
and Target Dependency Structures. In M. Carl & A.
Way (eds.) Recent Advances in Example-Based Ma-
chine Translation, Kluwer Academic Publishers, Dor-
drecht, The Netherlands, pp.397?420.
Andy Way and Nano Gough. 2005. Comparing
Example-Based and Statistical Machine Translation.
Natural Language Engineering [in press].
190
Proceedings of the Workshop on Statistical Machine Translation, pages 86?93,
New York City, June 2006. c?2006 Association for Computational Linguistics
Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation 
 
Karolina Owczarzak Declan Groves Josef Van Genabith Andy Way 
National Centre for Language Technology 
School of Computing 
Dublin City University 
Dublin 9, Ireland 
{owczarzak,dgroves,josef,away}@computing.dcu.ie 
 
 
Abstract 
In this paper we present a novel method 
for deriving paraphrases during automatic 
MT evaluation using only the source and 
reference texts, which are necessary for 
the evaluation, and word and phrase 
alignment software. Using target language 
paraphrases produced through word and 
phrase alignment a number of alternative 
reference sentences are constructed auto-
matically for each candidate translation. 
The method produces lexical and low-
level syntactic paraphrases that are rele-
vant to the domain in hand, does not use 
external knowledge resources, and can be 
combined with a variety of automatic MT 
evaluation system. 
1 Introduction 
Since their appearance, BLEU (Papineni et al, 
2002) and NIST (Doddington, 2002) have been the 
standard tools used for evaluating the quality of 
machine translation. They both score candidate 
translations on the basis of the number of n-grams 
it shares with one or more reference translations 
provided. Such automatic measures are indispen-
sable in the development of machine translation 
systems, because they allow the developers to con-
duct frequent, cost-effective, and fast evaluations 
of their evolving models.  
These advantages come at a price, though: an 
automatic comparison of n-grams measures only 
the string similarity of the candidate translation to 
one or more reference strings, and will penalize 
any divergence from them. In effect, a candidate 
translation expressing the source meaning accu-
rately and fluently will be given a low score if the 
lexical choices and syntactic structure it contains, 
even though perfectly legitimate, are not present in 
at least one of the references. Necessarily, this 
score would not reflect a much more favourable 
human judgment that such a translation would re-
ceive. 
The limitations of string comparison are the 
reason why it is advisable to provide multiple ref-
erences for a candidate translation in the BLEU- or 
NIST-based evaluation in the first place. While 
(Zhang and Vogel, 2004) argue that increasing the 
size of the test set gives even more reliable system 
scores than multiple references, this still does not 
solve the inadequacy of BLEU and NIST for sen-
tence-level or small set evaluation. On the other 
hand, in practice even a number of references do 
not capture the whole potential variability of the 
translation. Moreover, often it is the case that mul-
tiple references are not available or are too difficult 
and expensive to produce: when designing a statis-
tical machine translation system, the need for large 
amounts of training data limits the researcher to 
collections of parallel corpora like Europarl 
(Koehn, 2005), which provides only one reference, 
namely the target text; and the cost of creating ad-
ditional reference translations of the test set, usu-
ally a few thousand sentences long, often exceeds 
the resources available. Therefore, it would be de-
sirable to find a way to automatically generate le-
gitimate translation alternatives not present in the 
reference(s) already available. 
86
In this paper, we present a novel method that 
automatically derives paraphrases using only the 
source and reference texts involved in for the 
evaluation of French-to-English Europarl transla-
tions produced by two MT systems: statistical 
phrase-based Pharaoh (Koehn, 2004) and rule-
based Logomedia.1 In using what is in fact a minia-
ture bilingual corpus our approach differs from the 
mainstream paraphrase generation based on mono-
lingual resources. We show that paraphrases pro-
duced in this way are more relevant to the task of 
evaluating machine translation than the use of ex-
ternal lexical knowledge resources like thesauri or 
WordNet2, in that our paraphrases contain both 
lexical equivalents and low-level syntactic vari-
ants, and in that, as a side-effect, evaluation bitext-
derived paraphrasing naturally yields domain-
specific paraphrases. The paraphrases generated 
from the evaluation bitext are added to the existing 
reference sentences, in effect creating multiple ref-
erences and resulting in a higher score for the can-
didate translation. Our hypothesis, confirmed by 
the experiments in this paper, is that the scores 
raised by additional references produced in this 
way will correlate better with human judgment 
than the original scores. 
The remainder of this paper is organized as fol-
lows: Section 2 describes related work; Section 3 
describes our method and presents examples of 
derived paraphrases; Section 4 presents the results 
of the comparison between the BLUE and NIST 
scores for a single-reference translation and the 
same translation using the paraphrases automati-
cally generated from the bitext, as well as the cor-
relations between the scores and human judgment; 
Section 5 discusses ongoing work; Section 6 con-
cludes. 
2 
2.1 
                                                          
Related work 
Word and phrase alignment 
Several researchers noted that the word and 
phrase alignment used in training translation mod-
els in Statistical MT can be used for other purposes 
as well. (Diab and Resnik, 2002) use second lan-
guage alignments to tag word senses. Working on 
an assumption that separate senses of a L1 word 
2.2 
1 http://www.lec.com/ 
2 http://wordnet.princeton.edu/ 
can be distinguished by its different translations in 
L2, they also note that a set of possible L2 transla-
tions for a L1 word may contain many synonyms. 
(Bannard and Callison-Burch, 2005), on the other 
hand, conduct an experiment to show that para-
phrases derived from such alignments can be se-
mantically correct in more than 70% of the cases. 
Automatic MT evaluation 
The insensitivity of BLEU and NIST to per-
fectly legitimate variation has been raised, among 
others, in (Callison-Burch et al, 2006), but the 
criticism is widespread. Even the creators of BLEU 
point out that it may not correlate particularly well 
with human judgment at the sentence level (Pap-
ineni et al, 2002), a problem also noted by (Och et 
al., 2003) and (Russo-Lassner et al, 2005). A side 
effect of this phenomenon is that BLEU is less re-
liable for smaller data sets, so the advantage it pro-
vides in the speed of evaluation is to some extent 
counterbalanced by the time spent by developers 
on producing a sufficiently large test data set in 
order to obtain a reliable score for their system.  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic machine translation metrics. Some of 
them concentrate mainly on the word reordering 
aspect, like Maximum Matching String (Turian et 
al., 2003) or Translation Error Rate (Snover et al, 
2005). Others try to accommodate both syntactic 
and lexical differences between the candidate 
translation and the reference, like CDER (Leusch 
et al, 2006), which employs a version of edit dis-
tance for word substitution and reordering; 
METEOR (Banerjee and Lavie, 2005), which uses 
stemming and WordNet synonymy; and a linear 
regression model developed by (Russo-Lassner et 
al., 2005), which makes use of stemming, Word-
Net synonymy, verb class synonymy, matching 
noun phrase heads, and proper name matching. 
A closer examination of these metrics suggests 
that the accommodation of lexical equivalence is 
as difficult as the appropriate treatment of syntactic 
variation, in that it requires considerable external 
knowledge resources like WordNet, verb class da-
tabases, and extensive text preparation: stemming, 
tagging, etc. The advantage of our method is that it 
produces relevant paraphrases with nothing more 
than the evaluation bitext and a widely available 
word and phrase alignment software, and therefore 
can be used with any existing evaluation metric. 
87
3 Contextual bitext-derived paraphrases 
The method presented in this paper rests on a 
combination of two simple ideas. First, the compo-
nents necessary for automatic MT evaluation like 
BLEU or NIST, a source text and a reference text, 
constitute a miniature parallel corpus, from which 
word and phrase alignments can be extracted 
automatically, much like during the training for a 
statistical machine translation system. Second, tar-
get language words ei1, ?,  ein aligned as the likely 
translations to a source language word fi are often 
synonyms or near-synonyms of each other. This 
also holds for phrases: target language phrases epi1, 
?, epin aligned with a source language phrase fpi 
are often paraphrases of each other. For example, 
in our experiment, for the French word question 
the most probable automatically aligned English 
translations are question, matter, and issue, which 
in English are practically synonyms. Section 3.2 
presents more examples of such equivalent expres-
sions.  
3.1 
3.2 
                                                          
Experimental design 
For our experiment, we used two test sets, 
each consisting of 2000 sentences, drawn ran-
domly from the test section of the Europarl parallel 
corpus. The source language was French and the 
target language was English. One of the test sets 
was translated by Pharaoh trained on 156,000 
French-English sentence pairs. The other test set 
was translated by Logomedia, a commercially 
available rule-based MT system. Each test set con-
sisted therefore of three files: the French source 
file, the English translation file, and the English 
reference file. 
Each translation was evaluated by the BLEU 
and NIST metrics first with the single reference, 
then with the multiple references for each sentence 
using the paraphrases automatically generated 
from the source-reference mini corpus. A subset of 
a 100 sentences was randomly extracted from each 
test set and evaluated by two independent human 
judges with respect to accuracy and fluency; the 
human scores were then compared to the BLEU 
and NIST scores for the single-reference and the 
automatically generated multiple-reference files. 
Word alignment and phrase extraction 
We used the GIZA++ word alignment soft-
ware3 to produce initial word alignments for our 
miniature bilingual corpus consisting of the source 
French file and the English reference file, and the 
refined word alignment strategy of (Och and Ney, 
2003; Koehn et al, 2003; Tiedemann, 2004) to 
obtain improved word and phrase alignments. 
For each source word or phrase fi that is 
aligned with more than one target words or 
phrases, its possible translations ei1, ..., ein were 
placed in a list as equivalent expressions (i.e. 
synonyms, near-synonyms, or paraphrases of each 
other). A few examples are given in (1). 
 
(1) agreement - accordance 
adopted - implemented 
matter - lot - case 
funds - money 
arms - weapons 
area - aspect  
question ? issue ? matter 
we would expect - we cer-
tainly expect 
bear on - are centred 
around 
 
Alignment divides target words and 
phrases into equivalence sets; each set corresponds 
to one source word/phrase that was originally 
aligned with the target elements. For example, for 
the French word citoyens three English words were 
deemed to be the most appropriate translations: 
people, public, and citizens; therefore these three 
words constitute an equivalence set. Another 
French word population was aligned with two 
English translations: population and people; so the 
word people appears in two equivalence set (this 
gives rise to the question of equivalence transitiv-
ity, which will be discussed in Section 3.3). From 
the 2000-sentence evaluation bitext we derived 769 
equivalence sets, containing in total 1658 words or 
phrases. Each set contained on average two or 
three elements. In effect, we produced at least one 
equivalent expression for 1658 English words or 
phrases. 
An advantage of our method is that the tar-
get paraphrases and words come ordered with re-
3 http://www.fjoch.com/GIZA++ 
88
spect to their likelihood of being the translation of 
the source word or phrase ? each of them is as-
signed a probability expressing this likelihood, so 
we are able to choose only the most likely transla-
tions, according to some experimentally estab-
lished threshold. The experiment reported here was 
conducted without such a threshold, since the word 
and phrase alignment was of a very high quality. 
3.3 
3.4 
3.5 
Domain-specific lexical and syntactic 
paraphrases 
It is important to notice here how the para-
phrases produced are more appropriate to the task 
at hand than synonyms extracted from a general-
purpose thesaurus or WordNet. First, our para-
phrases are contextual - they are restricted to only 
those relevant to the domain of the text, since they 
are derived from the text itself. Given the context 
provided by our evaluation bitext, the word area in 
(1) turns out to be only synonymous with aspect, 
and not with land, territory, neighbourhood, divi-
sion, or other synonyms a general-purpose thesau-
rus or WordNet would give for this entry. This 
allows us to limit our multiple references only to 
those that are likely to be useful in the context pro-
vided by the source text. Second, the phrase align-
ment captures something neither a thesaurus nor 
WordNet will be able to provide: a certain amount 
of syntactic variation of paraphrases. Therefore, we 
know that a string such as we would expect in (1), 
with the sequence noun-aux-verb, might be para-
phrased by we certainly expect, a sequence of 
noun-adv-verb. 
Open and closed class items 
One important conclusion we draw from 
analysing the synonyms obtained through word 
alignment is that equivalence is limited mainly to 
words that belong to open word classes, i.e. nouns, 
verbs, adjectives, adverbs, but is unlikely to extend 
to closed word classes like prepositions or pro-
nouns. For instance, while the French preposition ? 
can be translated in English as to, in, or at, depend-
ing on the context, it is not the case that these three 
prepositions are synonymous in English. The divi-
sion is not that clear-cut, however: within the class 
of pronouns, he, she, and you are definitely not 
synonymous, but the demonstrative pronouns this 
and that might be considered equivalent for some 
purposes. Therefore, in our experiment we exclude 
prepositions and in future work we plan to examine 
the word alignments more closely to decide 
whether to exclude any other words. 
Creating multiple references 
After the list of synonyms and paraphrases is 
extracted from the evaluation bitext, for each 
reference sentence a string search replaces every 
eligible word or phrase with its equivalent(s) from 
the paraphrase list, one at a time, and the resulting 
string is added to the array of references. The 
original string is added to the array as well. This 
process results in a different number of reference 
sentences for every test sentence, depending on 
whether there was anything to replace in the refer-
ence and how many paraphrases we have available 
for the original substring. One example of this 
process is shown in (2). 
 
(2) Original reference: 
i admire the answer mrs parly 
gave this morning but we have 
turned a blind eye to that 
Paraphrase 1: 
i admire the reply mrs parly 
gave this morning but we have 
turned a blind eye to that 
Paraphrase 2: 
i admire the answer mrs parly 
gave this morning however we 
have turned a blind eye to 
that  
Paraphrase 3: 
i admire the answer mrs parly 
gave this morning but we have 
turned a blind eye to it 
 
Transitivity 
As mentioned before, an interesting question 
that arises here is the potential transitivity of our 
automatically derived synonyms/paraphrases. It 
could be argued that if the word people is equiva-
lent to public according to one set from our list, 
and to the word population according to another 
set, then public can be thought of as equivalent to 
population. In this case, the equivalence is not con-
troversial. However, consider the following rela-
tion: if sure in one of the equivalence sets is 
synonymous to certain, and certain in a different 
89
set is listed as equivalent to some, then treating 
sure and some as synonyms is a mistake. In our 
experiment we do not allow synonym transitivity; 
we only use the paraphrases from equivalence sets 
containing the word/phrase we want to replace.  
Multiple simultaneous substitution 
Note that at the moment the references we are 
producing do not contain multiple simultaneous 
substitutions of equivalent expressions; for exam-
ple, in (2) we currently do not produce the follow-
ing versions: 
 
(3) Paraphrase 4:  
i admire the reply mrs parly 
gave this morning however we 
have turned a blind eye to 
that 
Paraphrase 5: 
i admire the answer mrs parly 
gave this morning however we 
have turned a blind eye to it 
Paraphrase 6: 
i admire the reply mrs parly 
gave this morning but we have 
turned a blind eye to it 
 
This can potentially prevent higher n-grams being 
successfully matched if two or more equivalent 
expressions find themselves within the range of n-
grams being tested by BLEU and NIST. To avoid 
combinatorial problems, implementing multiple 
simultaneous substitutions could be done using a 
lattice, much like in (Pang et al, 2003). 
4 Results 
As expected, the use of multiple references 
produced by our method raises both the BLEU and 
NIST scores for translations produced by Pharaoh 
(test set PH) and Logomedia (test set LM). The 
results are presented in Table 1. 
 
 BLEU NIST 
PH single ref 0.2131 6.1625 
PH multi ref 0.2407 7.0068 
LM single ref 0.1782 5.5406 
LM multi ref 0.2043 6.3834 
 
Table 1. Comparison of single-reference and multi-
reference scores for test set PH and test set LM 
 
The hypothesis that the multiple-reference 
scores reflect better human judgment is also con-
firmed. For 100-sentence subsets (Subset PH and 
Subset LM) randomly extracted from our test sets 
PH and LM, we calculated Pearson?s correlation 
between the average accuracy and fluency scores 
that the translations in this subset received from 
two human judges (for each subset) and the single-
reference and multiple-reference sentence-level 
BLEU and NIST scores.  
There are two issues that need to be noted at 
this point. First, BLEU scored many of the sen-
tences as zero, artificially leveling many of the 
weaker translations.4 This explains the low, al-
though still statistically significant (p value < 
0.015) correlation with BLEU for both single and 
multiple reference translations. Using a version of 
BLEU with add-one smoothing we obtain consid-
erably higher correlations. Table 2 shows Pear-
son?s correlation coefficient for BLEU, BLEU 
with add-one smoothing, NIST, and human judg-
ments for Subsets PH. Multiple paraphrase refer-
ences produced by our method consistently lead to 
a higher correlation with human judgment for 
every metric.6 
 
                           Subset PH 
Metric  
single 
ref 
multi 
ref 
H & BLEU 0.297 0.307 
H & BLEU smoothed 0.396 0.404 
H & NIST  0.323 0.355 
 
Table 2. Pearson?s correlation between human 
judgment and single-reference and multiple-
reference BLEU, smoothed BLEU, and NIST for 
subset PH (of test set PH)  
 
The second issue that requires explanation is 
the lower general scores Logomedia?s translation 
received on the full set of 2000 sentences, and the 
extremely low correlation of its automatic evalua-
tion with human judgment, irrespective of the 
number of references. It has been noticed (Calli-
                                                          
4 BLEU uses a geometric average while calculating the sen-
tence-level score and will score a sentence as 0 if it does not 
have at least one 4-gram.  
5 A critical value for Pearson?s correlation coefficient for the 
sample size between 90 and 100 is 0.267, with p < 0.01. 
6 The significance of the rise in scores was confirmed in a 
resampling/bootstrapping test, with p < 0.0001. 
90
son-Burch et al, 2006) that BLEU and NIST fa-
vour n-gram based MT models such as Pharaoh, so 
the translation produced by Logomedia scored 
lower on the automatic evaluation, even though the 
human judges rated Logomedia output higher than 
Pharaoh?s translation. Both human judges consis-
tently gave very high scores to most sentences in 
subset LM (Logomedia), and as a consequence 
there was not enough variation in the scores as-
signed by them to create a good correlation with 
the BLEU and NIST scores. The average human 
scores for the subsets PH and LM and the coeffi-
cients of variation are presented in Table 3. It is 
easy to see that Logomedia?s translation received a 
higher mean score (on a scale 0 to 5) from the hu-
man judges and with less variance than Pharaoh. 
 
 Mean score  Variation 
Subset PH 3.815 19.1% 
Subset LM 4.005 16.25% 
 
Table 3. Human judgment mean scores and coeffi-
cients of variation for Subset PH and Subset LM 
 
As a result of the consistently high human scores 
for Logomedia, none of the Pearson?s correlations 
computed for Subset LM is high enough to be sig-
nificant. The values are lower than the critical 
value 0.164 corresponding to p < 0.10. 
 
                          Subset LM 
Metric  
single 
ref 
multi 
ref 
H & BLEU 0.046* 0.067* 
H & BLEU smoothed 0.163* 0.151* 
H & NIST  0.078* 0.116* 
 
Table 4. Pearson?s correlation between human 
judgment and single-reference and multiple-
reference BLEU, smoothed BLEU, and NIST for 
subset LM (of test set LM). * denotes values with p >  
0.10. 
5 Current and future work 
We would like to experiment with the way in 
which the list of equivalent expressions is pro-
duced. One possible development would be to de-
rive the expressions from a very large training 
corpus used by a statistical machine translation 
system, following (Bannard and Callison-Burch, 
2005), for instance, and use it as an external wider-
purpose knowledge resource (rather than a current 
domain-tailored resource as in our experiment), 
which would be nevertheless improve on a thesau-
rus in that it would also include phrase equivalents 
with some syntactic variation. According to (Ban-
nard and Callison-Burch, 2005), who derived their 
paraphrases automatically from a corpus of over a 
million German-English Europarl sentences, the 
baseline syntactic and semantic accuracy of the 
best paraphrases (those with the highest probabil-
ity) reaches 48.9% and 64.5%, respectively. That 
is, by replacing a phrase with its one most likely 
paraphrase the sentence remained syntactically 
well-formed in 48.9% of the cases and retained its 
meaning in 65% of the cases. 
In a similar experiment we generated para-
phrases from a French-English Europarl corpus of 
700,000 sentences. The data contained a consid-
erably higher level of noise than our previous ex-
periment on the 2000-sentence test set, even 
though we excluded any non-word entities from 
the results. Like (Bannard and Callison-Burch, 
2005), we used the product of probabilities p(fi|ei1) 
and p(ei2|fi) to determine the best paraphrase for a 
given English word ei1. We then compared the ac-
curacy across four samples of data. Each sample 
contained 50 randomly drawn words/phrases and 
their paraphrases. For the first two samples, the 
paraphrases were derived from the initial 2000-
sentence corpus; for the second two, the para-
phrases were derived from the 700,000-sentence 
corpus. For each corpus, one of the two samples 
contained only one best paraphrase for each entry, 
while the other listed all possible paraphrases. We 
then evaluated the quality of each paraphrase with 
respect to its syntactic and semantic accuracy. In 
terms of syntax, we considered the paraphrase ac-
curate either if it had the same category as the 
original word/phrase; in terms of semantics, we 
relied on human judgment of similarity. Tables 5 
and 6 summarize the syntactic and semantic accu-
racy levels in the samples. 
 
                       Paraphrases 
Derived from 
Best All 
2000-sent. corpus 59% 60% 
700,000-sent. corpus 70% 48% 
 
Table 5. Syntactic accuracy of paraphrases 
 
 
91
                       Paraphrases 
Derived from 
Best All 
2000-sent. corpus 83% 74% 
700,000-sent. corpus 76% 68% 
 
Table 6. Semantic accuracy of paraphrases 
 
Although it has to be kept in mind that these 
percentages were taken from relatively small sam-
ples, an interesting pattern emerges from compar-
ing the results. It seems that the average syntactic 
accuracy of all paraphrases decreases with in-
creased corpus size, but the syntactic accuracy of 
the one best paraphrase improves. This reflects the 
idea behind word alignment: the bigger the corpus, 
the more potential alignments there are for a given 
word, but at the same time the better their order in 
terms of probability and the likelihood to obtain 
the correct translation. Interestingly, the same pat-
tern is not repeated for semantic accuracy, but 
again, these samples are quite small. In order to 
address this issue, we plan to repeat the experiment 
with more data. 
Additionally, it should be noted that certain 
expressions, although not completely correct syn-
tactically, could be retained in the paraphrase lists 
for the purposes of machine translation evaluation. 
Consider the case where our equivalence set looks 
like this: 
 
(4) abandon ? abandoning ? 
abandoned 
 
The words in (4) are all inflected forms of the verb 
abandon, and although they would produce rather 
ungrammatical paraphrases, those ungrammatical 
paraphrases still allow us to score our translation 
higher in terms of BLEU or NIST if it contains one 
of the forms of abandon than when it contains 
some unrelated word like piano instead. This is 
exactly what other scoring metrics mentioned in 
Section 2 attempt to obtain with the use of stem-
ming or prefix matching. 
6 Conclusions 
In this paper we present a novel combination 
of existing ideas from statistical machine transla-
tion and paraphrase generation that leads to the 
creation of multiple references for automatic MT 
evaluation, using only the source and reference 
files that are required for the evaluation. The 
method uses simple word and phrase alignment 
software to find possible synonyms and para-
phrases for words and phrases of the target text, 
and uses them to produce multiple reference sen-
tences for each test sentence, raising the BLEU and 
NIST evaluation scores and reflecting human 
judgment better. The advantage of this method 
over other ways to generate paraphrases is that (1) 
unlike other methods, it does not require extensive 
parallel monolingual paraphrase corpora, but it 
extracts equivalent expressions from the miniature 
bilingual corpus of the source and reference 
evaluation files; (2) unlike other ways to accom-
modate synonymy in automatic evaluation, it does 
not require external lexical knowledge sources like 
thesauri or WordNet; (3) it extracts only synonyms 
that are relevant to the domain in hand; and (4) the 
equivalent expressions it produces include a certain 
amount of syntactic paraphrases.  
The method is general and it can be used with 
any automatic evaluation metric that supports mul-
tiple references. In our future work, we plan to ap-
ply it to newly developed evaluation metrics like 
CDER and TER that aim to allow for syntactic 
variation between the candidate and the reference, 
therefore bringing together solutions for the two 
shortcomings of automatic evaluation systems: 
insensitivity to allowable lexical differences and 
syntactic variation. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Proceed-
ings of the ACL 2005 Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or Sum-
marization: 65-73. 
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proceed-
ings of the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL 2005): 597-604. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. To appear in Pro-
ceedings of EACL-2006. 
Mona Diab and Philip Resnik. 2002. An unsupervised 
Method for Word Sense Tagging using Parallel Cor-
pora. Proceedings of the 40th Annual Meeting of the 
92
Association for Computational Linguistics, Philadel-
phia, PA. 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. Pro-
ceedings of Human Language Technology Confer-
ence 2002: 138?145. 
Philipp Koehn, Franz Och and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. Proceedings of 
the  Human Language Technology Conference (HLT-
NAACL 2003): 48-54. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. Machine translation: From real users to re-
search. 6th Conference of the Association for 
Machine Translation in the Americas (AMTA 2004): 
115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. To appear in Proceedings of the 11th 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2006). 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19?51. 
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. 
Syntax for statistical machine translation. Technical 
report, Center for Language and Speech Processing, 
John Hopkins University, Baltimore, MD.  
Bo Pang, Kevin Knight and Daniel Marcu. 2003. Syn-
tax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences. 
Proceedings of Human Language Technology-North 
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL) 2003: 181?188. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
ACL: 311-318. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-
TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-
versity of Maryland, College Park, MD. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula and Ralph Weischedel. 
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. Technical Report LAMP-
TR-126, CS-TR-4755, UMIACS-TR-2005-58, Uni-
versity of Maryland, College Park. MD. 
J?rg Tiedemann. 2004. Word to word alignment strate-
gies. Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004): 
212-218. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. 
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalua-
tion metrics. TMI-2004: Proceedings of the 10th 
Conference on Theoretical and Methodological Is-
sues in Machine Translation: 85-94. 
93
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80?87,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Dependency-Based Automatic Evaluation for Machine Translation 
Karolina Owczarzak Josef van Genabith Andy Way  
National Centre for Language Technology  
School of Computing, Dublin City University  
Dublin 9, Ireland  
{owczarzak,josef,away}@computing.dcu.ie  
    
 
Abstract 
We present a novel method for evaluating 
the output of Machine Translation (MT), 
based on comparing the dependency 
structures of the translation and reference 
rather than their surface string forms. Our 
method uses a treebank-based, wide-
coverage, probabilistic Lexical-Functional 
Grammar (LFG) parser to produce a set of 
structural dependencies for each 
translation-reference sentence pair, and 
then calculates the precision and recall for 
these dependencies. Our dependency-
based evaluation, in contrast to most 
popular string-based evaluation metrics, 
will not unfairly penalize perfectly valid 
syntactic variations in the translation. In 
addition to allowing for legitimate 
syntactic differences, we use paraphrases 
in the evaluation process to account for 
lexical variation. In comparison with 
other metrics on 16,800 sentences of 
Chinese-English newswire text, our 
method reaches high correlation with 
human scores. An experiment with two 
translations of 4,000 sentences from 
Spanish-English Europarl shows that, in 
contrast to most other metrics, our method 
does not display a high bias towards 
statistical models of translation. 
1 Introduction 
Since their appearance, string-based evaluation 
metrics such as BLEU (Papineni et al, 2002) and 
NIST (Doddington, 2002) have been the standard 
tools used for evaluating MT quality. Both score a 
candidate translation on the basis of the number of 
n-grams shared with one or more reference 
translations. Automatic measures are indispensable 
in the development of MT systems, because they 
allow MT developers to conduct frequent, cost-
effective, and fast evaluations of their evolving 
models.  
These advantages come at a price, though: an 
automatic comparison of n-grams measures only 
the string similarity of the candidate translation to 
one or more reference strings, and will penalize 
any divergence from them. In effect, a candidate 
translation expressing the source meaning 
accurately and fluently will be given a low score if 
the lexical and syntactic choices it contains, even 
though perfectly legitimate, are not present in at 
least one of the references. Necessarily, this score 
would differ from a much more favourable human 
judgement that such a translation would receive. 
The limitations of string comparison are the 
reason why it is advisable to provide multiple 
references for a candidate translation in BLEU- or 
NIST-based evaluations. While Zhang and Vogel 
(2004) argue that increasing the size of the test set 
gives even more reliable system scores than 
multiple references, this still does not solve the 
inadequacy of BLEU and NIST for sentence-level 
or small set evaluation. In addition, in practice 
even a number of references do not capture the 
whole potential variability of the translation. 
Moreover, when designing a statistical MT system, 
the need for large amounts of training data limits 
the researcher to collections of parallel corpora 
such as Europarl (Koehn, 2005), which provides 
only one reference, namely the target text; and the 
cost of creating additional reference translations of 
the test set, usually a few thousand sentences long, 
is often prohibitive. Therefore, it would be 
desirable to find an evaluation method that accepts 
legitimate syntactic and lexical differences 
80
between the translation and the reference, thus 
better mirroring human assessment. 
In this paper, we present a novel method that 
automatically evaluates the quality of translation 
based on the dependency structure of the sentence, 
rather than its surface form. Dependencies abstract 
away from the particulars of the surface string (and 
CFG tree) realization and provide a ?normalized? 
representation of (some) syntactic variants of a 
given sentence. The translation and reference files 
are analyzed by a treebank-based, probabilistic 
Lexical-Functional Grammar (LFG) parser (Cahill 
et al, 2004), which produces a set of dependency 
triples for each input. The translation set is 
compared to the reference set, and the number of 
matches is calculated, giving the precision, recall, 
and f-score for that particular translation.   
In addition, to allow for the possibility of valid 
lexical differences between the translation and the 
references, we follow Kauchak and Barzilay 
(2006) and Owczarzak et al (2006) in adding a 
number of paraphrases in the process of evaluation 
to raise the number of matches between the 
translation and the reference, leading to a higher 
score. 
Comparing the LFG-based evaluation method 
with other popular metrics: BLEU, NIST, General 
Text Matcher (GTM) (Turian et al, 2003), 
Translation Error Rate (TER) (Snover et al, 
2006)1, and METEOR (Banerjee and Lavie, 2005), 
we show that combining dependency 
representations with paraphrases leads to a more 
accurate evaluation that correlates better with 
human judgment. 
The remainder of this paper is organized as 
follows: Section 2 gives a basic introduction to 
LFG; Section 3 describes related work; Section 4 
describes our method and gives results of two 
experiments on different sets of data: 4,000 
sentences from Spanish-English Europarl and 
16,800 sentences of Chinese-English newswire text 
from the Linguistic Data Consortium?s (LDC) 
Multiple Translation project; Section 5 discusses 
ongoing work; Section 6 concludes. 
                                                 
1 As we focus on purely automatic metrics, we omit 
HTER (Human-Targeted Translation Error Rate) here. 
2 Lexical-Functional Grammar 
In Lexical-Functional Grammar (Bresnan, 2001) 
sentence structure is represented in terms of 
c(onstituent)-structure and f(unctional)-structure. 
C-structure represents the surface string word order 
and the hierarchical organisation of phrases in 
terms of CFG trees. F-structures are recursive 
feature (or attribute-value) structures, representing 
abstract grammatical relations, such as subj(ect), 
obj(ect), obl(ique), adj(unct), approximating to 
predicate-argument structure or simple logical 
forms. C-structure and f-structure are related in 
terms of functional annotations (attribute-value 
structure equations) in c-structure trees, describing 
f-structures.  
While c-structure is sensitive to surface word 
order, f-structure is not. The sentences John 
resigned yesterday and Yesterday, John resigned 
will receive different tree representations, but 
identical f-structures, shown in (1). 
 
(1) C-structure:                         F-structure: 
 
              S 
                  
      
 NP                      VP 
   |                     
John       
              V               NP-TMP 
               |                      | 
       resigned       yesterday 
                         
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
                     S 
                  
      
    NP       NP       VP 
      |                 |            | 
Yesterday  John        V              
                                    | 
                            resigned                             
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
 
Notice that if these two sentences were a 
translation-reference pair, they would receive a 
less-than-perfect score from string-based metrics. 
For example, BLEU with add-one smoothing2 
gives this pair a score of barely 0.3781. 
The f-structure can also be described as a flat 
set of triples. In triples format, the f-structure in (1) 
could be represented as follows: {subj(resign, 
john), pers(john, 3), num(john, sg), tense(resign, 
                                                 
2 We use smoothing because the original BLEU gives 
zero points to sentences with fewer than one four-gram. 
81
past), adj(resign, yesterday), pers(yesterday, 3), 
num(yesterday, sg)}. 
Cahill et al (2004) presents Penn-II Treebank-
based LFG parsing resources. Her approach 
distinguishes 32 types of dependencies, including 
grammatical functions and morphological 
information. This set can be divided into two major 
groups: a group of predicate-only dependencies 
and non-predicate dependencies. Predicate-only 
dependencies are those whose path ends in a 
predicate-value pair, describing grammatical 
relations. For example, for the f-structure in (1), 
predicate-only dependencies would include: 
{subj(resign, john), adj(resign, yesterday)}.3  
In parser evaluation, the quality of the f-
structures produced automatically can be checked 
against a set of gold standard sentences annotated 
with f-structures by a linguist. The evaluation is 
conducted by calculating the precision and recall 
between the set of dependencies produced by the 
parser, and the set of dependencies derived from 
the human-created f-structure. Usually, two 
versions of f-score are calculated: one for all the 
dependencies for a given input, and a separate one 
for the subset of predicate-only dependencies. 
In this paper, we use the parser developed by 
Cahill et al (2004), which automatically annotates 
input text with c-structure trees and f-structure 
dependencies, reaching high precision and recall 
rates. 4  
3 Related work 
The insensitivity of BLEU and NIST to perfectly 
legitimate syntactic and lexical variation has been 
raised, among others, in Callison-Burch et al 
(2006), but the criticism is widespread. Even the 
creators of BLEU point out that it may not 
correlate particularly well with human judgment at 
the sentence level (Papineni et al, 2002). A side 
                                                 
3 Other predicate-only dependencies include: 
apposition,  complement, open complement, 
coordination, determiner, object, second object, 
oblique, second oblique, oblique agent, possessive, 
quantifier, relative clause, topic, relative clause 
pronoun. The remaining non-predicate dependencies 
are: adjectival degree, coordination surface form, focus, 
complementizer forms: if, whether, and that, modal, 
number, verbal particle, participle, passive, person, 
pronoun surface form, tense, infinitival clause. 
4 http://lfg-demo.computing.dcu.ie/lfgparser.html 
effect of this phenomenon is that BLEU is less 
reliable for smaller data sets, so the advantage it 
provides in the speed of evaluation is to some 
extent counterbalanced by the time spent by 
developers on producing a sufficiently large test 
set in order to obtain a reliable score for their 
system.  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic MT evaluation metrics. Some of them 
concentrate mainly on word order, like General 
Text Matcher (Turian et al, 2003), which 
calculates precision and recall for translation-
reference pairs, weighting contiguous matches 
more than non-sequential matches, or Translation 
Error Rate (Snover et al, 2005), which computes 
the number of substitutions, inserts, deletions, and 
shifts necessary to transform the translation text to 
match the reference. Others try to accommodate 
both syntactic and lexical differences between the 
candidate translation and the reference, like CDER 
(Leusch et al, 2006), which employs a version of 
edit distance for word substitution and reordering; 
or METEOR (Banerjee and Lavie, 2005), which 
uses stemming and WordNet synonymy. Kauchak 
and Barzilay (2006) and Owczarzak et al (2006) 
use paraphrases during BLEU and NIST evaluation 
to increase the number of matches between the 
translation and the reference; the paraphrases are 
either taken from WordNet5 in Kauchak and 
Barzilay (2006) or derived from the test set itself 
through automatic word and phrase alignment in 
Owczarzak et al (2006). Another metric making 
use of synonyms is the linear regression model 
developed by Russo-Lassner et al (2005), which 
makes use of stemming, WordNet synonymy, verb 
class synonymy, matching noun phrase heads, and 
proper name matching. Kulesza and Schieber 
(2004), on the other hand, train a Support Vector 
Machine using features like proportion of n-gram 
matches and word error rate to judge a given 
translation?s distance from human-level quality. 
Nevertheless, these metrics use only string-
based comparisons, even while taking into 
consideration reordering. By contrast, our 
dependency-based method concentrates on 
utilizing linguistic structure to establish a 
comparison between translated sentences and their 
reference.  
                                                 
5 http://wordnet.princeton.edu/ 
82
4 LFG f-structure in MT evaluation 
The process underlying the evaluation of f-
structure quality against a gold standard can be 
used in automatic MT evaluation as well: we parse 
the translation and the reference, and then, for each 
sentence, we check the set of translation 
dependencies against the set of reference 
dependencies, counting the number of matches. As 
a result, we obtain the precision and recall scores 
for the translation, and we calculate the f-score for 
the given pair. Because we are comparing two 
outputs that were produced automatically, there is 
a possibility that the result will not be noise-free. 
To assess the amount of noise that the parser 
may introduce we conducted an experiment where 
100 English Europarl sentences were modified by 
hand in such a way that the position of adjuncts 
was changed, but the sentence remained 
grammatical and the meaning was not changed. 
This way, an ideal parser should give both the 
source and the modified sentence the same f-
structure, similarly to the case presented in (1). The 
modified sentences were treated like a translation 
file, and the original sentences played the part of 
the reference. Each set was run through the parser. 
We evaluated the dependency triples obtained from 
the ?translation? against the dependency triples for 
the ?reference?, calculating the f-score, and applied 
other metrics (TER, METEOR, BLEU, NIST, and 
GTM) to the set in order to compare scores. The 
results, inluding the distinction between f-scores 
for all dependencies and predicate-only 
dependencies, appear in Table 1. 
 
 baseline modified 
TER 0.0 6.417 
METEOR   1.0 0.9970 
BLEU 1.0000 0.8725 
NIST 11.5232 11.1704 (96.94%) 
GTM 100 99.18 
dep f-score  100 96.56 
dep_preds f-score 100 94.13 
Table 1. Scores for sentences with reordered adjuncts 
 
The baseline column shows the upper bound for a 
given metric: the score which a perfect translation, 
word-for-word identical to the reference, would 
obtain.6 In the other column we list the scores that 
the metrics gave to the ?translation? containing 
reordered adjunct. As can be seen, the dependency 
and predicate-only dependency scores are lower 
than the perfect 100, reflecting the noise 
introduced by the parser.  
To show the difference between the scoring 
based on LFG dependencies and other metrics in 
an ideal situation, we created another set of a 
hundred sentences with reordered adjuncts, but this 
time selecting only those reordered sentences that 
were given the same set of dependencies by the 
parser (in other words, we simulated having the 
ideal parser). As can be seen in Table 2, other 
metrics are still unable to tolerate legitimate 
variation in the position of adjuncts, because the 
sentence surface form differs from the reference; 
however, it is not treated as an error by the parser. 
 
 baseline modified 
TER 0.0 7.841 
METEOR   1.0 0.9956 
BLEU 1.0000 0.8485 
NIST 11.1690 10.7422 (96.18%) 
GTM 100 99.35 
dep f-score  100 100 
dep_preds f-score 100 100 
Table 2. Scores for sentences with reordered adjuncts in 
an ideal situation 
4.1 Initial experiment ? Europarl 
In the first experiment, we attempted to determine 
whether the dependency-based measure is biased 
towards statistical MT output, a problem that has 
been observed for n-gram-based metrics like 
BLEU and NIST. Callison-Burch et al (2006) 
report that BLEU and NIST favour n-gram-based 
MT models such as Pharaoh (Koehn, 2004), so the 
translations produced by rule-based systems score 
lower on the automatic evaluation, even though 
human judges consistently rate their output higher 
than Pharaoh?s translation. Others repeatedly 
                                                 
6 Two things have to be noted here: (1) in case of NIST 
the perfect score differs from text to text, which is why 
we provide the percentage points as well, and (2) in case 
of TER the lower the score, the better the translation, so 
the perfect translation will receive 0, and there is no 
upper bound on the score, which makes this particular 
metric extremely difficult to directly compare with 
others. 
83
observed this tendency in previous research as 
well; in one experiment, reported in Owczarzak et 
al. (2006), where the rule-based system 
Logomedia7 was compared with Pharaoh, BLEU 
scored Pharaoh 0.0349 points higher, NIST scored 
Pharaoh 0.6219 points higher, but human judges 
scored Logomedia output 0.19 points higher (on a 
5-point scale).  
4.1.1 Experimental design 
In order to check for the existence of a bias in the 
dependency-based metric, we created a set of 
4,000 sentences drawn randomly from the Spanish-
English subset of Europarl (Koehn, 2005), and we 
produced two translations: one by a rule-based 
system Logomedia, and the other by the standard 
phrase-based statistical decoder Pharaoh, using 
alignments produced by GIZA++8 and the refined 
word alignment strategy of Och and Ney (2003). 
The translations were scored with a range of 
metrics: BLEU, NIST, GTM, TER, METEOR, and 
the dependency-based method. 
4.1.2 Adding synonyms 
Besides the ability to allow syntactic variants as 
valid translations, a good metric should also be 
able to accept legitimate lexical variation. We 
introduced synonyms and paraphrases into the 
process of evaluation, creating new best-matching 
references for the translations using either 
paraphrases derived from the test set itself 
(following Owczarzak et al (2006)) or WordNet 
synonyms (as in Kauchak and Barzilay (2006)). 
 
Bitext-derived paraphrases 
Owczarzak et al (2006) describe a simple way to 
produce a list of paraphrases, which can be useful 
in MT evaluation, by running word alignment 
software on the test set that is being evaluated. 
Paraphrases derived in this way are specific to the 
domain at hand and contain low-level syntactic 
variants in addition to word-level synonymy. 
Using the standard GIZA++ software and the 
refined word alignment strategy of Och and Ney 
(2003) on our test set of 4,000 Spanish-English 
sentences, the method generated paraphrases for 
just over 1100 items. These paraphrases served to 
                                                 
7 http://www.lec.com/ 
8 http://www.fjoch.com/GIZA++ 
create new individual best-matching references for 
the Logomedia and Pharaoh translations. Due to 
the small size of the paraphrase set, only about 
20% of reference sentences were actually modified 
to better reflect the translation. This, in turn, led to 
little difference in scores. 
WordNet synonyms 
To maximize the number of matches between a 
translation and a reference, Kauchak and Barzilay 
(2006) use WordNet synonyms during evaluation. 
In addition, METEOR also has an option of 
including WordNet in the evaluation process. As in 
the case of bitext-derived paraphrases, we used 
WordNet synonyms to create new best-matching 
references for each of the two translations. This 
time, given the extensive database containing 
synonyms for over 150,000 items, around 70% of 
reference sentences were modified: 67% for 
Pharaoh, and 75% for Logomedia. Note that the 
number of substitutions is higher for Logomedia; 
this confirms the intuition that the translation 
produced by Pharaoh, trained on the domain which 
is also the source of the reference text, will need 
fewer lexical replacements than Logomedia, which 
is based on a general non-domain-specific model. 
4.1.3 Results 
Table 3 shows the difference between the scores 
which Pharaoh?s and Logomedia?s translations 
obtained from each metric: a positive number 
shows by how much Pharaoh?s score was higher 
than Logomedia?s, and a negative number reflects 
Logomedia?s higher score (the percentages are 
absolute values). As can be seen, all the metrics 
scored Pharaoh higher, inlcuding METEOR and 
the dependency-based method that were boosted 
with WordNet. The values in the table are sorted in 
descending order, from the largest to the lowest 
advantage of Pharaoh over Logomedia. 
Interestingly, next to METEOR boosted with 
WordNet, it is the dependency-based method, and 
especially the predicates-only version, that shows 
the least bias towards the phrase-based translation. 
In the next step, we selected from this set smaller 
subsets of sentences that were more and more 
similar in terms of translation quality (as 
determined by a sentence?s BLEU score). As the 
similarity of the translation quality increased, most 
metrics lowered their bias, as is shown in Table 4. 
The first column shows the case where the 
sentences chosen differed at the most by 0.05 
84
points BLEU score; in the second column the 
difference was lowered to 0.01; and in the third 
column to 0.005. The numbers following the hash 
signs in the header row indicate the number of 
sentences in a given set.  
 
metric PH score ? LM score 
TER 1.997 
BLEU 7.16% 
NIST 6.58% 
dep 4.93% 
dep+paraphr 4.80% 
GTM 3.89% 
METEOR 3.80% 
dep_preds 3.79% 
dep+paraphr_preds 3.70% 
dep+WordNet 3.55% 
dep+WordNet_preds 2.60% 
METEOR+WordNet 1.56% 
Table 3. Difference between scores assigned to Pharaoh 
and Logomedia. Positive numbers show by how much 
Pharaoh?s score was higher than Logomedia?s. Legend: 
dep = dependency f-score, paraph = paraphrases, _preds = 
predicate-only f-score.  
 
~ 0.05 #1692 ~ 0.01 #567 ~ 0.005 #335 
NIST 2.29% NIST 1.76% NIST 1.48% 
BLEU 0.95% BLEU 0.42% BLEU 0.59% 
GTM 0.94% GTM 0.29% GTM -0.09% 
d+p 0.67% d 0.04% d+p -0.15% 
d 0.61% d+p 0.02% d -0.24% 
d+WN -0.29% d+WN -0.78% d+WN -0.99% 
d+p_pr -0.70% M -0.99% d+p_pr -1.30% 
d_pr -0.75% d_pr -1.37% d_pr -1.43% 
M -1.03% d+p_pr -1.38% M -1.57% 
d+WN_pr -1.43% d+WN_pr -1.97% d+WN_pr -1.94% 
M+WN -2.51% M+WN -2.21% M+WN -2.74% 
TER -1.579 TER -1.228 TER -1.739 
Table 4. Difference between scores assigned to Pharaoh 
and Logomedia for sets of increasing similarity. Positive 
numbers show Pharaoh?s advantage, negative numbers 
show Logomedia?s advantage. Legend: d = dependency f-
score, p = paraphrases, _pr = predicate-only f-score, M = 
METEOR, WN = WordNet.  
 
These results confirm earlier suggestions that 
the predicate-only version of the dependency-
based evaluation is less biased in favour of the 
statistical MT system than the version that includes 
all dependency types. Adding a sufficient number 
of lexical choices reduces the bias even further; 
although again, paraphrases generated from the test 
set only are too few to make a significant 
difference. Similarly to METEOR, the 
dependency-based method shows on the whole 
lower bias than other metrics. However, we cannot 
be certain that the underlying scores vary linearly 
with each other and with human judgements, as we 
have no framework of reference such as human 
segment-level assessment of translation quality in 
this case. Therefore, the correlation with human 
judgement is analysed in our next experiment.   
4.2 Correlation with human judgement ? 
MultiTrans 
To calculate how well the dependency-based 
method correlates with human judgement, and how 
it compares to the correlation shown by other 
metrics, we conducted an experiment on Chinese-
English newswire text.  
4.2.1 Experimental design 
We used the data from the Linguistic Data 
Consortium Multiple Translation Chinese (MTC) 
Parts 2 and 4. The data consists of multiple 
translations of Chinese newswire text, four human-
produced references, and segment-level human 
scores for a subset of the translation-reference 
pairs. Although a single translated segment was 
always evaluated by more than one judge, the 
judges used a different reference every time, which 
is why we treated each translation-reference-
human score triple as a separate segment. In effect, 
the test set created from this data contained 16,800 
segments. As in the previous experiment, the 
translation was scored using BLEU, NIST, GTM, 
TER, METEOR, and the dependency-based 
method. 
4.2.2 Results 
We calculated Pearson?s correlation coefficient for 
segment-level scores that were given by each 
metric and by human judges. The results of the 
correlation are shown in Table 5. Note that the 
correlation for TER is negative, because in TER 
zero is the perfect score, in contrast to other 
metrics where zero is the worst possible score; 
however, this time the absolute values can be 
easily compared to each other. Rows are ordered 
85
by the highest value of the (absolute) correlation 
with the human score. 
First, it seems like none of the metrics is very 
good at reflecting human fluency judgments; the 
correlation values in the first column are 
significantly lower than the correlation with 
accuracy. However, the dependency-based method 
in almost all its versions has decidedly the highest 
correlation in this area. This can be explained by 
the method?s sensitivity to the grammatical 
structure of the sentence: a more grammatical 
translation is also a translation that is more fluent. 
 
H_FL  H_AC  H_AVE  
d+WN 0.168 M+WN 0.294 M+WN 0.255 
d   0.162 M   0.278 d+WN 0.244 
d+WN_pr 0.162 NIST 0.273 M   0.242 
BLEU 0.155 d+WN 0.266 NIST 0.238 
d_pr 0.154 GTM 0.260 d   0.236 
M+WN 0.153 d  0.257 GTM 0.230 
M   0.149 d+WN_pr 0.232 d+WN_pr 0.220 
NIST 0.146 d_pr 0.224 d_pr 0.212 
GTM 0.146 BLEU 0.199 BLEU 0.197 
TER -0.133 TER -0.192 TER -0.182 
Table 5. Pearson?s correlation between human scores and 
evaluation metrics. Legend: d = dependency f-score, _pr = 
predicate-only f-score, M = METEOR, WN = WordNet, 
H_FL = human fluency score, H_AC = human accuracy 
score, H_AVE = human average score.9 
 
Second, and somewhat surprisingly, in this 
detailed examination the relative order of the 
metrics changed. The predicate-only version of the 
dependency-based method appears to be less 
adequate for correlation with human scores than its 
non-restricted versions. As to the correlation with 
human evaluation of translation accuracy, our 
method currently falls short of METEOR and even 
NIST. This is caused by the fact that both 
METEOR and NIST assign relatively little 
importance to the position of a specific word in a 
sentence, therefore rewarding the translation for 
content rather than linguistic form. For our 
dependency-based method, the noise introduced by 
the parser might be the reason for low correlation: 
if even one side of the translation-reference pair 
contains parsing errors, this may lead to a less 
reliable score. An obvious solution to this problem, 
                                                 
9 In general terms, an increase of 0.015 between any two 
scores is significant with a 95% confidence interval. 
which we are examining at the moment, is to 
include a number of best parses for each side of the 
evaluation. 
High correlation with human judgements of 
fluency and lower correlation with accuracy results 
in a high second place for our dependency-based 
method when it comes to the average correlation 
coefficient. The WordNet-boosted dependency-
based method scores only slightly lower than 
METEOR with WordNet. These results are very 
encouraging, especially as we see a number of 
ways the dependency-based method could be 
further developed.  
5 Current and future work 
While the idea of a dependency-based method is a 
natural step in the direction of a deeper linguistic 
analysis for MT evaluation, it does require an LFG 
grammar and parser for the target language. There 
are several obvious areas for improvement with 
respect to the method itself. First, we would also 
like to adapt the process of translation-reference 
dependency comparison to include n-best parsers 
for the input sentences, as well as some basic 
transformations which would allow an even deeper 
logical analysis of input (e.g. passive to active 
voice transformation). 
 Second, we want to repeat both 
experiments using a paraphrase set derived from a 
large parallel corpus, rather than the test set, as 
described in Owczarzak et al (2006). While 
retaining the advantage of having a similar size to 
a corresponding set of WordNet synonyms, this set 
will also capture low-level syntactic variations, 
which can increase the number of matches and the 
correlation with human scores. 
 Finally, we want to take advantage of the 
fact that the score produced by the dependency-
based method is the proportional average of f-
scores for a group of up to 32 (but usually far 
fewer) different dependency types. We plan to 
implement a set of weights, one for each 
dependency type, trained in such a way as to 
maximize the correlation of the final dependency f-
score with human evaluation.  
6 Conclusions 
In this paper we present a novel way of 
evaluating MT output. So far, all metrics relied on 
86
comparing translation and reference on a string 
level. Even given reordering, stemming, and 
synonyms for individual words, current methods 
are still far from reaching human ability to assess 
the quality of translation. Our method compares 
the sentences on the level of their grammatical 
structure, as exemplified by their f-structure 
dependency triples produced by an LFG parser. 
The dependency-based method can be further 
augmented by using paraphrases or WordNet 
synonyms, and is available in full version and 
predicate-only version. In our experiments we 
showed that the dependency-based method 
correlates higher than any other metric with human 
evaluation of translation fluency, and shows high 
correlation with the average human score. The use 
of dependencies in MT evaluation is a rather new 
idea and requires more research to improve it, but 
the method shows potential to become an accurate 
evaluation metric.  
 
Acknowledgements 
This work was partly funded by Microsoft Ireland 
PhD studentship  2006-8  for the first author of the 
paper. We would also like to thank our reviewers 
for their insightful comments. All remaining errors 
are our own. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the ACL 2005 Workshop on Intrinsic 
and Extrinsic Evaluation Measures for MT and/or 
Summarization: 65-73. 
Joan Bresnan. 2001. Lexical-Functional Syntax, 
Blackwell, Oxford. 
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef 
van Genabith, and Andy Way. 2004. Long-Distance 
Dependency Resolution in Automatically Acquired 
Wide-Coverage PCFG-Based LFG Approximations, 
In Proceedings of ACL-04: 320-327 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. Proceedings of  
EACL 2006: 249-256 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. 
Proceedings of HLT 2002: 138-145. 
David Kauchak and Regina Barzilay. 2006. 
Paraphrasing for Automatic Evaluation. Proceedings 
of HLT-NAACL 2006: 45-462. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. Proceedings of the AMTA 2004 Workshop 
on Machine Translation: From real users to 
research: 115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. 
Alex Kulesza and Stuart M. Shieber. 2004. A learning 
approach to improving sentence-level MT evaluation. 
In Proceedings of the TMI 2004: 75-84. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. Proceedings of EACL 2006: 241-248. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19-51. 
Karolina Owczarzak, Declan Groves, Josef van 
Genabith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation. 
Proceedings of the HLT-NAACL 2006 Workshop on 
Statistical Machine Translation: 86-93. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
WeiJing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
ACL 2002: 311-318. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-TR-
125/CS-TR-4754/UMIACS-TR-2005-57, University 
of Maryland, College Park, MD. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula. 2006. A Study of 
Translation Error Rate with Targeted Human 
Annotation. Proceedings of AMTA 2006: 223-231. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. 
Ying Zhang and Stephan Vogel. 2004. Measuring 
confidence intervals for the machine translation 
evaluation metrics. Proceedings of TMI 2004: 85-94. 
87
Proceedings of the Second Workshop on Statistical Machine Translation, pages 104?111,
Prague, June 2007. c?2007 Association for Computational Linguistics
Labelled Dependencies in Machine Translation Evaluation 
Karolina Owczarzak Josef van Genabith Andy Way 
National Centre for Language Technology 
School of Computing, Dublin City University 
Dublin 9, Ireland 
{owczarzak,josef,away}@computing.dcu.ie 
    
 
Abstract 
We present a method for evaluating the 
quality of Machine Translation (MT) 
output, using labelled dependencies 
produced by a Lexical-Functional 
Grammar (LFG) parser. Our dependency-
based method, in contrast to most popular 
string-based evaluation metrics, does not 
unfairly penalize perfectly valid syntactic 
variations in the translation, and the 
addition of WordNet provides a way to 
accommodate lexical variation. In 
comparison with other metrics on 16,800 
sentences of Chinese-English newswire 
text, our method reaches high correlation 
with human scores.  
1 Introduction 
Since the creation of BLEU (Papineni et al, 2002) 
and NIST (Doddington, 2002), the subject of 
automatic evaluation metrics for MT has been 
given quite a lot of attention. Although widely 
popular thanks to their speed and efficiency, both 
BLEU and NIST have been criticized for 
inadequate accuracy of evaluation at the segment 
level (Callison-Burch et al, 2006). As string 
based-metrics, they are limited to superficial 
comparison of word sequences between a 
translated sentence and one or more reference 
sentences, and are unable to accommodate any 
legitimate grammatical variation when it comes to 
lexical choices or syntactic structure of the 
translation, beyond what can be found in the 
multiple references. A natural next step in the field 
of evaluation was to introduce metrics that would 
better reflect our human judgement by accepting 
synonyms in the translated sentence or evaluating 
the translation on the basis of what syntactic 
features it shares with the reference. 
Our method follows and substantially extends 
the earlier work of Liu and Gildea (2005), who use 
syntactic features and unlabelled dependencies to 
evaluate MT quality, outperforming BLEU on 
segment-level correlation with human judgement. 
Dependencies abstract away from the particulars of 
the surface string (and syntactic tree) realization 
and provide a ?normalized? representation of 
(some) syntactic variants of a given sentence.  
While Liu and Gildea (2005) calculate n-gram 
matches on non-labelled head-modifier sequences 
derived by head-extraction rules from syntactic 
trees, we automatically evaluate the quality of 
translation by calculating an f-score on labelled 
dependency structures produced by a Lexical-
Functional Grammar (LFG) parser. These 
dependencies differ from those used by Liu and 
Gildea (2005), in that they are extracted according 
to the rules of the LFG grammar and they are 
labelled with a type of grammatical relation that 
connects the head and the modifier, such as 
subject, determiner, etc. The presence of 
grammatical relation labels adds another layer of 
important linguistic information into the 
comparison and allows us to account for partial 
matches, for example when a lexical item finds 
itself in a correct relation but with an incorrect 
partner. Moreover, we use a number of best parses 
for the translation and the reference, which serves 
to decrease the amount of noise that can be 
introduced by the process of parsing and extracting 
dependency information. 
The translation and reference files are 
analyzed by a treebank-based, probabilistic LFG 
parser (Cahill et al, 2004), which produces a set of 
dependency triples for each input. The translation 
set is compared to the reference set, and the 
number of matches is calculated, giving the 
104
precision, recall, and f-score for each particular 
translation.   
In addition, to allow for the possibility of valid 
lexical differences between the translation and the 
references, we follow Kauchak and Barzilay 
(2006) in adding a number of synonyms in the 
process of evaluation to raise the number of 
matches between the translation and the reference, 
leading to a higher score. 
In an experiment on 16,800 sentences of 
Chinese-English newswire text with segment-level 
human evaluation from the Linguistic Data 
Consortium?s (LDC) Multiple Translation project, 
we compare the LFG-based evaluation method 
with other popular metrics like BLEU, NIST, 
General Text Matcher (GTM) (Turian et al, 2003), 
Translation Error Rate (TER) (Snover et al, 
2006)1, and METEOR (Banerjee and Lavie, 2005), 
and we show that combining dependency 
representations with synonyms leads to a more 
accurate evaluation that correlates better with 
human judgment. Although evaluated on a 
different test set, our method also outperforms the 
correlation with human scores reported in Liu and 
Gildea (2005). 
The remainder of this paper is organized as 
follows: Section 2 gives a basic introduction to 
LFG; Section 3 describes related work; Section 4 
describes our method and gives results of the 
experiment on the Multiple Translation data; 
Section 5 discusses ongoing work; Section 6 
concludes. 
2 Lexical-Functional Grammar 
In Lexical-Functional Grammar (Kaplan and 
Bresnan, 1982; Bresnan, 2001) sentence structure 
is represented in terms of c(onstituent)-structure 
and f(unctional)-structure. C-structure represents 
the word order of the surface string and the 
hierarchical organisation of phrases in terms of 
CFG trees. F-structures are recursive feature (or 
attribute-value) structures, representing abstract 
grammatical relations, such as subj(ect), obj(ect), 
obl(ique), adj(unct), etc., approximating to 
predicate-argument structure or simple logical 
forms. C-structure and f-structure are related in 
                                                 
1 We omit HTER (Human-Targeted Translation Error 
Rate), as it is not fully automatic and requires human 
input. 
terms of functional annotations (attribute-value 
structure equations) in c-structure trees, describing 
f-structures.  
While c-structure is sensitive to surface 
rearrangement of constituents, f-structure abstracts 
away from the particulars of the surface 
realization. The sentences John resigned yesterday 
and Yesterday, John resigned will receive different 
tree representations, but identical f-structures, 
shown in (1). 
 
(1) C-structure:                         F-structure: 
 
              S 
                  
      
 NP                      VP 
   |                     
John       
              V               NP-TMP 
               |                      | 
       resigned       yesterday 
                         
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
                     S 
                  
      
    NP       NP       VP 
      |                 |            | 
Yesterday  John        V              
                                    | 
                            resigned                             
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
 
Note that if these sentences were a translation-
reference pair, they would receive a less-than-
perfect score from string-based metrics. For 
example, BLEU with add-one smoothing2 gives 
this pair a score of barely 0.3781. This is because, 
although all three unigrams from the ?translation? 
(John; resigned; yesterday) are present in the 
reference, which contains four items including the 
comma (Yesterday; ,; John; resigned), the 
?translation? contains only one bigram (John 
resigned) that matches the ?reference? (Yesterday 
,; , John; John resigned), and no matching 
trigrams. 
The f-structure can also be described in terms 
of a flat set of triples. In triples format, the f-
structure in (1) is represented as follows: 
{subj(resign, john), pers(john, 3), num(john, sg), 
tense(resign, past), adj(resign, yesterday), 
pers(yesterday, 3), num(yesterday, sg)}. 
                                                 
2 We use smoothing because the original BLEU metric 
gives zero points to sentences with fewer than one four-
gram. 
105
Cahill et al (2004) presents a set of Penn-II 
Treebank-based LFG parsing resources. Their 
approach distinguishes 32 types of dependencies, 
including grammatical functions and 
morphological information. This set can be divided 
into two major groups: a group of predicate-only 
dependencies and non-predicate dependencies. 
Predicate-only dependencies are those whose path 
ends in a predicate-value pair, describing 
grammatical relations. For example, for the f-
structure in (1), predicate-only dependencies would 
include: {subj(resign, john), adj(resign, 
yesterday)}.  
Other predicate-only dependencies include: 
apposition, complement, open complement, 
coordination, determiner, object, second object, 
oblique, second oblique, oblique agent, possessive, 
quantifier, relative clause, topic, and relative 
clause pronoun. The remaining non-predicate 
dependencies are: adjectival degree, coordination 
surface form, focus, complementizer forms: if, 
whether, and that, modal, number, verbal particle, 
participle, passive, person, pronoun surface form, 
tense, and infinitival clause. 
In parser evaluation, the quality of the f-
structures produced automatically can be checked 
against a set of gold standard sentences annotated 
with f-structures by a linguist. The evaluation is 
conducted by calculating the precision and recall 
between the set of dependencies produced by the 
parser, and the set of dependencies derived from 
the human-created f-structure. Usually, two 
versions of f-score are calculated: one for all the 
dependencies for a given input, and a separate one 
for the subset of predicate-only dependencies. 
In this paper, we use the parser developed by 
Cahill et al (2004), which automatically annotates 
input text with c-structure trees and f-structure 
dependencies, obtaining high precision and recall 
rates. 3  
3 Related work 
3.1 String-based metrics 
The insensitivity of BLEU and NIST to perfectly 
legitimate syntactic and lexical variation has been 
raised, among others, in Callison-Burch et al 
(2006), but the criticism is widespread. Even the 
                                                 
3 A demo of the parser can be found at http://lfg-
demo.computing.dcu.ie/lfgparser.html 
creators of BLEU point out that it may not 
correlate particularly well with human judgment at 
the sentence level (Papineni et al, 2002).  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic MT evaluation metrics. Some of them 
concentrate mainly on word order, like General 
Text Matcher (Turian et al, 2003), which 
calculates precision and recall for translation-
reference pairs, weighting contiguous matches 
more than non-sequential matches, or Translation 
Error Rate (Snover et al, 2006), which computes 
the number of substitutions, insertions, deletions, 
and shifts necessary to transform the translation 
text to match the reference. Others try to 
accommodate both syntactic and lexical 
differences between the candidate translation and 
the reference, like CDER (Leusch et al, 2006), 
which employs a version of edit distance for word 
substitution and reordering; or METEOR 
(Banerjee and Lavie, 2005), which uses stemming 
and WordNet synonymy. Kauchak and Barzilay 
(2006) and Owczarzak et al (2006) use 
paraphrases during BLEU and NIST evaluation to 
increase the number of matches between the 
translation and the reference; the paraphrases are 
either taken from WordNet4 in Kauchak and 
Barzilay (2006) or derived from the test set itself 
through automatic word and phrase alignment in 
Owczarzak et al (2006). Another metric making 
use of synonyms is the linear regression model 
developed by Russo-Lassner et al (2005), which 
makes use of stemming, WordNet synonymy, verb 
class synonymy, matching noun phrase heads, and 
proper name matching. Kulesza and Shieber 
(2004), on the other hand, train a Support Vector 
Machine using features such as proportion of n-
gram matches and word error rate to judge a given 
translation?s distance from human-level quality.  
3.2 Dependency-based metric 
The metrics described above use only string-based 
comparisons, even while taking into consideration 
reordering. By contrast, Liu and Gildea (2005) 
present three metrics that use syntactic and 
unlabelled dependency information. Two of these 
metrics are based on matching syntactic subtrees 
between the translation and the reference, and one 
                                                 
4 http://wordnet.princeton.edu/ 
106
is based on matching headword chains, i.e. 
sequences of words that correspond to a path in the 
unlabelled dependency tree of the sentence. 
Dependency trees are created by extracting a 
headword for each node of the syntactic tree, 
according to the rules used by the parser of Collins 
(1999), where every subtree represents the 
modifier information for its root headword. The 
dependency trees for the translation and the 
reference are converted into flat headword chains, 
and the number of overlapping n-grams between 
the translation and the reference chains is 
calculated. Our method, extending this line of 
research with the use of labelled LFG 
dependencies, partial matching, and n-best parses, 
allows us to considerably outperform Liu and 
Gildea?s (2005) highest correlations with human 
judgement (they report 0.144 for the correlation 
with human fluency judgement, 0.202 for the 
correlation with human overall judgement), 
although it has to be kept in mind that such 
comparison is only tentative, as their correlation is 
calculated on a different test set. 
4 LFG f-structure in MT evaluation 
LFG-based automatic MT evaluation reflects the 
same process that underlies the evaluation of 
parser-produced f-structure quality against a gold 
standard: we parse the translation and the 
reference, and then, for each sentence, we check 
the set of labelled translation dependencies against 
the set of labelled reference dependencies, 
counting the number of matches. As a result, we 
obtain the precision and recall scores for the 
translation, and we calculate the f-score for the 
given pair.  
4.1 Determining parser noise 
Because we are comparing two outputs that were 
produced automatically, there is a possibility that 
the result will not be noise-free, even if the parser 
fails to provide a parse only in 0.1% of cases. 
To assess the amount of noise that the parser 
introduces, Owczarzak et al (2006) conducted an 
experiment where 100 English sentences were 
hand-modified so that the position of adjuncts was 
changed, but the sentence remained grammatical 
and the meaning was not influenced. This way, an 
ideal parser should give both the source and the 
modified sentence the same f-structure, similarly to 
the example presented in (1). The modified 
sentences were treated like a translation file, and 
the original sentences played the part of the 
reference. Each set was run through the parser, and 
the dependency triples obtained from the 
?translation? were compared against the 
dependency triples for the ?reference?, calculating 
the f-score. Additionally, the same ?translation-
reference? set was scored with other metrics (TER, 
METEOR, BLEU, NIST, and GTM). The results, 
including the distinction between f-scores for all 
dependencies and predicate-only dependencies, 
appear in Table 1. 
 
 baseline modified 
TER 0.0 6.417 
METEOR   1.0 0.9970 
BLEU 1.0000 0.8725 
NIST 11.5232 11.1704 (96.94%) 
GTM 100 99.18 
dep f-score  100 96.56 
dep_preds f-score 100 94.13 
Table 1. Scores for sentences with reordered adjuncts 
 
The baseline column shows the upper bound for a 
given metric: the score which a perfect translation, 
word-for-word identical to the reference, would 
obtain.5 The other column lists the scores that the 
metrics gave to the ?translation? containing 
reordered adjunct. As can be seen, the dependency 
and predicate-only dependency scores are lower 
than the perfect 100, reflecting the noise 
introduced by the parser. 
 We propose that the problem of parser 
noise can be alleviated by introducing a number of 
best parses into the comparison between the 
translation and the reference. Table 2 shows how 
increasing the number of parses available for 
comparison brings our method closer to an ideal 
noise-free parser.  
 
                                                 
5 Two things have to be noted here: (1) in the case of 
NIST the perfect score differs from text to text, which is 
why the percentage points are provided along the 
numerical score, and (2) in the case of TER the lower 
the score, the better the translation, so the perfect 
translation will receive 0, and there is no upper bound 
on the score, which makes this particular metric 
extremely difficult to directly compare with others. 
107
 dependency f-score 
1 best 96.56 
2 best 97.31 
5 best 97.90 
10 best 98.31 
20 best 98.59 
30 best 98.74 
50 best 98.79 
baseline 100 
Table 2.  Dependency f-scores for sentences with reordered 
adjuncts with n-best parses available 
 
It has to be noted, however, that increasing the 
number of parses beyond a certain threshold does 
little to further improve results, and at the same 
time it considerably decreases the efficiency of the 
method, so it is important to find the right balance 
between these two factors. In our opinion, the 
optimal value would be 10-best parses. 
4.2 Correlation with human judgement ? 
MultiTrans 
4.2.1 Experimental design 
To evaluate the correlation with human 
assessment, we used the data from the Linguistic 
Data Consortium Multiple Translation Chinese 
(MTC) Parts 2 and 4, which consists of multiple 
translations of Chinese newswire text, four human-
produced references, and segment-level human 
scores for a subset of the translation-reference 
pairs. Although a single translated segment was 
always evaluated by more than one judge, the 
judges used a different reference every time, which 
is why we treated each translation-reference-
human score triple as a separate segment. In effect, 
the test set created from this data contained 16,800 
segments. As in the previous experiment, the 
translation was scored using BLEU, NIST, GTM, 
TER, METEOR, and our labelled dependency-
based method. 
4.2.2 Labelled dependency-based method 
We examined a number of modifications of the 
dependency-based method in order to find out 
which one gives the highest correlation with 
human scores. The correlation differences between 
immediate neighbours in the ranking were often 
too small to be statistically significant; however, 
there is a clear overall trend towards improvement.  
Besides the plain version of the dependency f-
score, we also looked at the f-score calculated on 
predicate dependencies only (ignoring ?atomic? 
features such as person, number, tense, etc.), which 
turned out not to correlate well with human 
judgements. 
Another addition was the use of 2-, 10-, or 50-
best parses of the translation and reference 
sentences, which partially neutralized parser noise 
and resulted in increased correlations.  
We also created a version where predicate 
dependencies of the type subj(resign,John) are split 
into two parts, each time replacing one of the 
elements participating in the relation with a 
variable, giving in effect subj(resign,x) and 
subj(y,John). This lets us score partial matches, 
where one correct lexical object happens to find 
itself in the correct relation, but with an incorrect 
?partner?.  
Lastly, we added WordNet synonyms into the 
matching process to accommodate lexical 
variation, and to compare our WordNet-enhanced 
method with the WordNet-enhanced version of 
METEOR.  
4.2.3 Results 
We calculated Pearson?s correlation coefficient for 
segment-level scores that were given by each 
metric and by human judges. The results of the 
correlation are shown in Table 3. Note that the 
correlation for TER is negative, because in TER 
zero is the perfect score, in contrast to other 
metrics where zero is the worst possible score; 
however, this time the absolute values can be 
easily compared to each other. Rows are ordered 
by the highest value of the (absolute) correlation 
with the human score. 
First, it seems like none of the metrics is very 
good at reflecting human fluency judgments; the 
correlation values in the first column are 
significantly lower than the correlation with 
accuracy. This finding has been previously 
reported, among others, in Liu and Gildea (2005). 
However, the dependency-based method in almost 
all its versions has decidedly the highest 
correlation in this area. This can be explained by 
the method?s sensitivity to the grammatical 
structure of the sentence: a more grammatical 
translation is also a translation that is more fluent. 
As to the correlation with human evaluation of 
translation accuracy, our method currently falls 
108
short of METEOR. This is caused by the fact that 
METEOR assign relatively little importance to the 
position of a specific word in a sentence, therefore 
rewarding the translation for content rather than 
linguistic form. Interestingly, while METEOR, 
with or without WordNet, considerably 
outperforms all other metrics when it comes to the 
correlation with human judgements of translation 
accuracy, it falls well behind most versions of our 
dependency-based method in correlation with 
human scores of translation fluency. 
Surprisingly, adding partial matching to the 
dependency-based method resulted in the greatest 
increase in correlation levels, to the extent that the 
partial-match versions consistently outperformed 
versions with a larger number of parses available 
but without the partial match. The most interesting 
effect was that the partial-match versions (even 
those with just a single parse) offered results 
comparable to or higher than the addition of 
WordNet to the matching process when it comes to 
accuracy and overall judgement. 
5 Current and future work 
Fluency and accuracy are two very different 
aspects of translation quality, each with its own set 
of conditions along which the input is evaluated. 
Therefore, it seems unfair to expect a single 
automatic metric to correlate highly with human 
judgements of both at the same time. This pattern 
is very noticeable in Table 3: if a metric is 
(relatively) good at correlating with fluency, its 
accuracy correlation suffers (GTM might serve as 
an example here), and the opposite holds as well 
(see METEOR?s scores). It does not mean that any 
improvement that increases the method?s 
correlation with one aspect will result in a decrease 
in the correlation with the other aspect; but it does 
suggest that a possible way of development would 
be to target these correlations separately, if we 
want our automated metrics to reflect human 
scores better. At the same time, string-based 
metrics might have already exhausted their 
potential when it comes to increasing their 
correlation with human evaluation; as has been 
pointed out before, these metrics can only tell us 
that two strings differ, but they cannot distinguish 
legitimate grammatical variance from 
ungrammatical variance. As the quality of MT  
 
 
Table 3. Pearson?s correlation between human scores and 
evaluation metrics. Legend: d = dependency f-score, _pr = 
predicate-only f-score, 2, 10, 50 = n-best parses; var = 
partial-match version; M = METEOR, WN = WordNet6 
 
improves, the community will need metrics that are 
more sensitive in this respect. After all, the true 
quality of MT depends on producing grammatical 
output which describes the same concept as the 
source utterance, and the string identity with a 
reference is only a very selective approximation of 
this goal.  
                                                 
6 In general terms, an increase of 0.022 or more between 
any two scores in the same column is significant with a 
95% confidence interval. The statistical significance of 
correlation differences was calculated using Fisher?s z? 
transformation and the general formula for confidence 
interval. 
 
fluency  accuracy  average  
d_50+WN 0.177 M+WN 0.294 M+WN 0.255 
d+WN 0.175 M   0.278 d_50_var 0.252 
d_50_var 0.174 d_50_var 0.273 d_50+WN 0.250 
GTM 0.172 NIST 0.273 d_10_var 0.250 
d_10_var 0.172 d_10_var 0.273 d_2_var 0.247 
d_50 0.171 d_2_var 0.270 d+WN 0.244 
d_2_var 0.168 d_50+WN 0.269 d_50 0.243 
d_10 0.168 d_var 0.266 d_var 0.243 
d_var 0.165 d_50 0.262 M   0.242 
d_2 0.164 d_10 0.262 d_10 0.242 
d   0.161 d+WN 0.260 NIST 0.238 
BLEU 0.155 d_2 0.257 d_2 0.237 
M+WN 0.153 d  0.256 d   0.235 
M   0.149 d_pr 0.240 d_pr 0.216 
NIST 0.146 GTM 0.203 GTM 0.208 
d_pr 0.143 BLEU 0.199 BLEU 0.197 
TER -0.133 TER -0.192 TER -0.182 
109
 In order to maximize the correlation with 
human scores of fluency, we plan to look more 
closely at the parser output, and implement some 
basic transformations which would allow an even 
deeper logical analysis of input (e.g. passive to 
active voice transformation). 
  Additionally, we want to take advantage of 
the fact that the score produced by the dependency-
based method is the proportional average of 
matches for a group of up to 32 (but usually far 
fewer) different dependency types. We plan to 
implement a set of weights, one for each 
dependency type, trained in such a way as to 
maximize the correlation of the final dependency f-
score with human evaluation. In a preliminary 
experiment, for example, assigning a low weight to 
the topic dependency increases our correlations 
slightly (this particular case can also be seen as a 
transformation into a more basic logical form by 
removing non-elementary dependency types). 
 In a similar direction, we want to 
experiment more with the f-score calculations. 
Initial check shows that assigning a higher weight 
to recall than to precision improves results. 
 To improve the correlation with accuracy 
judgements, we would like to experiment using a 
paraphrase set derived from a large parallel corpus, 
as described in Owczarzak et al (2006). While 
retaining the advantage of having a similar size to 
a corresponding set of WordNet synonyms, this set 
will also capture low-level syntactic variations, 
which can increase the number of matches.  
6 Conclusions 
In this paper we present a linguistically-
motivated method for automatically evaluating the 
output of Machine Translation. Most currently 
used popular metrics rely on comparing translation 
and reference on a string level. Even given 
reordering, stemming, and synonyms for individual 
words, current methods are still far from reaching 
human ability to assess the quality of translation, 
and there exists a need in the community to 
develop more dependable metrics. Our method 
explores one such direction of development, 
comparing the sentences on the level of their 
grammatical structure, as exemplified by their f-
structure labelled dependency triples produced by 
an LFG parser. In our experiments we showed that 
the dependency-based method correlates higher 
than any other metric with human evaluation of 
translation fluency, and shows high correlation 
with the average human score. The use of 
dependencies in MT evaluation has not been 
extensively researched before (one exception here 
would be Liu and Gildea (2005)), and requires 
more research to improve it, but the method shows 
potential to become an accurate evaluation metric.  
 
Acknowledgements 
This work was partly funded by Microsoft Ireland 
PhD studentship  2006-8  for the first author of the 
paper. We would also like to thank our reviewers 
and Dan Melamed for their insightful comments. 
All remaining errors are our own. 
 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization at the Association for Computational 
Linguistics Conference 2005: 65-73. Ann Arbor, 
Michigan. 
Joan Bresnan. 2001. Lexical-Functional Syntax, 
Blackwell, Oxford. 
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef 
van Genabith, and Andy Way. 2004. Long-Distance 
Dependency Resolution in Automatically Acquired 
Wide-Coverage PCFG-Based LFG Approximations, 
In Proceedings of Association for Computational 
Linguistics 2004: 320-327. Barcelona, Spain. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. Proceedings of the 
European Chapter of the Association for 
Computational Linguistics 2006: 249-256. Oslo, 
Norway. 
Michael J. Collins. 1999. Head-driven Statistical 
Models for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania, Philadelphia. 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. 
Proceedings of Human Language Technology 
Conference 2002: 138-145. San Diego, California. 
Kaplan, R. M., and J. Bresnan. 1982. Lexical-functional 
Grammar: A Formal System for Grammatical 
110
Representation.  In J. Bresnan (ed.), The Mental 
Representation of Grammatical Relations.  MIT 
Press, Cambridge. 
David Kauchak and Regina Barzilay. 2006. 
Paraphrasing for Automatic Evaluation. Proceedings 
of Human Language Technology ? North American 
Chapter of the Association for Computational 
Linguistics Conference 2006: 45-462. New York, 
New York. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. Proceedings of the Workshop on Machine 
Translation: From real users to research at the 
Association for Machine Translation in the Americas 
Conference 2004: 115-124. Washington, DC. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. Phuket, Thailand. 
Alex Kulesza and Stuart M. Shieber. 2004. A learning 
approach to improving sentence-level MT evaluation. 
In Proceedings of the Conference on Theoretical and 
Methodological Issues in Machine Translation 2004: 
75-84. Baltimore, Maryland. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. Proceedings of European Chapter of the 
Association for Computational Linguistics 
Conference 2006: 241-248. Trento, Italy. 
Ding Liu and Daniel Gildea. 2005. Syntactic Features 
for Evaluation of Machine Translation. In 
Proceedings of the Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine 
Translation and/or Summarization at the Association 
for Computational Linguistics Conference 2005. Ann 
Arbor, Michigan. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19-51. 
Karolina Owczarzak, Declan Groves, Josef van 
Genabith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation. 
Proceedings of the Workshop on Statistical Machine 
Translation at the Human Language Technology ? 
North American Chapter of the Association for 
Computational Linguistics Conference 2006: 86-93. 
New York, New York. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
WeiJing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
Association for Computational Linguistics 
Conference 2002: 311-318. Philadelphia, 
Pennsylvania. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-TR-
125/CS-TR-4754/UMIACS-TR-2005-57, University 
of Maryland, College Park, Maryland. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula. 2006. A Study of 
Translation Error Rate with Targeted Human 
Annotation. Proceedings of the Association for 
Machine Translation in the Americas Conference 
2006: 223-231. Boston, Massachusetts. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. New Orleans, Luisiana. 
Ying Zhang and Stephan Vogel. 2004. Measuring 
confidence intervals for the machine translation 
evaluation metrics. Proceedings of Conference on 
Theoretical and Methodological Issues in Machine 
Translation 2004: 85-94. Baltimore, Maryland. 
111
Proceedings of the Third Workshop on Statistical Machine Translation, pages 171?174,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
MATREX: the DCU MT System for WMT 2008
John Tinsley, Yanjun Ma, Sylwia Ozdowska, Andy Way
National Centre for Language Technology
Dublin City University
Dublin 9, Ireland
{jtinsley, yma, sozdowska, away}@computing.dcu.ie
Abstract
In this paper, we give a description of the ma-
chine translation system developed at DCU
that was used for our participation in the eval-
uation campaign of the Third Workshop on
Statistical Machine Translation at ACL 2008.
We describe the modular design of our data-
driven MT system with particular focus on
the components used in this participation. We
also describe some of the significant modules
which were unused in this task.
We participated in the EuroParl task for the
following translation directions: Spanish?
English and French?English, in which we em-
ployed our hybrid EBMT-SMT architecture to
translate. We also participated in the Czech?
English News and News Commentary tasks
which represented a previously untested lan-
guage pair for our system. We report results
on the provided development and test sets.
1 Introduction
In this paper, we present the Data-Driven MT sys-
tems developed at DCU, MATREX (Machine Trans-
lation using Examples). This system is a hybrid sys-
tem which exploits EBMT and SMT techniques to
build a combined translation model.
We participated in both the French?English and
Spanish?English EuroParl tasks. In these two tasks,
we monolingually chunk both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004). We then align these chunks
using a dynamic programming, edit-distance-style
algorithm and combine them with phrase-based
SMT-style chunks into a single translation model.
We also participated in the Czech?English News
Commentary and News tasks. This language pair
represents a new challenge for our system and pro-
vides a good test of its flexibility.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the chunking and chunk
alignment strategies used for the shared task. In Sec-
tion 3, we outline the complete system setup for the
shared task, and in Section 4 we give some results
and discussion thereof.
2 The MATREX System
The MATREX system is a modular hybrid data-
driven MT system, built following established De-
sign Patterns, which exploits aspects of both the
EBMT and SMT paradigms. It consists of a num-
ber of extendible and re-implementable modules, the
most significant of which are:
? Word Alignment Module: outputs a set of word
alignments given a parallel corpus,
? Chunking Module: outputs a set of chunks
given an input corpus,
? Chunk Alignment Module: outputs aligned
chunk pairs given source and target chunks ex-
tracted from comparable corpora,
? Decoder: returns optimal translation given a
set of aligned sentence, chunk/phrase and word
pairs.
In some cases, these modules may comprise
wrappers around pre-existing software. For exam-
ple, our system configuration for the shared task
incorporates a wrapper around GIZA++ (Och and
Ney, 2003) for word alignment and a wrapper
around Moses (Koehn et al, 2007) for decoding. It
171
should be noted, however, that the complete system
is not limited to using only these specific module
choices. The following subsections describe those
modules unique to our system.
2.1 Marker-Based Chunking
The chunking module used for the shared task is
based on the Marker Hypothesis, a psycholinguistic
constraint which posits that all languages are marked
for surface syntax by a specific closed set of lex-
emes or morphemes which signify context. Using a
set of closed-class (or ?marker?) words for a particu-
lar language, such as determiners, prepositions, con-
junctions and pronouns, sentences are segmented
into chunks. A chunk is created at each new occur-
rence of a marker word with the restriction that each
chunk must contain at least one content (or non-
marker) word. An example of this chunking strategy
for English and Spanish is given in Figure 1.
2.2 Chunk Alignment
In order to align the chunks obtained by the chunk-
ing procedures described in Section 2.1, we make
use of an ?edit-distance-style? dynamic program-
ming alignment algorithm.
In the following, a denotes an alignment between
a target sequence e consisting of I chunks and a
source sequence f consisting of J chunks. Given
these sequences of chunks, we are looking for the
most likely alignment a?:
a? = argmax
a
P(a|e, f) = argmax
a
P(a, e|f).
We first consider alignments such as those ob-
tained by an edit-distance algorithm, i.e.
a = (t1, s1)(t2, s2) . . . (tn, sn),
with ?k ? J1, nK, tk ? J0, IK and sk ? J0, JK, and
?k < k?:
tk ? tk? or tk? = 0,
sk ? sk? or sk? = 0,
where tk = 0 (resp. sk = 0) denotes a non-aligned
target (resp. source) chunk.
We then assume the following model:
P(a, e|f) = ?kP(tk, sk, e|f) = ?kP(etk |fsk),
where P(e0|fj) (resp. P(ei|f0)) denotes an ?inser-
tion? (resp. ?deletion?) probability.
Assuming that the parameters P(etk |fsk) are
known, the most likely alignment is computed by
a simple dynamic-programming algorithm.1
Instead of using an Expectation-Maximization al-
gorithm to estimate these parameters, as commonly
done when performing word alignment (Brown
et al, 1993; Och and Ney, 2003), we directly com-
pute these parameters by relying on the information
contained within the chunks. The conditional prob-
ability P(etk |fsk) can be computed in several ways.
In our experiments, we have considered three main
sources of knowledge: (i) word-to-word translation
probabilities, (ii) word-to-word cognates, and (iii)
chunk labels. These sources of knowledge are com-
bined in a log-linear framework. The weights of
the log-linear model are not optimised; we experi-
mented with different sets of parameters and did not
find any significant difference as long as the weights
stay in the interval [0.5 ? 1.5]. Outside this inter-
val, the quality of the model decreases. More details
about the combination of knowledge sources can be
found in (Stroppa and Way, 2006).
2.3 Unused Modules
There are numerous other features available in our
system which, due to time constraints, were not ex-
ploited for the purposes of the shared task. They
include:
? Word packing (Ma et al, 2007): a bilingually
motivated packing of words that changes the
basic unit of the alignment process in order to
simplify word alignment.
? Supertagging (Hassan et al, 2007b): incorpo-
rating lexical syntactic descriptions, in the form
of supertags, to the language model and target
side of the translation model in order to better
inform decoding.
? Source-context features (Stroppa et al, 2007):
use memory-based classification to incorporate
context-informed features on the source side of
the translation model.
? Treebank-based phrase extraction (Tinsley
et al, 2007): extract word and phrase align-
ments based on linguistically informed sub-
sentential alignment of the parallel data.
1This algorithm is actually a classical edit-distance al-
gorithm in which distances are replaced by opposite-log-
conditional probabilities.
172
English: [I voted] [in favour] [of the strategy presented] [by the council] [concerning relations] [with
Mediterranean countries]
Spanish: [He votado] [a favor] [de la estrategia presentada] [por el consejo] [relativa las relaciones]
[con los pa??ses mediterrane?os]
Figure 1: English and Spanish Marker-Based chunking
Filter criteria es?en fr?en cz?en
Initial Total 1258778 1288074 1096941
Blank Lines 5632 4200 2
Length 6794 8361 2922
Fertility 120 82 1672
Final Total 1246234 1275432 1092345
Table 1: Summary of pre-processing on training data.
3 Shared Task Setup
The following section describes the system setup
using the Spanish?English and French?English Eu-
roParl, and Czech?English CzEng training data.
3.1 Pre-processing
For all tasks we initially tokenised the data (Czech
data was already tokenised) and removed blank
lines. We then filtered out sentence pairs based on
length (>100 words) and fertility (9:1 word ratio).
Finally we lowercased the data. Details of this pre-
processing are given in Table 1.
3.2 System Configuration
As mentioned in Section 2, our word alignment
module employs a wrapper around GIZA++.
We built a 5-gram language model based the tar-
get side of the training data. This was done using
the SRI Language Modelling toolkit (Stolcke, 2002)
employing linear interpolation and modified Kneser-
Ney discounting (Chen and Goodman, 1996).
Our phrase-table comprised a combination of
marker-based chunk pairs2, extracted as described
in Sections 2.1 and 2.2, and word-alignment-based
phrase pairs extracted using the ?grow-diag-final?
method of Koehn et al (2003), with a maximum
phrase length of 7 words. Phrase translation proba-
bilities were estimated by relative frequency over all
phrase pairs and were combined with other features,
2This module was omitted from the Czech?English system
as we have yet to verify whether marker-based chunking is ap-
propriate for Czech.
System BLEU (-EBMT) BLEU (+EBMT)
es?en 0.3283 0.3287
fr?en 0.2768 0.2770
cz?en 0.2235 -
Table 2: Summary of results on developments sets de-
vtest2006 for EuroParl tasks and nc-test2007 for cz?en
tasks.
System BLEU (-EBMT) BLEU (+EBMT)
es?en 0.3274 0.3285
fr?en 0.3163 0.3174
cz?en (news) 0.1458 -
cz?en (nc) 0.2217 -
Table 3: Summary of results on 2008 test data.
such as a reordering model, in a log-linear combina-
tion of functions.
We tuned our system on the development set de-
vtest2006 for the EuroParl tasks and on nc-test2007
for Czech?English, using minimum error-rate train-
ing (Och, 2003) to optimise BLEU score.
Finally, we carried out decoding using a wrapper
around the Moses decoder.
3.3 Post-processing
Case restoration was carried out by training the sys-
tem outlined above - without the EBMT chunk ex-
traction - to translate from the lowercased version
of the applicable target language training data to the
truecased version. We have previously shown this
approach to be very effective for both case and punc-
tuation restoration (Hassan et al, 2007a). The trans-
lations were then detokenised.
4 Results
The system output is evaluated with respect to
BLEU score. Results on the development sets and
test sets for each task are given in Tables 2 and 3
respectively, where ?-EBMT? indicates that EBMT
chunk modules were not used, and ?+EBMT? indi-
cates that they were used.
173
4.1 Discussion
Those configurations which incorporated the EBMT
chunks improved slightly over those which did not.
Groves (2007) has shown previously that combin-
ing EBMT and SMT translation models can lead to
considerable improvement over the baseline systems
from which they are derived. The results achieved
here lead us to believe that on such a large scale
there may be a more effective way to incorporate the
EBMT chunks.
Previous work has shown the EBMT chunks to
have higher precision than their SMT counterparts,
but they lack sufficient recall when used in isola-
tion (Groves, 2007). We believe that increasing their
influence in the translation model may lead to im-
proved translation accuracy. One experiment to this
effect would be to add the EBMT chunks as a sep-
arate phrase table in the log-linear model and allow
the decoder to chose when to use them.
Finally, we intend to exploit the unused modules
of the system in future experiments to investigate
their effects on the tasks presented here.
Acknowledgments
This work is supported by Science Foundation Ireland
(grant nos. 05/RF/CMS064 and OS/IN/1732). Thanks
also to the reviewers for their insightful comments and
suggestions.
References
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mercer,
R. L. (1993). The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Chen, S. F. and Goodman, J. (1996). An Empirical Study
of Smoothing Techniques for Language Modeling. In
Proceedings of the Thirty-Fourth Annual Meeting of
the Association for Computational Linguistics, pages
310?318, San Francisco, CA.
Gough, N. and Way, A. (2004). Robust Large-Scale
EBMT with Marker-Based Segmentation. In Proceed-
ings of the 10th International Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI-04), pages 95?104, Baltimore, MD.
Groves, D. (2007). Hybrid Data-Driven Models of Ma-
chine Translation. PhD thesis, Dublin City University,
Dublin, Ireland.
Hassan, H., Ma, Y., and Way, A. (2007a). MATREX: the
DCU Machine Translation System for IWSLT 2007. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 69?75, Trento, Italy.
Hassan, H., Sima?an, K., and Way, A. (2007b). Su-
pertagged Phrase-based Statistical Machine Transla-
tion. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL?07),
pages 288?295, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-
erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran,
C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and
Herbst, E. (2007). Moses: Open Source Toolkit for
Statistical Machine Translation. In Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague, Czech
Republic.
Koehn, P., Och, F. J., and Marcu, D. (2003). Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology (NAACL ?03), pages 48?54, Ed-
monton, Canada.
Ma, Y., Stroppa, N., and Way, A. (2007). Boostrap-
ping Word Alignment via Word Packing. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL?07), pages 304?311,
Prague, Czech Republic.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan., Sapporo,
Japan.
Och, F. J. and Ney, H. (2003). A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51.
Stolcke, A. (2002). SRILM - An Extensible Language
Modeling Toolkit. In Proceedings of the Interna-
tional Conference Spoken Language Processing, Den-
ver, CO.
Stroppa, N., van den Bosch, A., and Way, A. (2007).
Exploiting Source Similarity for SMT using Context-
Informed Features. In Proceedings of the 11th Interna-
tional Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-07), pages 231?
240, Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the DCU ma-
chine translation system for IWSLT 2006. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Translation, pages 31?36, Kyoto, Japan.
Tinsley, J., Hearne, M., and Way, A. (2007). Exploiting
Parallel Treebanks to Improve Phrase-Based Statisti-
cal Machine Translation. In Proceedings of the Sixth
International Workshop on Treebanks and Linguistic
Theories (TLT-07), pages 175?187, Bergen, Norway.
174
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69?77,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Word Alignment Using Syntactic Dependencies
Yanjun Ma1 Sylwia Ozdowska1 Yanli Sun2 Andy Way1
1 School of Computing, Dublin City University, Dublin, Ireland
{yma,sozdowska,away}@computing.dcu.ie
2 School of Applied Language and Intercultural Studies,
Dublin City University, Dublin, Ireland
yanli.sun2@mail.dcu.ie
Abstract
We introduce a word alignment framework
that facilitates the incorporation of syntax en-
coded in bilingual dependency tree pairs. Our
model consists of two sub-models: an anchor
word alignment model which aims to find a set
of high-precision anchor links and a syntax-
enhanced word alignment model which fo-
cuses on aligning the remaining words relying
on dependency information invoked by the ac-
quired anchor links. We show that our syntax-
enhanced word alignment approach leads to a
10.32% and 5.57% relative decrease in align-
ment error rate compared to a generative word
alignment model and a syntax-proof discrim-
inative word alignment model respectively.
Furthermore, our approach is evaluated ex-
trinsically using a phrase-based statistical ma-
chine translation system. The results show
that SMT systems based on our word align-
ment approach tend to generate shorter out-
puts. Without length penalty, using our word
alignments yields statistically significant im-
provement in Chinese?English machine trans-
lation in comparison with the baseline word
alignment.
1 Introduction
Automatic word alignment can be defined as the
problem of determining translational correspon-
dences at word level given a parallel corpus of
aligned sentences. Bilingual word alignment is a
fundamental component of most approaches to sta-
tistical machine translation (SMT). Dominant ap-
proaches to word alignment can be classified into
two main schools: generative and discriminative
word alignment models.
Generative word alignment models, initially de-
veloped at IBM (Brown et al, 1993), and then
augmented by an HMM-based model (Vogel et al,
1996), have provided powerful modeling capability
for word alignment. However, it is very difficult to
incorporate new features into these models. Dis-
criminative word alignment models, based on dis-
criminative training of a set of features (Liu et al,
2005; Moore, 2005), on the other hand, are more
flexible to incorporate new features, and feature se-
lection is essential to the performance of the system.
Syntactic annotation of bilingual corpora, which
can be obtained more efficiently and accurately with
the advances in monolingual language processing,
is a potential information source for word align-
ment tasks. For example, Part-of-Speech (POS) tags
of source and target words can be used to tackle
the data sparseness problem in discriminative word
alignment (Liu et al, 2005; Blunsom and Cohn,
2006). Shallow parsing has also been used to pro-
vide relevant information for alignment (Ren et al,
2007; Sun et al, 2000). Deeper syntax, e.g. phrase
or dependency structures, has been shown useful in
generative models (Wang and Zhou, 2004; Lopez
and Resnik, 2005), heuristic-based models (Ayan et
al., 2004; Ozdowska, 2004) and even for syntac-
tically motivated models such as ITG (Wu, 1997;
Cherry and Lin, 2006).
In this paper, we introduce an approach to im-
prove word alignment by incorporating syntactic de-
pendencies. Our approach is motivated by the fact
that words tend to be dependent on each other. If
69
we can first obtain a set of reliable anchor links, we
could take advantage of the syntactic dependencies
relating unaligned words to aligned anchor words to
expand the alignment. Figure 1 gives an illustrating
example. Note that the link (2, 4) can be easily iden-
tified, but the link involving the fourth Chinese word
(a function word denoting ?time?) (4, 4) is hard. In
such cases, we can make use of the dependency re-
lationship (?tclause?) between c2 and c4 to help the
alignment process. Given such an observation, our
model is composed of two related alignment models.
The first one is an anchor alignment model which is
used to find a set of anchor links; the other one is a
syntax-enhanced alignment model aiming to process
the words left unaligned after anchoring.
Figure 1: How syntactic dependencies can help word
alignment: an example
The remainder of this paper is organized as fol-
lows. In Section 2, we introduce our syntax-
enhanced discriminative word alignment approach.
The feature functions used are described in Sec-
tion 3. Experimental setting and results are pre-
sented in Section 4 and 5 respectively. In Section 6,
we compare our approach with other related word
alignment approaches. Section 7 concludes the pa-
per and gives avenues for future work.
2 Word Alignment Model
2.1 Notation
While in this paper we focus on Chinese?English,
the method proposed is applicable to any language
pair. The notation will assume Chinese?English
word alignment and Chinese?English MT. Here we
adopt a notation similar to (Brown et al, 1993).
Given a Chinese sentence cJ1 consisting of J words
{c1, ..., cJ} and an English sentence eI1 consisting of
I words e1, ..., eI , we define the alignment A be-
tween cJ1 and eI1 as a subset of the Cartesian product
of the word positions:
A ? {(j, i) : j = 1, ..., J ; i = 1, ..., I}
Our alignment representation is restricted so that
each source word can only be aligned to one tar-
get word. The alignment A consists of associations
j ? i = aj from a source position j to a target po-
sition i = aj . The ?null? alignment aj = 0 with the
?empty? word e0 is used to account for source words
that are not aligned to any target word.
We use A? to denote a subset of A. The indices of
the K source words involved in A? are represented
as ?K1 and the corresponding target indices for ?k
are represented as a?k . The unaligned source words
are represented as ??.
2.2 General Model
Given a source sentence cJ1 and target sentence eI1,
we seek to find the optimum alignment A? such that:
A? = argmax
A
P (A|cJ1 , eI1) (1)
We use a model (2) that directly models the link-
age between source and target words similarly to (It-
tycheriah and Roukos, 2005). We decompose this
model into an anchor alignment model (3) and a
syntax-enhanced model (4) by distinguishing the an-
chor alignment from the non-anchor alignment.
p(A|cJ1 , eI1) =
J
?
j=0
p(aj |cJ1 , eI1, aj?11 ) (2)
= 1Z ? p?(A?|c
J
1 , eI1) ? (3)
?
j???
p(aj|cJ1 , eI1, aj?11 , A?) (4)
2.3 Anchor Alignment Model
The anchor alignment model p?(A?) aims to find a
set of high precision links. Various approaches can
be used for this purpose. In this paper we adopted
the following two approaches.
2.3.1 Heuristics-based Approach
The problem of word alignment is regarded as a
process of word linkage disambiguation, i.e. choos-
ing the correct links between words from all com-
peting hypothesis (Melamed, 2000; Deng and Gao,
2007).
70
We constrain the link probabilities in such a way
that:
?i? ? {1, ..., I}, i? 6= i : p((j, i))p((j, i?)) > ?1 (5)
?j? ? {1, ..., J}, j? 6= j : p((j, i))p((j?, i)) > ?2 (6)
Condition (5) implies that for the source word cj ,
the link with the target word ei is more probable
(with reliability threshold ?1) than the link with any
other target word. Condition (6) guarantees that for
the target word ei, cj is the only most probable (with
threshold ?2) source word to be linked to.
2.3.2 Intersected Generative Word Alignment
Models
We can use the asymmetric IBM models for bidi-
rectional word alignment and get the intersection.
2.4 Syntax-Enhanced Word Alignment Model
The syntax-enhanced model is used to model the
alignment of the words left unaligned after anchor-
ing. We directly model the linkage between source
and target words using a discriminative word align-
ment framework where various features can be in-
corporated. Given a source word cj and the target
sentence eI1, we search for the alignment aj such
that:
a?j = argmax
aj
{p?M1 (aj |c
J
1 , eI1, a
j?1
1 , A?)} (7)
= argmax
aj
{?Mm=1 ?mhm(cJ1 , eI1, a
j
1, A?, Tc, Te)}
In this decision rule, we assume that a set of highly
reliable anchor alignments A? has been obtained,
and Tc (resp. Te) is used to denote the dependency
structure for source (resp. target) language. In such
a framework, various machine learning techniques
can be used for parameter estimation.
3 Feature Function for Syntax-Enhanced
Model
The various features used in our syntax-enhanced
model can be classified into three groups: statistics-
based features, syntactic features and relative distor-
tion features.
3.1 Statistics-based Features
3.1.1 IBM model 1 score
IBM model 1 is a position-independent word
alignment model which is often used to boot-
strap parameters for more complex models. Model
1 models the conditional distribution and uses a
uniform distribution for the dependencies between
source word positions and target word positions.
Pr(cJ1 , aJ1 |eI1) =
p(J |I)
(I + 1)J
J
?
j=1
p(cj |eaj ) (8)
3.1.2 Log-likelihood ratio
The log-likelihood ratio statistic has been found to
be accurate for modeling the associations between
rare events (Dunning, 1993). It has also been suc-
cessfully used to measure the associations between
word pairs (Melamed, 2000; Moore, 2005). Given
the following contingency table:
cj ?cj
ei a b
?ei c d
the log-likelihood ratio can be defined as:
G2(cj , ei) = ?2log
B(a|a + b, p1)B(c|c + d, p2)
B(a|a+ b, p)B(c|c + d, p)
where B(k|n, p) = (nk )pk(1 ? p)n?k are binomial
probabilities. The probability parameters can be ob-
tained using maximum likelihood estimates:
p1 =
a
a+ b , p2 =
c
c+ d (9)
p = a+ ca + b+ c+ d (10)
3.1.3 POS translation probability
The POS tags can provide effective information
for addressing the data sparseness problem using the
lexical features (Liu et al, 2005; Blunsom and Cohn,
2006). The POS translation probability can be easily
obtained using maximum likelihood estimation from
an annotated corpus:
Pr(Tc|Te) =
COL(Tc, Te)
COF (Te)
(11)
71
where Tc is a Chinese word?s POS tag and Te is an
English word?s POS tag. COL(Tc, Te) is the count
of Tc and Te being linked to each other in the corpus,
and COF (Te) is the frequency of Te in the corpus.
3.2 Syntactic Features
The dependency relation Re (resp. Rc) between two
English (resp. Chinese) words ei and ei? (resp. cj
and cj?) in the dependency tree of the English sen-
tence eI1 (resp. Chinese sentence cJ1 ) can be repre-
sented as a triple <ei, Re, ei?>(resp. <cj , Rc, ej?>).
Given cJ1 , eI1 and their syntactic dependency trees
TcJ1 , TeI1 , if ei is aligned to cj and ei? aligned to
cj? , according to the dependency correspondence as-
sumption (Hwa et al, 2002), there exists a triple
<cj , Rc, cj?>.
While we are not aiming to justify the feasibil-
ity of the dependency correspondence assumption
by proving to what extent Re = Rc under the con-
dition described above, we do believe that cj and cj?
are likely to be dependent on each other. Given the
anchor alignment A?, a candidate link (j, i) and the
dependency trees, we can design four classes of fea-
ture functions.
3.2.1 Agreement features
The agreement features can be further classi-
fied into dependency agreement features and depen-
dency label agreement features. Given a candidate
link (j, i) and the anchor alignment A?, the depen-
dency agreement (DA) feature function is defined as
follows:
hDA?1 =
?
?
?
?
?
1 if ? <cj, Rc, cj?>, <ei, Re, ei?>
and (j?, i?) ? A?,
0 otherwise.
(12)
By changing the dependency direction between the
words cj and cj? , we can derive another dependency
agreement feature:
hDA?2 =
?
?
?
?
?
1 if ? <cj? , Rc, cj>, <ei? , Re, ei>
and (j?, i?) ? A?,
0 otherwise.
(13)
We can define the dependency label agreement fea-
ture1 as follows:
hDLA?1 =
?
?
?
?
?
1 if ? <cj , Rc, cj?>, <ei, Re, ei?>
and (j?, i?) ? A?,Rc = Re,
0 otherwise.
(14)
Similarly we can obtain hDLA?2 by changing the
dependency direction.
3.2.2 Source word dependency features
Given a candidate link (j, i) and anchor alignment
A?, source language dependency features are used
to capture the dependency label between a source
word cj and a source anchor word ck ? ?. For
example, a feature function relating to dependency
type ?PRD? can be defined as:
hsrc?1?PRD =
?
?
?
?
?
1 if ? <cj, Rc, cj?>
and Rc =?PRD?,
0 otherwise.
(15)
By changing the direction we can obtain
hsrc?2?PRD.
3.2.3 Target word dependency features
Target word dependency features can be defined
in a similar way as source word dependency fea-
tures.
3.2.4 Target anchor feature
The target anchor feature defines whether the tar-
get word ei is an anchor word.
hsrc?1?PRD =
{
1 if i ? a?,
0 otherwise.
(16)
3.3 Relative distortion feature
We can design features encoding the relative dis-
tortion information which can be used to evaluate
a candidate link by computing its relative position
change with respect to the anchor alignment. The
relative position change of a candidate link l = (j, i)
is formally defined as follows:
1Note that we used the same dependency parser for source
and target language parsing.
72
D(l) = min(|dL|, |dR|) (17)
dL = (j ? jL) ? (i? iL) (18)
dR = (j ? jR)? (i? iR) (19)
where (iL, jL) is the leftmost anchor link of l,
(iR, jR) is the rightmost anchor link of l. The less
the relative position changes, the more likely the
candidate link is. With a set of anchor alignments,
we can obtain the distribution of the relative posi-
tion changes from an annotated corpus using maxi-
mum likelihood estimation. In our experiments, we
used the following four probabilities: p(D = 0),
p(D = 1, 2), p(D = 3, 4) and p(D > 4).
4 Experimental Setting
4.1 Data
The experiments were carried out using the
Chinese?English datasets provided within the
IWSLT 2007 evaluation campaign (Fordyce, 2007),
extracted from the Basic Travel Expression Corpus
(BTEC) (Takezawa et al, 2002). This multilingual
speech corpus contains sentences similar to those
that are usually found in phrase-books for tourists
going abroad.
We tagged all the sentences in the training and de-
vset3 using a maximum entropy-based POS tagger?
MXPOST (Ratnaparkhi, 1996), trained on the Penn
English and Chinese Treebanks. Both Chinese and
English sentences are parsed using the Malt depen-
dency parser (Nivre et al, 2007), which achieved
84% and 88% labelled attachment scores for Chi-
nese and English respectively.
4.1.1 Word Alignment
We manually annotated word alignments on de-
vset3. Since manual word alignment is an ambigu-
ous task, we also explicitly allow for ambiguous
alignments, i.e. the links are marked as sure (S) or
possible (P) (Och and Ney, 2003). IWSLT devset3
consists of 502 sentence pairs after cleaning. We
used the first 300 sentence pairs for training, the fol-
lowing 50 sentence pairs as validation set and the
last 152 sentence pairs for testing.
4.1.2 Machine Translation
Training was performed using the default training
set (39,952 sentence pairs), to which we added the
set devset1 (506 sentence pairs).2 We used devset2
(506 sentence pairs, 16 references) to tune various
parameters in the MT system and IWSLT 2007 test
set (489 sentence pairs, 6 references) for testing.
4.2 Alignment Training and Search
In our experiments, we treated anchor alignment and
syntax-enhanced alignment as separate processes in
a pipeline. The anchor alignments are kept fixed so
that the parameters in the syntax-enhanced model
can be optimized.3 We used the support vector ma-
chine (SVM) toolkit?SVM light4 to optimize the
parameters in (7). Our model is constrained in such
a way that each source word can only be aligned to
one target word. Therefore, in training, we trans-
form each possible link involving the words left un-
aligned after anchoring into an event. In testing, the
source words are consumed in sequence and the tar-
get words serve as states. The SVM dual variable
was used to measure the reliability of each candidate
link and the alignment link for each word is made
independently, which makes the alignment search
much easier. A threshold t was set as the minimal
reliability score for each link. t is optimized accord-
ing to alignment error rate (21) on the validation set.
4.3 Baselines
4.3.1 Word Alignment
We used the GIZA++ implementation of IBM
word alignment model 4 (Brown et al, 1993; Och
and Ney, 2003) for word alignment, and the heuris-
tics described in (Och and Ney, 2003) to derive the
intersection and refined alignment.
4.3.2 Machine Translation
We use a standard log-linear phrase-based SMT
(PB-SMT) model as a baseline: GIZA++ implemen-
tation of IBM word alignment model 4,5 the refine-
2More specifically, we chose the first English reference from
the 16 references and the Chinese sentence to construct new
sentence pairs.
3Note our anchor alignment does not achieve 100% preci-
sion. Since we performed precision-oriented alignment for the
anchor alignment model, the errors in anchor alignment will not
bring much noise into the syntax-enhanced model.
4http://svmlight.joachims.org/
5More specifically, we performed 5 iterations of Model 1, 5
iterations of HMM, 3 iterations of Model 3, and 3 iterations of
Model 4.
73
ment and phrase-extraction heuristics described in
(Koehn et al, 2003), minimum-error-rate training
(Och, 2003), a trigram language model with Kneser-
Ney smoothing trained with SRILM (Stolcke, 2002)
on the English side of the training data, and Moses
(Koehn et al, 2007) to decode.
4.4 Evaluation
We evaluate the intrinsic quality of predicted align-
ment A with precision, recall and alignment error
rate (AER). Slightly differently from (Och and Ney,
2003), we use possible alignments in computing re-
call.
recall = |A ? P ||P | , precision =
|A ? P |
|A| (20)
AER(S,P ;A) = 1? |A ? S|+ |A ? P ||A| + |S| (21)
We also extrinsically measure the word alignment
quality via a Chinese?English translation task. The
translation output is measured using BLEU (Pap-
ineni et al, 2002).
5 Experimental Results
5.1 Word Alignment
We performed word alignment bidirectionally using
our approach to obtain the union and compared our
results with two strong baselines based on generative
word alignment models. The results are shown in
Table 1. We can see that both the syntax-enhanced
model based on HMM intersection anchors (Syntax-
HMM) and on IBM model 4 anchors (Syntax-Model
4) are better than the pure generative word alignment
models. Our approach is superior in precision with
a disadvantage in recall. The best result achieved
10.32% relative decrease in AER compared to the
baseline when we use IBM model 4 intersection to
obtain the set of anchor alignments.
model precision recall f-score AER
HMM refined 0.8043 0.7592 0.7811 0.2059
Syntax-HMM 0.8744 0.7304 0.7959 0.1845
Model 4 refined 0.7941 0.7987 0.7964 0.1929
Syntax-Model 4 0.8566 0.7685 0.8102 0.1730
Table 1: Comparing syntax-enhanced approach with gen-
erative word alignment
5.1.1 The Influence of Anchor Alignment
Quality
As we can see in Table 2, our precision-oriented
approach to acquire anchor alignments was accom-
plished quite well. All four different anchor align-
ment models achieved high precision. However, the
recall differs dramatically, with model 4 achieving
the highest recall and the heuristics-based approach
receiving the lowest. To investigate the influence
anchor model precision recall f-measure AER
Heuristics 0.9774 0.4047 0.5724 0.3947
Model 1 0.9509 0.5011 0.6563 0.3157
HMM 0.9802 0.5327 0.6903 0.2809
Model 4 0.9777 0.5677 0.7179 0.2533
Table 2: Performance of anchor alignment
of the anchor alignment model, we first obtained
the intersection of the words left unaligned after an-
choring using each of the anchor alignment models.
We evaluate the alignment of these words against
the gold-standard alignments involving these words.
The influence of anchor alignment on the perfor-
mance of the syntax-enhanced model can be seen
in Table 3. The performance of the syntax-enhanced
model is closely related to that of the anchor align-
ment method. As can be seen from Table 2 and
3, HMM anchoring achieves the best precision and
so does the syntax-enhanced alignment; IBM model
4 achieves the best recall and so does the syntax-
enhanced alignment. Finally, the best alignment per-
formances are obtained with IBM model 4 anchor-
ing, with the difference in recall between HMM and
IBM model 4 anchoring being more significant than
the difference in precision.
anchor model precision recall f-score AER
Heuristics 0.4505 0.3270 0.3790 0.6210
Model 1 0.5538 0.3894 0.4573 0.5427
HMM 0.5932 0.3611 0.4489 0.5511
Model 4 0.5660 0.4216 0.4832 0.5168
Table 3: Influence of anchor alignment in syntax-
enhanced model
5.1.2 The Influence of Syntactic Dependencies
on Word Alignment
The influence of incorporating syntactic depen-
dencies into the word alignment process is shown
74
in Table 4. Syntax plays a positive role in all differ-
ent anchor alignment configurations. The influence
grows proportionally to the strength of the anchor
alignment model. With the Model 4 intersection
used as the set of anchor alignments, adding syn-
tactic dependency features into the syntax-enhanced
alignment model yields a 5.57% relative decrease in
AER.
model precision recall f-score AER
Heuristics
no syntax 0.8362 0.6751 0.7470 0.2302
w. syntax 0.8376 0.6894 0.7563 0.2240
Model 1
no syntax 0.8759 0.6902 0.7720 0.2045
w. syntax 0.8542 0.7160 0.7790 0.2011
HMM
no syntax 0.8655 0.7168 0.7841 0.1952
w. syntax 0.8744 0.7304 0.7959 0.1845
Model 4
no syntax 0.8697 0.7340 0.7961 0.1832
w. syntax 0.8566 0.7685 0.8102 0.1730
Table 4: Influence of syntactic dependencies on word
alignment
5.1.3 Contribution of Different Feature Classes
We interpret the contribution of each feature in
terms of feature weights in SVM model training.
The weights for the most discriminative features in
each feature class in Chinese?English word align-
ment (using HMM intersection as anchor align-
ment) are shown in Table 5. As we can see, all
statistics-based features are informative. Two target
dependency features are informative: ?PRD? denot-
ing ?predicative? dependency, and ?AMOD? denot-
ing ?adjective/adverb modifier? dependency.
weight
Model 1 Score 0.1416
POS 0.0540
Log-likelihood Ratio 0.0856
relative distortion 0.0606
DA-1 0.0227
DLA-2 0.0927
tgt-1-PRD 0.0961
tgt-2-AMOD 0.0621
Table 5: Weights of some informative features
5.2 Machine Translation
Research has shown that an increase in AER does
not necessarily imply an improvement in translation
quality (Liang et al, 2006) and vice-versa (Vilar et
al., 2006). Hereafter, we used a Chinese?English
MT task to extrinsically evaluate the quality of our
word alignment.
Table 6 shows the influence of our word align-
ment approach on MT quality.6 On development set,
we achieved statistically significant improvement
using both our syntax-enhanced models?Syntax-
HMM (p<0.002) and Syntax-Model 4 (p<0.008).
On the test set, we observed that the MT output
based on our alignment model tends to be shorter
than the reference translations and the BLEU score
is considerably penalized. If we ignore the length
penalty (?BP? in Table 6) in significance testing, the
improvement on test set is also statistically signif-
icant: p<0.04 for both Syntax-HMM and Syntax-
Model 4. However, an indepth manual analysis
needs to be carried out in order to determine the ex-
act nature of the shorter sentences derived.
dev. set test set
Baseline 0.5412 0.3510 (BP=0.96)
Syntax-HMM 0.6015 0.3409 (BP=0.86)
Syntax-Model 4 0.5834 0.3585 (BP=0.91)
Table 6: The Influence of Word Alignment on MT
6 Comparison with Previous Work
Our syntax-enhanced model is a discriminative word
alignment model. Certain generative word align-
ment models (e.g. HMM or IBM 4) also take
the first-order dependencies into account. How-
ever, long distance dependencies between words are
hard to incorporate into these models because of
the explosive number of parameters. On the other
hand, like existing discriminative models, our ap-
proach uses a set of informative features based on
co-occurrence statistics, e.g. log-likelihood ratio
and DICE score. The advantage of our approach is
the mechanism by which syntactic features may be
incorporated.
6Note that the only difference between our MT system and
the baseline PB-SMT system is the word alignment component.
75
Some previous research also tried to make use
of syntax in word alignment. (Wang and Zhou,
2004) investigated the benefit of monolingual pars-
ing for alignment. They learned a generalized word
association measure (crosslingual word similarities)
based on monolingual dependency structures and
improved alignment performances over IBM model
2 and certain heuristic-based models. (Cherry and
Lin, 2006) used dependency structures as soft con-
straints to improve word alignment in an ITG frame-
work. Compared to these models, our approach di-
rectly takes advantage of dependency relations as
they are transformed into feature functions incorpo-
rated into a discriminative word alignment frame-
work.
7 Conclusion and Future Work
In this paper, we proposed a model that can facili-
tate the incorporation of syntax into word alignment
and measured the combination of a set of syntactic
features. Experimental results have shown that syn-
tax is useful in word alignment and especially effec-
tive in improving the recall. We have also observed
that in our word alignment framework, the two sub-
models are closely related and the quality of the an-
chor alignment model plays an important role in the
system performance.
The promising results will lead us to improve our
model in the following aspects. First, the two sub-
models in our approach are two separate processes
performed in pipeline. We plan to jointly optimize
the two models in one go. Second, some of our
experiments used complex IBM models, e.g. IBM
Model 4, to obtain anchor alignment. We plan to
boostrap the alignment using simple heuristics with-
out relying on complex IBM models. Third, the
alignment searching process assumed the alignment
link for each word is made independently. A feasible
markovian assumption will be tested for searching.
Fourth, a comparison with traditional discriminative
word alignment models is also necessary to justify
the merits of our approach. Finally, we also plan to
adapt our approach to larger data sets and more lan-
guage pairs.
Acknowledgments
This work is supported by Science Foundation Ire-
land (grant number OS/IN/1732). Prof. Rebecca
Hwa from University of Pittsburgh and Dr. Yang
Liu from the Institute of Computing Technology,
Chinese Academy of Sciences, are kindly acknowl-
edged for providing us with their word alignment
guidelines. We would also like to thank the anony-
mous reviewers for their insightful comments.
References
Necip Fazil Ayan, Bonnie Dorr, and Nizar Habash.
2004. Multi-align: Combining linguistic and statis-
tical techniques to improve alignments for adaptable
mt. In Proceedings of the 6th Conference of the AMTA
(AMTA-2004), pages 17?26, Washington DC.
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 65?72, Sydney, Australia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 105?112, Sydney, Australia.
Yonggang Deng and Yuqing Gao. 2007. Guiding sta-
tistical word alignment models with prior knowledge.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 1?8,
Prague, Czech Republic.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Cameron Shaw Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 1?12, Trento, Italy.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspondence
using annotation projection. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 392?399, Philadelphia, PA.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of Human Language
76
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 89?
96, Vancouver, British Columbia, Canada.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 104?111, New York, NY.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 459?466, Ann Arbor, MI.
Adam Lopez and Philip Resnik. 2005. Improved HMM
alignment models for languages with scarce resources.
In Proceedings of the ACL Workshop on Building and
Using Parallel Texts, pages 83?86, Ann Arbor, Michi-
gan, June.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 81?88, Vancouver, British Columbia, Canada.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Ervin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan.
Sylwia Ozdowska. 2004. Identifying correspondences
between words: an approach based on a bilingual syn-
tactic analysis of French/English parallel corpora. In
Proceedings of the COLING?04 Workshop on Multi-
lingual Linguistic Resources, pages 49?56, Geneva,
Switzerland.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Eric Brill and Ken-
neth Church, editors, Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 133?142, Somerset, NJ.
Dengjun Ren, Hua Wu, and Haifeng Wang. 2007. Im-
proving statistical word alignment with various clues.
In Machine Translation Summit XI, pages 391?397,
Copenhagen, Denmark.
Andrea Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO.
Le Sun, Youbing Jin, Lin Du, and Yufang Sun. 2000.
Word alignment of English-Chinese bilingual corpus
based on chunks. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical Methods in Natural
Language Processing and very large corpora, pages
110?116.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world.
In Proceedings of Third International Conference on
Language Resources and Evaluation 2002, pages 147?
152, Las Palmas, Spain.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
AER: Do we need to ?improve? our alignments? In
Proceedings of the International Workshop on Spoken
Language Translation, pages 205?212, Kyoto, Japan.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th International Con-
ference on Computational Linguistics, pages 836?841,
Copenhagen, Denmark.
Wei Wang and Ming Zhou. 2004. Improving word align-
ment models using structured monolingual corpora.
In Dekang Lin and Dekai Wu, editors, Proceedings
of Conference on Empirical Methods in Natural Lan-
guage Processing, pages 198?205, Barcelona, Spain.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
77
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 95?99,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2009
Jinhua Du, Yifan He, Sergio Penkale, Andy Way
Centre for Next Generation Localisation
Dublin City University
Dublin 9, Ireland
{jdu,yhe,spenkale,away}@computing.dcu.ie
Abstract
In this paper, we describe the machine
translation system in the evaluation cam-
paign of the Fourth Workshop on Statisti-
cal Machine Translation at EACL 2009.
We describe the modular design of our
multi-engine MT system with particular
focus on the components used in this par-
ticipation.
We participated in the translation task
for the following translation directions:
French?English and English?French, in
which we employed our multi-engine ar-
chitecture to translate. We also partic-
ipated in the system combination task
which was carried out by the MBR de-
coder and Confusion Network decoder.
We report results on the provided devel-
opment and test sets.
1 Introduction
In this paper, we present a multi-engine MT
system developed at DCU, MATREX (Machine
Translation using Examples). This system exploits
EBMT, SMT and system combination techniques
to build a cascaded translation framework.
We participated in both the French?English and
English-French News tasks. In these two tasks,
we employ three individual MT system which are
1) Baseline: phrase-based system (PB); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chun-
ker (Gough and Way, 2004). 3) HPB: a typical
hierarchical phrase-based system (Chiang, 2005).
Meanwhile, we also use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final result.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypothe-
sis as the alignment reference for the Confusion
Network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search and generate the translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide results on the development and
test sets. Section 4 is our conclusion.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits aspects
of both the EBMT and SMT paradigms.
This architecture includes three individual sys-
tems which are phrase-based, example-based and
hierarchical phrase-based.
The combination structure is the MBR decoder
and CN decoder, which is based on the word-level
combination strategy.
In the final stage, we use a new rescoring mod-
ule to process the N -best list generated by the
combination module. See Figure 1 as a detailed
illustration.
2.2 Example-Based Machine Translation
EBMT obtains resources using the Marker Hy-
pothesis (Green, 1979), a psycholinguistic con-
straint which posits that all languages are marked
for surface syntax by a specific closed set of lex-
emes or morphemes which signify context. Given
a set of closed-class words we segment each sen-
tence into chunks, creating a chunk at each new
occurrence of a marker word, with the restriction
that each segment must contain at least one non-
marker word (Gough and Way, 2004).
95
Mutiple 1-best
MBR Decoder
CN/MERT
System 
Combination
HPB Baseline EBMT
Dev/MERT
Decoding
Rescore/MERT
Rescore/MERT
TestSet
Recaser
Rescore
Mutiple 1-best
MBR Decoder
CN Decoder
Rescore
Recaser
Figure 1: System Framework
We then align these segments using an edit-
distance-style algorithm, in which the insertion
and deletion probabilities depend on word-to-
word translation probabilities and word-to-word
cognates (Stroppa and Way, 2006).
We extracted phrases of at most 7 words on
each side. We then merged these phrases with the
phrases extracted by the baseline system adding
word alignment information, and used this system
seeded with this additional information.
2.3 Hierarchical Machine Translation
HPB translation system is a re-implementation of
the hierarchical phrase translation model which is
based on PSCFG (Chiang, 2005). We generate re-
cursively PSCFG rules from the initial rules as
N ? f1 . . . fm/e1 . . . en
where N is a rule which is initial or includes non-
terminals.
M ? fi . . . fj/eu . . . ev
where 1 ? i ? j ? m and 1 ? u ? v ? n, at
which point a new rule can be obtained, named,
N ? f i?11 Xkfmj+1/eu?11 Xkenv+1
where k is an index for the nonterminal X . The
number of nonterminals permitted in a rule is no
more than two.
When extracting hierarchical rules,we set some
limitations that initial rules are of no more than
7 words in length and other rules should have
no more than 5 terminals and nonterminals, and
we disallow rules with adjacent source-side and
target-side nonterminals.
The decoder is an enhanced CYK-style chart
parser that maximizes the derivation probability
and spans up to 12 source words. A 4-gram lan-
guage model generated by SRI Language Model-
ing toolkit (SRILM) (Stolcke, 2002) is used in the
cube-pruning process. The search space is pruned
with a chart cell size limit of 50.
2.4 System Combination
For multiple system combination, we implement
an MBR-CN framework as shown in Figure 1. In-
stead of using a single system output as the skele-
ton, we employ a minimum Bayes-risk decoder
to select the best single system output from the
merged N -best list by minimizing the BLEU (Pa-
pineni et al, 2002) loss.
The confusion network is built by the output of
MBR as the backbone which determines the word
order of the combination. The other hypotheses
are aligned against the backbone based on the TER
metric. NULL words are allowed in the alignment.
Each arc in the CN represents an alternative word
at that position in the sentence and the number of
votes for each word is counted when constructing
the network. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
Also, we use MERT (Och, 2003) to tune the
weights of confusion network.
2.5 Rescore
Rescore is a very important part in post-processing
which can select a better hypothesis from the N -
best list. We add some new global features in
rescore model. The features we used are as fol-
lows:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram POS language model (Ratna-
parkhi, 1996; Schmid, 1994);
96
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT algorithm.
3 Experimental Setup
The following section describes the system and
experimental setup for the French-English and
English-French translation tasks.
3.1 Statistics of Data
Parallel Corpus
We used Europarl and Giga data for this evalua-
tion. The statistics of parallel data are shown in
Table 1.
Corpra Sen Token-En Token-Fr Len
Europarl 1.46M 39,240,672 42,252,067 80
Giga 2M 48,648,104 57,869,002 65
Table 1: Statistics of Parallel Data
In this table, Sen indicates the number of sentence
pairs; Len denotes the maximum sentence length
of each corpus. This year the translation task is
only evaluated on News Domain. Experimental re-
sults showed that giga data is more correlated than
Europarl and the BLEU score is significantly im-
proved(See Table 4).
Monolingual Corpus
In this evaluation, we trained a small 4-gram lan-
guage model using data in Table 1 and a large 4-
gram language model using data in Table 2. We
configured these two LMs for Baseline and EBMT
systems while HPB only used the large one.
Language Sen Token Source
English 9,966,838 240,849,221 E/N/NC
French 9,966,838 260,520,313 E/N/NC
Table 2: Statistics of Monolingual Data
In the above table, E/N/NC refers to Eu-
roparl/News/New Commentary corpus.
3.2 Pre-Processing
We preprocessed both Europarl and Giga Release
1 corpus. For the Europarl corpus, we removed
the reserved characters in GIZA++ and tokenized
and lowercased the corpus with tools provided by
WMT09. The Giga corpus was too large for our
resource, so we performed sentence selection be-
fore cleaning, in the following steps.
? We split the Giga corpus into even segments,
each segment consisting of 20 lines.
? We trained an SVM classifier on English side
with positive examples from the monolin-
gual news data and negative examples from
noisy sentences (numbers, meaningless word
combinations, and random segments) from
the Giga corpus. We used ?-ly? and ?-ing?
to approximate adverbs and present partici-
ples and did not use other POS-induced fea-
tures, as in (Ferizis and Bailey, 2006). We
added these features to remove noise: aver-
age length of sentences, frequency of capital-
ized characters, frequency of numerical char-
acters and short word penalty (equals to 1
when average length of words < 4, and 0
otherwise). We used the classifier to remove
20% segments of lowest scores.
? We selected 1, 600 words having the highest
mutual information scores with monolingual
training data against the Giga corpus.
? We selected 100, 000 segments where these
words occurred most frequently. However
the sentence was dropped if the length ratio
between English and French was larger than
1.5 or less than 0.67.
3.3 System Configuration
The two language models were done using the
SRILM employing linear interpolation and modi-
fied K-N discounting (Chen and Goodman, 1996).
The configuration for the three systems is listed
in Table 3.
System P-Table Length LM Features
Baseline-E 55.9M 7 2 15
Baseline-G 58.4M 7 2 15
EBMT 59.4M 7 2 15
HPB 122M 5 1 8
Table 3: Statistics of MT Systems
In this table, E indicates the Europarl corpus
97
which is used for all three systems, and G stands
for the Giga corpus which is only used for the
Baseline system. We can see from Table 3 that
the size of the HPB phrase-table is more than 2
times as large as the other phrase tables. How to
filter and process such a huge hierarchical table is
a challenging problem.
We tuned our systems on the development set
devset2009-a and devset2009-b, and performed
the crossover experiment by these two devsets.
3.4 Experimental Results
The system output is evaluated with respect to
BLEU score. In Table 4, we used devset2009-b
to tune the various parameters in our three single
systems and devset2009-a for testing. In terms of
the Europarl data, we can see that the three sys-
tems we used achieved similar performance on the
test set for both translation directions, with the
Baseline-E system yielding slightly better results
than the other two.
System Fr-En En-Fr
Baseline-E 22.24 22.68
Baseline-G 24.90 ?1
EBMT 22.04 22.12
HPB 21.69 21.12
MBR 25.11 22.68
CN 25.24 22.76
Rescore 25.40 22.97
Table 4: Experimental Results on Devset2009-a
We then used the translations of the devset2009-
a produced by each system to tune the parame-
ters of our system combination module. From Ta-
ble 4, we can see that using MBR and confusion
network decoding leads to a slight improvement
over the strongest single system, i.e. the baseline
Phrase-Based SMT system. Rescoring the N -best
lists yielded an increase of 0.5 (2.0 relative) ab-
solute BLEU points over the baseline for French?
English Translation and 0.29 (1.28 relative) abso-
lute BLEU points for English?French Translation.
Table 5 is the results on 2009 Test Data. The
scores with a slash in the last two rows are low-
ercased and cased respectively. From the table we
1Not much time to do the experiments on English-French
direction. EBMT and HPB just used the Europarl corpus.
2The official automatic result is scored on 2525 sentences
out of the whole 3007 sentences in test set. The other 502
sentences are used as the development set for combination
evaluation task.
System Fr-En En-Fr
Baseline-E 25.64 24.47
Baseline-G 26.75 ?
EBMT 25.67 24.43
HPB 25.20 24.19
Combination 27.20/25.14 25.26/22.28
Official-Auto2 26.86/24.93 23.78/22.14
Table 5: Summary of Results on 2009 Test Data
can see that combination yielded 0.45 and 0.79 ab-
solute BLEU points over the best single system for
Fr-En and En-Fr direction respectively. However,
1.93 (7.2 relative) and 1.64 (6.58 relative) BLEU
points are dropped between cased and lowercased
results of both directions. Accordingly, training an
effective recasing model is very important for our
future work.
4 Conclusion
This paper presents our machine translation sys-
tem in WMT2009 shared task campaign. We de-
veloped a multi-engine framework which com-
bined the output results of the three MT sys-
tems and generated a new N -best list after CN
decoding. Then by using some global features
the rescoring model generated the final translation
output. The experimental result proved that the
combination module and rescoring module are ef-
fective in our framework.
We also applied simple yet effective methods
of genre and topical classification to remove noise
and out-of-domain sentences in the Giga corpus,
from which we built better translation models than
from Europarl.
In future work, we will refine our system frame-
work to investigate its effect on the tasks pre-
sented here, and we will develop more powerful
post-processing tools such as recaser to reduce the
BLEU loss.
Acknowledgments
This work is supported by Science Foundation Ireland (Grant
No. 07/CE/I1142). Thanks also to the reviewers for their
insightful comments and suggestions.
References
Chen, S. F. and Goodman, J. (1996). An Empirical Study of
Smoothing Techniques for Language Modeling. In Pro-
ceedings of the Thirty-Fourth Annual Meeting of the As-
sociation for Computational Linguistics, pages 310?318,
San Francisco, CA.
Chiang, D. (2005). A Hierarchical Phrase-Based Model for
Statistical Machine Translation. In Proceedings of the
98
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 263?270, Ann Arbor,
MI.
Ferizis, G. and Bailey, P. (2006). Towards practical genre
classification of web documents. In Proceedings of the
15th international conference on World Wide Web (WWW
?06), pages 1013?1014, New York, USA.
Fiscus, J. G. (1997). A post-processing system to yield re-
duced word error rates: Recognizer output voting error
reduction (ROVER). In Proceedings 1997 IEEE Work-
shop on Automatic Speech Recognition and Understand-
ing (ASRU), pages 347?352, Santa Barbara, CA.
Gough, N. and Way, A. (2004). Robust Large-Scale EBMT
with Marker-Based Segmentation. In Proceedings of
the 10th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-04),
pages 95?104, Baltimore, MD.
Green, T. (1979). The Necessity of Syntax Markers. Two
experiments with artificial languages. Journal of Verbal
Learning and Behavior, 18:481?496.
Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk De-
coding for Statistical Machine Translation. In Proceed-
ings of the Joint Meeting of the Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL
2004), pages 169?176, Boston, MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Finding con-
sensus in speech recognition: Word error minimization
and other applications of confusion networks. Computer
Speech and Language, 14(4):373?400.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a Method for Automatic Evaluation of Machine
Translation. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-02),
pages 311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy Model for
Part-Of-Speech Tagging. In Proceedings of the Empiri-
cal Methods in Natural Language Processing Conference
(EMNLP), pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S., Schwartz, R., Ayan,
N. F., and Dorr, B. J. (2007). Combining outputs from
multiple machine translation systems. In Proceedings
of the Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL
2007), pages 228?235, Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In Proceedings of International Con-
ference on New Methods in Language Processing, pages
44?49, Manchester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L., and
Makhoul, J. (2006). A study of translation edit rate with
targeted human annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Translation in the
Americas (AMTA 2006), pages 223?231, Cambridge, MA.
Stolcke, A. (2002). SRILM - An Extensible Language Mod-
eling Toolkit. In Proceedings of the International Confer-
ence Spoken Language Processing, pages 901?904, Den-
ver, CO.
Stroppa, N. and Way, A. (2006). MaTrEx: the DCU machine
translation system for IWSLT 2006. In Proceedings of the
International Workshop on Spoken Language Translation,
pages 31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities
for Statistical Machine Translation. In Proceedings of the
Joint Meeting of the Human Language Technology Con-
ference and the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL 2006),
pages 72?77, New York, USA.
99
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 47?55,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Web Service Integration for Next Generation Localisation 
David Lewis, Stephen Curran,  
Kevin Feeney, Zohar Etzioni, 
John Keeney 
Andy Way Reinhard Sch?ler 
Centre for Next Generation Localisation 
Knowledge and Data Engineering 
Group 
School of Computing Centre for Localisation  
Research 
Trinity College Dublin, Ireland Dublin City Universit 
y, Ireland 
University of Limerick,  
Ireland 
{Dave.Lewis|Stephen.curran|K
evin.Feeney|etzioniz|John.Ke
eney}@cs.tcd.ie 
away@computing.dcu.ie 
 
Reinhard.Schaler@ul.ie 
 
 
Abstract 
Developments in Natural Language Processing technol-
ogies promise a variety of benefits to the localization 
industry, both in its current form in performing bulk 
enterprise-based localization and in the future in sup-
porting personalized web-based localization on increa-
singly user-generated content. As an increasing variety 
of natural language processing services become availa-
ble, it is vital that the localization industry employs the 
flexible software integration techniques that will enable 
it to make best use of these technologies. To date how-
ever, the localization industry has been slow reap the 
benefits of modern integration technologies such as web 
service integration and orchestration. Based on recent 
integration experiences, we examine how the localiza-
tion industry can best exploit web-based integration 
technologies in developing new services and exploring 
new business models  
? Introduction 
Research and development of natural language 
processing technologies are leading to a variety of 
advances in areas such as text analytics and ma-
chine translation that have a range of commercial 
applications. The Localization Industry in particu-
lar, is strategically well placed to make good use of 
these advances as it faces the challenge of localiz-
ing accelerating volumes of digital content that is 
being targeted at increasingly global markets of 
this content. It needs to exploit the benefits of NLP 
technologies to reduce the cost of translation and 
minimise the time to market of this digital content.  
Furthermore, where the localization industry best 
learns how to efficiently and flexibly employ  NLP 
technologies in the localization of digital content it 
will be ideally placed to develop new services and 
exploit new business opportunities offered by the 
WWW. In particular, today?s localization tech-
niques are not able to keep pace with the WWW?s 
ability to dynamically compose and personalize 
existing content and to support rapid development 
of large volumes of user generated content. To 
meet this challenge, localization processes must 
effectively employ NLP to move from manually 
centered, professional batch activities to highly 
automated, highly participative continuous activi-
ties. To do this, the technologies of the WWW 
need to be employed to dynamically combine NLP 
technologies and leverage different levels of hu-
man linguistic abilities and knowledge to best ac-
complish the task at hand.   
In this paper we examine how this vision, which 
we term Next Generation Localization, can be sup-
ported by current web-based, service-oriented 
software integration techniques such as web ser-
vice integration and orchestration. Based on recent 
integration experience we review the current issues 
in using open interoperability standards and web 
services to the integration of commercial localiza-
tion platforms and NLP software. We then describe 
some generic definitions for NLP web services and 
how these provide flexibility in developing new 
localization service compositions. Finally, we out-
line the major software integration challenges fac-
ing the localization industry and describe how 
these are being addressed at Ireland?s Centre for 
Next Generation Localization (CNGL). 
47
? Next Generation Localization 
Traditional localization technologies and 
workflows are no longer able to cope with the es-
calating growth in volume. Traditional localization 
methods are not adequate to manage, localize and 
personalize unpredictable, on-line, multilingual, 
digital content. Machine Translation (MT) needs to 
be integrated into translation and post-editing 
workflows together with human translators. Novel 
machine-learning-based language technologies can 
automatically provide metadata annotations (la-
bels) to localization input in order to automate lo-
calization standardization and management. 
 
 
 
 
Figure 1: Example use of Web Service Orchestration in 
a Localisation Workflow 
 
For Next Generation Localisation to be 
achieved, the individual components need to be 
interoperable and easily reconfigurable. The com-
plexity of the resulting systems poses substantial 
software engineering challenges and crucially re-
quires detailed user requirement studies, technical 
and user interface standards, as well as support for 
rapid prototyping and formative evaluation early 
on in the software lifecycle. Blueprints for an in-
dustrial environment for Next Generation Localisa-
tion, which we term a Localisation Factory, are 
needed to guide the development of localisation 
services systems integrating advanced language, 
digital content and localisation management tech-
nologies. However, in order to successfully 
achieve the goal of technical interoperability these 
services crucially needs to be supplemented by 
standardised localisation processes and workflows 
for the Localisation Factory. Figure 1 gives an 
overview of a typical localisation workflow, that 
would be used for translating the content such as 
the use manual for a product, into multiple lan-
guages for different target markets. Typically this 
involves segmenting the content into sentences, 
looking up previously translated sentences from a 
Translation Memory (MT), before passing untrans-
lated segments to a Machine Translation (TM) ser-
vice to generate further candidate translations. 
Next, the job is passed to professional translators, 
who can accept automated translations or provide 
their own translations. Current practice in perform-
ing such workflows uses localisation platforms 
such as SDL?s Idiom WorldServer to integrate 
Translation Memory databases, Machine Transla-
tion packages and the routing of jobs to translators 
who typically work remotely under the manage-
ment of a localisation service provision agency.  
The localization industry has already underta-
ken a number of separate standardization activities 
to support interoperability between different locali-
sation applications. The Localisation Industry 
Standards Association (LISA ? www.lisa.org) has 
developed various localisation standards: 
? Translation Memory Exchange (TMX) for ex-
changing TM database content.  Many TM tool 
providers have implemented support for TMX 
in their products. 
? Term Base eXchange (TBX): XML Terminol-
ogy Exchange Standard. An XML linking 
standard, called Term Link, is also being in-
vestigated.  
? Segmentation Rules eXchange (SRX), for ex-
changing the rule by which content is original-
ly segmented. There has been very little sup-
port to date for SRX because segmentation is 
the main component that distinguished TM 
tools.  Segmentation has direct consequences 
for the level of reuse of a TM.  A TM's value is 
significantly reduced without the segmentation 
rules that were used to build it.   
? Global information management Metrics eX-
change (GMX): A partially populated family 
of standards of globalization and localization-
related metrics  
The Organization for the Advancement of Struc-
tured Information Standards (OASIS ? www.oasis-
open.org), which produces e-business standards 
has had a number of initiatives: 
? XML Localisation Interchange File Format 
(XLIFF):  XLIFF is the most common open 
standard for the exchange of localisable con-
48
tent and localisation process information be-
tween tools in a workflow.  Many tool provid-
ers have implemented support for XLIFF in 
their products. 
? Trans-WS  for automating the translation and 
localization process as a Web service.  There 
has not been much adoption of this standard.  
Work on the development and maintenance of 
the standard seems to be at a stand-still.  
? Open Architecture for XML Authoring and 
Localization: A recently started group looking 
at linking many existing localisation standards 
The W3C, which develops many web stan-
dards, has an Internationalisation Activity 
(www.w3.org/International) working on enabling 
the use Web technologies with different languages, 
scripts, and cultures. Specific standardisation in-
cludes the Internationalisation Tag Set to support 
internationalisation of XML Schema/DTDs. 
To date, therefore, standard localisation proc-
esses and workflows addressing common interop-
erability issues have not yet been widely adopted. 
Outside of proprietary scenarios, digital publishers 
and service providers cannot integrate their proc-
esses and technologies and cannot provide inde-
pendent performance measures. This implies lost 
business opportunities for many and missed oppor-
tunities for significant performance improvement 
for most of the stakeholders. We now examine 
how web services may help improve this situation. 
? Service Oriented Localization Integra-
tion 
The Centre for Next Generation Localisation 
[cngl] is developing a number of systems in order 
to investigate the issues that arise in integrating 
centralized workflows with community-based 
value creation. It aims to make full use of Service-
Oriented Architecture [erl]. This advocates 
software integration through well defined 
functional interfaces that can be invoked remotely, 
typically using the Web?s HTTP protocol with 
input and output parameters encoded in XML. The 
W3C have standardized an XML format, The Web 
Service Description Language (WSDL), for 
describing and exchanging such service 
definitions. Web services can be composed into 
more complicated applications using explicit 
control and data flow models that can be directly 
executed by workflow engines. This allows new 
workflow applications to be defined declaratively 
and immediately executed, thus greatly reducing 
the integration costs of developing new workflows 
and increasing the flexibility to modify existing 
ones. Such web-service based service composition 
is known as Web Service Orchestration. OASIS 
has standardized web service orchestration 
language called the Business Process Execution 
Language (BPEL), which has resulted in the 
development of several commercial execution 
platform and BPEL workflow definition tools, 
which support workflow definition through drag-
and drop interfaces. In CNGL, web services and 
web service orchestration are used  for integrating 
components and operating workflows between 
potential partners in the commercial localization 
value chain. This provides a high degree of 
flexibility in integrating the different language 
technologies and localization products into 
different workflow configurations for the project, 
while avoiding reliance on any single proprietary 
platform. As an initial exploration of this space a 
system integration trial was undertaken. The use of 
BPEL for integrating NLP software has previously 
been used in the LanguageGrid project, but is a 
purely in support of academic research integration. 
Our work aimed flexibility instantiate commercial 
localisation workflow using NLP software 
wrapped in services that are orchestrated using 
BPEL, while, as indicated in Figure 1, still 
integrating with commercial localisation workflow 
tools. This exploration also included extending the 
human element of the localisation workflow by 
soliciting translations from a body of volunteer 
translators. This is seen as more appropriate if the 
required translation is not time constrained and it 
often forms part of a customer relationship 
strategy. Quality management may require 
involvement of volunteer post-editors, and 
incomplete or poor translations may ultimately still 
need to be referred to professional translators. 
Thus our workflows can be configured to oper-
ate in parallel to provide alternative translations. In 
the professional localization workflow, after the 
MT stage, the candidate translation would be re-
turned to the SDL Worldserver platform via which 
professional translators and post-editors are able to 
complete the task. In the crowd-sourcing variation, 
this manual step is instead performed by passing 
the job to a similar application implemented as a 
49
plug-in to the Drupal collaborative content man-
agement system. 
Our implementation uses the XLIFF format as a 
standard for encapsulating the various transforma-
tions that happen to a resource as it passes through 
the localisation process. It should be noted, how-
ever, that support for XLIFF is partial at best in 
most localisation tools. Where the standard is sup-
ported, there are often different, specific flavours 
used, and embedded elements within the XLIFF 
can be lost as the resource passes through various 
stages in the process.  Another problem with in-
corporating current tools in our service-oriented 
framework is that some of them, such as IBM?s 
UIMA, are designed to function in a batch mode ? 
which does not map cleanly to services.  Neverthe-
less, despite a range of practical problems, it was 
in general possible to engineer service front-ends 
for most of these tools so that they can be inte-
grated into a composable service infrastructure. In 
the following section we proceed to detail the de-
sign of the generic web services we defined for this 
system and discuss the option undertaken in their 
implementation. 
3.1 Web Service Definitions 
The OASIS TWS working group remains the 
only real attempt to define web-services to support 
the localization process.  However, TWS has a li-
mited scope.  Rather than aiming to support the 
dynamic composition of language services into 
flexible localization workflows, it concentrates on 
supporting the negotiation of ?jobs? between ser-
vice providers.  It is primarily intended to support 
the efficient out-sourcing of localization and trans-
lation jobs and it does not address the composition 
of language-services to form automated 
workflows.   
Therefore, in order to deploy web-services to 
support such composition, there is little standardi-
sation to rely on.  Thus, a first step in addressing 
the problem is to design a set of web-services and 
their interfaces suitable for the task.   In designing 
these services, it is worthwhile to recall the general 
goals of service-oriented architectures; the services 
should be designed to be as flexible and general as 
possible and they should neither be tightly coupled 
to one another, nor to the overall system which 
they are part of.  Furthermore, in keeping with the 
general trends in service designs [foster], variabili-
ty in service behavior should generally be sup-
ported through the passed data-structures rather 
than through different function signatures.  
Bearing these design goals in mind, we can be-
gin to analyse the basic requirements of localisa-
tion with a view to translating these requirements 
into concrete service definitions.  However, in or-
der to further simplify this task, we adopt certain 
assumptions about the data-formats that will be 
deployed.  Firstly, we assume that UTF-8 is the 
universal character encoding scheme in use across 
our services.  Secondly, we assume that XLIFF is 
employed as the standard format for exchanging 
localization data between different parts of the lo-
calisation process.  
XLIFF is primarily focused on describing a re-
source in terms of source segments and target seg-
ments.  Essentially, it assumes the following mod-
el: a localization job can be divided up into a set of 
translatable resources.  Each of these resources is 
represented as an XLIFF file.  Each resource can 
be further sub-divided into a sequence of translata-
ble segments (which may be defined by an SRX 
configuration). Each of these source segments can 
be associated with a number of target segments, 
which represent the source segment translated into 
a target language.  Finally, XLIFF also supports 
the association of various pieces of meta-data with 
each resource or with the various elements into 
which the resource is sub-divided.  
This simple basic structure allows us to define a 
very simple set of general web-services, each of 
which serves to transform the XLIFF in some way.  
These three basic classes of services transform the 
XLIFF inputs in the following ways: 
1. Addition of target segments.   
2. Sorting of target candidates 
3. Addition of meta-data.  
 
Thus, we adopt these service-types as the set of 
basic, general service interfaces that our services 
will implement.  They allow us to apply a wide 
range of useful language-technology processes to 
localization content through an extremely simple 
set of service interfaces.  To give some examples 
of how concrete services map onto these basic in-
terfaces: 
? A machine translation service is a manifesta-
tion of type 1.  It adds translations, as target 
segments, for  source segments  in the XLIFF 
file 
50
? A translation memory leveraging service is, 
similarly, implemented as a service of type 1. 
It can be considered as a special case of a 
translation service. 
? Our basic service-design supports the applica-
tion of multiple TM and MT services to each 
XLIFF file, potentially producing multiple 
translation candidates for each source segment.  
There are various situations where there is a 
need to order these candidates ? for example to 
choose which one will actually be used in the 
final translation, or to present a sorted list to a 
human user to allow them to most convenient-
ly select the candidate that is most likely to be 
selected by them.  These services can be im-
plemented using the common type 2 interface.  
? A wide range of text analytics service can be 
implemented as services of type 3.  For exam-
ple, domain identification, language identifica-
tion and various tagging services are all instan-
tiations of this type.  
Although these service types are generic, in terms 
of the transformations that they apply to the XLIFF 
content, they may be very different in terms of 
their management and configuration.  Thus, it is 
neither possible nor desirable to devise generic 
management interfaces ? these interfaces need to 
be tailored to the particular requirements of each 
specific service.  Thus, each service really consists 
of two specifications ? an implementation of the 
generic interface which allows the service to be 
easily integrated as a standard component into a 
workflow that transforms the XLIFF content, and a 
specific interface that defines how the service can 
be configured and managed.  The following section 
provides several examples of specific services and 
their management interfaces.  
Although XLIFF provides significant support for 
management of the transformation of resources as 
they proceed through the localisation workflow, it 
is not a universal solution. It is an inherently re-
source-oriented standard and it is thus not well 
suited for the aggregation of meta-data that has 
broader scope than that of the translatable resource.  
For example, in the course of a localisation 
workflow, we may wish to store state information 
relating to the user, the project, the workflow itself 
or various other entities that are not expressible as 
XLIFF resources. Therefore, a service-oriented 
localization workflow has a need for a service 
which allows the setting and retrieving of such me-
ta-data. The following section also includes a basic 
outline of a service which can provide such func-
tionality across the localization workflow.  
Finally, it should be pointed out that BPEL 
does not provide a universal solution to the prob-
lem of constructing workflows.  It is primarily de-
signed to facilitate the orchestration of automated 
web-services and does not map well to human 
processes. This has been acknowledged in the pro-
posed BPEL4People extension and the incorpora-
tion of better support for human tasks is also a key 
motivating factor for the development of the 
YAWL workflow specification language ? a BPEL 
alternative [vanderaalst].  To overcome this limita-
tion, we have designed a general purpose service 
which allows components to query the state of hu-
man tasks within the workflow ? this allows 
workflows to be responsive to the progress of hu-
man tasks (e.g. by redirecting a task that is taking 
too long).   
3.2 An MT Web Service 
As part of our work within CNGL in the devel-
opment of a Localisation Factory we have engi-
neered a web service capable of leveraging transla-
tions from multiple automated translation compo-
nents.  The service operates by taking in an XLIFF 
document, iterating the segments of the document 
and getting a translation from each of the transla-
tion components for each segment.  These transla-
tions are attached to the segment within the XLIFF 
and the service returns the final XLIFF document 
back to the client.  The service can be configured 
to use any permutation of the automated translation 
components depending on the workflow in which 
the service finds itself operating.  Some translation 
components may be inappropriate in a given 
workflow context and may be removed.  The ser-
vice also allows for the weighting of translations 
coming from different translation components so 
that certain translations are preferred above others. 
The service implementation leverages transla-
tion from two open web based translation systems 
Microsoft Live Translator [mslive] and Yahoo Ba-
belfish [babelfish].  Microsoft Live Translator can 
be accessed through a web service interface.  Ya-
hoo Babelfish has no web service interface so get-
ting back translations is implemented through a 
screen-scraping technique on the HTML document 
returned.   
51
The service also makes use of MaTrEx [ma-
trex], a hybrid statistical/example-based machine 
translation system developed by our partner uni-
versity Dublin City University. MaTreX makes use 
of the open-source Moses decoder [moses]. Trans-
lation models are created using MaTreX and are 
passed to the Moses decoder which performs that 
translation from source to target language. We took 
the Moses decoder and wrapped it in a web ser-
vice.  The web service pipes segments for transla-
tion to Moses which responds with translations.  
This translation model is produced based on 
aligned source and target corpora of content repre-
sentative of the content passing through the 
workflow. 
Finally we have taken a translation memory 
product LanguageExchange from Alchemy, an 
industrial partner within the project, and added that 
to the list of automated translation components 
available to our service.  This allows any previous 
human translations to be leveraged during the au-
tomated translation process. 
The service is engineered using Business 
Process Execution Language (BPEL) to orchestrate 
the calling of the various translation components 
that compose the service.  BPEL allows those 
managing the service to easily compose a particu-
lar configuration of the service.  Translation com-
ponents can be easily added or removed from the 
service.  The tool support around BPEL means that 
the user does not need a background in program-
ming to  develop a particular configuration of the 
components. 
One problem we encountered implementing the 
MT service as a wrapper around existing compo-
nents was that they are unable to handle internal 
markup within the segments.  Segments passing 
through a localisation workflow are likely to con-
tain markup to indicate particular formatting of the 
text.  The machine translation components are only 
able to handle free text and the markup is not pre-
served during translation. Another problem en-
countered in using free web services over the In-
ternet was that implementations did not encourage 
volume invocations, with source IP addresses re-
questing high volumes being blacklisted. 
 
3.3 A Text Analytics  Web Service 
We have implemented a generic text-
categorization service to provide text-analytic sup-
port for localization workflows.  It takes an XLIFF 
file as input and produces an XLIFF file as output, 
transforming it by adding meta-data (a type 3 
transform). The meta-data can be added either on a 
file-basis or on a segment basis, depending on the 
requirements of the workflow as expressed in the 
service?s configuration. The service provides a 
simple and generic XLIFF transformation as part 
of the localization workflow, while the manage-
ment interface provides flexible configurability.  
The management interface is designed in order 
to support multiple text analytic engines, each of 
which can support multiple categorization schema 
at once.  Our implementation uses two text en-
gines, the open source TextCat package [textcat] 
and IBM?s Fragma software [fragma].  The follow-
ing operations are provided by the service:  
 
Operation createSchema: The createSchema 
function creates a new categorisation schema based 
on a provided set of training data, which can op-
tionally be provided by an RSS feed for ongoing 
training data updates.  
Operation getEngines: This returns a list (en-
coded in XML) of the categorisation engines that 
are available to the Service. This allows the client 
to specify that a specific categorisation engine be 
used in subsequent requests. 
Operation viewSchema: This returns a list of the 
categories contained within a schema (and the de-
tails of the engine that was used to create it). 
Operation addData: This operation adds a piece 
of training data to a categorisation schema - i.e. it 
allows components to tell the service that a piece 
of text has a known category of categoryID accord-
ing to the schema with schemaID. 
Operation categorise: This provides a categorisa-
tion of text provided as an XLIFF segment, accord-
ing to a specified schema taken form the list sup-
ported by the service. 
3.4 A Crowd-sourcing Web Service 
In order to allow the localization workflow to in-
corporate crowd-sourcing, by which we mean col-
laborative input from a volunteer web-based user-
community, we have designed and implemented a 
web-service interface. This interface is designed to 
52
allow stages in the localization job to be handed 
off to such a community.  From the point of view 
of the workflow, the important thing is that the 
localisation requirements can be adequately speci-
fied and that the status of the job can be ascer-
tained by other elements in the workflow ? allow-
ing them to react to the progress (or lack thereof) 
in the task and, for example, to allow the job to be 
redirected to another process when it is not pro-
gressing satisfactorily.  
Our service design is focused on supporting 
crowd-sourcing, but it is intended to extend it to 
offer general-purpose support for the integration of 
human-tasks into a BPEL workflow.  It serves as a 
testbed and proof of concept for the development 
of a generic localization human task interface. The 
initial specification has been derived from the 
TWS specification [tws], but incorporates several 
important changes. Firstly, it is greatly simplified 
by removing all the quote-related functions and 
replacing them with the RequestJob and SubmitJob 
functions and combining all of the job control 
functions into a single updateJob function and 
combining the two job list functions into one. 
TWS, as a standard focused on support for lo-
calization outsourcing ? hence the concentration on 
negotiating ?quotes? between partners.  Our re-
quirements are quite different ? we cannot assume 
that there is any price, or even any formal agree-
ment which governs crowd-sourcing.  Indeed, in 
general, a major problem with TWS which hin-
dered its uptake is that it assumed a particular 
business model ? in practice localization jobs are 
not so automated, nor so quick that automated 
price negotiation is a particularly desired feature.  
Such information can be incorporated into a Job 
Description data structure, but a generic human-
task interface should not assume any particular 
business model ? hence the significant changes 
between our API and that of TWS.  Nevertheless, 
there is much clear and well-structured thinking 
contained in the TWS standard ? how best to de-
scribe language pairs, jobs and various other com-
monly referenced ideas in a localization workflow.  
By using TWS as a base, we can take advantage of 
all of that work rather than designing our own da-
ta-structures from scratch. The main operation are 
as follows: 
Operation requestJob: The JobDescription input 
parameter is an XML format which contains de-
tails of the job that is being requested. The returned 
datatype is the details of the job that is offered by 
the service. These are not necessarily the same. For 
example, the requested job might contain several 
language pairs, but the returned description might 
not contain all of these language pairs as some of 
those requested might not be available in the ser-
vice. Generally, it can be assumed that the service 
will make its ?best effort? to fulfill the require-
ments and the returned data will be as close as it 
can get to the requirements submitted.  
Operation submitJob: This operation works ex-
actly as the one above, except for the fact that it 
submits the job to the service with the particular 
JobDescription required and receives back the 
JobDescription that will actually be carried out.  
Operation retrieveJobList: This accepts a Job-
Description  input parameter, an XML format 
which contains a ?filter? on the various active jobs. 
The operation will return a list of all of the jobs 
which match that specified in the JobdDescription 
argument.  
Operation updateJob: A JobDescription input 
parameter is an XML format which contains a de-
scription of the various changes to the job that are 
being requested. The function will return a descrip-
tion which details the new, updated state of the job 
(note that the service does not have to follow all 
the requested changes and might ignore them).  
Operation retrieveJob:  A JobDescription input 
parameter is an XML format which contains a ?fil-
ter? on the various jobs. The operation returns a 
URI from which the client can retrieve the loca-
lised content corresponding to the filters. 
Operation associateResource: This functions as-
sociates a resource (TM / Glossary / etc) with a 
particular job. The returned value is the URI of the 
resource (which may be different than the passed 
ResURI). The types of resource supported will 
need to be decided upon.  
? Future Work: Translation Quality 
The next challenge to applying these techniques 
to workable industrial workflows is to fully ad-
dress the metrology of such workflows. The cur-
rent approach does not support the instrumentation 
of web services to provide quality measurements. 
Further, such quality measures need to be provided 
in a way that is relevant to the quality of the 
workflow as a whole and the business-driven key 
performance indicators which it aims to support.  
53
However, the integration of translation quality 
metrics across different forms of workflow and 
different industrial workflow components and lin-
guistic technologies has been widely identified as 
requiring considerable further investigation. Even 
the most basic metric used in commercial 
workflow, the word count against which transla-
tion effort is estimated, is calculated differently by 
different workflow systems. This particular case 
has already been addressed by LISA though its 
proposal for Global information management Me-
trics eXchange (GMX) [gmx].  
It is hardly surprising, therefore, that closing the 
gap between the metrics typically used by MT sys-
tem developers and what is needed to support the 
use of MT in commercial localization workflows is 
likely to be even more challenging. For example, 
metrics such as BLEU [bleu] are well-understood 
by MT developers used to participating in large-
scale open MT evaluations such as NIST; a BLEU 
score of 0.8 (say) means either that one?s MT sys-
tem is extremely good, or that the task is quite 
simple, or both, or even that there are a large num-
ber of reference translations against which the sys-
tem output is being compared. On the other hand, a 
score of 0.2 means that the quality is poor, that 
there is probably only one reference translation 
against which candidate translations are being eva-
luated, or that the task is a very complex one.  
However, neither score means anything (much) 
to a potential user. In the localization industry, 
Translation Memory is much more widely used, 
and there users and vendors use a different metric, 
namely fuzzy match score, i.e. how closely a pre-
viously translated source sentence matches the cur-
rent input string. Users typically ?know? that a 
score of around 70% fuzzy match is useful, whe-
reas for a lower scored sentence it is likely to be 
quicker to translate this from scratch.   
One of our research goals in the CNGL is to 
bring these two communities closer together by 
developing a translation quality metric that speaks 
to both sets of people, developers and users. One 
step in the right direction might be the Translation 
Edit Rate metric [ter], which measures the number 
of editing commands (deletions, substitutions, and 
insertions) that need to be carried out in order to 
transform the MT output into the reference transla-
tion(s). This is being quite widely used in the MT 
community (cf. the Global Autonomous Language 
Exploitation (GALE) project) by MT developers, 
and speaks a language that users understand well. 
User studies will very much inform the directions 
that such research will take, but there are reasons 
to believe that the gap can be bridged.   
Supposing then that such hurdles can be over-
come, broadly speaking, the quality of a translation 
process might be dependent on multiple factors, 
each of which could be measured both intrinsically 
and extrinsically, including; 
? Source and destination languages 
? Content domain 
? Diversity of vocabulary  
? Repetitiveness of text 
? Length and complexity of sentences 
? Availability of relevant translation memories 
? The cost and time incurred per translated word 
 
Often control of quality of the translation process 
can be impacted most directly by the quality of the 
human translators and the degree of control exerted 
over the source text. Different levels of linguistic 
quality assurance may be undertaken and post-
editors (who are often more experienced translators 
and therefore more expensive) are involved in 
handling incomplete or missing translations. How-
ever, even in professional translation environ-
ments, translation quality is regarded as relatively 
subjective and exact measurement of the quality of 
translation is therefore problematic. 
? Conclusion 
In this paper we have discussed some the chal-
lenges faced in taking a web service integration 
and orchestration approach to the development of 
next generation localization workflows. Based on 
our experiences of using these approaches to inte-
grate both existing localization products and cut-
ting edge research prototypes in MT , TA and 
crowd-sourcing, new, innovative localisation 
workflows can be rapidly assembled. The maturity 
of the BPEL standard and the design of general 
purpose, reusable web service interfaces are key to 
this success.  
 
Acknowledgments: This research is supported 
by the Science Foundation Ireland (Grant 
07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Trinity College 
Dublin. 
54
References  
[babelfish] Yahoo Babelfish Machine Translation 
http://babelfish.yahoo.com/ 6th Feb 2009 
 [drupal] Drupal Content Management System 
http://www.drupal.org 6th Feb 2009 
[bleu] Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. In 40th Annual Meeting of the 
Association for Computational Linguistics, Philadel-
phia, PA., pp.311?318. 
[bpel] Web Services Business Process Execution Lan-
guage Version 2.0, OASIS Standard, 11 April 2007, 
Downloaded from http://docs.oasis-
open.org/wsbpel/2.0/OS/wsbpel-v2.0-0S.html 6th 
Feb 2009 
[erl] Erl, Thomas, Service-oriented Architecture: Con-
cepts, Technology, and Design. Upper Saddle River: 
Prentice Hall  2005 
[foster] Foster, I., Parastatidis, S., Watson, P., and 
Mckeown, M. 2008. How do I model state?: Let me 
count the ways. Commun. ACM 51, 9 (Sep. 2008), 
34-41. 
[fragma] Alexander Troussov, Mayo Takeuchi, 
D.J.McCloskey, 
http://atroussov.com/uploads/TSD2004_LangID_wor
d_fragments.pdf 6th Feb 2009 
[gmx] Global Information Management Metrics Vo-
lume (GMX-V) 1.0 Specification Version 1.0, 26 
February 2007, downloaded from http://www.xml-
intl.com/docs/specification/GMX-V.html on 6th Feb 
2009 
[langexchange] Alchemy Language Exchange 
http://www.alchemysoftware.ie/products/alchemy_la
nguage_exchange.html 6th Feb 2009 
[matrex] MaTrEx Machine Translation - John Tinsley, 
Yanjun Ma, Sylwia Ozdowska, Andy Way. 
http://doras.dcu.ie/559/1/Tinsleyetal_WMT08.pdf  
[moses] Moses decoder http://www.statmt.org/moses/ 
9th March 2009 
[mslive] Microsoft Live Translator 
http://www.windowslivetranslator.com/ 6th Feb 2009 
[ter] Matt Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of 
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas, 
Cambridge, MA., pp.223?231. 
 [textcat] Java Text Categorisation 
http://textcat.sourceforge.net/ 6th Feb 2009 
 [tbx] Termbase eXchange Format 
http://www.lisa.org/Term-Base-eXchange.32.0.html 
6th March 2009 
 [tmx] Translation Memory eXchange 
http://www.lisa.org/Translation-Memory-e.34.0.html 
6th March 2009 
[tws] Translation Web Services Specification: 
http://www.oasis-
open.org/committees/download.php/24350/trans-ws-
spec-1.0.3.html 
[vanderaalst] Van Der Aalst, W.M.P. Ter Hofstede, 
A.H.M. ?YAWL: Yet another workflow language? In-
formation Systems, Volume 30, Issue 4, June 2005, 
Pages 245-275   
[xliff] XML Localisation Interchange File Format 
http://docs.oasis-open.org/xliff/v1.2/os/xliff-
core.html 6th March 2009 
55
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 104?107,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
English?Hindi Transliteration Using Context-Informed PB-SMT:    
the DCU System for NEWS 2009 
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar Srivastava,  
Sudip Kumar Naskar and Andy Way 
CNGL, School of Computing 
Dublin City University, Dublin 9, Ireland 
{rhaque,sdandapat,snaskar,asrivastava,away}@computing.dcu.ie 
 
Abstract 
This paper presents English?Hindi translit-
eration in the NEWS 2009 Machine Translit-
eration Shared Task adding source context 
modeling into state-of-the-art log-linear 
phrase-based statistical machine translation 
(PB-SMT). Source context features enable us 
to exploit source similarity in addition to tar-
get similarity, as modelled by the language 
model. We use a memory-based classification 
framework that enables efficient estimation of 
these features while avoiding data sparseness 
problems.We carried out experiments both at 
character and transliteration unit (TU) level. 
Position-dependent source context features 
produce significant improvements in terms of 
all evaluation metrics. 
1 Introduction 
Machine Transliteration is of key importance in 
many cross-lingual natural language processing 
applications, such as information retrieval, ques-
tion answering and machine translation (MT). 
There are numerous ways of performing auto-
matic transliteration, such as noisy channel mod-
els (Knight and Graehl, 1998), joint source chan-
nel models (Li et al, 2004), decision-tree models 
(Kang and Choi, 2000) and statistical MT models 
(Matthews, 2007). 
For the shared task, we built our machine 
transliteration system based on phrase-based sta-
tistical MT (PB-SMT) (Koehn et al, 2003) using 
Moses (Koehn et al, 2007).  We adapt PB-SMT 
models for transliteration by translating charac-
ters rather than words as in character-level trans-
lation systems (Lepage & Denoual, 2006). How-
ever, we go a step further from the basic PB-
SMT model by using source-language context 
features (Stroppa et al, 2007). We also create 
translation models by constraining the character-
level segmentations, i.e. treating a consonant-
vowel cluster as one transliteration unit.  
The remainder of the paper is organized as fol-
lows. In section 2 we give a brief overview of 
PB-SMT. Section 3 describes how context-
informed features are incorporated into state-of-
art log-linear PB-SMT. Section 4 includes the 
results obtained, together with some analysis. 
Section 5 concludes the paper. 
2 Log-Linear PB-SMT  
Translation is modelled in PB-SMT as a decision 
process, in which the translation Ie1 = e1 . . .  eI of 
a source sentence Jf1 = f1 . . . fJ is chosen to 
maximize (1): 
)1()().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
?  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modelled as a (log-linear) 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
?
?
?
m
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?   
                             )(log 1
I
LM eP??                  (2) 
where k
K sss ...11 ?  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )
?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik ; bk, jk), 
          
kk iik
eee ...? 11 ??? , 
                      
kk jbk
fff ...? ?                              (3) 
The translational features involved depend 
only on a pair of source/target phrases and do not 
take into account any context of these phrases. 
This means that each feature mh   in (2) can be 
rewritten as in (4): 
104
?
?
?
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(           (4) 
where mh? is a feature that applies to a single 
phrase-pair. Thus (2) can be rewritten as: 
? ??
? ??
?
K
k
K
k
kkkkkkm
m
m
m sefhsefh
1 11
),?,?(?),?,?(??        (5) 
where, m
m
m
mhh ??
1
?
?
? ? . In this context, the transla-
tion process amounts to: (i) choosing a segmen-
tation of the source sentence, (ii) translating each 
source phrase. 
3 Source Context Features in Log-
Linear PB-SMT 
The context of a source phrase kf?  is defined as 
the sequence before and after a focus phrase kf?  
=
kk ji
ff ... . Source context features (Stroppa et 
al., 2007) include the direct left and right context 
words (in our case, character/TU instead of word) 
of length l (resp. lii kk ff ?? ...1  and ljj kk ff ?? ...1 ) of 
a given focus phrase kf? = kk ji ff ... . A window of 
size 2l+1 features including the focus phrase is 
formed. Thus lexical contextual information (CI) 
can be described as in (6): 
CI = }...,...{ 11 ljjili kkkk ffff ????                    (6) 
As in (Haque et al, 2009), we considered a 
context window of ?1 and ?2 (i.e. l=1, 2) for our 
experiments. 
One natural way of expressing a context-
informed feature is as the conditional probability 
of the target phrase given the source phrase and 
its context information, as in (7): 
mh? ( kf? ,CI( kf? ), ke? , sk) = log P( ke? | kf? , CI( kf? ))  (7) 
3.1 Memory-Based Classification 
As (Stroppa et al, 2007) point out, directly esti-
mating P( ke? | kf? , CI( kf? )) using relative fre-
quencies is problematic. Indeed, Zens and Ney 
(2004) showed that the estimation of P( ke? | kf? ) 
using relative frequencies results in the overesti-
mation of the probabilities of long phrases, so 
smoothing factors in the form of lexical-based 
features are often used to counteract this bias 
(Foster et al, 2006). In the case of context-
informed features, since the context is also taken 
into account, this estimation problem can only 
become worse. To avoid such problems, in this 
work we use three memory-based classifiers: 
IGTree, IB1 and TRIBL 1  (Daelemans et al, 
2005). When predicting a target phrase given a 
source phrase and its context, the source phrase 
is intuitively the feature with the highest predic-
tion power; in all our experiments, it is the fea-
ture with the highest gain ratio (GR).  
In order to build the set of examples required 
to train the classifier, we modify the standard 
phrase-extraction method of (Koehn et al, 2003) 
to extract the context of the source phrases at the 
same time as the phrases themselves. Importantly, 
therefore, the context extraction comes at no ex-
tra cost.  
We refer interested readers to (Stroppa et al, 
2007) and (Haque et al, 2009) as well as the ref-
erences therein for more details of how Memory-
Based Learning (MBL) is used for classification 
of source examples for use in the log-linear MT 
framework. 
3.2 Implementation Issues 
We split named entities (NE) into characters. We 
break NEs into transliteration units (TU), which 
bear close resemblance to syllables. We split 
English NEs into TUs having C*V* pattern and 
Hindi NEs are divided into TUs having Ch+M 
pattern (M: Hindi Matra / vowel modifier, Ch: 
Characters other than Matras). We carry out ex-
periments on both character-level (C-L) and TU-
level (TU-L) data. We use a 5-gram language 
model for all our experiments. The Moses PB-
SMT system serves as our baseline system. 
The distribution of target phrases given a 
source phrase and its contextual information is 
normalised to estimate P( ke? | kf? ,CI( kf? )). There-
fore our expected feature is derived as in (8): 
mblh? = log P( ke? | kf? ,CI( kf? ))                         (8) 
As for the standard phrase-based approach, 
their weights are optimized using Minimum Er-
ror Rate Training (MERT) of (Och, 2003) for 
each of the experiments. 
As (Stroppa et al, 2007) point out, PB-SMT 
decoders such as Pharaoh (Koehn, 2004) or 
Moses (Koehn, 2007) rely on a static phrase-
table represented as a list of aligned phrases ac-
companied with several features. Since these fea-
                                               
1 An implementation of IGTree, IB1 and TRIBL is available 
in the TiMBL software package (http://ilk.uvt.nl/timbl). 
 
105
tures do not express the context in which those 
phrases occur, no context information is kept in 
the phrase-table, and there is no way to recover 
this information from the phrase-table. 
In order to take into account the context-
informed features for use with such decoders, the 
devset and testset that need to be translated are 
pre-processed. Each token appearing in the test-
set and devset is assigned a unique id. First we 
prepare the phrase table using the training data. 
Then we generate all possible phrases from the 
devset and testset. These devset and testset 
phrases are then searched for in the phrase table, 
and if found, then the phrase along with its con-
textual information is given to MBL for classifi-
cation. MBL produces class distributions accord-
ing to the maximum-match of the features con-
tained in the source phrase. We derive new 
scores from this class distribution and merge 
them with the initial information contained in the 
phrase table to take into account our feature 
functions ( mblh? ) in the log-linear model (2). 
In this way we create a dynamic phrase table 
containing both the standard and the context-
informed features. The new phrase table contains 
the source phrase (represented by the sequence 
of ids of the words composing the phrase), target 
phrase and the new score. 
Similarly, replacing all the words by their ids 
in the development set, we perform MERT using 
our new phrase table to optimize the feature 
weights. We translate the test set (words repre-
sented by ids) using our new phrase table. 
4 Results and Analysis 
We used 10,000 NEs from the NEWS 2009 Eng-
lish?Hindi training data (Kumaran and Kellner, 
2007) for the standard submission, and the addi-
tional English?Hindi parallel person names data 
(105,905 distinct name pairs) of the Election 
Commission of India2 for the non-standard sub-
missions. In addition to the baseline Moses sys-
tem, we carried out three different set of experi-
ments on IGTree, IB1 and TRIBL. Each of these 
experiments was carried out on both the standard 
data and the combined larger data, both at char-
acter level and the TU level, and considering 
?1/?2 tokens as context. For each experiment, 
we produce the 10-best distinct hypotheses. The 
results are shown in Table 1. 
We observed that many of the (unseen) TUs in 
the testset remain untranslated in TU-L systems 
                                               
2 http://www.eci.gov.in/DevForum/Fullname.asp 
due to the problems of data sparseness. When-
ever a TU-L system fails to translate a TU, we 
fallback on the corresponding C-L system to 
translate the TU as a post-processing step. 
The accuracy of the TU-L baseline system 
(0.391) is much higher compared to the C-L 
baseline system (0.290) on standard dataset. Fur-
thermore, contextual modelling of the source 
language gives an accuracy of 0.416 and 0.399 
for TU-L system and C-L system respectively. 
Similar trends are observed in case of larger 
dataset. However, the highest accuracy (0.445) 
has been achieved with the TU-L system using 
the larger dataset. 
5 Conclusion 
In this work, we employed source context model-
ing into the state-of-the-art log-linear PB-SMT 
for the English?Hindi transliteration task. We 
have shown that taking source context into ac-
count substantially improve the system perform-
ance (an improvement of 43.44% and 26.42% 
respectively for standard and larger datasets). 
IGTree performs best for TU-L systems while 
TRIBL seems to perform better for C-L systems 
on both standard and non-standard datasets. 
Acknowledgements 
We would like to thank Antal van den Bosch for 
his input on the use of memory based classifiers. 
We are grateful to SFI (http://www.sfi.ie) for 
generously sponsoring this research under grant 
07/CE/I1142. 
References  
Adimugan Kumaran and Tobias Kellner. A generic 
framework for machine transliteration. Proc. of the 
30th SIGIR, 2007. 
Byung-Ju Kang and Key-Sun Choi. Automatic trans-
literation and back-transliteration by decision tree 
learning. 2000. Proc. of LREC-2000, Athens, 
Greece, pp. 1135-1141. 
David Matthews. 2007. Machine Transliteration of 
Proper Names. Master's Thesis, University of Ed-
inburgh, Edinburgh, United Kingdom. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statisti-
cal machine translation. Proc. of ACL 2002, Phila-
delphia, PA, pp. 295?302. 
George Foster, Roland Kuhn, and Howard Johnson. 
2006. Phrasetable smoothing for statistical machine 
translation.  Proc. of EMNLP-2006, Sydney, Aus-
tralia, pp. 53-61. 
106
Table1: Experimental Results (S/B ? Standard / Big data, S*? TM on Standard data, but LM on Big data, 
C/TU ? Character / TU level, SD? Standard submission, NSD? Non-standard submission). Better results with 
bold faces have not been submitted in the NEWS 2009 Machine Transliteration Shared Task. 
Haizhou Li, Zhang Min and Su Jian. 2004. A joint 
source-channel model for machine translitera-
tion. Proc. of ACL 2004, Barcelona, Spain, 
pp.159-166. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, 
24(4):559-612. 
Nicolas Stroppa, Antal van den Bosch and Andy 
Way. 2007. Exploiting Source Similarity for 
SMT using Context-Informed Features. Proc. of  
TMI-2007, Sk?vde, Sweden, pp. 231-240. 
Peter F. Brown, S. A. D. Pietra, V. J. D. Pietra and 
R. L. Mercer. 1993. The mathematics of statisti-
cal machine translation: parameter estimation. 
Computational Linguistics 19 (2), pp. 263-311. 
Philipp Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. Proc. of HLT-
NAACL 2003, Edmonton, Canada, pp. 48-54. 
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trans-
lation models. Machine translation: from real 
users to research: Proc. of AMTA 2004, Berlin: 
Springer Verlag, 2004, pp. 115-124. 
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. 
Shen, C. Moran, R. Zens, C. Dyer,  O. Bojar, A. 
Constantin and E. Herbst. 2007. Moses: open 
source toolkit for statistical machine translation. 
Proc. of ACL, Prague, Czech Republic, pp. 177-
180. 
Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma 
and Andy Way. 2009. Using Supertags as Source 
Language Context in SMT. Proc. of EAMT-09, 
Barcelona, Spain, pp. 234-241. 
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine trans-
lation. Proc. of HLT/NAACL 2004, Boston, MA, 
pp. 257?264. 
Walter Daelemans & Antal van den Bosch. 2005. 
Memory-based language processing. Cambridge, 
UK, Cambridge University Press. 
Yves Lepage and Etienne Denoual. 2006. Objective 
evaluation of the analogy-based machine transla-
tion system ALEPH. Proc. of the 12th Annual 
Meeting of the Association of NLP, pp. 873-876. 
 S/B C/TU Context ACC M-F-Sc MRR MAP_ref MAP_10 MAP_sys 
C 0 .290 .814 .393 .286 .131 .131  
S TU 0 .391 .850 .483 .384 .160 .160 
C 0 .352 .830 .463 .346 .156 .156 
 
Baseline 
Moses  
B TU 0 .407 .853 .500 .402 .165 .165 
?1 .391 .858 .501 .384 .166 .166  
C ?2 .386 .860 .479 .379 .155 .155 
?1 .406 .858 .466 .398 .178 .178 
 
S 
 
TU ?2 .359 .838 .402 .349 .165 .165 
?1 .431 .865 .534 .423 .177 .177  
C ?2 (NSD1) .420 .867 .519 .413 .170 .170 
?1 .437 .863 .507 .429 .191 .191 
 
 
 
 
IB1  
B 
 
TU ?2 .427 .862 .487 .418 .194 .194 
?1 .372 .849 .482 .366 .160 .160  
C ?2 .371 .847 .476 .364 .156 .156 
?1 .412 .859 .486 .404 .164 .164 
 
S 
 
TU ?2 .416 .860 .493 .409 .166 .166 
?1 .413 .855 .518 .406 .173 .173  
C ?2 (NSD2) .407 .856 .507 .399 .168 .168 
?1 .445 .864 .527 .440 .176 .176 
 
 
 
 
IGTree  
B 
 
TU ?2 .427 .861 .516 .422 .173 .173 
?1 .382 .854 .493 .375 .164 .164  
C ?2 (SD) .399 .863 .488 .392 .157 .157 
?1 .408 .858 .474 .400 .181 .181 
 
S 
 
TU ?2 .395 .857 .453 .385 .182 .182 
?1 .439 .866 .543 .430 .179 .179  
C ?2 (NSD3) .421 .864 .519 .415 .171 .171 
?1 .444 .863 .512 .436 .193 .193 
 
 
 
 
TRIBL  
B 
 
TU ?2 .439 .865 .497 .430 .197 .197 
 S* C ?2 (NSD4) .419 .868 .464 .419 .338 .338 
107
Wide-Coverage Deep Statistical Parsing
Using Automatic Dependency
Structure Annotation
Aoife Cahill?
Dublin City University
Michael Burke??,?
Dublin City University
IBM Center for Advanced Studies
Ruth O?Donovan??
Dublin City University
Stefan Riezler?
Palo Alto Research Center
Josef van Genabith??,?
Dublin City University
IBM Center for Advanced Studies
Andy Way??,?
Dublin City University
IBM Center for Advanced Studies
A number of researchers have recently conducted experiments comparing ?deep? hand-crafted
wide-coverage with ?shallow? treebank- and machine-learning-based parsers at the level of
dependencies, using simple and automatic methods to convert tree output generated by the
shallow parsers into dependencies. In this article, we revisit such experiments, this time using
sophisticated automatic LFG f-structure annotation methodologies with surprising results. We
compare various PCFG and history-based parsers to find a baseline parsing system that fits
best into our automatic dependency structure annotation technique. This combined system of
syntactic parser and dependency structure annotation is compared to two hand-crafted, deep
constraint-based parsers, RASP and XLE. We evaluate using dependency-based gold standards
? Now at the Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Germany. E-mail: aoife.
cahill@ims.uni-stuttgart.de.
?? National Centre for Language Technology, Dublin City University, Dublin 9, Ireland.
? IBM Dublin Center for Advanced Studies (CAS), Dublin 15, Ireland.
? Now at Google Inc., Mountain View, CA.
Submission received: 24 August 2005; revised submission received: 20 March 2007; accepted for publication:
2 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
and use the Approximate Randomization Test to test the statistical significance of the results.
Our experiments show that machine-learning-based shallow grammars augmented with so-
phisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-
coverage constraint grammars. Currently our best system achieves an f-score of 82.73% against
the PARC 700 Dependency Bank, a statistically significant improvement of 2.18% over the
most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system
and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically significant
3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsing
system.
1. Introduction
Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g.,
Penn-II WSJ Section 23 trees) reporting traditional PARSEVALmetrics (Black et al 1991)
of labeled and unlabeled bracketing precision, recall and f-score measures, number of
crossing brackets, complete matches, and so forth. Although tree-based parser evalua-
tion provides valuable insights into the performance of grammars and parsing systems,
it is subject to a number of (related) drawbacks:
1. Bracketed trees do not always provide NLP applications with enough
information to carry out the required tasks: Many applications involve
a deeper analysis of the input in the form of semantically motivated
information such as deep dependency relations, predicate?argument
structures, or simple logical forms.
2. A number of alternative, but equally valid tree representations can
potentially be given for the same input. To give just a few examples: In
English, VPs containing modals and auxiliaries can be analyzed using
(predominantly) binary branching rules (Penn-II [Marcus et al 1994]), or
employ flatter analyses where modals and auxiliaries are sisters of the
main verb (AP treebank [Leech and Garside 1991]), or indeed do without
a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank
bracketing guidelines can use ?traditional? CFG categories such as S, NP,
and so on (Penn-II) or a maximal projection-inspired analysis with IPs
and DPs (Chinese Penn Treebank [Xue et al 2004]).
3. Because a tree-based gold standard for parser evaluation must adopt a
particular style of linguistic analysis (reflected in the geometry and
nomenclature of the nodes in the trees), evaluation of statistical parsers
and grammars that are derived from particular treebank resources (as
well as hand-crafted grammars/parsers) can suffer unduly if the gold
standard deviates systematically from the (possibly) equally valid style
of linguistic analysis provided by the parser.
Problems such as these have motivated research on more abstract, dependency-
based parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll
et al 2002; Clark and Hockenmaier 2002; King et al 2003; Preiss 2003; Kaplan et al
2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are ap-
proximations of abstract predicate-argument-adjunct (or more basic head-dependent)
82
Cahill et al Statistical Parsing Using Automatic Dependency Structures
structures, providing a more normalized representation abstracting away from the
particulars of surface realization or CFG-tree representation, which enables meaningful
cross-parser evaluation.
A related contrast holds between shallow and deep grammars and parsers.1 In
addition to defining a language (as a set of strings), deep grammars relate strings to in-
formation/meaning, often in the form of predicate?argument structure, dependency re-
lations,2 or logical forms. By contrast, a shallow grammar simply defines a language and
may associate syntactic (e.g., CFG tree) representations with strings. Natural languages
do not always interpret linguistic material locally where the material is encountered
in the string (or tree). In order to obtain accurate and complete predicate?argument,
dependency, or logical form representations, a hallmark of deep grammars is that they
usually involve a long-distance dependency (LDD) resolution mechanism.
Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language
Tools [Briscoe et al 1987], the Core Language Engine [Alshawi and Pulman 1992], the
Alpino Dutch dependency parser [Bouma, van Noord, andMalouf 2000], the Xerox Lin-
guistic Environment [Butt et al 2002], the RASP dependency parser [Carroll and Briscoe
2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al 2004]).
Wide-coverage, deep-grammar development, particularly in rich formalisms such as
LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard
and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting
an instance of the (in-)famous ?knowledge acquisition bottleneck? familiar from other
areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars
(Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al 2002)
have, in fact, been successfully scaled to unrestricted input.
The last 15 years have seen extensive efforts on treebank-based automatic gram-
mar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995;
Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein
and Manning 2003). These grammars are wide-coverage and robust and in contrast
to manual grammar development, machine-learning-based grammar acquisition in-
curs relatively low development cost. With few notable exceptions,3 however, these
treebank-induced wide-coverage grammars are shallow: They usually do not attempt
to resolve LDDs nor do they associate strings with meaning representations.
Over the last few years, addressing the knowledge acquisition bottleneck in deep
constraint-based grammar development, a growing body of research has emerged to au-
tomatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia
1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii
2003], LFG [Cahill et al 2002b, 2004]). To a first approximation, these approaches can
be classified as ?conversion?- or ?annotation?-based. TAG-based approaches convert
1 Our use of the terms ?shallow? and ?deep? parsers/grammars follows Kaplan et al (2004) where
a ?shallow parser? does not relate strings to meaning representations. This deviates from a more
common use of the terms where, for example, a ?shallow parser? refers to (often finite-state-based)
parsers (or chunkers) that may produce partial bracketings of input strings.
2 By dependency relations we mean deep, fine-grained, labeled dependencies that encode long-distance
dependencies and passive information, for example. These differ from the types of unlabeled
dependency relations in other work such as (McDonald and Pereira 2006).
3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins
Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range
of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced
material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and
Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of
CFG-based parsers. None of them map strings into dependencies.
83
Computational Linguistics Volume 34, Number 1
treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches
convert trees into CCG derivations fromwhich CCG categories can be extracted. HPSG-
and LFG-based grammar induction methods automatically annotate treebank trees
with (typed) attribute-value structure information for the extraction of constraint-based
grammars and lexical resources.
Two recent papers (Preiss 2003; Kaplan et al 2004) have started tying together
the research strands just sketched: They use dependency-based parser evaluation to
compare wide-coverage parsing systems using hand-crafted, deep, constraint-based
grammars with systems based on a simple version of treebank-based deep grammar
acquisition technology in the conversion paradigm. In the experiments, tree output
generated by Collins?s Model 1, 2, and 3 (1999) and Charniak?s (2000) parsers, for
example, are automatically translated into dependency structures and evaluated against
gold-standard dependency banks.
Preiss (2003) uses the grammatical relations and the CBS 500 Dependency Bank
described in Carroll, Briscoe, and Sanfilippo (1998) to compare a number of parsing
systems (Briscoe and Carroll 1993; Collins?s 1997 models 1 and 2; and Charniak 2000)
using a simple version of the conversion-based deep grammar acquisition process (i.e.,
reading off grammatical relations fromCFG parse trees produced by the treebank-based
shallow parsers). The article also reports on a task-based evaluation experiment to rank
the parsers using the grammatical relations as input to an anaphora resolution system.
Preiss concluded that parser ranking using grammatical relations reflected the absolute
ranking (between treebank-induced parsers) using traditional tree-based metrics, but
that the difference between the performance of the parsing algorithms narrowed when
they carried out the anaphora resolution task. Her results show that the hand-crafted
deep unification parser (Briscoe and Carroll 1993) outperforms the machine-learned
parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision
and recall on grammatical relations.4 Kaplan et al (2004) compare their deep, hand-
crafted, LFG-based XLE parsing system (Riezler et al 2002) with Collins?s (1999) model
3 using a simple conversion-based approach, capturing dependencies from the tree
output of the machine-learned parser, and evaluating both parsers against the PARC
700 Dependency Bank (King et al 2003). They conclude that the hand-crafted, deep
grammar outperforms the state-of-the-art treebank-based shallow parser on the level of
dependency representation, at the price of a small decrease in parsing speed.
Both Preiss (2003) and Kaplan et al (2004) emphasize that they use rather basic
versions of the conversion-based deep grammar acquisition technology outlined herein.
In this article we revisit the experiments carried out by Preiss and Kaplan et al, this time
using the sophisticated and fine-grained treebank- and annotation-based, deep, probabilis-
tic LFG grammar acquisitionmethodology developed in Cahill et al (2002b), Cahill et al
(2004), O?Donovan et al (2004), and Burke (2006) with a number of surprising results:
1. Evaluating against the PARC 700 Dependency Bank (King et al 2003)
using a retrained version of Bikel?s (2002) parser, the best automatically
induced, deep LFG resources achieve an f-score of 82.73%. This is an
improvement of 3.13 percentage points over the previously best published
results established by Kaplan et al (2004) who use a hand-crafted,
wide-coverage, deep LFG and the XLE parsing system. This is also a
4 The numbers given are difficult to compare as the results for the Briscoe and Carroll (1993) parser were
captured for a richer set of grammatical relations than those for Collins (1997) and Charniak (2000).
84
Cahill et al Statistical Parsing Using Automatic Dependency Structures
statistically significant improvement of 2.18 percentage points over the
most recent improved results presented in this article for the XLE system.
2. Evaluating against the Carroll, Briscoe, and Sanfilippo (1998) CBS 500
gold-standard dependency bank using a retrained version of Bikel?s (2002)
parser, the best Penn-II treebank-based, automatically acquired, deep LFG
resources achieve an f-score of 80.23%. This is a statistically significant
improvement of 3.66 percentage points over Carroll and Briscoe (2002),
who use a hand-crafted, wide-coverage, deep, unification grammar and
the RASP parsing system.
Evaluation results on a reannotated version (Briscoe and Carroll 2006) of the PARC
700 Dependency Bank were recently published in Clark and Curran (2007), reporting
f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and Carroll
(2006) point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different.
The article is structured as follows: In Section 2, we outline the automatic LFG
f-structure annotation algorithm and the pipeline parsing architecture of Cahill et al
(2002b), Cahill et al (2004), and Burke (2006). In Section 3, we present our experiment
design. In Section 4, using the DCU 105 Dependency Bank as our development set, we
evaluate a number of treebank-induced LFG parsing systems against the automatically
generated Penn-II WSJ Section 22 Dependency Bank test set. We use the Approximate
Randomization Test (Noreen 1989) to test for statistical significance and choose the best
parsing system for the evaluations against the wide-coverage, hand-crafted RASP and
LFG grammars of Carroll and Briscoe (2002) and Kaplan et al (2004) using the CBS
500 and PARC 700 Dependency Banks in Section 5. In Section 6, we discuss results and
issues raised by our methodology, outline related and future research and conclude in
Section 7.
2. Methodology
In this section, we briefly outline LFG and present our automatic f-structure annotation
algorithm and parsing architecture. The parsing architecture enables us to integrate
PCFG- and history-based parsers, which allows us to compare these parsers at the level
of dependency structures, rather than just trees.
2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a constraint-based theory of grammar. It (minimally) posits two levels of
representation, c(onstituent)-structure and f(unctional)-structure. C-structure is rep-
resented by context-free phrase-structure trees, and captures surface grammatical
configurations such as word order. The nodes in the trees are annotated with functional
equations (attribute-value structure constraints, for example (?OBJ)=?) which are
resolved (in the case of well-formed strings) to produce an f-structure. F-structures
are recursive attribute-value matrices, representing abstract syntactic functions, which
85
Computational Linguistics Volume 34, Number 1
Figure 1
C- and f-structures for the sentence U.N. signs treaty.
approximate to basic predicate-argument-adjunct structures or dependency relations.5
Figure 1 shows the c- and f-structures for the string U.N. signs treaty. Each node in the
c-structure is annotated with f-structure equations, for example (? SUBJ)= ?. The
uparrows (?) point to the f-structure associated with the mother node, downarrows
(?) to that of the local node. In a complete parse tree, these ? and ? meta variables are
instantiated to unique tree node identifiers and a set of constraints (a set of terms in an
equality logic) is generated which (if satisfiable) generates an f-structure.
2.2 Automatic F-Structure Annotation Algorithm
Deep grammars can be induced from treebank resources if the treebank encodes
enough information to support the derivation of deep grammatical information, such
as predicate?argument structures, deep dependency relations, or logical forms. Many
second generation treebanks such as Penn-II provide information to support the compi-
lation of meaning representations, for example in the form of traces relating displaced
linguistic material to where it should be interpreted semantically. The f-structure anno-
tation algorithm exploits configurational and categorial information, as well as traces
and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II
CFG trees with LFG f-structure information.
Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse
the tree and deterministically add f-structure equations to the phrasal and leaf nodes
of the tree, resulting in an f-structure annotated version of the tree. The annotations are
then collected and passed on to a constraint solver which generates an f-structure (if the
constraints are satisfiable). We use a simple graph-unification-based constraint solver
(Eisele and Do?rre 1986), extended to handle path, set-valued, disjunctive, and existential
constraints. Given parser output without Penn-II style annotations and traces, the same
algorithm is used to assign annotations to each node in the tree, whereas a separate
module is applied at the level of f-structure to resolve any long-distance dependencies
(see Section 2.3).
5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms
(QLFs), and Underspecified Discourse Representation Structures (UDRSs).
86
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 1
A complete list of the Penn-II functional labels.
Tag Description
Form/function discrepancies
-ADV clausal and NP adverbials
-NOM non NPs that function as NPs
Grammatical role
-DTV dative
-LGS logical subjects in passives
-PRD non VP predicates
-PUT locative complement of put
-SBJ surface subject
-TPC topicalized and fronted constituents
-VOC vocatives
Adverbials
-BNF benefactive
-DIR direction and trajectory
-EXT extent
-LOC location
-MNR manner
-PRP purpose and reason
-TMP temporal phrases
Miscellaneous
-CLR closely related to verb
-CLF true clefts
-HLN headlines and datelines
-TTL titles
The f-structure annotation algorithm is described in detail in Cahill et al (2002a),
McCarthy (2003), Cahill et al (2004), and Burke (2006). In brief, the algorithm is modular
with four components (Figure 3), taking Penn-II trees as input and automatically adding
LFG f-structure equations to each node in the tree.
Lexical Information. Lexical information is generated automatically by macros for each
of the POS classes in Penn-II. To give a simple example, third-person plural noun
Penn-II POS-word sequences of the form NNS word are automatically associated with
the equations (?PRED) = word?, (?NUM) = pl and (?PERS) = 3rd, where word? is the
lemmatized word.
Left?Right Context Annotation. The Left?Right context annotation component identifies
the heads of Penn-II trees using a modified version of the head finding rules of
Magerman (1994). This partitions each local subtree (of depth one) into a local head, a
left context (left sisters), and a right context (right sisters). The contexts together with
information about the local mother and daughter categories and (if present) Penn-II
87
Computational Linguistics Volume 34, Number 1
Figure 2
Trees for the sentence U.N. signs treaty, the headline said before and after automatic f-structure
annotation, with the f-structure automatically produced.
Figure 3
F-structure annotation algorithm modules.
88
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 2
Sample from an NP Annotation matrix.
Left context Head Right context
DT: (?SPEC DET)=? NN, NNS, NNP, NNPS, NP: RRC, SBAR: (?RELMOD)=?
CD: (?SPEC QUANT)=? ?=? PP: ??(?ADJUNCT)
ADJP, JJ, NN, NNP: ??(?ADJUNCT) NP: ??(?APP)
functional tag labels (Table 1) are used by the f-structure annotation algorithm. For each
Penn-II mother (i.e., phrasal) category an Annotation matrix expresses generalizations
about how to annotate immediate daughters dominated by the mother category relative
to their location in relation to the local head. To give a (much simplified) example,
the head finding rules for NPs state that the rightmost nominal (NN, NNS, NNP, . . . )
not preceded by a comma or ?-?6 is likely to be the local head. The Annotation ma-
trix for NPs states (inter alia) that heads are annotated ?=?, that DTs (determiners)
to the left of the head are annotated (? SPEC DET) = ?, NPs to the right of the head as
??(? APP) (appositions). Table 2 provides a sample extract from the NP Annotation
matrix. Figure 4 provides an example of the application of the NP and PP Annotation
matrices to a simple tree.
For each phrasal category, Annotation matrices are constructed by inspecting the
most frequent Penn-II rule types expanding the category such that the token occurrences
of these rule types cover more than 85% of all occurrences of expansions of that category
in Penn-II. For NP rules, for example, this means that we analyze the most frequent
102 rule types expanding NP, rather than the complete set of more than 6,500 Penn-II
NP rule types, in order to populate the NP Annotation matrix. Annotation matrices
generalize to unseen rule types as, in the case of NPs, these may also feature DTs to
the left of the local head and NPs to the right and similarly for rule types expanding
other categories.
Coordination. In order to support the modularity, maintainability, and extendability of
the annotation algorithm, the Left?Right Annotation matrices apply only to local trees
of depth one, which do not feature coordination. This keeps the statement of Annotation
matrices perspicuous and compact. The Penn-II treatment of coordination is (inten-
tionally) flat. The annotation algorithm has modules for like- and unlike-constituent
coordination. Coordinated constituents are elements of a COORD set and annotated ??
(? COORD). The Coordination module reuses the Left?Right context Annotation ma-
trices to annotate any remaining nodes in a local subtree containing a coordinating
conjunction. Figure 5 provides a VP-coordination example (with right-node-raising).
Catch-All and Clean-Up. The Catch-All and Clean-Up module provides defaults to cap-
ture remaining unannotated nodes (Catch-All) and corrects (Clean-Up) overgeneraliza-
tions resulting from the application of the Left?Right context Annotation matrices. The
Left?Right Annotation matrices are allowed a certain amount of overgeneralization as
this facilitates the perspicuous statement of generalizations and a separate statement of
exceptions, supporting the modularity and maintainability of the annotation algorithm.
PPs under VPs are a case in point. The VP Annotation matrix analyses PPs to the right
of the local VP head as adjuncts: ? ? (?ADJUNCT). The Catch-All and Clean-Up module
6 If the rightmost nominal is preceded by a comma or ?-?, it is likely to be an apposition to the head.
89
Computational Linguistics Volume 34, Number 1
Figure 4
Automatically annotated Penn-II tree (fragment) and f-structure (simplified) for Gerry Purdy,
director of marketing.
uses Penn-II functional tag (Table 1) information (if present), for example -CLR (closely
related to local head), to replace the original adjunct analysis by an oblique argument
analysis: (?OBL)=?. An example of this is provided by the PP-CLR in the left VP-conjunct
in Figure 5. In other cases, argument?adjunct distinctions are encoded configurationally
in Penn-II (without the use of -CLR tags). To give a simple example, the NP Anno-
tation matrix indiscriminately associates SBARs to the right of the local head with
(? RELMOD) = ?. However, some of these SBARs are actually arguments of the local
NP head and, unlike SBAR relative clauses which are Chomsky-adjoined to NP (i.e.,
relative clauses are daughters of an NP mother and sisters of a phrasal NP head), SBAR
arguments are sisters of non-phrasal NP heads.7 In such cases, the Catch-All and Clean-
Up module rewrites the original relative clause analysis into the correct complement
argument analysis (?COMP)=?. Figure 6 shows the COMP f-structure analyses for an
example NP containing an internal SBAR argument (rather than relative clause) node.
Traces. The Traces module translates traces and coindexed material in Penn-II trees
representing long-distance dependencies into corresponding reentrancies at f-structure.
Penn-II provides a rich arsenal of trace types to relate ?displaced? material to where it
7 Structural information of this kind is not encoded in the Annotation matrices; compare Table 2.
90
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 5
Automatically annotated Penn-II tree (fragment) and resulting f-structure for asked for and
received refunds.
should be interpreted semantically. The f-structure annotation algorithm coverswh- and
wh-less relative clause constructions, interrogatives, control and raising constructions,
right-node-raising, and general ICH (interpret constituent here) traces. Figure 5 gives an
example that shows the interplay between coordination, right-node-raising traces and
the corresponding automatically generated reentrancies at f-structure.
2.3 Parsing Architecture
The pipeline parsing architecture of Cahill et al (2004) and Cahill (2004) for parsing raw
text into LFG f-structures is shown in Figure 7. In this model, PCFGs or history-based
lexicalized parsers are extracted from the unannotated treebank and used to parse raw
text into trees. The resulting parse trees are then passed to the automatic f-structure
annotation algorithm to generate f-structures.8
Compared to full Penn-II treebank trees, the output of standard probabilistic
parsers is impoverished: Parsers do not normally output Penn-II functional tag an-
notations (Table 1) nor do they indicate/resolve long-distance dependencies, recorded
8 In the integratedmodel (Cahill et al 2004; Cahill 2004), we extract f-structure annotated PCFGs
(A-PCFGs) from the f-structure annotated treebank, where each non-terminal symbol in the grammar
has been augmented with LFG functional equations, such as NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?].
We treat a non-terminal symbol followed by annotations as a monadic category for grammar extraction
and parsing. Parsing with A-PCFGs results in annotated parse trees, from which an f-structure can be
generated. In this article we only use the pipeline parsing architecture.
91
Computational Linguistics Volume 34, Number 1
Figure 6
Automatically annotated Penn-II tree (fragment) and f-structure for signs that managers
expect declines.
in terms of a fine-grained system of empty productions (traces) and coindexation in
the full Penn-II treebank trees. The f-structure annotation algorithm, as described in
Section 2.2, makes use of Penn-II functional tag information (if present) and relies on
traces and coindexation to capture LDDs in terms of corresponding reentrancies at
f-structure.
Penn-II functional labels are used by the annotation algorithm to discriminate
between adjuncts and (oblique) arguments. PP-sisters to a head verb are analyzed as
arguments iff they are labeled -CLR, -PUT, -DTV or -BNF, for example. Conversely,
functional labels (e.g., -TMP) are also used to analyze certain NPs as adjuncts, and
-LGS labels help to identify logical subjects in passive constructions. In the absence of
functional labels, the annotation algorithm will default to decisions based on simple
structural, configurational, and CFG-category information (and, for example, conserva-
tively analyze a PP sister to a head verb as an adjunct, rather than as an argument).
In Sections 3 and 4 we present a number of treebank-based parsers (in particular the
PCFGs and a version of Bikel?s history-based, lexicalized generative parser) trained to
output CFG categories with Penn-II functional tags. We achieve this through a simple
92
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 7
Treebank-based LFG parsing architecture.
masking and un-masking operation where functional tags are joined with their local
CFG category label to form a new (larger) set of (monadic) CFG category labels (e.g.,
PP-CLR goes to PP CLR) for training and parsing (for Bikel, the parser head-finding rules
are also adjusted to the expanded set of categories). After parsing, the Penn-II functional
tags are unmasked and available to the f-structure annotation algorithm.
The Traces component in the f-structure annotation algorithm (Figure 3) translates
LDDs represented in terms of traces and coindexation in the original Penn-II treebank
trees into corresponding reentrancies at f-structure. Most probabilistic treebank-based
parsers, however, do not indicate/resolve LDDs, and the Traces component of the an-
notation algorithm does not apply. Initially, the f-structures produced for parser output
trees in the architecture in Figure 7 are therefore LDD-unresolved: They are incomplete
(or proto) f-structures, where displaced material (e.g., the values of FOCUS, TOPIC, and
TOPICREL attributes [wh- and wh-less relative clauses, topicalization, and interrogative
constructions] at f-structure) is not yet linked to the appropriate argument grammati-
cal functions (or elements of adjunct sets) for the governing local PRED. A dedicated
LDD Resolution component in the architecture in Figure 7 turns parser output proto-
f-structures into fully LDD-resolved proper f-structures, without traces and coindexa-
tion in parse trees.
Consider the following fragment of a proper Penn-II treebank tree (Figure 8), where
the LDD between the WHNP in the relative clause and the embedded direct object
position of the verb reward is indicated in terms of the trace *T*-3 and its coindexation
with the antecedent WHNP-3. Note further that the control relation between the subject
of the verbs wanted and reward is similarly expressed in terms of traces (*T*-2) and
coindexation (NP-SBJ-2). From the treebank tree, the f-structure annotation algorithm
is able to derive a fully resolved f-structure where the LDD and the control relation are
captured in terms of corresponding reentrancies (Figure 9).
93
Computational Linguistics Volume 34, Number 1
Figure 8
Penn-II treebank tree with LDD indicated in terms of traces (empty productions) and
coindexation and f-structure annotations generated by the annotation algorithm.
Now consider the corresponding ?impoverished? (but otherwise correct) parser
output tree (Figure 10) for the same string: The parser output does not explicitly record
the control relation nor the LDD.
Given this parser output tree, prior to the LDD resolution component in the parsing
architecture (Figure 7), the f-structure annotation algorithmwould initially construct the
partial (proto-) f-structure in Figure 11, where the LDD indicated by the TOPICREL func-
tion is unresolved (i.e., the value of TOPICREL is not coindexedwith the OBJ grammatical
function of the embedded verb reward). The control relation (shared subject between
the two verbs in the relative clause) is in fact captured by the annotation algorithm in
terms of a default annotation (? SUBJ) = (? SUBJ) on sole argument VPs to the right of
head verbs (as often, even in the full Penn-II treebank trees, control relations are not
consistently captured through explicit argument traces).
In LFG, LDD resolution operates at the level of f-structure, using functional un-
certainty equations (regular expressions over paths in f-structure [Kaplan and Zaenen
1989] relating f-structure components in different parts of an f-structure), obviating
traces and coindexation in c-structure trees. For the example in Figure 10, a functional
uncertainty equation of the form (?TOPICREL) = (?[COMP|XCOMP]? [SUBJ|OBJ]) would
be associated with the WHNP daughter node of the SBAR relative clause. The equation
states that the value of the TOPICREL attribute is token-identical (re-entrant) with the
value of a SUBJ or OBJ function, reached through a path along any number (including
94
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 9
Fully LDD-resolved f-structure.
Figure 10
Impoverished parser output tree: LDDs not captured.
zero) of COMP or XCOMP attributes. This equation, together with subcategorization
frames (LFG semantic forms) for the local PREDs and the usual LFG completeness and
coherence conditions, resolve the partial proto-f-structure in Figure 11 into the fully
LDD-resolved proper f-structure in Figure 9.
95
Computational Linguistics Volume 34, Number 1
Figure 11
Proto-f-structure: LDDs not captured.
Following Cahill et al (2004), in our parsing architecture (Figure 7) we model
LFG LDD resolution using automatically induced finite approximations of functional-
uncertainty equations and subcategorization frames from the f-structure-annotated
Penn-II treebank (O?Donovan et al 2004) in an LDD resolution component. From the
fully LDD-resolved f-structures from the Penn-II training section treebank trees we
learn probabilistic LDD resolution paths (reentrancies in f-structure), conditional on
LDD type (Table 3), and subcategorization frames, conditional on lemma (and voice)
(Table 4). Table 3 lists the eight most probable TOPICREL paths (out of a total of 37
TOPICREL paths acquired). The totality of these paths constitutes a finite subset of the
reference language definde by the full functional uncertainty equation (?TOPICREL) =
(?[COMP|XCOMP]? [SUBJ|OBJ]). Given an unresolved LDD type (such as TOPICREL in
the parser output for the relative clause example in Figure 11), admissible LDD res-
olutions assert a reentrancy between the value of the LDD trigger (here, TOPICREL)
and a grammatical function (or adjunct set element) of an embedded local predicate,
subject to the conditions that (i) the local predicate can be reached from the LDD trigger
using the LDD path; (ii) the grammatical function terminates the LDD path; (iii) the
grammatical function is not already present (at the relevant level of embedding in
the local f-structure); and (vi) the local predicate subcategorizes for the grammatical
function in question.9 Solutions satisfying (i)?(iv) are ranked using the product of
LDD path and subcategorization frame probabilities and the highest ranked solution
(possibly involving multiple interacting LDDs for a single f-structure) is returned by
the algorithm (for details and comparison against alternative LDD resolution methods,
see Cahill et al 2004).10
For our example (Figure 11), the highest ranked LDD resolution is for LDD path
(?TOPICREL) = (? XCOMP OBJ) and the local subcat frame REWARD?? SUBJ, ? OBJ?. This
9 Conditions (i)?(iv) are suitably adapted for LDD resolutions terminating in adjunct sets.
10 In our experiments we do not use the limited LDD resolution for wh-phrases provided by Collins?s Model
3 parser as better results are achieved using the purely f-structure-based LDD resolution as shown in
Cahill et al (2004).
96
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 3
Most frequent wh-TOPICREL paths.
wh-TOPICREL Probability wh-TOPICREL Probability
subj .7583 xcomp .0830
obj .0458 xcomp:obj .0338
xcomp:xcomp .0168 xcomp:subj .0109
comp .0097 comp:subj .0073
Table 4
Most frequent semantic forms for active and passive (p) occurrences of the verb want and
reward.
Semantic form Probability
want([subj,xcomp]) .6208
want([subj,obj]) .2496
want([subj,obj,xcomp]) .1008
want([subj]) .0096
want([subj,obj,obl]) .0048
want([subj,obj,part]),p) .5000
want([subj,obl]),p) .1667
want([subj,part]),p) .1667
want([subj]),p) .1667
reward([subj,obj]) .8000
reward([subj,obj,obl]) .2000
reward([subj]),p) 1.0000
(together with the subject control equation described previously) turns the parser-
output proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in
(Figure 9).
The full pipeline parsing architecture with the LDD resolution (rather than the
Traces component for LDD resolved Penn-II treebank trees) component (and the LDD
path and subcategorization frame extraction) is given in Figure 7.
The pipeline architecture supports flexible integration of treebank-based PCFGs
or state-of-the-art, history-based, and lexicalized parsers (Collins 1999; Charniak 2000;
Bikel 2002) and enables dependency-based evaluation of such parsers.
3. Experiment Design
In our experiments we compare four history-based parsers for integration into the
pipeline parsing architecture described in Section 2.3:
 Collins?s 1999 Models 311
 Charniak?s 2000 maximum-entropy inspired parser12
11 Downloaded from ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz.
12 Downloaded from ftp://ftp.cs.brown.edu/pub/nlparser/.
97
Computational Linguistics Volume 34, Number 1
 Bikel?s 2002 emulation of Collins Model 213
 a retrained version of Bikel?s (2002) parser which retains Penn-II functional
tags
Input for Collins?s and Bikel?s parsers was pre-tagged using the MXPOST POS tag-
ger (Ratnaparkhi 1996). Charniak?s parser provides its own POS tagger. The combined
system of best history-based parser and automatic f-structure annotation is compared
to two probabilistic parsing systems based on hand-crafted, wide-coverage, constraint-
based, deep grammars:
 the RASP parsing system (Carroll and Briscoe 2002)
 the XLE parsing system (Riezler et al 2002; Kaplan et al 2004)
Both hand-crafted grammars perform their own POS tagging, resolve LDDs, and
associate strings with dependency relations (in the form of grammatical relations or
LFG f-structures).
We evaluate the parsers against a number of gold-standard dependency banks.
We use the DCU 105 Dependency Bank (Cahill et al 2002a) as our development set
for the treebank-based LFG parsers. We use the f-structure annotation algorithm to
automatically generate a gold-standard test set from the original Section 22 treebank
trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best
treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.
Following the experimental setup in Kaplan et al (2004), we use the Penn-II Section 23-
based PARC 700 Dependency Bank (King et al 2003) to evaluate the treebank-induced
LFG resources against the hand-crafted XLE grammar and parsing system of Riezler
et al (2002) and Kaplan et al Following Preiss (2003), we use the SUSANNE Based CBS
500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebank-
induced LFG resources against the hand-crafted RASP grammar and parsing system
(Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al 2002).
For each gold standard, our experiment design is as follows: We parse automati-
cally tagged input14 sentences with the treebank- and machine-learning-based parsers
trained on WSJ Sections 02?21 in the pipeline architecture, pass the resulting parse
trees to our automatic f-structure annotation algorithm, collect the f-structure equations,
pass them to a constraint-solver which generates an f-structure, resolve long-distance
dependencies at f-structure following Cahill et al (2004) and convert the resulting LDD-
resolved f-structures into dependency representations using the formats and software
of Crouch et al (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations)
and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS
500 evaluation). In the experiments we did not use any additional annotations such as
-A (for argument) that can be generated by some of the history-based parsers (Collins
1999) as the f-structure annotation algorithm is designed for Penn-II trees (which do
not contain such annotations). We also did not use the limited LDD resolution for wh-
relative clauses provided by Collins?s Model 3 as better results are achieved by LDD
13 This was developed at the University of Pennsylvania by Dan Bikel and is freely available to download
from http://www.cis.upenn.edu/?dbikel/software.html.
14 Tags were automatically assigned either by the parsers themselves or by the MXPOST tagger
(Ratnaparkhi 1996).
98
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 5
Results of tree-based evaluation on all sentences WSJ section 23, Penn-II.
Parser Labeled
f-score (%)
PCFG 73.03
Parent-PCFG 78.05
Collins M3 88.33
Charniak 89.73
Bikel 88.32
Bikel+Tags 87.53
resolution on f-structure (Cahill et al 2004). A complete set of parameter settings for the
parsers is provided in the Appendix.
In order to evaluate the treebank-induced LFG resources against the PARC 700
and the CBS 500 dependency banks, a certain amount of automatic mapping is re-
quired to account for systematic differences in linguistic analysis, feature geometry,
and nomenclature at the level of dependencies. This is discussed in Sections 5.1 and
5.2. Throughout, we use the Approximate Randomization Test (Noreen 1989) to test the
statistical significance of the results.
4. Choosing a Treebank-Based LFG Parsing System
In this section, we choose the best treebank-based LFG parsing system for the compar-
isons with the hand-crafted XLE and RASP resources in Section 5. We use the DCU
105 Dependency Bank as our development set and carry out comparative evaluation
and statistical significance testing on the larger, automatically generated WSJ Section 22
Dependency Bank as a test set. The system based on Bikel?s (2002) parser retrained to
retain Penn-II functional tags (Table 1) achieves overall best results.
4.1 Tree-Based Evaluation against WSJ Section 23
For reference, we include the traditional CFG-tree-based comparison for treebank-
induced parsers. The parsers are trained on Sections 02 to 21 of the Penn-II Treebank and
tested on Section 23. The published results15 on these experiments for the history-based
parsers are given in Table 5. We also include figures for a PCFG and a Parent-PCFG (a
PCFG which has undergone the parent transformation [Johnson 1999]). These PCFGs
are induced following standard treebank preprocessing steps, including elimination of
empty nodes, but following Cahill et al (2004), they do include Penn-II functional tags
(Table 1), as these tags contain valuable information for the automatic f-structure anno-
tation algorithm (Section 2.2). These tags are removed for the tree-based evaluation.
The results show that the history-based parsers produce considerably better trees
than the more basic PCFGs (with and without parent transformations). Charniak?s
(2000) parser scores best with an f-score of 89.73% on all sentences in Section 23. The
15 Where there were no published results available for Section 23, we calculated them using the
downloadable versions of the parsers.
99
Computational Linguistics Volume 34, Number 1
vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentage
points. The hand-crafted XLE and RASP grammars achieve around 80% coverage
(measured in terms of complete spanning parse) on Section 23 and use a variety of
(longest) fragments combining techniques to generate dependency representations for
the remaining 20% of Section 23 strings. By contrast, the treebank-induced PCFGs and
history-based parsers all achieve coverage of over 99.9%. Given that the history-based
parsers score considerably better than PCFGs on trees, we would also expect them to
produce dependency structures of substantially higher quality.
4.2 Using DCU 105 as a Development Set
The DCU 105 (Cahill et al 2002a) is a hand-crafted gold-standard dependency bank
for 105 sentences, randomly chosen from Section 23 of the Penn-II Treebank.16 This is a
relatively small gold standard, initially developed to evaluate the automatic f-structure
annotation algorithm. We parse the 105 tagged sentences into LFG f-structures with
each of the treebank-induced parsers in the pipeline parsing and f-structure annotation
architecture. The f-structures of the gold standard and the f-structures returned by the
parsing systems are converted into dependency triples following Crouch et al (2002)
and Riezler et al (2002) and we also use their software for evaluation. The following
dependency triples are produced by the f-structure in Figure 1:
subj(sign?0,U.N.?1)
obj(sign?0,treaty?2)
num(U.N.?1,sg)
pers(U.N.?1,3)
num(treaty?2,sg)
pers(treaty?3,3)
tense(sign?0,present)
We evaluate preds-only f-structures (i.e., where paths in f-structures end in a PRED
value: the predicate-argument-adjunct structure skeleton) and all grammatical func-
tions (GFs) including number, tense, person, and so on. The results are given in Table 6.
With one main exception, Tables 5 and 6 confirm the general expectation that
the better the trees produced by the parsers, the better the f-structures automatically
generated for those trees. The exception is Bikel+Tags. The automatic f-structure an-
notation algorithm will exploit Penn-II functional tag information if present to generate
appropriate f-structure equations (see Section 2.2). It will default to possibly less reliable
configurational and categorial information if Penn-II tags are not present in the trees.
In order to test whether the retention of Penn-II functional labels in the history-
based parser output will improve LFG f-structure-based dependency results, we use
Bikel?s (2002) training software,17 and retrain the parser on a version of the Penn-II
treebank (Sections 02 to 21) with the Penn-II functional tag labels (Table 1) annotated
in such a way that the resulting history-based parser will retain them (Section 2.3). The
retrained parser (Bikel+Tags) then produces CFG-trees with Penn-II functional labels
and these are used by the f-structure annotation algorithm. We evaluate the f-structure
dependencies against the DCU 105 (Table 6) and achieve an f-score of 82.92% preds-only
16 It is publicly available for download from: http://nclt.computing.dcu.ie/gold105.txt.
17 We use Bikel?s software rather than Charniak?s for this experiment as the former proved more stable
during the retraining phase.
100
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 6
Treebank-induced parsers: results of dependency-based evaluation against DCU 105.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.24 79.90
Parent-PCFG 75.84 83.58
Collins M3 77.84 85.08
Charniak 79.61 85.66
Bikel 79.39 86.56
Bikel+Tags 82.92 88.30
Table 7
Treebank induced parsers: breakdown by dependency relation of preds-only evaluation against
DCU 105.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.73 71 72 76 73 79
APP 0.68 61 0 55 70 65
COMP 2.31 60 61 66 61 73
COORD 5.73 64 73 77 67 76
DET 9.58 91 93 96 96 96
FOCUS 0.04 100 100 0 100 100
OBJ 16.42 82 84 86 85 90
OBJ2 0.07 80 57 50 57 50
OBL 2.17 58 24 27 23 63
OBL2 0.07 50 0 0 0 67
OBL AG 0.43 40 96 96 92 92
POSS 2.88 80 83 82 82 79
QUANT 1.85 70 67 69 70 70
RELMOD 1.78 50 78 67 78 73
SUBJ 14.74 80 81 83 85 85
TOPIC 0.46 85 87 96 96 89
TOPICREL 1.85 61 80 80 79 74
XCOMP 5.20 90 92 79 93 93
and 88.3% all GFs. A detailed breakdown by dependency is given in Table 7. The system
based on the retrained parser is nowmuch better able to identify oblique arguments and
overall preds-only accuracy has improved by 3.53% over the original Bikel experiment
and 3.31% over Charniak?s parser, even though Charniak?s parser performs more than
2% better on the tree-based scores in Table 5 and even though the retrained parser drops
0.79% against the original Bikel parser on the tree-based scores.18
Inspection of the results broken down by grammatical function (Table 7) for the
preds-only evaluation against the DCU 105 shows that just over one third of all depen-
dency triples in the gold standard are adjuncts. SUBJ(ects) and OBJ(ects) together make
up a further 30%.
18 The figures suggest that retraining Charniak?s parser to retain Penn-II functional tags is likely to produce
even better dependency scores than those achieved by Bikel?s retrained parser.
101
Computational Linguistics Volume 34, Number 1
Table 7 shows that the treebank-based LFG system using Collins?s Models 3 is
unable to identify APP(osition). This is due to Collins?s treatment of punctuation and
the fact that punctuation is often required to reliably identify apposition.19 None of
the original history-based parsers produced trees which enabled the annotation algo-
rithm to identify second oblique dependencies (OBL2), and they generally performed
considerably worse than Parent-PCFG when identifying OBL(ique) dependencies. This
is because the automatic f-structure annotation algorithm is cautious to the point of
undergeneralization when identifying oblique arguments. In many cases, the algorithm
relies on the presence of, for example, a -CLR Penn-II functional label (indicating that the
phrase is closely related to the verb), and the history-based (Collins M3, Charniak, and
Bikel) parsers do not produce these labels, whereas Parent-PCFG (as well as PCFG) are
trained to retain Penn-II functional labels. Parent-PCFG, by contrast, performs poorly
for oblique agents (OBL AG, agentive by-phrases in passive constructions), whereas the
history-based parsers are able to identify these with considerable accuracy. This is be-
cause Parent-PCFG often erroneously finds oblique agents, even when the preposition
is not by, as it never has enough context in which to distinguish by prepositional phrases
from other PPs. The history-based parsers produce trees from which the automatic
f-structure annotation algorithm can better identify RELMOD and TOPICREL dependen-
cies than Parent-PCFG. This, in turn, leads to improved long distance dependency
resolution which improves overall accuracy.
The DCU 105 development set is too small to support reliable statistical significance
testing of the performance ranking of the six treebank-based LFG parsing systems. In
order to carry out significance testing to select the best treebank-based LFG parsing
system for comparative evaluation against the hand-crafted deep XLE and RASP re-
sources, we move to a larger dependency-based evaluation data set: the gold-standard
dependency bank automatically generated fromWSJ Section 22.
4.3 Evaluation against WSJ Section 22 Dependencies
In an experimental setup similar to that of Hockenmaier and Steedman (2002),20 we
evaluate each parser against a large automatically generated gold standard. The gold-
standard dependency bank is automatically generated by annotating the original 1,700
treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure an-
notation algorithm. We then evaluate the f-structures generated from the tree output
of the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22
strings against the automatically produced f-structures for the original Section 22 Penn-II
treebank trees. The results are given in Table 8.
Compared to Table 6 for the DCU 105 gold standard, most scores are up, particularly
so for the history-based parsers. This trend is possibly due to the fact that the WSJ
19 The annotation algorithm relies on Penn-II-style punctuation patterns where an NP apposition follows a
nominal head separated by a comma ([NP [NP Bush ] , [NP the president ] ]), all three sisters of the same
mother node, while the trees produced by Collins?s parser attach the comma low in the tree ([NP [NP
Bush,] [NP the president ] ]). Although it would be trivial to carry out a tree transformation on the Collins
output to raise the punctuation to the expected level, we have not done this here.
20 This corresponds to experiments where the original Penn-II Section 23 treebank trees are automatically
converted into CCG derivations, which are then used as a gold standard to evaluate the CCG parser
trained on Sections 02?21. A similar methodology is used for the evaluation of treebank-based HPSG
resources (Miyao, Ninomiya, and Tsujii 2003) where Penn-II treebank trees are automatically annotated
with HPSG typed-feature structure information.
102
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 8
Results of dependency-based evaluation against the automatically generated gold standard for
WSJ Section 22.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.76 80.44
Parent-PCFG 74.92 83.04
Collins M3 79.30 86.00
Charniak 81.35 86.96
Bikel 81.40 87.00
Bikel+Tags 83.06 87.63
Section 22 gold standard is generated automatically from the original ?perfect? Penn-II
treebank trees using the automatic f-structure annotation algorithm, whereas the DCU
105 has been created manually without regard as to whether or not the f-structure
annotation algorithm could ever generate the f-structures, even given the ?perfect?
trees.
The LFG system based on Bikel?s retrained parser achieves the highest f-score of
83.06% preds-only and 87.63% all GFs. Parent-PCFG achieves an f-score of 74.92%
preds-only and 83.04% all GFs. Table 9 provides a breakdown by feature of the preds-
only evaluation.
Table 9 shows that, once again, the automatic f-structure annotation algorithm is
not able to identify any cases of apposition from the output of Collins?s Model 3 parser.
Apart from Bikel?s retrained parser, none of the history-based parsers are able to identify
Table 9
Breakdown by dependency of results of preds-only evaluation against the automatically
generated Section 22 gold standard.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.77 70 75 78 78 80
APP 0.74 61 0 77 77 71
COMP 1.35 60 72 70 70 80
COORD 5.11 74 78 82 82 81
DET 10.72 88 91 92 92 91
FOCUS 0.02 27 67 88 88 71
OBJ 16.17 80 85 87 87 88
OBJ2 0.07 15 30 32 32 71
OBL 1.92 50 19 21 21 73
OBL2 0.07 47 3 3 3 69
OBL AG 0.31 50 90 89 89 85
POSS 2.47 86 91 91 91 91
QUANT 2.12 89 89 93 93 92
RELMOD 1.84 51 71 72 72 69
SUBJ 15.45 75 80 81 81 82
TOPIC 0.44 81 85 84 84 76
TOPICREL 1.82 61 72 74 75 69
XCOMP 5.62 81 87 81 81 88
103
Computational Linguistics Volume 34, Number 1
Figure 12
Approximate Randomization Test for statistical significance testing.
OBJ2, OBL or OBL2 dependencies very well, although Parent-PCFG is able to produce
trees from which it is easier to identify obliques (OBL), because of the Penn-II functional
-CLR label. The automatic annotation algorithm is unable to identify RELMOD depen-
dencies satisfactorily from the trees produced by parsing with Parent-PCFG, although
the history-based parsers score reasonably well for this function. Whereas Charniak?s
parser is able to identify some dependencies better than Bikel?s retrained parser, overall
the system based on Bikel?s retrained parser performs better when evaluating against
the dependencies in WSJ Section 22.
In order to determine whether the results are statistically significant, we use the Ap-
proximate Randomization Test (Noreen 1989).21 This test is an example of a computer-
intensive statistical hypothesis test. Such tests are designed to assess result differences
with respect to a test statistic in cases where the sampling distribution of the test statistic
is unknown. Comparative evaluations of outputs of parsing systems according to test
statistics, such as differences in f-score, are examples of this situation. The test statistics
are computed by accumulating certain count variables over the sentences in the test
set. In the case of f-score, variable tuples consisting of the number of dependency-
relations in the parse for the system translation, the number of dependency-relations
in the parse for the reference translation, and the number of matching dependency-
relations between system and reference parse, are accumulated over the test set.
Under the null hypothesis, the compared systems are not different, thus any vari-
able tuple produced by one of the systems could just as likely have been produced by
the other system. So shuffling the variable tuples between the two systems with equal
probability, and recomputing the test statistic, creates an approximate distribution of
the test statistic under the null hypothesis. For a test set of S sentences there are 2S
different ways to shuffle the variable tuples between the two systems. Approximate
randomization produces shuffles by random assignments instead of evaluating all 2S
possible assignments. Significance levels are computed as the percentage of trials where
the pseudo statistic, that is the test statistic computed on the shuffled data, is greater
than or equal to the actual statistic, that is the test statistic computed on the test data. A
sketch of an algorithm for approximate randomization testing is given in Figure 12.
21 Applications of this test to natural language processing problems can be found in Chinchor et al (1993)
and Yeh (2000).
104
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 10
Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for
approximate randomization test for 10,000,000 randomizations.
PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
PCFG - - - - - -
Parent-PCFG <.0001 - - - - -
Collins M3 <.0001 <.0001 - - - -
Charniak <.0001 <.0001 <.0001 - - -
Bikel <.0001 <.0001 <.0001 .0003 - -
Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 -
Table 10 gives the p-values (the smallest fixed level at which the null hypothesis can
be rejected) for comparing each parser against all of the other parsers. We test for sig-
nificance at the 95% level. Because we are doing a pairwise comparison of six systems,
giving 15 comparisons, the p-value needs to be below .0034 for there to be a significant
difference at the 95% level.22 For each parser, the values in the row corresponding to
that parser represent the p-values for those parsers that achieve a lower f-score than
that parser. This shows that the system based on Bikel?s retrained parser is significantly
better than those based on the other parsers with a statistical significance of >95%. For
the XLE and RASP comparisons, we will use the f-structure-annotation algorithm and
Bikel retrained-based LFG system.
5. Cross-Formalism Comparison of Treebank-Induced and Hand-Crafted Grammars
From the experiments in Section 4, we choose the treebank-based LFG system using
the retrained version of Bikel?s parser (which retains Penn-II functional tag labels) to
compare against parsing systems using deep, hand-crafted, constraint-based grammars
at the level of dependencies. We report on two experiments. In the first experiment
(Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained
parser-based LFG system against the hand-crafted, wide-coverage LFG and XLE pars-
ing system (Riezler et al 2002; Kaplan et al 2004) on the PARC 700 Dependency Bank
(King et al 2003). In the second experiment (Section 5.2), we evaluate against the hand-
crafted, wide-coverage unification grammar and RASP parsing system of Carroll and
Briscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998).
5.1 Evaluation against PARC 700
The PARC 700 Dependency Bank (King et al 2003) provides dependency relations
(including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of
the Penn-II Treebank. In order to evaluate the parsers, we follow the experimental setup
of Kaplan et al (2004) with a split of 560 dependency structures for the test set and 140
for the development set. The set of features (Table 12, later in this article) evaluated
in the experiment form a proper superset of preds-only, but a proper subset of all
22 Based on Cohen (1995, p. 190): ?e ? 1 ? (1 ? ?c )m, where m is the number of pairwise comparisons, ?e is
the experiment-wise error, and ?c is the per-comparison error.
105
Computational Linguistics Volume 34, Number 1
Figure 13
PARC 700 conversion software.
grammatical functions (preds-only ? PARC ? all GFs). This feature set was selected in
Kaplan et al because the features carry important semantic information. There are sys-
tematic differences between the PARC 700 dependencies and the f-structures generated
in our approach as regards feature geometry, feature nomenclature, and the treatment
of named entities. In order to evaluate against the PARC 700 test set, we automatically
map the f-structures produced by our parsers to a format similar to that of the PARC
700 Dependency Bank. This is done with conversion software in a post-processing stage
on the f-structure annotated trees (Figure 13).
The conversion software is developed on the 140-sentence development set of the
PARC 700, except for the Multi-Word Expressions section. Following the experimental
setup of Kaplan et al (2004), we mark up multi-word expression predicates based on
the gold-standard PARC 700 Dependency Bank.
Multi-Word Expressions The f-structure annotation algorithm analyzes the internal
structure of all noun phrases fully. In Figure 14, for example, BT is analyzed as
an ADJUNCT modifier of the head securities, whereas PARC 700 analyzes this and
other (more complex) named entities as multi-word expression predicates. The
conversion software transforms the output of the f-structure annotation algorithm
into the multi-word expression predicate format.
Feature Geometry In constructions such as Figure 2, the f-structure annotation algo-
rithm analyzes say as the main PRED with what is said as the value of a COMP
argument. In the PARC 700, these constructions are analyzed in such a way that
what is said/reported provides the top level f-structure whereas other material
(who reported, etc.) is analyzed in terms of ADJUNCTs modifying the top level
f-structure. A further systematic structural divergence is provided by the analysis
Figure 14
Named entity and OBL AG feature geometry mapping.
106
Cahill et al Statistical Parsing Using Automatic Dependency Structures
of passive oblique agent constructions (Figure 14): The f-structure annotation
algorithm generates a complex internal analysis of the oblique agent PP, whereas
the PARC analysis encodes a flat representation. The conversion software adjusts
the output of the f-structure annotation algorithm to the PARC-style encoding of
linguistic information.
Feature Nomenclature There are a number of systematic differences between feature
names used by the automatic annotation algorithm and PARC 700: For example,
DET is DET FORM in the PARC 700, COORD is CONJ, FOCUS is FOCUS INT. Nomen-
clature differences are treated in terms of a simple relabeling by the conversion
software.
Additional Features A number of features in the PARC 700 are not produced by the au-
tomatic annotation algorithm. These include: AQUANT for adjectival quantifiers,
MOD for NP-internal modifiers, and STMT TYPE for statement type (declarative,
interrogative, etc.). Additional features (and their values) are automatically gen-
erated by the mapping software, using categorial, configurational, and already
produced f-structure annotation information, extending the original annotation
algorithm.
XCOMP Flattening The automatic annotation algorithm treats both auxiliary and
modal verb constructions in terms of hierarchically cascading XCOMPs, whereas
in PARC 700 the temporal and aspectual information expressed by auxiliary verbs
is represented in terms of a flat analysis and features (Figure 15). The conversion
software automatically flattens the f-structures produced by the automatic anno-
tation algorithm into the PARC-style encoding.
For full details of the mapping, see Burke et al (2004).
In our parsing experiments, we used the most up-to-date version of the hand-
crafted, wide-coverage, deep LFG resources and XLE parsing system with improved
results over those reported in Kaplan et al (2004): This latest version achieves 80.55%
f-score, a 0.95 percentage point improvement on the previous 79.6%. The XLE parsing
system combines a large-scale, hand-crafted LFG for English and a statistical disam-
biguation component to choose the most likely analysis among those returned by
the symbolic parser. The statistical component is a log-linear model trained on 10,000
partially labeled structures from the WSJ. The results of the parsing experiments are
presented in Table 11. We also include a figure for the upper bound of each system.23
Using Bikel?s retrained parser, the treebank-based LFG system achieves an f-score of
82.73%, and the hand-crafted grammar and XLE-based system achieves an f-score
of 80.55%. The approximate randomization test produced a p-value of .0054 for this
pairwise comparison, showing that this result difference is statistically significant at
the 95% level. Evaluation results on a reannotated version (Briscoe and Carroll 2006) of
the PARC 700 Dependency Bank were recently published in Clark and Curran (2007),
reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and
23 The upper bound for the treebank-based LFG system is determined by taking the original Penn-II WSJ
Section 23 trees corresponding to the PARC 700 strings, automatically annotating them with the
f-structure annotation algorithm, and evaluating the f-structures against the PARC 700 dependencies. The
upper bound for the XLE system is determined by selecting the XLE parse that scores best against the
PARC 700 dependencies for each of the PARC 700 strings. It is interesting to note that the upper bound
for the treebank-based system is only 1.18 percentage points higher than that for the XLE system. Apart
from the two different methods for establishing the upper bounds, this is most likely due to the fact that
the mapping required for evaluating the treebank-based LFG system against PARC 700 is lossy (cf. the
discussion in Section 6).
107
Computational Linguistics Volume 34, Number 1
Figure 15
DCU 105 and PARC 700 analyses for the sentence Unlike 1987, interest rates have been falling
this year.
Carroll point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different. Kaplan et al and our experiments use a fine-grained
feature set of 34 features (Table 12), while the Briscoe and Carroll scheme uses 17
features.
A breakdown by dependency relation for each system is given in Table 12. The
treebank-induced grammar system can better identify DET FORM, SUBORD FORM, and
Table 11
Results of evaluation against the PARC 700 Dependency Bank following the experimental setup
of Kaplan et al (2004).
Bikel+Tags XLE p-Value
F-score 82.73 80.55 .0054
Upper bound 86.83 85.65 -
108
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 12
Breakdown by dependency relation of results of evaluation against PARC 700.
Dep. Percent of deps. F-score (%)
Bikel+Tags XLE
ADEGREE 6.17 80 82
ADJUNCT 14.32 68 66
AQUANT 0.06 78 61
COMP 1.23 80 74
CONJ 2.64 73 69
COORD FORM 1.20 83 90
DET FORM 4.61 97 91
FOCUS 0.02 0 36
MOD 2.74 74 67
NUM 19.82 91 89
NUMBER 1.42 89 83
NUMBER TYPE 2.10 94 86
OBJ 8.92 87 78
OBJ THETA 0.05 43 31
OBL 0.83 55 69
OBL AG 0.22 82 76
OBL COMPAR 0.07 38 56
PASSIVE 1.14 80 88
PCASE 0.25 79 68
PERF 0.41 89 90
POSS 0.98 88 80
PRECOORD FORM 0.03 0 91
PROG 0.97 89 81
PRON FORM 2.54 92 94
PRON INT 0.03 0 33
PRON REL 0.57 74 72
PROPER 3.56 83 93
PRT FORM 0.22 80 41
QUANT 0.34 77 80
STMT TYPE 5.23 87 80
SUBJ 8.51 78 78
SUBORD FORM 0.93 47 42
TENSE 5.02 95 90
TOPIC REL 0.57 56 73
XCOMP 2.29 80 78
PRT FORM dependencies24 and achieves higher f-scores for OBJ and POSS. However, the
hand-crafted parsing system can better identify FOCUS, OBL(ique) arguments, PRECO-
ORD FORM and TOPICREL relations.
5.2 Evaluation against CBS 500
We also compare the hand-crafted, deep, probabilistic unification grammar-based RASP
parsing system of Carroll and Briscoe (2002) to our treebank- and retrained Bikel
24 DET FORM, SUBORD FORM, and PRT FORM (and in general X FORM) dependencies record (semantically
relevant) surface forms in f-structure for X-type closed class categories.
109
Computational Linguistics Volume 34, Number 1
Figure 16
CBS 500 conversion software.
parser-based LFG system. The RASP parsing system is a domain-independent, robust
statistical parsing system for English, based on a hand-written, feature-based unification
grammar. A probabilistic parse selection model conditioned on the structural parse
context, degree of support for a subanalysis in the parse forest, and lexical informa-
tion (when available) chooses the most likely parses. For this experiment, we evaluate
against the CBS 500,25 developed by Carroll, Briscoe, and Sanfilippo (1998) in order to
evaluate a precursor of the RASP parsing resources. The CBS 500 contains dependency
structures (including some long distance dependencies26) for 500 sentences chosen at
random from the SUSANNE corpus (Sampson 1995), but subject to the constraint that
they are parsable by the parser in Carroll, Briscoe, and Sanfilippo. Aswith the PARC 700,
there are systematic differences between the f-structures produced by our methodology
and the dependency structures of the CBS 500. In order to be able to evaluate against
the CBS 500, we automatically map our f-structures into a format similar to theirs. We
did not split the data into a heldout and a test set when developing the mapping, so
that a comparison could be made with other systems that report evaluations against
the CBS 500. The following CBS 500-style grammatical relations are produced from the
f-structure in Figure 1:
(ncsubj sign U.N.)
(dobj sign treaty)
Some mapping is carried out (as in the evaluation against the PARC 700) on the f-
structure annotated trees, and the remaining mapping is carried out on the f-structures
(Figure 16). As with the PARC 700 mapping, all mappings are carried out automatically.
The following phenomena were dealt with on the f-structure annotated trees:
Auxiliary verbs (xcomp flattening) XCOMPS were flattened to promote the main verb
to the top level, while maintaining a list of auxiliary and modal verbs and their
relation to one another.
Treatment of topicalized sentences The predicate of the topicalized sentence became
the main predicate and any other top level material became an adjunct.
Multi-word expressions Multi-word expressions (such as according to) were not
marked up in the parser input, but captured in the annotated trees and the
annotations adjusted accordingly.
Treatment of the verbs be and become Our automatic annotation algorithm does not
treat the verbs be and become differently from any other verbs when they are used
transitively. This analysis conflicted with the CBS 500 analysis, so was changed to
match theirs.
25 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
26 The long distance dependencies include passive, wh-less relative clauses, control verbs, and so forth.
110
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 13
Results of dependency evaluation against the CBS 500 (Carroll, Briscoe, and Sanfillipo 1998).
Bikel+Tags RASP p-Value
F-score 80.23 76.57 <.0001
The following are the main mappings carried out on the f-structures:
Encoding of Passive We treat passive as a feature in our automatic f-structure annota-
tion algorithm, whereas the CBS 500 triples encode this information indirectly.
Objects of Prepositional Phrases No dependency was generated for these objects, as
there was no corresponding dependency in the CBS 500 analyses.
Nomenclature Differences There were some trivial mappings to account for differ-
ences in nomenclature, for example OBL in our analyses became IOBJ in the
mapped dependencies.
Encoding of wh-less relative clauses These are encoded by means of reentrancies in
f-structure, but were encoded in a more indirect way in the mapped dependencies
to match the CBS 500 annotation format.
To carry out the experiments, we POS-tagged the tokenized CBS 500 sentences with
the MXPOST tagger (Ratnaparkhi 1996) and parsed the tag sequences with our Penn-II
and Bikel retrained-based LFG system. We use the evaluation software of Carroll,
Briscoe, and Sanfilippo (1998)27 to evaluate the grammatical relations produced by each
parser. The results are given in Table 13.
Our LFG system based on Bikel?s retrained parser achieves an f-score of 80.23%,
whereas the hand-crafted RASP grammar and parser achieves an f-score of 76.57%.
Crouch et al (2002) report that their XLE system achieves an f-score of 76.1% for the
same experiment. A detailed breakdown by dependency is given in Table 14. The
LFG system based on Bikel?s retrained parser is able to better identify MOD(ifier) de-
pendency relations, ARG MOD (the relation between a head and a semantic argument
which is syntactically realized as a modifier, for example by-phrases), IOBJ (indirect
object) and AUXiliary relations. RASP is able to better identify XSUBJ (clausal subjects
controlled from without), CSUBJ (clausal subjects), and COMP (clausal complement)
relations. Again we use the Approximate Randomization Test to test the parsing results
for statistical significance. The p-value for the test comparing our system using Bikel?s
retrained parser against RASP is <.0001. The treebank-based LFG system using Bikel?s
retrained parser is significantly better than the hand-crafted, deep, unification grammar-
based RASP parsing system with a statistical significance of >95%.
6. Discussion and Related Work
At the moment, we can only speculate as to why our treebank-based LFG resources
outperform the hand-crafted XLE and RASP grammars.
In Section 4, we observed that the treebank-induced LFG resources have consid-
erably wider coverage (>99.9% measured in terms of complete spanning parse) than
27 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
111
Computational Linguistics Volume 34, Number 1
Table 14
Breakdown by grammatical relation for results of evaluation against CBS 500.
Dep. Percent of deps. F-score (%)
Bikel+Tags RASP
DEPENDANT 100.00 80.23 76.57
MOD 48.96 80.30 75.29
NCMOD 30.42 85.37 72.98
XMOD 1.60 70.05 55.88
CMOD 2.61 75.60 53.08
DETMOD 14.06 95.85 91.97
ARG MOD 0.51 80.00 64.52
ARG 43.70 78.28 77.57
SUBJ 13.10 79.84 83.57
NCSUBJ 12.99 87.84 84.32
XSUBJ 0.06 0.00 88.89
CSUBJ 0.04 0.00 22.22
SUBJ OR DOBJ 18.22 81.21 83.84
COMP 12.38 76.73 71.87
OBJ 7.34 76.05 69.53
DOBJ 5.11 84.55 84.57
OBJ2 0.25 48.00 43.84
IOBJ 1.98 59.04 47.60
CLAUSAL 5.04 77.74 75.37
XCOMP 4.03 80.00 84.11
CCOMP 1.01 69.61 75.14
AUX 4.76 94.94 88.27
CONJ 2.06 68.84 69.09
the hand-crafted grammars (?80% for XLE and RASP grammars on unseen treebank
text). Both XLE and RASP use a number of (largest) fragment-combining techniques
to achieve full coverage. If coverage is a significant component in the performance
difference observed between the hand-crafted and treebank-induced resources, then
it is reasonable to expect that the performance difference is more pronounced with
increasing sentence length (with shorter sentences being simpler and more likely to
be within the coverage of the hand-crafted grammars). In other words, we expect the
hand-crafted, deep, precision grammars to do better on shorter sentences (more likely to
be within their coverage), whereas the treebank-induced grammars should show better
performance on longer strings (less likely to be within the coverage of the hand-crafted
grammars).
In order to test this hypothesis, we carried out a number of experiments:
First, we plotted the sentence length distribution for the 560 PARC 700 test sen-
tences and the 500 CBS 500 sentences (Figures 17 and 18). Both gold standards are
approximately normally distributed, with the CBS 500 distribution possibly showing
the effects of being chosen subject to the constraint that the strings are parsable by the
parser in Carroll, Briscoe, and Sanfilippo (1998). For each case we use the mean, ?, and
two standard deviations, 2?, to the left and right of themean to exclude sentence lengths
not supported by sufficient observations: For PARC 700, ? = 23.27, ?? 2? = 2.17, and
?+ 2? = 44.36; for CBS 500, ? = 17.27, ?? 2? = 1.59, and ?+ 2? = 32.96. Both the
PARC 700 and the CB 500 distributions are positively skewed. For the PARC 700, ?? 2?
is actually outside the observed data range, whereas for CB 500, ?? 2? almost coincides
112
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 17
Distribution of sentence frequency by sentence length in the PARC 700 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
with the left border. It is therefore useful to further constrain the ?2? range by a
sentence count threshold of ? 5.28 This results in a sentence length range of 4?41 for
PARC 700 and 4?32 for CBS 500.
Second, in order to test whether fragment parses increase with sentence length, we
plotted the percentage of fragment parses over sentence length for the XLE parses of the
560-sentence test set of the PARC 700 (we did not do this for the CBS 500 as its strings
are selected to be fully parsable by RASP). Figure 19 shows that the number of fragment
parses tends to increase with sentence length.
Third, we plotted the average dependency f-score for the hand-crafted and the
treebank-induced resources against sentence lengths. Figure 20 shows the results for
PARC 700, Figure 21 for CBS 500.
In both cases, contrary to our (perhaps somewhat naive) assumption, the graphs
show that the treebank-induced resources outperform the hand-crafted resources
within (most of) the 4?41 and 4?32 sentence length bounds, with the results for the very
short and the very long strings outside those bounds not being supported by sufficient
data points.
In the parsing literature, results are often also provided for strings with lengths
?40. Below we give those results and statistical significance testing for the PARC 700
and CBS 500 (Tables 15 and 16). The results show that the Bikel retrained?based LFG
system achieves a higher dependency f-score on sentences of length ?40 than on all
sentences, whereas the XLE system achieves a slightly lower score on sentences of
length ?40. The Bikel-retrained system achieves an f-score of 83.18%, a statistically
28 Note that because the distributions in Figures 17 and 18 are Bezier interpolated, the constraint does not
guarantee that every sentence length within the range occurs five or more times.
113
Computational Linguistics Volume 34, Number 1
Figure 18
Distribution of sentence frequency by sentence length in the CB 500 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
Figure 19
Percentage of fragment sentences for XLE parsing system per sentence length with Bezier
interpolation.
significant improvement of 2.67 percentage points over the XLE system on sentences of
length?40. Against the CBS 500, Bikel?s retrained system achieves a weighted f-score of
82.58%, a statistically significant improvement of 3.87 percentage points over the RASP
system which achieves a weighted f-score of 78.81% on sentences of length ?40.
114
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 20
Average f-score by sentence length for PARC 700 test set with Bezier interpolation.
Figure 21
Average f-score by sentence length for CB 500 test set with Bezier interpolation.
Finally, we test whether the strict preds-only dependency evaluation has an ef-
fect on the results for the PARC 700 experiments. Recall that following Kaplan et al
(2004) for the PARC 700 evaluation we used a set of semantically relevant grammatical
functions that is a superset of preds-only and a subset of all-GFs. A preds-only based
evaluation is ?stricter? and tends to produce lower scores as it directly reflects the effects
115
Computational Linguistics Volume 34, Number 1
Table 15
Evaluation and significance testing of sentences length ?40 against the PARC 700.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 82.73 83.18
XLE 80.55 80.51
p-value .0054 .0010
Table 16
Evaluation and significance testing of sentences length ?40 against the CBS 500.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 80.23 82.58
RASP 76.57 78.81
p-value <.0001 <.0001
Table 17
Preds-only evaluation against the PARC 700 Dependency Bank.
All GFs Preds only
f-score f-score
Bikel + Tags 82.73 77.40
XLE 80.55 74.31
of predicate?argument/adjunct misattachments in the resulting dependency represen-
tations (while local functions such as NUM(ber), for example, can score properly even
if the local predicate is misattached). Table 1729 below gives the results for preds-only
evaluation30 on the PARC 700 for all sentence lengths. The results show that the Bikel-
retrained treebank-based LFG resource achieves an f-score of 77.40%, 5.33 percentage
points lower than the score for all the PARC dependencies. The XLE system achieves
an f-score of 74.31%, 6.24 percentage points lower than the score for all the PARC
dependencies and a 3.09 percentage point drop against the results obtained by the
Bikel-retrained treebank-based LFG resources. The performance of the Bikel retrained?
based LFG system suffers less than the XLE system when preds-only dependencies are
evaluated.
It is important to note that in our f-structure annotation algorithm and treebank-
based LFG parsing architectures, we do not claim to provide fully adequate statistical
models. It is well known (Abney 1997) that PCFG- or history-based parser approxima-
tions to general constraint-based grammars can yield inconsistent probability models
29 We do not include a p-value here as the breakdown by function per sentence was not available to us for
the XLE data.
30 The dependency relations we include in preds-only evaluation are: ADJUNCT, AQUANT, COMP, CONJ,
COORD FORM, DET FORM, FOCUS INT, MOD, NUMBER, OBJ, OBJ THETA, OBL, OBL AG, OBL COMPAR,
POSS, PRON INT, PRON REL, PRT FORM, QUANT, SUBJ, SUBORD FORM, TOPIC REL, XCOMP.
116
Cahill et al Statistical Parsing Using Automatic Dependency Structures
due to loss of probability mass: The parser successfully returns the highest ranked
parse tree but the constraint solver cannot resolve the f-structure equations and the
probability mass associated with that tree is lost. Research on adequate probability
models for constraint-based grammars is important (Bouma, van Noord, and Malouf
2000; Miyao and Tsujii 2002; Riezler et al 2002; Clark and Curran 2004). In this context,
it is interesting to compare parser performance against upper bounds. For the PARC
700 evaluation, the upper bound for the XLE-based resources is 85.65%, against 86.83%
for the treebank-based LFG resources. XLE-based parsing currently achieves 94.05%
(f-score 80.55%) of its upper bound using a discriminative disambiguation method,
whereas the treebank-based LFG resource achieves 95.28% (f-score 82.73%) of its upper
bound.
Although this seems to indicate that the two disambiguationmodels achieve similar
results, the figures are actually very difficult to compare. In the case of the XLE, the
upper bound is established by unpacking all parses for a string and scoring the best
match against the gold standard (rather than letting the probability model select a
parse). By contrast, in the case of the treebank-based LFG resources, we use the original
?perfect? Penn-II treebank trees (rather than the trees produced by the parser), auto-
matically annotate those trees with the f-structure annotation algorithm, and score the
results against the PARC 700 (it is not feasible to generate all parses for a string, there
are simply too many for treebank-induced resources). The upper bound computed in
this fashion for the treebank-based LFG resource (86.83%) is relatively low. The main
reason is that the automatic mapping required to relate the f-structures generated by the
treebank-based LFG resources to the PARC 700 dependencies is lossy. This is indicated
by comparing the upper bound for the treebank-based LFG resources for the PARC 700
against the upper bound for the DCU 105 gold standard, where little or no mapping
(apart from the feature-structure to dependency-triple conversion) is required: Scoring
the f-structure annotations for the original treebank trees results in 86.83% against PARC
700 versus 96.80% against DCU 105.
Our discussion shows how delicate it can be to compare parsing systems and
their disambiguation models. Ultimately what is required is an evaluation strategy that
separates out and clearly distinguishes between the grammar, parsing algorithm, and
disambiguation model and is capable of assessing different combinations of these core
components. Of course, this will not always be possible andmoving towards it is part of
a much more extended research agenda, well beyond the scope of the research reported
in the present article. Our approach, and previous approaches, evaluate systems at
the highest level of granularity, that of the complete package: the combined grammar-
parser-disambiguation model. The results show that machine-learning-based resources
can outperform deep, state-of-the-art hand-crafted resources with respect to the quality
of dependencies generated.
Treebank-based, deep and wide-coverage constraint-based grammar acquisition
has become an important research topic: Starting with the seminal TAG-based work
of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao,
Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present re-
search on inducing Penn-II treebank-based HPSGs with log-linear probability models.
Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced
CCG-basedmodels including LDD resolution. It would be interesting to conduct a com-
parative evaluation involving treebank-based HPSG, CCG, and LFG resources against
the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive compar-
ison of machine-learning-based with hand-crafted, deep, wide-coverage resources such
as those used in the XLE or RASP parsing systems.
117
Computational Linguistics Volume 34, Number 1
Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank
(Kingsbury, Palmer, andMarcus 2002)-based evaluations of their automatically induced
CCG and HPSG resources. PropBank-based evaluations provide valuable information
to rank parsing systems. Currently, however, PropBank-based evaluations are some-
what partial: They only represent (and hence score) verbal arguments and disregard a
raft of other semantically important dependencies (e.g., temporal and aspectual infor-
mation, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS
500 Dependency Banks.31
7. Conclusions
Parser comparison is a non-trivial and time-consuming exercise. We extensively eval-
uated four machine-learning-based shallow parsers and two hand-crafted, wide-
coverage deep probabilistic parsers involving four gold-standard dependency banks,
using the Approximate Randomization Test (Noreen 1989) to test for statistical signifi-
cance. We used a sophisticated method for automatically producing deep dependency
relations from Penn-II-style CFG-trees (Cahill et al 2002b, 2004) to compare shallow
parser output at the level of dependency relation and revisit experiments carried out by
Preiss (2003) and Kaplan et al (2004).
Our main findings are twofold:
1. Using our CFG-tree-to-dependency annotation technology, together with treebank- and
machine-learning-based parsers, we can outperform hand-crafted, wide-coverage, deep,
probabilistic, constraint-based grammars and parsers.
This result is surprising for two reasons. First, it is established against two externally-
provided dependency banks (the PARC 700 and the CBS 500 gold standards), originally
designed using the hand-crafted, wide-coverage, probabilistic grammars for the XLE
(Riezler et al 2002) and RASP (Carroll and Briscoe 2002) parsing systems to evaluate
those systems. What is more, the SUSANNE corpus-based CBS 500 constitutes an
instance of domain variation for the Penn-II-trained LFG resources, likely to adversely
affect scores. Second, the treebank- and machine-learning-based LFG resources require
automatic mapping to relate f-structure output of the treebank-based parsing systems
to the representation format in the PARC 700 and CBS 500 Dependency Banks. These
mappings are partial and lossy: That is, they do not cover all of the systematic dif-
ferences between f-structure and dependency bank representations and introduce a
certain amount of error in what they are designed to capture, that is they both over-
and undergeneralize, again adversely affecting scores. Improvements of the mappings
should lead to a further improvement in the dependency scores.
2. Parser evaluation at the level of dependency representation still requires non-trivial
mappings between different dependency representation formats.
31 In a sense, PropBank does not yet provide a single agreed upon gold standard: Role information is
provided indirectly and an evaluation gold-standard has to be computed from this. In doing so, choices
have to be made as regards the representation of shared arguments, the analysis of coordinate structures,
and so forth, and it is not clear that the same choices are currently made for evaluations carried out by
different research groups.
118
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Earlier we criticized tree-based parser evaluation on the grounds that equally valid
different tree-typologies can be associated with strings, and identified this as a major
obstacle to fair and unbiased parser evaluation. Dependency-based parser evaluation,
as it turns out, is not entirely free of this criticism either: There are significant systematic
differences between the PARC 700 dependency and the CBS 500 dependency represen-
tations; there are significant systematic differences between the LFG f-structures gener-
ated by the hand-crafted, wide-coverage grammars of Riezler et al (2002) and Kaplan
et al (2004) and those of the treebank-induced and f-structure annotation algorithm
based resources of Cahill et al (2004). These differences require careful implementation
of mappings if parsers are not to be unduly penalized for systematic and motivated
differences at the level of dependency representation. By and large, these differences
are, however, less pronounced than differences on CFG tree representations, making
dependency-based parser evaluation a worthwhile and rewarding exercise.
Summarizing our results, we find that against the DCU 105 development set, the
treebank- and f-structure annotation algorithm-based LFG system using Bikel?s parser
retrained to retain Penn-II functional tag labels performs best, achieving f-scores of
82.92% preds-only and 88.3% all grammatical functions. Against the automatically
generated WSJ Section 22 Dependency Bank, the system using Bikel?s retrained parser
achieves the highest results, achieving f-scores of 83.06% preds-only and 87.861% all
GFs. This is statistically significantly better than all other parsers. In order to evaluate
against the PARC 700 and CBS 500 gold standards, we automaticallymap the dependen-
cies produced by our treebank-based LFG system into a format compatible with the gold
standards. Against the PARC 700 Dependency Bank, the treebank-based LFG system
using Bikel?s retrained parser achieves an f-score of 82.73%, a statistically significant
improvement of 2.18% against the most up-to-date results of the hand-crafted XLE-
based parsing resources. Against the CBS 500, the treebank-based LFG system using
Bikel?s retrained parser achieved the highest f-score of 80.23%, a statistically significant
improvement of 3.66 percentage points on the highest previously published results
for the same experiment with the hand-crafted RASP resources in Carroll and Briscoe
(2002).
Appendix A. Parser Parameter Settings
This section provides a complete list of the parameter settings used for each of the
parsers described in this article.
Parser Parameters
Collins Model 3 We used the Collins parser with its default settings of a
beam size of 10,000 and where the values of the following
flags are set to 1: punctuation-flag, distaflag, distvflag,
and npbflag. Input was pre-tagged using the MXPOST
POS tagger (Ratnaparkhi 1996). We parse the input file
with all three models and use the scripts provided to
merge the outputs into the final parser output file. Note
that this file has been cleaned of all -A functional tags and
trace nodes.
Charniak We used the parser dated August 2005 and ran the
parser using the data provided in the download on pre-
tokenized sentences of length ?200. Input was automati-
cally tagged by the parser.
119
Computational Linguistics Volume 34, Number 1
Bikel Emulation of We used version 0.9.9b of the parser trained on the file
Collins Model 2 of observed events made available on the downloads
page. We used the collins.properties file and a maximum
heap size of 1,500 MB. Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996).
Bikel + Functional Tags We used version 0.9.9b of the parser trained on a version
of Sections 02?21 of the Penn-II treebank where functional
tags were not ignored by the parser. We updated the de-
fault head-finding rules to deal with the new categories.
We also trained on all sentences from the training set
(the default collins.properties file is set to ignore trees of
more than 500 tokens). Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996) and the maxi-
mum heap size was 1,500 MB.
RASP Weused a file of parser output provided through personal
communication with John Carroll. (Tagging is carried out
automatically by the parser.)
XLE We used a file of parser output provided by the Natural
Language Theory and Technology group at the Palo Alto
Research Center. (Tagging is carried out automatically by
the parser.)
Acknowledgments
We are grateful to our anonymous reviewers
whose insightful comments have improved
the article significantly. We would like to
thank John Carroll for discussion and help
with reproducing the RASP parsing results;
Michael Collins, Eugene Charniak, Dan
Bikel, and Helmut Schmid for making their
parsing engines available; and Ron Kaplan
and the team at PARC for discussion,
feedback, and support. Part of the research
presented here has been supported by
Science Foundation Ireland grants
04/BR/CS0370 and 04/IN/I527, Enterprise
Ireland Basic Research Grant SC/2001/0186,
an Irish Research Council for Science,
Engineering and Technology Ph.D.
studentship, an IBM Ph.D. studentship and
support from IBM?s Centre for Advanced
Studies (CAS) in Dublin.
References
Abney, Stephen. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Alshawi, Hiyan and Stephen Pulman, 1992.
Ellipsis, Comparatives, and Generation,
chapter 13. The MIT Press, Cambridge,
MA.
Baldwin, Timothy, Emily Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen.
2004. Road-testing the English Resource
Grammar over the British National
Corpus. In Proceedings of the Fourth
International Conference on Language
Resources and Evaluation (LREC 2004),
pages 2047?2050, Lisbon, Portugal.
Bikel, Dan. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of HLT 2002,
pages 24?27, San Diego, CA.
Black, Ezra, Steven Abney, Dan Flickenger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Fred Jelineck, Judith Klavans, Mark
Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic
coverage of english grammars. In
Proceedings of the Speech and Natural
Language Workshop, pages 306?311, Pacific
Grove, CA.
Bod, Rens. 2003. An efficient implementation
of a new DOP model. In Proceedings of the
Tenth Conference of the European Chapter of
the Association for Computational Linguistics
(EACL?03), pages 19?26, Budapest,
Hungary.
Bouma, Gosse, Gertjan van Noord, and
Robert Malouf. 2000. Alpino:
Wide-coverage computational analysis of
dutch. In Walter Daelemans, Khalil
Sima?an, Jorn Veenstra, and Jakub Zavrel,
editors, Computational Linguistics in The
Netherlands 2000. Rodopi, Amsterdam,
pages 45?59.
120
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford, England.
Briscoe, Edward and John Carroll. 1993.
Generalized probabilistic LR parsing of
natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1):25?59.
Briscoe, Ted and John Carroll. 2006.
Evaluating the accuracy of an
unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the
COLING/ACL 2006 Main Conference Poster
Sessions, pages 41?48, Sydney, Australia.
Briscoe, Edward, Claire Grover, Bran
Boguraev, and John Carroll. 1987. A
formalism and environment for the
development of a large grammar of
English. In Proceedings of the 10th
International Joint Conference on AI,
pages 703?708, Milan, Italy.
Burke, Michael. 2006. Automatic Treebank
Annotation for the Acquisition of LFG
Resources. Ph.D. thesis, School of
Computing, Dublin City University,
Dublin, Ireland.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Evaluation of an automatic
annotation algorithm against the PARC
700 Dependency Bank. In Proceedings of the
Ninth International Conference on LFG,
pages 101?121, Christchurch, New Zealand.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The Parallel Grammar
Project. In Proceedings of COLING 2002,
Workshop on Grammar Engineering and
Evaluation, pages 1?7, Taipei, Taiwan.
Cahill, Aoife. 2004. Parsing with Automatically
Acquired, Wide-Coverage, Robust,
Probabilistic LFG Approximations. Ph.D.
thesis, School of Computing, Dublin City
University, Dublin, Ireland.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 320?327,
Barcelona, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002a.
Automatic annotation of the Penn
Treebank with LFG f-structure
information. In Proceedings of the LREC
Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping
Annotated Language Data, pages 8?15, Las
Palmas, Canary Islands, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002b. Parsing
with PCFGs and automatic f-structure
annotation. In Proceedings of the Seventh
International Conference on LFG,
pages 76?95, Palo Alto, CA.
Carroll, John and Edward Briscoe. 2002.
High precision extraction of grammatical
relations. In Proceedings of the 19th
International Conference on Computational
Linguistics (COLING), pages 134?140,
Taipei, Taiwan.
Carroll, John, Edward Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and new proposal. In Proceedings of
the International Conference on Language
Resources and Evaluation, pages 447?454,
Granada, Spain.
Carroll, John, Anette Frank, Dekang Lin,
Detlef Prescher, and Hans Uszkoreit,
editors. 2002. HLT Workhop: ?Beyond
PARSEVAL ? Towards improved evaluation
measures for parsing systems?, Las Palmas,
Canary Islands, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In Proceedings of the
Thirteenth National Conference on
Artificial Intelligence, pages 1031?1036,
Menlo Park, CA.
Charniak, Eugene. 2000. A maximum
entropy inspired parser. In Proceedings
of the First Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2000),
pages 132?139, Seattle, WA.
Chinchor, Nancy, Lynette Hirschman, and
David D. Lewis. 1993. Evaluating message
understanding systems: An analysis of the
Third Message Understanding Conference
(MUC-3). Computational Linguistics,
19(3):409?449.
Clark, Stephen and James Curran. 2004.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL-04), pages 104?111,
Barcelona, Spain.
Clark, Stephen and James Curran. 2007.
Formalism-independent parser evaluation
with CCG and DepBank. In Proceedings of
the 45th Annual Meeting of the Association
for Computational Linguistics (ACL 2007),
pages 248?255, Prague, Czech Republic
http://www.aclweb-org/anthology/P/
P07/P07-1032.
Clark, Stephen and Julia Hockenmaier. 2002.
Evaluating a wide-coverage CCG parser.
121
Computational Linguistics Volume 34, Number 1
In Proceedings of the LREC 2002 Beyond
Parseval Workshop, pages 60?66, Las
Palmas, Canary Islands, Spain.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid, Spain.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia, PA.
Crouch, Richard, Ron Kaplan, Tracy Holloway
King, and Stefan Riezler. 2002. A
comparison of evaluation metrics for a
broad coverage parser. In Proceedings of
the LREC Workshop: Beyond PARSEVAL?
Towards Improved Evaluation Measures for
Parsing Systems, pages 67?74, Las Palmas,
Canary Islands, Spain.
Dalrymple, Mary. 2001. Lexical-Functional
Grammar. London, Academic Press.
Dienes, Pe?ter and Amit Dubey. 2003.
Antecedent recovery: Experiments with a
trace tagger. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, pages 33?40, Sapporo,
Japan.
Eisele, Andreas and Jochen Do?rre. 1986. A
lexical functional grammar system in
Prolog. In Proceedings of the 11th International
Conference on Computational Linguistics
(COLING 1986), pages 551?553, Bonn.
Flickinger, Dan. 2000. On building a more
efficient grammar by exploiting types.
Natural Language Engineering, 6(1):15?28.
Gaizauskas, Rob. 1995. Investigations into
the grammar underlying the Penn
Treebank II. Research Memorandum
CS-95-25, Department of Computer
Science, Univeristy of Sheffield, UK.
Gildea, Daniel and Julia Hockenmaier.
2003. Identifying semantic roles using
combinatory categorial grammar.
In Proceedings of the 2003 Conference
on Empirical Methods in Natural
Language Processing, pages 57?64,
Sapporo, Japan.
Hockenmaier, Julia. 2003. Parsing with
generative models of predicate?argument
structure. In Proceedings of the 41st Annual
Conference of the Association for
Computational Linguistics, pages 359?366,
Sapporo, Japan.
Hockenmaier, Julia and Mark Steedman.
2002. Generative models for statistical
parsing with combinatory categorial
grammar. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 335?342,
Philadelphia, PA.
Johnson, Mark. 1999. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for
recovering empty nodes and their
antecedents. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 136?143,
Philadelphia, PA.
Kaplan, Ron and Joan Bresnan. 1982.
Lexical functional grammar, a formal
system for grammatical representation.
In Joan Bresnan, editor, The Mental
Representation of Grammatical Relations.
MIT Press, Cambridge, MA,
pages 173?281.
Kaplan, Ron, Stefan Riezler, Tracy Holloway
King, John T. Maxwell, Alexander
Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the
Human Language Technology Conference and
the 4th Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL?04),
pages 97?104, Boston, MA.
Kaplan, Ronald and Annie Zaenen. 1989.
Long-distance dependencies, constituent
structure and functional uncertainty. In
Mark Baltin and Anthony Kroch, editors,
Alternative Conceptions of Phrase Structure,
pages 17?42, University of Chicago Press,
Chicago.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and Ron
Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th
International Workshop on Linguistically
Interpreted Corpora (LINC-03), pages 1?8,
Budapest, Hungary.
Kingsbury, Paul, Martha Palmer, and
Mitch Marcus. 2002. Adding semantic
annotation to the Penn TreeBank. In
Proceedings of the Human Language
Technology Conference, pages 252?256,
San Diego, CA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Leech, Geoffrey and Roger Garside. 1991.
Running a grammar factory: On the
122
Cahill et al Statistical Parsing Using Automatic Dependency Structures
compilation of parsed corpora, or
?treebanks?. In Stig Johansson and
Anna-Brita Stenstro?m, editors, English
Computer Corpora: Selected Papers. Mouton
de Gruyter, Berlin, pages 15?32.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In Proceedings
of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL 2004),
pages 328?335, Barcelona, Spain.
Lin, Dekang. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of the International
Joint Conference on AI, pages 1420?1427,
Montre?al, Canada.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Department of Computer
Science, Stanford University, CA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings
of the ARPA Workshop on Human
Language Technology, pages 110?115,
Princeton, NJ.
McCarthy, Maire?ad. 2003. Design and
Evaluation of the Linguistic Basis of an
Automatic F-Structure Annotation Algorithm
for the Penn-II Treebank. Master?s thesis,
School of Computing, Dublin City
University, Dublin, Ireland.
McDonald, Ryan and Fernando Pereira. 2006.
Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 81?88,
Trento, Italy.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2003. Probabilistic modeling
of argument structures including non-local
dependencies. In Proceedings of the
Conference on Recent Advances in Natural
Language Processing (RANLP),
pages 285?291, Borovets, Bulgaria.
Miyao, Yusuke and Jun?ichi Tsujii. 2002.
Maximum entropy estimation for feature
forests. In Proceedings of Human Language
Technology Conference (HLT 2002),
pages 292?297, San Diego, CA.
Miyao, Yusuke and Jun?ichi Tsujii.
2004. Deep linguistic analysis
for the accurate identification of
predicate?argument relations. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2004), pages 1392?1397,
Geneva, Switzerland.
Noreen, Eric W. 1989. Computer Intensive
Methods for Testing Hypotheses: An
Introduction. Wiley, New York.
O?Donovan, Ruth, Michael Burke, Aoife
Cahill, Josef van Genabith, and Andy
Way. 2004. Large-scale induction and
evaluation of lexical resources from the
Penn-II treebank. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 368?375,
Barcelona, Spain.
Pollard, Carl and Ivan Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI
Publications, Stanford, CA.
Preiss, Judita. 2003. Using grammatical
relations to compare parsers. In Proceedings
of the Tenth Conference of the European
Chapter of the Association for Computational
Linguistics (EACL?03), pages 291?298,
Budapest, Hungary.
Ratnaparkhi, Adwait. 1996. A maximum
entropy part-of-speech tagger. In
Proceedings of the Empirical Methods in
Natural Language Processing Conference,
pages 133?142, Philadelphia, PA.
Riezler, Stefan, Tracy King, Ronald Kaplan,
Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing
theWall Street Journal using a
lexical-functional grammar and
discriminative estimation techniques.
In Proceedings of the 40th Annual
Conference of the Association for
Computational Linguistics (ACL-02),
pages 271?278, Philadelphia, PA.
Sampson, Geoffrey. 1995. English for the
Computer: The SUSANNE Corpus and
Analytic Scheme. Clarendon Press,
Oxford, England.
Tsuruoka, Yoshimasa, Yusuke Miyao,
and Jun?ichi Tsujii. 2004. Towards
efficient probabilistic HPSG parsing:
Integrating semantic and syntactic
preference to guide the parsing.
In Proceedings of IJCNLP-04 Workshop:
Beyond shallow analyses?Formalisms
and statistical modeling for deep
analyses, Hainan Island, China. [No
page numbers].
van Genabith, Josef and Richard Crouch.
1996. Direct and underspecified
interpretations of LFG f-structures. In 16th
International Conference on Computational
Linguistics (COLING 96), pages 262?267,
Copenhagen, Denmark.
123
Computational Linguistics Volume 34, Number 1
van Genabith, Josef and Richard
Crouch. 1997. On interpreting
f-structures as UDRSs. In Proceedings
of ACL-EACL-97, pages 402?409,
Madrid, Spain.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora. In
Proceedings of the 5th Natural Language
Processing Pacific Rim Symposium
(NLPRS-99), pages 398?403, Beijing,
China.
Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and
Martha Palmer. 2004. The Penn Chinese
treebank: Phrase structure annotation of a
large corpus. Natural Language Engineering,
10(4):1?30.
Yeh, Alexander. 2000. More accurate tests
for the statistical significance of result
differences. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING, 2000), pages 947?953,
Saarbru?cken, Germany.
124
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 286?294,
Beijing, August 2010
A Discriminative Latent Variable-Based ?DE? Classifier
for Chinese?English SMT
Jinhua Du and Andy Way
CNGL, School of Computing
Dublin City University
{jdu, away}@computing.dcu.ie
Abstract
Syntactic reordering on the source-side
is an effective way of handling word or-
der differences. The { (DE) construc-
tion is a flexible and ubiquitous syntac-
tic structure in Chinese which is a ma-
jor source of error in translation quality.
In this paper, we propose a new classi-
fier model ? discriminative latent vari-
able model (DPLVM) ? to classify the
DE construction to improve the accuracy
of the classification and hence the transla-
tion quality. We also propose a new fea-
ture which can automatically learn the re-
ordering rules to a certain extent. The ex-
perimental results show that the MT sys-
tems using the data reordered by our pro-
posed model outperform the baseline sys-
tems by 6.42% and 3.08% relative points
in terms of the BLEU score on PB-SMT
and hierarchical phrase-based MT respec-
tively. In addition, we analyse the impact
of DE annotation on word alignment and
on the SMT phrase table.
1 Introduction
Syntactic structure-based reordering has been
shown to be significantly helpful for handling
word order issues in phrase-based machine trans-
lation (PB-SMT) (Xia and McCord, 2004; Collins
et al, 2005; Wang et al, 2007; Li et al, 2007;
Elming, 2008; Chang et al, 2009). It is well-
known that in MT, it is difficult to translate be-
tween Chinese?English because of the different
word orders (cf. the different orderings of head
nouns and relative clauses). Wang et al (2007)
pointed out that Chinese differs from English in
several important respects, such as relative clauses
appearing before the noun being modified, prepo-
sitional phrases often appearing before the head
they modify, etc. Chang et al (2009) argued
that many of the structural differences are re-
lated to the ubiquitous Chinese structural parti-
cle phrase { (DE) construction, used for a wide
range of noun modification constructions (both
single word and clausal) and other uses. They
pointed out that DE is a major source of word
order error when a Chinese sentence is translated
into English due to the different ways that the DE
construction can be translated.
In this paper, we focus on improving the clas-
sification accuracy of DE constructions in Chi-
nese as well as investigating its impact on trans-
lation quality. From the grammatical perspective,
the {(DE) in Chinese represents the meaning of
?noun modification? which generally is shown in
the form of a Noun phrase (NP) [A DE B]. A in-
cludes all the words in the NP before DE and B
contains all the words in the NP after DE. Wang
et al (2007) first introduced a reordering of the
DE construction based on a set of rules which
were generated manually and achieved significant
improvements in translation quality. Chang et
al. (2009) extended this work by classifying DE
into 5 finer-grained categories using a log-linear
classifier with rich features in order to achieve
higher accuracy both in reordering and in lexical
choice. Their experiments showed that a higher
286
accuracy of the DE classification improved the ac-
curacy of reordering component, and further indi-
rectly improved the translation quality in terms of
BLEU (Papineni et al, 2002) scores.
We regard the DE classification as a labeling
task, and hence propose a new model to label the
DE construction using a discriminative latent vari-
able algorithm (DPLVM) (Morency et al, 2007;
Sun and Tsujii, 2009), which uses latent vari-
ables to carry additional information that may not
be expressed by those original labels and capture
more complicated dependencies between DE and
its corresponding features. We also propose a new
feature defined as ?tree-pattern? which can auto-
matically learn the reordering rules rather than us-
ing manually generated ones.
The remainder of this paper is organised as fol-
lows. In section 2, we introduce the types of
word order errors caused by the DE construc-
tion. Section 3 describes the closely related work
on DE construction. In section 4, we detail our
proposed DPLVM algorithm and its adaptation to
our task. We also describe the feature templates
as well as the proposed new feature used in our
model. In section 5, the classification experiments
are conducted to compare the proposed classifica-
tion model with a log-linear model. Section 6 re-
ports comparative experiments conducted on the
NIST 2008 data set using two sets of reordered
and non-reordered data. Meanwhile, in section 7,
an analysis on how the syntactic DE reordering
affects word alignment and phrase table is given.
Section 8 concludes and gives avenues for future
work.
2 The Problem of Chinese DE
Construction Translation
Although syntactic reordering is an effective
way of significantly improving translation quality,
word order is still a major error source between
Chinese and English translation. Take examples
in Figure 1 as an illustration. The errors of three
translation results in Figure 1 are from different
MT systems, and many errors relate to incorrect
reordering for the{ (DE) structure.
These three translations are from different Hi-
ero systems. Although Hiero has an inherent re-
ordering capability, none of them correctly re-
Source: h?(local) ?(a) ?XColing 2010: Poster Volume, pages 374?382,
Beijing, August 2010
Integrating N-best SMT Outputs into a TM System
Yifan He Yanjun Ma Andy Way Josef van Genabith
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yhe,yma,away,josef}@computing.dcu.ie
Abstract
In this paper, we propose a novel frame-
work to enrich Translation Memory (TM)
systems with Statistical Machine Trans-
lation (SMT) outputs using ranking. In
order to offer the human translators mul-
tiple choices, instead of only using the
top SMT output and top TM hit, we
merge the N-best output from the SMT
system and the k-best hits with highest
fuzzy match scores from the TM sys-
tem. The merged list is then ranked ac-
cording to the prospective post-editing ef-
fort and provided to the translators to aid
their work. Experiments show that our
ranked output achieve 0.8747 precision at
top 1 and 0.8134 precision at top 5. Our
framework facilitates a tight integration
between SMT and TM, where full advan-
tage is taken of TM while high quality
SMT output is availed of to improve the
productivity of human translators.
1 Introduction
Translation Memories (TM) are databases that
store translated segments. They are often used to
assist translators and post-editors in a Computer
Assisted Translation (CAT) environment by re-
turning the most similar translated segments. Pro-
fessional post-editors and translators have long
been relying on TMs to avoid duplication of work
in translation.
With the rapid development in statistical ma-
chine translation (SMT), MT systems are begin-
ning to generate acceptable translations, espe-
cially in domains where abundant parallel corpora
exist. It is thus natural to ask if these translations
can be utilized in some way to enhance TMs.
However advances in MT are being adopted
only slowly and sometimes somewhat reluctantly
in professional localization and post-editing envi-
ronments because of 1) the usefulness of the TM,
2) the investment and effort the company has put
into TMs, and 3) the lack of robust SMT confi-
dence estimation measures which are as reliable
as fuzzy match scores (cf. Section 4.1.2) used in
TMs. Currently the localization industry relies on
TM fuzzy match scores to obtain both a good ap-
proximation of post-editing effort and an estima-
tion of the overall translation cost.
In a forthcoming paper, we propose a trans-
lation recommendation model to better integrate
MT outputs into a TM system. Using a binary
classifier, we only recommend an MT output to
the TM-user when the classifier is highly confi-
dent that it is better than the TM output. In this
framework, post-editors continue to work with the
TM while benefiting from (better) SMT outputs;
the assets in TMs are not wasted and TM fuzzy
match scores can still be used to estimate (the up-
per bound of) post-editing labor.
In the previous work, the binary predictor
works on the 1-best output of the MT and TM sys-
tems, presenting either the one or the other to the
post-editor. In this paper, we develop the idea fur-
ther by moving from binary prediction to ranking.
We use a ranking model to merge the k-best lists
of the two systems, and produce a ranked merged
374
list for post-editing. As the list is an enriched ver-
sion of the TM?s k-best list, the TM related assets
are better preserved and the cost estimation is still
valid as an upper bound.
More specifically, we recast SMT-TM integra-
tion as a ranking problem, where we apply the
Ranking SVM technique to produce a ranked list
of translations combining the k-best lists of both
the MT and the TM systems. We use features in-
dependent of the MT and TM systems for rank-
ing, so that outputs from MT and TM can have
the same set of features. Ideally the transla-
tions should be ranked by their associated post-
editing efforts, but given the very limited amounts
of human annotated data, we use an automatic
MT evaluation metric, TER (Snover et al, 2006),
which is specifically designed to simulate post-
editing effort to train and test our ranking model.
The rest of the paper is organized as follows:
we first briefly introduce related research in Sec-
tion 2, and review Ranking SVMs in Section 3.
The formulation of the problem and experiments
with the ranking models are presented in Sections
4 and 5. We analyze the post-editing effort ap-
proximated by the TER metric in Section 6. Sec-
tion 7 concludes and points out avenues for future
research.
2 Related Work
There has been some work to help TM users to
apply MT outputs more smoothly. One strand is
to improve the MT confidence measures to bet-
ter predict post-editing effort in order to obtain a
quality estimation that has the potential to replace
the fuzzy match score in the TM. To the best of
our knowledge, the first paper in this area is (Spe-
cia et al, 2009a), which uses regression on both
the automatic scores and scores assigned by post-
editors. The method is improved in (Specia et
al., 2009b), which applies Inductive Confidence
Machines and a larger set of features to model
post-editors? judgment of the translation quality
between ?good? and ?bad?, or among three levels
of post-editing effort.
Another strand is to integrate high confidence
MT outputs into the TM, so that the ?good? TM
entries will remain untouched. In our forthcoming
paper, we recommend SMT outputs to a TM user
when a binary classifier predicts that SMT outputs
are more suitable for post-editing for a particular
sentence.
The research presented here continues the line
of research in the second strand. The difference
is that we do not limit ourselves to the 1-best out-
put but try to produce a k-best output in a rank-
ing model. The ranking scheme also enables us
to show all TM hits to the user, and thus further
protects the TM assets.
There has also been work to improve SMT us-
ing the knowledge from the TM. In (Simard and
Isabelle, 2009), the SMT system can produce a
better translation when there is an exact or close
match in the corresponding TM. They use regres-
sion Support Vector Machines to model the qual-
ity of the TM segments. This is also related to
our work in spirit, but our work is in the opposite
direction, i.e. using SMT to enrich TM.
Moreover, our ranking model is related to
reranking (Shen et al, 2004) in SMT as well.
However, our method does not focus on produc-
ing better 1-best translation output for an SMT
system, but on improving the overall quality of the
k-best list that TM systems present to post-editors.
Some features in our work are also different in na-
ture to those used in MT reranking. For instance
we cannot use N-best posterior scores as they do
not make sense for the TM outputs.
3 The Support Vector Machines
3.1 The SVM Classifier
Classical SVMs (Cortes and Vapnik, 1995) are
binary classifiers that classify an input instance
based on decision rules which minimize the reg-
ularized error function in (Eq. 1):
min
w,b,?
1
2w
Tw + C
l?
i=1
?i
subject to: yi(wT xi + b) > 1 ? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {1,?1} are l training in-
stances. w is the weight vector, ? is the relaxation
variable and C > 0 is the penalty parameter.
3.2 Ranking SVM for SMT-TM Integration
The SVM classification algorithm is extended to
the ranking case in (Joachims, 2002). For a cer-
375
tain group of instances, the Ranking SVM aims
at producing a ranking r that has the maximum
Kendall?s ? coefficient with the the gold standard
ranking r?.
Kendall?s ? measures the relevance of two rank-
ings: ?(ra, rb) = P?QP+Q , where P and Q arethe amount of concordant and discordant pairs in
ra and rb. In practice, this is done by building
constraints to minimize the discordant pairs Q.
Following the basic idea, we show how Ranking
SVM can be applied to MT-TM integration as fol-
lows.
Assume that for each source sentence s, we
have a set of outputs from MT, M and a set of
outputs from TM, T. If we have a ranking r(s)
over translation outputs M?T where for each
translation output d ? M?T, (di, dj) ? r(s) iff
di <r(s) dj , we can rewrite the ranking constraints
as optimization constraints in an SVM, as in Eq.
(2).
min
w,b,?
1
2w
Tw + C? ?
subject to:
?(di, dj) ? r(s1) : w(?(s1, di)? ?(s1, dj)) > 1 ? ?i,j,1
...
?(di, dj) ? r(sn) : w(?(sn, di)? ?(sn, dj)) > 1? ?i,j,n
?i,j,k > 0
(2)
where ?(sn, di) is a feature vector of translation
output di given source sentence sn. The Ranking
SVM minimizes the discordant number of rank-
ings with the gold standard according to Kendall?s
? .
When the instances are not linearly separable,
we use a mapping function ? to map the features
xi (?(sn, di) in the case of ranking) to high di-
mensional space, and solve the SVMwith a kernel
function K in where K(xi, xj) = ?(xi)T?(xj).
We perform our experiments with the Radial
Basis Function (RBF) kernel, as in Eq. (3).
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (3)
4 The Ranking-based Integration Model
In this section we present the Ranking-based
SMT-TM integration model in detail. We first in-
troduce the k-best lists in MT (called N-best list)
and TM systems (called m-best list in this section)
and then move on to the problem formulation and
the feature set.
4.1 K-Best Lists in SMT and TM
4.1.1 The SMT N-best List
The N-best list of the SMT system is generated
during decoding according to the internal feature
scores. The features include language and transla-
tion model probabilities, reordering model scores
and a word penalty.
4.1.2 The TM M-Best List and the Fuzzy
Match Score
The m-best list of the TM system is gener-
ated in descending fuzzy match score. The fuzzy
match score (Sikes, 2007) uses the similarity of
the source sentences to predict a level to which a
translation is reusable or editable.
The calculation of fuzzy match scores is one of
the core technologies in TM systems and varies
among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Lev-
enshtein, 1966) between the source and TM en-
try, normalized by the length of the source as in
Eq. (4), as most of the current implementations
are based on edit distance while allowing some
additional flexible matching.
FuzzyMatch(t) = min
e
EditDistance(s, e)
Len(s) (4)
where s is the source side of the TM hit t, and e
is the source side of an entry in the TM.
4.2 Problem Formulation
Ranking lists is a well-researched problem in
the information retrieval community, and Ranking
SVMs (Joachims, 2002), which optimizes on the
ranking correlation ? have already been applied
successfully in machine translation evaluation (Ye
et al, 2007). We apply the same method here to
rerank a merged list of MT and TM outputs.
Formally given an MT-produced N-best list
M = {m1,m2, ...,mn}, a TM-produced m-best
list T = {t1, t2, ..., tm} for a input sentence s,
we define the gold standard using the TER met-
ric (Snover et al, 2006): for each d ? M?T,
(di, dj) ? r(s) iff TER(di) < TER(dj). We
train and test a Ranking SVM using cross vali-
dation on a data set created according to this cri-
terion. Ideally the gold standard would be cre-
ated by human annotators. We choose to use TER
376
as large-scale annotation is not yet available for
this task. Furthermore, TER has a high correla-
tion with the HTER score (Snover et al, 2006),
which is the TER score using the post-edited MT
output as a reference, and is used as an estimation
of post-editing effort.
4.3 The Feature Set
When building features for the Ranking SVM, we
are limited to features that are independent of the
MT and TM system. We experiment with system-
independent fluency and fidelity features below,
which capture translation fluency and adequacy,
respectively.
4.3.1 Fluency Features
Source-side Language Model Scores. We
compute the LM probability and perplexity of the
input source sentence on a language model trained
on the source-side training data of the SMT sys-
tem, which is also the TM database. The inputs
that have lower perplexity on this language model
are more similar to the data set on which the SMT
system is built.
Target-side LanguageModel Scores. We com-
pute the LM probability and perplexity as a mea-
sure of the fluency of the translation.
4.3.2 Fidelity Features
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM performs well
enough, these two sentences should be the same
or very similar. Therefore the fuzzy match score
here gives an estimation of the confidence level of
the output.
The IBMModel 1 Score. We compute the IBM
Model 1 score in both directions to measure the
correspondence between the source and target, as
it serves as a rough estimation of how good a
translation it is on the word level.
5 Experiments
5.1 Experimental Settings
5.1.1 Data
Our raw data set is an English?French trans-
lation memory with technical translation from a
multi-national IT security company, consisting of
51K sentence pairs. We randomly select 43K to
train an SMT system and translate the English side
of the remaining 8K sentence pairs, which is used
to run cross validation. Note that the 8K sentence
pairs are from the same TM, so that we are able to
create a gold standard by ranking the TER scores
of the MT and TM outputs.
Duplicated sentences are removed from the
data set, as those will lead to an exact match in
the TM system and will not be translated by trans-
lators. The average sentence length of the training
set is 13.5 words and the size of the training set
is comparable to the (larger) translation memories
used in the industry.
5.1.2 SMT and TM systems
We use a standard log-linear PB-SMT
model (Och and Ney, 2002): GIZA++ imple-
mentation of IBM word alignment model 4, the
phrase-extraction heuristics described in (Koehn
et al, 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing trained with SRILM (Stolcke, 2002)
on the English side of the training data, and
Moses (Koehn et al, 2007) to decode. We train a
system in the opposite direction using the same
data to produce the pseudo-source sentences.
We merge distinct 5-best lists from MT and TM
systems to produce a new ranking. To create the
distinct list for the SMT system, we search over
a 100-best list and keep the top-5 distinct out-
puts. Our data set consists of mainly short sen-
tences, leading to many duplications in the N-best
output of the SMT decoder. In such cases, top-
5 distinct outputs are good representations of the
SMT?s output.
5.2 Training, Tuning and Testing the
Ranking SVM
We run training and prediction of the Ranking
SVM in 4-fold cross validation. We use the
377
SVMlight1 toolkit to perform training and testing.
When using the Ranking SVM with the RBF
kernel, we have two free parameters to tune on:
the cost parameter C in Eq. (1) and the radius
parameter ? in Eq. (3). We optimize C and
? using a brute-force grid search before running
cross-validation and maximize precision at top-5,
with an inner 3-fold cross validation on the (outer)
Fold-1 training set. We search within the range
[2?6, 29], the step size is 2 on the exponent.
5.3 The Gold Standard
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























  
  
  0%
  20%
  40%
  60%
  80%
  100%
Top1 Top3 Top5
Go
ld S
tan
dar
d %
TM
MT
Figure 1: MT and TM?s percentage in gold stan-
dard
Figure 1 shows the composition of translations
in the gold standard. Each source sentence is asso-
ciated with a list of translations from two sources,
i.e. MT output and TM matches. This list of
translations is ranked from best to worst accord-
ing TER scores. The figure shows that over 80%
of the translations are from the MT system if we
only consider the top-1 translation. As the num-
ber of top translations we consider increases, more
TM matches can be seen. On the one hand, this
does show a large gap in quality between MT out-
put and TM matches; on the other hand, however,
it also reveals that we will have to ensure two ob-
jectives in ranking: the first is to rank the 80%
MT translations higher and the second is to keep
the 20% ?good? TM hits in the Top-5. We design
our evaluation metrics accordingly.
5.4 Evaluation Metrics
The aim of this research is to provide post-editors
with translations that in many cases are easier to
1http://svmlight.joachims.org/
edit than the original TM output. As we formulate
this as a ranking problem, it is natural to measure
the quality of the ranking output by the number
of better translations that are ranked high. Some-
times the top TM output is the easiest to edit; in
such a case we need to ensure that this translation
has a high rank, otherwise the system performance
will degrade.
Based on this observation, we introduce the
idea of relevant translations, and our evaluation
metrics: PREC@k and HIT@k.
Relevant Translations. We borrow the idea
of relevence from the IR community to define
the idea of translations worth ranking high. For
a source sentence s which has a top TM hit t,
we define an MT/TM output m as relevant, if
TER(m) ? TER(t). According to the defini-
tion, relevant translations should need no more
post-edits than the original top hit from the TM
system. Clearly the top TM hit is always relevant.
PREC@k. We calculate the precision
(PREC@k) of the ranking for evaluation. As-
suming that there are n relevant translations in
the top k list for a source sentence s, we have
PREC@k= n/k for s. We test PREC@k, for
k = 1...10, in order to evaluate the overall quality
of the ranking.
HIT@k. We also estimate the probability of
having one of the relevant translations in the top
k, denoted as HIT@k. For a source sentence s,
HIT@k equals to 1 if there is at least one relevant
translation in top k, and 0 otherwise. This mea-
sures the quality of the best translation in top k,
which is the translation the post-editor will find
and work on if she reads till the kth place in the
list. HIT@k equals to 1.0 at the end of the list.
We report the mean PREC@k and HIT@k for
all s with the 0.95 confidence interval.
5.5 Experimental Results
In Table 1 we report PREC@k and HIT@k
for k = 1..10. The ranking receives 0.8747
PREC@1, which means that most of the top
ranked translations have at least the same quality
as the top TM output. We notice that precision re-
mains above 0.8 till k = 5, leading us to conclude
that most of the relevant translations are ranked in
the top-5 positions in the list.
378
Table 1: PREC@k and HIT@k of Ranking
PREC % HIT %
k=1 87.47?1.60 87.47?1.60
k=2 85.42?1.07 93.36?0.53
k=3 84.13?0.94 95.74?0.61
k=4 82.79?0.57 97.08?0.26
k=5 81.34?0.51 98.04?0.23
k=6 79.26?0.59 99.41?0.25
k=7 74.99?0.53 99.66?0.29
k=8 70.87?0.59 99.84?0.10
k=9 67.23?0.48 99.94?0.08
k=10 64.00?0.46 100.0?0.00
Using the HIT@k scores we can further con-
firm this argument. The HIT@k score grows
steadily from 0.8747 to 0.9941 for k = 1...6, so
most often there will be at least one relevant trans-
lation in top-6 for the post-editor to work with.
After that room for improvement becomes very
small.
In sum, both of the PREC@k scores and the
HIT@k scores show that the ranking model effec-
tively integrates the two translation sources (MT
and TM) into one merged k-best list, and ranks
the relevant translations higher.
Table 2: PREC@k - MT and TM Systems
MT % TM %
k=1 85.87?1.32 100.0?0.00
k=2 82.52?1.60 73.58?1.04
k=3 80.05?1.11 62.45?1.14
k=4 77.92?0.95 56.11?1.11
k=5 76.22?0.87 51.78?0.78
To measure whether the ranking model is ef-
fective compared to pure MT or TM outputs, we
report the PREC@k of those outputs in Table 2.
The k-best output used in this table is ranked by
the MT or TM system, without being ranked by
our model. We see the ranked outputs consistently
outperform the MT outputs for all k = 1...5 w.r.t.
precision at a significant level, indicating that our
system preserves some high quality hits from the
TM.
The TM outputs alone are generally of much
lower quality than the MT and Ranked outputs, as
is shown by the precision scores for k = 2...5. But
TM translations obtain 1.0 PREC@1 according to
the definition of the PREC calculation. Note that
it does not mean that those outputs will need less
post-editing (cf. Section 6.1), but rather indicates
that each one of these outputs meet the lowest ac-
ceptable criterion to be relevant.
6 Analysis of Post-Editing Effort
A natural question follows the PREC and HIT
numbers: after reading the ranked k-best list, will
the post-editors edit less than they would have to if
they did not have access to the list? This question
would be best answered by human post-editors in
a large-scale experimental setting. As we have not
yet conducted a manual post-editing experiment,
we try to measure the post-editing effort implied
by our model with the edit statistics captured by
the TER metric, sorted into four types: Insertion,
Substitution, Deletion and Shift. We report the av-
erage number of edits incurred along with the 0.95
confidence interval.
6.1 Top-1 Edit Statistics
We report the results on the 1-best output of TM,
MT and our ranking system in Table 3.
In the single best results, it is easy to see that
the 1-best output from the MT system requires
the least post-editing effort. This is not surpris-
ing given the distribution of the gold standard in
Section 5.3, where most MT outputs are of better
quality than the TM hits.
Moreover, since TM translations are generally
of much lower quality as is indicated by the num-
bers in Table 3 (e.g. 2x as many substitutions
and 3x as many deletions compared to MT), un-
justly including very few of them in the ranking
output will increase loss in the edit statistics. This
explains why the ranking model has better rank-
ing precision in Tables 1 and 2, but seems to in-
cur more edit efforts. However, in practice post-
editors can neglect an obvious ?bad? translation
very quickly.
6.2 Top-k Edit Statistics
We report edit statistics of the Top-3 and Top-5
outputs in Tables 4 and 5, respectively. For each
system we report two sets of statistics: the Best-
statistics calculated on the best output (according
379
Table 3: Edit Statistics on Ranked MT and TM Outputs - Single Best
Insertion Substitution Deletion Shift
TM-Top1 0.7554 ? 0.0376 4.2461 ? 0.0960 2.9173 ? 0.1027 1.1275 ? 0.0509
MT-Top1 0.9959 ? 0.0385 2.2793 ? 0.0628 0.8940 ? 0.0353 1.2821 ? 0.0575
Rank-Top1 1.0674 ? 0.0414 2.6990 ? 0.0699 1.1246 ? 0.0412 1.2800 ? 0.0570
to TER score) in the list, and the Mean- statistics
calculated on the whole Top-k list.
The Mean- numbers allow us to have a general
overview of the ranking quality, but it is strongly
influenced by the poor TM hits that can easily be
neglected in practice. To control the impact of
those TM hits, we rely on the Best- numbers to es-
timate the edits performed on the translations that
are more likely to be used by post-editors.
In Table 4, the ranking output?s edit statistics
is closer to the MT output than the Top-1 case
in Table 3. Table 5 continues this tendency, in
which the Best-in-Top5 Ranking output requires
marginally less Substitution and Deletion opera-
tions and significantly less Insertion and Shift op-
erations (starred) than its MT counterpart. This
shows that when more of the list is explored, the
advantage of the ranking model ? utilizing mul-
tiple translation sources ? begins to compensate
for the possible large number of edits required by
poor TM hits and finally leads to reduced post-
editing effort.
There are several explanations to why the rel-
ative performance of the ranking model improves
when k increases, as compared to other models.
The most obvious explanation is that a single poor
translation is less likely to hurt edit statistics on
a k-best list with large k, if most of the transla-
tions in the k-best list are of good quality. We see
from Tables 1 and 2 that the ranking output is of
better quality than the MT and TM outputs w.r.t.
precision. For a larger k, the small number of in-
correctly ranked translations are less likely to be
chosen as the Best- translation and hold back the
Best- numbers.
A further reason is related to our ranking model
which optimizes on Kendall?s ? score. Accord-
ingly the output might not be optimal when we
evaluate the Top-1 output, but will behave better
when we evaluate on the list. This is also in ac-
cordance with our aim, which is to enrich the TM
with MT outputs and help the post-editor, instead
of choosing the translation for the post-editor.
6.3 Comparing the MT, TM and Ranking
Outputs
One of the interesting findings from Tables 3 and
4 is that according to the TER edit statistics, the
MT outputs generally need a smaller number of
edits than the TM and Ranking outputs. This cer-
tainly confirms the necessity to integrate MT into
today?s TM systems.
However, this fact should not lead to the con-
clusion that TMs should be replaced by MT com-
pletely. First of all, all of our experiments exclude
exact TM matches, as those translations will sim-
ply be reused and not translated. While this is a
realistic setting in the translation industry, it re-
moves all sentences for which the TM works best
from our evaluations.
Furthermore, Table 5 shows that the Best-in-
Top5 Ranking output performs better than the MT
outputs, hence there are TM outputs that lead to
smaller number of edits. As k increases, the rank-
ing model is able to better utilize these outputs.
Finally, in this task we concentrate on rank-
ing useful translations higher, but we are not in-
terested in how useless translations are ranked.
Ranking SVM optimizes on the ranking of the
whole list, which is slightly different from what
we actually require. One option is to use other
optimization techniques that can make use of this
property to get better Top-k edit statistics for a
smaller k. Another option is obviously to perform
regression directly on the number of edits instead
of modeling on the ranking. We plan to explore
these ideas in future work.
7 Conclusions and Future Work
In this paper we present a novel ranking-based
model to integrate SMT into a TM system, in or-
der to facilitate the work of post-editors. In such
380
Table 4: Edit Statistics on Ranked MT and TM Outputs - Top 3
Insertion Substitution Deletion Shift
TM-Best-in-Top3 0.4241 ? 0.0250 3.7395 ? 0.0887 2.9561 ? 0.0966 0.9738 ? 0.0505
TM-Mean-Top3 0.6718 ? 0.0200 5.1428 ? 0.0559 3.6192 ? 0.0649 1.3233 ? 0.0310
MT-Best?in-Top3 0.7696 ? 0.0351 1.9210 ? 0.0610 0.7706 ? 0.0332 1.0842 ? 0.0545
MT-Mean-Top3 1.1296 ? 0.0229 2.4405 ? 0.0368 0.9341 ? 0.0209 1.3797 ? 0.0344
Rank-Best-in-Top3 0.8170 ? 0.0355 2.0744 ? 0.0608 0.8410 ? 0.0338 1.0399 ? 0.0529
Rank-Mean-Top3 1.0942 ? 0.0234 2.7437 ? 0.0392 1.0786 ? 0.0231 1.3309 ? 0.0334
Table 5: Edit Statistics on Ranked MT and TM Outputs
Insertion Substitution Deletion Shift
TM-Best-in-Top5 0.4239 ? 0.0250 3.7319 ? 0.0885 2.9552 ? 0.0967 0.9673 ? 0.0504
TM-Mean-Top5 0.6143 ? 0.0147 5.5092 ? 0.0473 3.9451 ? 0.0521 1.3737 ? 0.0240
MT-Best-in-Top5 0.7690 ? 0.0351 1.9163 ? 0.0610 0.7685 ? 0.0332 1.0811 ? 0.0544
MT-Mean-Top5 1.1912 ? 0.0182 2.5326 ? 0.0291 0.9487 ? 0.0165 1.4305 ? 0.0272
Rank-Best-in-Top5 0.7246 ? 0.0338* 1.8887 ? 0.0598 0.7562 ? 0.0327 0.9705 ? 0.0515*
Rank-Mean-Top5 1.1173 ? 0.0181 2.8777 ? 0.0312 1.1585 ? 0.0200 1.3675 ? 0.0260
a model, the user of the TM will be presented
with an augmented k-best list, consisting of trans-
lations from both the TM and theMT systems, and
ranked according to ascending prospective post-
editing effort.
From the post-editors? point of view, the TM
remains intact. And unlike in the binary transla-
tion recommendation, where only one translation
recommendation is provided, the ranking model
offers k-best post-editing candidates, enabling the
user to use more resources when translating. As
we do not actually throw away any translation pro-
duced from the TM, the assets represented by the
TM are preserved and the related estimation of the
upper bound cost is still valid.
We extract system independent features from
theMT and TM outputs and use Ranking SVMs to
train the ranking model, which outperforms both
the TM?s and MT?s k-best list w.r.t. precision at k,
for all ks.
We also analyze the edit statistics of the inte-
grated k-best output using the TER edit statistics.
Our ranking model results in slightly increased
number of edits compared to the MT output (ap-
parently held back by a small number of poor TM
outputs that are ranked high) for a smaller k, but
requires less edits than both the MT and the TM
output for a larger k.
This work can be extended in a number of ways.
Most importantly, We plan to conduct a user study
to validate the effectiveness of the method and
to gather HTER scores to train a better ranking
model. Furthermore, we will try to experiment
with learning models that can further reduce the
number of edit operations on the top ranked trans-
lations. We also plan to improve the adaptability
of this method and apply it beyond a specific do-
main and language pair.
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 07/CE/I1142) as part of
the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We
thank Symantec for providing the TM database
and the anonymous reviewers for their insightful
comments.
References
Cortes, Corinna and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Joachims, Thorsten. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 133?142, New York, NY, USA.
381
Koehn, Philipp., Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
(NAACL/HLT-2003), pages 48 ? 54, Edmonton, Al-
berta, Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions (ACL-2007), pages 177?
180, Prague, Czech Republic.
Levenshtein, Vladimir Iosifovich. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10(8):707?710.
Och, Franz Josef and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 295?302,
Philadelphia, PA, USA.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics (ACL-2003), pages 160?167,
Morristown, NJ, USA.
Shen, Libin, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative reranking for machine trans-
lation. In HLT-NAACL 2004: Main Proceedings,
pages 177?184, Boston, Massachusetts, USA. As-
sociation for Computational Linguistics.
Sikes, Richard. 2007. Fuzzy matching in theory and
practice. Multilingual, 18(6):39 ? 43.
Simard, Michel and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted
translation environment. In Proceedings of the
Twelfth Machine Translation Summit (MT Summit
XII), pages 120 ? 127, Ottawa, Ontario, Canada.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas (AMTA-2006), pages 223?231,
Cambridge, MA, USA.
Specia, Lucia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009a. Esti-
mating the sentence-level quality of machine trans-
lation systems. In Proceedings of the 13th An-
nual Conference of the European Association for
Machine Translation (EAMT-2009), pages 28 ? 35,
Barcelona, Spain.
Specia, Lucia, Craig Saunders, Marco Turchi, Zhuo-
ran Wang, and John Shawe-Taylor. 2009b. Improv-
ing the confidence of machine translation quality
estimates. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 136 ?
143, Ottawa, Ontario, Canada.
Stolcke, Andreas. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, volume 2, pages 901?904, Denver, CO,
USA.
Ye, Yang, Ming Zhou, and Chin-Yew Lin. 2007.
Sentence level machine translation evaluation as a
ranking. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 240?247,
Prague, Czech Republic.
382
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 420?429,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Facilitating Translation Using Source Language Paraphrase Lattices
Jinhua Du, Jie Jiang, Andy Way
CNGL, School of Computing
Dublin City University, Dublin, Ireland
{jdu, jjiang, away}@computing.dcu.ie
Abstract
For resource-limited language pairs, coverage
of the test set by the parallel corpus is an
important factor that affects translation qual-
ity in two respects: 1) out of vocabulary
words; 2) the same information in an input
sentence can be expressed in different ways,
while current phrase-based SMT systems can-
not automatically select an alternative way
to transfer the same information. Therefore,
given limited data, in order to facilitate trans-
lation from the input side, this paper pro-
poses a novel method to reduce the transla-
tion difficulty using source-side lattice-based
paraphrases. We utilise the original phrases
from the input sentence and the correspond-
ing paraphrases to build a lattice with esti-
mated weights for each edge to improve trans-
lation quality. Compared to the baseline sys-
tem, our method achieves relative improve-
ments of 7.07%, 6.78% and 3.63% in terms
of BLEU score on small, medium and large-
scale English-to-Chinese translation tasks re-
spectively. The results show that the proposed
method is effective not only for resource-
limited language pairs, but also for resource-
sufficient pairs to some extent.
1 Introduction
In recent years, statistical MT systems have been
easy to develop due to the rapid explosion in data
availability, especially parallel data. However, in
reality there are still many language pairs which
lack parallel data, such as Urdu?English, Chinese?
Italian, where large amounts of speakers exist for
both languages; of course, the problem is far worse
for pairs such as Catalan?Irish. For such resource-
limited language pairs, sparse amounts of parallel
data would cause the word alignment to be inac-
curate, which would in turn lead to an inaccurate
phrase alignment, and bad translations would re-
sult. Callison-Burch et al (2006) argue that lim-
ited amounts of parallel training data can lead to the
problem of low coverage in that many phrases en-
countered at run-time are not observed in the train-
ing data and so their translations will not be learned.
Thus, in recent years, research on addressing the
problem of unknown words or phrases has become
more and more evident for resource-limited lan-
guage pairs.
Callison-Burch et al (2006) proposed a novel
method which substitutes a paraphrase for an un-
known source word or phrase in the input sentence,
and then proceeds to use the translation of that para-
phrase in the production of the target-language re-
sult. Their experiments showed that by translating
paraphrases a marked improvement was achieved in
coverage and translation quality, especially in the
case of unknown words which previously had been
left untranslated. However, on a large-scale data set,
they did not achieve improvements in terms of auto-
matic evaluation.
Nakov (2008) proposed another way to use para-
phrases in SMT. He generates nearly-equivalent syn-
tactic paraphrases of the source-side training sen-
tences, then pairs each paraphrased sentence with
the target translation associated with the original
sentence in the training data. Essentially, this
method generates new training data using para-
phrases to train a new model and obtain more useful
420
phrase pairs. However, he reported that this method
results in bad system performance. By contrast,
real improvements can be achieved by merging the
phrase tables of the paraphrase model and the orig-
inal model, giving priority to the latter. Schroeder
et al (2009) presented the use of word lattices
for multi-source translation, in which the multiple
source input texts are compiled into a compact lat-
tice, over which a single decoding pass is then per-
formed. This lattice-based method achieved positive
results across all data conditions.
In this paper, we propose a novel method us-
ing paraphrases to facilitate translation, especially
for resource-limited languages. Our method does
not distinguish unknown words in the input sen-
tence, but uses paraphrases of all possible words
and phrases in the source input sentence to build a
source-side lattice to provide a diverse and flexible
list of source-side candidates to the SMT decoder
so that it can search for a best path and deliver the
translation with the highest probability. In this case,
we neither need to change the phrase table, nor add
new features in the log-linear model, nor add new
sentences in the training data.
The remainder of this paper is organised as fol-
lows. In Section 2, we define the ?translation diffi-
culty? from the perspective of the source side, and
then examine how well the test set is covered by
the phrase table and the parallel training data . Sec-
tion 3 describes our paraphrase lattice method and
discusses how to set the weights for the edges in the
lattice network. In Section 4, we report comparative
experiments conducted on small, medium and large-
scale English-to-Chinese data sets. In Section 5,
we analyse the influence of our paraphrase lattice
method. Section 6 concludes and gives avenues for
future work.
2 What Makes Translation Difficult?
2.1 Translation Difficulty
We use the term ?translation difficulty? to explain
how difficult it is to translate the source-side sen-
tence in three respects:
? The OOV rates of the source sentences in the
test set (Callison-Burch et al, 2006).
? Translatability of a known phrase in the input
sentence. Some particular grammatical struc-
tures on the source side cannot be directly
translated into the corresponding structures on
the target side. Nakov (2008) presents an ex-
ample showing how hard it is to translate an En-
glish construction into Spanish. Assume that an
English-to-Spanish SMT system has an entry
in its phrase table for ?inequality of income?,
but not for ?income inequality?. He argues that
the latter phrase is hard to translate into Span-
ish where noun compounds are rare: the correct
translation in this case requires a suitable Span-
ish preposition and a reordering, which are hard
for the system to realize properly in the target
language (Nakov, 2008).
? Consistency between the reference and the
target-side sentence in the training corpus.
Nakov (2008) points out that if the target-side
sentence in the parallel corpus is inconsistent
with the reference of the test set, then in some
cases, a test sentence might contain pieces that
are equivalent, but syntactically different from
the phrases learned in training, which might re-
sult in practice in a missed opportunity for a
high-quality translation. In this case, if we use
paraphrases for these pieces of text, then we
might improve the opportunity for the transla-
tion to approach the reference, especially in the
case where only one reference is available.
2.2 Coverage
As to the first aspect ? coverage ? we argue that
the coverage rate of the new words or unknown
words are more and more becoming a ?bottleneck?
for resource-limited languages. Furthermore, cur-
rent SMT systems, either phrase-based (Koehn et al,
2003; Chiang, 2005) or syntax-based (Zollmann and
Venugopal, 2006), use phrases as the fundamental
translation unit, so how much the phrase table and
training data can cover the test set is an important
factor which influences the translation quality. Ta-
ble 1 shows the statistics of the coverage of the test
set on English-to-Chinese FBIS data, where we can
see that the coverage of unigrams is very high, es-
pecially when the data is increased to the medium
size (200K), where unigram coverage is greater than
90%. Based on the observations of the unknown un-
421
20K Cov.(%) 200K Cov.(%)
PL Tset PT Corpus in PT in Corpus PT Corpus in PT in Corpus
1 5,369 3,785 4,704 70.5 87.61 4,941 5,230 92.03 97.41
2 24,564 8,631 15,109 35.14 61.51 16,803 21,071 68.40 85.78
3 37,402 4,538 12,091 12.13 32.33 12,922 22,531 34.55 60.24
4 41,792 1,703 6,150 4.07 14.72 5,974 14,698 14.29 35.17
5 43,008 626 2,933 1.46 6.82 2,579 8,425 5.99 19.59
6 43,054 259 1,459 0.6 3.39 1,192 4,856 2.77 11.28
7 42,601 119 821 0.28 1.93 581 2,936 1.36 6.89
8 41,865 51 505 0.12 1.21 319 1,890 0.76 4.51
9 40,984 34 341 0.08 0.83 233 1,294 0.57 3.16
10 40,002 22 241 0.05 0.6 135 923 0.34 2.31
Table 1: The coverage of the test set by the phrase table and the parallel corpus based on different amount of the
training data. ?PL? indicates the Phrase Length N , where {1 <= N <= 10}; ?20K? and ?200K? represent the sizes
of the parallel data for model training and phrase extraction; ?Cov.? indicates the coverage rate; ?Tset? represents the
number of unique phrases with the length N in the Test Set; ?PT? represents the number of phrases of the Test Set
occur in the Phrase Table; ?Corpus? indicates the number of phrases of the Test Set appearing in the parallel corpus;
?in PT? indicates the coverage of the phrases in the Test Set by the phrase table and correspondingly ?in Corpus?
represents the coverage of the phrases in the Test Set by the Parallel Corpus.
igrams, we found that most are named entities (NEs)
such as person name, location name, etc. From the
bigram phrases, the coverage rates begin to signifi-
cantly decline. It can also be seen that phrases con-
taining more than 5 words rarely appear either in the
phrase table or in the parallel corpus, which indi-
cates that data sparseness is severe for long phrases.
Even if the size of the corpus is significantly in-
creased (e.g. from 20K to 200K), the coverage of
long phrases is still quite low.
With respect to these three aspects of the transla-
tion difficulty, especially for data-limited language
pairs, we propose a more effective method to make
use of the paraphrases to facilitate translation pro-
cess.
3 Paraphrase Lattice for Input Sentences
In this Section, we propose a novel method to em-
ploy paraphrases to reduce the translation difficulty
and in so doing increase the translation quality.
3.1 Motivation
Our idea to build a paraphrase lattice for SMT is in-
spired by the following points:
? Handling unknown words is a challenging issue
for SMT, and using paraphrases is an effective
way to facilitate this problem (Callison-Burch
et al, 2006);
? The method of paraphrase substitution does not
show any significant improvement, especially
on a large-scale data set in terms of BLEU (Pa-
pineni et al, 2002) scores (Callison-Burch et
al., 2006);
? Building a paraphrase lattice might provide
more translation options to the decoder so that
it can flexibly search for the best path.
The major contributions of our method are:
? We consider all N -gram phrases rather than
only unknown phrases in the test set, where
{1 <= N <= 10};
? We utilise lattices rather than simple substitu-
tion to facilitate the translation process;
? We propose an empirical weight estimation
method to set weights for edges in the word lat-
tice, which is detailed in Section 3.4.
3.2 Paraphrase Acquisition
Paraphrases are alternative ways to express the same
or similar meaning given a certain original word,
phrase or segment. The paraphrases used in our
method are generated from the parallel corpora
based on the algorithm in (Bannard and Callison-
Burch, 2005), in which paraphrases are identified
422
by pivoting through phrases in another language.
In this algorithm, the foreign language translations
of an English phrase are identified, all occurrences
of those foreign phrases are found, and all English
phrases that they translate as are treated as potential
paraphrases of the original English phrase (Callison-
Burch et al, 2006). A paraphrase has a probability
p(e2|e1) which is defined as in (2):
p(e2|e1) =
?
f
p(f |e1)p(e2|f) (1)
where the probability p(f |e1) is the probability that
the original English phrase e1 translates as a particu-
lar phrase f in the other language, and p(e2|f) is the
probability that the candidate paraphrase e2 trans-
lates as the foreign language phrase.
p(e2|f) and p(f |e1) are defined as the transla-
tion probabilities which can be calculated straight-
forwardly using maximum likelihood estimation by
counting how often the phrases e and f are aligned
in the parallel corpus as in (2) and (3):
p(e2|f) ?
count(e2, f)
?
e2 count(e2, f)
(2)
p(f |e1) ?
count(f, e1)
?
f count(f, e1)
(3)
3.3 Construction of Paraphrase Lattice
To present paraphrase options to the PB-SMT de-
coder, lattices with paraphrase options are con-
structed to enrich the source-language sentences.
The construction process takes advantage of the cor-
respondence between detected paraphrases and po-
sitions of the original words in the input sentence,
then creates extra edges in the lattices to allow the
decoder to consider paths involving the paraphrase
words.
An toy example is illustrated in Figure 1: given
a sequence of words {w1, . . . , wN} as the input,
two phrases ? = {?1, . . . , ?p} and ? = {?1, . . . , ?q}
are detected as paraphrases for S1 = {wx, . . . , wy}
(1 ? x ? y ? N ) and S2 = {wm, . . . , wn}
(1 ? m ? n ? N ) respectively. The following
steps are taken to transform them into word lattices:
1. Transform the original source sentence into
word lattices. N + 1 nodes (?k, 0 ? k ? N )
... ...wx wm... wy
...
 1
...
Source side 
sentence
Generated 
lattice
!1 !2 ? !p
 1  2 ?  q
Paraphrase A
Paraphrase B
!1
!2 ... !p
 2
 q
... ... wn
...wx wm wy wn... ... ...
Figure 1: An example of lattice-based paraphrases for an
input sentence
are created, and N edges (referred to as ?ORG-
E? edges) labeled with wi (1 ? i ? N ) are
generated to connect them sequentially.
2. Generate extra nodes and edges for each of the
paraphrases. Taking ? as an example, firstly,
p ? 1 nodes are created, and then p edges
(referred as ?NEW-E? edges) labeled with ?j
(1 ? j ? p) are generated to connect node
?x?1, p? 1 nodes and ?y?1.
Via step 2, word lattices are generated by adding
new nodes and edges coming from paraphrases.
Note that to build word lattices, paraphrases with
multi-words are broken into word sequences, and
each of the words produces one extra edge in the
word lattices as shown in the bottom part in Figure 1.
Figure 2 shows an example of constructing the
word lattice for an input sentence which is from the
test set used in our experiments.1 The top part in
Figure 2 represents nodes (double-line circles) and
edges (solid lines) that are constructed by the orig-
inal words from the input sentence, while the bot-
tom part in Figure 2 indicates the final word lattice
with the addition of new nodes (single-line circles)
and new edges (dashed lines) which come from the
paraphrases. We can see that the paraphrase lattice
increases the diversity of the source phrases so that it
can provide more flexible translation options during
the decoding process.
1Figure 2 contains paths that are duplicates except for the
weights. We plan to handle this in future work.
423
Figure 2: An example of how to build a paraphrase lattice for an input sentence
3.4 Weight Estimation
Estimating and normalising the weight for each edge
in the word lattice is a challenging issue when the
edges come from different sources. In this section,
we propose an empirical method to set the weights
for the edges by distinguishing the original (?ORG-
E?) and new (?NEW-E?) edges in the lattices. The
aim is to utilize the original sentences as the ref-
erences to weight the edges from paraphrases, so
that decoding paths going through ?ORG-E? edges
will tend to have higher scores than those which use
?NEW-E? ones. The assumption behind this is that
the paraphrases are alternatives for the original sen-
tences, so decoding paths going though them ought
to be penalised.
Therefore, for all the ?ORG-E? edges, their
weights in the lattice are set to 1.0 as the reference.
Thus, in the log-linear model, decoding paths going
though these edges are not penalised because they
do not come from the paraphrases.
By contrast, ?NEW-E? are divided into two
groups for the calculation of weights:
? For ?NEW-E? edges which are outgoing edges
of the lattice nodes that come from the original
sentences, the probabilities p(es|ei)2 of their
2es indicates the source phrase S, ei represents one of the
corresponding paraphrases are utilised to pro-
duce empirical weights. Supposing that a set of
paraphrases X = {x1, . . . , xk} start at node A
which comes from the original sentence, so that
X are sorted descendingly based on the proba-
bilities p(es|ei), their corresponding edges for
node A are G = {g1, . . . , gk}, then the weights
are calculated as in (4):
w(ei) =
1
k + i
(1 <= i <= k) (4)
where k is a predefined parameter to trade off
between decoding speed and the number of
potential paraphrases being considered. Thus,
once a decoding path goes though one of these
edges, it will be penalised according to its para-
phrase probabilities.
? For all other ?NEW-E? edges, their weights
are set to 1.0, because the paraphrase penalty
has been counted in their preceding ?NEW-E?
edges.
Figure 2 illustrates the weight estimation results.
Nodes coming from the original sentences are drawn
in double-line circles (e.g. nodes 0 to 7), while
paraphrases of S.
424
nodes created from paraphrases are shown in single-
line circles (e.g. nodes 8 to 10). ?ORG-E? edges are
drawn in solid lines and ?NEW-E? edges are shown
using dashed lines. As specified previously, ?ORG-
E? edges are all weighted by 1.0 (e.g. edge labeled
?the? from node 0 to 1). By contrast, ?NEW-E?
edges in the first group are weighted by equation
(4) (e.g. edges in dashed lines start from node 0
to node 2 and 8), while others in the second group
are weighted by 1.0 (e.g. edge labeled ?training?
from node 8 to 2). Note that penalties of the paths
going through paraphrases are counted by equation
(4), which is represented by the weights of ?NEW-
E? edges in the first group. For example, starting
from node 2, paths going to node 9 and 10 are pe-
nalised because lattice weights are also considered
in the log-linear model. However, other edges do
not imply penalties since their weights are set to 1.0.
The reason to set al weights for the ?ORG-E?
edges to a uniform weight (e.g. 1.0) instead of a
lower empirical weight is to avoid excessive penal-
ties for the original words. For example, in Fig-
ure 2, the original edge from node 3 to 4 (con-
tinue) has a weight of 1.0, so the paths going though
the original edges from node 2 to 4 (will continue)
have a higher lattice score (1.0 ? 1.0 = 1.0) than
the paths going through the edges of paraphrases
(e.g. will resume (score: 0.125 ? 1.0 = 0.125) and
will go (score: 0.11 ? 1.0 = 0.11)), or any other
mixed paths that goes through original edges and
paraphrase edges, such as will continuous (score:
1.0 ? 0.125 = 0.125). The point is that we should
have more trust when translating the original words,
but if we penalise (set weights < 1.0) the ?ORG-
E? edges whenever there is a paraphrase for them,
then when considering the context of the lattice,
paraphrases will be favoured systematically. That is
why we just penalise the ?NEW-E? edges in the first
group and set other weights to 1.0.
As to unknown words in the input sentence, even
if we give them a prioritised weight, they would
be severely penalised in the decoding process. So
we do not need to distinguish unknown words when
building and weighting the paraphrase lattice.
4 Experiments
4.1 System and Data Preparation
For our experiments, we use Moses (Koehn et al,
2007) as the baseline system which can support
lattice decoding. We also realise a paraphrase
substitution-based system (Para-Sub)3 based on the
method in (Callison-Burch, 2006) to compare with
the baseline system and our proposed paraphrase
lattice-based (Lattice) system.
The alignment is carried out by GIZA++ (Och
and Ney, 2003) and then we symmetrized the word
alignment using the grow-diag-final heuristic. The
maximum phrase length is 10 words. Parameter tun-
ing is performed using Minimum Error Rate Train-
ing (Och, 2003).
The experiments are conducted on English-to-
Chinese translation. In order to fully compare our
proposed method with the baseline and the ?Para-
Sub? system, we perform the experiments on three
different sizes of training data: 20K, 200K and 2.1
million pairs of sentences. The former two sizes of
data are derived from FBIS,4 and the latter size of
data consists of part of HK parallel corpus,5 ISI par-
allel data,6 other news data and parallel dictionar-
ies from LDC. All the language models are 5-gram
which are trained on the monolingual part of parallel
data.
The development set (devset) and the test set for
experiments using 20K and 200K data sets are ran-
domly extracted from the FBIS data. Each set in-
cludes 1,200 sentences and each source sentence
has one reference. For the 2.1 million data set, we
use a different devset and test set in order to verify
whether our proposed method can work on a lan-
guage pair with sufficient resources. The devset is
the NIST 2005 Chinese-English current set which
has only one reference for each source sentence and
the test set is the NIST 2003 English-to-Chinese
current set which contains four references for each
source sentence. All results are reported in BLEU
and TER (Snover et al, 2006) scores.
3We use ?Para-Sub? to represent their system in the rest of
this paper.
4This is a multilingual paragraph-aligned corpus with LDC
resource number LDC2003E14.
5LDC number: LDC2004T08.
6LDC number: LDC2007T09.
425
20K 200K
SYS BLEU CI 95% pair-CI 95% TER BLEU CI 95% pair-CI 95% TER
Baseline 14.42 [-0.81, +0.74] ? 75.30 23.60 [-1.03, +0.97] ? 63.56
Para-Sub 14.78 [-0.78, +0.82] [+0.13, +0.60] 73.75 23.41 [-1.04, +1.00] [-0.46, +0.09] 63.84
Lattice 15.44 [-0.85, +0.84] [+0.74, +1.30] 73.06 25.20 [-1.11, +1.15] [+1.19, +2.01] 62.37
Table 2: Comparison between the baseline, ?Para-Sub? and our ?Lattice? (paraphrase lattice) method.
The paraphrase data set used in our lattice-based
and the ?Para-Sub? systems is same which is de-
rived from the ?Paraphrase Phrase Table?7 of TER-
Plus (Snover et al, 2009). The parameter k in equa-
tion 4 is set to 7.
4.2 Paraphrase Filtering
The more edges there are in a lattice, the more
complicated the decoding is in the search process.
Therefore, in order to reduce the complexity of the
lattice and increase decoding speed, we must fil-
ter out some potential noise in the paraphrase table.
Two measures are taken to optimise the paraphrases
when building a paraphrase lattice:
? Firstly, we filter out all the paraphrases whose
probability is less than 0.01;
? Secondly, given a source-side input sentence,
we retrieve all possible paraphrases and their
probabilities for source-side phrases which ap-
pear in the paraphrase table. Then we remove
the paraphrases which are not occurred in the
?phrase table? of the SMT system. This mea-
sure intends to avoid adding new ?unknown
words? to the source-side sentence. After
this measure, we can acquire the final para-
phrases which can be denoted as a quadru-
ple < SEN ID,Span, Para, Prob >, where
?SEN ID? indicates the ID of the input sen-
tence, ?Span? represents the span of the source-
side phrase in the original input sentence,
?Para? indicates the paraphrase of the source-
side phrase, and ?Prob? represents the probabil-
ity between the source-side phrase and its para-
phrase, which is used to set the weight of the
edge in the lattice. The quadruple is used to
construct the weighted lattice.
7http://www.umiacs.umd.edu/?snover/terp/
downloads/terp-pt.v1.tgz.
4.3 Experimental Results
The experimental results conducted on small and
medium-sized data sets are shown in Table 2. The
95% confidence intervals (CI) for BLEU scores are
independently computed on each of three systems,
while the ?pair-CI 95%? are computed relative to
the baseline system only for ?Para-Sub? and ?Lat-
tice? systems. All the significance tests use boot-
strap and paired-bootstrap resampling normal ap-
proximation methods (Zhang and Vogel, 2004).8
Improvements are considered to be significant if the
left boundary of the confidence interval is larger
than zero in terms of the ?pair-CI 95%?. It can
be seen that 1) our ?Lattice? system outperforms
the baseline by 1.02 and 1.6 absolute (7.07% and
6.78% relative) BLEU points in terms of the 20K
and 200K data sets respectively, and our system also
decreases the TER scores by 2.24 and 1.19 (2.97%
and 1.87% relative) points than the baseline system.
In terms of the ?pair-CI 95%?, the left boundaries
for 20K and 200K data are respectively ?+0.74? and
?+1.19?, which indicate that the ?Lattice? system is
significantly better than the baseline system on these
two data sets. 2) The ?Para-Sub? system performs
slightly better (0.36 absolute BLEU points) than the
baseline system on the 20K data set, but slightly
worse (0.19 absolute BLEU points) than the baseline
on the 200K data set, which indicates that the para-
phrase substitution method used in (Callison-Burch
et al, 2006) does not work on resource-sufficient
data sets. In terms of the ?pair-CI 95%?, the left
boundary for 20K data is ?+0.13?, which indicates
that it is significantly better than the baseline sys-
tem, while the left boundary is ?-0.46? for 200K
data, which indicates that the ?Para-Sub? system
is significantly worse than the baseline system. 3)
comparing the ?Lattice? system with the ?Para-Sub?
8http://projectile.sv.cmu.edu/research/
public/tools/bootStrap/tutorial.htm.
426
SYS BLEU CI 95% pair-CI 95% NIST TER
Baseline 14.04 [-0.73, +0.40] ? 6.50 74.88
Para-Sub 14.13 [-0.56, +0.56] [-0.18, +0.40] 6.52 74.43
Lattice 14.55 [-0.75, +0.32] [+0.15,+0.83] 6.55 73.28
Table 3: Comparison between the baseline and our paraphrase lattice method on a large-scale data set.
system, the ?pair-CI 95%? for 20K and 200K data
are respectively [+0.41, +0.92] and [+1.40, +2.17],
which indicates that the ?Lattice? system is signif-
icantly better than the ?Para-Sub? system on these
two data sets as well. 4) In terms of the two metrics,
our proposed method achieves the best performance,
which shows that our method is effective and consis-
tent on different sizes of data.
In order to verify our method on large-scale
data, we also perform experiments on 2.1 million
sentence-pairs of English-to-Chinese data as de-
scribed in Section 4.1. The results are shown in Ta-
ble 3. From Table 3, it can be seen that the ?Lattice?
system achieves an improvement of 0.51 absolute
(3.63% relative) BLEU points and a decrease of 1.6
absolute (2.14% relative) TER points compared to
the baseline. In terms of the ?pair-CI 95%?, the left
boundary for the ?Lattice? system is ?+0.15? which
indicates that it is significantly better than the base-
line system in terms of BLEU. Interestingly, in our
experiment, the ?Para-Sub? system also outperforms
the baseline on those three automatic metrics. How-
ever, in terms of the ?pair-CI 95%?, the left bound-
ary for the ?Para-Sub? system is ?-0.18? which indi-
cates that it is not significantly better than the base-
line system in terms of BLEU. The results also show
that our proposed method is effective and consistent
even on a large-scale data set.
It also can be seen that the improvement on 2.1
million sentence-pairs is less than that of the 20K
and 200K data sets. That is, as the size of the train-
ing data increases, the problems of data sparseness
decrease, so that the coverage of the test set by the
parallel corpus will correspondingly increase. In this
case, the role of paraphrases in decoding becomes a
little weaker. On the other hand, it might become a
kind of noise to interfere with the exact translation
of the original source-side phrases when decoding.
Therefore, our proposed method may be more ap-
propriate for language pairs with limited resources.
5 Analysis
5.1 Coverage of Paraphrase Test Set
The coverage rate of the test set by the phrase ta-
ble is an important factor that could influence the
translation result, so in this section we examine the
characteristics of the updated test set that adds in the
paraphrases. We take the 200K data set to examine
the coverage issue. Table 4 is an illustration to com-
pare the new coverage and the old coverage (without
paraphrases) on medium sized training data.
PL Tset PT New Cov.(%) Old Cov.(%)
1 9,264 8,994 97.09 92.03
2 32,805 25,796 78.63 68.40
3 39,918 15,708 39.35 34.55
4 42,247 6,479 15.34 14.29
5 43,088 2,670 6.20 5.99
6 43,066 1,204 2.80 2.77
7 42,602 582 1.37 1.36
8 41,865 319 0.76 0.76
9 40,984 233 0.57 0.57
10 40,002 135 0.34 0.34
Table 4: The coverage of the paraphrase-added test set by
the phrase table on medium size of the training data.
From Table 4, we can see that the coverage of un-
igrams, bigrams, trigrams and 4-grams goes up by
about 5%, 10%, 5% and 1%, while from 5-grams
there is only a slight or no increase in coverage.
These results show that 1) most of the paraphrases
that are added in are lower-order n-grams; 2) the
paraphrases can increase the coverage of the input
by handling the unknown words to some extent.
However, we observed that most untranslated
words in the ?Para-Sub? and ?Lattice? systems are
still NEs, which shows that in our paraphrase table,
there are few paraphrases for the NEs. Therefore,
to further improve the translation quality using para-
phrases, we also need to acquire the paraphrases for
NEs to increase the coverage of unknown words.
427
Source:     whether or the albanian rebels can be genuinely disarmed completely is the main challenge to nato .
Ref:           ??    ??    ??    ?    ??    ??    ?    ??    ?    ??    ??    ?    ??    ??    ?
Baseline:   ??    ?    ??    ??    ?    ?    ??    disarmed    ??    ?    ??    ?    ??   ??   ?
Para-Sub:  ??    ??    ??    ??    ??    ??    ??    ??    ?    ??    ?    ??    ??    ?
Lattice:      ??   ??  ??   ??    ??    ??    ??  ? ?? ??  ?    ??    ?    ??    ??   ? 
Figure 3: An example from three systems to compare the processing of OOVs
5.2 Analysis on Translation Results
In this section, we give an example to show the ef-
fectiveness of using paraphrase lattices to deal with
unknown words. The example is evaluated accord-
ing to both automatic evaluation and human evalua-
tion at sentence level.
See Figure 3 as an illustration of how the
paraphrase-based systems process unknown words.
According to the word alignments between the
source-side sentence and the reference, the word
?disarmed? is translated into two Chinese words ??
?? and ????. These two Chinese words are dis-
continuous in the reference, so it is difficult for the
PB-SMT system to correctly translate the single En-
glish word into a discontinuous Chinese phrase. In
fact in this example, ?disarmed? is an unknownword
and it is kept untranslated in the result of the base-
line system. In the ?Para-Sub? system, it is trans-
lated into ?`? based on a paraphrase pair PP1 =
?disarmed ? disarmament ? 0.087? and its transla-
tion pair T1 = ?disarmament ? `?. The number
?0.087? is the probability p1 that indicates to what
extent these two words are paraphrases. It can be
seen that although ?`? is quite different from the
meaning of ?disarmed?, it is understandable for hu-
man in some sense. In the ?Lattice? system, the
word ?disarmed? is translated into three Chinese
words ?: / ??? based on a paraphrase pair
PP2 = ?disarmed ? demilitarized ? 0.099? and its
translation pair T2 = ?demilitarized ? : / ?
??. The probability p2 is slightly greater than p1.
We argue that the reason that the ?Lattice? system
selects PP2 and T2 rather than PP1 and T1 is be-
cause of the weight estimation in the lattice. That
is, PP2 is more prioritised, while PP1 is more pe-
nalised based on equation (4).
From the viewpoint of human evaluation, the
paraphrase pair PP2 is more appropriate than PP1,
and the translation T2 is more similar to the origi-
nal meaning than T1. The sentence-level automatic
evaluation scores for this example in terms of BLEU
and TER metrics are shown in Table 5.
SYS BLEU TER
Baseline 20.33 66.67
Para-Sub 21.78 53.33
Lattice 23.51 53.33
Table 5: Comparison on sentence-level scores in terms of
BLEU and TER metrics.
The BLEU score of the ?Lattice? system is much
higher than the baseline, and the TER score is quite
a bit lower than the baseline. Therefore, from the
viewpoint of automatic evaluation, the translation
from the ?Lattice? system is also better than those
from the baseline and ?Para-Sub? systems.
6 Conclusions and Future Work
In this paper, we proposed a novel method using
paraphrase lattices to facilitate the translation pro-
cess in SMT. Given an input sentence, our method
firstly discovers all possible paraphrases from a
paraphrase database for N -grams (1 <= N <= 10)
in the test set, and then filters out the paraphrases
which do not appear in the phrase table in order to
avoid adding new unknown words on the input side.
We then use the original words and the paraphrases
to build a word lattice, and set the weights to priori-
tise the original edges and penalise the paraphrase
edges. Finally, we import the lattice into the de-
coder to perform lattice decoding. The experiments
are conducted on English-to-Chinese translation us-
ing the FBIS data set with small and medium-sized
amounts of data, and on a large-scale corpus of 2.1
428
million sentence pairs. We also performed compar-
ative experiments for the baseline, the ?Para-Sub?
system and our paraphrase lattice-based system. The
experimental results show that our proposed system
significantly outperforms the baseline and the ?Para-
Sub? system, and the effectiveness is consistent on
the small, medium and large-scale data sets.
As for future work, firstly we plan to propose a
pruning algorithm for the duplicate paths in the lat-
tice, which will track the edge generation with re-
spect to the path span, and thus eliminate duplicate
paths. Secondly, we plan to experiment with another
feature function in the log-linear model to discount
words derived from paraphrases, and use MERT to
assign an appropriate weight to this feature function.
Acknowledgments
Many thanks to the reviewers for their insight-
ful comments and suggestions. This work is sup-
ported by Science Foundation Ireland (Grant No.
07/CE/I1142) as part of the Centre for Next Genera-
tion Localisation (www.cngl.ie) at Dublin City Uni-
versity.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In 43rd An-
nual meeting of the Association for Computational
Linguistics, Ann Arbor, MI, pages 597?604.
Chris Callison-Burch, Philipp Koehn and Miles Osborne.
2006. Improved statistical machine translation using
paraphrases. In Proceedings of HLT-NAACL 2006:
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL,
NY, USA, pages 17?24.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In 43rd Annual
meeting of the Association for Computational Linguis-
tics, Ann Arbor, MI, pages 263?270.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003: conference combining Hu-
man Language Technology conference series and the
North American Chapter of the Association for Com-
putational Linguistics conference series, Edmonton,
Canada, pages 48?54.
Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Wade Shen, C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and
Evan Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In ACL 2007: demo
and poster sessions, Prague, Czech Republic, pages
177?180.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of ACL-08:HLT. Third Work-
shop on Statistical Machine Translation, Columbus,
Ohio, USA, pages 147?150.
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In 41st Annual meeting
of the Association for Computational Linguistics, Sap-
poro, Japan, pages 160?167.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: aMethod for Automatic Eval-
uation of Machine Translation. In 40th Annual meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA, pages 311?318.
Josh Schroeder, Trevor Cohn and Philipp Koehn. 2009.
Word Lattices for Multi-source Translation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL, Athens, Greece, pages 719?727.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, Cam-
bridge, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J.Dorr and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, Athens,
Greece, pages 259?268.
Ying Zhang and Stephan Vogel. 2004. Measuring Con-
fidence Intervals for the Machine Translation Evalua-
tion Metrics. In Proceedings of the 10th International
Conference on Theoretical and Methodological Issues
in Machine Translation (TMI), pages 85?94.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of HLT-NAACL 2006: Proceedings of
the Workshop on Statistical Machine Translation, New
York, pages 138?141.
429
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622?630,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bridging SMT and TM with Translation Recommendation
Yifan He Yanjun Ma Josef van Genabith Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yhe,yma,josef,away}@computing.dcu.ie
Abstract
We propose a translation recommendation
framework to integrate Statistical Machine
Translation (SMT) output with Transla-
tion Memory (TM) systems. The frame-
work recommends SMT outputs to a TM
user when it predicts that SMT outputs are
more suitable for post-editing than the hits
provided by the TM. We describe an im-
plementation of this framework using an
SVM binary classifier. We exploit meth-
ods to fine-tune the classifier and inves-
tigate a variety of features of different
types. We rely on automatic MT evalua-
tion metrics to approximate human judge-
ments in our experiments. Experimental
results show that our system can achieve
0.85 precision at 0.89 recall, excluding ex-
act matches. Furthermore, it is possible for
the end-user to achieve a desired balance
between precision and recall by adjusting
confidence levels.
1 Introduction
Recent years have witnessed rapid developments
in statistical machine translation (SMT), with con-
siderable improvements in translation quality. For
certain language pairs and applications, automated
translations are now beginning to be considered
acceptable, especially in domains where abundant
parallel corpora exist.
However, these advances are being adopted
only slowly and somewhat reluctantly in profes-
sional localization and post-editing environments.
Post-editors have long relied on translation memo-
ries (TMs) as the main technology assisting trans-
lation, and are understandably reluctant to give
them up. There are several simple reasons for
this: 1) TMs are useful; 2) TMs represent con-
siderable effort and investment by a company or
(even more so) an individual translator; 3) the
fuzzy match score used in TMs offers a good ap-
proximation of post-editing effort, which is useful
both for translators and translation cost estimation
and, 4) current SMT translation confidence esti-
mation measures are not as robust as TM fuzzy
match scores and professional translators are thus
not ready to replace fuzzy match scores with SMT
internal quality measures.
There has been some research to address this is-
sue, see e.g. (Specia et al, 2009a) and (Specia et
al., 2009b). However, to date most of the research
has focused on better confidence measures for MT,
e.g. based on training regression models to per-
form confidence estimation on scores assigned by
post-editors (cf. Section 2).
In this paper, we try to address the problem
from a different perspective. Given that most post-
editing work is (still) based on TM output, we pro-
pose to recommend MT outputs which are better
than TM hits to post-editors. In this framework,
post-editors still work with the TM while benefit-
ing from (better) SMT outputs; the assets in TMs
are not wasted and TM fuzzy match scores can
still be used to estimate (the upper bound of) post-
editing labor.
There are three specific goals we need to
achieve within this framework. Firstly, the rec-
ommendation should have high precision, other-
wise it would be confusing for post-editors and
may negatively affect the lower bound of the post-
editing effort. Secondly, although we have full
access to the SMT system used in this paper,
our method should be able to generalize to cases
where SMT is treated as a black-box, which is of-
622
ten the case in the translation industry. Finally,
post-editors should be able to easily adjust the rec-
ommendation threshold to particular requirements
without having to retrain the model.
In our framework, we recast translation recom-
mendation as a binary classification (rather than
regression) problem using SVMs, perform RBF
kernel parameter optimization, employ posterior
probability-based confidence estimation to sup-
port user-based tuning for precision and recall, ex-
periment with feature sets involvingMT-, TM- and
system-independent features, and use automatic
MT evaluation metrics to simulate post-editing ef-
fort.
The rest of the paper is organized as follows: we
first briefly introduce related research in Section 2,
and review the classification SVMs in Section 3.
We formulate the classification model in Section 4
and present experiments in Section 5. In Section
6, we analyze the post-editing effort approximated
by the TER metric (Snover et al, 2006). Section
7 concludes the paper and points out avenues for
future research.
2 Related Work
Previous research relating to this work mainly fo-
cuses on predicting the MT quality.
The first strand is confidence estimation for MT,
initiated by (Ueffing et al, 2003), in which pos-
terior probabilities on the word graph or N-best
list are used to estimate the quality of MT out-
puts. The idea is explored more comprehensively
in (Blatz et al, 2004). These estimations are often
used to rerank the MT output and to optimize it
directly. Extensions of this strand are presented
in (Quirk, 2004) and (Ueffing and Ney, 2005).
The former experimented with confidence esti-
mation with several different learning algorithms;
the latter uses word-level confidence measures to
determine whether a particular translation choice
should be accepted or rejected in an interactive
translation system.
The second strand of research focuses on com-
bining TM information with an SMT system, so
that the SMT system can produce better target lan-
guage output when there is an exact or close match
in the TM (Simard and Isabelle, 2009). This line
of research is shown to help the performance of
MT, but is less relevant to our task in this paper.
A third strand of research tries to incorporate
confidence measures into a post-editing environ-
ment. To the best of our knowledge, the first paper
in this area is (Specia et al, 2009a). Instead of
modeling on translation quality (often measured
by automatic evaluation scores), this research uses
regression on both the automatic scores and scores
assigned by post-editors. The method is improved
in (Specia et al, 2009b), which applies Inductive
Confidence Machines and a larger set of features
to model post-editors? judgement of the translation
quality between ?good? and ?bad?, or among three
levels of post-editing effort.
Our research is more similar in spirit to the third
strand. However, we use outputs and features from
the TM explicitly; therefore instead of having to
solve a regression problem, we only have to solve
a much easier binary prediction problem which
can be integrated into TMs in a straightforward
manner. Because of this, the precision and recall
scores reported in this paper are not directly com-
parable to those in (Specia et al, 2009b) as the lat-
ter are computed on a pure SMT system without a
TM in the background.
3 Support Vector Machines for
Translation Quality Estimation
SVMs (Cortes and Vapnik, 1995) are binary clas-
sifiers that classify an input instance based on de-
cision rules which minimize the regularized error
function in (1):
min
w,b,?
1
2w
Tw + C
l
?
i=1
?i
s. t. yi(wT?(xi) + b) > 1? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {+1,?1} are l training
instances that are mapped by the function ? to a
higher dimensional space. w is the weight vec-
tor, ? is the relaxation variable and C > 0 is the
penalty parameter.
Solving SVMs is viable using the ?kernel
trick?: finding a kernel function K in (1) with
K(xi, xj) = ?(xi)T?(xj). We perform our ex-
periments with the Radial Basis Function (RBF)
kernel, as in (2):
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (2)
When using SVMs with the RBF kernel, we
have two free parameters to tune on: the cost pa-
rameter C in (1) and the radius parameter ? in (2).
In each of our experimental settings, the param-
eters C and ? are optimized by a brute-force grid
623
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
4 Translation Recommendation as
Binary Classification
We use an SVM binary classifier to predict the rel-
ative quality of the SMT output to make a recom-
mendation. The SVM classifier uses features from
the SMT system, the TM and additional linguis-
tic features to estimate whether the SMT output is
better than the hit from the TM.
4.1 Problem Formulation
As we treat translation recommendation as a bi-
nary classification problem, we have a pair of out-
puts from TM and MT for each sentence. Ideally
the classifier will recommend the output that needs
less post-editing effort. As large-scale annotated
data is not yet available for this task, we use auto-
matic TER scores (Snover et al, 2006) as the mea-
sure for the required post-editing effort. In the fu-
ture, we hope to train our system on HTER (TER
with human targeted references) scores (Snover et
al., 2006) once the necessary human annotations
are in place. In the meantime we use TER, as TER
is shown to have high correlation with HTER.
We label the training examples as in (3):
y =
{
+1 if TER(MT) < TER(TM)
?1 if TER(MT) ? TER(TM) (3)
Each instance is associated with a set of features
from both the MT and TM outputs, which are dis-
cussed in more detail in Section 4.3.
4.2 Recommendation Confidence Estimation
In classical settings involving SVMs, confidence
levels are represented as margins of binary predic-
tions. However, these margins provide little in-
sight for our application because the numbers are
only meaningful when compared to each other.
What is more preferable is a probabilistic confi-
dence score (e.g. 90% confidence) which is better
understood by post-editors and translators.
We use the techniques proposed by (Platt, 1999)
and improved by (Lin et al, 2007) to obtain the
posterior probability of a classification, which is
used as the confidence score in our system.
Platt?s method estimates the posterior probabil-
ity with a sigmod function, as in (4):
Pr(y = 1|x) ? PA,B(f) ?
1
1 + exp(Af + B) (4)
where f = f(x) is the decision function of the
estimated SVM. A and B are parameters that min-
imize the cross-entropy error function F on the
training data, as in Eq. (5):
min
z=(A,B)
F (z) = ?
l
?
i=1
(tilog(pi) + (1? ti)log(1? pi)),
where pi = PA,B(fi), and ti =
{N++1
N++2
if yi = +1
1
N?+2
if yi = ?1
(5)
where z = (A,B) is a parameter setting, and
N+ and N? are the numbers of observed positive
and negative examples, respectively, for the label
yi. These numbers are obtained using an internal
cross-validation on the training set.
4.3 The Feature Set
We use three types of features in classification: the
MT system features, the TM feature and system-
independent features.
4.3.1 The MT System Features
These features include those typically used in
SMT, namely the phrase-translation model scores,
the language model probability, the distance-based
reordering score, the lexicalized reordering model
scores, and the word penalty.
4.3.2 The TM Feature
The TM feature is the fuzzy match (Sikes, 2007)
cost of the TM hit. The calculation of fuzzy match
score itself is one of the core technologies in TM
systems and varies among different vendors. We
compute fuzzy match cost as the minimum Edit
Distance (Levenshtein, 1966) between the source
and TM entry, normalized by the length of the
source as in (6), as most of the current implemen-
tations are based on edit distance while allowing
some additional flexible matching.
hfm(t) = min
e
EditDistance(s, e)
Len(s) (6)
where s is the source side of t, the sentence to
translate, and e is the source side of an entry in the
TM. For fuzzy match scores F , this fuzzy match
cost hfm roughly corresponds to 1?F . The differ-
ence in calculation does not influence classifica-
tion, and allows direct comparison between a pure
TM system and a translation recommendation sys-
tem in Section 5.4.2.
624
4.3.3 System-Independent Features
We use several features that are independent of
the translation system, which are useful when a
third-party translation service is used or the MT
system is simply treated as a black-box. These
features are source and target side LM scores,
pseudo source fuzzy match scores and IBM model
1 scores.
Source-Side Language Model Score and Per-
plexity. We compute the language model (LM)
score and perplexity of the input source sentence
on a LM trained on the source-side training data of
the SMT system. The inputs that have lower per-
plexity or higher LM score are more similar to the
dataset on which the SMT system is built.
Target-Side Language Model Perplexity. We
compute the LM probability and perplexity of the
target side as a measure of fluency. Language
model perplexity of the MT outputs are calculated,
and LM probability is already part of the MT sys-
tems scores. LM scores on TM outputs are also
computed, though they are not as informative as
scores on the MT side, since TM outputs should
be grammatically perfect.
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM system performs
well enough, these two sentences should be the
same or very similar. Therefore, the fuzzy match
score here gives an estimation of the confidence
level of the output. We compute this score for both
the MT output and the TM hit.
The IBM Model 1 Score. The fuzzy match
score does not measure whether the hit could be
a correct translation, i.e. it does not take into ac-
count the correspondence between the source and
target, but rather only the source-side information.
For the TM hit, the IBM Model 1 score (Brown
et al, 1993) serves as a rough estimation of how
good a translation it is on the word level; for the
MT output, on the other hand, it is a black-box
feature to estimate translation quality when the in-
formation from the translation model is not avail-
able. We compute bidirectional (source-to-target
and target-to-source) model 1 scores on both TM
and MT outputs.
5 Experiments
5.1 Experimental Settings
Our raw data set is an English?French translation
memory with technical translation from Syman-
tec, consisting of 51K sentence pairs. We ran-
domly selected 43K to train an SMT system and
translated the English side of the remaining 8K
sentence pairs. The average sentence length of
the training set is 13.5 words and the size of the
training set is comparable to the (larger) TMs used
in the industry. Note that we remove the exact
matches in the TM from our dataset, because ex-
act matches will be reused and not presented to the
post-editor in a typical TM setting.
As for the SMT system, we use a stan-
dard log-linear PB-SMT model (Och and Ney,
2002): GIZA++ implementation of IBM word
alignment model 4,1 the refinement and phrase-
extraction heuristics described in (Koehn et
al., 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained with
SRILM (Stolcke, 2002) on the English side of the
training data, and Moses (Koehn et al, 2007) to
decode. We train a system in the opposite direc-
tion using the same data to produce the pseudo-
source sentences.
We train the SVM classifier using the lib-
SVM (Chang and Lin, 2001) toolkit. The SVM-
training and testing is performed on the remaining
8K sentences with 4-fold cross validation. We also
report 95% confidence intervals.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 4-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameterC in (1) we search in the range
[2?5, 215], and for parameter ? (2) we search in the
range [2?15, 23]. The step size is 2 on the expo-
nent.
5.2 The Evaluation Metrics
We measure the quality of the classification by
precision and recall. Let A be the set of recom-
mended MT outputs, and B be the set of MT out-
puts that have lower TER than TM hits. We stan-
dardly define precision P , recall R and F-value as
in (7):
1More specifically, we performed 5 iterations of Model 1,
5 iterations of HMM, 3 iterations of Model 3, and 3 iterations
of Model 4.
625
P = |A
?
B|
|A| , R =
|A
?
B|
|B| and F =
2PR
P + R (7)
5.3 Recommendation Results
In Table 1, we report recommendation perfor-
mance using MT and TM system features (SYS),
system features plus system-independent features
(ALL:SYS+SI), and system-independent features
only (SI).
Table 1: Recommendation Results
Precision Recall F-Score
SYS 82.53?1.17 96.44?0.68 88.95?.56
SI 82.56?1.46 95.83?0.52 88.70?.65
ALL 83.45?1.33 95.56?1.33 89.09?.24
From Table 1, we observe that MT and TM
system-internal features are very useful for pro-
ducing a stable (as indicated by the smaller con-
fidence interval) recommendation system (SYS).
Interestingly, only using some simple system-
external features as described in Section 4.3.3 can
also yield a system with reasonably good per-
formance (SI). We expect that the performance
can be further boosted by adding more syntactic
and semantic features. Combining all the system-
internal and -external features leads to limited
gains in Precision and F-score compared to using
only system-internal features (SYS) only. This in-
dicates that at the default confidence level, current
system-external (resp. system-internal) features
can only play a limited role in informing the sys-
tem when current system-internal (resp. system-
external) features are available. We show in Sec-
tion 5.4.2 that combing both system-internal and -
external features can yield higher, more stable pre-
cision when adjusting the confidence levels of the
classifier. Additionally, the performance of system
SI is promising given the fact that we are using
only a limited number of simple features, which
demonstrates a good prospect of applying our rec-
ommendation system to MT systems where we do
not have access to their internal features.
5.4 Further Improving Recommendation
Precision
Table 1 shows that classification recall is very
high, which suggests that precision can still be im-
proved, even though the F-score is not low. Con-
sidering that TM is the dominant technology used
by post-editors, a recommendation to replace the
hit from the TM would require more confidence,
i.e. higher precision. Ideally our aim is to obtain
a level of 0.9 precision at the cost of some recall,
if necessary. We propose two methods to achieve
this goal.
5.4.1 Classifier Margins
We experiment with different margins on the train-
ing data to tune precision and recall in order to
obtain a desired balance. In the basic case, the
training example would be marked as in (3). If we
label both the training and test sets with this rule,
the accuracy of the prediction will be maximized.
We try to achieve higher precision by enforc-
ing a larger bias towards negative examples in the
training set so that some borderline positive in-
stances would actually be labeled as negative, and
the classifier would have higher precision in the
prediction stage as in (8).
y =
{
+1 if TER(SMT) + b < TER(TM)
?1 if TER(SMT) + b > TER(TM)
(8)
We experiment with b in [0, 0.25] usingMT sys-
tem features and TM features. Results are reported
in Table 2.
Table 2: Classifier margins
Precision Recall
TER+0 83.45?1.33 95.56?1.33
TER+0.05 82.41?1.23 94.41?1.01
TER+0.10 84.53?0.98 88.81?0.89
TER+0.15 85.24?0.91 87.08?2.38
TER+0.20 87.59?0.57 75.86?2.70
TER+0.25 89.29?0.93 66.67?2.53
The highest accuracy and F-value is achieved
by TER + 0, as all other settings are trained
on biased margins. Except for a small drop in
TER+0.05, other configurations all obtain higher
precision than TER+ 0. We note that we can ob-
tain 0.85 precision without a big sacrifice in recall
with b=0.15, but for larger improvements on pre-
cision, recall will drop more rapidly.
When we use b beyond 0.25, the margin be-
comes less reliable, as the number of positive
examples becomes too small. In particular, this
causes the SVM parameters we tune on in the first
fold to become less applicable to the other folds.
This is one limitation of using biased margins to
626
obtain high precision. The method presented in
Section 5.4.2 is less influenced by this limitation.
5.4.2 Adjusting Confidence Levels
An alternative to using a biased margin is to output
a confidence score during prediction and to thresh-
old on the confidence score. It is also possible to
add this method to the SVM model trained with a
biased margin.
We use the SVM confidence estimation tech-
niques in Section 4.2 to obtain the confidence
level of the recommendation, and change the con-
fidence threshold for recommendation when nec-
essary. This also allows us to compare directly
against a simple baseline inspired by TM users. In
a TM environment, some users simply ignore TM
hits below a certain fuzzy match score F (usually
from 0.7 to 0.8). This fuzzy match score reflects
the confidence of recommending the TM hits. To
obtain the confidence of recommending an SMT
output, our baseline (FM) uses fuzzy match costs
hFM ? 1?F (cf. Section 4.3.2) for the TM hits as
the level of confidence. In other words, the higher
the fuzzy match cost of the TM hit is (lower fuzzy
match score), the higher the confidence of recom-
mending the SMT output. We compare this base-
line with the three settings in Section 5.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
Pre
cis
ion
Confidence
SISysAllFM
Figure 1: Precision Changes with Confidence
Level
Figure 1 shows that the precision curve of FM
is low and flat when the fuzzy match costs are
low (from 0 to 0.6), indicating that it is unwise to
recommend an SMT output when the TM hit has
a low fuzzy match cost (corresponding to higher
fuzzy match score, from 0.4 to 1). We also observe
that the precision of the recommendation receives
a boost when the fuzzy match costs for the TM
hits are above 0.7 (fuzzy match score lower than
0.3), indicating that SMT output should be recom-
mended when the TM hit has a high fuzzy match
cost (low fuzzy match score). With this boost, the
precision of the baseline system can reach 0.85,
demonstrating that a proper thresholding of fuzzy
match scores can be used effectively to discrimi-
nate the recommendation of the TM hit from the
recommendation of the SMT output.
However, using the TM information only does
not always find the easiest-to-edit translation. For
example, an excellent SMT output should be rec-
ommended even if there exists a good TM hit (e.g.
fuzzy match score is 0.7 or more). On the other
hand, a misleading SMT output should not be rec-
ommended if there exists a poor but useful TM
match (e.g. fuzzy match score is 0.2).
Our system is able to tackle these complica-
tions as it incorporates features from the MT and
the TM systems simultaneously. Figure 1 shows
that both the SYS and the ALL setting consistently
outperform FM, indicating that our classification
scheme can better integrate the MT output into the
TM system than this naive baseline.
The SI feature set does not perform well when
the confidence level is set above 0.85 (cf. the de-
scending tail of the SI curve in Figure 1). This
might indicate that this feature set is not reliable
enough to extract the best translations. How-
ever, when the requirement on precision is not that
high, and the MT-internal features are not avail-
able, it would still be desirable to obtain transla-
tion recommendations with these black-box fea-
tures. The difference between SYS and ALL is
generally small, but ALL performs steadily better
in [0.5, 0,8].
Table 3: Recall at Fixed Precision
Recall
SYS @85PREC 88.12?1.32
SYS @90PREC 52.73?2.31
SI @85PREC 87.33?1.53
ALL @85PREC 88.57?1.95
ALL @90PREC 51.92?4.28
5.5 Precision Constraints
In Table 3 we also present the recall scores at 0.85
and 0.9 precision for SYS, SI and ALL models to
demonstrate our system?s performance when there
is a hard constraint on precision. Note that our
system will return the TM entry when there is an
exact match, so the overall precision of the system
627
is above the precision score we set here in a ma-
ture TM environment, as a significant portion of
the material to be translated will have a complete
match in the TM system.
In Table 3 for MODEL@K, the recall scores are
achieved when the prediction precision is better
than K with 0.95 confidence. For each model, pre-
cision at 0.85 can be obtained without a very big
loss on recall. However, if we want to demand
further recommendation precision (more conser-
vative in recommending SMT output), the recall
level will begin to drop more quickly. If we use
only system-independent features (SI), we cannot
achieve as high precision as with other models
even if we sacrifice more recall.
Based on these results, the users of the TM sys-
tem can choose between precision and recall ac-
cording to their own needs. As the threshold does
not involve training of the SMT system or the
SVM classifier, the user is able to determine this
trade-off at runtime.
Table 4: Contribution of Features
Precision Recall F Score
SYS 82.53?1.17 96.44?0.68 88.95?.56
+M1 82.87?1.26 96.23?0.53 89.05?.52
+LM 82.82?1.16 96.20?1.14 89.01?.23
+PS 83.21?1.33 96.61?0.44 89.41?.84
5.6 Contribution of Features
In Section 4.3.3 we suggested three sets of
system-independent features: features based on
the source- and target-side language model (LM),
the IBMModel 1 (M1) and the fuzzy match scores
on pseudo-source (PS). We compare the contribu-
tion of these features in Table 4.
In sum, all the three sets of system-independent
features improve the precision and F-scores of the
MT and TM system features. The improvement
is not significant, but improvement on every set of
system-independent features gives some credit to
the capability of SI features, as does the fact that
SI features perform close to SYS features in Table
1.
6 Analysis of Post-Editing Effort
A natural question on the integration models is
whether the classification reduces the effort of the
translators and post-editors: after reading these
recommendations, will they translate/edit less than
they would otherwise have to? Ideally this ques-
tion would be answered by human post-editors in
a large-scale experimental setting. As we have
not yet conducted a manual post-editing experi-
ment, we conduct two sets of analyses, trying to
show which type of edits will be required for dif-
ferent recommendation confidence levels. We also
present possible methods for human evaluation at
the end of this section.
6.1 Edit Statistics
We provide the statistics of the number of edits
for each sentence with 0.95 confidence intervals,
sorted by TER edit types. Statistics of positive in-
stances in classification (i.e. the instances in which
MT output is recommended over the TM hit) are
given in Table 5.
When an MT output is recommended, its TM
counterpart will require a larger average number
of total edits than the MT output, as we expect. If
we drill down, however, we also observe that many
of the saved edits come from the Substitution cat-
egory, which is the most costly operation from the
post-editing perspective. In this case, the recom-
mended MT output actually saves more effort for
the editors than what is shown by the TER score.
It reflects the fact that TM outputs are not actual
translations, and might need heavier editing.
Table 6 shows the statistics of negative instances
in classification (i.e. the instances in which MT
output is not recommended over the TM hit). In
this case, the MT output requires considerably
more edits than the TM hits in terms of all four
TER edit types, i.e. insertion, substitution, dele-
tion and shift. This reflects the fact that some high
quality TM matches can be very useful as a trans-
lation.
6.2 Edit Statistics on Recommendations of
Higher Confidence
We present the edit statistics of recommendations
with higher confidence in Table 7. Comparing Ta-
bles 5 and 7, we see that if recommended with
higher confidence, the MT output will need sub-
stantially less edits than the TM output: e.g. 3.28
fewer substitutions on average.
From the characteristics of the high confidence
recommendations, we suspect that these mainly
comprise harder to translate (i.e. different from
the SMT training set/TM database) sentences, as
indicated by the slightly increased edit operations
628
Table 5: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.5
Insertion Substitution Deletion Shift
MT 0.9849 ? 0.0408 2.2881 ? 0.0672 0.8686 ? 0.0370 1.2500 ? 0.0598
TM 0.7762 ? 0.0408 4.5841 ? 0.1036 3.1567 ? 0.1120 1.2096 ? 0.0554
Table 6: Edit Statistics when NOT Recommending MT Outputs in Classification, confidence=0.5
Insertion Substitution Deletion Shift
MT 1.0830 ? 0.1167 2.2885 ? 0.1376 1.0964 ? 0.1137 1.5381 ? 0.1962
TM 0.7554 ? 0.0376 1.5527 ? 0.1584 1.0090 ? 0.1850 0.4731 ? 0.1083
Table 7: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.85
Insertion Substitution Deletion Shift
MT 1.1665 ? 0.0615 2.7334 ? 0.0969 1.0277 ? 0.0544 1.5549 ? 0.0899
TM 0.8894 ? 0.0594 6.0085 ? 0.1501 4.1770 ? 0.1719 1.6727 ? 0.0846
on the MT side. TM produces much worse edit-
candidates for such sentences, as indicated by
the numbers in Table 7, since TM does not have
the ability to automatically reconstruct an output
through the combination of several segments.
6.3 Plan for Human Evaluation
Evaluation with human post-editors is crucial to
validate and improve translation recommendation.
There are two possible avenues to pursue:
? Test our system on professional post-editors.
By providing them with the TM output, the
MT output and the one recommended to edit,
we can measure the true accuracy of our
recommendation, as well as the post-editing
time we save for the post-editors;
? Apply the presented method on open do-
main data and evaluate it using crowd-
sourcing. It has been shown that crowd-
sourcing tools, such as the Amazon Me-
chanical Turk (Callison-Burch, 2009), can
help developers to obtain good human judge-
ments on MT output quality both cheaply and
quickly. Given that our problem is related to
MT quality estimation in nature, it can poten-
tially benefit from such tools as well.
7 Conclusions and Future Work
In this paper we present a classification model to
integrate SMT into a TM system, in order to facili-
tate the work of post-editors. Insodoing we handle
the problem of MT quality estimation as binary
prediction instead of regression. From the post-
editors? perspective, they can continue to work in
their familiar TM environment, use the same cost-
estimation methods, and at the same time bene-
fit from the power of state-of-the-art MT. We use
SVMs to make these predictions, and use grid
search to find better RBF kernel parameters.
We explore features from inside the MT sys-
tem, from the TM, as well as features that make
no assumption on the translation model for the bi-
nary classification. With these features we make
glass-box and black-box predictions. Experiments
show that the models can achieve 0.85 precision at
a level of 0.89 recall, and even higher precision if
we sacrifice more recall. With this guarantee on
precision, our method can be used in a TM envi-
ronment without changing the upper-bound of the
related cost estimation.
Finally, we analyze the characteristics of the in-
tegrated outputs. We present results to show that,
if measured by number, type and content of ed-
its in TER, the recommended sentences produced
by the classification model would bring about less
post-editing effort than the TM outputs.
This work can be extended in the following
ways. Most importantly, it is useful to test the
model in user studies, as proposed in Section 6.3.
A user study can serve two purposes: 1) it can
validate the effectiveness of the method by mea-
suring the amount of edit effort it saves; and 2)
the byproduct of the user study ? post-edited sen-
tences ? can be used to generate HTER scores
to train a better recommendation model. Further-
more, we want to experiment and improve on the
adaptability of this method, as the current experi-
ment is on a specific domain and language pair.
629
Acknowledgements
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank Symantec for providing the TM database and the
anonymous reviewers for their insightful comments.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for ma-
chine translation. In The 20th International Conference
on Computational Linguistics (Coling-2004), pages 315 ?
321, Geneva, Switzerland.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263 ? 311.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Mechani-
cal Turk. In The 2009 Conference on Empirical Methods
in Natural Language Processing (EMNLP-2009), pages
286 ? 295, Singapore.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
?cjlin/libsvm.
Corinna Cortes and Vladimir Vapnik. 1995. Support-vector
networks. Machine learning, 20(3):273 ? 297.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In The 1995 International
Conference on Acoustics, Speech, and Signal Processing
(ICASSP-95), pages 181 ? 184, Detroit, MI.
Philipp. Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In The 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technol-
ogy (NAACL/HLT-2003), pages 48 ? 54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In The 45th Annual Meet-
ing of the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster Ses-
sions (ACL-2007), pages 177 ? 180, Prague, Czech Re-
public.
Vladimir Iosifovich Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10(8):707 ? 710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vector
machines. Machine Learning, 68(3):267 ? 276.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics (ACL-
2002), pages 295 ? 302, Philadelphia, PA.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In The 41st Annual Meet-
ing on Association for Computational Linguistics (ACL-
2003), pages 160 ? 167.
John C. Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood meth-
ods. Advances in Large Margin Classifiers, pages 61 ? 74.
Christopher B. Quirk. 2004. Training a sentence-level ma-
chine translation confidence measure. In The Fourth In-
ternational Conference on Language Resources and Eval-
uation (LREC-2004), pages 825 ? 828, Lisbon, Portugal.
Richard Sikes. 2007. Fuzzy matching in theory and practice.
Multilingual, 18(6):39 ? 43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation en-
vironment. In The Twelfth Machine Translation Sum-
mit (MT Summit XII), pages 120 ? 127, Ottawa, Ontario,
Canada.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In The 2006
conference of the Association for Machine Translation in
the Americas (AMTA-2006), pages 223 ? 231, Cambridge,
MA.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco
Turchi, and Nello Cristianini. 2009a. Estimating the
sentence-level quality of machine translation systems. In
The 13th Annual Conference of the European Association
for Machine Translation (EAMT-2009), pages 28 ? 35,
Barcelona, Spain.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang,
and John Shawe-Taylor. 2009b. Improving the confidence
of machine translation quality estimates. In The Twelfth
Machine Translation Summit (MT Summit XII), pages 136
? 143, Ottawa, Ontario, Canada.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In The Seventh International Confer-
ence on Spoken Language Processing, volume 2, pages
901 ? 904, Denver, CO.
Nicola Ueffing and Hermann Ney. 2005. Application
of word-level confidence measures in interactive statisti-
cal machine translation. In The Ninth Annual Confer-
ence of the European Association for Machine Translation
(EAMT-2005), pages 262 ? 270, Budapest, Hungary.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation.
In The Ninth Machine Translation Summit (MT Summit
IX), pages 394 ? 401, New Orleans, LA.
630
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1239?1248,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Consistent Translation using Discriminative Learning:
A Translation Memory-inspired Approach?
Yanjun Ma? Yifan He? Andy Way? Josef van Genabith?
? Baidu Inc., Beijing, China
yma@baidu.com
?Centre for Next Generation Localisation
School of Computing, Dublin City University
{yhe,away,josef}@computing.dcu.ie
Abstract
We present a discriminative learning method
to improve the consistency of translations in
phrase-based Statistical Machine Translation
(SMT) systems. Our method is inspired by
Translation Memory (TM) systems which are
widely used by human translators in industrial
settings. We constrain the translation of an in-
put sentence using the most similar ?transla-
tion example? retrieved from the TM. Differ-
ently from previous research which used sim-
ple fuzzy match thresholds, these constraints
are imposed using discriminative learning to
optimise the translation performance. We ob-
serve that using this method can benefit the
SMT system by not only producing consis-
tent translations, but also improved translation
outputs. We report a 0.9 point improvement
in terms of BLEU score on English?Chinese
technical documents.
1 Introduction
Translation consistency is an important factor
for large-scale translation, especially for domain-
specific translations in an industrial environment.
For example, in the translation of technical docu-
ments, lexical as well as structural consistency is es-
sential to produce a fluent target-language sentence.
Moreover, even in the case of translation errors, con-
sistency in the errors (e.g. repetitive error patterns)
are easier to diagnose and subsequently correct by
translators.
?This work was done while the first author was in the Cen-
tre for Next Generation Localisation at Dublin City University.
In phrase-based SMT, translation models and lan-
guage models are automatically learned and/or gen-
eralised from the training data, and a translation is
produced by maximising a weighted combination of
these models. Given that global contextual informa-
tion is not normally incorporated, and that training
data is usually noisy in nature, there is no guaran-
tee that an SMT system can produce translations in
a consistent manner.
On the other hand, TM systems ? widely used by
translators in industrial environments for enterprise
localisation by translators ? can shed some light on
mitigating this limitation. TM systems can assist
translators by retrieving and displaying previously
translated similar ?example? sentences (displayed as
source-target pairs, widely called ?fuzzy matches? in
the localisation industry (Sikes, 2007)). In TM sys-
tems, fuzzy matches are retrieved by calculating the
similarity or the so-called ?fuzzy match score? (rang-
ing from 0 to 1 with 0 indicating no matches and 1
indicating a full match) between the input sentence
and sentences in the source side of the translation
memory.
When presented with fuzzy matches, translators
can then avail of useful chunks in previous transla-
tions while composing the translation of a new sen-
tence. Most translators only consider a few sen-
tences that are most similar to the current input sen-
tence; this process can inherently improve the con-
sistency of translation, given that the new transla-
tions produced by translators are likely to be similar
to the target side of the fuzzy match they have con-
sulted.
Previous research as discussed in detail in Sec-
1239
tion 2 has focused on using fuzzy match score as
a threshold when using the target side of the fuzzy
matches to constrain the translation of the input
sentence. In our approach, we use a more fine-
grained discriminative learning method to determine
whether the target side of the fuzzy matches should
be used as a constraint in translating the input sen-
tence. We demonstrate that our method can consis-
tently improve translation quality.
The rest of the paper is organized as follows:
we begin by briefly introducing related research in
Section 2. We present our discriminative learning
method for consistent translation in Section 3 and
our feature design in Section 4. We report the exper-
imental results in Section 5 and conclude the paper
and point out avenues for future research in Section
6.
2 Related Research
Despite the fact that TM and MT integration has
long existed as a major challenge in the localisation
industry, it has only recently received attention in
main-stream MT research. One can loosely combine
TM and MT at sentence (called segments in TMs)
level by choosing one of them (or both) to recom-
mend to the translators using automatic classifiers
(He et al, 2010), or simply using fuzzy match score
or MT confidence measures (Specia et al, 2009).
One can also tightly integrate TM with MT at the
sub-sentence level. The basic idea is as follows:
given a source sentence to translate, we firstly use
a TM system to retrieve the most similar ?example?
source sentences together with their translations. If
matched chunks between input sentence and fuzzy
matches can be detected, we can directly re-use the
corresponding parts of the translation in the fuzzy
matches, and use an MT system to translate the re-
maining chunks.
As a matter of fact, implementing this idea is
pretty straightforward: a TM system can easily de-
tect the word alignment between the input sentence
and the source side of the fuzzy match by retracing
the paths used in calculating the fuzzy match score.
To obtain the translation for the matched chunks, we
just require the word alignment between source and
target TM matches, which can be addressed using
state-of-the-art word alignment techniques. More
importantly, albeit not explicitly spelled out in pre-
vious work, this method can potentially increase the
consistency of translation, as the translation of new
input sentences is closely informed and guided (or
constrained) by previously translated sentences.
There are several different ways of using the
translation information derived from fuzzy matches,
with the following two being the most widely
adopted: 1) to add these translations into a phrase
table as in (Bic?ici and Dymetman, 2008; Simard and
Isabelle, 2009), or 2) to mark up the input sentence
using the relevant chunk translations in the fuzzy
match, and to use an MT system to translate the parts
that are not marked up, as in (Smith and Clark, 2009;
Koehn and Senellart, 2010; Zhechev and van Gen-
abith, 2010). It is worth mentioning that translation
consistency was not explicitly regarded as their pri-
mary motivation in this previous work. Our research
follows the direction of the second strand given that
consistency can no longer be guaranteed by con-
structing another phrase table.
However, to categorically reuse the translations
of matched chunks without any differentiation could
generate inferior translations given the fact that the
context of these matched chunks in the input sen-
tence could be completely different from the source
side of the fuzzy match. To address this problem,
both (Koehn and Senellart, 2010) and (Zhechev and
van Genabith, 2010) used fuzzy match score as a
threshold to determine whether to reuse the transla-
tions of the matched chunks. For example, (Koehn
and Senellart, 2010) showed that reusing these trans-
lations as large rules in a hierarchical system (Chi-
ang, 2005) can be beneficial when the fuzzy match
score is above 70%, while (Zhechev and van Gen-
abith, 2010) reported that it is only beneficial to a
phrase-based system when the fuzzy match score is
above 90%.
Despite being an informative measure, using
fuzzy match score as a threshold has a number of
limitations. Given the fact that fuzzy match score
is normally calculated based on Edit Distance (Lev-
enshtein, 1966), a low score does not necessarily
imply that the fuzzy match is harmful when used
to constrain an input sentence. For example, in
longer sentences where fuzzy match scores tend to
be low, some chunks and the corresponding trans-
lations within the sentences can still be useful. On
1240
the other hand, a high score cannot fully guarantee
the usefulness of a particular translation. We address
this problem using discriminative learning.
3 Constrained Translation with
Discriminative Learning
3.1 Formulation of the Problem
Given a sentence e to translate, we retrieve the most
similar sentence e? from the translation memory as-
sociated with target translation f ?. The m com-
mon ?phrases? e?m1 between e and e? can be iden-
tified. Given the word alignment information be-
tween e? and f ?, one can easily obtain the corre-
sponding translations f? ?m1 for each of the phrases in
e?m1 . This process can derive a number of ?phrase
pairs? < e?m, f? ?m >, which can be used to specify
the translations of the matched phrases in the input
sentence. The remaining words without specified
translations will be translated by an MT system.
For example, given an input sentence e1e2 ? ? ?
eiei+1 ? ? ? eI , and a phrase pair < e?, f? ? >, e? =
eiei+1, f? ? = f ?jf
?
j+1 derived from the fuzzy match,
we can mark up the input sentence as:
e1e2 ? ? ? <tm=?f ?jf ?j+1?> eiei+1 < /tm> ? ? ? eI .
Our method to constrain the translations using
TM fuzzy matches is similar to (Koehn and Senel-
lart, 2010), except that the word alignment between
e? and f ? is the intersection of bidirectional GIZA++
(Och and Ney, 2003) posterior alignments. We use
the intersected word alignment to minimise the noise
introduced by word alignment of only one direction
in marking up the input sentence.
3.2 Discriminative Learning
Whether the translation information from the fuzzy
matches should be used or not (i.e. whether the input
sentence should be marked up) is determined using
a discriminative learning procedure. The translation
information refers to the ?phrase pairs? derived us-
ing the method described in Section 3.1. We cast
this problem as a binary classification problem.
3.2.1 Support Vector Machines
SVMs (Cortes and Vapnik, 1995) are binary classi-
fiers that classify an input instance based on decision
rules which minimise the regularised error function
in (1):
min
w,b,?
1
2
wT w + C
l
?
i=1
?i
s. t. yi(wT?(xi) + b) > 1? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {+1,?1} are l training in-
stances that are mapped by the function ? to a higher
dimensional space. w is the weight vector, ? is the
relaxation variable and C > 0 is the penalty param-
eter.
Solving SVMs is viable using a kernel function
K in (1) with K(xi, xj) = ?(xi)T?(xj). We per-
form our experiments with the Radial Basis Func-
tion (RBF) kernel, as in (2):
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (2)
When using SVMs with the RBF kernel, we have
two free parameters to tune on: the cost parameter
C in (1) and the radius parameter ? in (2).
In each of our experimental settings, the param-
eters C and ? are optimised by a brute-force grid
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
The SVM classifier will thus be able to predict
the usefulness of the TM fuzzy match, and deter-
mine whether the input sentence should be marked
up using relevant phrase pairs derived from the fuzzy
match before sending it to the SMT system for trans-
lation. The classifier uses features such as the fuzzy
match score, the phrase and lexical translation prob-
abilities of these relevant phrase pairs, and addi-
tional syntactic dependency features. Ideally the
classifier will decide to mark up the input sentence
if the translations of the marked phrases are accurate
when taken contextual information into account. As
large-scale manually annotated data is not available
for this task, we use automatic TER scores (Snover
et al, 2006) as the measure for training data annota-
tion.
We label the training examples as in (3):
y =
{
+1 if TER(w. markup) < TER(w/o markup)
?1 if TER(w/o markup) ? TER(w. markup)
(3)
Each instance is associated with a set of features
which are discussed in more detail in Section 4.
1241
3.2.2 Classification Confidence Estimation
We use the techniques proposed by (Platt, 1999) and
improved by (Lin et al, 2007) to convert classifica-
tion margin to posterior probability, so that we can
easily threshold our classifier (cf. Section 5.4.2).
Platt?s method estimates the posterior probability
with a sigmoid function, as in (4):
Pr(y = 1|x) ? PA,B(f) ?
1
1 + exp(Af + B)
(4)
where f = f(x) is the decision function of the esti-
mated SVM. A and B are parameters that minimise
the cross-entropy error function F on the training
data, as in (5):
min
z=(A,B)
F (z) = ?
l
?
i=1
(tilog(pi) + (1 ? ti)log(1? pi)),
where pi = PA,B(fi), and ti =
{
N++1
N++2 if yi = +1
1
N?+2 if yi = ?1
(5)
where z = (A,B) is a parameter setting, and
N+ and N? are the numbers of observed positive
and negative examples, respectively, for the label yi.
These numbers are obtained using an internal cross-
validation on the training set.
4 Feature Set
The features used to train the discriminative classi-
fier, all on the sentence level, are described in the
following sections.
4.1 The TM Feature
The TM feature is the fuzzy match score, which in-
dicates the overall similarity between the input sen-
tence and the source side of the TM output. If the
input sentence is similar to the source side of the
matching segment, it is more likely that the match-
ing segment can be used to mark up the input sen-
tence.
The calculation of the fuzzy match score itself is
one of the core technologies in TM systems, and
varies among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Leven-
shtein, 1966) between the source and TM entry, nor-
malised by the length of the source as in (6), as
most of the current implementations are based on
edit distance while allowing some additional flexi-
ble matching.
hfm(e) = min
s
EditDistance(e, s)
Len(e)
(6)
where e is the sentence to translate, and s is the
source side of an entry in the TM. For fuzzy match
scores F , hfm roughly corresponds to 1? F .
4.2 Translation Features
We use four features related to translation probabil-
ities, i.e. the phrase translation and lexical probabil-
ities for the phrase pairs < e?m, f? ?m > derived us-
ing the method in Section 3.1. Specifically, we use
the phrase translation probabilities p(f? ?m|e?m) and
p(e?m|f? ?m), as well as the lexical translation prob-
abilities plex(f? ?m|e?m) and plex(e?m|f? ?m) as calcu-
lated in (Koehn et al, 2003). In cases where mul-
tiple phrase pairs are used to mark up one single
input sentence e, we use a unified score for each
of the four features, which is an average over the
corresponding feature in each phrase pair. The intu-
ition behind these features is as follows: phrase pairs
< e?m, f? ?m > derived from the fuzzy match should
also be reliable with respect to statistically produced
models.
We also have a count feature, i.e. the number of
phrases used to mark up the input sentence, and a
binary feature, i.e. whether the phrase table contains
at least one phrase pair < e?m, f? ?m > that is used to
mark up the input sentence.
4.3 Dependency Features
Given the phrase pairs < e?m, f? ?m > derived from
the fuzzy match, and used to translate the corre-
sponding chunks of the input sentence (cf. Sec-
tion 3.1), these translations are more likely to be co-
herent in the context of the particular input sentence
if the matched parts on the input side are syntacti-
cally and semantically related.
For matched phrases e?m between the input sen-
tence and the source side of the fuzzy match, we de-
fine the contextual information of the input side us-
ing dependency relations between words em in e?m
and the remaining words ej in the input sentence e.
We use the Stanford parser to obtain the depen-
dency structure of the input sentence. We add
a pseudo-label SYS PUNCT to punctuation marks,
whose governor and dependent are both the punc-
tuation mark. The dependency features designed to
capture the context of the matched input phrases e?m
are as follows:
1242
Coverage features measure the coverage of de-
pendency labels on the input sentence in order to
obtain a bigger picture of the matched parts in the
input. For each dependency label L, we consider its
head or modifier as covered if the corresponding in-
put word em is covered by a matched phrase e?m.
Our coverage features are the frequencies of gov-
ernor and dependent coverage calculated separately
for each dependency label.
Position features identify whether the head and
the tail of a sentence are matched, as these are the
cases in which the matched translation is not af-
fected by the preceding words (when it is the head)
or following words (when it is the tail), and is there-
fore more reliable. The feature is set to 1 if this hap-
pens, and to 0 otherwise. We distinguish among the
possible dependency labels, the head or the tail of
the sentence, and whether the aligned word is the
governor or the dependent. As a result, each per-
mutation of these possibilities constitutes a distinct
binary feature.
The consistency feature is a single feature which
determines whether matched phrases e?m belong to
a consistent dependency structure, instead of being
distributed discontinuously around in the input sen-
tence. We assume that a consistent structure is less
influenced by its surrounding context. We set this
feature to 1 if every word in e?m is dependent on an-
other word in e?m, and to 0 otherwise.
5 Experiments
5.1 Experimental Setup
Our data set is an English?Chinese translation mem-
ory with technical translation from Symantec, con-
sisting of 87K sentence pairs. The average sentence
length of the English training set is 13.3 words and
the size of the training set is comparable to the larger
TMs used in the industry. Detailed corpus statistics
about the training, development and test sets for the
SMT system are shown in Table 1.
The composition of test subsets based on fuzzy
match scores is shown in Table 2. We can see that
sentences in the test sets are longer than those in the
training data, implying a relatively difficult trans-
lation task. We train the SVM classifier using the
libSVM (Chang and Lin, 2001) toolkit. The SVM-
Train Develop Test
SENTENCES 86,602 762 943
ENG. TOKENS 1,148,126 13,955 20,786
ENG. VOC. 13,074 3,212 3,115
CHI. TOKENS 1,171,322 10,791 16,375
CHI. VOC. 12,823 3,212 1,431
Table 1: Corpus Statistics
Scores Sentences Words W/S
(0.9, 1.0) 80 1526 19.0750
(0.8, 0.9] 96 1430 14.8958
(0.7, 0.8] 110 1596 14.5091
(0.6, 0.7] 74 1031 13.9324
(0.5, 0.6] 104 1811 17.4135
(0, 0.5] 479 8972 18.7307
Table 2: Composition of test subsets based on fuzzy
match scores
training and validation is on the same training sen-
tences1 as the SMT system with 5-fold cross valida-
tion.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 5-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameter C in (1), we search in the range
[2?5, 215], while for parameter ? (2) we search in the
range [2?15, 23]. The step size is 2 on the exponent.
We conducted experiments using a standard log-
linear PB-SMT model: GIZA++ implementation of
IBM word alignment model 4 (Och and Ney, 2003),
the refinement and phrase-extraction heuristics de-
scribed in (Koehn et al, 2003), minimum-error-
rate training (Och, 2003), a 5-gram language model
with Kneser-Ney smoothing (Kneser and Ney, 1995)
trained with SRILM (Stolcke, 2002) on the Chinese
side of the training data, and Moses (Koehn et al,
2007) which is capable of handling user-specified
translations for some portions of the input during de-
coding. The maximum phrase length is set to 7.
5.2 Evaluation
The performance of the phrase-based SMT system
is measured by BLEU score (Papineni et al, 2002)
and TER (Snover et al, 2006). Significance test-
1We have around 87K sentence pairs in our training data.
However, for 67.5% of the input sentences, our MT system pro-
duces the same translation irrespective of whether the input sen-
tence is marked up or not.
1243
ing is carried out using approximate randomisation
(Noreen, 1989) with a 95% confidence level.
We also measure the quality of the classification
by precision and recall. Let A be the set of pre-
dicted markup input sentences, and B be the set
of input sentences where the markup version has a
lower TER score than the plain version. We stan-
dardly define precision P and recall R as in (7):
P =
|A?B|
|A| , R =
|A?B|
|B| (7)
5.3 Cross-fold translation
In order to obtain training samples for the classifier,
we need to label each sentence in the SMT training
data as to whether marking up the sentence can pro-
duce better translations. To achieve this, we translate
both the marked-up versions and plain versions of
the sentence and compare the two translations using
the sentence-level evaluation metric TER.
We do not make use of additional training data to
translate the sentences for SMT training, but instead
use cross-fold translation. We create a new training
corpus T by keeping 95% of the sentences in the
original training corpus, and creating a new test cor-
pus H by using the remaining 5% of the sentences.
Using this scheme we make 20 different pairs of cor-
pora (Ti,Hi) in such a way that each sentence from
the original training corpus is in exactly one Hi for
some 1 ? i ? 20. We train 20 different systems
using each Ti, and use each system to translate the
corresponding Hi as well as the marked-up version
of Hi using the procedure described in Section 3.1.
The development set is kept the same for all systems.
5.4 Experimental Results
5.4.1 Translation Results
Table 3 contains the translation results of the SMT
system when we use discriminative learning to mark
up the input sentence (MARKUP-DL). The first row
(BASELINE) is the result of translating plain test
sets without any markup, while the second row is
the result when all the test sentences are marked
up. We also report the oracle scores, i.e. the up-
perbound of using our discriminative learning ap-
proach. As we can see from this table, we obtain sig-
nificantly inferior results compared to the the Base-
line system if we categorically mark up all the in-
TER BLEU
BASELINE 39.82 45.80
MARKUP 41.62 44.41
MARKUP-DL 39.61 46.46
ORACLE 37.27 48.32
Table 3: Performance of Discriminative Learning (%)
put sentences using phrase pairs derived from fuzzy
matches. This is reflected by an absolute 1.4 point
drop in BLEU score and a 1.8 point increase in TER.
On the other hand, both the oracle BLEU and TER
scores represent as much as a 2.5 point improve-
ment over the baseline. Our discriminative learning
method (MARKUP-DL), which automatically clas-
sifies whether an input sentence should be marked
up, leads to an increase of 0.7 absolute BLEU points
over the BASELINE, which is statistically signifi-
cant. We also observe a slight decrease in TER com-
pared to the BASELINE. Despite there being much
room for further improvement when compared to the
Oracle score, the discriminative learning method ap-
pears to be effective not only in maintaining transla-
tion consistency, but also a statistically significant
improvement in translation quality.
5.4.2 Classification Confidence Thresholding
To further analyse our discriminative learning ap-
proach, we report the classification results on the test
set using the SVM classifier. We also investigate the
use of classification confidence, as described in Sec-
tion 3.2.2, as a threshold to boost classification pre-
cision if required. Table 4 shows the classification
and translation results when we use different con-
fidence thresholds. The default classification con-
fidence is 0.50, and the corresponding translation
results were described in Section 5.4.1. We inves-
tigate the impact of increasing classification confi-
dence on the performance of the classifier and the
translation results. As can be seen from Table 4,
increasing the classification confidence up to 0.70
leads to a steady increase in classification precision
with a corresponding sacrifice in recall. The fluc-
tuation in classification performance has an impact
on the translation results as measured by BLEU and
TER. We can see that the best BLEU as well as TER
scores are achieved when we set the classification
confidence to 0.60, representing a modest improve-
1244
Classification Confidence
0.50 0.55 0.60 0.65 0.70 0.75 0.80
BLEU 46.46 46.65 46.69 46.59 46.34 46.06 46.00
TER 39.61 39.46 39.32 39.36 39.52 39.71 39.71
P 60.00 68.67 70.31 74.47 72.97 64.28 88.89
R 32.14 29.08 22.96 17.86 13.78 9.18 4.08
Table 4: The impact of classification confidence thresholding
ment over the default setting (0.50). Despite the
higher precision when the confidence is set to 0.7,
the dramatic decrease in recall cannot be compen-
sated for by the increase in precision.
We can also observe from Table 4 that the recall
is quite low across the board, and the classification
results become unstable when we further increase
the level of confidence to above 0.70. This indicates
the degree of difficulty of this classification task, and
suggests some directions for future research as dis-
cussed at the end of this paper.
5.4.3 Comparison with Previous Work
As discussed in Section 2, both (Koehn and Senel-
lart, 2010) and (Zhechev and van Genabith, 2010)
used fuzzy match score to determine whether the in-
put sentences should be marked up. The input sen-
tences are only marked up when the fuzzy match
score is above a certain threshold. We present the
results using this method in Table 5. From this ta-
Fuzzy Match Scores
0.50 0.60 0.70 0.80 0.90
BLEU 45.13 45.55 45.58 45.84 45.82
TER 40.99 40.62 40.56 40.29 40.07
Table 5: Performance using fuzzy match score for classi-
fication
ble, we can see an inferior performance compared to
the BASELINE results (cf. Table 3) when the fuzzy
match score is below 0.70. A modest gain can only
be achieved when the fuzzy match score is above
0.8. This is slightly different from the conclusions
drawn in (Koehn and Senellart, 2010), where gains
are observed when the fuzzy match score is above
0.7, and in (Zhechev and van Genabith, 2010) where
gains are only observed when the score is above 0.9.
Comparing Table 5 with Table 4, we can see that
our classification method is more effective. This
confirms our argument in the last paragraph of Sec-
tion 2, namely that fuzzy match score is not informa-
tive enough to determine the usefulness of the sub-
sentences in a fuzzy match, and that a more compre-
hensive set of features, as we have explored in this
paper, is essential for the discriminative learning-
based method to work.
FM Scores w. markup w/o markup
[0,0.5] 37.75 62.24
(0.5,0.6] 40.64 59.36
(0.6,0.7] 40.94 59.06
(0.7,0.8] 46.67 53.33
(0.8,0.9] 54.28 45.72
(0.9,1.0] 44.14 55.86
Table 6: Percentage of training sentences with markup
vs without markup grouped by fuzzy match (FM) score
ranges
To further validate our assumption, we analyse
the training sentences by grouping them accord-
ing to their fuzzy match score ranges. For each
group of sentences, we calculate the percentage of
sentences where markup (and respectively without
markup) can produce better translations. The statis-
tics are shown in Table 6. We can see that for sen-
tences with fuzzy match scores lower than 0.8, more
sentences can be better translated without markup.
For sentences where fuzzy match scores are within
the range (0.8, 0.9], more sentences can be better
translated with markup. However, within the range
(0.9, 1.0], surprisingly, actually more sentences re-
ceive better translation without markup. This indi-
cates that fuzzy match score is not a good measure to
predict whether fuzzy matches are beneficial when
used to constrain the translation of an input sentence.
5.5 Contribution of Features
We also investigated the contribution of our differ-
ent feature sets. We are especially interested in
the contribution of dependency features, as they re-
1245
Example 1
w/o markup after policy name , type the name of the policy ( it shows new host integrity
policy by default ) .
Translation ????????????????? (????? ???????
??????
w. markup after policy name <tm translation=????????????? ??
?? ?????????>, type the name of the policy ( it shows new host
integrity policy by default ) .< /tm>
Translation ????????????????????? ???? ????????
Reference ????????????????????? ???? ????????
Example 2
w/o markup changes apply only to the specific scan that you select .
Translation ??????????????
w. markup changes apply only to the specific scan that you select <tm translation=???>.< /tm>
Translation ???????????????
Reference ???????????????
flect whether translation consistency can be captured
using syntactic knowledge. The classification and
TER BLEU P R
TM+TRANS 40.57 45.51 52.48 27.04
+DEP 39.61 46.46 60.00 32.14
Table 7: Contribution of Features (%)
translation results using different features are re-
ported in Table 7. We observe a significant improve-
ment in both classification precision and recall by
adding dependency (DEP) features on top of TM
and translation features. As a result, the translation
quality also significantly improves. This indicates
that dependency features which can capture struc-
tural and semantic similarities are effective in gaug-
ing the usefulness of the phrase pairs derived from
the fuzzy matches. Note also that without including
the dependency features, our discriminative learning
method cannot outperform the BASELINE (cf. Ta-
ble 3) in terms of translation quality.
5.6 Improved Translations
In order to pinpoint the sources of improvements by
marking up the input sentence, we performed some
manual analysis of the output. We observe that the
improvements can broadly be attributed to two rea-
sons: 1) the use of long phrase pairs which are miss-
ing in the phrase table, and 2) deterministically using
highly reliable phrase pairs.
Phrase-based SMT systems normally impose a
limit on the length of phrase pairs for storage and
speed considerations. Our method can overcome
this limitation by retrieving and reusing long phrase
pairs on the fly. A similar idea, albeit from a dif-
ferent perspective, was explored by (Lopez, 2008),
where he proposed to construct a phrase table on the
fly for each sentence to be translated. Differently
from his approach, our method directly translates
part of the input sentence using fuzzy matches re-
trieved on the fly, with the rest of the sentence trans-
lated by the pre-trained MT system. We offer some
more insights into the advantages of our method by
means of a few examples.
Example 1 shows translation improvements by
using long phrase pairs. Compared to the refer-
ence translation, we can see that for the underlined
phrase, the translation without markup contains (i)
word ordering errors and (ii) a missing right quota-
tion mark. In Example 2, by specifying the transla-
tion of the final punctuation mark, the system cor-
rectly translates the relative clause ?that you select?.
The translation of this relative clause is missing
when translating the input without markup. This
improvement can be partly attributed to the reduc-
tion in search errors by specifying the highly reliable
translations for phrases in an input sentence.
6 Conclusions and Future Work
In this paper, we introduced a discriminative learn-
ing method to tightly integrate fuzzy matches re-
trieved using translation memory technologies with
phrase-based SMT systems to improve translation
consistency. We used an SVM classifier to predict
whether phrase pairs derived from fuzzy matches
could be used to constrain the translation of an in-
1246
put sentence. A number of feature functions includ-
ing a series of novel dependency features were used
to train the classifier. Experiments demonstrated
that discriminative learning is effective in improving
translation quality and is more informative than the
fuzzy match score used in previous research. We re-
port a statistically significant 0.9 absolute improve-
ment in BLEU score using a procedure to promote
translation consistency.
As mentioned in Section 2, the potential improve-
ment in sentence-level translation consistency us-
ing our method can be attributed to the fact that
the translation of new input sentences is closely in-
formed and guided (or constrained) by previously
translated sentences using global features such as
dependencies. However, it is worth noting that
the level of gains in translation consistency is also
dependent on the nature of the TM itself; a self-
contained coherent TM would facilitate consistent
translations. In the future, we plan to investigate
the impact of TM quality on translation consistency
when using our approach. Furthermore, we will ex-
plore methods to promote translation consistency at
document level.
Moreover, we also plan to experiment with
phrase-by-phrase classification instead of sentence-
by-sentence classification presented in this paper,
in order to obtain more stable classification results.
We also plan to label the training examples using
other sentence-level evaluation metrics such as Me-
teor (Banerjee and Lavie, 2005), and to incorporate
features that can measure syntactic similarities in
training the classifier, in the spirit of (Owczarzak et
al., 2007). Currently, only a standard phrase-based
SMT system is used, so we plan to test our method
on a hierarchical system (Chiang, 2005) to facilitate
direct comparison with (Koehn and Senellart, 2010).
We will also carry out experiments on other data sets
and for more language pairs.
Acknowledgments
This work is supported by Science Foundation Ire-
land (Grant No 07/CE/I1142) and part funded under
FP7 of the EC within the EuroMatrix+ project (grant
No 231720). The authors would like to thank the
reviewers for their insightful comments and sugges-
tions.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, MI.
Ergun Bic?ici and Marc Dymetman. 2008. Dynamic
translation memory: Using statistical machine trans-
lation to improve translation memory. In Proceedings
of the 9th Internation Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 454?465, Haifa, Israel.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
David Chiang. 2005. A hierarchical Phrase-Based model
for Statistical Machine Translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, MI.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume 1,
pages 181?184, Detroit, MI.
Philipp Koehn and Jean Senellart. 2010. Convergence of
translation memory and statistical machine translation.
In Proceedings of AMTA Workshop on MT Research
and the Translation Industry, pages 21?31, Denver,
CO.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of the 2003 Human Language Technology Conference
and the North American Chapter of the Association
for Computational Linguistics, pages 48?54, Edmon-
ton, AB, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
1247
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Vladimir Iosifovich Levenshtein. 1966. Binary codes ca-
pable of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707?710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vec-
tor machines. Machine Learning, 68(3):267?276.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 104?111,
Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classifiers,
pages 61?74.
Richard Sikes. 2007. Fuzzy matching in theory and prac-
tice. Multilingual, 18(6):39?43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation
environment. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 120 ?
127, Ottawa, Ontario, Canada.
James Smith and Stephen Clark. 2009. EBMT for SMT:
A new EBMT-SMT hybrid. In Proceedings of the 3rd
International Workshop on Example-Based Machine
Translation, pages 3?10, Dublin, Ireland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas (AMTA-2006), pages 223?231, Cam-
bridge, MA, USA.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
confidence of machine translation quality estimates.
In Proceedings of the Twelfth Machine Translation
Summit (MT Summit XII), pages 136 ? 143, Ottawa,
Ontario, Canada.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO.
Ventsislav Zhechev and Josef van Genabith. 2010.
Seeding statistical machine translation with translation
memory output through tree-based structural align-
ment. In Proceedings of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation, pages 43?
51, Beijing, China.
1248
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 487?496,
Dublin, Ireland, August 23-24, 2014.
RTM-DCU: Referential Translation Machines for Semantic Similarity
Ergun Bic?ici
Centre for Global Intelligent Content
School of Computing
Dublin City University, Dublin, Ireland.
ebicici@computing.dcu.ie
Andy Way
Centre for Global Intelligent Content
School of Computing,
Dublin City University, Dublin, Ireland.
away@computing.dcu.ie
Abstract
We use referential translation machines
(RTMs) for predicting the semantic simi-
larity of text. RTMs are a computational
model for identifying the translation acts
between any two data sets with respect
to interpretants selected in the same do-
main, which are effective when making
monolingual and bilingual similarity judg-
ments. RTMs judge the quality or the se-
mantic similarity of text by using retrieved
relevant training data as interpretants for
reaching shared semantics. We derive fea-
tures measuring the closeness of the test
sentences to the training data via inter-
pretants, the difficulty of translating them,
and the presence of the acts of transla-
tion, which may ubiquitously be observed
in communication. RTMs provide a lan-
guage independent approach to all simi-
larity tasks and achieve top performance
when predicting monolingual cross-level
semantic similarity (Task 3) and good re-
sults in semantic relatedness and entail-
ment (Task 1) and multilingual semantic
textual similarity (STS) (Task 10). RTMs
remove the need to access any task or do-
main specific information or resource.
1 Semantic Similarity Judgments
We introduce a fully automated judge for seman-
tic similarity that performs well in three seman-
tic similarity tasks at SemEval-2014, Semantic
Evaluation Exercises - International Workshop on
Semantic Evaluation (Nakov and Zesch, 2014).
RTMs provide a language independent solution for
the semantic textual similarity (STS) task (Task
10) (Agirre et al., 2014), achieve top perfor-
mance when predicting monolingual cross-level
semantic similarity (Task 3) (Jurgens et al., 2014),
and achieve good results in the semantic related-
ness and entailment task (Task 1) (Marelli et al.,
2014a).
Referential translation machine (Section 2) is
a computational model for identifying the acts of
translation for translating between any given two
data sets with respect to a reference corpus se-
lected in the same domain. An RTM model is
based on the selection of interpretants, training
data close to both the training set and the test set,
which allow shared semantics by providing con-
text for similarity judgments. In semiotics, an in-
terpretant I interprets the signs used to refer to the
real objects (Bic?ici, 2008). Each RTM model is
a data translation and translation prediction model
between the instances in the training set and the
test set and translation acts are indicators of the
data transformation and translation. RTMs present
an accurate and language independent solution for
making semantic similarity judgments.
We describe the tasks we participated below.
Section 2 describes the RTM model and the fea-
tures used. Section 3 presents the training and test
results we obtain on the three tasks we competed
and the last section concludes.
Task 1 Evaluation of Compositional Distribu-
tional Semantic Models on Full Sentences
through Semantic Relatedness and Entail-
ment (SRE) (Marelli et al., 2014a):
Given two sentences, produce a related-
ness score indicating the extent to which
the sentences express a related meaning: a
number in the range [1, 5].
We model the problem as a translation perfor-
mance prediction task where one possible inter-
pretation is obtained by translating S
1
(the source
to translate, S) to S
2
(the target translation, T).
Since linguistic processing can reveal deeper sim-
ilarity relationships, we also look at the translation
task at different granularities of information: plain
487
text (R for regular) and after lemmatization (L).
We lowercase all text.
Task 3 Cross-Level Semantic Similarity
(CLSS) (Jurgens et al., 2014):
Given two text from different levels, pro-
duce a semantic similarity rating: a num-
ber in the range [0, 4].
CLSS task targets semantic similarity compar-
isons between text having different levels of gran-
ularity and we address the following level cross-
ings: paragraph to sentence, sentence to phrase,
and phrase to word. We model the problem as
a translation performance prediction task among
text from different levels.
Task 10 Multilingual Semantic Textual Similarity
(MSTS) (Agirre et al., 2014)
Given two sentences S
1
and S
2
in the same
language, quantify the degree of similar-
ity: a number in the range [0, 5].
MSTS task addresses the problem in English
and Spanish (score range is [0, 4]). We model the
problem as a translation performance prediction
task between S
1
and S
2
.
2 Referential Translation Machine
(RTM)
Referential translation machines provide a compu-
tational model for quality and semantic similarity
judgments in monolingual and bilingual settings
using retrieval of relevant training data (Bic?ici,
2011; Bic?ici and Yuret, 2014) as interpretants for
reaching shared semantics (Bic?ici, 2008). RTMs
are a language independent approach and achieve
top performance when predicting the quality of
translations (Bic?ici, 2013; Bic?ici and Way, 2014)
and when predicting monolingual cross-level se-
mantic similarity (Jurgens et al., 2014), and good
performance when evaluating the semantic relat-
edness of sentences and their entailment (Marelli
et al., 2014a), as an automated student answer
grader (Bic?ici and van Genabith, 2013b), and
when judging the semantic similarity of sen-
tences (Bic?ici and van Genabith, 2013a; Agirre et
al., 2014). We improve the RTM models by:
? using a parameterized, fast implementation
of FDA, FDA5, and our Parallel FDA5 in-
stance selection model (Bic?ici et al., 2014),
? better modeling of the language in which
Algorithm 1: Referential Translation Machine
Input: Training set train, test set test,
corpus C, and learning model M .
Data: Features of train and test, F
train
and F
test
.
Output: Predictions of similarity scores on
the test q?.
1 FDA5(train,test, C)? I
2 MTPP(I,train)? F
train
3 MTPP(I,test)? F
test
4 learn(M,F
train
)?M
5 predict(M,F
test
)? q?
similarity judgments are made with improved
optimization and selection of the LM data,
? using a general domain corpus to select inter-
pretants from,
? increased feature set for also modeling the
structural properties of sentences,
? extended learning models.
We use the Parallel FDA5 (Feature Decay Algo-
rithms) instance selection model for selecting the
interpretants (Bic?ici et al., 2014; Bic?ici and Yuret,
2014) this year, which allows efficient parameteri-
zation, optimization, and implementation of FDA,
and build an MTPP model (Section 2.1). We view
that acts of translation are ubiquitously used dur-
ing communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different lan-
guages and paraphrasing or communication also
contain acts of translation. When creating sen-
tences, we use our background knowledge and
translate information content according to the cur-
rent context.
The inputs to the RTM algorithm Algorithm 1
are a training set train, a test set test, some
corpus C, preferably in the same domain as the
training and test sets, and a learning model. Step 1
selects the interpretants, I, relevant to both the
training and test data. Steps 2 and 3 use I to map
train and test to a new space where similari-
ties between translation acts can be derived more
easily. Step 4 trains a learning model M over the
training features, F
train
, and Step 5 obtains the
predictions. Figure 1 depicts the RTM.
488
Figure 1: RTM depiction.
Our encouraging results in the semantic simi-
larity tasks increase our understanding of the acts
of translation we ubiquitously use when commu-
nicating and how they can be used to predict the
semantic similarity of text. RTM and MTPP mod-
els are not data or language specific and their mod-
eling power and good performance are applicable
in different domains and tasks. RTM expands the
applicability of MTPP by making it feasible when
making monolingual quality and similarity judg-
ments and it enhances the computational scalabil-
ity by building models over smaller and more rel-
evant set of interpretants.
2.1 The Machine Translation Performance
Predictor (MTPP)
MTPP (Bic?ici et al., 2013) is a state-of-the-art
and top performing machine translation perfor-
mance predictor, which uses machine learning
models over features measuring how well the test
set matches the training set to predict the quality
of a translation without using a reference trans-
lation. MTPP measures the coverage of individ-
ual test sentence features found in the training set
and derives indicators of the closeness of test sen-
tences to the available training data, the difficulty
of translating the sentence, and the presence of
acts of translation for data transformation.
2.2 MTPP Features for Translation Acts
MTPP feature functions use statistics involving
the training set and the test sentences to deter-
mine their closeness. Since they are language
independent, MTPP allows quality estimation to
be performed extrinsically. MTPP uses n-gram
features defined over text or common cover link
(CCL) (Seginer, 2007) structures as the basic units
of information over which similarity calculations
are made. Unsupervised parsing with CCL ex-
tracts links from base words to head words, rep-
resenting the grammatical information instantiated
in the training and test data.
We extend the MTPP model we used last
year (Bic?ici, 2013) in its learning module and the
features included. Categories for the features (S
for source, T for target) used are listed below
where the number of features are given in brackets
for S and T, {#S, #T}, and the detailed descriptions
for some of the features are presented in (Bic?ici et
al., 2013). The number of features for each task
differs since we perform an initial feature selection
step on the tree structural features (Section 2.3).
The number of features are in the range 337?437.
? Coverage {56, 54}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Perplexity {45, 45}: Measures the fluency of
the sentences according to language models
(LM). We use both forward ({30}) and back-
ward ({15}) LM features for S and T.
? TreeF {0, 10-110}: 10 base features and up
to 100 selected features of T among parse tree
structures (Section 2.3).
? Retrieval Closeness {16, 12}: Measures the
degree to which sentences close to the test set
are found in the selected training set, I, using
FDA (Bic?ici and Yuret, 2011a) and BLEU,
F
1
(Bic?ici, 2011), dice, and tf-idf cosine sim-
ilarity metrics.
? IBM2 Alignment Features {0, 22}: Calcu-
lates the sum of the entropy of the dis-
tribution of alignment probabilities for S
(
?
s?S
?p log p for p = p(t|s) where s and
t are tokens) and T, their average for S and
T, the number of entries with p ? 0.2 and
p ? 0.01, the entropy of the word align-
ment between S and T and its average, and
word alignment log probability and its value
in terms of bits per word. We also com-
pute word alignment percentage as in (Ca-
margo de Souza et al., 2013) and potential
BLEU, F
1
, WER, PER scores for S and T.
? IBM1 Translation Probability {4, 12}: Cal-
culates the translation probability of test
sentences using the selected training set,
I (Brown et al., 1993).
? Feature Vector Similarity {8, 8}: Calculates
similarities between vector representations.
489
CCL
numB depthB avg depthB R/L avg R/L
24.0 9.0 0.375 2.1429 3.401
2
1 1
1
1 13
1
1 2
1
1 8
1
2 10
1
3 1
1
3 4
1
5 1
1
7 15
Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).
? Entropy {2, 8}: Calculates the distributional
similarity of test sentences to the training set
over top N retrieved sentences (Bic?ici et al.,
2013).
? Length {6, 3}: Calculates the number of
words and characters for S and T and their
average token lengths and their ratios.
? Diversity {3, 3}: Measures the diver-
sity of co-occurring features in the training
set (Bic?ici et al., 2013).
? Synthetic Translation Performance {3, 3}:
Calculates translation scores achievable ac-
cording to the n-gram coverage.
? Character n-grams {5}: Calculates cosine
between character n-grams (for n=2,3,4,5,6)
obtained for S and T (B?ar et al., 2012).
? Minimum Bayes Retrieval Risk {0, 4}: Cal-
culates the translation probability for the
translation having the minimum Bayes risk
among the retrieved training instances.
? Sentence Translation Performance {0, 3}:
Calculates translation scores obtained ac-
cording to q(T,R) using BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), or
F
1
(Bic?ici and Yuret, 2011b) for q.
? LIX {1, 1}: Calculates the LIX readability
score (Wikipedia, 2013; Bj?ornsson, 1968) for
S and T.
1
2.3 Bracketing Tree Structural Features
We use the parse tree outputs obtained by CCL
to derive features based on the bracketing struc-
ture. We derive 5 statistics based on the geometric
properties of the parse trees: number of brackets
used (numB), depth (depthB), average depth (avg
1
LIX=
A
B
+ C
100
A
, where A is the number of words, C is
words longer than 6 characters, B is words that start or end
with any of ?.?, ?:?, ?!?, ??? similar to (Hagstr?om, 2012).
depthB), number of brackets on the right branches
over the number of brackets on the left (R/L)
2
, av-
erage right to left branching over all internal tree
nodes (avg R/L). The ratio of the number of right
to left branches shows the degree to which the sen-
tence is right branching or not. Additionally, we
capture the different types of branching present
in a given parse tree identified by the number of
nodes in each of its children.
Table 1 depicts the parsing output obtained by
CCL for the following sentence from WSJ23
3
:
Many fund managers argue that now ?s the time
to buy .
We use Tregex (Levy and Andrew, 2006) for vi-
sualizing the output parse trees presented on the
left. The bracketing structure statistics and fea-
tures are given on the right hand side. The root
node of each tree structural feature represents the
number of times that feature is present in the pars-
ing output of a document.
3 SemEval-14 Results
We develop individual RTM models for each task
and subtask that we participate at SemEval-2014
with the RTM-DCU team name. The interpre-
tants are selected from the LM corpora distributed
by the translation task of WMT14 (Bojar et al.,
2014) and the LM corpora provided by LDC for
English (Parker et al., 2011) and Spanish (
?
Angelo
Mendonc?a, 2011)
4
. We use the Stanford POS tag-
ger (Toutanova et al., 2003) to obtain the lemma-
tized corpora for the SRE task. For each RTM
2
For nodes with uneven number of children, the nodes in
the odd child contribute to the right branches.
3
Wall Street Journal (WSJ) corpus section 23, distributed
with Penn Treebank version 3 (Marcus et al., 1993).
4
English Gigaword 5th, Spanish Gigaword 3rd edition.
490
model, we extract the features both on the train-
ing set and the test set. The number of instances
we select for the interpretants in each task is given
in Table 2.
Task Setting Train LM
Task 1, SRE English 770 10770
Task 3, CLSS Par2S 302 2802
Task 3, CLSS S2Phrase 202 2702
Task 3, CLSS Phrase2W 102 2602
Task 10, MSTS English 504 8002
Task 10, MSTS English OnWN 504 8004
Task 10, MSTS Spanish 502 8002
Table 2: Number of sentences in I (in thousands)
selected for each task.
We use ridge regression (RR), support vector
regression (SVR) with RBF (radial basis func-
tions) kernel (Smola and Sch?olkopf, 2004), and
extremely randomized trees (TREE) (Geurts et al.,
2006) as the learning models. TREE is an en-
semble learning method over randomized decision
trees. These models learn a regression function
using the features to estimate a numerical target
value. We also use these learning models after
a feature subset selection with recursive feature
elimination (RFE) (Guyon et al., 2002) or a di-
mensionality reduction and mapping step using
partial least squares (PLS) (Specia et al., 2009),
both of which are described in (Bic?ici et al., 2013).
We optimize the learning parameters, the num-
ber of features to select, the number of dimen-
sions used for PLS, and the parameters for paral-
lel FDA5. More detailed descriptions of the opti-
mization processes are given in (Bic?ici et al., 2013;
Bic?ici et al., 2014). We optimize the learning pa-
rameters by selecting ? close to the standard devi-
ation of the noise in the training set (Bic?ici, 2013)
since the optimal value for ? is shown to have
linear dependence to the noise level for different
noise models (Smola et al., 1998). At testing time,
the predictions are bounded to obtain scores in the
corresponding ranges. We obtain the confidence
scores using support vector classification (SVC).
3.1 Task 1: Semantic Relatedness and
Entailment
MSTS contains sentence pairs from the SICK
(Sentences Involving Compositional Knowledge)
data set (Marelli et al., 2014b), which contain sen-
tence pairs that contain rich lexical, syntactic and
semantic phenomena. Official evaluation metric
in SRE is the Pearson?s correlation score, which
is used to select the top systems on the training
set. SRE task allows the submission of 5 entries.
We present the performance of the top 5 individ-
ual RTM models on the training set in Table 3.
ACC is entailment accuracy, r
P
is Pearson?s corre-
lation, r
S
is Spearman?s correlation, MSE is mean
squared error, MAE is mean absolute error, and
RAE is relative absolute error. L uses the lem-
matized corpora and R uses the true-cased corpora
corresponding to regular. R+L correspond to the
perspective using the features from both R and L,
which doubles the number of features. We com-
pute the entailment by SVC.
Data Model ACC r
P
r
S
MSE MAE RAE
L SVR 67.52 .7372 .6918 .6946 .5511 .6856
L PLS-SVR 67.04 .7539 .6927 .6763 .5369 .668
R+L PLS-SVR 66.76 .75 .6879 .6815 .539 .6705
R+L SVR 66.66 .7295 .6814 .7027 .5591 .6956
L PLS-RR 66.56 .7247 .6765 .7054 .5687 .7075
Table 3: SRE training results of the top 5 RTM
systems selected.
SRE challenge results on the test set are given
in Table 4. The setting R using PLS-SVR learning
becomes the 8th out of 17 submissions when pre-
dicting the semantic relatedness and 17th out of 18
submissions when predicting the entailment.
Data Model ACC r
P
r
S
RMSE MAE RAE
R PLS-SVR 67.20 .7639 .6877 .655 .5246 .6645
R+L PLS-SVR 67.65 .7688 .6918 .6492 .5194 .658
L SVR 67.65 .7559 .6887 .664 .531 .6726
R+L SVR 67.44 .7625 .6899 .6555 .5251 .6651
R PLS-SVR 66.61 .7570 .6683 .6637 .5324 .6744
Table 4: RTM-DCU test results on the SRE task.
Model r
P
RMSE MAE RAE
Par2S TREE 0.8013 0.8345 0.6277 0.5083
Par2S PLS-TREE 0.7737 0.8824 0.673 0.5449
Par2S SVR 0.7718 0.8863 0.6791 0.5499
S2Phrase TREE 0.6756 0.9887 0.7746 0.6665
S2Phrase PLS-TREE 0.6119 1.0616 0.8582 0.7384
S2Phrase SVR 0.6059 1.0662 0.8668 0.7458
Phrase2W TREE 0.201 1.3275 1.1353 0.9706
Phrase2W RR 0.1255 1.3463 1.1594 0.9912
Phrase2W SVR 0.0847 1.3548 1.1663 0.9972
Table 5: CLSS training results of the top 3 RTM
systems for each subtask. Levels correspond to
paragraph to sentence (Par2S), sentence to phrase
(S2Phrase), and phrase to word (Phrase2W).
491
3.2 Task 3: Cross-Level Semantic Similarity
CLSS contains sentence pairs from different gen-
res including text from newswire, travel, reviews,
metaphoric text, community question answering
sites, idiomatic text, descriptions, lexicographic
text, and search. Official evaluation metric in
CLSS is the sum of the Pearson?s correlation
scores for different levels
5
. CLSS task allows the
submission of 3 entries per subtask. We present
the performance of the top 3 individual RTM mod-
els on the training set in Table 5. RMSE is the root
mean squared error. As the compared text size de-
crease, the performance decrease since it can be-
come harder and more ambiguous to find the simi-
larity using less context. RTM-DCU results on the
CLSS challenge test set are provided in Table 6.
Model r
P
RMSE MAE RAE
Par2S TREE .8445 .7417 .5622 .4579
Par2S PLS-TREE .7847 .853 .6456 .5258
Par2S SVR .7858 .8428 .6539 .5325
S2Phrase TREE .75 .8827 .7053 .6255
S2Phrase PLS-TREE .6979 .9491 .7781 .69
S2Phrase SVR .6631 .9835 .7992 .7088
Phrase2W TREE .3053 1.3351 1.14 .9488
Phrase2W RR .2207 1.3644 1.1574 .9633
Phrase2W SVR .1712 1.3792 1.1792 .9815
Table 6: RTM-DCU test results on CLSS for the
top 3 RTM systems for each subtask.
Table 7 lists the results along with their ranks
for r
P
and r
S
, Spearman?s correlation, out of
CHECK submissions. The baseline in Table 7
is normalized longest common substring (LCS)
scaled in the range [0, 4]. Top individual rank row
lists the ranks in each subtask. We present the re-
sults for both our official and late (about 1 day)
submissions including word to sense (W2S) re-
sults
6
. RTM-DCU is able to obtain the top result
in Par2S in the CLSS task.
3.3 Task 10: Multilingual Semantic Textual
Similarity
MSTS contains sentence pairs from different do-
mains: sense definitions from semantic lexical re-
sources such as OnWN (from OntoNotes (Prad-
han et al., 2007) and WordNet (Miller, 1995)) and
FNWN (from FrameNet (Baker et al., 1998) and
WordNet), news headlines, image descriptions,
news title tweet comments, deft forum and news,
5
Giving advantage to participants submitting to all levels.
6
W2S results for the late submission is obtained from the
LCS baseline to calculate the ranks.
r
P
Par2S S2Phrase Phrase2W W2S Rank
LCS 0.527 0.562 0.165 0.109 25
Official
0.780 0.677 0.208 14
0.747 0.588 0.164 19
0.786 0.666 0.171 18
Late
0.845 0.750 0.305 0.109 6
0.785 0.698 0.221 0.109 13
0.786 0.663 0.171 0.109 17
Top Rank 1 5 3
r
S
Par2S S2Phrase Phrase2W W2S Rank
LCS 0.527 0.562 0.165 0.13 23
Official
0.780 0.677 0.208 17
0.747 0.588 0.164 22
0.786 0.666 0.171 18
Late
0.829 0.734 0.295 0.13 8
0.778 0.687 0.219 0.13 15
0.778 0.667 0.166 0.13 16
Top Rank 1 5 5
Table 7: RTM-DCU test results on CLSS.
paraphrases. Official evaluation metric in MSTS
is the Pearson?s correlation score.
MSTS task provides 7622 training instances
and 3750 test instances. For the OnWN domain,
1316 training instances are available and therefore,
we build a separate RTM model for this domain.
Separate modeling of the OnWN dataset results
with higher confidence scores on the test instances
than we would obtain using the overall model to
predict. MSTS task allows the submission of 3 en-
tries per subtask. We present the performance of
the top 3 individual RTM models on the training
set in Table 8.
Lang Model r
P
RMSE MAE RAE
E
n
g
l
i
s
h
TREE 0.6931 1.0627 0.8058 0.6649
PLS-TREE 0.6875 1.0753 0.8038 0.6632
PLS-SVR 0.6884 1.0698 0.8157 0.6730
O
n
W
N TREE 0.8094 0.9295 0.694 0.5245
PLS-TREE 0.7953 0.9604 0.7203 0.5444
PLS-SVR 0.7888 0.9779 0.7234 0.5468
S
p
a
n
i
s
h
TREE 0.6513 0.7341 0.5904 0.7508
PLS-TREE 0.4157 0.9007 0.7108 0.9039
PLS-SVR 0.4239 1.1427 0.8293 1.0545
Table 8: MSTS training results on the English, En-
glish OnWN, and Spanish tasks.
RTM results on the MSTS challenge test set are
provided in Table 9 along with the RTM results in
STS 2013 (Bic?ici and van Genabith, 2013a). Ta-
ble 10 and Table 11 lists the official results on En-
glish and Spanish tasks with rankings calculated
according to weighted r
P
, which weights accord-
ing to the number of instances in each domain.
RTM-DCU is able to become 10th in the OnWN
domain and 19th overall out of 38 submissions in
MSTS English and 18th out of 22 submissions in
492
Model r
P
RMSE MAE RAE
E
n
g
l
i
s
h
deft-forum
TREE .4341 1.4306 1.1609 1.0908
PLS-TREE .3965 1.4115 1.1472 1.078
PLS-SVR .3078 1.6277 1.3482 1.2669
deft-news
TREE .6974 1.1469 .9032 .8716
PLS-TREE .6811 1.1229 .8769 .8462
PLS-SVR .5562 1.2803 .9835 .9491
headlines
TREE .6199 1.1495 .9254 .7845
PLS-TREE .6125 1.1552 .9314 .7896
PLS-SVR .6301 1.1041 .8807 .7467
images
TREE .6995 1.2034 .9499 .7395
PLS-TREE .6656 1.2298 .9692 .7545
PLS-SVR .6474 1.4406 1.1057 .8607
OnWN
TREE .8058 1.3122 1.0028 .5585
PLS-TREE .7992 1.2997 .9815 .5467
PLS-SVR .8004 1.2913 .9449 .5263
tweet-news
TREE .6882 .9869 .831 .8093
PLS-TREE .6691 1.0101 .8433 .8213
PLS-SVR .5531 1.0633 .8653 .8427
S
p
a
n
i
s
h
News
TREE .7 1.5185 1.351 1.4141
PLS-TREE .6253 1.6523 1.4464 1.514
PLS-SVR .6411 1.554 1.3196 1.3813
Wikipedia
TREE .4216 1.5433 1.298 1.3579
PLS-TREE .3689 1.6655 1.4015 1.4662
PLS-SVR .4242 1.5998 1.3141 1.3748
S
T
S
2
0
1
3
E
n
g
l
i
s
h
headlines
L+S SVR .6552 1.5649 1.2763 1.0231
L+P+S SVR .651 1.4845 1.1984 .9607
L+P+S SVR TL .6385 1.4878 1.2008 .9626
OnWN
L+S SVR .6943 1.7065 1.3545 .8255
L+P+S SVR .6971 1.6737 1.333 .8124
L+P+S SVR TL .6755 1.7124 1.3598 .8287
SMT
L+S SVR .3005 .8833 .6886 1.6132
L+P+S SVR .2861 .8810 .6821 1.598
L+P+S SVR TL .3098 .8635 .6547 1.5339
FNWN
L+S SVR .2016 1.2957 1.0604 1.2633
L+P+S SVR .118 1.4369 1.1866 1.4136
L+P+S SVR TL .1823 1.3245 1.0962 1.3059
Table 9: RTM-DCU test results on MSTS for the
top 3 RTM systems for each subtask as well as
RTM results in STS 2013 (Bic?ici and van Gen-
abith, 2013a).
MSTS Spanish. The performance difference be-
tween MSTS English and MSTS Spanish may be
due to the fewer training data available for the
MSTS Spanish task, which may be decreasing the
performance of our supervised learning approach.
3.4 RTMs Across Tasks and Years
We compare the difficulty of tasks according to the
RAE levels achieved. RAE measures the error rel-
ative to the error when predicting the actual mean.
A high RAE is an indicator that the task is hard.
In Table 12, we list the RAE obtained for differ-
ent tasks and subtasks, also listing RTM results in
STS 2013 (Bic?ici and van Genabith, 2013a) and
RTM results (Bic?ici and Way, 2014) on the quality
estimation task (QET) (Bojar et al., 2014) where
post-editing effort (PEE), human-targeted transla-
Model Wikipedia News Weighted r
P
Rank
TREE 0.4216 0.7000 0.5878 18
PLS-TREE 0.3689 0.6253 0.5219 20
PLS-SVR 0.4242 0.6411 0.5537 19
Table 11: RTM-DCU test results on MSTS Span-
ish task. Rankings are calculated according to the
weighted Pearson?s correlation.
tion edit rate (HTER), or post-editing time (PET)
of translations are predicted.
The best results are obtained for the CLSS
Par2S subtask, which may be due to the larger
contextual information that paragraphs can pro-
vide for the RTM models. For the SRE task, we
can only reduce the error with respect to knowing
and predicting the mean by about 35%. Prediction
of bilingual similarity as in quality estimation of
translation can be expected to be harder and RTMs
achieve state-of-the-art performance in this task as
well (Bic?ici and Way, 2014).
4 Conclusion
Referential translation machines provide a clean
and intuitive computational model for automati-
cally measuring semantic similarity by measur-
ing the acts of translation involved and achieve to
be the top on some semantic similarity tasks at
SemEval-2014. RTMs make quality and seman-
tic similarity judgments possible based on the re-
trieval of relevant training data as interpretants for
reaching shared semantics.
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University, in part by SFI
(13/TIDA/I2740) for the project ?Monolin-
gual and Bilingual Text Quality Judgments
with Translation Performance Prediction?
(www.computing.dcu.ie/?ebicici/
Projects/TIDA RTM.html), and in part
by the European Commission through the QT-
LaunchPad FP7 project (No: 296347). We
also thank the SFI/HEA Irish Centre for High-
End Computing (ICHEC) for the provision of
computational facilities and support.
493
Model deft-forum deft-news headlines images OnWN tweet-news Weighted r
P
Rank
TREE .4341 .6974 .6199 .6995 .8058 .6882 .6706 20
PLS-TREE .3965 .6811 .6125 .6656 .7992 .6691 .6513 23
PLS-SVR .3078 .5562 .6301 .6475 .8004 .5531 .6076 27
Top Rank 17 16 25 26 16 13
W
i
t
h
C
o
n
f
.
TREE .4181 .6846 .6216 .6981 .8331 .6870 .6729 19
PLS-TREE .3831 .6739 .6094 .6629 .8260 .6691 .6534 23
PLS-SVR .2731 .5526 .6330 .6441 .8246 .5683 .6110 26
Top Rank 18 18 23 27 10 14
Table 10: RTM-DCU test results with ranks on MSTS English task.
Task Subtask Domain Model RAE
SRE English SICK
R PLS-SVR .6645
R+L PLS-SVR .6580
L SVR .6726
R+L SVR .6651
R PLS-SVR .6744
CLSS
Par2S
Mixed
TREE .4579
S2Phrase TREE .6255
Phrase2W TREE .9488
MSTS
English
deft-forum PLS-TREE 1.078
deft-news PLS-TREE .8462
headlines PLS-SVR .7467
images TREE .7395
OnWN PLS-SVR .5263
tweet-news TREE .8093
Spanish
News PLS-SVR 1.3813
Wikipedia TREE 1.3579
STS 2013 English
headlines L+P+S SVR .9607
OnWN L+P+S SVR .8124
SMT L+P+S SVR TL 1.5339
FNWN L+S SVR 1.2633
QET PEE
Spanish-English Europarl FS-RR .9000
Spanish-English Europarl PLS-RR .9409
English-German Europarl PLS-TREE .8883
English-German Europarl TREE .8602
English-Spanish Europarl TREE 1.0983
English-Spanish Europarl PLS-TREE 1.0794
German-English Europarl RR .8204
German-English Euruparl PLS-RR .8437
QET HTER
English-Spanish Europarl SVR .8532
English-Spanish Europarl TREE .8931
QET PET
English-Spanish Europarl SVR .7223
English-Spanish Europarl RR .7536
Table 12: Best RTM-DCU RAE test results for different tasks and subtasks as well as STS 2013 re-
sults (Bic?ici and van Genabith, 2013a) and results from quality estimation task of translation (Bojar et
al., 2014; Bic?ici and Way, 2014).
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilin-
gual semantic textual similarity. In Proc. of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proc. of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics - Volume 1, ACL
?98, pages 86?90, Stroudsburg, PA, USA.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics ? Volume 1: Proc. of the main conference and
the shared task, and Volume 2: Proc. of the Sixth In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2012), pages 435?440, Montr?eal, Canada, 7-
8 June. Association for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013a. CNGL-
494
CORE: Referential translation machines for measur-
ing semantic similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, Georgia, USA, 13-14 June. Asso-
ciation for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013b. CNGL:
Grading student answers by acts of translation. In
*SEM 2013: The Second Joint Conference on Lex-
ical and Computational Semantics and Proc. of the
Seventh International Workshop on Semantic Eval-
uation (SemEval 2013), Atlanta, Georgia, USA, 14-
15 June. Association for Computational Linguistics.
Ergun Bic?ici and Andy Way. 2014. Referential trans-
lation machines for predicting translation quality. In
Proc. of the Ninth Workshop on Statistical Machine
Translation, Baltimore, USA, June. Association for
Computational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proc. of the Sixth Workshop on Sta-
tistical Machine Translation, pages 272?283, Ed-
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT sys-
tem for machine translation, system combination,
and evaluation. In Proc. of the Sixth Workshop on
Statistical Machine Translation, pages 323?329, Ed-
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation, 27:171?192, December.
Ergun Bic?ici, Qun Liu, and Andy Way. 2014. Paral-
lel FDA5 for fast deployment of accurate statistical
machine translation systems. In Proc. of the Ninth
Workshop on Statistical Machine Translation, Bal-
timore, USA, June. Association for Computational
Linguistics.
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Referential translation machines
for quality estimation. In Proc. of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multia-
gent and Grid Systems.
Carl Hugo Bj?ornsson. 1968. L?asbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Ond?rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311, June.
Jos?e Guilherme Camargo de Souza, Christian Buck,
Marco Turchi, and Matteo Negri. 2013. FBK-
UEdin participation to the WMT13 quality estima-
tion shared task. In Proc. of the Eighth Workshop
on Statistical Machine Translation, pages 352?358,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proc. of the second interna-
tional conference on Human Language Technology
Research, pages 138?145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389?422.
Kenth Hagstr?om. 2012. Swedish readabil-
ity calculator. https://github.com/keha76/Swedish-
Readability-Calculator.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-level semantic similarity. In Proc. of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Roger Levy and Galen Andrew. 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. In Proc. of the fifth international
conference on Language Resources and Evaluation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313?330, June.
495
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014a. SemEval-2014 Task 1: Evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. In Proc. of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland, August.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation of
compositional distributional semantic models. In
Proc. of LREC 2014, Reykjavik, Iceland.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41, November.
Preslav Nakov and Torsten Zesch, editors. 2014.
Proc. of SemEval-2014 Semantic Evaluation Exer-
cises - International Workshop on Semantic Evalua-
tion. Dublin, Ireland, 23-24 August.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Sameer S. Pradhan, Eduard H. Hovy, Mitchell P.
Marcus, Martha Palmer, Lance A. Ramshaw, and
Ralph M. Weischedel. 2007. Ontonotes: a unified
relational semantic representation. Int. J. Semantic
Computing, 1(4):405?419.
Yoav Seginer. 2007. Learning Syntactic Structure.
Ph.D. thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Sch?olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222, August.
A. J. Smola, N. Murata, B. Sch?olkopf, and K.-R.
M?uller. 1998. Asymptotically optimal choice of ?-
loss for support vector machines. In L. Niklasson,
M. Boden, and T. Ziemke, editors, Proc. of the Inter-
national Conference on Artificial Neural Networks,
Perspectives in Neural Computing, pages 105?110,
Berlin. Springer.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proc. of the 13th Annual Conference of
the European Association for Machine Translation
(EAMT), pages 28?35, Barcelona, May. EAMT.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proc. of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 173?180, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Wikipedia. 2013. LIX. http://en.wikipedia.org/
wiki/LIX.
David Graff Denise DiPersio
?
Angelo Mendonc?a,
Daniel Jaquette. 2011. Spanish Gigaword third edi-
tion, Linguistic Data Consortium.
496
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143?148,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2010
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat, Pratyush Banerjee, Ankit K. Srivastava,
Jinhua Du, Pavel Pecina, Sudip Kumar Naskar, Mikel L. Forcada, Andy Way
CNGL, School of Computing
Dublin City University, Dublin 9, Ireland
{ spenkale, rhaque, sdandapat, pbanerjee, asrivastava, jdu, ppecina, snaskar, mforcada, away }@computing.dcu.ie
Abstract
This paper describes the DCU machine
translation system in the evaluation cam-
paign of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics
in ACL-2010. We describe the modular
design of our multi-engine machine trans-
lation (MT) system with particular focus
on the components used in this partici-
pation. We participated in the English?
Spanish and English?Czech translation
tasks, in which we employed our multi-
engine architecture to translate. We also
participated in the system combination
task which was carried out by the MBR
decoder and confusion network decoder.
1 Introduction
In this paper, we present the DCU multi-engine
MT system MATREX (Machine Translation using
Examples). This system exploits example-based
MT, statistical MT (SMT), and system combina-
tion techniques.
We participated in the English?Spanish (en?
es) and English?Czech (en?cs) translation
tasks. For these two tasks, we employ several
individual MT systems: 1) Baseline: phrase-
based SMT (Koehn et al, 2007); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004); 3) Factored translation
model (Koehn and Hoang, 2007); 4) Source-side
context-informed (SSCI) systems (Stroppa et al,
2007); 5) the moses-chart (a Moses imple-
mentation of the hierarchical phrase-based (HPB)
approach of Chiang (2007)) and 6) Apertium (For-
cada et al, 2009) rule-based machine translation
(RBMT). Finally, we use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final translation.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypoth-
esis as the alignment reference for the confusion
network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search for the best translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide evaluation results on the test set.
Section 4 concludes the paper.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits as-
pects of both the EBMT and SMT paradigms.
The architecture includes various individual sys-
tems: phrase-based, example-based, hierarchical
phrase-based and tree-based MT.
The combination structure uses the MBR and
CN decoders, and is based on a word-level com-
bination strategy (Du et al, 2009). In the final
stage, we use a new rescoring module to process
the N -best list generated by the combination mod-
ule. Figure 1 illustrates the architecture.
2.2 Example-Based Machine Translation
The EBMT system uses a language-specific, re-
duced set of closed-class marker morphemes or
lexemes (Gough and Way, 2004) to define a way
to segment sentences into chunks, which are then
aligned using an edit-distance-style algorithm, in
which edit costs depend on word-to-word transla-143
Figure 1: System Framework.
tion probabilities and the amount of word-to-word
cognates (Stroppa and Way, 2006).
Once these phrase pairs were obtained they
were merged with the phrase pairs extracted by
the baseline system adding word alignment infor-
mation.
2.3 Apertium RBMT
Apertium1 is a free/open-source platform for
RBMT. The current version of the en?es system
in Apertium was used for the system combination
task (section 2.7), and its morphological analysers
and part-of-speech taggers were used to build a
factored Moses model.
2.4 Factored Translation Model
We also used a factored model for the en?es
translation task. Factored models (Koehn and
Hoang, 2007) facilitate the translation by break-
ing it down into several factors which are further
combined using a log-linear model (Och and Ney,
2002).
We used three factors in our factored translation
model, which are used in two different decoding
paths: a surface form (SF) to SF translation factor,
a lemma to lemma translation factor, and a part-of-
speech (PoS) to PoS translation factor.
Finally, we used two decoding paths based on
1http://www.apertium.org
the above three translation factors: an SF to SF
decoding path and a path which maps lemma to
lemma, PoS to PoS, and an SF generated using
the TL lemma and PoS. The lemmas and PoS for
en and es were obtained using Apertium (sec-
tion 2.3).
2.5 Source-Side Context-informed PB-SMT
One natural way to express a context-informed
feature (h?MBL) is to view it as the conditional
probability of the target phrases (e?k) given the
source phrase (f?k) and its source-side context in-
formation (CI):
h?MBL = logP (e?k|f?k,CI(f?k)) (1)
We use a memory-based machine learning
(MBL) classifier (TRIBL:2 Daelemans and
van den Bosch (2005)) that is able to estimate
P (e?k|f?k,CI(f?k)) by similarity-based reasoning
over memorized nearest-neighbour examples of
source?target phrase translations. In equation (1),
SSCI may include any feature (lexical, syntactic,
etc.), which can provide useful information to
disambiguate a given source phrase. In addition
to using local words and PoS-tags as features,
as in (Stroppa et al, 2007), we incorporate
grammatical dependency relations (Haque et al,
2009a) and supertags (Haque et al, 2009b) as
syntactic source context features in the log-linear
PB-SMT model.
In addition to the above feature, we derived a
simple binary feature h?best, defined in (2):
h?best =
{
1 if e?k maximizes P (e?k|f?k,CI(f?k))
0 otherwise
(2)
We performed experiments by integrating these
two features, h?MBL and h?best, directly into the
log-linear framework of Moses.
2.6 Hierarchical PB-SMT model
For the en?cs translation task, we built
a weighted synchronous context-free grammar
model (Chiang, 2007) of translation that uses
the bilingual phrase pairs of PB-SMT as a start-
ing point to learn hierarchical rules. We used
the open-source Tree-Based translation system
moses-chart3 to perform this experiment.
2An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl
3http://www.statmt.org/moses/?n=Moses.SyntaxTutorial144
2.7 System Combination
For multiple system combination, we used an
MBR-CN framework (Du et al, 2009, 2010) as
shown in Figure 1. Due to the varying word or-
der in the MT hypotheses, it is essential to define
the backbone which determines the general word
order of the CN. Instead of using a single system
output as the skeleton, we employ an MBR de-
coder to select the best single system output Er
from the merged N -best list by minimizing the
BLEU (Papineni et al, 2002) loss, as in (3):
r = argmin
i
Ns?
j=1
(1? BLEU(Ej , Ei)) (3)
where Ns indicates the number of translations in
the merged N -best list, and {Ei}Nsi=1 are the trans-
lations themselves. In our task, we only merge the
1-best output of each individual system.
The CN is built by aligning other hypotheses
against the backbone, based on the TER metric.
Null words are allowed in the alignment. Ei-
ther votes or different confidence measures are as-
signed to each word in the network. Each arc in
the CN represents an alternative word at that po-
sition in the sentence and the number of votes for
each word is counted when constructing the net-
work. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
We use MERT (Och, 2003) to tune the weights
of the CN.
2.8 Rescoring
Rescoring is a very important part in post-
processing which can select a better hypothesis
from the N -best list. We augmented our previ-
ous rescoring model (Du et al, 2009) with more
large-scale data. The features we used include:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram PoS language model (Schmid,
1994; Ratnaparkhi, 1996);
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT.
3 Experimental Setup
This section describes our experimental setup for
the en?cs and en?es translation tasks.
3.1 Data
Bilingual data: In the experiments we used data
sets provided by the workshop organizers. For the
en?cs translation table extraction we employed
both parallel corpora (News-Commentary10 and
CzEng 0.9), and for the en?es experiments, we
used the Europarl(Koehn, 2005), News Commen-
tary and United Nations parallel data. We used a
maximum sentence length of 80 for en?es and
40 for en?cs. Detailed statistics are shown in Ta-
ble 1.
Corpus Langs. Sent. Source
tokens
Target
tokens
Europarl en?es 1.6M 43M 45M
News-comm en?es 97k 2.4M 2.7M
UN en?es 5.9M 160M 190M
News-Comm en?cs 85k 1.8M 1.6M
CzEng en?cs 7.8M 80M 69M
Table 1: Statistics of en?cs and en?es parallel data.
Monolingual data: For language modeling pur-
poses, in addition to the target parts of the bilin-
gual data, we used the monolingual News corpus
for cs; and the Gigaword corpus for es. For both
languages, we used the SRILM toolkit (Stolcke,
2002) to train a 5-gram language model using all
monolingual data provided. However, for en?es
we used the IRSTLM toolkit (Federico and Cet-
tolo, 2007) to train a 5-gram language model using
the es Gigaword corpus. Both language models
use modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Statistics for the monolingual
corpora are given in Table 2.
Corpus Language Sentences Tokens
E/N/NC/UN es 9,6M 290M
Gigaword es 40M 1,2G
News cs 13M 210M
Table 2: Statistics of Monolingual Data. E/N/NC/UN
refers to Europarl/News/News Commentary/United Nations
corpora.
For all the systems except Apertium, we first
lowercase and tokenize all the monolingual and
bilingual data using the tools provided by the
WMT10 organizers. After translation, system
combination output is detokenised and true-cased.145
3.2 English?Czech (en?cs) Experiments
The CzEng corpus (Bojar and Z?abokrtsky?, 2009)
is a collection of parallel texts from sources of dif-
ferent quality and as such it contains some noise.
As the first step, we discarded those sentence pairs
having more than 10% of non-Latin characters.
The CzEng corpus is quite large (8M sen-
tence pairs). Although we were able to build
a vanilla SMT system on all parallel data avail-
able (News-Commentary + CzEng), we also at-
tempted to build additional systems using News-
Commentary data (which we considered in-
domain) and various in-domain subsets of CzEng
hoping to achieve better results on domain-
specific data.
For our first system, we selected 128,218 sen-
tence pairs from CzEng labeled as news. For the
other two systems, we selected subsets of 2M and
4M sentence pairs identified as most similar to
the development sets (as a sample of in-domain
data) based on cosine similarity of their represen-
tation in a TF-IDF weighted vector space model
(cf. Byrne et al (2003)). We also applied the
pseudo-relevavance-feedback technique for query
expansion (Manning et al, 2008) to select another
subset with 2M sentence pairs.
We used the output of 15 systems for sys-
tem combination for the en?cs translation task.
Among these, 5 systems were built using Moses
and varying the size of the training data (DCU-
All, DCU-Ex2M, DCU-4M, DCU-2M and DCU-
News); 9 context-informed PB-SMT systems
(DCU-SSCI-*) using (combinations of) various
context features (word, PoS, supertags and depen-
dency relations) trained only on the News Com-
mentary data (marked with ? in Table 4); and one
system using the moses-chart decoder, also
trained on the news commentary data.
3.3 English?Spanish (en?es) Experiments
Three baseline systems using Moses were built,
where we varied the amount of training data used:
? epn: This system uses all of the Europarl and
News-Commentary parallel data.
? UN-half: This system uses the data suplied
to ?epn?, plus an additional 2.1M sentences
pairs randomly selected from the United Na-
tions corpus.
? all: This system uses all of the available par-
allel data.
For en?es we also obtained output from the
factored model (trained only on the news com-
mentary corpus) and the Apertium RBMT sys-
tem. We also derived phrase alignments using the
MaTrEx EBMT system (Stroppa and Way, 2006),
and added those phrase translations in the Moses
phrase table. The systems marked with ? use a
language model built using the Spanish Gigaword
corpus, in addition to the one built using the pro-
vided monolingual data. These 6 sets of system
outputs are then used for system combination.
3.4 Experimental Results
The evaluation results for en?es and en?cs ex-
periments are shown in Table 3 and Table 4 re-
spectively. The output of the systems marked ?
were submitted in the shared tasks.
System BLEU NIST METEOR TER
DCU-half ?? 29.77% 7.68 59.86% 59.55%
DCU-all ?? 29.63% 7.66 59.82% 59.74%
DCU-epn ?? 29.45% 7.66 59.71% 59.64%
DCU-ebmt ?? 29.38% 7.62 59.59% 60.11%
DCU-factor 22.58% 6.56 54.94% 67.65%
DCU-apertium 19.22% 6.37 49.68% 67.68%
DCU-system-
combination ? 30.42% 7.78 60.56% 58.71%
Table 3: en?es experimental results.
System BLEU NIST METEOR TER
DCU-All 10.91% 4.60 39.18% 81.76%
DCU-Ex2M 10.63% 4.56 39.12% 81.96%
DCU-4M 10.61% 4.56 39.26% 82.04%
DCU-2M 10.48% 4.58 39.35% 81.56%
DCU-Chart 9.34% 4.25 37.04% 83.87%
DCU-News 8.64% 4.16 36.27% 84.96%
DCU-SSCI-ccg? 8.26% 4.02 34.76% 85.58%
DCU-SSCI-
supertag-pair? 8.11% 3.95 34.93% 86.63%
DCU-SSCI-
ccg-ltag? 8.09% 3.96 34.90% 86.62%
DCU-SSCI-PR? 8.06% 4.00 34.89% 85.99%
DCU-SSCI-base? 8.05% 3.97 34.61% 86.02%
DCU-SSCI-PRIR? 8.03% 3.99 34.81% 85.98%
DCU-SSCI-ltag? 8.00% 3.95 34.57% 86.41%
DCU-SSCI-PoS? 7.91% 3.94 34.57% 86.51%
DCU-SSCI-word? 7.57% 3.88 34.16% 87.14%
DCU-system-
combination ? 13.22% 4.98 40.39% 78.59%
Table 4: en?cs experimental results.
4 Conclusion
This paper presents the Dublin City University
MT system in WMT2010 shared task campaign.
This was DCU?s first attempt to translate from en
to es and cs in any shared task. We developed a
multi-engine framework which combined the out-
puts of several individual MT systems and gener-
ated a new N -best list after CN decoding. Then by146
using some global features, the rescoring model
generated the final translation output. The experi-
mental results demonstrated that the combination
module and rescoring module are effective in our
framework for both language pairs, and produce
statistically significant improvements as measured
by bootstrap resampling methods (Koehn, 2004)
on BLEU over the single best system.
Acknowledgements: This work is supported
by Science Foundation Ireland (Grant No.
07/CE/I1142) and by PANACEA, a 7th Frame-
work Research Programme of the European
Union, contract number 7FP-ITC-248064. M.L.
Forcada?s sabbatical stay at Dublin City Univer-
sity is supported by Science Foundation Ireland
through ETS Walton Award 07/W.1/I1802 and by
the Universitat d?Alacant (Spain).
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics,
92:63?83.
Byrne, W., Khudanpur, S., Kim, W., Kumar, S.,
Pecina, P., Virga, P., Xu, P., and Yarowsky, D.
(2003). The Johns Hopkins University 2003
Chinese?English machine translation system.
In Proceedings of MT Summit IX, pages 447?
450, New Orleans, LA.
Chen, S. F. and Goodman, J. (1996). An Empir-
ical Study of Smoothing Techniques for Lan-
guage Modeling. In Proc. 34th Ann. Meeting of
the Association for Computational Linguistics,
pages 310?318, San Francisco, CA.
Chiang, D. (2007). Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daelemans, W. and van den Bosch, A. (2005).
Memory-Based Language Processing (Studies
in Natural Language Processing). Cambridge
University Press, New York, NY.
Du, J., He, Y., Penkale, S., and Way, A. (2009).
MaTrEx: The DCU MT System for WMT2009.
In Proc. 3rd Workshop on Statistical Machine
Translation, EACL 2009, pages 95?99, Athens,
Greece.
Du, J., Pecina, P., and Way, A. (2010). An
Augmented Three-Pass System Combination
Framework: DCU Combination System for
WMT 2010. In Proc. ACL 2010 Joint Workshop
in Statistical Machine Translation and Metrics
Matr, Uppsala, Greece.
Federico, M. and Cettolo, M. (2007). Efficient
Handling of N-gram Language Models for Sta-
tistical Machine Translation. In Proceedings
of the Second Workshop on Statistical Machine
Translation, pages 88?95, Prague, Czech Re-
public.
Fiscus, J. G. (1997). A post-processing sys-
tem to yield reduced word error rates: Recog-
nizer output voting error reduction (ROVER).
In Proceedings 1997 IEEE Workshop on Auto-
matic Speech Recognition and Understanding
(ASRU), pages 347?352, Santa Barbara, CA.
Forcada, M. L., Tyers, F. M., and Ram??rez-
Sa?nchez, G. (2009). The free/open-source ma-
chine translation platform Apertium: Five years
on. In Proceedings of the First International
Workshop on Free/Open-Source Rule-Based
Machine Translation FreeRBMT?09, pages 3?
10.
Gough, N. and Way, A. (2004). Robust Large-
Scale EBMT with Marker-Based Segmenta-
tion. In Proceedings of the 10th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-04), pages
95?104, Baltimore, MD.
Haque, R., Naskar, S. K., Bosch, A. v. d., and
Way, A. (2009a). Dependency relations as
source context in phrase-based smt. In Proc.
23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 170?179,
Hong Kong, China.
Haque, R., Naskar, S. K., Ma, Y., and Way, A.
(2009b). Using supertags as source language
context in SMT. In EAMT-2009: Proceed-
ings of the 13th Annual Conference of the Eu-
ropean Association for Machine Translation,
pages 234?241, Barcelona, Spain.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Proceedings
of EMNLP, volume 4, pages 388?395.
Koehn, P. (2005). Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit X, pages 79?86, Phuket,
Thailand.
Koehn, P. and Hoang, H. (2007). Factored Trans-
lation Models. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural147
Language Learning (EMNLP-CoNLL), pages
868?876, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague,
Czech Republic.
Kumar, S. and Byrne, W. (2004). Minimum
Bayes-Risk Decoding for Statistical Machine
Translation. In Proceedings of the Joint Meet-
ing of the Human Language Technology Con-
ference and the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL 2004), pages 169?176, Boston,
MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Find-
ing consensus in speech recognition: Word er-
ror minimization and other applications of con-
fusion networks. Computer Speech and Lan-
guage, 14(4):373?400.
Manning, C. D., Raghavan, P., and Schu?tze, H.
(2008). Introduction to Information Retrieval.
Cambridge University Press.
Och, F. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan.
Och, F. and Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In Proceedings of ACL,
volume 2, pages 295?302.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages
311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy
Model for Part-Of-Speech Tagging. In Pro-
ceedings of the Empirical Methods in Natural
Language Processing Conference (EMNLP),
pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S.,
Schwartz, R., Ayan, N. F., and Dorr, B. J.
(2007). Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chap-
ter of the Association for Computational Lin-
guistics (HLT-NAACL 2007), pages 228?235,
Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing, pages 44?49, Manch-
ester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Amer-
icas (AMTA 2006), pages 223?231, Cambridge,
MA.
Stolcke, A. (2002). SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of
the International Conference Spoken Language
Processing, pages 901?904, Denver, CO.
Stroppa, N., van den Bosch, A., and Way, A.
(2007). Exploiting Source Similarity for SMT
using Context-Informed Features. In Proceed-
ings of the 11th International Conference on
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-07), pages 231?240,
Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the
DCU machine translation system for IWSLT
2006. In Proceedings of the International Work-
shop on Spoken Language Translation, pages
31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Poste-
rior Probabilities for Statistical Machine Trans-
lation. In Proceedings of the Joint Meeting of
the Human Language Technology Conference
and the North American Chapter of the As-
sociation for Computational Linguistics (HLT-
NAACL 2006), pages 72?77, New York, NY.
148
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 290?295,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
An Augmented Three-Pass System Combination Framework:
DCU Combination System for WMT 2010
Jinhua Du, Pavel Pecina, Andy Way
CNGL, School of Computing
Dublin City University
Dublin 9, Ireland
{jdu,ppecina,away}@computing.dcu.ie
Abstract
This paper describes the augmented three-
pass system combination framework of
the Dublin City University (DCU) MT
group for the WMT 2010 system combi-
nation task. The basic three-pass frame-
work includes building individual confu-
sion networks (CNs), a super network, and
a modified Minimum Bayes-risk (mCon-
MBR) decoder. The augmented parts for
WMT2010 tasks include 1) a rescoring
component which is used to re-rank the
N -best lists generated from the individual
CNs and the super network, 2) a new hy-
pothesis alignment metric ? TERp ? that
is used to carry out English-targeted hy-
pothesis alignment, and 3) more differ-
ent backbone-based CNs which are em-
ployed to increase the diversity of the
mConMBR decoding phase. We took
part in the combination tasks of English-
to-Czech and French-to-English. Exper-
imental results show that our proposed
combination framework achieved 2.17 ab-
solute points (13.36 relative points) and
1.52 absolute points (5.37 relative points)
in terms of BLEU score on English-to-
Czech and French-to-English tasks re-
spectively than the best single system. We
also achieved better performance on hu-
man evaluation.
1 Introduction
In several recent years, system combination has
become not only a research focus, but also a pop-
ular evaluation task due to its help in improving
machine translation quality. Generally, most com-
bination approaches are based on a confusion net-
work (CN) which can effectively re-shuffle the
translation hypotheses and generate a new target
sentence. A CN is essentially a directed acyclic
graph built from a set of translation hypotheses
against a reference or ?backbone?. Each arc be-
tween two nodes in the CN denotes a word or to-
ken, possibly a null item, with an associated pos-
terior probability.
Typically, the dominant CN is constructed at the
word level by a state-of-the-art framework: firstly,
a minimum Bayes-risk (MBR) decoder (Kumar
and Byrne, 2004) is utilised to choose the back-
bone from a merged set of hypotheses, and then
the remaining hypotheses are aligned against the
backbone by a specific alignment approach. Cur-
rently, most research in system combination has
focused on hypothesis alignment due to its signif-
icant influence on combination quality.
A multiple CN or ?super-network? framework
was firstly proposed in Rosti et al (2007) who
used each of all individual system results as the
backbone to build CNs based on the same align-
ment metric, TER (Snover et al, 2006). A consen-
sus network MBR (ConMBR) approach was pre-
sented in (Sim et al, 2007), where MBR decod-
ing is employed to select the best hypothesis with
the minimum cost from the original single system
outputs compared to the consensus output.
Du and Way (2009) proposed a combination
strategy that employs MBR, super network, and
a modified ConMBR (mConMBR) approach to
construct a three-pass system combination frame-
work which can effectively combine different hy-
pothesis alignment results and easily be extended
to more alignment metrics. Firstly, a number of
individual CNs are built based on different back-
bones and different kinds of alignment metrics.
Each network generates a 1-best output. Secondly,
a super network is constructed combining all the
individual networks, and a consensus is generated
based on a weighted search model. In the third290
pass, all the 1-best hypotheses coming from sin-
gle MT systems, individual networks, and the su-
per network are combined to select the final result
using the mConMBR decoder.
In the system combination task of WMT 2010,
we adopted an augmented framework by extend-
ing the strategy in (Du and Way, 2009). In addi-
tion to the basic three-pass architecture, we aug-
ment our combination system as follows:
? We add a rescoring component in Pass 1 and
Pass 2.
? We introduce the TERp (Snover et al, 2009)
alignment metric for the English-targeted
combination.
? We employ different backbones and hypothe-
sis alignment metrics to increase the diversity
of candidates for our mConMBR decoding.
The remainder of this paper is organised as fol-
lows. In Section 2, we introduce the three hy-
pothesis alignment methods used in our frame-
work. Section 3 details the steps for building our
augmented three-pass combination framework. In
Section 4, a rescoring model with rich features
is described. Then, Sections 5 and 6 respec-
tively report the experimental settings and exper-
imental results on English-to-Czech and French-
to-English combination tasks. Section 7 gives our
conclusions.
2 Hypothesis Alignment Methods
Hypothesis alignment plays a vital role in the CN,
as the backbone sentence determines the skeleton
and the word order of the consensus output.
In the combination evaluation task, we inte-
grated TER (Snover et al, 2006), HMM (Ma-
tusov et al, 2006) and TERp (Snover et al,
2009) into our augmented three-pass combination
framework. In this section, we briefly describe
these three methods.
2.1 TER
The TER (Translation Edit Rate) metric measures
the ratio of the number of edit operations between
the hypothesis E? and the reference Eb to the total
number of words in Eb. Here the backbone Eb is
assumed to be the reference. The allowable edits
include insertions (Ins), deletions (Del), substitu-
tions (Sub), and phrase shifts (Shft). The TER of
E? compared to Eb is computed as in (1):
TER(E?, Eb) = Ins + Del + Sub + ShftNb ? 100% (1)
where Nb is the total number of words in Eb. The
difference between TER and Levenshtein edit dis-
tance (or WER) is the sequence shift operation al-
lowing phrasal shifts in the output to be captured.
The phrase shift edit is carried out by a greedy
algorithm and restricted by three constraints: 1)
The shifted words must exactly match the refer-
ence words in the destination position. 2) The
word sequence of the hypothesis in the original
position and the corresponding reference words
must not exactly match. 3) The word sequence
of the reference that corresponds to the desti-
nation position must be misaligned before the
shift (Snover et al, 2006).
2.2 HMM
The hypothesis alignment model based on HMM
(Hidden Markov Model) considers the align-
ment between the backbone and the hypoth-
esis as a hidden variable in the conditional
probability Pr(E?|Eb). Given the backbone
Eb = {e1, . . . , eI} and the hypothesis E? =
{e?1, . . . , e?J}, which are both in the same lan-
guage, the probability Pr(E?|Eb) is defined as in
(2):
Pr(E?|Eb) =
?
A
Pr(E?, A|Eb) (2)
where the alignemnt A ? {(j, i) : 1 ? j ?
J ; 1 ? i ? I}, i and j represent the word po-
sition in Eb and E? respectively. Hence, the align-
ment issue is to seek the optimum alignment A?
such that:
A? = argmax
A
P (A|eI1, e?J1 ) (3)
For the HMM-based model, equation (2) can be
represented as in (4):
Pr(E?|Eb) =
?
aJj
J?
j=1
[p(aj |aj?1, I) ? p(e?j |eaj )] (4)
where p(aj |aj?1, I) is the alignment probability
and p(e?j |ei) is the translation probability.
2.3 TER-Plus
TER-Plus (TERp) is an extension of TER that
aligns words in the hypothesis and reference not
only when they are exact matches but also when
the words share a stem or are synonyms (Snover
et al, 2009). In addition, it uses probabilistic
phrasal substitutions to align phrases in the hy-
pothesis and reference. In contrast to the use of291
the constant edit cost for all operations such as
shifts, insertion, deleting or substituting in TER,
all edit costs in TERp are optimized to maximize
correlation with human judgments.
TERp uses all the edit operations of TER ?
matches, insertions, deletions, substitutions, and
shifts ? as well as three new edit operations:
stem matches, synonym matches, and phrase sub-
stitutions (Snover et al, 2009). TERp employs
the Porter stemming algorithm (Porter, 1980) and
WordNet (Fellbaum, 1998) to perform the ?stem
match? and ?synonym match? respectively. Se-
quences of words in the reference are considered
to be paraphrases of a sequence of words in the
hypothesis if that phrase pair occurs in the TERp
phrase table (Snover et al, 2009).
In our experiments, TERp was used for the
French-English system combination task, and we
used the default configuration of optimised edit
costs.
3 Augmented Three-Pass Combination
Framework
The construction of the augmented three-pass
combination framework is shown in Figure 1.
Hypotheses Set
BLEU TER TERp
MBR
BLEU TER TERp
Top M Single
HMM TER TERp
Alignment
Individual CNs
Nbest 
Re-ranking Super CN Networks
mConMBR
Pass 1
Pass 2
Pass 3
N Single MT 
Systems
Figure 1: Three-Pass Combination Framework
In Figure 1, the dashed boxes labeled ?TERp?
indicate that the TERp alignment is only appli-
cable for English-targeted hypothesis alignment.
The lines with arrows pointing to ?mConMBR?
represent adding outputs into the mConMBR de-
coding component. ?Top M Single? indicates that
the 1-best results from the best M individual MT
systems are also used as backbones to build in-
dividual CNs under different alignment metrics.
The three dashed boxes represent Pass 1, Pass 2
and Pass 3 respectively. The steps can be sum-
marised as follows:
Pass 1: Specific Metric-based Single Networks
1. Merge all the 1-best hypotheses from single
MT systems into a new N -best set Ns.
2. Utilise the standard MBR decoder to se-
lect one from the Ns as the backbone given
some specific loss function such as TER,
BLEU (Papineni et al, 2002) and TERp; Ad-
ditionally, in order to increase the diversity
of candidates used for Pass 2 and Pass 3, we
also use the 1-best hypotheses from the top
M single MT systems as the backbone. Add
the backbones generated by MBR into Ns.
3. Perform the word alignment between the dif-
ferent backbones and the other hypotheses
via the TER, HMM, TERp (only for English)
metrics.
4. Carry out word reordering based on word
alignment (TER and TERp have completed
the reordering in the process of scoring) and
build individual CNs (Rosti et al, 2007);
5. Decode the single networks and export the 1-
best outputs and the N -best lists separately.
Add these 1-best outputs into Ns.
Pass 2: Super-Network
1. Connect the single networks using a start
node and an end node to form a super-
network based on multiple hypothesis align-
ment and different backbones. In this evalu-
ation, we set uniform weights for these dif-
ferent individual networks when building the
super network(Du and Way, 2009).
2. Decode the super network and generate a
consensus output as well as the N -best list.
Add the 1-best result into Ns.
3. Rescore the N -best lists from all individual
networks and super network and add the new
1-best results into Ns.
Pass 3: mConMBR
1. Rename the set Ns as a new set Ncon;
2. Use mConMBR decoding to search for the
best final result from Ncon. In this step, we
set a uniform distribution between the candi-
dates in Ncon.292
4 Rescoring Model
We adapted our previous rescoring model (Du
et al, 2009) to larger-scale data. The features we
used are as follows:
? Direct and inverse IBM model;
? 4-gram and 5-gram target language model;
? 3, 4, and 5-gram Part-of-Speech (POS) lan-
guage model (Schmid, 1994; Ratnaparkhi,
1996);
? Sentence-length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
best list (Zens and Ney, 2006);
? Minimum Bayes Risk cost. This process is
similar to the calculation of the MBR decod-
ing in which we take the current hypothesis
in the N -best list as the ?backbone?, and then
calculate and sum up all the Bayes risk cost
between the backbone and each of the rest of
the N -best list using BLEU metric as the loss
function;
? Length ratio between source and target sen-
tence.
The weights are optimized via the MERT algo-
rithm (Och, 2003).
5 Experimental Settings
We participated in the English?Czech and
French?English system combination tasks.
In our system combination framework, we use
a large-scale monolingual data to train language
models and carry out POS-tagging.
5.1 English-Czech
Training Data
The statistics of the data used for language models
training are shown in Table 1.
Monolingual Number of
Corpus tokens (Cz) sentences
News-Comm 2,214,757 84,706
CzEng 81,161,278 8,027,391
News 205,600,053 13,042,040
Total 288,976,088 21,154,137
Table 1: Statistics of data in the En?Cz task
All the data are provided by the workshop
organisers. 1 In Table 1, ?News-Comm? indi-
cates the data set of News-Commentary v1.0 and
1http://www.statmt.org/wmt10/translation-task.html
?CzEng? is the Czech?English corpus v0.9 (Bo-
jar and Z?abokrtsky?, 2009). ?News? is the Czech
monolingual News corpus.
As to our CN and rescoring components,
we use ?News-Comm+CzEng? to train a
4-gram language model and use ?News-
Comm+CzEng+News? to train a 5-gram
language model. Additionally, we per-
form POS tagging (Hajic?, 2004) for ?News-
Comm+CzEng+News? data, and train 3-gram,
4-gram, and 5-gram POS-tag language models.
Devset and Testset
The devset includes 455 sentences and the testset
contains 2,034 sentences. Both data sets are pro-
vided by the workshop organizers. Each source
sentence has only one reference. There are 11 MT
systems in the En-Cz track and we use all of them
in our combination experiments.
5.2 French-English
Training Data
The statistics of the data used for language models
training and POS tagging are shown in Table 2.
Monolingual Number of
Corpus tokens (En) sentences
News-Comm 2,973,711 125,879
Europarl 50,738,215 1,843,035
News 1,131,527,255 48,648,160
Total 1,184,234,384 50,617,074
Table 2: Statistics of data in the Fr?En task
?News? is the English monolingual News
corpus. We use ?News-Comm+Europarl? to
train a 4-gram language model and use ?News-
Comm+Europarl+News? to train a 5-gram lan-
guage model. We also perform POS tagging (Rat-
naparkhi, 1996) for all available data, and train
3-gram, 4-gram and, 5-gram POS-tag language
models.
Devset and Testset
We also use all the 1-best results to carry out sys-
tem combination. There are 14 MT systems in the
Fr-En track and we use all of them in our combi-
nation experiments.
6 Experimental Results
In this section, all the results are reported on de-
vsets in terms of BLEU and NIST scores.
6.1 English?Czech
In this task, we only used one hypothesis align-
ment method ? TER ? to carry out hypothesis293
alignment. However, in order to increase diversity
for our 3-pass framework, in addition to using the
output from MBR decoding as the backbone, we
also separately selected the top 4 individual sys-
tems (SYS1, SYS4, SYS6, and SYS11 in our sys-
tem set) in terms of BLEU scores on the devset as
the backbones so that we can build multiple indi-
vidual CNs for the super network. All the results
are shown in Table 3.
SYS BLEU4 NIST
Worst 9.09 3.83
Best 17.28 4.99
SYS1 15.11 4.76
SYS4 12.67 4.40
SYS6 17.28 4.99
SYS11 15.75 4.81
CN-SYS1 17.36 5.12
CN-SYS4 16.94 5.10
CN-SYS6 17.91 5.13
CN-SYS11 17.45 5.09
CN-MBR 18.29 5.15
SuperCN 18.44 5.17
mConMBR-BAS 18.60 5.18
mConMBR-New 18.84 5.11
Table 3: Automatic evaluation of the combination
results on the En-Cz devset.
?Worst? indicates the 1-best hypothesis from
the worst single system, the ?Best? is the 1-best
hypothesis from the best single system (SYS11)).
?CN-SYSX? denotes that we use SYSX (X =
1, 4, 6, 11 and MBR) as the backbone to build an
individual CN. ?mConMBR-BAS? stands for the
original three-pass combination framework with-
out rescoring component, while ?mConMBR-
New? indicates the proposed augmented combina-
tion framework. It can be seen from Table 3 that 1)
in all individual CNs, the CN-MBR achieved the
best performance; 2) SuperCN and mConMBR-
New improved by 1.16 (6.71% relative) and 1.56
(9.03% relative) absolute BLEU points compared
to the best single MT system. 3) our new
three-pass combination framework achieved the
improvement of 0.24 absolute (1.29% relative)
BLEU points than the original framework.
The final results on the test set are shown in Ta-
ble 4.
SYS BLEU4 human eval.(%win)
Best 16.24 70.38
mConMBR-BAS 17.91 -
mConMBR-New 18.41 2 75.17
Table 4: Evaluation of the combination results on
the En-Cz testset.
It can be seen that our ?mConMBR-New?
framework performs better than the best single
system and our original framework ?mConMBR-
BAS? in terms of automatic BLEU scores and hu-
man evaluation for the English-to-Czech task. In
this task campaign, we achieved top 1 in terms of
the human evaluation.
6.2 French?English
We used three hypothesis alignment methods ?
TER, TERp and HMM ? to carry out word align-
ment between the backbone and the rest of the
hypotheses. Apart from the backbone generated
from MBR, we separately select the top 5 individ-
ual systems (SYS1, SYS10, SYS11, SYS12, and
SYS13 in our system set) respectively as the back-
bones using HMM, TER and TERp to carry out
hypothesis alignment so that we can build more
individual CNs for the super network to increase
the diversity of candidates for mConMBR. The re-
sults are shown in Table 5.3
SYS BLEU4(%) NIST
Worst 15.04 4.97
Best 28.88 6.71
CN-SYS1-TER 29.56 6.78
CN-SYS1-HMM 29.60 6.84
CN-SYS1-TERp 29.77 6.83
CN-MBR-TER 30.16 6.91
CN-MBR-HMM 30.19 6.92
CN-MBR-TERp 30.27 6.92
SuperCN 30.58 6.90
mConMBR-BAS 30.74 7.01
mConMBR-New 31.02 6.96
Table 5: Automatic evaluation of the combination
results on the Fr-En devset.
?CN-MBR-X? represents the different possi-
ble hypothesis alignment methods (X = {TER,
HMM, TERp}) which are used to build indi-
vidual CNs using the output from MBR de-
coding as the backbone. We can see that the
SuperCN and mConMBR-New respectively im-
proved by 1.7 absolute (5.89% relative) and 2.88
absolute (9.97% relative) BLEU points compared
to the best single system. Furthermore, our aug-
mented framework ?mConMBR-New? achieved
the improvement of 0.28 absolute (0.91% relative)
BLEU points than the original three-pass frame-
work as well.
2This score was measured in-house on the refer-
ence provided by the organizer using metric mteval-v13
(ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl).
3In this Table, we take SYS1 as an example to show the
results using a single MT system as the backbone under the
three alignment metrics.294
The final results on the test set are shown in Ta-
ble 6.
SYS BLEU4 human eval.(%win)
Best 28.30 66.84
mConMBR-BAS 29.21 -
mConMBR-New 29.82 2 72.15
Table 6: Evaluation of the combination results on
Fr-En test set.
It can be seen that our ?mConMBR-New?
framework performs the best than the best single
system and our original framework ?mConMBR-
BAS? in terms of automatic BLEU scores and hu-
man evaluation for the French?English task.
7 Conclusions and Future Work
We proposed an augmented three-pass mul-
tiple system combination framework for the
WMT2010 system combination shared task. The
augmented parts include 1) a rescoring model to
select the potential 1-best result from the indi-
vidual CNs and super network to increase the di-
versity for ?mConMBR? decoding; 2) a new hy-
pothesis alignment metric ?TERp? for English-
targeted alignment; 3) 1-best results from the top
M individual systems employed to build CNs
to augment the ?mConMBR? decoding. We
took part in the English-to-Czech and French-to-
English tasks. Experimental results reported on
test set of these two tasks showed that our aug-
mented framework performed better than the best
single system in terms of BLEU scores and hu-
man evaluation. Furthermore, the proposed aug-
mented framework achieved better results than our
basic three-pass combination framework (Du and
Way, 2009) as well in terms of automatic evalua-
tion scores. In the released preliminary results, we
achieved top 1 and top 3 for the English-to-Czech
and French-to-English tasks respectively in terms
of human evaluation.
As for future work, firstly we plan to do further
experiments using automatic weight-tuning algo-
rithm to tune our framework. Secondly, we plan
to examine how the differences between the hy-
pothesis alignment metrics impact on the accuracy
of the super network. We also intend to integrate
more alignment metrics to the networks and verify
on the other language pairs.
Acknowledgments
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University
and has been partially funded by PANACEA, a 7th Frame-
work Research Programme of the European Union (contract
number: 7FP-ITC-248064) as well as partially supported by
the project GA405/09/0278 of the Grant Agency of the Czech
Republic. Thanks also to the reviewers for their insightful
comments.
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9: Large Par-
allel Treebank with Rich Annotation. Prague Bulletin of
Mathematical Linguistics, 92.
Du, J., He, Y., Penkale, S., and Way, A. (2009). MaTrEx:
The DCU MT System for WMT2009. In Proceedings of
the EACL-WMT 2009, pages 95?99, Athens, Greece.
Du, J. and Way, A. (2009). A Three-pass System Com-
bination Framework by Combining Multiple Hypothesis
Alignment Methods. In Proceedings of the International
Conference on Asian Language Processing (IALP), pages
172?176, Singapore.
Fellbaum, C., editor (1998). WordNet: an electronic lexical
database. MIT Press.
Hajic?, J. (2004). Disambiguation of Rich Inflection (Compu-
tational Morphology of Czech), volume 1. Charles Uni-
versity Press, Prague.
Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk De-
coding for Statistical Machine Translation. In Proceed-
ings of the HLT-NAACL 2004, pages 169?176, Boston,
MA.
Matusov, E., Ueffing, N., and Ney, H. (2006). Computing
consensus translation from multiple machine translation
systems using enhanced hypotheses alignment. In Pro-
ceedings of EACL?06, pages 33?40.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a Method for Automatic Evaluation of Machine
Translation. In Proceedings of the ACL-02, pages 311?
318, Philadelphia, PA.
Porter, M. F. (1980). An algorithm for suffix stripping, pro-
gram.
Ratnaparkhi, A. (1996). A Maximum Entropy Model
for Part-of-Speech Tagging. In Proceedings of the
EMNLP?96, pages 133?142, Philadelphia, PA.
Rosti, A., Matsoukas, S., and Schwartz, R. (2007). Improved
Word-Level System Combination for Machine Transla-
tion. In Proceedings of ACL?07, pages 312?319.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In Proceedings of International Con-
ference on New Methods in Language Processing, pages
44?49, Manchester, UK.
Sim, K., Byrne, W., Gales, M., Sahbi, H., and Woodland, P.
(2007). Consensus network decoding for statistical ma-
chine translation system combination. In Proceedings of
the ICASSP?07, pages 105?108.
Snover, M., Dorr, B., Schwartz, R., Micciula, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In Proceedings of the
AMTA?06), pages 223?231, Cambridge, MA.
Snover, M., Madnani, N., J.Dorr, B., and Schwartz, R.
(2009). Fluency, adequacy, or HTER? Exploring different
human judgments with a tunable MT metric. In Proceed-
ings of the WMT?09, pages 259?268, Athens, Greece.
Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities
for Statistical Machine Translation. In Proceedings of the
HLT-NAACL?06), pages 72?77, New York, USA.
295
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349?353,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The DCU Dependency-Based Metric in WMT-MetricsMATR 2010
Yifan He Jinhua Du Andy Way Josef van Genabith
Centre for Next Generation Localisation
School of Computing
Dublin City University
Dublin 9, Ireland
{yhe,jdu,away,josef}@computing.dcu.ie
Abstract
We describe DCU?s LFG dependency-
based metric submitted to the shared eval-
uation task of WMT-MetricsMATR 2010.
The metric is built on the LFG F-structure-
based approach presented in (Owczarzak
et al, 2007). We explore the following
improvements on the original metric: 1)
we replace the in-house LFG parser with
an open source dependency parser that
directly parses strings into LFG depen-
dencies; 2) we add a stemming module
and unigram paraphrases to strengthen the
aligner; 3) we introduce a chunk penalty
following the practice of METEOR to re-
ward continuous matches; and 4) we intro-
duce and tune parameters to maximize the
correlation with human judgement. Exper-
iments show that these enhancements im-
prove the dependency-based metric?s cor-
relation with human judgement.
1 Introduction
String-based automatic evaluation metrics such as
BLEU (Papineni et al, 2002) have led directly
to quality improvements in machine translation
(MT). These metrics provide an alternative to ex-
pensive human evaluations, and enable tuning of
MT systems based on automatic evaluation results.
However, there is widespread recognition in
the MT community that string-based metrics are
not discriminative enough to reflect the translation
quality of today?s MT systems, many of which
have gone beyond pure string-based approaches
(cf. (Callison-Burch et al, 2006)).
With that in mind, a number of researchers have
come up with metrics which incorporate more so-
phisticated and linguistically motivated resources.
Examples include METEOR (Banerjee and Lavie,
2005; Lavie and Denkowski, 2009) and TERP
(Snover et al, 2010), both of which now uti-
lize stemming, WordNet and paraphrase informa-
tion. Experimental and evaluation campaign re-
sults have shown that these metrics can obtain bet-
ter correlation with human judgements than met-
rics that only use surface-level information.
Given that many of today?s MT systems incor-
porate some kind of syntactic information, it was
perhaps natural to use syntax in automatic MT
evaluation as well. This direction was first ex-
plored by (Liu and Gildea, 2005), who used syn-
tactic structure and dependency information to go
beyond the surface level matching.
Owczarzak et al (2007) extended this line of
research with the use of a term-based encoding of
Lexical Functional Grammar (LFG:(Kaplan and
Bresnan, 1982)) labelled dependency graphs into
unordered sets of dependency triples, and calculat-
ing precision, recall, and F-score on the triple sets
corresponding to the translation and reference sen-
tences. With the addition of partial matching and
n-best parses, Owczarzak et al (2007)?s method
considerably outperforms Liu and Gildea?s (2005)
w.r.t. correlation with human judgement.
The EDPM metric (Kahn et al, 2010) im-
proves this line of research by using arc labels
derived from a Probabilistic Context-Free Gram-
mar (PCFG) parse to replace the LFG labels,
showing that a PCFG parser is sufficient for pre-
processing, compared to a dependency parser in
(Liu and Gildea, 2005) and (Owczarzak et al,
2007). EDPM also incorporates more information
sources: e.g. the parser confidence, the Porter
stemmer, WordNet synonyms and paraphrases.
Besides the metrics that rely solely on the de-
pendency structures, information from the depen-
dency parser is a component of some other metrics
that use more diverse resources, such as the textual
entailment-based metric of (Pado et al, 2009).
In this paper we extend the work of (Owczarzak
et al, 2007) in a different manner: we use an
349
adapted version of the Malt parser (Nivre et al,
2006) to produce 1-best LFG dependencies and
allow triple matches where the dependency la-
bels are different. We incorporate stemming, syn-
onym and paraphrase information as in (Kahn et
al., 2010), and at the same time introduce a chunk
penalty in the spirit of METEOR to penalize dis-
continuous matches. We sort the matches accord-
ing to the match level and the dependency type,
and weight the matches to maximize correlation
with human judgement.
The remainder of the paper is organized as fol-
lows. Section 2 reviews the dependency-based
metric. Sections 3, 4, 5 and 6 introduce our im-
provements on this metric. We report experimen-
tal results in Section 7 and conclude in Section 8.
2 The Dependency-Based Metric
In this section, we briefly review the metric pre-
sented in (Owczarzak et al, 2007).
2.1 C-Structure and F-Structure in LFG
In Lexical Functional Grammar (Kaplan and Bres-
nan, 1982), a sentence is represented as both a hi-
erarchical c-(onstituent) structure which captures
the phrasal organization of a sentence, and a f-
(unctional) structure which captures the functional
relations between different parts of the sentence.
Our metric currently only relies on the f-structure,
which is encoded as labeled dependencies in our
metric.
2.2 MT Evaluation as Dependency Triple
Matching
The basic method of (Owczarzak et al, 2007) can
be illustrated by the example in Table 1.
The metric in (Owczarzak et al, 2007) performs
triple matching over the Hyp- and Ref-Triples and
calculates the metric score using the F-score of
matching precision and recall. Let m be the num-
ber of matches, h be the number of triples in the
hypothesis and e be the number of triples in the
reference. Then we have the matching precision
P = m/h and recall R = m/e. The score of the
hypothesis in (Owczarzak et al, 2007) is the F-
score based on the precision and recall of match-
ing as in (1):
Fscore = 2PRP +R (1)
Table 1: Sample Hypothesis and Reference
Hypothesis
rice will be held talks in egypt next week
Hyp-Triples
adjunct(will, rice)
xcomp(will, be)
adjunct(talks, held)
xcomp(be, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
adjunct(talks, week)
Reference
rice to hold talks in egypt next week
Ref-Triples
obl(rice, to)
obj(hold, to)
adjunct(week, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
obj(hold, week)
2.3 Details of the Matching Strategy
(Owczarzak et al, 2007) uses several techniques
to facilitate triple matching. First of all, consider-
ing that the MT-generated hypotheses have vari-
able quality and are sometimes ungrammatical,
the metric will search the 50-best parses of both
the hypothesis and reference and use the pair that
has the highest F-score to compensate for parser
noise.
Secondly, the metric performs complete or par-
tial matching according to the dependency labels,
so the metric will find more matches on depen-
dency structures that are presumably more infor-
mative.
More specifically, for all except the LFG
Predicate-Only labeled triples of the form
dep(head, modifier), the method does not
allow a match if the dependency labels (deps)
are different, thus enforcing a complete match.
For the Predicate-Only dependencies, par-
tial matching is allowed: i.e. two triples are con-
sidered identical even if only the head or the
modifier are the same.
Finally, the metric also uses linguistic resources
for better coverage. Besides using WordNet syn-
onyms, the method also uses the lemmatized out-
put of the LFG parser, which is equivalent to using
350
an English lemmatizer.
If we do not consider these additional lin-
guistic resources, the metric would find the fol-
lowing matches in the example in Table 1:
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next), as these three
triples appear both in the reference and in the hy-
pothesis.
2.4 Points for Improvement
We see several points for improvement from Table
1 and the analysis above.
? More linguistic resources: we can use more
linguistic resources than WordNet in pursuit
of better coverage.
? Using the 1-best parse instead of 50-best
parses: the parsing model we currently use
does not produce k-best parses and using only
the 1-best parse significantly improves the
speed of triple matching. We allow ?soft?
triple matches to capture the triple matches
which we might otherwise miss using the 1-
best parse.
? Rewarding continuous matches: it
would be more desirable to reflect
the fact that the 3 matching triples
adjunct(talks, in), obj(in,
egypt) and adjunct(week, next)
are continuous in Table 1.
We introduce our improvements to the metric
in response to these observations in the following
sections.
3 Producing and Matching LFG
Dependency Triples
3.1 The LFG Parser
The metric described in (Owczarzak et al, 2007)
uses the DCU LFG parser (Cahill et al, 2004)
to produce LFG dependency triples. The parser
uses a Penn treebank-trained parser to produce
c-structures (constituency trees) and an LFG f-
structure annotation algorithm on the c-structure
to obtain f-structures. In (Owczarzak et al, 2007),
triple matching on f-structures produced by this
paradigm correlates well with human judgement,
but this paradigm is not adequate for the WMT-
MetricsMatr evaluation in two respects: 1) the in-
house LFG annotation algorithm is not publicly
available and 2) the speed of this paradigm is not
satisfactory.
We instead use the Malt Parser1 (Nivre et al,
2006) with a parsing model trained on LFG de-
pendencies to produce the f-structure triples. Our
collaborators2 first apply the LFG annotation algo-
rithm to the Penn Treebank training data to obtain
f-structures, and then the f-structures are converted
into dependency trees in CoNLL format to train
the parsing model. We use the liblinear (Fan et
al., 2008) classification module to for fast parsing
speed.
3.2 Hard and Soft Dependency Matching
Currently our parser produces only the 1-best
outputs. Compared to the 50-best parses in
(Owczarzak et al, 2007), the 1-best parse limits
the number of triple matches that can be found. To
compensate for this, we allow triple matches that
have the same Head and Modifier to consti-
tute a match, even if their dependency labels are
different. Therefore for triples Dep1(Head1,
Mod1) and Dep2(Head2, Mod2), we allow
three types of match: a complete match if
the two triples are identical, a partial match if
Dep1=Dep2 and Head1=Head2, and a soft
match if Head1=Head2 and Mod1=Mod2.
4 Capturing Variations in Language
In (Owczarzak et al, 2007), lexical variations at
the word-level are captured by WordNet. We
use a Porter stemmer and a unigram paraphrase
database to allow more lexical variations.
With these two resources combined, there are
four stages of word level matching in our sys-
tem: exact match, stem match, WordNet match and
unigram paraphrase match. The stemming mod-
ule uses Porter?s stemmer implementation3 and the
WordNet module uses the JAWS WordNet inter-
face.4 Our metric only considers unigram para-
phrases, which are extracted from the paraphrase
database in TERP5 using the script in the ME-
TEOR6 metric.
1http://maltparser.org/index.html
2O?zlem C?etinog?lu and Jennifer Foster at the National
Centre for Language Technology, Dublin City University
3http://tartarus.org/?martin/
PorterStemmer/
4http://lyle.smu.edu/?tspell/jaws/
index.html
5http://www.umiacs.umd.edu/?snover/
terp/
6http://www.cs.cmu.edu/?alavie/METEOR/
351
5 Adding Chunk Penalty to the
Dependency-Based Metric
The metric described in (Owczarzak et al, 2007)
does not explicitly consider word order and flu-
ency. METEOR, on the other hand, utilizes this in-
formation through a chunk penalty. We introduce
a chunk penalty to our dependency-based metric
following METEOR?s string-based approach.
Given a reference r = wr1...wrn, we denote
wri as ?covered? if it is the head or modifier of
a matched triple. We only consider the wris that
appear as head or modifier in the reference
triples. After this notation, we follow METEOR?s
approach by counting the number of chunks in
the reference string, where a chunk wrj ...wrk is
a sequence of adjacent covered words in the refer-
ence. Using the hypothesis and reference in Ta-
ble 1 as an example, the three matched triples
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next) will cover a con-
tinuous word sequence in the reference (under-
lined), constituting one single chunk:
rice to hold talks (in) egypt next week
Based on this observation, we introduce a simi-
lar chunk penalty Pen as in METEOR in our met-
ric, as in 2:
Pen = ? ? ( #chunks#matches )
? (2)
where ? and ? are free parameters, which we tune
in Section 6.2. We add this penalty to the depen-
dency based metric (cf. Eq. (1)), as in Eq. (3).
score = (1? Pen) ? Fscore (3)
6 Parameter Tuning
6.1 Parameters of the Metric
In our metric, dependency triple matches can be
categorized according to many criteria. We as-
sume that some matches are more critical than
others and encode the importance of matches by
weighting them differently. The final match will
be the sum of weighted matches, as in (4):
m =
?
?tmt (4)
where ?t and mt are the weight and number of
match category t. We categorize a triple match ac-
cording to three perspectives: 1) the level of match
L={complete, partial}; 2) the linguistic resource
used in matching R={exact, stem, WordNet, para-
phrase}; and 3) the type of dependency D. To
avoid too large a number of parameters, we only
allow a set of frequent dependency types, along
with the type other, which represents all the other
types and the type soft for soft matches. We have
D={app, subj, obj, poss, adjunct, topicrel, other,
soft}.
Therefore for each triple match m, we can have
the type of the match t ? L?R?D.
6.2 Tuning
In sum, we have the following parameters to tune
in our metric: precision weight ?, chunk penalty
parameters ?, ?, and the match type weights
?1...?n. We perform Powell?s line search (Press et
al., 2007) on the sufficient statistics of our metric
to find the set of parameters that maximizes Pear-
son?s ? on the segment level. We perform the op-
timization on the MT06 portion of the NIST Met-
ricsMATR 2010 development set with 2-fold cross
validation.
7 Experiments
We experiment with four settings of the metric:
HARD, SOFT, SOFTALL and WEIGHTED in or-
der to validate our enhancements. The first two
settings compare the effect of allowing/not al-
lowing soft matches, but only uses WordNet as
in (Owczarzak et al, 2007). The third setting ap-
plies our additional linguistic features and the final
setting tunes parameter weights for higher correla-
tion with human judgement.
We report Pearson?s r, Spearman?s ? and
Kendall?s ? on segment and system levels on the
NIST MetricsMATR 2010 development set using
Snover?s scoring tool.7
Table 2: Correlation on the Segment Level
r ? ?
HARD 0.557 0.586 0.176
SOFT 0.600 0.634 0.213
SOFTALL 0.633 0.662 0.235
WEIGHTED 0.673 0.709 0.277
Table 2 shows that allowing soft triple matches
and using more linguistic features all lead
to higher correlation with human judgement.
Though the parameters might somehow overfit on
7http://www.umiacs.umd.edu/?snover/
terp/scoring/
352
the data set even if we apply cross validation, this
certainly confirms the necessity of weighing de-
pendency matches according to their types.
Table 3: Correlation on the System Level
r ? ?
HARD 0.948 0.905 0.786
SOFT 0.964 0.905 0.786
SOFTALL 0.975 0.976 0.929
WEIGHTED 0.989 1.000 1.000
When considering the system-level correlation
in Table 3, the trend is very similar to that of the
segment level. The improvements we introduce all
lead to improvements in correlation with human
judgement.
8 Conclusions and Future Work
In this paper we describe DCU?s dependency-
based MT evaluation metric submitted to WMT-
MetricsMATR 2010. Building upon the LFG-
based metric described in (Owczarzak et al,
2007), we use a publicly available parser instead
of an in-house parser to produce dependency la-
bels, so that the metric can run on a third party
machine. We improve the metric by allowing more
lexical variations and weighting dependency triple
matches depending on their importance according
to correlation with human judgement.
For future work, we hope to apply this method
to languages other than English, and performmore
refinement on dependency type labels and linguis-
tic resources.
Acknowledgements
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank O?zlem C?etinog?lu and Jennifer Foster for providing
us with the LFG parsing model for the Malt Parser, as well as
the anonymous reviewers for their insightful comments.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the ACL
Workshop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization, pages
65?72, Ann Arbor, MI.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-distance depen-
dency resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings of the
42nd Meeting of the Association for Computational Lin-
guistics (ACL-2004), pages 319?326, Barcelona, Spain.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluation the role of bleu in machine trans-
lation research. In Proceedings of 11th Conference of the
European Chapter of the Association for Computational
Linguistics, pages 249?256, Trento, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2010. Expected dependency pair match: predicting trans-
lation quality with expected syntactic structure. Machine
Translation.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammatical
representation. The mental representation of grammatical
relations, pages 173?281.
Alon Lavie andMichael J. Denkowski. 2009. he meteor met-
ric for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summarization,
pages 25?32, Ann Arbor, MI.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In In The fifth international conference on Lan-
guage Resources and Evaluation (LREC-2006), pages
2216?2219, Genoa, Italy.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation eval-
uation. In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 104?111, Prague, Czech
Republic.
Sebastian Pado, Michel Galley, Dan Jurafsky, and Christo-
pher D. Manning. 2009. Robust machine translation
evaluation with entailment features. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 297?305,
Suntec, Singapore.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguistics
(ACL-2002), pages 311?318, Philadelphia, PA.
William H. Press, Saul A. Teukolsky, William T. Vetterling,
and Brian P. Flannery. 2007. Numerical Recipes 3rd Edi-
tion: The Art of Scientific Computing. Cambridge Univer-
sity Press, New York, NY.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2010. Ter-plus: paraphrase, semantic, and
alignment enhancements to translation edit rate. Machine
Translation.
353
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46?54,
Beijing, August 2010
Handling Named Entities and Compound Verbs in             
Phrase-Based Statistical Machine Translation 
Santanu Pal*, Sudip Kumar Naskar?, Pavel Pecina?,  
Sivaji Bandyopadhyay* and Andy Way? 
*Dept. of Comp. Sc. & Engg. 
Jadavpur University 
santanupersonal1@gmail.com, sivaji_cse_ju@yahoo.com 
?CNGL, School of Computing 
Dublin City University 
{snaskar, ppecina, away}@computing.dcu.ie 
 
Abstract 
Data preprocessing plays a crucial role in 
phrase-based statistical machine transla-
tion (PB-SMT). In this paper, we show 
how single-tokenization of two types of 
multi-word expressions (MWE), namely 
named entities (NE) and compound 
verbs, as well as their prior alignment 
can boost the performance of PB-SMT. 
Single-tokenization of compound verbs 
and named entities (NE) provides sig-
nificant gains over the baseline PB-SMT 
system. Automatic alignment of NEs 
substantially improves the overall MT 
performance, and thereby the word 
alignment quality indirectly. For estab-
lishing NE alignments, we transliterate 
source NEs into the target language and 
then compare them with the target NEs. 
Target language NEs are first converted 
into a canonical form before the com-
parison takes place. Our best system 
achieves statistically significant im-
provements (4.59 BLEU points absolute, 
52.5% relative improvement) on an Eng-
lish?Bangla translation task. 
1 Introduction 
Statistical machine translation (SMT) heavily 
relies on good quality word alignment and 
phrase alignment tables comprising translation 
knowledge acquired from a bilingual corpus. 
Multi-word expressions (MWE) are defined 
as ?idiosyncratic interpretations that cross word 
boundaries (or spaces)? (Sag et al, 2002). Tradi-
tional approaches to word alignment following 
IBM Models (Brown et al, 1993) do not work 
well with multi-word expressions, especially 
with NEs, due to their inability to handle many-
to-many alignments. Firstly, they only carry out 
alignment between words and do not consider 
the case of complex expressions, such as multi-
word NEs. Secondly, the IBM Models only al-
low at most one word in the source language to 
correspond to a word in the target language 
(Marcu, 2001, Koehn et al, 2003). 
In another well-known word alignment ap-
proach, Hidden Markov Model (HMM: Vogel et 
al., 1996), the alignment probabilities depend on 
the alignment position of the previous word. It 
does not explicitly consider many-to-many 
alignment either. 
We address this many-to-many alignment 
problem indirectly. Our objective is to see how 
to best handle the MWEs in SMT. In this work, 
two types of MWEs, namely NEs and compound 
verbs, are automatically identified on both sides 
of the parallel corpus. Then, source and target 
language NEs are aligned using a statistical 
transliteration method. We rely on these auto-
matically aligned NEs and treat them as transla-
tion examples. Adding bilingual dictionaries, 
which in effect are instances of atomic transla-
tion pairs, to the parallel corpus is a well-known 
practice in domain adaptation in SMT (Eck et 
al., 2004; Wu et al, 2008). We modify the paral-
lel corpus by converting the MWEs into single 
tokens and adding the aligned NEs in the parallel 
corpus in a bid to improve the word alignment, 
and hence the phrase alignment quality. This 
46
preprocessing results in improved MT quality in 
terms of automatic MT evaluation metrics. 
The remainder of the paper is organized as 
follows. In section 2 we discuss related work. 
The System is described in Section 3.  Section 4 
includes the results obtained, together with some 
analysis. Section 5 concludes, and provides ave-
nues for further work. 
2 Related Work 
Moore (2003) presented an approach for si-
multaneous NE identification and translation. He 
uses capitalization cues for identifying NEs on 
the English side, and then he applies statistical 
techniques to decide which portion of the target 
language corresponds to the specified English 
NE. Feng et al (2004) proposed a Maximum 
Entropy model based approach for English?
Chinese NE alignment which significantly out-
performs IBM Model4 and HMM. They consid-
ered 4 features: translation score, transliteration 
score, source NE and target NE's co-occurrence 
score, and the distortion score for distinguishing 
identical NEs in the same sentence. Huang et al 
(2003) proposed a method for automatically ex-
tracting NE translingual equivalences between 
Chinese and English based on multi-feature cost 
minimization. The costs considered are translit-
eration cost, word-based translation cost, and NE 
tagging cost. 
Venkatapathy and Joshi (2006) reported a dis-
criminative approach of using the compositional-
ity information about verb-based multi-word 
expressions to improve word alignment quality. 
(Ren et al, 2009) presented log likelihood ratio-
based hierarchical reducing algorithm to auto-
matically extract bilingual MWEs, and investi-
gated the usefulness of these bilingual MWEs in 
SMT by integrating bilingual MWEs into Moses 
(Koehn et al, 2007) in three ways. They ob-
served the highest improvement when they used 
an additional feature to represent whether or not 
a bilingual phrase contains bilingual MWEs. 
This approach was generalized in Carpuat and 
Diab (2010). In their work, the binary feature 
was replaced by a count feature representing the 
number of MWEs in the source language phrase. 
Intuitively, MWEs should be both aligned in 
the parallel corpus and translated as a whole. 
However, in the state-of-the-art PB-SMT, it 
could well be the case that constituents of an 
MWE are marked and aligned as parts of con-
secutive phrases, since PB-SMT (or any other 
approaches to SMT) does not generally treat 
MWEs as special tokens. Another problem SMT 
suffers from is that verb phrases are often 
wrongly translated, or even sometimes deleted in 
the output in order to produce a target sentence 
considered good by the language model. More-
over, the words inside verb phrases seldom show 
the tendency of being aligned one-to-one; the 
alignments of the words inside source and target 
verb phrases are mostly many-to-many, particu-
larly so for the English?Bangla language pair. 
These are the motivations behind considering 
NEs and compound verbs for special treatment 
in this work. 
By converting the MWEs into single tokens, 
we make sure that PB-SMT also treats them as a 
whole. The objective of the present work is two-
fold; firstly to see how treatment of NEs and 
compound verbs as a single unit affects the 
overall MT quality, and secondly whether prior 
automatic alignment of these single-tokenized 
MWEs can bring about any further improvement 
on top of that. 
We carried out our experiments on an Eng-
lish?Bangla translation task, a relatively hard 
task with Bangla being a morphologically richer 
language. 
3 System Description 
3.1 PB-SMT 
Translation is modeled in SMT as a decision 
process, in which the translation Ie1 = e1 . . . ei . . 
. eI of a source sentence
Jf1 = f1 . . . fj . . . fJ is 
chosen to maximize (1): 
)().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
=      (1)  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modeled as a log-linear 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
 
47
?
=
=
M
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?  
)(log 1
I
LM eP?+        (2)     
where k
k sss ...11 =  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik, bk, jk), 
          
kk iik
eee ...? 11 +?= , 
         
kk jbk
fff ...? = .          (3) 
and each feature mh?  in (2) can be rewritten as in 
(4): 
?
=
=
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(                  (4) 
where mh? is a feature that applies to a single 
phrase-pair. It thus follows (5): 
? ??
= ==
=
K
k
K
k
kkkkkkm
M
m
m sefhsefh
1 11
),?,?(?),?,?(??      (5) 
where m
M
m
mhh ??
1
?
=
= ? .            
3.2 Preprocessing of the Parallel Corpus 
The initial English?Bangla parallel corpus is 
cleaned and filtered using a semi-automatic 
process. We employed two kinds of multi-word 
information: compound verbs and NEs. Com-
pound verbs are first identified on both sides of 
the parallel corpus. Chakrabarty et al (2008) 
analyzed and identified a category of V+V com-
plex predicates called lexical compound verbs 
for Hindi. We adapted their strategy for identifi-
cation of compound verbs in Bangla. In addition 
to V+V construction, we also consider N+V and 
ADJ+V structures. 
NEs are also identified on both sides of trans-
lation pairs. NEs in Bangla are much harder to 
identify than in English (Ekbal and Bandyop-
adhyay, 2009). This can be attributed to the fact 
that (i) there is no concept of capitalization in 
Bangla; and (ii) Bangla common nouns are often 
used as proper names. In Bangla, the problem is 
compounded by the fact that suffixes (case 
markers, plural markers, emphasizers, specifiers) 
are also added to proper names, just like to any 
other common nouns. As a consequence, the ac-
curacy of Bangla NE recognizers (NER) is much 
poorer compared to that for English. Once the 
compound verbs and the NEs are identified on 
both sides of the parallel corpus, they are con-
verted into and replaced by single tokens. When 
converting these MWEs into single tokens, we 
replace the spaces with underscores (?_?). Since 
there are already some hyphenated words in the 
corpus, we do not use hyphenation for this pur-
pose; besides, the use of a special word separator 
(underscore in our case) facilitates the job of 
deciding which single-token (target language) 
MWEs to detokenize into words comprising 
them, before evaluation. 
3.3 Transliteration  Using Modified Joint 
Source-Channel Model 
Li et al (2004) proposed a generative framework 
allowing direct orthographical mapping of trans-
literation units through a joint source-channel 
model, which is also called n-gram translitera-
tion model. They modeled the segmentation of 
names into transliteration units (TU) and their 
alignment preferences using maximum likeli-
hood via EM algorithm (Dempster et al, 1977). 
Unlike the noisy-channel model, the joint 
source-channel model tries to capture how 
source and target names can be generated simul-
taneously by means of contextual n-grams of the 
transliteration units. For K aligned TUs, they 
define the bigram model as in (6): 
 )...,,...,(),( 2121 KK bbbeeePBEP =  
  ),...,,,( 21 KbebebeP ><><><=  
   ? ><><= K
=k
k bebeP
1
1-k
1 ),|,(         (6) 
where E refers to the English name and B the 
transliteration in Bengali, while ei and bi refer to 
the ith English and Bangla segment (TU) respec-
tively. 
Ekbal et al (2006) presented a modification to 
the joint source-channel model to incorporate 
different contextual information into the model 
for Indian languages. They used regular expres-
sions and language-specific heuristics based on 
consonant and vowel patterns to segment names 
into TUs. Their modified joint source-channel 
model, for which they obtained improvement 
48
over the original joint source-channel model, 
essentially considers a trigram model for the 
source language and a bigram model for the tar-
get, as in (7). 
 ? +><><= K
=k
kk ebebePBEP
1
11-k ),,|,(),(   (7) 
Ekbal et al (2006) reported a word agreement 
ratio of 67.9% on an English?Bangla translit-
eration task. In the present work, we use the 
modified joint source-channel model of (Ekbal 
et al, 2006) to translate names for establishing 
NE alignments in the parallel corpus. 
3.4 Automatic Alignment of NEs through 
Transliteration 
We first create an NE parallel corpus by extract-
ing the source and target (single token) NEs 
from the NE-tagged parallel translations in 
which both sides contain at least one NE. For 
example, we extract the NE translation pairs 
given in (9) from the sentence pair shown in (8), 
where the NEs are shown as italicized. 
(8a) Kirti_Mandir , where Mahatma_Gandhi 
was born , today houses a photo exhibition on 
the life and times of the Mahatma , a library, a 
prayer hall and other memorabilia . 
(8b) ??????_??n? , ?????? ???t?_??n? ??n????? , 
???????? ?????? ???t?? ???? o ??i ????? 
?????????? e??? ??tp????????? , e??? ??i?b?? o 
e??? p?????? ?? e?? a????? s ????????? ??????t 
??? ? 
(9a) Kirti_Mandir Mahatma_Gandhi Mahatma 
(9b) ??????_??n? ???t?_??n? ???t?? 
Then we try to align the source and target NEs 
extracted from a parallel sentence, as illustrated 
in (9). If both sides contain only one NE then the 
alignment is trivial, and we add such NE pairs to 
seed another parallel NE corpus that contains 
examples having only one token in both side. 
Otherwise, we establish alignments between the 
source and target NEs using transliteration. We 
use the joint source-channel model of translitera-
tion (Ekbal et al, 2006) for this purpose.  
If both the source and target side contains n 
number of NEs, and the alignments of n-1 NEs 
can be established through transliteration or by 
means of already existing alignments, then the 
nth alignment is trivial. However, due to the rela-
tive performance difference of the NERs for the 
source and target language, the number of NEs 
identified on the source and target sides is al-
most always unequal (see Section 4). Accord-
ingly, we always use transliteration to establish 
alignments even when it is assumed to be trivial. 
Similarly, for multi-word NEs, intra-NE word 
alignments are established through translitera-
tion or by means of already existing alignments. 
For a multi-word source NE, if we can align all 
the words inside the NE with words inside a tar-
get NE, then we assume they are translations of 
each other. Due to the relatively poor perform-
ance of the Bangla NER, we also store the im-
mediate left and right neighbouring words for 
every NE in Bangla, just in case the left or the 
right word is a valid part of the NE but is not 
properly tagged by the NER. 
As mentioned earlier, since the source side 
NER is much more reliable than the target side 
NER, we transliterate the English NEs, and try 
to align them with the Bangla NEs. For aligning 
(capitalized) English words to Bangla words, we 
take the 5 best transliterations produced by the 
transliteration system for an English word, and 
compare them against the Bangla words. Bangla 
NEs often differ in their choice of matras (vowel 
modifiers). Thus we first normalize the Bangla 
words, both in the target NEs and the transliter-
ated ones, to a canonical form by dropping the 
matras, and then compare the results. In effect, 
therefore, we just compare the consonant se-
quences of every transliteration candidate with 
that of a target side Bangla word; if they match, 
then we align the English word with the Bangla 
word. 
???? (? + ??+ ? + ?) -- ????? (? + ?? + ? + ?? + ?) 
      (10) 
The example in (10) illustrates the procedure. 
Assume, we are trying to align ?Niraj? with 
???????. The transliteration system produces 
?????? from the English word ?Niraj? and we 
compare ?????? with ???????. Since the conso-
nant sequences match in both words, ?????? is 
considered a spelling variation of ???????, and 
the English word ?Niraj? is aligned to the 
Bangla word ???????. 
In this way, we achieve word-level align-
ments, as well as NE-level alignments. (11) 
shows the alignments established from (8). The 
word-level alignments help to establish new 
49
word / NE alignments. Word and NE alignments 
obtained in this way are added to the parallel 
corpus as additional training data. 
(11a) Kirti-Mandir  ? ??????-??n?  
(11b) Kirti ? ?????? 
(11c) Mandir  ? ??n? 
(11d) Mahatma-Gandhi ? ???t?-??n?  
(11e) Mahatma ? ???t? 
(11f) Gandhi ? ??n? 
(11g) Mahatma ? ???t?? 
3.5 Tools and Resources Used 
A sentence-aligned English?Bangla parallel 
corpus containing 14,187 parallel sentences from 
a travel and tourism domain was used in the pre-
sent work. The corpus was obtained from the 
consortium-mode project ?Development of Eng-
lish to Indian Languages Machine Translation 
(EILMT) System? 1. 
The Stanford Parser2 and the CRF chunker3 
were used for identifying compound verbs in the 
source side of the parallel corpus. The Stanford 
NER4 was used to identify NEs on the source 
side (English) of the parallel corpus. 
The sentences on the target side (Bangla) 
were POS-tagged by using the tools obtained 
from the consortium mode project ?Develop-
ment of Indian Languages to Indian Languages 
Machine Translation (ILILMT) System?. NEs in 
Bangla are identified using the NER system of 
Ekbal and Bandyopadhyay (2008). We use the 
Stanford Parser, Stanford NER and the NER for 
Bangla along with the default model files pro-
vided, i.e., with no additional training. 
The effectiveness of the MWE-aligned paral-
lel corpus developed in the work is demonstrated 
by using the standard log-linear PB-SMT model 
as our baseline system: GIZA++ implementation 
of IBM word alignment model 4, phrase-
extraction heuristics described in (Koehn et al, 
2003), minimum-error-rate training (Och, 2003) 
on a held-out development set, target language 
model with Kneser-Ney smoothing (Kneser and 
                                                 
1 The EILMT and ILILMT projects are funded by the De-
partment of Information Technology (DIT), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
2 http://nlp.stanford.edu/software/lex-parser.shtml 
3 http://crfchunker.sourceforge.net/ 
4 http://nlp.stanford.edu/software/CRF-NER.shtml 
Ney, 1995) trained with SRILM (Stolcke, 2002), 
and Moses decoder (Koehn et al, 2007). 
4 Experiments and Results 
We randomly extracted 500 sentences each for 
the development set and testset from the initial 
parallel corpus, and treated the rest as the train-
ing corpus. After filtering on maximum allow-
able sentence length of 100 and sentence length 
ratio of 1:2 (either way), the training corpus con-
tained 13,176 sentences. In addition to the target 
side of the parallel corpus, a monolingual Bangla 
corpus containing 293,207 words from the tour-
ism domain was used for the target language 
model. We experimented with different n-gram 
settings for the language model and the maxi-
mum phrase length, and found that a 4-gram 
language model and a maximum phrase length 
of 4 produced the optimum baseline result. We 
therefore carried out the rest of the experiments 
using these settings. 
English Bangla In training set 
T U T U 
Compound verbs 4,874 2,289 14,174 7,154
Single-word NEs 4,720 1,101 5,068 1,175
2-word NEs 4,330 2,961 4,147 3,417
>2 word NEs 1,555 1,271 1,390 1,278
Total NEs 10,605 5,333 10,605 5,870
Total NE words 22,931 8,273 17,107 9,106
Table 1.  MWE statistics (T - Total occur-
rence, U ? Unique). 
Of the 13,676 sentences in the training and 
development set, 13,675 sentences had at least 
one NE on both sides, only 22 sentences had 
equal number of NEs on both sides, and 13,654 
sentences had an unequal number of NEs. Simi-
larly, for the testset, all the sentences had at least 
one NE on both sides, and none had an equal 
number of NEs on both sides. It gives an indica-
tion of the relative performance differences of 
the NERs. 6.6% and 6.58% of the source tokens 
belong to NEs in the training and testset respec-
tively. These statistics reveal the high degree of 
NEs in the tourism domain data that demands 
special treatment. Of the 225 unique NEs ap-
pearing on the source side of the testset, only 65 
NEs are found in the training set.  
50
Experiments Exp BLEU METEOR NIST WER PER TER 
Baseline 1 8.74 20.39 3.98 77.89 62.95 74.60
NEs of any length as Single 
Token (New-MWNEaST) 
2 9.15 18.19 3.88 77.81 63.85 74.61
NEs of length >2 as  
Single Tokens (MWNE-
aST) 
3 8.76 18.78 3.86 78.31 63.78 75.15
 
 
NEs as Single  
Tokens  
(NEaST) 
2-Word NEs as Single To-
kens (2WNEaST) 
4 9.13 17.28 3.92 78.12 63.15 74.85
Compound Verbs as  Single Tokens 
(CVaST) ? 
5 9.56 15.35 3.96 77.60 63.06 74.46
Alignment of NEs of any 
length (New-MWNEA) ? 
6 13.33 24.06 4.44 74.79 60.10 71.25
Alignment of NEs of length 
upto 2 (New-2WNEA) ? 
7 10.35 20.93 4.11 76.49 62.20 73.05
Alignment of NEs of length 
>2 (MWNEA) ? 
8 12.39 23.13 4.36 75.51 60.58 72.06
 
 
 
 
NE Alignment 
(NEA) 
Alignment of NEs of length 
2 (2WNEA) ? 
9 11.2 23.14 4.26 76.13 60.72 72.57
New-MWNEaST 10 8.62 16.64 3.73 78.41 65.21 75.47
MWNEaST 11 8.74 14.68 3.84 78.40 64.05 75.40
 
CVaST 
+NEaST 2WNEaST 12 8.85 16.60 3.86 78.17 63.90 75.33
New-MWNEA? 13 11.22 21.02 4.16 75.99 61.96 73.06
New-2WNEA? 14 10.07 17.67 3.98 77.08 63.35 74.18
MWNEA? 15 10.34 16.34 4.07 77.12 62.38 73.88
 
CVaST +NEA 
2WNEA? 16 10.51 18.92 4.08 76.77 62.28 73.56
Table 2.  Evaluation results for different experimental setups (The ??? marked systems produce 
statistically significant improvements on BLEU over the baseline system).
Table 1 shows the MWE statistics of the 
parallel corpus as identified by the NERs. The 
average NE length in the training corpus is 
2.16 for English and 1.61 for Bangla. As can 
be seen from Table 1, 44.5% and 47.8% of the 
NEs are single-word NEs in English and 
Bangla respectively, which suggests that prior 
alignment of the single-word NEs, in addition 
to multi-word NE alignment, should also be 
beneficial to word and phrase alignment. 
Of all the NEs in the training and develop-
ment sets, the transliteration-based alignment 
process was able to establish alignments of 
4,711 single-word NEs, 4,669 two-word NEs 
and 1,745 NEs having length more than two. 
It is to be noted that, some of the single-word 
NE alignments, as well as two-word NE 
alignments, result from multi-word NE align-
ment. 
We analyzed the output of the NE align-
ment module and observed that longer NEs 
were aligned better than the shorter ones, 
which is quite intuitive, as longer NEs have 
more tokens to be considered for intra-NE 
alignment. Since the NE alignment process is 
based on transliteration, the alignment method 
does not work where NEs involve translation 
or acronyms. We also observed that English 
multi-word NEs are sometimes fused together 
into single-word NEs. 
We performed three sets of experiments: 
treating compound verbs as single tokens, 
treating NEs as single tokens, and the combi-
nation thereof. Again for NEs, we carried out 
three types of preprocessing: single-
tokenization of (i) two-word NEs, (ii) more 
than two-word NEs, and (iii) NEs of any 
length. We make distinctions among these 
three to see their relative effects. The devel-
opment and test sets, as well as the target lan-
guage monolingual corpus (for language mod-
eling), are also subjected to the same preproc-
essing of single-tokenizing the MWEs. For 
NE alignment, we performed experiments us-
ing 4 different settings: alignment of (i) NEs 
of length up to two, (ii) NEs of length two, 
51
(iii) NEs of length greater than two, and (iv) 
NEs of any length. Before evaluation, the sin-
gle-token (target language) underscored 
MWEs are expanded back to words compris-
ing the MWEs. 
Since we did not have the gold-standard 
word alignment, we could not perform intrin-
sic evaluation of the word alignment. Instead 
we carry out extrinsic evaluation on the MT 
quality using the well known automatic MT 
evaluation metrics: BLEU (Papineni et al, 
2002), METEOR (Banerjee and Lavie, 2005), 
NIST (Doddington, 2002), WER, PER and 
TER (Snover et al, 2006). As can be seen 
from the evaluation results reported in Table 
2, baseline Moses without any preprocessing 
of the dataset produces a BLEU score of 8.74. 
The low score can be attributed to the fact that 
Bangla, a morphologically rich language, is 
hard to translate into. Moreover, Bangla being 
a relatively free phrase order language (Ekbal 
and Bandyopadhyay, 2009) ideally requires 
multiple set of references for proper evalua-
tion. Hence using a single reference set does 
not justify evaluating translations in Bangla. 
Also the training set was not sufficiently large 
enough for SMT. Treating only longer than 2-
word NEs as single tokens does not help im-
prove the overall performance much, while 
single tokenization  of two-word NEs as single 
tokens produces some improvements (.39 
BLEU points absolute, 4.5% relative). Con-
sidering compound verbs as single tokens 
(CVaST) produces a .82 BLEU point im-
provement (9.4% relative) over the baseline. 
Strangely, when both compound verbs and 
NEs together are counted as single tokens, 
there is hardly any improvement. By contrast, 
automatic NE alignment  (NEA) gives a huge 
impetus to system performance, the best of 
them (4.59 BLEU points absolute, 52.5% rela-
tive improvement) being the alignment of NEs 
of any length that produces the best scores 
across all metrics. When NEA is combined 
with CVaST, the improvements are substan-
tial, but it can not beat the individual im-
provement on NEA. The (?) marked systems 
produce statistically significant improvements 
as measured by bootstrap resampling method 
(Koehn, 2004) on BLEU over the baseline 
system. Metric-wise individual best scores are 
shown in bold in Table 2. 
5 Conclusions and Future Work 
In this paper, we have successfully shown 
how the simple yet effective preprocessing of 
treating two types of MWEs, namely NEs and 
compound verbs, as single-tokens, in conjunc-
tion with prior NE alignment can boost the 
performance of PB-SMT system on an Eng-
lish?Bangla translation task. Treating com-
pound verbs as single-tokens provides signifi-
cant gains over the baseline PB-SMT system. 
Amongst the MWEs, NEs perhaps play the 
most important role in MT, as we have clearly 
demonstrated through experiments that auto-
matic alignment of NEs by means of translit-
eration improves the overall MT performance 
substantially across all automatic MT evalua-
tion metrics. Our best system yields 4.59 
BLEU points improvement over the baseline, 
a 52.5% relative increase. We compared a 
subset of the output of our best system with 
that of the baseline system, and the output of 
our best system almost always looks better in 
terms of either lexical choice or word order-
ing. The fact that only 28.5% of the testset 
NEs appear in the training set, yet prior auto-
matic alignment of the NEs brings about so 
much improvement in terms of MT quality, 
suggests that it not only improves the NE 
alignment quality in the phrase table, but word 
alignment and phrase alignment quality must 
have also been improved significantly. At the 
same time, single-tokenization of MWEs 
makes the dataset sparser, but yet improves 
the quality of MT output to some extent. Data-
driven approaches to MT, specifically for 
scarce-resource language pairs for which very 
little parallel texts are available, should benefit 
from these preprocessing methods. Data 
sparseness is perhaps the reason why single-
tokenization of NEs and compound verbs, 
both individually and in collaboration, did not 
add significantly to the scores. However, a 
significantly large parallel corpus can take 
care of the data sparseness problem introduced 
by the single-tokenization of MWEs. 
The present work offers several avenues for 
further work. In future, we will investigate 
how these automatically aligned NEs can be 
52
used as anchor words to directly influence the 
word alignment process. We will look into 
whether similar kinds of improvements can be 
achieved for larger datasets, corpora from dif-
ferent domains and for other language pairs. 
We will also investigate how NE alignment 
quality can be improved, especially where 
NEs involve translation and acronyms. We 
will also try to perform morphological analy-
sis or stemming on the Bangla side before NE 
alignment. We will also explore whether dis-
criminative approaches to word alignment can 
be employed to improve the precision of the 
NE alignment. 
Acknowledgements 
This research is partially supported by the Sci-
ence Foundation Ireland (Grant 07/CE/I1142) 
as part of the Centre for Next Generation Lo-
calisation (www.cngl.ie) at Dublin City Uni-
versity, and EU projects PANACEA (Grant 
7FP-ITC-248064) and META-NET (Grant 
FP7-ICT-249119). 
References 
Banerjee, Satanjeev, and Alon Lavie. 2005. An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
proceedings of the ACL-2005 Workshop on In-
trinsic and Extrinsic Evaluation Measures for 
MT and/or Summarization, pp. 65-72. Ann Ar-
bor, Michigan., pp. 65-72. 
Brown, Peter F., Stephen A. Della Pietra, Vincent 
J. Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
parameter estimation. Computational Linguis-
tics, 19(2):263-311. 
Carpuat, Marine, and Mona Diab. 2010. Task-
based Evaluation of Multiword Expressions: a 
Pilot Study in Statistical Machine Translation. 
In Proceedings of Human Language Technology 
conference and the North American Chapter of 
the Association for Computational Linguistics 
conference (HLT-NAACL 2010), Los Angeles, 
CA, pp. 242-245. 
Chakrabarti, Debasri, Hemang Mandalia, Ritwik 
Priya, Vaijayanthi Sarma, and Pushpak Bhat-
tacharyya. 2008. Hindi compound verbs and 
their automatic extraction. In Proceedings 
of  the 22nd International Conference on Com-
putational Linguistics (Coling 2008), Posters 
and demonstrations, Manchester, UK, pp. 27-
30. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 
1977). Maximum Likelihood from Incomplete 
Data via the EM Algorithm. Journal of the 
Royal Statistical Society, Series B (Methodo-
logical) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation 
of machine translation quality using n-gram 
cooccurrence statistics. In Proceedings of the 
Second International Conference on Human 
Language Technology Research (HLT-2002), 
San Diego, CA, pp. 128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation 
in the medical domain using the Unified Medi-
cal Language System. In Proceedings of  the 
20th International Conference on Computational 
Linguistics (COLING 2004), Ge-
neva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. 
Voted NER system using appropriate unlabeled 
data. In proceedings of the ACL-IJCNLP-2009 
Named Entities Workshop (NEWS 2009), 
Suntec, Singapore, pp. 202-210. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. 
Maximum Entropy Approach for Named Entity 
Recognition in Indian Languages. International 
Journal for Computer Processing of Lan-
guages (IJCPOL), Vol. 21(3):205-237. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. 
A new approach for English-Chinese named en-
tity alignment. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2004), Barcelona, 
Spain, pp. 372-379. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 
2003. Automatic extraction of named entity 
translingual equivalence based on multi-feature 
cost minimization. In Proceedings of the ACL-
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, 2003, 
Sapporo, Japan, pp. 9-16. 
Kneser, Reinhard, and Hermann Ney. 1995. Im-
proved backing-off for m-gram language model-
ing. In Proceedings of the IEEE Internation 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP), vol. 1, pp. 181-184. De-
troit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of HLT-NAACL 2003: 
53
conference combining Human Language Tech-
nology conference series and the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics conference series,  Edmonton, 
Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ond?ej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: open source toolkit for 
statistical machine translation. In Proceedings of 
the 45th Annual meeting of the Association for 
Computational Linguistics (ACL 2007): Pro-
ceedings of demo and poster sessions, Prague, 
Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests 
for machine translation evaluation. In  EMNLP-
2004: Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Proc-
essing, 25-26 July 2004, Barcelona, Spain, pp. 
388-395. 
Marcu, Daniel. 2001. Towards a Unified Approach 
to Memory- and Statistical-Based Machine 
Translation. In Proceedings of the 39th Annual 
Meeting of the Association for Computational 
Linguistics (ACL 2001), Toulouse, France, pp. 
386-393. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases from parallel corpora. In 
Proceedings of 10th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics (EACL 2003), Budapest, 
Hungary; pp. 259-266. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sap-
poro, Japan, pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-2002), Philadelphia, PA, pp. 311-318. 
Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, and 
Yun Huang. 2009. Improving statistical ma-
chine translation using domain bilingual multi-
word expressions. In Proceedings of the 2009 
Workshop on Multiword Expressions, ACL-
IJCNLP 2009, Suntec, Singapore, pp. 47-54. 
Sag, Ivan A., Timothy Baldwin, Francis Bond, 
Ann Copestake and Dan Flickinger. 2002. Mul-
tiword expressions: A pain in the neck for NLP. 
In Proceedings of the 3rd International Confer-
ence on Intelligent Text Processing and Compu-
tational Linguistics (CICLing-2002), Mexico 
City, Mexico, pp. 1-15. 
Snover, Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), Cambridge, 
MA, pp. 223-231. 
Vogel, Stephan, Hermann Ney, and Christoph 
Tillmann. 1996. HMM-based word alignment in 
statistical translation. In Proceedings of the 16th 
International Conference on Computational 
Linguistics (COLING 1996), Copenhagen, pp. 
836-841. 
Venkatapathy, Sriram, and Aravind K. Joshi. 2006. 
Using information about multi-word expres-
sions for the word-alignment task. In Proceed-
ings of Coling-ACL 2006: Workshop on Multi-
word Expressions: Identifying and Exploiting 
Underlying Properties, Sydney, pp. 20-27. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 
2008. Domain adaptation for statistical machine 
translation with domain dictionary and mono-
lingual corpora. In Proceedings of the 22nd In-
ternational Conference on Computational Lin-
guistics (COLING 2008),  Manchester, UK, pp. 
993-1000. 
54
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19?27,
COLING 2010, Beijing, August 2010.
Source-side Syntactic Reordering Patterns with Functional Words for
Improved Phrase-based SMT
Jie Jiang, Jinhua Du, Andy Way
CNGL, School of Computing, Dublin City University 
{jjiang,jdu,away}@computing.dcu.ie
Abstract
Inspired by previous source-side syntactic
reordering methods for SMT, this paper
focuses on using automatically learned
syntactic reordering patterns with func-
tional words which indicate structural re-
orderings between the source and target
language. This approach takes advan-
tage of phrase alignments and source-side
parse trees for pattern extraction, and then
filters out those patterns without func-
tional words. Word lattices transformed
by the generated patterns are fed into PB-
SMT systems to incorporate potential re-
orderings from the inputs. Experiments
are carried out on a medium-sized cor-
pus for a Chinese?English SMT task. The
proposed method outperforms the base-
line system by 1.38% relative on a ran-
domly selected testset and 10.45% rela-
tive on the NIST 2008 testset in terms
of BLEU score. Furthermore, a system
with just 61.88% of the patterns filtered
by functional words obtains a comparable
performance with the unfiltered one on the
randomly selected testset, and achieves
1.74% relative improvements on the NIST
2008 testset.
1 Introduction
Previous work has shown that the problem of
structural differences between language pairs in
SMT can be alleviated by source-side syntactic
reordering. Taking account for the integration
with SMT systems, these methods can be divided
into two different kinds of approaches (Elming,
2008): the deterministic reordering and the non-
deterministic reordering approach.
To carry out the deterministic approach, syntac-
tic reordering is performed uniformly on the train-
ing, devset and testset before being fed into the
SMT systems, so that only the reordered source
sentences are dealt with while building during
the SMT system. In this case, most work is fo-
cused on methods to extract and to apply syntac-
tic reordering patterns which come from manually
created rules (Collins et al, 2005; Wang et al,
2007a), or via an automatic extraction process tak-
ing advantage of parse trees (Collins et al, 2005;
Habash, 2007). Because reordered source sen-
tence cannot be undone by the SMT decoders (Al-
Onaizan et al, 2006), which implies a systematic
error for this approach, classifiers (Chang et al,
2009b; Du & Way, 2010) are utilized to obtain
high-performance reordering for some specialized
syntactic structures (e.g. DE construction in Chi-
nese).
On the other hand, the non-deterministic ap-
proach leaves the decisions to the decoders to
choose appropriate source-side reorderings. This
is more flexible because both the original and
reordered source sentences are presented in the
inputs. Word lattices generated from syntactic
structures for N-gram-based SMT is presented
in (Crego et al, 2007). In (Zhang et al, 2007a;
Zhang et al, 2007b), chunks and POS tags are
used to extract reordering rules, while the gener-
ated word lattices are weighted by language mod-
els and reordering models. Rules created from a
syntactic parser are also utilized to form weighted
n-best lists which are fed into the decoder (Li et
al., 2007). Furthermore, (Elming, 2008; Elm-
19
ing, 2009) uses syntactic rules to score the output
word order, both on English?Danish and English?
Arabic tasks. Syntactic reordering information is
also considered as an extra feature to improve PB-
SMT in (Chang et al, 2009b) for the Chinese?
English task. These results confirmed the effec-
tiveness of syntactic reorderings.
However, for the particular case of Chinese
source inputs, although the DE construction has
been addressed for both PBSMT and HPBSMT
systems in (Chang et al, 2009b; Du & Way,
2010), as indicated by (Wang et al, 2007a), there
are still lots of unexamined structures that im-
ply source-side reordering, especially in the non-
deterministic approach. As specified in (Xue,
2005), these include the bei-construction, ba-
construction, three kinds of de-construction (in-
cluding DE construction) and general preposition
constructions. Such structures are referred with
functional words in this paper, and all the con-
structions can be identified by their correspond-
ing tags in the Penn Chinese TreeBank. It is in-
teresting to investigate these functional words for
the syntactic reordering task since most of them
tend to produce structural reordering between the
source and target sentences.
Another related work is to filter the bilingual
phrase pairs with closed-class words (Sa?nchez-
Mart??nez, 2009). By taking account of the word
alignments and word types, the filtering process
reduces the phrase tables by up to a third, but still
provide a system with competitive performance
compared to the baseline. Similarly, our idea is to
use special type of words for the filtering purpose
on the syntactic reordering patterns.
In this paper, our objective is to exploit
these functional words for source-side syntac-
tic reordering of Chinese?English SMT in the
non-deterministic approach. Our assumption is
that syntactic reordering patterns with functional
words are the most effective ones, and others can
be pruned for both speed and performance.
To validate this assumption, three systems are
compared in this paper: a baseline PBSMT sys-
tem, a syntactic reordering system with all pat-
terns extracted from a corpus, and a syntactic re-
ordering system with patterns filtered with func-
tional words. To accomplish this, firstly the lat-
tice scoring approach (Jiang et al, 2010) is uti-
lized to discover non-monotonic phrase align-
ments, and then syntactic reordering patterns are
extracted from source-side parse trees. After that,
functional word tags specified in (Xue, 2005) are
adopted to perform pattern filtering. Finally, both
the unfiltered pattern set and the filtered one are
used to transform inputs into word lattices to
present potential reorderings for improving PB-
SMT system. A comparison between the three
systems is carried out to examine the performance
of syntactic reordering as well as the usefulness of
functional words for pattern filtering.
The rest of this paper is organized as follows:
in section 2 we describe the extraction process of
syntactic reordering patterns, including the lattice
scoring approach and the extraction procedures.
Then section 3 presents the filtering process used
to obtain patterns with functional words. After
that, section 4 shows the generation of word lat-
tices with patterns, and experimental setup and re-
sults included related discussion are presented in
section 5. Finally, we give our conclusion and av-
enues for future work in section 6.
2 Syntactic reordering patterns
extraction
Instead of top-down approaches such as (Wang
et al, 2007a; Chang et al, 2009a), we use a
bottom-up approach similar to (Xia et al, 2004;
Crego et al, 2007) to extract syntactic reordering
patterns from non-monotonic phrase alignments
and source-side parse trees. The following steps
are carried out to extract syntactic reordering pat-
terns: 1) the lattice scoring approach proposed
in (Jiang et al, 2010) is used to obtain phrase
alignments from the training corpus; 2) reorder-
ing regions from the non-monotonic phrase align-
ments are used to identify minimum treelets for
pattern extraction; and 3) the treelets are trans-
formed into syntactic reordering patterns which
are then weighted by their occurrences in the
training corpus. Details of each of these steps are
presented in the rest of this section.
2.1 Lattice scoring for phrase alignments
The lattice scoring approach is proposed in (Jiang
et al, 2010) for the SMT data cleaning task.
20
To clean the training corpus, word alignments
are used to obtain approximate decoding results,
which are then used to calculate BLEU (Papineni
et al, 2002) scores to filter out low-scoring sen-
tences pairs. The following steps are taken in
the lattice scoring approach: 1) train an initial
PBSMT model; 2) collect anchor pairs contain-
ing source and target phrase positions from word
alignments generated in the training phase; 3)
build source-side lattices from the anchor pairs
and the translation model; 4) search on the source-
side lattices to obtain approximate decoding re-
sults; 5) calculate BLEU scores for the purpose of
data cleaning.
Note that the source-side lattices in step 3 come
from anchor pairs, so each edge in the lattices con-
tain both the source and target phrase positions.
Thus the outputs of step 4 contain phrase align-
ments on the training corpus. These phrase align-
ments are used to identify non-monotonic areas
for the extraction of reordering patterns.
2.2 Reordering patterns
Non-monotonic regions of the phrase alignments
are examined as potential source-side reorderings.
By taking a bottom-up approach, the reordering
regions are identified and mapped to minimum
treelets on the source parse trees. After that, syn-
tactic reordering patterns are transformed from
these minimum treelets.
In this paper, reordering regions A and B indi-
cating swapping operations on the source side are
only considered as potential source-side reorder-
ings. Thus, given reordering regions AB, this im-
plies (1):
AB ? BA (1)
on the source-side word sequences. Referring to
the phrase alignment extraction in the last section,
each non-monotonic phrase alignment produces
one reordering region. Furthermore, for each re-
ordering region identified, all of its sub-areas in-
dicating non-monotonic alignments are also at-
tempted to produce more reordering regions.
To represent the reordering region using syn-
tactic structure, given the extracted reordering re-
gions AB, the following steps are taken to map
them onto the source-side parse trees, and to gen-
erate corresponding patterns:
1. Generate a parse tree for each of the source
sentences. The Berkeley parser (Petrov,
2006) is used in this paper. To obtain sim-
pler tree structures, right-binarization is per-
formed on the parse trees, while tags gener-
ated from binarization are not distinguished
from the original ones (e.g. @V P and V P
are the same).
2. Map reordering regions AB onto the parse
trees. Denote NA as the set of leaf nodes in
region A and NB for region B. The mapping
is carried out on the parse tree to find a mini-
mum treelet T , which satisfies the following
two criteria: 1) there must exist a path from
each node in NA ? NB to the root node of
T ; 2) each leaf node of T can only be the
ancestor of nodes in NA or NB (or none of
them).
3. Traverse T in pre-order to obtain syntactic
reordering pattern P . Label all the leaf nodes
of T with A or B as reorder options, which
indicate that the descendants of nodes with
label A are supposed to be swapped with
those with label B.
Instead of using subtrees, we use treelets to
refer the located parse tree substructures, since
treelets do not necessarily go down to leaf nodes.
Since phrase alignments cannot always be per-
fectly matched with parse trees, we also expand
AB to the right and/or the left side with a limited
number of words to find a minimum treelet. In
this situation, a minimum number of ancestors of
expanded tree nodes are kept in T but they are as-
signed the same labels as those from which they
have been expanded. In this case, the expanded
tree nodes are considered as the context nodes of
syntactic reordering patterns.
Figure 1 illustrates the extraction process. Note
the symbol @ indicates the right-binarization sym-
bols (e.g. @V P in the figure). In the figure, tree
T (surrounded by dashed lines) is the minimum
treelet mapped from the reordering region AB.
Leaf node NP is labeled by A, V P is labeled by
B, and the context node P is also labeled by A.
Leaf nodes labeled A or B are collected into node
sequences LA or LB to indicate the reordering op-
21
A B
T
Figure 1: Reordering pattern extraction
erations. Thus the syntactic reordering pattern P
is obtained from T as in (2):
P = {V P (PP (P NP ) V P )|O = {LA, LB}}
(2)
where the first part of P is the V P with its tree
structure, and the second part O indicates the re-
ordering scheme, which implies that source words
corresponding with descendants of LA are sup-
posed to be swapped with those of LB .
2.3 Pattern weights estimation
We use preo to represent the chance of reordering
when a treelet is located by a pattern on the parse
tree. It is estimated by the number of reorderings
for each of the occurrences of the pattern as in (3):
preo(P ) =
count{reorderings of P}
count{observation of P} (3)
By contrast, one syntactic pattern P usually con-
tains several reordering schemes (specified in for-
mula (2)), each of them weighted as in (4):
w(O,P ) = count{reorderings of O in P}count{reorderings of P}
(4)
Generally, a syntactic reordering pattern is ex-
pressed as in (5):
P = {tree | preo | O1, w1, ? ? ? , On, wn} (5)
where tree is the tree structures of the pattern,
preo is the reordering probability, Oi and wi are
the reordering schemes and weights (1 ? i ? n).
3 Patterns with functional words
Some of the patterns extracted may not benefit
the final system since the extraction process is
controlled by phrase alignments rather than syn-
tactic knowledge. Inspired by the study of DE
constructions (Chang et al, 2009a; Du & Way,
2010), we assume that syntactic reorderings are
indicated by functional words for the Chinese?
English task. To incorporate the knowledge of
functional words into the extracted patterns, in-
stead of directly specifying the syntactic struc-
ture from the linguistic aspects, we use functional
word tags to filter the extracted patterns. In this
case, we assume that all patterns containing func-
tional words tend to produce meaningful syntactic
reorderings. Thus the filtered patterns carry the re-
ordering information from the phrase alignments
as well as the linguistic knowledge. Thus the
noise produced in phrase alignments and the size
of pattern set can be reduced, so that the speed and
the performance of the system can be improved.
The functional word tags used in this paper are
shown in Table 1, which come from (Xue, 2005).
We choose them as functional words because nor-
mally they imply word reorders between Chinese
and English sentence pairs.
Tag Description
BA ba-construction
DEC de (1st kind) in a relative-clause
DEG associative de (1st kind)
DER de (2nd kind) in V-de const. & V-de-R
DEV de (3rd kind) before VP
LB bei in long bei-construction
P preposition excluding bei and ba
SB bei in short bei-construction
Table 1: Syntactic reordering tags for functional
words
Note that there are three kinds of de-
constructions, but only the first kind is the DE
construction in (Chang et al, 2009a; Du & Way,
2010). After the filtering process, both the unfil-
tered pattern set and the filtered one are used to
build different syntactic reordering PBSMT sys-
tems for comparison purpose.
22
4 Word lattice construction
Both the devset and testset are transformed into
word lattices by the extracted patterns to incor-
porate potential reorderings. Figure 2 illustrates
this process: treelet T ? is matched with a pat-
tern, then its leaf nodes {a1, ? ? ? am} ? LA (span-
ning {w1, ? ? ? , wp}) are swapped with leaf nodes
{b1, ? ? ? , bn} ? LB (spanning {v1, ? ? ? , vq}) on
the generated paths in the word lattice.
T?
a1
am b1
bn
... ...
... ...
w1 w2 ... wp v1 v2 vq...
w1 w2 ... wp v1 v2 vq...
w2...
wpv1
v2 ...
... ...
vq w1
Sub parse tree 
matched with 
a pattern
Source side 
sentence
Generated 
lattice
Figure 2: Incorporating potential reorderings into
lattices
We sort the matched patterns by preo in formula
(5), and only apply a pre-defined number of re-
orderings for each sentence. For each lattice node,
if we denote E0 as the edge from the original sen-
tence, while patterns {P1, ? ? ? , Pi, ? ? ? , Pk} are ap-
plied to this node, then E0 is weighted as in (6):
w(E0) = ? +
k?
i=1
{(1? ?)k ? {1? preo(Pi)}}
(6)
where preo(Pi) is the pattern weight in formula
(3), and ? is the base probability to avoid E0 be-
ing equal to zero. Suppose {Es, ? ? ? , Es+r?1} are
generated by r reordering schemes of Pi, then Ej
is weighted as in (7):
w(Ej) =
(1 ? ?)
k ?preo(Pi)?
ws?j+1(Pi)?r
t=1 wt(Pi)
(7)
where wt(Pi) is the reordering scheme in formula
(5), and s <= j < s + r. Reordering patterns
with the same root lattice node share equal proba-
bilities in formula (6) and (7).
5 Experiments and results
We conducted our experiments on a medium-sized
corpus FBIS (a multilingual paragraph-aligned
corpus with LDC resource number LDC2003E14)
for the Chinese?English SMT task. The Cham-
pollion aligner (Ma, 2006) is utilized to perform
sentence alignment. A total number of 256,911
sentence pairs are obtained, while 2,000 pairs for
devset and 2,000 pairs for testset are randomly se-
lected, which we call FBIS set. The rest of the
data is used as the training corpus.
The baseline system is Moses (Koehn et
al., 2007), and GIZA++1 is used to perform
word alignment. Minimum error rate training
(MERT) (Och, 2003) is carried out for tuning. A
5-gram language model built via SRILM2 is used
for all the experiments in this paper.
Experiments results are reported on two differ-
ent sets: the FBIS set and the NIST set. For the
NIST set, the NIST 2005 testset (1,082 sentences)
is used as the devset, and the NIST 2008 test-
set (1,357 sentences) is used as the testset. The
FBIS set contains only one reference translation
for both devset and testset, while NIST set has
four references.
5.1 Pattern extraction and filtering with
functional words
The lattice scoring approach is carried out with
the same baseline system as specified above to
produce the phrase alignments. The initial PB-
SMT system in the lattice scoring approach is
tuned with the FBIS devset to obtain the weights.
As specified in section 2.1, phrase alignments are
generated in the step 4 of the lattice scoring ap-
proach.
From the generated phrase alignments and
source-side parse trees of the training corpus,
we obtain 48,285 syntactic reordering patterns
(57,861 reordering schemes) with an average
number of 11.02 non-terminals. For computa-
tional efficiency, any patterns with number of non-
terminal less than 3 and more than 9 are pruned.
This procedure leaves 18,169 syntactic reordering
patterns (22,850 reordering schemes) with a aver-
1http://fjoch.com/GIZA++.html
2http://www.speech.sri.com/projects/srilm/
23
age number of 7.6 non-terminals. This pattern set
is used to built the syntactic reordering PBSMT
system without pattern filtering, which here after
we call the ?unfiltered system?.
Using the tags specified in Table 1, the ex-
tracted syntactic reordering patterns without func-
tional words are filtered out, while only 6,926 syn-
tactic reordering patterns (with 9,572 reordering
schemes) are retained. Thus the pattern set are
reduced by 61.88%, and over half of them are
pruned by the functional word tags. The filtered
pattern set is used to build the syntactic reorder-
ing PBSMT system with pattern filtering, which
we refer as the ?filtered system?.
Type Tag Patterns Percent
ba-const. BA 222 3.20%
bei-const. LB 97 2.79%SB 96
de-const. (1st) DEC 1662 60.11%DEG 2501
de-const. (2nd) DER 52 0.75%
de-const. (3rd) DEV 178 2.57%
preposition P 2591 37.41%
excl. ba & bei
Table 2: Statistics on the number of patterns for
each type of functional word
Statistics on the patterns with respect to func-
tional word types are shown in Table 2. The num-
ber of patterns for each functional word in the fil-
tered pattern set are illustrated, and percentages of
functional word types are also reported. Note that
some patterns contain more than one kind of func-
tional word, so that the percentages of functional
word types do not sum to one.
As demonstrated in Table 2, the first kind of de-
construction takes up 60.11% of the filtered pat-
tern set, and is the main type of patterns used in
our experiment. This indicates that more than half
of the patterns are closely related to the DE con-
struction examined in (Chang et al, 2009b; Du
& Way, 2010). However, the general preposi-
tion construction (excluding bei and ba) accounts
for 37.41% of the filtered patterns, which implies
that it is also a major source of syntactic reorder-
ing. By contrast, other constructions have much
smaller amount of percentages, so have a minor
impact on our experiments.
5.2 Word lattice construction
As specified in section 4, for both unfiltered and
the filtered systems, both the devset and testset
are converted into word lattices with the unfiltered
and filtered syntactic reordering patterns respec-
tively. To avoid a dramatic increase in size of the
lattices, the following constraints are applied: for
each source sentence, the maximum number of re-
ordering schemes is 30, and the maximum span of
a pattern is 30.
For the lattice construction, the base probabil-
ity in (6) and (7) is set to 0.05. The two syntac-
tic reordering PBSMT systems also incorporate
the built-in reordering models (distance-based and
lexical reordering) of Moses, and their weights in
the log-linear model are tuned with respect to the
devsets.
The effects of the pattern filtering by functional
words are also reported in Table 3. For both the
FBIS and NIST sets, the average number of nodes
in word lattices are illustrated before and after pat-
tern filtering. From the table, it is clear that the
pattern filtering procedure dramatically reduces
the input size for the PBSMT system. The reduc-
tion is up to 37.99% for the NIST testset.
Data set Unfiltered Filtered Reduced
FBIS dev 183.13 131.38 28.26%
FBIS test 183.68 136.56 25.65%
NIST dev 175.78 115.89 34.07%
NIST test 149.13 92.48 37.99%
Table 3: Comparison of the average number of
nodes in word lattices
5.3 Results on FBIS set
Three systems are compared on the FBIS set:
the baseline PBSMT system, and the syntactic
reordering systems with and without pattern fil-
tering. Since the built-in reordering models of
Moses are enabled, several values of the distor-
tion limit (DL) parameter are chosen to validate
consistency. The evaluation results on the FBIS
set are shown in Table 4.
As shown in Table 4, the syntactic reordering
systems with and without pattern filtering outper-
24
System DL BLEU NIST METE
Baseline
0 22.32 6.45 52.51
6 23.67 6.63 54.07
10 24.52 6.66 54.04
12 24.57 6.69 54.31
Unfiltered
0 23.92 6.60 54.30
6 24.57 6.68 54.64
10 24.98 6.71 54.67
12 24.84 6.69 54.65
Filtered
0 23.71 6.60 54.11
6 24.65 6.68 54.61
10 24.87 6.71 54.84
12 24.91 6.7 54.51
Table 4: Results on FBIS testset (DL = distortion
limit, METE=METEOR)
form the baseline system for each of the distortion
limit parameters in terms of the BLEU, NIST and
METEOR scores (scores in bold face). By con-
trast, the filtered systems has a comparable perfor-
mance with the unfiltered system: for some of the
distortion limits, the filtered systems even outper-
forms the unfiltered system (scores in bold face,
e.g. BLEU and NIST for DL=12, METEOR for
DL=10).
The best performance of the baseline system
is obtained with distortion limit 12 (underlined);
the best performance of the unfiltered system is
achieved with distortion limit 10 (underlined);
while for the filtered system, the best BLEU score
is accomplished with distortion limit 12 (under-
lined), and the best NIST and METEOR scores
are shown with distortion limit 10 (underlined).
Thus the unfiltered system outperforms the base-
line by 0.41 (1.67% relative) BLEU points, 0.02
(0.30% relative) NIST points and 0.36 (0.66%
relative) METEOR points. By contrast, the fil-
tered system outperforms the baseline by 0.34
(1.38% relative) BLEU points, 0.02 (0.30% rel-
ative) NIST points and 0.53 (0.98% relative) ME-
TEOR points.
Compared with the unfiltered system, pattern
filtering with functional words degrades perfor-
mance by 0.07 (0.28% relative) in term of BLEU,
but improves the system by 0.17 (0.31% rela-
tive) in term of METEOR, while the two systems
achieved the same best NIST score.
These results indicates that the filtered system
has a comparable performance with the unfiltered
one on the FBIS set, while both of them outper-
form the baseline.
5.4 Results on NIST set
The evaluation results on the NIST set are illus-
trated in Table 5.
System DL BLEU NIST METE
Baseline
0 14.43 5.75 45.03
6 15.61 5.88 45.75
10 15.73 5.78 45.27
12 15.89 6.16 45.88
Unfiltered
0 16.77 6.54 47.16
6 17.25 6.67 47.65
10 17.15 6.64 47.78
12 16.88 6.56 47.17
Filtered
0 16.79 6.64 47.67
6 17.55 6.71 48.06
10 17.51 6.72 48.15
12 17.37 6.72 48.08
Table 5: Results on NIST testset (DL = distortion
limit, METE=METEOR)
From Table 5, the unfiltered system outper-
forms the baseline system for each of the distor-
tion limits in terms of the BLEU, NIST and ME-
TEOR scores (scores in bold face). By contrast,
the filtered system also outperform the unfiltered
system for each of the distortion limits in terms of
the three evaluation methods (scores in bold face).
The best performance of the baseline system
is obtained with distortion limit 12 (underlined),
while the best performance of the unfiltered sys-
tem is obtained with distortion limit 6 for BLEU
and NIST, and 10 for METEOR (underlined). For
the filtered system, the best BLEU score is shown
with distortion limit 6, and the best NIST and ME-
TEOR scores are accomplished with distortion
limit 10 (underlined). Thus the unfiltered system
outperforms the baseline by 1.36 (8.56% relative)
BLEU points, 0.51 (8.28% relative) NIST points
and 1.90 (4.14% relative) METEOR points. By
contrast, the filtered system outperforms the base-
line by 1.66 (10.45% relative) BLEU points, 0.56
(9.52% relative) NIST points and 2.27 (4.95% rel-
ative) METEOR points.
25
Compared with the unfiltered system, patterns
with functional words boost the performance by
0.30 (1.74% relative) in term of BLEU, 0.05
(0.75% relative) in term of NIST, and 0.37 (0.77%
relative) in term of METEOR.
These results demonstrate that the pattern filter-
ing improves the syntactic reordering system on
the NIST set, while both of them significantly out-
perform the baseline.
5.5 Discussion
Experiments in the previous sections demonstrate
that: 1) the two syntactic reordering systems im-
prove the PBSMT system by providing potential
reorderings obtained from phrase alignments and
parse trees; 2) patterns with functional words play
a major role in the syntactic reordering process,
and filtering the patterns with functional words
maintains or even improves the system perfor-
mance for Chinese?English SMT task. Further-
more, as shown in the previous section, pattern
filtering prunes the whole pattern set by 61.88%
and also reduces the sizes of word lattices by up
to 37.99%, thus the whole syntactic reordering
procedure for the original inputs as well as the
tuning/decoding steps are sped up dramatically,
which make the proposed methods more useful in
the real world, especially for online SMT systems.
From the statistics on the filtered pattern set
in Table 2, we also argue that the first kind
of de-construction and general preposition (ex-
cluding bei and ba) are the main sources of
Chinese?English syntactic reordering. Previous
work (Chang et al, 2009b; Du & Way, 2010)
showed the advantages of dealing with the DE
construction. In our experiments too, even though
all the patterns are automatically extracted from
phrase alignments, these two constructions still
dominate the filtered pattern set. This result con-
firms the effectiveness of previous work on DE
construction, and also highlights the importance
of the general preposition construction in this task.
6 Conclusion and future work
Syntactic reordering patterns with functional
words are examined in this paper. The aim is to
exploit these functional words within the syntactic
reordering patterns extracted from phrase align-
ments and parse trees. Three systems are com-
pared: a baseline PBSMT system, a syntactic re-
ordering system with all patterns extracted from a
corpus and a syntactic reordering system with pat-
terns filtered with functional words. Evaluation
results on a medium-sized corpus showed that the
two syntactic reordering systems consistently out-
perform the baseline system. The pattern filtering
with functional words prunes 61.88% of patterns,
but still maintains a comparable performance with
the unfiltered one on the randomly select testset,
and even obtains 1.74% relative improvement on
the NIST 2008 testset.
In future work, the structures of patterns con-
taining functional words will be investigated to
obtain fine-grained analysis on such words in this
task. Furthermore, experiments on larger corpora
as well as on other language pairs will also be car-
ried out to validation our method.
Acknowledgements
This research is supported by Science Foundation
Ireland (Grant 07/CE/I1142) as part of the Centre
for Next Generation Localisation (www.cngl.ie) at
Dublin City University. Thanks to Yanjun Ma for
the sentence-aligned FBIS corpus.
References
Yaser Al-Onaizan and Kishore Papineni 2006. Dis-
tortion models for statistical machine translation.
Coling-ACL 2006: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics, pages 529-536, Sydney,
Australia.
Pi-Chuan Chang, Dan Jurafsky, and Christopher
D.Manning 2009a. Disambiguating DE for
Chinese?English machine translation. Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 215-223, Athens, Greece.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009b. Discriminative
reordering with Chinese grammatical features. Pro-
ceedings of SSST-3: Third Workshop on Syntax and
Structure in Statistical Translation, pages 51-59,
Boulder, CO.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. ACL-2005: 43rd Annual meeting of the
26
Association for Computational Linguistics, pages
531-540, University of Michigan, Ann Arbor, MI.
Josep M. Crego, and Jose? B. Marin?o. 2007. Syntax-
enhanced N-gram-based SMT. MT Summit XI,
pages 111-118, Copenhagen, Denmark.
Jinhua Du and Andy Way. 2010. The Impact of
Source-Side Syntactic Reordering on Hierarchical
Phrase-based SMT. EAMT 2010: 14th Annual Con-
ference of the European Association for Machine
Translation, Saint-Raphae?l, France.
Jakob Elming. 2008. Syntactic reordering integrated
with phrase-based SMT. Coling 2008: 22nd In-
ternational Conference on Computational Linguis-
tics, Proceedings of the conference, pages 209-216,
Manchester, UK.
Jakob Elming, and Nizar Habash. 2009. Syntac-
tic reordering for English-Arabic phrase-based ma-
chine translation. Proceedings of the EACL 2009
Workhop on Computational Approaches to Semitic
Languages, pages 69-77, Athens, Greece.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. MT Summit XI, pages
215-222, Copenhagen, Denmark.
Jie Jiang, Andy Way, Julie Carson-Berndsen. 2010.
Lattice Score-Based Data Cleaning For Phrase-
Based Statistical Machine Translation. EAMT
2010: 14th Annual Conference of the European As-
sociation for Machine Translation, Saint-Raphae?l,
France.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. ACL 2007: proceedings of demo and
poster sessions, pp. 177-180, Prague, Czech Repub-
lic.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. ACL 2007: proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 720-727, Prague, Czech
Republic.
Xiaoyi Ma. 2006. Champollion: A Robust Paral-
lel Text Sentence Aligner. LREC 2006: Fifth In-
ternational Conference on Language Resources and
Evaluation, pp.489-492, Genova, Italy.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. ACL-2003:
41st Annual meeting of the Association for Compu-
tational Linguistics, pp. 160-167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method For Automatic
Evaluation of Machine Translation. ACL-2002:
40th Annual meeting of the Association for Compu-
tational Linguistics, pp.311-318, Philadelphia, PA.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. Coling-ACL 2006:
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 433-440, Sydney, Australia.
Felipe Sa?nchez-Mart??nez and Andy Way. 2009.
Marker-based filtering of bilingual phrase pairs for
SMT. EAMT-2009: Proceedings of the 13th An-
nual Conference of the European Association for
Machine Translation, pages 144-151, Barcelona,
Spain.
Chao Wang, Michael Collins, and Philipp Koehn.
2007a. Chinese syntactic reordering for statistical
machine translation. EMNLP-CoNLL-2007: Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
737-745, Prague, Czech Republic.
Fei Xia, and Michael McCord 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. Coling 2004: 20th International
Conference on Computational Linguistics, pages
508-514, University of Geneva, Switzerland.
Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2), pages 207-238.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
Proceedings of the 47th Annual Meeting of the ACL
and the 4th IJCNLP, pages 333-341, Suntec, Singa-
pore.
Yuqi Zhang, Richard Zens, and Hermann Ney 2007a.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statisti-
cal machine translation. SSST, NAACL-HLT-2007
AMTA Workshop on Syntax and Structure in Statis-
tical Translation, pages 1-8, Rochester, NY.
Yuqi Zhang, Richard Zens, and Hermann Ney 2007b.
Improved chunk-level reordering for statistical ma-
chine translation. IWSLT 2007: International Work-
shop on Spoken Language Translation, pages 21-28,
Trento, Italy.
27
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101?109,
COLING 2010, Beijing, August 2010.
HMM Word-to-Phrase Alignment with Dependency Constraints
Yanjun Ma Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yma,away}@computing.dcu.ie
Abstract
In this paper, we extend the HMM word-
to-phrase alignment model with syntac-
tic dependency constraints. The syn-
tactic dependencies between multiple
words in one language are introduced
into the model in a bid to produce co-
herent alignments. Our experimental re-
sults on a variety of Chinese?English
data show that our syntactically con-
strained model can lead to as much as
a 3.24% relative improvement in BLEU
score over current HMM word-to-phrase
alignment models on a Phrase-Based
Statistical Machine Translation system
when the training data is small, and
a comparable performance compared to
IBM model 4 on a Hiero-style system
with larger training data. An intrin-
sic alignment quality evaluation shows
that our alignment model with depen-
dency constraints leads to improvements
in both precision (by 1.74% relative) and
recall (by 1.75% relative) over the model
without dependency information.
1 Introduction
Generative word alignment models including
IBM models (Brown et al, 1993) and HMM
word alignment models (Vogel et al, 1996) have
been widely used in various types of Statisti-
cal Machine Translation (SMT) systems. This
widespread use can be attributed to their robust-
ness and high performance particularly on large-
scale translation tasks. However, the quality
of the alignment yielded from these models is
still far from satisfactory even with significant
amounts of training data; this is particularly true
for radically different languages such as Chinese
and English.
The weakness of most generative models of-
ten lies in the incapability of addressing one to
many (1-to-n), many to one (n-to-1) and many
to many (m-to-n) alignments. Some research di-
rectly addresses m-to-n alignment with phrase
alignment models (Marcu and Wong, 2002).
However, these models are unsuccessful largely
due to intractable estimation (DeNero and Klein,
2008). Recent progress in better parameteri-
sation and approximate inference (Blunsom et
al., 2009) can only augment the performance of
these models to a similar level as the baseline
where bidirectional word alignments are com-
bined with heuristics and subsequently used to
induce translation equivalence (e.g. (Koehn et
al., 2003)). The most widely used word align-
ment models, such as IBM models 3 and 4, can
only model 1-to-n alignment; these models are
often called ?asymmetric? models. IBM models
3 and 4 model 1-to-n alignments using the notion
of ?fertility?, which is associated with a ?defi-
ciency? problem despite its high performance in
practice.
On the other hand, the HMM word-to-phrase
alignment model tackles 1-to-n alignment prob-
lems with simultaneous segmentation and align-
ment while maintaining the efficiency of the
models. Therefore, this model sets a good ex-
ample of addressing the tradeoffs between mod-
elling power and modelling complexity. This
model can also be seen as a more generalised
101
case of the HMM word-to-word model (Vogel et
al., 1996; Och and Ney, 2003), since this model
can be reduced to an HMM word-to-word model
by restricting the generated target phrase length
to one. One can further refine existing word
alignment models with syntactic constraints (e.g.
(Cherry and Lin, 2006)). However, most re-
search focuses on the incorporation of syntactic
constraints into discriminative alignment mod-
els. Introducing syntactic information into gen-
erative alignment models is shown to be more
challenging mainly due to the absence of appro-
priate modelling of syntactic constraints and the
?inflexibility? of these generative models.
In this paper, we extend the HMM word-to-
phrase alignment model with syntactic depen-
dencies by presenting a model that can incor-
porate syntactic information while maintaining
the efficiency of the model. This model is based
on the observation that in 1-to-n alignments,
the n words bear some syntactic dependencies.
Leveraging such information in the model can
potentially further aid the model in producing
more fine-grained word alignments. The syn-
tactic constraints are specifically imposed on the
n words involved in 1-to-n alignments, which
is different from the cohesion constraints (Fox,
2002) as explored by Cherry and Lin (2006),
where knowledge of cross-lingual syntactic pro-
jection is used. As a syntactic extension of the
open-source MTTK implementation (Deng and
Byrne, 2006) of the HMM word-to-phrase align-
ment model, its source code will also be released
as open source in the near future.
The remainder of the paper is organised as fol-
lows. Section 2 describes the HMM word-to-
phrase alignment model. In section 3, we present
the details of the incorporation of syntactic de-
pendencies. Section 4 presents the experimental
setup, and section 5 reports the experimental re-
sults. In section 6, we draw our conclusions and
point out some avenues for future work.
2 HMM Word-to-Phrase Alignment
Model
In HMM word-to-phrase alignment, a sentence
e is segmented into a sequence of consecutive
phrases: e = vK1 , where vk represents the kth
phrase in the target sentence. The assumption
that each phrase vk generated as a translation of
one single source word is consecutive is made to
allow efficient parameter estimation. Similarly
to word-to-word alignment models, a variable
aK1 is introduced to indicate the correspondence
between the target phrase index and a source
word index: k ? i = ak indicating a mapping
from a target phrase vk to a source word fak . A
random process ?k is used to specify the num-
ber of words in each target phrase, subject to the
constraints J =
?K
k=1 ?k, implying that the to-
tal number of words in the phrases agrees with
the target sentence length J .
The insertion of target phrases that do not cor-
respond to any source words is also modelled
by allowing a target phrase to be aligned to a
non-existent source word f0 (NULL). Formally,
to indicate whether each target phrase is aligned
to NULL or not, a set of indicator functions
?K1 = {?1, ? ? ? , ?K} is introduced (Deng and
Byrne, 2008): if ?k = 0, then NULL ? vk; if
?k = 1, then fak ? vk.
To summarise, an alignment a in an HMM
word-to-phrase alignment model consists of the
following elements:
a = (K,?K1 , aK1 , ?K1 )
The modelling objective is to define a condi-
tional distribution P (e,a|f) over these align-
ments. Following (Deng and Byrne, 2008),
P (e,a|f) can be decomposed into a phrase count
distribution (1) modelling the segmentation of a
target sentence into phrases (P (K|J, f) ? ?K
with scalar ? to control the length of the hy-
pothesised phrases), a transition distribution (2)
modelling the dependencies between the current
link and the previous links, and a word-to-phrase
translation distribution (3) to model the degree
to which a word and a phrase are translational to
each other.
P (e,a|f) = P (vK1 ,K, aK1 , ?K1 , ?K1 |f)
= P (K|J, f) (1)
P (aK1 , ?K1 , ?K1 |K,J, f) (2)
P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)(3)
102
The word-to-phrase translation distribution
(3) is formalised as in (4):
P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)
=
K?
k=1
pv(vk|?k ? fak , ?k) (4)
Note here that we assume that the translation
of each target phrase is conditionally indepen-
dent of other target phrases given the individual
source words.
If we assume that each word in a target phrase
is translated with a dependence on the previ-
ously translated word in the same phrase given
the source word, we derive the bigram transla-
tion model as follows:
pv(vk|fak , ?k, ?k) = pt1(vk[1]|?k, fak)
?k?
j=2
pt2(vk[j]|vk[j ? 1], ?k, fak)
where vk[1] is the first word in phrase vk, vk[j]
is the jth word in vk, pt1 is an unigram transla-
tion probability and pt2 is a bigram translation
probability. The intuition is that the first word
in vk is firstly translated by fak and the transla-
tion of the remaining words vk[j] in vk from fak
is dependent on the translation of the previous
word vk[j ? 1] from fak . The use of a bigram
translation model can address the coherence of
the words within the phrase vk so that the qual-
ity of phrase segmentation can be improved.
3 Syntactically Constrained HMM
Word-to-Phrase Alignment Models
3.1 Syntactic Dependencies for
Word-to-Phrase Alignment
As a proof-of-concept, we performed depen-
dency parsing on the GALE gold-standard word
alignment corpus using Maltparser (Nivre et al,
2007).1 We find that 82.54% of the consec-
utive English words have syntactic dependen-
cies and 77.46% non-consecutive English words
have syntactic dependencies in 1-to-2 Chinese?
English (ZH?EN) word alignment (one Chi-
nese word aligned to two English words). For
1http://maltparser.org/
English?Chinese (EN?ZH) word alignment, we
observe that 75.62% of the consecutive Chinese
words and 71.15% of the non-consecutive Chi-
nese words have syntactic dependencies. Our
model represents an attempt to encode these lin-
guistic intuitions.
3.2 Component Variables and Distributions
We constrain the word-to-phrase alignment
model with a syntactic coherence model. Given
a target phrase vk consisting of ?k words, we
use the dependency label rk between words vk[1]
and vk[?k] to indicate the level of coherence.
The dependency labels are a closed set obtained
from dependency parsers, e.g. using Maltparser,
we have 20 dependency labels for English and
12 for Chinese in our data. Therefore, we have
an additional variable rK1 associated with the se-
quence of phrases vK1 to indicate the syntactic
coherence of each phrase, defining P (e,a|f) as
below:
P (rK1 , vK1 ,K, aK1 , ?K1 , ?K1 |f) = P (K|J, f)
P (aK1 , ?K1 , ?K1 |K,J, f)P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)
P (rK1 |aK1 , ?K1 , ?K1 , vK1 ,K, J, f) (5)
The syntactic coherence distribution (5) is
simplified as in (6):
P (rK1 |aK1 , ?K1 , ?K1 , vK1 ,K, J, f)
=
K?
k=1
pr(rk; ?, fak , ?k) (6)
Note that the coherence of each target phrase
is conditionally independent of the coherence of
other target phrases given the source words fak
and the number of words in the current phrase
?k. We name the model in (5) the SSH model.
SSH is an abbreviation of Syntactically con-
strained Segmental HMM, given the fact that
the HMM word-to-phrase alignment model is a
Segmental HMM model (SH) (Ostendorf et al,
1996; Murphy, 2002).
As our syntactic coherence model utilises syn-
tactic dependencies which require the presence
of at least two words in target phrase vk, we
therefore model the cases of ?k = 1 and ?k ? 2
103
separately. We rewrite (6) as follows:
pr(rk; ?, fak , ?k) ={
p?k=1(rk; ?, fak ) if ?k = 1
p?k?2(rk; ?, fak ) if ?k ? 2
where p?k=1 defines the syntactic coherence
when the target phrase only contains one word
(?k = 1) and p?k?2 defines the syntactic co-
herence of a target phrase composed of multiple
words (?k ? 2). We define p?k=1 as follows:
p?k=1(rk; ?, fak ) ? pn(?k = 1; ?, fak )
where the coherence of the target phrase (word)
vk is defined to be proportional to the probability
of target phrase length ?k = 1 given the source
word fak . The intuition behind this model is that
the syntactic coherence is strong iff the probabil-
ity of the source fak fertility ?k = 1 is high.
For p?k?2, which measures the syntactic co-
herence of a target phrase consisting of more
than two words, we use the dependency label rk
between words vk[1] and vk[?k] to indicate the
level of coherence. A distribution over the values
rk ? R = {SBJ,ADJ, ? ? ? } (R is the set of de-
pendency types for a specific language) is main-
tained as a table for each source word associated
with all the possible lengths ? ? {2, ? ? ? ,N})
of the target phrase it can generate, e.g. we set
N = 4 for ZH?EN alignment and N = 2 for
EN?ZH alignment in our experiments.
Given a target phrase vk containing ?k(?k ?
2) words, it is possible that there are no depen-
dencies between the first word vk[1] and the last
word vk[?k]. To account for this fact, we intro-
duce a indicator function ? as in below:
?(vk[1], ?k) =
?
??
??
1 if vk[1] and vk[?k]have
syntactic dependencies
0 otherwise
We can thereafter introduce a distribution p?(?),
where p?(? = 0) = ? (0 ? ? ? 1) and
p?(? = 0) = 1? ? , with ? indicating how likely
it is that the first and final words in a target phrase
do not have any syntactic dependencies. We can
set ? to a small number to favour target phrases
satisfying the syntactic constraints and to a larger
number otherwise. The introduction of this vari-
able enables us to tune the model towards our
different end goals. We can now define p?k?2
as:
p?k?2(rk; ?, fak) = p(rk|?; ?, fak )p?(?)
where we insist that p(rk|?; ?, fak ) = 1 if
? = 0 (the first and last words in the target
phrase do not have syntactic dependencies) to
reflect the fact that in most arbitrary consecu-
tive word sequences the first and last words do
not have syntactic dependencies, and otherwise
p(rk|?; ?, fak ) if ? = 1 (the first and last words
in the target phrase have syntactic dependen-
cies).
3.3 Parameter Estimation
The Forward-Backward Algorithm (Baum,
1972), a version of the EM algorithm (Dempster
et al, 1977), is specifically designed for unsu-
pervised parameter estimation of HMM models.
The Forward statistic ?j(i, ?, ?) in our model
can be calculated recursively over the trellis as
follows:
?j(i, ?, ?) = {
?
i?,??,??
?j??(i?, ??, ??)pa(i|i?, ?; I)}
pn(?; ?, fi)?pt1(ej??+1|?, fi)
j?
j?=j??+2
pt2(ej? |ej??1, ?, fi)pr(rk; ?, fi, ?)
which sums up the probabilities of every path
that could lead to the cell ?j, i, ??. Note that the
syntactic coherence term pr(rk; ?, fi, ?) can ef-
ficiently be added into the Forward procedure.
Similarly, the Backward statistic ?j(i, ?, ?) is
calculated over the trellis as below:
?j(i, ?, ?) =
?
i?,??,??
?j+??(i?, ??, ??)pa(i?|i, h?; I)
pn(??; ??, fi?)?pt1(ej+1|??, fi?)
j+???
j?=j+2
pt2(ej? |ej??1, ??, fi?)pr(rk; ??, fi? , ??)
Note also that the syntactic coherence term
pr(rk; ??, fi? , ??) can be integrated into the Back-
ward procedure efficiently.
104
Posterior probability can be calculated based
on the Forward and Backward probabilities.
3.4 EM Parameter Updates
The Expectation step accumulates fractional
counts using the posterior probabilities for each
parameter during the Forward-Backward passes,
and the Maximisation step normalises the counts
in order to generate updated parameters.
The E-step for the syntactic coherence model
proceeds as follows:
c(r?; f, ??) =
?
(f ,e)?T
?
i,j,?,fi=f
?j(i, ?, ? = 1)
?(?, ??)?(?j(e, ?), r?)
where ?j(i, ?, ?) is the posterior probability that
a target phrase tjj??+1 is aligned to source word
fi, and ?j(e, ?) is the syntactic dependency label
between ej??+1 and ej . The M-step performs
normalisation, as below:
pr(r?; f, ??) =
c(r?; f, ??)?
r c(r; f, ??)
Other component parameters can be estimated
in a similar manner.
4 Experimental Setup
4.1 Data
We built the baseline word alignment and
Phrase-Based SMT (PB-SMT) systems using ex-
isting open-source toolkits for the purposes of
fair comparison. A collection of GALE data
(LDC2006E26) consisting of 103K (2.9 million
English running words) sentence pairs was firstly
used as a proof of concept (?small?), and FBIS
data containing 238K sentence pairs (8 million
English running words) was added to construct a
?medium? scale experiment. To investigate the
intrinsic quality of the alignment, a collection
of parallel sentences (12K sentence pairs) for
which we have manually annotated word align-
ment was added to both ?small? and ?medium?
scale experiments. Multiple-Translation Chinese
Part 1 (MTC1) from LDC was used for Mini-
mum Error-Rate Training (MERT) (Och, 2003),
and MTC2, 3 and 4 were used as development
test sets. Finally the test set from NIST 2006
evaluation campaign was used as the final test
set.
The Chinese data was segmented using the
LDC word segmenter. The maximum-entropy-
based POS tagger MXPOST (Ratnaparkhi, 1996)
was used to tag both English and Chinese texts.
The syntactic dependencies for both English and
Chinese were obtained using the state-of-the-art
Maltparser dependency parser, which achieved
84% and 88% labelled attachment scores for
Chinese and English respectively.
4.2 Word Alignment
The GIZA++ (Och and Ney, 2003) implementa-
tion of IBM Model 4 (Brown et al, 1993) is used
as the baseline for word alignment. Model 4 is
incrementally trained by performing 5 iterations
of Model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4. We
compared our model against the MTTK (Deng
and Byrne, 2006) implementation of the HMM
word-to-phrase alignment model. The model
training includes 10 iterations of Model 1, 5 it-
erations of Model 2, 5 iterations of HMM word-
to-word alignment, 20 iterations (5 iterations re-
spectively for phrase lengths 2, 3 and 4 with un-
igram translation probability, and phrase length
4 with bigram translation probability) of HMM
word-to-phrase alignment for ZH?EN alignment
and 5 iterations (5 iterations for phrase length
2 with uniform translation probability) of HMM
word-to-phrase alignment for EN?ZH. This con-
figuration is empirically established as the best
for Chinese?English word alignment. To allow
for a fair comparison between IBM Model 4
and HMM word-to-phrase alignment models, we
also restrict the maximum fertility in IBM model
4 to 4 for ZH?EN and 2 for EN?ZH (the default
is 9 in GIZA++ for both ZH?EN and EN?ZH).
?grow-diag-final? heuristic described in (Koehn
et al, 2003) is used to derive the refined align-
ment from bidirectional alignments.
4.3 MT system
The baseline in our experiments is a standard
log-linear PB-SMT system. With the word align-
ment obtained using the method described in
105
section 4.2, we perform phrase-extraction using
heuristics described in (Koehn et al, 2003), Min-
imum Error-Rate Training (MERT) (Och, 2003)
optimising the BLEU metric, a 5-gram language
model with Kneser-Ney smoothing (Kneser and
Ney, 1995) trained with SRILM (Stolcke, 2002)
on the English side of the training data, and
MOSES (Koehn et al, 2007) for decoding. A
Hiero-style decoder Joshua (Li et al, 2009) is
also used in our experiments. All significance
tests are performed using approximate randomi-
sation (Noreen, 1989) at p = 0.05.
5 Experimental Results
5.1 Alignment Model Tuning
In order to find the value of ? in the SSH model
that yields the best MT performance, we used
three development test sets using a PB-SMT sys-
tem trained on the small data condition. Figure 1
shows the results on each development test set
using different configurations of the alignment
models. For each system, we obtain the mean
of the BLEU scores (Papineni et al, 2002) on
the three development test sets, and derive the
optimal value for ? of 0.4, which we use here-
after for final testing. It is worth mentioning
that while IBM model 4 (M4) outperforms other
models including the HMM word-to-word (H)
and word-to-phrase (SH) alignment model in our
current setup, using the default IBM model 4 set-
ting (maximum fertility 9) yields an inferior per-
formance (as much as 8.5% relative) compared
to other models.
 0.11
 0.115
 0.12
 0.125
 0.13
 0.135
 0.14
M4 H SH SSH-0.05
SSH-0.1
SSH-0.2
SSH-0.3
SSH-0.4
SSH-0.5
SSH-0.6
BL
EU
 s
co
re
alignment systems
MTC2
MTC3
MTC4
Figure 1: BLEU score on development test set
using PB-SMT system
PB-SMT Hiero
small medium small medium
H 0.1440 0.2591 0.1373 0.2595
SH 0.1418 0.2517 0.1372 0.2609
SSH 0.1464 0.2518 0.1356 0.2624
M4 0.1566 0.2627 0.1486 0.2660
Table 1: Performance of PB-SMT using different
alignment models on NIST06 test set
5.2 Translation Results
Table 1 shows the performance of PB-SMT and
Hiero systems using a small amount of data for
alignment model training on the NIST06 test set.
For the PB-SMT system trained on the small data
set, using SSH word alignment leads to a 3.24%
relative improvement over SH, which is statis-
tically significant. SSH also leads to a slight
gain over the HMM word-to-word alignment
model (H). However, when the PB-SMT system
is trained on larger data sets, there are no sig-
nificant differences between SH and SSH. Addi-
tionally, both SH and SSH models underperform
H on the medium data condition, indicating that
the performance of the alignment model tuned
on the PB-SMT system with small training data
does not carry over to PB-SMT systems with
larger training data (cf. Figure 1). IBM model
4 demonstrates stronger performance over other
models for both small and medium data condi-
tions.
For the Hiero system trained on a small data
set, no significant differences are observed be-
tween SSH, SH and H. On a larger training set,
we observe that SSH alignment leads to better
performance compared to SH. Both SH and SSH
alignments achieved higher translation quality
than H. Note that while IBM model 4 outper-
forms other models on a small data condition, the
difference between IBM model 4 and SSH is not
statistically significant on a medium data condi-
tion. It is also worth pointing out that the SSH
model yields significant improvement over IBM
model 4 with the default fertility setting, indicat-
ing that varying the fertility limit in IBM model
4 has a significant impact on translation quality.
In summary, the SSH model which incorpo-
rates syntactic dependencies into the SH model
achieves consistently better performance than
106
ZH?EN EN?ZH
P R P R
H 0.5306 0.3752 0.5282 0.3014
SH 0.5378 0.3802 0.5523 0.3151
SSH 0.5384 0.3807 0.5619 0.3206
M4 0.5638 0.3986 0.5988 0.3416
Table 2: Intrinsic evaluation of the alignment us-
ing different alignment models
SH in both PB-SMT and Hiero systems under
both small and large data conditions. For a
PB-SMT system trained on the small data set,
the SSH model leads to significant gains over
the baseline SH model. The results also en-
tail an observation concerning the suitability of
different alignment models for different types
of SMT systems; trained on a large data set,
our SSH alignment model is more suitable to
a Hiero-style system than a PB-SMT system,
as evidenced by a lower performance compared
to IBM model 4 using a PB-SMT system, and
a comparable performance compared to IBM
model 4 using a Hiero system.
5.3 Intrinsic Evaluation
In order to further investigate the intrinsic qual-
ity of the word alignment, we compute the Preci-
sion (P), Recall (R) and F-score (F) of the align-
ments obtained using different alignment mod-
els. As the models investigated here are asym-
metric models, we conducted intrinsic evalua-
tion for both alignment directions, i.e. ZH?EN
word alignment where one Chinese word can be
aligned to multiple English words, and EN?ZH
word alignment where one English word can be
aligned to multiple Chinese words.
Table 2 shows the results of the intrinsic eval-
uation of ZH?EN and EN?ZH word alignment
on a small data set (results on the medium data
set follow the same trend but are left out due
to space limitations). Note that the P and R
are all quite low, demonstrating the difficulty of
Chinese?English word alignment in the news do-
main. For the ZH?EN direction, using the SSH
model does not lead to significant gains over SH
in P or R. For the EN?ZH direction, the SSH
model leads to a 1.74% relative improvement in
P, and a 1.75% relative improvement in R over
the SH model. Both SH and SSH lead to gains
over H for both ZH?EN and EN?ZH directions,
while gains in the EN?ZH direction appear to be
more pronounced. IBM model 4 achieves signif-
icantly higher P over other models while the gap
in R is narrow.
Relating Table 2 to Table 1, we observe that
the HMM word-to-word alignment model (H)
can still achieve good MT performance despite
the lower P and R compared to other mod-
els. This provides additional support to previ-
ous findings (Fraser and Marcu, 2007b) that the
intrinsic quality of word alignment does not nec-
essarily correlate with the performance of the re-
sulted MT system.
5.4 Alignment Characteristics
In order to further understand the characteristics
of the alignment that each model produces, we
investigated several statistics of the alignment re-
sults which can hopefully reveal the capabilities
and limitations of each model.
5.4.1 Pairwise Comparison
Given the asymmetric property of these align-
ment models, we can evaluate the quality of the
links for each word and compare the alignment
links across different models. For example, in
ZH?EN word alignment, we can compute the
links for each Chinese word and compare those
links across different models. Additionally, we
can compute the pairwise agreement in align-
ing each Chinese word for any two alignment
models. Similarly, we can compute the pairwise
agreement in aligning each English word in the
EN?ZH alignment direction.
For ZH?EN word alignment, we observe that
the SH and SSH models reach a 85.94% agree-
ment, which is not surprising given the fact that
SSH is a syntactic extension over SH, while IBM
model 4 and SSH reach the smallest agreement
(only 65.09%). We also observe that there is a
higher agreement between SSH and H (76.64%)
than IBM model 4 and H (69.58%). This can be
attributed to the fact that SSH is still a form of
HMM model while IBM model 4 is not. A simi-
lar trend is observed for EN?ZH word alignment.
107
ZH?EN EN?ZH
1-to-0 1-to-1 1-to-n 1-to-0 1-to-1 1-to-n
con. non-con. con. non-con.
HMM 0.3774 0.4693 0.0709 0.0824 0.4438 0.4243 0.0648 0.0671
SH 0.3533 0.4898 0.0843 0.0726 0.4095 0.4597 0.0491 0.0817
SSH 0.3613 0.5092 0.0624 0.0671 0.3990 0.4835 0.0302 0.0872
M4 0.2666 0.5561 0.0985 0.0788 0.3967 0.4850 0.0592 0.0591
Table 3: Alignment types using different alignment models
5.4.2 Alignment Types
Again, by taking advantage of the asymmet-
ric property of these alignment models, we can
compute different types of alignment. For both
ZH?EN (EN?ZH) alignment, we divide the links
for each Chinese (English) word into 1-to-0
where each Chinese (English) word is aligned
to the empty word ?NULL? in English (Chi-
nese), 1-to-1 where each Chinese (English) word
is aligned to only one word in English (Chinese),
and 1-to-n where each Chinese (English) word
is aligned to n (n ? 2) words in English (Chi-
nese). For 1-to-n links, depending on whether
the n words are consecutive, we have consecu-
tive (con.) and non-consecutive (non-con.) 1-to-
n links.
Table 3 shows the alignment types in the
medium data track. We can observe that for
ZH?EN word alignment, both SH and SSH pro-
duce far more 1-to-0 links than Model 4. It can
also be seen that Model 4 tends to produce more
consecutive 1-to-n links than non-consecutive 1-
to-n links. On the other hand, the SSH model
tends to produce more non-consecutive 1-to-n
links than consecutive ones. Compared to SH,
SSH tends to produce more 1-to-1 links than 1-
to-n links, indicating that adding syntactic de-
pendency constraints biases the model towards
only producing 1-to-n links when the n words
follow coherence constraint, i.e. the first and last
word in the chunk have syntactic dependencies.
For example, among the 6.24% consecutive ZH?
EN 1-to-n links produced by SSH, 43.22% of
them follow the coherence constraint compared
to just 39.89% in SH. These properties can have
significant implications for the performance of
our MT systems given that we use the grow-
diag-final heuristics to derive the symmetrised
word alignment based on bidirectional asymmet-
ric word alignments.
6 Conclusions and Future Work
In this paper, we extended the HMM word-to-
phrase word alignment model to handle syntac-
tic dependencies. We found that our model was
consistently better than that without syntactic de-
pendencies according to both intrinsic and ex-
trinsic evaluation. Our model is shown to be ben-
eficial to PB-SMT under a small data condition
and to a Hiero-style system under a larger data
condition.
As to future work, we firstly plan to investi-
gate the impact of parsing quality on our model,
and the use of different heuristics to combine
word alignments. Secondly, the syntactic co-
herence model itself is very simple, in that it
only covers the syntactic dependency between
the first and last word in a phrase. Accordingly,
we intend to extend this model to cover more so-
phisticated syntactic relations within the phrase.
Furthermore, given that we can construct dif-
ferent MT systems using different word align-
ments, multiple system combination can be con-
ducted to avail of the advantages of different sys-
tems. We also plan to compare our model with
other alignment models, e.g. (Fraser and Marcu,
2007a), and test this approach on more data and
on different language pairs and translation direc-
tions.
Acknowledgements
This research is supported by the Science Foundation Ire-
land (Grant 07/CE/I1142) as part of the Centre for Next
Generation Localisation (www.cngl.ie) at Dublin City Uni-
versity. Part of the work was carried out at Cambridge Uni-
versity Engineering Department with Dr. William Byrne.
The authors would also like to thank the anonymous re-
viewers for their insightful comments.
108
References
Baum, Leonard E. 1972. An inequality and associ-
ated maximization technique in statistical estimation for
probabilistic functions of Markov processes. Inequali-
ties, 3:1?8.
Blunsom, Phil, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of ACL-IJCNLP
2009, pages 782?790, Singapore.
Brown, Peter F., Stephen A. Della-Pietra, Vincent J. Della-
Pietra, and Robert L. Mercer. 1993. The mathematics of
Statistical Machine Translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Cherry, Colin and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative train-
ing. In Proceedings of the COLING-ACL 2006, pages
105?112, Sydney, Australia.
Dempster, Arthur, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
DeNero, John and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-08:
HLT, Short Papers, pages 25?28, Columbus, OH.
Deng, Yonggang and William Byrne. 2006. MTTK: An
alignment toolkit for Statistical Machine Translation. In
Proceedings of HLT-NAACL 2006, pages 265?268, New
York City, NY.
Deng, Yonggang and William Byrne. 2008. HMM word
and phrase alignment for Statistical Machine Transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Fox, Heidi. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the EMNLP 2002, pages
304?3111, Philadelphia, PA, July.
Fraser, Alexander and Daniel Marcu. 2007a. Getting the
structure right for word alignment: LEAF. In Pro-
ceedings of EMNLP-CoNLL 2007, pages 51?60, Prague,
Czech Republic.
Fraser, Alexander and Daniel Marcu. 2007b. Measuring
word alignment quality for Statistical Machine Transla-
tion. Computational Linguistics, 33(3):293?303.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE ICASSP, volume 1, pages 181?
184, Detroit, MI.
Koehn, Philipp, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT-NAACL 2003, pages 48?54, Edmonton, AB,
Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
Statistical Machine Translation. In Proceedings of ACL
2007, pages 177?180, Prague, Czech Republic.
Li, Zhifei, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation. In
Proceedings of the WMT 2009, pages 135?139, Athens,
Greece.
Marcu, Daniel and William Wong. 2002. A Phrase-Based,
joint probability model for Statistical Machine Transla-
tion. In Proceedings of EMNLP 2002, pages 133?139,
Philadelphia, PA.
Murphy, Kevin. 2002. Hidden semi-markov models (seg-
ment models). Technical report, UC Berkeley.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Ervin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency parsing.
Natural Language Engineering, 13(2):95?135.
Noreen, Eric W. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Och, Franz and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Compu-
tational Linguistics, 29(1):19?51.
Och, Franz. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In Proceedings of ACL 2003,
pages 160?167, Sapporo, Japan.
Ostendorf, Mari, Vassilios V. Digalakis, and Owen A. Kim-
ball. 1996. From HMMs to segment models: A uni-
fied view of stochastic modeling for speech recognition.
IEEE Transactions on Speech and Audio Processing,
4(5):360?378.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In Proceedings of ACL
2002, pages 311?318, Philadelphia, PA.
Ratnaparkhi, Adwait. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142, Somerset, NJ.
Stolcke, Andreas. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO.
Vogel, Stefan, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of ACL 1996, pages 836?841,
Copenhagen, Denmark.
109
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 26?34,
Beijing, August 2010
Multi-Word Expression-Sensitive Word Alignment
Tsuyoshi Okita1, Alfredo Maldonado Guerra2, Yvette Graham3, Andy Way1
{CNGL1, NCLT3} / School of Computing / Dublin City University,
CNGL / School of Computer Science and Statistics / Trinity College Dublin2
{tokita,ygraham,away}@computing.dcu.ie, maldonaa@scss.tcd.ie
Abstract
This paper presents a new word align-
ment method which incorporates knowl-
edge about Bilingual Multi-Word Expres-
sions (BMWEs). Our method of word
alignment first extracts such BMWEs in
a bidirectional way for a given corpus and
then starts conventional word alignment,
considering the properties of BMWEs in
their grouping as well as their alignment
links. We give partial annotation of align-
ment links as prior knowledge to the word
alignment process; by replacing the max-
imum likelihood estimate in the M-step
of the IBM Models with the Maximum A
Posteriori (MAP) estimate, prior knowl-
edge about BMWEs is embedded in the
prior in this MAP estimate. In our exper-
iments, we saw an improvement of 0.77
Bleu points absolute in JP?EN. Except
for one case, our method gave better re-
sults than the method using only BMWEs
grouping. Even though this paper does
not directly address the issues in Cross-
Lingual Information Retrieval (CLIR), it
discusses an approach of direct relevance
to the field. This approach could be
viewed as the opposite of current trends
in CLIR on semantic space that incorpo-
rate a notion of order in the bag-of-words
model (e.g. co-occurences).
1 Introduction
Word alignment (Brown et al, 1993; Vogel et
al., 1996; Och and Ney, 2003a; Graca et al,
2007) remains key to providing high-quality trans-
lations as all subsequent training stages rely on its
performance. It alone does not effectively cap-
ture many-to-many word correspondences, but in-
stead relies on the ability of subsequent heuristic
phrase extraction algorithms, such as grow-diag-
final (Koehn et al, 2003), to resolve them.
Some aligned corpora include implicit partial
alignment annotation, while for other corpora a
partial alignment can be extracted by state-of-
the-art techniques. For example, implicit tags
such as reference number within the patent cor-
pus of Fujii et al (2010) provide (often many-to-
many) correspondences between source and tar-
get words, while statistical methods for extract-
ing a partial annotation, like Kupiec et al (1993),
extract terminology pairs using linguistically pre-
defined POS patterns. Gale and Church (1991)
extract pairs of anchor words, such as num-
bers, proper nouns (organization, person, title),
dates, and monetary information. Resnik and
Melamed (1997) automatically extract domain-
specific lexica. Moore (2003) extracts named-
entities. In Machine Translation, Lambert and
Banchs (2006) extract BMWEs from a phrase ta-
ble, which is an outcome of word alignment fol-
lowed by phrase extraction; this method does not
alter the word alignment process.
This paper introduces a new method of incorpo-
rating previously known many-to-many word cor-
respondences into word alignment. A well-known
method of incorporating such prior knowledge
in Machine Learning is to replace the likelihood
maximization in the M-step of the EM algorithm
with either the MAP estimate or the Maximum
Penalized Likelihood (MPL) estimate (McLach-
26
lan and Krishnan, 1997; Bishop, 2006). Then, the
MAP estimate allows us to incorporate the prior,
a probability used to reflect the degree of prior be-
lief about the occurrences of the events.
A small number of studies have been carried
out that use partial alignment annotation for word
alignment. Firstly, Graca et al (2007) introduce
a posterior regularization to employ the prior that
cannot be easily expressed over model parameters
such as stochastic constraints and agreement con-
straints. These constraints are set in the E-step to
discard intractable alignments contradicting these
constraints. This mechanism in the E-step is in a
similar spirit to that in GIZA++ for IBM Model
3 and 4 which only searches around neighbour-
ing alignments around the Viterbi alignment. For
this reason, this algorithm is not intended to be
used combined with IBM Models 3 and 4. Al-
though theoretically it is possible to incorporate
partial annotation with a small change in its code,
Graca et al do not mention it. Secondly, Tal-
bot (2005) introduces a constrained EM method
which constrains the E-step to incorporate par-
tial alignment into word alignment,1 which is in
a similar manner to Graca et al (2007). He con-
ducted experiments using partial alignment anno-
tation based on cognate relations, a bilingual dic-
tionary, domain-specific bilingual semantic anno-
tation, and numerical pattern matching. He did
not incorporate BMWEs. Thirdly, Callison-Burch
et al (2004) replace the likelihood maximization
in the M-step with mixed likelihood maximiza-
tion, which is a convex combination of negative
log likelihood of known links and unknown links.
The remainder of this paper is organized as fol-
lows: in Section 2 we define the anchor word
alignment problem. In Section 3 we include
a review of the EM algorithm with IBM Mod-
els 1-5, and the HMM Model. Section 4 de-
scribes our own algorithm based on the combina-
tion of BMWE extraction and the modified word
alignment which incorporates the groupings of
BMWEs and enforces their alignment links; we
explain the EM algorithm with MAP estimation
1Although the code may be similar in practice to our Prior
Model I, his explanation to modify the E-step will not be
applied to IBM Models 3 and 4. Our view is to modify the
M-step due to the same reason above, i.e. GIZA++ searches
only over the alignment space around the Viterbi alignment.
pair GIZA++(no prior) Ours(with prior)
EN-FR fin ini prior fin ini prior
is NULL 1 .25 0 0 .25 .25
rosy en 1 .5 0 0 .5 .2
that . 1 .25 0 0 .25 .25
life la 1 .25 0 0 .25 0
. c? 1 .25 0 0 .25 .25
that c? 0 .25 0 1 .25 .25
is est 0 .25 0 1 .25 .25
life vie 0 .5 0 1 .5 1
rosy rose 0 .25 0 1 .25 .2
Table 1: The benefit of prior knowledge of anchor
words.
with three kinds of priors. In Section 5 our exper-
imental results are presented, and we conclude in
Section 6.
2 Anchor Word Alignment Problem
The input to standard methods of word alignment
is simply the sentence-aligned corpus, whereas
our alignment method takes in additionally a par-
tial alignment. We assume, therefore, the avail-
ability of a partial alignment, for example via a
MWE extraction tool. Let e? denote an English
sentence, and e denote an English word, through-
out this paper. The anchor word alignment prob-
lem is defined as follows:
Definition 1 (Anchor Word Alignment Problem)
Let (e?, f?) = {(e?1, f?1), . . . , (e?n, f?n)} be a parallel
corpus. By prior knowledge we additionally
have knowledge of anchor words (e?, f?) =
{(senti, te1, tf1 , pose1, posf1 , lengthe, lengthf ),
. . ., (sentk, ten , tfn , posen , posfn , lengthe,
lengthf )} where senti denotes sentence ID,
posei denotes the position of tei in a sentence e?i,
and lengthe (and lengthf ) denotes the sentence
length of the original sentence which includes
ei. Under a given (e?, f?) and (e?, f?), our objective
is to obtain word alignments. It is noted that an
anchor word may include a phrase pair which
forms n-to-m mapping objects.
Table 1 shows two example phrase pairs for
French to English c?est la vie and that is life, and
la vie en rose and rosy life with the initial value
for the EM algorithm, the prior value and the fi-
27
Statistical MWE extraction method
97|||groupe socialiste|||socialist group|||26|||26
101|||monsieur poettering|||mr poettering|||1|||4
103|||monsieur poettering|||mr poettering|||1|||11
110|||monsieur poettering|||mr poettering|||1|||9
117|||explication de vote|||explanation of vote|||28|||26
Heuristic-based MWE extraction method
28|||the wheel 2|||?? ?||| 25||| 5
28|||the primary-side fixed armature 13|||? ? ? ?
? ?? ? ? ?||| 13||| 9
28|||the secondary-side rotary magnet 7|||? ? ? ?
? ????? ?||| 15||| 11
Table 2: Example of MWE pairs in Europarl cor-
pus (FR-EN) and NTCIR patent corpus (JP-EN).
There are 5 columns for each term: sentence num-
ber, source term, target term, source position, and
target position. The number appended to each
term from the patent corpus (lower half) is a ref-
erence number. In this corpus, all the important
technical terms have been identified and annotated
with reference numbers.
nal lexical translation probability for Giza++ IBM
Model 4 and that of our modified Giza++. Our
modified Giza++ achieves the correct result when
anchor words ?life? and ?vie? are used to assign a
value to the prior in our model.
3 Word Alignment
We review two models which address the prob-
lem of word alignment. The aim of word align-
ment is to obtain the model parameter t among
English and French words, ei and fj respectively.
We search for this model parameter under some
model M where M is chosen by IBM Models 1-
5 and the HMM model. We introduce the latent
variable a, which is an alignment function with
the hypothesis that each e and f correspond to this
latent variable. (e, f, a) is a complete data set, and
(e, f) is an incomplete data set.
3.1 EM Algorithm
We follow the description of the EM algorithm for
IBM Models of Brown et al (1993) but introduce
the parameter t explicitly. In this model, the pa-
rameter t represents the lexical translation proba-
bilities t(ei|fj). It is noted that we use e|f rather
than f |e following the notation of Koehn (2010).
One important remark is that the Viterbi align-
ment of the sentence pair (e?, f?) = (eJ1 , f I1 ), which
is obtained as in (1):
Eviterbi : a?J1 = argmaxaJ1
p??(f, a|e) (1)
provides the best alignment for a given log-
likelihood distribution p??(f, a|e). Instead of sum-
ming, this step simplifies the E-step. However, un-
der our modification of maximum likelihood esti-
mate with MAP estimate, this simplification is not
a correct approximation of the summation since
our surface in the E-step is greatly perturbed by
the prior. There is no guarantee that the Viterbi
alignment is within the proximity of the target
alignment (cf. Table 1).
Let z be the latent variable, t be the parameters,
and x be the observations. The EM algorithm is
an iterative procedure repeating the E-step and the
M-step as in (2):
EEXH : q(z;x) =p(z|x; ?) (2)
MMLE : t? = argmax
t
Q(t, told)
= argmax
t
?
x,z
q(z|x) log p(x, z; t)
In the E-step, our knowledge of the values of the
latent variables in a is given only by the poste-
rior distribution p(a|e, f, t). Hence, the (negative
log)-likelihood of complete data (e, f, a), which
we denote by ? log p(t|e, f, a), is obtained over
all possible alignments a. We use the current pa-
rameter values told to find the posterior distribu-
tion of the latent variables given by p(a|e, f, told).
We then use this posterior distribution to find the
expectation of the complete data log-likelihood
evaluated for parameter value t. This expectation
is given by
?
a p(a|e, f, told) log p(e, f, a|t).
In the M-step, we use a maximal likelihood es-
timation to minimize negative log-likelihood in
order to determine the parameter t; note that t is
a lexical translation probability. Instead of using
the log-likelihood log p(a, e, f |t), we use the ex-
pected complete data log-likelihood over all the
possible alignments a that we obtained in the E-
28
step, as in (3):
MMLE : t? = argmax
t
Q(t, told) (3)
= c(f |e; f, e)?
e c(f |e; f, e)
where an auxiliary function c(e|f ; e, f) for IBM
Model 1 introduced by Brown et al is defined as
c(f |e; f, e) =
?
a
p(a|e, f)
m?
j=1
?(f, fj)?(e, eaj )
and where the Kronecker-Delta function ?(x, y) is
1 if x = y and 0 otherwise. This auxiliary func-
tion is convenient since the normalization factor of
this count is also required. We note that if we use
the MAP estimate, the E-step remains the same as
in the maximum likelihood case, whereas in the
M-step the quantity to be minimized is given by
Q(t, told) + log p(t). Hence, we search for the
value of t which maximizes the following equa-
tion:
MMAP : t? = argmax
t
Q(t, told) + log p(t)
3.2 HMM
A first-order Hidden Markov Model (Vogel et al,
1996) uses the sentence length probability p(J |I),
the mixture alignment probability p(i|j, I), and
the translation probability, as in (4):
p(f |e) = p(J |I)
J?
j=1
p(fj|ei) (4)
Suppose we have a training set of R observation
sequences Xr, where r = 1, ? ? ? , R, each of which
is labelled according to its class m, where m =
1, ? ? ? ,M , as in (5):
p(i|j, I) = r(i? j
I
J )?I
i?=1 r(i? ? j IJ )
(5)
The HMM alignment probabilities p(i|i?, I) de-
pend only on the jump width (i ? i?). Using a set
of non-negative parameters s(i? i?), we have (6):
p(i|i?, I) = s(i ? i
?)
?I
l=1 s(l ? i?)
(6)
4 Our Approach
Algorithm 1 Overall Algorithm
Given: a parallel corpus,
1. Extract MWEs by Algorithm 2.
2. Based on the results of Step 1, specify a set
of anchor word alignment links in the format of
anchor word alignment problem (cf. Definition
1 and Table 2).
3. Group MWEs in source and target text.
4. Calculate the prior in order to embed knowl-
edge about anchor words.
5. Calculate lexical translation probabilities
with the prior.
6. Obtain alignment probabilities.
7. Ungroup of MWEs in source and target text.
Algorithm 1 consists of seven steps. We use the
Model I prior for the case where our prior knowl-
edge is sparse and evenly distributed throughout
the corpus, whereas we use the Model II prior
when our prior knowledge is dense in a partial
corpus. A typical example of the former case
is when we use partial alignment annotation ex-
tracted throughout a corpus for bilingual terminol-
ogy. A typical example of the latter case is when a
sample of only a few hundred lines from the cor-
pus have been hand-annotated.
4.1 MWE Extraction
Our algorithm of extracting MWEs is a statisti-
cal method which is a bidirectional version of Ku-
piec (1993). Firstly, Kupiec presents a method to
extract bilingual MWE pairs in a unidirectional
manner based on the knowledge about typical
POS patterns of noun phrases, which is language-
dependent but can be written down with some ease
by a linguistic expert. For example in French they
are N N, N prep N, and N Adj. Secondly, we take
the intersection (or union) of extracted bilingual
MWE pairs.2
2In word alignment, bidirectional word alignment by tak-
ing the intersection or union is a standard method which
improves its quality compared to unidirectional word align-
ment.
29
Algorithm 2 MWE Extraction Algorithm
Given: a parallel corpus and a set of anchor
word alignment links:
1. We use a POS tagger (Part-Of-Speech Tag-
ger) to tag a sentence on the SL side.
2. Based on the typical POS patterns for the SL,
extract noun phrases on the SL side.
3. Count n-gram statistics (typically n =
1, ? ? ? , 5 are used) on the TL side which jointly
occur with each source noun phrase extracted
in Step 2.
4. Obtain the maximum likelihood counts of
joint phrases, i.e. noun phrases on the SL side
and n-gram phrases on the TL side.
5. Repeat the same procedure from Step 1 to 4
reversing the SL and TL.
6. Intersect (or union) the results in both direc-
tions.
Let SL be the source language side and TL be
the target language side. The procedure is shown
in Algorithm 2. We informally evaluated the
MWE extraction tool following Kupiec (1993) by
manually inspecting the mapping of the 100 most
frequent terms. For example, we found that 93 of
the 100 most frequent English terms in the patent
corpus were correctly mapped to their Japanese
translation.
Depending on the corpus, we can use more
prior knowledge about implicit alignment links.
For example in some categories of patent and
technical documents corpora,3 we can use heuris-
tics to extract the ?noun phrase? + ?reference
number? from both sides. This is due to the fact
that terminology is often labelled with a unique
reference number, which is labelled on both the
SL and TL sides.
4.2 Prior Model I
Prior for Exhaustive Alignment Space IBM
Models 1 and 2 implement a prior for all possible
3Unlike other language pairs, the availability of
Japanese?English parallel corpora is quite limited: the NT-
CIR patent corpus (Fujii et al, 2010) of 3 million sentence
pairs (the latest NTCIR-8 version) for the patent domain and
JENAAD corpus (Utiyama and Isahara, 2003) of 150k sen-
tence pairs for the news domain. In this regard, the patent
domain is particularly important for this particular language
pair.
Algorithm 3 Prior Model I for IBM Model 1
Given: parallel corpus e?, f? ,
anchor words biTerm
initialize t(e|f ) uniformly
do until convergence
set count(e|f ) to 0 for all e,f
set total(f) to 0 for all f
for all sentence pairs (e?s,f?s)
prior(e|f)s = getPriorModelI(e?, f? , biT erm)
for all words e in e?s
totals(e) = 0
for all words f in f?s
totals(e) += t(e|f )
for all words e in e?s
for all words f in f?s
count(e|f )+=t(e|f)/totals(e)? prior(e|f)s
total(f) += t(e|f)/totals(e) ? prior(e|f)s
for all f
for all e
t(e|f ) = count(e|f)/total(f)
alignments exhaustively. Such a prior requires the
following two conditions. Firstly, partial knowl-
edge about the prior that we use in our context is
defined as follows. Let us denote a bilingual term
list T = {(s1, t1), . . . , (sm, tm)}. For example
with IBM Model 1: Let us define the following
prior p(e|f, e, f ;T ) from Equation (4):
p(e|f, e, f ;T ) =
?
?
?
1 (ei = si, fj = tj)
0 (ei = si, fj 6= tj)
0 (ei 6= si, fj = tj)
uniform (ei 6= si, fj 6= tj)
Secondly, this prior should be proper for the ex-
haustive case and non-proper for the sampled
alignment space where by proper we mean that the
probability is normalized to 1. Algorithm 3 shows
the pseudo-code for Prior Model I. Note that if
the prior is uniform in the MAP estimation, this is
equivalent to maximum likelihood estimation.
Prior for Sampled Alignment (Function) Space
Due to the exponential costs introduced by fertil-
ity, null token insertion, and distortion probability,
IBM Models 3 and 4 do not consider all (I + 1)J
alignments exhaustively, but rather a small subset
in the E-step. Each iteration only uses the sub-
set of all the alignment functions: this sampling
30
is not uniform, as it only includes the best possi-
ble alignment with all its neighbouring alignments
which differ from the best alignment by one word
(this can be corrected by a move operation) or two
words (this can be corrected by a swap operation).
If we consider the neighbouring alignment via
a move or a swap operation, two issues arise.
Firstly, the fact that these two neighbouring align-
ments are drawn from different underlying distri-
butions needs to be taken into account, and sec-
ondly, that the application of a move and a swap
operation alters a row or column of a prior ma-
trix (or indices of the prior) since either operation
involves the manipulation of links.
Algorithm 4 Pseudo-code for Prior Model II Ex-
haustive Alignment Space
def getPriorModelII(e?,f? ,biTerm):
for i in sentence:
for e in e?i:
allWordsi = length of sentence e?
for f in f?i:
if (e, f ) in biTerm:
n= num of anchor words in i
uni(e|f)i = allWordsi?nallWordsi
expSum(e|f) += uni(e|f)i ? n
else:
countSum(e|f)i += n
countSum(e|f) += count(e|f)i
for e in alle:
for f in allf :
prior(e|f) = expSum(e|f) + countSum(e|f)
return prior(e|f)
Prior for Jump Width i? One implementation
of HMM is to use the forward-backward algo-
rithm. A prior should be embedded within the
forward-backward algorithm. From Equation (6),
there are three cases which depend on whether
ai and its neighbouring alignment ai?1 are deter-
mined by our prior knowledge about anchor words
or not. When both ai and aj are determined, this
probability is expressed as in (7):
p(i? i?; I) =
?
?
?
0 (else) (7)
1 (ei = si, fj = tj for ai) and
(e?i = s?i, f ?j = t?j for aj)
When either ai or aj is determined, this probabil-
ity is expressed as in (8):4
p(i? i?; I) =
?
???
???
0 (condition 1) (8)
1 (condition 2)
1
(m?#eai?????#eai+m)
(else)
(uniform distribution)
When neither ai nor aj is determined, this proba-
bility is expressed as in (9): 5
p(i? i?; I) =
?
????
????
0 (condition 3) (9)
1 (condition 4)
m?i?
(m?#eai?????#eai+m)2
(else)
(Pascal?s triangle distribution)
4.3 Prior Model II
Prior Model II assumes that we have prior knowl-
edge only in some part of the training corpus. A
typical example is when a small part of the corpus
has a hand-crafted ?gold standard? annotation.
Prior for Exhaustive Alignment Space Prior
Model II is used to obtain the prior probability
p(e|f) over all possible combinations of e and f .
In contrast to Prior Model I, which computes the
prior probability p(e|f) for each sentence, Prior
Model II computes the prior probability globally
for all sentences in the corpus. Algorithm 4 shows
the pseudo-code for Prior Model II Exhaustive
Alignment Space.
4condition 1 is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
?condition 2? is as follows:
((ei = si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei 6= si, fj = tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i = s?i, f ?j 6= t?j for aj))
5
?condition 3? is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
?condition 4? is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i 6= s?i, f ?j = t?j for aj)) or
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j 6= t?j for aj)) or
((ei 6= si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj)) or
((ei = si, fj 6= tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
31
Prior for Sampled Alignment (Function) Space
This is identical to that of the Prior Model II ex-
haustive alignment space with only a difference in
the normalization process.
Prior for Jump Width i? This categorization of
Prior Model II is the same as that of Prior Model I
for for Jump Width i? (see Section 4.2). Note that
Prior Model II requires more memory compared
to the Prior Model I.6
5 Experimental Settings
The baseline in our experiments is a standard
log-linear phrase-based MT system based on
Moses. The GIZA++ implementation (Och and
Ney, 2003a) of IBM Model 4 is used as the base-
line for word alignment, which we compare to
our modified GIZA++. Model 4 is incrementally
trained by performing 5 iterations of Model 1, 5
iterations of HMM, 5 iterations of Model 3, and
5 iterations of Model 4. For phrase extraction the
grow-diag-final heuristics are used to derive the
refined alignment from bidirectional alignments.
We then perform MERT while a 5-gram language
model is trained with SRILM. Our implementa-
tion is based on a modified version of GIZA++
(Och and Ney, 2003a). This modification is on the
function that reads a bilingual terminology file,
the function that calculates priors, the M-step in
IBM Models 1-5, and the forward-backward algo-
rithm in the HMM Model. Other related software
tools are written in Python and Perl: terminol-
ogy concatenation, terminology numbering, and
so forth.
6 Experimental Results
We conduct an experimental evaluation on the
NTCIR-8 corpus (Fujii et al, 2010) and on Eu-
roparl (Koehn, 2005). Firstly, MWEs are ex-
tracted from both corpora, as shown in Table 3.
In the second step, we apply our modified version
of GIZA++ in which we incorporate the results of
6This is because it needs to maintain potentially an ??m
matrix, where ? denotes the number of English tokens in the
corpus and m denotes the number of foreign tokens, even if
the matrix is sparse. Prior Model I only requires an ?? ? m?
matrix where ?? is the number of English tokens in a sentence
and m? is the number of foreign tokens in a sentence, which
is only needed until this information is incorporated in a pos-
terior probability during the iterative process.
corpus language size #unique #all
MWEs MWEs
statistical method
NTCIR EN-JP 200k 1,121 120,070
europarl EN-FR 200k 312 22,001
europarl EN-ES 200k 406 16,350
heuristic method
NTCIR EN-JP 200k 50,613 114,373
Table 3: Statistics of our MWE extraction method.
The numbers of MWEs are from 0.08 to 0.6 MWE
/ sentence pair in our statistical MWE extraction
methods.
MWE extraction. Secondly, in order to incorpo-
rate the extracted MWEs, they are reformatted as
shown in Table 2. Thirdly, we convert all MWEs
into a single token, i.e. we concatenate them with
an underscore character. We then run the modi-
fied version of GIZA++ and obtain a phrase and
reordering table. In the fourth step, we split the
concatenated MWEs embedded in the third step.
Finally, in the fifth step, we run MERT, and pro-
ceed with decoding before automatically evaluat-
ing the translations.
Table 4 shows the results where ?baseline? in-
dicates no BMWE grouping nor prior, and ?base-
line2? represents a BMWE grouping but without
the prior. Although ?baseline2? (BMWE group-
ing) shows a drop in performance in the JP?EN
/ EN?JP 50k sentence pair setting, Prior Model I
results in an increase in performance in the same
setting. Except for EN?ES 200k, our Prior Model
I was better than ?baseline2?. For EN?JP NT-
CIR using 200k sentence pairs, we obtained an
absolute improvement of 0.77 Bleu points com-
pared to the ?baseline?; for EN?JP using 50k sen-
tence pairs, 0.75 Bleu points; and for ES?EN Eu-
roparl corpus using 200k sentence pairs, 0.63 Bleu
points. In contrast, Prior Model II did not work
well. The possible reason for this is the misspec-
ification, i.e. the modelling by IBM Model 4 was
wrong in terms of the given data. One piece of ev-
idence for this is that most of the enforced align-
ments were found correct in a manual inspection.
For EN?JP NTCIR using the same corpus of
200k, although the number of unique MWEs ex-
32
size EN-JP Bleu JP-EN Bleu
50k baseline 16.33 baseline 22.01
50k baseline2 16.10 baseline2 21.71
50k prior I 17.08 prior I 22.11
50k prior II 16.02 prior II 20.02
200k baseline 23.42 baseline 21.68
200k baseline2 24.10 baseline2 22.32
200k prior I 24.22 prior I 22.45
200k prior II 23.22 prior II 21.00
size FR-EN Bleu EN-FR Bleu
50k baseline 17.68 baseline 17.80
50k baseline2 17.76 baseline2 18.00
50k prior I 17.81 prior I 18.02
50k prior II 17.01 prior II 17.30
200k baseline 18.40 baseline 18.20
200k baseline2 18.80 baseline2 18.50
200k prior I 18.99 prior I 18.60
200k prior II 18.20 prior II 17.50
size ES-EN Bleu EN-ES Bleu
50k baseline 16.21 baseline 15.17
50k baseline2 16.61 baseline2 15.60
50k prior I 16.91 prior I 15.87
50k prior II 16.15 prior II 14.60
200k baseline 16.87 baseline 17.62
200k baseline2 17.40 baseline2 18.21
200k prior I 17.50 prior I 18.20
200k prior II 16.50 prior II 17.10
Table 4: Results. Baseline is plain GIZA++ /
Moses (without BMWE grouping / prior), base-
line2 is with BMWE grouping, prior I / II are with
BMWE grouping and prior.
tracted by the statistical method and the heuris-
tic method varies significantly, the total number
of MWEs by each method becomes comparable.
The resulting Bleu score for the heuristic method
(24.24 / 22.48 Blue points for 200k EN?JP / JP?
EN) is slightly better than that of the statistical
method. The possible reason for this is related
to the way the heuristic method groups terms in-
cluding reference numbers, while the statistical
method does not. As a result, the complexity of
the alignment model simplifies slightly in the case
of the heuristic method.
7 Conclusion
This paper presents a new method of incorporat-
ing BMWEs into word alignment. We first de-
tect BMWEs in a bidirectional way and then use
this information to do groupings and to enforce
already known alignment links. For the latter pro-
cess, we replace the maximum likelihood estimate
in the M-step of the EM algorithm with the MAP
estimate; this replacement allows the incorpora-
tion of the prior in the M-step of the EM algo-
rithm. We include an experimental investigation
into incorporating extracted BMWEs into a word
aligner. Although there is some work which incor-
porates BMWEs in groupings, they do not enforce
alignment links.
There are several ways in which this work can
be extended. Firstly, although we assume that our
a priori partial annotation is reliable, if we extract
such MWEs automatically, we cannot avoid erro-
neous pairs. Secondly, we assume that the rea-
son why our Prior Model II did not work was due
to the misspecification (or wrong modelling). We
would like to check this by discriminative mod-
elling. Thirdly, although here we extract BMWEs,
we can extend this to extract paraphrases and non-
literal expressions.
8 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 07/CE/I1142) as part of
the Centre for Next Generation Localisation
(http://www.cngl.ie) at Dublin City Uni-
versity and Trinity College Dublin. We would also
like to thank the Irish Centre for High-End Com-
puting.
References
Bishop, Christopher M. 2006. Pattern Recognition
and Machine Learning. Springer. Cambridge, UK
Brown, Peter F., Vincent .J.D Pietra, Stephen
A.D.Pietra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Param-
eter Estimation. Computational Linguistics. 19(2),
pp. 263?311.
Callison-Burch, Chris, David Talbot and Miles Os-
borne. 2004. Statistical Machine Translation with
33
Word- and Sentence-Aligned Parallel Corpora. Pro-
ceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?04),
Main Volume. Barcelona, Spain, pp. 175?182.
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, Sayori Shimohata. 2010. Overview of the
Patent Translation Task at the NTCIR-8 Workshop.
Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pp. 293?302.
Graca, Joao de Almeida Varelas, Kuzman Ganchev,
Ben Taskar. 2007. Expectation Maximization
and Posterior Constraints. In Neural Information
Processing Systems Conference (NIPS), Vancouver,
BC, Canada, pp. 569?576.
Gale, William, and Ken Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of the 29th Annual Meeting of the Associ-
ation for Computational Linguistics. Berkeley CA,
pp. 177?184.
Koehn, Philipp, Franz Och, Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics. Edmonton,
Canada. pp. 115?124.
Koehn, Philipp. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit. Phuket, Thailand, pp.79-86.
Koehn, Philipp, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan,
W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar,
A. Constantin, and E. Herbst, 2007. Moses: Open
source toolkit for Statistical Machine Translation.
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, Prague, Czech Republic, pp. 177?180.
Koehn, Philipp. 2010. Statistical Machine Transla-
tion. Cambridge University Press. Cambridge, UK.
Kupiec, Julian. 1993. An Algorithm for finding Noun
Phrase Correspondences in Bilingual Corpora. In
Proceedings of the 31st Annual Meeting of Associa-
tion for Computational Linguistics. Columbus. OH.
pp. 17?22.
Lambert, Patrik and Rafael Banchs. 2006. Group-
ing Multi-word Expressions According to Part-Of-
Speech in Statistical Machine Translation. In Pro-
ceedings of the EACL Workshop on Multi-Word-
Expressions in a Multilingual Context. Trento, Italy,
pp. 9?16.
McLachlan, Geoffrey J. and Thriyambakam Krishnan,
1997. The EM Algorithm and Extensions. Wiley
Series in probability and statistics. New York, NY.
Moore, Robert C.. 2003. Learning Translations of
Named-Entity Phrases from Parallel Corpora. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Budapest, Hungary. pp. 259?266.
Moore, Robert C.. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP). Barcelona,
Spain, pp. 333?340.
Och, Franz and Herman Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics. 29(1), pp. 19?51.
Resnik, Philip and I. Dan Melamed, 1997. Semi-
Automatic Acquisition of Domain-Specific Transla-
tion Lexicons. Proceedings of the 5th Applied Nat-
ural Language Processing Conference. Washington,
DC., pp. 340?347.
Talbot, David. 2005. Constrained EM for parallel text
alignment, Natural Language Engineering, 11(3):
pp. 263?277.
Utiyama, Masao and Hitoshi Isahara. 2003. Reliable
Measures for Aligning Japanese-English News Arti-
cles and Sentences, In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics. Sapporo, Japan, pp. 72?79.
Vogel, Stephan, Hermann Ney, Christoph Tillmann
1996. HMM-Based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics.
Copenhagen, Denmark, pp. 836?841.
34
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 31?40,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Incorporating Source-Language Paraphrases into Phrase-Based SMT with
Confusion Networks
Jie Jiang,? Jinhua Du,? and Andy Way?
?CNGL, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland
{jjiang, away}@computing.dcu.ie
?School of Automation and Information Engineering,
Xi?an University of Technology, Xi?an, Shaanxi, China
jhdu@xaut.edu.cn
Abstract
To increase the model coverage, source-
language paraphrases have been utilized to
boost SMT system performance. Previous
work showed that word lattices constructed
from paraphrases are able to reduce out-of-
vocabulary words and to express inputs in
different ways for better translation quality.
However, such a word-lattice-based method
suffers from two problems: 1) path dupli-
cations in word lattices decrease the capac-
ities for potential paraphrases; 2) lattice de-
coding in SMT dramatically increases the
search space and results in poor time effi-
ciency. Therefore, in this paper, we adopt
word confusion networks as the input struc-
ture to carry source-language paraphrase in-
formation. Similar to previous work, we use
word lattices to build word confusion net-
works for merging of duplicated paths and
faster decoding. Experiments are carried out
on small-, medium- and large-scale English?
Chinese translation tasks, and we show that
compared with the word-lattice-based method,
the decoding time on three tasks is reduced
significantly (up to 79%) while comparable
translation quality is obtained on the large-
scale task.
1 Introduction
With the rapid development of large-scale parallel
corpus, research on data-driven SMT has made good
progress to the real world applications. Currently,
for a typical automatic translation task, the SMT
system searches and exactly matches the input sen-
tences with the phrases or rules in the models. Obvi-
ously, if the following two conditions could be sat-
isfied, namely:
? the words in the parallel corpus are highly
aligned so that the phrase alignment can be per-
formed well;
? the coverage of the input sentence by the paral-
lel corpus is high;
then the ?exact phrase match? translation method
could bring a good translation.
However, for some language pairs, it is not easy
to obtain a huge amount of parallel data, so it is not
that easy to satisfy these two conditions. To allevi-
ate this problem, paraphrase-enriched SMT systems
have been proposed to show the effectiveness of in-
corporating paraphrase information. In terms of the
position at which paraphrases are incorporated in the
MT-pipeline, previous work can be organized into
three different categories:
? Translation model augmentation with para-
phrases (Callison-Burch et al, 2006; Marton et
al., 2009). Here the focus is on the translation
of unknown source words or phrases in the in-
put sentences by enriching the translation table
with paraphrases.
? Training corpus augmentation with para-
phrases (Bond et al, 2008; Nakov, 2008a;
Nakov, 2008b). Paraphrases are incorporated
into the MT systems by expanding the training
data.
? Word-lattice-based method with para-
phrases (Du et al, 2010; Onishi et al,
31
2010). Instead of augmenting the transla-
tion table, source-language paraphrases are
constructed to enrich the inputs to the SMT
system. Another directly related work is to
use word lattices to deal with multi-source
translation (Schroeder et al, 2009), in which
paraphrases are actually generated from the
alignments of difference source sentences.
Comparing these three methods, the word-lattice-
based method has the least overheads because:
? The translation model augmentation method
has to re-run the whole MT pipeline once
the inputs are changed, while the word-lattice-
based method only need to transform the new
input sentences into word lattices.
? The training corpus augmentation method re-
quires corpus-scale expansion, which drasti-
cally increases the computational complexity
on large corpora, while the word-lattice-based
method only deals with the development set
and test set.
In (Du et al, 2010; Onishi et al, 2010), it is also
observed that the word-lattice-based method per-
formed better than the translation model augmen-
tation method on different scales and two different
language pairs in several translation tasks. Thus
they concluded that the word-lattice-based method
is preferable for this task.
However, there are still some drawbacks for the
word-lattice-based method:
? In the lattice construction processing, dupli-
cated paths are created and fed into SMT de-
coders. This decreases the paraphrase capacity
in the word lattices. Note that we use the phrase
?paraphrase capacity? to represent the amount
of paraphrases that are actually built into the
word lattices. As presented in (Du et al, 2010),
only a limited number of paraphrases are al-
lowed to be used while others are pruned during
the construction process, so duplicate paths ac-
tually decrease the number of paraphrases that
contribute to the translation quality.
? The lattice decoding in SMT decoder have
a very high computational complexity which
makes the system less feasible in real time ap-
plication.
Therefore, in this paper, we use confusion net-
works (CNs) instead of word lattices to carry para-
phrase information in the inputs for SMT decoders.
CNs are constructed from the aforementioned word
lattices, while duplicate paths are merged to increase
paraphrase capacity (e.g. by admitting more non-
duplicate paraphrases without increasing the input
size). Furthermore, much less computational com-
plexity is required to perform CN decoding instead
of lattice decoding in the SMT decoder. We car-
ried out experiments on small-, medium- and large-
scale English?Chinese translation tasks to compare
against a baseline PBSMT system, the translation
model augmentation of (Callison-Burch et al, 2006)
method and the word-lattice-based method of (Du et
al., 2010) to show the effectiveness of our novel ap-
proach.
The motivation of this work is to use CN as
the compromise between speed and quality, which
comes from previous studies in speech recog-
nition and speech translation: in (Hakkani-Tu?r
et al, 2005), word lattices are transformed into
CNs to obtain compact representations of multiple
aligned ASR hypotheses in speech understanding;
in (Bertoldi et al, 2008), CNs are also adopted
instead of word lattices as the source-side inputs
for speech translation systems. The main contribu-
tion of this paper is to show that this compromise
also works for SMT systems incorporating source-
language paraphrases in the inputs.
Regarding the use of paraphrases SMT system,
there are still other two categories of work that are
related to this paper:
? Using paraphrases to improve system optimiza-
tion (Madnani et al, 2007). With an English?
English MT system, this work utilises para-
phrases to reduce the number of manually
translated references that are needed in the
parameter tuning process of SMT, while pre-
served a similar translation quality.
? Using paraphrases to smooth translation mod-
els (Kuhn et al, 2010; Max, 2010). Either
cluster-based or example-based methods are
32
proposed to obtain better estimation on phrase
translation probabilities with paraphrases.
The rest of this paper is organized as follows:
In section 2, we present an overview of the word-
lattice-based method and its drawbacks. Section 3
proposes the CN-based method, including the build-
ing process and its application on paraphrases in
SMT. Section 4 presents the experiments and results
of the proposed method as well as discussions. Con-
clusions and future work are then given in Section
5.
2 Word-lattice-based method
Compared with translation model augmentation
with paraphrases (Callison-Burch et al, 2006),
word-lattice-based paraphrasing for PBSMT is in-
troduced in (Du et al, 2010). A brief overview of
this method is given in this section.
2.1 Lattice construction from paraphrases
The first step of the word-lattice-based method is to
generate paraphrases from parallel corpus. The al-
gorithm in (Bannard and Callison-Burch, 2005) is
used for this purpose by pivoting through phrases
in the source- and the target- languages: for each
source phrase, all occurrences of its target phrases
are found, and all the corresponding source phrases
of these target phrases are considered as the potential
paraphrases of the original source phrase (Callison-
Burch et al, 2006). A paraphrase probability
p(e2|e1) is defined to reflect the similarities between
two phrases, as in (1):
p(e2|e1) =
?
f
p(f |e1)p(e2|f) (1)
where the probability p(f |e1) is the probability that
the original source phrase e1 translates as a partic-
ular phrase f on the target side, and p(e2|f) is the
probability that the candidate paraphrase e2 trans-
lates as the source phrase. Here p(e2|f) and p(f |e1)
are defined as the translation probabilities estimated
using maximum likelihood by counting the observa-
tions of alignments between phrases e and f in the
w
x
w
y
...
q
1
q
2
...
q
m
...
w
x+1
w
y
...
w
x+1
w
x-1
w
y+1
q
1
q
2 
? q
m
......
w
x
...
w
y+1
w
x-1
Figure 1: Construct word lattices from paraphrases.
parallel corpus, as in (2) and (3):
p(e2|f) ?
count(e2, f)
?
e2 count(e2, f)
(2)
p(f |e1) ?
count(f, e1)
?
f count(f, e1)
(3)
The second step is to transform input sentences
in the development and test sets into word lattices
with paraphrases extracted in the first step. As il-
lustrated in Figure 1, given a sequence of words
{w1, . . . , wN} as the input, for each of the para-
phrase pairs found in the source sentence (e.g. pi =
{q1, . . . , qm} for {wx, . . . , wy}), add in extra nodes
and edges to make sure those phrases coming from
paraphrases share the same start nodes and end
nodes with that of the original ones. Subsequently
the following empirical methods are used to assign
weights on paraphrases edges:
? Edges originating from the input sentences are
assigned weight 1.
? The first edges for each of the paraphrases are
calculated as in (4):
w(e1pi) =
1
k + i
(1 <= i <= k) (4)
where 1 stands for the first edge of paraphrase
pi, and i is the probability rank of pi among
those paraphrases sharing with a same start
node, while k is a predefined constant as a
trade-off parameter for efficiency and perfor-
mance, which is related to the paraphrase ca-
pacity.
? The rest of the edges corresponding to the para-
phrases are assigned weight 1.
33
The last step is to modify the MT pipeline to tune
and evaluate the SMT system with word lattice in-
puts, as is described in (Du et al, 2010; Onishi et
al., 2010).
For further discussion, a real example of the gen-
erated word lattice is illustrated in Figure 2. In
the word lattice, double-line circled nodes and solid
lined edges come from originated from the origi-
nal sentence, while others are generated from para-
phrases. Word, weight and ranking of each edge are
displayed in the figure. By adopting such an input
structure, the diversity of the input sentences is in-
creased to provide more flexible translation options
during the decoding process, which has been shown
to improve translation performance (Du et al, 2010).
2.2 Path duplication and decoding efficiency
As can be seen in Figure 2, the construction pro-
cess in the previous steps tends to generate duplicate
paths in the word lattices. For example, there are two
paths from node 6 to node 11 with the same words
?secretary of state? but different edge probabilities
(the path via node 27 and 28 has the probability
1/12, while the path via node 26 and 9 has the prob-
ability 1/99). This is because the aforementioned
straightforward construction process does not track
path duplications from different spans on the source
side. Since the number of admitted paraphrases is
restricted by parameter k in formula (4), the path
duplication will decrease the paraphrase capacity to
a certain extend.
Moreover, state of the art PBSMT decoders (e.g.
Moses (Koehn et al, 2007)) have a much higher
computational complexity for lattice structures than
for sentences. Thus even though only the test sen-
tences need to be transformed into word lattices, de-
coding time is still too slow for real-time applica-
tions.
Motivated by transforming ASR word-graphs into
CNs (Bertoldi et al, 2008), we adopt CN as the
trade-off between efficiency and quality. We aim to
merge duplicate paths in the word lattices to increase
paraphrase capacity, and to speed up the decoding
process via CN decoding. Details of the proposed
method are presented in the following section.
3 Confusion-network-based method
CNs are weighted direct graphs where each path
from the start node to the end node goes through
all the other nodes. Each edge is labelled with a
word and a probability (or weight). Although it is
commonly required to normalize the probability of
edges between two consecutive nodes to sum up to
one, from the point of view of the decoder, this is
not a strict constraint as long as any score is pro-
vided (similar to the weights on the word lattices in
the last section, and we prefer to call it ?weight? in
this case).
The benefits of using CNs are:
1. the ability to represent the original word lattice
with a highly compact structure;
2. all hypotheses in the word lattice are totally or-
dered, so that the decoding algorithm is mostly
retained except for the collection of translation
options and the handeling of  edges (Bertoldi
et al, 2008), which requires much less compu-
tational resources than the lattice decoding.
The rest of this section details the construction pro-
cess of the CNs and the application in paraphrase-
enriched SMT.
3.1 Confusion Network building
We build our CN from the aforementioned word lat-
tices. Previous studies provide several methods to
do this. (Mangu et al, 2000) propose a method to
cluster lattice words on the similarity of pronuncia-
tions and frequency of occurrence, and then to create
CNs using cluster orders. Although this method has
a computational complexity of O(n3), the SRILM
toolkit (Stolcke, 2002) provides a modified algo-
rithm which runs much faster than the original ver-
sion. In (Hakkani-Tu?r et al, 2005), a pivot algorithm
is proposed to form CNs by normalizing the topol-
ogy of the input lattices.
In this paper, we use the modified method
of (Mangu et al, 2000) provided by the SRILM
toolkit to convert word lattices into CNs. Moreover,
we aim to obtain CNs with the following guidelines:
? Cluster the lattice words only by topological
orders and edge weights without considering
word similarity. The objective is to reduce the
34
Figure 2: An example of a real paraphrase lattice. Note that it is a subsection of the whole word lattice that is too big
to fit into this page, and edge weights have been evenly distributed for CN conversion as specified by formula (5).
impact of path duplications in the building pro-
cess, since duplicate words will bias the impor-
tance of paths.
? Assign edge weights by the ranking of para-
phrase probabilities, rather than by poste-
rior probabilities from the modified method
of (Mangu et al, 2000). This is similar to that
given in formula (4). The reason for this is to
reduce the impact of path duplications on the
calculation of weights.
Thus, we modified the construction process as fol-
lows:
1. For each of the input word lattices, replace
word texts with unique identifiers (to make the
lattice alignment uncorrelated to the word sim-
ilarity, since in this case, all words in the lattice
are different from each other).
2. Evenly distribute edge weights for each of the
lattices by modifying formula (4) as in (5):
w(ejpi) =
1
Mi
?
(k + i)
(1 <= i <= k) (5)
where 1 <= j <= Mi, given e
j
pi is the j
th
edge of paraphrase pi, and Mi is the number of
words in pi. This is to avoid large weights on
the paraphrase edges for lattice alignments.
3. Transform the weighted word lattices into CNs
with the SRILM toolkit, and the paraphrase
ranking information is carried on the edges.
4. Replace the word texts in step 1, and then for
each column of the CN, merge edges with same
words by keeping those with the highest rank-
ing (a smaller number indicates a higher rank-
ing, and edges from the original sentences will
always have the highest ranking). Note that to
assign ranking for each  edge which does not
appear in the word lattice, we use the ranking of
non-original edges (in the same column) which
have the closest posterior probability to it. (As-
sign ranking 1 if failed to find a such edge).
5. Reassign the edge weights: 1) edges from orig-
inal sentences are assigned with weight 1; 2)
edges from paraphrases are assigned with an
35
empirical method as in (6):
w(ecnpi ) =
1
k + i
(1 <= i <= k) (6)
where ecnpi are edges corresponding with para-
phrase pi, and i is the probability rank of pi in
formula (4), while k is also defined in formula
(4).
A real example of a constructed CN is depicted
in Figure 3, which is correspondent with the word
lattice in Figure 2. Unlike the word lattices, all the
nodes in the CN are generated from the original sen-
tence, while solid lined edges come from the orig-
inal sentence, and dotted lined edges correspond to
paraphrases.
As in shown in the Figures, duplicate paths in the
word lattices have been merged into CN edges by
step 4. For example, the two occurrences of ?sec-
retary of state? in the word lattices (one path from
node 6 to 11 via 27 and 28, and one path from node
6 to 11 via 26 and 9 in the word lattice) are merged
to keep the highest-ranked path in the CN (note
there is one  edge between node 9 and 10 to ac-
complish the merging operation). Furthermore, each
edge in the CN is assigned a weight by formula (6).
This weight assignment procedure penalizes paths
from paraphrases according to the paraphrase prob-
abilities, in a similar manner to the aforementioned
word-lattice-based method.
3.2 Modified MT pipeline
By transforming word lattices into CNs, dupli-
cate paths are merged. Furthermore the new fea-
tures on the edges are introduced by formula (6),
which is then tuned on the development set using
MERT (Och, 2003) in the log-linear model (Och and
Ney, 2002). Since the SMT decoders are able to
perform CN decoding (Bertoldi et al, 2008) in an
efficient multi-stack decoding way, decoding time is
drastically reduced compared to lattice decoding.
The training steps are then modified as fol-
lows: 1) Extract phrase table, reordering table, and
build target-side language models from parallel and
monolingual corpora respectively for the PBSMT
model; 2) Transform source sentences in the devel-
opment set into word lattices, and then transform
them into CNs using the method proposed in Sec-
tion 3.1; 3) Tune the PBSMT model on the CNs via
the development set. Note that the overhead of the
evaluation steps are: transform each test set sentence
into a word lattice, and also transform them into a
CN, then feed them into the SMT decoder to obtain
decoding results.
4 Experiments
4.1 Experimental setup
Experiments were carried out on three English?
Chinese translation tasks. The training corpora com-
prise 20K, 200K and 2.1 million sentence pairs,
where the former two corpora are derived from FBIS
corpus1 which is sentence-aligned by Champollion
aligner (Ma, 2006), the latter corpus comes from
HK parallel corpus,2 ISI parallel corpus,3 other news
data and parallel dictionaries from LDC.
The development set and the test set for the 20K
and 200K corpora are randomly selected from the
FBIS corpus, each of which contains 1,200 sen-
tences, with one reference. For the 2.1 million cor-
pus, the NIST 2005 Chinese?English current set
(1,082 sentences) with one reference is used as the
development set, and NIST 2003 English?Chinese
current set (1,859 sentences) with four references is
used as the test set.
Three baseline systems are built for comparison:
Moses PBSMT baseline system (Koehn et al, 2007),
a realization of the translation model augmentation
system described in (Callison-Burch et al, 2006)
(named ?Para-Sub? hereafter), and the word-lattice
based system proposed in (Du et al, 2010).
Word alignments on the parallel corpus are per-
formed using GIZA++ (Och and Ney, 2003) with
the ?grow-diag-final? refinement. Maximum phrase
length is set to 10 words and the parameters in the
log-linear model are tuned by MERT (Och, 2003).
All the language models are 5-gram built with the
SRILM toolkit (Stolcke, 2002) on the monolingual
part of the parallel corpora.
4.2 Paraphrase acquisition
The paraphrases data for all paraphrase-enriched
system is derived from the ?Paraphrase Phrase Ta-
1Paragraph-aligned corpus with LDC number
LDC2003E14.
2LDC number: LDC2004T08.
3LDC number: LDC2007T09.
36
Figure 3: An example of a real CN converted from a paraphrase lattice. Note that it is a subsection of the whole CN
that is converted from the word lattice in Figure 2.
ble?4 of TER-Plus (Snover et al, 2009). Further-
more, the following two steps are taken to filter out
noise paraphrases as described in (Du et al, 2010):
1. Filter out paraphrases with probabilities lower
than 0.01.
2. Filter out paraphrases which are not observed
in the phrase table. This objective is to guar-
antee that no extra out-of-vocabulary words are
introduced into the paraphrase systems.
The filtered paraphrase table is then used to generate
word lattices and CNs.
4.3 Experimental results
The results are reported in BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) scores.
Table 1 compares the performance of four sys-
tems on three translation tasks. As can be observed
from the Table, for 20K and 200K corpora, the
word-lattice-based system accomplished the best re-
sults. For the 20K corpus, the CN outperformed
the baseline PBSMT by 0.31 absolute (2.15% rel-
ative) BLEU points and 1.5 absolute (1.99% rela-
tive) TER points. For the 200K corpus, it still out-
performed the ?Para-Sub? by 0.06 absolute (0.26%
relative) BLEU points and 0.15 absolute (0.23% rel-
ative) TER points. Note that for the 2.1M corpus,
although CN underperformed the best word lattice
by an insignificant amount (0.06 absolute, 0.41%
4http://www.umiacs.umd.edu/?snover/terp/
downloads/terp-pt.v1.tgz
relative) in terms of BLEU points, it has the best
performance in terms of TER points (0.22 abso-
lute, 0.3% relative than word lattice). Furthermore,
the CN outperformed ?Para-Sub? by 0.36 absolute
(2.55% relative) BLEU points and 1.37 absolute
(1.84% relative) TER points, and also beat the base-
line PBSMT system by 0.45 absolute (3.21% rela-
tive) BLEU points and 1.82 absolute (2.43% rela-
tive) TER points. The paired 95% confidence in-
terval of significant test (Zhang and Vogel, 2004)
between the ?Lattice? and ?CN? system is [-0.19,
+0.38], which also suggests that the two system has
a comparable performance in terms of BLEU.
In Table 2, decoding time on test sets is re-
ported to compare the computational efficiency of
the baseline PBSMT, word-lattice-based and CN-
based methods. Note that word lattice construc-
tion time and CN building time (including word lat-
tice construction and conversion from word lattices
into CNs with the SRILM toolkit (Stolcke, 2002))
are counted in the decoding time and illustrated in
the table within parentheses respectively. Although
both word-lattice-based and CN-based methods re-
quire longer decoding times than the baseline PB-
SMT system, it is observed that compared with the
word lattices, CNs reduced the decoding time signif-
icantly on three tasks, namely 52.06% for the 20K
model, 75.75% for the 200K model and 78.88% for
the 2.1M model. It is also worth noting that the
?Para-Sub? system has a similar decoding time with
baseline PBSMT since only the translation table is
modified.
37
20K 200K 2.1M
System BLEU TER BLEU TER BLEU TER
Baseline PBSMT 14.42 75.30 23.60 63.65 14.04 74.88
Para-Sub 14.78 73.75 23.41 63.84 14.13 74.43
Word-lattice-based 15.44 73.06 25.20 62.37 14.55 73.28
CN-based 14.73 73.8 23.47 63.69 14.49 73.06
Table 1: Comparison on PBSMT, ?Para-Sub?, word-lattice and CN-based methods.
System
FBIS testset (1,200 inputs) NIST testset (1,859 inputs)
20K model 200K model 2.1M model
Baseline 21 min 41 min 37 min
Lattice 102 min (+ 15 sec) 398 min (+ 20 sec) 559 min (+ 21 sec)
CN 48 min (+ 61 sec) 95 min (+ 96 sec) 116 min (+ 129 sec)
Table 2: Decoding time comparison of PBSMT, word-lattice (?Lattice?) and CN-based (?CN?) methods.
4.4 Discussion
From the performance and decoding time reported in
the last section, it is obvious that on large scale cor-
pora, the CN-based method significantly reduced the
computational complexity while preserved the sys-
tem performance of the best lattice-based method.
Thus it makes the paraphrase-enriched SMT system
more applicable to real-world applications. On the
other hand, for small- and medium-scale data, CNs
can be used as a compromise between speed and
quality, since decoding time is much less than word
lattices, and compared with the ?Para-Sub? system,
the only overhead is the transforming of the input
sentences.
It is also interesting that the relative performance
of the CNs increases gradually with the size of the
training corpus, which indicates that it is more suit-
able for models built from large scale data. Consid-
ering the decoding time, it is preferable to use CNs
instead of word lattices for such translation tasks.
However, for the small- and medium-scale data, the
CN system is not competitive even compared with
the baseline. In this case it suggests that, on these
two tasks, the coverage issue is not solved by in-
corporating paraphrases with the CN structure. It
might because of the ambiguity that introduced by
CNs harms the decoder to choose the appropriate
source words from paraphrases. On the other hand,
this ambiguity could be decreased with translation
models trained on a large corpus, which provides
enough observations for the decoders to favour para-
phrases.
5 Conclusion and future work
In this paper, CNs are used instead of word lattices
to incorporate paraphrases into SMT. Transforma-
tion from word lattices into CNs is used to merge
path duplications, and decoding time is drastically
reduced with CN decoding. Experiments are carried
out on small-, medium- and large-scale English?
Chinese translation tasks and confirm that compared
with word lattices, it is much more computationally
efficient to use CNs, while no loss of performance is
observed on the large-scale task.
In the future, we plan to apply more features
such as source-side language models and phrase
length (Onishi et al, 2010) on the CNs to obtain bet-
ter system performance. Furthermore, we will carry
out this work on other language pairs to show the ef-
fectiveness of paraphrases in SMT systems. We will
also investigate the reason for its lower performance
on the small- and medium-scale corpora, as well as
the impact of the paraphrase filtering procedure on
translation quality.
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. Thanks to the reviewers
for their invaluable comments and suggestions.
38
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In 43rd An-
nual meeting of the Association for Computational
Linguistics, Ann Arbor, MI, pages 597?604.
Nicola Bertoldi, Richard Zens, Marcello Federico, and
Wade Shen 2008. Efficient Speech Translation
Through Confusion Network Decoding. In IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 16(8), pages 1696?1705.
Francis Bond, Eric Nichols, Darren Scott Appling and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), Hawaii, pages 150?
157.
Chris Callison-Burch, Philipp Koehn and Miles Osborne.
2006. Improved Statistical Machine Translation Using
Paraphrases. In Proceedings of the Human Language
Technology conference - North American chapter of
the Association for Computational Linguistics (HLT-
NAACL), NY, pages 17?24.
Jinhua Du, Jie Jiang and Andy Way. 2010. Facilitating
Translation Using Source Language Paraphrase Lat-
tices. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Cambridge, MA, pages 420?429.
Dilek Hakkani-Tu?r, Fre?de?ric Be?chet, Giuseppe Riccardi
and Gokhan Tur. 2005. Beyond ASR 1-best: Using
word confusion networks in spoken language under-
standing. In Computer Speech and Language (2005):
20(4), pages 495?514.
Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Wade Shen, C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and
Evan Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In ACL 2007: demo
and poster sessions, Prague, Czech Republic, pages
177?180.
Roland Kuhn, Boxing Chen, George Foster and Evan
Stratford. 2010. Phrase Clustering for Smoothing TM
Probabilities - or, How to Extract Paraphrases from
Phrase Tables. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), Beijing, China, pages 608?616.
Xiaoyi Ma. 2006. Champollion: A Robust Parallel Text
Sentence Aligner. LREC 2006: Fifth International
Conference on Language Resources and Evaluation,
Genova, Italy, pages 489?492.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik and Bon-
nie J. Dorr. 2007. Using Paraphrases for Parameter
Tuning in Statistical Machine Translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, Prague, Czech Republic, pages 120?127.
Lidia Mangu, Eric Brill and Andreas Stolcke. 2000.
Finding Consensus in Speech Recognition: Word Er-
ror Minimization and Other Applications of Confusion
Networks. In Computer Speech and Language 14 (4),
pages 373?400.
Yuval Marton, Chris Callison-Burch and Philip Resnik.
2009. Improved Statistical Machine Translation Us-
ing Monolingually-Derived Paraphrases. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), Singapore, pages
381?390.
Aure?lien Max. 2010. Example-Based Paraphrasing
for Improved Phrase-Based Statistical Machine Trans-
lation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
Cambridge, MA, pages 656?666.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases In Pro-
ceedings of the European Conference on Artificial In-
telligence (ECAI), Patras, Greece, pages 338?342.
Preslav Nakov. 2008b. Improving English-Spanish sta-
tistical machine translation: experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of ACL-08:HLT. Third Work-
shop on Statistical Machine Translation, Columbus,
Ohio, USA, pages 147?150.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA, pages 295?302.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1), pages 19?51.
Takashi Onishi, Masao Utiyama and Eiichiro, Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. In Proceedings of the ACL 2010 Confer-
ence Short Papers, Uppsala, Sweden, pages 1?5.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method For Automatic
Evaluation of Machine Translation. ACL-2002: 40th
Annual meeting of the Association for Computational
Linguistics, pp.311-318, Philadelphia, PA.
Josh Schroeder, Trevor Cohn and Philipp Koehn. 2009.
Word Lattices for Multi-Source Translation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), Athens, Greece,
pages 719?727.
39
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, Cam-
bridge, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J.Dorr and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, Athens,
Greece, pages 259?268.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP),
Denver, Colorado, pages 901?904.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI). pages 85?94.
40
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 48?58,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Combining EBMT, SMT, TM and IR Technologies for Quality and Scale
Sandipan Dandapat1, Sara Morrissey1, Andy Way2, Joseph van Genabith1
1 CNGL, School of Computing
Dublin City University, Glasnevin, Dublin 9, Ireland
{sdandapat,smorri,josef}@computing.dcu.ie
2 Applied Language Solutions, Delph, UK
andy.way@appliedlanguage.com
Abstract
In this paper we present a hybrid statisti-
cal machine translation (SMT)-example-based
MT (EBMT) system that shows significant
improvement over both SMT and EBMT base-
line systems. First we present a runtime
EBMT system using a subsentential transla-
tion memory (TM). The EBMT system is fur-
ther combined with an SMT system for effec-
tive hybridization of the pair of systems. The
hybrid system shows significant improvement
in translation quality (0.82 and 2.75 abso-
lute BLEU points) for two different language
pairs (English?Turkish (En?Tr) and English?
French (En?Fr)) over the baseline SMT sys-
tem. However, the EBMT approach suffers
from significant time complexity issues for a
runtime approach. We explore two methods to
make the system scalable at runtime. First, we
use an heuristic-based approach. Secondly, we
use an IR-based indexing technique to speed
up the time-consuming matching procedure of
the EBMT system. The index-based match-
ing procedure substantially improves run-time
speed without affecting translation quality.
1 Introduction
State-of-the-art phrase-based SMT (Koehn, 2010a)
is the most successful MT approach in many large
scale evaluations, such as WMT,1 IWSLT2 etc. At
the same time, work continues in the area of EBMT.
Some recent EBMT systems include Cunei (Phillips,
1http://www.statmt.org/wmt11/
2http://www.iwslt2011.org/
2011), CMU-EBMT (Brown, 2011) and OpenMa-
TrEx (Dandapat et al, 2010). The success of an
SMT system often depends on the amount of parallel
training corpora available for the particular language
pair. However, low translation accuracy has been
observed for language pairs with limited training re-
sources (Islam et al, 2010; Khalilov et al, 2010).
SMT systems effectively discard the actual training
data once the models (translation model and lan-
guage model) have been estimated. This can lead to
their inability to guarantee good quality translation
for sentences closely matching those in the train-
ing corpora. By contrast, EBMT systems usually
maintain a linked relationship between the full sen-
tence pairs in source and target texts. Because of this
EBMT systems can often capture long range depen-
dencies and rich morphology at runtime. In contrast
to SMT, however, most EBMT models lack a well-
formed probability model, which restricts the use of
statistical information in the translation process.
Keeping these in mind, our objective is to de-
velop a good quality MT system choosing the best
approach for each input in the form of a hybrid SMT-
EBMT approach. It is often the case that an EBMT
system produces a good translation where SMT sys-
tems fail and vice versa (Dandapat et al, 2011).
An EBMT system relies on past translations to
derive the target output for a given input. Run-
time EBMT approaches generally do not include
any training stage, which has the advantage of not
having to depend on time-consuming preprocessing.
On the other hand, their runtime complexity can be
considerable. This is due to the time-consuming
matching stage at runtime that finds the example
48
(or set of examples) which most closely matches
the source-language sentence to be translated. This
matching step often uses some variation of string
edit-distance measures (Levenshtein, 1965) which
has quadratic time complexity.3 This is quite time-
consuming even when a moderate amount of train-
ing examples are used for the matching procedure.
We adopt two alternative approaches to tackle the
above problem. First we use heuristics which are of-
ten useful to avoid some of the computations. For a
input sentence, in the matching process, we may not
need to compute the string edit distance with all sen-
tences in the example base. In order to prune some
of the computation, we rely on the fact that the in-
put sentence and its closest match sentence from the
example-base are likely to have a similar sentence
length. Search engine indexing is an effective way
of storing data for fast and accurate retrieval of in-
formation. During retrieval, a set of documents are
extracted based on their similarity to the input query.
In our second approach, we use this concept to effi-
ciently retrieve a potential set of suitable candidate
sentences from the example-base to find the closest
match. We index the entire example-base consider-
ing each source-side sentence as a document for the
indexer. We show that improvements can be made
with our approach in terms of time complexity with-
out affecting the translation quality.
The remainder of this paper is organized as fol-
lows. The next section presents work related to our
EBMT approach. Section 3 describes the MT sys-
tems used in our experiments. Section 4 focuses on
the two techniques used to make the system scalable.
Section 5 presents the experiments in detail. Section
6 presents and discusses the results and provides an
error analysis. We conclude in Section 7.
2 Related Work
The EBMT framework was first introduced by Na-
gao (1984) as the ?MT by analogy principle?. The
two main approaches to EBMT are distinguished
by the inclusion or exclusion of a preprocess-
ing/training stage. Approaches that incorporate a
3Ukkonen (1983) gave an algorithm for computing edit-
distance with the worst case complexity O(md), where m is
the length of the string and d is their edit distance. This is ef-
fective when m  d. We use word-based edit distance, so m
is shorter in length.
training stage are commonly called ?compiled ap-
proaches? (Cicekli and Gu?venir, 2001). Approaches
that do not include a training stage are often referred
to as ?pure? or ?runtime? EBMT approaches, e.g.
(Lepage and Denoual, 2005). These approaches
have the advantage that they do not depend on any
time-consuming preprocessing stages. On the other
hand, their runtime complexity can be considerable.
EBMT is often linked with the related concept of
translation memory (TM). A TM essentially stores
source- and target-language translation pairs for ef-
fective reuse of previous translations originally cre-
ated by human translators. TMs are often used to
store examples for EBMT systems. After retriev-
ing a set of examples with associated translations,
EBMT systems automatically extract translations of
suitable fragments and combine them to produce a
grammatical target output.
Phrase-based SMT systems (Koehn, 2010a), pro-
duce a source?target algned subsentential phrase
table which can be adapted as an additional TM
to be used in a CAT environment (Simard, 2003;
Bic?ici and Dymetman, 2008; Bourdaillet et al,
2009; Simard and Isabelle, 2009). Koehn and Senel-
lart (2010b) use SMT to produce the translation of
the non-matched fragments after obtaining the TM-
based match. EBMT phrases have also been used
to populate the knowledge database of an SMT sys-
tem (Groves et al, 2006). However, to the best of
our knowledge, the use of SMT phrase tables within
an EBMT system as an additional sub-sentential TM
has not been attempted so far. Some work has been
carried out to integrate MT in a CAT environment
to translate the whole segment using the MT sys-
tem when no sufficiently well matching translation
unit (TU) is found in the TM. The TransType sys-
tem (Langlais et al, 2002) integrates an SMT sys-
tem within a text editor to suggest possible continua-
tions of the translations being typed by the translator.
By contrast, our approach attempts to integrate the
subsentential TM obtained using SMT techniques
within an EBMT system.
3 MT Systems
The SMT system used in our hybrid SMT-
EBMT approach is the vanilla Moses4 decoder.
4http://www.statmt.org/moses/
49
Moses (Koehn et al, 2007) is a set of SMT tools
that include routines to automatically train a transla-
tion model for any language pair and an efficient de-
coder to find the most probable translation. Due to
lack of space and the wide usage of Moses, here we
focus more on the novel EBMT system we have de-
veloped for our hybrid SMT-EBMT approach. The
EBMT system described in this section is based on
previous work (Dandapat et al, 2010) and some of
the material has been reproduced here to make the
paper complete.
Like all other EBMT systems, our particular ap-
proach comprises three stages: matching, alignment
and recombination. Our EBMT system also uses a
subsentential TM in addition to the sentence aligned
example-base. Using the original TM as a train-
ing set, additional subsentential TUs (words and
phrases) are extracted from it based on word align-
ments and phrase pairs produced by Moses. These
subsentential TUs are used for alignment and recom-
bination stages of our EBMT system.
3.1 Building a Subsentential TM for EBMT
A TM for EBMT usually contains TUs linked at
the sentence, phrasal and word level. TUs can be
derived manually or automatically (e.g. using the
marker-hypothesis (Groves et al, 2006)). Usually,
TUs are linguistically motivated translation units.
In this paper however, we explore a different route,
as manual construction of high-quality TMs is time
consuming and expensive. Furthermore, only con-
sidering linguistically motivated TUs may limit the
matching potential of a TM. Because of this, we
used SMT technology to automatically create the
subsentential part of our TM at the phrase (i.e.
no longer necessarily linguistically motivated) and
word level. Based on Moses word alignment (using
GIZA++ (Och and Ney, 2003)) and phrase table con-
struction, we construct the additional TM for further
use within an EBMT approach.
Firstly, we add entries to the TM based on the
aligned phrase pairs from the Moses phrase table us-
ing the following two scores:
1. Direct phrase translation probabilities: ?(t|s)
2. Direct lexical weight: lex(t|s)
Table 1 shows an example of phrase pairs with the
associated probabilities learned by Moses. We keep
all target equivalents in a sorted order based on the
Table 1: Moses phrase equivalence probabilities.
English (s) Turkish (t) p(t|s) lex(t|s)
a hotel bir otel 0.826087 0.12843
a hotel bir otelde 0.086957 0.07313
a hotel otel mi 0.043478 0.00662
a hotel otel 0.043478 0.22360
above probabilities. This helps us in the matching
procedure, but during recombination we only con-
sider the most probable target equivalent. The fol-
lowing shows the resulting TUs in the TM for the
English source phrase a hotel.
a hotel? {bir otel, bir otelde, otel, otem mi}
Secondly, we add entries to the TM based on the
source-to-target word-aligned file. We also keep
the multiple target equivalents for a source word in
a sorted order. This essentially adds source- and
target-language equivalent word pairs into the TM.
Note that the entries in the TM may contain in-
correct source-target equivalents due to unreliable
word/phrase alignments produced by Moses.
3.2 EBMT Engine
The overview of the three stages of the EBMT en-
gine is given below:
Matching: In this stage, we find a sentence pair
?sc, tc? from the example-base that closely matches
with the input sentence s. We used a fuzzy-match
score (FMS) based on a word-level edit distance
metric (Wagner and Fischer, 1974) to find the closest
matching source-side sentence from the example-
base ({si}N1 ) based on Equation (i).
score(s, si) = 1? ED(s, si)/max(|s|, |si|) (i)
where |x| denotes the length (in words) of a sen-
tence, and ED(x, y) refers to the word-level edit dis-
tance between x and y. The EBMT system considers
the associated translation tc of the closest matching
source sentence sc, to build a skeleton for the trans-
lation of the input sentence s.
Alignment: After retrieving the closest fuzzy-
matched sentence pair ?sc, tc?, we identify the non-
matching fragments from the skeleton translation tc
in two steps.
50
Firstly, we find the matched and non-matched
segments between s and sc using edit distance
trace. Given the two sentences (s and sc), the al-
gorithm finds the minimum possible number of op-
erations (substitutions, additions and deletions) re-
quired to change the closest match sc into the in-
put sentence s. For example, consider the input
sentence s = w1w2w3w4w5w6w7w8 and sc =
w?1w
?
3w4w5w7w8w
?
9. Figure 1 shows the matched
and non-matched sequence between s and sc using
edit-distance trace.
s = w1 w2 w3 w4 w5 w6 w7 w8 ?
| | | | |
sc = w1 ? w?3 w4 w5 ? w7 w8 w
?
9
?
s = w1 w2 w3 w4 w5 w6 w7 w8 null
| ? | ? | ?
sc = w1 w
?
3 w4 w5 null w7 w8 w
?
9
Figure 1: Extraction of matched (underlined) and non-
matched (boxed) segments between s and sc.
Secondly, we align each non-matched segment in
sc with its associated translation using the TM and
the GIZA++ alignment. Based on the source-target
aligned pair in the TM, we mark the mismatched
segment in tc. We find the longest possible seg-
ment from the non-matched segment in sc that has a
matching target equivalent in tc based on the source-
target equivalents in the TM. We continue the pro-
cess recursively until no further segments of the non-
matched segment in sc can be matched with tc us-
ing the TM. Remaining non-matching segments in
sc are then aligned with segments in tc using the
GIZA++ word alignment information.
Recombination: In the recombination stage, we
add or substitute segments from the input sentence s
with the skeleton translation equivalent tc. We also
delete some segments from tc that have no corre-
spondence in s. After obtaining the source segments
(needs to be added or substituted in tc) from the in-
put s, we use our subsentential TM to translate these
segments. Details of the recombination process are
given in Algorithm 1.
3.3 An Illustrative Example
As a running example, for the input sentence in (1a)
the corresponding closest fuzzy-matched sentence
Algorithm 1 recombination(X,TM)
In: source segment X ,
subsentential translation memory TM
Out: translation of source segment X
1: mark all words of X as untranslated
(untranslatedPortions(X)? {X})
2: repeat
3: U = untranslatedPortions(X)
4: x = longest subsegment in untranslatedPortions(X)
such that (x, tx) ? TM;
5: substitute(X,x ? tx) {substitute x with its target
equivalent tx in X}
6: remove x from untranslatedPortions(X)
7: until (untranslatedPortions(X) = U )
8: return X
pair ?sc, tc? is shown in (1b) and (1c). The portion
marked with angled brackets in (1c) are aligned with
the mismatched portion in (1b). The character and
the following number in angled brackets indicate the
edit operation (?s? indicates substitution) and the in-
dex of the mismatched segment from the alignment
process respectively.
1. (a) s: i ?d like a <s#0:present> for <s#1:my
mother> .
(b) sc: i ?d like a <s#0:shampoo> for
<s#1:greasy hair> .
(c) tc: <s#1:yag?l? sac?lar> ic?in bir
<s#0:s?ampuan> istiyorum .
During recombination, we need to replace two
segments in (1c) {yag?l? sac?lar (greasy hair) and
s?ampuan (shampoo)} with the two corresponding
source segments in (1a) {my mother and present}
as an intermediate stage (2) along the way towards
producing a target equivalent.
(2)<1:my mother> ic?in bir <0:present> istiyorum .
Furthermore, replacing the untranslated segments
in (2) with the translations obtained using TM, we
derive the output translation in (3) of the original in-
put sentence in (1).
(3) <annem> ic?in bir <hediye> istiyorum .
4 Scalability
The main motivation of scalability is to improve
the speed of the EBMT system when using a large
example-base. The matching procedure in an EBMT
system finds the example (or a set of examples)
which closely matches the source-language string to
51
be translated. All matching processes necessarily in-
volve a distance or similarity measure. The most
widely used distance measure in EBMT matching
is Levenshtein distance (Levenshtein, 1965; Wagner
and Fischer, 1974) which has quadratic time com-
plexity. In our EBMT system, we find the clos-
est sentence at runtime from the whole example-
base for a given input sentence using the edit dis-
tance matching score. Thus, the matching step of
the EBMT system is a time-consuming process with
a runtime complexity of O(nm2), where n denotes
the size of the example-base and m denotes the av-
erage length (in words) of a sentence. Due to a
significant runtime complexity, the EBMT system
can only handle a moderate size example-base in the
matching stage. However, it is important to handle a
large example-base to improve the quality of an MT
system. In order to make the system scalable with
a larger example-base, we adopt two approaches for
finding the closest matching sentences efficiently.
4.1 Grouping
Our first attempt is heuristic-based. We divide the
example-base into bins based on sentence length. It
is anticipated that the sentence from the example-
base that most closely matches an input sentence
will fall into the group which has comparable length
to the length of the input sentence. First, we divide
the example-baseE into different bins based on their
word-level length E =
?l
i=1Ei and Ei
?
Ej = ?
for all i 6= j where 0 ? i, j ? l. Ei denotes the
set of sentences with length i and l is the maximum
length of a sentence in E. In order to find the clos-
est match for a test sentence (s of length k), we only
consider examples EG =
?x
m=0Ek?m, where x in-
dicates the window size. In our experiment, we con-
sider the value of x from 0 to 2. We find the closest-
match sc from EG for a given test sentence s. EG
has fewer sentences compared to E which will ef-
fectively reduce the time of the matching procedure.
4.2 Indexing
Our second approach to addressing time complexity
is to use indexing. We index the complete example-
base using an open-source IR engine SMART5 and
retrieve a potential set of candidate sentences (likely
5An open source IR system from Cornell University. ftp:
//ftp.cs.cornell.edu/pub/smart/
to contain the closest match sentence) from the
example-base. Unigrams extracted from the sen-
tences of the example-base are indexed using the
language model (LM) and complete sentences are
considered as retrievable units. In LM-based re-
trieval we assume that a given query is generated
from a unigram document language model. The ap-
plication of the LM retrieval model in our case re-
turns a sorted list of sentences from the example-
base ordered by the estimated probabilities of gen-
erating the given input sentence.
In order to improve the run-time performance,
we integrate the SMART retrieval engine within the
matching procedure of our EBMT system. The re-
trieval engine estimates a potential set of candidate
close-matching sentences from the example-base E
for a test sentence s. We assume that the closest
source-side match sc of the input sentence s can
take the value from the set EIR(s), where EIR(s) is
the potential set of close-matching sentences com-
puted by the LM-based retrieval engine. We have
used the top 50 candidate sentences from EIR(s).
Since the IR engine tries to retrieve the document
(sentences from E) for a given query (input) sen-
tence, it is likely to retrieve the closest match sen-
tence sc in the set EIR(s). Due to a much re-
duced set of possibilities, this approach improves the
run-time performance of the EBMT system without
hampering system accuracy. Finding this potential
set of candidate sentences will be much faster than
traditional edit-distance-based retrieval on the full
example-base as the worst case run time of the re-
triever is O(
?
?wi
si), where wi is a word in the in-
put sentence and si is the number of sentences in the
example-base that contain wi. Finding a set of can-
didate sentences took only 0.3 seconds and 116 sec-
onds, respectively, for 414 and 10,000 example in-
put sentences given 20k and 250k sentence example-
base in our En?Tr and En?Fr experiment on a 3GHz
Core 2 Duo machine with 4GB RAM.
5 Experiments
We conduct different experiments to report the ac-
curacy of our EBMT systems for En?Tr and En?Fr
translation tasks. In order to compare the perfor-
mance of our approaches we use two baseline sys-
tems. We use the Moses SMT system as one base-
52
line. Furthermore, based on the matching step (Sec-
tion 3.2) of the EBMT approach, we obtain the clos-
est target-side equivalent (the skeleton sentence) and
consider this as the baseline output for the input to
be translated. This is referred to as TM in the exper-
iment below. We will consider this as the baseline
accuracy for our EBMT using TM approach.
In addition, we conduct two experiments with our
EBMT system. After obtaining the skeleton trans-
lation through the matching and alignment steps, in
the recombination step, we use TM to translate any
unmatched segments based on Algorithm 1. We call
this EBMTTM.
We found that there are cases where the
EBMTTM system produces the correct translation
but SMT fails and vice-versa (Dandapat et al, 2011).
In order to further improve translation quality, we
use a combination of EBMT and SMT. Here we use
some features to decide whether to rely on the out-
put produced by the EBMTTM system. These fea-
tures include fuzzy match scoreFMS (as in (i)) and
the number of mismatched segments in each of s,
sc, tc (EqUS6 as in (1)). We assume that the transla-
tions of an input sentence s produced by EBMTTM
and SMT systems are respectively TEBMT(s) and
TSMT(s). If the value of FMS is greater than some
threshold and EqUS exists between s and sc, we
rely on the output TEBMT(s); otherwise we take the
output from TSMT(s). We refer to this system as
EBMTTM + SMT.
To test the scalability of the system, we con-
ducted two more experiments based on the ap-
proach described in Section 4. First, we con-
ducted an experiment based on the sentence length-
based grouping heuristics (Section 4.1). We re-
fer to this system asEBMTTM + SMT+ groupi,
where i indicates the window size while compar-
ing the length of the input sentence with the bins.
We conduct a second experiment based on the LM-
based indexing technique (Section 4.2) we have used
to retrieve a potential set of candidate sentences
from the indexed example-base. We call this sys-
tem EBMTTM + SMT+ index. Note that the
EBMTTM + SMT system is used as the baseline
accuracy while conducting the experiments for scal-
6If s, sc and tc agree in the number of mismatched segments,
EqUS evaluates to 1, otherwise 0.
ability of the EBMT system.
5.1 Data Used for Experiments
We used two data sets for all our experiments rep-
resenting two language pairs of different size and
type. In the first data-set, we have used the En?Tr
corpus from IWSLT09.7 The training data consists
of 19,972 parallel sentences. We used the IWSLT09
development set as our testset which consists of 414
sentences. The IWSLT09 data set is comprised of
short sentences (with an average of 9.5 words per
sentence) from a particular domain (the C-STAR
project?s Basic Travel Expression Corpus).
Our second data set consists of an En?Fr
corpus from the European Medicines Agency
(EMEA)8 (Tiedemann and Nygaard, 2009). The
training data consists of 250,806 unique parallel sen-
tences.9 As a testset we use a set of 10,000 ran-
domly drawn sentences disjoint from the training
corpus. This data also represents a particular domain
(medicine) but with longer sentence lengths (with an
average of 18.8 words per sentence) compared to the
IWSLT09 data.
6 Results and Observations
We used BLEU (Papineni et al, 2002) for automatic
evaluation of our EBMT systems. Table 2 shows
the accuracy obtained for both En?Tr and En?Fr by
the EBMTTM system described in Section 3. Here
we have two baseline systems (SMT and TM) as de-
scribed in the first two experiments in Section 5.
Table 2: Baseline BLEU scores of the two systems
and the scores for EBMTTM system.
System Language pairs
En?Tr En?Fr
SMT 23.59 55.04
TM 15.60 40.23
EBMTTM 20.08 48.31
Table 2 shows that EBMTTM has a lower system
accuracy than SMT for both the language pairs, but
7http://mastarpj.nict.go.jp/IWSLT2009/2009/12/downloads.html
8http://opus.lingfil.uu.se/EMEA.php
9A large number of duplicate sentences exists in the original
corpus (approximately 1M sentences). We remove duplicates
and consider sentences with unique translation equivalents.
53
better scores than TM alone. Tables 3 and 4 show
that combining EBMT with SMT systems shows im-
provements of 0.82 and 2.75 BLEU absolute over
the SMT baseline (Table 2) for both the En?Tr and
the En?Fr data sets. In each case, the improvement
of EBMTTM + SMT over the baseline SMT is sta-
tistically significant (reliability of 98%) using boot-
strap resampling (Koehn, 2004).
Table 3: En?Tr MT system accuracies of the com-
bined systems (EBMTTM + SMT) with different
combining factors. The second column indicates the
number (and percentage) of sentences translated by
the EBMTTM system during combination.
System: EBMTTM + SMT
Condition times
EBMTTM
used
BLEU
(in %)
FMS>0.85 35 (8.5%) 24.22
FMS>0.80 114 (27.5%) 23.99
FMS>0.70 197 (47.6%) 22.74
FMS>0.80 OR
(FMS>0.70 & EqUS)
165 (40.0%) 23.87
FMS>0.85 & EqUS 24 (5.8%) 24.41
FMS>0.80 & EqUS 76 (18.4%) 24.19
FMS>0.70 & EqUS 127 (30.7%) 24.08
Table 4: En?Fr MT system accuracies for the com-
bined systems (EBMTTM + SMT) with different
combining factors.
System: EBMTTM + SMT
Condition times
EBMTTM
used
BLEU
(in %)
FMS>0.85 3323 (33.2%) 57.79
FMS>0.80 4300 (43.0%) 57.55
FMS>0.70 5283 (52.8%) 57.05
FMS>0.60 6148 (61.5%) 56.25
FMS>0.80 OR
(FMS>0.70 & EqUS)
4707 (47.1%) 57.46
FMS>0.85 & EqUS 2358 (23.6%) 57.24
FMS>0.80 & EqUS 2953 (29.5%) 57.16
FMS>0.70 & EqUS 3360 (33.6%) 57.08
A particular objective of our work is to scale the
runtime EBMT system to a larger amount of train-
ing examples. We experiment with the two ap-
proaches described in Section 4 to improve the run
time of the system. Table 5 compares the run time of
the three systems (EBMTTM, EBMTTM + groupi
and EBMTTM + index) for both En?Tr and En?Fr
translation. Note that the SMT decoder takes 140
seconds and 310 minutes respectively for En?Tr and
En?Fr translation test sets.
Table 5: Running time of the three different systems.
System Language pairs
En?Tr En?Fr
(seconds) (minutes)
SMT 140.0 310.0
EBMTTM 295.9 2267.0
EBMTTM + group0 34.0 63.4
EBMTTM + group1 96.2 183.5
EBMTTM + group2 148.5 301.4
EBMTTM + index 2.7 2.6
Both the grouping and indexing methodologies
proved successful for system scalability with a max-
imum speedup of almost 2 orders of magnitude. We
also need to estimate the accuracy while combining
grouping and indexing techniques with the baseline
system (EBMTTM + SMT) to understand their rel-
ative performance. Table 6 provides the system ac-
curacy using the grouping and indexing techniques
for both the language pairs. We report the transla-
tion quality under three conditions. Similar trends
have been observed for other conditions.
6.1 Observations and Discussions
We find that the EBMTTM system has a lower ac-
curacy on its own compared to baseline SMT for
both the language pairs (Table 2). Nevertheless,
there are sentences which are better translated by the
EBMTTM approach compared to SMT, although
the overall document translation score is higher with
SMT. Thus, we combined the two systems based on
different features and found that the combined sys-
tem performs better. The highest relative improve-
ments in BLEU score are 3.47% and 1.05% respec-
tively for En?Tr and En?Fr translation. We found
that if an input has a high fuzzy match score (FMS)
with the example-base, then the EBMTTM system
does better compared to SMT. With our current ex-
perimental setup, we found that an FMS over 0.8
showed an improvement for En?Tr and a FMS over
0.6 showed improvement for En?Fr over the SMT
system. Figure 2 shows the effect in the translation
54
Table 6: BLEU scores of the three different systems for En?Tr and En?Fr under different conditions. i
denotes the number of bins considered during grouping.
Condition System
EBMTTM + SMT EBMTTM + SMT EBMTTM + SMT
+groupi +index
i=0 i=?1 i=?2
En?Tr
FMS>0.85 24.22 24.18 24.18 24.23 24.24
FMS>0.80 OR (FMS>0.70 & EqUS) 23.87 23.34 23.90 24.40 24.37
FMS>0.85 & EqUS 24.41 24.17 24.38 24.34 24.39
En?Fr
FMS>0.85 57.79 56.47 57.48 57.76 57.92
FMS>0.80 OR (FMS>0.70 & EqUS) 57.46 55.69 57.07 57.33 57.56
FMS>0.85 & EqUS 57.24 56.48 57.23 57.29 57.32
quality when different FMS thresholds were used to
combine the two systems.
However, FMS might not be the only factor for
triggering the EBMTTM system. We considered
EqUs as another factor which showed improvement
for En?Tr but showed negative effect for En?Fr.
Though an FMS over 0.7 for En?Tr shows no im-
provement in overall system accuracy, inclusion of
the EqUs feature along with FMS shows improve-
ment. Thus, the EBMTTM system is sometimes
more effective when the number of unmatched seg-
ment matches in s, sc and tc.
These observations show the effective use of our
EBMT approach in terms of translation quality.
However, we found that the EBMTTM system has
a very considerable runtime complexity. In order to
translate 414 test sentences from English into Turk-
ish, the basic EBMT system takes 295.9 seconds.
The situation becomes worse when using the large
example-base for En?Fr translation. Here, we found
that the system takes around 38 hours to translate
10k source English sentences into French. This is
a significant time complexity by any standard for a
runtime approach. However, both grouping and in-
dexing reduce the time complexity of the approach
considerably. The time reduction with grouping de-
pends on the number of bins considered to find the
closest sentence during the matching stage. Systems
with a lower number of bins take less time but cause
more of a drop in translation quality. The effect is
more prominent with the En?Fr system which uses
a larger example-base. We found a drop of abso-
lute 1.32 BLEU points while considering a single
bucket whose length is equal to the length of the
test sentence. This configuration takes 63 minutes to
translate 10k English sentences into French. There
is only a drop of 0.03 BLEU points when consider-
ing the 5 nearest bins (?2) for a given test sentence.
Nevertheless, there is not much of a reduction but it
increases the run time to 5 hours for the translation
of 10k sentences. Thus, the group-based method is
not effective enough to balance system accuracy and
run time.
Incorporation of the indexing technique into the
matching stage of EBMT shows the highest effi-
ciency gains in run time. Translating 10k sen-
tences from English into French takes only 158 sec-
onds. It is also interesting to note that with index-
ing, the BLEU score remained the same or even in-
creased. This is due to the fact that, compared to
FMS-based matching, a different closest-matching
sentence sc is selected for some of the input sen-
tences while using indexing, thus resulting in a dif-
ferent outcome to the system. Figure 3 compares
the number of times the EBMTTM + SMT + index
system is used in the hybrid system and the num-
ber of same closest-matching sentences selected by
EBMTTM + SMT + index systems under different
conditions for En?Tr. The use of index-based candi-
date selection for EBMT matching shows effective
55
 48.31
 55.04
 57.79
 0.2  0.4  0.6  0.8 0.9 1
BLE
U (%
)
Fuzzy Match Score
EBMTTM+SMTSMTEBMTTM
(a) En?Fr
 20.08
 23.59 24.22
 0.3  0.4  0.6  0.8 0.9  1
BLE
U (%
)
Fuzzy Match Score
EBMTTM+SMTSMTEBMTTM
(b) En?Tr
Figure 2: Effect of FMS in the combined EBMTTM + SMT system.
Table 7: The effect of indexing in selection sc and
in final translation.
Input: zeffix belongs to a group of medicines called
antivirals.
Ref : zeffix appartient a` une classe de
me?dicaments appele?s antiviraux.
baseline EBMTTM system
sc: simulect belongs to a group of medicines
called immunosuppressants.
st: simulect fait parti d ? une classe de
me?dicaments appele?s immunosuppresseurs.
Output: zeffix fait parti d ? une classe de
me?dicaments appele?s antiviraux.
EBMTTM + SMT + index system
sc: diacomit belongs to a group of medicines
called antiepileptics.
st: diacomit appartient a` un groupe de
me?dicaments appele?s antie?pileptiques.
Output: zeffix appartient a` un groupe de
me?dicaments appele?s antiviraux.
improvement in translation time, and BLEU scores
remained the same or increased. Due to the selec-
tion of different closest-matching sentence sc, some-
times the system produces better quality translation
which increases the system level BLEU score. Ta-
ble 7 shows one such En?Fr example where an
index-based technique produced a better translation
than the baseline (EBMTTM + SMT) system.
7 Conclusion
Our experiments show that EBMT approaches work
better compared to the SMT-based system for cer-
tain sentences when a high fuzzy match score is
Figure 3: Number of times EBMTTM + SMT + index
used in the hybrid system and the number of times
the same closest-matching sentences are selected by the
systems. a=FMS>0.85, b=FMS>0.85 & EqUS and
c=FMS>0.80 OR (FMS>0.70 & EqUS)
obtained for the input sentence with the example-
base. Thus a feature-based combination of EBMT-
and SMT-based systems produces better translation
quality than either of the individual systems. Inte-
gration of a SMT technology-based sub-sentential
TM with the EBMT framework (EBMTTM) has im-
proved translation quality in our experiments.
Our baseline EBMTTM system is a runtime ap-
proach which has high time complexity when us-
ing a large example-base. We found that the inte-
gration of IR-based indexing substantially improves
run time without affecting BLEU score. So far our
systems have been tested using moderately sized
example-bases from a closed domain corpus. In our
future work, we plan to use a much larger example-
base and wider-domain corpora.
56
Acknowledgments
This research is supported by Science Foundation
Ireland (Grants 07/CE/I1142, Centre for Next Gen-
eration Localisation).
References
S. Armstrong, C. Caffrey, M. Flanagan, D. Kenny, M.
O?Hagan and A. Way. 2006. Improving the Quality
of Automated DVD Subtitles via Example-Based Ma-
chine Translation. Translating and the Computer 28,
[no page number], London: Aslib, UK.
E. Bic?ici and M. Dymetman. 2008. Dynamic Translation
Memory: Using Statistical Machine Translation to Im-
prove Translation Memory. In Gelbukh, Alexander F.,
editor, In Proceedings of the 9th International Confer-
ence on Intelligent Text Processing and Computational
Linguistics (CICLing), volume 4919 of Lecture Notes
in Computer Science, pp 3-57 Springer Verlag.
J. Bourdaillet, S. Huet, F. Gotti, G. Lapalme and P.
Langlais. 2009. Enhancing the bilingual concor-
dancer TransSearch with word-level alignment. In
Proceedings, volume 5549 of Lecture Notes in Artifi-
cial Intelligence: 22nd Canadian Conference on Ar-
tificial of Intelligence (Canadian AI 2009), Springer-
Verlag, pp. 27-38.
R. D. Brown. 2011. The CMU-EBMT machine transla-
tion system. Machine Translation, 25(2):179?195.
I. Cicekli and H. A. Gu?venir. 2001. Learning trans-
lation templates from bilingual translation examples.
Applied Intelligence, 15(1):57?76.
S. Dandapat, S. Morrissey, A. Way and M.L. Forcada.
2011. Using Example-Based MT to Support Sta-
tistical MT when Translating Homogeneous Data in
Resource-Poor Settings. In Proceedings of the 15th
Annual Meeting of the European Association for Ma-
chine Translation (EAMT 2011), pp. 201-208. Leu-
ven, Belgium.
S. Dandapat, M.L. Forcada, D. Groves, S. Penkale,
J. Tinsley and A. Way. 2010. OpenMaTrEx:
a free/open-source marker-driven example-based ma-
chine translation system. In Proceedings of the 7th In-
ternational Conference on Natural Language Process-
ing (IceTAL 2010), pp. 121-126. Reykjav??k, Iceland.
D. Groves and A. Way. 2006. Hybridity in MT: Exper-
iments on the Europarl Corpus. In Proceedings of the
11th Conference of the European Association for Ma-
chine Translation (EAMT 2006), pp. 115-124. Oslo,
Norway.
M. Islam, J. Tiedemann and A. Eisele. 2010. English?
Bengali Phrase-based Machine Translation. In Pro-
ceedings of the 14th Annual Conference of the Eu-
ropean Association of Machine Translation, (EAMT
2010), [no page number], Saint-Raphae?l, France.
M. Khalilov, J.A.R. Fonollosa, I. Skadina, E. Bralitis
and L. Pretkalnina. 2010. English?Latvian SMT: the
Challenge of Translating into a Free Word Order Lan-
guage. In Proceedings of the 2nd International Work-
shop on Spoken Language Technologies for Under-
resourced Languages (SLTU 2010), [no page num-
ber], Saint-Raphae?l, France.
P. Koehn. 2010. Statistical Machine Translation, Cam-
bridge University Press, Cambridge, UK.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin and E.
Herbst. 2007. Moses: open source toolkit for statisti-
cal machine translation. In Proceedings of the Demon-
stration and Poster Sessions at the 45th Annual Meet-
ing of the Association of Computational Linguistics
(ACL 2007), pp. 177-180. Prague, Czech Republic.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2004), pp. 388-395. Barcelona,
Spain.
P. Koehn and J. Senellart. 2004. Convergence of Trans-
lation Memory and Statistical Machine Translation. In
Proceedings of the AMTA workshop on MT Research
and the Translation Industry, pp. 21-23. Denever, CO.
P. Langlais, G. Lapalme and M. Loranger. 2002.
Development-evaluation cycles to boost translator?s
productivity. Machine Translation, 15(4):77?98.
Y. Lepage and E. Denoual. 2005. Purest ever example-
based machine translation: Detailed presentation and
assessment. Machine Translation, 19(3-4):251?282.
V. I. Levenshtein. 1965. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Dok-
lady Akademii Nauk SSSR, 163(4):845-848., English
translation in Soviet Physics Doklady,10(8), 707-710.
C. D. Manning, P. Raghavan and H. Schu?tze. 2008. In-
troduction to Information Retrieval. Cambridge Uni-
versity Press, Cambridge, UK.
M. Nagao. 1984. A Framework of a Machine Translation
between Japanese and English by Analogy Principle.
In Elithorn, A. and Banerji, R., editors, Artificial Hu-
man Intelligence, pp. 173?180, North-Holland, Ams-
terdam.
F. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, (ACL 2002), pp. 311?318, Philadelphia, PA.
57
A. B. Phillips. 2011. Cunei: open-source machine trans-
lation with relevance-based models of each translation
instance. Machine Translation, 25(2):161?177.
M. Simard and P. Isabelle. 2009. Phrase-based Machine
Translation in a Computer-assisted Translation Mem-
ory. In Proceedings of the 12th Machine Translation
Summit, (MT Summit XII), pp. 120?127, Ottawa,
Canada.
M. Simard. 2003. Translation spotting for translation
memories. In Proceedings of the HLT-NAACL 2003,
Workshop on Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pp. 65?72,
Edmonton, Canada.
H. Somers. 2003. An Overview of EBMT. In M. Carl
and A. Way , editors, Recent Advances in Example-
based Machine Translation, pp. 3-57, Kluwer Aca-
demic Publishers, Dordrecht, The Netherlands.
J. Tiedemann and L. Nygaard. 2009. News from OPUS
- A Collection of Multilingual Parallel Corpora with
Tools and Interfaces, in N. Nicolov, K. Bontcheva,
G. Angelova, R. Mitkov. (eds.), Recent Advances in
Natural Language Processing, V:237?248, John Ben-
jamins, Amsterdam, The Netherlands.
E. Ukkonen. 1983. On Approximate String Matching. In
Proceedings of International Conference on Founda-
tions of Computing Theory, (FCT 1983), pp. 487?496,
Borgholm, Sweden.
R. Wagner and M. Fischer. 1974. The String-to-String
Correction Problem. Journal of the Association for
Computing Machinery, 21:168?173.
58
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 59?65,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Parallel FDA5 for Fast Deployment of Accurate
Statistical Machine Translation Systems
Ergun Bic?ici
Centre for Next Generation Localisation
School of Computing
Dublin City University
ergun.bicici@computing.dcu.ie
Qun Liu
Centre for Next Generation Localisation
School of Computing
Dublin City University
qliu@computing.dcu.ie
Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
away@computing.dcu.ie
Abstract
We use parallel FDA5, an efficiently pa-
rameterized and optimized parallel im-
plementation of feature decay algorithms
for fast deployment of accurate statistical
machine translation systems, taking only
about half a day for each translation di-
rection. We build Parallel FDA5 Moses
SMT systems for all language pairs in
the WMT14 translation task and obtain
SMT performance close to the top Moses
systems with an average of 3.49 BLEU
points difference using significantly less
resources for training and development.
1 Introduction
Parallel FDA5 is developed for fast deployment
of accurate statistical machine translation systems
using an efficiently parameterized and optimized
parallel implementation of feature decay algo-
rithms (Bic?ici and Yuret, 2014). Parallel FDA5
takes about half a day for each translation direc-
tion. We achieve SMT performance that is on par
with the top constrained Moses SMT systems.
Statistical machine translation (SMT) is a data
intensive problem. If you have the translations for
the source sentences you are translating in your
training set or even portions of it, then the trans-
lation task becomes easier. If some tokens are not
found in the training data then you cannot trans-
late them and if some translated word do not ap-
pear in your language model (LM) corpus, then it
becomes harder for the SMT engine to find its cor-
rect position in the translation. The importance of
parallel FDA5 increases with the proliferation of
training material available for building SMT sys-
tems. Table 2 presents the statistics of the avail-
able training and LM corpora for the constrained
(C) systems as well as the statistics of the Parallel
FDA5 selected training and LM corpora.
Parallel FDA5 runs separate FDA5 models on
randomized subsets of the training data and com-
bines the selections afterwards. We run parallel
FDA5 SMT experiments using Moses (Koehn et
al., 2007) in all language pairs in WMT14 (Bojar
et al., 2014) and obtain SMT performance close to
the top constrained Moses systems training using
all of the training material. Parallel FDA5 allows
rapid prototyping of SMT systems for a given tar-
get domain or task and can be very useful for MT
in target domains with limited resources or in dis-
aster and crisis situations (Lewis et al., 2011).
2 Parallel FDA5 for Instance Selection
2.1 FDA5
FDA is developed mainly for building high per-
formance SMT systems using fewer yet relevant
data that is selected for increasing the coverage of
the test set features while maximizing their diver-
sity (Bic?ici and Yuret, 2011; Bic?ici, 2011). Par-
allel FDA parallelize instance selection and sig-
nificantly reduces the time to deploy accurate MT
systems in the presence of large training data from
weeks to half a day and still achieve state-of-
the-art SMT performance (Bic?ici, 2013). FDA5
is developed for efficient parameterization, opti-
mization, and implementation of FDA (Bic?ici and
Yuret, 2014). FDA5 can be used in both trans-
ductive learning scenarios where test set is used to
select the training data or in active learning sce-
narios where training set itself is used to obtain a
sorting of the training data and select.
We run transductive learning experiments in
this work such that the instance selection is per-
formed for the given test set. According to
SMT experiments performed on the 2 million sen-
tence English-German section of the Europarl cor-
pus (Bic?ici and Yuret, 2014), FDA5 can increase
the performance by 0.41 BLEU points compared
to using all of the available training data and by
59
Algorithm 1: Parallel FDA5
Input: Parallel training sentences U , test set
features F , and desired number of
training instances N .
Output: Subset of the parallel sentences to be
used as the training data L ? U .
1 U ? shuffle(U)
2 U ,M ? split(U , N)
3 L? {}
4 foreach U
i
? U do
5 ?L
i
, s
i
? ? FDA5(U
i
,F ,M)
6 L? L ? ?L
i
, s
i
?
7 L ? merge(L)
3.22 BLEU points compared to random selection.
FDA5 is also used for selecting the training set
in the WMT14 medical translation task (Calixto
et al., 2014) and the tuning set in the WMT14
German-English translation task (Li et al., 2014).
FDA5 has 5 parameters that effect the instance
scores based on the three formulas used:
? Initialization:
init(f) = log(|U|/C
U
(f))
i
|f |
l
(1)
? Decay:
decay(f) = init(f)(1+C
L
(f))
?c
d
C
L
(f)
(2)
? Sentence score:
sentScore(S) =
1
|S|
s
?
f?F (S)
fvalue(f)
(3)
C
L
(f) returns the count of feature f in L. d
is the feature score polynomial decay factor, c is
the feature score exponential decay factor, s is
the sentence score length exponent, i is the initial
feature score idf exponent, and l is the initial
feature score n-gram length exponent. FDA5 is
available at http://github.com/bicici/FDA
and the FDA5 optimizer is available at
http://github.com/bicici/FDAOptimization.
2.2 Parallel FDA5
Parallel FDA5 (ParFDA5) is presented in Algo-
rithm 1, which first shuffles the training sentences,
U and runs individual FDA5 models on the multi-
ple splits from which equal number of sentences,
M , are selected. We use ParFDA5 for select-
ing parallel training data and LM data for build-
ing SMT systems. merge combines k sorted ar-
rays, L
i
, into one sorted array in O(Mk log k) us-
ing their scores, s
i
, where Mk is the total number
of elements in all of the input arrays.
1
ParFDA5
makes FDA5 more scalable to domains with large
training corpora and allows rapid deployment of
SMT systems. By selecting from random splits of
the original corpus, we work with different n-gram
feature distributions in each split and prevent fea-
ture values from becoming negligible, which can
enhance the diversity.
2.3 Language Model Data Selection
We select the LM training data with ParFDA5
based on the following observation (Bic?ici, 2013):
No word not appearing in the training
set can appear in the translation.
It is impossible for an SMT system to translate a
word unseen in the training corpus nor can it trans-
late it with a word not found in the target side of
the training set
2
. Thus we are only interested
in correctly ordering the words appearing in the
training corpus and collecting the sentences that
contain them for building the LM. At the same
time, a compact and more relevant LM corpus is
also useful for modeling longer range dependen-
cies with higher order n-gram models. We use
1-gram features for LM corpus selection since we
don?t know which phrases will be generated by the
translation model. After the LM corpus selection,
the target side of the parallel training data is added
to the LM corpus.
3 Results
We run ParFDA5 SMT experiments for all lan-
guage pairs in both directions in the WMT14
translation task (Bojar et al., 2014), which include
English-Czech (en-cs), English-German (en-de),
English-French (en-fr), English-Hindi (en-hi), and
English-Russian (en-ru). We true-case all of the
corpora, use 150-best lists during tuning, set the
LM order to a value between 7 and 10 for all lan-
guage pairs, and train the LM using SRILM (Stol-
cke, 2002). We set the maximum sentence length
filter to 126 and for GIZA++ (Och and Ney, 2003),
1
(Cormen et al., 2009), question 6.5-9. Merging k sorted
lists into one sorted list using a min-heap for k-way merging.
2
Unless the translation is a verbatim copy of the source.
60
S ? T
Training Data LM Data
Data #word S (M) #word T (M) #sent (K) SCOV TCOV #word (M) TCOV
en-cs C 253.5 223.4 16068 0.8282 0.7046 717.0 0.8539
en-cs ParFDA5 22.0 19.6 1205 0.8161 0.6062 325.8 0.8238
cs-en C 223.4 253.5 16068 0.7046 0.8282 5541.9 0.9552
cs-en ParFDA5 19.3 22.0 1205 0.7046 0.7581 351.0 0.9132
en-de C 116.0 109.5 4511 0.812 0.7101 1573.8 0.8921
en-de ParFDA5 16.7 16.8 845 0.8033 0.6316 206.9 0.8184
de-en C 109.5 116.0 4511 0.7101 0.812 5446.8 0.9525
de-en ParFDA5 17.8 19.6 845 0.7087 0.753 339.5 0.9082
en-fr C 1096.1 1287.8 40344 0.8885 0.9163 2534.5 0.9611
en-fr ParFDA5 22.6 26.6 1008 0.8735 0.8412 737.4 0.9491
fr-en C 1287.8 1096.1 40344 0.9163 0.8885 6255.8 0.9675
fr-en ParFDA5 20.9 19.3 1008 0.8963 0.7845 463.4 0.9282
en-hi C 3.4 5.0 306 0.5467 0.5986 36.3 0.7972
en-hi ParFDA5 3.3 4.9 254 0.5467 0.5976 41.2 0.8115
hi-en C 5.0 3.4 306 0.5986 0.5467 5350.4 0.9473
hi-en ParFDA5 5.0 3.3 284 0.5985 0.5466 966.8 0.9209
en-ru C 49.6 46.1 2531 0.7992 0.6823 590.8 0.8679
en-ru ParFDA5 19.6 18.6 1107 0.7991 0.6388 282.1 0.8447
ru-en C 46.1 49.6 2531 0.6823 0.7992 5380.6 0.9567
ru-en ParFDA5 16.6 19.4 1107 0.6821 0.7586 225.1 0.9009
Table 2: The data statistics for the available training and LM corpora for the constrained (C) submissions
compared with the ParFDA5 selected training and LM corpora statistics. #words is in millions (M) and
#sents is in thousands (K).
S ? T d c s i l
T
r
a
i
n
i
n
g
,
n
=
2
en-de 1.0 0.5817 1.4176 5.0001 -3.154
de-en 1.0 1.0924 1.3604 5.0001 -4.341
en-cs 1.0 0.0676 0.8299 5.0001 -0.8788
cs-en 1.0 1.5063 0.7777 3.223 -2.3824
en-ru 1.0 0.6519 1.6877 5.0001 -1.1888
ru-en 1.0 1.607 3.0001 0.0 -1.8247
en-hi 1.0 3.0001 3.0001 1.5701 -1.5699
hi-en 1.0 0.0 1.1001 5.0001 -0.8264
en-fr 1.0 0.8143 0.801 3.5996 -1.3394
fr-en 1.0 0.19 1.0106 5.0001 1.238
L
M
,
n
=
1
en-de 1.0 0.1924 1.0487 5.0001 4.9404
de-en 1.0 1.7877 3.0001 3.1213 -0.4147
en-cs 1.0 0.4988 1.1586 5.0001 -5.0001
cs-en 0.9255 0.2787 0.7439 3.7264 -2.0564
en-ru 1.0 1.4419 2.239 1.5543 -0.5097
ru-en 1.0 2.4844 3.0001 4.6669 3.7978
en-hi 1.0 0.0 0.0 5.0001 -4.944
hi-en 1.0 0.3053 3.0001 5.0001 4.1216
en-fr 1.0 3.0001 2.0452 3.0229 3.4364
fr-en 1.0 0.7467 0.7641 5.0001 5.0001
Table 1: Optimized ParFDA5 parameters for se-
lecting the training set using 2-grams or the LM
corpus using 1-grams.
max-fertility is set to 10, with the number of itera-
tions set to 7,3,5,5,7 for IBM models 1,2,3,4, and
the HMM model and 70 word classes are learned
over 3 iterations with the mkcls tool during train-
ing. The development set contains 5000 sentences,
2000 of which are randomly sampled from pre-
vious years? development sets (2008-2012) and
3000 come from the development set for WMT14.
3.1 Optimized ParFDA5 Parameters
Table 1 presents the optimized ParFDA5 parame-
ters obtained using the development set. Transla-
tion direction specific differences are visible. A
negative value for l shows that FDA5 prefers
shorter features, which we observe mainly when
the target language is English. We also observe
higher exponential decay rates when the target lan-
guage is mainly English. For optimizing the pa-
rameters for selecting LM corpus instances, we
still use a parallel corpus and instead of optimiz-
ing for TCOV, we optimize for SCOV such that
we select instances that are relevant for the target
training corpus but still maximize the coverage of
source features and be able to represent the source
sentences within a translation task. The selected
LM corpus is prepared for a translation task.
3.2 Data Selection
We select the same number of sentences with Par-
allel FDA (Bic?ici, 2013), which is roughly 15%
of the training corpus for en-de, 35% for ru-en,
6% for cs-en, and 2% for en-fr. After the training
set selection, we select the LM data using the tar-
get side of the training set as the target domain to
select LM instances for. For en and fr, we have
access to the LDC Gigaword corpora (Parker et
al., 2011; Graff et al., 2011), from which we ex-
tract only the story type news. We select 15 mil-
lion sentences for each LM not including the se-
61
S ? T
Time (Min) Space (MB)
ParFDA5 Moses
Overall
Moses
Train LM Total Train Tune Total PT LM ALL
en-cs 5 28 34 375 702 1162 1196 1871 5865 19746
cs-en 7 65 72 358 448 867 939 1808 4906 18650
en-de 8 29 38 302 1059 1459 1497 1676 2923 18313
de-en 8 85 93 358 474 924 1017 1854 5219 19247
en-fr 23 60 84 488 781 1372 1456 2309 9577 24362
fr-en 21 99 120 315 490 897 1017 1845 4888 17466
en-hi 2 9 11 91 366 511 522 269 817 4292
hi-en 1 36 37 91 330 467 504 285 9697 3845
en-ru 11 25 35 358 369 837 872 2174 4770 21283
ru-en 10 62 71 309 510 895 966 1939 2735 19537
Table 3: The space and time required for building the ParFDA5 Moses SMT systems. The sizes are in
MB and time in minutes. PT stands for the phrase table. ALL does not contain the size of the LM.
BLEUc
S ? en en? T
cs-en de-en fr-en hi-en ru-en en-cs en-de en-fr en-hi en-ru
WMT14C 0.288 0.28 0.35 0.139 0.318 0.21 0.201 0.358 0.111 0.287
ParFDA5 0.256 0.239 0.319 0.105 0.282 0.172 0.168 0.325 0.07 0.257
diff 0.032 0.041 0.031 0.034 0.036 0.038 0.033 0.033 0.041 0.03
LM order 9 9 9 9 9 9 9 7 10 9
Table 4: BLEUc for the top constrained result in WMT14 (WMT14C) and for ParFDA5 results, their
difference to WMT14C, and the LM order used are presented. Average difference is 3.49 BLEU points.
lected training set, which is added later. The statis-
tics of the ParFDA5 selected training data and the
available training data for the constrained transla-
tion task is given in Table 2. The size of the LM
corpora includes both the LDC and the monolin-
gual LM corpora provided by WMT14. Table 2
shows the significant size differences between the
constrained dataset (C) and the ParFDA5 selected
data. Table 2 also present the source and target
coverage (SCOV and TCOV) in terms of the 2-
grams of the test set observed in the training data
or the LM data. The quality of the training cor-
pus can be measured by TCOV, which is found to
correlate well with the BLEU performance achiev-
able (Bic?ici and Yuret, 2011; Bic?ici, 2011).
3.3 Computing Statistics
We quantify the time and space requirements for
running ParFDA5 SMT systems for each trans-
lation direction. The space and time required
for building the ParFDA5 Moses SMT systems
are given in Table 3 where the sizes are in MB
and the time in minutes. PT stands for the
phrase table. We used Moses version 2.1.1, from
www.statmt.org/moses. Building a ParFDA5
Moses SMT system takes about half a day.
3.4 Translation Results
The results of our two ParFDA5 SMT experiments
for each language pair and their tokenized BLEU
performance, BLEUc, together with the LM order
used and the top constrained submissions to the
WMT14 are given in Table 4
3
, which use phrase-
based Moses for comparison
4
. We observed sig-
nificant gains (+0.23 BLEU points) using higher
order LMs last year (Bic?ici, 2013) and therefore
we use LMs of order 7 to 10. The test set con-
tains 10,000 sentences and only 3000 of which are
used for evaluation, which can make the transduc-
tive learning application of ParFDA5 harder. In
the transductive learning setting, ParFDA5 is se-
lecting target test task specific SMT resources and
therefore, having irrelevant instances in the test set
may decrease the performance by causing FDA5
to select more domain specific data and less task
specific. ParFDA5 significantly reduces the time
required for training, development, and deploy-
ment of an SMT system for a given translation
3
We use the results from matrix.statmt.org.
4
Phrase-based Moses systems usually rank in the top 3.
62
ppl
OOV log OOV = ?19 log OOV = ?11
Translation T order train FDA5 FDA5 LM % red. train FDA5 FDA5 LM % red. train FDA5 FDA5 LM % red.
en-cs en
3
866 1205 525 0.39
1764 1731 938 0.47 1370 1218 805 0.41
4 1788 1746 877 0.51 1389 1229 753 0.46
5 1799 1752 868 0.52 1398 1233 745 0.47
6 1802 1753 867 0.52 1400 1234 744 0.47
cs-en cs
3
557 706 276 0.5
480 419 333 0.31 408 342 307 0.25
4 487 422 292 0.4 415 344 269 0.35
5 495 424 285 0.42 421 346 263 0.38
6 497 425 284 0.43 423 346 262 0.38
en-de en
3
1666 2116 744 0.55
1323 1605 747 0.44 831 890 607 0.27
4 1307 1596 689 0.47 821 885 560 0.32
5 1307 1596 680 0.48 822 885 553 0.33
6 1308 1596 679 0.48 822 885 552 0.33
de-en de
3
691 849 417 0.4
482 498 394 0.18 386 379 345 0.11
4 470 490 344 0.27 376 373 301 0.2
5 470 490 336 0.29 377 373 293 0.22
6 471 490 334 0.29 377 373 292 0.23
en-fr en
3
270 411 153 0.43
185 167 173 0.07 173 151 166 0.04
4 170 160 135 0.21 159 144 130 0.19
5 171 160 126 0.27 160 145 121 0.24
fr-en fr
3
306 604 179 0.42
349 325 275 0.21 320 275 261 0.19
4 338 321 235 0.3 310 271 224 0.28
5 342 322 228 0.33 314 272 217 0.31
en-hi en
3
2035 2123 950 0.53
242 246 114 0.53 168 168 96 0.43
4 237 241 87 0.63 164 165 73 0.55
5 238 242 78 0.67 165 165 66 0.6
6 239 242 75 0.68 165 165 64 0.62
hi-en hi
3
1842 1860 623 0.66
1894 1898 482 0.75 915 911 377 0.59
4 1910 1914 398 0.79 923 919 312 0.66
5 1915 1919 378 0.8 925 921 296 0.68
6 1915 1919 378 0.8 926 921 296 0.68
en-ru en
3
959 1176 585 0.39
1067 1171 668 0.37 814 840 566 0.3
4 1053 1159 603 0.43 803 831 511 0.36
5 1052 1159 591 0.44 802 831 501 0.38
6 1052 1159 588 0.44 802 831 498 0.38
ru-en ru
3
558 689 340 0.39
385 398 363 0.06 334 334 333 0.0
4 377 391 325 0.14 327 328 298 0.09
5 378 392 318 0.16 328 329 292 0.11
6 378 392 318 0.16 328 329 291 0.11
Table 5: Perplexity comparison of the LM built from the training corpus (train), ParFDA5 selected
training corpus (FDA5), and the ParFDA5 selected LM corpus (FDA5 LM). % red. column lists the
percentage of reduction.
task. The average difference to the top constrained
submission in WMT14 is 3.49 BLEU points. For
en-ru and en-cs, true-casing the LM using a true-
caser trained on all of the available training data
decreased the performance by 0.5 and 0.9 BLEU
points respectively and for cs-en and fr-en, in-
creased the performance by 0.2 and 0.5 BLEU
points. We use the true-cased LM results using
a true-caser trained on all of the available train-
ing data for all language pairs where for hi-en,
the true-caser is trained on the ParFDA5 selected
training data.
3.5 LM Data Quality
A LM training data selected for a given transla-
tion task allows us to train higher order language
models, model longer range dependencies better,
and at the same time, achieve lower perplexity
as given in Table 5. We compare the perplexity
of the ParFDA5 selected LM with a LM trained
on the ParFDA5 selected training data and a LM
trained using all of the available training corpora.
To be able to compare the perplexities, we take
the OOV tokens into consideration during calcu-
lations (Bic?ici, 2013). We present results for the
cases when we handle OOV words with a cost
of ?19 or ?11 each in Table 5. We are able to
achieve significant reductions in the number of
OOV tokens and the perplexity, reaching up to
66% reduction in the number of OOV tokens and
up to 80% reduction in the perplexity.
63
BLEUc
S ? en en? T
cs-en de-en fr-en ru-en en-cs en-de en-fr en-ru
ParFDA5 0.256 0.239 0.319 0.282 0.172 0.168 0.325 0.257
ParFDA 0.243 0.241 0.254 0.223 0.171 0.179 0.238 0.173
diff 0.013 -0.002 0.065 0.059 0.001 -0.011 0.087 0.084
Table 7: Parallel FDA5 WMT14 results compared with parallel FDA WMT13 results. Training set sizes
are given in millions (M) of words on the target side. Average difference is 3.7 BLEU points.
BLEUc
S ? en en? T
cs-en fr-en en-cs en-fr
ParFDA5 0.256 0.319 0.172 0.325
ParFDA5 15% 0.248 0.321 0.178 0.333
diff -0.008 0.002 0.006 0.008
Table 6: ParFDA5 results, ParFDA5 results using
15% of the training set, and their difference.
3.6 Using 15% of the Available Training Set
In the FDA5 results (Bic?ici and Yuret, 2014),
we found that selecting 15% of the best train-
ing set size maximizes the performance for the
English-German out-of-domain translation task
and achieves 0.41 BLEU points improvement over
a baseline system using all of the available train-
ing data. We run additional experiments select-
ing 15% of the training data for fr-en and cs-en
language pairs to see the effect of increased train-
ing sets selected with ParFDA5. The results are
given in Table 6 where most of the results improve.
The slight performance decrease for cs-en may be
due to using a true-caser trained on only the se-
lected training data. We observe larger gains in
the en? T translations.
3.7 ParFDA5 versus Parallel FDA
We compare this year?s results with the results
we obtained last year (Bic?ici, 2013) in Table 7.
The task setting is different in WMT14 since the
test set contains 10,000 sentences but only 3000
of these are used as the actual test set, which
can make the transductive learning application of
ParFDA5 harder. We select the same number of
instances for the training sets but 5 million more
instances for the LM corpus this year. The aver-
age difference to the top constrained submission
in WMT13 was 2.88 BLEU points (Bic?ici, 2013)
and this has increased to 3.49 BLEU points in
WMT14. On average, the performance improved
3.7 BLEU points when compared with ParFDA re-
sults last year. For the fr-en, en-fr, and en-ru trans-
lation directions, we observe increases in the per-
formance. This may be due to better modeling of
the target domain by better parameterization and
optimization that FDA5 is providing. We observe
some decrease in the performance in en-de and de-
en results. Since the training material remained
the same for WMT13 and WMT14 and the mod-
eling power of FDA5 increased, building a domain
specific rather than a task specific ParFDA5 model
may be the reason for the decrease.
4 Conclusion
We use parallel FDA5 for solving computational
scalability problems caused by the abundance of
training data for SMT models and LMs and still
achieve SMT performance that is on par with
the top performing SMT systems. Parallel FDA5
raises the bar of expectations from SMT with
highly accurate translations and lower the bar to
entry for SMT into new domains and tasks by al-
lowing fast deployment of SMT systems in about
half a day. Parallel FDA5 enables a shift from gen-
eral purpose SMT systems towards task adaptive
SMT solutions.
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University and in part by the
European Commission through the QTLaunchPad
FP7 project (No: 296347). We also thank the
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational
facilities and support.
References
Ergun Bic?ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283, Ed-
64
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Ond?rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Iacer Calixto, Ali Hosseinzadeh Vahid, Xiaojun Zhang,
Jian Zhang, Xiaofeng Wu, Andy Way, and Qun Liu.
2014. Experiments in medical translation shared
task at wmt 2014. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Baltimore,
USA, June. Association for Computational Linguis-
tics.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2009. Introduction to
Algorithms (3. ed.). MIT Press.
David Graff,
?
Angelo Mendonc?a, and Denise DiPersio.
2011. French Gigaword third edition, Linguistic
Data Consortium.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177?180, Prague, Czech Republic, June.
William Lewis, Robert Munro, and Stephan Vogel.
2011. Crisis mt: Developing a cookbook for mt
in crisis situations. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
501?511, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Liangyou Li, Xiaofeng Wu, Santiago Cortes Vaillo,
Jun Xie, Jia Xu, Andy Way, and Qun Liu. 2014.
The dcu-ictcas-tsinghua mt system at wmt 2014 on
german-english translation task. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, Baltimore, USA, June. Association for Compu-
tational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing, pages 901?904.
65
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 136?141,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The DCU-ICTCAS MT system at WMT 2014 on German-English
Translation Task
Liangyou Li
?
, Xiaofeng Wu
?
, Santiago Cort
?
es Va??llo
?
Jun Xie
?
, Andy Way
?
, Qun Liu
??
?
CNGL Centre for Global Intelligent Content, School of Computing
Dublin City University, Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
{liangyouli,xiaofengwu,scortes,away,qliu}@computing.dcu.ie
xiejun@ict.ac.cn
Abstract
This paper describes the DCU submis-
sion to WMT 2014 on German-English
translation task. Our system uses phrase-
based translation model with several pop-
ular techniques, including Lexicalized
Reordering Model, Operation Sequence
Model and Language Model interpolation.
Our final submission is the result of sys-
tem combination on several systems which
have different pre-processing and align-
ments.
1 Introduction
On the German-English translation task of WMT
2014, we submitted a system which is built with
Moses phrase-based model (Koehn et al., 2007).
For system training, we use all provided
German-English parallel data, and conducted sev-
eral pre-processing steps to clean the data. In ad-
dition, in order to improve the translation quali-
ty, we adopted some popular techniques, includ-
ing three Lexicalized Reordering Models (Axel-
rod et al., 2005; Galley and Manning, 2008), a 9-
gram Operation Sequence Model (Durrani et al.,
2011) and Language Model interpolation on sev-
eral datasets. And then we use system combina-
tion on several systems with different settings to
produce the final outputs.
Our phrase-based systems are tuned with k-best
MIRA (Cherry and Foster, 2012) on development
set. We set the maximum iteration to be 25.
The Language Models in our systems are
trained with SRILM (Stolcke, 2002). We trained
Corpus Filtered Out (%)
Bilingual 7.17
Monolingual (English) 1.05
Table 1: Results of language detection: percentage
of filtered out sentences
a 5-gram model with Kneser-Ney discounting
(Chen and Goodman, 1996).
In the next sections, we will describe our system
in detail. In section 2, we will explain our pre-
processing steps on corpus. Then in section 3, we
will describe some techniques we have tried for
this task and the experiment results. In section 4,
our final configuration for submitted system will
be presented. And we conclude in the last section.
2 Pre-processing
We use all the training data for German-English
translation, including Europarl, News Commen-
tary and Common Crawl. The first thing we no-
ticed is that some Non-German and Non-English
sentences are included in our training data. So we
apply Language Detection (Shuyo, 2010) for both
monolingual and bilingual corpora. For mono-
lingual data (only including English sentences in
our task), we filter out sentences which are detect-
ed as other language with probability more than
0.999995. And for bilingual data, A sentence
pair is filtered out if the language detector detect-
s a different language with probability more than
0.999995 on either the source or the target. The
filtering results are given in Table 1.
In our experiment, German compound word-
s are splitted based on frequency (Koehn and
136
Knight, 2003). In addition, for both monolingual
and bilingual data, we apply tokenization, nor-
malizing punctuation and truecasing using Moses
scripts. For parallel training data, we also filter out
sentence pairs containing more than 80 tokens on
either side and sentence pairs whose length ratio
between source and target side is larger than 3.
3 Techniques
In our preliminary experiments, we take newstest
2013 as our test data and newstest 2008-2012 as
our development data. In total, we have more
than 10,000 sentences for tuning. The tuning step
would be very time-consuming if we use them al-
l. So in this section, we use Feature Decay Al-
gorithm (FDA) (Bic?ici and Yuret, 2014) to select
2000 sentences as our development set. Table 2
shows that system performance does not increase
with larger tuning set and the system using only
2K sentences selected by FDA is better than the
baseline tuned with all the development data.
In this section, alignment model is trained
by MGIZA++ (Gao and Vogel, 2008) with
grow-diag-final-and heuristic function.
And other settings are mostly default values in
Moses.
3.1 Lexicalized Reordering Model
German and English have different word order
which brings a challenge in German-English ma-
chine translation. In our system, we adopt three
Lexicalized Reordering Models (LRMs) for ad-
dressing this problem. They are word-based LRM
(wLRM), phrase-based LRM (pLRM) and hierar-
chal LRM (hLRM).
These three models have different effect on the
translation. Word-based and phrase-based LRMs
are focus on local reordering phenomenon, while
hierarchical LRM could be applied into longer re-
ordering problem. Figure 1 shows the differences
(Galley and Manning, 2008). And Table 3 shows
effectiveness of different LRMs.
In our system based on Moses, we
use wbe-msd-bidirectional-fe,
phrase-msd-bidirectional-fe and
hier-mslr-bidirectional-fe to specify
these three LRMs. From Table 2, we could see
that LRMs significantly improves the translation.
Figure 1: Occurrence of a swap according to
the three orientation models: word-based, phrase-
based, and hierarchical. Black squares represen-
t word alignments, and gray squares represen-
t blocks identified by phrase-extract. In (a), block
b
i
= (e
i
, f
a
i
) is recognized as a swap according to
all three models. In (b), b
i
is not recognized as a
swap by the word-based model. In (c), b
i
is rec-
ognized as a swap only by the hierarchical model.
(Galley and Manning, 2008)
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) (Durrani
et al., 2011) explains the translation procedure as
a linear sequence of operations which generates
source and target sentences in parallel. Durrani
et al. (2011) defined four translation operations:
Generate(X,Y), Continue Source Concept, Gener-
ate Source Only (X) and Generate Identical, as
well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward. These oper-
ations are described as follows.
? Generate(X,Y) make the words in Y and the
first word in X added to target and source
string respectively.
? Continue Source Concept adds the word in
the queue from Generate(X,Y) to the source
string.
? Generate Source Only (X) puts X in the
source string at the current position.
? Generate Identical generates the same word
for both sides.
? Insert Gap inserts a gap in the source side for
future use.
? Jump Back (W) makes the position for trans-
lation be the Wth closest gap to the current
position.
? Jump Forward moves the position to the in-
dex after the right-most source word.
137
Systems Tuning Set newstest 2013
Baseline ? 24.1
+FDA ? 24.2
+LRMs 24.0 25.4
+OSM 24.4 26.2
+LM Interpolation 24.6 26.4
+Factored Model ? 25.9
+Sparse Feature 25.6 25.9
+TM Combination 24.1 25.4
+OSM Interpolation 24.4 26.0
Table 2: Preliminary results on tuning set and test set (newstest 2013). All scores on test set are case-
sensitive BLEU[%] scores. And scores on tuning set are case-insensitive BLEU[%] directly from tuning
result. Baseline uses all the data from newstest 2008-2012 for tuning.
Systems Tuning Set (uncased) newstest 2013
Baseline+FDA ? 24.2
+wLRM 23.8 25.1
+pLRM 23.9 25.2
+hLRM 24.0 25.4
+pLRM 23.8 25.1
+hLRM 23.7 25.2
Table 3: System BLEU[%] scores when different LRMs are adopted.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is:
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
In this paper we train a 9-gram OSM on train-
ing data and integrate this model directly into log-
linear framework (OSM is now available to use
in Moses). Our experiment shows OSM improves
our system by about 0.8 BLEU (see Table 2).
3.3 Language Model Interpolation
In our baseline, Language Model (LM) is trained
on all the monolingual data provided. In this sec-
tion, we try to build a large language model by in-
cluding data from English Gigaword fifth edition
(only taking partial data with size of 1.6G), En-
glish side of UN corpus and English side of 10
9
French-English corpus. Instead of training a s-
ingle model on all data, we interpolate language
models trained on each subset (monolingual data
provided is splitted into three parts: News 2007-
2013, Europarl and News Commentary) by tuning
weights to minimize perplexity of language model
measured on the target side of development set.
In our experiment, after interpolation, the lan-
guage model doesn?t get a much lower perplexity,
but it slightly improves the system, as shown in
Table 2.
3.4 Other Tries
In addition to the techniques mentioned above, we
also try some other approaches. Unfortunately al-
l of these methods described in this section are
non-effective in our experiments. The results are
shown in Table 2.
? Factored Model (Koehn and Hoang, 2007):
We tried to integrate a target POS factored
model into our system with a 9-gram POS
language model to address the problem of
word selection and word order. But ex-
periment doesn?t show improvement. The
English POS is from Stanford POS Tagger
(Toutanova et al., 2003).
? Translation Model Combination: In this ex-
periment, we try to use the method of (Sen-
nrich, 2012) to combine phrase tables or re-
ordering tables from different subsets of data
138
to minimize perplexity measured on develop-
ment set. We try to split the training data in
two ways. One is according to data source,
resulting in three subsets: Europarl, News
Commentary and Common Crawl. Another
one is to use data selection. We use FDA to
select 200K sentence pairs as in-domain data
and the rest as out-domain data. Unfortunate-
ly both experiments failed. In Table 2, we on-
ly report results of phrase table combination
on FDA-based data sets.
? OSM Interpolation: Since OSM in our sys-
tem could be taken as a special language
model, we try to use the idea of interpolation
similar with language model to make OSM
adapted to some data. Training data are s-
plitted into two subsets with FDA. We train
9-gram OSM on each subsets and interpolate
them according to OSM trained on the devel-
opment set.
? Sparse Features: For each source phrase,
there is usually more than one corresponding
translation option. Each different translation
may be optimal in different contexts. Thus
in our systems, similar to (He et al., 2008)
which proposed a Maximum Entropy-based
rule selection for the hierarchical phrase-
based model, features which describe the
context of phrases, are designed to select the
right translation. But different with (He et
al., 2008), we use sparse features to mod-
el the context. And instead of using syn-
tactic POS, we adopt independent POS-like
features: cluster ID of word. In our experi-
ment mkcls was used to cluster words into 50
groups. And all features are generalized to
cluster ID.
4 Submission
Based on our preliminary experiments in the sec-
tion above, we use LRMs, OSM and LM inter-
polation in our final system for newstest 2014.
But as we find that Language Models trained on
UN corpus and 10
9
French-English corpus have
a very high perplexity and in order to speed up
the translation by reducing the model size, in this
section, we interpolate only three language model-
s from monolingual data provided, English Giga-
word fifth edition and target side of training data.
In addition, we also try some different methods for
final submission. And the results are shown in Ta-
ble 4.
? Development Set Selection: Instead of using
FDA which is dependent on test set, we use
the method of (Nadejde et al., 2013) to se-
lect tuning set from newstest 2008-2013 for
the final system. We only keep 2K sentences
which have more than 30 words and higher
BLEU score. The experiment result is shown
in Table 4 ( The system is indicated as Base-
line).
? Pre-processing: In our preliminary exper-
iments, sentences are tokenized without
changing hyphen. Thus we build another sys-
tem where all the hyphens are tokenized ag-
gressively.
? SyMGIZA++: Better alignment could lead to
better translation. So we carry out some ex-
periments on SyMGIZA++ aligner (Junczys-
Dowmunt and Sza, 2012), which modifies the
original IBM/GIZA++ word alignment mod-
els to allow to update the symmetrized mod-
els between chosen iterations of the original
training algorithms. Experiment shows this
new alignment improves translation quality.
? Multi-alignment Selection: We also try to use
multi-alignment selection (Tu et al., 2012)
to generate a ?better? alignment from three
alignmens: MGIZA++ with function grow-
diag-final-and, SyMGIZA++ with function
grow-diag-final-and and fast alignment (Dy-
er et al., 2013). Although this method show
comparable or better result on development
set, it fails on test set.
Since we build a few systems with different
setting on Moses phrase-based model, a straight-
forward thinking is to obtain the better transla-
tion from several different translation systems. So
we use system combination (Heafield and Lavie,
2010) on the 1-best outputs of three systems (in-
dicated with
?
in table 4). And this results in our
best system so far, as shown in Table 4. In our final
submission, this result is taken as primary.
5 Conclusion
This paper describes our submitted system to
WMT 2014 in detail. This system is based on
139
Systems Tuning Set newstest 2014
Baseline
?
34.2 25.6
+SyMGIZA++
?
34.3 26.0
+Multi-Alignment Selection 34.4 25.6
+Hyphen-Splitted 33.9 25.9
+SyMGIZA++
?
34.0 26.0
+Multi-Alignment Selection 34.0 25.7
System Combination ? 26.5
Table 4: Experiment results on newstest 2014. We report case-sensitive BLEU[%] score on test set and
case-insensitive BLEU[%] on tuning set which is directly from tuning result. Baseline is the phrase-based
system with LRMs, OSM and LM interpolation on smaller datasets, tuned with selected development set.
Systems indicated with
?
are used for system combination.
Moses phrase-based model, and integrates Lexi-
calized Reordering Models, Operation Sequence
Model and Language Model interpolation. Al-
so system combination is used on several system-
s which have different pre-processing and align-
ment.
Acknowledgments
This work is supported by EC Marie-Curie initial
training Network EXPERT (EXPloiting Empiri-
cal appRoaches to Translation) project (http:
//expert-itn.eu). Thanks to Johannes Lev-
eling for his help on German compound splitting.
And thanks to Jia Xu and Jian Zhang for their ad-
vice and help on this paper and experiments.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAA-
CL HLT ?12, pages 427?436, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of NAACL.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321?328, Manchester, UK,
August. Coling 2008 Organizing Committee.
Kenneth Heafield and Alon Lavie. 2010. Combining
machine translation output with open source: The
Carnegie Mellon multi-engine machine translation
scheme. The Prague Bulletin of Mathematical Lin-
guistics, 93:27?36, January.
Marcin Junczys-Dowmunt and Arkadiusz Sza. 2012.
Symgiza++: Symmetrized word alignment models
for statistical machine translation. In Pascal Bouvry,
MieczysawA. Kopotek, Franck Leprvost, Magorza-
ta Marciniak, Agnieszka Mykowiecka, and Henryk
140
Rybiski, editors, Security and Intelligent Informa-
tion Systems, volume 7053 of Lecture Notes in Com-
puter Science, pages 379?390. Springer Berlin Hei-
delberg.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the Tenth Conference on European Chapter of the
Association for Computational Linguistics - Volume
1, EACL ?03, pages 187?193, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh?s syntax-based machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170?
176, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 539?549,
Avignon, France, April. Association for Computa-
tional Linguistics.
Nakatani Shuyo. 2010. Language detection library for
java.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of the Internation-
al Conference Spoken Language Processing, pages
901?904, Denver, CO.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
141
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171?177,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Abu-MaTran at WMT 2014 Translation Task:
Two-step Data Selection and RBMT-Style Synthetic Rules
Raphael Rubino
?
, Antonio Toral
?
, Victor M. S
?
anchez-Cartagena
??
,
Jorge Ferr
?
andez-Tordera
?
, Sergio Ortiz-Rojas
?
, Gema Ram??rez-S
?
anchez
?
,
Felipe S
?
anchez-Mart??nez
?
, Andy Way
?
?
Prompsit Language Engineering, S.L., Elche, Spain
{rrubino,vmsanchez,jferrandez,sortiz,gramirez}@prompsit.com
?
NCLT, School of Computing, Dublin City University, Ireland
{atoral,away}@computing.dcu.ie
?
Dep. Llenguatges i Sistemes Inform`atics, Universitat d?Alacant, Spain
fsanchez@dlsi.ua.es
Abstract
This paper presents the machine trans-
lation systems submitted by the Abu-
MaTran project to the WMT 2014 trans-
lation task. The language pair concerned
is English?French with a focus on French
as the target language. The French to En-
glish translation direction is also consid-
ered, based on the word alignment com-
puted in the other direction. Large lan-
guage and translation models are built us-
ing all the datasets provided by the shared
task organisers, as well as the monolin-
gual data from LDC. To build the trans-
lation models, we apply a two-step data
selection method based on bilingual cross-
entropy difference and vocabulary satura-
tion, considering each parallel corpus in-
dividually. Synthetic translation rules are
extracted from the development sets and
used to train another translation model.
We then interpolate the translation mod-
els, minimising the perplexity on the de-
velopment sets, to obtain our final SMT
system. Our submission for the English to
French translation task was ranked second
amongst nine teams and a total of twenty
submissions.
1 Introduction
This paper presents the systems submitted by the
Abu-MaTran project (runs named DCU-Prompsit-
UA) to the WMT 2014 translation task for the
English?French language pair. Phrase-based sta-
tistical machine translation (SMT) systems were
submitted, considering the two translation direc-
tions, with the focus on the English to French di-
rection. Language models (LMs) and translation
models (TMs) are trained using all the data pro-
vided by the shared task organisers, as well as
the Gigaword monolingual corpora distributed by
LDC.
To train the LMs, monolingual corpora and the
target side of the parallel corpora are first used
individually to train models. Then the individ-
ual models are interpolated according to perplex-
ity minimisation on the development sets.
To train the TMs, first a baseline is built us-
ing the News Commentary parallel corpus. Sec-
ond, each remaining parallel corpus is processed
individually using bilingual cross-entropy differ-
ence (Axelrod et al., 2011) in order to sepa-
rate pseudo in-domain and out-of-domain sen-
tence pairs, and filtering the pseudo out-of-
domain instances with the vocabulary saturation
approach (Lewis and Eetemadi, 2013). Third,
synthetic translation rules are automatically ex-
tracted from the development set and used to train
another translation model following a novel ap-
proach (S?anchez-Cartagena et al., 2014). Finally,
we interpolate the four translation models (base-
line, in-domain, filtered out-of-domain and rules)
by minimising the perplexity obtained on the de-
velopment sets and investigate the best tuning and
decoding parameters.
The reminder of this paper is organised as fol-
lows: the datasets and tools used in our experi-
ments are described in Section 2. Then, details
about the LMs and TMs are given in Section 3 and
Section 4 respectively. Finally, we evaluate the
performance of the final SMT system according to
different tuning and decoding parameters in Sec-
tion 5 before presenting conclusions in Section 6.
171
2 Datasets and Tools
We use all the monolingual and parallel datasets
in English and French provided by the shared task
organisers, as well as the LDC Gigaword for the
same languages
1
. For each language, a true-case
model is trained using all the data, using the train-
truecaser.perl script included in the MOSES tool-
kit (Koehn et al., 2007).
Punctuation marks of all the monolingual and
parallel corpora are then normalised using the
script normalize-punctuation.perl provided by the
organisers, before being tokenised and true-cased
using the scripts distributed with the MOSES tool-
kit. The same pre-processing steps are applied to
the development and test sets. As development
sets, we used all the test sets from previous years
of WMT, from 2008 to 2013 (newstest2008-2013).
Finally, the training parallel corpora are cleaned
using the script clean-corpus-n.perl, keeping the
sentences longer than 1 word, shorter than 80
words, and with a length ratio between sentence
pairs lower than 4.
2
The statistics about the cor-
pora used in our experiments after pre-processing
are presented in Table 1.
For training LMs we use KENLM (Heafield et
al., 2013) and the SRILM tool-kit (Stolcke et al.,
2011). For training TMs, we use MOSES (Koehn
et al., 2007) version 2.1 with MGIZA++ (Och and
Ney, 2003; Gao and Vogel, 2008). These tools are
used with default parameters for our experiments
except when explicitly said.
The decoder used to generate translations is
MOSES using features weights optimised with
MERT (Och, 2003). As our approach relies on
training individual TMs, one for each parallel cor-
pus, our final TM is obtained by linearly interpo-
lating the individual ones. The interpolation of
TMs is performed using the script tmcombine.py,
minimising the cross-entropy between the TM
and the concatenated development sets from 2008
to 2012 (noted newstest2008-2012), as described
in Sennrich (2012). Finally, we make use of the
findings from WMT 2013 brought by the win-
ning team (Durrani et al., 2013) and decide to use
the Operation Sequence Model (OSM), based on
minimal translation units and Markov chains over
sequences of operations, implemented in MOSES
1
LDC2011T07 English Gigaword Fifth Edition,
LDC2011T10 French Gigaword Third Edition
2
This ratio was empirically chosen based on words fertil-
ity between English and French.
Corpus Sentences (k) Words (M)
Monolingual Data ? English
Europarl v7 2,218.2 59.9
News Commentary v8 304.2 7.4
News Shuffled 2007 3,782.5 90.2
News Shuffled 2008 12,954.5 308.1
News Shuffled 2009 14,680.0 347.0
News Shuffled 2010 6,797.2 157.8
News Shuffled 2011 15,437.7 358.1
News Shuffled 2012 14,869.7 345.5
News Shuffled 2013 21,688.4 495.2
LDC afp 7,184.9 869.5
LDC apw 8,829.4 1,426.7
LDC cna 618.4 45.7
LDC ltw 986.9 321.1
LDC nyt 5,327.7 1,723.9
LDC wpb 108.8 20.8
LDC xin 5,121.9 423.7
Monolingual Data ? French
Europarl v7 2,190.6 63.5
News Commentary v8 227.0 6.5
News Shuffled 2007 119.0 2.7
News Shuffled 2008 4,718.8 110.3
News Shuffled 2009 4,366.7 105.3
News Shuffled 2010 1,846.5 44.8
News Shuffled 2011 6,030.1 146.1
News Shuffled 2012 4,114.4 100.8
News Shuffled 2013 9,256.3 220.2
LDC afp 6,793.5 784.5
LDC apw 2,525.1 271.3
Parallel Data
10
9
Corpus
21,327.1
549.0 (EN)
642.5 (FR)
Common Crawl 3,168.5
76.0 (EN)
82.7 (FR)
Europarl v7 1,965.5
52.5 (EN)
56.7 (FR)
News Commentary v9 181.3
4.5 (EN)
5.3 (FR)
UN 12,354.7
313.4 (EN)
356.5 (FR)
Table 1: Data statistics after pre-processing of the
monolingual and parallel corpora used in our ex-
periments.
and introduced by Durrani et al. (2011).
3 Language Models
The LMs are trained in the same way for both
languages. First, each monolingual and parallel
corpus is considered individually (except the par-
allel version of Europarl and News Commentary)
and used to train a 5-gram LM with the modified
Kneser-Ney smoothing method. We then interpo-
late the individual LMs using the script compute-
best-mix available with the SRILM tool-kit (Stol-
cke et al., 2011), based on their perplexity scores
on the concatenation of the development sets from
2008 to 2012 (the 2013 version is held-out for the
tuning of the TMs).
172
The final LM for French contains all the word
sequences from 1 to 5-grams contained in the
training corpora without any pruning. However,
with the computing resources at our disposal, the
English LMs could not be interpolated without
pruning non-frequent n-grams. Thus, n-grams
with n ? [3; 5] with a frequency lower than 2 were
removed. Details about the final LMs are given in
Table 2.
1-gram 2-gram 3-gram 4-gram 5-gram
English 13.4 198.6 381.2 776.3 1,068.7
French 6.0 75.5 353.2 850.8 1,354.0
Table 2: Statistics, in millions of n-grams, of the
interpolated LMs.
4 Translation Models
In this Section, we describe the TMs trained for
the shared task. First, we present the two-step data
selection process which aims to (i) separate in and
out-of-domain parallel sentences and (ii) reduce
the total amount of out-of-domain data. Second,
a novel approach for the automatic extraction of
translation rules and their use to enrich the phrase
table is detailed.
4.1 Parallel Data Filtering and Vocabulary
Saturation
Amongst the parallel corpora provided by the
shared task organisers, only News Commentary
can be considered as in-domain regarding the de-
velopment and test sets. We use this training
corpus to build our baseline SMT system. The
other parallel corpora are individually filtered us-
ing bilingual cross-entropy difference (Moore and
Lewis, 2010; Axelrod et al., 2011). This data
filtering method relies on four LMs, two in the
source and two in the target language, which
aim to model particular features of in and out-of-
domain sentences.
We build the in-domain LMs using the source
and target sides of the News Commentary paral-
lel corpus. Out-of-domain LMs are trained on a
vocabulary-constrained subset of each remaining
parallel corpus individually using the SRILM tool-
kit, which leads to eight models (four in the source
language and four in the target language).
3
3
The subsets contain the same number of sentences and
the same vocabulary as News Commentary.
Then, for each out-of-domain parallel corpus,
we compute the bilingual cross-entropy difference
of each sentence pair as:
[H
in
(S
src
)?H
out
(S
src
)] + [H
in
(S
trg
)?H
out
(S
trg
)] (1)
where S
src
and S
trg
are the source and the tar-
get sides of a sentence pair, H
in
and H
out
are
the cross-entropies of the in and out-of-domain
LMs given a sentence pair. The sentence pairs are
then ranked and the lowest-scoring ones are taken
to train the pseudo in-domain TMs. However,
the cross-entropy difference threshold required to
split a corpus in two parts (pseudo in and out-of-
domain) is usually set empirically by testing sev-
eral subset sizes of the top-ranked sentence pairs.
This method is costly in our setup as it would lead
to training and evaluating multiple SMT systems
for each of the pseudo in-domain parallel corpora.
In order to save time and computing power,
we consider only pseudo in-domain sentence pairs
those with a bilingual cross-entropy difference be-
low 0, i.e. those deemed more similar to the
in-domain LMs than to the out-of-domain LMs
(H
in
< H
out
). A sample of the distribution of
scores for the out-of-domain corpora is shown in
Figure 1. The resulting pseudo in-domain corpora
are used to train individual TMs, as detailed in Ta-
ble 3.
-4
-2
 0
 2
 4
 6
 8
 10
0 2k 4k 6k 8k 10k
Bilin
gual
 Cro
ss-E
ntro
py D
iffer
ence
Sentence Pairs
Common CrawlEuroparl10^9UN
Figure 1: Sample of ranked sentence-pairs (10k)
of each of the out-of-domain parallel corpora with
bilingual cross-entropy difference
The results obtained using the pseudo in-
domain data show BLEU (Papineni et al., 2002)
scores superior or equal to the baseline score.
Only the Europarl subset is slightly lower than
the baseline, while the subset taken from the 10
9
corpus reaches the highest BLEU compared to the
other systems (30.29). This is mainly due to the
173
size of this subset which is ten times larger than
the one taken from Europarl. The last row of Ta-
ble 3 shows the BLEU score obtained after interpo-
lating the four pseudo in-domain translation mod-
els. This system outperforms the best pseudo in-
domain one by 0.5 absolute points.
Corpus Sentences (k) BLEU
dev
Baseline 181.3 27.76
Common Crawl 208.3 27.73
Europarl 142.0 27.63
10
9
Corpus 1,442.4 30.29
UN 642.4 28.91
Interpolation - 30.78
Table 3: Number of sentence pairs and BLEU
scores reported by MERT on English?French new-
stest2013 for the pseudo in-domain corpora ob-
tained by filtering the out-of-domain corpora with
bilingual cross-entropy difference. The interpola-
tion of pseudo in-domain models is evaluated in
the last row.
After evaluating the pseudo in-domain parallel
data, the remaining sentence pairs for each cor-
pora are considered out-of-domain according to
our filtering approach. However, they may still
contain useful information, thus we make use of
these corpora by building individual TMs for each
corpus (in a similar way we built the pseudo in-
domain models). The total amount of remaining
data (more than 33 million sentence pairs) makes
the training process costly in terms of time and
computing power. In order to reduce these costs,
sentence pairs with a bilingual cross-entropy dif-
ference higher than 10 were filtered out, as we no-
ticed that most of the sentences above this thresh-
old contain noise (non-alphanumeric characters,
foreign languages, etc.).
We also limit the size of the remaining data by
applying the vocabulary saturation method (Lewis
and Eetemadi, 2013). For the out-of-domain sub-
set of each corpus, we traverse the sentence pairs
in the order they are ranked by perplexity differ-
ence and filter out those sentence pairs for which
we have seen already each 1-gram at least 10
times. Each out-of-domain subset from each par-
allel corpus is then used to train a TM before inter-
polating them to create the pseudo out-of-domain
TM. The results reported by MERT obtained on
the newstest2013 development set are detailed in
Table 4.
Mainly due to the sizes of the pseudo out-of-
Corpus Sentences (k) BLEU
dev
Baseline 181.3 27.76
Common Crawl 1,598.7 29.84
Europarl 461.9 28.87
10
9
Corpus 5,153.0 30.50
UN 1,707.3 29.03
Interpolation - 31.37
Table 4: Number of sentence pairs and BLEU
scores reported by MERT on English?French
newstest2013 for the pseudo out-of-domain cor-
pora obtained by filtering the out-of-domain cor-
pora with bilingual cross-entropy difference, keep-
ing sentence pairs below an entropy score of 10
and applying vocabulary saturation. The interpo-
lation of pseudo out-of-domain models is evalu-
ated in the last row.
domain subsets, the reported BLEU scores are
higher than the baseline for the four individual
SMT systems and the interpolated one. This latter
system outperforms the baseline by 3.61 absolute
points. Compared to the results obtained with the
pseudo in-domain data, we observe a slight im-
provement of the BLEU scores using the pseudo
out-of-domain data. However, despite the com-
paratively larger sizes of the latter datasets, the
BLEU scores reached are not that higher. For in-
stance with the 10
9
corpus, the pseudo in and out-
of-domain subsets contain 1.4 and 5.1 million sen-
tence pairs respectively, and the two systems reach
30.3 and 30.5 BLEU. These scores indicate that
the pseudo in-domain SMT systems are more ef-
ficient on the English?French newstest2013 devel-
opment set.
4.2 Extraction of Translation Rules
A synthetic phrase-table based on shallow-transfer
MT rules and dictionaries is built as follows. First,
a set of shallow-transfer rules is inferred from the
concatenation of the newstest2008-2012 develop-
ment corpora exactly in the same way as in the
UA-Prompsit submission to this translation shared
task (S?anchez-Cartagena et al., 2014). In sum-
mary, rules are obtained from a set of bilingual
phrases extracted from the parallel corpus after
its morphological analysis and part-of-speech dis-
ambiguation with the tools in the Apertium rule-
based MT platform (Forcada et al., 2011).
The extraction algorithm commonly used in
phrase-based SMT is followed with some added
heuristics which ensure that the bilingual phrases
174
extracted are compatible with the bilingual dic-
tionary. Then, many different rules are generated
from each bilingual phrase; each of them encodes
a different degree of generalisation over the partic-
ular example it has been extracted from. Finally,
the minimum set of rules which correctly repro-
duces all the bilingual phrases is found based on
integer linear programming search (Garfinkel and
Nemhauser, 1972).
Once the rules have been inferred, the phrase
table is built from them and the original rule-
based MT dictionaries, following the method
by S?anchez-Cartagena et al. (2011), which was
one of winning systems
4
(together with two on-
line SMT systems) in the pairwise manual evalu-
ation of the WMT11 English?Spanish translation
task (Callison-Burch et al., 2011). This phrase-
table is then interpolated with the baseline TM and
the results are presented in Table 5. A slight im-
provement over the baseline is observed, which
motivates the use of synthetic rules in our final MT
system. This small improvement may be related
to the small coverage of the Apertium dictionar-
ies: the English?French bilingual dictionary has a
low number of entries compared to more mature
language pairs in Apertium which have around 20
times more bilingual entries.
System BLEU
dev
Baseline 27.76
Baseline+Rules 28.06
Table 5: BLEU scores reported by MERT on
English?French newstest2013 for the baseline
SMT system standalone and with automatically
extracted translation rules.
5 Tuning and Decoding
We present in this Section a short selection of our
experiments, amongst 15+ different configura-
tions, conducted on the interpolation of TMs, tun-
ing and decoding parameters. We first interpolate
the four TMs: the baseline, the pseudo in and out-
of-domain, and the translation rules, minimising
the perplexity obtained on the concatenated de-
velopment sets from 2008 to 2012 (newstest2008-
2012). We investigate the use of OSM trained on
pseudo in-domain data only or using all the paral-
lel data available. Finally, we make variations of
4
No other system was found statistically significantly bet-
ter using the sign test at p ? 0.1.
the number of n-bests used by MERT.
Results obtained on the development set new-
stest2013 are reported in Table 6. These scores
show that adding OSM to the interpolated trans-
lation models slightly degrades BLEU. However,
by increasing the number of n-bests considered by
MERT to 200-bests, the SMT system with OSM
outperforms the systems evaluated previously in
our experiments. Adding the synthetic translation
rules degrades BLEU (as indicated by the last row
in the Table), thus we decide to submit two sys-
tems to the shared task: one without and one with
synthetic rules. By submitting a system without
synthetic rules, we also ensure that our SMT sys-
tem is constrained according to the shared task
guidelines.
System BLEU
dev
Baseline 27.76
+ pseudo in + pseudo out 31.93
+ OSM 31.90
+ MERT 200-best 32.21
+ Rules 32.10
Table 6: BLEU scores reported by MERT on
English?French newstest2013 development set.
As MERT is not suitable when a large number
of features are used (our system uses 19 fetures),
we switch to the Margin Infused Relaxed Algo-
rithm (MIRA) for our submitted systems (Watan-
abe et al., 2007). The development set used is
newstest2012, as we aim to select the best decod-
ing parameters according to the scores obtained
when decoding the newstest2013 corpus, after de-
truecasing and de-tokenising using the scripts dis-
tributed with MOSES. This setup allowed us to
compare our results with the participants of the
translation shared task last year. We pick the de-
coding parameters leading to the best results in
terms of BLEU and decode the official test set of
WMT14 newstest2014. The results are reported in
Table 7. Results on newstest2013 show that the de-
coding parameters investigation leads to an over-
all improvement of 0.1 BLEU absolute. The re-
sults on newstest2014 show that adding synthetic
rules did not help improving BLEU and degraded
slightly TER (Snover et al., 2006) scores.
In addition to our English?French submission,
we submitted a French?English translation. Our
French?English MT system is built on the align-
ments obtained from the English?French direc-
tion. The training processes between the two sys-
175
System BLEU13A TER
newstest2013
Best tuning 31.02 60.77
cube-pruning (pop-limit 10000) 31.04 60.71
increased table-limit (100) 31.06 60.77
monotonic reordering 31.07 60.69
Best decoding 31.14 60.66
newstest2014
Best decoding 34.90 54.70
Best decoding + Rules 34.90 54.80
Table 7: Case sensitive results obtained with
our final English?French SMT system on new-
stest2013 when experimenting with different de-
coding parameters. The best parameters are kept
to translate the WMT14 test set (newstest2014)
and official results are reported in the last two
rows.
tems are identical, except for the synthetic rules
which are not extracted for the French?English
direction. Tuning and decoding parameters for
this latter translation direction are the best ones
obtained in our previous experiments on this
shared task. The case-sensitive scores obtained
for French?English on newstest2014 are 35.0
BLEU13A and 53.1 TER, which ranks us at the
fifth position for this translation direction.
6 Conclusion
We have presented the MT systems developed by
the Abu-MaTran project for the WMT14 trans-
lation shared task. We focused on the French?
English language pair and particularly on the
English?French direction. We have used a two-
step data selection process based on bilingual
cross-entropy difference and vocabulary satura-
tion, as well as a novel approach for the extraction
of synthetic translation rules and their use to en-
rich the phrase table. For the LMs and the TMs,
we rely on training individual models per corpus
before interpolating them by minimising perplex-
ity according to the development set. Finally, we
made use of the findings of WMT13 by including
an OSM model.
Our English?French translation system was
ranked second amongst nine teams and a total of
twenty submissions, while our French?English
submission was ranked fifth. As future work,
we plan to investigate the effect of adding to the
phrase table synthetic translation rules based on
larger dictionaries. We also would like to study the
link between OSM and the different decoding pa-
rameters implemented in MOSES, as we observed
inconsistent results in our experiments.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain Adaptation Via Pseudo In-domain
Data Selection. In Proceedings of EMNLP, pages
355?362.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceed-
ings of WMT, pages 22?64.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL/HLT,
pages 1045?1054.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of WMT, pages 112?119.
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nord-
falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-
tonio P?erez-Ortiz, Felipe S?anchez-Mart??nez, Gema
Ram??rez-S?anchez, and Francis M Tyers. 2011.
Apertium: A Free/Open-source Platform for Rule-
based Machine Translation. Machine Translation,
25(2):127?144.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Robert S Garfinkel and George L Nemhauser. 1972.
Integer Programming, volume 4. Wiley New York.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Mod-
ified Kneser-Ney Language Model Estimation. In
Proceedings of ACL, pages 690?696.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL, Interactive Poster and Demonstra-
tion Sessions, pages 177?180.
176
William D. Lewis and Sauleh Eetemadi. 2013. Dra-
matically Reducing Training Data Size Through Vo-
cabulary Saturation. In Proceedings of WMT, pages
281?291.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In Pro-
ceedings of ACL, pages 220?224.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, volume 1, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, pages 311?318.
V??ctor M. S?anchez-Cartagena, Felipe S?anchez-
Mart??nez, and Juan Antonio P?erez-Ortiz. 2011. In-
tegrating Shallow-transfer Rules into Phrase-based
Statistical Machine Translation. In Proceedings of
MT Summit XIII, pages 562?569.
V??ctor M. S?anchez-Cartagena, Juan Antonio P?erez-
Ortiz, and Felipe S?anchez-Mart??nez. 2014. The
UA-Prompsit Hybrid Machine Translation System
for the 2014 Workshop on Statistical Machine
Translation. In Proceedings of WMT.
Rico Sennrich. 2012. Perplexity Minimization for
Translation Model Domain Adaptation in Statisti-
cal Machine Translation. In Proceedings of EACL,
pages 539?549.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of AMTA, pages 223?231.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and Out-
look. In Proceedings of ASRU.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online Large-margin Train-
ing for Statistical Machine Translation. In Proceed-
ings of EMNLP.
177
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215?220,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task
Xiaofeng Wu, Rejwanul Haque*, Tsuyoshi Okita
Piyush Arora, Andy Way, Qun Liu
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
{xf.wu,tokita,parora,away,qliu}@computing.dcu.ie
*Lingo24, Edinburgh, UK
rejwanul.haque@lingo24.com
Abstract
This paper describes the DCU-Lingo24
submission to WMT 2014 for the Hindi-
English translation task. We exploit
miscellaneous methods in our system,
including: Context-Informed PB-SMT,
OOV Word Conversion (OWC), Multi-
Alignment Combination (MAC), Oper-
ation Sequence Model (OSM), Stem-
ming Align and Normal Phrase Extraction
(SANPE), and Language Model Interpola-
tion (LMI). We also describe various pre-
processing steps we tried for Hindi in this
task.
1 Introduction
This paper describes the DCU-Lingo24 submis-
sion to WMT 2014 for the Hindi-English transla-
tion task.
All our experiments on WMT 2014 are built
upon the Moses phrase-based model (PB-SMT)
(Koehn et al., 2007) and tuned with MERT
(Och, 2003). Starting from this baseline system,
we exploit various methods including Context-
Informed PB-SMT (CIPBSMT), zero-shot learn-
ing (Palatucci et al., 2009) using neural network-
based language modelling (Bengio et al., 2000;
Mikolov et al., 2013) for OOV word conversion,
various lexical reordering models (Axelrod et al.,
2005; Galley and Manning, 2008), various Mul-
tiple Alignment Combination (MAC) (Tu et al.,
2012), Operation Sequence Model (OSM) (Dur-
rani et al., 2011) and Language Model Interpola-
tion(LMI).
In the next section, the preprocessing steps are
explained. In Section 3 a detailed explanation of
the technique we exploit is provided. Then in Sec-
tion 4, we provide our experimental results and re-
sultant discussion.
2 Pre-processing Steps
We use all the training data provided for Hindi?
English translation. Following Bojar et al. (2010),
we apply a number of normalisation methods on
the Hindi corpus. The HindEnCorp parallel cor-
pus compiles several sources of parallel data. We
observe that the source-side (Hindi) of the TIDES
data source contains font-related noise, i.e. many
Hindi sentences are a mixture of two different en-
codings: UTF-8
1
and WX
2
notations. We pre-
pared a WX-to-UTF-8 font conversion script for
Hindi which converts all WX encoded characters
into UTF-8, thus removing all WX encoding ap-
pearing in the TIDES data.
We also observe that a portion of the English
training corpus contained the following bracket-
like sequences of characters: -LRB-, -LSB-, -
LCB-, -RRB-, -RSB-, and -RCB-.
3
For consis-
tency, those character sequences in the training
data were replaced by the corresponding brackets.
For English ? both monolingual and the target
side of the bilingual data ? we perform tokeniza-
tion, normalization of punctuation, and truecasing.
For parallel training data, we filter sentences pairs
containing more than 80 tokens on either side and
1
http://en.wikipedia.org/wiki/UTF-8
2
http://en.wikipedia.org/wiki/WX_notation
3
The acronyms stand for (Left|Right)
(Round|Square|Curly) Bracket.
215
sentence pairs with length difference larger than 3
times.
3 Techniques Deployed
3.1 Combination of Various Lexical
Reordering Model (LRM)
Clearly, Hindi and English have quite different
word orders, so we adopt three lexical reordering
models to address this problem. They are word-
based LRM and phrase-based LRM, which mainly
focus on local reordering phenomena, and hierar-
chical phrase-based LRM, which mainly focuses
on longer distance reordering (Galley and Man-
ning, 2008).
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) of Dur-
rani et al. (2011) defines four translation opera-
tions: Generate(X,Y), Continue Source Concept,
Generate Source Only (X) and Generate Identical,
as well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is calculated as in (1):
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
We employ a 9-order OSM in our framework.
3.3 Language Model Interpolation (LMI)
We build a large language model by including data
from the English Gigaword fifth edition, the En-
glish side of the UN corpus, the English side of the
10
9
French?English corpus and the English side of
the Hindi?English parallel data provided by the or-
ganisers. We interpolate language models trained
using each dataset, with the monolingual data pro-
vided split into three parts (news 2007-2013, Eu-
roparl (?) and news commentary) and the weights
tuned to minimize perplexity on the target side of
the devset.
The language models in our systems are trained
with SRILM (Stolcke, 2002). We train a 5-gram
model with Kneser-Ney discounting (Chen and
Goodman, 1996).
3.4 Context-informed PB-SMT
Haque et al. (2011) express a context-dependent
phrase translation as a multi-class classification
problem, where a source phrase with given addi-
tional context information is classified into a dis-
tribution over possible target phrases. The size of
this distribution needs to be limited, and would
ideally omit irrelevant target phrase translations
that the standard PB-SMT (Koehn et al., 2003) ap-
proach would normally include. Following Haque
et al. (2011), we derive a context-informed feature
?
h
mbl
that is expressed as the conditional probabil-
ity of the target phrase e?
k
given the source phrase
?
f
k
and its context information (CI), as in (2):
?
h
mbl
= log P(e?
k
|
?
f
k
,CI(
?
f
k
)) (2)
Here, CI may include any feature that can pro-
vide useful information to disambiguate the given
source phrase. In our experiment, we use CCG su-
pertag (Steedman, 2000) as a contextual features.
CCG supertag expresses the specific syntactic be-
haviour of a word in terms of the arguments it
takes, and more generally the syntactic environ-
ment in which it appears.
We consider the CCG supertags of the context
words, as well as of the focus phrase itself. In our
model, the supertag of a multi-word focus phrase
is the concatenation of the supertags of the words
composing that phrase. We generate a window
of size 2l + 1 features (we set l:=2), including
the concatenated complex supertag of the focus
phrase. Accordingly, the supertag-based contex-
tual information (CI
st
) is described as in (3):
CI
st
(
?
f
k
) = {st(f
i
k
?l
), ..., st(f
i
k
?1
), st(
?
f
k
),
st(f
j
k
+1
), ..., st(f
j
k
+l
)}
(3)
For the Hindi-to-English translation task, we use
part-of-speech (PoS) tags
4
of the source phrase
and the neighbouring words as the contextual fea-
ture, owing to the fact that supertaggers are readily
available only for English.
We use a memory-based machine learning
(MBL) classifier (TRIBL: (Daelemans, 2005))
5
that is able to estimate P(e?
k
|
?
f
k
,CI(
?
f
k
)) by
similarity-based reasoning over memorized
nearest-neighbour examples of source?target
phrase translations. Thus, we derive the feature
?
h
mbl
defined in Equation (2). In addition to
?
h
mbl
,
4
In order to obtain PoS tags of Hindi words,
we used the LTRC shallow parser for Hindi from
http://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin-
4.0.fc8.tar.gz.
5
An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl.
216
we derive a simple two-valued feature
?
h
best
,
defined in Equation (4):
?
h
best
=
{
1 if e?
k
maximizes P(e?
k
|
?
f
k
,CI(
?
f
k
))
u 0 otherwise
(4)
where
?
h
best
is set to 1 when e?
k
is one of the tar-
get phrases with highest probability according to
P(e?
k
|
?
f
k
,CI(
?
f
k
)) for each source phrase
?
f
k
; oth-
erwise
?
h
best
is set to 0.000001. We performed ex-
periments by integrating these two features
?
h
mbl
and
?
h
best
directly into the log-linear model of
Moses. Their weights are optimized using mini-
mum error-rate training (MERT)(Och, 2003) on a
held-out development set for each of the experi-
ments.
3.5 Morphological Segmentation
Haque et al. (2012) applied a morphological suffix
separation process in a Bengali-to-English trans-
lation task and showed that suffix separation sig-
nificantly reduces data sparseness in the Bengali
corpus. They also showed an SMT model trained
on the suffix-stripped training data significantly
outperforms the state-of-the-art PB-SMT baseline.
Like Bengali, Hindi is a morphologically very rich
and highly inflected Indian language. As done
previously for Bengali-to-English (Haque et al.,
2012), we employ a suffix-stripping method for
lemmatizing inflected Hindi words in the WMT
Hindi-to-English translation task. Following Das-
gupta and Ng (2006), we developed an unsu-
pervised morphological segmentation method for
Hindi. We also used a Hindi lightweight stem-
mer (Ramanathan and Rao, 2003) in order to pre-
pare a training corpus with only Hindi stems. We
prepared Hindi-to-English SMT systems on the
both types of training data (i.e. suffix-stripped and
stemmed).
6
3.6 Multi-Alignment Combination (MAC)
Word alignment is a critical component of MT
systems. Various methods for word alignment
have been proposed, and different models can pro-
duce signicantly different outputs. For example,
Tu et al. (2012) demonstrates that the alignment
agreement between the two best-known alignment
tools, namely Giza++(Och and Ney, 2003) and
6
Suffixes were separated and completely removed from
the training data.
the Berkeley aligner
7
(Liang et al., 2006), is be-
low 70%. Taking into consideration the small size
of the the corpus, in order to extract more ef-
fective phrase tables, we concatenate three align-
ments: Giza++ with grow-diag-final-and, Giza++
with intersection, and that derived from the Berke-
ley aligner.
3.7 Stemming Alignment and Normal Phrase
Extraction (SANPE)
The rich morphology of Hindi will cause word
alignment sparsity, which results in poor align-
ment quality. Furthermore, word stemming on
the Hindi side usually results in too many English
words being aligned to one stemmed Hindi word,
i.e. we encounter the problem of phrase over-
extraction. Therefore, we conduct word alignment
with the stemmed version of Hindi, and then at
the phrase extraction step, we replace the stemmed
form with the original Hindi form.
3.8 OOV Word Conversion Method
Our algorithm for OOV word conversion uses the
recently developed zero-shot learning (Palatucci
et al., 2009) using neural network language mod-
elling (Bengio et al., 2000; Mikolov et al., 2013).
The same technique is used in (Okita et al., 2014).
This method requires neither parallel nor compa-
rable corpora, but rather two monolingual corpora.
In our context, we prepare two monolingual cor-
pora on both sides, which are neither parallel nor
comparable, and a small amount of already known
correspondences between words on the source and
target sides (henceforth, we refer to this as the
?dictionary?). Then, we train both sides with the
neural network language model, and use a contin-
uous space representation to project words to each
other on the basis of a small amount of correspon-
dences in the dictionary. The following algorithm
shows the steps involved:
1. Prepare the monolingual source and target
sentences.
2. Prepare the dictionary which consists of U
entries of source and target sentences com-
prising non-stop-words.
3. Train the neural network language model on
the source side and obtain the real vectors of
X dimensions for each word.
7
http://code.google.com/p/berkeleyaligner/
217
4. Train the neural network language model on
the target side and obtain the real vectors of
X dimensions for each word.
5. Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary items in two continuous spaces us-
ing canonical component analysis (CCA).
In our experiments we use U the same as the en-
tries of Wiki corpus, which is provided among
WMT14 corpora, and X as 50. The resulted pro-
jection by this algorithm can be used as the OOV
word conversion which projects from the source
language which among OOV words into the tar-
get language. The overall algorithm which uses
the projection which we build in the above step is
shown in the following.
1. Collect unknown words in the translation out-
puts.
2. Do Hindi named-entity recognition (NER) to
detect noun phrases.
3. If they are noun phrases, do the projection
from each unknown word in the source side
into the target words (We use the projection
prepared in the above steps). If they are not
noun phrases, run the transliteration to con-
vert each of them.
We perform Hindi NER by training CRF++ (Kudo
et al., 2004) using the Hindi named entity corpus,
and use the Hindi shallow parser (Begum et al.,
2008) for preprocessing of the inputs.
4 Results and Discussion
4.1 Data
We conduct our experiments on the standard
datasets released in the WMT14 shared translation
task. We use HindEnCorp
8
(Bojar et al., 2014)
parallel corpus for MT system building. We also
used the CommonCrawl Hindi monolingual cor-
pus (Bojar et al., 2014) in order to build an addi-
tional language model for Hindi.
For the Hindi-to-English direction, we also em-
ployed monolingual English data used in the other
translation tasks for building the English language
model.
8
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4.2 Moses Baseline
We employ a standard Moses PB-SMT model as
our baseline. The Hindi side is preprocessed but
unstemmed. We use Giza++ to perform word
alignment, the phrase table is extracted via the
grow-diag-final-and heuristic and the max-phrase-
length is set to 7.
4.3 Automatic Evaluation
Experiments BLEU
Moses Baseline 8.7
Context-Based 9.4
Context-Based + CommonCrawl LM 11.4
Table 1: BLEU scores of the English-to-Hindi MT
Systems on the WMT test set.
Experiments BLEU
Moses Baseline 10.1
Context-Based 10.1
Suffix-Stripped 10.0
OWC 11.2
OSM 10.3
Three LRMs 10.5
MAC 10.7
SANPE 10.6
LMI 10.9
LMI+SANPE+MAC+ThreeLRMs+OSM 11.7
Table 2: BLEU scores of the Hindi-to-English MT
Systems on the WMT test set.
We prepared a number of MT systems for both
English-to-Hindi and Hindi-to-English, and sub-
mitted their runs in the WMT 2014 Evaluation
Matrix. The BLEU scores of the different English-
to-Hindi MT systems (Moses Baseline, Context-
Based (CCG) MT system, and Context-Based
(CCG) MT system with an additional LM built
on the CommonCrawl Hindi monolingual corpus
(Bojar et al., 2014)) on the WMT 2014 English-
to-Hindi test set are reported in Table 1. As can
be seen from Table 1, Context-Based (CCG) MT
system produces 0.7 BLEU points improvement
(8.04% relative) over the Moses Baseline. When
we add an additional large LM built on the Com-
monCrawl data to the Context-Based (CCG) MT
system, we achieved a 2 BLEU-point improve-
ment (21.3% relative) (cf. last row in Table 1) over
218
the Context-Based (CCG) MT system.
9
The BLEU scores of the different Hindi-to-
English MT systems on the WMT 2014 Hindi-
to-English test set are reported in Table 2. The
first row of Table 2 shows the BLEU score for
the Baseline MT system. We note that the per-
formance of the Context-Based (PoS) MT system
obtains identical performance to the Moses base-
line (10.1 BLEU points) on the WMT 2014 Hindi-
to-English test set.
We employed a source language (Hindi) nor-
malisation technique, namely suffix separation,
but unfortunately this did not bring about any
improvement for the Hindi-to-English translation
task. The improvement gained by individually
employing OSM, three lexical reordering mod-
els, Multi-alignment Combination, Stem-align and
normal Phrase Extraction and Language Model In-
terpolation can be seen in Table 2. Our best sys-
tem is achieved by combining OSM, Three LMR,
MAC, SANPE and LMI, which results in a 1.6
BLEU point improvement over the Baseline.
5 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Rafiya Begum, Samar Husain, Arun Dhwaj,
Dipti Misra Sharma, Lakshmi Bai, and Rajeev
Sangal. 2008. Dependency annotation scheme for
indian languages. In Proceedings of The Third In-
ternational Joint Conference on Natural Language
Processing (IJCNLP).
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems.
Ond Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in english-to-hindi machine translation.
In LREC.
9
Please note that this is an unconstrained submission.
Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak,
A. Tamchyna, and Daniel Zeman. 2014. Hindi-
english and hindi-only corpus for machine transla-
tion. In LREC.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Walter Daelemans. 2005. Memory-based language
processing. Cambridge University Press.
Sajib Dasgupta and Vincent Ng. 2006. Unsupervised
morphological parsing of bengali. Language Re-
sources and Evaluation, 40(3-4):311?330.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal van den
Bosch, and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine translation, 25(3):239?
285.
Rejwanul Haque, Sergio Penkale, Jie Jiang, and Andy
Way. 2012. Source-side suffix stripping for bengali-
to-english smt. In Asian Language Processing
(IALP), 2012 International Conference on, pages
193?196. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
219
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. ArXiv.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way,
and Qun Liu. 2014. Dcu terminology translation
system for medical query subtask at wmt14.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), December.
Ananthakrishnan Ramanathan and Durgesh D Rao.
2003. A lightweight stemmer for hindi. In the Pro-
ceedings of EACL.
Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference Spoken Language Processing,
pages 901?904, Denver, CO.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
220
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 239?245,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The DCU Terminology Translation System for the Medical Query Subtask
at WMT14
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way, Qun Liu
Dublin City University, School of Computing
Glasnevin, Dublin 9
Ireland
{tokita,avahid,away,qliu}@computing.dcu.ie
Abstract
This paper describes the Dublin City
University terminology translation system
used for our participation in the query
translation subtask in the medical trans-
lation task in the Workshop on Statisti-
cal Machine Translation (WMT14). We
deployed six different kinds of terminol-
ogy extraction methods, and participated
in three different tasks: FR?EN and EN?
FR query tasks, and the CLIR task. We
obtained 36.2 BLEU points absolute for
FR?EN and 28.8 BLEU points absolute
for EN?FR tasks where we obtained the
first place in both tasks. We obtained 51.8
BLEU points absolute for the CLIR task.
1 Introduction
This paper describes the terminology translation
system developed at Dublin City University for
our participation in the query translation subtask at
the Workshop on Statistical Machine Translation
(WMT14). We developed six kinds of terminol-
ogy extraction methods for the problem of medi-
cal terminology translation, especially where rare
and new words are considered. We have several
motivations which we address before providing a
description of the actual algorithms undeprinning
our work.
First, terminology translation cannot be seen
just as a simple extension of the translation process
if we use an analogy from human translation. Ter-
minology translation can be considered as more
important and a quite different task than transla-
tion per se, so we need a considerably different
way of solving this particular problem. Bilingual
terminology selection has been claimed to be the
touchstone in human translation, especially where
scientific and legal translation are concerned. Ter-
minology selection is often the hardest and most
time-consuming process in the translation work-
flow. Depending on the particular requirements of
the use-case (Way, 2013), users may not object to
disfluent translations, but will invariably be very
sensitive to the wrong selection of terminology,
even if the meaning of the chosen terms is correct.
This is especially true if this selected terminology
does not match with that preferred by the users
themselves, in which case users are likely to ex-
press some kind of complaint; it may even be that
the entire translation is rejected as sub-standard or
inappropriate on such grounds.
Second, we look at how to handle new and rare
words. If we inspect the process of human trans-
lation more closely, it is easy to identify several
differences compared to the methods used in sta-
tistical MT (SMT). Unless stipulated by the client,
the selection of bilingual terminology can be a
highly subjective process. Accordingly, it is not
necessarily the bilingual term-pair with the highest
probability that is chosen by the human translator.
It is often the case that statistical methods often
forget about or delete less frequent n-grams, but
rely on more frequent n-grams using maximum
likelihood or Maximum A Priori (MAP) meth-
ods. If some terminology is highly suitable, a
human translator can use it quite freely. Further-
more, there are a lot of new words in reality for
which new target equivalents have to be created by
the translators themselves, so the question arises
as to how human translators actually select ap-
propriate new terminology. Transliteration, which
is often supported by many Asian languages in-
cluding Hindi, Japanese, and Chinese, is perhaps
the easiest things to do under such circumstances.
Slight modifications of alphabets/accented charac-
ters can sometimes successfully create a valid new
term, even for European languages.
The remainder of this paper is organized as fol-
lows. Section 2 describes our algorithms. Our
decoding strategy in Section 3. Our experimen-
239
tal settings and results are presented in Section 4,
and we conclude in Section 5.
2 Our Methods
Apart from the conventional statistical approach to
extract bilingual terminology, this medical query
task reminds us of two frequently occurring prob-
lems which are often ignored: (i) ?Can we forget
about terminology which occurs only once in a
corpus??, and (ii) ?What can we do if the termi-
nology does not occur in a corpus?? These two
problems require computationally quite different
approaches than what is usually done in the stan-
dard statistical approach. Furthermore, the medi-
cal query task in WMT14 provides a wide range of
corpora: parallel and monolingual corpora, as well
as dictionaries. These two interesting aspects mo-
tivate our extraction methods which we present in
this section, including one relatively new Machine
Learning algorithm of zero-shot learning arising
from recent developments in the neural network
community (Bengio et al., 2000; Mikolov et al.,
2013b).
2.1 Translation Model
Word alignment (Brown et al., 1993) and phrase
extraction (Koehn et al., 2003) can capture bilin-
gual word- and phrase-pairs with a good deal of
accuracy. We omit further details of these stan-
dard methods which are freely available elsewhere
in the SMT literature (e.g. (Koehn, 2010)).
2.2 Extraction from Parallel Corpora
(Okita et al., 2010) addressed the problem of
capturing bilingual term-pairs from parallel data
which might otherwise not be detected by the
translation model. Hence, the requirement in
Okita et al. is not to use SMT/GIZA++ (Och and
Ney, 2003) to extract term-pairs, which are the
common focus in this medical query translation
task.
The classical algorithm of (Kupiec, 1993) used
in (Okita et al., 2010) counts the statistics of ter-
minology c(e
term
i
, f
term
j
|s
t
) on the source and
the target sides which jointly occur in a sentence
s
t
after detecting candidate terms via POS tag-
ging, which are then summed up over the entire
corpus
?
N
t=1
c(e
term
i
, f
term
j
|s
t
). Then, the al-
gorithm adjusts the length of e
term
i
and f
term
j
.
It can be said that this algorithm captures term-
pairs which occur rather frequently. However, this
apparent strength can also be seen in disadvanta-
geous terms since the search for terminology oc-
curs densely in each of the sentences which in-
creases the computational complexity of this algo-
rithm, and causes the method to take a consider-
able time to run. Furthermore, if we suppose that
most frequent term-pairs are to be extracted via a
standard translation model (as described briefly in
the previous section), our efforts to search among
frequent pairs is not likely to bring about further
gain.
It is possible to approach this in a reverse man-
ner: ?less frequent pairs can be outstanding term
candidates?. Accordingly, if our aim changes to
capture only those less frequent pairs, the situation
changes dramatically. The number of terms we
need to capture is considerably decreased. Many
sentences do not include any terminology at all,
and only a relatively small subset of sentences in-
cludes a few terms, such that term-pairs become
sparse with regard to sentences. Term-pairs can
be found rather easily if a candidate term-pair co-
occurs on the source and the target sides and on
the condition that the items in the term-pair actu-
ally correspond with one another.
This condition can be easily checked in various
ways. One way is to translate the source side of
the targeted pairs with the alignment option in the
Moses decoder (Koehn et al., 2007), which we did
in this evaluation campaign. Another way is to use
asupervised aligner, such as the Berkeley aligner
(Haghighi et al., 2009), to align the targeted pairs
and check whether they are actually aligned or not.
We assume two predefined sets of terms at
the outset, E
term
= {e
term
1
, . . . , e
term
n
} and
F
term
= {f
term
1
, . . . , f
term
n
}. We search for
possible alignment links between the term-pair
only when they co-occur in the same sentence.
One obvious advantage of this approach is the
computational complexity which is fairly low.
Note that the result of (Okita et al., 2010)
shows that the frequency-based approach of (Ku-
piec, 1993) worked well for NTCIR patent termi-
nology (Fujii et al., 2010), which otherwise would
have been difficult to capture via the traditional
SMT/GIZA++ method. In contrast, however, this
did not work well on the Europarl corpus (Koehn,
2005).
240
2.3 Terminology Dictionaries
Terminology dictionaries themselves are obvi-
ously among the most important resources for
bilingual term-pairs. In this medical query transla-
tion subtask, two corpora are provided for this pur-
pose: (i) Unified Medical Language System cor-
pus (UMLS corpus),
1
and (ii) Wiki entries.
2
2.4 Extraction from Terminology
Dictionaries: lower-order n-grams
Terminology dictionaries provide reliable higher-
order n-gram pairs. However, they do not often
provide the correspondences between the lower-
order n-grams contained therein. For example, the
UMLS corpus provides a term-pair of ?abdominal
compartment syndrome ||| syndrome du compar-
timent abdominal? (EN|||FR). However, such ter-
minology dictionaries often do not explicitly pro-
vide the correspondent pairs ?abdominal ||| ab-
dominal? (EN|||FR) or ?syndrome ||| syndrome?
(EN|||FR). Clearly, these terminology dictionaries
implicitly provide the correspondent pairs. Note
that UMLS and Wiki entries provide terminol-
ogy dictionaries. Hence, it is possible to obtain
some suggestion by higher order n-gram models if
we know their alignments between words on the
source and target sides. Algorithm 1 shows the
overall procedure.
Algorithm 1 Lower-order n-gram extraction algo-
rithm
1: Perform monolingual word alignment for
higher-order n-gram pairs.
2: Collect only the reliable alignment pairs (i.e.
discard unreliable alignment pairs).
3: Extract the lower-order word pairs of our in-
terest.
2.5 Extraction from Monolingual Corpora:
Transliteration and Abbreviation
Monolingual corpora can be used in various ways,
including:
1. Transliteration: Many languages support the
fundamental mechanism of between Euro-
pean and Asian languages. Japanese even
supports a special alphabet ? katakana ? for
this purpose. Chinese and Hindi also per-
mit transliteration using their own alphabets.
1
http://www.nlm.nih.gov/research/umls/.
2
http://www.wikipedia.org.
However, even among European languages,
this mechanism makes it possible to find
possible translation counterparts for a given
term. In this query task, we did this only
for the French-to-English direction and only
for words containing accented characters (by
rule-based conversion).
2. Abbreviation: It is often the case that abbre-
viations should be resolved in the same lan-
guage. If the translation includes some ab-
breviation, such as ?C. difficile?, this needs
to be investigated exhaustively in the same
language. However, in the specific domain
of medical terminology, it is quite likely that
possible phrase matches will be successfully
identified.
2.6 Extraction from Monolingual Corpora:
Zero-Shot Learning
Algorithm 2 Algorithm to connect two word em-
bedding space
1: Prepare the monolingual source and target
sentences.
2: Prepare the dictionary which consists of U
entries of source and target sentences among
non-stop-words.
3: Train the neural network language model on
the source side and obtain the continuous
space real vectors of X dimensions for each
word.
4: Train the neural network language model on
the target side and obtain the continuous space
real vectors of X dimensions for each word.
5: Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary in two continuous spaces using
canonical component analysis (CCA).
Another interesting terminology extraction
method requires neither parallel nor comparable
corpora, but rather just monolingual corpora on
both sides (possibly unrelated to each other) to-
gether with a small amount of dictionary entries
which provide already known correspondences
between words on the source and target sides
(henceforth, we refer to this as the ?dictionary?).
This method uses the recently developed zero-shot
learning (Palatucci et al., 2009) using neural net-
work language modelling (Bengio et al., 2000;
Mikolov et al., 2013b). Then, we train both sides
241
with the neural network language model, and use
a continuous space representation to project words
to each other on the basis of a small amount of
correspondences in the dictionary. If we assume
that each continuous space is linear (Mikolov et
al., 2013c), we can connect them via linear projec-
tion (Mikolov et al., 2013b). Algorithm 2 shows
this situation.
In our experiments we use U the same as the
entries of Wiki and X as 50. Algorithm 3 shows
the algorithm to extract the counterpart of OOV
words.
Algorithm 3 Algorithm to extract the counterpart
of OOV words.
1: Prepare the projection by Algorithm 2.
2: Detect unknown words in the translation out-
puts.
3: Do the projection of it (the source word) into
the target word using the trained linear map-
pings in the training step.
3 Decoding Strategy
We deploy six kinds of extraction methods: (1)
translation model, (2) extraction from parallel cor-
pora, (3) terminology dictionaries, (4) lower-order
n-grams, (5) transliteration and abbreviation, and
(6) zero-shot learning. Among these we deploy
four of them ? (2), (4), (5) and (6) ? in a limited
context, while the remaining two are used with-
out any context, mainly owing to time constraints;
only when we did not find the correspondent pairs
via (1) and (3), did we complement this by the
other methods.
The detected bilingual term-pairs using (1) and
(3) can be combined using various methods. One
way is to employ a method similar to (confu-
sion network-based) system combination (Okita
and van Genabith, 2011; Okita and van Genabith,
2012). First we make a lattice: if we regard one
candidate of (1) and two candidates in (3) as trans-
lation outputs where the words of two candidates
in (3) are connected using an underscore (i.e. one
word), we can make a lattice. Then, we can deploy
monotonic decoding over them. If we do this for
the devset and then apply it to the test set, we can
incorporate a possible preference learnt from the
development set, i.e. whether the query transla-
tor prefers method (1) or UMLS/Wiki translation.
MERT process and language model are applied in
a similar manner with (confusion network-based)
system combination (cf. (Okita and van Genabith,
2011)).
We note also that a lattice structure is useful for
handling grammatical coordination. Since queries
are formed by real users, reserved words for
database query such as ?AND? (or ?ET? (FR)) and
?OR? (or ?OU? (FR)) are frequently observed in
the test set. Furthermore, there is repeated use of
?and? more than twice, for example ?douleur ab-
nominal et Helicobacter pylori et cancer?, which
makes it very difficult to detect the correct coor-
dination boundaries. The lattice on the input side
can express such ambiguity at the cost of splitting
the source-side sentence in a different manner.
4 Experimental Results
The baseline is obtained in the following way. The
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 4 is used as the baseline for word
alignment: Model 4 is incrementally trained by
performing 5 iterations of Model 1, 5 iterations of
HMM, 3 iterations of Model 3, and 3 iterations
of Model 4. For phrase extraction the grow-diag-
final heuristics described in (Koehn et al., 2003) is
used to derive the refined alignment from bidirec-
tional alignments. We then perform MERT (Och,
2003) which optimizes parameter settings using
the BLEUmetric (Papineni et al., 2002), while a 5-
gram language model is derived with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained using
SRILM (Stolcke, 2002). We use the whole train-
ing corpora including the WMT14 translation task
corpora as well as medical domain data. UMLS
and Wikipedia are used just as training corpora for
the baseline.
For the extraction from parallel corpora (cf.
Section 2.2), we used Genia tagger (Tsuruoka and
Tsujii, 2005) and the Berkeley parser (Petrov and
Klein, 2007). For the zero-shot learning (cf. Sec-
tion 2.6) we used scikit learn (Pedregosa et al.,
2011), word2vec (Mikolov et al., 2013a), and a
recurrent neural network (Mikolov, 2012). Other
tools used are in-house software.
Table 2 shows the results for the FR?EN query
task. We obtained 36.2 BLEU points absolute,
which is an improvement of 6.3 BLEU point ab-
solute (21.1% relative) over the baseline. Table
3 shows the results for the EN?FR query task.
We obtained 28.8 BLEU points absolute, which
is an improvement of 8.7 BLEU points abso-
242
lute (43% relative) over the baseline. Our sys-
tem was the best system for both of these tasks.
These improvements over the baseline were sta-
tistically significant by a paired bootstrap test
(Koehn, 2004).
Query task FR?EN
Our method baseline
BLEU 36.2 29.9
BLEU cased 30.9 26.5
TER 0.340 0.443
Table 1: Results for FR?EN query task.
extraction LM MERT BLEU (cased)
(1) - (6) all Y 30.9
(1), (2), (3) all Y 30.3
(1), (3), (6) all Y 30.1
(1), (3), (4) all Y 29.1
(1), (3), (5) all Y 29.0
(1) and (3) all Y 29.0
(1) and (3) medical Y 27.5
(1) and (3) WMT Y 27.0
(1) and (3) medical N 25.1
(1) and (3) WMT N 24.3
(1) medical Y 25.9
(1) WMT Y 25.0
Table 2: Table shows the effects of extraction
methods, language model and MERT process. All
the measurements are by BLEU (cased). In this
table, ?medical? indicates a language model built
on all the medical corpora while ?WMT? indicates
a language model built on all the non-medical cor-
pora. Note that some sentence in testset can be
considered as non-medical domain. Extraction
methods (1) - (6) correspond to those described in
Section 2.1 - 2.6.
Table 4 shows the results for CLIR task. We
obtained 51.8 BLEU points absolute, which is an
improvement of 9.4 BLEU point absolute (22.2%
relative) over the baseline. Although CLIR task al-
lowed 10-best lists, our submission included only
1-best list. This resulted in the score of P@5 of
0.348 and P@10 of 0.346 which correspond to
the second place, despite a good result in terms
of BLEU. This is since unlike BLEU score P@5
and P@10 measure whether the whole elements
in reference and hypothesis are matched or not.
We noticed that our submission included a lot of
Query task EN?FR
Our method baseline
BLEU 28.8 20.1
BLEU cased 27.7 18.7
TER 0.483 0.582
Table 3: Results for EN?FR query task.
near miss sentences only in terms of capitaliza-
tion: ?abnominal pain and Helicobacter pylori and
cancer? (reference) and ?abnominal pain and heli-
cobacter pylori and cancer? (submission). These
are counted as incorrect in terms of P@5 and
P@10.
3
Noted that after submission we obtained
the revised score of P@5 of 0.560 and P@10 of
0.560 with the same method but with 2-best lists
which handles the capitalization varieties.
CLIR task FR?EN
Our method baseline
BLEU 51.8 42.2
BLEU cased 46.0 38.3
TER 0.364 0.398
P@5 0.348 (0.560
?
) ?
P@10 0.346 (0.560
?
) ?
NDCG@5 0.306 ?
NDCG@10 0.307 ?
MAP 0.2252 ?
Rprec 0.2358 ?
bpref 0.3659 ?
relRet 1524 ?
Table 4: Results for CLIR task.
5 Conclusion
This paper provides a description of the Dublin
City University terminology translation system for
our participation in the query translation subtask
in the medical translation task in the Workshop on
Statistical Machine Translation (WMT14). We de-
ployed six different kinds of terminology extrac-
tion methods. We obtained 36.2 BLEU points ab-
solute for FR?EN, and 28.8 BLEU points abso-
lute for EN?FR tasks, obtaining first place on both
tasks. We obtained 51.8 BLEU points absolute for
the CLIR task.
3
The method which incorporates variation in capitaliza-
tion in its n-best lists outperforms the best result in terms of
P@5 and P@10.
243
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 07/CE/I1142) as part of CNGL
at Dublin City University.
References
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems, pages
1137?1155.
Peter F. Brown, Vincent J.D Pietra, Stephen A.D.Pietra,
and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, Vol.19, Issue 2,
pages 263?311.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2010. Overview of the
patent translation task at the NTCIR-8 workshop.
In Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pages 293?
302.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised itg models. In In Proceedings of the Confer-
ence of Association for Computational Linguistics,
pages 923?931.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computationa Linguistics (HLT / NAACL
2003), pages 115?124.
Philipp Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit, pages 79?86.
Philipp Koehn. 2010. Statistical machine translation.
Cambridge University Press.
Julian. Kupiec. 1993. An algorithm for finding Noun
phrase correspondences in bilingual corpora. In
Proceedings of the 31st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 17?22.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at International Conference on Learning Represen-
tations.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. ArXiv:1309.4168.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT 2005), pages 746?
751.
Tomas Mikolov. 2012. Statistical language models
based on neural networks. PhD thesis at Brno Uni-
versity of Technology.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Tsuyoshi Okita and Josef van Genabith. 2011. DCU
Confusion Network-based System Combination for
ML4HMT. Shared Task on Applying Machine
Learning techniques to optimising the division of
labour in Hybrid MT (ML4HMT-2011, collocated
with LIHMT-2011), pages 93?98.
Tsuyoshi Okita and Josef van Genabith. 2012. Mini-
mum Bayes Risk Decoding with Enlarged Hypoth-
esis Space in System Combination. In Proceed-
ings of 13th International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING 2012), pages 40?51.
Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette
Graham, and Andy Way. 2010. Multi-word
expression-sensitive word alignment. In Proceed-
ings of the Fourth International Workshop On Cross
Ling ual Information Access (CLIA2010, collocated
with COLING2010), pages 26?34.
244
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), pages 1410?1418.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: A Method For Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceedings
of AAAI (Nectar Track), pages 1663?1666.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of the
Conference on Human Language Technology / Em-
pirical Methods on Natural Language Processing
(HLT/EMNLP 2005), pages 467?474.
Andy Way. 2013. Traditional and emerging use-cases
for machine translation. In Proceedings of Translat-
ing and the Computer 35.
245
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 313?321,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Referential Translation Machines for Predicting Translation Quality
Ergun Bic?ici
Centre for Next Generation Localisation
School of Computing
Dublin City University, Dublin, Ireland.
ergun.bicici@computing.dcu.ie
Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University, Dublin, Ireland.
away@computing.dcu.ie
Abstract
We use referential translation machines
(RTM) for quality estimation of translation
outputs. RTMs are a computational model
for identifying the translation acts between
any two data sets with respect to interpre-
tants selected in the same domain, which
are effective when making monolingual
and bilingual similarity judgments. RTMs
achieve top performance in automatic, ac-
curate, and language independent predic-
tion of sentence-level and word-level sta-
tistical machine translation (SMT) qual-
ity. RTMs remove the need to access any
SMT system specific information or prior
knowledge of the training data or models
used when generating the translations and
achieve the top performance in WMT13
quality estimation task (QET13). We im-
prove our RTM models with the Parallel
FDA5 instance selection model, with ad-
ditional features for predicting the trans-
lation performance, and with improved
learning models. We develop RTM mod-
els for each WMT14 QET (QET14) sub-
task, obtain improvements over QET13 re-
sults, and rank 1st in all of the tasks and
subtasks of QET14.
1 Introduction
We use referential translation machines (RTM) for
quality estimation of translation outputs, which is
a computational model for identifying the acts of
translation for translating between any given two
data sets with respect to a reference corpus se-
lected in the same domain. RTMs reduce our de-
pendence on any task dependent resource. Predic-
tion of translation quality is important because the
expected translation performance can help in esti-
mating the effort required for correcting the trans-
lations during post-editing by human translators.
Bicici et al. (2013) develop the Machine Trans-
lation Performance Predictor (MTPP), a state-of-
the-art, language independent, and SMT system
extrinsic machine translation performance predic-
tor, which can predict translation quality by look-
ing at the test source sentences and becomes the
2nd overall after also looking at the translation
outputs as well in QET12 (Callison-Burch et al.,
2012). RTMs achieve the top performance in
QET13 (Bojar et al., 2013), ranking 1st or 2nd in
all of the subtasks. RTMs rank 1st in all of the
tasks and subtasks of QET14 (Bojar et al., 2014).
Referential translation models (Section 2)
present an accurate and language independent so-
lution for predicting the performance of natural
language tasks such as the quality estimation of
translation. We improve our RTM models (Bic?ici,
2013) by:
? using a parameterized, fast implementation
of FDA, FDA5, and our Parallel FDA5 in-
stance selection model (Bic?ici et al., 2014),
? better modeling of the language in which
similarity judgments are made with improved
optimization and selection of the LM data,
? increased feature set for also modeling the
structural properties of sentences,
? extended learning models.
2 Referential Translation Machine
(RTM)
Referential translation machines provide a compu-
tational model for quality and semantic similarity
judgments in monolingual and bilingual settings
using retrieval of relevant training data (Bic?ici,
2011; Bic?ici and Yuret, 2014) as interpretants for
reaching shared semantics (Bic?ici, 2008). RTMs
achieve top performance when predicting the qual-
ity of translations in QET14 and QET13 (Bic?ici,
313
2013), top performance when predicting mono-
lingual cross-level semantic similarity (Jurgens
et al., 2014), good performance when evaluat-
ing the semantic relatedness of sentences and
their entailment (Marelli et al., 2014), and a
language independent solution and good perfor-
mance when judging the semantic similarity of
sentences (Agirre et al., 2014; Bic?ici and Way,
2014).
RTM is a computational model for identifying
the acts of translation for translating between any
given two data sets with respect to a reference
corpus selected in the same domain. An RTM
model is based on the selection of interpretants,
data close to both the training set and the test set,
which allow shared semantics by providing con-
text for similarity judgments. In semiotics, an in-
terpretant I interprets the signs used to refer to the
real objects (Bic?ici, 2008). Each RTM model is
a data translation model between the instances in
the training set and the test set. We use the Parallel
FDA5 (Feature Decay Algorithms) instance selec-
tion model for selecting the interpretants (Bic?ici
et al., 2014; Bic?ici and Yuret, 2014) this year,
which allows efficient parameterization, optimiza-
tion, and implementation of FDA, and build an
MTPP model (Section 2.1). We view that acts of
translation are ubiquitously used during commu-
nication:
Every act of communication is an act of
translation (Bliss, 2012).
Given a training set train, a test set test, and
some corpus C, preferably in the same domain as
the training and test sets, the RTM steps are:
1. FDA5(train,test, C)? I
2. MTPP(I,train)? F
train
3. MTPP(I,test)? F
test
4. learn(M,F
train
)?M
5. predict(M,F
test
)? q?
Step 1 selects the interpretants, I, relevant to both
the training and test data. Steps 2 and 3 use I
to map train and test to a new space where
similarities between translation acts can be derived
more easily. Step 4 trains a learning modelM over
the training features, F
train
, and Step 5 obtains
the predictions. RTM relies on the representative-
ness of I as a medium for building data translation
models between train and test.
Our encouraging results in QET provides a
greater understanding of the acts of translation we
ubiquitously use and how they can be used to pre-
dict the performance of translation and judging the
semantic similarity between text. RTM and MTPP
models are not data or language specific and their
modeling power and good performance are appli-
cable in different domains and tasks.
2.1 The Machine Translation Performance
Predictor (MTPP)
MTPP (Bic?ici et al., 2013) is a state-of-the-art and
top performing machine translation performance
predictor, which uses machine learning models
over features measuring how well the test set
matches the training set to predict the quality of
a translation without using a reference translation.
2.2 MTPP Features for Translation Acts
MTPP measures the coverage of individual test
sentence features found in the training set and
derives indicators of the closeness of test sen-
tences to the available training data, the difficulty
of translating the sentence, and the presence of
acts of translation for data transformation. Fea-
ture functions use statistics involving the training
set and the test sentences to determine their close-
ness. Since they are language independent, MTPP
allows quality estimation to be performed extrin-
sically. MTPP uses n-gram features defined over
text or common cover link (CCL) (Seginer, 2007)
structures as the basic units of information over
which similarity calculations are made. Unsuper-
vised parsing with CCL extracts links from base
words to head words, representing the grammati-
cal information instantiated in the training and test
data.
We extend the MTPP model we used last
year (Bic?ici, 2013) in its learning module and the
features included. Categories for the features (S
for source, T for target) used are listed below
where the number of features are given in brackets
for S and T, {#S, #T}, and the detailed descriptions
for some of the features are presented in (Bic?ici et
al., 2013). The number of features for each task
differs since we perform an initial feature selection
step on the tree structural features (Section 2.3).
The number of features are in the range 337?437.
? Coverage {56, 54}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Perplexity {45, 45}: Measures the fluency of
the sentences according to language models
314
(LM). We use both forward ({30}) and back-
ward ({15}) LM features for S and T.
? TreeF {0, 10-110}: 10 base features and up
to 100 selected features of T among parse tree
structures (Section 2.3).
? Retrieval Closeness {16, 12}: Measures the
degree to which sentences close to the test set
are found in the selected training set, I, using
FDA (Bic?ici and Yuret, 2011a) and BLEU,
F
1
(Bic?ici, 2011), dice, and tf-idf cosine sim-
ilarity metrics.
? IBM2 Alignment Features {0, 22}: Calcu-
lates the sum of the entropy of the dis-
tribution of alignment probabilities for S
(
?
s?S
?p log p for p = p(t|s) where s and
t are tokens) and T, their average for S and
T, the number of entries with p ? 0.2 and
p ? 0.01, the entropy of the word align-
ment between S and T and its average, and
word alignment log probability and its value
in terms of bits per word. We also com-
pute word alignment percentage as in (Ca-
margo de Souza et al., 2013) and potential
BLEU, F
1
, WER, PER scores for S and T.
? IBM1 Translation Probability {4, 12}: Cal-
culates the translation probability of test
sentences using the selected training set,
I (Brown et al., 1993).
? Feature Vector Similarity {8, 8}: Calculates
similarities between vector representations.
? Entropy {2, 8}: Calculates the distributional
similarity of test sentences to the training set
over top N retrieved sentences (Bic?ici et al.,
2013).
? Length {6, 3}: Calculates the number of
words and characters for S and T and their
average token lengths and their ratios.
? Diversity {3, 3}: Measures the diversity of
co-occurring features in the training set.
? Synthetic Translation Performance {3, 3}:
Calculates translation scores achievable ac-
cording to the n-gram coverage.
? Character n-grams {5}: Calculates cosine
between character n-grams (for n=2,3,4,5,6)
obtained for S and T (B?ar et al., 2012).
? Minimum Bayes Retrieval Risk {0, 4}: Cal-
culates the translation probability for the
translation having the minimum Bayes risk
among the retrieved training instances.
? Sentence Translation Performance {0, 3}:
Calculates translation scores obtained ac-
cording to q(T,R) using BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), or
F
1
(Bic?ici and Yuret, 2011b) for q.
? LIX {1, 1}: Calculates the LIX readability
score (Wikipedia, 2013; Bj?ornsson, 1968) for
S and T.
1
For Task 1.1, we have additionally used com-
parative BLEU, NIST, and F
1
scores as additional
features, which are obtained by comparing the
translations with each other and averaging the re-
sult (Bic?ici, 2011).
2.3 Bracketing Tree Structural Features
We use the parse tree outputs obtained by CCL
to derive features based on the bracketing struc-
ture. We derive 5 statistics based on the geometric
properties of the parse trees: number of brackets
used (numB), depth (depthB), average depth (avg
depthB), number of brackets on the right branches
over the number of brackets on the left (R/L)
2
, av-
erage right to left branching over all internal tree
nodes (avg R/L). The ratio of the number of right
to left branches shows the degree to which the sen-
tence is right branching or not. Additionally, we
capture the different types of branching present
in a given parse tree identified by the number of
nodes in each of its children.
Table 1 depicts the parsing output obtained by
CCL for the following sentence from WSJ23
3
:
Many fund managers argue that now ?s the time
to buy .
We use Tregex (Levy and Andrew, 2006) for vi-
sualizing the output parse trees presented on the
left. The bracketing structure statistics and fea-
tures are given on the right hand side. The root
node of each tree structural feature represents the
number of times that feature is present in the pars-
ing output of a document.
3 RTM in the Quality Estimation Task
We participate in all of the four challenges of the
quality estimation task (QET) (Bojar et al., 2014),
which include English to Spanish (en-es), Span-
ish to English (es-en), English to German (en-
de), and German to English (de-en) translation di-
rections. There are two main categories of chal-
lenges: sentence-level prediction (Task 1.*) and
1
LIX=
A
B
+ C
100
A
, where A is the number of words, C is
words longer than 6 characters, B is words that start or end
with any of ?.?, ?:?, ?!?, ??? similar to (Hagstr?om, 2012).
2
For nodes with uneven number of children, the nodes in
the odd child contribute to the right branches.
3
Wall Street Journal (WSJ) corpus section 23, distributed
with Penn Treebank version 3 (Marcus et al., 1993).
315
CCL
numB depthB avg depthB R/L avg R/L
24.0 9.0 0.375 2.1429 3.401
2
1 1
1
1 13
1
1 2
1
1 8
1
2 10
1
3 1
1
3 4
1
5 1
1
7 15
Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).
word-level prediction (Task 2). Task 1.1 is about
predicting post-editing effort (PEE), Task 1.2 is
about predicting HTER (human-targeted transla-
tion edit rate) (Snover et al., 2006) scores of trans-
lations, Task 1.3 is about predicting post-editing
time (PET), and Task 2 is about binary, ternary, or
multi-class classification of word-level quality.
For each task, we develop individual RTM mod-
els using the parallel corpora and the LM corpora
distributed by the translation task (WMT14) (Bo-
jar et al., 2014) and the LM corpora provided by
LDC for English (Parker et al., 2011) and Span-
ish (
?
Angelo Mendonc?a, 2011)
4
. The parallel cor-
pora contain 4.5M sentences for de-en with 110M
words for de and 116M words for en and 15.1M
sentences for en-es with 412M words for en and
462M words for es. We do not use any resources
provided by QET including data, software, or
baseline features. Instance selection for the train-
ing set and the language model (LM) corpus is
handled by parallel FDA5 (Bic?ici et al., 2014),
whose parameters are optimized for each transla-
tion task. LM are trained using SRILM (Stolcke,
2002). We tokenize and true-case all of the cor-
pora. The true-caser is trained on all of the avail-
able training corpus using Moses (Koehn et al.,
2007). Table 2 lists the number of sentences in
the training and test sets for each task.
For each task or subtask, we select 375 thousand
(K) training instances from the available parallel
training corpora as interpretants for the individual
RTM models using parallel FDA5. We add the
selected training set to the 3 million (M) sentences
selected from the available monolingual corpora
for each LM corpus. The statistics of the training
data selected by and used as interpretants in the
4
English Gigaword 5th, Spanish Gigaword 3rd edition.
Task Train Test
Task 1.1 (en-es) 3816 600
Task 1.1 (es-en) 1050 450
Task 1.1 (en-de) 1400 600
Task 1.1 (de-en) 1050 450
Task 1.2 (en-es) 896 208
Task 1.3 (en-es) 650 208
Task 2 (en-es) 1957 382
Task 2 (es-en) 900 150
Task 2 (en-de) 715 150
Task 2 (de-en) 350 100
Table 2: Number of sentences in different tasks.
RTM models is given in Table 3. The details of
instance selection with parallel FDA5 are provided
in (Bic?ici et al., 2014).
Task S T
Task 1.1 (en-es) 6.2 6.9
Task 1.1 (es-en) 7.9 7.4
Task 1.1 (en-de) 6.1 6
Task 1.1 (de-en) 6.9 6.4
Task 1.2 (en-es) 6.1 6.7
Task 1.3 (en-es) 6.2 6.8
Task 2 (en-es) 6.2 6.8
Task 2 (es-en) 7.5 7
Task 2 (en-de) 5.9 5.9
Task 2 (de-en) 6.3 6.8
Table 3: Number of words in I (in millions) se-
lected for each task (S for source, T for target).
3.1 Learning Models and Optimization:
We use ridge regression (RR), support vector re-
gression (SVR) with RBF (radial basis functions)
kernel (Smola and Sch?olkopf, 2004), and ex-
316
Task Translation Model r RMSE MAE RAE
Task1.1
es-en FS-RR 0.3512 0.6394 0.5319 0.9114
es-en PLS-RR 0.3579 0.6746 0.5488 0.9405
en-de PLS-TREE 0.2922 0.7496 0.6223 0.9404
en-de TREE 0.2845 0.7485 0.6241 0.9431
en-es TREE 0.4485 0.619 0.45 0.9271
en-es PLS-TREE 0.4354 0.6213 0.4723 0.973
de-en RR 0.3415 0.7475 0.6245 0.9653
de-en PLS-RR 0.3561 0.7711 0.6236 0.9639
Task1.2
en-es SVR 0.4769 0.203 0.1378 0.8443
en-es TREE 0.4708 0.2031 0.1372 0.8407
Task1.3
en-es SVR 0.6974 21543 14866 0.6613
en-es RR 0.6991 21226 15325 0.6817
Table 4: Training performance of the top 2 individual RTM models prepared for different tasks.
tremely randomized trees (TREE) (Geurts et al.,
2006) as the learning models. TREE is an en-
semble learning method over randomized decision
trees. These models learn a regression function
using the features to estimate a numerical target
value. We also use these learning models after
a feature subset selection with recursive feature
elimination (RFE) (Guyon et al., 2002) or a di-
mensionality reduction and mapping step using
partial least squares (PLS) (Specia et al., 2009),
both of which are described in (Bic?ici et al., 2013).
We optimize the learning parameters, the num-
ber of features to select, the number of dimen-
sions used for PLS, and the parameters for paral-
lel FDA5. More detailed descriptions of the opti-
mization processes are given in (Bic?ici et al., 2013;
Bic?ici et al., 2014). We optimize the learning pa-
rameters by selecting ? close to the standard de-
viation of the noise in the training set (Bic?ici,
2013) since the optimal value for ? is shown to
have linear dependence to the noise level for dif-
ferent noise models (Smola et al., 1998). We select
the top 2 systems according to their performance
on the training set. For Task 2, we use both Global
Linear Models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) we developed last
year (Bic?ici, 2013). GLM relies on Viterbi de-
coding, perceptron learning, and flexible feature
definitions. GLMd extends the GLM framework
by parallel perceptron training (McDonald et al.,
2010) and dynamic learning with adaptive weight
updates in the perceptron learning algorithm:
w = w + ? (?(x
i
, y
i
)? ?(x
i
,
?
y)) , (1)
where ? returns a global representation for in-
stance i and the weights are updated by ?, which
dynamically decays the amount of the change dur-
ing weight updates at later stages and prevents
large fluctuations with updates.
3.2 Training Results
We use mean absolute error (MAE), relative
absolute error (RAE), root mean squared error
(RMSE), and correlation (r) to evaluate (Bic?ici,
2013). DeltaAvg (Callison-Burch et al., 2012) cal-
culates the average quality difference between the
top n ? 1 quartiles and the overall quality for the
test set. Table 4 provides the training results.
3.3 Test Results
Task 1.1: Predicting the Post-Editing Effort for
Sentence Translations: Task 1.1 is about pre-
dicting post-editing effort (PEE) and their rank-
ing. The results on the test set are given in Ta-
ble 5 where QuEst (Shah et al., 2013) SVR lists
the baseline system results. Rank lists the overall
ranking in the task out of about 10 submissions.
We obtain the rankings by sorting according to the
predicted scores and randomly assigning ranks in
case of ties. RTMs with SVR PLS learning is able
to achieve the top rank in this task.
Task 1.2: Predicting HTER of Sentence Trans-
lations Task 1.2 is about predicting HTER
(human-targeted translation edit rate) (Snover et
al., 2006), where case insensitive translation edit
rate (TER) scores obtained by TERp (Snover et
al., 2009) and their ranking. We derive features
over sentences that are true-cased. The results on
the test set are given in Table 6 where the ranks are
out of about 11 submissions. We are also able to
achieve the top ranking in this task.
317
Ranking Translations DeltaAvg r Rank
en-es
TREE 0.26 -0.41 1
PLS-TREE 0.26 -0.38 2
QuEst SVR 0.14 -0.22
es-en
PLS-RR 0.20 -0.35 2
FS-RR 0.19 -0.36 3
QuEst SVR 0.12 -0.21
en-de
TREE 0.39 -0.54 1
PLS-TREE 0.33 -0.42 2
QuEst SVR 0.23 -0.34
de-en
RR 0.38 -0.51 1
PLS-RR 0.35 -0.45 2
QuEst SVR 0.21 -0.25
Scoring Translations MAE RMSE Rank
en-es
TREE 0.49 0.61 1
PLS-TREE 0.49 0.61 2
QuEst SVR 0.52 0.66
es-en
FS-RR 0.53 0.64 1
PLS-RR 0.55 0.71 2
QuEst SVR 0.57 0.68
en-de
TREE 0.58 0.68 1
PLS-TREE 0.60 0.71 2
QuEst SVR 0.64 0.76
de-en
RR 0.55 0.67 1
PLS-RR 0.57 0.74 2
QuEst SVR 0.65 0.78
Table 5: RTM-DCU Task1.1 results on the test set
and baseline results.
Ranking Translations DeltaAvg r Rank
en-es
SVR 9.31 0.53 1
TREE 8.57 0.48 2
QuEst SVR 5.08 0.31
Scoring Translations MAE RMSE Rank
en-es
SVR 13.40 16.69 2
TREE 14.03 17.48 4
QuEst SVR 15.23 19.48
Table 6: RTM-DCU Task1.2 results on the test set
and baseline results.
Task 1.3: Predicting Post-Editing Time for Sen-
tence Translations Task 1.3 involves the predic-
tion of the post-editing time (PET) for a translator
to post-edit the MT output. The results on the test
set are given in Table 7 where the ranks are out of
about 10 submissions. RTMs become the top in all
metrics with RR and SVR learning models.
Task 2: Prediction of Word-level Translation
Quality Task 2 is about binary, ternary, or multi-
class classification of word-level quality. We de-
velop individual RTM models for each subtask and
use the GLM and GLMd learning models (Bic?ici,
2013), for predicting the quality at the word-level.
The features used are similar to last year?s (Bic?ici,
2013) and broadly categorized as CCL links, word
context based on surrounding words, word align-
ments, word lengths, word locations, word pre-
fixes and suffixes, and word forms (i.e. capital,
Ranking Translations DeltaAvg r Rank
en-es
RR 17.02 0.68 1
SVR 16.60 0.67 2
QuEst SVR 14.71 0.57
Scoring Translations MAE RMSE Rank
en-es
SVR 16.77 26.17 1
RR 17.50 25.97 7
QuEst SVR 21.49 34.28
Table 7: RTM-DCU Task1.3 results on the test set
and baseline results.
contains digit or punctuation).
The results on the test set are given in Table 8
where the ranks are out of about 8 submissions.
RTMs with GLM or GLMd learning becomes the
top this task as well.
Model
Binary Ternary Multi-class
wF
1
Rank wF
1
Rank wF
1
Rank
en-es
GLM 0.351 6 0.299 5 0.268 1
GLMd 0.329 7 0.266 6 0.032 7
es-en
GLM 0.269 2 0.220 2 0.087 1
GLMd 0.291 1 0.239 1 0.082 2
en-de
GLM 0.453 1 0.211 2 0.150 1
GLMd 0.369 2 0.219 1 0.125 2
en-es
GLM 0.261 1 0.083 2 0.024 2
GLMd 0.230 2 0.086 1 0.031 1
Table 8: RTM-DCU Task 2 results on the test set.
wF
1
is the average weighted F
1
score.
3.4 RTMs Across Tasks and Years
We compare the difficulty of tasks according to the
RAE levels achieved. RAE measures the error rel-
ative to the error when predicting the actual mean.
A high RAE is an indicator that the task is hard. In
Table 9, we list the test results including the RAE
obtained for different tasks and subtasks including
RTM results at QET13 (Bic?ici, 2013). The best
results are obtained for Task 1.3, which shows that
we can only reduce the error with respect to know-
ing and predicting the mean by about 28%.
4 Conclusion
Referential translation machines achieve top per-
formance in automatic, accurate, and language in-
dependent prediction of sentence-level and word-
level statistical machine translation (SMT) qual-
ity. RTMs remove the need to access any SMT
system specific information or prior knowledge of
the training data or models used when generating
the translations.
318
Task Translation Model r RMSE MAE RAE
Task1.1
es-en FS-RR 0.3285 0.6373 0.5308 0.9
es-en PLS-RR 0.3105 0.7124 0.5549 0.9409
en-de PLS-TREE 0.4427 0.7091 0.6028 0.8883
en-de TREE 0.5256 0.6788 0.5838 0.8602
en-es TREE 0.4087 0.6114 0.4938 1.0983
en-es PLS-TREE 0.4163 0.6084 0.4852 1.0794
de-en RR 0.5399 0.6735 0.5513 0.8204
de-en PLS-RR 0.4878 0.737 0.567 0.8437
Task1.2
en-es SVR 0.5499 0.1669 0.134 0.8532
en-es TREE 0.5175 0.1748 0.1403 0.8931
Task1.3
en-es SVR 0.6336 26174 16770 0.7223
en-es RR 0.6359 25966 17496 0.7536
QET13 Task1.1 en-es
PLS-SVR 0.5596 0.1683 0.1326 0.8849
SVR 0.5082 0.1728 0.1385 0.924
QET13 Task1.3 en-es
PLS-SVR 0.6752 86.62 49.62 0.6919
SVR 0.6682 90.36 49.21 0.6862
Table 9: Test performance of the top 2 individual RTM models prepared for different tasks and RTM
results from QET13 on similar tasks (Bic?ici, 2013).
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University and in part by the
European Commission through the QTLaunchPad
FP7 project (No: 296347). We also thank the
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational
facilities and support.
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics ? Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 435?440, Montr?eal,
Canada, 7-8 June. Association for Computational
Linguistics.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In SemEval-2014: Semantic Evaluation Exercises
- International Workshop on Semantic Evaluation,
Dublin, Ireland, 23-24 August.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 272?283,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system
for machine translation, system combination, and
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 323?329,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation, 27:171?192, December.
Ergun Bic?ici, Qun Liu, and Andy Way. 2014. Par-
allel FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, Baltimore, USA, June. Association for Compu-
tational Linguistics.
319
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Referential translation machines
for quality estimation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multia-
gent and Grid Systems.
Carl Hugo Bj?ornsson. 1968. L?asbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc. of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 1?44, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Ond?rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proc. of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Jos?e Guilherme Camargo de Souza, Christian Buck,
Marco Turchi, and Matteo Negri. 2013. FBK-
UEdin participation to the WMT13 quality estima-
tion shared task. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 352?
358, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389?422.
Kenth Hagstr?om. 2012. Swedish readabil-
ity calculator. https://github.com/keha76/Swedish-
Readability-Calculator.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland, August.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177?180, Prague, Czech Republic, June.
Roger Levy and Galen Andrew. 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. In Proceedings of the fifth interna-
tional conference on Language Resources and Eval-
uation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313?330, June.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. SemEval-2014 Task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland, August.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464, Los Angeles, California,
June. Association for Computational Linguistics.
320
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Yoav Seginer. 2007. Learning Syntactic Structure.
Ph.D. thesis, Universiteit van Amsterdam.
Kashif Shah, Eleftherios Avramidis, Ergun Bic?ici, and
Lucia Specia. 2013. Quest - design, implementation
and extensions of a framework for machine transla-
tion quality estimation. Prague Bull. Math. Linguis-
tics, 100:19?30.
Alex J. Smola and Bernhard Sch?olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222, August.
A. J. Smola, N. Murata, B. Sch?olkopf, and K.-R.
M?uller. 1998. Asymptotically optimal choice of
?-loss for support vector machines. In L. Niklas-
son, M. Boden, and T. Ziemke, editors, Proceedings
of the International Conference on Artificial Neural
Networks, Perspectives in Neural Computing, pages
105?110, Berlin. Springer.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas,.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 259?268, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Con-
ference of the European Association for Machine
Translation (EAMT), pages 28?35, Barcelona, May.
EAMT.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing, pages 901?904.
Wikipedia. 2013. Lix.
http://en.wikipedia.org/wiki/LIX.
David Graff Denise DiPersio
?
Angelo Mendonc?a,
Daniel Jaquette. 2011. Spanish Gigaword third edi-
tion, Linguistic Data Consortium.
321
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 122?131,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Transformation and Decomposition for Efficiently Implementing and
Improving Dependency-to-String Model In Moses
Liangyou Li
?
, Jun Xie
?
, Andy Way
?
and Qun Liu
??
?
CNGL Centre for Global Intelligent Content, School of Computing
Dublin City University, Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
{liangyouli,away,qliu}@computing.dcu.ie
junxie@ict.ac.cn
Abstract
Dependency structure provides grammat-
ical relations between words, which have
shown to be effective in Statistical Ma-
chine Translation (SMT). In this paper, we
present an open source module in Moses
which implements a dependency-to-string
model. We propose a method to trans-
form the input dependency tree into a cor-
responding constituent tree for reusing the
tree-based decoder in Moses. In our ex-
periments, this method achieves compara-
ble results with the standard model. Fur-
thermore, we enrich this model via the
decomposition of dependency structure,
including extracting rules from the sub-
structures of the dependency tree during
training and creating a pseudo-forest in-
stead of the tree per se as the input dur-
ing decoding. Large-scale experiments
on Chinese?English and German?English
tasks show that the decomposition ap-
proach improves the baseline dependency-
to-string model significantly. Our sys-
tem achieves comparable results with the
state-of-the-art hierarchical phrase-based
model (HPB). Finally, when resorting to
phrasal rules, the dependency-to-string
model performs significantly better than
Moses HPB.
1 Introduction
Dependency structure models relations between
words in a sentence. Such relations indicate
the syntactic function of one word to another
word. As dependency structure directly encodes
semantic information and has the best inter-lingual
phrasal cohesion properties (Fox, 2002), it is be-
lieved to be helpful to translation.
In recent years, dependency structure has been
widely used in SMT. For example, Shen et al.
(2010) present a string-to-dependency model by
using the dependency fragments of the neighbour-
ing words on the target side, which makes it easier
to integrate a dependency language model. How-
ever such string-to-tree systems run slowly in cu-
bic time (Huang et al., 2006).
Another example is the treelet approach
(Menezes and Quirk, 2005; Quirk et al., 2005),
which uses dependency structure on the source
side. Xiong et al. (2007) extend the treelet ap-
proach to allow dependency fragments with gaps.
As the treelet is defined as an arbitrary connected
sub-graph, typically both substitution and inser-
tion operations are adopted for decoding. How-
ever, as translation rules based on the treelets
do not encode enough reordering information di-
rectly, another heuristic or separate reordering
model is usually needed to decide the best target
position of the inserted words.
Different from these works, Xie et al. (2011)
present a dependency-to-string (Dep2Str) model,
which extracts head-dependent (HD) rules from
word-aligned source dependency trees and target
strings. As this model specifies reordering infor-
mation in the HD rules, during translation only the
substitution operation is needed, because words
are reordered simultaneously with the rule being
applied. Meng et al. (2013) and Xie et al. (2014)
extend the model by augmenting HD rules with the
help of either constituent tree or fixed/float struc-
ture (Shen et al., 2010). Augmented rules are cre-
ated by the combination of two or more nodes in
122
the HD fragment, and are capable of capturing
translations of non-syntactic phrases. However,
the decoder needs to be changed correspondingly
to handle these rules.
Attracted by the simplicity of the Dep2Str
model, in this paper we describe an easy way to
integrate the model into the popular translation
framework Moses (Koehn et al., 2007). In or-
der to share the same decoder with the conven-
tional syntax-based model, we present an algo-
rithm which transforms a dependency tree into a
corresponding constituent tree which encodes de-
pendency information in its non-leaf nodes and is
compatible with the Dep2Str model. In addition,
we present a method to decompose a dependency
structure (HD fragment) into smaller parts which
enrich translation rules and also allow us to cre-
ate a pseudo-forest as the input. ?Pseudo? means
the forest is not obtained by combining several
trees from a parser, but rather that it is created
based on the decomposition of an HD fragment.
Large-scale experiments on Chinese?English and
German?English tasks show that the transforma-
tion and decomposition are effective for transla-
tion.
In the remainder of the paper, we first describe
the Dep2Str model (Section 2). Then we describe
how to transform a dependency tree into a con-
stituent tree which is compatible with the Dep2Str
model (Section 3). The idea of decomposition in-
cluding extracting sub-structural rules and creat-
ing a pseudo-forest is presented in Section 4. Then
experiments are conducted to compare translation
results of our approach with the state-of-the-art
HPB model (Section 5). We conclude in Section 6
and present avenues for future work.
2 Dependency-to-String Model
In the Dep2Str model (Xie et al., 2011), the HD
fragment is the basic unit. As shown in Figure
1, in a dependency tree, each non-leaf node is the
head of some other nodes (dependents), so an HD
fragment is composed of a head node and all of its
dependents.
1
In this model, there are two kinds of rules for
translation. One is the head rule which specifies
the translation of a source word:
Juxing
??? holds
1
In this paper, HD fragment of a node means the HD frag-
ment with this node as the head. Leaf nodes have no HD
fragments.
Boliweiya
????/NN
Juxing
??/VV
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Figure 1: Example of a dependency tree, with
head-dependent fragments being indicated by dot-
ted lines.
The other one is the HD rule which consists of
three parts: the HD fragment s of the source
side (maybe containing variables), a target string
t (maybe containing variables) and a one-to-one
mapping ? from variables in s to variables in t, as
in:
s = (
Boliweiya
????)
Juxing
?? (x
1
:
Xuanju
?? )
t = Bolivia holds x
1
? = {x
1
:
Xuanju
??? x
1
}
where the underlined element denotes the leaf
node. Variables in the Dep2Str model are con-
strained either by words (like x
1
:??) or Part-of-
Speech (POS) tags (like x
1
:NN).
Given a source sentence with a dependency tree,
a target string and the word alignment between the
source and target sentences, this model first an-
notates each node N with two annotations: head
span and dependency span.
2
These two spans
specify the corresponding target position of a node
(by the head span) or sub-tree (by the depen-
dency span). After annotation, acceptable HD
fragments
3
are utilized to induce lexicalized HD
2
Some definitions: Closure clos(S) of set S is the small-
est superset of S in which the elements (integers) are contin-
uous. Let H be the set of indexes of target words aligned to
node N . Head span hsp(N) of node N is clos(H). Head
span hsp(N) is consistent if it does not overlap with head
span of any other node. Dependency span dsp(N) of node
N is the union of all consistent head spans in the subtree
rooted at N .
3
A head-dependent fragment is acceptable if the head
span of the head node is consistent and none of the depen-
dency spans of its dependents is empty. We could see that
in an acceptable fragment, the head span of the head node
and dependency spans of dependents are not overlapped with
each other.
123
          Boliweiya Juxing  Xuanju
R ule:  ( ????)  ?? ( x1 :??) Boliv ia hold s  x1
                       Xuanju
R ule:  ( x1: N N )  ?? x1 elec tions
           Guohui
R ule:  ?? p ar liam ent
           Zongtong Yu      Guohui
R ule:    ( ??)    ( ?)  x1:?? p r es id ential and  x1
Boliweiya
????/NN
Juxing
??/VV
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Zongtong
??/NN
Yu
?/CC
Guohui
??/NNBoliv ia hold s elec tions
Boliv ia hold s
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Boliv ia hold s  p r es id ential and  p ar liam ent elec tions
Boliv ia hold s  p r es id ential and
Guohui
??/NN elec tions
( a)
( b )
( c )
( d )
( e)
Figure 2: Example of a derivation. Underlined el-
ements indicate leaf nodes.
rules (the head node and leaf node are represented
by words, while the internal nodes are replaced by
variables constrained by word) and unlexicalized
HD rules (nodes are replaced by variables con-
strained by POS tags).
In HD rules, an internal node denotes the whole
sub-tree and is always a substitution site. The head
node and leaf nodes can be represented by either
words or variables. The target side corresponding
to an HD fragment and the mapping between vari-
ables are determined by the head span of the head
node and the dependency spans of the dependents.
A translation can be obtained by applying rules
to the input dependency tree. Figure 2 shows a
derivation for translating a Chinese sentence into
an English string. The derivation proceeds from
top to bottom. Variables in the higher-level HD
rules are substituted by the translations of lower
HD rules recursively.
The final translation is obtained by finding the
best derivation d
?
from all possible derivations
D which convert the source dependency structure
into a target string, as in Equation (1):
d
?
= argmax
d?D
p(d) ? argmax
d?D
?
i
?
i
(d)
?
i
(1)
where ?
i
(d) is the ith feature defined in the deriva-
tion d, and ?
i
is the weight of the feature.
3 Transformation of Dependency Trees
In this section, we introduce an algorithm to trans-
form a dependency tree into a corresponding con-
stituent tree, where words of the source sentence
are leaf nodes and internal nodes are labelled with
head words or POS tags which are constrained by
dependency information. Such a transformation
makes it possible to use the traditional tree-based
decoder to translate a dependency tree, so we can
easily integrate the Dep2Str model into the popu-
lar framework Moses.
In a tree-based system, the CYK algorithm
(Kasami, 1965; Younger, 1967; Cocke and
Schwartz, 1970) is usually employed to translate
the input sentence with a tree structure. Each time
a continuous sequence of words (a phrase) in the
source sentence is translated. Larger phrases can
be translated by combining translations of smaller
phrases.
In a constituent tree, the source words are leaf
nodes and all non-leaf nodes covering a phrase are
labelled with categories which are usually vari-
ables defined in the tree-based model. For trans-
lating a phrase covered by a non-leaf node, the de-
coder for the constituent tree can easily find ap-
plied rules by directly matching variables in these
rules to tree nodes. However, in a dependency tree,
each internal node represents a word of the source
sentence. Variables covering a phrase cannot be
recognized directly. Therefore, to share the same
decoder with the constituent tree, the dependency
tree needs to be transformed into a constituent-
style tree.
As we described in Section 2, each variable in
the Dep2Str model represents a word (for the head
and leaf node) or a sequence of continuous words
(for the internal node). Thus it is intuitive to use
these variables to label non-leaf nodes of the pro-
duced constituent tree. Furthermore, in order to
preserve the dependency information of each HD
fragment, the created constituent node needs to be
constrained by the dependency information in the
HD fragment.
Our transformation algorithm is shown in Al-
gorithm 1, which proceeds recursively from top to
bottom on each HD fragment. There are a maxi-
mum of three types of nodes in an HD fragment:
head node, leaf nodes, and internal nodes. The
124
Algorithm 1 Algorithm for transforming a depen-
dency tree to constituent tree. Dnode means node
in dependency tree. Cnode means node in con-
stituent tree.
function CNODE(label, span)
create a new Cnode CN
CN.label? label
CN.span? span
end function
function TRANSFNODE(Dnode H)
pos? POS of H
constrain pos . with H0, like: NN:H0
CNODE(label,H.position)
for each dependent N of H do
pos? POS of N
word? word of N
constrain pos . with Li or Ri, like: NN:R1
constrain word . with Li or Ri
if N is leaf then
CNODE(pos,N.position)
else
CNODE(word,H.span)
CNODE(pos,H.span)
TRANSFNODE(N )
end if
end for
end function
leaf nodes and internal nodes are dependents of
the head node. For the leaf node and head node,
we create constituent nodes that just cover one
word. For an internal node N , we create con-
stituent nodes that cover all the words in the sub-
tree rooted at N . In Algorithm 1, N.position
means the position of the word represented by the
node N . N.span denotes indexes of words cov-
ered by the sub-tree rooted at node N .
Taking the dependency tree in Figure 1 as an
example, its transformation result for integration
with Moses is shown in Figure 3. In the Dep2Str
model, leaf nodes can be replaced by a vari-
able constrained by its POS tag, so for leaf node
?
Zongtong
?? ? in HD fragment ?
Zongtong
(??)
Yu
(?)
Guohui
???,
we create a constituent node ?NN:L2?, where
?NN? is the POS tag and ?L2? denotes that the leaf
node is the second left dependent of the head node.
For the internal node ?
Guohui
??? in the HD fragment
?
Guohui
(??)
Xuanju
???, we create two constituent nodes
Boliweiya
????
Juxing
??
Zongtong
??
Yu
?
Guohui
??
Xuanju
??
N N : L 1 V V : H 0 N N : L 2 C C : L 1 N N : H 0N N : H 0
N N : L 1
N N : R 1
S
Guohui
??: L 1
Xuanju
??: R 1
Figure 3: The corresponding constituent tree af-
ter transforming the dependency tree in Figure 1.
Note in our implementation, we do not distinguish
the leaf node and internal node of a dependency
tree in the produced constituent tree and induced
rules.
which cover all words in the dependency sub-tree
rooted at this node, with one of them labelled by
the word itself. Both nodes are constrained by de-
pendency information ?L1?. After such a transfor-
mation is conducted on each HD fragment recur-
sively, we obtain a constituent tree.
This transformation makes our implementation
of the Dep2Str model easier, because we can use
the tree-to-string decoder in Moses. All we need
to do is to write a new rule extractor which extracts
head rules and HD rules (see Section 2) from the
word-aligned source dependency trees and target
strings, and represents these rules in the format de-
fined in Moses.
4
Note that while this conversion is performed
on an input dependency tree during decoding, the
training part, including extracting rules and cal-
culating translation probabilities, does not change,
so the model is still a dependency-to-string model.
4
Taking the rule in Section 2 as an example, its represen-
tation in Moses is:
s =
Boliweiya
????
Juxing
??
Xuanju
[??:R1][X] [H1]
t = Bolivia holds
Xuanju
[??:R1][X] [X]
? = {2 ? 2}
where ?H1? denotes the position of the head word is 1, ?R1?
indicates the first right dependent of the head word, ?X? is the
general label for the target side and ? is the set of alignments
(the index-correspondences between s and t). The format has
been described in detail at http://www.statmt.org/
moses/?n=Moses.SyntaxTutorial.
125
In addition, our transformation is different from
other works which transform a dependency tree
into a constituent tree (Collins et al., 1999; Xia and
Palmer, 2001). In this paper, the produced con-
stituent tree still preserves dependency relations
between words, and the phrasal structure is di-
rectly derived from the dependency structure with-
out refinement. Accordingly, the constituent tree
may not be a linguistically well-formed syntactic
structure. However, it is not a problem for our
model, because in this paper what matters is the
dependency structure which has already been en-
coded into the (ill-formed) constituent tree.
4 Decomposition of Dependency
Structure
The Dep2Str model treats a whole HD fragment
as the basic unit, which may result in a sparse-
data problem. For example, an HD fragment with
a verb as head typically consists of more than four
nodes (Xie et al., 2011). Thus in this section, in-
spired by the treelet approach, we describe a de-
composition method to make use of smaller frag-
ments.
In an HD fragment of a dependency tree, the
head determines the semantic category, while
the dependent gives the semantic specification
(Zwicky, 1985; Hudson, 1990). Accordingly, it
is reasonable to assume that in an HD fragment,
dependents could be removed or new dependents
could be attached as needed. Thus, in this paper,
we assume that a large HD fragment is formed by
attaching dependents to a small HD fragment. For
simplicity and reuse of the decoder, such an at-
tachment is carried out in one step. This means
that an HD fragment is decomposed into two
smaller parts in a possible decomposition. This
decomposition can be formulated as Equation (2):
L
i
? ? ?L
1
HR
1
? ? ?R
j
= L
m
? ? ?L
1
HR
1
? ? ?R
n
+ L
i
? ? ?L
m+1
HR
n+1
? ? ?R
j
subject to
i ? 0, j ? 0
i ? m ? 0, j ? n ? 0
i+ j > m+ n > 0
(2)
whereH denotes the head node, L
i
denotes the ith
left dependent and R
j
denotes the jth right depen-
dent. Figure 4 shows an example.
s m ar t/ JJ
v er y/ R BS he/ P R P
s m ar t/ JJ
is / V BZ
S he/ P R P
s m ar t/ JJ
is / V BZ v er y/ R B
+
Figure 4: An example of decomposition on a head-
dependent fragment.
Algorithm 2 Algorithm for the decomposition of
an HD fragment into two sub-fragments. Index of
nodes in a fragment starts from 0.
function DECOMP(HD fragment frag)
fset ? {}
len? number of nodes in frag
hidx? the index of head node in frag
for s = 0 to hidx do
for e = hidx to len? 1 do
if 0 < e? s < len? 1 then
create sub-fragment core
core? nodes from s to e
add core to fset
create sub-fragment shell
initialize shell with head node
shell? nodes not in core
add shell to fset
end if
end for
end for
end function
Such a decomposition of an HD fragment en-
ables us to create translation rules extracted from
sub-structures and create a pseudo-forest from
the input dependency tree to make better use of
smaller rules.
4.1 Sub-structural Rules
In the Dep2Str model, rules are extracted on
an entire HD fragment. In this paper, when
the decomposition is considered, we also extract
sub-structural rules by taking each possible sub-
fragment as a new HD fragment. The algorithm
for recognizing the sub-fragments is shown in Al-
gorithm 2.
In Algorithm 2, we find all possible decom-
126
positions of an HD fragment. Each decom-
position produces two sub-fragments: core and
shell. Both core and shell include the head node.
core contains the dependents surrounding the head
node, with the remaining dependents belonging to
shell. Taking Figure 4 as an example, the bottom-
right part is core, while the bottom-left part is
shell. Each core and shell could be seen as a
new HD fragment. Then HD rules are extracted as
defined in the Dep2Str model.
Note that different from the augmented HD
rules, where Meng et al. (2013) annotate rules with
combined variables and Xie et al. (2014) create
special rules from HD rules at runtime by com-
bining several nodes, our sub-structural rules are
standard HD rules, which are extracted from the
connected sub-structures of a larger HD fragment
and can be used directly in the model.
4.2 Pseudo-Forest
Although sub-structural rules are effective in our
experiments (see Section 5), we still do not use
them to their best advantage, because we only en-
rich smaller rules in our model. During decod-
ing, for a large input HD fragment, the model is
still more likely to resort to glue rules. However,
the idea of decomposition allows us to create a
pseudo-forest directly from the dependency tree to
alleviate this problem to some extent.
As described above, an HD fragment can be
seen as being created by combining two smaller
fragments. This means, for an HD fragment in the
input dependency tree, we can translate one of its
sub-fragments first, then obtain the whole trans-
lation by combining with translations of another
sub-fragment. From Algorithm 2, we know that
the sub-fragment core covers a continuous phrase
of the source sentence. Accordingly, we can trans-
late this fragment first and then build the whole
translation by translating another sub-fragment
shell. Figure 5 gives an example of translating
an HD fragment by combining the translations of
its sub-fragments.
Instead of taking the dependency tree as the in-
put and looking for all rules for translating sub-
fragments of a whole HD, we directly encode the
decomposition into the input dependency tree with
the result being a pseudo-forest. Based on the
transformation algorithm in Section 3, the pseudo-
forest can also be represented in the constituent-
tree style, as shown in Figure 6.
          Yu  Guohui
R ule:  ( ?)  ?? and  p ar lim ent
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Zongtong
R ule:  ( ??)  x 1 : N N p r es id ential x1
p r es id ential and  p ar liam ent
Zongtong
??        and  p ar liam ent
Guohui
??/NN
( a)
( b )
( c )
Figure 5: An example of translating a large HD
fragment with the help of translations of its de-
composed fragments.
S
N N : L 1 V V : H 0
N N : L 2 C C : L 1 N N : H 0N N : H 0
N N : R 1
Xuanju
??: R 1
N N : L 1
Guohui
??: L 1
Boliweiya
????
Juxing
??
Zongtong
??
Yu
?
Guohui
??
Xuanju
??
N N : L 1
N N : H 0
V V : H 0
V V : H 0
Figure 6: An example of a pseudo-forest for the
dependency tree in Figure 1. It is represented us-
ing the constituent-tree style described in Section
3. Edges drawn in the same type of line are owned
by the same sub-tree. Solid lines are shared edges.
In the pseudo-forest, we actually only create a
forest structure for each HD fragment. For ex-
ample, based on Figure 5, we create a constituent
node labelled with ?NN:H0? that covers the sub-
fragment ?
Yu
(?)
Guohui
???. In so doing, a new node la-
belled with ?NN:L1? is also created, which covers
the Node ?
Zongtong
?? ?, because it is now the first left
dependent in the sub-fragment ?
Zongtong
(??)
Guohui
?? ?.
Compared to the forest-based model (Mi et al.,
2008), such a pseudo-forest cannot efficiently re-
duce the influence of parsing errors, but it is easily
available and compatible with the Dep2Str Model.
127
corpus sentences words(ch) words(en)
train 1,501,652 38,388,118 44,901,788
dev 878 22,655 26,905
MT04 1,597 43,719 52,705
MT05 1,082 29,880 35,326
Table 1: Chinese?English corpus. For the English
dev and test sets, words counts are averaged across
4 references.
corpus sentences words(de) words(en)
train 2,037,209 52,671,991 55,023,999
dev 3,003 72,661 74,753
test12 3,003 72,603 72,988
test13 3,000 63,412 64,810
Table 2: German?English corpus. In the dev and
test sets, there is only one English reference for
each German sentence.
5 Experiments
We conduct large-scale experiments to exam-
ine our methods on the Chinese?English and
German?English translation tasks.
5.1 Data
The Chinese?English training corpus is from
the LDC data, including LDC2002E18,
LDC2003E07, LDC2003E14, LDC2004T07,
the Hansards portion of LDC2004T08 and
LDC2005T06. We take NIST 2002 as the de-
velopment set to tune weights, and NIST 2004
(MT04) and NIST 2005 (MT05) as the test data to
evaluate the systems. Table 1 provides a summary
of the Chinese?English corpus.
The German?English training corpus is from
WMT 2014, including Europarl V7 and News
Commentary. News-test 2011 is taken as the de-
velopment set, while News-test 2012 (test12) and
News-test 2013 (test13) are our test sets. Table 2
provides a summary of the German?English cor-
pus.
5.2 Baseline
For both language pairs, we filter sentence pairs
longer than 80 words and keep the length ratio
less than or equal to 3. English sentences are to-
kenized with scripts in Moses. Word alignment is
performed by GIZA++ (Och and Ney, 2003) with
the heuristic function grow-diag-final-and (Koehn
et al., 2003). We use SRILM (Stolcke, 2002) to
Systems MT05
XJ 33.91
D2S 33.79
Table 3: BLEU score [%] of the Dep2Str model
before (XJ) and after (D2S) dependency tree be-
ing transformed. Systems are trained on a selected
1.2M Chinese?English corpus.
train a 5-gram language model on the Xinhua por-
tion of the English Gigaword corpus 5th edition
with modified Kneser-Ney discounting (Chen and
Goodman, 1996). Minimum Error Rate Train-
ing (Och, 2003) is used to tune weights. Case-
insensitive BLEU (Papineni et al., 2002) is used to
evaluate the translation results. Bootstrap resam-
pling (Koehn, 2004) is also performed to compute
statistical significance with 1000 iterations.
We implement the baseline Dep2Str model
in Moses with methods described in this paper,
which is denoted as D2S. The first experiment we
do is to sanity check our implementation. Thus
we take a separate system (denoted as XJ) for
comparison which implements the Dep2Str model
based on (Xie et al., 2011). As shown in Table
3, using the transformation of dependency trees,
the Dep2Str model implemented in Moses (D2S)
is comparable with the standard implementation
(XJ).
In the rest of this section, we describe exper-
iments which compare our system with Moses
HPB (default setting), and test whether our de-
composition approach improves performance over
the baseline D2S.
As described in Section 2, the Dep2Str model
only extracts phrase rules for translating a source
word (head rule). This model could be enhanced
by including phrase rules that cover more than one
source word. Thus we also conduct experiments
where phrase pairs
5
are added into our system. We
set the length limit for phrase 7.
5.3 Chinese?English
In the Chinese?English translation task, the Stan-
ford Chinese word segmenter (Chang et al., 2008)
is used to segment Chinese sentences into words.
The Stanford dependency parser (Chang et al.,
2009) parses a Chinese sentence into the projec-
tive dependency tree.
5
In this paper, the use of phrasal rules is similar to that of
the HPB model, so they can be handled by Moses directly.
128
Systems MT04 MT05
Moses HPB 35.56 33.99
D2S 33.93 32.56
+pseudo-forest 34.28 34.10
+sub-structural rules 34.78 33.63
+pseudo-forest 35.46 34.13
+phrase 36.76* 34.67*
Table 4: BLEU score [%] of our method and
Moses HPB on the Chinese?English task. We use
bold font to indicate that the result of our method
is significantly better than D2S at p ? 0.01 level,
and * to indicate the result is significantly better
than Moses HPB at p ? 0.01 level.
Table 4 shows the translation results. We find
that the decomposition approach proposed in this
paper, including sub-structural rules and pseudo-
forest, improves the baseline system D2S sig-
nificantly (absolute improvement of +1.53/+1.57
(4.5%/4.8%, relative)). As a result, our sys-
tem achieves comparable (-0.1/+0.14) results with
Moses HPB. After including phrasal rules, our
system performs significantly better (absolute im-
provement of +1.2/+0.68 (3.4%/2.0%, relative))
than Moses HPB on both test sets.
6
5.4 German?English
We tokenize German sentences with scripts in
Moses and use mate-tools
7
to perform morpho-
logical analysis and parse the sentence (Bohnet,
2010). Then the MaltParser
8
converts the parse
result into the projective dependency tree (Nivre
and Nilsson, 2005).
Experimental results in Table 5 show that incor-
porating sub-structural rules improves the base-
line D2S system significantly (absolute improve-
ment of +0.47/+0.63, (2.3%/2.8%, relative)), and
achieves a slightly better (+0.08) result on test12
than Moses HPB. However, in the German?
English task, the pseudo-forest produces a neg-
ative effect on the baseline system (-0.07/-0.45),
despite the fact that our system combining both
methods together is still better (+0.2/+0.11) than
the baseline D2S. In the end, by resorting to
6
In our preliminary experiments, phrasal rules are also
able to significantly improve our system on their own on both
Chinese?English and German?English tasks, but the best per-
formance is achieved by combining them with sub-structural
rules and/or pseudo-forest.
7
http://code.google.com/p/mate-tools/
8
http://www.maltparser.org/
Systems test12 test13
Moses HPB 20.44 22.77
D2S 20.05 22.13
+pseudo-forest 19.98 21.68
+sub-structural rules 20.52 22.76
+phrase 20.91* 23.46*
+pseudo-forest 20.25 22.24
+phrase 20.75* 23.20*
Table 5: BLEU score [%] of our method and
Moses HPB on German?English task. We use
bold font to indicate that the result of our method
is significantly better than baseline D2S at p ?
0.01 level, and * to indicate the result is signifi-
cantly better than Moses HPB at p ? 0.01 level.
Systems
# Rules
CE task DE task
Moses HPB 388M 684M
D2S 27M 41M
+sub-structural rules 116M 121M
+phrase 215M 274M
Table 6: The number of rules in different sys-
tems On the Chinese?English (CE) and German?
English (DE) corpus. Note that pseudo-forest (not
listed) does not influence the number of rules.
phrasal rules, our system achieves the best perfor-
mance overall which is significantly better (abso-
lute improvement of +0.47/+0.59 (2.3%/2.6%, rel-
ative)) than Moses HPB.
5.5 Discussion
Besides long-distance reordering (Xie et al.,
2011), another attraction of the Dep2Str model is
its simplicity. It can perform fast translation with
fewer rules than HPB. Table 6 shows the number
of rules in each system. It is easy to see that all of
our systems use fewer rules than HPB. However,
the number of rules is not proportional to transla-
tion quality, as shown in Tables 4 and 5.
Experiments on the Chinese?English corpus
show that it is feasible to translate the dependency
tree via transformation for the Dep2Str model de-
scribed in Section 2. Such a transformation causes
the model to be easily integrated into Moses with-
out making changes to the decoder, while at the
same time producing comparable results with the
standard implementation (shown in Table 3).
The decomposition approach proposed in this
129
paper also shows a positive effect on the base-
line Dep2Str system. Especially, sub-structural
rules significantly improve the Dep2Str model on
both Chinese?English and German?English tasks.
However, experiments show that the pseudo-forest
significantly improves the D2S system on the
Chinese?English data, while it causes translation
quality to decline on the German?English data.
Since using the pseudo-forest in our system is
aimed at translating larger HD fragments via split-
ting it into pieces, we hypothesize that when trans-
lating German sentences, the pseudo-forest ap-
proach more likely results in much worse rules be-
ing applied. This is probably due to the shorter
Mean Dependency Distance (MDD) and freer
word order of German sentences(Eppler, 2013).
6 Conclusion
In this paper, we present an open source mod-
ule which integrates a dependency-to-string model
into Moses.
This module transforms an input depen-
dency tree into a corresponding constituent tree
during decoding which makes Moses perform
dependency-based translation without necessitat-
ing any changes to the decoder. Experiments on
Chinese?English show that the performance if our
system is comparable with that of the standard
dependency-based decoder.
Furthermore, we enhance the model by de-
composing head-dependent fragments into smaller
pieces. This decomposition enriches the Dep2Str
model with more rules during training and allows
us to create a pseudo-forest as input instead of
a dependency tree during decoding. Large-scale
experiments on Chinese?English and German?
English tasks show that this decomposition can
significantly improve the baseline dependency-
to-string model on both language pairs. On
the German?English task, sub-structural rules are
more useful than the pseudo-forest input. In the
end, by resorting to phrasal rules, our system
performs significantly better than the hierarchical
phrase-based model in Moses.
Our implementation of the dependency-to-
string model with methods described in this pa-
per is available at http://computing.dcu.
ie/
?
liangyouli/dep2str.zip. In the fu-
ture, we would like to conduct more experiments
on other language pairs to examine this model,
as well as reducing the restrictions on decompo-
sition.
Acknowledgments
This research has received funding from the Peo-
ple Programme (Marie Curie Actions) of the Eu-
ropean Union?s Seventh Framework Programme
FP7/2007-2013/ under REA grant agreement no.
317471. This research is also supported by the
Science Foundation Ireland (Grant 12/CE/I2267)
as part of the Centre for Next Generation Local-
isation at Dublin City University. The authors of
this paper also thank the reviewers for helping to
improve this paper.
References
Bernd Bohnet. 2010. Very High Accuracy and Fast
Dependency Parsing is Not a Contradiction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 89?97, Beijing,
China.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese Word Seg-
mentation for Machine Translation Performance. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 224?232, Columbus,
Ohio.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative Re-
ordering with Chinese Grammatical Relations Fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59, Boulder, Colorado.
Stanley F. Chen and Joshua Goodman. 1996. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. In Proceedings of the 34th Annual
Meeting on Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California.
John Cocke and Jacob T. Schwartz. 1970. Program-
ming Languages and Their Compilers: Preliminary
Notes. Technical report, Courant Institute of Math-
ematical Sciences, New York University, New York,
NY.
Michael Collins, Lance Ramshaw, Jan Haji?c, and
Christoph Tillmann. 1999. A Statistical Parser for
Czech. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 505?512, College
Park, Maryland.
Eva M. Duran Eppler. 2013. Dependency Distance
and Bilingual Language Use: Evidence from Ger-
man/English and Chinese/English Data. In Proceed-
ings of the Second International Conference on De-
pendency Linguistics (DepLing 2013), pages 78?87,
Prague, August.
130
Heidi J. Fox. 2002. Phrasal Cohesion and Statis-
tical Machine Translation. In Proceedings of the
ACL-02 Conference on Empirical Methods in Nat-
ural Language Processing - Volume 10, pages 304?
3111, Philadelphia.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop
on Computationally Hard Problems and Joint Infer-
ence in Speech and Language Processing, pages 1?
8, New York City, New York.
Richard Hudson. 1990. English Word Grammar.
Blackwell, Oxford, UK.
Tadao Kasami. 1965. An Efficient Recognition and
Syntax-Analysis Algorithm for Context-Free Lan-
guages. Technical report, Air Force Cambridge Re-
search Lab, Bedford, MA.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of EMNLP 2004, pages 388?395, Barcelona, Spain,
July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration Ses-
sions, pages 177?180, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, pages 48?54, Edmonton, Canada.
Arul Menezes and Chris Quirk. 2005. Dependency
Treelet Translation: The Convergence of Statistical
and Example-Based Machine-translation? In Pro-
ceedings of the Workshop on Example-based Ma-
chine Translation at MT Summit X, September.
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u,
and Qun Liu. 2013. Translation with Source Con-
stituency and Dependency Trees. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1066?1076, Seattle,
Washington, USA, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
Based Translation. In Proceedings of ACL-08: HLT,
pages 192?199, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 99?106, Ann Arbor,
Michigan.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, pages 160?167,
Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 271?279, Ann
Arbor, Michigan, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Transla-
tion. Computational Linguistics, 36(4):649?671,
December.
Andreas Stolcke. 2002. SRILM-an Extensible Lan-
guage Modeling Toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing,
pages 257?286, November.
Fei Xia and Martha Palmer. 2001. Converting De-
pendency Structures to Phrase Structures. In Pro-
ceedings of the First International Conference on
Human Language Technology Research, pages 1?5,
San Diego.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A Novel
Dependency-to-string Model for Statistical Machine
Translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216?226, Edinburgh, United Kingdom.
Jun Xie, Jinan Xu, and Qun Liu. 2014. Augment
Dependency-to-String Translation with Fixed and
Floating Structures. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics, pages 2217?2226, Dublin, Ireland.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A De-
pendency Treelet String Correspondence Model for
Statistical Machine Translation. In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 40?47, Prague, June.
Daniel H. Younger. 1967. Recognition and Parsing of
Context-Free Languages in Time n
3
. Information
and Control, 10(2):189?208.
Arnold M. Zwicky. 1985. Heads. Journal of Linguis-
tics, 21:1?29, 3.
131
Proceedings of the 4th International Workshop on Computational Terminology, pages 42?51,
Dublin, Ireland, August 23 2014.
Bilingual Termbank Creation via Log-Likelihood Comparison and
Phrase-Based Statistical Machine Translation
Rejwanul Haque, Sergio Penkale, Andy Way
?
Lingo24, Edinburgh, UK
{rejwanul.haque, sergio.penkale}@lingo24.com
?
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
Bilingual termbanks are important for many natural language processing (NLP) applications, es-
pecially in translation workflows in industrial settings. In this paper, we apply a log-likelihood
comparison method to extract monolingual terminology from the source and target sides of a
parallel corpus. Then, using a Phrase-Based Statistical Machine Translation model, we create a
bilingual terminology with the extracted monolingual term lists. We manually evaluate our novel
terminology extraction model on English-to-Spanish and English-to-Hindi data sets, and observe
excellent performance for all domains. Furthermore, we report the performance of our monolin-
gual terminology extraction model comparing with a number of the state-of-the-art terminology
extraction models on the English-to-Hindi datasets.
1 Introduction
Terminology plays an important role in various NLP tasks including Machine Translation (MT) and
Information Retrieval. It is also exploited in human translation workflows, where it plays a key role
in ensuring translation consistency and reducing ambiguity across large translation projects involving
multiple files and translators over a long period of time. The creation of monolingual and bilingual
terminological resources using human experts are, however, expensive and time-consuming tasks. In
contrast, automatic terminology extraction is much faster and less expensive, but cannot be guaranteed
to be error-free. Accordingly, in real NLP applications, a manual inspection is required to amend or
discard anomalous items from an automatically extracted terminology list.
The automatic terminology extraction task starts with selecting candidate terms from the input domain
corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are
regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word
sequences are regarded as candidate terms (Deane, 2005).
Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et
al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and
TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which
is composed of two consecutive and independent processes:
1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences)
independently from the source and target sides of a parallel corpus,
2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based
Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003).
We then evaluate our novel bilingual terminology extraction model on various domain corpora consid-
ering English-to-Spanish and low-resourced and less-explored English-to-Hindi language-pairs and see
excellent performance for all data sets.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
42
The remainder of the paper is organized as follows. In Section 2, we discuss related work. In Section
3, we describe our two-stage terminology extraction model. Section 4 presents the results and analyses
of our experiments, while Section 5 concludes, and provides avenues for further work.
2 Related Work
Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can
be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches
dominate this field, with some of the leading work including the use of frequency-based filtering (Daille
et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Gar-
side, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm
(Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007).
In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006)
demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, sim-
ilarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both
sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the
most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998),
Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignment-
based terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike
our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might
be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied
statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the lat-
ter focusing on improving monolingual term extraction, rather than on obtaining a bilingual term list. In
contrast, we build a PB-SMT model (Koehn et al., 2003) from the input parallel corpus, which we use
to align a source term to one or more target terms. While Rayson and Garside (2000) and Gelbukh et al.
(2010) only allowed the extraction of single-word terms, we focus on extraction of up to 3-gram terms.
3 Methodology
In this section, we describe our two-stage bilingual terminology extraction model. In the first stage, we
extract monolingual terms independently from either side of a sentence-aligned domain-specific parallel
corpus. In the second stage, the extracted source terms are aligned to one or more extracted target terms
using a PB-SMT model.
3.1 Monolingual Terminology Extraction
The monolingual term extraction task involves the identification of terms from a list of candidate terms
formed from all n-gram word sequences from the monolingual domain corpus (i.e. in our case, each side
of the domain parallel corpus, cf. Section 4.1). On both source and target sides, we used lists of language-
specific stop-words and punctuation marks in order to filter out anomalous items from the candidate
termlists. In order to rank the candidate terms in those lists, we used a log-likelihood comparison method
that compares the frequencies of each candidate term in both the domain corpus and the large general
corpus used as a reference.
1
The log-likelihood (LL) value of a candidate term (C
n
) is calculated using equation (1) from Gelbukh
et al. (2010).
LL = 2 ? ((F
d
? log(F
d
/E
d
)) + (F
g
? log(F
g
/E
g
))) (1)
where F
d
and F
g
are the frequencies of C
n
in the domain corpus and the generic reference corpus,
respectively. E
d
and E
g
are the expected frequencies of C
n
, which are calculated using (2) and (3).
E
d
= N
n
d
? (F
d
+ F
g
)/(N
n
d
+ N
n
g
) (2)
E
g
= N
n
g
? (F
d
+ F
g
)/(N
n
d
+ N
n
g
) (3)
1
Before the term-extraction process begins, we apply a number of preprocessing methods including tokenisation to the input
domain corpus and the generic reference corpus.
43
where N
n
d
and N
n
g
are the numbers of n-grams in the domain corpus and reference corpus, respectively.
Thus, each candidate term is associated with a weight (LL value) which is used to sort the candidate
terms: those candidates with the highest weights have the most significant differences in frequency in the
two corpora. However, we are interested in those candidate terms that are likely to be terms in the domain
corpus. Gelbukh et al. (2010) used the condition in (4) in order to filter out those candidate terms whose
relative frequencies are bigger in the domain corpus than in the reference corpus, and we do likewise.
F
d
/N
n
d
> F
g
/N
n
g
(4)
In contrast with Gelbukh et al. (2010), we extract multi-word terms up to 3-grams, whereas they focused
solely on extracting single word terms.
3.2 Creating a Bilingual Termbank
We obtained source and target termlists from the bilingual domain corpus using the approach described
in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the
extracted source and target termlists.
This section provides a mathematical derivation of the PB-SMT model to show how we scored can-
didate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilin-
gual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability
P(e
I
1
|f
J
1
) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually
comprise M translational features, and the language model, as in (5):
log P(e
I
1
|f
J
1
) =
M
?
m=1
?
m
h
m
(f
J
1
, e
I
1
, s
K
1
) + ?
LM
log P(e
I
1
) (5)
where e
I
1
= e
1
, ..., e
I
is the probable candidate translation for the given input sentence f
J
1
= f
1
, ..., f
J
and s
K
1
= s
1
, ..., s
k
denotes a segmentation of the source and target sentences respectively into the se-
quences of phrases (
?
f
1
, ...,
?
f
k
) and (e?
1
, ..., e?
k
) such that (we set i
0
:= 0):
?k ? [1,K] s
k
:= (i
k
; b
k
, j
k
), (b
k
corresponds to starting index of f
k
)
e?
k
:= e?
i
k?1
+1
, ..., e?
i
k
,
?
f
k
:=
?
f
b
k
, ...,
?
f
j
k
Each feature h
m
in (5) can be rewritten as in (6):
h
m
(f
J
1
, e
I
1
, s
K
1
) =
K
?
k=1
?
h
m
(
?
f
k
, e?
k
, s
k
) (6)
Therefore, the translational features in (5) can be rewritten as in (7):
M
?
m=1
?
m
h
m
(f
J
1
, e
I
1
, s
K
1
) =
M
?
m=1
?
m
K
?
k=1
?
h
m
(
?
f
k
, e?
k
, s
k
) (7)
In equation (7),
?
h
m
is a feature defined on phrase-pairs (
?
f
k
, e?
k
), and ?
m
is the feature weight of
?
h
m
.
These weights (?
m
) are optimized using minimum error-rate training (MERT) (Och, 2003) on a held-out
500 sentence-pair development set for each of the experiments.
We create a list of probable source?target term-pairs by taking each source and target term from the
source and target termlists, respectively, provided that those source?target term-pairs are present in the
PB-SMT phrase-table. We calculate a weight (w) for each source?target term-pair (essentially, a phrase-
pair, i.e. (e?
k
,
?
f
k
)) using (8):
2
w(e?
k
,
?
f
k
) =
M
?
m=1
?
m
?
h
m
(
?
f
k
, e?
k
) (8)
2
Equation (8) is derived from the right-hand side of equation (7) for a single source?target phrase-pair.
44
In order to calculate w, we used the four standard PB-SMT translational features (
?
h
m
), namely forward
phrase translation log-probability (log P(e?
k
|
?
f
k
)), its inverse (log P(
?
f
k
|e?
k
)), the lexical log-probability
(log P
lex
(e?
k
|
?
f
k
)), and its inverse (log P
lex
(
?
f
k
|e?
k
)). We considered a higher threshold value for weights
and considered those term-pairs whose weights exceeded this threshold. For each source term, we con-
sidered a maximum of the four highest-weighted target terms.
Domain Parallel Corpus
Domain Sentences Words (English)
English-to-Spanish
Banking, Finance and Economics 50,112 548,594
Engineering 91,896 1,165,384
IT 33,148 367,046
Tourism and Travel 50,042 723,088
Science 79,858 1,910,482
Arts and Culture 9,124 100,620
English-to-Hindi
EILMT 7,096 173,770
EMILLE 9,907 159,024
Launchpad 67,663 380,546
KDE4 84,089 324,289
Reference Corpus
Language Sentences Words
English 4,000,000 82,048,154
Spanish 4,132,386 128,005,190
Hindi 10,000,000 182,066,982
Table 1: Corpus Statistics.
4 Experiments and Discussion
4.1 Data Used
We conducted experiments on several data domains for two different language-pairs, English-to-Spanish
and English-to-Hindi. For English-to-Spanish, we worked with client-provided data taken from six dif-
ferent domains in the form of translation memories. For English-to-Hindi, we used three parallel corpora
from three different sources (EILMT, EMILLE and Launchpad) taken from HindEnCorp
3
(Bojar et al.,
2014) released for the WMT14 shared translation task,
4
and a parallel corpus of KDE4 localization files
5
(Tiedemann, 2009). The EMILLE corpus contains leaflets from the UK Government and various local
authorities. The domain of the EILMT
6
corpus is tourism.
We used data from a collection of translated documents from the United Nations (MultiUN)
7
(Tiede-
mann, 2009) and the European Parliament (Koehn et al., 2005) as the monolingual English and Spanish
reference corpora. We used the HindEnCorp monolingual corpus (Bojar et al., 2014) as the monolingual
Hindi reference corpus. The statistics of the data used in our experiments are shown in Table 1.
4.2 Runtime Performance
Our terminology extraction model is composed of two main processes: (i) Moses training and tuning
(restricting the number of iterations of MERT to a maximum of 6), and (ii) terminology extraction. In
Table 2, we report the actual runtimes of these two processes on the six domain corpora. As Table
3
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4
http://www.statmt.org/wmt14/
5
http://opus.lingfil.uu.se/KDE4.php
6
English-to-Indian Language Machine Translation (EILMT) is a Ministry of IT, Govt. of India sponsored project.
7
http://opus.lingfil.uu.se/MultiUN.php
45
2 demonstrates, both MT system-building (training and tuning combined) and terminology extraction
processes are very short on each corpus. Given the crucial influence of bilingual terminology on quality
in translation workflows, we believe that the creation of such assets from scratch in less than 30 minutes
may prove to be a significant breakthrough for translators.
MT System Terminology
Building Extraction
English-to-Spanish
Banking, Finance and Economics 05:49 04:23
Engineering 06:47 04:33
IT 04:10 04:31
Tourism and Travel 05:34 04:24
Science 15:26 04:52
Arts and Culture 03:20 04:16
English-to-Hindi
EILMT 12:41 15:47
EMILLE 05:41 17.18
Launchpad 04:37 24.11
KDE4 04:05 16:50
Table 2: Runtimes (minutes:seconds) for MT system-building and bilingual terminology extraction on
the different domain data sets.
4.3 Human Evaluation
Of course, it is one thing to rapidly create translation assets such as bilingual termbanks, and another en-
tirely to ensure the quality of such resources. Accordingly, we evaluated the performance of our bilingual
terminology extraction model on each English-to-Spanish and English-to-Hindi domain corpus reported
in Table 1, with the evaluation goals being twofold: (i) measuring the accuracy of the monolingual ter-
minology extraction process, and (ii) measuring the accuracy of our novel bilingual terminology creation
model.
As mentioned in Section 3.2, a source term may be aligned with up to four target terms. For evaluation
purposes, we considered the top-100 source terms based on the LL values (cf. (1)) and their target coun-
terparts (i.e. one to four target terms). The quality of the extracted terms was judged by native Spanish
and Hindi speakers, both with excellent English skills, and the evaluation results are reported in Table
3. Note that we were not able to measure recall of the term extraction model on the domain corpora due
to the unavailability of a reference terminology set. The evaluator counted the number of valid terms in
the source term list for the domain in question, and the percentage of valid terms with respect to the total
number of terms (i.e. 100) is reported in the second column in Table 3. We refer to this as VST (Valid
Source Terms). For each valid source term there are one to four target terms that are ranked according to
the weights in (8). In theory, therefore, the top-ranked target term is the most suitable target translation of
the aligned source term. The evaluator counted the number of instances where the top-ranked target term
was a suitable target translation of the source term; the percentage with respect to the number of valid
source terms is shown in the third column in Table 3, and denoted as VTT (Valid Target Terms). The
evaluator also reported the number of cases where any of the four target terms was a suitable translation
of the source term; the percentage with respect to the number of valid source terms is given in the fourth
column in Table 3. Furthermore, the evaluator counted the number of instances where any of the four
target terms with minor editing can be regarded as suitable target translation; the percentage with respect
to the number of valid source terms is reported in the last column of Table 3. In Table 4, we show three
English?Spanish term-pairs extracted by our automatic term extractor where the target terms (Spanish)
are slightly incorrect. In all these examples the edit distance between the correct term and the one pro-
posed by our automatic extraction method is quite low, meaning that just a few keystrokes can transform
46
the candidate term into the correct one. In these cases editing the candidate term is much cheaper (in
terms of time) than creating the translations from scratch.
VST VTT1 VTT4 VTTME4
(%) (%) (%) (%)
English-to-Spanish
Banking, Finance and Economics 76 92.1 93.4 94.7
Engineering 84 90.5 91.7 94.1
IT 89 90.0 97.8 97.8
Tourism and Travel 72 86.1 93.1 93.1
Science 94 93.6 93.6 93.6
Arts and Culture 89 91.9 96.5 96.5
English-to-Hindi
EILMT 91 81.3 83.5 96.7
EMILLE 79 62.1 83.5 98.7
Launchpad 88 95.4 98.8 98.8
KDE4 79 88.6 89.8 94.9
Table 3: Manual evaluation results obtained on the top-100 term pairs. VST: Valid Source Terms, VTT1:
Valid Target Terms (1-best), VTT4: Valid Target Terms (4-best), VTTME4: Valid Target Terms with
Minor Editing (4-best).
Source Terms Target Terms Target Terms Edit
(using Bilingual Term Extractor) corrected with Minor Editing Distance
Shutter Obturaci?on Obturador 5
comment: wrong choice of inflection is likely caused by the term being most frequently used as
?shutter speed?
Lenses Objetivos EF Objetivos 3
comment: The qualifier ?EF? should not be present in the target, as it is not in the source
Leave Cancel Cancelaci?on Vacaciones Cancelaci?on de Vacaciones 3
comment: The preposition ?de? is missing in the target term
Table 4: Slightly wrong target terms corrected with minor editing.
In Table 3, we see that the accuracy of the monolingual term extraction model varies from 72% to 94%
for both English-to-Spanish and English-to-Hindi. For English-to-Spanish, the accuracy of our bilingual
terminology creation model ranges from 86.1% to 93.6%, 91.7% to 97.8% and 93.1% to 97.8% when
the 1-best, 4-best and 4-best with slightly edited target terms are considered, respectively. For English-
to-Hindi, the accuracy of our bilingual terminology creation model ranges from 62.1% to 95.4%, 83.5%
to 98.8% and 94.9% to 98.8% when the 1-best, 4-best and 4-best with slightly edited target terms are
considered, respectively.
We are greatly encouraged by these results, as they demonstrate that our novel bilingual termbank
creation method is robust in the face of the somewhat noisy monolingual term-extraction results; as a
consequence, if better methods for suggesting monolingual term candidates are proposed, we expect the
performance of our bilingual term-creation model to improve accordingly.
We calculated the distributions of unigram, bigram and trigram in the valid source terms (cf. Table 3)
and reported in Table 5. We also calculated the percentages of their distributions in the valid source terms
averaged over all 10 data sets. As can be seen from Table 3, the percentage of the average distribution of
the trigram terms is quite low (i.e. 2.5%). This result justifies our decision for extraction of up to 3-gram
terms.
47
Unigram Bigram Trigram
English-to-Spanish
Banking, Finance and Economics 55 20 1
Engineering 64 18 2
IT 75 12 2
Tourism and Travel 49 18 5
Science 91 3 0
Arts and Culture 76 10 3
English-to-Hindi
EILMT 73 17 1
EMILLE 35 37 7
Launchpad 85 3 0
KDE4 74 5 0
Average 80.4% 17.0 % 2.5%
Table 5: Distributions of unigram, bigram and trigram in the valid source term pairs (cf. second column
in Table 3).
4.4 Comparison: Monolingual Terminology Extraction
In this section we report the performance of our monolingual terminology extraction model (cf. Section
3.1) comparing with the performance of several state-of-the-art terminology extraction algorithms capa-
ble of recognising multiword terms. In order to extract monolingual multiword terms we used the JATE
toolkit
8
(Zhang et al., 2008). This toolkit first extracts candidate terms from a corpus using linguistic
tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The
JATE toolkit is currently available only for the English language. For evaluation purposes, we considered
the source-side of the English-to-Hindi domain corpora.
Algorithm Reference EILMT EMILLE Launchpad KDE4
LLC (Bilingual) cf. VST in Table 3 91 79 88 79
LLC 77 53 80 71
STF 46 04 54 44
ACTF 42 15 62 48
TF-IDF 50 36 45 17
Glossex Kozakov et al. (2004) 76 43 76 71
JK Justeson & Katz (1995) 42 13 58 42
NC-Value Frantzi et al. (2000) 46 34 52 25
RIDF Church & Gale (1995) 27 16 23 21
TermEx Sclano et al. (2007) 42 08 46 41
C-Value Ananiadou (1994) 49 44 62 40
Weirdness Ahmed et al. (1999) 77 57 82 63
Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Fre-
quency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz
For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)).
The automatic term extraction algorithms in JATE assign weights (domain representativeness) to the
candidate terms giving an indication of the likelihood of being a good domain-specific term. The quality
of the extracted terms (top-100 highest weighted) was judged by an evaluator with excellent English
skills, and the evaluation results are reported in Table 6. The evaluator counted the number of valid terms
8
https://code.google.com/p/jatetoolkit/
48
in the highest weighted 100 terms that were extracted using different state-of-the-art term extraction
algorithms.
The third row of Table 6 represents the percentage of the valid source terms extracted by our log-
likelihood comparison (LLC) based monolingual term extraction algorithm. The next three rows rep-
resent three basic monolingual term extraction algorithms (STF: simple term frequency, ACTF: aver-
age corpus term frequency and TF-IDF) available in the JATE toolkit. The last seven rows represent
seven state-of-the-art terminology extraction algorithms. As can be seen from Table 6, LLC is the best-
performing algorithm with the Weirdness (Ahmad et al., 1999) and the Glossex (Kozakov et al., 2004)
algorithms on the EILMT and the KDE4 corpora, respectively. The LLC is also the second-best per-
forming algorithm on the EMILLE and the Lauchpad corpora.
We see in Table 6 that the percentage of valid source terms is quite low on the EMILLE corpus.
This might be caused by it containing information leaflets in a variety of domains (consumer, education,
housing, health, legal, social), which might bring down the percentage of valid source terms on this
corpus.
Note that the percentage of valid source terms (VST) reported in Table 3 is calculated taking the
top-100 source terms from the bilingual term-pair list that were extracted using the method described in
Section 3.2. For comparison purposes we again report this percentage (VST in Table 3) in the second row
in Table 6. Our bilingual term extraction method discards any anomalous pairs from the initial candidate
term-pair list (cf. Section 3.2). This essentially removes some of the source entries that are not pertinent
to the domain. As a result, the percentage of the valid source terms extracted applying our bilingual
terminology extraction method (Table 3) is higher than the percentage of the valid source terms extracted
applying our monolingual terminology extraction algorithm (LLC) (Table 6). We clearly see from Tables
3 and 6 that this bilingual approach to term extraction not only achieves remarkable performance on the
bilingual task, but that when used in a monolingual context it outperforms most state-of-the-art extraction
algorithms, and is comparable with the best ones. We should also note that JATE?s implementation of
these algorithms (including Weirdness) uses language-dependent modules such as a lemmatizer, unlike
our implementation of LLC which is language-independent.
5 Conclusions and Future Work
In this paper we presented a bilingual multi-word terminology extraction model based on two inde-
pendent consecutive processes. Firstly, we employed a log-likelihood comparison method to extract
source and target terms independently from both sides of a parallel domain corpus. Secondly, we used
a PB-SMT model to align source terms to one or more target terms. The manual evaluation results
on ten different domain corpora of two syntactically divergent language-pairs showed the accuracy of
our bilingual terminology extraction model to be very high, especially in the light of the rather noisier
monolingual candidate terms presented to it. Given the reported high levels of performance ? minimum
levels of 93.1% and 94.9% in the 4-best set-up across all six domains for English-to-Spanish and all four
domains for English-to-Hindi, respectively ? we are convinced that the extracted bilingual multiword
termbanks are useful ?as is?, and with a small amount of post-processing from domain experts would be
completely error-free.
The proposed bilingual terminology extraction model has been tested on a highly investigated
language-pair, English-to-Spanish, and a less-explored and low-resourced English-to-Indic language-
pair, English-to-Hindi. Interestingly, the performance of the bilingual terminology extraction model
is excellent for the both language-pairs. We also tested several state-of-the-art monolingual terminol-
ogy extraction algorithms including our own (log-likelihood comparison) on the source-side of the four
English-to-Hindi domain data sets. According to the manual evaluation results, our monolingual multi-
word term extraction model proves to be the best-performing algorithm on two domain data sets and the
second best-performing algorithm on the remaining two domain data sets. Our monolingual multiword
terminology extraction method is clearly comparable to the state-of-the-art monolingual terminology
extraction algorithms.
In this work, we considered all n-gram word sequences from the domain corpus as candidate terms.
49
In future work, we would like to incorporate the candidate phrasal term identification model of Deane
(2005), which would omit irrelevant multiword units, and help us extend our evaluation beyond the top-
100 terms. We also plan to demonstrate the impact of the created termbanks on translator productivity in
a number of workflows ? different language pairs, domains, and levels of post-editing ? in an industrial
setting.
Acknowledgements
This work was partially supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of
CNGL at Dublin City University, and by Grant 610879 for the Falcon project funded by the European
Commission.
References
S. Ananiadou. 1994. A methodology for automatic term recognition. In COLING: 15th International Conference
on Computational Linguistics, pages 1034?1038.
K. Ahmad, L. Gillam and L. Tostevin. 1999. University of Surrey Participation in TREC8: Weirdness Indexing for
Logical Document Extrapolation and Retrieval (WILDER). In the Eighth Text REtrieval Conference (TREC-8).
National Institute of Standards and Technology, Gaithersburg, MD., pp.717?724.
R. Basili, A. Moschitti, M. Pazienza and F. Zanzotto. 2001. A contrastive approach to term extraction. In Pro-
ceedings of the 4th Conference on Terminology and Artificial Intelligence (TIA 2001). Nancy, France, 10pp.
K. Church and W. Gale. 1995. Inverse Document Frequency (IDF): A Measure of Deviation from Poisson. In
Proceedings of the 3rd Workshop on Very Large Corpora, pages 121?130. Cambridge, MA.
B. Daille, E. Gaussier and J-M. Lang?e. 1994. Towards automatic extraction of monolingual and bilingual termi-
nology. In COLING 94, The 15th International Conference on Computational Linguistics, Proceedings. Kyoto,
Japan, pp.515?521.
P. Deane. 2007. A nonparametric method for extraction of candidate phrasal terms. In ACL-05: 43rd Annual
Meeting of the Association for Computational Linguistics. Ann Arbor, Michigan, USA, pp.605?613.
K. Frantzi, S. Ananiadou and H. Mima. 2000. Automatic Recognition of Multi-word Terms: the C-value/NC-value
Method. International Journal of Digital Libraries. 3(2): 115?130.
E. Gaussier. 1998. Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Cor-
pora. In COLING-ACL ?98, 36th Annual Meeting of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics, Proceedings of the Conference, Volume II. Montreal,
Quebec, Canada, pp.444?450.
A. Gelbukh, G. Sidorov, E. Lavin-Villa and L. Chanona-Hernandez. 2010. Automatic Term Extraction Using
Log-Likelihood Based Comparison with General Reference Corpus. In 15th International Conference on Ap-
plications of Natural Language to Information Systems, NLDB 2010, Proceedings. LNCS vol. 6177. Berlin:
Springer. pp.248?255.
L. Ha, G. Fernandez, R. Mitkov and G. Corpas. 2008. Mutual bilingual terminology extraction. In LREC 2008:
6th Language Resources and Evaluation Conference. Marrakech, Morocco, pp.1818?1824.
T. He, T., X. Zhang and Y. Xinghuo. 2006. An Approach to Automatically Constructing Domain Ontology. In
Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation, PACLIC 2006.
Wuhan, China, pp.150?157.
J. S. Justeson, and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for
identification in text. Natural language engineering, 1(1) 9?27.
S. Kim, T. Baldwin and M-Y. Kan. 2009. An Unsupervised Approach to Domain-Specific Term Extraction. In
Proceedings of the Australasian Language Technology Association Workshop 2009. Sydney, Australia, pp.94?
98.
P. Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit X: The Tenth
Machine Translation Summit. Phuket, Thailand, pp.79?86.
50
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical
Machine Translation. In ACL 2007, Proceedings of the Interactive Poster and Demonstration Sessions. Prague,
Czech Republic, pp.177?180.
P. Koehn, F. Och and H. Ney. 2003. Statistical Phrase-Based Translation. In HLT-NAACL 2003: conference
combining Human Language Technology conference series and the North American Chapter of the Association
for Computational Linguistics conference series. Edmonton, Canada, pp. 48?54.
L. Kozakov, Y. Park, T. H. Fin, Y. Drissi, Y. N. Doganata, and T. Cofino. 2004. Glossary extraction and knowledge
in large organisations via semantic web technologies. In Proceedings of the 6th International Semantic Web
Conference and the 2nd Asian Semantic Web Conference.
J. Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In 31st Annual
Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Columbus, Ohio,
USA, pp.17?22.
E. Lefever, L. Macken and V. Hoste. 2009. Language-Independent Bilingual Terminology Extraction from a
Multilingual Parallel Corpus. In EACL ?09 Proceedings of the 12th Conference of the European Chapter of the
Association for Computational Linguistics. Athens, Greece, pp.496?504.
F. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In 41st Annual Meeting of the
Association for Computational Linguistics, Proceedings of the Conference. Sapporo, Japan, pp.160?167.
F. Och and H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Transla-
tion. In 40th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.
Philadelphia, PA, USA, pp.295?302.
O. Bojar, V. Diatka, P. Rychl?y, P. Stra?n?ak, A. Tamchyna, and D. Zeman. 2014. Hindi-English and Hindi-only
Corpus for Machine Translation. In Proceedings of the Ninth International Language Resources and Evaluation
Conference (LREC?14). Reykjavik, Iceland.
P. Pantel and D. Lin. 2001. A Statistical Corpus-Based Term Extractor. In E. Stroulia and S. Matwin (eds.)
Advances in Artificial Intelligence, 14th Biennial Conference of the Canadian Society for Computational Studies
of Intelligence, AI 2001, Ottawa, Canada, Proceedings. LNCS vol. 2056. Berlin: Springer, pp.36?46.
P. Rayson and R. Garside. 2000. Comparing corpora using frequency profiling. In Proceedings of the Workshop
on Comparing Corpora, held in conjunction with the 38th Annual Meeting of the Association for Computational
Linguistics (ACL 2000). Hong Kong, pp.1?6.
F. Sclano and P. Velardi. 2007. TermExtractor: a web application to learn the shared terminology of emergent web
communities. In Proceedings of the 3rd International Conference on Interoperability for Enterprise Software
and Applications (I-ESA 2007). Funchal, Madeira Island, Portugal, pp.287?290.
J. Tiedemann. 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces.
In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds.) Recent Advances in Natural Language
Processing (vol V), pages 237?248, John Benjamins, Amsterdam/Philadelphia.
Z. Zhang, J. Iria, C. Brewster and F. Ciravegna. 2008. A Comparative Evaluation of Term Recognition Algorithms.
In Proceedings of The sixth international conference on Language Resources and Evaluation, (LREC 2008),
pages 2108?2113, Marrakech, Morocco.
51
