Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 430?438,
Beijing, August 2010
A Structured Vector Space Model for Hidden Attribute Meaning
in Adjective-Noun Phrases
Matthias Hartung and Anette Frank
Computational Linguistics Department
Heidelberg University
{hartung, frank}@cl.uni-heidelberg.de
Abstract
We present an approach to model hid-
den attributes in the compositional se-
mantics of adjective-noun phrases in a
distributional model. For the represen-
tation of adjective meanings, we refor-
mulate the pattern-based approach for at-
tribute learning of Almuhareb (2006) in
a structured vector space model (VSM).
This model is complemented by a struc-
tured vector space representing attribute
dimensions of noun meanings. The com-
bination of these representations along the
lines of compositional semantic principles
exposes the underlying semantic relations
in adjective-noun phrases. We show that
our compositional VSM outperforms sim-
ple pattern-based approaches by circum-
venting their inherent sparsity problems.
1 Introduction
In formal semantic theory, the compositional se-
mantics of adjective-noun phrases can be modeled
in terms of selective binding (Pustejovsky, 1995),
i.e. the adjective selects one of possibly several
roles or attributes1 from the semantics of the noun.
(1) a. a blue car
b. COLOR(car)=blue
In this paper, we define a distributional frame-
work that models the compositional process un-
derlying the modification of nouns by adjectives.
1In the original statement of the theory, adjectives se-
lect qualia roles that can be considered as collections of at-
tributes.
We focus on property-denoting adjectives as they
are valuable for acquiring concept representations
for, e.g., ontology learning. An approach for au-
tomatic subclassification of property-denoting ad-
jectives is presented in Hartung and Frank (2010).
Our goal is to expose, for adjective-noun phrases
as in (1a), the attribute in the semantics of the
noun that is selected by the adjective, while not
being overtly realized on the syntactic level. The
semantic information we intend to capture for (1a)
is formalized in (1b).
Ideally, this kind of knowledge could be ex-
tracted from corpora by searching for patterns that
paraphrase (1a), e.g. the color of the car is blue.
However, linguistic patterns that explicitly relate
nouns, adjectives and attributes are very rare.
We avoid these sparsity issues by reducing
the triple r=?noun, attribute, adjective? that
encodes the relation illustrated in (1b) to tu-
ples r?=?noun, attribute? and r??=?attribute,
adjective?, as suggested by Turney and Pantel
(2010) for similar tasks. Both r? and r?? can be
observed much more frequently in text corpora
than r. Moreover, this enables us to model adjec-
tive and noun meanings as distinct semantic vec-
tors that are built over attributes as dimensions.
Based on these semantic representations, we make
use of vector composition operations in order to
reconstruct r from r? and r??. This, in turn, al-
lows us to infer complete noun-attribute-adjective
triples from individually acquired noun-attribute
and adjective-attribute representations.
The contributions of our work are as follows:
(i) We propose a framework for attribute selection
based on structured vector space models (VSM),
using as meaning dimensions attributes elicited
430
by adjectives; (ii) we complement this novel rep-
resentation of adjective meaning with structured
vectors for noun meanings similarly built on at-
tributes as meaning dimensions; (iii) we propose a
composition of these representations that mirrors
principles of compositional semantics in mapping
adjective-noun phrases to their corresponding on-
tological representation; (iv) we propose and eval-
uate several metrics for the selection of meaning-
ful components from vector representations.
2 Related Work
Adjective-noun meaning composition has not
been addressed in a distributional framework be-
fore (cf. Mitchell and Lapata (2008)). Our ap-
proach leans on related work on attribute learning
for ontology induction and recent work in distri-
butional semantics.
Attribute learning. Early approaches to at-
tribute learning include Hatzivassiloglou and
McKeown (1993), who cluster adjectives that de-
note values of the same attribute. A weakness
of their work is that the type of the attribute
cannot be made explicit. More recent attempts
to attribute learning from adjectives are Cimiano
(2006) and Almuhareb (2006). Cimiano uses at-
tributes as features to arrange sets of concepts in a
lattice. His approach to attribute acquisition har-
nesses adjectives that occur frequently as concept
modifiers in corpora. The association of adjec-
tives with their potential attributes is performed by
dictionary look-up in WordNet (Fellbaum, 1998).
Similarly, Almuhareb (2006) uses adjectives and
attributes as (independent) features for the pur-
pose of concept learning. He acquires adjective-
attribute pairs using a pattern-based approach.
As a major limitation, these approaches are
confined to adjective-attribute pairs. The poly-
semy of adjectives that can only be resolved in the
context of the modified noun is entirely neglected.
From a methodological point of view, our work
is similar to Almuhareb?s, as we will also build
on lexico-syntactic patterns for attribute selection.
However, we extend the task to involve nouns and
rephrase his approach in a distributional frame-
work based on the composition of structured vec-
tor representations.
Distributional semantics. We observe two re-
cent trends in distributional semantics research:
(i) The use of VSM tends to shift from mea-
suring unfocused semantic similarity to captur-
ing increasingly fine-grained semantic informa-
tion by incorporating more linguistic structure.
Following Baroni and Lenci (to appear), we re-
fer to such models as structured vector spaces.
(ii) Distributional methods are no longer confined
to word meaning, but are noticeably extended to
capture meaning on the phrase level. Prominent
examples for (i) are Pado? and Lapata (2007) and
Rothenha?usler and Schu?tze (2009) who use syn-
tactic dependencies rather than single word co-
occurrences as dimensions of semantic spaces.
Erk and Pado? (2008) extend this idea to the ar-
gument structure of verbs, while also accounting
for compositional meaning aspects by modelling
predication over arguments. Hence, their work is
also representative for (ii).
Baroni et al (2010) use lexico-syntactic pat-
terns to represent concepts in a structured VSM
whose dimensions are interpretable as empirical
manifestations of properties. We rely on similar
techniques for the acquisition of structured vec-
tors, whereas our work focusses on exposing the
hidden meaning dimensions involved in composi-
tional processes underlying concept modification.
The commonly adopted method for modelling
compositionality in VSM is vector composition
(Mitchell and Lapata, 2008; Widdows, 2008).
Showing the benefits of vector composition for
language modelling, Mitchell and Lapata (2009)
emphasize its potential to become a standard
method in NLP.
The approach pursued in this paper builds on
both lines of research sketched in (i) and (ii) in
that we model a specific meaning layer in the se-
mantics of adjectives and nouns in a structured
VSM. Vector composition is used to expose their
hidden meaning dimensions on the phrase level.
3 Structured Vector Representations for
Adjective-Noun Meaning
3.1 Motivation
Contrary to prior work, we model attribute selec-
tion as involving triples of nouns, attributes and
431
C
O
LO
R
D
IR
EC
TI
O
N
D
U
R
AT
IO
N
SH
A
PE
SI
ZE
SM
EL
L
SP
EE
D
TA
ST
E
TE
M
PE
R
AT
U
R
E
W
EI
G
H
T
ve 1 1 0 1 45 0 4 0 0 21
vb 14 38 2 20 26 0 45 0 0 20
ve ? vb 14 38 0 20 1170 0 180 0 0 420
ve + vb 15 39 2 21 71 0 49 0 0 41
Figure 1: Vectors for enormous (ve) and ball (vb)
adjectives, as in (2). The triple r can be bro-
ken down into tuples r? = ?noun, attribute? and
r?? = ?attribute, adjective?. Previous learning
approaches focussed on r? (Cimiano, 2006) or r??
(Almuhareb, 2006) only.
(2) a. a bluevalue carconcept
b. ATTR(concept) = value
In semantic composition of adjective-noun
compounds, the adjective (e.g. blue) contributes a
value for an attribute (here: COLOR) that charac-
terizes the concept evoked by the noun (e.g. car).
Thus, the attribute in (2) constitutes a ?hidden
variable? that is not overtly expressed in (2a), but
constitutes the central axis that relates r? and r??.
Structured vectors built on extraction patterns.
We model the semantics of adjectives and nouns
in a structured VSM that conveys the hidden re-
lationship in (2). The dimensions of the model
are defined by attributes, such as COLOR, SIZE
or SPEED, while the vector components are deter-
mined on the basis of carefully selected acquisi-
tion patterns that are tailored to capturing the par-
ticular semantic information of interest for r? and
r??. In this respect, lexico-syntactic patterns serve
a similar purpose as dependency relations in Pado?
and Lapata (2007) or Rothenha?usler and Schu?tze
(2009). The upper part of Fig. 1 displays exam-
ples of vectors we build for adjectives and nouns.
Composing vectors along hidden dimensions.
The fine granularity of lexico-syntactic patterns
that capture the triple r comes at the cost of their
sparsity when applied to corpus data. Therefore,
we construct separate vector representations for
r? and r??. Eventually, these representations are
joined by vector composition to reconstruct the
triple r. Apart from avoiding sparsity issues,
this compositional approach has several prospects
from a linguistic perspective as well.
Ambiguity and disambiguation. Building vec-
tors with attributes as meaning dimensions en-
ables us to model (i) ambiguity of adjectives with
regard to the attributes they select, and (ii) the dis-
ambiguation capacity of adjective and noun vec-
tors when considered jointly. Consider, for exam-
ple, the phrase enormous ball that is ambiguous
for two reasons: enormous may select a set of pos-
sible attributes (SIZE or WEIGHT, among others),
while ball elicits several attributes in accordance
with its different word senses2. As seen in Fig. 1,
these ambiguities are nicely captured by the sep-
arate vector representations for the adjective and
the noun (upper part); by composing these repre-
sentations, the ambiguity is resolved (lower part).
3.2 Building a VSM for Adjective-Noun
Meaning
In this section, we introduce the methods we ap-
ply in order to (i) acquire vector representations
for adjectives and nouns, (ii) select appropriate at-
tributes from them, and (iii) compose them.
3.2.1 Attribute Acquisition Patterns
We use the following patterns3 for the ac-
quisition of vectors capturing the tuple r?? =
?attribute, adjective?. Even though some of
these patterns (A1 and A4) match triples of nouns,
attributes and adjectives, we only use them for the
extraction of binary tuples (underlined), thus ab-
stracting from the modified noun.
(A1) ATTR of DT? NN is|was JJ
(A2) DT? RB? JJ ATTR
(A3) DT? JJ or JJ ATTR
(A4) DT? NN?s ATTR is|was JJ
(A5) is|was|are|were JJ in|of ATTR
To acquire noun vectors capturing the tuple
r? = ?noun, attribute?, we rely on the follow-
ing patterns. Again, we only extract pairs, as indi-
cated by the underlined elements.
(N1) NN with|without DT? RB? JJ? ATTR
(N2) DT ATTR of DT? RB? JJ? NN
(N3) DT NN?s RB? JJ? ATTR
(N4) NN has|had a|an RB? JJ? ATTR
2WordNet senses for the noun ball include, among others:
1. round object [...] in games; 2. solid projectile, 3. object
with a spherical shape, 4. people [at a] dance.
3Some of these patterns are taken from Almuhareb (2006)
and Sowa (2000). The descriptions rely on the Penn Tagset
(Marcus et al, 1999). ? marks optional elements.
432
3.2.2 Target Filtering
Some of the adjectives extracted by A1-A5 are
not property-denoting and thus represent noise.
This affects in particular pattern A2, which ex-
tracts adjectives like former or more, or relational
ones such as economic or geographic.
This problem may be addressed in different
ways: By target filtering, extractions can be
checked against a predicative pattern P1 that is
supposed to apply to property-denoting adjectives
only. Vectors that fail this test are suppressed.
(P1) DT NN is|was JJ
Alternatively, extractions obtained from low-
confidence patterns can be awarded reduced
weights by means of a pattern value function (de-
fined in 3.3; cf. Pantel and Pennacchiotti (2006)).
3.2.3 Attribute Selection
We intend to use the acquired vectors in order
to detect attributes that are implicit in adjective-
noun meaning. Therefore, we need a method
that selects appropriate attributes from each vec-
tor. While, in general, this task consists in dis-
tinguishing semantically meaningful dimensions
from noise, the requirements are different depend-
ing on whether attributes are to be selected from
adjective or noun vectors. This is illustrated in
Fig. 1, a typical configuration, with one vector
representing a typical property-denoting adjective
that exhibits relatively strong peaks on one or
more dimensions, whereas noun vectors show a
tendency for broad and flat distributions over their
dimensions. This suggests using a strict selection
function (choosing few very prominent dimen-
sions) for adjectives and a less restrictive one (li-
censing the inclusion of more dimensions of lower
relative prominence) for nouns. Moreover, we are
interested in finding a selection function that re-
lies on as few free parameters as possible in order
to avoid frequency or dimensionality effects.
MPC Selection (MPC). An obvious method
for attribute selection is to choose the most promi-
nent component from any vector (i.e., the highest
absolute value). If a vector exhibits several peaks,
all other components are rejected, their relative
importance notwithstanding. MPC obviously fails
to capture polysemy of targets, which affects ad-
jectives such as hot, in particular.
Threshold Selection (TSel). TSel recasts the
approach of Almuhareb (2006), in selecting all di-
mensions as attributes whose components exceed
a frequency threshold. This avoids the drawback
of MPC, but introduces a parameter that needs to
be optimized. Also, it is difficult to apply absolute
thresholds to composed vectors, as the range of
their components is subject to great variation, and
it is unclear whether the method will scale with
increased dimensionality.
Entropy Selection (ESel). In information the-
ory, entropy measures the average uncertainty in
a probability distribution (Manning and Schu?tze,
1999). We define the entropy H(v) of a
vector v=?v1, . . . , vn? over its components as
H(v) = ??ni=1 P (vi) log P (vi), where P (vi) =
vi/
?n
i=1 vi.
We use H(v) to assess the impact of singular
vector components on the overall entropy of the
vector: We expect entropy to detect components
that contribute noise, as opposed to those that con-
tribute important information.
We define an algorithm for entropy-based at-
tribute selection that returns a list of informa-
tive dimensions. The algorithm successively sup-
presses (combinations of) vector components one
by one. Given that a gain of entropy is equiva-
lent to a loss of information and vice versa, we as-
sume that every combination of components that
leads to an increase in entropy when being sup-
pressed is actually responsible for a substantial
amount of information. The algorithm includes a
back-off to MPC for the special case that a vector
contains a single peak (i.e., H(v) = 0), so that,
in principle, it should be applicable to vectors of
any kind. Vectors with very broad distributions
over their dimensions, however, pose a problem
to this method. For ball in Fig. 1, for instance, the
method does not select any dimension.
Median Selection (MSel). As a further method
we rely on the median m that can be informally
defined as the value that separates the upper from
the lower half of a distribution (Krengel, 2003).
It is less restrictive than MPC and TSel and over-
comes the particular drawback of ESel. Using this
measure, we choose all dimensions whose compo-
nents exceed m. Thus, for the vector representing
433
Pattern Label # Hits (Web) # Hits (ukWaC)
A1 2249 815
A2 36282 72737
A3 3370 1436
A4 ? 7672
A5 ? 3768
N1 ? 682
N2 ? 5073
N3 ? 953
N4 ? 56
Table 1: Number of pattern hits on the Web (Al-
muhareb, 2006) and on ukWaC
ball, WEIGHT, DIRECTION, SHAPE, SPEED and
SIZE are selected.
3.2.4 Vector Composition
We use vector composition as a hinge to com-
bine adjective and noun vectors in order to recon-
struct the triple r=?noun, attribute, adjective?.
Mitchell and Lapata (2008) distinguish two major
classes of vector composition operations, namely
multiplicative and additive operations, that can be
extended in various ways. We use their standard
definitions (denoted ? and +, henceforth). For
our task, we expect ? to perform best as it comes
closest to the linguistic function of intersective ad-
jectives, i.e. to select dimensions that are promi-
nent both for the adjective and the noun, whereas
+ basically blurs the vector components, as can
be seen in the lower part of Fig. 1.
3.3 Model Parameters
We follow Pado? and Lapata (2007) in defining a
semantic space as a matrix M = B?T relating a
set of target elements T to a set of basis elements
B. Further parameters and their instantiations we
use in our model are described below. We use p to
denote an individual lexico-syntactic pattern.
The basis elements of our VSM are nouns de-
noting attributes. For comparison, we use the at-
tributes selected by Almuhareb (2006): COLOR,
DIRECTION, DURATION, SHAPE, SIZE, SMELL,
SPEED, TASTE, TEMPERATURE, WEIGHT.
The context selection function cont(t) deter-
mines the set of patterns that contribute to the rep-
resentation of each target word t ? T . These are
the patterns A1-A5 and N1-N4 (cf. Section 3.2.1).
The target elements represented in the vector
space comprise all adjectives TA that match the
patterns A1 to A5 in the corpus, provided they ex-
ceed a frequency threshold n. During develop-
ment, n was set to 5 in order to filter noise.
As for the target nouns TN , we rely on a repre-
sentative dataset compiled by Almuhareb (2006).
It contains 402 nouns that are balanced with re-
gard to semantic class (according to the WordNet
supersenses), ambiguity and frequency.
As association measure that captures the
strength of the association between the elements
of B and T , we use raw frequency counts4 as ob-
tained from the PoS-tagged and lemmatized ver-
sion of the ukWaC corpus (Baroni et al, 2009).
Table 1 gives an overview of the number of hits
returned by these patterns.
The basis mapping function ? creates the di-
mensions of the semantic space by mapping each
extraction of a pattern p to the attribute it contains.
The pattern value function enables us to sub-
divide dimensions along particular patterns. We
experimented with two instantiations: pvconst
considers, for each dimension, all patterns, while
weighting them equally. pvf (p) awards the ex-
tractions of pattern p with weight 1, while setting
the weights for all patterns different from p to 0.
4 Experiments
We evaluate the performance of the structured
VSM on the task of inferring attributes from
adjective-noun phrases in three experiments: In
Exp1 and Exp2, we evaluate vector representa-
tions capturing r? and r?? independently of one an-
other. Exp3 investigates the selection of hidden
attributes from vector representations constructed
by composition of adjective and noun vectors.
We compare all results against different gold
standards. In Exp1, we follow Almuhareb (2006),
evaluating against WordNet 3.0. For Exp2 and
Exp3, we establish gold standards manually: For
Exp2, we construct a test set of nouns annotated
with their corresponding attributes. For Exp3, we
manually annotate adjective-noun phrases with
the attributes appropriate for the whole phrase. All
experiments are evaluated in terms of precision,
recall and F1 score.
4We experimented with the conditional probability ratio
proposed by Mitchell and Lapata (2009). As it performed
worse on our data, we did not consider it any further.
434
4.1 Exp1: Attribute Selection for Adjectives
The first experiment evaluates the performance of
structured vector representations on attribute se-
lection for adjectives. We compare this model
against a re-implementation of Almuhareb (2006).
Experimental settings and gold standard. To
reconstruct Almuhareb?s approach, we ran his pat-
terns A1-A3 on the ukWaC corpus. Table 1 shows
the number of hits when applied to the Web (Al-
muhareb, 2006) vs. ukWaC. A1 and A3 yield less
extractions on ukWaC as compared to the Web.5
We introduced two additional patterns, A4 and
A5, that contribute about 10,000 additional hits.
We adopted Almuhareb?s manually chosen thresh-
olds for attribute selection for A1-A3; for A4, A5
and a combination of all patterns, we manually se-
lected optimal thresholds.
We experiment with pvconst and all variants of
pvf (p) for pattern weighting (see sect. 3.3). For
attribute selection, we compare TSel (as used by
Almuhareb), ESel and MSel.
The gold standard consists of all adjectives that
are linked to at least one of the ten attributes
we consider by WordNet?s attribute relation
(1063 adjectives in total).
Evaluation results. Results for Exp1 are dis-
played in Table 2. The settings of pv are given in
the rows, the attribute selection methods (in com-
bination with target filtering6) in the columns.
The results for our re-implementation of Al-
muhareb?s individual patterns are comparable to
his original figures7, except for A3 that seems to
suffer from quantitative differences of the under-
lying data. Combining all patterns leads to an
improvement in precision over (our reconstruc-
tion of) Almuhareb?s best individual pattern when
TSel and target filtering are used in combina-
tion. MPC and MSel perform worse (not reported
here). As for target filtering, A1 and A3work best.
Both TSel and ESel benefit from the combina-
tion with the target filter, where the largest im-
provement (and the best overall result) is observ-
5The difference for A2 is an artifact of Almuhareb?s ex-
traction methodology.
6Regarding target filtering, we only report the best filter
pattern for each configuration.
7P(A1)=0.176, P(A2)=0.218, P(A3)=0.504
MPC ESel MSel
P R F P R F P R F
pvf (N1) 0.22 0.06 0.10 0.29 0.04 0.07 0.22 0.09 0.13
pvf (N2) 0.29 0.18 0.23 0.20 0.06 0.09 0.28 0.39 0.33
pvf (N3) 0.34 0.05 0.09 0.20 0.02 0.04 0.25 0.08 0.12
pvf (N4) 0.25 0.02 0.04 0.29 0.02 0.03 0.26 0.02 0.05
pvconst 0.29 0.18 0.22 0.20 0.06 0.09 0.28 0.43 0.34
Table 3: Evaluation results for Experiment 2
able for ESel on pattern A1 only. This is the
pattern that performs worst in Almuhareb?s orig-
inal setting. From this, we conclude that both
ESel and target filtering are valuable extensions
to pattern-based structured vector spaces if preci-
sion is in focus. This also underlines a finding
of Rothenha?usler and Schu?tze (2009) that VSMs
intended to convey specific semantic information
rather than mere similarity benefit primarily from
a linguistically adequate choice of contexts.
Similar to Almuhareb, recall is problematic.
Even though ESel leads to slight improvements,
the scores are far from satisfying. With Al-
muhareb, we note that this is mainly due to a
high number of extremely fine-grained adjectives
in WordNet that are rare in corpora.8
4.2 Exp2: Attribute Selection for Nouns
Exp2 evaluates the performance of attribute selec-
tion from noun vectors tailored to the tuple r??.
Construction of the gold standard. For eval-
uation, we created a gold standard by manually
annotating a set of nouns with attributes. This
gold standard builds on a random sample ex-
tracted from TN (cf. section 3.3). Running N1-
N4 on ukWaC returned semantic vectors for 216
concepts. From these, we randomly sampled 100
concepts that were manually annotated by three
human annotators.
The annotators were provided a matrix consist-
ing of the nouns and the set of ten attributes for
each noun. Their task was to remove all inappro-
priate attributes. They were free to decide how
many attributes to accept for each noun. In order
to deal with word sense ambiguity, the annotators
were instructed to consider all senses of a noun
and to retain every attribute that was acceptable
for at least one sense.
Inter-annotator agreement amounts to ?= 0.69
(Fleiss, 1971). Cases of disagreement were ad-
judicated by majority-voting. The gold standard
435
Almuhareb (reconstr.) VSM (TSel + Target Filter) VSM (ESel) VSM (ESel + Target Filter)
P R F Thr P R F Patt Thr P R F P R F Patt
pvf (A1) = 1 0.183 0.005 0.009 5 0.300 0.004 0.007 A3 5 0.231 0.045 0.076 0.519 0.035 0.065 A3
pvf (A2) = 1 0.207 0.039 0.067 50 0.300 0.033 0.059 A1 50 0.084 0.136 0.104 0.240 0.049 0.081 A3
pvf (A3) = 1 0.382 0.020 0.039 5 0.403 0.014 0.028 A1 5 0.192 0.059 0.090 0.375 0.027 0.050 A1
pvf (A4) = 1 0.301 0.020 0.036 A3 10 0.135 0.055 0.078 0.272 0.020 0.038 A1
pvf (A5) = 1 0.295 0.008 0.016 A3 24 0.105 0.056 0.073 0.315 0.024 0.045 A3
pvconst 0.420 0.024 0.046 A1 183 0.076 0.152 0.102 0.225 0.054 0.087 A3
Table 2: Evaluation results for Experiment 1
contains 424 attributes for 100 nouns.
Evaluation results. Results for Exp2 are given
in Table 3. Performance is lower in comparison to
Exp1. We hypothesize that the tuple r?? might not
be fully captured by overt linguistic patterns. This
needs further investigation in future research.
Against this background, MPC is relatively pre-
cise, but poor in terms of recall. ESel, being
designed to select more than one prominent di-
mension, counterintuitively fails to increase re-
call, suffering from the fact that many noun vec-
tors show a rather flat distribution without any
strong peak. MSel turns out to be most suitable
for this task: Its precision is comparable to MPC
(with N3 as an outlier), while recall is consider-
ably higher. Overall, these results indicate that at-
tribute selection for adjectives and nouns, though
similar, should be viewed as distinct tasks that re-
quire different attribute selection methods.
4.3 Exp3: Attribute Selection for
Adjective-Noun Phrases
In this experiment, we compose noun and adjec-
tive vectors in order to yield a new combined rep-
resentation. We investigate whether the seman-
tic information encoded by the components of this
new vector is sufficiently precise to disambiguate
the attribute dimensions of the original represen-
tations (see section 3.1) and, thus, to infer hidden
attributes from adjective-noun phrases (see (2)) as
advocated by Pustejovsky (1995).
Construction of the gold standard. For evalu-
ation, we created a manually annotated test set of
adjective-noun phrases. We selected a subset of
property-denoting adjectives that are appropriate
modifiers for the nouns from TN using the pred-
icative pattern P1 (see sect. 3) on ukWaC. This
8For instance: bluish-lilac, chartreuse or pink-lavender
as values of the attribute COLOR.
yielded 2085 adjective types that were further re-
duced to 386 by frequency filtering (n = 5). We
sampled our test set from all pairs in the carte-
sian product of the 386 adjectives and 216 nouns
(cf. Exp2) that occurred at least 5 times in a sub-
section of ukWaC. To ensure a sufficient number
of ambiguous adjectives in the test set, sampling
proceeded in two steps: First, we sampled four
nouns each for a manual selection of 15 adjectives
of all ambiguity levels in WordNet. This leads to
60 adjective-noun pairs. Second, another 40 pairs
were sampled fully automatically.
The test set was manually annotated by the
same annotators as in Exp2. They were asked to
remove all attributes that were not appropriate for
a given adjective-noun pair, either because it is not
appropriate for the noun or because it is not se-
lected by the adjective. Further instructions were
as in Exp2, in particular regarding ambiguity.
The overall agreement is ?=0.67. After adjudi-
cation by majority voting, the resulting gold stan-
dard contains 86 attributes for 76 pairs. 24 pairs
could not be assigned any attribute, either because
the adjective did not denote a property, as in pri-
vate investment, or the most appropriate attribute
was not offered, as in blue day or new house.
We evaluate the vector composition methods
discussed in section 3.2.4. Individual vectors for
the adjectives and nouns from the test pairs were
constructed using all patterns A1-A5 and N1-N4.
For attribute selection, we tested MPC, ESel and
MSel. The results are compared against three
baselines: BL-P implements a purely pattern-
based method, i.e. running the patterns that ex-
tract the triple r (A1, A4, N1, N3 and N4, with
JJ and NN instantiated accordingly) on the pairs
from the test set. BL-N and BL-Adj are back-offs
for vector composition, taking the respective noun
or adjective vector, as investigated in Exp1 and
Exp2, as surrogates for a composed vector.
436
MPC ESel MSel
P R F P R F P R F
? 0.60 0.58 0.59 0.63 0.46 0.54 0.27 0.72 0.39
+ 0.43 0.55 0.48 0.42 0.51 0.46 0.18 0.91 0.30
BL-Adj 0.44 0.60 0.50 0.51 0.63 0.57 0.23 0.83 0.36
BL-N 0.27 0.35 0.31 0.37 0.29 0.32 0.17 0.73 0.27
BL-P 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Table 4: Evaluation results for Experiment 3
Evaluation results. Results are given in Table
4. Attribute selection based on the composition of
adjective and noun vectors yields a considerable
improvement of both precision and recall as com-
pared to the individual results obtained in Exp1
and Exp2. Comparing the results of Exp3 against
the baselines reveals two important aspects of our
work. First, the complete failure of BL-P9 un-
derlines the attractiveness of our method to build
structured vector representations from patterns of
reduced complexity. Second, vector composition
is suitable for selecting hidden attributes from
adjective-noun phrases that are jointly encoded
by adjective and noun vectors: Both composition
methods we tested outperform BL-N.
However, the choice of the composition method
matters: ? performs best with a maximum pre-
cision of 0.63. This confirms our expectation
that vector multiplication is a good approxima-
tion for attribute selection in adjective-noun se-
mantics. Being outperformed by BL-Adj in most
categories, + is less suited for this task.
All selection methods outperform BL-Adj in
precision. Comparing MPC and ESel, ESel
achieves better precision when combined with the
?-operator, while doing worse for recall. The
robust performance of MPC is not surprising as
the test set contains only ten adjective-noun pairs
that are still ambiguous with regard to the at-
tributes they elicit. The stronger performance of
the entropy-based method with the ?-operator is
mainly due to its accuracy on detecting false posi-
tives, in that it is able to return ?empty? selections.
In terms of precision, MSel did worse in general,
while recall is decent. This underlines that vector
composition generally promotes meaningful com-
ponents, but MSel is too inaccurate to select them.
Given the performance of the baselines and
the noun vectors in Exp2, we consider this a
very promising result for our approach to attribute
9The patterns used yield no hits for the test pairs at all.
selection from structured vector representations.
The results also corroborate the insufficiency of
previous approaches to attribute learning from ad-
jectives alone.
5 Conclusions and Outlook
We proposed a structured VSM as a framework
for inferring hidden attributes from the composi-
tional semantics of adjective-noun phrases.
By reconstructing Almuhareb (2006), we
showed that structured vector representations of
adjective meaning consistently outperform sim-
ple pattern-based learning, up to 13 pp. in preci-
sion. A combination of target filtering and pat-
tern weighting turned out to be effective here, by
selecting particulary meaningful lexico-syntactic
contexts and filtering adjectives that are not
property-denoting. Further studies need to inves-
tigate this phenomenon and its most appropriate
formulation in a vector space framework.
Moreover, the VSM offers a natural represen-
tation for sense ambiguity of adjectives. Compar-
ing attribute selection methods on adjective and
noun vectors shows that they are sensitive to the
distributional structure of the vectors, and need to
be chosen with care. Future work will investigate
these selection methods in high-dimensional vec-
tors spaces, by using larger sets of attributes.
Exp3 shows that the composition of pattern-
based adjective and noun vectors robustly reflects
aspects of meaning composition in adjective-noun
phrases, with attributes as a hidden dimension.
It also suggests that composition is effective in
disambiguation of adjective and noun meanings.
This hypothesis needs to be substantiated in fur-
ther experiments.
Finally, we showed that composition of vectors
representing complementary meaning aspects can
be beneficial to overcome sparsity effects. How-
ever, our compositional approach meets its lim-
its if the patterns capturing adjective and noun
meaning in isolation are too sparse to acquire suf-
ficiently populated vector components from cor-
pora. For future work, we envisage using vector
similarity to acquire structured vectors for infre-
quent targets from semantic spaces that convey
less linguistic structure to address these remain-
ing sparsity issues.
437
References
Almuhareb, Abdulrahman. 2006. Attributes in Lexi-
cal Acquisition. Ph.D. Dissertation, Department of
Computer Science, University of Essex.
Baroni, Marco and Alessandro Lenci. to appear.
Distributional Memory. A General Framework for
Corpus-based Semantics. Computational Linguis-
tics.
Baroni, Marco, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources and Evaluation, 43(3):209?226.
Baroni, Marco, Brian Murphy, Eduard Barbu, and
Massimo Poesio. 2010. Strudel. A Corpus-based
Semantic Model of Based on Properties and Types.
Cognitive Science, 34:222?254.
Cimiano, Philipp. 2006. Ontology Learning and Pop-
ulation from Text. Algorithms, Evaluation and Ap-
plications. Springer.
Erk, Katrin and Sebastian Pado?. 2008. A Structured
Vector Space Model for Word Meaning in Context.
In Proceedings of EMNLP, Honolulu, HI.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge, Mass.
Fleiss, Joseph L. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Hartung, Matthias and Anette Frank. 2010. A
Semi-supervised Type-based Classification of Ad-
jectives. Distinguishing Properties and Relations.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, Valletta,
Malta, May.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1993. Towards the Automatic Identification of Ad-
jectival Scales. Clustering Adjectives According to
Meaning. In Proceedings of the 31st Annual Meet-
ing of the Association of Computational Linguistics,
pages 172?182.
Krengel, Ulrich. 2003. Wahrscheinlichkeitstheorie
und Statistik. Vieweg, Wiesbaden.
Manning, Christopher D. and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Mas-
sachusetts.
Marcus, Mitchell P., Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3,
ldc99t42. CD-ROM. Philadelphia, Penn.: Linguis-
tic Data Consortium.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June.
Mitchell, Jeff and Mirella Lapata. 2009. Lan-
guage Models Based on Semantic Composition. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singa-
pore, August 2009, pages 430?439, Singapore, Au-
gust.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based Construction of Semantic Space
Models. Computational Linguistics, 33:161?199.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia, 17?21 July 2006, pages 113?120.
Pustejovsky, James. 1995. The Generative Lexicon.
MIT Press, Cambridge, Mass.
Rothenha?usler, Klaus and Hinrich Schu?tze. 2009. Un-
supervised Classification with Dependency Based
Word Spaces. In Proceedings of the EACL Work-
shop on Geometrical Models of Natural Language
Semantics (GEMS), pages 17?24, Athens, Greece,
March.
Sowa, John F. 2000. Knowledge Representation.
Logical, Philosophical, and Computational Foun-
dations. Brooks Cole.
Turney, Peter D. and Patrick Pantel. 2010. From Fre-
quency to Meaning. Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Widdows, Dominic. 2008. Semantic Vector Products.
Some Initial Investigations. In Proceedings of the
2nd Conference on Quantum Interaction, Oxford,
UK, March.
438
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 540?551,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploring Supervised LDA Models for Assigning Attributes to
Adjective-Noun Phrases
Matthias Hartung and Anette Frank
Computational Linguistics Department
Heidelberg University
{hartung,frank}@cl.uni-heidelberg.de
Abstract
This paper introduces an attribute selection
task as a way to characterize the inherent mea-
ning of property-denoting adjectives in adjec-
tive-noun phrases, such as e.g. hot in hot sum-
mer denoting the attribute TEMPERATURE,
rather than TASTE. We formulate this task
in a vector space model that represents adjec-
tives and nouns as vectors in a semantic space
defined over possible attributes. The vectors
incorporate latent semantic information ob-
tained from two variants of LDA topic mod-
els. Our LDA models outperform previous ap-
proaches on a small set of 10 attributes with
considerable gains on sparse representations,
which highlights the strong smoothing power
of LDA models. For the first time, we extend
the attribute selection task to a new data set
with more than 200 classes. We observe that
large-scale attribute selection is a hard prob-
lem, but a subset of attributes performs ro-
bustly on the large scale as well. Again, the
LDA models outperform the VSM baseline.
1 Introduction
Corpus-based statistical modeling of semantics is
gaining increased attention in computational linguis-
tics. This field of research includes distributional
vector space models (VSMs), i.e., models that rep-
resent the semantics of words or phrases as vectors
over high-dimensional cooccurrence data (Turney
and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as
well as latent variable models (LVMs) which aggre-
gate distributional observations in ?hidden?, or latent
variables, thereby reducing the dimensionality of the
data. An example of the latter are topic models (Blei
et al, 2003), which have recently been applied to
modeling selectional preferences of verbs (Ritter et
al., 2010; ?O Se?aghdha, 2010), or word sense disam-
biguation (Li et al, 2010).
A topic that is increasingly studied in distribu-
tional semantics is the semantics of adjectives, both
in isolation (Almuhareb, 2006) and in compositional
adjective-noun phrases (Hartung and Frank, 2010;
Guevara, 2010; Baroni and Zamparelli, 2010).
In this paper, we propose a new approach to a
problem we denote as attribute selection: The task is
to predict the hidden attribute meaning expressed by
a property-denoting adjective in composition with
a noun. The adjective hot, e.g., may denote at-
tributes such as TEMPERATURE, TASTE or EMO-
TIONALITY. These adjective meanings can be com-
bined with nouns such as tea, soup or debate, which
can be characterized in terms of attributes as well.
The goal of the task is to determine the hidden at-
tribute meaning predicated over the noun in a given
adjective-noun phrase, as illustrated in (1).
(1) a. a hotvalue summerconcept
b. TEMPERATURE(summer) = hot
It is by way of the composition of adjective and
noun that specific attributes are selected from the ad-
jective?s space of possible attribute meanings, and
typically lead to a disambiguation of the adjective
and possibly the noun. Hartung and Frank (2010)
were the first to model this insight in a VSM by rep-
resenting the meaning of adjectives and nouns in se-
mantic vectors defined over attributes. The meaning
of adjective-noun phrases is computed by means of
540
C
O
L
O
R
D
IR
E
C
T
IO
N
D
U
R
A
T
IO
N
SH
A
PE
SI
Z
E
SM
E
L
L
SP
E
E
D
TA
ST
E
T
E
M
PE
R
A
T
U
R
E
W
E
IG
H
T
~e 1 1 0 1 45 0 4 0 0 21
~b 14 38 2 20 26 0 45 0 0 20
~e ?~b 14 38 0 20 1170 0 180 0 0 420
~e+~b 15 39 2 21 71 0 49 0 0 41
Figure 1: Vectors for enormous (~e) and ball (~b)
vector composition, such that the ?hidden? attribute
meaning of the phrase can be ?selected? as a promi-
nent component from the composed vector. This is
illustrated in Fig. 1 for the adjective enormous (~e)
in combination with the noun ball (~b), with alter-
native composition operations: vector multiplication
(?) and addition (+).1 Both yield SIZE as the most
prominent component in the composed vector.
In the present paper we offer a new approach to
this formalization of the compositional meaning of
adjectives and nouns that owes to both distributional
VSMs and LVMs. Through this combination, we
attempt to improve on earlier work in Almuhareb
(2006) and Hartung and Frank (2010), which are
both embedded in a purely distributional setting.
Specifically, we use Latent Dirichlet Allocation
(LDA; Blei et al (2003)) to train an attribute model
that captures semantic information encoded in ad-
jectives and nouns independently of one another.
Following Hartung and Frank (2010), this model is
embedded into a VSM that employs vector com-
position to combine the meaning of adjectives and
nouns. We present two variants of LDA that differ
in the way attributes are associated with the induced
LDA topics: Controled LDA (C-LDA) and Labeled
LDA (L-LDA; Ramage et al (2009)). Both will be
presented in detail in Section 3.
Our aims in this paper are two-fold: (i) We inves-
tigate LDA as a modeling framework in the attribute
selection task, as its use of topics as latent variables
may alleviate inherent sparsity problems faced by
prior work using pattern-based (Almuhareb, 2006)
or vector space models (Hartung and Frank, 2010).
(ii) While these prior approaches were restricted to
a confined set of 10 attributes, we will we apply our
1The figure is adopted from the distributional setting of Har-
tung and Frank (2010), with component values defined by pat-
tern frequency counts for the chosen attribute nouns.
models on a much larger space of attributes, to probe
their capacity on a more realistic data set.
The remainder of this paper is divided as fol-
lows. Section 2 reviews related work on distribu-
tional models of adjective semantics, and introduces
the two frameworks in which we ground our ap-
proach: LVMs and VSMs. In Section 3 we introduce
two LDA models for attribute selection: C-LDA and
L-LDA. Section 4 describes the settings for two ex-
periments: In the first experiment, we perform at-
tribute selection confined to a space of 10 attributes
to compare against prior work. In the second setting
we perform attribute selection on a large scale, using
206 attributes. Section 5 presents and discusses the
results. Section 6 concludes.
2 Related Work
Distributional models of adjective semantics.
Almuhareb (2006) aims at capturing the relationship
between adjectives and attributes based on lexico-
syntactic patterns, such as the ATTR of the * is ADJ.
Apart from inherent sparsity issues, his approach
does not account for the compositional nature of the
problem, as the contextual information contributed
by a noun is neglected: For instance, his model is
unable to predict that hot is unlikely to denote TASTE
in the context of summer, other than in hot meal.
Compositionality of adjective-noun phrases and
how it can be adequately modeled in VSMs is
the main concern in Baroni and Zamparelli (2010)
and Guevara (2010), who are in search of the
best composition operator for combining adjective
with noun meanings. While these works adhere
to a purely latent representation of meaning, Har-
tung and Frank (2010) include attributes as sym-
bolic ?hidden? meanings of adjectives, nouns and
adjective-noun phrases in a distributional VSM.
Finally, a large body of work dealing with com-
positionality in distributional frameworks is not con-
fined to the special case of adjective-noun composi-
tion (Mitchell and Lapata (2008), Rudolph and Gies-
brecht (2010), i.a.). All these approaches regard
composition as a process combining vectors (or ma-
trices, resp.) to yield a new, contextualized vector
representation within the same semantic space.
Latent Dirichlet Allocation, aka. Topic Models
(TMs). LDA is a generative probabilistic model
541
for document collections. Each document is repre-
sented as a mixture over latent topics, where each
topic is a probability distribution over words (Blei et
al., 2003). These topics can be used as dense fea-
tures for, e.g., document clustering. Depending on
the number of topics, which has to be pre-specified,
the dimensionality of the document representation
can be considerably reduced in comparison to sim-
ple bag-of-words models. The remainder of this pa-
per will assume some familiarity with LDA and the
LDA terminology as introduced in Blei et al (2003).
Recent work investigates ways of accommodating
supervision with LDA, e.g. supervised topic models
(Blei and McAuliffe, 2007), Labeled LDA (L-LDA)
(Ramage et al, 2009) or DiscLDA (Lacoste-Julien
et al, 2008). We will discuss L-LDA in Section 3.
Distributional VSMs and TMs. The idea to inte-
grate topic models and VSMs goes back to Mitchell
and Lapata (2009) who build a distributional model
with dimensions set to topics over bag-of-words fea-
tures. In their setting, LDA merely serves the pur-
pose of dimensionality reduction, whereas our par-
ticular motivation is to use topics as probabilistic
indicators for the prediction of attributes as seman-
tic target categories in adjective-noun composition.
Mitchell and Lapata (2010) compare VSMs defined
over bags of context words vs. latent topics in a sim-
ilarity judgement task. Their results indicate that a
multiplicative setting works best for vector compo-
sition in word-based models, while vector addition
is better suited for topic vectors.
3 Topic Models for Attribute Selection
3.1 Using LDA for modeling lexical semantics
Recently, LDA has been used for problems in lexical
semantics, where the primary goal is not document
modeling but the induction of semantic knowledge
from high-dimensional co-occurrence data. Ritter et
al. (2010) and ?O Se?aghdha (2010) model selectional
restrictions of verbs by inducing topic distributions
that characterize ?mixtures of topics? observed in
verb argument positions. As a basis for LDA mod-
eling, they collect pseudo-documents, i.e. bags of
words that co-occur in syntactic argument positions.
We apply a similar idea to the attribute selection
problem: we collect pseudo-documents that char-
acterize attributes by adjectives and nouns that co-
occur with the attribute nouns in local contextual re-
lations. The topic distributions obtained from fitting
an LDA model to the collection of these pseudo-
documents can then be injected into semantic vector
representations for adjectives and nouns.
In its original statement, LDA is a fully unsuper-
vised process (apart from the desired number of top-
ics which has to be specified in advance) that es-
timates topic distributions over documents ?d and
topic-word distributions ?t with topics represented
as latent variables. Estimating these parameters on a
document collection yields topic proportions P (t|d)
and topic distributions P (w|t) that can be used to
compute a smooth distribution P (w|d) as in (2),
where t denotes a latent topic, w a word and d a
document in the corpus.
P (w|d) =
?
t
P (w|t)P (t|d) (2)
Being designed for exploratory rather than dis-
criminative analysis, LDA does not intend condi-
tioning of words or topics on external categories.
That is, the resulting topics cannot be related to pre-
viously defined target categories. For attribute se-
lection, the LDA-inferred topics need to be linked
to semantic attributes. Therefore, we apply two ex-
tensions of standard LDA that are capable of taking
supervised category information into account, either
implicitly or directly, by including an additional ob-
servable variable into the generative process.
In general, LVMs can be expected to overcome
sparsity issues that are frequently encountered in
distributional models. This positive smoothing ef-
fect is achieved by marginalization over the latent
variables (cf. Prescher et al (2000)). For instance, it
is unlikely to observe a dependency path linking the
adjective mature to the attribute MATURITY. Such
a relation is more likely for young, for example. If
young co-occurs with mature in a different pseudo-
document (AGE might be a candidate), this results in
a situation where (i) young and mature share one or
more latent topics and (ii) the topic proportions for
the attributes MATURITY and AGE will become sim-
ilar to the extent of common words in their pseudo-
documents. Consequently, the final attribute model
is expected to assign a (small) positive probability to
the relation between mature and MATURITY without
observing it in the training data.
542
3.2 Controled LDA
The generative story behind C-LDA is equivalent to
standard LDA. However, the collection of pseudo-
documents used as input to C-LDA is structured in
a controled way such that each document conveys
semantic information that specifically characterizes
the individual categories of interest (attributes, in
our case). In line with the distributional hypothesis
(Harris, 1968), we consider the pseudo-documents
constructed in this way as distributional fingerprints
of the meaning of the corresponding attribute.
The contents of the pseudo-documents are se-
lected along syntactic dependency paths linking
each attribute noun to meaningful context words (ad-
jectives and nouns).2 A corpus consisting of the two
sentences in (3), e.g., yields a pseudo-document for
the attribute noun SPEED containing car and fast.
(3) What is the speed of this car? The machine
runs at a very fast speed.
Though we are ultimately interested in triples of
attributes, adjectives and nouns that define the com-
positional semantics of adjective-noun phrases (cf.
(1)), C-LDA is only exposed to binary tuples be-
tween attributes and adjectives or nouns, respec-
tively. This is in line with Hartung and Frank
(2010), who obtained substantial performance im-
provements by splitting the ternary relation into two
binary relations.
Presenting LDA with pseudo-documents that cha-
racterize individual target attributes imports super-
vision into the LDA process in two respects: the
estimated topic proportions P (t|d) will be highly
attribute-specific, and similarly so for the topic dis-
tributions P (w|t). This makes the model more ex-
pressive for the ultimate labeling task. Moreover,
since C-LDA collects pseudo-documents focused on
individual target attributes, we are able to link exter-
nal categories to the generative process by heuristi-
cally labeling pseudo-documents with their respec-
tive attribute as target category. Thus, we approx-
imate P (w|a), the probability of a word given an
attribute, by P (w|d) as obtained from LDA:
2The dependency paths, together with the set of attribute
nouns of interest, have to be manually specified. See the sup-
plementary material for the full list of dependency paths used.
1 For each topic k ? {1, . . . , K}:
2 Generate ?k = (?k,1, . . . , ?k,V )T ? Dir(? | ?)
3 For each document d:
4 For each topic k ? {1, . . . ,K}
5 Generate ?(d)k ? {0, 1} ? Bernoulli(? | ?k)
6 Generate ?(d) = L(d) ? ?
7 Generate ?(d) = (?l1 , . . . , ?lMd )
T ? Dir(? | ?(d))
8 For each i in {1, . . . , Nd}:
9 Generate zi ? {?(d)1 , . . . , ?(d)Md} ? Mult(? | ?
(d))
10 Generate wi ? {1, . . . , V } ? Mult(? | ?zi)
Figure 2: L-LDA generative process (Ramage et al 2009)
P (w|a) ? P (w|d) =
?
t
P (w|t)P (t|d) (4)
3.3 Labeled LDA
L-LDA (Ramage et al, 2009) extends standard LDA
to include supervision for specific target categories,
yet in a different way: (i) The generative process
includes a second observed variable, i.e. each doc-
ument is explicitly labeled with a target category.
A document may be labeled with an arbitrary num-
ber of categories; unlabeled documents are also pos-
sible. However, L-LDA permits only binary as-
signments of categories to documents; probabilistic
weights over categories are not intended. (ii) Con-
trary to LDA, where the number of topics has to be
specified in advance, L-LDA sets this parameter to
the number of unique target categories. Moreover,
the model is constrained such that documents may
be assigned only those topics that correspond to their
observable category label(s). That is, latent topics
t in the standard formulation of LDA (2) are con-
strained to correspond to explicit labels a.
More specifically, L-LDA extends the generative
process of LDA by constraining the topic distribu-
tions over documents ?(d) to only those topics that
correspond to the document?s set of labels ?(d). This
is done by projecting the parameter vector of the
Dirichlet topic prior ? to a lower-dimensional vec-
tor ?(d) whose topic dimensions correspond to the
document labels.
This extension is integrated in steps 5 and 6 of
Fig. 2: First, in step 5, the document?s labels ?(d)
are generated for each topic k. The resulting vector
of document?s labels ?(d) = {k | ?(d)k = 1} is used
to define a document-specific label projection matrix
543
L(d)|?(d)|?K , such that L
(d)
ij = 1 if ?
(d)
i = j, and 0 oth-
erwise. This matrix is used in step 6 to project the
Dirichlet topic prior ? to a lower-dimensional vec-
tor ?(d), whose topic dimensions correspond to the
document labels. Topic proportions are then, in step
7, generated for this reduced parameter space.
In our instantiation of L-LDA, we collect pseudo-
documents for attributes exactly as for C-LDA. Doc-
uments are labeled with exactly one category, the at-
tribute noun. Note that, even though the relationship
between documents and topics is fixed, the one be-
tween topics and words is not. Any word occurring
in more than one document will be assigned a non-
zero probability for each corresponding topic.
Thus, with regard to attribute modeling, C-LDA
and L-LDA build an interesting pair of opposites:
The L-LDA model assumes that attributes are se-
mantically primitive in the sense that they cannot
be decomposed into smaller topical units, whereas
words may be associated with several attributes at
the same time. C-LDA, at the other end of the spec-
trum, licenses semantic variability on both the at-
tribute and the word level. Particularly, a word might
be associated with some of the topics underlying an
attribute, but not with all of them, and an attribute
can be characterized by multiple topics.
3.4 Vector Space Framework
For integrating the information obtained from C-
LDA or L-LDA into a distributional VSM, we fol-
low Hartung and Frank (2010): Adjectives and
nouns are modeled as independent semantic vectors
along their relationship to attributes; the most promi-
nent attribute(s) that represent the hidden meaning
of adjective-noun phrases are selected from their
composition (cf. Fig. 1).
The dimensions of the VSM are set to the pre-
selected attributes. Semantic vectors are computed
for all adjectives and nouns occurring at least five
times in the pseudo-documents. Vector component
values v?w,a? are derived from the C-LDA and L-
LDA models in different ways: with C-LDA we
obtain P (w|a) by approximation from P (w|d) (cf.
equation (4)), while in L-LDA we obtain P (w|a) di-
rectly from the induced topic-word distribution ?t,
through labeled topics t = a (cf. equation (2)).
Vector composition is defined as vector multipli-
cation (?) or vector addition (+).
For attribute selection on the composed vector, we
use two methods we found to perform best in Har-
tung and Frank (2010): Entropy Selection (ESel)
and Most Prominent Component (MPC). ESel mea-
sures entropy over the vector components to identify
components that encode a high amount of informa-
tion. It selects all attributes that lead to an increase of
entropy when suppressed from the vector represen-
tation. If no informative components can be detected
in a vector due to a very broad, flat distribution of
the probability mass (cf. ~b in Fig. 1), ESel yields an
empty list. MPC always chooses exactly one vector
component, i.e. the one with the highest value.
4 Experimental Settings
Attribute selection over small and large semantic
spaces. We evaluate the performance of the VSMs
based on C-LDA and L-LDA in two experimental
settings, contrasting the problem of attribute selec-
tion on semantic spaces of radically different dimen-
sionality, using sets of 10 vs. 206 attributes.
Evaluation measures. We evaluate against two
gold standards consisting of adjective-noun phrases
(or adjective-noun pairs) and their associated at-
tribute meanings. We report precision, recall and
f1-score. Where appropriate, we test differences in
the performance of various model configurations for
statistical significance in a randomized permutation
test (Yeh, 2000), using the sigf tool (Pado?, 2006).
Baselines. We compare our models against two
baselines, PATTVSM and DEPVSM. PATTSVM is
reconstructed from Hartung and Frank (2010). It is
grounded in a selection of lexical patterns that iden-
tify the target elements (adjectives and nouns) for
the vector basis elements (i.e., the attribute nouns)
in a local context window. The component values
are defined using raw frequency counts over the ex-
tracted patterns. DEPVSM is similar to PATTVSM;
however, it relies on dependency paths that connect
the target elements and attributes in local contexts.
The paths are identical to the ones used for con-
structing pseudo-documents in C-LDA and L-LDA.
As in PATTVSM, the vector components are set to
raw frequencies over extracted paths.
Implementations. To implement our models, we
rely on MALLET (McCallum, 2002) for C-LDA and
544
the Stanford Topic Modeling Toolbox3 for L-LDA.
In both cases, we run 1000 iterations of Gibbs sam-
pling, using default values for all hyperparameters.
Data set for attribute selection over 10 attributes.
The first experiment is conducted on the data set
used in Hartung and Frank (2010). It consists of
100 adjective-noun pairs manually annotated for
ten attributes: COLOR, DIRECTION, DURATION,
SHAPE, SIZE, SMELL, SPEED, TASTE, TEMPER-
ATURE, WEIGHT. To enable comparison, the di-
mensions of our models are set to exactly these at-
tributes.
Data set for attribute selection over a large se-
mantic space (206 attributes). In the second ex-
periment, we max out the attribute selection task
to a much larger set of attributes in order to an-
alyze the difficulty of the task on more represen-
tative data. We automatically construct a data set
of adjective-noun phrases labeled with appropriate
attributes from WordNet 3.0 (Fellbaum, 1998), re-
lying on the assumption that examples given in
glosses correspond to the respective word sense of
the adjective. We first extract all adjectives that
are linked to at least one attribute synset by the
attribute relation. Next, we run the glosses of
these adjectives (3592 in number) through TreeTag-
ger (Schmid, 1994) to find examples of adjectives
modifying nouns in attributive constructions. The
resulting adjective-noun phrases are labeled with the
attribute label linked to the given adjective sense.
This method yields 7901 labeled adjective-noun
phrases. They are divided into development and test
data according to a sampling procedure that respects
the following criteria: (i) Both sets must contain
all attributes with an equal number of phrases for
each attribute; (ii) phrases with both elements con-
tained in CoreWordNet4 are preferred, while others
are only considered if necessary to satisfy the first
criterion. This procedure yields 496/345 phrases
in the development/test set, distributed over 206 at-
tributes5.
3http://nlp.stanford.edu/software/tmt/.
4A subset of WordNet restricted to the 5000 most fre-
quently used word senses. Available from: http://
wordnetcode.princeton.edu/standoff-files/
core-wordnet.txt
5If an attribute provides only one example, this was added
to the development set. Therefore, the test set only comprises
Training data. The pseudo-documents are collec-
ted from dependency paths obtained from section 2
of the parsed pukWaC corpus (Baroni et al, 2009).
5 Discussion of Results
5.1 Experiment 1
In Experiment 1, we evaluate the performance
of C-LDA and L-LDA on the attribute selection
task over 10 attributes against the pattern-based
and dependency-based models PATTVSM and DE-
PVSM as competitive baselines. Besides a com-
parison to standard VSMs, we are especially in-
terested in the relative performance of the LDA
models. Given that C-LDA and L-LDA estimate
attribute-specific topic distributions in the structured
pseudo-documents under different assumptions re-
garding the correspondence of attributes and topics
(cf. Sec. 3.2 and 3.3), we expect the two LDA vari-
ants to differ in their capability to capture the topic
distributions in the labeled pseudo-documents.
5.1.1 Attribute Selection for 10 Attributes
Tables 1 and 2 summarize the results for at-
tribute selection over 10 attributes against the la-
beled adjective-noun pairs in the test set, using ESel
and MPC as selection functions on vectors com-
posed by multiplication (Table 1) and addition (Ta-
ble 2). The results reported for C-LDA correspond
to the best performing model (with number of top-
ics set to 42, as this setting yields the best and most
constant results over both composition operators).
C-LDA shows highest f-scores and recall over all
settings, and highest precision with vector addition.6
In line with Mitchell and Lapata (2010) (cf. Sec. 2),
we obtain the best overall results with vector addi-
tion (ESel: P: 0.55, R: 0.66, F: 0.61; MPC: P: 0.59,
R: 0.71, F: 0.64). The difference between C-LDA
and L-LDA is small but significant for vector mul-
tiplication; for vector addition, it is not significant.
Compared to the LDA models, the VSM baselines
206 attributes, while all models were trained on 262 attributes
obtained from WordNet in the first extraction step.
6In Tables 1 and 2, statistical significance of the differences
between the models is marked by the superscripts L, D and P,
denoting a significant difference over L-LDA, DepVSM and
PattVSM, respectively. All differences reported are significant
at p < 0.05, except for the difference between C-LDA and L-
LDA in Table 3 (p < 0.1).
545
ESel MPC
P R F P R F
C-LDA 0.58 0.65 0.61L,P 0.57 0.64 0.60
L-LDA 0.68 0.54 0.60D 0.55 0.61 0.58D
DepVSM 0.48 0.58 0.53P 0.57 0.60 0.58
PattVSM 0.63 0.46 0.54 0.60 0.58 0.59
Table 1: Attribute selection over 10 attributes (?)
ESel MPC
P R F P R F
C-LDA 0.55 0.66 0.61D,P 0.59 0.71 0.64
L-LDA 0.53 0.57 0.55D,P 0.50 0.45 0.47D,P
DepVSM 0.38 0.65 0.48P 0.57 0.60 0.58
PattVSM 0.71 0.35 0.47 0.47 0.56 0.51
Table 2: Attribute selection over 10 attributes (+)
are competitive, but tend to perform lower. This ef-
fect is statistically significant for ESel with vector
multiplication: each of the LDA models statistically
significantly outperforms one of the VSM models,
DEPVSM and PATTVSM. With ESel and vector
addition, both LDA models outperform both VSM
models statistically significantly. The LDAESel,+
models outperform the PATTVSMESel,+ model of
Hartung and Frank (2010) by a high margin in
f-score: +0.14 for C-LDA; +0.08 for L-LDA.
Compared to the stronger multiplicative settings
PATTVSMESel,? and PATTVSMMPC,? this still
represents a plus of +0.07 and +0.02 in f-score, re-
spectively. We further observe a clear improvement
of the LDA models over the VSM models in terms of
recall (+0.20, C-LDAESel,+ vs. PATTVSMESel,?),
at the expense of some loss in precision (-0.08, C-
LDAESel,+ vs. PATTVSMESel,?). This clearly con-
firms a stronger generalization power of LDA com-
pared to VSM models.
With regard to selection functions, we observe
that MPC tends to perform better for the VSM mod-
els, while ESel is more suitable in the LDA models.
Figures 3 and 4 display the overall performance
curve ranging over different topic numbers for C-
LDAESel,+ and C-LDAESel,? ? compared to the
remaining models that are not dependent on topic
size. For topic numbers smaller than the attribute set
size, C-LDA underperforms, for obvious reasons.
Increasing ranges of topic numbers to 60 does not
show a linear effect on performance. Parameter set-
tings with performance drops below the VSM base-
lines are rare, which holds particularly for vector ad-
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0  10  20  30  40  50  60
F-
Sc
or
e
Num. Topics
C-LDA
L-LDA
DepVSM
PattVSM
Figure 3: Performance of C-LDAESel,? for different
topic numbers, compared against all other models
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0  10  20  30  40  50  60
F-
Sc
or
e
Num. Topics
C-LDA
L-LDA
DepVSM
PattVSM
Figure 4: Performance of C-LDAESel,+ for different
topic numbers, compared against all other models
dition at topic ranges larger than 10. With vector
addition, C-LDA outperforms L-LDA in almost all
configurations, yet at an overall lower performance
level of L-LDA (0.55 with addition vs. 0.6 with mul-
tiplication). Note that in the multiplicative setting,
C-LDA reaches the performance of L-LDA only in
its best configurations, while with vector addition it
obtains high performance that exceeds L-LDA?s top
f-score of 0.6 for topic ranges between 10 and 20.
Based on these observations, vector addition
seems to offer the more robust setting for C-LDA,
the model that is less strict with regard to topic-
attribute correspondences. Vector multiplication, on
the other hand, is more suitable for L-LDA and its
stricter association of topics with class labels.
5.1.2 Smoothing Power of LDA Models
Our hypothesis was that LDA models should be
better suited for dealing with sparse data, compared
546
ESel MPC
P R F P R F
C-LDA 0.39 0.31 0.35 0.37 0.27 0.32
L-LDA 0.30 0.18 0.23 0.20 0.18 0.19
DepVSM 0.20 0.10 0.13 0.37 0.26 0.30
PattVSM 0.00 0.00 0.00 0.00 0.00 0.00
Table 3: Performance figures on sparse vectors (?)
ESel MPC
P R F P R F
C-LDA 0.43 0.33 0.38 0.44 0.28 0.34
L-LDA 0.34 0.16 0.22 0.37 0.18 0.24
DepVSM 0.16 0.17 0.17 0.36 0.21 0.27
PattVSM 0.13 0.04 0.06 0.17 0.25 0.20
Table 4: Performance figures on sparse vectors (+)
to pattern-based or purely distributional approaches.
While this is broadly confirmed in the above results
by global gains in recall, we conduct a special evalu-
ation focused on those pairs in the test set that suffer
from sparse data. We selected all adjective and noun
vectors that did not yield any positive component
values in the PATTVSM model. The 22 adjective-
noun pairs in the test set affected by these ?zero vec-
tors? were evaluated using the remaining models.
The results in Tables 3 and 4 yield a very clear pic-
ture: C-LDA obtains highest precision, recall and
f-score across all settings, followed by L-LDA and
DEPVSMESel, while their ranks are reversed when
using MPC. Again, MPC works better for the VSM
models, ESel for the LDA models. Vector addition
performs best for C-LDA with f-scores of 0.38 and
0.34 ? outperforming the pattern-based results on
sparse vectors by orders of magnitude.
5.2 Experiment 2
Experiment 2 is designed to max out the space of
attributes to be modeled, to assess the capacity of
both LDA models and the DEPVSM baseline model
in the attribute selection task on a large attribute
space.7 In contrast to Experiment 1, with its con-
fined semantic space of 10 target attributes, this rep-
resents a huge undertaking.
5.2.1 Large-scale Attribute Selection
Table 5 (column all) displays the performance of
all models on attribute selection over a range of 206
7We did not apply PATTVSM to this large-scale experiment,
as only poor performance can be expected.
all property
? + ? +
C-LDA 0.04 0.02 0.18L,D 0.10D
L-LDA 0.03 0.04 0.15 0.15
DepVSM 0.02 0.02 0.12 0.07
Table 5: Performance figures (in f-score) of C-LDAESel
on 206 (all) and 73 property attributes (property)
all property
P R F P R F
WIDTH 0.67 1.00 0.80 1.00 0.50 0.67
WEIGHT 0.80 0.57 0.67 0.50 0.57 0.53
MAGNETISM 0.50 1.00 0.67
SPEED 0.50 0.50 0.50 1.00 0.50 0.67
TEXTURE 0.33 1.00 0.50 0.33 1.00 0.50
DURATION 0.50 0.50 0.50 1.00 1.00 1.00
TEMPERATURE 0.30 0.75 0.43 0.43 0.75 0.55
AGE 0.33 0.50 0.40
THICKNESS 1.00 0.25 0.40 0.50 0.13 0.20
DEGREE 1.00 0.20 0.33
LENGTH 0.17 1.00 0.29 0.50 1.00 0.67
DEPTH 1.00 0.14 0.25 1.00 0.86 0.92
ACTION 0.17 0.50 0.25
LIGHT 0.33 0.17 0.22 0.20 0.17 0.18
POSITION 0.14 0.25 0.18 0.20 0.25 0.22
SHARPNESS 1.00 1.00 1.00
SERIOUSNESS 0.50 1.00 0.67
COLOR 0.13 0.25 0.17 0.29 0.50 0.36
LOYALTY 1.00 1.00 1.00
average 0.49 0.54 0.51 0.63 0.63 0.63
Table 6: Attribute selection on 206 attributes (all) and 73
property attributes (property); performance figures of C-
LDAESel,? for best attributes (F>0)
dimensions, contrasting vector addition and multi-
plication. The number of topics was set to 400. As
the overall performance is close to 0 for both com-
position methods, no parameter setting can be iden-
tified as particularly suited for this large-scale at-
tribute selection task. The differences between the
three models are very small and not significant8.
5.2.2 Focused Evaluation and Data Analysis
To gain a deeper insight into the modeling capac-
ity of the LDA models for this large-scale selection
task, Table 6 (column all) presents a partial evalua-
tion of attributes that could be assigned to adjective-
noun pairs with an f-score >0 by C-LDAESel,?.
Despite the disappointing overall performance of
8Again, statistically significant differences are marked by
superscripts (cf. footnote 6). All differences reported are sig-
nificant at ? < 0.05.
547
prediction correct
thin layer THICKNESS THICKNESS
heavy load WEIGHT WEIGHT
shallow water DEPTH DEPTH
short holiday DURATION DURATION
attractive force MAGNETISM MAGNETISM
short hair LENGTH LENGTH
serious book DIFFICULTY MIND
blue line COLOR UNION
weak president POSITION POWER
fluid society REPUTE CHANGEABLENESS
short flight DISTANCE DURATION
rough bark TEXTURE EVENNESS
faint heart CONSTANCY COWARDICE
Table 7: Sample of correct and false predictions of C-
LDAESel,? in Experiment 2
the LDA models on this large attribute space, it is
remarkable that C-LDA is able to induce distinctive
topic distributions for a number of attributes with up
to 0.51 f-score with balanced precision and recall,
a moderate drop of only -0.10 relative to the corre-
sponding model induced over 10 attributes.
Raising the attribute selection task from 10 to 206
attributes poses a true challenge to our models, by
the sheer size and diversity of the semantic space
considered. Table 7 gives an insight into the nature
of the data and the difficulty of the task, by listing
correct and false preditions of C-LDA for a small
sample of adjective-noun pairs. Possible explana-
tions for false predictions are manifold, among them
near misses (e.g. serious book, weak president, short
flight, rough bark), idiomatic expressions (e.g. faint
heart, blue line) or questionable labels provided by
WordNet (e.g. serious book).
As seen above, C-LDA achieves relatively high
performance figures on selected attributes (cf. Table
6, col. all). In order to identify what makes these
attributes different from others that resist success-
ful modeling, we investigated three factors: (i) the
amount of training data available for each attribute,
(ii) the ambiguity rate per attribute, and (iii) their
ontological subtype.
(i) Measuring the dependence between training
data size and f-score per attribute shows that a large
amount of training data is generally helpful, but not
the decisive factor (Pearson?s r = 0.19, p < 0.01).
(ii) The ambiguity rate ARattr per attribute attr
is computed by averaging over all test pairs TPattr
labeled with attr, counting the total number of at-
tributes attr? that are associated with each adjective
in pairs ?adj, n? ? TPattr in WordNet:
ARattr =
?
attr?
?
?adj,n??TPattr |?adj, attr??WN |
|TPattr |
Correlating this figure with the performance per at-
tribute in terms of f-score yields only a small pos-
itive correlation (Pearson?s r = 0.23, p < 0.01).
In fact, the qualitative analysis in Table 7 shows that
C-LDA is capable of assigning meaningful attributes
to adjective-noun phrases not only in easy, but also
ambiguous cases (cf. shallow water, where DEPTH
is the only attribute provided for shallow in Word-
Net vs. short holiday, short hair or short flight).
(iii) Although the 206 attributes used in Exp. 2 are
rather diverse, including concepts such as HEIGHT,
KINDNESS or INDIVIDUALITY, we observe a high
number of attributes from Exp. 1 that are success-
fully modeled in Exp. 2 (5 out of 10, cf. column
all in Table 6). Given that they are categorized into
the property class in WordNet9, we presume that the
varying performance across attributes might be in-
fluenced by their ontological subtype. This hypoth-
esis is validated in a replication of Exp. 2, with train-
ing data limited to the 73 attributes pertaining to the
property subtype in WordNet. The test set was re-
stricted accordingly, resulting in 112 pairs that are
linked to a property attribute.
The overall performance of the models in this ex-
periment is shown in Table 5 (column property):
With vector multiplication, the best-performing op-
eration across all models, all models benefit consid-
erably (+0.10 or more). C-LDA shows the largest
improvement, significantly outperforming both L-
LDA and DEPVSM. With vector addition, the per-
formance gains are slightly lower in general. In
this setting, L-LDA shows higher f-score than C-
LDA, though this difference is not statistically sig-
nificant. Still, C-LDA significantly outranges DE-
PVSM. Note that we can not show a significant dif-
ference between C-LDAESel,? and L-LDAESel,+,
so the comparison between these models remains in-
conclusive here. Note further that the affinity of C-
LDA with vector addition and L-LDA with vector
multiplication, respectively, is inverted in the large-
scale experiment (cf. Table 5).
9WordNet separates attributes into properties, qualities and
states, among several others.
548
While these overall results are far from satisfac-
tory, they still clearly indicate that the LDA models
work effectively for at least a subset of attributes,
and outperform the VSM baseline.
Again, a more detailed analysis is given in Ta-
ble 6 (column property), showing the performance
of the best individual property attributes (F>0) in
the restricted experiment. Average performance of
the best property attributes with F>0, individually,
amounts to F=0.6310. In comparison to the unres-
tricted setting (cf. column all), nearly all property
attributes benefit from model training on selective
data. Exceptions are WIDTH, WEIGHT, THICKNESS,
AGE, DEGREE and LIGHT. Thus, apparently, some
of the adjectives associated with non-property at-
tributes in the full set provide some discriminative
power that is helpful to distinguish property types.
In a qualitative analysis of the 133 non-property
attributes filtered out in this experiment, we find that
the WordNet-SUMO mapping (Niles, 2003) does
not provide differentiating definitions for about 60%
of these attributes, linking them instead to a single
subjective assessment attribute. This suggests that
in many cases the distinctions drawn by WordNet
are too subtle even for humans to reproduce.
6 Conclusion
This paper explored the use of LDA topic models
in a semantic labeling task that predicts attributes
as ?hidden? meanings in the compositional seman-
tics of adjective-noun phrases. LDA topic models
are expected to alleviate sparsity problems of dis-
tributional VSMs as encountered in prior work, by
incorporating latent semantic information about at-
tribute nouns. We investigated two variants of LDA
that employ different degrees of supervision for as-
sociating topics with attributes.
Our contributions are as follows. We proposed
two LDA models for the attribute selection task that
import supervision for a target category parameter
in different ways: L-LDA (Ramage et al, 2009)
embeds the target categories into the LDA process,
by defining a 1:1 correspondence of topics and tar-
get categories. C-LDA, by contrast, does not af-
fect the LDA generative process. Here, we heuris-
10In comparison, L-LDAESel,? yields an average f-score of
0.47 for attributes with F>0 in the property setting.
tically equate pseudo-documents with target cate-
gories, to approximate category-specific word-topic
distributions. By adhering to standard LDA, C-LDA
accommodates a greater variety in the distributions
of topics to attribute-specific documents and words,
as compared to L-LDA. Combining standard LDA
topic modeling with a means of interpreting the in-
duced topics relative to a set of external categories,
C-LDA offers greater flexibility and expressiveness.
Our experimental results show that modeling at-
tributes as latent or explicit topics with C-LDA and
L-LDA, respectively, outperforms the purely distri-
butional baseline model DEPVSM and PATTVSM
of prior work. Targeted evaluation on sparse data
points confirms that LDA models help to overcome
inherent sparsity effects of VSMs. C-LDA and L-
LDA are close in performance in Experiment 1. C-
LDA outperforms L-LDA only with optimal topic
parameter settings.
Finally, we probed the modeling capacity of LDA
and VSM models on a vast space of 206 attributes.
This task proved to be extremely difficult. However,
we obtain respectable results on a subset of attributes
denoting properties, where C-LDA performs best in
quantitative performance measures. It yields high-
est f-scores in full and partial evaluation ? both with
the full-size attribute model, and when training and
testing is restricted to property attributes. The differ-
ences are small, but statistically significant between
the LDA models and the VSM baseline in a setting
restricted to property attributes.
Data analysis indicates that our models perform
more robustly on concrete attributes in contrast to
abstract attribute types that lack clear categorization.
This suggests that our approach to attribute selec-
tion is most appropriate for detecting attributes that
reflect clear ontological distinctions.
However, there is ample space for improvement.
In Hartung and Frank (2011), we show that the
quality of the noun vectors lags behind the adjec-
tive vectors. This clearly affects the performance
of our models in cases where the semantic contri-
bution of the noun is decisive for disambiguation.
Future work will focus on ways to enhance the noun
vector representations through additional contextual
features, to make them denser and more articulated
in structure.
549
References
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Ph.D. Dissertation, Department of Com-
puter Science, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional Memory. A General Framework for Corpus-
based Semantics. Computational Linguistics, 36:673?
721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, East
Stroudsburg, PA, pages 1183?1193.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky Wide Web: A Collection of Very
Large Linguistically Processed Web-Crawled Corpora.
Language Resources and Evaluation, 43:209?226.
D. Blei and J. McAuliffe. 2007. Supervised topic mod-
els. Neural Information Processing Systems, 21.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
Stroudsburg, PA. Association for Computational Lin-
guistics.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley.
Matthias Hartung and Anette Frank. 2010. A Structured
Vector Space Model for Hidden Attribute Meaning in
Adjective-Noun Phrases. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING), Beijing, China, August.
Matthias Hartung and Anette Frank. 2011. Assessing in-
terpretable, attribute-related meaning representations
for adjective-noun phrases in a similarity prediction
task. In Proceedings of GEometrical Models of Nat-
ural Language Semantics (GEMS-2011), Edinburgh,
UK.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative Learning for Dimen-
sionality Reduction and Classification. In NIPS, vol-
ume 22.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, pages 1138?1147, Upp-
sala, Sweden.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio, June.
Jeff Mitchell and Mirella Lapata. 2009. Language Mod-
els Based on Semantic Composition. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, Singapore, August 2009,
pages 430?439, Singapore, August.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34:1388?1429.
Ian Niles. 2003. Mapping WordNet to the SUMO Ontol-
ogy. In Proceedings of the IEEE International Knowl-
edge Engineering conference, pages 23?26, June.
Diarmuid ?O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435?444, Uppsala, Sweden, July.
Association for Computational Linguistics.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using a
probabilistic class-based lexicon for lexical ambiguity
resolution. In Proceedings of the 18th COLING, pages
649?655.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Singapore, August 2009, pages 248?256.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 424?434, Uppsala, Sweden, July. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 907?916. Asso-
ciation for Computational Linguistics, July.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing (NeMLaP). Manchester, U.K., 14?
16 September 1994.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
550
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the Fourth Conference on Computational Lan-
guage Learning (CoNLL-2000) and the Second Learn-
ing Language in Logic Workshop, Lisbon, Portugal,
pages 947?953.
551
A Resource-Poor Approach for
Linking Ontology Classes to
Wikipedia Articles
Nils Reiter
Matthias Hartung
Anette Frank
University of Heidelberg (Germany)
email: reiter@cl.uni-heidelberg.de
Abstract
The applicability of ontologies for natural language processing depends
on the ability to link ontological concepts and relations to their realisa-
tions in texts. We present a general, resource-poor account to create such
a linking automatically by extracting Wikipedia articles corresponding to
ontology classes. We evaluate our approach in an experiment with the
Music Ontology. We consider linking as a promising starting point for
subsequent steps of information extraction.
381
382 Reiter, Hartung, and Frank
1 Introduction
Ontologies are becoming increasingly popular as a means for formal, machine-read-
able modelling of domain knowledge, in terms of concepts and relations. Linking
ontological concepts and relations to their natural language equivalents is of utmost
importance for ontology-based applications in natural language processing. Providing
larger quantities of text that clearly belongs to a given ontological concept is a pre-
requisite for further steps towards ontology population with relations and instances.
We thus consider this work as a point of departure for future work on populating and
lexicalizing ontologies, and their use in semantic processing.
In this paper we present a method that provides relevant textual sources for a do-
main ontology by linking ontological classes to the most appropriate Wikipedia arti-
cles describing the respective ontological class. The paper is structured as follows:
We discuss related work in Section 2. Section 3 presents our method for linking on-
tology classes to Wikipedia articles. The method is implemented and tested using the
music ontology (Raimond et al, 2007) and a Wikipedia dump of 2007. We present
this experiment in Section 4 and its evaluation in Section 5. Section 6 concludes and
gives an outlook on directions of future work.
2 Related Work
Our goal is to detect the most appropriateWikipedia article for a given ontology class.
As Wikipedia is a domain-independent resource, it usually contains many more senses
for one concept name than does a domain-specific ontology. Thus, one of the chal-
lenges we meet is the need for disambiguation between multiple candidate articles
with respect to one specific ontology class.1 Therefore, we compare our approach to
previous work on sense disambiguation. Since in our approach, we aim at minimiz-
ing the degree of language- and resource dependency, our focus is on the amount of
external knowledge used.
One method towards sense disambiguation that has been studied is to use different
kinds of text overlap: Ruiz-Casado et al (2005) calculate vector similarity between a
Wikipedia article and WordNet glosses based on term frequencies. Obviously, such
glosses are not available for all languages, domains and applications. Wu and Weld
(2007) and Cucerzan (2007) calculate the overlap between contexts of named entities
and candidate articles from Wikipedia, using overlap ratios or similarity scores in a
vector space model, respectively. Both approaches disambiguate named entities using
textual context. Since our aim is to acquire concept-related text sources, these methods
are not applicable.
A general corpus-based approach has been proposed by Reiter and Buitelaar (2008):
Using a domain corpus and a domain-independent reference corpus, they select the
article with the highest domain relevance score among multiple candidates. This ap-
proach works reasonably well but relies on the availability of domain-specific corpora
and fails at selecting the appropriate among multiple in-domain senses. In contrast,
our resource-poor approach does not rely on additional textual resources, as ontologies
usually do not contain contexts for classes.
1Mihalcea (2007) shows that Wikipedia can indeed be used as a sense inventory for sense
disambiguation.
A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 383
3 Linking Ontology classes to Wikipedia articles
This section briefly reviews relevant information about Wikipedia and describes our
method for linking ontology classes to Wikipedia articles. Our algorithm consists of
two steps: (i) extracting candidate articles from Wikipedia and (ii) selecting the most
appropriate one. The algorithm is independent of the choice of a specific ontology.2
3.1 Wikipedia
The online encyclopedia Wikipedia currently comprises more than 2,382,000 articles
in about 250 languages. Wikipedia is interesting for our approach because it is semi-
structured and articles usually talk about one specific topic.
The structural elements in Wikipedia that we rely on are links between articles,
inter-language links, disambiguation and redirect pages. Inter-language links refer to
an article about the same topic in a different language. Disambiguation pages collect
the different senses of a given term. Redirect pages point to other pages, allowing for
spelling variations, abbreviations and synonyms.
3.2 Extracting the candidate articles
The first step of our algorithm is to extract candidate articles for ontology classes. The
method we employ is based on Reiter and Buitelaar (2008). The algorithm starts with
the English label LC of an ontology class C, and tries to retrieve the article that bears
the same title.3 Any Wikipedia page P retrieved by this approach falls into one of
three categories:
1. P is an article: The template {{otheruses}} in the article indicates that a
disambiguation page exists which lists further candidate articles for C. The
disambiguation page is then retrieved and we proceed with step 2. Otherwise,
P is considered to be the only article for C.
2. P is a disambiguation page: The algorithm extracts all links on P and considers
every linked page as a candidate article.4
3. P is a redirect page: The redirect is being followed and the algorithm checks
the different cases once again.
3.3 Features for the classifier
We now discuss the features we apply to disambiguate candidate articles retrieved by
our candidate extraction method with regard to the respective ontology class. Some
features use structural properties of both Wikipedia and the ontology, others are based
on shallow linguistic processing.
2It is still dependent on the language used for coding ontological concepts (here English). In future
work we aim at bridging between languages using Wikipedia?s inter-language links or other multi-lingual
resources.
3We use common heuristics to cope with CamelCase, underscore whitespace alternation etc.
4Note that, apart from pointing to different readings of a term, disambiguation pages sometimes include
pages that are clearly not a sense of the given term. Distinguishing these from true/appropriate readings of
the term is not trivial.
384 Reiter, Hartung, and Frank
Domain relevance
Wikipedia articles can be classified according to their domain relevance by computing
the proportion of domain terms they contain. In this paper, we explore several variants
of matching a set of domain terms against the article in question:
Class labels. The labels of all concepts in the ontology are used as a set of domain
terms.
? We extract the nouns from the POS-tagged candidate article. The relative fre-
quency of domain terms is then computed for the complete article and for nouns
only, both for types and for tokens.
? We compute the frequency of domain terms in the first paragraph only, assuming
it contains domain relevant key terms.
? The redirects pointing to the article in question, i.e., spelling variations and
synonyms, are extracted. We then compute their relative frequency in the set of
class labels.
Comments. As most ontologies contain natural language comments for classes, we
use them to retrieve domain terms. All class comments extracted from the ontology are
POS-tagged. We use all nouns as domain terms and compute their relative frequencies
in the article.
Class vs. Instance
It is intuitively clear that a class in the ontology needs to be linked to a Wikipedia
article representing a class rather than an instance.5 We extract the following features
in order to detect whether an article represents a class or an instance, thus being able
to reject certain articles as inappropriate link targets for a particular class.
Translation distance. Instances inWikipedia are usually named entities (NEs). Thus,
the distinction between concepts and instances can, to a great extent, be rephrased as
the problem of NE detection. As our intention is to develop a linking algorithm which
is, in principle, language-independent, we decided to rely on the inter-language links
provided by Wikipedia. The basic idea is that NEs are very similar across different
languages (at least in languages using the same script), while concepts show a greater
variation in their surface forms across different languages. Thus, for the inter-language
links on the article in question that use latin script, we compute the average string sim-
ilarity in terms of Levenshtein Distance (Levenshtein, 1966) between the title of the
page and its translations.
Templates. Wikipedia offers a number of structural elements that might be useful
in order to distinguish instances from concepts. In particular, the infobox template
is used to express structured information about instances of a certain type and some
of their properties. Thus, we consider articles containing an infobox template to
correspond to an instance.
5We are aware of the fact that the distinction between classes and instances is problematic on both sides:
Ontologies described in OWL Full or RDF do not distinguish clearly between classes and instances and
Wikipedia does not provide an explicit distinction either.
A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 385
4 Experiment
4.1 The Music Ontology
We test our approach on the Music Ontology (MO) (Raimond et al, 2007). The MO
has been developed for the annotation of musical entities on the web and provides
capabilities to encode data about artists, their albums, tracks on albums and the process
of creating musical items.
The ontology defines 53 classes and 129 musical properties (e.g. melody) in its
namespace, 78 external classes are referenced. Most of the classes are annotated with
comments in natural language. TheMO is connected to several other ontologies (W3C
time6, timeline7, event8, FOAF9), making it an interesting resource for domain rele-
vant IE tasks and generalisation of the presented techniques to further domains. The
MO is defined in RDF and freely available10.
4.2 Experimental Setup
The experiment is divided into two steps: candidate page selection and classification
(see Section 3). For candidate selection we extract Wikipedia pages with titles that
are near-string identical to the 53 class labels. 28 of them are disambiguation pages.
From these pages, we extract the links and use them as candidates. The remaining 25
are directly linked to a single Wikipedia article.
To test our classification features, we divide the overall set of ontology classes in
training and test sets of 43 and 10 classes, respectively, that need to be associated
with their most appropriate candidate article. We restrict the linking to one most
appropriate article. For the classification step, we extract the features discussed in
Section 3.
Since the candidate set of pages shows a heavily skewed distribution in favour of
negative instances, we generate an additional training set by random oversampling
(Batista et al, 2004) in order to yield training data with a more uniform distribution
of positive and negative instances.
5 Evaluation
For evaluation, the ambiguous concepts in the ontology have been manually linked
to Wikipedia articles. The linking was carried out independently by three annotators,
all of them computational linguists. Each annotator was presented the class label,
its comment as provided by the ontology and the super class from which the class
inherits. On the Wikipedia side, all pages found by our candidate extraction method
were presented to the annotators.
The inter-annotator agreement is ? = 0.68 (Fleiss? Kappa). For eight concepts, all
three annotators agreed that none of the candidate articles is appropriate and for ten all
three agreed on the same article. These figures underline the difficulty of the problem,
as the information contained in domain ontologies and Wikipedia varies substantially
with respect to granularity and structure.
6www.w3.org/TR/owl-time/
7motools.sourceforge.net/timeline/timeline.html
8motools.sourceforge.net/event/event.html
9xmlns.com/foaf/spec/
10musicontology.com
386 Reiter, Hartung, and Frank
Candidate article selection. Candidate selection yields 16 candidate articles per
concept on average. These articles contain 1567 tokens on average. The minimal
and maximal number of articles per concepts are 3 and 38, respectively.
Candidate article classification. We train a decision tree11 using both the original
and the oversampled training sets as explained above.
Table 1: Results after training on original and over-sampled data
Positives Negatives Average
orig. samp. orig. samp. orig. samp.
P 1 0.63 0.87 0.97 0.94 0.80
R 0.17 0.83 1 0.91 0.58 0.87
F 0.27 0.71 0.93 0.94 0.75 0.83
Table 1 displays precision, recall and f-score results for positive and negative in-
stances as well as their average. As the data shows, oversampling can increase the
performance considerably. We suspect this to be caused not only by the larger training
set, but primarily by the more uniform distribution.
The table shows further that the negative instances can be classified reliably us-
ing the original or oversampled data set. However, as we intend to select positive
appropriate Wikipedia articles rather than to deselect inappropriate ones, we are par-
ticularly interested in good performance for the positive instances. We observe that
this approach identifies positive instances (i.e., appropriate Wikipedia articles) with a
reasonable performancewhen using the oversampled training set. It is noteworthy that
not a single feature performs better than with an f-measure of 0.6 when used alone.
The figures shown in Table 1 are obtained using the combination of all features.
Table 2: Results for combination of best features only
Positives Negatives
P 0.60 1.00
R 1.00 0.88
F 0.75 0.94
In Table 2, we present the results for the best performing features taken together
(using oversampling on the training set): nountypes-classlabels (F-measure: 0.6),
langlinks (0.5), redirects-classlabels (0.5), nountokens-classlabels (0.44),
fulltextclasslabels (0.44). Recall improves considerably, while there is a small
decrease in precision.
6 Conclusions
We have presented ongoing research on linking ontology classes to appropriate Wiki-
pedia articles. We consider this task a necessary step towards automatic ontology
lexicalization and population from texts.
11We used the ADTree implementation in the Weka toolkit www.cs.waikato.ac.nz/ml/weka/.
A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 387
The crucial challenge in this task is to deal with the high degree of ambiguity that
is introduced by the fact that Wikipedia covers a large amount of fine-grained infor-
mation for numerous domains. This leads to a great number of potential candidate
articles for a given ontology class.
Our approach to this problem is independent of the particular ontology that is used
as a starting point. Moreover, it merely depends on a set of rather shallow but effec-
tive features which can be easily extracted from the domain ontology and Wikipedia,
respectively. From the results we derived in our experiments with the Music Ontol-
ogy, we conclude that our approach is feasible and yields reasonable results even for
small domain ontologies, provided we can overcome highly skewed distributions of
the training examples due to an overwhelming majority of negative instances. In fu-
ture work we will apply the methods described here to different domain ontologies and
use the selectedWikipedia articles as a starting point for extracting instances, relations
and attributes.
Acknowledgements. We kindly thank our annotators for their effort and R?diger
Wolf for technical support.
References
Batista, G., R. Prati, and M. C. Monard (2004). A Study of the Behavior of Several
Methods for BalancingMachine Learning Training Data. SIGKDD Explorations 6,
20?29.
Cucerzan, S. (2007). Large-Scale Named Entity Disambiguation Based on Wikipedia
Data. In Proc. of EMNLP, Prague.
Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions,
and reversals. Soviet Physics Doklady 10, 707?710.
Mihalcea, R. (2007). Using Wikipedia for Automatic Word Sense Disambiguation.
In Proc. of NAACL-07, Rochester, New York, pp. 196?203.
Raimond, Y., S. Abdallah, M. Sandler, and F. Giasson (2007). The Music Ontol-
ogy. In Proc. of the 8th International Conference on Music Information Retrieval,
Vienna, Austria.
Reiter, N. and P. Buitelaar (2008). Lexical Enrichment of Biomedical Ontologies. In
Information Retrieval in Biomedicine: Natural Language Processing for Knowl-
edge Integration. IGI Global, to appear.
Ruiz-Casado, M., E. Alfonseca, and P. Castells (2005). Automatic Assignment of
Wikipedia Encyclopedic Entries to WordNet Synsets. In Proc. of the 3rd Atlantic
Web Intelligence Conference, Volume 3528, Lodz, Poland, pp. 380?385.
Wu, F. and D. S. Weld (2007). Autonomously Semantifying Wikipedia. In Proc. of
the Conference on Information and Knowledge Management, Lisboa, Portugal.
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 52?61,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Assessing Interpretable, Attribute-related Meaning Representations for
Adjective-Noun Phrases in a Similarity Prediction Task
Matthias Hartung and Anette Frank
Computational Linguistics Department
Heidelberg University
{hartung,frank}@cl.uni-heidelberg.de
Abstract
We present a distributional vector space model
that incorporates Latent Dirichlet Allocation
in order to capture the semantic relation hold-
ing between adjectives and nouns along inter-
pretable dimensions of meaning: The meaning
of adjective-noun phrases is characterized in
terms of ontological attributes that are promi-
nent in their compositional semantics. The
model is evaluated in a similarity prediction
task based on paired adjective-noun phrases
from the Mitchell and Lapata (2010) bench-
mark data. Comparing our model against a
high-dimensional latent word space, we ob-
serve qualitative differences that shed light
on different aspects of similarity conveyed
by both models and suggest integrating their
complementary strengths.
1 Introduction
This paper offers a comparative evaluation of two
types of accounts to the compositional meaning of
adjective-noun phrases. This comparison is embed-
ded in a similarity judgement task that determines
the semantic similarity of pairs of adjective-noun
phrases. All models we consider establish the sim-
ilarity of adjective-noun pairs by measuring simi-
larity between vectors representing the meaning of
the individual adjective-noun phrases. However, the
models we investigate differ in the type of interpreta-
tion they assign to adjectives, nouns and the phrases
composed from them.
One type of approach is represented by the clas-
sical vector space model (VSM) of Mitchell and La-
pata (2010; henceforth: M&L). It represents the se-
mantics of adjective-noun phrases in latent seman-
tic space, based on dimensions defined by bags of
context words. This classical model will be com-
pared against a compositional analysis of adjective-
noun phrases that represents adjectives and nouns
along interpretable dimensions of meaning, i.e. dis-
crete ontological attributes such as SIZE, COLOR,
SPEED, WEIGHT. Here, lexical vectors for adjec-
tives and nouns define possible attribute meanings as
component values; vector composition is intended
to elicit those attributes that are prominent in the
meaning of the whole phrase. For instance, a com-
posed vector representation of the phrase hot pep-
per is expected to yield high component values on
the dimensions TASTE and SMELL, rather than TEM-
PERATURE. The underlying relations between ad-
jectives and nouns, respectively, and the attributes
they denote is captured by way of latent semantic in-
formation obtained from Latent Dirichlet Allocation
(LDA; Blei et al (2003)). Thus, we treat attributes
as an abstract meaning layer that generalizes over
latent topics inferred by LDA and utilize this inter-
pretable layer as the dimensions of our VSM.
This approach has been shown to be effective
in an attribute selection task (Hartung and Frank,
2011), where the goal is to predict the most promi-
nent attribute(s) ?hidden? in the compositional se-
mantics of adjective-noun phrases. In this paper,
our main interest is to assess the potential of mod-
eling adjective semantics in terms of discrete, inter-
pretable attribute meanings in a similarity judgement
task, as opposed to a representation in latent seman-
tic space that is usually applied to tasks of this kind.
52
For this purpose, we rely on the evaluation data
set of M&L which serves as a shared benchmark in
the GEMS 2011 workshop. Their similarity judge-
ment task, being tailored to measuring latent simi-
larity, represents a true challenge for an analysis fo-
cused on discrete ontological attributes.
Our results show that the latent semantic model
of M&L cannot be beaten by an interpreted anal-
ysis based on LDA topic models. However, we
show substantial performance improvements of the
interpreted analysis in specific settings with adapted
training and test sets that enable focused compar-
ison. An interesting outcome of our investiga-
tions is that ? using an interpreted LDA analysis of
adjective-noun phrases ? we uncover divergences in
the notions of similarity underlying the judgement
task that go virtually unnoticed in a latent semantic
VSM, while they need to be clearly distinguished in
models focused on interpretable representations.
The paper is structured as follows: After a brief
summarization of related work, Section 3 introduces
Controled LDA, a weakly supervised extension to
standard LDA, and explains how it can be utilized to
inject interpretable meaning dimensions into VSMs.
In Section 4, we describe the parameters and exper-
imental settings for comparing our model to M&L?s
word-based latent VSM in a similarity prediction
task. Section 5 presents the results of this experi-
ment, followed by a thorough qualitative analysis of
the specific strengths and weaknesses of both mod-
els in Section 6. Section 7 concludes.
2 Related Work
Recent work in distributional semantics has engen-
dered different perspectives on how to character-
ize the semantics of adjectives and adjective-noun
phrases.
Almuhareb (2006) aims at capturing the seman-
tics of adjectives in terms of attributes they denote
using lexico-syntactic patterns. His approach suf-
fers from severe sparsity problems and does not ac-
count for the compositional nature of adjective-noun
phrases, as it disregards the meaning contributed by
the noun. It is therefore unable to perform disam-
biguation of adjectives in the context of a noun.
Baroni and Zamparelli (2010) and Guevara
(2010) focus on how best to represent composition-
ality in adjective-noun phrases considering differ-
ent types of composition operators. These works
adhere to a fully latent representation of mean-
ing, whereas Hartung and Frank (2010) assign sym-
bolic attribute meanings to adjectives, nouns and
composed phrases by incorporating attributes as di-
mensions in a compositional VSM. By holding the
attribute meaning of adjectives and nouns in dis-
tinct vector representations and combining them
through vector composition, their approach im-
proves on both weaknesses of Almuhareb?s work.
However, their account is still closely tied to Al-
muhareb?s pattern-based approach in that counts of
co-occurrence patterns linking adjectives and nouns
to attributes are used to populate the vector represen-
tations. These, however, are inherently sparse. The
resulting model therefore still suffers from sparsity
of co-occurrence data.
Finally, Latent Dirichlet Allocation, originally de-
signed for tasks such as text classification and doc-
ument modeling (Blei et al, 2003), found its way
into lexical semantics. Ritter et al (2010) and
?O Se?aghdha (2010), e.g., model selectional restric-
tions of verb arguments by inducing topic distribu-
tions that characterize mixtures of topics observed in
verb argument positions. Mitchell and Lapata (2009,
2010) were the first to use LDA-inferred topics as
dimensions in VSMs.
Hartung and Frank (2011) adopt a similar ap-
proach, by embedding LDA into a VSM for
adjective-noun meaning composition, with LDA
topics providing latent variables for attribute mean-
ings. That is, contrary to M&L, LDA is used to
convey information about interpretable semantic at-
tributes rather than latent topics. In fact, Hartung
and Frank (2011) are able to show that ?injecting?
topic distributions inferred from LDA into a VSM
alleviates sparsity problems that persisted with the
pattern-based VSM of Hartung and Frank (2010).
Baroni et al (2010) highlight two strengths of
VSMs that incorporate interpretable dimensions of
meaning: cognitive plausibility and effectiveness in
concept categorization tasks. In their model, con-
cepts are characterized in terms of salient proper-
ties and relations (e.g., children have parents, grass
is green). However, their approach concentrates on
nouns. Open questions are (i) whether it can be ex-
tended to further word classes, and (ii) whether the
53
interpreted meaning layers are interoperable across
word classes, to cope with compositionality. The
present paper extends their work by offering a test
case for an interpretable, compositional VSM, ap-
plied to adjective-noun composition with attributes
as a shared meaning layer. Moreover, to our knowl-
edge, we are the first to expose such a model to a
pairwise similarity judgement task.
3 Attribute Modeling based on LDA
3.1 Controled LDA
This section introduces Controled LDA (C-LDA), a
weakly supervised variant of LDA. We use C-LDA
to model attribute information that pertains to ad-
jectives and nouns individually. This information is
?injected? into a vector-space framework as a ba-
sis for computing the attributes that are prominent
in compositional adjective-noun phrases.
In its original statement, LDA is a fully unsu-
pervised process that estimates topic distributions
over documents ?d and word-topic distributions ?t
with topics represented as hidden variables. Esti-
mating these parameters on a document collection
yields topic proportions P (t|d) and topic distribu-
tions P (w|t) that can be used to compute a smooth
distribution P (w|d) as in (1), where t denotes a la-
tent topic, w a word and d a document in the corpus.
P (w|d) =
?
t
P (w|t)P (t|d) (1)
While the generative story underlying both mod-
els is identical, C-LDA extends standard LDA by
?implicitly? taking supervised category information
into account. This allows for linking latent topics to
interpretable semantic attributes. The idea is to col-
lect pseudo-documents in a controlled way such that
each document conveys semantic information about
one specific attribute. The pseudo-documents are
selected along syntactic dependency paths linking
the respective attribute noun to meaningful context
words (adjectives and nouns). A corpus consisting
of the two sentences in (2), e.g., yields a pseudo-
document for the attribute noun SPEED containing
car and fast.
(2) What is the speed of this car? The machine
runs at a very fast speed.
Note that, though we are ultimately interested
in triples between attributes, adjectives and nouns
that are conveyed by the compositional semantics
of adjective-noun phrases, C-LDA is only exposed
to binary tuples between attributes and adjectives or
nouns, respectively. This is in line with the findings
of Hartung and Frank (2010), who obtained sub-
stantial performance improvements by splitting the
triples into separate binary relations.
3.2 Embedding C-LDA into a VSM
The main difference of C-LDA compared to stan-
dard LDA is that the estimated topic proportions
P (t|d) of the former will be highly attribute-
specific, and similarly so for the topic distributions
P (w|t). We experiment with two variants of VSMs
that differ in the way they integrate attribute infor-
mation inferred from C-LDA, denoted as C-LDA-A
and C-LDA-T.
In C-LDA-A, the dimensions of the space are in-
terpretable attributes. The vector components re-
lating a target word w to an attribute a are set to
P (w|a). This probability is obtained from C-LDA
by constructing the pseudo-documents as distribu-
tional fingerprints of the respective attribute, as de-
scribed in Section 3.1 above:
P (w|a) ? P (w|d) =
?
t
P (w|t)P (t|d) (3)
C-LDA-T capitalizes on latent topics as dimen-
sions; the vector components are set to the topic pro-
portions P (w|t) as directly obtained from C-LDA.1
4 Parameters and Experimental Settings
Data. Our experiments are based on the adjective-
noun section of M&L?s 2010 evaluation data set2. It
consists of 108 pairs of adjective-noun phrases that
were rated for similarity by human judges.
1The ?topics as dimensions? approach has also been used
by Mitchell and Lapata (2010) for dimensionality reduction. In
their word space model, however, this setting leads to a decrease
in performance on adjective-noun phrases. Therefore, we do
not compare ourselves to this instantiation of their model in this
paper.
2Available from: http://homepages.inf.ed.ac.
uk/s0453356/share
54
Models. We contrast the two LDA-based models
(i, ii) C-LDA-A and C-LDA-T with two standard
VSMs: (iii) a re-implementation of the latent VSM
of M&L and (iv) a dependency-based VSM (De-
pVSM) which relies on dependency paths that con-
nect the target elements and attribute nouns in local
contexts. The paths are identical to the ones used
for constructing pseudo-documents in (i) and (ii).
Thus, DepVSM relies on the same information as
C-LDA-A and C-LDA-T, without capitalizing on the
smoothing power provided by LDA.
In the C-LDA models, we experiment with several
topic number settings. Depending on the number of
attributes |A| contained in the training material (see
below), we train one model instance for each topic
number in the range from 0.5 ? |A| to 2 ? |A|. For our
LDA implementations, we use MALLET (McCal-
lum, 2002). We run 1000 iterations of Gibbs sam-
pling with hyperparameters set to the default values.
Training data. For C-LDA-A, C-LDA-T and De-
pVSM we apply two different training scenarios:
In the first setting, we collect pseudo-documents
instantiating 262 attribute nouns that are linked to
adjectives by an attribute relation in WordNet
(Fellbaum, 1998). The topic distributions induced
from this data cover the broadest space of attribute
meanings we could produce from WordNet3. In a
second setting, we assume the presence of an ?or-
acle? that confines the training data to a subset of
33 attribute nouns that are linked to those adjectives
that actually occur in the M&L test set, to allow for
a focused evaluation. In both C-LDA variants, all
adjectives and nouns occurring at least five times in
the pseudo-documents become target elements in the
VSM. The pseudo-documents are collected along
dependency paths extracted from section 2 of the
pukWaC corpus (Baroni et al, 2009). The same set-
tings are used for training the DepVSM model.
As the M&L model is not intended to reflect at-
tribute meaning, the training data for this model re-
mains constant. Like M&L, we set the target el-
ements of this model to all types contained in the
complete evaluation data set (including nouns, ad-
3Note that in Hartung and Frank (2011) only a subset of
these attributes, mainly those characterized as properties in
WordNet, could be successfully modeled, at overall moderate
performance levels.
jectives and verbs) and select the 2000 context words
that co-occur most frequently with these targets in
pukWaC 2 as the dimensions of the space.
Filters on test set. Given the different types of
?semantic gist? of the models described above, we
expect that the LDA models perform best on those
test pairs that involve attributes known to the model.
To test this expectation, we compile a restricted test
set containing 43 pairs (adj1 n1, adj2 n2) where
both adj1 and adj2 bear an attribute meaning accord-
ing to WordNet.
Composition operators. In our experiments, we
use a subset of the operators proposed by Mitchell
and Lapata (2010) to obtain a compositional repre-
sentation of adjective-noun phrases from individual
vectors: vector multiplication (?; best operator in
M&L?s experiments on adjective-noun phrases) and
vector addition (+). Besides, in order to assess the
contribution of individual vectors in the composi-
tion process, we experiment with two ?composition
surrogates? by taking the individual adjective (ADJ-
only) or noun vector (N-only) as the result of the
composition process.
Evaluating the models. The models described
above are evaluated against the human similarity
judgements data provided by Mitchell and Lapata
(2010) as follows: We compute the cosine similar-
ity between the composed vectors representing the
adjective-noun phrases in each test pair. Next, we
measure the correlation between the model scores
and the human judgements in terms of Spearman?s
?, where each human rating is treated as an indi-
vidual data point. The correlation coefficient finally
reported is the average over all instances4 of one
model. For completeness, we also report the corre-
lation score of the best model instance and the stan-
dard deviation over all model instances.
5 Discussion of Results
Results on complete test set. Table 1 displays the
results achieved by the VSMs based on C-LDA and
4In fact, only those model instances resulting in a significant
correlation with the human judgements (p < 0.05) are taken
into account. This way, we eliminate both inefficient and overly
optimistic model instances.
55
+ ? ADJ-only N-only
avg best ? avg best ? avg best ? avg best ?
26
2
at
tr
s C-LDA-A 0.19 0.25 0.05 0.15 0.20 0.04 0.17 0.23 0.04 0.11 0.23 0.06
C-LDA-T 0.19 0.24 0.02 0.28 0.31 0.02 0.20 0.24 0.02 0.18 0.24 0.03
M&L 0.21 0.34 0.19 0.27
DepVSM -0.09 -0.09 -0.14 -0.08
33
at
tr
s C-LDA-A 0.23 0.27 0.02 0.21 0.24 0.01 0.27 0.29 0.01 0.17 0.22 0.02
C-LDA-T 0.21 0.28 0.03 0.14 0.23 0.04 0.22 0.27 0.03 0.10 0.21 0.06
M&L 0.21 0.34 0.19 0.27
DepVSM 0.21 0.20 0.27 0.19
Table 1: Correlation coefficients (Spearman?s ?) for different training sets, complete test set
+ ? ADJ-only N-only
avg best ? avg best ? avg best ? avg best ?
26
2
at
tr
s
(fi
lte
re
d) C-LDA-A 0.22 0.31 0.07 0.12 0.30 0.11 0.18 0.30 0.08 0.17 0.28 0.07C-LDA-T 0.25 0.30 0.03 0.26 0.35 0.04 0.24 0.29 0.04 0.19 0.23 0.04
M&L 0.38 0.40 0.24 0.43
DepVSM 0.08 -0.09 0.06 -0.07
33
at
tr
s
(fi
lte
re
d) C-LDA-A 0.29 0.32 0.02 0.31 0.36 0.02 0.34 0.38 0.02 0.09 0.18 0.04
C-LDA-T 0.26 0.36 0.05 0.14 0.30 0.09 0.28 0.38 0.07 0.03 0.18 0.08
M&L 0.38 0.40 0.24 0.43
DepVSM 0.34 0.32 0.35 0.19
Table 2: Correlation coefficients (Spearman?s ?) for different training sets and filtered test sets
the M&L word space model on the full adjective-
noun test set. The table is split into an upper and a
lower part containing the different results obtained
from training on 262 and 33 attributes, respectively.
Each multicolumn shows the performance achieved
by one of the different composition operators pre-
sented in Section 4, as well as results obtained from
predicting similarity on the basis of raw adjective
(ADJ-only) and noun (N-only) vectors.
First and foremost, we observe best overall per-
formance for the M&L model when combined with
multiplicative vector composition (? = 0.34), even
though the best results for this setting reported in
M&L (2010) (? = 0.46) cannot be reproduced.
Nevertheless, the C-LDA models show a consid-
erable performance improvement when the training
material is constrained to appropriate attributes by
an oracle (cf. Sect. 4). Another interesting obser-
vation is that the individual adjective and noun vec-
tors produced by M&L and the C-LDA models, re-
spectively, show diametrically opposed performance
(cf. 3rd and 4th multicolumn in Table 1).
More in detail, C-LDA-A achieves relative im-
provements across all composition operators when
comparing the 33-ATTR to the 262-ATTR setting.
Contrasting C-LDA-A and C-LDA-T, the latter is
clearly more effective on the larger training set, es-
pecially in combination with the ? operator (? =
0.28). This might be due to the intersective character
of multiplication, which requires densely populated
components in both the adjective and the noun vec-
tor. This requirement meets best with the C-LDA-T
model as long as the number of topics provided is
large. The + operator, on the other hand, combines
better with C-LDA-A. In the 33-ATTR setting, this
combination even outperforms vector addition un-
der the M&L model. Generally, C-LDA-A performs
better on the smaller training set, where it leaves C-
LDA-T behind in every configuration. This high-
lights that an interpretable, attribute-related meaning
layer generalizing over latent topics can be effective
if a small, discriminative set of attributes is available
for training. Otherwise, C-LDA-T seems to be more
powerful for the present similarity judgement task.
Analyzing the performance of the composition
surrogates ADJ-only and N-only in the restricted 33-
ATTR setting reveals an interesting twist in the qual-
ity of adjective vs. noun vectors: While M&L gen-
56
erally yields better results on noun vectors alone (as
compared to adjective vectors), C-LDA-A clearly
outperforms M&L in predicting similarity based on
adjective meanings in isolation. In this configura-
tion, M&L is also outperformed by the (very strong)
dependency baseline which is, in turn, only slightly
beaten by C-LDA-A in its best configuration. In
fact, it is the ADJ-only surrogate under the C-LDA-
A model in its best setting (? = 0.29) that comes
closest to the overall best-performing M&L model.
This indicates that modeling attributes in the latent
semantics of adjectives can be informative for the
present similarity prediction task. The poor quality
of the noun vectors, however, limits the overall per-
formance of the C-LDA models considerably.
Results on filtered test set. As can be seen from
Table 2, our expectation that C-LDA-A and C-
LDA-T should benefit from limiting the test set to
instances related to attribute meanings is largely
met. We observe overall improvement of correla-
tion scores; also the characteristics of the individual
models observed in Table 1 remain unchanged.
However, M&L benefits from filtering as well,
and in some configurations, e.g. under vector addi-
tion, the relative improvement is even bigger for the
latent word space models. This shows that M&L
and our C-LDA models are not fully complemen-
tary, i.e. some aspects of attribute similarity are also
covered by latent models.
Neverthelesss, the adjective/noun twist observed
for individual vector performance is corroborated:
C-LDA-A?s adjective vectors outperform those of
M&L by ten points (33 attributes, filtered setting;
compared to six points on the complete test set),
whereas the performance of the noun vectors drops
even further. Again, the DepVSM baseline performs
very strong on the adjective vectors in isolation,
which clearly underlines that our dependency-based
context selection procedure is effective. On the other
hand, the individual noun vectors produced by M&L
even yield the best overall result on the filtered test
data, thus outperforming both composition methods.
Differences in adjective and noun vectors. In or-
der to highlight qualitative differences of the indi-
vidual adjective and noun vectors across the various
models, we analyzed their informativeness in terms
of entropy. The intuition is as follows: The lower the
262 attrs 33 attrs
avg ? avg ?
C-LDA-A (JJ) 1.20 0.48 0.83 0.27
C-LDA-A (NN) 1.66 0.72 1.23 0.46
C-LDA-T (JJ) 0.92 0.04 0.50 0.04
C-LDA-T (NN) 1.10 0.06 0.60 0.02
M&L (JJ) 2.74 0.91 2.74 0.91
M&L (NN) 2.96 0.33 2.96 0.33
DepVSM (JJ) 0.48 0.61 0.65 0.32
DepVSM (NN) 0.38 0.67 0.96 0.21
Table 3: Average entropy of individual adjective and
noun vectors across different models
entropy exhibited by a vector, the more pronounced
are its most prominent components. On the contrary,
high entropy indicates a rather broad, less accen-
tuated distribution of the probability mass over the
vector components (cf. Hartung and Frank (2010)).
The results of this analysis are displayed in Ta-
ble 3. With regard to the C-LDA models, we observe
lower entropy in adjective vectors compared to noun
vectors across both training settings, which corre-
sponds to their relative performance in the similar-
ity prediction task. This indicates that C-LDA cap-
tures the relation between adjectives and attributes
in a very pronounced way, and that this information
proves valuable for similarity prediction.
The DepVSM model shows inconsistent results
with regard to the different training sets. While the
pattern observed for the C-LDA models is confirmed
on the limited training set, training on the full set of
262 attributes results in more accentuated noun vec-
tors. Given the huge standard deviations, however,
we suppose that these figures are not very reliable.5
The correspondence between lower entropy and
better performance we could observe for C-LDA-
A and C-LDA-T is, however, not confirmed by the
M&L word space model, as their adjective vectors
exhibit lower entropy on average6, while they per-
sistently underperform relative to the noun vectors
5In fact, unlike the C-LDA models and M&L, DepVSM
faces severe sparsity problems on the large training set, as be-
comes evident from the average total frequency mass per vector:
Noun vectors accumulate 704 cooccurrence counts over 262 di-
mensions on average, while adjective vectors are populated with
1555 counts on average (652 vs. 1052 counts over 33 dimen-
sions on the small training set).
6The entropy values of M&L are not directly comparable to
those of the C-LDA models and DepVSM; M&L entropies are
generally higher due to the higher dimensionality of the model.
57
(cf. Tables 1 and 2). Note, however, that the en-
tropy values of individual adjective vectors disperse
widely around the mean (?=0.91). This suggests
that a considerable proportion of M&L?s adjective
vectors is rather evenly distributed.
Analyzing the individual performance of noun
vectors in terms of entropy is less conclusive. While
the noun vectors consistently exhibit relatively high
entropy, their varying performance across the dif-
ferent models cannot be explained. We hypothesize
that the characteristics of the different models might
be more decisive instead: Apparently, attributes as
an abstract meaning layer are appropriate for mod-
eling the contribution of adjectives to phrase simi-
larity, whereas the contribution of nouns seems to
be captured more effectively by M&L-like distribu-
tions along bags of context words.
6 Error Analysis
In order to gain deeper insight into the strengths
and weaknesses of C-LDA-A and M&L, we
extracted the ten most similar/dissimilar pairs
(+Sim/?SimC-LDA-A/M&L; cf. Table 4) according
to system predictions, as well as the ten pairs
on which system and human raters show high-
est/lowest agreement in terms of similarity scores
(+Agr/?AgrC-LDA-A/M&L; cf. Table 5), for the best-
performing model instance of C-LDA-A and M&L
in the unfiltered 33-ATTR setting, respectively.
All pairs in +SimC-LDA-A and +SimM&L exhibit
matching attributes. +SimC-LDA-A contains two pairs
involving contrastive attribute values (vs. four in
+SimM&L): long period ? short time, hot weather
? cold air. Obviously, C-LDA-A is not prepared to
recognize this type of dissimilarity, as it does not
model the semantics and orientation of attribute val-
ues, and so assigns overly optimistic similarity rates.
While this deficiency is explained for C-LDA, it is
unexpected for M&L, where in +SimM&L we find
pairs such as old person ? elderly lady with similar-
ity ratings that are almost identical to antonymous
pairs discussed above, such as high price ? low cost.
We further observe a striking difference regarding
overall similarity ratings in both systems: We find
high scores of 0.88 on average within +SimC-LDA-A,
as opposed to 0.52 in +SimM&L. The difference
is less marked regarding ?Sim. Similarly, we
find overall low average similarity rates (0.2) in
+AgrM&L, whereas +AgrC-LDA-A achieves somewhat
higher rates (0.27). While all examples point to-
wards dissimilarity, C-LDA-A shows more discrim-
inative power, as exemplified by hot weather ? el-
derly lady (lowest rating) vs. central authority ? lo-
cal office (highest rating). This suggests that, over-
all, C-LDA-A disposes of a more discriminative se-
mantic representation to judge similarity ? which of
course can also go astray.
The disagreement set ?AgrC-LDA-A contains the
antonymous adjectives with high similarity ratings
from +SimC-LDA-A, of course. We also note a high
proportion (5/10) of pairs involving adjectives with
vague and highly ambiguous attribute meanings,
such as good, new, certain, general. These are dif-
ficult to capture, especially in combination with ab-
stract noun concepts such as information, effect or
circumstance.
An interesting type of similarity is represented by
early evening ? previous day. In this case, we ob-
serve a contrast in the semantics of the nouns in-
volved, while the pair exhibits strong similarity on
the attribute level, which is reflected in the system?s
similarity score. This type of similarity is reminis-
cent of relational analogies investigated in Turney
(2008). A related example is rural community ? fed-
eral assembly. Unlike the human judges, C-LDA
predicts high similarity for both pairs.
The examples given in ?AgrM&L, by contrast,
clearly point to a lack in capturing adjective seman-
tics, with misjudgements such as effective way ? effi-
cient use, large number ? vast amount or large quan-
tity ? great majority.
Turning to ?AgrC-LDA-A again, we find 9/10 items
exhibit values greater than 0.67 (average: 0.78).
This means the model yields a high number of
false positives in rating similarity (with explanations
and some reservations just discussed). All items in
?AgrM&L, by contrast, have values below 0.36 (av-
erage: 0.16). That is, we again observe that this
model assigns lower similarity scores. This is con-
firmed by a comparative analysis of average sim-
ilarity scores on the entire test set: C-LDA-A;+
yields an average similarity of 0.48 (?=0.05) over
all instances, while M&L;? yields 0.16 on average
(?=0.16). The human ratings (after normalization
to the scale from 0 to 1) amount to 0.39 (?=0.26).
58
SIMILARITY
C-LDA-A; + M&L; ?
+Sim
long period ? short time 0.95 important part ? significant role 0.66
hot weather ? cold air 0.95 certain circumstance ? particular case 0.60
different kind ? various form 0.91 right hand ? left arm 0.56
better job ? good place 0.89 long period ? short time 0.55
different part ? various form 0.88 old person ? elderly lady 0.54
social event ? special circumstance 0.88 high price ? low cost 0.54
better job ? good effect 0.88 black hair ? dark eye 0.48
similar result ? good effect 0.85 general principle ? basic rule 0.44
social activity ? political action 0.82 special circumstance ? particular case 0.43
early evening ? previous day 0.80 hot weather ? cold air 0.43
?Sim
early stage ? long period 0.11 old person ? right hand 0.03
northern region ? early age 0.11 new information ? further evidence 0.03
earlier work ? early evening 0.11 early stage ? dark eye 0.01
elderly woman ? black hair 0.10 practical difficulty ? cold air 0.01
practical difficulty ? cold air 0.08 left arm ? elderly woman 0.01
small house ? old person 0.07 hot weather ? elderly lady 0.00
left arm ? elderly woman 0.06 national government ? cold air 0.00
hot weather ? further evidence 0.06 black hair ? right hand 0.00
dark eye ? left arm 0.05 hot weather ? further evidence 0.00
national government ? cold air 0.03 better job ? economic problem 0.00
Table 4: Similarity scores predicted by optimal C-LDA-A and M&L model instances; 33-ATTR setting
AGREEMENT
C-LDA-A; + M&L; ?
+Agr
major issue ? american country 0.29 similar result ? good effect 0.29
efficient use ? little room 0.29 small house ? important part 0.14
economic condition ? american country 0.29 national government ? new information 0.12
public building ? central authority 0.29 major issue ? social event 0.26
northern region ? industrial area 0.28 new body ? significant role 0.11
new life ? economic development 0.42 social event ? special circumstance 0.25
new body ? significant role 0.13 economic development ? rural community 0.32
hot weather ? elderly lady 0.13 new technology ? public building 0.18
social event ? low cost 0.13 high price ? short time 0.10
central authority ? local office 0.44 new body ? whole system 0.24
?Agr
early evening ? previous day 0.80 effective way ? efficient use 0.29
rural community ? federal assembly 0.67 federal assembly ? national government 0.24
new information ? general level 0.68 vast amount ? high price 0.10
similar result ? good effect 0.85 different kind ? various form 0.24
better job ? good effect 0.88 vast amount ? large quantity 0.36
social event ? special circumstance 0.88 large number ? vast amount 0.31
better job ? good place 0.89 older man ? elderly woman 0.00
certain circumstance ? particular case 0.22 earlier work ? early stage 0.00
hot weather ? cold air 0.95 large number ? great majority 0.09
long period ? short time 0.95 large quantity ? great majority 0.04
Table 5: Test pairs showing high and low agreement between systems and human raters, together with system similarity
scores as obtained from optimal model instances; 33-ATTR setting
59
While these means are not fully comparable as they
are the result of different composition operations,
the standard deviations suggest that M&L?s similar-
ity predictions are dispersed over a larger range of
the scale, while the C-LDA scores show only small
variation. This missing spread might be one of the
reasons for C-LDA?s lower performance.
In summary, we note one obvious shortcoming in
the C-LDA-A model, in that it does not capture dis-
similarity due to distinct contrastive meanings of at-
tribute values in cases of similarity on the noun and
attribute levels. With its focus on attribute seman-
tics, however, C-LDA-A is able to capture similar-
ity due to relational analogies, as in early evening
? previous day (0.8), whereas the latent model of
M&L is clearly noun-oriented, and thus predicts a
low similarity of 0.2 for this pair.
We conclude that the proposed attribute analysis
of adjective-noun pairs implements an inherently re-
lational form of similarity. Noun semantics is cap-
tured only indirectly, through the range of attributes
found relevant for the noun. The current model also
fully neglects the meaning of scalar attribute values.
Whether a more comprehensive analysis of inter-
preted adjective-noun meanings is able to succeed
in a paired similarity prediction task is an open issue
to be explored in future work.
7 Conclusion
In this paper, we presented a distributional VSM
that incorporates latent semantic information char-
acterizing ontological attributes in the meaning of
adjective-noun phrases, as obtained from C-LDA, a
weakly supervised variant of LDA. Originally de-
signed for an attribute selection task (Hartung and
Frank, 2011), this model faces a true challenge when
evaluated in a pairwise similarity judgement task
against a high-dimensional word space model, such
as M&L?s VSM. In fact, our model is unable to com-
pete with M&L even in its best configurations.
Thorough analysis reveals, however, that the qual-
ity of individual adjective and noun vectors is dia-
metric across the two models: C-LDA, capitalizing
on interpretable ontological dimensions, produces
effective adjective vectors, whereas its noun repre-
sentations lag behind. The inverse situation is ob-
served for the word-based latent VSM of M&L.
One qualification is in order, though: In its cur-
rent state, the C-LDA model relies on an ?oracle?
that pre-selects the attributes involved in the test set
for the model to be trained on. Although one could
argue that tailoring the context words to the target
words has a similar effect in our re-implementation
of M&L, interferences of this kind are not desirable
in principle. Future work will need to explore in
more detail possible attribute ranges with regard to
their usefulness for different tasks and data sets.
Our comparative investigaton of the specific
strengths and weaknesses of the models indicates
that they focus on different aspects of similarity:
M&L, possibly due to its higher and more discrim-
inative dimensionality, tends to produce more ef-
ficient noun vectors. Overall, this model accords
better with human similarity judgements across di-
verse aspects of similarity than the more focused
attribute-oriented LDA models. The C-LDA mod-
els focus on a specific, interpretable meaning di-
mension shared by adjectives and nouns, with a ten-
dency for stronger modeling capacity for adjectives.
They are currently not prepared to capture dissimi-
larity in cases of contrastive attribute values, while
on the positive side, they effectively cope with re-
lational analogies, both with similar and dissimilar
noun meanings.
Our findings suggest that adding more discrimina-
tive power to the noun representations and scalar in-
formation about attribute values to the adjective vec-
tors might be beneficial. Further research is needed
to investigate how to combine interpretable seman-
tic representations tailored to specific relations, as
captured by C-LDA, with M&L-like bag-of-words
representations in a single distributional model.
Applying interpreted models to the present simi-
larity rating task will still remain a challenge, as it
involves mapping diverse mixtures of aspects and
grades of similarity to human judgements. How-
ever, if the performance of an integrated model can
compete with a purely latent semantic analysis, this
offers a clear advantage for more general tasks that
require linking phrase meaning to symbolic knowl-
edge bases such as (multilingual) ontologies, or for
application scenarios that involve discrete seman-
tic labels, such as text classification based on topic
modeling (Blei et al, 2003) or fine-grained named
entity classification (Ekbal et al, 2010).
60
References
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Ph.D. Dissertation, Department of Com-
puter Science, University of Essex.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, East
Stroudsburg, PA, pages 1183?1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: A Col-
lection of Very Large Linguistically Processed Web-
crawled Corpora. Journal of Language Resources and
Evaluation, 43(3):209?226.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel. A Corpus-based Seman-
tic Model based on Properties and Types. Cognitive
Science, 34:222?254.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet Allocation. JMLR, 3:993?1022.
Asif Ekbal, Eva Sourjikova, Anette Frank, and Simone
Ponzetto. 2010. Assessing the Challenge of Fine-
grained Named Entity Recognition and Classification.
In Proceedings of the ACL 2010 Named Entity Work-
shop (NEWS), Uppsala, Sweden.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
Stroudsburg, PA. Association for Computational Lin-
guistics.
Matthias Hartung and Anette Frank. 2010. A Structured
Vector Space Model for Hidden Attribute Meaning in
Adjective-Noun Phrases. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING), Beijing, China, August.
Matthias Hartung and Anette Frank. 2011. Exploring
Supervised LDA Models for Assigning Attributes to
Adjective-Noun Phrases. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, UK.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio, June.
Jeff Mitchell and Mirella Lapata. 2009. Language Mod-
els Based on Semantic Composition. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, Singapore, August 2009,
pages 430?439, Singapore, August.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive Sci-
ence, 34:1388?1429.
Diarmuid ?O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435?444, Uppsala, Sweden, July.
Association for Computational Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 905?912,
Manchester, UK.
61
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 118?127,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Towards Gene Recognition from Rare and
Ambiguous Abbreviations using a Filtering Approach
Matthias Hartung
?
, Roman Klinger
?
, Matthias Zwick
?
and Philipp Cimiano
?
?
Semantic Computing Group
Cognitive Interaction Technology ?
Center of Excellence (CIT-EC)
Bielefeld University
33615 Bielefeld, Germany
{mhartung,rklinger,cimiano}
@cit-ec.uni-bielefeld.de
?
Research Networking
Boehringer Ingelheim Pharma GmbH
Birkendorfer Str. 65
88397 Biberach, Germany
matthias.zwick
@boehringer-ingelheim.com
Abstract
Retrieving information about highly am-
biguous gene/protein homonyms is a chal-
lenge, in particular where their non-protein
meanings are more frequent than their pro-
tein meaning (e. g., SAH or HF). Due to
their limited coverage in common bench-
marking data sets, the performance of exist-
ing gene/protein recognition tools on these
problematic cases is hard to assess.
We uniformly sample a corpus of eight am-
biguous gene/protein abbreviations from
MEDLINEr and provide manual annota-
tions for each mention of these abbrevia-
tions.
1
Based on this resource, we show
that available gene recognition tools such
as conditional random fields (CRF) trained
on BioCreative 2 NER data or GNAT tend
to underperform on this phenomenon.
We propose to extend existing gene recog-
nition approaches by combining a CRF
and a support vector machine. In a cross-
entity evaluation and without taking any
entity-specific information into account,
our model achieves a gain of 6 points
F
1
-Measure over our best baseline which
checks for the occurrence of a long form
of the abbreviation and more than 9 points
over all existing tools investigated.
1 Introduction
In pharmaceutical research, a common task is to
gather all relevant information about a gene, e. g.,
from published articles or abstracts. The task of rec-
ognizing the mentions of genes or proteins can be
understood as the classification problem to decide
1
The annotated corpus is available for future research at
http://dx.doi.org/10.4119/unibi/2673424.
whether the entity of interest denotes a gene/protein
or something else. For highly ambiguous short
names, this task can be particularly challenging.
Consider, for instance, the gene acyl-CoA syn-
thetase medium-chain family member 3 which has
synonyms protein SA homolog or SA hypertension-
associated homolog, among others, with abbrevia-
tions ACSM3, and SAH.
2
Standard thesaurus-based
search engines would retrieve results where SAH
denotes the gene/protein of interest, but also oc-
currences in which it denotes other proteins (e. g.,
ATX1 antioxidant protein 1 homolog
3
) or entities
from semantic classes other than genes/proteins
(e. g., the symptom sub-arachnoid hemorrhage).
For an abbreviation such as SAH, the use as de-
noting a symptom or another semantic class dif-
ferent from genes/proteins is more frequent by a
factor of 70 compared to protein-denoting men-
tions according to our corpus analysis, such that
the retrieval precision for acyl-CoA synthetase by
the occurrence of the synonym SAH is only about
0.01, which is totally unacceptable for practical
applications.
In this paper, we discuss the specific challenge
of recognizing such highly ambiguous abbrevia-
tions. We consider eight entities and show that
common corpora for gene/protein recognition are
of limited value for their investigation. The abbre-
viations we consider are SAH, MOX, PLS, CLU,
CLI, HF, AHR and COPD (cf. Table 1). Based
on a sample from MEDLINE
4
, we show that these
names do actually occur in biomedical text, but
are underrepresented in corpora typically used for
benchmarking and developing gene/protein recog-
nition approaches.
2
http://www.ncbi.nlm.nih.gov/gene/6296
3
http://www.ncbi.nlm.nih.gov/gene/
443451
4
http://www.nlm.nih.gov/pubs/
factsheets/medline.html
118
Synonym Other names Other meaning EntrezGene ID
SAH acyl-CoA synthetase medium-chain family
member 3; ACSM3
subarachnoid hemorrhage;
S-Adenosyl-L-homocysteine hydrolase
6296
MOX monooxygenase, DBH-like 1 moxifloxacin; methylparaoxon 26002
PLS POLARIS partial least squares; primary lateral sclerosis 3770598
CLU clusterin; CLI covalent linkage unit 1191
CLI clusterin; CLU clindamycin 1191
HF complement factor H; CFH high frequency; heart failure; Hartree-Fock 3075
AHR aryl hydrocarbon receptor; bHLHe76 airway hyperreactivity 196
COPD archain 1; ARCN1; coatomer protein
complex, subunit delta
Chronic Obstructive Pulmonary Disease 22819; 372
Table 1: The eight synonyms for genes/proteins which are subject of analysis in this paper and their long
names together with frequent other meanings.
We propose a machine learning-based filtering
approach to detect whether a mention in question
actually denotes a gene/protein or not and show
that for the eight highly ambiguous abbreviations
that we consider, the performance of our approach
in terms of F
1
measure is higher than for a state-of-
the-art tagger based on conditional random fields
(CRF), a freely available dictionary-based approach
and an abbreviation resolver. We evaluate differ-
ent parameters and their impact in our filtering
approach and discuss the results. Note that this
approach does not take any information about the
specific abbreviation into account and can therefore
be expected to generalize to names not considered
in our corpus.
The main contributions of this paper are:
(i) We consider the problem of recognizing
highly ambiguous abbreviations that fre-
quently do not denote proteins as a task that
has so far attracted only limited attention.
(ii) We show that the recognition of such ambigu-
ous mentions is important as their string rep-
resentation is frequent in collections such as
MEDLINE.
(iii) We show, however, that this set of ambiguous
names is underrepresented in corpora com-
monly used for system design and develop-
ment. Such corpora do not provide a suffi-
cient data basis for studying the phenomenon
or for training systems that appropriately han-
dle such ambiguous abbreviation. We con-
tribute a manually annotated corpus of 2174
occurrences of ambiguous abbreviations.
(iv) We propose a filtering method for classifying
ambiguous abbreviations as denoting a pro-
tein or not. We show that this method has a
positive impact on the overall performance of
named entity recognition systems.
2 Related Work
The task of gene/protein recognition consists in
the classification of terms as actually denoting a
gene/protein or not. The task is typically either
tackled by using machine learning or dictionary-
based approaches. Machine learning approaches
rely on appropriate features describing the local
context of the term to be classified and induce a
model to perform the classification from training
data. Conditional random fields have shown to
yield very good results on the task (Klinger et al.,
2007; Leaman and Gonzalez, 2008; Kuo et al.,
2007; Settles, 2005).
Dictionary-based approaches rely on an explicit
dictionary of gene/protein names that are matched
in text. Such systems are common in practice due
to the low overhead required to adapt and maintain
the system, essentially only requiring to extend the
dictionary. Examples of commercial systems are
ProMiner (Fluck et al., 2007) or I2E (Bandy et al.,
2009); a popular free system is made available by
Hakenberg et al. (2011).
Such dictionary-based systems typically incorpo-
rate rules for filtering false positives. For instance,
in ProMiner (Hanisch et al., 2003), ambiguous syn-
onyms are only accepted based on external dictio-
naries and matches in the context. Abbreviations
are only accepted if a long form matches all parts of
the abbreviation in the context (following Schwartz
and Hearst (2003)). Similarly, Hakenberg et al.
(2008) discuss global disambiguation on the doc-
ument level, such that all mentions of a string in
one abstract are uniformly accepted as denoting an
entity or not.
A slightly different approach is taken by the web-
service GeneE
5
(Schuemie et al., 2010): Entering a
query as a gene/protein in the search field generates
5
http://biosemantics.org/geneE
119
MEDLINE BioCreative2 GENIA
Protein # Tokens % tagged # Tokens % of genes # Tokens % of genes
SAH 30019 6.1 % 2 0 % 0
MOX 16007 13.1 % 0 0
PLS 11918 25.9 % 0 0
CLU 1077 29.1 % 0 0
CLI 1957 4.8 % 4 0 % 0
HF 42563 7.9 % 8 62.5 % 4 0 %
AHR 21525 75.7 % 12 91.7 % 0
COPD 44125 0.6 % 6 0 % 0
Table 2: Coverage of ambiguous abbreviations in MEDLINE, BioCreative2 and GENIA corpora. The
percentage of tokens tagged as a gene/protein in MEDLINE (% tagged) is determined with a conditional
random field in the configuration described by Klinger et al. (2007), but without dictionary-based features
to foster the usage of contextual features). The percentages of genes/proteins (% of genes) in BC2 and
GENIA are based on the annotations in these corpora.
a query to e. g. PubMedr
6
with the goal to limit
the number of false positives.
Previous to the common application of CRFs,
other machine learning methods have been popu-
lar as well for the task of entity recognition. For
instance, Mitsumori et al. (2005) and Bickel et al.
(2004) use a support vector machine (SVM) with
part-of-speech information and dictionary-based
features, amongst others. Zhou et al. (2005) use an
ensemble of different classifiers for recognition.
In contrast to this application of a classifier
to solve the recognition task entirely, other ap-
proaches (including the one in this paper) aim at
filtering specifically ambiguous entities from a pre-
viously defined set of challenging terms. For in-
stance, Al-mubaid (2006) utilize a word-based clas-
sifier and a mutual information-based feature selec-
tion to achieve a highly discriminating list of terms
which is applied for filtering candidates.
Similarly to our approach, Tsuruoka and Tsujii
(2003) use a classifier, in their case a na??ve Bayes
approach, to learn which entities to filter from
the candidates generated by a dictionary-based ap-
proach. They use word based features in the con-
text including the candidate itself. Therefore, the
approach is focused on specific entities.
Gaudan et al. (2005) use an SVM and a dictio-
nary of long forms of abbreviations to assign them
a specific meaning, taking contextual information
into account. However, their machine learning ap-
proach is trained on each possible sense of an ab-
breviation. In contrast, our approach consists in
deciding if a term is used as a protein or not. Fur-
ther, we do not train to detect specific, previously
given senses.
6
http://www.ncbi.nlm.nih.gov/pubmed/
Xu et al. (2007) apply text similarity measures to
decide about specific meanings of mentions. They
focus on the disambiguation between different en-
tities. A corpus for word sense disambiguation is
automatically built based on MeSH annotations by
Jimeno-Yepes et al. (2011). Okazaki et al. (2010)
build a sense inventory by automatically applying
patterns on MEDLINE and use this in a logistic
regression approach.
Approaches are typically evaluated on freely
available resources like the BioCreative Gene Men-
tion Task Corpus, to which we refer as BC2 (Smith
et al., 2008), or the GENIA Corpus (Kim et al.,
2003). When it comes to identifying particular pro-
teins by linking the protein in question to some
protein in an external database ? a task we do
not address in this paper ? the BioCreative Gene
Normalization Task Corpus is a common resource
(Morgan et al., 2008).
In contrast to these previous approaches, our
method is not tailored to a particular set of entities
or meanings, as the training methodology abstracts
from specific entities. The model, in fact, knows
nothing about the abbreviations to be classified and
does not use their surface form as a feature, such
that it can be applied to any unseen gene/protein
term. This leads to a simpler model that is applica-
ble to a wide range of gene/protein term candidates.
Our cross-entity evaluation regime clearly corrobo-
rates this.
3 Data
We focus on eight ambiguous abbreviations of
gene/protein names. As shown in Table 2, these
homonyms occur relatively frequently in MEDLINE
but are underrepresented in the BioCreative 2 entity
120
Protein Pos. Inst. Neg. Inst. Total
SAH 5 349 354
MOX 62 221 283
PLS 1 206 207
CLU 235 30 265
CLI 11 211 222
HF 2 353 355
AHR 53 80 133
COPD 0 250 250
Table 3: Number of instances per protein in the
annotated data set and their positive/negative distri-
bution
recognition data set and the GENIA corpus which
are both commonly used for developing and evalu-
ating gene recognition approaches. We compiled
a corpus from MEDLINE by randomly sampling
100 abstracts for each of the eight abbreviations (81
for MOX) such that each abstract contains at least
one mention of the respective abbreviation. One
of the authors manually annotated the mentions
of the eight abbreviations under consideration to
be a gene/protein entity or not. These annotations
were validated by another author. Both annotators
disagreed in only 2% of the cases. The numbers
of annotations, including their distribution over
positive and negative instances, are summarized
in Table 3. The corpus is made publicly available
at http://dx.doi.org/10.4119/unibi/
2673424 (Hartung and Zwick, 2014).
In order to alleviate the imbalance of positive
and negative examples in the data, additional pos-
itive examples have been gathered by manually
searching PubMed
7
. At this point, special attention
has been paid to extract only instances denoting the
correct gene/protein corresponding to the full long
name, as we are interested in assessing the impact
of examples of a particularly high quality. This
process yields 69 additional instances for AHR
(distributed over 11 abstracts), 7 instances (3 ab-
stracts) for HF, 14 instances (2 abstracts) for PLS
and 15 instances (7 abstracts) for SAH. For the
other gene/proteins in our dataset, no additional
positive instances of this kind could be retrieved
using PubMed. In the following, this process will
be referred to as manual instance generation. This
additional data is used for training only.
7
http://www.ncbi.nlm.nih.gov/pubmed
4 Gene Recognition by Filtering
We frame gene/protein recognition from ambigu-
ous abbreviations as a filtering task in which a set
of candidate tokens is classified into entities and
non-entities. In this paper, we assume the candi-
dates to be generated by a simple dictionary-based
approach taking into account all tokens that match
the abbreviation under consideration.
4.1 Filtering Strategies
We consider the following filtering approaches:
? SVM classifies the occurring terms based on a
binary support vector machine.
? CRF classifies the occurring terms based on
a conditional random field (configured as de-
scribed by Klinger et al. (2007)) trained on the
concatenation of BC2 data and our newly gen-
erated corpus. This setting thus corresponds
to state-of-the-art performance on the task.
? CRF?SVM considers the candidate an entity
if both the standard CRF and the SVM from
the previous steps yield a positive prediction.
? HRCRF?SVM is the same as the previous
step, but the output of the CRF is optimized
towards high recall by joining the recognition
of entities of the five most likely Viterbi paths.
? CRF?SVM is similar to the first setting, but
the output of the CRF is taken into account as
a feature in the SVM.
4.2 Features for Classification
Our classifier uses local contextual and global fea-
tures. Local features focus on the immediate con-
text of an instance, whereas global features encode
abstract-level information. Throughout the follow-
ing discussion, t
i
denotes a token at position i that
corresponds to a particular abbreviation to be classi-
fied in an abstract A. Note that we blind the actual
representation of the entity to be able to generalize
to all genes/proteins, not being limited to the ones
contained in our corpus.
4.2.1 Local Information
The feature templates context-left and context-right
collect the tokens immediately surrounding an ab-
breviation in a window of size 6 (left) and 4 (right)
in a bag-of-words-like feature generation. Addi-
tionally, the two tokens from the immediate context
on each side are combined into bigrams.
The template abbreviation generates features if
t
i
occurs in brackets. It takes into account the min-
imal Levenshtein distance (ld, Levenshtein (1966))
121
between all long forms L of the abbreviation (as
retrieved from EntrezGene) in comparison to each
string on the left of t
i
(up to a length of seven,
denoted by t
k:i
as the concatenation of tokens
t
k
, . . . , t
i
). Therefore, the similarity value sim(t
i
)
taken into account is given by
sim(t
i
) = max
l?L;k?[1:7]
1?
ld(t
k:i?1
, l)
max(|t
i
|, |l|)
,
where the denominator is a normalization term.
The features used are generated by cumulative bin-
ning of sim(t
i
).
The feature tagger
local
takes the prediction of the
CRF for t
i
into account. Note that this feature is
only used in the CRF?SVM setting.
4.2.2 Global Information
The feature template unigrams considers each word
in A as a feature. There is no normalization or
frequency weighting. Stopwords are ignored
8
. Oc-
currences of the same string as t
i
are blinded.
The feature tagger
global
collects all tokens in A
other than t
i
that are tagged as an entity by the CRF.
In addition, the cardinality of these entities in A is
taken into account by cumulative binning.
The feature long form holds if one of the long
forms previously defined to correspond with the ab-
breviation occurs in the text (in arbitrary position).
Besides using all features, we perform a greedy
search for the best feature set by wrapping the best
model configuration. A detailed discussion of the
feature selection process follows in Section 5.3.
4.2.3 Feature Propagation
Inspired by the ?one sense per discourse? heuristic
commonly adopted in word sense disambiguation
(Gale et al., 1992), we apply two feature combi-
nation strategies. In the following, n denotes the
number of occurrences of the abbreviation in an
abstract.
In the setting propagation
all
, n ? 1 identical
linked instances are added for each occurrence.
Each new instance consists of the disjunction of
the feature vectors of all occurrences. Based on
the intuition that the first mention of an abbrevia-
tion might carry particularly valuable information,
propagation
first
introduces one additional linked in-
stance for each occurrence, in which the feature
vector is joined with the first occurrence.
8
Using the stopword list at http://www.ncbi.nlm.
nih.gov/books/NBK3827/table/pubmedhelp.
T43/, last accessed on March 25, 2014
Setting P R F
1
SVM 0.81 0.45 0.58
CRF?SVM 0.99 0.26 0.41
HRCRF?SVM 0.95 0.27 0.42
CRF?SVM 0.83 0.49 0.62
CRF?SVM+FS 0.97 0.74 0.84
GNAT 0.73 0.45 0.56
CRF 0.55 0.43 0.48
AcroTagger 0.92 0.63 0.75
Long form 0.98 0.65 0.78
lex 0.18 1.00 0.32
Table 4: Overall micro-averaged results over eight
genes/proteins. For comparison, we show the re-
sults of a default run of GNAT (Hakenberg et al.,
2011), a CRF trained on BC2 data (Klinger et al.,
2007), AcroTagger (Gaudan et al., 2005), and a
simple approach of accepting every token of the
respective string as a gene/protein entity (lex). Fea-
ture selection is denoted with +FS.
In both settings, all original and linked instances
are used for training, while during testing, original
instances are classified by majority voting on their
linked instances. For propagation
all
, this results in
classifying each occurrence identically.
5 Experimental Evaluation
5.1 Experimental Setting
We perform a cross-entity evaluation, in which we
train the support vector machine (SVM) on the ab-
stracts of 7 genes/proteins from our corpus and test
on the abstracts for the remaining entities, i. e., the
model is evaluated only on tokens representing en-
tities which have never been seen labeled during
training. The CRFs are trained analogously with
the difference that the respective set used for train-
ing is augmented with the BioCreative 2 Training
data. The average numbers of precision, recall and
F
1
measure are reported.
As a baseline, we report the results of a simple
lexicon-based approach assuming that all tokens
denote an entity in all their occurrences (lex). In ad-
dition, the baseline of accepting an abbreviation as
gene/protein if the long form occurs in the same ab-
stract is reported (Long form). Moreover, we com-
pare our results with the publicly available toolkit
GNAT (Hakenberg et al., 2011)
9
and the CRF ap-
9
The gene normalization functionality of GNAT is not
taken into account here. We acknowledge that this comparison
122
proach as described in Section 4. In addition, we
take into account the AcroTagger
10
that resolves
abbreviations to their most likely long form which
we manually map to denoting a gene/protein or not.
5.2 Results
5.2.1 Overall results
In Table 4, we summarize the results of the recogni-
tion strategies introduced in Section 4. The lexical
baseline clearly proves that a simple approach with-
out any filtering is not practical. GNAT adapts well
to ambiguous short names and turns out as a com-
petitive baseline, achieving an average precision of
0.73. In contrast, the filtering capacity of a stan-
dard CRF is, at best, mediocre. The long form
baseline is very competitive with an F
1
measure of
0.78 and a close-to-perfect precision. The results of
AcroTagger are similar to this long form baseline.
We observe that the SVM outperforms the CRF
in terms of precision and recall (by 10 percentage
points in F
1
). Despite not being fully satisfactory
either, these results indicate that global features
which are not implemented in the CRF are of im-
portance. This is confirmed by the CRF?SVM
setting, where CRF and SVM are stacked: This fil-
tering procedure achieves the best precision across
all models and baselines, whereas the recall is still
limited. Despite being designed for exactly this
purpose, the HRCRF?SVM combination can only
marginally alleviate this problem, and only at the
expense of a drop in precision.
The best trade-off between precision and recall
is offered by the CRF?SVM combination. This
setting is not only superior to all other variants of
combining a CRF with an SVM, but outperforms
GNAT by 6 points in F
1
score, while being inferior
to the long form baseline. However, performing
feature selection on this best model using a wrapper
approach (CRF?SVM+FS) leads to the overall
best result of F
1
= 0.84, outperforming all other
approaches and all baselines.
5.2.2 Individual results
Table 5 summarizes the performance of all filter-
ing strategies broken down into individual entities.
Best results are achieved for AHR, MOX and CLU.
COPD forms a special case as no examples for the
might be seen as slightly inappropriate as the focus of GNAT
is different.
10
ftp://ftp.ebi.ac.uk/pub/software/
textmining/abbreviation_resolution/, ac-
cessed April 23, 2014
occurrence as a gene/protein are in the data; how-
ever the results show that the system can handle
such a special distribution.
SVM and CRF are mostly outperformed by a
combination of both strategies (except for CLI and
HF), which shows that local and global features
are highly complementary in general. Complemen-
tary cases generally favor the CRF?SVM strategy,
except for PLS, where stacking is more effective.
In SAH, the pure CRF model is superior to all
combinations of CRF and SVM. Apparently, the
global information as contributed by the SVM is
less effective than local contextual features as avail-
able to the CRF in these cases. In SAH and CLI,
moreover, the best performance is obtained by the
AcroTagger.
5.2.3 Impact of instance generation
All results reported in Tables 4 and 5 refer to con-
figurations in which additional training instances
have been created by manual instance generation.
The impact of this method is analyzed in Table 6.
The first column reports the performance of our
models on the randomly sampled training data. In
order to obtain the results in the second column,
manual instance generation has been applied.
The results show that all our recognition mod-
els generally benefit from additional information
that helps to overcome the skewed class distribu-
tion of the training data. Despite their relatively
small quantity and uneven distribution across the
gene/protein classes, including additional exter-
nal instances yields a strong boost in all mod-
els. The largest difference is observed in SVM
(?F
1
= +0.2) and CRF?SVM (?F
1
= +0.16).
Importantly, these improvements include both pre-
cision and recall.
5.3 Feature Selection
The best feature set (cf. CRF?SVM+FS in Ta-
ble 4) is determined by a greedy search using a
wrapper approach on the best model configuration
CRF?SVM. The results are depicted in Table 7.
In each iteration, the table shows the best feature
set detected in the previous iteration and the results
for each individual feature when being added to
this set. In each step, the best individual feature
is kept for the next iteration. The feature analysis
starts from the long form feature as strong base-
line. The added features are, in that order, context,
tagger
global
, and propagation
all
.
Overall, feature selection yields a considerable
123
AHR CLI CLU COPD
Setting P R F
1
P R F
1
P R F
1
P R F
1
SVM 1.00 0.72 0.84 0.30 0.27 0.29 1.00 0.41 0.58 0.00 1.00 0.00
CRF?SVM 1.00 0.70 0.82 0.00 0.00 0.00 1.00 0.15 0.26 1.00 1.00 1.00
HRCRF?SVM 1.00 0.70 0.82 1.00 0.00 0.00 1.00 0.16 0.28 1.00 1.00 1.00
CRF?SVM 0.96 0.83 0.89 0.30 0.27 0.29 1.00 0.40 0.57 0.00 1.00 0.00
CRF?SVM+FS 0.93 0.98 0.95 0.50 0.09 0.15 0.99 0.84 0.91 1.00 1.00 1.00
GNAT 0.74 0.66 0.70 1.00 0.18 0.31 0.97 0.52 0.68 1.00 1.00 1.00
CRF 0.52 0.98 0.68 0.00 0.00 0.00 1.00 0.20 0.33 0.00 1.00 0.00
AcroTagger 1.00 0.60 0.75 1.00 0.82 0.90 1.00 0.00 0.00 1.00 1.00 1.00
Long form 1.00 0.96 0.98 1.00 0.09 0.17 0.99 0.80 0.88 1.00 1.00 1.00
lex 0.40 1.00 0.57 0.05 1.00 0.09 0.89 1.00 0.94 0.00 1.00 0.00
HF MOX PLS SAH
Setting P R F
1
P R F
1
P R F
1
P R F
1
SVM 0.25 1.00 0.40 0.87 0.44 0.58 0.14 1.00 0.25 0.00 0.00 0.00
CRF?SVM 1.00 0.00 0.00 1.00 0.39 0.56 1.00 1.00 1.00 1.00 0.00 0.00
HRCRF?SVM 1.00 0.00 0.00 1.00 0.39 0.56 0.20 1.00 0.33 1.00 0.00 0.00
CRF?SVM 0.25 1.00 0.40 0.91 0.63 0.74 0.50 1.00 0.67 1.00 0.00 0.00
CRF?SVM+FS 1.00 0.00 0.00 1.00 0.37 0.54 0.00 0.00 0.00 1.00 0.00 0.00
GNAT 1.00 0.00 0.00 0.38 0.08 0.14 0.00 0.00 0.00 0.00 0.00 0.0
CRF 0.00 0.00 0.00 0.43 0.90 0.59 0.14 1.00 0.25 1.00 0.50 0.67
AcroTagger 0.33 1.00 0.50 1.00 0.00 0.00 1.00 0.00 0.00 1.00 0.60 0.75
Long form 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00
lex 0.01 1.00 0.02 0.22 1.00 0.36 0.00 1.00 0.01 0.01 1.00 0.03
Table 5: Results for the eight genes/proteins and results for our different recognition schemes.
randomly sampled +instance generation
P R F
1
?P ?R ?F
1
SVM 0.73 0.25 0.38 +0.08 +0.20 +0.20
CRF?SVM 1.00 0.17 0.29 -0.01 +0.09 +0.13
HRCRF?SVM 0.97 0.18 0.30 -0.02 +0.09 +0.12
CRF?SVM 0.79 0.32 0.46 +0.05 +0.17 +0.16
CRF?SVM+FS 0.99 0.60 0.75 -0.02 +0.14 +0.09
Table 6: Impact of increasing the randomly sampled training set by adding manually curated additional
positive instances (+instance generation), measured in terms of the increase in precision, recall and F
1
(?P, ?R, ?F
1
).
boost in recall, while precision remains almost con-
stant. Surprisingly, the unigrams feature has a par-
ticularly strong negative impact on overall perfor-
mance.
While the global information contributed by the
CRF turns out very valuable, accounting for most
of the improvement in recall, local tagger informa-
tion is widely superseded by other features. Like-
wise, the abbreviation feature does not provide any
added value to the model beyond what is known
from the long form feature.
Comparing the different feature propagation
strategies, we observe that propagation
all
outper-
forms propagation
first
.
5.4 Discussion
Our experiments show that the phenomena inves-
tigated pose a challenge to all gene recognition
paradigms currently available in the literature, i. e.,
dictionary-based, machine-learning-based (e. g. us-
ing a CRF), and classification-based filtering.
Our results indicate that stacking different meth-
ods suffers from a low recall in early steps of the
workflow. Instead, a greedy approach that consid-
ers all occurrences of an abbreviation as input to
a filtering approach yields the best performance.
Incorporating information from a CRF as features
into a SVM outperforms all baselines at very high
levels of precision; however, the recall still leaves
room for improvement.
124
Iter. Feature Set P R F
1
?F
1
1 long form 0.98 0.65 0.78
+propagation
1
st
0.98 0.65 0.78 +0.00
+propagation
all
0.98 0.65 0.78 +0.00
+tagger
local
0.72 0.81 0.76 -0.02
+tagger
global
0.55 0.79 0.65 -0.13
+context 0.98 0.67 0.79 +0.01
+abbreviation 0.98 0.65 0.78 +0.00
+unigrams 0.71 0.43 0.53 -0.25
2 long form
+context 0.98 0.67 0.79
+propagation
1
st
0.98 0.67 0.79 +0.00
+propagation
all
0.96 0.70 0.81 +0.02
+tagger
local
0.98 0.70 0.82 +0.03
+tagger
global
0.97 0.72 0.83 +0.04
+abbreviation 0.98 0.67 0.80 +0.01
+unigrams 0.77 0.39 0.52 -0.27
3 long form
+context
+tagger
global
0.97 0.72 0.83
+propagation
1
st
0.97 0.71 0.82 -0.01
+propagation
all
0.97 0.74 0.84 +0.01
+tagger
local
0.97 0.72 0.82 -0.01
+abbreviation 0.97 0.72 0.82 -0.01
+unigrams 0.77 0.44 0.56 -0.27
4 long form
+context
+tagger
global
+propagation
all
0.97 0.74 0.84
+tagger
local
0.90 0.66 0.76 -0.08
+abbreviation 0.97 0.74 0.84 -0.00
+unigrams 0.80 0.49 0.61 -0.23
Table 7: Greedy search for best feature combina-
tion in CRF?SVM (incl. additional positives).
In a feature selection study, we were able to show
a largely positive overall impact of features that
extend local contextual information as commonly
applied by state-of-the-art CRF approaches. This
ranges from larger context windows for collecting
contextual information over abstract-level features
to feature propagation strategies. However, feature
selection is not equally effective in all individual
classes (cf. Table 5).
The benefits due to feature propagation indi-
cate that several instances of the same abbreviation
in one abstract should not be considered indepen-
dently of one another, although we could not verify
the intuition that the first mention of an abbrevia-
tion introduces particularly valuable information
for classification.
Overall, our results seem encouraging as the ma-
chinery and the features used are in general suc-
cessful in determining whether an abbreviation ac-
tually denotes a gene/protein or not. The best pre-
cision/recall balance is obtained by adding CRF
information as features into the classifier.
As we have shown in the cross-entity experi-
ment setting, the system is capable of generalizing
to other unseen entities. For a productive system,
we assume our workflow to be applied to specific
abbreviations such that the performance on other
entities (and therefore on other corpora) is not sub-
stantially influenced.
6 Conclusions and Outlook
The work reported in this paper was motivated from
the practical need for an effective filtering method
for recognizing genes/proteins from highly ambigu-
ous abbreviations. To the best of our knowledge,
this is the first approach to tackle gene/protein
recognition from ambiguous abbreviations in a
systematic manner without being specific for the
particular instances of ambiguous gene/protein
homonyms considered.
The proposed method has been proven to allow
for an improvement in recognition performance
when added to an existing NER workflow. Despite
being restricted to eight entities so far, our approach
has been evaluated in a strict cross-entity manner,
which suggests sufficient generalization power to
be extended to other genes as well.
In future work, we plan to extend the data set
to prove the generalizability on a larger scale and
on an independent test set. Furthermore, an inclu-
sion of the features presented in this paper into the
CRF will be evaluated. Moreover, assessing the
impact of the global features that turned out benefi-
cial in this paper on other gene/protein inventories
seems an interesting path to explore. Finally, we
will investigate the prospects of our approach in an
actual black-box evaluation setting for information
retrieval.
Acknowledgements
Roman Klinger has been funded by the ?It?s
OWL? project (?Intelligent Technical Systems
Ostwestfalen-Lippe?, http://www.its-owl.
de/), a leading-edge cluster of the German Min-
istry of Education and Research. We thank J?org
Hakenberg and Philippe Thomas for their support
in performing the baseline results with GNAT. Ad-
ditionally, we thank the reviewers of this paper for
their very helpful comments.
125
References
Hisham Al-mubaid. 2006. Biomedical term disam-
biguation: An application to gene-protein name dis-
ambiguation. In In IEEE Proceedings of ITNG06.
Judith Bandy, David Milward, and Sarah McQuay.
2009. Mining protein-protein interactions from pub-
lished literature using linguamatics i2e. Methods
Mol Biol, 563:3?13.
Steffen Bickel, Ulf Brefeld, Lukas Faulstich, J?org Hak-
enberg, Ulf Leser, Conrad Plake, and Tobias Schef-
fer. 2004. A support vector machine classifier for
gene name recognition. In In Proceedings of the
EMBO Workshop: A Critical Assessment of Text
Mining Methods in Molecular Biology.
Juliane Fluck, Heinz Theodor Mevissen, Marius Os-
ter, and Martin Hofmann-Apitius. 2007. ProMiner:
Recognition of Human Gene and Protein Names
using regularly updated Dictionaries. In Proceed-
ings of the Second BioCreative Challenge Evalua-
tion Workshop, pages 149?151, Madrid, Spain.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the Workshop on Speech and Natural
Language, pages 233?237, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Sylvain Gaudan, Harald Kirsch, and Dietrich Rebholz-
Schuhmann. 2005. Resolving abbreviations to their
senses in medline. Bioinformatics, 21(18):3658?
3664.
J?org Hakenberg, Conrad Plake, Robert Leaman,
Michael Schroeder, and Graciela Gonzalez. 2008.
Inter-species normalization of gene mentions with
GNAT. Bioinformatics, 24(16):i126?i132, Aug.
J?org Hakenberg, Martin Gerner, Maximilian Haeus-
sler, Ills Solt, Conrad Plake, Michael Schroeder,
Graciela Gonzalez, Goran Nenadic, and Casey M.
Bergman. 2011. The GNAT library for local and
remote gene mention normalization. Bioinformatics,
27(19):2769?2771, Oct.
Daniel Hanisch, Juliane Fluck, Heinz-Theodor Mevis-
sen, and Ralf Zimmer. 2003. Playing biology?s
name game: identifying protein names in scientific
text. Pac Symp Biocomput, pages 403?414.
Matthias Hartung and Matthias Zwick. 2014. A cor-
pus for the development of gene/protein recognition
from rare and ambiguous abbreviations. Bielefeld
University. doi:10.4119/unibi/2673424.
Antonio J Jimeno-Yepes, Bridget T McInnes, and
Alan R Aronson. 2011. Exploiting mesh indexing
in medline to generate a data set for word sense dis-
ambiguation. BMC bioinformatics, 12(1):223.
J-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus?semantically annotated corpus for bio-
textmining. Bioinformatics, 19 Suppl 1:i180?i182.
Roman Klinger, Christoph M. Friedrich, Juliane Fluck,
and Martin Hofmann-Apitius. 2007. Named
Entity Recognition with Combinations of Condi-
tional Random Fields. In Proceedings of the Sec-
ond BioCreative Challenge Evaluation Workshop,
Madrid, Spain, April.
Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang,
Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, Chun-
Nan Hsu, and I-Fang Chung. 2007. Rich feature
set, unication of bidirectional parsing and dictionary
filtering for high f-score gene mention tagging. In
Proceedings of the Second BioCreative Challenge
Evaluation Workshop, Madrid, Spain, April.
Robert Leaman and Graciela Gonzalez. 2008. Ban-
ner: An executable survey of advances in biomed-
ical named entity recognition. In Russ B. Altman,
A. Keith Dunker, Lawrence Hunter, Tiffany Murray,
and Teri E. Klein, editors, Pacific Symposium on Bio-
computing, pages 652?663. World Scientific.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Tomohiro Mitsumori, Sevrani Fation, Masaki Mu-
rata, Kouichi Doi, and Hirohumi Doi. 2005.
Gene/protein name recognition based on support
vector machine using dictionary as features. BMC
Bioinformatics, 6 Suppl 1:S8.
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,
Aaron M. Cohen, Juliane Fluck, Patrick Ruch, Anna
Divoli, Katrin Fundel, Robert Leaman, Jrg Haken-
berg, Chengjie Sun, Heng-hui Liu, Rafael Torres,
Michael Krauthammer, William W. Lau, Hongfang
Liu, Chun-Nan Hsu, Martijn Schuemie, K Bretonnel
Cohen, and Lynette Hirschman. 2008. Overview of
biocreative ii gene normalization. Genome Biol, 9
Suppl 2:S3.
Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2010. Building a high-quality sense inventory
for improved abbreviation disambiguation. Bioinfor-
matics, 26(9):1246?1253, May.
Martijn J. Schuemie, Ning Kang, Maarten L. Hekkel-
man, and Jan A. Kors. 2010. Genee: gene and pro-
tein query expansion with disambiguation. Bioinfor-
matics, 26(1):147?148, Jan.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. Pac Symp Biocomput, pages 451?
462.
Burr Settles. 2005. Abner: an open source tool for au-
tomatically tagging genes, proteins and other entity
names in text. Bioinformatics, 21(14):3191?3192,
Jul.
Larry Smith, Lorraine K. Tanabe, Rie Johnson nee J.
Ando, Cheng-Ju J. Kuo, I-Fang F. Chung, Chun-
Nan N. Hsu, Yu-Shi S. Lin, Roman Klinger,
126
Christoph M. Friedrich, Kuzman Ganchev, Man-
abu Torii, Hongfang Liu, Barry Haddow, Craig A.
Struble, Richard J. Povinelli, Andreas Vlachos,
William A. Baumgartner, Lawrence Hunter, Bob
Carpenter, Richard Tzong-Han T. Tsai, Hong-Jie J.
Dai, Feng Liu, Yifei Chen, Chengjie Sun, Sophia Ka-
trenko, Pieter Adriaans, Christian Blaschke, Rafael
Torres, Mariana Neves, Preslav Nakov, Anna Divoli,
Manuel Ma?na L?opez, Jacinto Mata, and W. John
Wilbur. 2008. Overview of BioCreative II gene
mention recognition. Genome biology, 9 Suppl
2(Suppl 2):S2+.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2003. Boost-
ing precision and recall of dictionary-based pro-
tein name recognition. In Proceedings of the ACL
2003 Workshop on Natural Language Processing in
Biomedicine, pages 41?48, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Hua Xu, Jung-Wei Fan, George Hripcsak, Eneida A
Mendonc?a, Marianthi Markatou, and Carol Fried-
man. 2007. Gene symbol disambiguation us-
ing knowledge-based profiles. Bioinformatics,
23(8):1015?1022.
GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su, and
SoonHeng Tan. 2005. Recognition of protein/gene
names from text using an ensemble of classifiers.
BMC Bioinformatics, 6 Suppl 1:S7.
127
Proceedings of Third Workshop on Semantic Web and Information Extraction, pages 25?32,
Dublin, Ireland, 24 August, 2014.
Ontology-based Extraction of Structured Information from Publications
on Preclinical Experiments for Spinal Cord Injury Treatments
Benjamin Paassen
?
, Andreas St?ckel
?
, Raphael Dickfelder
?
, Jan Philip G?pfert
?
,
Tarek Kirchhoffer
?
, Nicole Brazda
?,?
, Hans Werner M?ller
?,?
,
Roman Klinger
?
, Matthias Hartung
?
, Philipp Cimiano
?1
?
Semantic Computing Group, CIT-EC, Bielefeld University, 33615 Bielefeld, Germany
?
Molecular Neurobiology, Neurology, HHU D?sseldorf, 40225 D?sseldorf, Germany
?
Center for Neuronal Regeneration, Life Science Center, 40225 D?sseldorf, Germany
{bpaassen,astoecke,rdickfel,jgoepfert}@techfak.uni-bielefeld.de
tarek.kirchhoffer@cnr.de, {nicole.brazda,hanswerner.mueller}@uni-duesseldorf.de
{rklinger,mhartung,cimiano}@cit-ec.uni-bielefeld.de
Abstract
Preclinical research in the field of central nervous system trauma advances at a fast pace, currently
yielding over 8,000 new publications per year, at an exponentially growing rate. This amount of
published information by far exceeds the capacity of individual scientists to read and understand the
relevant literature. So far, no clinical trial has led to therapeutic approaches which achieve functional
recovery in human patients.
In this paper, we describe a first prototype of an ontology-based information extraction system that
automatically extracts relevant preclinical knowledge about spinal cord injury treatments from nat-
ural language text by recognizing participating entity classes and linking them to each other. The
evaluation on an independent test corpus of manually annotated full text articles shows a macro-
average F
1
measure of 0.74 with precision 0.68 and recall 0.81 on the task of identifying entities
participating in relations.
1 Introduction
Injury to the central nervous system of adult mammals typically results in lasting deficits, like permanent
motor and sensor impairments, due to a lack of profound neural regeneration. Specifically, patients who have
sustained spinal cord injuries (SCI) usually remain partially paralyzed for the rest of their lives. Preclinical
research in the field of central nervous system trauma advances at fast pace, currently yielding over 8,000
new publications per year, at an exponentially growing rate, with a total amount of approximately 160,000
PubMed-listed papers today.
2
However, translational neuroscience faces a strong disproportion between the immense preclinical re-
search effort and the lack of successful clinical trials in SCI therapy: So far, no therapeutic approach has
led to functional recovery in human patients (Filli and Schwab, 2012). As the vast amount of published in-
formation by far exceeds the capacity of individual scientists to read and understand the relevant knowledge
(Lok, 2010), the selection of promising therapeutic interventions for clinical trials is notoriously based on
incomplete information (Prinz et al., 2011; Steward et al., 2012).
Thus, automatic information extraction methods are needed to gather structured, actionable knowledge
from large amounts of unstructured text that describe outcomes of preclinical experiments in the SCI do-
main. Being stored in a database, such knowledge provides a highly valuable resource enabling curators
and researchers to objectively assess the prospective success of experimental therapies in humans, and sup-
ports the cost-effective execution of meta studies based on all previously published data. First steps towards
such a database have already been undertaken by manually extracting the desired information from a limited
number of papers (Brazda et al., 2013), which is not feasible on a large scale, though.
In this paper, we present a first prototype of an automated ontology-based information extraction system
for the acquisition of structured knowledge about experimental SCI therapies. As main contributions, we
point out the highly relational problem structure by describing the entity classes and relations relevant for
1
The first four authors contributed equally.
2
As in this query to the database PubMed (link to http://www.ncbi.nlm.nih.gov/pubmed), as of April 2014.
25
Named Entity Recognition Relation Extraction
Rule-based
Ontology-based
Regular
Expression
Rule-based
recombination
Token
lookup
Candidate
generation
Candidate
filtering
Inp
ut 
PD
F
Ontological
reduction
Relations
Ou
tpu
t
Animal
Injury
Treatment
Resulttex
t e
xtr
act
ion
Figure 1: Workflow of our implementation, from the input PDF document to the generation of the output
relations. Named entity recognition is described in Section 3.1, relation extraction in Section 3.2.
knowledge representation in the domain, and provide a cascaded workflow that is capable of extracting these
relational structures from unstructured text with an average F
1
measure of 0.74.
2 Related Work
Our workflow for acquiring structured information in the domain of spinal cord injury treatments is an
example of ontology-based information extraction systems (Wimalasuriya and Dou, 2010): Large amounts
of unstructured natural language text are processed through a mechanism guided by an ontology, in order to
extract predefined types of information. Our long-term goal is to represent all relevant information on SCI
treatments in structured form, similar to other automatically populated databases in the biomedical domain,
such as STRING-DB for protein-protein interactions (Franceschini et al., 2013), among others.
A strong focus in biomedical information extraction has long been on named entity recognition, for which
machine-learning solutions such as conditional random fields (Lafferty et al., 2001) or dictionary-based
systems (Schuemie et al., 2007; Hanisch et al., 2005; Hakenberg et al., 2011) are available which tackle
the respective problem with decent performance and for specific entity classes such as organisms (Pafilis
et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity
recognition, covering other domains as well, can be found in Nadeau and Sekine (2007).
The use case described in this paper, however, involves a highly relational problem structure in the sense
that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge,
which corresponds most closely to the problem structure encountered in event extraction, as triggered by
the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP
shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines
in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account
by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms
and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012).
With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et
al. (2008), in that a combination of gazetteers and extraction rules is derived from the underlying ontology,
in order to adapt the workflow to the domain of interest. A schema in terms of a reporting standard has
recently been proposed by the MIASCI-consortium (Lemmon et al., 2014, Minimum Information About a
Spinal Cord Injury Experiment). To the best of our knowledge, our work is the first attempt at automated
information extraction in the SCI domain.
3 Method and Architecture
An illustration of the proposed workflow is shown in Figure 1. Based on the unstructured information
management architecture (UIMA, Ferrucci and Lally (2004)), full text PDF documents serve as input to the
workflow. Plain text and structural information are extracted from these documents using Apache PDFBox
3
.
The proposed system extracts relations which we define as templates that contain slots, each of which is
to be filled by an instance of a particular entity class (cf. Table 1). At the same time, a particular instance
can be a filler for different slots (cf. Figure 2). We argue that a relational approach is essential to information
extraction in the SCI domain as (i) many instances of entity classes found in the text do not convey relevant
3
Apache PDFBox ? A Java PDF Library http://pdfbox.apache.org/
26
Relation Entity Class Example Method Resource Count
Integer ?42?, ?2k?, ?1,000? R Regular Expressions
Float ?4.23?, ?8.12 ? 10
-8
? R Regular Expressions
Roman Number ?XII?, ?MCLXII? R Regular Expressions
Word Number ?seventy-six" O Word Number List 99
Range ?2-4? R QTY + PARTICLE + QTY
Language Quantifier ?many?, ?all? O Quantifier List 11
Time ?2 h?, ?14 weeks? R QTY + TIME UNIT
Duration ?for 2h? R PARTICLE + TIME
Animal
Organism ?dog?, ?rat?, ?mice? O NCBI Taxonomy 67657
Laboratory Animal ?Long-Evans rats? O Special Laboratory Animals 5
Sex ?male?, ?female? O Gender List 2
Exact Age ?14 weeks old? R TIME + AGE PARTICLE
Age ?adult?, ?juvenile? O Age Expressions 2
Weight ?200 g? R QTY + WEIGHT UNIT
Number ?44?, ?seventy-six? R QTY
Injury
Injury Type ?compression? O Injury Type List 7
Injury Device ?NYU Impactor? O Injury Device List 21
Vertebral Position ?T4?, ?T8-9? R Regular Expressions
Injury Height ?cervical?, ?thoracic? O Injury Height Expressions 4
Treatment
Drug ?EPO?, ?inosine? O MeSH 14000
Delivery ?subcutaneous?, ?i.v.? O Delivery Dictionary 34
Dosage ?14 ml/kg? R QTY + UNIT
Result
Investigation Method ?walking analysis? O Method List 117
Significance ?significant? O Significance Quantifiers 2
Trend ?decreased?, ?improved? O Trend Dictionary 4
p Value ?p < 0.05? R P + QTY 4
Table 1: A detailed list of relations and the entity classes whose instances are valid slot fillers for them.
Examples for instances of each entity class are also shown, as well as the extraction method, and resources
used for extraction. Instances are either extracted from the text using regular expressions (R) or on a
lookup in our ontology database (O). Resources in italics were specifically created for this application,
resources in SMALL CAPITALS are regular expression-based recombinations of other entities. Entity
classes in bold face are required arguments for relation extraction (cf. Section 3.2). The count specifies
the number of elements in the respective resource.
information on their own, but only in combination with other instances (e. g., surgical devices mentioned in
the text are only relevant if used to inflict a spincal cord injury to the animals in an experimental group), and
(ii) a holistic picture of a preclinical experiment can only be captured by aggregating several relations (e. g.,
a certain p value being mentioned in the text implies a particular treatment of one group of animals to be
significantly different from another treatment of a control group).
We take four relations (Animal, Injury, Treatment and Result) into account which capture the semantic
essence of a preclinical experiment: Laboratory animals are injured, then treated and the effect of the treat-
ment is measured. Table 1 provides an overview of all entity classes and relations. The workflow consists
of two steps: Firstly, rule- and ontology-based named entity recognition (NER) is performed (cf. Section
3.1). Secondly, the pool of entities recognized during NER serves as a basis for relation extraction (cf.
Section 3.2).
3.1 Ontology-based Named Entity Recognition
We store ontological information in a relational database as a set of directed graphs, accompanied by a
dictionary for efficient token lookup. Each entity is stored with possible linguistic surface forms (e. g.,
?Wistar rats? as a surface form of the Wistar rat entity from the class Laboratory Animal). Each surface
form s is tokenized (on white space and non-alphanumeric symbols, including transformation to lowercase,
e. g., leading to tokens ?wistar? and ?rats?) and normalized (stemming, removal of special characters and
stop words) resulting in a set of dictionary keys (e. g., ?wistar? and ?rat?). The resources used as content
for the ontology are shown in Table 1. We use specifically crafted resources for our use case
4
as well as the
4
Resources built specifically are made publicly available at http://opensource.cit-ec.de/projects/scie
27
Five adult male guinea pigs weighing 200-250 g.
Animal Animal
Organism: guinea pigs
Weight: 200-250 g
Age: adult
Sex: male
Number: Five (5)
Organism: guinea pigs
Weight: -
Age: adult
Sex: male
Number: 200
Figure 2: Two example instances of the
Animal relation that can be generated
from the same text. Given its entity
class, the number 200 is a valid filler for
the ?number? slot as well as the ?weight?
slot. Both candidates are generated and
ranked according to their probability (cf.
Equation 4). The manually defined con-
straints of p
sem
ensure that 200 cannot
fill both slots at the same time.
NCBI taxonomy
5
and the Medical Subject Headings
6
(MeSH). The process of ontology-based NER consists
of (i) token lookup in the dictionary, (ii) candidate generation, (iii) probabilistic candidate filtering and (iv)
ontological reduction (cf. Figure 1).
Token lookup. For each token t in the document, the corresponding surface form tokens s
t
are retrieved
from the database. A confidence value p
conf
based on the Damerau-Levenshtein-Distance without swaps
(dld, Damerau (1964)) is calculated as
p
conf
(t, s
t
) := max
{
0, 1?min
t
?
?s
t
dld(t
?
, t)
|t
?
|
}
, (1)
where |t| denotes the number of characters in token t. Assuming to find t = ?rat? in the text with the
according surface form s
t
= (?wistar?, ?rats?), p
conf
(t, s
t
) = 1 ?
1
4
= 0.75. Tokens with p
conf
< 0.5 are
discarded.
Candidate generation. A candidate h for matching the surface form tokens s
h
is a list of tokens (t
h
1
, . . . , t
h
n
)
from the text. Candidates are constructed using all possible combinations of matching tokens for each surface
form token (as retrieved above). To keep this tractable, we restrict the search space to combinations with the
proximity d(t
h
k
, t
h
`
) ? 9 for all t
h
k
, t
h
`
? h, where d(u, v) := N
W
(u, v) + 3 ? N
S
(u, v) + 10 ? N
P
(u, v)
models the distance between two tokens u and v in the text withN
W
, N
S
, N
P
denoting the number of words,
sentences and paragraphs between u and v. In our example, a candidate would be h = (?rat?).
Candidate filtering. For a candidate h and the surface form tokens s
h
it refers to, we calculate a total
match probability, taking into account the distance d(u, v) of all tokens in the candidate, the confidence
p
conf
(t
?
, s
h
) that the token actually belongs to the surface form, and the ratio
?
t
?
?h
|t
?
|/
?
t?s
h
|t| of the
surface form tokens covered by the candidate:
p
match
(h, s
h
) =
1
?
t?s
h
|t|
max
t?h
?
t
?
?h
(
p
3
dist
(t, t
?
) ? p
conf
(t
?
, s
h
) ? |t
?
|
)
, (2)
where p
?
dist
(u, v) := exp
(
?
d(u, v)
2
2?
2
)
(3)
models the confidence that two tokens u and v belong together given their distance in the text. In our example
of the candidate h = (?rat?) with the surface form tokens s
h
= (?wistar?, ?rats?) is p
match
(h, s
h
) =
1 ? 0.75 ?
3
6+4
= 0.225. Candidates with p
match
< 0.7 are discarded. The resulting set of all recognized
candidates is denoted with H .
Ontological reduction. As the algorithm ignores the hierarchical information provided by the ontologies,
we may obtain overlapping matches for ontologically related entities. Therefore, in case of overlapping
entities that are related in an ?is a? relationship in the ontology, only the more specific one is kept. Assume
for instance the candidates ?Rattus norvegicus? and ?Rattus norvegicus albus?, where the latter is more
specific and therefore accepted.
3.2 Relation Extraction
We frame relation extraction as a template filling task such that each slot provided by a relation has to be
assigned a filler of the correct entity class. Entity classes for the four relations of interest are shown in
5
Sayers et al. (2012), database limited to vertebrates: http://www.ncbi.nlm.nih.gov/taxonomy/?term=
txid7742[ORGN
6
Lipscomb (2000), excerpt of drugs from Descriptor and Supplemental: https://www.nlm.nih.gov/mesh/
28
Table 1, where required slots are in bold face, whereas all other slots are optional.
The slot filling process is based on testing all combinations of appropriate entities taking into account
their proximity and additional constraints. In more detail, we define the set of all recognized relationsR
?
of
a type ? as
R
?
=
?
?
?
r
?
? P(H)
?
?
?
?
?
?
p
sem
(r
?
)
n
?
?
?
h?r
?
,h6=g(r
?
)
p
match
(h, s
h
) min
t?h,t
?
?g(r
?
)
p
?
?
dist
(t, t
?
) > 0.2
?
?
?
(4)
where P(H) denotes the power set over all candidates H recognized by NER. g(r
?
) returns the filler for
the required slot of r
?
, p
match
and p
dist
are defined as in Section 3.1 and p
sem
implements manually defined
constraints on r
?
: A wrongly typed filler h for one slot of r
?
leads to p
sem
(r
?
) = 0, as does a negative number
in the Number slot of the Animal relation. Animal Numbers larger than 100 or Animal Weights smaller than
1 g or larger than 1 t are punished. All other cases lead to p
sem
(r
?
) = 1. Note that p
match
(h, s
h
) = 1 for
candidates h retrieved by rule-based entity recognition. Further, we set ?
Animal
= ?
Treatment
= 6, ?
Injury
= 10
and ?
Result
= 15.
4 Experiments
4.1 Data Set
Overall 1186
Organism 58
Weight 32
Sex 33
Age 17
Injury Height 35
Injury Type 62
Injury Device 23
Drug 134
Dosage 106
Delivery 70
Investigation Method 129
Trend 219
Significance 137
p Value 131
Table 2: The number of anno-
tations in our evaluation set for
each entity class.
The workflow is evaluated against an independent, manually annotated
corpus of 32 complete papers which contain 1186 separate annotations
of entities, produced by domain experts
7
. Information about relations
is not provided in the corpus. Only entities which participate in the
description of the preclinical experiment are marked. The frequencies
of annotations among the different classes are shown in Table 2.
4.2 Experimental Settings
We evaluate the system with regard to two different tasks: extraction (?Is
the approach able to extract relevant information from the text, without
regard to the exact location of the information??) and annotation (?Is the
system able to annotate relevant information at the correct location as in-
dicated by medical experts??). Furthermore, we distinguish between an
all instances setting, where we consider all instances independently, and
a fillers only setting, where only those annotations in the system output
are considered, that are fillers in a relation (i.e. the fillers only-setting
evaluates a subset of the all instances-setting). The relation extraction
procedure is not evaluated separately. For each setting, we report preci-
sion, recall, and F
1
measure.
Taking the architecture into account, we have the following hypotheses: (i) For the all instances setting we
expect high recall, but low precision. (ii) For the fillers only setting, precision should increase notably. (iii)
Comparing the all entities and the fillers only setting, recall should remain at the same level. We therefore
expect the extraction task to be simpler than the annotation task: For any information to be annotated at
the correct position, it must have been extracted correctly. On the other hand, information that has been
extracted correctly, can still be found at a ?wrong? location in the text. Thus, we expect a drop of precision
and recall when moving from extraction to annotation.
4.3 Results
The results are presented in Table 3: For each relation mentioned in Section 3, and the entity classes partic-
ipating in it, we report precision, recall and F
1
-measure
8
. This is done for all four combinations of setting
and task. For each relation we also provide the macro-average of precision, recall and F
1
-measure over all
entity classes considered in that relation and the overall average.
7
Performed in Prot?g? http://protege.stanford.edu/ with the plug-in Knowtator http://knowtator.
sourceforge.net/ (Ogren, 2006)
8
Note that VertebralPosition and InjuryHeight are merged in the result table, as are Organism and Laboratory Animal and
Age and Exact Age. The Animal Number was excluded from the evaluation as it has not been annotated in our evaluation set.
29
Task Extraction Annotation
Setting All Instances Fillers Only All Instances Fillers Only
Entity Class Prec. Rec. F
1
Prec. Rec. F
1
Prec. Rec. F
1
Prec. Rec. F
1
Overall Average 0.58 0.95 0.72 0.68 0.81 0.74 0.13 0.77 0.22 0.21 0.51 0.30
Animal Average 0.62 0.99 0.76 0.82 0.94 0.87 0.12 0.91 0.21 0.31 0.81 0.44
Organism 0.41 1.00 0.58 0.88 0.90 0.89 0.02 1.00 0.04 0.24 0.66 0.35
Weight 0.20 1.00 0.33 0.52 0.94 0.67 0.08 0.97 0.15 0.49 0.91 0.64
Sex 0.85 0.99 0.91 0.87 0.98 0.92 0.18 0.94 0.30 0.26 0.94 0.41
Age 1.00 0.95 0.97 1.00 0.93 0.96 0.19 0.71 0.30 0.23 0.71 0.35
Injury Average 0.63 0.94 0.76 0.74 0.75 0.75 0.12 0.72 0.21 0.18 0.38 0.24
Injury Height 0.42 0.98 0.59 0.56 0.74 0.64 0.10 0.91 0.18 0.24 0.51 0.33
Injury Type 0.70 0.91 0.79 0.81 0.73 0.77 0.07 0.48 0.12 0.18 0.35 0.24
Injury Device 0.78 0.93 0.85 0.86 0.79 0.82 0.20 0.77 0.32 0.11 0.28 0.16
Treatment Average 0.45 0.91 0.61 0.53 0.78 0.63 0.14 0.72 0.23 0.19 0.54 0.28
Drug 0.10 0.98 0.18 0.24 0.69 0.36 0.01 0.74 0.02 0.10 0.42 0.16
Dosage 1.00 0.81 0.90 1.00 0.76 0.86 0.30 0.52 0.38 0.32 0.46 0.38
Delivery 0.26 0.95 0.41 0.34 0.89 0.49 0.11 0.89 0.20 0.15 0.74 0.25
Result Average 0.59 0.93 0.72 0.60 0.75 0.67 0.13 0.71 0.22 0.15 0.30 0.20
Investigation Method 0.29 0.96 0.45 0.27 0.79 0.40 0.03 0.66 0.06 0.02 0.16 0.04
Trend 0.37 0.91 0.53 0.44 0.78 0.56 0.06 0.63 0.11 0.07 0.27 0.11
Significance 0.70 0.90 0.79 0.70 0.71 0.70 0.17 0.69 0.27 0.22 0.39 0.28
p Value 1.00 0.96 0.98 1.00 0.71 0.83 0.27 0.86 0.41 0.30 0.36 0.33
Table 3: The macro-averaged evaluation results for each class given in precision, recall and F
1
measure.
For the extraction task with all instances setting, recall is close to 100% for all entity classes considered
in the Animal relation. It is 81% for Dosages. The rule-based recognition for Dosages (as for Ages and p
Values) is very precise: All recognized entities have been annotated by medical experts somewhere in the
document. This strong difference between entity classes can be observed in the annotation task and the fillers
only setting as well: The best average performance in F
1
-measure is achieved for entity classes that are part
of the Animal relation. Precision is best for Dosages, Ages and p Values.
The recall for the all instances setting is high in both the extraction and in the annotation task. However,
the number of annotated instances (29,628 annotations in total) is about 25 times higher than the number of
expert annotations, which leads to low precision especially in the annotation task. For the fillers only setting,
the number of annotations decreases dramatically (to 4069 annotations); at the same time, precision improves.
Regarding the comparison of both tasks, precision and recall are both notably lower in the annotation task,
for the all entities setting, as well as for the fillers only setting. The overall recall is lower by 14 percentage
points (pp) in the extraction task and by 26 pp in the annotation task when considering the fillers only setting.
The decrease is most pronounced for Investigation Methods in the annotation task with a drop of 50 pp.
4.4 Discussion
The results are promising for named entity recognition. Recall is close-to-perfect in the extraction task
and acceptable in the annotation task. The results for relation extraction leave space for improvement: An
increase in precision can be observed but the decrease in recall is too substantial. The Animal relation is an
exception, where an increase in F
1
measure is observed for the fillers only setting for nearly all entity classes,
leading to 0.87 F
1
for Animals in the extraction task.
An error analysis revealed that for the fillers only setting, most false positives (55%) are due to the fact
that the medical experts did not annotate all occurrences of the correct entity, but only one or a few. 18% are
due to ambiguities of surface forms (for instance the abbreviation ?it? for ?intrathecal? leads to many false
positives). Regarding false negatives, 41% are due to missing entries in our ontology database and further
26% are caused by wrong treatment of characters (mostly wrong transcriptions of characters from the PDF).
30
5 Conclusion and Outlook
We described the challenge of extracting relational descriptions about preclinical experiments on spinal cord
injury from scientific literature. To tackle that challenge, we introduced a cascaded approach of named
entity recognition, followed by relation extraction. Our results show that the first step can be achieved by
relying strongly on domain-specific ontologies. We show that modeling relations as aggregated entities,
and extracting them using a distance filtering principle combined with domain specific knowledge, yields
promising results, specifically for the Animal relation.
Future work will focus on improving the recognition at the correct position in the text. This is a pre-
requisite to actually tackle and evaluate the relation extraction not only on the basis of detected participating
entities. Therefore, improved relation detection approaches will be implemented which relax the assumption
that relevant entities are found close-by in the text. In addition, we will relax the assumption that different
slots of the annotation are all equally important. Finally, we will address aggregation beyond individual
relations in order to allow for a fully accurate holistic assessment of experimental therapies.
Our system offers a semantic analysis of scientific papers on spinal cord injuries. This lays groundwork
for populating a comprehensive semantic database on preclinical studies of SCI treatment approaches as de-
scribed by Brazda et al. (2013), laying ground and supporting transfer from preclinical to clinical knowledge
in the future.
References
N. Brazda, M. Kruse, F. Kruse, T. Kirchhoffer, R. Klinger, and H.-W. M?ller. 2013. The CNR preclinical database
for knowledge management in spinal cord injury research. Abstracts of the Society of Neurosciences, 148(22).
P. Buitelaar, P. Cimiano, A. Frank, M. Hartung, and S. Racioppa. 2008. Ontology-based information extraction
and integration from heterogeneous data sources. Int. J. Hum.-Comput. Stud., 66(11):759?788.
F. J. Damerau. 1964. A Technique for Computer Detection and Correction of Spelling Errors. Commun. ACM,
7(3):171?176, March.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. 2004. The Automatic
Content Extraction (ACE) program: tasks, data, and evaluation. In Proceedings of LREC 2004, pages 837?840.
A. Doms and M. Schroeder. 2005. GoPubMed: exploring PubMed with the Gene Ontology. Nucleic Acids Res,
33(Web Server issue):W783?W786, Jul.
D. Ferrucci and A. Lally. 2004. Building an example application with the Unstructured Information Management
Architecture. IBM Systems Journal, 43(3):455?475.
L. Filli and M. E. Schwab. 2012. The rocky road to translation in spinal cord repair. Ann Neurol, 72(4):491?501.
A. Franceschini, D. Szklarczyk, S. Frankild, M. Kuhn, M. Simonovic, A. Roth, J. Lin, P. Minguez, P. Bork, C. von
Mering, and L. J. Jensen. 2013. STRING v9.1: protein-protein interaction networks, with increased coverage
and integration. Nucleic Acids Res, 41(Database issue):D808?D815, Jan.
J. Hakenberg, M. Gerner, M. Haeussler, I. Solt, C. Plake, M. Schroeder, G. Gonzalez, G. Nenadic, and C. M.
Bergman. 2011. The GNAT library for local and remote gene mention normalization. Bioinformatics,
27(19):2769?2771, Oct.
D. Hanisch, K. Fundel, H.-T. Mevissen, R. Zimmer, and J. Fluck. 2005. ProMiner: rule-based protein and gene
entity recognition. BMC Bioinformatics, 6 Suppl 1:S14.
M. Hofmann-Apitius, J. Fluck, L. Furlong, O. Fornes, C. Kolarik, S. Hanser, M. Boeker, S. Schulz, F. Sanz,
R. Klinger, T. Mevissen, T. Gattermayer, B. Oliva, and C. M. Friedrich. 2008. Knowledge environments
representing molecular entities for the virtual physiological human. Philos Trans A Math Phys Eng Sci,
366(1878):3091?3110, Sep.
H. Ji and R. Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proceedings of
ACL-08: HLT, pages 254?262, Columbus, Ohio, June. Association for Computational Linguistics.
A. Jimeno, E. Jimenez-Ruiz, V. Lee, S. Gaudan, R. Berlanga, and D. Rebholz-Schuhmann. 2008. Assessment of
disease named entity recognition on a corpus of annotated sentences. BMC Bioinformatics, 9 Suppl 3:S3.
31
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In Proceedings of ICML 2001, pages 282?289. Morgan Kaufmann.
V. P. Lemmon, A. R. Ferguson, P. G. Popovich, X.-M. Xu, D. M. Snow, M. Igarashi, C. E. Beattie, J. L. Bixby
et al. 2014. Minimum Information About a Spinal Cord Injury Experiment (MIASCI) ? a proposed reporting
standard for spinal cord injury experiments. Neurotrauma. in press.
C. E. Lipscomb. 2000. Medical Subject Headings (MeSH). Bull Med Libr Assoc, 88(3):265?266, Jul.
C. Lok. 2010. Literature mining: Speed reading. Nature, 463(7280):416?418, Jan.
D. Nadeau and S. Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investiga-
tiones, 30(1):3?26.
C. Nedellec, R. Bossy, J.-D. Kim, J. jae Kim, T. Ohta, S. Pyysalo, and P. Zweigenbaum, editors. 2013. Proceedings
of the BioNLP Shared Task 2013 Workshop. Association for Computational Linguistics, Sofia, Bulgaria, August.
P. V. Ogren. 2006. Knowtator: a prot?g? plug-in for annotated corpus construction. In Proceedings NAACL/HLT
2006, pages 273?275, Morristown, NJ, USA. Association for Computational Linguistics.
E. Pafilis, S. P. Frankild, L. Fanini, S. Faulwetter, C. Pavloudi, A. Vasileiadou, C. Arvanitidis, and L. J. Jensen.
2013. The SPECIES and ORGANISMS Resources for Fast and Accurate Identification of Taxonomic Names
in Text. PLoS One, 8(6):e65390.
F. Prinz, T. Schlange, and K. Asadullah. 2011. Believe it or not: how much can we rely on published data on
potential drug targets? Nat Rev Drug Discov, 10(9):712, Sep.
H. Saggion, A. Funk, D. Maynard, and K. Bontcheva. 2007. Ontology-Based Information Extraction for Business
Intelligence. In K. A. et al., editor, The Semantic Web, volume 4825 of Lecture Notes in Computer Science,
pages 843?856. Springer.
G. K. Savova, J. J. Masanz, P. V. Ogren, J. Zheng, S. Sohn, K. C. Kipper-Schuler, and C. G. Chute. 2010. Mayo
clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and
applications. J Am Med Inform Assoc, 17(5):507?513.
E. W. Sayers, T. Barrett, D. A. Benson, E. Bolton, S. H. Bryant, K. Canese, V. Chetvernin, D. M. Church, M. Dicuc-
cio, S. Federhen, M. Feolo, I. M. Fingerman, L. Y. Geer, W. Helmberg, Y. Kapustin, S. Krasnov, D. Landsman,
D. J. Lipman, Z. Lu, T. L. Madden, T. Madej, D. R. Maglott, A. Marchler-Bauer, V. Miller, I. Karsch-Mizrachi,
J. Ostell, A. Panchenko, L. Phan, K. D. Pruitt, G. D. Schuler, E. Sequeira, S. T. Sherry, M. Shumway, K. Sirotkin,
D. Slotta, A. Souvorov, G. Starchenko, T. A. Tatusova, L. Wagner, Y. Wang, W. J. Wilbur, E. Yaschenko, and
J. Ye. 2012. Database resources of the National Center for Biotechnology Information. Nucleic Acids Res,
40(Database issue):D13?D25, Jan.
M. Schuemie, R. Jelier, and J. Kors. 2007. Peregrine: lightweight gene name normalization by dictionary lookup.
In Proceedings of the Biocreative 2 workshop 2007, page 131?140, Madrid, Spain, April.
O. Steward, P. G. Popovich, W. D. Dietrich, and N. Kleitman. 2012. Replication and reproducibility in spinal cord
injury research. Exp Neurol, 233(2):597?605, Feb.
S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda. 2008. Linguistic Resources and Evaluation
Techniques for Evaluation of Cross-Document Automatic Content Extraction. In Proceedings of the Language
Resources and Evaluation Conference, pages 2706?2709.
P. Thomas, J. Starlinger, A. Vowinkel, S. Arzt, and U. Leser. 2012. GeneView: a comprehensive semantic search
engine for PubMed. Nucleic Acids Res, 40:W585?W591, Jul.
J. Tsujii, J.-D. Kim, and S. Pyysalo, editors. 2011. Proceedings of BioNLP Shared Task 2011 Workshop. Associa-
tion for Computational Linguistics, Portland, Oregon, USA, June.
J. Tsujii, editor. 2009. Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task. Associa-
tion for Computational Linguistics, Boulder, Colorado, June.
D. C. Wimalasuriya and D. Dou. 2010. Ontology-based information extraction: An introduction and a survey of
current approaches. Journal of Information Science, 36(3):306?323.
32
