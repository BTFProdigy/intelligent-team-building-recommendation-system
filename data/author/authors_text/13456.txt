Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 349?357,
Beijing, August 2010
EMDC: A Semi-supervised Approach for Word Alignment
Qin Gao
Language Technologies Institute
Carnegie Mellon University
qing@cs.cmu.edu
Francisco Guzman
Centro de Sistemas Inteligentes
Tecnolo?gico de Monterrey
guzmanhe@gmail.com
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
stephan.vogel@cs.cmu.edu
Abstract
This paper proposes a novel semi-
supervised word alignment technique
called EMDC that integrates discrimina-
tive and generative methods. A discrim-
inative aligner is used to find high preci-
sion partial alignments that serve as con-
straints for a generative aligner which
implements a constrained version of the
EM algorithm. Experiments on small-size
Chinese and Arabic tasks show consistent
improvements on AER. We also experi-
mented with moderate-size Chinese ma-
chine translation tasks and got an aver-
age of 0.5 point improvement on BLEU
scores across five standard NIST test sets
and four other test sets.
1 Introduction
Word alignment is a crucial component in sta-
tistical machine translation (SMT). From a Ma-
chine Learning perspective, the models for word
alignment can be roughly categorized as gener-
ative models and discriminative models. The
widely used word alignment tool, i.e. GIZA++
(Och and Ney, 2003), implements the well-known
IBM models (Brown et al, 1993) and the HMM
model (Vogel et al, 1996), which are genera-
tive models. For language pairs such as Chinese-
English, the word alignment quality is often un-
satisfactory. There has been increasing interest on
using manual alignments in word alignment tasks,
which has resulted in several discriminative mod-
els. Ittycheriah and Roukos (2005) proposed to
use only manual alignment links in a maximum
entropy model, which is considered supervised.
Also, a number of semi-supervised word align-
ers have been proposed (Taskar et al, 2005; Liu
et al, 2005; Moore, 2005; Blunsom and Cohn,
2006; Niehues and Vogel, 2008). These methods
use held-out manual alignments to tune weights
for discriminative models, while using the model
parameters, model scores or alignment links from
unsupervised word aligners as features. Callison-
Burch et. al. (2004) proposed a method to interpo-
late the parameters estimated by sentence-aligned
and word-aligned corpus. Also, there are recent
attempts to combine multiple alignment sources
using alignment confidence measures so as to im-
prove the alignment quality (Huang, 2009).
In this paper, the question we address is
whether we can jointly improve discriminative
models and generative models by feeding the in-
formation we get from the discriminative aligner
back into the generative aligner. Examples of
this line of research include Model 6 (Och and
Ney, 2003) and the EMD training approach pro-
posed by Fraser and Marcu (2006) and its ex-
tension called LEAF aligner (Fraser and Marcu,
2007). These approaches use labeled data to tune
additional parameters to weight different compo-
nents of the IBM models such as the lexical trans-
lation model, the distortion model and the fertility
model. These methods are proven to be effective
in improving the quality of alignments. However,
the discriminative training in these methods is re-
stricted in using the model components of gener-
ative models, in other words, incorporating new
features is difficult.
Instead of using discriminative training meth-
ods to tune the weights of generative models,
in this paper we propose to use a discrimina-
tive word aligner to produce reliable constraints
for the EM algorithm. We call this new train-
ing scheme EMDC (Expectation-Maximization-
Discrimination-Constraint). The methodology
can be viewed as a variation of bootstrapping. It
enables the generative models to interact with dis-
criminative models at the data level instead of the
model level. Furthermore, with a discriminative
349
word aligner that uses generative word aligner?s
output as features, we create a feedback loop that
can iteratively improve the quality of both align-
ers. The major contributions of this paper are: 1)
The EMDC training scheme, which ties the gen-
erative and discriminative aligners together and
enables future research on integrating other dis-
criminative aligners. 2) An extended generative
aligner based on GIZA++ that allows to perform
constrained EM training.
In Section 2, we present the EMDC training
scheme. Section 3 provides details of the con-
strained EM algorithm. In Section 4, we intro-
duce the discriminative aligner and link filtering.
Section 5 provides the experiment set-up and the
results. Section 6 concludes the paper.
2 EMDC Training Scheme
The EMDC training scheme consists of
three parts, namely EM, Discrimination, and
Constraints. As illustrated in Figure 1, a large
unlabeled training set is first aligned with a gen-
erative aligner (GIZA++ for the purpose of this
paper). The generative aligner outputs the model
parameters and the Viterbi alignments for both
source-to-target and target-to-source directions.
Afterwards, a discriminative aligner (we use the
one described in (Niehues and Vogel, 2008)),
takes the lexical translation model, fertility model
and Viterbi alignments from both directions as
features, and is tuned to optimize the AER on a
small manually aligned tuning set. Afterwards,
the alignment links generated by the discrimina-
tive aligner are filtered according to their likeli-
hood, resulting in a subset of links that has high
precision and low recall. The next step is to put
these high precision alignment links back into the
generative aligner as constraints. A conventional
generative word aligner does not support this type
of constraints. Thus we developed a constrained
EM algorithm that can use the links from a partial
alignment as constraints and estimate the model
parameters by marginalizing likelihoods.
After the constrained EM training is performed,
we repeat the procedure and put the updated gen-
erative models and Viterbi alignment back into the
discriminative aligner. We can either fix the num-
ber of iterations, or stop the procedure when the
gain on AER of a small held-out test set drops be-
Figure 1: Illustration of EMDC training scheme
low a threshold.
The key components for the system are:
1. A generative aligner that can make use of re-
liable alignment links as constraints and im-
prove the models/alignments.
2. A discriminative aligner that outputs con-
fidence scores for alignment links, which
allows to obtain high-precision-low-recall
alignments.
While in this paper we derive the reliable links
by filtering the alignment generated by a discrimi-
native aligner, such partial alignments may be ob-
tained from other sources as well: manual align-
ments, specific named entity aligner, noun-phrase
aligner, etc.
As we mentioned in Section 1, the discrimina-
tive aligner is not restricted to use features param-
eters of generative models and Viterbi alignments.
However, including the features from generative
models is required for iterative training, because
the improvement on the quality of these features
can in turn improve the discriminative aligner. In
our experiments, the discriminative aligner makes
heavy use of the Viterbi alignment and the model
parameters from the generative aligner. Nonethe-
less, one can easily replace the discriminative
aligner or add new features to it without modify-
ing the training scheme. The open-ended prop-
erty of the training scheme makes it a promising
method to integrate different aligners.
In the next two sections, we will describe the
key components of this framework in detail.
3 Constrained EM algorithm
In this section we will briefly introduce the con-
strained EM algorithm we used in the experiment,
350
further details of the algorithm can be found in
(Gao et al, 2010).
The IBM Models (Brown et al, 1993) are a
series of generative models for word alignment.
GIZA++ (Och and Ney, 2003), the most widely
used implementation of IBM models and HMM
(Vogel et al, 1996), employs EM algorithm to es-
timate the model parameters. For simpler models
such as Model 1 and Model 2, it is possible to
obtain sufficient statistics from all possible align-
ments in the E-step. However, for fertility-based
models such as Models 3, 4, and 5, enumerating
all possible alignments is NP-complete. To over-
come this limitation, GIZA++ adopts a greedy
hill-climbing algorithm, which uses simpler mod-
els such as HMM or Model 2 to generate a ?center
alignment? and then tries to find better alignments
among its neighbors. The neighbors of an align-
ment aJ1 = [a1, a2, ? ? ? , aJ ] with aj ? [0, I] are
defined as alignments that can be generated from
aJ1 by one of the following two operators:
1. The move operator m[i,j], that changes aj :=
i, i.e. arbitrarily sets word fj in the target
sentence to align to the word ei in source sen-
tence;
2. The swap operator s[j1,j2] that exchanges aj1
and aj2 .
The algorithm will update the center alignment
as long as a better alignment can be found, and
finally outputs a local optimal alignment. The
neighbor alignments of the final center alignment
are then used in collecting the counts for the M-
Step. Och and Ney (2003) proposed a fast imple-
mentation of the hill-climbing algorithm that em-
ploys two matrices, i.e. Moving MatrixMI?J and
Swapping Matrix SJ?J . Each cell of the matrices
stores the value of likelihood difference after ap-
plying the corresponding operator.
We define a partial alignment constraint of a
sentence pair (fJ1 , eI1) as a set of links: ?JI =
{(i, j)|0 ? i < I, 0 ? j < J}. Given a set of
constraints, an alignment aJ1 = [a1, a2, ? ? ? , aj ]
on the sentence pair fJ1 , eI1, the translation proba-
bility of Pr(fJ1 |eI1) will be zero if the alignment
is inconsistent with the constraints. Constraints
(0, j) or (i, 0) are used to explicitly represent that
word fj or ei is aligned to the empty word.
Under the assumptions of the IBM models,
there are two situations that aJ1 is inconsistent with
?JI :
1. Target word misalignment: The IBM mod-
els assume that one target word can only be
aligned to one source word. Therefore, if the
target word fj aligns to a source word ei,
while the constraint ?JI suggests fj should be
aligned to ei? , the alignment violates the con-
straint and thus is considered inconsistent.
2. Source word to empty word misalignment: if
a source word is aligned to the empty word,
it cannot be aligned to any concrete target
word.
However, the partial alignments, which allow
n-to-n alignments, may already violate the 1-to-n
alignment restriction of the IBM models. In these
cases, we relax the condition in situation 1 that if
the alignment link aj? is consistent with any one
of the conflicting target-to-source constraints, it
will be considered consistent. Also, we arbitrarily
assign the source word to empty word constraints
higher priorities than other constraints, because
unlike situation 1, it does not have the problem
of conflicting with other constraints.
3.1 Constrained hill-climbing algorithm
To ensure that resulting center alignment be
consistent with the constraints, we need to split
the hill-climbing algorithm into two stages: 1) op-
timize towards the constraints and 2) optimize to-
wards the optimal alignment under the constraints.
From a seed alignment, we first move the align-
ment towards the constraints by choosing a move
or swap operator that:
1. produces the alignment that has the highest
likelihood among alignments generated by
other operators,
2. eliminates at least one inconsistent link.
We iteratively update the alignment until no
other inconsistent link can be removed. The algo-
rithm implies that we force the seed alignment to
be closer to the constraints while trying to find the
best consistent alignment. Figure 2 demonstrates
the idea, given the constraints shown in (a), and
the seed alignment shown as solid links in (b), we
351
2005?
?
??
the
summer
of
2005Manual Alignment Link
(a)
2005?
?
??
the
summer
Of
2005Seed Alignment Consistent Alignment Center Alignment
(b)                    (c)
2005?
?
??
the
summer
of
2005
Figure 2: Illustration of Algorithm 1
move the inconsistent link to the dashed link by a
move operation.
After we find the consistent alignment, we pro-
ceed to optimize towards the optimal alignment
under the constraints. The algorithm sets the value
of the cells in moving/swapping matrices to nega-
tive if the corresponding operators will lead to an
inconsistent alignment. The moving matrix needs
to be processed only once, whereas the swapping
matrix needs to be updated every iteration, since
once the alignment is updated, the possible viola-
tions will also change.
If a source word ei is aligned to the empty word,
we set Mi,j = ?1,?j. The swapping matrix does
not need to be modified in this case because the
swapping operator will not introduce new links.
Because the cells that can lead to violations are
set to negative, the operators will never be picked
when updating the center alignments. This en-
sures the consistency of the final center alignment.
3.2 Count Collection
After finding the center alignment, we need to
collect counts from neighbor alignments so that
the M-step can normalize the counts to produce
the model parameters for the next step. In this
stage, we want to make sure all the inconsistent
alignments in the neighbor set of the center align-
ment be ruled out from the sufficient statistics, i.e.
have zero probability. Similar to the constrained
hill climbing algorithm, we can manipulate the
moving/swapping matrices to effectively exclude
inconsistent alignments. Since the original count
collection algorithm depends only on moving and
swapping matrices, we just need to bypass all the
cells which hold negative values, i.e. represent in-
consistent alignments.
We can also view the algorithm as forcing
the posteriors of inconsistent alignments to zero,
and therefore increase the posteriors of consistent
alignments. When no constraint is given, the algo-
rithm falls back to conventional EM, and when all
the alignments are known, the algorithm becomes
fully supervised. And if the alignment quality
can be improved if high-precision partial align-
ment links is given as constraints. In (Gao et al,
2010) we experimented with using a dictionary to
generate such constraints, and in (Gao and Vogel,
2010) we experimented with manual word align-
ments from Mechanical Turk. And in this paper
we try to use an alternative method that uses a dis-
criminative aligner and link filtering to generate
such constraints.
4 Discriminative Aligner and Link
Filtering
We employ the CRF-based discriminative word
aligner described in (Niehues and Vogel, 2008).
The aligner can use a variety of knowledge
sources as features, such as: the fertility and lex-
ical translation model parameters from GIZA++,
the Viterbi alignment from both source-to-target
and target-to-source directions. It can also make
use of first-order features which model the depen-
dency between different links, the Parts-of-Speech
tagging features, the word form similarity feature
and the phrase features. In this paper we use all
the features mentioned above except the POS and
phrase features.
The aligner is trained using a belief-
propagation (BP) algorithm, and can be optimized
to maximize likelihood or directly optimize to-
wards AER on a tuning set. The aligner outputs
confidence scores for alignment links, which
allows us to control the precision and recall
rate of the resulting alignment. Guzman et al
(2009) experimented with different alignments
produced by adjusting the filtering threshold for
the alignment links and showed that they could
get high-precision-low-recall alignments by hav-
ing a higher threshold. Therefore, we replicated
the confidence filtering procedures to produce
the partial alignment constraints. Afterwards
we iterate by putting the partial alignments back
to the constrained word alignment algorithm
described in section 3.
Although the discriminative aligner performs
well in supplying high precision constraints, it
does not model the null alignment explicitly.
352
Num. of
Sentences
Num. of Words Num. of
LinksSource Target
Ch-En 21,863 424,683 524,882 687,247
Ar-En 29,876 630,101 821,938 830,349
Table 1: Corpus statistics of the manual aligned
corpora
Threshold P R AER
Ch-En
0.6 71.30 58.12 35.96
0.7 75.24 54.03 37.11
0.8 85.66 44.19 41.70
0.9 93.70 37.95 45.98
Ar-En
0.6 72.35 59.87 34.48
0.7 77.55 55.58 35.25
0.8 80.07 50.89 37.77
0.9 83.74 44.16 42.17
Table 2: The qualities of the constraints
Hence we are currently not able to provide source
word to empty word alignment constraints which
have been proven to be effective in improving the
alignment quality in (Gao et al, 2010). Due to
space limitation, please refer to: (Niehues and Vo-
gel, 2008; Guzman et al, 2009) for further details
of the aligner and link filtering, respectively.
5 Experiments
To validate the proposed training scheme, we
performed two sets of experiments. First of all,
we experimented with a small manually aligned
corpus to evaluate the ability of the algorithm to
improve the AER. The experiment was performed
on Chinese to English and Arabic to English tasks.
Secondly, we experimented with a moderate size
corpus and performed translation tasks to observe
the effects in translation quality.
5.1 Effects on AER
In order to measure the effects of EMDC in
alignment quality, we experimented with Chinese-
English and Arabic-English manually aligned cor-
pora. The statistics of these sets are shown in Ta-
ble 1. We split the data into two fragments, the
first 100 sentences (Set A) and the remaining (Set
B). We trained generative IBM models using the
Set B, and tuned the discriminative aligner using
the Set A. We evaluated the AER on Set B, but in
any of the training steps the manual alignments of
Set B were not used.
In each iteration of EDMC, we load the model
parameters from the previous step and continue
training using the new constraints. Therefore, it is
important to compare the performance of contin-
uous training against an unconstrained baseline,
because variation in alignment quality could be
attributed to either the effect of more training it-
erations or to the effect of semi-supervised train-
ing scheme. In Figures 3 and 4 we show the
alignment quality for each iteration. Iteration 0 is
the baseline, which comes from standard GIZA++
training1. The grey dash curves represent uncon-
strained Model 4 training, and the curves with
start, circle, cross and diamond markers are con-
strained EM alignments with 0.6, 0.7, 0.8 and
0.9 filtering thresholds respectively. As we can
see from the results, when comparing only the
mono-directional trainings, the alignment quali-
ties improve over the unconstrained training in all
the metrics (precision, recall and AER). From Ta-
ble 2, we observe that the quality of discrimina-
tive aligner also improved. Nonetheless, when
we consider the heuristically symmetrized align-
ment2, we observe mixed results. For instance,
for the Chinese-English case we observe that AER
improves over iterations, but this is the result of
a increasingly higher recall rate in detriment of
precision. Ayan and Dorr (2006) pointed out
that grow-diag-final symmetrization tends to out-
put alignments with high recall and low precision.
However this does not fully explain the tendency
we observed between iterations. The character-
istics of the alignment modified by EDMC that
lead to larger improvements in mono-directional
trainings but a precision drop with symmetrization
heuristics needs to be addressed in future work.
Another observation is how the filtering thresh-
olds affect the results. As we can see in Table 3,
for Chinese to English word alignment, the largest
gain on the alignment quality is observed when
the threshold was set to 0.8, while for Arabic to
English, the threshold of 0.7 or 0.6 works better.
Table 2 shows the precision, recall, and AER of
the constraint links used in the constrained EM al-
1We run 5, 5, 3, 3 iterations of Model 1, HMM, Model 3
and Model 4 respectively.
2We used grow-diag-final-and
353
0 2 4 6 860
6264
66
%
Precision
 
 
0 2 4 6 85052
5456
5860
Recall
0 2 4 6 838
4042
4446
AER
UnconstrainedFiltered 0.6Filtered 0.7Filtered 0.8Filtered 0.9
(a) Arabic-English
0 2 4 6 859
6061
62
%
Precision
 
 
0 2 4 6 864
6668
7072
Recall
0 2 4 6 83334
3536
3738
39 AER
(b) English-Arabic
0 2 4 6 860.5
6161.5
6262.5
63
%
Precision
 
 
0 2 4 6 866
68
70
72 Recall
0 2 4 6 83233
3435
3637
AER
(c) Heuristically-symmetrized
Figure 3: Alignment qualities of each iteration for Arabic-English word alignment task. The grey dash
curves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamond
markers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 filtering thresholds respectively.
Source-Target Target-Source Heuristic Discriminative
P R AER P R AER P R AER P R AER
Ch
BL 68.22 46.88 44.43 65.35 55.05 40.25 69.15 57.47 37.23 67.45 59.77 36.62
NC +0.73 +0.71 -0.74 +1.14 +1.14 -1.15 +0.06 +1.07 -0.66 +0.15 +0.64 -0.42
0.6 +2.17 +2.28 -2.32 +1.17 +2.51 -1.97 -0.64 +2.65 -1.27 -0.39 +1.89 -0.87
0.7 +2.57 +2.32 -2.48 +1.94 +2.34 -2.19 -0.34 +2.30 -1.20 -0.28 +1.60 -0.76
0.8 +3.78 +3.27 -3.55 +2.94 +3.32 -3.18 -0.52 +3.32 -1.70 +0.69 +0.14 -0.89
0.9 +0.98 +1.13 -1.11 +1.48 +1.85 -1.71 -0.55 +1.94 -0.90 -0.58 +1.45 -0.54
Ar
BL 58.41 50.42 45.88 59.08 64.84 38.17 60.35 66.99 36.50 68.93 63.94 33.66
NC +2.98 +2.92 -2.96 +1.40 +2.06 -1.70 +0.97 +2.14 -1.49 -0.87 +2.37 -0.83
0.6 +6.69 +8.02 -7.47 +3.45 +6.70 -4.90 +2.62 +4.71 -3.55 +0.58 -0.55 +0.03
0.7 +8.38 +7.93 -8.16 +3.65 +5.26 -4.38 +2.83 +4.70 -3.67 +2.46 -0.42 -0.88
0.8 +6.48 +6.27 -6.39 +2.18 +3.54 -2.80 +1.81 +3.81 -2.70 +1.67 +2.30 -2.01
0.9 +4.02 +4.07 -4.07 +1.70 +3.10 -2.33 +0.62 +3.82 -2.03 +1.33 +2.70 -2.06
Table 3: Improvement on word alignment quality on small corpus after 8 iterations. BL stands for
baseline, and NC represents unconstrained Model 4 training, and 0.9, 0.8, 0.7, 0.6 are the thresholds
used in alignment link filtering.
gorithm, the numbers are averaged across all iter-
ations, the actual numbers of each iteration only
have small differences. Although one might ex-
pect that the quality of resulting alignment from
constrained EM be proportional to the quality of
constraints, from the numbers in Table 2 and 3,
we are not able to induce a clear relationship be-
tween them, and it could be language- or corpus-
dependent. However, in practice we nonetheless
use a held-out test set to tune this parameter. The
354
0 2 4 6 869
7071
72
%
Precision
 
 
0 2 4 6 84647
4849
5051
Recall
0 2 4 6 84041
4243
4445
AER
UnconstrainedFiltered 0.6Filtered 0.7Filtered 0.8Filtered 0.9
(a) Chinese-English
0 2 4 6 865.5
6666.5
6767.5
68
%
Precision
 
 
0 2 4 6 855
5657
5859
Recall
0 2 4 6 837
3839
4041
AER
(b) English-Chinese
0 2 4 6 868.6
68.869
69.2
%
Precision
 
 
0 2 4 6 857
5859
6061
Recall
0 2 4 6 835
36
37
38 AER
(c) Heuristically-symmetrized
Figure 4: Alignment qualities of each iteration for Chinese-English word alignment task. The grey dash
curves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamond
markers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 filtering thresholds respectively.
Ch-En En-Ch Heuristic Discriminative
P R AER P R AER P R AER P R AER
BL 73.51 50.14 40.38 68.82 57.66 37.31 72.98 60.23 34.01 72.10 61.63 33.55
NC 73.23 50.38 40.30 68.30 58.00 37.27 72.39 60.99 33.80 72.07 61.81 33.45
0.8 76.27 52.90 37.53 70.26 60.26 35.11 72.75 63.49 32.19 72.64 63.29 32.35
Table 4: Improvement on word alignment quality on moderate-size corpus, where BL and NC represents
baseline and non-constrained Model 4 training
relationship between quality of constraints and
alignment results is an interesting topic for future
research.
5.2 Effects on translation quality
In this experiment we run the whole machine
translation pipeline and evaluate the system on
BLEU score. We used the corpus LDC2006G05
which contains 25 million words as training set,
the same discriminative tuning set as previously
used (100 sentence pairs) and the remaining
21,763 sentence pairs from the hand-aligned cor-
pus of the previous experiment are held-out test
set for alignment qualities. A 4-gram language
model trained from English GigaWord V1 and V2
corpus was used. The AER scores on the held-
out test set are also provided for every iteration.
Based on the observation in last experiment, we
adopt the filtering threshold of 0.8.
Similar to previous experiment, the heuristi-
cally symmetrized alignments have lower preci-
sions than their EMDC counterparts, however the
gaps are smaller as shown in Table 4. We observe
2.85 and 2.21 absolute AER reduction on two di-
rections, after symmetrization the gain on AER
is 1.82. Continuing Model 4 training appears to
have minimal effect on AER, and the improve-
355
I M NIST GALE
mt06 mt02 mt03 mt04 mt05 mt08 ain db-nw db-wb dd-nw dd-wb aia
0 G 31.00 31.80 29.89 32.63 29.33 24.24 26.92 24.48 28.44 24.26
1 D 30.65 31.60 30.04 32.89 29.34 24.52 0.12 27.43 24.72 28.32 24.30 0.14G 31.35 31.91 30.35 32.75 29.40 24.16 0.15 27.39 24.50 28.22 24.60 0.15
2 D 31.61 32.31 30.40 33.06 29.49 24.11 0.33 28.17 24.42 28.58 24.36 0.34G 31.14 31.94 30.42 32.86 29.49 24.15 0.20 27.31 24.51 27.50 24.02 0.03
3 D 31.29 32.39 30.28 33.19 29.60 24.41 0.43 27.64 25.32 28.55 24.71 0.47G 30.94 31.95 30.15 32.71 29.38 24.22 0.12 27.63 24.61 28.80 25.05 0.29
4 D 30.80 32.04 30.51 33.24 29.49 24.61 0.46 27.61 25.27 28.72 24.98 0.53G 30.68 31.81 30.33 33.05 29.28 24.41 0.26 27.20 24.79 28.43 24.50 0.24
5 D 30.93 31.89 29.96 32.89 29.37 24.50 0.17 27.75 24.50 29.05 24.90 0.33G 31.16 32.28 30.72 33.30 29.83 24.30 0.51 27.32 25.05 28.60 25.44 0.54
Table 5: Improvement on translation alignment quality on moderate-size corpus, The column ain shows
the average improvement of BLEU scores for all NIST test sets (excluding the tuning set MT06), and
column aia is the average improvement on all unseen test sets. The column M indicates the alignment
source, G means the alignment comes from generative aligner, and D means discriminative aligner
respectively. The number of iterations is shown in column I.
ment mainly comes from the constraints.
In the experiment, we use the Moses toolkit to
extract phrases, tune parameters and decode. We
use the NIST MT06 test set as the tuning set,
NIST MT02-05 and MT08 as unseen test sets.
We also include results for four additional unseen
test sets used in GALE evaluations: DEV07-Dev
newswire part (dd-nw, 278 sentences) and We-
blog part (dd-wb, 345 sentences), Dev07-Blind
newswire part (db-nw, 276 sentences and Weblog
part (db-wb, 312 sentences). Table 5 presents the
average improvement on BLEU scores in each it-
eration. As we can see from the results, in all iter-
ations we got improvement on BLEU scores, and
the largest gain we have gotten is on the fifth it-
eration, which has 0.51 average improvement on
five NIST test sets, and 0.54 average improvement
across all nine test sets.
6 Conclusion
In this paper we presented a novel training
scheme for word alignment task called EMDC.
We also presented an extension of GIZA++ that
can perform constrained EM training. By inte-
grating it with a CRF-based discriminative word
aligner and alignment link filtering, we can im-
prove the alignment quality of both aligners itera-
tively. We experimented with small-size Chinese-
English and Arabic English and moderate-size
Chinese-English word alignment tasks, and ob-
served in all four mono-directional alignments
more than 3% absolute reduction on AER, with
the largest improvement being 8.16% absolute on
Arabic-to-English comparing to the baseline, and
5.90% comparing to Model 4 training with the
same numbers of iterations. On a moderate-size
Chinese-to-English tasks we also evaluated the
impact of the improved alignment on translation
quality across nine test sets. The 2% absolute
AER reduction resulted in 0.5 average improve-
ment on BLEU score.
Observations on the results raise several inter-
esting questions for future research, such as 1)
What is the relationship between the precision of
the constraints and the quality of resulting align-
ments after iterations, 2) The effect of using dif-
ferent discriminative aligners, 3) Using aligners
that explicitly model empty words and null align-
ments to provide additional constraints. We will
continue exploration on these directions.
The extended GIZA++ is released to the re-
search community as a branch of MGIZA++ (Gao
and Vogel, 2008), which is available online3.
Acknowledgement
This work is supported by NSF CluE Project
(NSF 08-560) and DARPA GALE project.
3Accessible on Source Forge, with the URL:
http://sourceforge.net/projects/mgizapp/
356
References
Ayan, Necip Fazil and Bonnie J. Dorr. 2006. Going
beyond aer: an extensive analysis of word align-
ments and their impact on mt. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 9?16.
Blunsom, Phil and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 65?72.
Brown, Peter F., Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. In Computational Linguistics,
volume 19(2), pages 263?331.
Callison-Burch, C., D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-
and sentence-aligned parallel corpora. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 175?183.
Fraser, Alexander and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 769?776.
Fraser, Alexander and Daniel Marcu. 2007. Get-
ting the structure right for word alignment: LEAF.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 51?60.
Gao, Qin and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the ACL 2008 Software Engineering, Testing, and
Quality Assurance Workshop, pages 49?57.
Gao, Qin and Stephan Vogel. 2010. Consensus ver-
sus expertise : A case study of word alignment with
mechanical turk. In NAACL 2010 Workshop on Cre-
ating Speech and Language Data With Mechanical
Turk, pages 30?34.
Gao, Qin, Nguyen Bach, and Stephan Vogel. 2010.
A semi-supervised word alignment algorithm with
partial manual alignments. In In Proceedings of
the ACL 2010 joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR (ACL-2010
WMT).
Guzman, Francisco, Qin Gao, and Stephan Vogel.
2009. Reassessment of the role of phrase extrac-
tion in pbsmt. In The twelfth Machine Translation
Summit.
Huang, Fei. 2009. Confidence measure for word
alignment. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 932?940.
Ittycheriah, Abraham and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 89?96.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 459?466.
Moore, Robert C. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 81?88.
Niehues, Jan. and Stephan. Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18?25.
Och, Franz Joseph and Hermann Ney. 2003. A
systematic comparison of various statistical align-
ment models. In Computational Linguistics, vol-
ume 1:29, pages 19?51.
Taskar, Ben, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 73?80.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM based word alignment in statis-
tical machine translation. In Proceedings of 16th In-
ternational Conference on Computational Linguis-
tics), pages 836?841.
357
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 12?17,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Tale about PRO and Monsters
Preslav Nakov, Francisco Guzma?n and Stephan Vogel
Qatar Computing Research Institute, Qatar Foundation
Tornado Tower, floor 10, PO box 5825
Doha, Qatar
{pnakov,fherrera,svogel}@qf.org.qa
Abstract
While experimenting with tuning on long
sentences, we made an unexpected discov-
ery: that PRO falls victim to monsters ?
overly long negative examples with very
low BLEU+1 scores, which are unsuitable
for learning and can cause testing BLEU
to drop by several points absolute. We
propose several effective ways to address
the problem, using length- and BLEU+1-
based cut-offs, outlier filters, stochastic
sampling, and random acceptance. The
best of these fixes not only slay and pro-
tect against monsters, but also yield higher
stability for PRO as well as improved test-
time BLEU scores. Thus, we recommend
them to anybody using PRO, monster-
believer or not.
1 Once Upon a Time...
For years, the standard way to do statistical ma-
chine translation parameter tuning has been to
use minimum error-rate training, or MERT (Och,
2003). However, as researchers started using mod-
els with thousands of parameters, new scalable op-
timization algorithms such as MIRA (Watanabe et
al., 2007; Chiang et al, 2008) and PRO (Hopkins
and May, 2011) have emerged. As these algo-
rithms are relatively new, they are still not quite
well understood, and studying their properties is
an active area of research.
For example, Nakov et al (2012) have pointed
out that PRO tends to generate translations that
are consistently shorter than desired. They
have blamed this on inadequate smoothing in
PRO?s optimization objective, namely sentence-
level BLEU+1, and they have addressed the prob-
lem using more sensible smoothing. We wondered
whether the issue could be partially relieved sim-
ply by tuning on longer sentences, for which the
effect of smoothing would naturally be smaller.
To our surprise, tuning on the longer 50% of the
tuning sentences had a disastrous effect on PRO,
causing an absolute drop of three BLEU points
on testing; at the same time, MERT and MIRA
did not have such a problem. While investigating
the reasons, we discovered hundreds of monsters
creeping under PRO?s surface...
Our tale continues as follows. We first explain
what monsters are in Section 2, then we present a
theory about how they can be slayed in Section 3,
we put this theory to test in practice in Section 4,
and we discuss some related efforts in Section 5.
Finally, we present the moral of our tale, and we
hint at some planned future battles in Section 6.
2 Monsters, Inc.
PRO uses pairwise ranking optimization, where
the learning task is to classify pairs of hypotheses
into correctly or incorrectly ordered (Hopkins and
May, 2011). It searches for a vector of weights
w such that higher evaluation metric scores cor-
respond to higher model scores and vice versa.
More formally, PRO looks for weights w such that
g(i, j) > g(i, j?) ? hw(i, j) > hw(i, j?), where
g is a local scoring function (typically, sentence-
level BLEU+1) and hw are the model scores for
a given input sentence i and two candidate hy-
potheses j and j? that were obtained using w. If
g(i, j) > g(i, j?), we will refer to j and j? as the
positive and the negative example in the pair.
Learning good parameter values requires nega-
tive examples that are comparable to the positive
ones. Instead, tuning on long sentences quickly
introduces monsters, i.e., corrupted negative ex-
amples that are unsuitable for learning: they are
(i) much longer than the respective positive ex-
amples and the references, and (ii) have very low
BLEU+1 scores compared to the positive exam-
ples and in absolute terms. The low BLEU+1
means that PRO effectively has to learn from pos-
itive examples only.
12
Avg. Lengths Avg. BLEU+1
iter. pos neg ref. pos neg
1 45.2 44.6 46.5 52.5 37.6
2 46.4 70.5 53.2 52.8 14.5
3 46.4 261.0 53.4 52.4 2.19
4 46.4 250.0 53.0 52.0 2.30
5 46.3 248.0 53.0 52.1 2.34
. . . . . . . . . . . . . . . . . .
25 47.9 229.0 52.5 52.2 2.81
Table 1: PRO iterations, tuning on long sentences.
Table 1 shows an optimization run of PRO when
tuning on long sentences. We can see monsters
after iterations in which positive examples are on
average longer than negative ones (e.g., iter. 1).
As a result, PRO learns to generate longer sen-
tences, but it overshoots too much (iter. 2), which
gives rise to monsters. Ideally, the learning algo-
rithm should be able to recover from overshoot-
ing. However, once monsters are encountered,
they quickly start dominating, with no chance for
PRO to recover since it accumulates n-best lists,
and thus also monsters, over iterations. As a result,
PRO keeps jumping up and down and converges to
random values, as Figure 1 shows.
By default, PRO?s parameters are averaged
over iterations, and thus the final result is quite
mediocre, but selecting the highest tuning score
does not solve the problem either: for example,
on Figure 1, PRO never achieves a BLEU better
than that for the default initialization parameters.
?
?
? ?
?
?
?
? ?
?
? ? ? ? ? ? ? ? ? ? ? ?
?
? ?
5 10 15 20 250
10
20
30
40
iteration
BLE
U sc
ore
?
?
?
?
? ?
?
? ?
?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1
2
3
4
5
Leng
th ra
tio
Figure 1: PRO tuning results on long sentences
across iterations. The dark-gray line shows the
tuning BLEU (left axis), the light-gray one is the
hypothesis/reference length ratio (right axis).
Figure 2 shows the translations after iterations
1, 3 and 4; the last two are monsters. The monster
at iteration 3 is potentially useful, but that at itera-
tion 4 is clearly unsuitable as a negative example.
Optimizer Objective BLEU
PRO sent-BLEU+1 44.57
MERT corpus-BLEU 47.53
MIRA pseudo-doc-BLEU 47.80
PRO (6= objective) pseudo-doc-BLEU 21.35
MIRA (6= objective) sent-BLEU+1 47.59
PRO, PC-smooth, ground fixed sent-BLEU+1 45.71
Table 2: PRO vs. MERT vs. MIRA.
We also checked whether other popular opti-
mizers yield very low BLEU scores at test time
when tuned on long sentences. Lines 2-3 in Ta-
ble 2 show that this is not the case for MERT and
MIRA. Since they optimize objectives that are dif-
ferent from PRO?s,1 we further experimented with
plugging MIRA?s objective into PRO and PRO?s
objective into MIRA. The resulting MIRA scores
were not much different from before, while PRO?s
score dropped even further; we also found mon-
sters. Next, we applied the length fix for PRO
proposed in (Nakov et al, 2012); this helped a
bit, but still left PRO two BLEU points behind
MERT2 and MIRA, and the monsters did not go
away. We can conclude that the monster problem
is PRO-specific, cannot be blamed on the objective
function, and is different from the length bias.
Note also that monsters are not specific to a
dataset or language pair. We found them when
tuning on the top-50% of WMT10 and testing on
WMT11 for Spanish-English; this yielded a drop
in BLEU from 29.63 (MERT) to 27.12 (PRO).
From run 110 /home/guzmanhe/NIST12/ems/preslav-mada-atb/tuning/tmp.110
**REF**: but we have to close ranks with each other and realize that in 
unity there is strength while in division there is weakness . 
-----------------------------------------------------
**IT1**: but we are that we add our ranks to some of us and that we know 
that in the strength and weakness in
**IT3**:, we are the but of the that that the , and , of ranks the the on 
the the our the our the some of we can include , and , of to the of we know 
the the our in of the of some people , force of the that that the in of the 
that that the the weakness Union the the , and
**IT4**: namely Dr Heba Handossah and Dr Mona been pushed aside because a 
larger story EU Ambassador to Egypt Ian Burg highlighted 've dragged us 
backwards and dragged our speaking , never balme your defaulting a December 
7th 1941 in Pearl Harbor ) we can include ranks will be joined by all 've 
dragged us backwards and dragged our $ 3.8 billion in tourism income 
proceeds Chamber are divided among themselves : some 've dragged us 
backwards and dragged our were exaggerated . Al @-@ Hakim namely Dr Heba 
Handossah and Dr Mona December 7th 1941 in Pearl Harbor ) cases might be 
known to us December 7th 1941 in Pearl Harbor ) platform depends on 
combating all liberal policies Track and Field Federation shortened strength 
as well face several challenges , namely Dr Heba Handossah and Dr Mona 
platform depends on combating all liberal policies the report forecast that 
the weak structure
**IT7**: , the sakes of our on and the , the we can include however , the Al 
ranks the the on the , to the = last of we , the long of the part of some of 
to the affect that the of some is the with ] us our to the affect that the 
with ] us our of the in baker , the cook , the on and the , the we know , 
has are in the heaven of to the affect that the of weakness of @-@ Ittihad 
@-@ Al the force , to 
Figure 2: Example reference translation and hy-
pothesis translations after iterations 1, 3 and 4.
The last two hypotheses are monsters.
1See (Cherry and Foster, 2012) for details on objectives.
2Also, using PRO to initialize MERT, as implemented in
Moses, yields 46.52 BLEU and monsters, but using MERT to
initialize PRO yields 47.55 and no monsters.
13
3 Slaying Monsters: Theory
Below we explain what monsters are and where
they come from. Then, we propose various mon-
ster slaying techniques to be applied during PRO?s
selection and acceptance steps.
3.1 What is PRO?
PRO is a batch optimizer that iterates between
(i) translation: using the current parameter values,
generate k-best translations, and (ii) optimization:
using the translations from all previous iterations,
find new parameter values. The optimization step
has four substeps:
1. Sampling: For each sentence, sample uni-
formly at random ? = 5000 pairs from the
set of all candidate translations for that sen-
tence from all previous iterations.
2. Selection: From these sampled pairs, select
those for which the absolute difference be-
tween their BLEU+1 scores is higher than
? = 0.05 (note: this is 5 BLEU+1 points).
3. Acceptance: For each sentence, accept the
? = 50 selected pairs with the highest abso-
lute difference in their BLEU+1 scores.
4. Learning: Assemble the accepted pairs for
all sentences into a single set and use it to
train a ranker to prefer the higher-scoring
sentence in each pair.
We believe that monsters are nurtured by PRO?s
selection and acceptance policies. PRO?s selec-
tion step filters pairs involving hypotheses that dif-
fer by less than five BLEU+1 points, but it does
not cut-off ones that differ too much based on
BLEU+1 or length. PRO?s acceptance step selects
? = 50 pairs with the highest BLEU+1 differ-
entials, which creates breeding ground for mon-
sters since these pairs are very likely to include
one monster and one good hypothesis.
Below we discuss monster slaying geared to-
wards the selection and acceptance steps of PRO.
3.2 Slaying at Selection
In the selection step, PRO filters pairs for which
the difference in BLEU+1 is less than five points,
but it has no cut-off on the maximum BLEU+1 dif-
ferentials nor cut-offs based on absolute length or
difference in length. Here, we propose several se-
lection filters, both deterministic and probabilistic.
Cut-offs. A cut-off is a deterministic rule that
filters out pairs that do not comply with some cri-
teria. We experiment with a maximal cut-off on
(a) the difference in BLEU+1 scores and (b) the
difference in lengths. These are relative cut-offs
because they refer to the pair, but absolute cut-offs
that apply to each of the elements in the pair are
also possible (not explored here). Cut-offs (a) and
(b) slay monsters by not allowing the negative ex-
amples to get much worse in BLEU+1 or in length
than the positive example in the pair.
Filtering outliers. Outliers are rare or extreme
observations in a sample. We assume normal dis-
tribution of the BLEU+1 scores (or of the lengths)
of the translation hypotheses for the same source
sentence, and we define as outliers hypotheses
whose BLEU+1 (or length) is more than ? stan-
dard deviations away from the sample average.
We apply the outlier filter to both the positive and
the negative example in a pair, but it is more im-
portant for the latter. We experiment with values
of ? like 2 and 3. This filtering slays monsters be-
cause they are likely outliers. However, it will not
work if the population gets riddled with monsters,
in which case they would become the norm.
Stochastic sampling. Instead of filtering ex-
treme examples, we can randomly sample pairs
according to their probability of being typical. Let
us assume that the values of the local scoring func-
tions, i.e., the BLEU+1 scores, are distributed nor-
mally: g(i, j) ? N(?, ?2). Given a sample of hy-
pothesis translations {j} of the same source sen-
tence i, we can estimate ? empirically. Then,
the difference ? = g(i, j) ? g(i, j?) would be
distributed normally with mean zero and variance
2?2. Now, given a pair of examples, we can calcu-
late their ?, and we can choose to select the pair
with some probability, according to N(0, 2?2).
3.3 Slaying at Acceptance
Another problem is caused by the acceptance
mechanism of PRO: among all selected pairs, it
accepts the top-? with the highest BLEU+1 dif-
ferentials. It is easy to see that these differentials
are highest for nonmonster?monster pairs if such
pairs exist. One way to avoid focusing primarily
on such pairs is to accept a random set of ? pairs,
among the ones that survived the selection step.
One possible caveat is that we can lose some of
the discriminative power of PRO by focusing on
examples that are not different enough.
14
TESTING TUNING (run 1, it. 25, avg.) TEST(tune:full)
Avg. for 3 reruns Lengths BLEU+1 Avg. for 3 reruns
PRO fix BLEU StdDev Pos Neg Ref Pos Neg BLEU StdDev
PRO (baseline) 44.70 0.266 47.9 229.0 52.5 52.2 2.8 47.80 0.052
Max diff. cut-off BLEU+1 max=10 ? 47.94 0.165 47.9 49.6 49.4 49.4 39.9 47.77 0.035
BLEU+1 max=20 ? 47.73 0.136 47.7 55.5 51.1 49.8 32.7 47.85 0.049
LEN max=5 ? 48.09 0.021 46.8 47.0 47.9 52.9 37.8 47.73 0.051
LEN max=10 ? 47.99 0.025 47.3 48.5 48.7 52.5 35.6 47.80 0.056
Outliers BLEU+1 ?=2.0 ? 48.05 0.119 46.8 47.2 47.7 52.2 39.5 47.47 0.090
BLEU+1 ?=3.0 47.12 1.348 47.6 168.0 53.0 51.7 3.9 47.53 0.038
LEN ?=2.0 46.68 2.005 49.3 82.7 53.1 52.3 5.3 47.49 0.085
LEN ?=3.0 47.02 0.727 48.2 163.0 51.4 51.4 4.2 47.65 0.096
Stoch. sampl. ? BLEU+1 46.33 1.000 46.8 216.0 53.3 53.1 2.4 47.74 0.035
? LEN 46.36 1.281 47.4 201.0 52.9 53.4 2.9 47.78 0.081
Table 3: Some fixes to PRO (select pairs with highest BLEU+1 differential, also require at least 5
BLEU+1 points difference). A dagger (?) indicates selection fixes that successfully get rid of monsters.
4 Attacking Monsters: Practice
Below, we first present our general experimental
setup. Then, we present the results for the var-
ious selection alternatives, both with the original
acceptance strategy and with random acceptance.
4.1 Experimental Setup
We used a phrase-based SMT model (Koehn et al,
2003) as implemented in the Moses toolkit (Koehn
et al, 2007). We trained on all Arabic-English
data for NIST 2012 except for UN, we tuned on
(the longest-50% of) the MT06 sentences, and we
tested on MT09. We used the MADA ATB seg-
mentation for Arabic (Roth et al, 2008) and true-
casing for English, phrases of maximal length 7,
Kneser-Ney smoothing, and lexicalized reorder-
ing (Koehn et al, 2005), and a 5-gram language
model, trained on GigaWord v.5 using KenLM
(Heafield, 2011). We dropped unknown words
both at tuning and testing, and we used minimum
Bayes risk decoding at testing (Kumar and Byrne,
2004). We evaluated the output with NIST?s scor-
ing tool v.13a, cased.
We used the Moses implementations of MERT,
PRO and batch MIRA, with the ?return-best-dev
parameter for the latter. We ran these optimizers
for up to 25 iterations and we used 1000-best lists.
For stability (Foster and Kuhn, 2009), we per-
formed three reruns of each experiment (tuning +
evaluation), and we report averaged scores.
4.2 Selection Alternatives
Table 3 presents the results for different selection
alternatives. The first two columns show the test-
ing results: average BLEU and standard deviation
over three reruns.
The following five columns show statistics
about the last iteration (it. 25) of PRO?s tuning
for the worst rerun: average lengths of the positive
and the negative examples and average effective
reference length, followed by average BLEU+1
scores for the positive and the negative examples
in the pairs. The last two columns present the re-
sults when tuning on the full tuning set. These are
included to verify the behavior of PRO in a non-
monster prone environment.
We can see in Table 3 that all selection mech-
anisms considerably improve BLEU compared to
the baseline PRO, by 2-3 BLEU points. However,
not every selection alternative gets rid of monsters,
which can be seen by the large lengths and low
BLEU+1 for the negative examples (in bold).
The max cut-offs for BLEU+1 and for lengths
both slay the monsters, but the latter yields much
lower standard deviation (thirteen times lower than
for the baseline PRO!), thus considerably increas-
ing PRO?s stability. On the full dataset, BLEU
scores are about the same as for the original PRO
(with small improvement for BLEU+1 max=20),
but the standard deviations are slightly better.
Rejecting outliers using BLEU+1 and ? = 3 is
not strong enough to filter out monsters, but mak-
ing this criterion more strict by setting ? = 2,
yields competitive BLEU and kills the monsters.
Rejecting outliers based on length does not
work as effectively though. We can think of two
possible reasons: (i) lengths are not normally dis-
tributed, they are more Poisson-like, and (ii) the
acceptance criterion is based on the top-? differ-
entials based on BLEU+1, not based on length.
On the full dataset, rejecting outliers, BLEU+1
and length, yields lower BLEU and less stability.
15
TESTING TUNING (run 1, it. 25, avg.) TEST(tune:full)
Avg. for 3 reruns Lengths BLEU+1 Avg. for 3 reruns
PRO fix BLEU StdDev Pos Neg Ref Pos Neg BLEU StdDev
PRO (baseline) 44.70 0.266 47.9 229.0 52.5 52.2 2.8 47.80 0.052
Rand. accept PRO, rand ?? 47.87 0.147 47.7 48.5 48.70 47.7 42.9 47.59 0.114
Outliers BLEU+1 ?=2.0, rand? 47.85 0.078 48.2 48.4 48.9 47.5 43.6 47.62 0.091
BLEU+1 ?=3.0, rand 47.97 0.168 47.6 47.6 48.4 47.8 43.6 47.44 0.070
LEN ?=2.0, rand? 47.69 0.114 47.8 47.8 48.6 47.9 43.6 47.48 0.046
LEN ?=3.0, rand 47.89 0.235 47.8 48.0 48.7 47.7 43.1 47.64 0.090
Stoch. sampl. ? BLEU+1, rand? 47.99 0.087 47.9 48.0 48.7 47.8 43.5 47.67 0.096
? LEN, rand? 47.94 0.060 47.8 47.9 48.6 47.8 43.6 47.65 0.097
Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (??) indicates that
random acceptance kills monsters. The asterisk (?) indicates improved stability over random acceptance.
Reasons (i) and (ii) arguably also apply to
stochastic sampling of differentials (for BLEU+1
or for length), which fails to kill the monsters,
maybe because it gives them some probability of
being selected by design. To alleviate this, we test
the above settings with random acceptance.
4.3 Random Acceptance
Table 4 shows the results for accepting training
pairs for PRO uniformly at random. To eliminate
possible biases, we also removed the min=0.05
BLEU+1 selection criterion. Surprisingly, this
setup effectively eliminated the monster problem.
Further coupling this with the distributional cri-
teria can also yield increased stability, and even
small further increase in test BLEU. For instance,
rejecting BLEU outliers with ? = 2 yields com-
parable average test BLEU, but with only half the
standard deviation.
On the other hand, using the stochastic sam-
pling of differentials based on either BLEU+1 or
lengths improves the test BLEU score while in-
creasing the stability across runs. The random
acceptance has a caveat though: it generally de-
creases the discriminative power of PRO, yielding
worse results when tuning on the full, nonmonster
prone tuning dataset. Stochastic selection does
help to alleviate this problem. Yet, the results are
not as good as when using a max cut-off for the
length. Therefore, we recommend using the latter
as a default setting.
5 Related Work
We are not aware of previous work that discusses
the issue of monsters, but there has been work on
a different, length problem with PRO (Nakov et
al., 2012). We have seen that its solution, fix the
smoothing in BLEU+1, did not work for us.
The stability of MERT has been improved using
regularization (Cer et al, 2008), random restarts
(Moore and Quirk, 2008), multiple replications
(Clark et al, 2011), and parameter aggregation
(Cettolo et al, 2011).
With the emergence of new optimization tech-
niques, there have been studies that compare sta-
bility between MIRA?MERT (Chiang et al, 2008;
Chiang et al, 2009; Cherry and Foster, 2012),
PRO?MERT (Hopkins and May, 2011), MIRA?
PRO?MERT (Cherry and Foster, 2012; Gimpel
and Smith, 2012; Nakov et al, 2012).
Pathological verbosity can be an issue when
tuning MERT on recall-oriented metrics such
as METEOR (Lavie and Denkowski, 2009;
Denkowski and Lavie, 2011). Large variance be-
tween the results obtained with MIRA has also
been reported (Simianer et al, 2012). However,
none of this work has focused on monsters.
6 Tale?s Moral and Future Battles
We have studied a problem with PRO, namely that
it can fall victim to monsters, overly long negative
examples with very low BLEU+1 scores, which
are unsuitable for learning. We have proposed sev-
eral effective ways to address this problem, based
on length- and BLEU+1-based cut-offs, outlier fil-
ters and stochastic sampling. The best of these
fixes have not only slayed the monsters, but have
also brought much higher stability to PRO as well
as improved test-time BLEU scores. These bene-
fits are less visible on the full dataset, but we still
recommend them to everybody who uses PRO as
protection against monsters. Monsters are inher-
ent in PRO; they just do not always take over.
In future work, we plan a deeper look at the
mechanism of monster creation in PRO and its
possible connection to PRO?s length bias.
16
References
Daniel Cer, Daniel Jurafsky, and Christopher Manning.
2008. Regularization and search for minimum error
rate training. In Proc. of Workshop on Statistical
Machine Translation, WMT ?08, pages 26?34.
Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-
erico. 2011. Methods for smoothing the optimizer
instability in SMT. MT Summit XIII: the Machine
Translation Summit, pages 32?39.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL-HLT ?12, pages 427?436.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, NAACL-HLT ?09, pages 218?226.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for statis-
tical machine translation: Controlling for optimizer
instability. In Proceedings of the Meeting of the As-
sociation for Computational Linguistics, ACL ?11,
pages 176?181.
Michael Denkowski and Alon Lavie. 2011. Meteor-
tuned phrase-based SMT: CMU French-English and
Haitian-English systems for WMT 2011. Techni-
cal report, CMU-LTI-11-011, Language Technolo-
gies Institute, Carnegie Mellon University.
George Foster and Roland Kuhn. 2009. Stabiliz-
ing minimum error rate training. In Proceedings
of the Workshop on Statistical Machine Translation,
StatMT ?09, pages 242?249.
Kevin Gimpel and Noah Smith. 2012. Structured ramp
loss minimization for machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL-HLT ?12, pages 221?231.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Workshop on Statistical
Machine Translation, WMT ?11, pages 187?197.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1352?1362.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics on Human Language Technology, HLT-
NAACL ?03, pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, IWSLT ?05.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the Meeting of the Association for Compu-
tational Linguistics, ACL ?07, pages 177?180.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
HLT-NAACL ?04, pages 169?176.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23:105?115.
Robert Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statisti-
cal machine translation. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING ?08, pages 585?592.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING ?12, pages 1979?1994.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Meeting of the Association for Computational Lin-
guistics, ACL ?03, pages 160?167.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological
tagging, diacritization, and lemmatization using lex-
eme models and feature ranking. In Proceedings
of the Meeting of the Association for Computational
Linguistics, ACL ?08, pages 117?120.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the Meeting of the Association for
Computational Linguistics, ACL ?12, pages 11?21.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, EMNLP-CoNLL ?07, pages
764?773.
17
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 298?303,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
QCRI at WMT12: Experiments in Spanish-English and German-English
Machine Translation of News Text
Francisco Guzma?n, Preslav Nakov, Ahmed Thabet, Stephan Vogel
Qatar Computing Research Institute
Qatar Foundation
Tornado Tower, floor 10, PO box 5825
Doha, Qatar
{fguzman,pnakov,ahawad,svogel}@qf.org.qa
Abstract
We describe the systems developed by the
team of the Qatar Computing Research Insti-
tute for the WMT12 Shared Translation Task.
We used a phrase-based statistical machine
translation model with several non-standard
settings, most notably tuning data selection
and phrase table combination. The evaluation
results show that we rank second in BLEU and
TER for Spanish-English, and in the top tier
for German-English.
1 Introduction
The team of the Qatar Computing Research Insti-
tute (QCRI) participated in the Shared Translation
Task of WMT12 for two language pairs:1 Spanish-
English and German-English. We used the state-of-
the-art phrase-based model (Koehn et al, 2003) for
statistical machine translation (SMT) with several
non-standard settings, e.g., data selection and phrase
table combination. The evaluation results show that
we rank second in BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006) for Spanish-English, and
in the top tier for German-English.
In Section 2, we describe the parameters of our
baseline system and the non-standard settings we
experimented with. In Section 3, we discuss our
primary and secondary submissions for the two lan-
guage pairs. Finally, in Section 4, we provide a short
summary.
1The WMT12 organizers invited systems translating be-
tween English and four other European languages, in both di-
rections: French, Spanish, German, and Czech. However, we
only participated in Spanish?English and German?English.
2 System Description
Below, in Section 2.1, we first describe our initial
configuration; then, we discuss our incremental im-
provements. We explored several non-standard set-
tings and extensions and we evaluated their impact
with respect to different baselines. These baselines
are denoted in the tables below by a #number that
corresponds to systems in Figures 1 for Spanish-
English and in Figure 2 for German-English.
We report case insensitive BLEU calculated on
the news2011 testing data using the NIST scoring
tool v.11b.
2.1 Initial Configuration
Our baseline system can be summarized as follows:
? Training: News Commentary + Europarl train-
ing bi-texts;
? Tuning: news2010;
? Testing: news2011;
? Tokenization: splitting words containing a
dash, e.g., first-order becomes first @-@ order;
? Maximum sentence length: 100 tokens;
? Truecasing: convert sentence-initial words to
their most frequent case in the training dataset;
? Word alignments: directed IBM model 4
(Brown et al, 1993) alignments in both direc-
tions, then grow-diag-final-and heuristics;
? Maximum phrase length: 7 tokens;
? Phrase table scores: forward & reverse phrase
translation probabilities, forward & reverse lex-
ical translation probabilities, phrase penalty;
298
? Language model: 5-gram, trained on the target
side of the two training bi-texts;
? Reordering: lexicalized, msd-bidirectional-fe;
? Detokenization: reconnecting words that were
split around dashes;
? Model parameter optimization: minimum error
rate training (MERT), optimizing BLEU.
2.2 Phrase Tables
We experimented with two non-standard settings:
Smoothing. The four standard scores associated
with each phrase pair in the phrase table (forward
& reverse phrase translation probabilities, forward
& reverse lexical translation probabilities) are nor-
mally used unsmoothed. We also experimented with
Good-Turing and Kneser-Ney smoothing (Chen and
Goodman, 1999). As Table 1 shows, the latter works
a bit better for both Spanish-English and German-
English.
es-en de-en
Baseline (es:#3,de:#4) 29.98 22.03
Good Turing 29.98 22.07
Kneser-Ney 30.16 22.30
Table 1: Phrase table smoothing.
Phrase table combination. We built two phrase
tables, one for News Commentary + Europarl and an
additional one for the UN bi-text. We then merged
them,2 adding additional features to each entry in
the merged phrase table: F1, F2, and F3. The
value of F1/F2 is 1 if the phrase pair came from the
first/second phrase table, and 0.5 otherwise, while
F3 is 1 if the phrase pair was in both tables, and 0.5
otherwise. We optimized the weights for all features,
including the additional ones, using MERT.3 Table 2
shows that this improves by +0.42 BLEU points.
2In theory, we should also re-normalize the conditional
probabilities (forward/reverse phrase translation probability,
and forward/reverse lexicalized phrase translation probability)
since they may not sum to one anymore. In practice, this is
not that important since the log-linear phrase-based SMT model
does not require that the phrase table features be probabilities
(e.g., F1, F2, F3, and the phrase penalty are not probabilities);
moreover, we have extra features whose impact is bigger.
3This is similar but different from (Nakov, 2008): when a
phrase pair appeared in both tables, they only kept the entry
from the first table, while we keep the entries from both tables.
es-en
Baseline (es:#7) 30.94
Merging (1) News+EP with (2) UN 31.36
Table 2: Phrase table merging.
2.3 Language Models
We built the language models (LM) for our systems
using a probabilistic 5-gram model with Kneser-
Ney (KN) smoothing. We experimented with LMs
trained on different training datasets. We used the
SRILM toolkit (Stolcke, 2002) for training the lan-
guage models, and the KenLM toolkit (Heafield
and Lavie, 2010) for binarizing the resulting ARPA
models for faster loading with the Moses decoder
(Koehn et al, 2007).
2.3.1 Using WMT12 Corpora Only
We trained 5-gram LMs on datasets provided by
the task organizers. The results are presented in
Table 3. The first line reports the baseline BLEU
scores using a language model trained on the target
side of the News Commentary + Europarl training
bi-texts. The second line shows the results when us-
ing an interpolation (minimizing the perplexity on
the news2010 tuning dataset) of different language
models, trained on the following corpora:
? the monolingual News Commentary corpus
plus the English sides of all training News
Commentary v.7 bi-texts (for French-English,
Spanish-English, German-English, and Czech-
English), with duplicate sentences removed
(5.5M word tokens; one LM);
? the News Crawl 2007-2011 corpora, (1213M
word tokens; separate LM for each of these five
years);
? the Europarl v.7 monolingual corpus (60M
word tokens; one LM);
? the English side of the Spanish-English UN bi-
text (360M word tokens; one LM).
The last line in Table 3 shows the results when
using an additional 5-gram LM in the interpolation,
one trained on the English side of the 109 French-
English bi-text (662M word tokens).
299
We can see that using these interpolations yields
very sizable improvements of 1.7-2.5 BLEU points
over the baseline. However, while the impact of
adding the 109 bi-text to the interpolation is clearly
visible for Spanish-English (+0.47 BLEU), it is al-
most negligible for German-English (+0.06 BLEU).
Corpora es-en de-en
Baseline (es:#1, de:#2) 27.34 20.01
News + EP + UN (interp.) 29.36 21.66
News + EP + UN + 109 (interp.) 29.83 21.72
Table 3: LMs using the provided corpora only.
2.3.2 Using Gigaword
In addition to the WMT12 data, we used the LDC
Gigaword v.5 corpus. We divided the corpus into
reasonably-sized chunks of text of about 2GB per
chunk, and we built a separate intermediate language
model for each chunk. Then, we interpolated these
language models, minimizing the perplexity on the
news2010 development set as with the previous
LMs. We experimented with two different strate-
gies for creating the chunks by segmenting the cor-
pus according to (a) data source, e.g., AFP, Xinhua,
etc., and (b) year of release. We thus compared the
advantages of interpolating epoch-consistent LMs
vs. source-coherent LMs. We trained individual
LMs for each of the segments and we added them
to a pool. Finally, we selected the ten most relevant
ones from this pool based on their perplexity on the
news2010 devset, and we interpolated them.
The results are shown in Table 4. The first line
shows the baseline, which uses an interpolation of
the nine LMs from the previous subsection. The
following two lines show the results when using an
LM trained on Gigaword only. We can see that for
Spanish-English, interpolation by year performs bet-
ter, while for German-English, it is better to use the
by-source chunks. However, the following two lines
show that when we translate with two LMs, one built
from the WMT12 data only and one built using Gi-
gaword data only, interpolation by year is preferable
for Gigaword for both language pairs. For our sub-
mitted systems, we used the LMs shown in bold in
Table 4: we used a single LM for Spanish-English
and two LMs for German-English.
Language Models es-en de-en
Baseline (es:#5, de:#6) 30.31 22.48
GW by year 30.68 22.32
GW by source 30.52 22.56
News-etc + GW by year 30.60 22.71
News-etc + GW by source 30.55 22.54
Table 4: LMs using Gigaword.
2.4 Parameter Tuning and Data Selection
Parameter tuning is a very important step in SMT.
The standard procedure consists of performing a se-
ries of iterations of MERT to choose the model pa-
rameters that maximize the translation quality on a
development set, e.g., as measured by BLEU. While
the procedure is widely adopted, it is also recognized
that the selection of an appropriate development set
is important since it biases the parameters towards
specific types of translations. This is illustrated in
Table 5, which shows BLEU on the news2011 testset
when using different development sets for MERT.
Devset es-en
news2008 29.47
news2009 29.14
news2010 29.61
Table 5: Using different tuning sets for MERT.
To address this problem, we performed a selection
of development data using an n-gram-based similar-
ity ranking. The selection was performed over a pool
of candidate sentences drawn from the news2008,
news2009, and news2010 tuning datasets. The sim-
ilarity metric was defined as follows:
sim(f, g) = 2match(f, g) ? lenpen(f, g) (1)
where 2match represents the number of bi-gram
matches between sentences f and g, and lenpen is
a length penalty to discourage unbalanced matches.
We penalized the length difference using an
inverted-squared sigmoid function:
lenpen(f, g) = 3? 4 ? sig
([
|f | ? |g|
?
]2
)
(2)
300
where |.| denotes the length of a sentence in num-
ber of words, ? controls the maximal tolerance to
differences, and sig is the sigmoid function.
To generate a suitable development set, we av-
eraged the similarity scores of candidate sentences
w.r.t. to the target testset. For instance:
sf =
1
|G|
?
g?G
sim(f, g) (3)
where G is the set of the test sentences.
Finally, we selected a pool of candidates f from
news2008, news2009 and news2011 to generate a
2000-best tuning set. The results when using each of
the above penalty functions are presented on Table 6.
devset es-en
baseline (es:#6) 30.68
selection (? = 5) 30.94
selection (? = 10) 30.90
Table 6: Selecting sentences for MERT.
The average length of the source-side sentences
in our selected sentence pairs was smaller than in
our baseline, the news2011 development dataset.
This means that our selected source-side sentences
tended to be shorter than in the baseline. Moreover,
the standard deviation of the sentence lengths was
smaller for our samples as well, which means that
there were fewer long sentences; this is good since
long sentences can take very long to translate. As
a result, we observed sizable speedup in parameter
tuning when running MERT on our selected tuning
datasets.
2.5 Decoding and Hypothesis Reranking
We experimented with two decoding settings:
(1) monotone at punctuation reordering (Tillmann
and Ney, 2003), and (2) minimum Bayes risk decod-
ing (Kumar and Byrne, 2004). The results are shown
in Table 7. We can see that both yield improvements
in BLEU, even if small.
2.6 System Combination
As the final step in our translation system, we per-
formed hypothesis re-combination of the output of
several of our systems using the Multi-Engine MT
system (MEMT) (Heafield and Lavie, 2010).
es-en de-en
Baseline (es:#2,de:#3) 29.83 21.72
+MP 29.98 22.03
Baseline (es:#4,de:#5) 30.16 22.30
+MBR 30.31 22.48
Table 7: Decoding parameters. Experiments with
monotone at punctuation (MP) reordering, and minimum
Bayes risk (MBR) decoding.
The results for the actual news2012 testset are
shown in Table 8: the system combination results
are our primary submission. We can see that system
combination yielded 0.4 BLEU points of improve-
ment for Spanish-English and 0.2-0.3 BLEU points
for German-English.
3 Our Submissions
Here we briefly describe the cumulative improve-
ments when applying the above modifications to our
baseline system, leading to our official submissions
for the WMT12 Shared Translation Task.
3.1 Spanish-English
The development of our final Spanish-English sys-
tem involved several incremental improvements,
which have been described above and which are
summarized in Figure 1. We started with a base-
line system (see Section 2.1), which scored 27.34
BLEU points. From there, using a large inter-
polated language model trained on the provided
data (see Section 2.3.1) yielded +2.49 BLEU points
of improvement. Monotone-at-punctuation de-
coding contributed an additional improvement of
+0.15, smoothing the phrase table using Kneser-Ney
boosted the score by +0.18, and using minimum
Bayes risk decoding added another +0.15 BLEU
points. Changing the language model to one trained
on Gigaword v.5 and interpolated by year yielded
+0.37 additional points of improvement. Another
+0.26 points came from tuning data selection. Fi-
nally, using the UN data in a merged phrase ta-
ble (see Section 2.2) yielded another +0.42 BLEU
points. Overall, we achieve a total improvement
over our initial baseline of about 4 BLEU points.
301
27.34 
29.83 29.98 30.16 30.31 
30.68 
30.94 
31.36 
25 
26 
27 
28 
29 
30 
31 
32 
1:BA
SELI
NE 
2:+W
MT-L
M 
3:+M
P 
4:+K
N 
5:+M
BR 
6:*G
IGA 
V5-L
M 
7:+T
UNE
-SEL
 
8:+P
T-ME
RGE
 
!
BL
EU
 v12
  sc
ore
  (n
ew
s-2
011
) 
Figure 1: Incremental improvements for the Spanish-English system.
3.2 German-English
Figure 2 shows a similar sequence of improvements
for our German-English system. We started with a
baseline (see Section 2.1) that scored 19.79 BLEU
points. Next, we performed compound splitting for
the German side of the training, the development
and the testing bi-texts, which yielded +0.22 BLEU
points of improvement. Using a large interpolated
language model trained on the provided corpora (see
Section 2.3.1) added another +1.71. Monotone-at-
punctuation decoding contributed +0.31, smoothing
the phrase table using Kneser-Ney boosted the score
by +0.27, and using minimum Bayes risk decoding
added another +0.18 BLEU points. Finally, adding a
second language model trained on the Gigaword v.5
corpus interpolated by year yielded +0.23 additional
BLEU points. Overall, we achieved about 3 BLEU
points of total improvement over our initial baseline.
3.3 Final Submissions
For both language pairs, our primary submission
was a combination of the output of several of our
best systems shown in Figures 1 and 2, which use
different experimental settings; our secondary sub-
mission was our best individual system, i.e., the
right-most one in Figures 1 and 2.
The official BLEU scores, both cased and lower-
cased, for our primary and secondary submissions,
as evaluated on the news2012 dataset, are shown
in Table 8. For Spanish-English, we achieved the
second highest BLEU and TER scores, while for
German-English we were ranked in the top tier.
news2012
lower cased
Spanish-English
Primary 34.0 32.9
Secondary 33.6 32.5
German-English
Primary 23.9 22.6
Secondary 23.6 22.4
Table 8: The official BLEU scores for our submissions
to the WMT12 Shared Translation Task.
4 Conclusion
We have described the primary and the secondary
systems developed by the team of the Qatar Com-
puting Research Institute for Spanish-English and
German-English machine translation of news text
for the WMT12 Shared Translation Task.
We experimented with phrase-based SMT, explor-
ing a number of non-standard settings, most notably
tuning data selection and phrase table combination,
which we described and evaluated in a cumulative
fashion. The automatic evaluation metrics,4 have
ranked our system second for Spanish-English and
in the top tier for German-English.
We plan to continue our work on data selection
for phrase table and the language model training, in
addition to data selection for tuning.
4The evaluation scores for WMT12 are available online:
http://matrix.statmt.org/
302
19.79 20.01 
21.72 
22.03 22.30 
22.48 
22.71 
18 
18.5 
19 
19.5 
20 
20.5 
21 
21.5 
22 
22.5 
23 
1:BA
SELI
NE 
2:+S
PLIT
 
3:WM
T-LM
 
4:+M
P 
5:+K
N 
6:+M
BR 
7:+G
IGA+
WMT
 LM 
BL
EU
 v12
  sc
ore
   (n
ew
s-2
011
) 
Figure 2: Incremental improvements for the German-English system.
Acknowledgments
We would like to thank the anonymous reviewers
for their useful comments, which have helped us im-
prove the text of this paper.
References
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
tical machine translation: parameter estimation. Com-
putational Linguistics, 19(2):263?311.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech & Language, 13(4):359?393.
Kenneth Heafield and Alon Lavie. 2010. Combin-
ing machine translation output with open source:
The Carnegie Mellon multi-engine machine transla-
tion scheme. The Prague Bulletin of Mathematical
Linguistics, 93(1):27?36.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL. Demonstration session, ACL ?07, pages
177?180, Prague, Czech Republic.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Susan Dumais, Daniel Marcu, and Salim
Roukos, editors, Proceedings of the Annual Meeting
of the North American chapter of the Association for
Computational Linguistics, HLT-NAACL ?04, pages
169?176, Boston, MA.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the Third Workshop
on Statistical Machine Translation, WMT ?07, pages
147?150, Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, ACL ?02, pages 311?318, Philadelphia,
PA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Annual Meetig of the Associa-
tion for Machine Translation in the Americas, AMTA
?06, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of Intl. Conf.
on Spoken Language Processing, volume 2 of ICSLP
?02, pages 901?904, Denver, CO.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
303
