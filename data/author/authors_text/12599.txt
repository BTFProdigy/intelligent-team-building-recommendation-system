Proceedings of the 12th Conference of the European Chapter of the ACL, pages 273?281,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Who is ?You?? Combining Linguistic and Gaze Features to Resolve
Second-Person References in Dialogue?
Matthew Frampton1, Raquel Ferna?ndez1, Patrick Ehlen1, Mario Christoudias2,
Trevor Darrell2 and Stanley Peters1
1Center for the Study of Language and Information, Stanford University
{frampton, raquelfr, ehlen, peters}@stanford.edu
2International Computer Science Institute, University of California at Berkeley
cmch@icsi.berkeley.edu, trevor@eecs.berkeley.edu
Abstract
We explore the problem of resolving the
second person English pronoun you in
multi-party dialogue, using a combination
of linguistic and visual features. First, we
distinguish generic and referential uses,
then we classify the referential uses as ei-
ther plural or singular, and finally, for the
latter cases, we identify the addressee. In
our first set of experiments, the linguistic
and visual features are derived from man-
ual transcriptions and annotations, but in
the second set, they are generated through
entirely automatic means. Results show
that a multimodal system is often prefer-
able to a unimodal one.
1 Introduction
The English pronoun you is the second most fre-
quent word in unrestricted conversation (after I
and right before it).1 Despite this, with the ex-
ception of Gupta et al (2007b; 2007a), its re-
solution has received very little attention in the lit-
erature. This is perhaps not surprising since the
vast amount of work on anaphora and reference
resolution has focused on text or discourse - medi-
ums where second-person deixis is perhaps not
as prominent as it is in dialogue. For spoken di-
alogue pronoun resolution modules however, re-
solving you is an essential task that has an impor-
tant impact on the capabilities of dialogue summa-
rization systems.
?We thank the anonymous EACL reviewers, and Surabhi
Gupta, John Niekrasz and David Demirdjian for their com-
ments and technical assistance. This work was supported by
the CALO project (DARPA grant NBCH-D-03-0010).
1See e.g. http://www.kilgarriff.co.uk/BNC_lists/
Besides being important for computational im-
plementations, resolving you is also an interesting
and challenging research problem. As for third
person pronouns such as it, some uses of you are
not strictly referential. These include discourse
marker uses such as you know in example (1), and
generic uses like (2), where you does not refer to
the addressee as it does in (3).
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button se-
quences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
However, unlike it, you is ambiguous between sin-
gular and plural interpretations - an issue that is
particularly problematic in multi-party conversa-
tions. While you clearly has a plural referent in
(4), in (3) the number of its referent is ambigu-
ous.2
(4) I don?t know if you guys have any questions.
When an utterance contains a singular referen-
tial you, resolving the you amounts to identifying
the individual to whom the utterance is addressed.
This is trivial in two-person dialogue since the cur-
rent listener is always the addressee, but in conver-
sations with multiple participants, it is a complex
problem where different kinds of linguistic and vi-
sual information play important roles (Jovanovic,
2007). One of the issues we investigate here is
2In contrast, the referential use of the pronoun it (as well
as that of some demonstratives) is ambiguous between NP
interpretations and discourse-deictic ones (Webber, 1991).
273
how this applies to the more concrete problem of
resolving the second person pronoun you.
We approach this issue as a three-step prob-
lem. Using the AMI Meeting Corpus (McCowan
et al, 2005) of multi-party dialogues, we first dis-
criminate between referential and generic uses of
you. Then, within the referential uses, we dis-
tinguish between singular and plural, and finally,
we resolve the singular referential instances by
identifying the intended addressee. We use multi-
modal features: initially, we extract discourse fea-
tures from manual transcriptions and use visual in-
formation derived from manual annotations, but
then we move to a fully automatic approach, us-
ing 1-best transcriptions produced by an automatic
speech recognizer (ASR) and visual features auto-
matically extracted from raw video.
In the next section of this paper, we give a brief
overview of related work. We describe our data in
Section 3, and explain how we extract visual and
linguistic features in Sections 4 and 5 respectively.
Section 6 then presents our experiments with man-
ual transcriptions and annotations, while Section
7, those with automatically extracted information.
We end with conclusions in Section 8.
2 Related Work
2.1 Reference Resolution in Dialogue
Although the vast majority of work on reference
resolution has been with monologic text, some re-
cent research has dealt with the more complex
scenario of spoken dialogue (Strube and Mu?ller,
2003; Byron, 2004; Arstein and Poesio, 2006;
Mu?ller, 2007). There has been work on the iden-
tification of non-referential uses of the pronoun it:
Mu?ller (2006) uses a set of shallow features au-
tomatically extracted from manual transcripts of
two-party dialogue in order to train a rule-based
classifier, and achieves an F-score of 69%.
The only existing work on the resolution of you
that we are aware of is Gupta et al (2007b; 2007a).
In line with our approach, the authors first disam-
biguate between generic and referential you, and
then attempt to resolve the reference of the ref-
erential cases. Generic uses of you account for
47% of their data set, and for the generic vs. ref-
erential disambiguation, they achieve an accuracy
of 84% on two-party conversations and 75% on
multi-party dialogue. For the reference resolution
task, they achieve 47%, which is 10 points over
a baseline that always classifies the next speaker
as the addressee. These results are achieved with-
out visual information, using manual transcripts,
and a combination of surface features and manu-
ally tagged dialogue acts.
2.2 Addressee Detection
Resolving the referential instances of you amounts
to determining the addressee(s) of the utterance
containing the pronoun. Recent years have seen
an increasing amount of research on automatic
addressee detection. Much of this work focuses
on communication between humans and computa-
tional agents (such as robots or ubiquitous com-
puting systems) that interact with users who may
be engaged in other activities, including interac-
tion with other humans. In these situations, it
is important for a system to be able to recognize
when it is being addressed by a user. Bakx et
al. (2003) and Turnhout et al (2005) studied this
issue in the context of mixed human-human and
human-computer interaction using facial orienta-
tion and utterance length as clues for addressee
detection, while Katzenmaier et al (2004) inves-
tigated whether the degree to which a user utter-
ance fits the language model of a conversational
robot can be useful in detecting system-addressed
utterances. This research exploits the fact that hu-
mans tend to speak differently to systems than to
other humans.
Our research is closer to that of Jovanovic
et al (2006a; 2007), who studied addressing in
human-human multi-party dialogue. Jovanovic
and colleagues focus on addressee identification in
face-to-face meetings with four participants. They
use a Bayesian Network classifier trained on sev-
eral multimodal features (including visual features
such as gaze direction, discourse features such as
the speaker and dialogue act of preceding utter-
ances, and utterance features such as lexical clues
and utterance duration). Using a combination of
features from various resources was found to im-
prove performance (the best system achieves an
accuracy of 77% on a portion of the AMI Meeting
Corpus). Although this result is very encouraging,
it is achieved with the use of manually produced
information - in particular, manual transcriptions,
dialogue acts and annotations of visual focus of at-
tention. One of the issues we aim to investigate
here is how automatically extracted multimodal
information can help in detecting the addressee(s)
of you-utterances.
274
Generic Referential Ref Sing. Ref Pl.
49.14% 50.86% 67.92% 32.08%
Table 1: Distribution of you interpretations
3 Data
Our experiments are performed using the AMI
Meeting Corpus (McCowan et al, 2005), a collec-
tion of scenario-driven meetings among four par-
ticipants, manually transcribed and annotated with
several different types of information (including
dialogue acts, topics, visual focus of attention, and
addressee). We use a sub-corpus of 948 utterances
containing you, and these were extracted from 10
different meetings. The you-utterances are anno-
tated as either discourse marker, generic or refer-
ential.
We excluded the discourse marker cases, which
account for only 8% of the data, and of the refer-
ential cases, selected those with an AMI addressee
annotation.3 The addressee of a dialogue act can
be unknown, a single meeting participant, two
participants, or the whole audience (three partici-
pants in the AMI corpus). Since there are very few
instances of two-participant addressee, we distin-
guish only between singular and plural addressees.
The resulting distribution of classes is shown in
Table 1.4
We approach the reference resolution task as a
two-step process, first discriminating between plu-
ral and singular references, and then resolving the
reference of the singular cases. The latter task re-
quires a classification scheme for distinguishing
between the three potential addressees (listeners)
for the given you-utterance.
In their four-way classification scheme,
Gupta et al (2007a) label potential addressees in
terms of the order in which they speak after the
you-utterance. That is, for a given you-utterance,
the potential addressee who speaks next is labeled
1, the potential addressee who speaks after that is
2, and the remaining participant is 3. Label 4 is
used for group addressing. However, this results
in a very skewed class distribution because the
next speaker is the intended addressee 41% of
the time, and 38% of instances are plural - the
3Addressee annotations are not provided for some dia-
logue act types - see (Jovanovic et al, 2006b).
4Note that the percentages of the referential singular and
referential plural are relative to the total of referential in-
stances.
L1 L2 L3
35.17% 30.34% 34.49%
Table 2: Distribution of addressees for singular you
remaining two classes therefore make up a small
percentage of the data.
We were able to obtain a much less skewed class
distribution by identifying the potential addressees
in terms of their position in relation to the current
speaker. The meeting setting includes a rectangu-
lar table with two participants seated at each of
its opposite longer sides. Thus, for a given you-
utterance, we label listeners as either L1, L2 or
L3 depending on whether they are sitting opposite,
diagonally or laterally from the speaker. Table 2
shows the resulting class distribution for our data-
set. Such a labelling scheme is more similar to Jo-
vanovic (2007), where participants are identified
by their seating position.
4 Visual Information
4.1 Features from Manual Annotations
We derived per-utterance visual features from the
Focus Of Attention (FOA) annotations provided
by the AMI corpus. These annotations track meet-
ing participants? head orientation and eye gaze
during a meeting.5 Our first step was to use the
FOA annotations in order to compute what we re-
fer to as Gaze Duration Proportion (GDP) values
for each of the utterances of interest - a measure
similar to the ?Degree of Mean Duration of Gaze?
described by (Takemae et al, 2004). Here a GDP
value denotes the proportion of time in utterance u
for which subject i is looking at target j:
GDPu(i, j) =
?
j
T (i, j)/Tu
were Tu is the length of utterance u in millisec-
onds, and T (i, j), the amount of that time that i
spends looking at j. The gazer i can only refer to
one of the four meeting participants, but the tar-
get j can also refer to the white-board/projector
screen present in the meeting room. For each utter-
ance then, all of the possible values of i and j are
used to construct a matrix of GDP values. From
this matrix, we then construct ?Highest GDP? fea-
tures for each of the meeting participants: such
5A description of the FOA labeling scheme is avail-
able from the AMI Meeting Corpus website http://corpus.
amiproject.org/documentations/guidelines-1/
275
For each participant Pi
? target for whole utterance
? target for first third of utterance
? target for second third of utterance
? target for third third of utterance
? target for -/+ 2 secs from you start time
? ratio 2nd hyp. target / 1st hyp. target
? ratio 3rd hyp. target / 1st hyp. target
? participant in mutual gaze with speaker
Table 3: Visual Features
features record the target with the highest GDP
value and so indicate whom/what the meeting par-
ticipant spent most time looking at during the ut-
terance.
We also generated a number of additional fea-
tures for each individual. These include firstly,
three features which record the candidate ?gazee?
with the highest GDP during each third of the ut-
terance, and which therefore account for gaze tran-
sitions. So as to focus more closely on where par-
ticipants are looking around the time when you
is uttered, another feature records the candidate
with the highest GDP -/+ 2 seconds from the start
time of the you. Two further features give some
indication of the amount of looking around that
the speaker does during an utterance - we hypoth-
esized that participants (especially the speaker)
might look around more in utterances with plu-
ral addressees. The first is the ratio of the sec-
ond highest GDP to the highest, and the second
is the ratio of the third highest to the highest. Fi-
nally, there is a highest GDP mutual gaze feature
for the speaker, indicating with which other indi-
vidual, the speaker spent most time engaged in a
mutual gaze.
Hence this gives a total of 29 features: seven
features for each of the four participants, plus one
mutual gaze feature. They are summarized in Ta-
ble 3. These visual features are different to those
used by Jovanovic (2007) (see Section 2). Jo-
vanovic?s features record the number of times that
each participant looks at each other participant
during the utterance, and in addition, the gaze di-
rection of the current speaker. Hence, they are not
highest GDP values, they do not include a mutual
gaze feature and they do not record whether par-
ticipants look at the white-board/projector screen.
4.2 Automatic Features from Raw Video
To perform automatic visual feature extraction, a
six degree-of-freedom head tracker was run over
each subject?s video sequence for the utterances
containing you. For each utterance, this gave 4 se-
quences, one per subject, of the subject?s 3D head
orientation and location at each video frame along
with 3D head rotational velocities. From these
measurements we computed two types of visual
information: participant gaze and mutual gaze.
The 3D head orientation and location of each
subject along with camera calibration information
was used to compute participant gaze information
for each video frame of each sequence in the form
of a gaze probability matrix. More precisely, cam-
era calibration is first used to estimate the 3D head
orientation and location of all subjects in the same
world coordinate system.
The gaze probability matrix is a 4 ? 5 matrix
where entry i, j stores the probability that subject
i is looking at subject j for each of the four sub-
jects and the last column corresponds to the white-
board/projector screen (i.e., entry i, j where j = 5
is the probability that subject i is looking at the
screen). Gaze probability G(i, j) is defined as
G(i, j) = G0e
??i,j2/?2
where ?i,j is the angular difference between the
gaze of subject i and the direction defined by the
location of subjects i and j. G0 is a normalization
factor such that
?
j G(i, j) = 1 and ? is a user-
defined constant (in our experiments, we chose
? = 15 degrees).
Using the gaze probability matrix, a 4 ? 1 per-
frame mutual gaze vector was computed that for
entry i stores the probability that the speaker and
subject i are looking at one another.
In order to create features equivalent to those
described in Section 4.1, we first collapse the
frame-level probability matrix into a matrix of bi-
nary values. We convert the probability for each
frame into a binary judgement of whether subject
i is looking at target j:
H(i, j) = ?G(i, j)
? is a binary value to evaluate G(i, j) > ?, where
? is a high-pass thresholding value - or ?gaze prob-
ability threshold? (GPT) - between 0 and 1.
Once we have a frame-level matrix of binary
values, for each subject i, we compute GDP val-
ues for the time periods of interest, and in each
case, choose the target with the highest GDP as the
candidate. Hence, we compute a candidate target
for the utterance overall, for each third of the ut-
terance, and for the period -/+ 2 seconds from the
276
you start time, and in addition, we compute a can-
didate participant for mutual gaze with the speaker
for the utterance overall.
We sought to use the GPT threshold which pro-
duces automatic visual features that agree best
with the features derived from the FOA annota-
tions. Hence we experimented with different GPT
values in increments of 0.1, and compared the re-
sulting features to the manual features using the
kappa statistic. A threshold of 0.6 gave the best
kappa scores, which ranged from 20% to 44%.6
5 Linguistic Information
Our set of discourse features is a simplified ver-
sion of those employed by Galley et al (2004) and
Gupta et al (2007a). It contains three main types
(summarized in Table 4):
? Sentential features (1 to 13) encode structural,
durational, lexical and shallow syntactic patterns
of the you-utterance. Feature 13 is extracted us-
ing the AMI ?Named Entity? annotations and in-
dicates whether a particular participant is men-
tioned in the you-utterance. Apart from this fea-
ture, all other sentential features are automatically
extracted, and besides 1, 8, 9, and 10, they are all
binary.
? Backward Looking (BL)/Forward Looking (FL)
features (14 to 22) are mostly extracted from ut-
terance pairs, namely the you-utterance and the
BL/FL (previous/next) utterance by each listener
Li (potential addressee). We also include a few
extra features which are not computed in terms of
utterance pairs. These indicate the number of par-
ticipants that speak during the previous and next 5
utterances, and the BL and FL speaker order. All
of these features are computed automatically.
? Dialogue Act (DA) features (23 to 24) use the
manual AMI dialogue act annotations to represent
the conversational function of the you-utterance
and the BL/FL utterance by each potential ad-
dressee. Along with the sentential feature based
on the AMI Named Entity annotations, these are
the only discourse features which are not com-
puted automatically. 7
6The fact that our gaze estimator is getting any useful
agreement with respect to these annotations is encouraging
and suggests that an improved tracker and/or one that adapts
to the user more effectively could work very well.
7Since we use the manual transcripts of the meetings, the
transcribed words and the segmentation into utterances or di-
alogue acts are of course not given automatically. A fully
automatic approach would involve using ASR output instead
of manual transcriptions? something which we attempt in
(1) # of you pronouns
(2) you (say|said|tell|told| mention(ed)|mean(t)|
sound(ed))
(3) auxiliary you
(4) wh-word you
(5) you guys
(6) if you
(7) you know
(8) # of words in you-utterance
(9) duration of you-utterance
(10) speech rate of you-utterance
(11) 1st person
(12) general case
(13) person Named Entity tag
(14) # of utterances between you- and BL/FL utt.
(15) # of speakers between you- and BL/FL utt.
(16) overlap between you- and BL/FL utt. (binary)
(17) duration of overlap between you- and BL/FL utt.
(18) time separation between you- and BL/FL utt.
(19) ratio of words in you- that are in BL/FL utt.
(20) # of participants that speak during prev. 5 utt.
(21) # of participants that speak during next 5 utt.
(22) speaker order BL/FL
(23) dialogue act of the you-utterance
(24) dialogue act of the BL/FL utterance
Table 4: Discourse Features
6 First Set of Experiments & Results
In this section we report our experiments and re-
sults when using manual transcriptions and anno-
tations. In Section 7 we will present the results
obtained using ASR output and automatically ex-
tracted visual information. All experiments (here
and in the next section) are performed using a
Bayesian Network classifier with 10-fold cross-
validation.8 In each task, we give raw overall ac-
curacy results and then F-scores for each of the
classes. We computed measures of information
gain in order to assess the predictive power of the
various features, and did some experimentation
with Correlation-based Feature Selection (CFS)
(Hall, 2000).
6.1 Generic vs. Referential Uses of You
We first address the task of distinguishing between
generic and referential uses of you.
Baseline. A majority class baseline that classi-
fies all instances of you as referential yields an ac-
curacy of 50.86% (see Table 1).
Results. A summary of the results is given in Ta-
ble 5. Using discourse features only we achieve
an accuracy of 77.77%, while using multimodal
Section 7.
8We use the the BayesNet classifier implemented in the
Weka toolkit http://www.cs.waikato.ac.nz/ml/weka/.
277
Features Acc F1-Gen F1-Ref
Baseline 50.86 0 67.4
Discourse 77.77 78.8 76.6
Visual 60.32 64.2 55.5
MM 79.02 80.2 77.7
Dis w/o FL 78.34 79.1 77.5
MM w/o FL 78.22 79.0 77.4
Dis w/o DA 69.44 71.5 67.0
MM w/o DA 72.75 74.4 70.9
Table 5: Generic vs. referential uses
(MM) yields 79.02%, but this increase is not sta-
tistically significant.
In spite of this, visual features do help to dis-
tinguish between generic and referential uses -
note that the visual features alone are able to beat
the baseline (p < .005). The listeners? gaze is
more predictive than the speaker?s: if listeners
look mostly at the white-board/projector screen in-
stead of another participant, then the you is more
likely to be referential. More will be said on this
in Section 6.2.1 in the analysis of the results for
the singular vs. plural referential task.
We found sentential features of the you-
utterance to be amongst the best predictors, es-
pecially those that refer to surface lexical proper-
ties, such as features 1, 11, 12 and 13 in Table 4.
Dialogue act features provide useful information
as well. As pointed out by Gupta et al (2007b;
2007a), a you pronoun within a question (e.g.
an utterance tagged as elicit-assess or
elicit-inform) is more likely to be referen-
tial. Eliminating information about dialogue acts
(w/o DA) brings down performance (p < .005),
although accuracy remains well above the baseline
(p < .001). Note that the small changes in perfor-
mance when FL information is taken out (w/o FL)
are not statistically significant.
6.2 Reference Resolution
We now turn to the referential instances of you,
which can be resolved by determining the ad-
dressee(s) of the given utterance.
6.2.1 Singular vs. Plural Reference
We start by trying to discriminate singular vs. plu-
ral interpretations. For this, we use a two-way
classification scheme that distinguishes between
individual and group addressing. To our knowl-
edge, this is the first attempt at this task using lin-
guistic information.9
9But see e.g. (Takemae et al, 2004) for an approach that
uses manually extracted visual-only clues with similar aims.
Baseline. A majority class baseline that consid-
ers all instances of you as referring to an individual
addressee gives 67.92% accuracy (see Table 1).
Results. A summary of the results is shown in
Table 6. There is no statistically significant differ-
ence between the baseline and the results obtained
when visual features are used alone (67.92% vs.
66.28%). However, we found that visual informa-
tion did contribute to identifying some instances of
plural addressing, as shown by the F-score for that
class. Furthermore, the visual features helped to
improve results when combined with discourse in-
formation: using multimodal (MM) features pro-
duces higher results than the discourse-only fea-
ture set (p < .005), and increases from 74.24% to
77.05% with CFS.
As in the generic vs. referential task, the white-
board/projector screen value for the listeners? gaze
features seems to have discriminative power -
when listeners? gaze features take this value, it is
often indicative of a plural rather than a singular
you. It seems then, that in our data-set, the speaker
often uses the white-board/projector screen when
addressing the group, and hence draws the listen-
ers? gaze in this direction. We should also note
that the ratio features which we thought might be
useful here (see Section 4.1) did not prove so.
Amongst the most useful discourse features
are those that encode similarity relations between
the you-utterance and an utterance by a potential
addressee. Utterances by individual addressees
tend to be more lexically cohesive with the you-
utterance and so if features such as feature 19 in
Table 4 indicate a low level of lexical similarity,
then this increases the likelihood of plural address-
ing. Sentential features that refer to surface lexical
patterns (features 6, 7, 11 and 12) also contribute
to improved results, as does feature 21 (number of
speakers during the next five utterances) - fewer
speaker changes correlates with plural addressing.
Information about dialogue acts also plays a
role in distinguishing between singular and plu-
ral interpretations. Questions tend to be addressed
to individual participants, while statements show a
stronger correlation with plural addressees. When
no DA features are used (w/o DA), the drop in per-
formance for the multimodal classifier to 71.19%
is statistically significant (p < .05). As for the
generic vs. referential task, FL information does
not have a significant effect on performance.
278
Features Acc F1-Sing. F1-Pl.
Baseline 67.92 80.9 0
Discourse 71.19 78.9 54.6
Visual 66.28 74.8 48.9
MM* 77.05 83.3 63.2
Dis w/o FL 72.13 80.1 53.7
MM w/o FL 72.60 79.7 58.1
Dis w/o DA 68.38 78.5 40.5
MM w/o DA 71.19 78.8 55.3
Table 6: Singular vs. plural reference; * = with Correlation-
based Feature Selection (CFS).
6.2.2 Detection of Individual Addressees
We now turn to resolving the singular referential
uses of you. Here we must detect the individual
addressee of the utterance that contains the pro-
noun.
Baselines. Given the distribution shown in Ta-
ble 2, a majority class baseline yields an accu-
racy of 35.17%. An off-line system that has access
to future context could implement a next-speaker
baseline that always considers the next speaker to
be the intended addressee, so yielding a high raw
accuracy of 71.03%. A previous-speaker base-
line that does not require access to future context
achieves 35% raw accuracy.
Results. Table 7 shows a summary of the re-
sults, and these all outperform the majority class
(MC) and previous-speaker baselines. When all
discourse features are available, adding visual in-
formation does improve performance (74.48% vs.
60.69%, p < .005), and with CFS, this increases
further to 80.34% (p < .005). Using discourse or
visual features alone gives scores that are below
the next-speaker baseline (60.69% and 65.52% vs.
71.03%). Taking all forward-looking (FL) infor-
mation away reduces performance (p < .05), but
the small increase in accuracy caused by taking
away dialogue act information is not statistically
significant.
When we investigated individual feature contri-
bution, we found that the most predictive features
were the FL and backward-looking (BL) speaker
order, and the speaker?s visual features (including
mutual gaze). Whomever the speaker spent most
time looking at or engaged in a mutual gaze with
was more likely to be the addressee. All of the vi-
sual features had some degree of predictive power
apart from the ratio features. Of the other BL/FL
discourse features, features 14, 18 and 19 (see Ta-
ble 4) were more predictive. These indicate that
utterances spoken by the intended addressee are
Features Acc F1-L1 F1-L2 F1-L3
MC baseline 35.17 52.0 0 0
Discourse 60.69 59.1 60.0 62.7
Visual 65.52 69.1 63.5 64.0
MM* 80.34 80.0 82.4 79.0
Dis w/o FL 52.41 50.7 51.8 54.5
MM w/o FL 66.55 68.7 62.7 67.6
Dis w/o DA 61.03 58.5 59.9 64.2
MM w/o DA 73.10 72.4 69.5 72.0
Table 7: Addressee detection for singular references; * =
with Correlation-based Feature Selection (CFS).
often adjacent to the you-utterance and lexically
similar.
7 A Fully Automatic Approach
In this section we describe experiments which
use features derived from ASR transcriptions and
automatically-extracted visual information. We
used SRI?s Decipher (Stolcke et al, 2008)10 in or-
der to generate ASR transcriptions, and applied
the head-tracker described in Section 4.2 to the
relevant portions of video in order to extract the
visual information. Recall that the Named Entity
features (feature 13) and the DA features used in
our previous experiments had been manually an-
notated, and hence are not used here. We again
divide the problem into the same three separate
tasks: we first discriminate between generic and
referential uses of you, then singular vs. plural
referential uses, and finally we resolve the ad-
dressee for singular uses. As before, all exper-
iments are performed using a Bayesian Network
classifier and 10-fold cross validation.
7.1 Results
For each of the three tasks, Figure 7 compares
the accuracy results obtained using the fully-
automatic approach with those reported in Section
6. The figure shows results for the majority class
baselines (MCBs), and with discourse-only (Dis),
and multimodal (MM) feature sets. Note that the
data set for the automatic approach is smaller,
and that the majority class baselines have changed
slightly. This is because of differences in the ut-
terance segmentation, and also because not all of
the video sections around the you utterances were
processed by the head-tracker.
In all three tasks we are able to significantly
outperform the majority class baseline, but the vi-
sual features only produce a significant improve-
10Stolcke et al (2008) report a word error rate of 26.9% on
AMI meetings.
279
Figure 1: Results for the manual and automatic systems; MCB = majority class baseline, Dis = discourse features, MM =
multimodal, * = with Correlation-based Feature Selection (CFS), FL = forward-looking, man = manual, auto = automatic.
ment in the individual addressee resolution task.
For the generic vs. referential task, the discourse
and multimodal classifiers both outperform the
majority class baseline (p < .001), achieving
accuracy scores of 68.71% and 68.48% respec-
tively. In contrast to when using manual transcrip-
tions and annotations (see Section 6.1), removing
forward-looking (FL) information reduces perfor-
mance (p < .05). For the referential singular
vs. plural task, the discourse and multimodal with
CFS classifier improve over the majority class
baseline (p < .05). Multimodal with CFS does
not improve over the discourse classifier - indeed
without feature selection, the addition of visual
features causes a drop in performance (p < .05).
Here, taking away FL information does not cause
a significant reduction in performance. Finally,
in the individual addressee resolution task, the
discourse, visual (60.78%) and multimodal clas-
sifiers all outperform the majority class baseline
(p < .005, p < .001 and p < .001 respec-
tively). Here the addition of visual features causes
the multimodal classifier to outperform the dis-
course classifier in raw accuracy by nearly ten per-
centage points (67.32% vs. 58.17%, p < .05), and
with CFS, the score increases further to 74.51%
(p < .05). Taking away FL information does
cause a significant drop in performance (p < .05).
8 Conclusions
We have investigated the automatic resolution of
the second person English pronoun you in multi-
party dialogue, using a combination of linguistic
and visual features. We conducted a first set of
experiments where our features were derived from
manual transcriptions and annotations, and then a
second set where they were generated by entirely
automatic means. To our knowledge, this is the
first attempt at tackling this problem using auto-
matically extracted multimodal information.
Our experiments showed that visual informa-
tion can be highly predictive in resolving the ad-
dressee of singular referential uses of you. Visual
features significantly improved the performance of
both our manual and automatic systems, and the
latter achieved an encouraging 75% accuracy. We
also found that our visual features had predictive
power for distinguishing between generic and ref-
erential uses of you, and between referential sin-
gulars and plurals. Indeed, for the latter task,
they significantly improved the manual system?s
performance. The listeners? gaze features were
useful here: in our data set it was apparently the
case that the speaker would often use the white-
board/projector screen when addressing the group,
thus drawing the listeners? gaze in this direction.
Future work will involve expanding our data-
set, and investigating new potentially predictive
features. In the slightly longer term, we plan to
integrate the resulting system into a meeting as-
sistant whose purpose is to automatically extract
useful information from multi-party meetings.
280
References
Ron Arstein and Massimo Poesio. 2006. Identifying
reference to abstract objects in dialogue. In Pro-
ceedings of the 10th Workshop on the Semantics and
Pragmatics of Dialogue (Brandial?06), pages 56?
63, Potsdam, Germany.
Ilse Bakx, Koen van Turnhout, and Jacques Terken.
2003. Facial orientation during multi-party inter-
action with information kiosks. In Proceedings of
INTERACT, Zurich, Switzerland.
Donna Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, University
of Rochester, Department of Computer Science.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving ?you? in multi-
party dialog. In Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue, Antwerp,
Belgium, September.
Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential ?you? in dialog. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Mark Hall. 2000. Correlation-based Feature Selection
for Machine Learning. Ph.D. thesis, University of
Waikato.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006a. Addressee identification in face-to-
face meetings. In Proceedings of the 11th Confer-
ence of the European Chapter of the ACL (EACL),
pages 169?176, Trento, Italy.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006b. A corpus for studying addressing
behaviour in multi-party dialogues. Language Re-
sources and Evaluation, 40(1):5?23. ISSN=1574-
020X.
Natasa Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, Enschede, The
Netherlands.
Michael Katzenmaier, Rainer Stiefelhagen, and Tanja
Schultz. 2004. Identifying the addressee in human-
human-robot interactions based on head pose and
speech. In Proceedings of the 6th International
Conference on Multimodal Interfaces, pages 144?
151, State College, Pennsylvania.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Christoph Mu?ller. 2006. Automatic detection of non-
referential It in spoken multi-party dialog. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 49?56, Trento, Italy.
Christoph Mu?ller. 2007. Resolving it, this, and that
in unrestricted multi-party dialog. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 816?823, Prague,
Czech Republic.
Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?r
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The icsi-sri spring
2007 meeting and lecture recognition system. In
Proceedings of CLEAR 2007 and RT2007. Springer
Lecture Notes on Computer Science.
Michael Strube and Christoph Mu?ller. 2003. A ma-
chine learning approach to pronoun resolution in
spoken dialogue. In Proceedings of ACL?03, pages
168?175.
Yoshinao Takemae, Kazuhiro Otsuka, and Naoki
Mukawa. 2004. An analysis of speakers? gaze
behaviour for automatic addressee identification in
multiparty conversation and its application to video
editing. In Proceedings of IEEE Workshop on Robot
and Human Interactive Communication, pages 581?
586.
Koen van Turnhout, Jacques Terken, Ilse Bakx, and
Berry Eggen. 2005. Identifying the intended
addressee in mixed human-humand and human-
computer interaction from non-verbal features. In
Proceedings of ICMI, Trento, Italy.
Bonnie Webber. 1991. Structure and ostension in
the interpretation of discourse deixi. Language and
Cognitive Processes, 6(2):107?135.
281
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156?163,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Modelling and Detecting Decisions in Multi-party Dialogue
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen, Matthew Purver, and Stanley Peters
Center for the Study of Language and Information
Stanford University
{raquel|frampton|ehlen|mpurver|peters}@stanford.edu
Abstract
We describe a process for automatically de-
tecting decision-making sub-dialogues in tran-
scripts of multi-party, human-human meet-
ings. Extending our previous work on ac-
tion item identification, we propose a struc-
tured approach that takes into account the dif-
ferent roles utterances play in the decision-
making process. We show that this structured
approach outperforms the accuracy achieved
by existing decision detection systems based
on flat annotations, while enabling the extrac-
tion of more fine-grained information that can
be used for summarization and reporting.
1 Introduction
In collaborative and organized work environments,
people share information and make decisions exten-
sively through multi-party conversations, usually in
the form of meetings. When audio or video record-
ings are made of these meetings, it would be valu-
able to extract important information, such as the
decisions that were made and the trains of reason-
ing that led to those decisions. Such a capability
would allow work groups to keep track of courses
of action that were shelved or rejected, and could al-
low new team members to get quickly up to speed.
Thanks to the recent availability of substantial meet-
ing corpora?such as the ISL (Burger et al, 2002),
ICSI (Janin et al, 2004), and AMI (McCowan et
al., 2005) Meeting Corpora?current research on the
structure of decision-making dialogue and its use for
automatic decision detection has helped to bring this
vision closer to reality (Verbree et al, 2006; Hsueh
and Moore, 2007b).
Our aim here is to further that research by ap-
plying a simple notion of dialogue structure to the
task of automatically detecting decisions in multi-
party dialogue. A central hypothesis underlying our
approach is that this task is best addressed by tak-
ing into account the roles that different utterances
play in the decision-making process. Our claim is
that this approach facilitates both the detection of
regions of discourse where decisions are discussed
and adopted, and also the identification of important
aspects of the decision discussions themselves, thus
opening the way to better and more concise report-
ing.
In the next section, we describe prior work on re-
lated efforts, including our own work on action item
detection (Purver et al, 2007). Sections 3 and 4 then
present our decision annotation scheme, which dis-
tinguishes several types of decision-related dialogue
acts (DAs), and the corpus used as data (in this study
a section of the AMI Meeting Corpus). Next, in Sec-
tion 5, we describe our experimental methodology,
including the basic conception of our classification
approach, the features we used in classification, and
our evaluation metrics. Section 6 then presents our
results, obtained with a hierarchical classifier that
first trains individual sub-classifiers to detect the dif-
ferent types of decision DAs, and then uses a super-
classifier to detect decision regions on the basis of
patterns of these DAs, achieving an F-score of 58%.
Finally, Section 7 presents some conclusions and di-
rections for future work.
2 Related Work
Recent years have seen an increasing interest in re-
search on decision-making dialogue. To a great
extent, this is due to the fact that decisions have
156
been shown to be a key aspect of meeting speech.
User studies (Lisowska et al, 2004; Banerjee et al,
2005) have shown that participants regard decisions
as one of the most important outputs of a meeting,
while Whittaker et al (2006) found that the develop-
ment of an automatic decision detection component
is critical to the re-use of meeting archives. Identify-
ing decision-making regions in meeting transcripts
can thus be expected to support development of a
wide range of applications, such as automatic meet-
ing assistants that process, understand, summarize
and report the output of meetings; meeting tracking
systems that assist in implementing decisions; and
group decision support systems that, for instance,
help in constructing group memory (Romano and
Nunamaker, 2001; Post et al, 2004; Voss et al,
2007).
Previously researchers have focused on the in-
teractive aspects of argumentative and decision-
making dialogue, tackling issues such as the detec-
tion of agreement and disagreement and the level
of emotional involvement of conversational partic-
ipants (Hillard et al, 2003; Wrede and Shriberg,
2003; Galley et al, 2004; Gatica-Perez et al, 2005).
From a perhaps more formal perspective, Verbree et
al. (2006) have created an argumentation scheme in-
tended to support automatic production of argument
structure diagrams from decision-oriented meeting
transcripts. Only Hsueh and Moore (2007a; 2007b),
however, have specifically investigated the auto-
matic detection of decisions.
Using the AMI Meeting Corpus, Hsueh and
Moore (2007b) attempt to identify the dialogue acts
(DAs) in a meeting transcript that are ?decision-
related?. The authors define these DAs on the ba-
sis of two kinds of manually created summaries: an
extractive summary of the whole meeting, and an
abstractive summary of the decisions made in the
meeting. Those DAs in the extractive summary that
support any of the decisions in the abstractive sum-
mary are then manually tagged as decision-related
DAs. They trained a Maximum Entropy classifier
to recognize this single DA class, using a variety of
lexical, prosodic, dialogue act and topical features.
The F-score they achieved was 0.35, which gives a
good indication of the difficulty of this task.
In our previous work (Purver et al, 2007), we at-
tempted to detect a particular kind of decision com-
mon in meetings, namely action items?public com-
mitments to perform a given task. In contrast to
the approach adopted by Hsueh and Moore (2007b),
we proposed a hierarchical approach where indi-
vidual classifiers were trained to detect distinct ac-
tion item-related DA classes (task description, time-
frame, ownership and agreement) followed by a
super-classifier trained on the hypothesized class la-
bels and confidence scores from the individual clas-
sifiers that would detect clusters of multiple classes.
We showed that this structured approach produced
better classification accuracy (around 0.39 F-score
on the task of detecting action item regions) than a
flat-classifier baseline trained on a single action item
DA class (around 0.35 F-score).
In this paper we extend this approach to the more
general task of detecting decisions, hypothesizing
that?as with action items?the dialogue acts in-
volved in decision-making dialogue form a rather
heterogeneous set, whose members co-occur in par-
ticular kinds of patterns, and that exploiting this
richer structure can facilitate their detection.
3 Decision Dialogue Acts
We are interested in identifying the main conver-
sational units in a decision-making process. We ex-
pect that identifying these units will help in detect-
ing regions of dialogue where decisions are made
(decision sub-dialogues), while also contributing to
identification and extraction of specific decision-
related bits of information.
Decision-making dialogue can be complex, often
involving detailed discussions with complicated ar-
gumentative structure (Verbree et al, 2006). Deci-
sion sub-dialogues can thus include a great deal of
information that is potentially worth extracting. For
instance, we may be interested in knowing what a
decision is about, what alternative proposals were
considered during the decision process, what argu-
ments were given for and against each of them, and
last but not least, what the final resolution was.
Extracting these and other potential decision com-
ponents is a challenging task, which we do not in-
tend to fully address in this paper. This initial study
concentrates on three main components we believe
constitute the backbone of decision sub-dialogues.
A typical decision sub-dialogue consists of three
main components that often unfold in sequence. (a)
157
key DDA class description
I issue utterances introducing the issue or topic under discussion
R resolution utterances containing the decision that is adopted
RP ? proposal ? utterances where the decision adopted is proposed
RR ? restatement ? utterances where the decision adopted is confirmed or restated
A agreement utterances explicitly signalling agreement with the decision made
Table 1: Set of decision dialogue act (DDA) classes
A topic or issue that requires some sort of conclu-
sion is initially raised. (b) One or more proposals are
considered. And (c) once some sort of agreement is
reached upon a particular resolution, a decision is
adopted.
Dialogue act taxonomies often include tags
that can be decision-related. For instance, the
DAMSL taxonomy (Core and Allen, 1997) in-
cludes the tags agreement and commit, as well
as a tag open-option for utterances that ?sug-
gest a course of action?. Similarly, the AMI
DA scheme1 incorporates tags like suggest,
elicit-offer-or-suggestion and assess.
These tags are however very general and do not cap-
ture the distinction between decisions and more gen-
eral suggestions and commitments.2 We therefore
devised a decision annotation scheme that classifies
utterances according to the role they play in the pro-
cess of formulating and agreeing on a decision. Our
scheme distinguishes among three main decision di-
alogue act (DDA) classes: issue (I), resolution (R),
and agreement (A). Class R is further subdivided into
resolution proposal (RP) and resolution restatement
(RR). A summary of the classes is given in Table 1.
Annotation of the issue class includes any utter-
ances that introduce the topic of the decision discus-
sion. For instance, in example (1) below, the utter-
ances ?Are we going to have a backup?? and ?But
would a backup really be necessary?? are tagged as
I. The classes RP and RR are used to annotate those
utterances that specify the resolution adopted?i.e.
the decision made. Annotation with the class RP
includes any utterances where the resolution is ini-
1A full description of the AMI Meeting Corpus DA scheme
is available at http://mmm.idiap.ch/private/ami/
annotation/dialogue acts manual 1.0.pdf, after
free registration.
2Although they can of course be used to aid the identification
process?see Section 5.3.
tially proposed (like the utterance ?I think maybe we
could just go for the kinetic energy. . . ?). Sometimes
decision discussions include utterances that sum up
the resolution adopted, like the utterance ?Okay,
fully kinetic energy? in (1). This kind of utterance
is tagged with the class RR. Finally, the agreement
class includes any utterances in which participants
agree with the (proposed) resolution, like the utter-
ances ?Yeah? and ?Good? as well as ?Okay? in di-
alogue (1).
(1) A: Are we going to have a backup?
Or we do just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.3
Note that an utterance can be assigned to more
than one of these classes. For instance, the utter-
ance ?Okay, fully kinetic energy? is annotated both
as RR and A. Similarly, each decision sub-dialogue
may contain more than one utterance corresponding
to each class, as we saw above for issue. While
we do not a priori require each of these classes to
be present for a set of utterances to be considered
a decision sub-dialogue, all annotated decision sub-
dialogues in our corpus include the classes I, RP and
A. The annotation process and results are described
in detail in the next section.
3This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
158
4 Data: Corpus & Annotation
In this study, we use 17 meetings from the AMI
Meeting Corpus (McCowan et al, 2005), a pub-
licly available corpus of multi-party meetings con-
taining both audio recordings and manual transcrip-
tions, as well as a wide range of annotated infor-
mation including dialogue acts and topic segmenta-
tion. Conversations are all in English, but they can
include native and non-native English speakers. All
meetings in our sub-corpus are driven by an elicita-
tion scenario, wherein four participants play the role
of project manager, marketing expert, interface de-
signer, and industrial designer in a company?s de-
sign team. The overall sub-corpus makes up a total
of 15,680 utterances/dialogue acts (approximately
920 per meeting). Each meeting lasts around 30
minutes.
Two authors annotated 9 and 10 dialogues each,
overlapping on two dialogues. Inter-annotator
agreement on these two dialogues was similar to
(Purver et al, 2007), with kappa values ranging
from 0.63 to 0.73 for the four DDA classes. The
highest agreement was obtained for class RP and the
lowest for class A.4
On average, each meeting contains around 40
DAs tagged with one or more of the DDA sub-
classes in Table 1. DDAs are thus very sparse, cor-
responding to only 4.3% of utterances. When we
look at the individual DDA sub-classes this is even
more pronounced. Utterances tagged as issue make
up less than 0.9% of utterances in a meeting, while
utterances annotated as resolution make up around
1.4%?1% corresponding to RP and less than 0.4%
to RR on average. Almost half of DDA utterances
(slightly over 2% of all utterances on average) are
tagged as belonging to class agreement.
We compared our annotations with the annota-
tions of Hsueh and Moore (2007b) for the 17 meet-
ings of our sub-corpus. The overall number of ut-
terances annotated as decision-related is similar in
the two studies: 40 vs. 30 utterances per meeting on
average, respectively. However, the overlap of the
annotations is very small leading to negative kappa
scores. As shown in Figure 1, only 12.22% of ut-
4The annotation guidelines we used are available on-
line at http://godel.stanford.edu/twiki/bin/
view/Calo/CaloDecisionDiscussionSchema

  









  !"
##Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 306?309,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Cascaded Lexicalised Classifiers for Second-Person Reference Resolution
Matthew Purver
Department of Computer Science
Queen Mary University of London
London E1 4NS, UK
mpurver@dcs.qmul.ac.uk
Raquel Ferna?ndez
ILLC
University of Amsterdam
1098 XH Amsterdam, Netherlands
raquel.fernandez@uva.nl
Matthew Frampton and Stanley Peters
CSLI
Stanford University
Stanford, CA 94305, USA
frampton,peters@csli.stanford.edu
Abstract
This paper examines the resolution of the
second person English pronoun you in
multi-party dialogue. Following previous
work, we attempt to classify instances as
generic or referential, and in the latter case
identify the singular or plural addressee.
We show that accuracy and robustness can
be improved by use of simple lexical fea-
tures, capturing the intuition that different
uses and addressees are associated with
different vocabularies; and we show that
there is an advantage to treating referen-
tiality and addressee identification as sep-
arate (but connected) problems.
1 Introduction
Resolving second-person references in dialogue is
far from trivial. Firstly, there is the referentiality
problem: while we generally conceive of the word
you1 as a deictic addressee-referring pronoun, it
is often used in non-referential ways, including as
a discourse marker (1) and with a generic sense
(2). Secondly, there is the reference problem: in
addressee-referring cases, we need to know who
the addressee is. In two-person dialogue, this is
not so difficult; but in multi-party dialogue, the ad-
dressee could in principle be any one of the other
participants (3), or any group of more than one (4):
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button
sequences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
(4) I don?t know if you guys have any questions.
1We include your, yours, yourself, yourselves.
This paper extends previous work (Gupta et al,
2007; Frampton et al, 2009) in attempting to au-
tomatically treat both problems: detecting refer-
ential uses, and resolving their (addressee) refer-
ence. We find that accuracy can be improved by
the use of lexical features; we also give the first
results for treating both problems simultaneously,
and find that there is an advantage to treating them
as separate (but connected) problems via cascaded
classifiers, rather than as a single joint problem.
2 Related Work
Gupta et al (2007) examined the referentiality
problem, distinguishing generic from referential
uses in multi-party dialogue; they found that 47%
of uses were generic and achieved a classification
accuracy of 75%, using various discourse features
and discriminative classifiers (support vector ma-
chines and conditional random fields). They at-
tempted the reference-resolution problem, using
only discourse (non-visual) features, but accuracy
was low (47%).
Addressee identification in general (i.e. in-
dependent of the presence of you) has been ap-
proached in various ways. Traum (2004) gives
a rule-based algorithm based on discourse struc-
ture; van Turnhout et al (2005) used facial ori-
entation as well as utterance features; and more
recently Jovanovic (2006; 2007) combined dis-
course and gaze direction features using Bayesian
networks, achieving 77% accuracy on a portion of
the AMI Meeting Corpus (McCowan et al, 2005)
of 4-person dialogues.
In recent work, therefore, Frampton et al
(2009) extended Gupta et al?s method to in-
clude multi-modal features including gaze direc-
tion, again using Bayesian networks on the AMI
corpus. This gave a small improvement on the ref-
306
erentiality problem (achieving 79% accuracy), and
a large improvement on the reference-resolution
task (77% accuracy distinguishing singular uses
from plural, and 80% resolving singular individ-
ual addressee reference).
However, they treated the two tasks in isola-
tion, and also broke the addressee-reference prob-
lem into two separate sub-tasks (singular vs. plu-
ral reference, and singular addressee reference). A
full computational you-resolution module would
need to treat all tasks (either simultaneously as one
joint classification problem, or as a cascaded se-
quence) ? with inaccuracy at one task necessar-
ily affecting performance at another ? and we ex-
amine this here. In addition, we examine the ef-
fect of lexical features, following a similar insight
to Katzenmaier et al (2004); they used language
modelling to help distinguish between user- and
robot-directed utterances, as people use different
language for the two ? we expect that the same is
true for human participants.
3 Method
We used Frampton et al (2009)?s AMI corpus
data: 948 ?you?-containing utterances, manu-
ally annotated for referentiality and accompanied
by the AMI corpus? original addressee annota-
tion. The very small number of two-person ad-
dressee cases were joined with the three-person
(i.e. all non-speaker) cases to form a single ?plu-
ral? class. 49% of cases are generic; 32% of
referential cases are plural, and the rest are ap-
proximately evenly distributed between the singu-
lar participants. While Frampton et al (2009) la-
belled singular reference by physical location rel-
ative to the speaker (giving a 3-way classification
problem), our lexical features are more suited to
detecting actual participant identity ? we there-
fore recast the singular reference task as a 4-way
classification problem and re-calculate their per-
formance figures (giving very similar accuracies).
Discourse Features We use Frampton et al
(2009)?s discourse features. These include sim-
ple durational and lexical/phrasal features (includ-
ing mention of participant names); AMI dialogue
act features; and features expressing the simi-
larity between the current utterance and previ-
ous/following utterances by other participants. As
dialogue act features are notoriously hard to tag
automatically, and ?forward-looking? information
about following utterances may be unavailable in
an on-line system, we examine the effect of leav-
ing these out below.
Visual Features Again we used Frampton et al
(2009)?s features, extracted from the AMI corpus
manual focus-of-attention annotations which track
head orientiation and eye gaze. Features include
the target of gaze (any participant or the meet-
ing whiteboard/projector screen) during each ut-
terance, and information about mutual gaze be-
tween participants. These features may also not
always be available (meeting rooms may not al-
ways have cameras), so we investigate the effect
of their absence below.
Lexical Features The AMI Corpus simulates a
set of scenario-driven business meetings, with par-
ticipants performing a design task (the design of
a remote control). Participants are given specific
roles to play, for example that of project manager,
designer or marketing expert. It therefore seems
possible that utterances directed towards particular
individuals will involve the use of different vocab-
ularies reflecting their expertise. Different words
or phrases may also be associated with generic
and referential discussion, and extracting these au-
tomatically may give benefits over attempting to
capture them using manually-defined features. To
exploit this, we therefore added the use of lexical
features: one feature for each distinct word or n-
gram seen more than once in the corpus. Although
such features may be corpus- or domain-specific,
they are easy to extract given a transcript.
4 Results and Discussion
4.1 Individual Tasks
We first examine the effect of lexical features on
the individual tasks, using 10-way cross-validation
and comparing performance with Frampton et al
(2009). Table 1 shows the results for the referen-
tiality task in terms of overall accuracy and per-
class F1-scores; ?MC Baseline? is the majority-
class baseline; results labelled ?EACL? are Framp-
ton et al (2009)?s figures, and are presented for
all features and for reduced feature sets which
might be more realistic in various situations: ?-V?
removes visual features; ?-VFD? removes visual
features, forward-looking discourse features and
dialogue-act tag features.
As can be seen, adding lexical features
(?+words? adds single word features, ?+3grams?
adds n-gram features of lengths 1-3) improves the
307
Features Acc Fgen Fref
MC Baseline 50.9 0 67.4
EACL 79.0 80.2 77.7
EACL -VFD 73.7 74.1 73.2
+words 85.3 85.7 84.9
+3grams 87.5 87.4 87.5
+3grams -VFD 87.2 86.9 87.6
3grams only 85.9 85.2 86.4
Table 1: Generic vs. referential uses
Features Acc Fsing Fplur
MC Baseline 67.9 80.9 0
EACL 77.1 83.3 63.2
EACL -VFD 71.4 81.5 37.1
+words 83.1 87.8 72.5
+3grams 85.9 90.0 76.6
+3grams -VFD 87.1 91.0 77.6
3grams only 86.9 90.8 77.0
Table 2: Singular vs. plural reference.
performance significantly ? accuracy is improved
by 8.5% absolute above the best EACL results,
which is a 40% reduction in error. Robustness to
removal of potentially problematic features is also
improved: removing all visual, forward-looking
and dialogue act features makes little difference.
In fact, using only lexical n-gram features, while
reducing accuracy by 2.6%, still performs better
than the best EACL classifier.
Table 2 shows the equivalent results for the
singular-plural reference distinction task; in this
experiment, we used a correlation-based fea-
ture selection method, following Frampton et al
(2009). Again, performance is improved, this time
giving a 8.8% absolute accuracy improvement, or
38% error reduction; robustness to removing vi-
sual and dialogue act features is also very good,
even improving performance.
For the individual reference task (again using
feature selection), we give a further ?NS baseline?
of taking the next speaker; note that this performs
rather well, but requires forward-looking informa-
tion so should not be compared to ?-F? results.
Results are again improved (Table 3), but the im-
provement is smaller: a 1.4% absolute accuracy
improvement (7% error reduction); we conclude
from this that visual information is most impor-
tant for this part of the task. Robustness to feature
unavailability still shows some improvement: ex-
Features Acc FP1 FP2 FP3 FP4
MC baseline 30.7 0 0 0 47.0
NS baseline 70.7 71.6 71.1 72.7 68.2
EACL 80.3 82.8 79.7 75.9 81.4
EACL -V 73.8 79.2 70.7 74.1 71.4
EACL -VFD 56.6 58.9 55.5 64.0 47.3
+words 81.4 83.9 79.7 79.3 81.8
+3grams 81.7 83.9 80.3 79.3 82.5
+3grams -V 74.8 81.3 71.7 75.2 71.4
+3grams -VFD 60.7 66.3 55.9 66.2 53.0
3grams only 60.7 63.1 58.1 52.9 63.4
3grams +NS 74.5 76.7 73.8 75.0 72.7
Table 3: Singular addressee detection.
cluding all visual, forward-looking and dialogue-
act features has less effect than on the EACL sys-
tem (60.7% vs. 56.6% accuracy), and a system
using only n-grams and the next speaker identity
gives a respectable 74.5%.
Feature Analysis We examined the contribu-
tion of particular lexical features using Informa-
tion Gain methods. For the referentiality task, we
found that generic uses of you were more likely
to appear in utterances containing words related to
the main meeting topic, such as button, channel,
or volume (properties of the to-be-designed remote
control). In contrast, words related to meeting
management, such as presentation, email, project
and meeting itself, were predictive of referential
uses. The presence of first person pronouns and
discourse and politeness markers such as okay,
please and thank you was also indicative of refer-
entiality, as were n-grams capturing interrogative
structures (e.g. do you).
For the plural/singular distinction, we found
that the plural first person pronoun we correlated
with plural references of you. Other predictive n-
grams for this task were you mean and you know,
which were indicative of singular and plural refer-
ences, respectively. Finally, for the individual ref-
erence task, useful lexical features included par-
ticipant names, and items related to their roles.
For instance, the n-grams sales, to sell and make
money correlated with utterances addressed to the
?marketing expert?, while utterances containing
speech recognition and technical were addressed
to the ?industrial designer?.
Discussion The best F-score of the three sub-
tasks is for the generic/referential distinction; the
308
Features Acc Fgen Fplur FP1 FP2 FP3 FP4
MC baseline 49.1 65.9 0 0 0 0 0
EACL 58.3 73.3 24.3 57.6 57.0 36.0 51.1
+3grams 60.9 74.8 42.0 57.7 52.2 35.6 50.2
3grams only 67.5 84.8 61.6 39.1 39.3 30.6 38.6
Cascade +3grams 78.1 87.4 59.1 64.1 76.4 75.0 82.6
Table 4: Combined task: generic vs. plural vs. singular addressee.
worst is for the detection of plural reference (Fplur
in Table 2). This is not surprising: humans find the
former task easy to annotate ? Gupta et al (2007)
report good inter-annotator agreement (? = 0.84)
? but the latter hard. In their analysis of the AMI
addressee annotations, Reidsma et al (2008) ob-
serve that most confusions amongst annotators are
between the group-addressing label and the labels
for individuals; whereas if annotators agree that an
utterance is addressed to an individual, they also
reach high agreement on that addressee?s identity.
4.2 Combined Task
We next combined the individual tasks into one
combined task; for each you instance, a 6-way
classification as generic, group-referring or refer-
ring to one of the 4 participants. This was at-
tempted both as a single classification exercise us-
ing a single Bayesian network; and as a cascaded
pipeline of the three individual tasks; see Table 4.
Both used correlation-based feature selection.
For the single joint classifier, n-grams again im-
prove performance over the EACL features. Using
only n-grams gives a significant improvement, per-
haps due to the reduction in the size of the feature
space on this larger problem. Accuracy is reason-
able (67.5%), but while F-scores are good for the
generic class (above 80%), others are low.
However, use of three cascaded classifiers
improves performance to 78% and gives large
per-class F-score improvements, exploiting
the higher accuracy of the first two stages
(generic/referential, singular/plural), and the fact
that different features are good for different tasks.
5 Conclusions
We have shown that the use of simple lexical fea-
tures can improve performance and robustness for
all aspects of second-person pronoun resolution:
referentiality detection and reference identifica-
tion. An overall 6-way classifier is feasible, and
cascading individual classifiers can help. Future
plans include testing on ASR transcripts, and in-
vestigating different classification techniques for
the joint task.
References
M. Frampton, R. Ferna?ndez, P. Ehlen, M. Christoudias,
T. Darrell, and S. Peters. 2009. Who is ?you?? com-
bining linguistic and gaze features to resolve second-
person references in dialogue. In Proceedings of the
12th Conference of the EACL.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.
2007. Resolving ?you? in multi-party dialog. In
Proceedings of the 8th SIGdial Workshop on Dis-
course and Dialogue.
N. Jovanovic, R. op den Akker, and A. Nijholt. 2006.
Addressee identification in face-to-face meetings. In
Proceedings of the 11th Conference of the EACL.
N. Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, The Netherlands.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004.
Identifying the addressee in human-human-robot in-
teractions based on head pose and speech. In Pro-
ceedings of the 6th International Conference on
Multimodal Interfaces.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-
coln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of the 5th International Conference on
Methods and Techniques in Behavioral Research.
D. Reidsma, D. Heylen, and R. op den Akker. 2008.
On the contextual analysis of agreement scores. In
Proceedings of the LREC Workshop on Multimodal
Corpora.
D. Traum. 2004. Issues in multi-party dialogues. In
F. Dignum, editor, Advances in Agent Communica-
tion, pages 201?211. Springer-Verlag.
K. van Turnhout, J. Terken, I. Bakx, and B. Eggen.
2005. Identifying the intended addressee in mixed
human-humand and human-computer interaction
from non-verbal features. In Proceedings of ICMI.
309
 	
	
	Classifying Ellipsis in Dialogue: A Machine Learning Approach
Raquel FERNA?NDEZ, Jonathan GINZBURG and Shalom LAPPIN
Department of Computer Science
King?s College London
Strand, London WC2R 2LS, UK
{raquel,ginzburg,lappin}@dcs.kcl.ac.uk
Abstract
This paper presents a machine learning approach
to bare sluice disambiguation in dialogue. We ex-
tract a set of heuristic principles from a corpus-based
sample and formulate them as probabilistic Horn
clauses. We then use the predicates of such clauses
to create a set of domain independent features to an-
notate an input dataset, and run two different ma-
chine learning algorithms: SLIPPER, a rule-based
learning algorithm, and TiMBL, a memory-based
system. Both learners perform well, yielding simi-
lar success rates of approx 90%. The results show
that the features in terms of which we formulate our
heuristic principles have significant predictive power,
and that rules that closely resemble our Horn clauses
can be learnt automatically from these features.
1 Introduction
The phenomenon of sluicing?bare wh-phrases
that exhibit a sentential meaning?constitutes
an empirically important construction which
has been understudied from both theoretical
and computational perspectives. Most theoret-
ical analyses (e.g. (Ross, 1969; Chung et al,
1995)), focus on embedded sluices considered
out of any dialogue context. They rarely look
at direct sluices?sluices used in queries to re-
quest further elucidation of quantified parame-
ters (e.g. (1a)). With a few isolated exceptions,
these analyses also ignore a class of uses we refer
to (following (Ginzburg and Sag, 2001) (G&S))
as reprise sluices. These are used to request
clarification of reference of a constituent in a
partially understood utterance, as in (1b).
(1) a. Cassie: I know someone who?s a good kisser.
Catherine: Who? [KP4, 512]1
b. Sue: You were getting a real panic then.
Angela: When? [KB6, 1888]
Our corpus investigation shows that the com-
bined set of direct and reprise sluices constitutes
1This notation indicates the British National Corpus
file (KP4) and the sluice sentence number (512).
more than 75% of all sluices in the British Na-
tional Corpus (BNC). In fact, they make up ap-
prox. 33% of all wh-queries in the BNC.
In previous work (Ferna?ndez et al, to ap-
pear), we implemented G&S?s analysis of di-
rect sluices as part of an interpretation module
in a dialogue system. In this paper we apply
machine learning techniques to extract rules for
sluice classification in dialogue.
In Section 2 we present our corpus study of
classifying sluices into dialogue types and dis-
cuss the methodology we used in this study.
Section 3 analyses the distribution patterns we
identify and considers possible explanations for
these patterns. In Section 4 we identify a num-
ber of heuristic principles for classifying each
sluice dialogue type and formulate these prin-
ciples as probability weighted Horn clauses. In
Section 5, we then use the predicates of these
clauses as features to annotate our corpus sam-
ples of sluices, and run two machine learning
algorithms on these data sets. The first ma-
chine learner used, SLIPPER, extracts opti-
mised rules for identifying sluice dialogue types
that closely resemble our Horn clause principles.
The second, TiMBL, uses a memory-based ma-
chine learning procedure to classify a sluice by
generalising over similar environments in which
the sluice occurs in a training set. Both algo-
rithms performed well, yielding similar success
rates of approximately 90%. This suggests that
the features in terms of which we formulated our
heuristic principles for classifying sluices were
well motivated, and both learning algorithms
that we used are well suited to the task of dia-
logue act classification for fragments on the ba-
sis of these features. We finally present our con-
clusions and future work in Section 6.
2 Corpus Study
2.1 The Corpus
Our corpus-based investigation of bare sluices
has been performed using the ?10 million word
dialogue transcripts of the BNC. The corpus of
bare sluices has been constructed using SCoRE
(Purver, 2001), a tool that allows one to search
the BNC using regular expressions.
The dialogue transcripts of the BNC contain
5183 bare sluices (i.e. 5183 sentences consist-
ing of just a wh-word). We distinguish between
the following classes of bare sluices: what, who,
when, where, why, how and which. Given that
only 15 bare which were found, we have also
considered sluices of the form which N. Includ-
ing which N, the corpus contains a total of 5343
sluices, whose distribution is shown in Table 1.
The annotation was performed on two differ-
ent samples of sluices extracted from the total
found in the dialogue transcripts of the BNC.
The samples were created by arbitrarily select-
ing 50 sluices of each class (15 in the case of
which). The first sample included all instances
of bare how and bare which found, making up a
total of 365 sluices. The second sample con-
tained 50 instances of the remaining classes,
making up a total of 300 sluices.
what why who where
3045 1125 491 350
when which N how which
107 160 50 15
Total: 5343
Table 1: Total of sluices in the BNC
2.2 The Annotation Procedure
To classify the sluices in the first sample of our
sub-corpus we used the categories described be-
low. The classification was done by 3 expert
annotators (the authors) independently.
Direct The utterer of the sluice understands
the antecedent of the sluice without difficulty.
The sluice queries for additional information
that was explicitly or implicitly quantified away
in the previous utterance.
(2) Caroline: I?m leaving this school.
Lyne: When? [KP3, 538]
Reprise The utterer of the sluice cannot un-
derstand some aspect of the previous utterance
which the previous (or possibly not directly pre-
vious) speaker assumed as presupposed (typi-
cally a contextual parameter, except for why,
where the relevant ?parameter? is something
like speaker intention or speaker justification).
(3) Geoffrey: What a useless fairy he was.
Susan: Who? [KCT, 1753]
Clarification The sluice is used to ask for
clarification about the previous utterance as a
whole.
(4) June: Only wanted a couple weeks.
Ada: What? [KB1, 3312]
Unclear It is difficult to understand what
content the sluice conveys, possibly because the
input is too poor to make a decision as to its
resolution, as in the following example:
(5) Unknown : <unclear> <pause>
Josephine: Why? [KCN, 5007]
After annotating the first sample, we decided
to add a new category to the above set. The
sluices in the second sample were classified ac-
cording to a set of five categories, including the
following:
Wh-anaphor The antecedent of the sluice is
a wh-phrase.
(6) Larna: We?re gonna find poison apple and I
know where that one is.
Charlotte: Where? [KD1, 2371]
2.3 Reliability
To evaluate the reliability of the annotation, we
use the kappa coefficient (K) (Carletta, 1996),
which measures pairwise agreement between a
set of coders making category judgements, cor-
recting for expected chance agreement. 2
The agreement on the coding of the first
sample of sluices was moderate (K = 52).3
There were important differences amongst
sluice classes: The lowest agreement was on the
annotation for why (K = 29), what (K = 32)
and how (K = 32), which suggests that these
categories are highly ambiguous. Examina-
tion of the coincidence matrices shows that the
largest confusions were between reprise and
clarification in the case of what, and be-
tween direct and reprise for why and how.
On the other hand, the agreement on classi-
fying who was substantially higher (K = 71),
with some disagreements between direct and
reprise.
Agreement on the annotation of the 2nd sam-
ple was considerably higher although still not
entirely convincing (K = 61). Overall agree-
ment was improved in all classes, except for
2K = P (A)?P (E)/1?P (E), where P(A) is the pro-
portion of actual agreements and P(E) is the proportion
of expected agreement by chance, which depends on the
number of relative frequencies of the categories under
test. The denominator is the total proportion less the
proportion of chance expectation.
3All values are shown as percentages.
where and who. Agreement on what improved
slightly (K = 39), and it was substantially
higher on why (K = 52), when (K = 62) and
which N (K = 64).
Discussion Although the three coders may
be considered experts, their training and famil-
iarity with the data were not equal. This re-
sulted in systematic differences in their anno-
tations. Two of the coders (coder 1 and coder
2) had worked more extensively with the BNC
dialogue transcripts and, crucially, with the def-
inition of the categories to be applied. Leaving
coder 3 out of the coder pool increases agree-
ment very significantly: K = 70 in the first
sample, and K = 71 in the second one. The
agreement reached by the more expert pair of
coders was high and stable. It provides a solid
foundation for the current classification. It also
indicates that it is not difficult to increase an-
notation agreement by relatively light training
of coders.
3 Results: Distribution Patterns
In this section we report the results obtained
from the corpus study described in Section 2.
The study shows that the distribution of read-
ings is significantly different for each class of
sluice. Subsection 3.2 outlines a possible expla-
nation of such distribution.
3.1 Sluice/Interpretation Correlations
The distribution of interpretations for each class
of sluice is shown in Table 2. The distributions
are presented as percentages of pairwise agree-
ment (i.e. agreement between pairs of coders),
leaving aside the unclear cases. This allows
us to see the proportion made up by each in-
terpretation for each sluice class, together with
any correlations between sluice and interpreta-
tion. Distributions are similar over both sam-
ples, suggesting that corpus size is large enough
to permit the identification of repeatable pat-
terns.
Table 2 reveals interesting correlations be-
tween sluice classes and preferred interpreta-
tions. The most common interpretation for
what is clarification, making up 69% in the
first sample and 66% in the second one. Why
sluices have a tendency to be direct (57%,
83%). The sluices with the highest probability
of being reprise are who (76%, 95%), which
(96%), which N (88%, 80%) and where (75%,
69%). On the other hand, when (67%, 65%) and
how (87%) have a clear preference for direct
interpretations.
1st Sample 2nd Sample
Dir Rep Cla Dir Rep Cla Wh-a
what 9 22 69 7 23 66 4
why 57 43 0 83 14 0 3
who 24 76 0 0 95 0 5
where 25 75 0 22 69 0 9
when 67 33 0 65 29 0 6
which N 12 88 0 20 80 0 0
which 4 96 0 ? ? ? ?
how 87 8 5 ? ? ? ?
Table 2: Distributions as pairwise agr percentages
3.2 Explaining the Frequency Hierarchy
In order to gain a complete perspective on sluice
distribution in the BNC, it is appropriate to
combine the (averaged) percentages in Table 2
with the absolute number of sluices contained in
the BNC (see Table 1), as displayed in Table 3:
whatcla 2040 whichNrep 135
whydir 775 whendir 90
whatrep 670 whodir 70
whorep 410 wheredir 70
whyrep 345 howdir 45
whererep 250 whenrep 35
whatdir 240 whichNdir 24
Table 3: Sluice Class Frequency - Estim. Tokens
For instance, although more than 70% of why
sluices are direct, the absolute number of why
sluices that are reprise exceeds the total num-
ber of when sluices by almost 3 to 1. Explicating
the distribution in Table 3 is important in or-
der to be able to understand among other issues
whether we would expect a similar distribution
to occur in a Spanish or Mandarin dialogue cor-
pus; similarly, whether one would expect this
distribution to be replicated across different do-
mains. Here we restrict ourselves to sketching
an explanation of a couple of striking patterns
exhibited in Table 3.
One such pattern is the low frequency of when
sluices, particularly by comparison with what
one might expect to be its close cousin?where;
indeed the direct/reprise splits are almost
mirror images for when v. where. Another very
notable pattern, alluded to above, is the high
frequency of why sluices.4
The when v. where contrast provides one ar-
gument against (7), which is probably the null
4As we pointed out above, sluices are a common
means of asking wh?interrogatives; in the case of why?
interrogatives, this is even stronger?close to 50% of all
such interrogatives in the BNC are sluices.
hypothesis w/r to the distribution of reprise
sluices:
(7) Frequency of antecedent hypothesis:
The frequency of a class of reprise sluices
is directly correlated with the frequency of
the class of its possible antecedents.
Clearly locative expressions do not outnum-
ber temporal ones and certainly not by the
proportion the data in Table 3 would require
to maintain (7).5 (Purver, 2004) provides ad-
ditional data related to this?clarification re-
quests of all types in the BNC that pertain to
nominal antecedents outnumber such CRs that
relate to verbal antecedents by 40:1, which does
not correlate with the relative frequency of nom-
inal v. verbal antecedents (about 1.3:1).
A more refined hypothesis, which at present
we can only state quite informally, is (8):
(8) Ease of grounding of antecedent hy-
pothesis: The frequency of a class of
reprise sluices is directly correlated with
the ease with which the class of its possible
antecedents can be grounded (in the sense
of (Clark, 1996; Traum, 1994)).
This latter hypothesis offers a route towards
explaining the when v. where contrast. There
are two factors at least which make ground-
ing a temporal parameter significantly easier on
the whole than grounding a locative parameter.
The first factor is that conversationalists typi-
cally share a temporal ontology based on a clock
and/or calendar. Although well structured loca-
tive ontologies do exist (e.g. grid points in a
map), they are far less likely to be common cur-
rency. The natural ordering of clock/calendar-
based ontologies reflected in grammatical de-
vices such as sequence of tense is a second fac-
tor that favours temporal parameters over loca-
tives.
From this perspective, the high frequency of
why reprises is not surprising. Such reprises
query either the justification for an antecedent
assertion or the goal of an antecedent query.
Speakers usually do not specify these explicitly.
In fact, what requires explanation is why such
5A rough estimate concerning the BNC can be ex-
tracted by counting the words that occur more than 1000
times. Of these approx 35k tokens are locative in nature
and could serve as antecedents of where; the correspond-
ing number for temporal expressions and when yields
approx 80k tokens. These numbers are derived from a
frequency list (Kilgarriff, 1998) of the demographic por-
tion of the BNC.
reprises do not occur even more frequently than
they actually do. To account for this, one has
to appeal to considerations of the importance of
anchoring a contextual parameter.6
A detailed explication of the distribution
shown in Table 3 requires a detailed model of
dialogue interaction. We have limited ourselves
to suggesting that the distribution can be expli-
cated on the basis of some quite general princi-
ples that regulate grounding.
4 Heuristics for sluice
disambiguation
In this section we informally describe a set
of heuristics for assigning an interpretation to
bare sluices. In subsection 4.2, we show how
our heuristics can be formalised as probabilistic
sluice typing constraints.
4.1 Description of the heuristics
To maximise accuracy we have restricted our-
selves to cases of three-way agreement among
the three coders when considering the distri-
bution patterns from which we obtained our
heuristics. Looking at these patters we have
arrived at the following general principles for
resolving bare sluice types.
What The most likely interpretation is
clarification. This seems to be the case
when the antecedent utterance is a fragment, or
when there is no linguistic antecedent. Reprise
interpretations also provide a significant propor-
tion (about 23%). If there is a pronoun (match-
ing the appropriate semantic constraints) in the
antecedent utterance, then the preferred inter-
pretation is reprise:
(9) Andy: I don?t know how to do it.
Nick: What? Garlic bread? [KPR, 1763]
Why The interpretation of why sluices tends
to be direct. However, if the antecedent is a
non-declarative utterance, or a negative declar-
ative, the sluice is likely to be a reprise.
(10) Vicki: Were you buying this erm newspaper
last week by any chance?
Frederick: Why? [KC3, 3388]
Who Sluices of this form show a very strong
preference for reprise interpretation. In the
majority of cases, the antecedent is either a
proper name (11), or a personal pronoun.
6Another factor is the existence of default strategies
for resolving such parameters, e.g. assuming that the
question asked transparently expresses the querier?s pri-
mary goal.
(11) Patrick: [...] then I realised that it was Fennite
Katherine: Who? [KCV, 4694]
Which/Which N Both sorts of sluices ex-
hibit a strong tendency to reprise. In the
overwhelming majority of reprise cases for both
which and which N, the antecedent is a definite
description like ?the button? in (12).
(12) Arthur: You press the button.
June: Which one? [KSS, 144]
Where The most likely interpretation of
where sluices is reprise. In about 70% of
the reprise cases, the antecedent of the sluice
is a deictic locative pronoun like ?there? or
?here?. Direct interpretations are preferred
when the antecedent utterance is declarative
with no overt spatial location expression.
(13) Pat: You may find something in there actually.
Carole: Where? [KBH, 1817]
When If the antecedent utterance is a declar-
ative and there is no time-denoting expression
other than tense, the sluice will be interpreted
as direct, as in example (14). On the other
hand, deictic temporal expressions like ?then?
trigger reprise interpretations.
(14) Caroline: I?m leaving this school.
Lyne: When? [KP3, 538]
How This class of sluice exhibits a very strong
tendency to direct (87%). It appears that
most of the antecedent utterances contain an
accomplishment verb.
(15) Anthony: I?ve lost the, the whole work itself
Arthur: How? [KP1, 631]
4.2 Probabilistic Constraints
The problem we are addressing is typing of bare
sluice tokens in dialogue. This problem is anal-
ogous to part-of-speech tagging, or to dialogue
act classification.
We formulate our typing constraints as Horn
clauses to achieve the most general and declar-
ative expression of these conditions. The an-
tecedent of a constraint uses predicates corre-
sponding to dialogue relations, syntactic prop-
erties, and lexical content. The predicate of the
consequent represents a sluice typing tag, which
corresponds to a maximal type in the HPSG
grammar that we used in implementing our di-
alogue system. Note that these constraints can-
not be formulated at the level of the lexical en-
tries of the wh-words since these distributions
are specific to sluicing and not to non-elliptical
wh-interrogatives.7 As a first example, consider
the following rule:
sluice(x), where(x),
ant utt(y,x),
contains(y,?there?) ? reprise(x) [.78]
This rule states that if x is a sluice construction
with lexical head where, and its antecedent ut-
terance (identified with the latest move in the
dialogue) contains the word ?there?, then x is a
reprise sluice. Note that, as in a probabilistic
context-free grammar (Booth, 1969), the rule is
assigned a conditional probability. In the exam-
ple above, .78 is the probability that the context
described in the antecedent of the clause pro-
duces the interpretation specified in the conse-
quent.8
The following three rules are concerned with
the disambiguation of why sluice readings. The
structure of the rules is the same as before. In
this case however, the disambiguation is based
on syntactic and semantic properties of the an-
tecedent utterance as a whole (like polarity or
mood), instead of focusing on a particular lexi-
cal item contained in such utterance.
sluice(x), why(x),
ant utt(y,x), non decl(y) ? reprise(x) [.93]
sluice(x), why(x),
ant utt(y,x), pos decl(y) ? direct(x) [.95]
sluice(x), why(x),
ant utt(y,x), neg decl(y) ? reprise(x) [.40]
5 Applying Machine Learning
To evaluate our heuristics, we applied machine
learning techniques to our corpus data. Our
aim was to evaluate the predictive power of the
features observed and to test whether the intu-
itive constraints formulated in the form of Horn
clause rules could be learnt automatically from
these features.
5.1 SLIPPER
We use a rule-based learning algorithm called
SLIPPER (for Simple Learner with Iterative
Pruning to Produce Error Reduction). SLIP-
PER (Cohen and Singer, 1999) combines the
7Thus, whereas Table 2 shows that approx. 70% of
who-sluices are reprise, this is clearly not the case
for non-elliptical who?interrogatives. For instance, the
KB7 block in the BNC has 33 non-elliptical who?
interrogatives. Of these at most 3 serve as reprise ut-
terances.
8These probabilities have been extracted manually
from the three-way agreement data.
separate-and-conquer approach used by most
rule learners with confidence-rated boosting to
create a compact rule set.
The output of SLIPPER is a weighted rule
set, in which each rule is associated with a con-
fidence level. The rule builder is used to find
a rule set that separates each class from the re-
maining classes using growing and pruning tech-
niques. To classify an instance x, one computes
the sum of the confidences that cover x: if the
sum is greater than zero, the positive class is
predicted. For each class, the only rule with
a negative confidence rating is a single default
rule, which predicts membership in the remain-
ing classes.
We decided to use SLIPPER for two main
reasons: (1) it generates transparent, relatively
compact rule sets that can provide interesting
insights into the data, and (2) its if-then rules
closely resemble our Horn clause constraints.
5.2 Experimental Setup
To generate the input data we took all three-
way agreement instances plus those instances
where there is agreement between coder 1 and
coder 2, leaving out cases classified as unclear.
We reclassified 9 instances in the first sample as
wh-anaphor, and also included these data.9 The
total data set includes 351 datapoints. These
were annotated according to the set of features
shown in Table 4.
sluice type of sluice
mood mood of the antecedent utterance
polarity polarity of the antecedent utterance
frag whether the antecedent utterance is
a fragment
quant presence of a quantified expression
deictic presence of a deictic pronoun
proper n presence of a proper name
pro presence of a pronoun
def desc presence of a definite description
wh presence of a wh word
overt presence of any other potential
antecedent expression
Table 4: Features
We use a total of 11 features. All features are
nominal. Except for the sluice feature that in-
dicates the sluice type, they are all boolean, i.e.
they can take as value either yes or no. The
features mood, polarity and frag refer to syn-
tactic and semantic properties of the antecedent
9We reclassified those instances that had motivated
the introduction of the wh-anaphor category for the sec-
ond sample. Given that there were no disagreements in-
volving this category, such reclassification was straight-
forward.
utterance as a whole. The remaining features,
on the other hand, focus on a particular lexical
item or construction contained in such utter-
ance. They will take yes as a value if this ele-
ment or construction exists and, it matches the
semantic restrictions imposed by the sluice type.
The feature wh will take a yes value only if there
is a wh-word that is identical to the sluice type.
Unknown or irrelevant values are indicated by
a question mark. This allows us to express, for
instance, that the presence of a proper name is
irrelevant to determine the interpretation of a
where sluice, while it is crucial when the sluice
type is who. The feature overt takes no as value
when there is no overt antecedent expression. It
takes yes when there is an antecedent expres-
sion not captured by any other feature, and it
is considered irrelevant (question mark value)
when there is an antecedent expression defined
by another feature.
5.3 Accuracy Results
We performed a 10-fold cross-validation on the
total data set, obtaining an average success rate
of 90.32%. Using leave-one-out cross-validation
we obtained an average success rate of 84.05%.
For the holdout method, we held over 100 in-
stances as a testing data, and used the reminder
(251 datapoints) for training. This yielded
a success rate of 90%. Recall, precision and
f-measure values are reported in Table 5.
category recall precision f-measure
direct 96.67 85.29 90.62
reprise 88.89 94.12 91.43
clarification 83.33 71.44 76.92
wh anaphor 80.00 100 88.89
Table 5: SLIPPER - Results
Using the holdout procedure, SLIPPER gen-
erated a set of 23 rules: 4 for direct, 13
for reprise, 1 for clarification and 1 for
wh-anaphor, plus 4 default rules, one for each
class. All features are used except for frag,
which indicates that this feature does not play a
significant role in determining the correct read-
ing. The following rules are part of the rule set
generated by SLIPPER:
direct not reprise|clarification|wh anaphor :-
overt=no, polarity=pos (+1.06296)
reprise not direct|clarification|wh anaphor :-
deictic=yes (+3.31703)
reprise not direct|clarification|wh anaphor :-
mood=non decl, sluice=why (+1.66429)
5.4 Comparing SLIPPER and TiMBL
Although SLIPPER seems to be especially well
suited for the task at hand, we decided to run a
different learning algorithm on the same train-
ing and testing data sets and compare the re-
sults obtained. For this experiment we used
TiMBL, a memory-based learning algorithm de-
veloped at Tilburg University (Daelemans et
al., 2003). As with all memory-based machine
learners, TiMBL stores representations of in-
stances from the training set explicitly in mem-
ory. In the prediction phase, the similarity be-
tween a new test instance and all examples in
memory is computed using some distance met-
ric. The system will assign the most frequent
category within the set of most similar exam-
ples (the k-nearest neighbours). As a distance
metric we used information-gain feature weight-
ing, which weights each feature according to the
amount of information it contributes to the cor-
rect class label.
The results obtained are very similar to the
previous ones. TiMBL yields a success rate of
89%. Recall, precision and f-measure values are
shown in Table 6. As expected, the feature that
received a lowest weighting was frag.
category recall precision f-measure
direct 86.60 86.60 86.6
reprise 88.89 90.50 89.68
clarification 83.33 71.44 76.92
wh anaphor 100 100 100
Table 6: TiMBL -Results
6 Conclusion and Further Work
In this paper we have presented a machine
learning approach to bare sluice classification
in dialogue using corpus-based empirical data.
From these data, we have extracted a set of
heuristic principles for sluice disambiguation
and formulated such principles as probability
weighted Horn clauses. We have then used
the predicates of these clauses as features to
annotate an input dataset, and ran two dif-
ferent machine learning algorithms: SLIPPER,
a rule-based learning algorithm, and TiMBL,
a memory-based learning system. SLIPPER
has the advantage of generating transparent
rules that closely resemble our Horn clause con-
straints. Both algorithms, however, perform
well, yielding to similar success rates of approx-
imately 90%. This shows that the features we
used to formulate our heuristic principles were
well motivated, except perhaps for the feature
frag, which does not seem to have a signifi-
cant predictive power. The two algorithms we
used seem to be well suited to the task of sluice
classification in dialogue on the basis of these
features.
In the future we will attempt to construct
an automatic procedure for annotating a dia-
logue corpus with the features presented here,
to which both machine learning algorithms ap-
ply.
References
T. Booth. 1969. Probabilistic representation
of formal languages. In IEEE Conference
Record of the 1969 Tenth Annual Symposium
of Switching and Automata Theory.
J. Carletta. 1996. Assessing agreement on clas-
sification tasks: the kappa statistics. Compu-
tational Linguistics, 2(22):249?255.
S. Chung, W. Ladusaw, and J. McCloskey.
1995. Sluicing and logical form. Natural Lan-
guage Semantics, 3:239?282.
H. H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
W. Cohen and Y. Singer. 1999. A simple, fast,
and effective rule learner. In Proc. of the 16th
National Conference on AI.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2003. TiMBL: Tilburg
Memory Based Learner, Reference Guide.
Technical Report ILK-0310, U. of Tilburg.
R. Ferna?ndez, J. Ginzburg, H. Gregory, and
S. Lappin. (to appear). SHARDS: Frag-
ment resolution in dialogue. In H. Bunt and
R. Muskens, editors, Computing Meaning,
volume 3. Kluwer.
J. Ginzburg and I. Sag. 2001. Interrogative
Investigations. CSLI Publications, Stanford,
California.
A. Kilgarriff. 1998. BNC Database and
Word Frequency Lists. www.itri.bton.ac.uk/
?Adam.Kilgarriff/ bnc-readme.html.
M. Purver. 2001. SCoRE: A tool for searching
the BNC. Technical Report TR-01-07, Dept.
of Computer Science, King?s College London.
M. Purver. 2004. The Theory and Use of Clari-
fication in Dialogue. Ph.D. thesis, King?s Col-
lege, London, forthcoming.
J. Ross. 1969. Guess who. In Proc. of the 5th
annual Meeting of the Chicago Linguistics So-
ciety, pages 252?286, Chicago. CLS.
D. Traum. 1994. A Computational Theory of
Grounding in Natural Language Conversa-
tion. Ph.D. thesis, University of Rochester,
Department of Computer Science, Rochester.
17
18
19
20
21
22
23
24
Proceedings of the 43rd Annual Meeting of the ACL, pages 231?238,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Scaling up from Dialogue to Multilogue: some principles and benchmarks
Jonathan Ginzburg and Raquel Ferna?ndez
Dept of Computer Science
King?s College, London
The Strand, London WC2R 2LS
UK
{ginzburg,raquel}@dcs.kcl.ac.uk
Abstract
The paper considers how to scale up dialogue
protocols to multilogue, settings with multiple
conversationalists. We extract two benchmarks
to evaluate scaled up protocols based on the
long distance resolution possibilities of non-
sentential utterances in dialogue and multi-
logue in the British National Corpus. In light
of these benchmarks, we then consider three
possible transformations to dialogue protocols,
formulated within an issue-based approach to
dialogue management. We show that one such
transformation yields protocols for querying
and assertion that fulfill these benchmarks.
1 Introduction
The development of dialogue systems in which a human
agent interacts using natural language with a computa-
tional system is by now a flourishing domain (see e.g.
(NLE, 2003)), buttressed by an increasing theoretical and
experimental literature on the properties of dialogue (see
e.g. recent work in the SEMDIAL and SIGDIAL confer-
ences). In contrast, the development of multilogue sys-
tems, in which conversation with 3 or more participants
ensue?is still in its early stages, as is the theoretical and
experimental study of multilogue. The fundamental issue
in tackling multilogue is: how can mechanisms motiv-
ated for dialogue (e.g. information states, protocols, up-
date rules etc) be scaled up to multilogue?
In this paper we extract from a conversational cor-
pus, the British National Corpus (BNC), several bench-
marks that characterize dialogue and multilogue inter-
action. These are based on the resolution possibilities
of non-sentential utterances (NSUs). We then use these
benchmarks to evaluate certain general transformations
whose application to a dialogue interaction system yield
a system appropriate for multilogue.
There are of course various plausible views of the rela-
tion between dialogue and multilogue. One possible ap-
proach to take is to view multilogue as a sequence of dia-
logues. Something like this approach seems to be adop-
ted in the literature on communication between autonom-
ous software agents. However, even though many situ-
ations considered in multiagent systems do involve more
than two agents, most interaction protocols are designed
only for two participants at a time. This is the case of
the protocol specifications provided by FIPA (Foundation
for Intelligent Physical Agents) for agent communication
language messages (FIPA, 2003). The FIPA interaction
protocols (IP) are most typically designed for two parti-
cipants, an initiator and a responder . Some IPs permit the
broadcasting of a message to a group of addressees, and
the reception of multiple responses by the original initi-
ator (see most particularly the Contract Net IP). However,
even though more than two agents participate in the com-
municative process, as (Dignum and Vreeswijk, 2003)
point out, such conversations can not be considered mul-
tilogue, but rather a number of parallel dialogues.
The Mission Rehearsal Exercise (MRE) Project
(Traum and Rickel, 2002), one of the largest multilogue
systems developed hitherto, is a virtual reality envir-
onment where multiple partners (including humans and
other autonomous agents) engage in multi-conversation
situations. The MRE is underpinned by an approach to
the modelling of interaction in terms of obligations that
different utterance types bring about originally proposed
for dialogue (see e.g. (Matheson et al , 2000)). In par-
ticular, this includes a model of the grounding process
(Clark, 1996) that involves recognition and construction
of common ground units (CGUs) (see (Traum, 2003)).
Modelling of obligations and grounding becomes more
complex when considering multilogue situations. The
model of grounding implemented in the MRE project can
only be used in cases where there is a single initiator and
responder. It is not clear what the model should be for
231
multiple addressees: should the contents be considered
grounded when any of the addressees has acknowledged
them? Should evidence of understanding be required
from every addressee?
Since their resolution is almost wholly reliant on con-
text, non sentential utterances provide a large testbed con-
cerning the structure of both dialogue and multilogue. In
section 2 we present data from the British National Cor-
pus (BNC) concerning the resolution of NSUs in dialogue
and multilogue. The main focus of this data is with the
distance between antecedent and fragment. We use this
to extract certain benchmarks concerning multilogue in-
teraction. Thus, acknowledgement and acceptance mark-
ers (e.g. ?mmh?, ?yeah?) are resolved with reference to
an utterance (assertion) which they ground (accept). The
data we provide shows that acknowledgements in mul-
tilogue, as in dialogue, are adjacent to their antecedent.
This provides evidence that, in general, a single addressee
serves to signal grounding. In contrast, BNC data indic-
ates the prevalence in multilogue of short answers that
are resolved using material from an antecedent question
located several turns back, whereas in dialogue short an-
swers are generally adjacent to their antecedent. This
provides evidence against reducing querying interaction
in multilogue to a sequence of dialogues. We show that
long distance short answers are a stable phenomenon for
multilogue involving both small (?5 persons) and large
(> 5 persons) groups, despite the apparently declining
interactivity with increasing group size flagged in exper-
imental work (see (Fay et al, 2000)).
In section 3 we sketch the basic principles of issue
based dialogue management which we use as a basis
for our subsequent investigations of multilogue interac-
tion. This will include information states and formula-
tion of protocols for querying and assertion in dialogue.
In section 4 we consider three possible transformations
on dialogue protocols into multilogue protocols. These
transformations are entirely general in nature and could
be applied to protocols stated in whatever specification
language. We evaluate the protocols that are generated
by these transformations with reference to the bench-
marks extracted in section 2. In particular, we show
that one such transformation, dubbed Add Side Parti-
cipants(ASP), yields protocols for querying and asser-
tion that fulfill these benchmarks. Finally, section 5
provides some conclusions and pointers to future work.
2 Long Distance Resolution of NSUs in
Dialogue and Multilogue: some
benchmarks
The work we present in this paper is based on empir-
ical evidence provided by corpus data extracted from the
British National Corpus (BNC).
2.1 The Corpus
Our current corpus is a sub-portion of the BNC conversa-
tional transcripts consisting of 14,315 sentences. The cor-
pus was created by randomly excerpting a 200-speaker-
turn section from 54 BNC files. Of these files, 29 are
transcripts of conversations between two dialogue parti-
cipants, and 25 files are multilogue transcripts.
A total of 1285 NSUs were found in our sub-corpus.
Table 1 shows the raw counts of NSUs found in the dia-
logue and multilogue transcripts, respectively.
NSUs BNC files
Dialogue 709 29
Multilogue 576 25
Total 1285 54
Table 1: Total of NSUs in Dialogue and Multilogue
All NSUs encountered within the corpus were clas-
sified according to the NSU typology presented in
(Ferna?ndez and Ginzburg, 2002). Additionally, the dis-
tance from their antecedent was measured.1 Table 2
shows the distribution of NSU categories and their ante-
cedent separation distance. The classes of NSU which
feature in our discussion below are boldfaced.
The BNC annotation includes tagging of units approx-
imating to sentences, as identified by the CLAWS seg-
mentation scheme (Garside, 1987). Each sentence unit is
assigned an identifier number. By default it is assumed
that sentences are non-overlapping and that their numer-
ation indicates temporal sequence. When this is not the
case because speakers overlap, the tagging scheme en-
codes synchronous speech by means of an alignment map
used to synchronize points within the transcription. How-
ever, even though information about simultaneous speech
is available, overlapping sentences are annotated with dif-
ferent sentence numbers.
In order to be able to measure the distance between
the NSUs encountered and their antecedents, all instances
were tagged with the sentence number of their antecedent
utterance. The distance we report is therefore measured
in terms of sentence numbers. It should however be noted
that taking into account synchronous speech would not
change the data reported in Table 2 in any significant
1This classification was done by one expert annotator. To
assess its reliability a pilot study of the taxonomy was per-
formed using two additional non-expert coders. These annot-
ated 50 randomly selected NSUs (containing a minimum of 2
instances of each NSU class, as labelled by the expert annot-
ator.). The agreement achieved by the three coders is reasonably
good, yielding a kappa score ? = 0.76. We also assessed the ac-
curacy of the coders? choices in choosing the antecedent utter-
ance using the expert annotator?s annotation as a gold standard.
Given this, one coder?s accuracy was 92%, whereas the other
coder?s was 96%.
232
Distance
NSU Class Example Total 1 2 3 4 5 6 >6
Acknowledgment Mm mm. 595 578 15 2
Short Answer Ballet shoes. 188 104 21 17 5 5 8 28
Affirmative Answer Yes. 109 104 4 1
Clarification Ellipsis John? 92 76 13 2 1
Repeated Ack. His boss, right. 86 81 2 3
Rejection No. 50 49 1
Factual Modifier Brilliant! 27 23 2 1 1
Repeated Aff. Ans. Very far, yes. 26 25 1
Helpful Rejection No, my aunt. 24 18 5 1
Check Question Okay? 22 15 7
Filler ... a cough. 18 16 1 1
Bare Mod. Phrase On the desk. 16 11 4 1
Sluice When? 11 10 1
Prop. Modifier Probably. 11 10 1
Conjunction Phrase Or a mirror. 10 5 4 1
Total 1285 1125 82 26 9 7 8 28
Percentage 100 87.6 6.3 2 0.6 0.5 0.6 2.1
Table 2: NSUs sorted by Class and Distance
way, as manual examination of all NSUs at more than
distance 3 reveals that the transcription portion between
antecedent and NSU does not contain any completely
synchronous sentences in such cases.
In the examples throughout the paper we shall use ital-
ics to indicate speech overlap. When italics are not used,
utterances take place sequentially.
2.2 NSU-Antecedent Separation Distance
The last row in Table 2 shows the distribution of NSU-
antecedent separation distances as percentages of the
total of NSUs found. This allows us to see that about
87% of NSUs have a distance of 1 sentence (i.e. the ante-
cedent was the immediately preceding sentence), and that
the vast majority (about 96%) have a distance of 3 sen-
tences or less.
Although the proportion of NSUs found in dialogue
and multilogue is roughly the same (see Table 1 above),
when taking into account the distance of NSUs from their
antecedent, the proportion of long distance NSUs in mul-
tilogue increases radically: the longer the distance, the
higher the proportion of NSUs that were found in multi-
logue. In fact, as Table 3 shows, NSUs that have a dis-
tance of 7 sentences or more appear exclusively in multi-
logue transcripts. These differences are significant (?2 =
62.24, p ? 0.001).
Adjacency of grounding and affirmation utterances
The data in table 2 highlights a fundamental charac-
teristic of the remaining majoritarian classes of NSUs,
Ack(nowledgements), Affirmative Answer, CE (clari-
fication ellipsis), Repeated Ack(nowledgements), and
Rejection. These are used either in grounding interac-
tion, or to affirm/reject propositions.2 The overwhelming
adjacency to their antecedent underlines the locality of
these interactions.
Long distance potential for short answers One strik-
ing result exhibited in Table 2 is the uneven distribution of
long distance NSUs across categories. With a few excep-
tions, NSUs that have a distance of 3 sentences or more
are exclusively short answers. Not only is the long dis-
tance phenomenon almost exclusively restricted to short
answers, but the frequency of long distance short answers
stands in strong contrast to the other NSUs classes; in-
deed, over 44% of short answers have more than distance
1, and over 24% have distance 4 or more, like the last
answer in the following example:
(1) Allan: How much do you think?
Cynthia: Three hundred pounds.
Sue: More.
Cynthia: A thousand pounds.
Allan: More.
Unknown: <unclear>
Allan: Eleven hundred quid apparently.
[BNC, G4X]
Long distance short answers primarily a multilogue
effect Table 4 shows the total number of short answers
found in dialogue and multilogue respectively, and the
proportions sorted by distance over those totals:
From this it emerges that short answers are more
common in multilogue than in dialogue?134(71%) v.
2Acknowledgements and acceptances are, in principle, dis-
tinct acts: the former involves indication that an utterance has
been understood, whereas the latter that an assertion is accepted.
In practice, though, acknowledgements in the form of NSUs
commonly simultaneously signal acceptances. Given this, cor-
pus studies of NSUs (e.g. (Ferna?ndez and Ginzburg, 2002)) of-
ten conflate the two.
233
Distance 1 2 3 4 5 6 >6
Dialogue 658 (59%) 37 (45%) 11 (45%) 1 (12%) 1 (14%) 1 (13%) 0 (0%)
Multilogue 467 (41%) 45 (55%) 15 (55%) 8 (88%) 6 (86%) 7 (87%) 28 (100%)
Table 3: NSUs in dialogue and multilogue sorted by distance
Short Answers Total # 1 2 3 > 3
Dialogue 54 82 9 9 0
Multilogue 134 44 11 8 37
Table 4: % over the totals found in dialogue and multilogue
54(29%). Also, the distance pattern exhibited by these
two groups is strikingly different: Only 18% of short an-
swers found in dialogue have a distance of more than 1
sentence, with all of them having a distance of at most 3,
like the short answer in (2).
(2) Malcolm: [...] cos what?s three hundred and
sixty divided by seven?
Anon 1: I don?t know.
Malcolm: Yes I don?t know either!
Anon 1: Fifty four point fifty one point four.
[BNC, KND]
This dialogue/multilogue asymmetry argues against re-
ductive views of multilogue as sequential dialogue.
Long Distance short answers and group size As
Table 4 shows, all short answers at more than distance
3 appear in multilogues. Following (Fay et al, 2000),
we distinguish between small groups (those with 3 to 5
participants) and large groups (those with more than 5
participants). The size of the group is determined by the
amount of participants that are active when a particular
short answer is uttered. We consider active participants
those that have made a contribution within a window of
30 turns back from the turn where the short answer was
uttered.
Table 5 shows the distribution of long distance short
answers (distance> 3) in small and large groups respect-
ively. This indicates that long distance short answers are
significantly more frequent in large groups (?2 = 22.17,
p ? 0.001), though still reasonably common in small
groups. A pragmatic account correlating group size and
frequency of long distance short answers is offered in the
final paragraph of section 3.
Group Size d > 3 d ? 3 Total
? 5 20 73 93
(21.5%) (78.5%)
> 5 26 15 41
(63%) (37%)
Table 5: Long distance short answers in small and large groups
Large group multilogues in the corpus are all tran-
scripts of tutorials, training sessions or seminars, which
exhibit a rather particular structure. The general pat-
tern involves a question being asked by the tutor or ses-
sion leader, the other participants then taking turns to an-
swer that question. The tutor or leader acts as turn man-
ager. She assigns the turn explicitly usually by addressing
the participants by their name without need to repeat the
question under discussion. An example is shown in (3):
(3) Anon1: How important is those three components
and what value would you put on them [...]
Anon3: Tone forty five. Body language thirty .
Anon1: Thank you.
Anon4: Oh.
Anon1: Melanie.
Anon5: twenty five.
Anon1: Yes.
Anon5: Tone of voice twenty five. [BNC, JYM]
Small group multilogues on the other hand have a more
unconstrained structure: after a question is asked, the par-
ticipants tend to answer freely. Answers by different par-
ticipants can follow one after the other without explicit
acknowledgements nor turn management, like in (4):.
(4) Anon 1: How about finance then? <pause>
Unknown 1: Corruption
Unknown 2: Risk <pause dur=30>
Unknown 3: Wage claims <pause dur=18>
2.3 Two Benchmarks of multilogue
The data we have seen above leads in particular to the fol-
lowing two benchmarks protocols for querying, assertion,
and grounding interaction in multilogue:
(5) a. Multilogue Long Distance short answers
(MLDSA): querying protocols for multilogue
must license short answers an unbounded num-
ber of turns from the original query.
b. Multilogue adjacency of ground-
ing/acceptance (MAG): assertion and ground-
ing protocols for multilogue should license
grounding/clarification/acceptance moves only
adjacently to their antecedent utterance.
MLDSA and MAG have a somewhat different status:
whereas MLDSA is a direct generalization from the data,
MAG is a negative constraint, posited given the paucity of
positive instances. As such MAG is more open to doubt
and we shall treat it as such in the sequel.
234
3 Issue based Dialogue Management:
basic principles
In this section we outline some of the basic principles
of Issue-based Dialogue Management, which we use as
a basis for our subsequent investigations of multilogue
interaction.
Information States We assume information states of
the kind developed in the KoS framework (e.g. (Gin-
zburg, 1996, forthcoming), (Larsson, 2002)) and imple-
mented in systems such as GODIS, IBIS, and CLARIE
(see e.g. (Larsson, 2002; Purver, 2004)). On this
view each dialogue participant?s view of the common
ground, their Dialogue Gameboard (DGB), is structured
by a number of attributes including the following three:
FACTS: a set of facts representing the shared assump-
tions of the CPs, LatestMove: the most recent groun-
ded move, and QUD (?questions under discussion?): a
partially ordered set?often taken to be structured as a
stack?consisting of the currently discussable questions.
Querying and Assertion Both querying and asser-
tion involve a question becoming maximal in the quer-
ier/asserter?s QUD:3 the posed question q for a query
where q is posed, the polar question p? for an assertion
where p is asserted. Roughly, the responder can sub-
sequently either choose to start a discussion (of q or p?)
or, in the case of assertion, to update her FACTS structure
with p. A dialogue participant can downdate q/p? from
QUD when, as far as her (not necessarily public) goals
dictate, sufficient information has been accumulated in
FACTS. The querying/assertion protocols (in their most
basic form) are summarized as follows:
(6)
querying assertion
LatestMove = Ask(A,q) LatestMove = Assert(A,p)
A: push q onto QUD; A: push p? onto QUD;
release turn; release turn
B: push q onto QUD; B: push p? onto QUD;
take turn; take turn;
make max-qud?specific; Option 1: Discuss p?
utterance4
take turn. Option 2: Accept p
LatestMove = Accept(B,p)
B: increment FACTS with p;
pop p? from QUD;
A: increment FACTS with p;
pop p? from QUD;
Following (Larsson, 2002; Cooper, 2004), one can
3In other words, pushed onto the stack, if one assumes QUD
is a stack.
4An utterance whose content is either a proposition p About
max-qud or a question q1 on which max-qud Depends. For the
latter see footnote 7. If one assumes QUD to be a stack, then
?max-qud?specific? will in this case reduce to ?q?specific?. But
the more general formulation will be important below.
decompose interaction protocols into conversational
update rules?functions from DGBs into DGBs using
Type Theory with Records (TTR). This allows simple
interfacing with the grammar, a Constraint-based Gram-
mar closely modelled on HPSG but formulated in TTR
(see (Ginzburg, forthcoming)).
Grounding Interaction Grounding an utterance u : T
(?the sign associated with u is of type T?) is modelled as
involving the following interaction. (a) Addressee B tries
to anchor the contextual parameters of T. If successful,
B acknowledges u (directly, gesturally or implicitly) and
responds to the content of u. (b) If unsuccessful, B poses
a Clarification Request (CR), that arises via utterance co-
ercion (see (Ginzburg and Cooper, 2001)). For reasons
of space we do not formulate an explicit protocol here?
the structure of such a protocol resembles the assertion
protocol. Our subsequent discussion of assertion can be
modified mutatis mutandis to grounding.
NSU Resolution We assume the account of NSU res-
olution developed in (Ginzburg and Sag, 2000). The
essential idea they develop is that NSUs get their main
predicates from context, specifically via unification with
the question that is currently under discussion, an entity
dubbed the maximal question under discussion (MAX-
QUD). NSU resolution is, consequently, tied to conver-
sational topic, viz. the MAX-QUD.5
Distance effects in dialogue short answers If one as-
sumes QUD to be a stack, this affords the potential for
non adjacent short answers in dialogue. These, as dis-
cussed in section 2, are relatively infrequent. Two com-
monly observed dialogue conditions will jointly enforce
adjacency between short answers and their interrogative
antecedents: (a) Questions have a simple, one phrase
answer. (b) Questions can be answered immediately,
without preparatory or subsequent discussion. For multi-
logue (or at least certain genres thereof), both these con-
ditions are less likely to be maintained: different CPs
can supply different answers, even assuming that relat-
ive to each CP there is a simple, one phrase answer. The
more CPs there are in a conversation, the smaller their
common ground and the more likely the need for cla-
rificatory interaction. A pragmatic account of this type
of the frequency of adjacency in dialogue short answers
seems clearly preferable to any actual mechanism that
would rule out long distance short answers. These can
be perfectly felicitous?see e.g. example (1) above which
5The resolution of NSUs, on the approach of (Ginzburg and
Sag, 2000), involves one other parameter, an antecedent sub-
utterance they dub the salient-utterance (SAL-UTT). This plays
a role similar to the role played by the parallel element in higher
order unification?based approaches to ellipsis resolution (see
e.g. (Pulman, 1997). For current purposes, we limit attention
to the MAX-QUD as the nucleus of NSU resolution.
235
would work fine if the turn uttered by Sue had been
uttered by Allan instead. Moreover such a pragmatic ac-
count leads to the expectation that the frequency of long
distance antecedents is correlated with group size, as in-
deed indicated by the data in table 5.
4 Scaling up Protocols
(Goffman, 1981) introduced the distinction between rat-
ified participants and overhearers in a conversation.
Within the former are located the speaker and participants
whom she takes into account in her utterance design?
the intended addressee(s) of a given utterance, as well
as side participants. In this section we consider three
possible principles of protocol extension, each of which
can be viewed as adding roles for participants from one
of Goffman?s categories. We evaluate the protocol that
results from the application of each such principle re-
lative to the benchmarks we introduced in section 2.3.
Seen in this light, the final principle we consider, Add
Side Participants (ASP), arguably, yields the best res-
ults. Nonetheless, these three principles would appear to
be complementary?the most general protocol for mul-
tilogue will involve, minimally, application of all three.6
We state the principles informally and framework inde-
pendently as transformations on operational construals of
the protocols. In a more extended presentation we will
formulate these as functions on TTR conversational up-
date rules.
The simplest principle is Add Overhearers (AOV).
This involves adding participants who merely observe the
interaction. They keep track of facts concerning a par-
ticular interaction, but their context is not facilitated for
them to participate:
(7) Given a dialogue protocol pi, add roles C1,. . . ,Cn
where each Ci is a silent participant: given an ut-
terance u0 classified as being of type T0, Ci up-
dates Ci.DGB.FACTS with the proposition u0 :
T0.
Applying AOV yields essentially multilogues which
are sequences of dialogues. A special case of this are
moderated multilogues, where all dialogues involve a
designated individual (who is also responsible for turn
assignment.). Restricting scaling up to applications of
AOV is not sufficient since inter alia this will not fulfill
the MLDSA benchmark.
A far stronger principle is Duplicate Responders
(DR):
(8) Given a dialogue protocol pi, add roles C1,. . . ,Cn
which duplicate the responder role.
6We thank an anonymous reviewer for ACL for convincing
us of this point.
Applying DR to the querying protocol yields the fol-
lowing protocol:
(9) Querying with multiple responders
1. LatestMove = Ask(A,q)
2. A: push q onto QUD; release turn
3. Resp1: push q onto QUD; take turn; make max-qud?
specific utterance; release turn
4. Resp2: push q onto QUD; take turn; make max-qud?
specific utterance; release turn
5. . . .
6. Respn: push q onto QUD; take turn; make max-qud?
specific utterance; release turn
This yields interactions such as (4) above. The query-
ing protocol in (9) licenses long distance short answers,
so satisfies the MLDSA benchmark. On the other hand,
the contextual updates it enforces will not enable it to deal
with the following (constructed) variant on (4), in other
words does not afford responders to comment on previ-
ous responders, as opposed to the original querier:
(10) A: Who should we invite for the conference?
B: Svetlanov.
C: No (=Not Svetlanov), Zhdanov
D: No (= Not Zhdanov, 6= Not Svetlanov), Gergev
Applying DR to the assertion protocol will yield the
following protocol:
(11) Assertion with multiple responders
1. LatestMove = Assert(A,p)
2. A: push p? onto QUD; release turn
3. Resp1: push p? onto QUD; take turn; ? Option 1:
Discuss p?, Option 2: Accept p ?
4. Resp2: push p? onto QUD; take turn; ? Option 1:
Discuss p?, Option 2: Accept p ?
5. . . .
6. Respn: push p? onto QUD; take turn; ? Option 1:
Discuss p?, Option 2: Accept p ?
One arguable problem with this protocol?equally
applicable to the corresponding DRed grounding
protocol?is that it licences long distance acceptance and
is, thus, inconsistent with the MAG benchmark. On the
other hand, it is potentially useful for interactions where
there is explicitly more than one direct addressee.
A principle intermediate between AOV and DR is Add
Side Participants (ASP):
(12) Given a dialogue protocol pi, add roles
C1,. . . ,Cn, which effect the same contextual up-
date as the interaction initiator.
Applying ASP to the dialogue assertion protocol yields
the following protocol:
(13) Assertion for a conversation involving
{A,B,C1,. . . ,Cn}
236
1. LatestMove = Assert(A,p)
2. A: push p? onto QUD; release turn
3. Ci: push p? onto QUD;
4. B: push p? onto QUD; take turn; ?Option 1: Accept
p, Option 2: Discuss p??
(14) 1. LatestMove = Accept(B,p)
2. B: increment FACTS with p; pop p? from QUD;
3. Ci:increment FACTS with p; pop p? from QUD;
4. A: increment FACTS with p; pop p? from QUD;
This protocol satisfies the MAG benchmark in that ac-
ceptance is strictly local. This is because it enforces
communal acceptance?acceptance by one CP can count
as acceptance by all other addressees of an assertion.
There is an obvious rational motivation for this, given the
difficulty of a CP constantly monitoring an entire audi-
ence (when this consists of more than one addressee) for
acceptance signals?it is well known that the effect of
visual access on turn taking is highly significant (Dabbs
and Ruback, 1987). It also enforces quick reaction to
an assertion?anyone wishing to dissent from p must get
their reaction in early i.e. immediately following the as-
sertion since further discussion of p? is not countenanced
if acceptance takes place. The latter can happen of course
as a consequence of a dissenter not being quick on their
feet; on this protocol to accommodate such cases would
require some type of backtracking.
Applying ASP to the dialogue querying protocol yields
the following protocol:
(15) Querying for a conversation involving
{ A,B,C1,. . . ,Cn}
1. LatestMove = Ask(A,q)
2. A: push q onto QUD; release turn
3. Ci: push q onto QUD;
4. B: push q onto QUD; take turn; make max-qud?
specific utterance.
This improves on the DR generated protocol be-
cause it does allow responders to comment on previous
responders?the context is modified as in the dialogue
protocol. Nonetheless, as it stands, this protocol won?t
fully deal with examples such as (4)?the issue intro-
duced by each successive participant takes precedence
given that QUD is assumed to be a stack. This can be
remedied by slightly modifying this latter assumption:
we will assume that when a question q is pushed onto
QUD it doesn?t subsume all existing questions in QUD,
but rather only those on which q does not depend:7
(16) q is QUDmod(dependence) maximal iff for any q0 in
QUD such that ?Depend(q, q1): q  q0.
7 The notion of dependence we assume here is one common
in work on questions, e.g. (Ginzburg and Sag, 2000), intuitively
corresponding to the notion of ?is a subquestion of?. q1 depends
on q2 iff any proposition p such that p resolves q2 also satisfies
p is about q1.
This is conceptually attractive because it reinforces
that the order in QUD has an intuitive semantic basis.
One effect this has is to ensure that any polar question
p? introduced into QUD, whether by an assertion or by
a query, subsequent to a wh-question q on which p? de-
pends does not subsume q. Hence, q will remain access-
ible as an antecedent for NSUs, as long as no new unre-
lated topic has been introduced. Assuming this modifica-
tion to QUD is implemented in the above ASP?generated
protocols, both MLDSA and MAG benchmarks are ful-
filled.
5 Conclusions and Further Work
In this paper we consider how to scale up dialogue proto-
cols to multilogue, settings with multiple conversation-
alists. We have extracted two benchmarks, MLDSA
and MAG, to evaluate scaled up protocols based on the
long distance resolution possibilities of NSUs in dialogue
and multilogue in the BNC. MLDSA, the requirement
that multilogue protocols license long distance short an-
swers, derives from the statistically significant increase
in frequency of long distance short answers in multi-
logue as opposed to dialogue. MAG, the requirement
that multilogue protocols enforce adjacency of accept-
ance and grounding interaction, derives from the over-
whelming locality of acceptance/grounding interaction
in multilogue, as in dialogue. In light of these bench-
marks, we then consider three possible transformations
to dialogue protocols formulated within an issue-based
approach to dialogue management. Each transformation
can be intuited as adding roles that correspond to dis-
tinct categories of an audience originally suggested by
Goffman. The three transformations would appear to be
complementary?it seems reasonable to assume that ap-
plication of all three (in some formulation) will be needed
for wide coverage of multilogue. MLDSA and MAG can
be fulfilled within an approach that combines the Add
Side Participants transformation on protocols with an
independently motivated modification of the structure of
QUD from a canonical stack to a stack where maximality
is conditioned by issue dependence.
With respect to long distance short answers our ac-
count licences their occurrence in dialogue, as in mul-
tilogue. We offer a pragmatic account for their low fre-
quency in dialogue, which indeed generalizes to explain
a statistically significant correlation we observe between
their increased incidence and increasing active participant
size. We plan to carry out more detailed work, both
corpus?based and experimental, in order to evaluate the
status of MAG and, correspondingly to assess just how
local acceptance and grounding interaction really are.
We also intend to implement multilogue protocols in
CLARIE so it can simulate multilogue. We will then eval-
uate its ability to process NSUs from the BNC.
237
Acknowledgements
We would like to thank three anonymous ACL review-
ers for extremely useful comments, which in particular
forced us to rethink some key issues. We would also like
to thank Pat Healey, Shalom Lappin, Richard Power, and
Matt Purver for discussion, and Zoran Macura and Yo
Sato for help in assessing the NSU taxonomy. Earlier
versions of this work were presented at colloquia at ITRI,
Brighton, and at the Universite? Paris, 7. The research
described here is funded by grant number RES-000-23-
0065 from the Economic and Social Research Council of
the United Kingdom.
References
Special issue on best practice in spoken language dia-
logue systems engineering. 2003. Natural Language
Engineering.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press, Cambridge.
Robin Cooper. 2004. A type theoretic approach to in-
formation state update in issue based dialogue man-
agement. Invited paper, Catalog?04, the 8th Workshop
on the Semantics and Pragmatics of Dialogue, Pompeu
Fabra University, Barcelona.
James Dabbs and R. Barry Ruback. 1987 Dimensions of
group process: amount and structure of vocal interac-
tion. Advances in Experimental Social Psychology 20,
pages 123?169.
Frank P.M. Dignum and Gerard A.W. Vreeswijk. 2003.
Towards a testbed for multi-party dialogues. In Pro-
ceedings of the first International Joint Conference on
Autonomous Agents and Multi-agent Systems (AAMAS
2003).
Nicholas Fay, Simon Garrod, and Jean Carletta. 2000.
Group discussion as interactive dialogue or serial
monologue. Psychological Science, pages 481?486.
Raquel Ferna?ndez and Jonathan Ginzburg. 2002. Non-
sentential utterances: A corpus study. Traitement auto-
matique des languages. Dialogue, 43(2):13?42.
FIPA. 2003. The foundation for intelligent
physical agents. interaction protocol specifications.
http://www.fipa.org.
Roger Garside. 1987. The CLAWS word-tagging sys-
tem, In Roger Garside et al editors, The computa-
tional analysis of English: a corpus-based approach,
Longman, Harlow, pages 30?41.
Jonathan Ginzburg and Robin Cooper. 2001. Resolv-
ing ellipsis in clarification. In Proceedings of the 39th
Meeting of the Association for Computational Lin-
guistics, Toulouse.
Jonathan Ginzburg and Ivan A. Sag. 2000. Interrogative
Investigations: the form, meaning and use of English
Interrogatives. Number 123 in CSLI Lecture Notes.
CSLI Publications, Stanford: California.
Jonathan Ginzburg. (forthcoming). Semantics and Inter-
action in Dialogue CSLI Publications and University
of Chicago Press.
Jonathan Ginzburg. 1996. Interrogatives: Questions,
facts, and dialogue. In Shalom Lappin, editor, Hand-
book of Contemporary Semantic Theory. Blackwell,
Oxford.
Erving Goffman 1981 Forms of Talk. University of
Pennsylvania Press, Philadelphia.
Staffan Larsson. 2002. Issue based Dialogue Manage-
ment. Ph.D. thesis, Gothenburg University.
Colin Matheson and Massimo Poesio and David Traum.
2000. Modelling Grounding and Discourse Obliga-
tions Using Update Rules. Proceedings of NAACL
2000, Seattle.
Stephen Pulman. 1997. Focus and higher order unifica-
tion. Linguistics and Philosophy, 20.
Matthew Purver. 2004. The Theory and Use of Clarific-
ation in Dialogue. Ph.D. thesis, King?s College, Lon-
don.
David Traum and Jeff Rickel. 2002. Embodied agents
for multi-party dialogue in immersive virtual world. In
Proceedings of the first International Joint Conference
on Autonomous Agents and Multi-agent Systems (AA-
MAS 2002), pages 766?773.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In H. Bunt,
editor, Proceedings of the 5th International Workshop
on Computational Semantics, pages 380?394, Tilburg.
ITK, Tilburg University.
238
 	
		ffClassifying Non-Sentential Utterances in
Dialogue: A Machine Learning Approach
Raquel Ferna?ndez?
Potsdam University
Jonathan Ginzburg??
King?s College London
Shalom Lappin?
King?s College London
In this article we use well-known machine learning methods to tackle a novel task, namely
the classification of non-sentential utterances (NSUs) in dialogue. We introduce a fine-grained
taxonomy of NSU classes based on corpus work, and then report on the results of several machine
learning experiments. First, we present a pilot study focused on one of the NSU classes in the
taxonomy?bare wh-phrases or ?sluices??and explore the task of disambiguating between
the different readings that sluices can convey. We then extend the approach to classify the
full range of NSU classes, obtaining results of around an 87% weighted F-score. Thus our
experiments show that, for the taxonomy adopted, the task of identifying the right NSU class
can be successfully learned, and hence provide a very encouraging basis for the more general
enterprise of fully processing NSUs.
1. Introduction
Non-sentential utterances (NSUs)?fragmentary utterances that do not have the form
of a full sentence according to most traditional grammars, but that nevertheless convey
a complete clausal meaning?are a common phenomenon in spoken dialogue. The
following are two examples of NSUs taken from the dialogue transcripts of the British
National Corpus (BNC) (Burnard 2000):
(1) a. A: Who wants Beethoven music?
B: Richard and James. [BNC: KB8 1024?1025]1
b. A: It?s Ruth?s birthday.
B: When? [BNC: KBW 13116?13117]
? Karl-Liebknecht Strasse 24-25, 14476 Golm, Germany. E-mail: raquel@ling.uni-potsdam.de.
?? The Strand, London WC2R 2LS, UK. E-mail: jonathan.ginzburg@kcl.ac.uk.
? The Strand, London WC2R 2LS, UK. E-mail: shalom.lappin@kcl.ac.uk.
1 This notation indicates the name of the file and the sentence numbers in the BNC.
Submission received: 24 September 2004; revised submission received: 10 November 2006; accepted for
publication: 9 March 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 3
Arguably the most important issue in the processing of NSUs concerns their resolu-
tion, that is, the recovery of a full clausal meaning from a form which is standardly
considered non-clausal. In the first of the examples, the NSU in bold face is a typical
?short answer,? which despite having the form of a simple NP would most likely be
understood as conveying the proposition Richard and James want Beethoven music. The
NSU in (1b) is an example of what has been called a ?sluice.? Again, despite being
realized by a bare wh-phrase, the meaning conveyed by the NSU could be paraphrased
as the question When is Ruth?s birthday?
Although short answers and short queries like those in (1) are perhaps two of the
most prototypical NSU classes, recent corpus studies (Ferna?ndez and Ginzburg 2002;
Schlangen 2003) show that other less well-known types of NSUs?each with its own
resolution constraints?are also pervasive in real conversations. This variety of NSU
classes, together with their inherent concise form and their highly context-dependent
meaning, often make NSUs ambiguous. Consider, for instance, example (2):
(2) a. A: I left it on the table.
B: On the table.
b. A: Where did you leave it?
B: On the table.
c. A: I think I put it er. . .
B: On the table.
d. A: Should I put it back on the shelf?
B: On the table.
An NSU like B?s response in (2a) can be understood either as a clarification question
or as an acknowledgment, depending on whether it is uttered with raising intonation
or not. In (2b), on the other hand, the NSU is readily understood as a short answer,
whereas in (2c) it fills a gap left by the previous utterance. Yet in the context of (2d) it
will most probably be understood as a sort of correction or a ?helpful rejection,? as we
shall call this kind of NSU later on in this article.
As different NSU classes are typically related to different resolution constraints, in
order to resolve NSUs appropriately systems need to be equipped in the first place with
the ability of identifying the intended kind of NSU. How this ability can be developed is
precisely the issue we address in this article. We concentrate on the task of automatically
classifying NSUs, which we approach using machine learning (ML) techniques. Our aim
in doing so is to develop a classification model whose output can be fed into a dialogue
processing system?be it a full dialogue system or, for instance, an automatic dialogue
summarization system?to boost its NSU resolution capability.
As we shall see, to run the ML experiments we report in this article, we an-
notate our data with small sets of meaningful features, instead of using large sets
of arbitrary features as is common in some stochastic approaches. We do this
with the aim of obtaining a better understanding of the different classes of NSUs,
their distribution, and their properties. For training, we use four machine learn-
ing systems: the rule induction learner SLIPPER (Cohen and Singer 1999), the
memory-based learner TiMBL (Daelemans et al 2003), the maximum entropy algo-
rithm MaxEnt (Le 2003), and the Weka toolkit (Witten and Frank 2000). From the
Weka toolkit we use the J4.8 decision tree learner, as well as a majority class pre-
dictor and a one-rule classifier to derive baseline systems that help us to evaluate
398
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
the difficulty of the classification task and the ML results obtained. The main
advantage of using several systems that implement different learning techniques is that
this allows us to factor out any algorithm-dependent effects that may influence our
results.
The article is structured as follows. In Section 2, we introduce the taxonomy of NSU
classes we adopt, present a corpus study done using the BNC, and give an overview
of the theoretical approach to NSU resolution we assume. After these introductory
sections, in Section 3 we present a pilot study that focuses on bare wh-phrases or sluices.
This includes a small corpus study and a preliminary ML experiment that concentrates
on disambiguating between the different interpretations that sluices can convey. We
obtain very encouraging results: around 80% weighted F-score (an 8% improvement
over a simple one-rule baseline). After this, in Section 4, we move on to the full range
of NSUs. We present our main experiments, whereby the ML approach is extended to
the task of classifying the full range of NSU classes in our taxonomy. The results we
achieve on this task are decidedly positive: around an 87% weighted F-score (a 25%
improvement over a four-rule baseline where only four features are used). Finally, in
Section 5, we offer conclusions and some pointers for future work.
2. A Taxonomy of NSUs
We propose a taxonomy that offers a comprehensive inventory of the kinds of NSUs
that can be found in conversation. The taxonomy includes 15 NSU classes. With a few
modifications, these follow the corpus-based taxonomy proposed by Ferna?ndez and
Ginzburg (2002). In what follows we exemplify each of the categories we use in our
work and characterize them informally.
Clarification Ellipsis (CE). We use this category to classify reprise fragments used to
clarify an utterance that has not been fully comprehended.
(3) a. A: There?s only two people in the class
B: Two people? [BNC: KPP 352?354]
b. A: [. . . ] You lift your crane out, so this part would come up.
B: The end? [BNC: H5H 27?28]
Check Question. This NSU class refers to short queries, usually realized by convention-
alized forms like alright? and okay?, that are requests for explicit feedback.
(4) A: So <pause> I?m allowed to record you.
Okay?
B: Yes. [BNC: KSR 5?7]
Sluice. We consider as sluices all wh-question NSUs, thereby conflating under this form-
based NSU class reprise and direct sluices like those in (5a) and (5b), respectively.2 In the
taxonomy of Ferna?ndez and Ginzburg (2002) reprise sluices are classified as CE. In the
taxonomy used in the experiments we report in this article, however, CE only includes
clarification fragments that are not bare wh-phrases.
2 This distinction is due to Ginzburg and Sag (2001). More on it will be discussed in Section 2.2.
399
Computational Linguistics Volume 33, Number 3
(5) a. A: Only wanted a couple weeks.
B: What? [BNC: KB1 3311?3312]
b. A: I know someone who?s a good kisser.
B: Who? [BNC: KP4 511?512]
Short Answer. This NSU class refers to typical responses to (possibly embedded) wh-
questions (6a)/(6b). Sometimes, however, wh-questions are not explicit, as in the context
of a short answer to a CE question, for instance (6c).
(6) a. A: Who?s that?
B: My Aunty Peggy. [BNC: G58 33?35]
b. A: Can you tell me where you got that information from?
B: From our wages and salary department. [BNC: K6Y 94?95]
c. A: Vague and?
B: Vague ideas and people. [BNC: JJH 65?66]
Plain Affirmative Answer and Plain Rejection. The typical context of these two classes
of NSUs is a polar question (7a), which can be implicit as in CE questions like (7b). As
shown in (7c), rejections can also be used to respond to assertions.
(7) a. A: Did you bring the book I told you?
B: Yes./ No.
b. A: That one?
B: Yeah. [BNC: G4K 106?107]
c. A: I think I left it too long.
B: No no. [BNC: G43 26?27]
Both plain affirmative answers and rejections are strongly indicated by lexical
material, characterized by the presence of a ?yes? word ( yeah, aye, yep. . . ) or the negative
interjection no.
Repeated Affirmative Answer. We distinguish plain affirmative answers like the ones
in (7) from repeated affirmative answers like the one in (8), which respond affirmatively
to a polar question by verbatim repetition or reformulation of (a fragment of) the
query.
(8) A: Did you shout very loud?
B: Very loud, yes. [BNC: JJW 571-572]
Helpful Rejection. The context of helpful rejections can be either a polar question or
an assertion. In the first case, they are negative answers that provide an appropriate
alternative (9a). As responses to assertions, they correct some piece of information in
the previous utterance (9b).
(9) a. A: Is that Mrs. John <last or full name>?
B: No, Mrs. Billy. [BNC: K6K 67-68]
400
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
b. A: Well I felt sure it was two hundred pounds a, a week.
B: No fifty pounds ten pence per person. [BNC: K6Y 112?113]
Plain Acknowledgment. The class plain acknowledgment refers to utterances (like
yeah, mhm, ok) that signal that a previous declarative utterance was understood and/or
accepted.
(10) A: I know that they enjoy debating these issues.
B: Mhm. [BNC: KRW 146?147]
Repeated Acknowledgment. This class is used for acknowledgments that, as repeated
affirmative answers, also repeat a part of the antecedent utterance, which in this case is
a declarative.
(11) A: I?m at a little place called Ellenthorpe.
B: Ellenthorpe. [BNC: HV0 383?384]
Propositional and Factual Modifiers. These two NSU classes are used to classify propo-
sitional adverbs like (12a) and factual adjectives like (12b), respectively, in stand-alone
uses.
(12) a. A: I wonder if that would be worth getting?
B: Probably not. [BNC: H61 81?82]
b. A: There?s your keys.
B: Oh great! [BNC: KSR 137?138]
Bare Modifier Phrase. This class refers to NSUs that behave like adjuncts modifying a
contextual utterance. They are typically PPs or AdvPs.
(13) A: [. . . ] they got men and women in the same dormitory!
B: With the same showers! [BNC: KST 992?996]
Conjunct. This NSU class is used to classify fragments introduced by conjunctions.
(14) A: Alistair erm he?s, he?s made himself coordinator.
B: And section engineer. [BNC: H48 141?142]
Filler. Fillers are NSUs that fill a gap left by a previous unfinished utterance.
(15) A: [. . . ] twenty two percent is er <pause>
B: Maxwell. [BNC: G3U 292?293]
2.1 The Corpus Study
The taxonomy of NSUs presented herein has been tested in a corpus study carried out
using the dialogue transcripts of the BNC. The study, which we describe here briefly,
supplies the data sets used in the ML experiments we will present in Section 4.
The present corpus of NSUs includes and extends the subcorpus used in Ferna?ndez
and Ginzburg (2002). It was created by manual annotation of a randomly selected
section of 200-speaker-turns from 54 BNC files. Of these files, 29 are transcripts of
401
Computational Linguistics Volume 33, Number 3
conversations between two dialogue participants, and 25 files are multi-party tran-
scripts. The total of transcripts used covers a wide variety of domains, from free conver-
sation to meetings, tutorials and training sessions, as well as interviews and transcripts
of medical consultations. The examined subcorpus contains 14,315 sentences. Sentences
in the BNC are identified by the CLAWS segmentation scheme (Garside 1987) and each
unit is assigned an identifier number.
We found a total of 1,299 NSUs, which make up 9% of the total of sentences in the
subcorpus. These results are in line with the rates reported in other recent corpus studies
of NSUs: 11.15% in (Ferna?ndez and Ginzburg 2002), 10.2% in (Schlangen and Lascarides
2003), 8.2% in (Schlangen 2005).3
The NSUs found were labeled according to the taxonomy presented previously
together with an additional class Other introduced to catch all NSUs that did not fall
in any of the classes in the taxonomy. All NSUs that could be classified with the tax-
onomy classes were additionally tagged with the sentence number of their antecedent
utterance. The NSUs not covered by the classification only make up 1.2% (16 instances)
of the total of NSUs found. Thus, with a rate of 98.8% coverage, the present taxonomy
offers a satisfactory coverage of the data.
The labeling of the entire corpus of NSUs was done by one expert annotator. To
assess the reliability of the annotation, a small study with two additional, non-expert
annotators was conducted. These annotated a total of 50 randomly selected instances
(containing a minimum of two instances of each NSU class as labeled by the expert
annotator) with the classes in the taxonomy. The agreement obtained by the three
annotators is reasonably good, yielding a ? score of 0.76. The non-expert annotators
were also asked to identify the antecedent sentence of each NSU. Using the expert
annotation as a gold standard, they achieved 96% and 92% accuracy in this task.
The distribution of NSU classes that emerged after the annotation of the subcorpus
is shown in detail in Table 1. By far the most common class can be seen to be Plain
Acknowledgment, which accounts for almost half of all NSUs found. This is followed
in frequency by Short Answer (14.5%) and Plain Affirmative Answer (8%). CE is the
most common class among the NSUs that denote questions (i.e., CE, Sluice, and Check
Question), making up 6.3% of all NSUs found.
2.2 Resolving NSUs: Theoretical Background and Implementation
The theoretical background we assume with respect to the resolution of NSUs derives
from the proposal presented in Ginzburg and Sag (2001), which in turn is based on the
theory of context developed by Ginzburg (1996, 1999).
Ginzburg and Sag (2001) provide a detailed analysis of a number of classes of
NSUs?including Short Answer, Sluice, and CE?couched in the framework of Head-
driven Phrase Structure Grammar (HPSG). They take NSUs to be first-class gram-
matical constructions whose resolution is achieved by combining the contribution of
the NSU phrase with contextual information?concretely, with the current question
under discussion, or QUD, which roughly corresponds to the current conversational
topic.4
3 For a comparison of our NSU taxonomy and the one proposed by Schlangen (2003), see Ferna?ndez (2006).
4 An anonymous reviewer asked about the distinction between NSUs that are meaning complete and those
which are not. In fact we take all NSUs to be interpreted as full propositions or questions.
402
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 1
Distribution of NSU classes.
NSU class Total %
Plain Acknowledgment 599 46.1
Short Answer 188 14.5
Plain Affirmative Answer 105 8.0
Repeated Acknowledgment 86 6.6
Clarification Ellipsis 82 6.3
Plain Rejection 49 3.7
Factual Modifier 27 2.0
Repeated Affirmative Answer 26 2.0
Helpful Rejection 24 1.8
Check Question 22 1.7
Filler 18 1.4
Bare Modifier Phrase 15 1.1
Propositional Modifier 11 0.8
Sluice 21 1.6
Conjunct 10 0.7
Other 16 1.2
Total 1,299 100
The simplest way of exemplifying this strategy is perhaps to consider a direct short
answer to an explicit wh-question, like the one shown in (16a).
(16) a. A: Who?s making the decisions?
B: The fund manager. (= The fund manager is making the decisions.)
[BNC: JK7 119?120]
b. QUD: ?(x).Make decision(x, t)
Resolution: Make decision( fm,t)
In this dialogue, the current QUD corresponds to the content of the previous utterance?
the wh-question Who?s making the decisions? Assuming a representation of questions as
lambda abstracts, the resolution of the short answer amounts to applying this question
to the phrasal content of the NSU, as shown in (16b) in an intuitive notation.5
Ginzburg and Sag (2001) distinguish between direct and reprise sluices. For direct
sluicing, the current QUD is a polar question p?, where p is required to be a quan-
tified proposition.6 The resolution of the direct sluice consists in constructing a wh-
question by a process that replaces the quantification with a simple abstraction. For
instance:
(17) a. A: A student phoned.
B: Who? (= Which student phoned?)
5 To simplify matters, throughout the examples in this section we use lambda abstraction for wh-questions
and a simple question mark operator for polar questions. For a far more accurate representation of
questions in HPSG and Type Theory with Records, see Ginzburg and Sag (2001) and Ginzburg (2005),
respectively.
6 In Ginzburg?s theory of context an assertion of a proposition p raises the polar question p? for discussion.
403
Computational Linguistics Volume 33, Number 3
b. QUD: ??xPhone(x, t)
Resolution: ?(x).Phone(x, t)
In the case of reprise sluices and CE, the current QUD arises in a somewhat less direct
way, via a process of utterance coercion or accommodation (Larsson 2002; Ginzburg and
Cooper 2004), triggered by the inability to ground the previous utterance (Traum 1994;
Clark 1996). The output of the coercion process is a question about the content of a
(sub)utterance which the addressee cannot resolve. For instance, if the original utterance
is the question Did Bo leave? in (18a), with Bo as the unresolvable sub-utterance, one
possible output from the coercion operations defined by Ginzburg and Cooper (2004) is
the question in (18b), which constitutes the current QUD, as well as the resolved content
of the reprise sluice in (18a).
(18) a. A: Did Bo leave?
B: Who? (= Who are you asking if s/he left?)
b. QUD: ?(b).Ask(A, ?Leave(b, t))
Resolution: ?(b).Ask(A, ?Leave(b, t))
The interested reader will find further details of this approach to NSU resolution and its
extension to other NSU classes in Ginzburg (forthcoming) and Ferna?ndez (2006).
The approach sketched here has been implemented as part of the SHARDS system
(Ginzburg, Gregory, and Lappin 2001; Ferna?ndez et al, in press), which provides a pro-
cedure for computing the interpretation of some NSU classes in dialogue. The system
currently handles short answers, direct and reprise sluices, as well as plain affirmative
answers to polar questions. SHARDS has been extended to cover several types of
clarification requests and used as a part of the information-state-based dialogue system
CLARIE (Purver 2004b). The dialogue system GoDiS (Larsson et al 2000; Larsson 2002)
also uses a QUD-based approach to handle short answers.
3. Pilot Study: Sluice Reading Classification
The first study we present focuses on the different interpretations or readings that
sluices can convey. We first describe a corpus study that aims at providing empirical
evidence about the distribution of sluice readings and establishing possible correlations
between these readings and particular sluice types. After this, we report the results of
a pilot machine learning experiment that investigates the automatic disambiguation of
sluice interpretations.
3.1 The Sluicing Corpus Study
We start by introducing the corpus of sluices. The next subsections describe the annota-
tion scheme, the reliability of the annotation, and the corpus results obtained.
Because sluices have a well-defined surface form?they are bare wh-words?we
were able to use an automatic mechanism to reliably construct our subcorpus of sluices.
This was created using SCoRE (Purver 2001), a tool that allows one to search the BNC
using regular expressions.
404
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 2
Total of sluices in the BNC.
what why who where which N when how which Total
3,045 1,125 491 350 160 107 50 15 5,343
The dialogue transcripts of the BNC contain 5,183 bare sluices (i.e., 5,183 sentences
consisting of just a wh-word). We distinguish between the following classes of bare
sluices: what, who, when, where, why, how, and which. Given that only 15 bare which were
found, we also considered sluices of the form which N. Including which N, the corpus
contains a total of 5,343 sluices, whose distribution is shown in Table 2.
For our corpus study, we selected a sample of sluices extracted from the total found
in the dialogue transcripts of the BNC. The sample was created by selecting all instances
of bare how (50) and bare which (15), and arbitrarily selecting 100 instances of each of the
remaining sluice classes, making up a total of 665 sluices.
Note that the sample does not reflect the frequency of sluice types found in the full
corpus. The inclusion of sufficient instances of the lesser frequent sluice types would
have involved selecting a much larger sample. Consequently it was decided to abstract
over the true frequencies to create a balanced sample whose size was manageable
enough to make the manual annotation feasible. We will return to the issue of the true
frequencies in Section 3.1.3.
3.1.1 Sluice Readings. The sample of sluices was classified according to a set of four
semantic categories?drawn from the theoretical distinctions introduced by Ginzburg
and Sag (2001)?corresponding to different sluice interpretations. The typology reflects
the basic direct/reprise divide and incorporates other categories that cover additional
readings, including an Unclear class intended for those cases that cannot easily be
classified by any of the other categories. The typology of sluice readings used was the
following:
Direct. Sluices conveying a direct reading query for additional information that was
explicitely or implicitly quantified away in the antecedent, which is understood without
difficulty. The sluice in (19) is an example of a sluice with direct reading: It asks for
additional temporal information that is implicitly quantified away in the antecedent
utterance.
(19) A: I?m leaving this school.
B: When? [BNC: KP3 537?538]
Reprise. Sluices conveying a reprise reading emerge as a result of an understanding
problem. They are used to clarify a particular aspect of the antecedent utterance corre-
sponding to one of its constituents, which was not correctly comprehended. In (20) the
reprise sluice has as antecedent constituent the pronoun he, whose reference could not
be adequately grounded.
(20) A: What a useless fairy he was.
B: Who? [BNC: KCT 1752?1753]
405
Computational Linguistics Volume 33, Number 3
Clarification. As reprise, this category also corresponds to a sluice reading that deals
with understanding problems. In this case the sluice is used to request clarification of
the entire antecedent utterance, indicating a general breakdown in communication. The
following is an example of a sluice with a clarification interpretation:
(21) A: Aye and what money did you get on it?
B: What?
A: What money does the government pay you? [BNC: KDJ 1077?1079]
Wh-anaphor. This category is used for the reading conveyed by sluices like (22), which
are resolved to a (possibly embedded) wh-question present in the antecedent utterance.
(22) A: We?re gonna find poison apple and I know where that one is.
B: Where? [BNC: KD1 2370?2371]
Unclear. We use this category to classify those sluices whose interpretation is difficult
to grasp, possibly because the input is too poor to make a decision as to its resolution,
as in the following example:
(23) A: <unclear> <pause>
B: Why? [BNC: KCN 5007]
3.1.2 Reliability. The coding of sluice readings was done independently by three dif-
ferent annotators. Agreement was moderate (? = 0.59). There were important differ-
ences among sluice classes: The lowest agreement was on the annotation of how (0.32)
and what (0.36), whereas the agreement on classifying who was substantially higher
(0.74).
Although the three coders may be considered ?experts,? their training and famil-
iarity with the data were not equal. This resulted in systematic differences in their
annotations. Two of the coders had worked more extensively with the BNC dialogue
transcripts and, crucially, with the definition of the categories to be applied. Leaving the
third annotator out of the coder pool increases agreement very significantly (? = 0.71).
The agreement reached by the more expert pair of coders was acceptable and, we believe,
provides a solid foundation for the current classification.7
3.1.3 Distribution Patterns. The sluicing corpus study shows that the distribution of read-
ings is significantly different for each class of sluice. The distribution of interpretations
is shown in Table 3, presented as row counts and percentages of those instances where
7 Besides the difficulty of annotating fine-grained semantic distinctions, we think that one of the reasons
why the ? score we obtain is not too high is that, as shall become clear in the next section, the present
annotation is strongly affected by the prevalence problem, which occurs when the distributions for
categories are skewed (highly unequal instantiation across categories). In order to control for differences
in prevalence, Di Eugenio and Glass (2004) propose an additional measure called PABAK
(prevalence-adjusted bias-adjusted kappa). In our case, we obtain a PABAK score of 0.60 for agreement
amongst the three coders, and a PABAK score of 0.80 for agreement between the pair of more expert
coders. A more detailed discussion of these issues can be found in Ferna?ndez (2006).
406
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 3
Distribution patterns.
Sluice Direct n (%) Reprise n (%) Clarification n (%) Wh-anaphor n (%)
what 7 (9.60) 17 (23.3) 48 (65.7) 1 (1.3)
why 55 (68.7) 24 (30.0) 0 (0) 1 (1.2)
who 10 (13.0) 65 (84.4) 0 (0) 2 (2.6)
where 31 (34.4) 56 (62.2) 0 (0) 3 (3.3)
when 50 (63.3) 27 (34.1) 0 (0) 2 (2.5)
which 1 (8.3) 11 (91.6) 0 (0) 0 (0)
whichN 19 (21.1) 71 (78.8) 0 (0) 0 (0)
how 23 (79.3) 3 (10.3) 3 (10.3) 0 (0)
at least two annotators agree, labeled taking the majority class and leaving aside cases
classified as Unclear.
Table 3 reveals significant correlations between sluice classes and preferred interpre-
tations (a chi square test yields ?2 = 438.53, p ? 0.001). The most common interpretation
for what is Clarification, making up more than 65%. Why sluices have a tendency to be
Direct (68.7%). The sluices with the highest probability of being Reprise are who (84.4%),
which (91.6), which N (78.8%), and where (62.2%). On the other hand, when (63.3%) and
how (79.3%) have a clear preference for Direct interpretations.
As explained in Section 3.1, the sample used in the corpus study does not reflect
the overall frequencies of sluice types found in the BNC. Now, in order to gain a
complete perspective on sluice distribution in the full corpus, it is therefore appropriate
to combine the percentages in Table 3 with the absolute number of sluices contained in
the BNC. The number of estimated tokens is displayed in Table 4.
For instance, the combination of Tables 3 and 4 allows us to see that although
almost 70% of why sluices are Direct, the absolute number of why sluices that are Reprise
exceeds the total number of when sluices by almost 3 to 1. Another interesting pattern
revealed by this data is the low frequency of when sluices, particularly by comparison
with what one might expect to be its close cousin, where. Indeed the Direct/Reprise
splits are almost mirror images for when versus where. Explicating the distribution in
Table 4 is important in order to be able to understand among other issues whether we
would expect a similar distribution to occur in a Spanish or Mandarin dialogue corpus;
similarly, whether one would expect this distribution to be replicated across different
domains.
Table 4
Sluice class frequency (estimated tokens).
whatcla 2,040 whichNrep 135
whydir 775 whendir 90
whatrep 670 whodir 70
whorep 410 wheredir 70
whyrep 345 howdir 45
whererep 250 whenrep 35
whatdir 240 whichNdir 24
407
Computational Linguistics Volume 33, Number 3
We will not attempt to provide an explanation for these patterns here. The reader
is invited to check a sketch of such an explanation for some of the patterns exhibited in
Table 4 in Ferna?ndez, Ginzburg, and Lappin (2004).
3.2 Automatic Disambiguation
In this section, we report a pilot study where we use machine learning to automatically
disambiguate between the different sluice readings using data obtained in the corpus
study presented previously.
3.2.1 Data. The data set used in this experiment was selected from our classified corpus
of sluices. To generate the input data for the ML experiments, all three-way agreement
instances plus those instances where there is agreement between the two coders with the
highest agreement were selected, leaving out cases classified as Unclear. The total data
set includes 351 datapoints. Of these, 106 are classified as Direct, 203 as Reprise, 24 as
Clarification, and 18 as Wh-anaphor. Thus, the classes in the data set have significantly
skewed distributions. However, as we are faced with a very small data set, we cannot
afford to balance the classes by leaving out a subset of the data. Hence, in this pilot study
the 351 data points are used in the ML experiments with their original distributions.
3.2.2 Features and Feature Annotation. In this pilot study?as well as in the extended
experiment we will present later on?instances were annotated with a small set of
features extracted automatically using the POS information encoded in the BNC. The
annotation procedure involves a simple algorithm which employs string searching and
pattern matching techniques that exploit the SGML mark-up of the corpus. The BNC
was automatically tagged using the CLAWS system developed at Lancaster University
(Garside 1987). The ?100 million words in the corpus were annotated according to a
set of 57 POS codes (known as the C5 tag-set) plus 4 codes for punctuation tags. A
list of these codes can be found in Burnard (2000). The BNC POS annotation process is
described in detail in Leech, Garside, and Bryant (1994).
Unfortunately the BNC mark-up does not include any coding of intonation. Our fea-
tures can therefore not use any intonational data, which would presumably be a useful
Table 5
Sluice features and values.
Feature Description Values
sluice type of sluice what, why, who, . . .
mood mood of the antecedent utterance decl, n decl
polarity polarity of the antecedent utterance pos, neg, ?
quant presence of a quantified expression yes, no, ?
deictic presence of a deictic pronoun yes, no, ?
proper n presence of a proper name yes, no, ?
pro presence of a pronoun yes, no, ?
def desc presence of a definite description yes, no, ?
wh presence of a wh word yes, no, ?
overt presence of any other potential antecedent expression yes, no, ?
408
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
source of information to distinguish, for instance, between question- and proposition-
denoting NSUs, between Plain Acknowledgment and Plain Affirmative Answer, and
between Reprise and Direct sluices.
To annotate the sluicing data, a set of 11 features was used. An overview of the
features and their values is shown in Table 5. Besides the feature sluice, which indicates
the sluice type, all the other features are concerned with properties of the antecedent
utterance. The features mood and polarity refer to syntactic and semantic properties
of the antecedent utterance as a whole. The remaining features, on the other hand,
focus on a particular lexical type or construction contained in the antecedent. These
features (quant, deictic, proper n, pro, def desc, wh, and overt) are not annotated
independently, but conditionally on the sluice type. That is, they will take yes as a
value if the element or construction in question appears in the antecedent and it matches
the semantic restrictions imposed by the sluice type. For instance, when a sluice with
value where for the feature sluice is annotated, the feature deictic, which encodes
the presence of a deictic pronoun, will take value yes only if the antecedent utterance
contains a locative deictic like here or there. Similarly the feature wh takes a yes value
only if there is a wh-word in the antecedent that is identical to the sluice type.
Unknown or irrelevant values are indicated by a question mark (?) value. This
allows us to express, for instance, that the presence of a proper name is irrelevant to
determining the interpretation of say a when sluice, although it is crucial when the sluice
type is who. The feature overt takes no as value when there is no overt antecedent
expression. It takes yes when there is an antecedent expression not captured by any
other feature, and it is considered irrelevant (? value) when there is an antecedent
expression defined by another feature.
The 351 data points were automatically annotated with the 11 features shown in
Table 5. The automatic annotation procedure was evaluated against a manual gold
standard, achieving an accuracy of 86%.
3.2.3 Baselines. Because sluices conveying a Reprise reading make up more than 57%
of our data set, relatively high results can already be achieved with a majority class
baseline that always predicts the class Reprise. This yields a 42.4% weighted F-score.
A slightly more interesting baseline can be obtained by using a one-rule classifier.
We use the implementation of a one-rule classifier provided in the Weka toolkit. For each
feature, the classifier creates a single rule which generates a decision tree where the root
is the feature in question and the branches correspond to its different values. The leaves
are then associated with the class that occurs most often in the data, for which that value
holds. The classifier then chooses the feature which produces the minimum error.
Figure 1
One-rule tree.
409
Computational Linguistics Volume 33, Number 3
Table 6
Baselines? results.
Sluice reading Recall Precision F1
Majority class baseline Reprise 100 57.80 73.30
weighted score 57.81 33.42 42.40
One-rule baseline Direct 72.60 67.50 70.00
Reprise 79.30 80.50 79.90
Clarification 100 64.90 78.70
weighted score 73.61 71.36 72.73
In this case the feature with the minimum error chosen by the one-rule classifier is
sluice. The classifier produces the one-rule tree in Figure 1. The branches of the tree
correspond to the sluice types; the interpretation with the highest probability for each
type of sluice is then predicted.
By using the feature sluice the one-rule tree implements the correlations between
sluice type and preferred interpretation that were discussed in Section 3.1.3. There, we
pointed out that these correlations were statistically significant. We can see now that
they are indeed a good rough guide for predicting sluice readings. As shown in Table 6,
the one-rule baseline dependent on the distribution patterns of the different sluice types
yields a 72.73% weighted F-score.
All results reported (here and in the remainder of the article) were obtained by
performing 10-fold cross-validation. They are presented as follows: The tables show
the recall, precision, and F-measure for each class. To calculate the overall performance
of the algorithm, these scores are normalized according to the relative frequency of
each class. This is done by multiplying each score by the total of instances of the
corresponding class and then dividing by the total number of datapoints in the data
set. The weighted overall recall, precision, and F-measure, shown in boldface for each
baseline in Table 6, is then the sum of the corresponding weighted scores. For each of
the baselines, the sluice readings not shown in the table obtain null scores.
3.2.4 ML Results. Finally, the four machine learning algorithms were run on the data
set annotated with the 11 features. Here, as well as in the more extensive experiment
we will present in Section 4, we use the following parameter settings with each of the
learners. Weka?s J4.8 decision tree learner is run using the default parameter settings.
With SLIPPER we use the option unordered, which finds a rule set that separates each
class from the remaining classes using growing and pruning techniques and in our case
yields slightly better results than the default setting. As for TiMBL, we run it using the
modified value difference metric (which performs better than the default overlap metric),
and keep the default settings for the number of nearest neighbors (k = 1) and feature
weighting method (gain ratio). Finally, with MaxEnt we use 40 iterations of the default
L-BFGS parameter estimation (Malouf 2002).
Overall, in this pilot study we obtain results of around 80% weighted F-score,
although there are some significant differences amongst the learners. MaxEnt gives the
lowest score (73.24% weighted F-score)?hardly over the one-rule baseline, and more
than 8 points lower than the best results, obtained with Weka?s J4.8 (81.80% weighted
F-score). The size of the data set seems to play a role in these differences, indicating that
MaxEnt does not perform so well with small data sets. A summary of weighted F-scores
is given in Table 7.
410
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 7
Comparison of weighted F-scores.
System Weighted F-score
Majority class baseline 42.40
One rule baseline 72.73
MaxEnt 73.24
TiMBL 79.80
SLIPPER 81.62
J4.8 81.80
Detailed recall, precision, and F-measure results for each learner are shown in
Appendix A. The results yielded by MaxEnt are almost equivalent to the ones achieved
with the one-rule baseline. With the other three learners, the use of contextual features
improves the results for Reprise and Direct by around 5 points each with respect to the
one-rule baseline. The results obtained with the one-rule baseline for the Clarification
reading, however, are hardly improved upon by any of the learners. In the case of
TiMBL the score is in fact lower?72.16 versus 78.70 weighted F-score. This leads us to
conclude that the best strategy is to interpret al what sluices as conveying a Clarification
reading.
The class Wh-anaphora, which, not being the majority interpretation for any sluice
type, was not predicted by the one-rule baseline nor by MaxEnt, now gives positive
results with the other three learners. The best result for this class is obtained with Weka?s
J4.8: 80% F-score.
The decision tree generated by Weka?s J4.8 algorithm is displayed in Figure 2. The
root of the tree corresponds to the feature wh, which makes a first distinction between
Figure 2
Weka?s J4.8 tree.
411
Computational Linguistics Volume 33, Number 3
Wh-anaphor and the other readings. If the value of this feature is yes, the class Wh-
anaphor is predicted. A negative value for this feature leads to the feature sluice. The
class with the highest probability is the only clue used to predict the interpretation
of the sluice types what, where, which, and whichN in a way parallel to the one-rule
baseline. Additional features are used for when, why, and who. A Direct reading is
predicted for a when sluice if there is no overt antecedent expression, whereas a Reprise
reading is preferred if the feature overt takes as value yes. For why sluices the mood
of the antecedent utterance is used to disambiguate between Reprise and Direct: If the
antecedent is declarative, the sluice is classified as Direct; if it is non-declarative it is
interpreted as Reprise. In the classification of who sluices three features are taken into
account: quant, pro, and proper n. The basic strategy is as follows: If the antecedent
utterance contains a quantifier and neither personal pronouns nor proper names appear,
the predicted class is Direct, otherwise the sluice is interpreted as Reprise.
3.2.5 Feature Contribution. Note that not all features are used in the tree generated by
Weka?s J4.8. The missing features are polarity, deictic, and def desc. Although they
don?t make any contribution to the model generated by the decision tree, examination
of the rules generated by SLIPPER shows that they are all used in the rule set induced by
this algorithm, albeit in rules with low confidence level. Despite the fact that SLIPPER
uses all features, the contribution of polarity, deictic, and def desc does not seem
to be very significant. When they are eliminated from the feature set, SLIPPER yields
very similar results to the ones obtained with the full set of features: 81.22% weighted F-
score versus the 81.66% obtained before. TiMBL on the other hand goes down a couple of
points, from 79.80% to 77.32% weighted F-score. No variation is observed with MaxEnt,
which seems to be using just the sluice type as a clue for classification.
4. Classifying the Full Range of NSUs
So far we have presented a study that has concentrated on fine-grained semantic dis-
tinctions of one of the classes in our taxonomy, namely Sluice, and have obtained very
encouraging results?around 80% weighted F-score (an improvement of 8 points over
a simple one-rule baseline). In this section we show that the ML approach taken can
be successfully extended to the task of classifying the full range of NSU classes in our
taxonomy.
We first present an experiment run on a restricted data set that excludes the classes
Plain Acknowledgement and Check Question, and then, in Section 4.6, report on a
follow-up experiment where all NSU classes are included.
4.1 Data
The data used in the experiments was selected from the corpus of NSUs following some
simplifying restrictions. Firstly, we leave aside the 16 instances classified as Other in the
corpus study (see Table 1). Secondly, we restrict the experiments to those NSUs whose
antecedent is the immediately preceding utterance. This restriction, which makes the
feature annotation task easier, does not pose a significant coverage problem, given that
the immediately preceding utterance is the antecedent for the vast majority of NSUs
(88%). The set of all NSUs, excluding those classified as Other, whose antecedent is the
immediately preceding utterance, contains a total of 1123 datapoints. See Table 8.
412
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 8
NSU subcorpus.
NSU class Total
Plain Acknowledgment 582
Short Answer 105
Affirmative Answer 100
Repeated Acknowledgment 80
CE 66
Rejection 48
Repeated Affirmative Answer 25
Factual Modifier 23
Sluice 20
Helpful Rejection 18
Filler 16
Check Question 15
Bare Modifier Phrase 10
Propositional Modifier 10
Conjunct 5
Total data set 1,123
Finally, as mentioned previously, the last restriction adopted concerns the instances
classified as Plain Acknowledgment and Check Question. Taking the risk of end-
ing up with a considerably smaller data set, we decided to leave aside these meta-
communicative NSU classes given that (1) plain acknowledgments make up more than
50% of the subcorpus leading to a data set with very skewed distributions; (2) check
questions are realized by the same kind of expressions as plain acknowledgments (okay,
right, etc.) and would presumably be captured by the same feature; and (3) a priori these
two classes seem two of the easiest types to identify (a hypothesis that was confirmed
after a second experiment?see Section 4.6). We therefore exclude plain acknowledg-
ments and check questions and concentrate on a more interesting and less skewed data
set containing all remaining NSU classes. This makes up a total of 526 data points
(1123 ? 582 ? 15). In Subsection 4.6 we shall compare the results obtained using this
restricted data set with those of a second experiment in which plain acknowledgements
and check questions are incorporated.
4.2 Features
A small set of features that capture the contextual properties that are relevant for
NSU classification was identified. In particular three types of properties that play an
important role in the classification task were singled out. The first one has to do with
semantic, syntactic, and lexical properties of the NSUs themselves. The second one
refers to the properties of its antecedent utterance. The third concerns relations between
the antecedent and the fragment. Table 9 shows an overview of the nine features used.
4.2.1 NSU Features. A set of four features are related to properties of the NSUs. These
are nsu cont, wh nsu, aff neg, and lex. The feature nsu cont is intended to distin-
guish between question-denoting (q value) and proposition-denoting (p value) NSUs.
The feature wh nsu encodes the presence of a wh-phrase in the NSU?it is primarily
introduced to identify Sluices. The features aff neg and lex signal the appearance of
413
Computational Linguistics Volume 33, Number 3
Table 9
NSU features and values.
Feature Description Values
nsu cont content of the NSU (either prop or question) p, q
wh nsu presence of a wh word in the NSU yes, no
aff neg presence of a yes/no word in the NSU yes, no, e(mpty)
lex presence of different lexical items in the NSU p mod, f mod, mod, conj, e
ant mood mood of the antecedent utterance decl, n decl
wh ant presence of a wh word in the antecedent yes, no
finished (un)finished antecedent fin, unf
repeat repeated words in NSU and antecedent 0-3
parallel repeated tag sequences in NSU and antecedent 0-3
particular lexical items. They include a value e(mpty) which allows us to encode the
absence of the relevant lexical items as well. The values of the feature aff neg indicate
the presence of either a yes or a no word in the NSU. The values of lex are invoked by the
appearance of modal adverbs (p mod), factual adjectives (f mod), and prepositions (mod)
and conjunctions (conj) in initial positions. These features are expected to be crucial to
the identification of Plain/Repeated Affirmative Answer and Plain/Helpful Rejection
on the one hand, and Propositional Modifiers, Factual Modifiers, Bare Modifier Phrases,
and Conjuncts on the other.
Note that the feature lex could be split into four binary features, one for each of its
non-empty values. This option, however, leads to virtually the same results. Hence, we
opt for a more compact set of features. This also applies to the feature aff neg.
4.2.2 Antecedent Features. We use the features ant mood, wh ant, and finished to encode
properties of the antecedent utterance. The first of these features distinguishes between
declarative and non-declarative antecedents. The feature wh ant signals the presence
of a wh-phrase in the antecedent utterance, which seems to be the best cue for classi-
fying Short Answers. As for the feature finished, it should help the learners identify
Fillers. The value unf is invoked when the antecedent utterance has a hesitant ending
(indicated, for instance, by a pause) or when there is no punctuation mark signalling a
finished utterance.
4.2.3 Similarity Features. The last two features, repeat and parallel, encode similarity
relations between the NSU and its antecedent utterance. They are the only numer-
ical features in the feature set. The feature repeat, which indicates the appearance
of repeated words between NSU and antecedent, is introduced as a clue to identify
Repeated Affirmative Answers and Repeated Acknowledgments. The feature parallel,
on the other hand, is intended to capture the particular parallelism exhibited by Helpful
Rejections. It signals the presence of sequences of POS tags common to the NSU and its
antecedent.
As in the sluicing experiment, all features were extracted automatically from the
POS information encoded in the BNC mark-up. However, as with the feature mood
in the sluicing study, some features like nsu cont and ant mood are high level features
that do not have straightforward correlates in POS tags. Punctuation tags (that would
correspond to intonation patterns in spoken input) help to extract the values of these
features, but the correspondence is still not unique. For this reason the automatic
414
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Figure 3
One-rule tree.
feature annotation procedure was again evaluated against a small sample of manually
annotated data. The feature values were extracted manually for 52 instances (?10% of
the total) randomly selected from the data set. In comparison with this gold standard,
the automatic feature annotation procedure achieves 89% accuracy. Only automatically
annotated data is used for the learning experiments.
4.3 Baselines
We now turn to examine some baseline systems that will help us to evaluate the
classification task. As before, the simplest baseline we can consider is a majority class
baseline that always predicts the class with the highest probability in the data set. In
the restricted data set used for the first experiment, this is the class Short Answer. The
majority class baseline yields a 6.7% weighted F-score.
When a one-rule classifier is run, we see that the feature that yields the minimum
error is aff neg. The one-rule baseline produces the one-rule decision tree in Fig-
ure 3, which yields a 32.5% weighted F-score (see Table 10). Plain Affirmative Answer
is the class predicted when the NSU contains a yes-word, Rejection when it contains a
no-word, and Short Answer otherwise.
Finally, we consider a more substantial baseline that uses the four NSU features.
Running Weka?s J4.8 decision tree classifier with these features creates a decision tree
with four rules, one for each feature used. The tree is shown in Figure 4.
Table 10
Baselines? results.
NSU Class Recall Precision F1
Majority class baseline ShortAns 100.00 20.10 33.50
weighted score 19.92 4.00 6.67
One-rule baseline ShortAns 95.30 30.10 45.80
AffAns 93.00 75.60 83.40
Reject 100.00 69.60 82.10
weighted score 45.93 26.73 32.50
Four-rule baseline CE 96.97 96.97 96.97
Sluice 100.00 95.24 97.56
ShortAns 94.34 47.39 63.09
AffAns 93.00 81.58 86.92
Reject 100.00 75.00 85.71
PropMod 100.00 100.00 100.00
FactMod 100.00 100.00 100.00
BareModPh 80.00 72.73 76.19
Conjunct 100.00 71.43 83.33
weighted score 70.40 55.92 62.33
415
Computational Linguistics Volume 33, Number 3
Figure 4
Four-rule tree.
The root of the tree corresponds to the feature nsu cont. It makes a first distinction
between question-denoting (q branch) and proposition-denoting NSUs (p branch). Not
surprisingly, within the q branch the feature wh nsu is used to distinguish between
Sluice and CE. The feature lex is the first node in the p branch. Its different values
capture the classes Conjunct, Propositional Modifier, Factual Modifier, and Bare Modi-
fier Phrase. The e(mpty) value for this feature takes us to the last, most embedded node
of the tree, realized by the feature aff neg, which creates a sub-tree parallel to the one-
rule tree in Figure 3. This four-rule baseline yields a 62.33% weighted F-score. Detailed
results for the three baselines considered are shown in Table 10.
4.4 Feature Contribution
As can be seen in Table 10, the classes Sluice, CE, Propositional Modifier, and Factual
Modifier achieve very high F-scores with the four-rule baseline?between 97% and
100%. These results are not improved upon by incorporating additional features nor
by using more sophisticated learners, which indicates that NSU features are sufficient
indicators to classify these NSU classes. This is in fact not surprising, given that the
disambiguation of Sluice, Propositional Modifier, and Factual Modifier is tied to the
presence of particular lexical items that are relatively easy to identify (wh-phrases and
certain adverbs and adjectives), whereas CE acts as a default category within question-
denoting NSUs.
There are, however, four NSU classes that are not predicted at all when only NSU
features are used. These are Repeated Affirmative Answer, Helpful Rejection, Repeated
Acknowledgment, and Filler. Because they are not associated with any leaf in the
tree, they yield null scores and therefore don?t appear in Table 10. Examination of
the confusion matrices shows that around 50% of Repeated Affirmative Answers were
classified as Plain Affirmative Answers, whereas the remaining 50%?as well as the
overwhelming majority of the other three classes just mentioned?were classified as
Short Answer. Acting as the default class, Short Answers achieves the lowest score:
63.09% F-score.
In order to determine the contribution of the antecedent features (ant mood, wh ant,
finished), as a next step these were added to the NSU features used in the four-
rule tree. When the antecedent features are incorporated, two additional NSU classes
are predicted. These are Repeated Acknowledgment and Filler, which achieve rather
416
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Figure 5
Node on a tree using NSU and antecedent features.
positive results: 74.8% and 64% F-score, respectively. We do not show the full results
obtained when NSU and antecedent features are used together. Besides the addition
of these two NSU classes, the results are very similar to those achieved with just NSU
features. The tree obtained when the antecedent features are incorporated to the NSU
features can be derived by substituting for the last node in the tree in Figure 4 the
tree in Figure 5. As can be seen in Figure 5, the features ant mood and finished con-
tribute to distinguish Repeated Acknowledgment and Filler from Short Answer, whose
F-score consequently rises, from 63.09% to 79%, due to an improvement in precision.
Interestingly, the feature wh ant does not have any contribution at this stage (although
it will be used by the learners when the similarity features are added). The general
weighted F-score obtained when NSU and antecedent features are combined is 77.87%.
A comparison of all weighted F-scores obtained will be shown in the next section, in
Table 11.
The use of NSU features and antecedent features is clearly not enough to account
for Repeated Affirmative Answer and Helpful Rejection, which obtain null scores.
4.5 ML Results
In this section we report the results obtained when the similarity features are included,
thereby using the full feature set, and the four machine learning algorithms are trained
on the data.
Although the classification algorithms implement different machine learning tech-
niques, they all yield very similar results: around an 87% weighted F-score. The max-
imum entropy model performs best, although the difference between its results and
Table 11
Comparison of weighted F-scores.
System Weighted F-score
Majority class baseline 6.67
One rule baseline 32.50
Four rule baseline (NSU features) 62.33
NSU and antecedent features 77.83
Full feature set:
- SLIPPER 86.35
- TiMBL 86.66
- J4.8 87.29
- MaxEnt 87.75
417
Computational Linguistics Volume 33, Number 3
those of the other algorithms is not statistically significant. Detailed recall, precision,
and F-measure scores are shown in Appendix B.
As seen in previous sections, the four-rule baseline algorithm that uses only NSU
features yields a 62.33% weighted F-score, whereas the incorporation of antecedent
features yields a 77.83% weighted F-score. The best result, the 87.75% weighted F-score
obtained with the maximal entropy model using all features, shows a 10% improvement
over this last result. As promised, a comparison of the scores obtained with the different
baselines considered and all learners used is given in Table 11.
Short Answers achieve high recall scores with the baseline systems (more than
90%). In the three baselines considered, Short Answer acts as the default category.
Therefore, even though the recall is high (given that Short Answer is the class with
the highest probability), precision tends to be quite low. The precision achieved for
Short Answer when only NSU features are used is ?47%. When antecedent features
are incorporated precision goes up to ?72%. Finally, the addition of similarity features
raises the precision for this class to ?82%. Thus, by using features that help to identify
other categories with the machine learners, the precision for Short Answers is improved
by around 36%, and the precision of the overall classification system by almost 33%:
from 55.90% weighted precision obtained with the four-rule baseline, to the 88.41%
achieved with the maximum entropy model using all features.
With the addition of the similarity features (repeat and parallel), the classes
Repeated Affirmative Answer and Helpful Rejection are predicted by the learners.
Although this contributes to the improvement of precision for Short Answer, the scores
yielded by these two categories are lower than the ones achieved with other classes. Re-
peated Affirmative Answer achieves nevertheless decent F-score, ranging from 56.96%
with SLIPPER to 67.20% with MaxEnt. The feature wh ant, for instance, is used to
distinguish Short Answer from Repeated Affirmative Answer. Figure 6 shows one of
the sub-trees generated by the feature repeat when Weka?s J4.8 is used with the full
feature set.
The class with the lowest scores is clearly Helpful Rejection. TiMBL achieves a
39.92% F-score for this class. The maximal entropy model, however, yields only a 10.37%
F-score. Examination of the confusion matrices shows that ?27% of Help Rejections
were classified as Rejection, ?26% as Short Answer, and ?15% as Repeated Acknowl-
edgement. This indicates that the feature parallel, introduced to identify this type of
NSUs, is not a good enough cue.
Figure 6
Node on a tree using the full feature set.
418
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Figure 7
One-rule tree.
4.6 Incorporating Plain Acknowledgment and Check Question
As explained in Section 4.1, the data set used in the experiments reported in the previous
section excluded the instances classified as Plain Acknowledgment and Check Question
in the corpus study. The fact that Plain Acknowledgment is the category with the highest
probability in the subcorpus (making up more than 50% of our total data set?see
Table 8), and that it does not seem particularly difficult to identify could affect the
performance of the learners by inflating the results. Therefore it was left out in order
to work with a more balanced data set and to minimize the potential for misleading
results. As the expressions used in plain acknowledgments and check questions are
very similar and they would in principle be captured by the same feature values, check
questions were left out as well. In a second phase the instances classified as Plain
Acknowledgment and Check Question were incorporated to measure their effect on
the results. In this section we discuss the results obtained and compare them with the
ones achieved in the initial experiment.
To generate the annotated data set an additional value ack was added to the feature
aff neg. This value is invoked to encode the presence of expressions typically used in
plain acknowledgments and/or check questions (mhm, right, okay, etc.). The total data
set (1,123 data points) was automatically annotated with the features modified in this
way, and the machine learners were then run on the annotated data.
4.6.1 Baselines. Given the high probability of Plain Acknowledgment, a simple majority
class baseline gives relatively high results: 35.31% weighted F-score. The feature with
the minimum error used to derive the one-rule baseline is again aff neg, this time with
the new value ack as part of its possible values (see Figure 7). The one-rule baseline
yields a weighted F-score of 54.26%.
The four-rule tree that uses only NSU features goes up to a weighted F-score
of 67.99%. In this tree the feature aff neg is now also used to distinguish between
CE and Check Question. Figure 8 shows the q branch of the tree. As the last node of
Figure 8
Node on the four-rule tree.
419
Computational Linguistics Volume 33, Number 3
the four-rule tree now corresponds to the tree in Figure 7, the class Plain Affirmative
Answer is not predicted when only NSU features are used.
When antecedent features are incorporated, Plain Affirmative Answers, Repeated
Acknowledgments, and Fillers are predicted, obtaining very similar scores to the ones
achieved in the experiment with the restricted data set. The feature ant mood is now
also used to distinguish between Plain Acknowledgment and Plain Affirmative Answer.
The last node in the tree is shown in Figure 9. The combined use of NSU features and
antecedent features yields a weighted F-score of 85.44%.
4.6.2 ML Results. As in the previous experiment, when all features are used the results
obtained are very similar across learners (around 92% weighted F-score), if slightly
lower with Weka?s J4.8 (89.53%). Detailed scores for each class are shown in Appen-
dix C. As expected, the class Plain Acknowledgment obtains a high F-score (?95% with
all learners). The F-score for Check Question ranges from 73% yielded by MaxEnt to
90% obtained with SLIPPER. The high score of Plain Acknowledgment combined with
its high probability raises the overall performance of the systems almost four points
over the results obtained in the previous experiment: from ?87% to ?92% weighted
F-score. The improvement with respect to the baselines, however, is not as large: we
now obtain a 55% improvement over the simple majority class baseline (from 35.31% to
92.21%), whereas in the experiment with the restricted data set the improvement with
respect to the majority class baseline is 81% (from 6.67% to 87.75% weighted F-score.).
Table 12 shows a comparison of all weighted F-scores obtained in this second
experiment.
It is interesting to note that even though the overall performance of the algorithms
is slightly higher than before (due to the reasons mentioned previously), the scores for
some NSU classes are actually lower. The most striking cases are perhaps the classes
Helpful Rejection and Conjunct, for which the maximum entropy model now gives null
scores (see Appendix C). We have already pointed out the problems encountered with
Helpful Rejection. As for the class Conjunct, although it yields good results with the
other learners, the proportion of this class (0.4%, 5 instances only) is now probably too
low to obtain reliable results.
A more interesting case is the class Affirmative Answer, which in TiMBL goes down
more than 10 points (from 93.61% to 82.42% F-score). The tree in Figure 7 provides a
clue to the reason for this. When the NSU contains a yes-word (second branch of the
tree) the class with the highest probability is now Plain Acknowledgment, instead of
Plain Affirmative Answer as before (see tree in Figure 3). This is due to the fact that,
Figure 9
Node on a tree using NSU and antecedent features.
420
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 12
Comparison of weighted F-scores.
System Weighted F-score
Majority class baseline 35.31
One rule baseline 53.03
Four rule baseline (NSU features) 67.99
NSU and antecedent features 85.44
Full feature set:
- J4.8 89.53
- SLIPPER 92.01
- TiMBL 92.02
- MaxEnt 92.21
at least in English, expressions like yeah (considered here as yes-words) are potentially
ambiguous between acknowledgments and affirmative answers.8 This ambiguity and
the problems it entails are also noted by Schlangen (2005), who addresses the problem
of identifying NSUs automatically. As he points out, the ambiguity of yes-words is
one of the difficulties encountered when trying to distinguish between backchannels
(plain acknowledgments in our taxonomy) and non-backchannel fragments. This is a
tricky problem for Schlangen as his NSU identification procedure does not have access
to the context. Although in the present experiments we do use features that capture
contextual information, determining whether the antecedent utterance is declarative or
interrogative (which one would expect to be the best clue to disambiguate between Plain
Acknowledgement and Plain Affirmative Answer) is not always trivial.
5. Conclusions
In this article we have presented results of several machine learning experiments where
we have used well-known machine learning techniques to address the novel task of
classifying NSUs in dialogue.
We first introduced a comprehensive NSU taxonomy based on corpus work carried
out using the dialogue transcripts of the BNC, and then sketched the approach to NSU
resolution we assume.
We then presented a pilot study focused on sluices, one of the NSU classes in our
taxonomy. We analyzed different sluice interpretations and their distributions in a small
corpus study and reported on a machine learning experiment that concentrated on
the task of disambiguating between sluice readings. This showed that the observed
correlations between sluice type and preferred interpretation are a good rough guide
for predicting sluice readings, which yields a 72% weighted F-score. Using a small set
of features that refer to properties of the antecedent utterance, we were able to improve
this result by 8%.
In the second part of this article we extended the machine learning approach used
in the sluicing experiment to the full range of NSU classes in our taxonomy. In order
to work with a more balanced set of data, the first run of this second experiment was
carried out using a restricted data set that excluded the classes Plain Acknowledgment
8 Arguably this ambiguity would not arise in French given that, according to Beyssade (2005), in French
the expressions used to acknowledge an assertion are different from those used in affirmative answers to
polar questions.
421
Computational Linguistics Volume 33, Number 3
and Check Question. We identified a small set of features that capture properties of
the NSUs, their antecedents and relations between them, and employed a series of
simple baseline methods to evaluate the classification task. The most successful of these
consists of a four-rule decision tree that only uses features related to properties of
the NSUs themselves. This gives a 62% weighted F-score. Not surprisingly, with this
baseline very high scores (over 95%) could be obtained for NSU classes that are defined
in terms of lexical or construction types, like Sluice and Propositional/Factual Modifier.
We then applied four learning algorithms to the data set annotated with all features
and improved the result of the four-rule baseline by 25%, obtaining a weighted F-score
of around 87% for all learners. The experiment showed that the classes that are most
difficult to identify are those that rely on relational features, like Repeated Affirmative
Answer and especially Helpful Rejection.
In a second run of the experiment we incorporated the instances classified as Plain
Acknowledgment and Check Question in the data set and ran the machine learners
again. The results achieved are very similar to those obtained in the previous run, if
slightly higher due to the high probability of the class Plain Acknowledgment. The
experiment did show however a potential confusion between Plain Acknowledgment
and Plain Affirmative Answer (observed elsewhere in the literature) that obviously had
not shown up in the previous run.
As typically different NSU classes are subjected to different resolution constraints,
identifying the correct NSU class is a necessary step towards the goal of fully processing
NSUs in dialogue. Our results show that, for the taxonomy we have considered, this task
can be successfully learned.
There are, however, several aspects that deserve further investigation. One of them
is the choice of features employed to characterize the utterances. In this case we have
opted for rather high-level features instead of using simple surface features, as is com-
mon in robust approaches to language understanding. As pointed out by an anonymous
reviewer, it would be worth exploring to what extent the performance of our current
approach could be improved by incorporating more low-level features, for instance by
the presence of closed-class function words.
Besides identifying the right NSU class, the processing and resolution of NSUs in-
volves other tasks that have not been addressed in this article and that are subjects of our
future research. For instance, we have abstracted here from the issue of distinguishing
NSUs from other sentential utterances. In our experiments the input fed to the learners
was in all cases a vector of features associated with an utterance that had already been
singled out as an NSU. Deciding whether an utterance is or is not an NSU is not an easy
task. This has for instance been addressed by Schlangen (2005), who obtains rather low
scores (42% F-measure). There is therefore a lot of room for improvement in this respect,
and indeed in the future we plan to explore ways of combining the classification task
addressed here with the NSU identification task.
Identifying and classifying NSUs are necessary conditions for resolving them. In
order to actually resolve them, however, the output of the classifier needs to be fed into
some extra module that takes care of this task. A route we plan to take in the future
is to integrate our classification techniques with the information state-based dialogue
system prototype CLARIE (Purver 2004a), which implements a procedure for NSU
resolution based on the theoretical assumptions sketched in Section 2.2. The taxonomy
which we have tested and presented here will provide the basis for classifying NSUs in
this dialogue processing system. The classification system will determine the templates
and procedures for interpretation that the system will apply to an NSU once it has
recognized its fragment type.
422
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Appendix A: Detailed ML Results for the Sluice Reading Classification Task
Learner Sluice Reading Recall Precision F1
Weka?s J4.8 Direct 71.70 79.20 75.20
Reprise 85.70 83.70 84.70
Clarification 100.00 68.60 81.40
Wh anaphor 66.70 100.00 80.00
weighted score 81.47 82.14 81.80
SLIPPER Direct 81.01 71.99 76.23
Reprise 83.85 86.49 85.15
Clarification 71.17 94.17 81.07
Wh anaphor 77.78 62.96 69.59
weighted score 81.81 81.43 81.62
TiMBL Direct 78.72 75.24 76.94
Reprise 83.08 83.96 83.52
Clarification 75.83 68.83 72.16
Wh anaphor 55.56 77.78 64.81
weighted score 79.85 79.98 79.80
MaxEnt Direct 65.22 75.56 70.01
Reprise 85.74 76.38 80.79
Clarification 89.17 70.33 78.64
Wh anaphor 0.00 0.00 0.00
weighted score 75.38 76.93 73.24
423
Computational Linguistics Volume 33, Number 3
Appendix B: Detailed ML Results for the Restricted NSU Classification Task
Weka?s J4.8 SLIPPER
NSU Class Recall Precision F1 Recall Precision F1
CE 97.00 97.00 97.00 93.64 97.22 95.40
Sluice 100.00 95.20 97.60 96.67 91.67 94.10
ShortAns 89.60 82.60 86.00 83.93 82.91 83.41
AffAns 92.00 95.80 93.90 93.13 91.63 92.38
Reject 95.80 80.70 87.60 83.60 100.00 91.06
RepAffAns 68.00 63.00 65.40 53.33 61.11 56.96
RepAck 85.00 89.50 87.20 85.71 89.63 87.62
HelpReject 22.20 33.30 26.70 28.12 20.83 23.94
PropMod 100.00 100.00 100.00 100.00 90.00 94.74
FactMod 100.00 100.00 100.00 100.00 100.00 100.00
BareModPh 80.00 100.00 88.90 100.00 80.56 89.23
ConjFrag 100.00 71.40 83.30 100.00 100.00 100.00
Filler 56.30 100.00 72.00 100.00 62.50 76.92
weighted score 87.62 87.68 87.29 86.21 86.49 86.35
TiMBL MaxEnt
NSU Class Recall Precision F1 Recall Precision F1
CE 94.37 91.99 93.16 96.11 96.39 96.25
Sluice 94.17 91.67 92.90 100.00 95.83 97.87
ShortAns 88.21 83.00 85.52 89.35 83.59 86.37
AffAns 92.54 94.72 93.62 92.79 97.00 94.85
Reject 95.24 81.99 88.12 100.00 81.13 89.58
RepAffAns 63.89 60.19 61.98 68.52 65.93 67.20
RepAck 86.85 91.09 88.92 84.52 81.99 83.24
HelpReject 35.71 45.24 39.92 5.56 77.78 10.37
PropMod 90.00 100.00 94.74 100.00 100.00 100.00
FactMod 97.22 100.00 98.59 97.50 100.00 98.73
BareModPh 80.56 100.00 89.23 69.44 100.00 81.97
ConjFrag 100.00 100.00 100.00 100.00 100.00 100.00
Filler 48.61 91.67 63.53 62.50 90.62 73.98
weighted score 86.71 87.25 86.66 87.11 88.41 87.75
424
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Appendix C: Detailed ML Results for the Full NSU Classification Task
Weka?s J4.8 SLIPPER
NSU Class Recall Precision F1 Recall Precision F1
Ack 95.00 96.80 95.90 96.67 95.71 96.19
CheckQu 100.00 83.30 90.90 86.67 100.00 92.86
CE 92.40 95.30 93.80 96.33 93.75 95.02
Sluice 100.00 95.20 97.60 94.44 100.00 97.14
ShortAns 83.00 80.70 81.90 85.25 84.46 84.85
AffAns 86.00 82.70 84.30 82.79 87.38 85.03
Reject 100.00 76.20 86.50 77.60 100.00 87.39
RepAffAns 68.00 65.40 66.70 67.71 72.71 70.12
RepAck 86.30 84.10 85.20 84.04 92.19 87.93
HelpReject 33.30 46.20 38.70 29.63 18.52 22.79
PropMod 60.00 100.00 75.00 100.00 100.00 100.00
FactMod 91.30 100.00 95.50 100.00 100.00 100.00
BareModPh 70.00 100.00 82.40 83.33 69.44 75.76
ConjFrag 100.00 71.40 83.30 100.00 100.00 100.00
Filler 37.50 50.00 42.90 70.00 56.33 62.43
weighted score 89.67 89.78 89.53 91.57 92.70 92.01
TiMBL MaxEnt
NSU Class Recall Precision F1 Recall Precision F1
Ack 95.71 95.58 95.64 95.54 94.59 95.06
CheckQu 77.78 71.85 74.70 63.89 85.19 73.02
CE 93.32 94.08 93.70 88.89 94.44 91.58
Sluice 100.00 94.44 97.14 88.89 94.44 91.58
ShortAns 87.79 88.83 88.31 88.46 84.91 86.65
AffAns 85.00 85.12 85.06 86.83 81.94 84.31
Reject 98.33 80.28 88.39 100.00 78.21 87.77
RepAffAns 58.70 55.93 57.28 69.26 62.28 65.58
RepAck 86.11 80.34 83.12 86.95 77.90 82.18
HelpReject 22.67 40.00 28.94 00.00 00.00 00.00
PropMod 100.00 100.00 100.00 44.44 100.00 61.54
FactMod 97.50 100.00 98.73 93.33 100.00 96.55
BareModPh 69.44 83.33 75.76 58.33 100.00 73.68
ConjFrag 100.00 100.00 100.00 00.00 00.00 00.00
Filler 44.33 55.00 49.09 62.59 100.00 76.99
weighted score 91.49 90.75 91.02 91.96 93.17 91.21
425
Computational Linguistics Volume 33, Number 3
Acknowledgments
This work was funded by grant
RES-000-23-0065 from the Economic and
Social Council of the United Kingdom and it
was undertaken while all three authors were
members of the Department of Computer
Science at King?s College London. We wish
to thank Lief Arda Nielsen and Matthew
Purver for useful discussion and suggestions
regarding machine learning algorithms. We
are grateful to two anonymous reviewers for
very helpful comments on an earlier draft of
this article. Their insights and suggestions
have resulted in numerous improvements.
Of course we remain solely responsible for
the ideas presented here, and for any errors
that may remain.
References
Beyssade, Claire and Jean-Marie Marandin.
2005. Contour meaning and dialogue
structure. Paper presented at the
workshop Dialogue Modelling and
Grammar, Paris, France.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus (World Edition).
Oxford University Computing Services.
Available from ftp:
//sable.ox.ac.uk/pub/ota/BNC/.
Clark, Herbert H. 1996. Using Language.
Cambridge University Press, Cambridge.
Cohen, William and Yoram Singer. 1999.
A simple, fast, and effective rule learner.
In Proceedings of the 16th National
Conference on Artificial Intelligence,
pages 335?342, Orlando, FL.
Daelemans, Walter, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 2003.
TiMBL: Tilburg Memory-Based Learner,
v. 5.0, Reference Guide. Technical
Report ILK-0310, University of
Tilburg.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Ferna?ndez, Raquel. 2006. Non-Sentential
Utterances in Dialogue: Classification,
Resolution and Use. Ph.D. thesis,
Department of Computer Science, King?s
College London, University of London.
Ferna?ndez, Raquel and Jonathan Ginzburg.
2002. Non-sentential utterances: A corpus
study. Traitement automatique des languages,
43(2):13?42.
Ferna?ndez, Raquel, Jonathan Ginzburg,
Howard Gregory, and Shalom Lappin.
In press. SHARDS: Fragment resolution
in dialogue. In H. Bunt and R. Muskens,
editors, Computing Meaning, volume 3.
Kluwer, Amsterdam.
Ferna?ndez, Raquel, Jonathan Ginzburg, and
Shalom Lappin. 2004. Classifying Ellipsis
in Dialogue: A Machine Learning
Approach. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 240?246, Geneva,
Switzerland.
Garside, Roger. 1987. The CLAWS
word-tagging system. In R. Garside,
G. Leech, and G. Sampson, editors, The
Computational Analysis of English: A
Corpus-Based Approach. Longman, Harlow,
pages 30?41.
Ginzburg, Jonathan. 1996. Interrogatives:
Questions, facts, and dialogue. In Shalom
Lappin, editor, Handbook of Contemporary
Semantic Theory. Blackwell, Oxford,
pages 385?422.
Ginzburg, Jonathan. 1999. Ellipsis resolution
with syntactic presuppositions. In
H. Bunt and R. Muskens, editors,
Computing Meaning: Current Issues in
Computational Semantics. Kluwer,
Amsterdam, pages 255?279.
Ginzburg, Jonathan. 2005. Abstraction and
ontology: Questions as propositional
abstracts in type theory with records.
Journal of Logic and Computation,
2(15):113?118.
Ginzburg, Jonathan. forthcoming. Semantics
and Interaction in Dialogue. CSLI
Publications and University of Chicago
Press, Stanford, California. Draft chapters
available from http://www.dcs.kcl.
ac.uk/staff/ginzburg.
Ginzburg, Jonathan and Robin Cooper.
2004. Clarification, ellipsis, and the nature
of contextual updates. Linguistics and
Philosophy, 27(3):297?366.
Ginzburg, Jonathan, Howard Gregory,
and Shalom Lappin. 2001. SHARDS:
Fragment resolution in dialogue. In
H. Bunt, I. van der Suis, and E. Thijsse,
editors, Proceedings of the Fourth
International Workshop on Computational
Semantics, pages 156?172, Tilburg,
The Netherlands.
Ginzburg, Jonathan and Ivan Sag. 2001.
Interrogative Investigations. CSLI
Publications, Stanford, California.
Larsson, Staffan. 2002. Issue-based Dialogue
Management. Ph.D. thesis, Go?teborg
University, Sweden.
Larsson, Staffan, Peter Ljunglo?f, Robin
Cooper, Elisabet Engdahl, and Stina
Ericsson. 2000. GoDiS: An accommodating
dialogue system. In Proceedings of
426
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
ANLP/NAACL-2000 Workshop on
Conversational Systems, pages 7?10,
Seattle, WA.
Le, Zhang. 2003. Maximum entropy
modeling toolkit for Python and
C++. Available from http://
homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
Leech, Geoffrey, Roger Garside, and
Michael Bryant. 1994. The large-scale
grammatical tagging of text: Experience
with the British National Corpus. In
N. Oostdijk and P. de Haan, editors,
Corpus-Based Research into Language.
Rodopi, Amsterdam, pages 47?63.
Malouf, Robert. 2002. A comparision of
algorithm for maximum entropy
parameter estimation. In Proceedings
of the Sixth Conference on Natural
Language Learning, pages 49?55, Taipei,
Taiwan.
Purver, Matthew. 2001. SCoRE: A tool for
searching the BNC. Technical Report
TR-01-07, Department of Computer
Science, King?s College London.
Purver, Matthew. 2004a. CLARIE: The
Clarification Engine. In J. Ginzburg and
E. Vallduv??, editors, Proceedings of the 8th
Workshop on the Semantics and Pragmatics of
Dialogue (Catalog), pages 77?84, Barcelona,
Spain.
Purver, Matthew. 2004b. The Theory and
Use of Clarification Requests in Dialogue.
Ph.D. thesis, King?s College, University
of London.
Schlangen, David. 2003. A Coherence-Based
Approach to the Interpretation of
Non-Sentential Utterances in Dialogue. Ph.D.
thesis, University of Edinburgh, Scotland.
Schlangen, David. 2005. Towards finding and
fixing fragments: Using ML to identify
non-sentential utterances and their
antecedents in multi-party dialogue. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 247?254, Ann Arbor, MI.
Schlangen, David and Alex Lascarides. 2003.
The interpretation of non-sentential
utterances in dialogue. In Proceedings of the
4th SIGdial Workshop on Discourse and
Dialogue, pages 62?71, Sapporo, Japan.
Traum, David. 1994. A Computational Theory
of Grounding in Natural Language
Conversation. Ph.D. thesis, University of
Rochester, Department of Computer
Science, Rochester, NY.
Witten, Ian H. and Eibe Frank. 2000.
Data Mining: Practical Machine
Learning Tools with Java Implementations.
Morgan Kaufmann, San Francisco.
Available from http://
www.cs.waikato.ac.nz/ml/weka.
427

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539?549,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Collective Annotation of Linguistic Resources:
Basic Principles and a Formal Model
Ulle Endriss and Raquel Ferna?ndez
Institute for Logic, Language & Computation
University of Amsterdam
{ulle.endriss|raquel.fernandez}@uva.nl
Abstract
Crowdsourcing, which offers new ways
of cheaply and quickly gathering large
amounts of information contributed by
volunteers online, has revolutionised the
collection of labelled data. Yet, to create
annotated linguistic resources from this
data, we face the challenge of having to
combine the judgements of a potentially
large group of annotators. In this paper
we investigate how to aggregate individual
annotations into a single collective anno-
tation, taking inspiration from the field of
social choice theory. We formulate a gen-
eral formal model for collective annotation
and propose several aggregation methods
that go beyond the commonly used major-
ity rule. We test some of our methods on
data from a crowdsourcing experiment on
textual entailment annotation.
1 Introduction
In recent years, the possibility to undertake large-
scale annotation projects with hundreds or thou-
sands of annotators has become a reality thanks to
online crowdsourcing methods such as Amazon?s
Mechanical Turk and Games with a Purpose. Al-
though these techniques open the door to a true
revolution for the creation of annotated corpora,
within the computational linguistics community
there so far is no clear understanding of how the
so-called ?wisdom of the crowds? could or should
be used to develop useful annotated linguistic re-
sources. Those who have looked into this increas-
ingly important issue have mostly concentrated on
validating the quality of multiple non-expert an-
notations in terms of how they compare to ex-
pert gold standards; but they have only used sim-
ple aggregation methods based on majority voting
to combine the judgments of individual annotators
(Snow et al, 2008; Venhuizen et al, 2013).
In this paper, we take a different perspective and
instead focus on investigating different aggrega-
tion methods for deriving a single collective an-
notation from a diverse set of judgments. For this
we draw inspiration from the field of social choice
theory, a theoretical framework for combining the
preferences or choices of several individuals into
a collective decision (Arrow et al, 2002). Our aim
is to explore the parallels between the task of ag-
gregating the preferences of the citizens participat-
ing in an election and the task of combining the
expertise of speakers taking part in an annotation
project. Our contribution consists in the formula-
tion of a general formal model for collective an-
notation and, in particular, the introduction of sev-
eral families of aggregation methods that go be-
yond the commonly used majority rule.
The remainder of this paper is organised as fol-
lows. In Section 2 we introduce some basic termi-
nology and argue that there are four natural forms
of collective annotation. We then focus on one of
them and present a formal model for it in Sec-
tion 3. We also formulate some basic principles
of aggregation within this model in the same sec-
tion. Section 4 introduces three families of ag-
gregation methods: bias-correcting majority rules,
greedy methods for identifying (near-)consensual
coalitions of annotators, and distance-based aggre-
gators. We test the former two families of aggrega-
tors, as well as the simple majority rule commonly
used in similar studies, in a case study on data ex-
tracted from a crowdsourcing experiment on tex-
tual entailment in Section 5. Section 6 discusses
related work and Section 7 concludes.
2 Four Types of Collective Annotation
An annotation task consists of a set of items, each
of which is associated with a set of possible cate-
gories (Artstein and Poesio, 2008). The categories
may be the same for all items or they may be item-
specific. For instance, dialogue act annotation
539
(Allen and Core, 1997; Carletta et al, 1997) and
word similarity rating (Miller and Charles, 1991;
Finkelstein et al, 2002) involve choosing from
amongst a set of categories?acts in a dialogue
act taxonomy or values on a scale, respectively?
which remains fixed for all items in the annotation
task. In contrast, in tasks such as word sense la-
belling (Kilgarriff and Palmer, 2000; Palmer et al,
2007; Venhuizen et al, 2013) and PP-attachment
annotation (Rosenthal et al, 2010; Jha et al, 2010)
coders need to choose a category amongst a set of
options specific to each item?the possible senses
of each word or the possible attachment points in
each sentence with a prepositional phrase.
In either case (one set of categories for all items
vs. item-specific sets of categories), annotators are
typically asked to identify, for each item, the cat-
egory they consider the best match. In addition,
they may be given the opportunity to indicate that
they cannot judge (the ?don?t know? or ?unclear?
category). For large-scale annotation projects run
over the Internet it is furthermore very likely that
an annotator will not be confronted with every sin-
gle item, and it makes sense to distinguish items
not seen by the annotator from items labelled as
?don?t know?. We refer to this form of annotation,
i.e., an annotation task where coders have the op-
tion to (i) label items with one of the available cat-
egories, to (ii) choose ?don?t know?, or to (iii) not
label an item at all, as plain annotation.
Plain annotation is the most common form of
annotation and it is the one we shall focus on in
this paper. However, other, more complex, forms
of annotation are also possible and of interest. For
instance, we may ask coders to rank the avail-
able categories (resulting in, say, a weak or par-
tial order over the categories); we may ask them to
provide a qualitative ratings of the available cat-
egories for each item (e.g., excellent match, good
match, etc.); or we may ask for quantitative rat-
ings (e.g., numbers from 1 to 100).1 We refer to
these forms of annotation as complex annotation.
We want to investigate how to aggregate the
information available for each item once annota-
tions by multiple annotators have been collected.
In line with the terminology used in social choice
theory and particularly judgment aggregation (Ar-
1Some authors have combined qualitative and quantitative
ratings; e.g., for the Graded Word Sense dataset of Erk et al
(2009) coders were asked to classify each relevant WordNet
sense for a given item on a 5-point scale: 1 completely differ-
ent, 2 mostly different, 3 similar, 4 very similar, 5 identical.
row, 1963; List and Pettit, 2002), let us call an ag-
gregation method independent if the outcome re-
garding a given item j only depends on the cate-
gories provided by the annotators regarding j it-
self (but not on, say, the categories assigned to a
different item j?). Independent aggregation meth-
ods are attractive due to their simplicity. They also
have some conceptual appeal: when deciding on
j maybe we should only concern ourselves with
what people have to say regarding j? On the other
hand, insisting on independence prevents us from
exploiting potentially useful information that cuts
across items. For instance, if a particular anno-
tator almost always chooses category c, then we
should maybe give less weight to her selecting c
for the item j at hand than when some other anno-
tator chooses c for j. This would call for methods
that do not respect independence, which we shall
refer to as general aggregation. Note that when
studying independent aggregation methods, with-
out loss of generality, we may assume that each
annotation task consists of just a single item.
In view of our discussion above, there are four
classes of approaches to collective annotation:
(1) Independent aggregation of plain annota-
tions. This is the simplest case, resulting in a
fairly limited design space. When, for a given
item, each annotator has to choose between
k categories (or abstain) and we do not per-
mit ourselves to use any other information,
then the only reasonable choice is to imple-
ment the plurality rule (Taylor, 2005), under
which the winning category is the category
chosen by the largest number of annotators.
In case there are exactly two categories avail-
able, the plurality rule is also called the ma-
jority rule. The only additional consideration
to make here (besides how to deal with ties)
is whether or not we may want to declare no
winner at all in case the plurality winner does
not win by a sufficiently significant margin or
does not make a particular quota. This is the
most common approach in the literature (see,
e.g., Venhuizen et al, 2013).
(2) Independent aggregation of complex annota-
tions. This is a natural generalisation of the
first approach, resulting in a wider range of
possible methods. We shall not explore it
here, but only point out that in case annotators
provide linear orders over categories, there is
a close resemblance to classical voting the-
540
ory (Taylor, 2005); in case only partial orders
can be elicited, recent work in computational
social choice on the generalisation of classi-
cal voting rules may prove helpful (Pini et al,
2009; Endriss et al, 2009); and in case an-
notators rate categories using qualitative ex-
pressions such as excellent match, the method
of majority judgment of Balinski and Laraki
(2011) should be considered.
(3) General aggregation of plain annotations.
This is the approach we shall discuss be-
low. It is related to voting in combinato-
rial domains studied in computational social
choice (Chevaleyre et al, 2008), and to both
binary aggregation (Dokow and Holzman,
2010; Grandi and Endriss, 2011) and judg-
ment aggregation (List and Pettit, 2002).
(4) General aggregation of complex annotations.
While appealing due to its great level of gen-
erality, this approach can only be tackled suc-
cessfully once approaches (2) and (3) are suf-
ficiently well understood.
3 Formal Model
Next we present our model for general aggregation
of plain annotations into a collective annotation.
3.1 Terminology and Notation
An annotation task is defined in terms of m items,
with each item j ? {1, . . . ,m} being associated
with a finite set of possible categories Cj . Anno-
tators are asked to provide an answer for each of
the items of the annotation task. In the context of
plain annotations, a valid answer for item j is an
element of the set Aj = Cj ? {?,?}.2 Here ?
represents the answer ?don?t know? and we use ?
to indicate that the annotator has not answered (or
even seen) the item at all. An annotation is a vec-
tor of answers by one annotator, one answer for
each item of the annotation task at hand, i.e., an
annotation is an element of the Cartesian product
A = A1 ? A2 ? ? ? ? ? Am. A typical element of
A will be denoted as A = (a1, . . . , am).
Let N = {1, . . . , n} be a finite set of n anno-
tators (or coders). A profile A = (A1, . . . , An) ?
An, for a given annotation task, is a vector of an-
notations, one for each annotator. That is, A is an
2As discussed earlier, in the context of complex annota-
tions, an answer could also be, say, a partial order on Cj or a
function associating elements of Cj with numerical ratings.
Item 1 Item 2 Item 3
Annotator 1 B A A
Annotator 2 B B B
Annotator 3 A B A
Majority B B A
Table 1: A profile with a collective annotation.
n?m-matrix; e.g., a3,7 is the answer that the 3rd
annotator provides for the 7th item.
We want to aggregate the information provided
by the annotators into a (single) collective anno-
tation. For the sake of simplicity, we use A also
as the domain of possible collective annotations
(even though the distinction between ? and?may
not be strictly needed here; they both indicate that
we do not want to commit to any particular cate-
gory). An aggregator is a function F : An ? A,
mapping any given profile into a collective annota-
tion, i.e., a labelling of the items in the annotation
task with corresponding categories (or ? or?). An
example is the plurality rule (also known as the
majority rule for binary tasks with |Cj | = 2 for
all items j), which annotates each item with the
category chosen most often.
Note that the collective annotation need not
coincide with any of the individual annotations.
Take, for example, a binary annotation task in
which three coders label three items with category
A or B as shown in Table 1. Here using the major-
ity rule to aggregate the annotations would result
in a collective annotation that does not fully match
any annotation by an individual coder.
3.2 Basic Properties
A typical task in social choice theory is to formu-
late axioms that formalise specific desirable prop-
erties of an aggregator F (Arrow et al, 2002). Be-
low we adapt three of the most basic axioms that
have been considered in the social choice litera-
ture to our setting and we briefly discuss their rel-
evance to collective annotation tasks.
We will require some additional notation: for
any profileA, item j, and possible answer a ? Aj ,
let NAj:a denote the set of annotators who chose
answer a for item j under profile A.
? F is anonymous if it treats coders symmetri-
cally, i.e., if for every permutation pi : N ? N ,
F (A1, . . . , An) = F (Api(1), . . . , Api(n)). In so-
cial choice theory, this is a fairness constraint.
For us, fairness per se is not a desideratum,
541
but when we do not have any a priori informa-
tion regarding the expertise of annotators, then
anonymity is a natural axiom to adopt.
? F is neutral if it treats all items symmetri-
cally, i.e., if for every two items j and j? with
the same set of possible categories (i.e., with
Cj = Cj?) and for every profile A, it is the case
that whenever NAj:a = NAj?:a for all answers
a ? Aj = Aj? , then F (A)j = F (A)j? . That
is, if the patterns of individual annotations of j
and j? are the same, then also their collective
annotation should coincide. In social choice
theory, neutrality is also considered a basic fair-
ness requirement (avoiding preferential treat-
ment one candidate in an election). In the con-
text of collective annotation there may be good
reasons to violate neutrality: e.g., we may use
an aggregator that assigns different default cat-
egories to different items and that can override
such a default decision only in the presence of
a significant majority (note that this is different
from anonymity: we will often not have any in-
formation on our annotators, but we may have
tangible information on items).3
? F is independent if the collective annotation of
any given item j only depends on the individual
annotations of j. Formally, F is independent if,
for every item j and every two profiles A and
A?, it is the case that wheneverNAj:a = NA?j:a for
all answers a ? Aj , then F (A)j = F (A?)j .
In social choice theory, independence is often
seen as a desirable albeit hard (or even impos-
sible) to achieve property (Arrow, 1963). For
collective annotation, we strongly believe that
it is not a desirable property: by considering
how annotators label other items we can learn
about their biases and we should try to exploit
this information to obtain the best possible an-
notation for the item at hand.
Note that the plurality/majority rule is indepen-
dent. All of the methods we shall propose in Sec-
tion 4 are both anonymous and neutral?except to
the extent to which we have to violate basic sym-
metry requirements in order to break ties between
categories chosen equally often for a given item.
None of our aggregators is independent.
3It would also be of interest to formulate a neutrality ax-
iom w.r.t. categories (rather than items). For two categories,
this idea has been discussed under the name of domain-
neutrality in the literature (Grandi and Endriss, 2011), but
for larger sets of categories it has not yet been explored.
Some annotation tasks might be subject to in-
tegrity constraints that determine the internal con-
sistency of an annotation. For example, if our
items are pairs of words and the possible cate-
gories include synonymous and antonymous, then
if item 1 is about words A and B, item 2 about
words B and C, and item 3 about words A and
C, then any annotation that labels items 1 and 2
as synonymous should not label item 3 as antony-
mous. Thus, a further desirable property that will
play a role for some annotation tasks is collective
rationality (Grandi and Endriss, 2011): if all in-
dividual annotations respect a given integrity con-
straint, then so should the collective annotation.
We can think of integrity constraints as impos-
ing top-down expert knowledge on an annotation.
However, for some annotation tasks, no integrity
constraints may be known to us in advance, even
though we may have reasons to believe that the
individual annotators do respect some such con-
straints. In that case, selecting one of the indi-
vidual annotations in the profile as the collective
annotation is the only way to ensure that these in-
tegrity constraints will be satisfied by the collec-
tive annotation (Grandi and Endriss, 2011). Of
course, to do so we would need to assume that
there is at least one annotator who has labelled all
items (and to be able to design a high-quality ag-
gregator in this way we should have a sufficiently
large number of such annotators to choose from),
which may not always be possible, particularly in
the context of crowdsourcing.
4 Three Families of Aggregators
In this section we instantiate our formal model by
proposing three families of methods for aggrega-
tion. Each of them is inspired, in part, by standard
approaches to desigining aggregation rules devel-
oped in social choice theory and, in part, by the
specific needs of collective annotation. Regard-
ing the latter point, we specifically emphasise the
fact that not all annotators can be expected to be
equally reliable (in general or w.r.t. certain items)
and we try to integrate the process of aggregation
with a process whereby less reliable annotators are
either given less weight or are excluded altogether.
4.1 Bias-Correcting Majority Rules
We first want to explore the following idea: If a
given annotator annotates most items with 0, then
we might want to assign less significance to that
542
choice for any particular item.4 That is, if an an-
notator appears to be biased towards a particular
category, then we might want to try to correct for
this bias during aggregation.
What follows applies only to annotation tasks
where every item is associated with the same set of
categories. For ease of exposition, let us further-
more assume that there are only two categories, 0
and 1, and that annotators do not make use of the
option to annotate with ? (?don?t know?).
For every annotator i ? N and every cate-
gory X ? {0, 1}, fix a weight wXi ? R. The
bias-correcting majority (BCM) rule for this fam-
ily of weights is defined as follows. Given profile
A, the collective category for item j will be 1 in
case?ai,j=1w1i >
?
ai,j=0w
0
i , and 0 otherwise.5
That is, we compute the overall weight for cate-
gory 1 by adding up the corresponding weights for
those coders that chose 1 for item j, and we do
accordingly for the overall weight for category 0;
finally, we choose as collective category that cate-
gory with the larger overall weight. Note that for
wXi ? 1 we obtain the simple majority rule.
Below we define three intuitively appealing
families of weights, and thereby three BCM rules.
However, before we do so, we first require some
additional notation. Fix a profile of annotations.
For X ? {0, 1}, let Freqi(X) denote the relative
frequency with which annotator i has chosen cat-
egory X . For instance, if i has annotated 20 items
and has chosen 1 in five cases, then Freqi(1) =
0.25. Similarly, let Freq(X) denote the frequency
of X across the entire profile.
Here are three ways of making the intuitive idea
of bias correction concrete:
(1) The complement-based BCM rule (ComBCM)
is defined by weights wXi = Freqi(1?X).
That is, the weight of annotator i for cate-
gory X is equal to her relative frequency of
having chosen the other category 1?X . For
example, if you annotate two items with 1 and
eight with 0, then each of your 1-annotations
will have weight 0.8, while each of your
0-annotations will only have weight 0.2.
(2) The difference-based BCM rule (DiffBCM) is
defined by weights wXi = 1 + Freq(X) ?
4A similar idea is at the heart of cumulative voting, which
requires a voter to distribute a fixed number of points amongst
the candidates (Glasser, 1959; Brams and Fishburn, 2002).
5For the sake of simplicity, our description here presup-
poses that ties are always broken in favour of 0. Other tie-
breaking rules (e.g., random tie-breaking) are possible.
Freqi(X). Recall that Freq(X) is the rela-
tive frequency ofX in the entire profile, while
Freqi(X) is the relative frequency of X in
the annotation of i. Hence, if i assigns cat-
egory X less often than the general popula-
tion, then her weight on X-choices will be in-
creased by the difference (and vice versa in
case she assigns X more often than the popu-
lation at large). For example, if you assign 1
in two out of ten cases, while in general cat-
egory 1 appears in exactly 50% of all annota-
tions, then your weight for a choice of 1 will
be 1 + 0.5? 0.2 = 1.3, while you weight for
a choice of 0 will only be 0.7.
(3) The relative BCM rule (RelBCM) is defined
by weights wXi = Freq(X)Freqi(X) . The idea is verysimilar to the DiffBCM rule. For the exam-
ple given above, your weight for a choice of
1 would be 0.5/0.2 = 2.5, while your weight
for a choice of 0 would be 0.5/0.8 = 0.625.
The main difference between the ComBCM rule
and the other two rules is that the former only takes
into account the possible bias of individual anno-
tators, while the latter two factor in as well the
possible skewness of the data (as reflected by the
labelling behaviour of the full set of annotators).
In addition, while ComBCM is specific to the
case of two categories, DiffBCM and RelBCM
immediately generalise to any number of cate-
gories. In this case, we add up the category-
specific weights as before and then choose the cat-
egory with maximal support (i.e., we generalise
the majority rule underlying the family of BCM
rules to the plurality rule).
We stress that our bias-correcting majority rules
do not violate anonymity (nor neutrality for that
matter). If we were to give less weight to a given
annotator based on, say, her name, this would con-
stitute a violation of anonymity; if we do so due to
properties of the profile at hand and if we do so in
a symmetric manner, then it does not.
4.2 Greedy Consensus Rules
Now consider the following idea: If for a given
item there is almost complete consensus amongst
those coders that annotated it with a proper cate-
gory (i.e., those who did not choose ? or ?), then
we should probably adopt their choice for the col-
lective annotation. Indeed, most aggregators will
make this recommendation. Furthermore, the fact
that there is almost full consensus for one item
543
may cast doubts on the reliability of coders who
disagree with this near-consensus choice and we
might want to disregard their views not only w.r.t.
that item but also as far as the annotation of other
items is concerned. Next we propose a family of
aggregators that implement this idea.
For simplicity, suppose that the only proper cat-
egories available are 0 and 1 and that annotators
do not make use of ? (but it is easy to generalise
to arbitrary numbers of categories and scenarios
where different items are associated with different
categories). Fix a tolerance value t ? {0, . . . ,m}.
The greedy consensus rule GreedyCRt works as
follows. First, initialise the set N ? with the full
population of annotators N . Then iterate the fol-
lowing two steps:
(1) Find the item with the strongest majority for
either 0 or 1 amongst coders in N ? and lock
in that value for the collective annotation.
(2) Eliminate all coders from N ? who disagree
on more than t items with the values locked
in for the collective annotation so far.
Repeat this process until the categories for all m
items have been settled.6 We may think of this as
a ?greedy? way of identifying a coalitionN ? with
high inter-annotator agreement and then applying
the majority rule to this coalition to obtain the col-
lective annotation.
To be precise, the above is a description of an
entire family of aggregators: Whenever there is
more than one item with a majority of maximal
strength, we could choose to lock in any one of
them. Also, when there is a split majority between
annotators in N ? voting 0 and those voting 1, we
have to use a tie-breaking rule to make a decision.
Additional heuristics may be used to make these
local decisions, or they may be left to chance.
Note that in case t = m, GreedyCRt is sim-
ply the majority rule (as no annotator will ever get
eliminated). In case t = 0, we end up with a coali-
tion of annotators that unanimously agree with all
of the categories chosen for the collective annota-
tion. However, this coalition of perfectly aligned
6There are some similarities to Tideman?s Ranked Pairs
method for preference aggregation (Tideman, 1987), which
works by fixing the relative rankings of pairs of alternatives
in order of the strength of the supporting majorities. In pref-
erence aggregation (unlike here), the population of voters is
not reduced in the process; instead, decisions against the ma-
jority are taken whenever this is necessary to guarantee the
transitivity of the resulting collective preference order.
annotators need not be the largest such coalition
(due to the greedy nature of our rule).
Note that greedy consensus rules, as defined
here, are both anonymous and neutral. Specifi-
cally, it is important not to confuse possible skew-
ness of the data with a violation of neutrality of the
aggregator.
4.3 Distance-based Aggregation
Our third approach is based on the notion of dis-
tance. We first define a metric on choices to be
able to say how distant two choices are. This in-
duces an aggregator that, for a given profile, re-
turns a collective choice that minimises the sum
of distances to the individual choices in the pro-
file.7 This opens up a wide range of possibilities;
we only sketch some of them here.
A natural choice is the adjusted Hamming dis-
tanceH : A?A ? R>0, which counts how many
items two annotations differ on:
H(A,A?) =
m?
j=1
?(aj , a?j)
Here ? is the adjusted discrete distance defined as
?(x, y) = 0 if x = y or x ? {?,?} or y ? {?,?},
and as ?(x, y) = 1 in all other cases.8
Once we have fixed a distance d on A (such
as H), this induces an aggregator Fd:
Fd(A) = argmin
A?A
n?
i=1
d(A,Ai)
To be precise, Fd is an irresolute aggregator that
might return a set of best annotations with minimal
distance to the profile.
Note that FH is simply the plurality rule. This
is so because every element of the Cartesian prod-
uct is a possible annotation. In the presence of in-
tegrity constraints excluding some combinations,
however, a distance-based rule allows for more so-
phisticated forms of aggregation (by choosing the
optimal annotation w.r.t. all feasible annotations).
We may also try to restrict the computation of
distances to a subset of ?reliable? annotators. Con-
sider the following idea: If a group of annota-
tors is (fairly) reliable, then they should have a
7This idea has been used in voting (Kemeny, 1959), belief
merging (Konieczny and Pino Pe?rez, 2002), and judgment
aggregation (Miller and Osherson, 2009).
8This ?, divided by m, is the same thing as what Artstein
and Poesio (2008) call the agreement value agrj for item j.
544
(fairly) high inter-annotator agreement. By this
reasoning, we should choose a group of annota-
tors ANN ? N that maximises inter-annotator
agreement in ANN and work with the aggrega-
tor argminA?A
?
i?ANN d(A,Ai). But this is too
simplistic: any singleton ANN = {i} will result
in perfect agreement. That is, while we can eas-
ily maximise agreement, doing so in a na??ve way
means ignoring most of the information collected.
In other words, we face the following dilemma:
? On the one hand, we should choose a small set
ANN (i.e., select few annotators to base our col-
lective annotation on), as that will allow us to
increase the (average) reliability of the annota-
tors taken into account.
? On the other hand, we should choose a large set
ANN (i.e., select many annotators to base our
collective annotation on), as that will increase
the amount of information exploited.
One pragmatic approach is to fix a minimum qual-
ity threshold regarding one of the two dimensions
and optimise in view of the other.9
5 A Case Study
In this section, we report on a case study in
which we have tested our bias-correcting major-
ity and greedy consensus rules.10 We have used
the dataset created by Snow et al (2008) for
the task of recognising textual entailment, orig-
inally proposed by Dagan et al (2006) in the
PASCAL Recognizing Textual Entailment (RTE)
Challenge. RTE is a binary classification task con-
sisting in judging whether the meaning of a piece
of text (the so-called hypothesis) can be inferred
from another piece of text (the entailing text).
The original RTE1 Challenge testset consists of
800 text-hypothesis pairs (such as T : ?Chre?tien
visited Peugeot?s newly renovated car factory?,
H: ?Peugeot manufactures cars?) with a gold
standard annotation that classifies each item as ei-
ther true (1)?in case H can be inferred from T?
or false (0). Exactly 400 items are annotated as
0 and exactly 400 as 1. Bos and Markert (2006)
performed an independent expert annotation of
9GreedyCRt is a greedy (rather than optimal) implemen-
tation of this basic idea, with the tolerance value t fixing a
threshold on (a particular form of) inter-annotator agreement.
10Since the annotation task and dataset used for our case
study do not involve any interesting integrity constraints, we
have not tested any distance-based aggregation rules.
this testset, obtaining 95% agreement between the
RTE1 gold standard and their own annotation.
The dataset of Snow et al (2008) includes 10
non-expert annotations for each of the 800 items
in the RTE1 testset, collected with Amazon?s Me-
chanical Turk. A quick examination of the dataset
shows that there are a total of 164 annotators who
have annotated between 20 items (124 annotators)
and 800 items each (only one annotator). Non-
expert annotations with category 1 (rather than 0)
are slightly more frequent (Freq(1) ? 0.57).
We have applied our aggregators to this data and
compared the outcomes with each other and to the
gold standard. The results are summarised in Ta-
ble 2 and discussed in the sequel. For each pair
we report the observed agreement Ao (proportion
of items on which two annotations agree) and, in
brackets, Cohen?s kappa ? = Ao?Ae1?Ae , with Ae be-ing the expected agreement for independent anno-
tators (Cohen, 1960; Artstein and Poesio, 2008).
Note that there are several variants of the major-
ity rule, depending on how we break ties. In Ta-
ble 2, Maj10 is the majority rule that chooses 1 in
case the number of annotators choosing 1 is equal
to the number of annotators choosing 0 (and ac-
cordingly for Maj01). For 65 out of the 800 items
there has been a tie (i.e., five annotators choose 0
and another five choose 1). This means that the tie-
breaking rule used can have a significant impact
on results. Snow et al (2008) work with a major-
ity rule where ties are broken uniformly at random
and report an observed agreement (accuracy) be-
tween the majority rule and the gold standard of
89.7%. This is confirmed by our results: 89.7%
is the mean of 87.5% (our result for Maj10) and
91.9% (our result for Maj01). If we break ties
in the optimal way (in view of approximating the
gold standard (which of course would not actu-
ally be possible without having access to that gold
standard), then we obtain an observed agreement
of 93.8%, but if we are unlucky and ties happen to
get broken in the worst possible way, we obtain an
observed agreement of only 85.6%.
For none of our bias-correcting majority rules
did we encounter any ties. Hence, for these ag-
gregators the somewhat arbitrary choices we have
to make when breaking ties are of no significance,
which is an important point in their favour. Ob-
serve that all of the bias-correcting majority rules
approximate the gold standard better than the ma-
jority rule with uniformly random tie-breaking.
545
Annotation Maj10 Maj01 ComBCM DiffBCM RelBCM GreedyCR0 GreedyCR15
Gold Standard 87.5% (.75) 91.9% (.84) 91.1% (.80) 91.5% (.81) 90.8% (.80) 86.6% (.73) 92.5% (.85)
Maj10 91.9% (.84) 88.9% (.76) 94.3% (.87) 94.0% (.87) 87.6% (.75) 91.5% (.83)
Maj01 96.0% (.91) 97.6% (.95) 96.9% (.93) 89.0% (.78) 96.1% (.92)
ComBCM 94.6% (.86) 94.4% (.86) 88.8% (.75) 93.9% (.86)
DiffBCM 98.8% (.97) 88.6% (.75) 94.8% (.88)
RelBCM 88.4% (.74) 93.8% (.86)
GreedyCR0 90.6% (.81)
Table 2: Observed agreement (and ?) between collective annotations and the gold standard.
Recall that the greedy consensus rule is in fact
a family of aggregators: whenever there is more
than one item with a maximal majority, we may
lock in any one of them. Furthermore, when there
is a split majority, then ties may be broken either
way. The results reported here refer to an imple-
mentation that always chooses the lexicographi-
cally first item amongst all those with a maximal
majority and that breaks ties in favour of 1. These
parameters yield neither the best or the worst ap-
proximations of the gold standard. We tested a
range of tolerance values. As an example, Table 2
includes results for tolerance values 0 and 15. The
coalition found for tolerance 0 consists of 46 an-
notators who all completely agree with the col-
lective annotation; the coalition found for toler-
ance 15 consists of 156 annotators who all dis-
agree with the collective annotation on at most
15 items. While GreedyCR0 appears to perform
rather poorly, GreedyCR15 approximates the gold
standard particularly well. This is surprising and
suggests, on the one hand, that eliminating only
the most extreme outlier annotators is a useful
strategy, and on the other hand, that a high-quality
collective annotation can be obtained from a group
of annotators that disagree substantially.11
6 Related Work
There is an increasing number of projects using
crowdsourcing methods for labelling data. On-
line Games with a Purpose, originally conceived
by von Ahn and Dabbish (2004) to annotate im-
ages, have been used for a variety of linguis-
tic tasks: Lafourcade (2007) created JeuxDeMots
to develop a semantic network by asking players
to label words with semantically related words;
Phrase Detectives (Chamberlain et al, 2008) has
been used to gather annotations on anaphoric co-
reference; and more recently Basile et al (2012)
11Recall that 124 out of 164 coders only annotated 20 items
each; a tolerance value of 15 thus is fairly lenient.
have developed the Wordrobe set of games for
annotating named entities, word senses, homo-
graphs, and pronouns. Similarly, crowdsourcing
via microworking sites like Amazon?s Mechanical
Turk has been used in several annotation experi-
ments related to tasks such as affect analysis, event
annotation, sense definition and word sense disam-
biguation (Snow et al, 2008; Rumshisky, 2011;
Rumshisky et al, 2012), amongst others.12
All these efforts face the problem of how to ag-
gregate the information provided by a group of
volunteers into a collective annotation. However,
by and large, the emphasis so far has been on is-
sues such as experiment design, data quality, and
costs, with little attention being paid to the aggre-
gation methods used, which are typically limited
to some form of majority vote (or taking averages
if the categories are numeric). In contrast, our fo-
cus has been on investigating different aggregation
methods for arriving at a collective annotation.
Our work has connections with the literature on
inter-annotator agreement. Agreement scores such
as kappa are used to assess the quality of an anno-
tation but do not play a direct role in constructing
one single annotation from the labellings of sev-
eral coders.13 The methods we have proposed, in
contrast, do precisely that. Still, agreement plays
a prominent role in some of these methods. In our
discussion of distance-based aggregation, we sug-
gested how agreement can be used to select a sub-
set of annotators whose individual annotations are
minimally distant from the resulting collective an-
notation. Our greedy consensus rule also makes
use of agreement to ensure a minimum level of
consensus. In both cases, the aggregators have the
effect of disregarding some outlier annotators.
12See also the papers presented at the NAACL 2010 Work-
shop on Creating Speech and Language Data with Amazon?s
Mechanical Turk (tinyurl.com/amtworkshop2010).
13Creating a gold standard often involves adjudication of
disagreements by experts, or even the removal of cases with
disagreement from the dataset. See, e.g., the papers cited by
Beigman Klebanov and Beigman (2009).
546
Other researchers have explored ways to di-
rectly identify ?low-quality? annotators. For in-
stance, Snow et al (2008) and Raykar et al (2010)
propose Bayesian methods for identifying and cor-
recting annotators? biases, while Ipeirotis et al
(2010) propose an algorithm for assigning a qual-
ity score to annotators that distinguishes intrinsic
error rate from an annotator?s bias. In our ap-
proach, we do not directly rate annotators or re-
calibrate their annotations?rather, some outlier
annotators get to play a marginal role in the re-
sulting collective annotation as a side effect of the
aggregation methods themselves.
Although in our case study we have tested our
aggregators by comparing their outcomes to a gold
standard, our approach to collective annotation it-
self does not assume that there is in fact a ground
truth. Instead, we view collective annotations as
reflecting the views of a community of speakers.14
This contrasts significantly with, for instance, the
machine learning literature, where there is a fo-
cus on estimating the hidden true label from a set
of noisy labels using maximum-likelihood estima-
tors (Dawid and Skene, 1979; Smyth et al, 1995;
Raykar et al, 2010).
In application domains where it is reasonable to
assume the existence of a ground truth and where
we are able to model the manner in which individ-
ual judgments are being distorted relative to this
ground truth, social choice theory provides tools
(using again maximum-likelihood estimators) for
the design of aggregators that maximise chances
of recovering the ground truth for a given model of
distortion (Young, 1995; Conitzer and Sandholm,
2005). In recent work, Mao et al (2013) have dis-
cussed the use of these methods in the context of
crowdsourcing. Specifically, they have designed
an experiment in which the ground truth is defined
unambiguously and known to the experiment de-
signer, so as to be able to extract realistic models
of distortion from the data collected in a crowd-
sourcing exercise.
7 Conclusions
We have presented a framework for combining
the expertise of speakers taking part in large-scale
14In some domains, such as medical diagnosis, it makes
perfect sense to assume that there is a ground truth. However,
in tasks related to linguistic knowledge and language use such
an assumption seems far less justified. Hence, a collective
annotation may be the closest we can get to a representation
of the linguistic knowledge/use of a linguistic community.
annotation projects. Such projects are becoming
more and more common, due to the availability
of online crowdsourcing methods for data annota-
tion. Our work is novel in several respects. We
have drawn inspiration from the field of social
choice theory to formulate a general formal model
for aggregation problems, which we believe sheds
light on the kind of issues that arise when trying
to build annotated linguistic resources from a po-
tentially large group of annotators; and we have
proposed several families of concrete methods for
aggregating individual annotations that are more
fine-grained that the standard majority rule that so
far has been used across the board. We have tested
some of our methods on a gold standard testset for
the task of recognising textual entailment.
Our aim has been conceptual, namely to point
out that it is important for computational linguists
to reflect on the methods used when aggregat-
ing annotation information. We believe that so-
cial choice theory offers an appropriate general
methodology for supporting this reflection. Im-
portantly, this does not mean that the concrete ag-
gregation methods developed in social choice the-
ory are immediately applicable or that all the ax-
ioms typically studied in social choice theory are
necessarily relevant to aggregating linguistic an-
notations. Rather, what we claim is that it is the
methodology of social choice theory which is use-
ful: to formally state desirable properties of ag-
gregators as axioms and then to investigate which
specific aggregators satisfy them. To put it dif-
ferently: at the moment, researchers in compu-
tational linguistics simply use some given aggre-
gation methods (almost always the majority rule)
and judge their quality on how they fare in specific
experiments?but there is no principled reflection
on the methods themselves. We believe that this
should change and hope that the framework out-
lined here can provide a suitable starting point.
In future work, the framework we have pre-
sented here should be tested more extensively, not
only against a gold standard but also in terms of
the usefulness of the derived collective annotations
for training supervised learning systems. On the
theoretial side, it would be interesting to study the
axiomatic properties of the methods of aggrega-
tion we have proposed here in more depth and to
define axiomatic properties of aggregators that are
specifically tailored to the task of collective anno-
tation of linguistic resources.
547
References
James Allen and Mark Core, 1997. DAMSL: Dialogue
Act Markup in Several Layers. Discourse Resource
Initiative.
Kenneth J. Arrow, Armatya K. Sen, and Kotaro Suzu-
mura, editors. 2002. Handbook of Social Choice
and Welfare. North-Holland.
Kenneth J. Arrow. 1963. Social Choice and Individual
Values. John Wiley and Sons, 2nd edition. First
edition published in 1951.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Michel Balinski and Rida Laraki. 2011. Majority
Judgment: Measuring, Ranking, and Electing. MIT
Press.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. A platform for collaborative se-
mantic annotation. In Proc. 13th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2012), pages 92?96.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
putational Linguistics, 35(4):495?503.
Johan Bos and Katja Markert. 2006. Recognising tex-
tual entailment with robust logical inference. In Ma-
chine Learning Challenges, volume 3944 of LNCS,
pages 404?426. Springer-Verlag.
Steven J. Brams and Peter C. Fishburn. 2002. Voting
procedures. In Kenneth J. Arrow, Armartya K. Sen,
and Kotaro Suzumura, editors, Handbook of Social
Choice and Welfare. North-Holland.
Jean Carletta, Stephen Isard, Anne H. Anderson,
Gwyneth Doherty-Sneddon, Amy Isard, and Jacque-
line C. Kowtko. 1997. The reliability of a dialogue
structure coding scheme. Computational Linguis-
tics, 23:13?31.
Jon Chamberlain, Massimo Poesio, and Udo Kr-
uschwitz. 2008. Addressing the resource bottleneck
to create large-scale annotated texts. In Semantics
in Text Processing. STEP 2008 Conference Proceed-
ings, volume 1 of Research in Computational Se-
mantics, pages 375?380. College Publications.
Yann Chevaleyre, Ulle Endriss, Je?ro?me Lang, and
Nicolas Maudet. 2008. Preference handling in com-
binatorial domains: From AI to social choice. AI
Magazine, 29(4):37?46.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
Vincent Conitzer and Tuomas Sandholm. 2005. Com-
mon voting rules as maximum likelihood estimators.
In Proc. 21st Conference on Uncertainty in Artificial
Intelligence (UAI-2005).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. In Machine Learning Challenges,
volume 3944 of LNCS, pages 177?190. Springer-
Verlag.
Alexander Philip Dawid and Allan M. Skene. 1979.
Maximum likelihood estimation of observer error-
rates using the EM algorithm. Applied Statistics,
28(1):20?28.
Elad Dokow and Ron Holzman. 2010. Aggregation
of binary evaluations. Journal of Economic Theory,
145(2):495?511.
Ulle Endriss, Maria Silvia Pini, Francesca Rossi, and
K. Brent Venable. 2009. Preference aggrega-
tion over restricted ballot languages: Sincerity and
strategy-proofness. In Proc. 21st International Joint
Conference on Artificial Intelligence (IJCAI-2009).
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proc. 47th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2009),
pages 10?18.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Gerald J. Glasser. 1959. Game theory and cumula-
tive voting for corporate directors. Management Sci-
ence, 5(2):151?156.
Umberto Grandi and Ulle Endriss. 2011. Binary ag-
gregation with integrity constraints. In Proc. 22nd
International Joint Conference on Artificial Intelli-
gence (IJCAI-2011).
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality Management on Amazon Mechanical
Turk. In Proc. 2nd Human Computation Workshop
(HCOMP-2010).
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara
Rosenthal, and Kathleen McKeown. 2010. Corpus
creation for new genres: A crowdsourced approach
to PP attachment. In Proc. NAACL-HLT Workshop
on Creating Speech and Language Data with Ama-
zon?s Mechanical Turk, pages 13?20.
John Kemeny. 1959. Mathematics without numbers.
Daedalus, 88:577?591.
Adam Kilgarriff and Martha Palmer. 2000. Introduc-
tion to the special issue on senseval. Computers and
the Humanities, 34(1):1?13.
Se?bastien Konieczny and Ramo?n Pino Pe?rez. 2002.
Merging information under constraints: A logical
framework. Journal of Logic and Computation,
12(5):773?808.
548
Mathieu Lafourcade. 2007. Making people play for
lexical acquisition with the JeuxDeMots prototype.
In Proc. 7th International Symposium on Natural
Language Processing.
Christian List and Philip Pettit. 2002. Aggregating sets
of judgments: An impossibility result. Economics
and Philosophy, 18(1):89?110.
Andrew Mao, Ariel D. Procaccia, and Yiling Chen.
2013. Better human computation through principled
voting. In Proc. 27th AAAI Conference on Artificial
Intelligence.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Michael K. Miller and Daniel Osherson. 2009. Meth-
ods for distance-based judgment aggregation. Social
Choice and Welfare, 32(4):575?601.
Martha Palmer, Hoa Trang Dang, and Christiane
Fellbaum. 2007. Making fine-grained and
coarse-grained sense distinctions, both manually
and automatically. Natural Language Engineering,
13(2):137?163.
Maria Silvia Pini, Francesca Rossi, K. Brent Venable,
and Toby Walsh. 2009. Aggregating partially or-
dered preferences. Journal of Logic and Computa-
tion, 19(3):475?502.
Vikas Raykar, Shipeng Yu, Linda Zhao, Gerardo Her-
mosillo Valadez, Charles Florin, Luca Bogoni, and
Linda Moy. 2010. Learning from crowds. The Jour-
nal of Machine Learning Research, 11:1297?1322.
Sara Rosenthal, William Lipovsky, Kathleen McKe-
own, Kapil Thadani, and Jacob Andreas. 2010. To-
wards semi-automated annotation for prepositional
phrase attachment. In Proc. 7th International Con-
ference on Language Resources and Evaluation
(LREC-2010).
Anna Rumshisky, Nick Botchan, Sophie Kushkuley,
and James Pustejovsky. 2012. Word sense inven-
tories by non-experts. In Proc. 8th International
Conference on Language Resources and Evaluation
(LREC-2012).
Anna Rumshisky. 2011. Crowdsourcing word sense
definition. In Proc. ACL-HLT 5th Linguistic Anno-
tation Workshop (LAW-V).
Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro
Perona, and Pierre Baldi. 1995. Inferring ground
truth from subjective labelling of venus images. Ad-
vances in Neural Information Processing Systems,
pages 1085?1092.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proc. Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2008), pages 254?263.
Alan D. Taylor. 2005. Social Choice and the Math-
ematics of Manipulation. Cambridge University
Press.
T. Nicolaus Tideman. 1987. Independence of clones
as a criterion for voting rules. Social Choice and
Welfare, 4(3):185?206.
Noortje Venhuizen, Valerio Basile, Kilian Evang, and
Johan Bos. 2013. Gamification for word sense la-
beling. In Proc. 10th International Conference on
Computational Semantics (IWCS-2013), pages 397?
403.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proc. SIGCHI Con-
ference on Human Factors in Computing Systems,
pages 319?326. ACM.
H. Peyton Young. 1995. Optimal voting rules. Journal
of Economic Perspectives, 9(1):51?64.
549
Proceedings of the 14th European Workshop on Natural Language Generation, pages 157?161,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Generation of Quantified Referring Expressions:
Evidence from Experimental Data
Dale Barr
Dept. of Psychology
University of Glasgow
dale.barr@glasgow.ac.uk
Kees van Deemter
Computing Science Dept.
University of Aberdeen
k.vdeemter@abdn.ac.uk
Raquel Ferna?ndez
ILLC
University of Amsterdam
raquel.fernandez@uva.nl
Abstract
We present the results from an elicitation
experiment in which human speakers were
asked to produced quantified referring ex-
pressions (QREs), as in ?The crate with
10 apples?, ?The crate with many apples?,
etc. These results suggest that some sub-
tle contextual factors govern the choice be-
tween different types of QREs, and that
numerals are highly preferred for subitiz-
able quantities despite the availability of
coarser-grained expressions.
1 Introduction
Speakers can express quantities in different ways.
For instance, a speaker may specify a meeting time
with the expression ?in the morning? or with the
more precise, numeric expression ?at 10:30am??;
she may choose to specify a temperature as ?5 de-
grees Celsius? or instead use the less precise but
more qualifying expression ?cold?. One area of
NLG where these choices are important is the gen-
eration of referring expressions. In particular, a
referent may be identified by means of some quan-
titative value or other (e.g., ?the tall man; ?the man
who is 198cm tall?), or by means of the number
of other entities to which it is related. Hence-
forth, let?s call these quantified referring expres-
sions (QREs). An example of a QRE arises, for
instance, when a person is identified by means of
the number of his children (?the man with 5 daugh-
ters?), when a directory is identified by means
of the number of files in it (?the directory with
520/many PDF files in it?), or when a crate is iden-
tified by means of the number of apples in it (?the
crate with 7 /a few apples?).
Green and van Deemter (2011) asked under
what circumstances it might be beneficial, for
a reader or hearer, for referring expressions of
this kind to contain vague expressions (e.g., like
many). The present paper addresses the same phe-
nomena focussing, more broadly, on all the differ-
ent ways in which reference may be achieved; un-
like these previous authors, we shall address this
question from the point of view of the speaker,
asking how human speakers refer in such cases,
rather than how useful a given referring expression
is to a hearer (e.g., as measured by their response
times in a manipulation task).
We start by making our research questions more
precise in the next section. We then describe the
production experiment we run online in Section 3
and present an analysis of the data in Section 4.
We end with some pointers on how our results
could inform an NLG module for QREs.
2 Research Questions
Suppose you want to point out one crate amongst
several crates with different numbers of apples.
You may use a numeral (?the crate with seven ap-
ples?) or, if the crate in question is the one with
the largest or smallest amount of apples, you may
use superlatives (?the crate with the most apples?),
comparatives (?with more apples?) or vague quan-
tifiers (?with many apples?); if your crate is the
only one with any apples in it at all, you might
simply say ?the crate with apples?). In many situ-
ations, several of these options are applicable. It
is not obvious, however, which of these is pre-
ferred. The Gricean Maxim of Quantity (Grice,
1975) urges speakers to make their contribution as
informative as, but not more informative than, it is
required for the current purposes of the exchange.
This might be taken to predict that speakers will
tend to use the most coarsely grained expression
that identifies the referent (unless they want some
nontrivial implicatures to be inferred). This would
predict, for example that it is odd to say ?the box
with 27 apples? when ?the box with apples? suf-
fices, because the latter contains a boolean prop-
erty (contains apples), whereas the former relies
157
Figure 1: Sample stimuli in contexts X , XY , XYY with big gap, and XYZ with small gap.
on a special case on what is essentially much more
finely grained property (contains x apples).
Our hunch, however, was that this is not the
whole story. For example, the literature on human
number processing suggests that numbers below 5
or 6 are handled almost effortlessly; these num-
bers are called subitizable (Kaufman et al, 1949)
Furthermore, we hypothesized that it matters to
what extent the number of apples in the target crate
?stands out?. We had the following expectations:
1. Speakers do not always use the coarsest-
grained level that is sufficient.
2. Whether a quantity is subitizable or not inter-
feres with the speakers? choice.
3. The frequency of vague forms (such as ?many?)
will be higher in contexts where the gap be-
tween the target quantity and the quantities in
the distractors is large than when it is small.1
We wanted to put these ideas to the test and, more
generally, find out how human speakers use QREs
in different contexts. Our interest was also in cre-
ating a corpus of human-produced QREs that can
serve future research.
3 Experimental Setup
The elicitation experiment was run online. Sub-
jects first encountered a screen with instructions.
They were told that they would be presented with
situations consisting of three squares, with each of
them having none, one or more shapes in it. In
each of these situations, one of the three squares
would be highlighted and subjects were asked to
describe this target square in a way that would en-
able a reader of their expression to identify it. Sub-
jects were told that the recipient of their descrip-
tion may see the three squares arranged differently
on the screen with their contents possibly being
scrambled around. That is, they were indirectly
asked to concentrate on the quantity of shapes in
1Later on we refer to vague forms as ?base?, a common
term used to describe the vague, unmodified form of relative
scalar adjectives (e.g., tall) as opposed to their comparative
(taller) and superlative (tallest) forms.
the squares (rather than on their relative position or
on the spatial configuration of the shapes in them).
Figure 1 shows some sample stimuli.
The experiment included a total of 20 items,
generated according to the following parameters:
? Subitizability: the amount of shapes in the tar-
get is within the subitizable range (SR) (1-4
shapes) or within a non-subitizable range (NR);
we included three non-subitizable ranges, with
around 10, 20, and 30 shapes, respectively.
? Context: we considered four types of scenarios:
1. X : only the target square is filled.
2. XY : two squares are filled.
3. XYY: all squares filled; with two ranges.
4. XYZ: all squares filled; with three ranges.
The symbol X in the first position stands for the
referent square, while the symbols in the other
two positions indicate for each of the other two
squares whether it contains a number of shapes
within the same range as the referent square
(X), within a different range (Y/Z), or whether
it does not contain any shapes at all ( ).
? Relative Size: the target contains either the
smallest or the largest amount of shapes.
? Gap Size: there is either a big or a small quan-
tity difference between the target and other
squares. A big gap size is only possible with
target squares that contain the largest amount of
shapes within a non-subitizable range and those
that contain the smallest amount of shapes
within a subitizable range.
Participants were recruited by publishing a call
in the Linguist List. A total of 82 subjects par-
ticipated in the experiment, including participants
who only responded to some items. We eliminated
6 sessions where the participant had responded to
less than 10 items. The final dataset includes 76
participants and a total of 1508 descriptions.
4 Results
Each description produced by the participants was
annotated with one of the categories in Table 1.
158
Category Examples
ABS [absolute] the one with pacmans / the square that?s not blank
BASE [base] the square with lots of dark dashes / it has a few crosses in it
COMP [comparative] the one with fewer dashes / the square with more crosses in it
NUM [numeric] the square with 11 black dots / 3 grey ovals
SUP [superlative] it has the largest number of purple squares / the square with the least minuses
OTH [other] about a dozen blue diamonds / big droup of circles in the centre
Table 1: Categories used to code the expressions produced by the participants.
The classification was first done automatically by
pattern matching and then revised manually.
To analyse the data, we used mixed-effects lo-
gistic regression with crossed random effects for
subjects and items (Baayen et al, 2008). All
models had by-subject and by-item random in-
tercepts, and by-subject random slopes for the
within-subject factors of context and range (subiti-
zability). The models were fit using maximum
likelihood estimation with p-values derived from
likelihood ratio tests. Model estimation was per-
formed using the lme4 package (Bates et al, 2013)
of R statistical software (R Core Team, 2013).
Table 2 shows the overall distribution of expres-
sion types used by the participants. As can be
seen, numerical expressions were the most com-
mon type of expression used overall (65%). We
found, however, that there was a strong subiti-
zability effect in the use of these expressions:
for non-subitizable targets, subjects used numer-
ical expressions only 39% of the time, while for
subitizable targets they did so 90% of the time.
This main effect of subitizability was significant
(?2(1) = 47.92, p < .001). There was high
variability across subjects in the effect (?2(1) =
25.00, p < .001), with a higher rate of numeri-
cal expressions associated with a smaller effect of
subitizability (r = ?.61). Note that 17 of the 82
subjects (? 20%) always used numerical expres-
sions, even when the target was not subitizable. Of
the remaining 65 subjects, 64 show a very signif-
icant preference for using numeric expressions to
describe targets within the subitizable range.
Figure 2 shows the proportion of expression
types for each type of context and subitizabil-
ABS BASE COMP NUM SUP OTH Total
NR 73 33 26 294 308 17 751
SR 51 1 0 684 21 0 757
Total 124 34 26 978 329 17 1508
Table 2: Row counts of expression types for non-
subitizable (NR) and subitizable (SR) targets.
ity condition.2 Sensitivity to context differed for
subitizable and non-subitizable targets, supported
by a reliable interaction between these factors
(?2(1) = 17.31, p < .001). Despite the strong
overall preference for numerical expressions with
subitizable targets, the effect of context was still
reliable (?2(1) = 22.63, p < .001). For subiti-
zable targets (Figure 2, bottom row), numeric ex-
pressions were almost always used (96%) except
in contexts where the target was the only filled
square (X ). In this context, participants occa-
sionally used absolute expressions instead (e.g. the
one with shapes) 33% of the time. In sum, subiti-
zable targets overwhelmingly triggered the use of
numerals, predominating even when a Gricean ac-
count would prefer coarser-grained expressions.
For non-subitizable targets (first row of plots
in Figure 2), in contexts without distractors (X )
absolute expressions were preferred over numer-
ical ones; this differed from the behaviour of
subitizable targets in this context, where numer-
ical expressions predominated (?2(1) = 4.25,
p = .039). In contexts with non-empty distrac-
tors (XY , XYY, and XYZ), expressions other than
numeric are used significantly more often than
they were for subitizable targets (?2(1) = 52.93,
p < .001). Superlative expressions (e.g. the
square with the least dots) were preferred in con-
texts where the three squares were filled (?2(1) =
7.74, p = .005). In contexts with one distractor
(XY ), superlatives were also rather common, and
comparative expressions (e.g. the one with fewer
dashes) occurred at higher rates than in other types
of context (?2(1) = 42.34, p < .001).
The comparison between the contexts with two
distractors (XYY and XYZ) suggests that they dif-
fered largely in the use of vague expressions
(BASE; e.g. the one with many diamonds), which
had a higher rate in context XYY where there
were only two quantity ranges (?2(1) = 5.01,
2Category OTH (other) is not shown in Figure 2 to avoid
clutter. Table 2 shows the row counts for all categories.
159
ABS BASE COMP NUM SUP
X__
non?s
ubitizab
le range
0.00
.10.2
0.30
.40.5
0.6
ABS BASE COMP NUM SUP
XY_
0.00
.10.2
0.30
.40.5
0.6
ABS BASE COMP NUM SUP
XYY
0.00
.10.2
0.30
.40.5
0.6
ABS BASE COMP NUM SUP
XYZ
0.00
.10.2
0.30
.40.5
0.6
ABS BASE COMP NUM SUP
subitiza
ble rang
e
0.0
0.2
0.4
0.6
0.8
1.0
ABS BASE COMP NUM SUP0.0
0.2
0.4
0.6
0.8
1.0
ABS BASE COMP NUM SUP0.0
0.2
0.4
0.6
0.8
1.0
ABS BASE COMP NUM SUP0.0
0.2
0.4
0.6
0.8
1.0
Figure 2: Proportion of expression types in each context for subitizable and non-subitizable targets.
p = .025). For this context we also found an ef-
fect of gap size (see Figure 3): the relative odds
of choosing a vague expression over a numeric or
superlative one is significantly higher when there
is a big difference between the target quantity and
the distractor quantities (?2(1) = 5.68, p = .017);
that is, when the chance of there being borderline
cases is reduced. A small gap between the quanti-
ties makes the preference for superlative (and thus
non-vague) expressions stronger.
Figure 3: The effect of gap size.
5 Conclusions
In line with our expectations (see Section 2), our
data are not easy to reconcile with the type of
Gricean account that predicts a preference for the
most coarsely grained QRE that identifies the tar-
get. The most obvious deviation from this Gricean
account arises from the subitizable items in our
study, where numerical expressions turned out to
be much preferred over other QREs. The natu-
ral explanation seems to be that such expressions
come naturally to speakers (and to hearers too as
shown by Green and van Deemter (2011)). In
other words, our study suggests an intriguing vari-
ant on Grice, in which the most relevant factor is
not one of informativeness ? as Grice?s writings
suggest ? but one of effort. It suggests that speak-
ers tend to produce expressions that identify the
referent with least effort.
Our expectation 3 was also confirmed: vague
forms (BASE) are more frequent with big gap
sizes, although they are not produced with high
frequency. (The same pattern of results was found
by van Deemter (2004)). Thus, in the scenarios
we considered vague QREs are never the most
favoured option. The high frequency of superla-
tives over comparatives is also noteworthy. Com-
paratives are used very seldom overall but are
more frequent in contexts with only one distractor
(XY ). This indicates that some speakers opt for
a less strong expression than a superlative (an ex-
pression that means more than x rather than more
than any other x) in contexts where this does not
lead to ambiguity. However, numerals and su-
perlatives are still largely preferred in those con-
texts.
These observations suggest that a given type of
situation (i.e., a given context + subitizability con-
dition) should not always map to the same type of
QRE. If human QRE behaviour is to be mimicked,
the best approach seems to be to use a stochastic
NLG program that seeks to replicate the frequen-
cies that are found in human usage.
The collected data is freely available at http:
//www.illc.uva.nl/?raquel/xprag/.
160
References
R. Baayen, D. Davidson, and D. Bates. 2008. Mixed-
effects modeling with crossed random effects for
subjects and items. Journal of memory and lan-
guage, 59(4):390?412.
D. Bates, M. Maechler, and B. Bolker, 2013. lme4:
Linear mixed-effects models using S4 classes. R
v. 0.999999-2.
M. Green and K. van Deemter. 2011. Vagueness
as cost reduction: An empirical test. In Proc. of
Production of Referring Expressions workshop at
CogSci 2011.
H. P. Grice. 1975. Logic and conversation. In The
Logic of Grammar, pages 64?75. Dickenson.
E. Kaufman, M. Lord, T. Reese, and J. Volkmann.
1949. The discrimination of visual number. Ameri-
can Journal of Psychology, 62(4):498?525.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation. v. 3.0.0.
K. van Deemter. 2004. Finetuning NLG through ex-
periments with human subjects: the case of vague
descriptions. In Proc. of the 3rd INLG Conference.
161
