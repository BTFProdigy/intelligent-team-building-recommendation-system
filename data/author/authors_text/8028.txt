MWEs as Non-propositional Content Indicators 
Kosho Shudo, Toshifumi Tanabe, Masahito Takahashi? and Kenji Yoshimura 
     Fukuoka University                      ?Kurume Institute of Technology                       
8-19-1, Nanakuma, Fukuoka,                   66-2228, Kamitsu, Kurume,  
814-0180 JAPAN                                    830-0052 JAPAN 
{shudo,tanabe,yosimura}@tl.fukuoka-u.ac.jp taka@cc.kurume-it.ac.jp 
 
Abstract 
We report that a proper employment of MWEs 
concerned enables us to put forth a tractable 
framework, which is based on a multiple 
nesting of semantic operations, for the 
processing of non-inferential, Non-
propositional Contents (NPCs) of natural 
Japanese sentences. Our framework is 
characterized by its broad syntactic and 
semantic coverage, enabling us to deal with 
multiply composite modalities and their 
semantic/pragmatic similarity. Also, the 
relationship between indirect (Searle, 1975) 
and direct speech, and equations peculiar to 
modal logic and its family (Mally, 1926; Prior, 
1967) are treated in the similarity paradigm. 
1 Introduction 
While proper treatment of the Propositional 
Content (PC) of a sentence is undoubtedly 
important in natural language processing (NLP), 
the Non-propositional Content (NPC) also plays a 
critical role in tasks such as discourse 
understanding, dialogue modeling, detecting 
speaker?s intension. We refer generically to the 
information which is provided by auxiliaries, 
adverbs, sentence-final particles or specific 
predicative forms in Japanese sentences as NPC. It 
is concerned with notions such as polarity, tense, 
aspect, voice, modality, and illocutionary act, 
which incorporate temporal, contingent, subjective, 
epistemic or attitudinal information into the PC.   
Though the inferential NPC e.g., implicature 
(Grice, 1975), has been discussed in semantics or 
pragmatics, it lies beyond the state-of-the-art 
technology of NLP. Besides, no systematic attempt 
to connect linguistic forms in the sentence with the 
non-inferential NPCs has been reported in NLP 
community. In this paper, we present a framework 
for the treatment of NPC of a sentence on the basis 
of the extensive, proper employment of multiword 
expressions (MWEs) indicating the NPCs in 
Japanese. In Japanese, which is a so-called SOV 
language, NPCs are typically indicated in the V-
final position by auxiliaries, particles and their 
various alternative multiword expressions. We 
have extracted extensively these expressions from 
large-scale Japanese linguistic data. We refer to 
these, including auxiliaries and ending-particles, as 
NPC indicators (NPCIs). The number of NPCIs 
amounts to 1,500, whereas that of auxiliaries and 
ending-particles is about 50, which is apparently 
insufficient for practical NLP tasks.  
Our model leads to dealing not only with some of 
illocutionary acts (Austin, 1962) but also with the 
logical operations peculiar to the family of modal 
logic, i.e., deontic (Mally, 1926) and temporal 
logic (Prior, 1967). 
We also present, in this paper, the idea of the 
similarity among NPCs within our framework. 
This is essential for text retrieval, paraphrasing, 
document summarization, example-based MT, etc. 
Some of the indirect speech acts (Searle, 1975) and 
axioms proper to the family of modal logic are 
treated formally in the similarity paradigm. 
In Section 2, we introduce an overview of our 
ongoing MWE resource development for general 
Japanese language processing. In Section 3, we 
introduce a framework for the treatment of NPC. A 
set of primitive functions to compose NPC is 
explained in Section 4. In Section 5, first, the 
relationship between the framework and Japanese 
syntax, and second, methods to identify NPCs of 
Japanese sentences and to apply them to a 
translation task are described. In Section 6, we 
formalize the similarity among NPCs within the 
framework. In Section 7, we present conclusions 
and comment on future work. 
2 Background MWE Resources 
The authors have been concerned with how to 
select atomic expressions of the sentence 
construction in NLP based on the semantic 
compositionality. Morphosyntactically, this 
problem is also serious for the processing of the 
agglutinative, space-free language like Japanese. 
Our research on this subject started in ?70s by 
extracting manually multiword expressions as 
MWEs from large-scale Japanese linguistic data in 
the general domain. We estimate that the amount of 
data examined is 200,000 sentences. 
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 32-39
In this Section, we present an overview of our 
ongoing development of Japanese MWE resources. 
We have extracted multiword expressions that take 
at least one of the following three features;  
 
f1: idiomaticity (semantic non-decomposability),  
f2: lexical rigidity (non-separability),  
f3: statistical boundness.  
 
The expression which causes the difficulty in 
composing its overall meaning from normal 
meanings of component words has f1.1  f2 includes 
the feature to allow other words to cut in between 
the component words. The expression whose 
components are bound each other with high 
conditional probability has f3. Each multiword 
expression selected as a MWE was endowed with a 
binary-valued basic triplet (f1, f2, f3). For example, 
an idiomatic, separable and not-statistically-bound 
expression, ??????? hone?wo?oru? ?make an 
effort (lit. break bone)? is endowed with (1,0,0) and 
compositional, separable and statistically-bound 
expression, ????? ???  gussuri?nemuru? 
?sleep soundly?, with (0,0,1). A dot ??? denotes a 
conventional word-boundary, hereafter. 
Fixed expressions, decomposable idioms, 
institutionalized phrases, syntactically-idiomatic 
phrases, light verb constructions discussed in (Sag 
et al, 2002) and proverbs might correspond 
roughly to the triplets, (1,1,0), (1,0,0), (0,0,1), 
(0,x,1), (0,x,1) and (1,1,1), respectively. 
MWEs, whose number amounts to 64,800 at 
present, are classified by their overall, grammatical 
functions as follows. Examples with a triplet and 
the current number of expressions are also given in 
the following. Compound nouns and proper nouns 
are excluded in the present study. 
 
Conceptual MWEs:  
nominal<10,000>:?? ?? ???  aka?no?tanin? 
(1,1,0) ?complete stranger (lit. red stranger)?; 
??????? turu?no?hitokoe? (1,1,0) ?the voice 
of authority (lit. one note of crane)? ;  etc. 
verbal-nominal<1,700>:? ? ? ? ? ? ? 
morai?naki? (1,1,0) ?weeping in sympathy (lit. 
received crying)?; ? ? ? ? ? ? ?  
rappa?nomi?(1,1,0) ?drinking direct from the 
bottle (lit. trumpet drink)?;  etc. 
verbal<34,000>: ? ? ? ? ? ? ? 
kami?simeru ?(1,1,0) ?chew well (lit. bite and 
fasten)?; ?? ???? ni?tumeru? (1,1,0) ?boil 
down (lit. boil and pack in)?;  etc. 
adjectival<4,300>: ? ? ? ? ? ? ? 
okorip?poi ?(0,1,0) ?irritable (lit. anger-ish)?; 
                                                     
1 At present f1 and presumably f2 will not be decided by any statistical method. 
??? ???  chuui?bukai? (1,1,0) ?careful (lit. 
deep in caution)?;  etc. 
adjectival-nominal<2,000>:??? ?? ????  
ikkan?no?owari? (1,1,0) ?the very end (lit. the 
end of a roll)?; ? ? ? ? ? ? ? 
sujigaki?doori?(0,1,0) ?as just planned (lit. just 
as a plot)?;  etc. 
adverbial<5,200>: ? ? ? ? ? ? ? ? 
waruku?suru?to? (1,1,0) ?if the worst happens (lit. 
if it worsens)?;  ? ? ? ? ? ? ?  
uttori?to?(0,1,0)?abstractedly?;  etc. 
adnominal<2,600>: ??? ?? ???  taai?no?nai? 
(1,0,1)?inconsiderable (lit. with no altruism)?; 
? ?????  danko?taru?(0,1,0) ?firm?; etc. 
connective<300>: ??? ???   sono?kekka? 
(1,1,0) ?consequently (lit. the result)?;  ??????
?????  sore?ha?sate?oki? (1,1,1) ?by the way 
(lit. setting it aside)?; etc. 
proverb-sentential<1,300>:? ? ? ? ? ? ? ? 
isoga?ba?maware? (1,1,1) ?Make haste slowly. 
(lit. go round if it is in a hurry.)?; ????????
???? shunmin?akatuki?wo?oboe?zu? (1,1,1) ?In 
spring one sleeps a sleep that knows no dawn.?; 
etc. 
proverb-sentential-incomplete<900>: ???????
??  yamai?ha?ki?kara? (1,1,0) ?Fancy may kill 
or more. (lit. Illness is brought from one?s 
feeling.)?; ? ? ? ? ? ? ? ? ? ? ? 
uma?no?mimi?ni?nenbutu? (1,1,1) ?A nod is as 
good as a wink to blind horse. (lit. buddhist?s 
invocation to the ear of a horse)? ; etc. 
 
Functional MWEs: 
relation-indicator(RI)<1,000>:? ? ? ? ? ? ? 
ni?tui?te? (1,1,0) ?about (lit. in touch with)?;  ???
???? ni?yot?te? (1,1,0) ?by (lit. depending 
on)?; ??????? to?tomo?ni? (1,1,0) ?with (lit. 
accompanied with)?; ?? ????  ni?okeru? 
(1,1,0) ?in? ,?on  (lit. placed in)?;  etc. 
NPCI<1,500>: See Section 4. 
 
Nominals listed above are those marked with a 
triplet (1,1,x). We exclude compound nouns with 
(0,0,x) and proper nouns, whose number amounts 
to quite large, in this study. They should be treated 
in some other way in NLP. A treatment of those 
compound nouns for Japanese language processing 
is reported in (Miyazaki et al, 1993). 
Formally, the triplet is expanded in the lexicon to a 
partly multi-valued 7-tuple (f1, f2, f3, f4, f5, f6, f7). 
The augmented features are as follows; 
 
f4: grammatical class (shown above) 
f5: syntactical, original internal-structure 
f6: morphosyntactical variation: (m1, m2, ... , m9) 
m1: possibility to be modified by adnominal 
m2: possibility to be modified by appredicative 
m3: auxiliaries insertable in between its words  
m4: particles insertable in between its words 
m5: deletable particles 
m6: particles by which those in it are replaced 
m7: constituents which can be reordered 
m8: possibility to be nominalized by inversion 
m9: possibility to be passivized 
f7: estimated relative frequency 
 
f6 was adopted to ensure the flexibility of MWEs, 
while controlling the number of headings.  
Thus, our lexicon is not simply a list of MWEs but 
designed as a resource proliferous to a total variety 
of idiosyncratic expressions. (Shudo et al, 1980, 
1988; Shudo, 1989; Yasutake et al, 1997). 
The present study focuses on a set of NPCIs and its 
relationship to the non-propositional structure of 
natural sentences. Some of our multiword NPCIs 
are treated in the general, rewriting framework for 
MT in (Shirai et al, 1993). 
3 Non-propositional Structures (NPSs) 
Let us consider the meaning of a sentence; 
 
(1) ?? ?? ??? ?? ??? ???? ???? ?? 
kare?ha?soko?ni?iru?bekide?nakat?ta? ?He should 
not have been there?,  
 
where a verb ??? iru? ?be?  is followed by 
three auxiliaries, ????  bekida? ?should?, ??? 
nai?  ?not? and ??  ta? ?-ed? which mean 
obligation, negation and past-tense, respectively, in 
the sentence-final position 2 . According to the 
occurrences of them, the solely literal paraphrase 
of (1) would be something like; 
 
(2) ???????????????????????????  
????? 
kare?ha?soko?ni?iru?bekida?to?iu?koto?ha?nakat?ta
? ?It was not necessary for him to be there?, 
 
However, this reading is not correct for (1). Rather, 
in contrast, its regular reading should be something 
like; 
 
(3) ?? ?? ??? ?? ?? ?? ?? ?? ???? 
kare?ga?soko?ni?i?ta?no?ha?mazui? ?It is evaluated 
in the negative that he was there?, 
 
By the way, it will be reasonable to think sentences 
                                                     
2 ???? bekida? and ???  nai ?  are inflected as ???? bekide? and ??
?? nakat?, respectively, in (1). 
(2) and (3) share a kernel sentence ????????
? ???  kare?ga?soko?ni?iru? ?He is there?, into 
which NPCs are incorporated successively, i.e., 
first - obligation, second - negation, third - past-
tense, in the case of (2), and first - past-tense, 
second - speaker?s-negative-evaluation, in the case 
of (3). Moreover, each stage of this incorporation 
would be regarded as mapping the utterance?s 
meaning from one to another, in parallel with a 
syntactic form being mapped from one to another.  
Hence, by introducing Non-propositional Primitive 
Functions (NPFs), e.g., OBLIGATION2, 
NEGATION1, PAST-TENSE, and NEG-EVAL, we 
can explain the Non-propositional Structure (NPS) 
of (2) as; 
 
(4)PAST-TENSE [NEGATION1 
[OBLIGATION2[? ? ? ? ? ? ? ? ? ? ? ? 
kare?ga?soko?ni?iru? ?He is there?] ] ] 
 
and NPS of (3), hence, of (1) as, 
 
(5)NEG-EVAL[PAST-TENSE[???????????
? kare?ga?soko?ni?iru? ?He is there?] ].3 
 
Here, a problem is that (4) is wrong for (1). In 
order to cope with this, while adopting a MWE, 
?????????? bekide?nakat?ta? as a NPCI 
with a triplet (1,0,0) which has a composite NPF, 
NEG-EVAL[PAST-TENSE[x]]4, we have designed 
our segmenter to prefer a longer segment by the 
least-cost evaluation.  
It should be noted that a composite of NPFs like 
this could be associated with a single NPCI. 5 This 
is caused by its idiomaticity, i.e., by the difficulty 
in decomposing it into semantically consistent sub-
forms. 
Investigating a reasonably sized set of Japanese 
linguistic data, keeping the strategy exemplified 
above in mind, revealed that NPS of a natural 
Japanese sentence can be generally formulated as a 
nested functional form; 
 
(6) Mn[Mn-1?[M2[M1[S]]]?],  
 
where S is a propositional, kernel sentence; Mi   
(1?i?n), a NPF. In the following, we use the 
                                                     
3 We use lower-suffixes to distinguish NPFs by the subtle differences in 
meaning, degree, etc. 
4 Another choice could be, first, to adopt a shorter MWE, ??????? bekide? 
nai? ?should not? as a NPCI indicating PROHIBITION2, second, to build a NPS, 
PAST-TENSE[PROHIBITION2[???????????? kare?ga?soko?ni?iru? ?He 
is there?]], and last, to apply the following similarity rule in order to obtain (5), 
unless it yields the overgeneralization; 
PAST-TENSE[PROHIBITION2[x]] ?NEG-EVAL[PAST-TENSE[x]].   
The similarity rules are discussed in Section 6. 
5 Another typical example is??? mai? which is a single auxiliary but has the 
meaning of ?will not?, i.e., GUESS2[NEGATION1[x]]. 
notation for a composite function,  
Mn?Mn-1??M2?M1, where Mn?Mn-1??M2?M1[S] = 
Mn[Mn-1?[M2[M1[S]]]?]. 
4 NPCIs, NPFs 
We have settled a set of 150 basic NPFs by 
classifying 1,500 NPCIs which had been extracted 
from the large-scale data. After manually 
extracting them, the data has been continuously 
checked and updated by comparing with various 
dictionaries and linguistic literature such as 
(Morita et al,1989).  
They are subclassified as follows, though the 
boundaries between subclasses are partly subtle. It 
should be noted that some NPCIs are semantically 
ambiguous, being included in different subclasses 
below. Examples of NPCIs and the number of 
NPFs are given in brackets, in the following list. 
 
F1:polarity <3>: 
NEGATION1(??? nai? ?not? ; ?????????
no?de?ha?nai?(1,0,0) ?not? ; etc.), 
NEGATION2(? ? ? ? ? ? ? ? ? ? ? ? ? ?
to?iu?wake?de?ha?nai?(1,0,0) ?not? ; etc.),etc. 
F2:tense <1>: 
PAST-TENSE(?? ta? V-ed ; ?? da? V-ed)  
F3:aspect-observational <9>: 
IMMEDI-AFT-TERMINATING (????????
ta?tokoro?da?(1,1,0) ?have just V-en?;  ?????
????????? ta?bakari?no?tokoro?da? (1,1,0)  
?have just V-en? ; etc.), 
IMMEDI-BEF-BEGINING(?????? ?????  
u?to?si?te?iru? (1,0,0)  ?be about to?; ????????
???? you?to?si?te?iru?  (1,0,0) ?be about to? ; 
etc.), 
PROGRESSING(????? te?iru? (1,0,0)  ?be V-
ing?;  ?????? tutu?aru?(1,1,0) ?be V-ing?;  
etc.), etc. 
F4:aspect-action <8>: 
INCHOATIVE(????? hajimeru? ?begin to?;  
? ? ?  dasu? ?begin to?; etc.), 
TERMINATIVE(????  owaru? ?finish V-
ing? ; ????  oeru? ?finish V-ing? ; etc.), 
CONTINUATIVE(???? tuzukeru? ?continue 
to? ;  ?????  nagaraeru? ?continue to?;  etc), 
etc. 
F5:voice <10>: 
PASSIVE(???  reru? ?be V-en? ;  ???? 
rareru? ?be V-en?), 
CAUSATIVE(???  seru? ?make?V?? ;  ??
??  saseru? ?make?V??), 
PAS-SUFFERING(???  reru? ?have?V-en? ; 
????  rareru? ?have?V-en? ;  etc.), 
PAS-BENE-TAKING1 (? ? ? ? ? ?
te?morau?(1,0,0)?ask ?V?; ? ? ? ? ? ? ?
te?itadaku? (1,0,0) ?ask? V? ;  etc.),  
BENE-TAKING(?????? te?kureru? (1,0,0) 
?V... for (someone)...? ; etc.), etc.  
F6:politeness-operator <3>: 
POLITENESS1 (???  masu? ; etc.) ,etc. 
F7:predicate-suffix <30>: 
TRIAL(?? ??? te?miru? (1,0,0) ?try to? ;  
etc.),etc. 
F8:modality <60>: 
NEG-EVAL(???????? beki?de?nai? (1,0,0) 
?should not? ; ?????????? no?ha?yoku?nai? 
(1,0,0) ?should not? ; etc.),  
OBLIGATION2(???????? hituyou?ga?aru ? 
(1,0,0) ?need?, ???? bekida? ?should?, etc. ),  
OBLIGATION1(? ? ? ? ? ? ? ? ? ? ? ?
nakere?ba?nara?nai? (1,1,1) ?have to? ;  etc.), 
PROHIBITION(? ? ? ? ? ? ? ? ? ?
te?ha?nara?nai?(1,0,1) ?should not?, etc.), 
CAPABILITY(??? uru? ; ?????????  
koto?ga?dekiru?(1,0,0) ?be able to?; etc.), 
GUESS1(??  u? ?will?), etc. 
F9:illocutionary-act <28>: 
IMPERATIVE(imperative-form of verb 
?imperative form?),  
INTERROGATIVE (? ? ka? ?interrogative 
form?;  ?? ??  no?ka?(1,1,0) ?interrogative 
form? ;  etc.),  PROHIBITIVE(? ?  na? 
?Don?t... ?), PERMISSIVE(?? ? ? ? (1,1,0)  
te?yoi?  ?You may...? ; ?? ?? ???? ???  
te?mo?kamawa?nai?(1,0,0) ?You may...?  ; etc.), 
REQUESTING(? ? ? ? ?   te?kure?(1,1,0) 
?Please...?; ?????? te?hosii?(1,1,0) ?I want 
you to...? ;  etc.), etc. 
5 Treatment of NPSs 
5.1 Sentence-final Structure in Japanese 
Employing MWEs as NPCIs enabled us to 
describe the outermost structure of a Japanese 
sentence by the following production rules; 
 
(7) S0?BP*?PRED, 
(8) Si?Si-1?mi, (1?i?n), 
 
where S0 denotes a kernel sentence; BP, a basic 
phrase called bunsetsu; PRED, a predicate of the 
kernel sentence; Si, a sentence, mi, a NPCI and a 
symbol ?*?, closure operator on the concatenation, 
???. In the following, we use predicative parts, 
PRED? m1? m2 ? ??? ? mn instead of full sentences, for 
simplicity. 
Our morphology model was developed so as to fit 
for the general semantic processing, adopting 
MWEs. It is a probabilistic finite automaton with 
150 states that prescribes minutely the internal 
structure of each BP and the predicative part. We 
leave its detail to (Shudo et al, 1980). 
5.2 Identifying NPS 
Based on our morphological analyzer, we have 
developed a segmenter (SEG) that segments the 
input predicative part into a PRED and each NPCI, 
and a NPS-constructor (NPSC) that constructs 
NPSs. For example, an input;  
 
(9) ? ? ? ? ? ? ? ? ? ? ? ? ? ?
yomanakerebanaranaidarou?  ?will have to 
read?   
 
is first segmented into 
 
(10) ??? /???? ?? ??? ??? /??? ?? 
yoma/?nakere?ba?nara?nai/?daro?u ? 
 
by SEG. Here, a slash ?/? denotes a segment-
boundary identified by SEG. Then, NPSC 
evaluates a function nps defined below. 
 
(11) nps(S0)=S0, 
nps(S0/m1/m2.?/mi)=Mik[?Mi2[Mi1[nps(S0/ 
m1/m2.?/mi-1)]]],(1?i?n), 
 
where Mik[?Mi2[Mi1[x]]] is a NPF (if k =1) or a 
composite of NPFs (if k?2) associated with mi. 
Hence, the computation of nps for (10) is; 
 
(12) nps(???/????????????/?????
yoma/?nakere?ba?nara?nai/?daro?u?) 
=GUESS2[nps(??? /???? ?? ??? ??? 
yoma/?nakere?ba?nara?nai? ?have to read?)] 
=GUESS2[OBLIGATION1[nps(???   yomu? 
?read?)]] 
=GUESS2[OBLIGATION1[? ? ?  yomu?  
?read?]],  
 
where GUESS2 and OBLIGATION1 are associated 
with ????? daro?u? ?will? and ??????????
?? nakere?ba?nara?nai ? ?have to?, respectively.  
In order to examine the adequacy and sufficiency 
of NPFs, we evaluated outputs of NPSC for 4,083 
input predicative parts, which had been taken 
randomly as a test set from newspaper articles and 
segmented by SEG. It produced a recall of 97.4% 
and a precision, 41.8%. The score of the recall 
seems to imply the sufficiency of the set of NPFs 
and NPCIs. Relatively low score of the precision is 
due to the system?s over-generation caused by the 
semantic ambiguities of NPCIs. Among various 
measures to be taken, firstly, semantic constraints 
to control the composition operation ??? may be 
effective to produce a better precision. The 
complete disambiguation measure is left to future 
work. 
5.3 Application to J/E Machine Translation 
We introduce here another experimental system, 
referred to as ENGL, whose input is the NPS of a 
sentence and whose output is its English forms, to 
demonstrate the usefulness of our formalism. 
ENGL simply realizes NPFs within English syntax.  
We assumed each NPF for English could be 
accomplished by applying rewriting rules of two 
types; i) V ? x ? Vv ? y and ii) S? x ? Sv ? y , where 
V is a verb or an auxiliary; Vv is V, a null string, or 
a variant of V; S, a sentence; Sv, a variant of S; and 
x, y, a null string or a string of specific words.  
Basically, a single rewriting rule is applied for a 
single NPF. However, occasionally, a NPF requires 
several rules to be applied successively. Also we 
may have no NPCI corresponding to a given NPF 
within the target language.  For example, 
POLITENESS, which is common in colloquial 
Japanese, has mostly no NPCI in English. 
For example, the computation for (12) is 
 
GUESS2 [OBLIGATION1 [??? yomu?]] 
= GUESS2 [OBLIGATION1 [read]] 
= GUESS2 [have to ? read] 
=will ? have to ? read, 
 
where the rewriting rules associated with  
NECESSITY1 and GUESS2 are V?have to ? Vroot 
and V?will ? Vroot , respectively. 
We give four more I/O examples  In (14), the 
instantaneous aspect of aruki ? hajimeru ; begin 
walk-ing excludes the possibility of the 
interpretations, PROGRESSING1, 
PROGRESSING2 and STATE-OF-THINGS of 
teiru, which remain in (13) or (15). This is because 
the system deals with concatenatability rules based 
on aspect features of the predicate.  (ENGL simply 
denotes the verb?s inflected form by -ed, -en, etc.)  
 
(13) nps(???/????? ; manan/?de?iru? ) 
=1 PROGRESSING1[study]= be study-ing, 
=2 PROGRESSING2[study]= have be-en study-
ing, 
=3 COMPLETED1[study]=have study-en 
 
(14)nps(? ? ? /? ? ? ? ? ? ? ? ; 
aruki/?hajime?te?iru?) 
=COMPLETED1[INCHOATIVE[walk]]= have 
begin-en walk-ing 
 
(15)nps(???/????? ; aisi/?te?iru? ) 
= STATE-OF-THINGS[love] = love 
 
(16)nps(????/????/???????/??????/?
? /? ?  ; 
ugokasi/?te?mi/?te?mo?yoi/?no?desho/?u/?ka?) 
=NTERROGATIVE[GUESS1[DECLARATION
[PERMISSIVE[TRIAL[move]]]]] 
= Will it be allowed that...try to move....? 
 
A small-scale experiment, for 300 NPSs extracted 
from sentences in technical papers has shown that 
ENGL produced a precision of 86% and a recall, 
80%. While these relatively high scores implies the 
fundamental validity of the NPF framework, more 
extensive tests will be required to make more 
reliable evaluation for the general domain, since 
technical papers tend to have less-complicated 
NPFs. In addition, further correction and 
refinement of synthesis rules for English will be 
necessary to obtain higher scores.  
6 Similarity between NPSs 
In this section, we show that our framework for the 
NPS description can be used properly to formalize 
some semantic or pragmatic relationship between 
non-propositionalized sentences. 
6.1 Logical Rules 
First, we discuss, here, the logical similarity 
relation, ? ?((?Fi)*)2, (1?i?8), which seems 
crutial for NLP tasks such as text retrieval or 
paraphrasing.6 We prefer the term, ?similarity? to 
?equivalence? here since it should be based on truth 
values taken in ?most situations?, or in some 
?similar? worlds. 7 
There are basic rules such as; 
 
(17) NEG-EVAL? NEGATION1  
?OBLIGATION2 
(18) NEGATION1?PERMISSION 
?PROHIBITION,  
(19) NEGATION1,2?NEGATION1,2 
? ?(identity function), 
(20) N??? ??N ?N for?N?(?Fi)* , (1?i?8),  
(21) POLITENESS ? ?, 
  
(17) asserts that, for example, an utterance, ?He 
has to go there.?  is similar to ?It is evaluated in 
the negative that he does not go there.?. Besides 
these basic rules, there is a set of logically notable 
rules. For example, from the observation that ??
? /? ? ? ? ? ? ? ? ? /? ? ? ? ? ? ? ? ?  
                                                     
6 While the NPF in Fi, (1?i?7) produces a truth conditional sentence, the NPF 
in F9 does not. The NPF in F8 produces a truth conditional sentence, unless it is 
used for the speaker?s epistemic judgment. 
7 But we do not enter further theoretical arguments here. 
hatarai/?te?bakari?iru/?wake?de?ha?nai?   ?do not 
always work?  is similar to ??????/??????
?   hataraka?nai/?toki?ga?aru? ?It happens 
occasionally that?do not work? the following rule 
will be induced; 
 
(22) NEGATION2?HIGHEST-FREQUENCY 
?LOW-FREQUENCY?NEGATION1. 
 
Also, ? ? ? /? ? ? /? ? ? ? ? ? ? 
hataraka/?naku/?te?mo?yoi? ?need not work?; ?It is 
allowed that...do not work?? and ???/?????
? ? ? ? ? ? ? /? ? ? ? ? ? ? 
hataraka/?nakere?ba?nara?nai/?koto?ha?nai? ?It is 
not obligatory that ?work??, will induce a rule; 
 
(23) PERMISSION?NEGATION1   
?NEGATION1?OBLIGATION. 
 
These rules can be generalized as (24), (24?) by 
introducing a ?duality? function, d defined below; 
 
 
        M, d(N)                          d(M), N 
----------------------------------------------------------- 
POSSIBILITY                     NECESSITY, 
HIGHEST-PROBABILITY, 
                                     HIGHEST-CERTAINTY 
LOW-FREQUENCY   HIGHEST-FREQUENCY, 
                                      HIGHEST-USUALITY 
PERMISSION                    OBLIGATION, 
                                  HIGHEST-INEVITABILITY  
----------------------------------------------------------- 
 
(24) NEGATION1,2?M ? d(M) ?NEGATION1,2, 
 
(24?) M ?NEGATION1,2? d(M) ?NEGATION1,2. 
 
We show two more examples; 
 
(22?) HIGHEST-FREQUENCY?NEGATION1 
?NEGATION2?LOW-FREQUENCY. 
nps(? ? ? /? ? ? ? ? ? ? ? ? ? ? ? 
hataraka/?nai?de?bakari?iru? ?It is always 
that?do not work?? ) ?nps(???/???????
? /? ? ? ? ? ? ? ? ? ? 
hataraku/?koto?ga?aru/?to?ha?ie?nai?  ?It does not 
happen that?sometimes work?? ). 
 
(23?) OBLIGATION?NEGATION1 
?NEGATION1?PERMISSION. 
nps(? ? ? /? ? ? ? ? ? ? ? ? ? 
hatarai/?te?ha?nara?nai? ?must not work? )  
?nps(??? /?? ??? /?? ??? ?? ?? ???  
hatarai/?te?yoi/?to?iu?koto?ha?nai? ?It is not 
permissible that?work?? ). 
 
Rule (24) corresponds to the axiom, ?????
??, in modal logic and its variants, e.g., deontic 
(Mally, 1926) or temporal (Prior, 1967) logic, 
where ? and ? are the necessity and possibility 
operator, respectively. 
6.2 Pragmatic Rules 
The similarity relation among the speaker?s 
attitude or intention toward the hearer is defined as 
a set, ??{ (a,b) | a,b ? (F1?F2??F9)* ? ((?i, 
1?i?l?fi?F9)?(?j, 1?j?m?gj?F9)), where 
a=f1?f2??fl, b=g1?g2??gm}. 
Some of the indirect speech acts (Searle, 1975) can 
be formulated as the similarity within our 
framework. Examples of the rules and their 
instances are; 
 
(25) REQUESTING  
?INTERROGATIVE?NEGATION1 
?CAPABILITY, 
?INTERROGATIVE?CAPABILITY, 
?POLITENESS?IMPERATIVE, 
?INTERROGATIVE?NEGATION1 
?BENE-TAKING, 
?INTERROGATIVE?NEGATION1 
?CAPABILITY?PASS-BENE-TAKING, 
?DESIRING?PASS-BENE-TAKING, 
?DESIRING?PASSIVE. 
nps(??/?????  mi/?te?kure?  ?Look at ?? ), 
? nps(??? /??? ?? ??? /??? /??  
miru/?koto?ga?deki/?nai/?ka?  ?Can?t you look 
at ??? ), 
?nps(???/?????????/?? 
miru/?koto?ga?dekiru/?ka? ?Can you look at 
???  ), 
?nps(??/????  mi/?nasai?  ?Please look at 
?? ),  
? nps(? ? /? ? ? ? ? /? ? ? /? ?  
mi/?te?kure/?nai/?ka?  ?Don?t you look at ? for 
me ??? ), 
? nps(? ? /? ? ? ? ? /? ? /? ? ? /? ?  
mi/?te?mora/?e/?nai/?ka?  ?Can?t I have you 
look at? for me??? ), 
?nps(??/??????/???  mi/?te?morai/?tai ? 
?I want you to look at ? for me?? ), 
?nps(??/???/???  mi/?rare/?tai?  ?I want 
you to look at ?? ). 
 
With respect to prohibition, invitation, 
permission and assertion, we have; 
 
(26) PROHIBITIVE 
?PROHIBITION, 
?NEGATION1?CAPABILITY. 
nps(???/??  hairu/?na?  ?Do not enter?? ) 
? nps(? ? ? /? ? ? ? ? ? ? ? ? ?  
hait/?te?ha?nara?nai?  ?You must not enter?? ) , 
? nps(? ? ? /? ? ? ? ? ? ? /? ? ?   
hairu/?koto?ga?deki/?nai?  ?You can not 
enter?? ),  
 
(27)INVITING 
?INTERROGATIVE? INVITING, 
?INTERROGATIVE?NEGATIVE1. 
nps(????/??  tabeyo/?u?  ?Let?s eat?? ) 
?nps(????/??/??  tabeyo/?u/?ka? 
      ?Will you eat???  ), 
?nps(???/???/??  tabe/?nai/?ka? 
     ?Don?t you eat???  ). 
 
(28)PERMISSIVE 
?POSSIBILITY. 
nps(??/?????  ki/?te?yoi?  ?You may wear?? ) 
? nps(? ? ? /? ? ? ? ? ? ? ? ? 
kiru/?koto?ga?dekiru?  ?You can wear?? ). 
 
(29)ASSERTING?PAST-TENSE? NEGATION1 
?INTERROGATIVE? PAST-TENSE 
nps(???/????/??/?? tabe/?nakat/?ta/?yo?;  
?I did not eat... ? ), 
?nps(??? /?? /???  tabe/?ta/?kai?; ?Did I 
eat ...? ? ). 
 
 
We have obtained approximately 30 pragmatic 
rules concerned with the NPCIs in Japanese. In the 
realistic tasks of NLP, application of these rules 
should be controlled by rather complicated 
conditions settled for each of them. For example, 
conditions for rules (25) ~ (28) will include that 
the agent of their complement sentence should be 
the second person, and for (29), the first.  Although 
the principle underlying these rules were discussed 
in a lot of literature, e.g., felicity condition in 
(Searle, 1975), etc., the whole picture has not been 
clarified for computational usage. 
7 Conclusions 
We have shown that as far as the non-inferential, 
Non-Propositional Content (NPC) in Japanese 
sentence is concerned, its semantic 
compositionality can be secured, provided 
sentence-final MWEs are adopted properly as 
NPCIs. Although the functional treatment of NPCs 
is not particularly new in the theoretical domain, 
our model is characterized by its broad 
syntactic/semantic coverage and its tractability in 
NLP. It connects syntax with semantics by actually 
defining 150 non-propositional functions (NPFs) 
for 1500 NPC indicators through a large-scale 
empirical study. The similarity equations presented 
here might lead to some formal system of 
?calculations? on the set of NPFs, which might be 
available for NLP in future.   
The syntactic coverage of our semantic/pragmatic 
model will surely be further broadened by 
investigating non-final parts of Japanese sentences. 
This research should focus on the sentence 
embedment whose main verb is epistemic or 
performative (Austin, 1962), and adverbs that take 
part in indicating NPCs.  
While the list of NPFs introduced in this paper will 
provide, we believe, a basis for analyzing the NPC 
of natural sentences, it might be possible, or rather 
necessary for particular task, to refine NPFs by 
enriching them with case-elements, more detailed 
degrees or subtle differences in meaning, etc.  
We have not solved the problem of semantically 
disambiguating each NPCI. Further, we know little 
about the language-dependency, consistency of the 
similarity rules. The language-dependency of NPS 
is interesting from the viewpoint of machine 
translation or comparative pragmatics. The 
frameworks presented here could hopefully 
provide tools for those comparative studies. 
References  
John L. Austin. 1962. How to Do Things with 
Words. Oxford U.P.  
H. Paul Grice. 1975. Logic and Conversation. In P. 
Cole and J. L. Morgan, editors, Syntax and 
Semantics Vol. 3, Speech Acts: 41-57. Academic 
Press.  
Ernst Mally. 1926. Grundgesetze des Sollens: 
Elemente der Logik des Willens. Universit?ts-
Buchhandlung: viii+85.  
Masahiro Miyazaki, Satoru Ikehara and Akio 
Yokoo. 1993. Combined Word Retrieval for 
Bilingual Dictionary Based on the Analysis of 
Compound Words. Trans. IPSJ 34-4: 743-754. 
(in Japanese) 
Yoshiyuki Morita and Masae Matsuki. 1989. 
Expression Pattern of Japanese. ALC Press. (in 
Japanese) 
Arthur Prior. 1967. Past, Present and Future. 
Clarendon press, Oxford.  
Iwan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake and Dan Flickinger. 2002. Multiword 
Expressions: A Pain in the Neck for NLP. The 
Proc. of the 3rd CICLING: 1-15. 
John R. Searle. 1975. Indirect Speech Acts. In P. 
Cole and J. L. Morgan, editors, Syntax and 
Semantics Vol. 3, Speech Acts: 59-82, Academic 
Press.   
Satoshi Shirai, Satoru Ikehara and Tsukasa  
Kawaoka. 1993. Effects of Automatic Rewriting 
of Source Language within a Japanese to 
English MT System. Fifth International 
Conference on Theoretical and Methodological 
Issues in Machine Translation: TMI-93: 226-239  
Kosho Shudo, Toshiko Narahara and Sho Yoshida. 
1980. Morphological Aspect of Japanese 
Language Processing. The Proc. of the 8th 
COLING: 1-8.  
Kosho Shudo, Kenji Yoshimura, Mitsuno Takeuti  
and Kenzo Tsuda. 1988. On the Idiomatic 
Expressions in Japanese Language ? An 
Approach through the Close and Extensive 
Investigation of Non-standard Usage of Words ? 
IPSJ SIG Notes, NL-66-1: 1-7. (in Japanese) 
Kosho Shudo. 1989. Fixed Collocations. Ministry 
of Education, Science, Sports and Culture, 
Grant-in-Aid for Scientific Research, 63101005. 
(in Japanese) 
Masako Yasutake, Yasuo Koyama, Kenji 
Yoshimura, Kosho Shudo. 1997. Fixed-
collocations and Their Permissible Variants. The 
Proc. of the 3rd Annual Meeting of ANLP: 449-
452. (in Japanese) 
 
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 161?170,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Comprehensive Dictionary of Multiword Expressions 
 
Kosho Shudo1, Akira Kurahone2, and Toshifumi Tanabe1 
1Fukuoka University, Nanakuma, Jonan-ku, Fukuoka, 814-0180, JAPAN 
{shudo,tanabe}@fukuoka-u.ac.jp 
2TechTran Ltd., Ikebukuro, Naka-ku, Yokohama, 231-0834, JAPAN 
kurahone@opentech.co.jp 
 
 
 
 
Abstract 
It has been widely recognized that one of the 
most difficult and intriguing problems in 
natural language processing (NLP) is how to 
cope with idiosyncratic multiword expressions. 
This paper presents an overview of the 
comprehensive dictionary (JDMWE) of 
Japanese multiword expressions. The JDMWE 
is characterized by a large notational, syntactic, 
and semantic diversity of contained expressions 
as well as a detailed description of their 
syntactic functions, structures, and flexibilities. 
The dictionary contains about 104,000 
expressions, potentially 750,000 expressions. 
This paper shows that the JDMWE?s validity 
can be supported by comparing the dictionary 
with a large-scale Japanese N-gram frequency 
dataset, namely the LDC2009T08, generated by 
Google Inc. (Kudo et al 2009). 
1 Introduction 
Linguistically idiosyncratic multiword expressions 
occur in authentic sentences with an unexpectedly 
high frequency. Since (Sag et al 2002), we have 
become aware that a proper solution of 
idiosyncratic multiword expressions (MWEs) is 
one of the most difficult and intriguing problems in 
NLP. In principle, the nature of the idiosyncrasy of 
MWEs is twofold: one is idiomaticity, i.e., non-
compositionality of meaning; the other is the 
strong probabilistic affinity between component 
words. Many attempts have been made to extract 
these expressions from corpora, mainly using 
automated methods that exploit statistical means. 
However, to our knowledge, no reliable, extensive 
solution has yet been made available, presumably 
because of the difficulty of extracting correctly 
without any human insight. Recognizing the 
crucial importance of such expressions, one of the 
authors of the current paper began in the 1970s to 
construct a Japanese electronic dictionary with 
comprehensive inclusion of idioms, idiom-like 
expressions, and probabilistically idiosyncratic 
expressions for general use. In this paper, we begin 
with an overview of the JDMWE (Japanese 
Dictionary of Multi-Word Expressions). It has 
approximately 104,000 dictionary entries and 
covers potentially at least 750,000 expressions. 
The most important features of the JDMWE are: 
1. A large notational, syntactic, and semantic 
diversity of contained expressions 
2. A detailed description of syntactic function and 
structure for each entry expression 
3. An indication of the syntactic flexibility of entry 
expressions (i.e., possibility of internal 
modification of constituent words) of entry 
expressions. 
In section 2, we outline the main features of the 
present study, first presenting a brief summary of 
significant previous work on this topic. In section 3, 
we propose and describe the criteria for selecting 
MWEs and introduce a number of classes of 
multiword expressions. In section 4, we outline the 
format and contents of the JDMWE, discussing the 
information on notational variants, syntactic 
functions, syntactic structures, and the syntactic 
flexibility of MWEs. In section 5, we describe and 
explain the contextual conditions stipulated in the 
JDMWE. In section 6, we illustrate some 
important statistical properties of the JDMWE by 
comparing the dictionary with a large-scale 
Japanese N-gram frequency dataset, the 
LDC2009T08, generated by Google Inc. (Kudo et 
al. 2009).  The paper ends with concluding remarks 
in section 7. 
161
2 Related Work 
Gross (1986) analyzed French compound adverbs 
and compound verbs. According to his estimate, 
the lexical stock of such words in French would be 
respectively 3.3 and 1.7 times greater than that of 
single-word adverbs and single-word verbs. 
Jackendoff (1997) notes that an English speaker?s 
lexicon would contain as many MWEs as single 
words. Sag et al (2002) pointed out that 41% of 
the entries of WordNet 1.7 (Fellbaum 1999) are 
multiword; and Uchiyama et al (2003) reported 
that 44% of Japanese verbs are VV-type 
compounds. These and other similar observations 
underscore the great need for a well-designed, 
extensive MWE lexicon for practical natural 
language processing.  
In the past, attempts have been made to produce 
an MWE dictionary. Examples include the 
following: Gross (1986) reported on a dictionary of 
French verbal MWEs with description of 22 
syntactic structures; Kuiper et al (2003) 
constructed a database of 13,000 English idioms 
tagged with syntactic structures; Villavicencio 
(2004) attempted to compile lexicons of English 
idioms and verb-particle constructions (VPCs) by 
augmenting existing single-word dictionaries with 
specific tables; Baptista et al (2004) reported on a 
dictionary of 3,500 Portuguese verbal MWEs with 
ten syntactic structures; Fellbaum et al (2006) 
reported corpus-based studies in developing 
German verb phrase idiom resources; and recently, 
Laporte et al (2008) have reported on a dictionary 
of 6,800 French adverbial MWEs annotated with 
15 syntactic structures. 
Our JDMWE approach differs from these 
studies in that it can treat more comprehensive 
types of MWEs. Our system can handle almost all 
types of MWEs except compositional compounds, 
named entities, acronyms, blends, politeness 
expressions, and functional expressions; in contrast, 
the types of MWEs that most of the other studies 
can deal with are limited to verb-object idioms, 
VPCs, verbal MWEs, support-verb constructions 
(SVCs) and so forth.  
Many attempts have been made to extract 
MWEs automatically using statistical corpus-based 
methods. For example, Pantel et al (2001) sought 
to extract Chinese compounds using mutual 
information and the log-likelihood measure. Fazly 
et al (2006) attempted to extract English verb-
object type idioms by recognizing their structural 
fixedness in terms of mutual information and 
relative entropy. Bannard (2007) tried to extract 
English syntactically fixed verb-noun 
combinations using pointwise mutual information, 
and so on.  
In spite of these and many similar efforts, it is 
still difficult to adequately extract MWEs from 
corpora using a statistical approach, because 
regarding the types of multiword expressions, 
realistically speaking, the corpus-wide distribution 
can be far from exhaustive. Paradoxically, to 
compile an MWE lexicon we need a reliable 
standard MWE lexicon, as it is impossible to 
evaluate the automatic extraction by recall rate 
without such a reference. The conventional idiom 
dictionaries published for human readers have been 
occasionally used for the evaluation of automatic 
extraction methods in some past studies. However, 
no conventional Japanese dictionary of idioms 
would suffice for an MWE lexicon for the practical 
NLP because they lack entries related to the 
diverse MWE objects we frequently encounter in 
common textual materials, such as quasi-idioms, 
quasi-clich?s, metaphoric fixed or partly fixed 
expressions. In addition, they provide no 
systematic information on the notational variants, 
syntactic functions, or syntactic structures of the 
entry expressions. The JDMWE is intended to 
circumvent these problems. 
In past Japanese MWE studies, Shudo et al 
(1980) compiled a lexicon of 3,500 functional 
multiword expressions and used the lexicon for a 
morphological analysis of Japanese. Koyama et al 
(1998) made a seven-point increase in the 
precision rate of kana-to-kanji conversion for a 
commercial Japanese word processor by using a 
prototype of the JDMWE with 65,000 MWEs. 
Baldwin et al (2003) discussed the treatment of 
Japanese MWEs in the framework of Sag et al 
(2002). Shudo et al (2004) pointed out the 
importance of the auxiliary-verbal MWEs and their 
non-propositional meanings (i.e., modality in a 
generalized sense).  Hashimoto et al (2009) 
studied a disambiguation method of semantically 
ambiguous idioms using 146 basic idioms. 
3 MWEs Selected for the JDMWE 
The human deliberate judgment is indispensable 
for the correct, extensive extraction of MWEs. In 
162
view of this, we have manually extracted 
multiword expressions that have definite syntactic, 
semantic, or communicative functions and are 
linguistically idiosyncratic from a variety of 
publications, such as newspaper articles, journals, 
magazines, novels, and dictionaries. In principle, 
the idiosyncrasy of MWEs is twofold: first, the 
semantic non-compositionality (i.e., idiomaticity); 
second, the strong probabilistic affinity between 
component words. Here we have treated them 
differently.  
 
The number of words included in a MWE ranges 
from two to eighteen. The length distribution is 
shown in Figure 1. 
 
0
5
10
15
20
25
30
35
40
45
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Length
C
o
n
s
t
it
u
e
n
t
 R
a
t
io
 (
%
)
 
Figure 1:  Length distribution of MWEs 
 
 
type example 
Idiom: Semantically 
Non-Compositional 
Expression 
?-?-?? aka-no-tanin  
(lit. red stranger) ?complete stranger? 
Morphologically or 
Syntactically Non-
Compositional 
Expression, Cranberry-
Type Expression 
?-?-?? to-ha-ie 
?however? 
SVC: Support-Verb 
Construction 
?? -?-???  hihan-wo-kuwaeru 
(lit. add criticism) ?criticize? 
Compound Noun; 
Compound Verb; 
Compound Adjective; 
Compound Adjective-
Verb 
??-???? uti-hisigareru 
(lit. be hit and smashed)  
?become depressed? 
Four-Character-Idiom ??-?? siri-meturetu ?incoherence? 
Metaphorical 
Expression 
?-?-?? inoti-no-kagiri 
(lit. limit of life) ?at the risk of life? 
Quasi-Idiom 
??-?-?? jisho-wo-hiku (lit. pull 
dictionary) ?look up in a dictionary? 
 
Table 1: Non-Compositional Expressions 
 
3.1 Non-Compositional MWEs 
In our approach, we use non-substitutability 
criterion to define a word string as an MWE, the 
logic being that an MWE expression is usually 
fixed in its form and the substitution of one of its 
constituent words would yield a meaningless 
expression or an expression with a meaning that is 
completely different from that of the original 
MWE expression. Formally, a word string w1w2?
wi?wn (2?n?18) is an MWE if it has a definite 
syntactic, semantic, or communicative function of 
its own, and if w1w2 ? wi? ? wn is either 
meaningless or has a meaning completely different 
from that of w1w2?wi?wn for some i, where wi? 
is any synonym or synonymous phrase of wi. For 
example, ?(w1)-?-?? aka-no-tanin  (lit. ?red 
stranger?) is selected because it has a definite 
nominal meaning of  ?complete stranger? and  
neither ??(w1?)-?-?? sinku-no-tanin nor ?
? ? (w1?)- ? - ? ? reddo-no-tanin means 
?complete stranger?. The evaluation of semantic 
relevance of MWEs was carried out by human 
judges entirely. It is just too difficult to judge the 
semantic relevance automatically and correctly. 
Table 1 shows a number of MWEs of this type.1 
3.2 Probabilistically Idiosyncratic MWEs 
An MWE must form a linguistic unit of its own. 
This and the following transition probability 
condition constitute another criterion that we adopt 
to define what an MWE is. Formally, a word string 
w1w2?wi?wn (2?n?18) is an MWE if it has a 
definite syntactic, semantic, or communicative 
function of its own, and if its forward or backward 
transition probability pf(wi+1|w1?wi) or pb(wi|wi+1
?wn), respectively is judged to be in the relatively 
high range for some i. With this definition, for 
example, ?-?-?? te-wo-komaneku ?fold arms? 
is selected as an MWE because it is a well-formed 
verb phrase and pb(? |? -?? ) is judged 
empirically to be very high. No general 
probabilistic threshold value can be fixed a priori 
because the value is expression-dependent. 
Although the probabilistic judgment was 
performed, for each expression in turn, on the basis 
of the developer?s empirical language model, the 
resulting dataset is consistent with this criterion on 
                                                          
1 These classes are not necessarily disjoint. 
163
the whole as shown in section 6.1. Table 2 lists 
some MWEs of this type.2 
 
type example 
Clich?, Stereotyped, 
Hackneyed, or  
Set Expression  
??-?-? fuuzen-no-tomosibi 
(lit. light in front of the wind)  
?candle flickering in the wind? 
Proverb, Old-Saying 
??-?-?? isoga-ba-maware 
(lit. make a detour when in a hurry)  
?more haste, less speed? 
Onomatopoeic or 
Mimetic Expression 
????-?-?? noronoro-to-aruku  
(lit. slouchingly walk) ?walk slowly? 
Quasi-Clich?, 
Institutionalized 
Phrase 
?-?-?-?-??? kata-no-ni-wo-orosu 
(lit. lower lord from the shoulder)  
?take a big load off one?s mind? 
 
Table 2: Probabilistically Idiosyncratic Expressions 
 
With entries like these, an NLP system can use the 
JDMWE as a reliable reference while effectively 
disambiguating the structures in the syntactic 
analysis process. 
 
Of the MWEs in the JDMWE, approximately 
38% and 92% of them were judged to meet 
criterion 3.1 and criterion 3.2, respectively. These 
are illustrated in Figure 2. 
 
 
 
Figure 2: Approximate constituent ratio of non-
compositional MWEs and probabilistically bound 
MWEs 
 
 
 
Figure 3: Example JDMWE entry 
                                                          
2 These classes are not necessarily disjoint. 
4 Contents of the JDMWE 
The JDMWE has approximately 104,000 entries, 
one for each MWE, composed of six fields, namely, 
Field-H, -N, -F, -S, -Cf, and -Cb.  The dictionary 
entry form of an MWE is stated in Field-H in the 
form of a non-segmented hira-kana (phonetic 
character) string. An example is given in Figure 3. 
4.1 Notational Information (Field-N) 
Japanese has three notational options: hira-kana, 
kata-kana, and kanji. The two kanas are 
phonological syllabaries. Kanji are originally 
Chinese idiographic characters. As we have many 
kanji characters that are both homophonic and 
synonymous, sentences can contain kanji 
replaceable by others. In addition, the inflectional 
suffix of some verbs can be absent in some 
contexts. The JDMWE has flexible conventions to 
cope with these characteristics. It uses brackets to 
indicate an optional word (or a series of 
interchangeable words marked off by the slash ?/?) 
in the Field-N description. Therefore, the entry 
whose Field-H (the first field) is?????? ki-
no-ii-yatu (lit. ?a guy who has a good spirit?) 
?good-natured guy?, can have (?/?)-?-(?/?/
?/?)?-(??/?/??) in its Field-N. The dash 
?-? is used as a word boundary indicator. This 
example can stand for twenty-four combinatorial 
variants, i.e., ??????,?, ?????,?,
??????. 
If fully expanded with this information, the 
JDMWE?s total number of MWEs can exceed 
750,000. 
4.2 Functional Information (Field-F) 
Linguistic functions of MWEs can be simply 
classified by means of codes, as shown in Tables 3 
and 4.  Field-F is filled with one of those codes 
which corresponds to a root node label in the 
syntactic tree representation of a MWE. 
 
code function size example 
Cdis 
Discourse- 
Connective 
1,000 
??-???-? ii-kaere-ba 
(lit. if (I) paraphrase)  
?in other words? 
Adv Adverbial 6,000 
???-? fusigi-to 
?strangely enough?  
Pren 
Prenominal- 
Adjectival 
13,700 ?-?? kaku-taru ?definite? 
164
Nom Nominal 12,000 
??-?-?? aku-no-tuyosa 
(lit. strong taste of lye) 
?strong harshness? 
Nd 
Nominal/ 
Dynamic 
4,700 
??-?? hitome-bore  
?love at first sight? 
Nk 
Nominal/State- 
describing 
5,400 
?-?-? ni-mai-jita  
?being double-tongued? 
Ver Verbal 49,000 
?-?-?? abura-wo-uru 
(lit. sell oil) ?idle away? 
Adj Adjectival 4,600 
?-?-??-??-??-??
me-ni-ire-temo-itaku-nai (lit. 
have no pain even if put into 
eyes) ?an apple in ones eye? 
K 
Adjective- 
Verbal 
3,500 
??-?? keiken-yutaka 
?abundant in experience? 
Ono 
Onomatopoeic 
or Mimetic 
Expression 
1,300 
????-? surasura-to 
?smoothly?, ?easily?, 
?fluently? 
  
Table 3: Syntactic Functions and Examples 
   
   
code function size  example 
_P 
Proverb, 
Old-Saying 
2,300 
??-?-??-?-??-? 
hyakubun-ha-ikken-ni-sika-zu (lit. 
hearing about something a 
hundred times is not as good as 
seeing it once) ?a picture is worth 
a thousand words? 
_Self 
Soliloquy, 
Monologue 
200 
??-?-?? komat-ta-naa  
?Oh boy, we?re in trouble!? 
_Call Call, Yell 150 
??-??-?-? 
sumi-mase-n-ga ?Excuse me.? 
_Grt Greeting 200 
??????-?? irasshai-mase 
?Welcome!? 
_Res Response 350 
??-???-??-? 
dou-itasi-masi-te  
?You?re welcome.? 
  
Table 4: Communicative Functions and Examples 
 
4.3 Structural Information (Field-S) 
4.3.1   Dependency Structure  
The dependency structure of an MWE is given in 
Field-S by a phrase marker bracketing the 
modifier-head pairs, using POS symbols for 
conceptual words.3 For example, an idiom ???-
? - ?  makka-na-uso (lit. ?crimson lie?) 
?downright lie? is given a marker [[K00 na] N]. 
This description represents the structure shown in 
Figure 4, where K00 and N are POS symbols 
denoting an adjective-verb stem and a noun, 
respectively. 
                                                          
3 The intra-sentential dependency relation in Japanese is 
unilateral, i.e., the left modifier depends on the right head. 
The JDMWE contains 49,000 verbal entries, 
making this the largest functional class in the 
JDMWE. For these verbal entries, more than 90 
patterns are actually used as structural descriptors 
in Field-S. This fact can indicate the broadness of 
the structural spectrum of Japanese verbal MWEs. 
Some examples are shown in Table 5.  
 
 
 
Figure 4: Example of dependency structure given 
in Field-S 
  
  
example of structural 
pattern of verbal MWE 
example of MWE 
[[N wo] V30] 
?-?-??? i-wo-tonaeru (lit. chant 
the difference) ?raise an objection? 
[[N ga] V30] 
??-?-?? yori-ga-modoru (lit. the 
twist comes undone) ?get reconciled? 
[[N ni] V30] 
?-?-??? te-ni-ireru 
(lit. put...into hands) ?get?, ?obtain? 
[[[[N no] N] ga] V30] 
??-?-?-?-??? bake-no-kawa-
ga-hageru (lit. peel off disguise) 
?expose the true colors? 
[[[[N no] N] ni] V30] 
?-?-?-?-?? tama-no-kosi-ni-
noru 
(lit. ride on a palanquin for the nobility)  
?marry into wealth? 
[[N de][[N wo] V30]] 
?-?-?-?-?? ago-de-hito-wo-tukau 
(lit. use person by a chin)  
?order a person around? 
[[N ni][[N ga] V30]] 
?-?-?-?-?? siri-ni-higa-tuku (lit. 
buttocks catch fire) ?get in great haste? 
[[V23 te] V30] 
??-?-??? ki-te-otosu 
(lit. cut and drop) ?cut off? 
[[V23 ba] V30] 
??-?-?? ute-ba-hibiku 
(lit. reverberate if hit) ?respond quickly? 
[[[[N ni] V23] te] V30] 
?-?-??-?-???  
taba-ni-nat-te-kakaru 
(lit. attack someone by becoming a 
bunch) ?attack all at once? 
[Adv [[N ga] V30]] 
???-??-?-??  
dotto-tukare-ga-deru  
(lit. fatigue bursts out)  
?being suddenly overcome with fatigue? 
 
Table 5: Examples of structural types of verbal 
MWEs (N: noun, V23: verb (adverbial form), V30: 
verb (end form), Adv: adverb, wo, ga, ni, no, de, te, 
and ba: particle)  
 
165
4.3.2   Coordinate Structure 
Approximately 2,500 MWEs in the JDMWE 
contain internal coordinate structures. This 
information is described in Field-S by bracketing 
with ?<? and ?>?, and the coordinated parts by ?(? 
and ?)?. The coordinative phrase specification 
usually requires that the conjuncts must be parallel 
with respect to the syntactic function of the 
constituents appearing in the bracketed description.  
For example, an expression ?-?-?-?-??-?-
?-?? ato-ha-no-to-nare-yama-to-nare (lit. ?the 
rest might become either a field or a mountain?) 
?what will be, will be?, has an internal coordinate 
structure. Thus, its Field-S is [[N ha]<([[N to] 
V60])([[N to] V60])>]. This description represents 
the structure shown in Figure 5, where V60 denotes 
an imperative form of the verb. 
  
  
 
 
Figure 5: Example of the coordinate structure 
shown by ?<? and ?>? in Field-S 
 
 
4.3.3   Non-phrasal Structure  
Approximately 250 MWEs in the JDMWE are 
syntactically ill-formed in the sense of context-free 
grammar but still form a syntactic unit on their 
own. For example, ??? -?? -?? -??
yurikago-kara-hakaba-made ?from the cradle to 
the grave? is an adjunct of two postpositional 
phrases but is often used as a state-describing noun 
as in ???-??-??-??-?-?? yurikago-
kara-hakaba-made-no-hoshou (lit. security of from 
cradle to grave) ?security from the cradle to the 
grave?. Thus Field-F and Field-S have a functional 
code Nk and a description [[N kara][[N made] $]], 
respectively. The symbol ?$? denotes a null 
constituent occupying the position of the governor 
on which this MWE depends. This structure is 
shown in Figure 6. 
 
 
Figure 6: Example of a non-phrasal expression 
with a null constituent marked with ?$? in Field-S 
 
The total number of structural types specified in 
Field-S is nearly 6,000. This indicates that 
Japanese MWEs present a wide structural variety. 
 
4.3.4   Internal Modifiability  
Some MWEs are not fixed-length word strings, but 
allow the occurrence of phrasal modifiers 
internally. In our system, this aspect is captured by 
prefixing a modifiable element of the structural 
description stated in the Field-S with an asterisk 
?*?. An adverbial MWE ?-?-??-?-?-? ue-
ni-nobe-ta-you-ni ?as I explained above? is one 
such MWE and thus has a description [[[[[N ni] 
*V23] ta] N] ni] in Field-S, meaning that the third 
element V23 is a verb that can be modified 
internally by adverb phrases. Since the asterisk 
designates such optional phrasal modification, our 
system allows   a   derivative   expression   like   ?
?  -?-?-?-???-??-?-?-? riyuu-wo-
ue-ni-kuwasiku-nobe-ta-you-ni ?as I explained in 
detail the reason above?, which contains two 
additional, internal modifiers. The structure is 
shown in Figure 7.4 
 
 
 
Figure 7: Example of internal modifiability marked 
by ?*? in Field-S 
 
                                                          
4 The positions to be taken by an internal modifier can be 
easily decided by the structural description given in Field-S 
along with the nest structure requirement. 
166
Roughly speaking, 30,000 MWEs in the JDMWE 
have no asterisk in their Field-S. Our rigid 
examination reveals that internal modification is 
not allowed for them. 
5 Contextual Condition (Field-Cf , Cb) 
Approximately 6,700 MWEs need to be classified 
differently because they require particular forward 
contexts, i.e., they require co-occurrence of a 
particular syntactic phrase in the context that 
immediately precedes them. For example, ?-?-
??  kao-wo-suru (lit. ?do face?) which is a 
support-verb construction, cannot occur without an 
immediately preceding adnominal modifier, e.g., 
the adjective ??? kanasii  ?sad?, yielding ??
?-?-?-?? kanasii-kao-wo-suru (lit. ?do sad 
face?) ?make a sad face?. This adnominal modifier 
co-occurrence requirement is stipulated in Field-Cf 
by a code <adnom. modifier>. There are about 30 
of these forward contextual requirements. 
Similarly, backward contextual requirements, of 
which there are about 70, are stated in Field-Cb. 
Approximately 300 MWEs require particular 
backward contexts. 
 
6 Statistical Properties 
Without a rule system of semantic composition, it 
is difficult to evaluate the validity of the JDMWE 
concerning idiomaticity. However, we can confirm 
that 3,600 Japanese standard idioms that Sato 
(2007) listed from five Japanese idiom dictionaries 
published for human readers are included in the 
JDMWE as a proper subset. In addition, the 
JDMWE contains the information about their 
syntactic functions, structures, and flexibilities. 
6.1 Comparison with Web N-gram  
Frequency Data 
We examined the statistical properties of the 
JDMWE using the Japanese Web N-gram, version 
1: LDC2009T08, which is a word N-gram (1?N?7) 
frequency dataset generated from 2 ? 1010 
sentences in a Japanese Web corpus, supplied by 
Google Inc. (Kudo et al 2009). We will refer to 
this (or the Web corpus examined) subsequently as 
GND. We will refer to trigram w1w2w3 as an NpV-
trigram only when w1 and w3 are restricted to a 
noun and a verb (end form), respectively, and w2 is 
one of the following case-particles: accusative ?
wo, subjective? ga, or dative? ni.5 We write the 
number of occurrences of an expression x, counted 
in the GND, as C(x). 
 
First, we obtain from the GND sets G, T, D, B, 
and Ri?s defined below, using a Japanese word 
dictionary IPADIC (Asahara et al 2003): 
 
G={w1w2w3| w1w2w3 ? GND, w1w2w3 is an  
NpV-trigram.}  
T={w1w2w3| w1w2w3 ? JDMWE, w1w2w3  is an 
NpV-trigram.} 
D={w1w2|?w3, w1w2w3? G} 
B={w1w2|?w3, w1w2w3? T} 
Ri={w1w2w3| w1w2w3? T, C(w1w2w3) is the i-th 
largest among C(w1w2v)?s for all w1w2v ? G}. 
 
We then found the following data: 
?|B|=10,548  
?|D|=110,822 
? |R1|=4,983, |R2|=1,495, |R3|=786, |R4|=433,  ?  
From these, we realize, for example, that 47.2% 
=(|R1|/|B|)?100 of trigrams in T have verbs that 
occur most frequently in the GND, succeeding the 
individual bigrams. An example of such a trigram 
is?????-?-??? akushon-wo-okosu  (lit. 
?raise action?) ?take action?. Similarly, 14.0%= 
(|R2|/|B|)?100 have the second most frequent verbs, 
7.5% have the third most frequent verbs, and so on. 
Figure 8(a) illustrates the results. From this, we can 
assume that the higher probability pf(w3|w1w2) a 
trigram w1w2w3 has, the more likely w3 is chosen 
for each w1w2 in the JDMWE. This is consistent 
with what we wrote in section 3.2. Figure 8(b) is 
the accumulative substitute of Figure 8(a). 
Extrapolating Figure 8(b) suggests that 10% of 
NpV-trigrams in the JDMWE do not occur in the 
GND. This implies that the size, i.e., 2?1010 
sentences of the Web corpus used by the GND is 
not sufficiently large to allow MWE extraction. 6 
 
                                                          
5 The NpV-trigrams represent the typical forms of shortest 
Japanese sentences, corresponding roughly to subject-verb, 
verb-object/direct, and verb-object/indirect constructions in 
English. 
6 Otherwise, the frequency cut-off point of 20 adopted in GND 
is too high.  
167
(a)
(b)
0
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Rank
C
o
n
s
t
i
t
u
e
n
t
 
R
a
t
i
o
 
(
%
)
 
Figure 8 (a): Constituent ratio (|Ri|/|B|)?100 for 
rank i of probability pf(w3|w1w2); (b): 
Accumulative variant of (a) for rank i of 
probability pf(w3|w1w2) 
 
 
Second, we calculate the (normalized) entropy 
Hf(w3|w1w2) for each w1w2 ? D defined below, 
where the probability pf(w3|w1w2) is estimated by 
C(w1w2w3)/C(w1w2). This provides a measure of 
the flatness of the pf(w3|w1w2) distribution 
canceling out the influence of the number N of 
verb types w3?s.  
 
Hf(w3|w1w2) 
= ? (
?
3w
pf(w3|w1w2) log pf(w3|w1w2)) / log N 
 
After arranging 110,822 bigrams in D in ascending 
order of Hf(w3|w1w2), we divided them into 20 
intervals A1, A2, ? , A20 each with an equal number 
of bigrams (5,542). We then examined how many 
bigrams in B were included in each interval. 
Figures 9(a) and (b) plot the resulting constituent 
ratio of the bigrams in B and the mean value of 
Hf(w3|w1w2)?s in each interval, respectively. We 
found, for example, that 1,262 out of 5,542 
bigrams are in B for the first interval, i.e., the 
constituent ratio is 22.8%=(1,262/5,542)? 100. 
Similarly, we obtain 22.5%=(1,248/5,542)?100 
for the second interval, 20.5%=(1,136/5,542)?100 
for the third, and so on. From this, we realize the 
macroscopic tendency that the larger the entropy 
Hf(w3|w1w2), or equivalently the perplexity of the 
succeeding verb w3, a bigram w1w2 has, the less 
likely it is adopted as a prefix of a trigram in T.  
Taking the results in Figure 8 and Figure 9 
together, we can presume that not only frequently 
but also exclusively occurring verbs would be the 
preferred choice in T. 
 
(a)
(b)
0
5
10
15
20
25
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Interval
C
o
n
s
t
i
t
u
e
n
t
 
R
a
t
i
o
 
(
%
)
0
0.2
0.4
0.6
0.8
1
A
v
e
r
a
g
e
 
E
n
t
r
o
p
y
 
Figure 9 (a): Constituent ratio of the bigrams in B 
among bigrams in D in interval k (1?k?20); (b): 
Mean value of entropies Hf(w3|w1w2)?s in the 
interval k (1?k?20) 
 
 
This suggests the general feasibility of the 
JDMWE, for its relative compactness, in 
effectively disambiguating the syntactic structures 
of input word strings. 
The above investigations were carried out on the 
forward conditional probabilities for restricted 
types of MWEs. However, the results imply a 
general validity of the JDMWE since the same 
criteria for selection were applied to all kinds of 
multiword expressions. 
6.2 Occurrences in Newspapers 
We examined 2,500 randomly selected sentences 
in Nikkei newspaper articles (published in 2009) to 
determine how many MWE tokens of the JDMWE 
occur in them. We found that in 100 sentences an 
average of 74 tokens of our MWEs were used. This 
suggests a large lexical coverage of the JDMWE. 
7 Concluding Remarks 
The JDMWE is a slotted tree bank for 
idiosyncratic multiword expressions, annotated 
with detailed notational, syntactic information.  
The idea underlying the JDMWE is that the 
volume and meticulousness of the lexical resource 
crucially affects the outcome of the rule-oriented, 
large-scale NLP. In view of this, the JDMWE was 
designed to encompass the wide range of linguistic 
objects related to Japanese MWEs, by placing 
importance on the recall rate in the selection of the 
168
candidate expressions. 7  The statistical properties 
clarified in this paper imply the general feasibility 
of the JDMWE at least in the probabilistic respect. 
Possible fields of application of the JDMWE 
include, for example:  
?Phrase-based machine translation 
?Phrase-based speech recognition 
?Phrase-based kana-to-kanji conversion 
?Search engine for Japanese corpus 
?Paraphrasing system 
?Japanese dialoguer 
?Japanese language education system 
Another aspect of the JDMWE is that it would 
provide linguists with lexicological data. For 
example, the usage of Japanese onomatopoeic 
adverbs, which are mostly bound probabilistically 
to specific verbs or adjectives, is extensively 
catalogued in the JDMWE. 
The first version of the JDMWE will be released 
after proofreading.8 If possible, we would like to 
add further information to each MWE on 
morphological variants, passivization, 
relativization, decomposability, paraphrasing, and 
semantic disambiguation for future versions. 
Acknowledgments 
We would like to thank the late Professors 
Toshihiko Kurihara and Sho Yoshida, who 
inspired our current research in the 1970s. Similar 
thanks go to Makoto Nagao. We are also grateful 
to everyone who assisted in the development of the 
JDMWE. Further special thanks go to Akira 
Shimazu, Takano Ogino, and Kenji Yoshimura for 
their encouragement and useful discussions, to 
those who worked on the LDC2009T08 and 
IPADIC, to the three anonymous reviewers for 
their valuable comments and advice, and to 
Stephan Howe for advice on matters of English 
style in the current paper. 
References  
Asahara, M. and Matsumoto, Y. 2003. IPADIC version 
2.7.0  User?s Manual (in Japanese).  NAIST, 
Information Science Division. 
                                                          
7 The time required to compile this dictionary is estimated at 
24,000 working hours. 
8 A portion of the JDMWE is available at http://jefi.info/. 
Baldwin, T. and Bond, F. 2003. Multiword Expressions: 
Some Problems for Japanese NLP. Proceedings of 
the 8th Annual Meeting of the Association for 
Natural Language Processing (Japan): 379?382. 
Bannard, C. 2007. A Measure of Syntactic Flexibility 
for Automatically Identifying Multiword Expressions 
in Corpora.  Proceedings of A Broader Perspective 
on Multiword Expressions, Workshop at the ACL 
2007 Conference: 1?8. 
Baptista, J., Correia, A., and Fernandes, G. 2004. 
Frozen Sentences of Portuguese: Formal Descriptions 
for NLP.  Proceedings of ACL 2004 Workshop on 
Multiword Expressions: Integrating Processing: 72?
79. 
Fazly, A. and Stevenson, S. 2006. Automatically 
Constructing a Lexicon of Verb Phrase Idiomatic 
Combinations.  Proceedings of the 11th Conference 
of the European Chapter of the ACL: 337?344. 
Fellbaum, C. (ed.) 1999. WordNet. An Electronic 
Lexical Database, Cambridge, MA: MIT Press. 
Fellbaum, C., Geyken, A., Herold, A., Koerner, F., and 
Neumann, G. 2006. Corpus-Based Studies of German 
Idioms and Light Verbs. International Journal of 
Lexicography, Vol. 19, No. 4: 349-360. 
Gross, M. 1986. Lexicon-Grammar. The Representation 
of Compound Words. Proceedings of the 11th 
International Conference on Computational 
Linguistics, COLING86:1?6. 
Hashimoto, C. and Kawahara, D. 2009. Compilation of 
an Idiom Example Database for Supervised Idiom 
Identification. Language Resource and Evaluation 
Vol. 43, No. 4 : 355-384. 
Jackendoff, R. 1997. The Architecture of Language 
Faculty. Cambridge, MA: MIT Press. 
Koyama, Y., Yasutake, M., Yoshimura, K., and Shudo, 
K. 1998. Large Scale Collocation Data and Their 
Application to Japanese Word Processor Technology.  
Proceedings of the 17th International Conference on 
Computational Linguistics, COLING98: 694?698. 
Kudo, T. and Kazawa, H. 2009. Japanese Web N-gram 
Version 1. Linguistic Data Consortium, Philadelphia. 
Kuiper, K., McCan, H., Quinn, H., Aitchison, T., and 
Van der Veer, K. 2003. SAID: A Syntactically Anno    
tated Idiom Dataset. Linguistic Data Consortium  
2003T10. 
Laporte, ?. and Voyatzi, S. 2008. An Electronic 
Dictionary of French Multiword Adverbs. 
Proceedings of the LREC Workshop towards a 
Shared Task for Multiword Expressions (MWE 
2008): 31?34. 
169
Pantel, P. and Lin, D. 2001. A Statistical Corpus-Based 
Term Extractor. Proceedings of the 14th Biennial 
Conference of the Canadian Society on 
Computational Studies of Intelligence, Springer-
Verlag: 36?46. 
Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and 
Flickinger, D. 2002. Multiword Expressions: A Pain 
in the Neck for NLP.  Proceedings of the 3rd 
International Conference on Intelligent Text 
Processing and Computational Linguistics, 
CICLING2002: 1?15. 
Sato, S. 2007. Compilation of a Comparative List of 
Basic Japanese Idioms from Five Sources (in 
Japanese). IPSJ SIG Notes 178: 1-6. 
Shudo, K., Narahara, T., and Yoshida, S. 1980. 
Morphological Aspect of Japanese Language 
Processing.  Proceedings of the 8th International 
Conference on Computational Linguistics, 
COLING80:  1?8. 
Shudo, K., Tanabe, T., Takahashi, M., and Yoshimura, 
K. 2004. MWEs as Non-Propositional Content 
Indicators. Proceedings of ACL 2004 Workshop on 
Multiword Expressions: Integrating Processing: 31?
39. 
Uchiyama, K. and Ishizaki, S. 2003. A Disambiguation  
of Compound Verbs.  Proceedings of ACL 2003. 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment: 81?88. 
Villavicencio, A. 2004. Lexical Encoding of MWEs. 
Proceedings of ACL 2004 Workshop on Multiword 
Expressions: Integrating Processing: 80?87. 
170
