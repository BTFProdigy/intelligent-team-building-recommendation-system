Concept Identif ication and Presentat ion in the Context  of  
Technical Text Summarizat ion 
Horac io  Sagg ion*and Guy  Lapa lme 
Ddpartement d' Informatique et Recherche Op~rationnelle 
Universit~ de Montreal 
CP 6128, Succ Centre-Ville 
Montreal, Quebec, Canada, H3C 3J7 
Fax: +1-514-343-5834 
{sagg ion ,  lapalme}@iro,  umontrea l ,  ca 
Abst rac t  
We describe a method of text summarization 
that produces indicative-informative abstracts 
/ 
for technical papers. The abstracts are gener- 
ated by a process of conceptual identification, 
topic extraction and re-generation. We have 
carried out an evaluation to assess indicative- 
ness and text acceptability relying on human 
judgment. The results o far indicate good per- 
formance in both tasks when compared with 
other summarization technologies. 
1 In t roduct ion  
We have specified a method of text summa- 
rization which produces indicative-informative 
abstracts for technical documents. The method 
was designed to identify the "topics" of a 
document and present them in an indicative 
abstract. Eventually, they can be elaborated in  
.specific ways. 
In Figure 1, we present an indicative abstract 
for the document "Facilitating designer-. 
? customer communication i the World Wide 
Web" (Internet Research: Electronic Net- 
working Applications and Policy, Vol 8, Issue 
5,1998) produced with our implementation 
of this method. The abstract includes a list 
of topics which are terms appearing in the 
automatic abstract (e.g. WebShaman)  or 
obtained from the source document by the 
process of term expansion (e.g. WWW 
technique obtained from technique).  It also 
* The first author is supported by Agence Canadienne 
de D~veloppement International (ACDI) and FundaciSn 
Antorchas (A-13671/1-47), Argentin a. He was previ- 
ously supported by Ministerio de EducaciSn de la Naci6n 
de la Repfiblica Argentina (ResoluciSn 1041/96) and De- 
partamento de ComputaciSn, Facultad e Ciencias Exac- 
tas y Naturales, UBA, Argentina. 
includes term elaborations which can be used 
to answer specific questions about the topics 
such as what  is topic? how top ic  is used? 
who developed topic? and what  are the  
advantages of topic? 
. 
In this paper, we will describe how we dealt 
with the problem of content selection and 
presentation and how we have evaluated our 
method of text summarization. 
2 Text  Summar izat ion  
The process of producing a summary from 
a source text consists of the following steps: 
(i) the interpretation of the text; (ii) the 
extraction of the relevant information which 
ideally includes the "topics" of the source; (iii) 
the condensation of the extracted information 
and construction of a summary representation; 
and (iv) the presentation of the summary rep- 
resentation to the reader in natural anguage. 
While some techniques exist for producing 
summaries for domain independent texts 
(Luhn, 1958; Marcu, 1997) it seems that 
domain specific texts require domain specific 
techniques (DeJong, 1982; Paice and Jones, 
1993). In our case, we are dealing with techni- 
cal articles which are the result of the complex 
process of scientific inquiry that starts with 
the. identification of a knowledge problem and 
eventually culminates with the discovery of 
an answer to it. Even if authors of technical 
articles write about several concepts in their 
articles, not all of them are topics. In order to 
address the issue of topic identification, content 
selection and presentation, we have studied 
alignments (manually produced) of sentences 
from professional bstracts with sentences from 
Abst ract  In t roduc ing  the Topics 
! 
! 
Virtual prototyping is a technique which has been suggested for use in, for example, telecommuni- 
cation product development asa high-end technology to achieve a quick digital model that could 
be used in the same way as a real prototype. Presents the design rationale of WebShaman, starting 
from the concept design perspective by introducing a set of requirements to support communication 
via a concept model between i dustrial designer and a customer. In the article, the authors uggest 
that virtual prototyping in collaborative use between designers i a potential technique to facilitate 
design and alleviate the problems created by geographical distance and complexities in the work 
between different parties. The technique, was implemented in the VRP project, allows compo- 
nent level manipulation ofa virtual prototype in a WWW (World Wide Web) browser. The user 
services, the software architecture, and the techniques ofWebShaman were developed iteratively 
during the fieldwork in order to illustrate the ideas and the feasibility of the system. The server is 
not much different from the other servers constructed tosupport synchronous collaboration. 
Identif ied Topics: 3D mode l  - V IRP I  p ro jec t  - WWW - WW-vV techn ique  - WebShaman - 
CAD sys tem - conceptua l .mode l -  cus tomer  - ob jec t -o r iented  mode l -  p roduct  - p roduct  
concept  - p roduct  des ign  - requ i rement  - s imu la t ion  mode l  - smar t  v i r tua l  p ro to type  
- so f tware  component  - sys tem-  techn ique  - techno logy  - use  - v i r tua l  component -  
v i r tua l  p ro to type  - v i r tua l  p ro to type  sys tem-  v i r tua l  p ro to typ ing  
I n fo rmat ion  about  the  Top ics  
An example of a conceptual model, a pen-shaped 'w i re less  user  in ter face  for a mobile 
telephone. 
A v i r tua l  p ro to type  is a computer -based  s imulat ion  of a prototype or a subsystem with a 
degree of functional realism, comparable to that of a physical prototype. 
A computer system implementing the high-end aspects  o f  v i r tua l  prototyping has been 
deve loped in the VRP project (VRP, 1998) at VTT Electronics, in Oulu, Finland. 
The two-and-a-half-year VIRPI pro jec t  cons is ts  o f  th ree  parts. 
Nowadays, CAD (computer -a ided  des ign)  sys tems are  used  as an aid in industrial, mechan- 
ical and electronics design for the specification and deve lopment  o f  a product .  
A v i r tua l  p ro to type  sys tem can  be  used  for concept testing in the early phase of product 
development. 
Figure h Indicative Abstract, Topics and Topic Elaboration 
I 
I 
i 
l 
i 
H 
I 
i 
I 
I 
I 
source documents. One of the alignments is 
presented in Table 1. The first column contains 
the information of the professional abstract. 
The second and third columns contain the infor- 
mation from the source document that matches 
the sentences of the professional bstract, and 
its location in the source document. We have 
produced 100 of these tables containing a 
total of 309 sentences of professional bstracts 
aligned with 568 sentences ofsource documents. 
These alignments allowed us to identify on 
one hand, concepts, relations and types of 
information usually conveyed in abstracts; and 
on the other hand, valid transformations in 
the source in order to produce a compact and 
coherent ext. The transformations include 
verb transformation, concept deletion, concept 
reformulation, structural deletion, parenthetical 
deletion, clause deletion, acronym expansion, 
2 
Professional  Abst ract  
Presents a more efficient 
Distributed Breadth-First 
Search algorithm for an 
asynchronous communication 
network. 
Source Document  
Efficient distributed breadth-first earch algo- 
rithm. 
In this paper we have presented a more effi- 
cient distributed algorithm which construct a
breadth-first search tree in an asynchronous 
communication network 
P /T  
-/Title 
Lst/- 
Presents a model and gives an First we present a model and give overview of lst/- 
overview of related research, related research. 
Analyzes the complexity of the algo- We analyse the complexity of our algorithm, lst/- 
rithm, and gives some examples of per- and give some examples of performance on 
formance on typical networks, typical networks. 
Table 1: LISA Abstract 1955 - Source Document: "Efficient distributed breadth-first search algo- 
rithm." S.A.M. Makki. Computer Communications, 19(8) Jul 96, p628-36. 
abbreviation, merge and split. In our corpus, 
89% of the sentences from the professional 
abstracts included at least one transformation. 
Results of the corpus study are detailed in 
(Saggion and Lapalme, 1998) and (Saggion and 
Lapalme, 2000). 
We have identified a total of 52 different 
types of information (coming from the corpus 
and from technical articles) for technical text 
summarization that we use to identify some of 
the main themes. These types include: the 
explicit topic of the document, the  situa- 
tion, the  ident i f icat ion of the  problem, the  
' identif ication of the  solution, the  research 
goal ,  the  explicit topic of a section, the  
? authors '  deve lopment ,  he  inferences, the  
descr ipt ion of a topical entity, the  def init ion 
? of a topical entity, the  re levance of a topical 
enthy, the  advantages,  etc. Information 
types are classified as indicative or informative 
depending on the type of abstract they con- 
tribute to (i.e. the  topic of a document is 
indicative while the  descr ipt ion of a topical 
entity is informative). Types of information are 
identified in sentences of the source document 
using co-occurrence of concepts and relations 
and specific linguistic patterns. Technical 
articles from different domains refer to specific 
concepts and relations (diseases and treatments 
in Medicine, atoms and chemical reactions 
in Chemistry, and theorems and proofs in 
Mathematics). We have focused on concepts 
and relations that are common across domains 
such as problem, solution, research need, 
experiment, relevance, researchers~ etc. 
3 Text  In terpretat ion  
Our approach to text summarization is based 
on a superficial analysis of the source docu- 
ment and on the implementation f some text 
re-generation techniques such as merging of top- 
ical information, re-expression of concepts and 
acronym expansion. The article (plain text 
in English without mark-up) is segmented in 
main units (title, author information, author 
abstract, keywords, main sections and refer- 
ences) using typographic information and some 
keywords. Each unit is passed through a bi- 
pos statistical tagger. In each unit, the sys- 
tem identifies titles, sentences and paragraphs, 
and then, sentences are interpreted using finite 
state transducers identifying and packing lin- 
guistic constructions and domain specific con- 
structions. Following that, a conceptual dictio- 
nary that relates lexical items to domain con- 
cepts and relations is used to associate seman- 
tic tags to the different structural elements in 
the sentence. Subsequently, terms (canonical 
form of noun groups), their associated semantic 
(head of the noun group) and theirs positions 
are extracted from each sentence and stored in 
an AVL tree (te~ t ree)  along with their fre- 
quency. A conceptual  index is created which 
specifies to which particular type of informa- 
tion each sentence could contribute. Finally, 
terms and words are extracted from titles and 
3 
stored in a list (the top ica l  s t ructure)  and 
acronyms and their expansions are recorded. 
3.1 Content  Select ion 
In order to represent types of information we use 
templates. In Table 2, we present he Topic 
of the  Document ,  Topic of the  Sect ion 
and Signal ing In fo rmat ion  templates. Also 
presented are some indicative and informative 
patterns. Indicative patterns contain variables, 
syntactic constructions, domain concepts and 
relations. Informative patterns also include one 
specific position for the topic under considera- 
tion. Each element of the pattern matches one 
or more elements of the sentence (conceptual, 
syntactic and lexical elements match one ele- 
ment while variables match zero or more). 
3.1.1 Ind icat iveness  
The system considers entences ~hat were iden- 
tified as carrying indicative information (their 
position is found in the conceptual  index). 
Given a sentence? S and a type of information 
T the system verifies if the sentence matches 
some of the patterns associated with type T. 
For each matched pattern, the system extracts 
information from the sentence and instantiates 
a template of type T. For example, the 
Content slot of the prob lem ident i f icat ion 
template is instantiated with all the sentence 
? :(avoiding references, structural elements and 
parenthetical expressions) while the What slot 
'of the topic of the  document  template is 
instantiated with a parsed sentence fragment 
? to the left or to the right of the make known 
relation depending on the attribute voice of the 
verb (active vs. passive). All the instantiated 
templates constitute the Indicative Data Base 
(IDB). 
The system matches the topical structure 
with the topic candidate slots from the IDB. 
The system selects one template for each term 
in that structure: the one with the greatest 
weight (heuristics are applied if there are more 
than one). The selected templates constitute 
the indicative content and the terms ap- 
pearing in the topic candidate slots and their 
expansions constitute the potential topics 
of the document. Expansions are obtained 
looking for terms in the term tree sharing 
the semantic of some terms in the indicative 
4 
content.  
The ind icat ive  content  is sorted using 
positional information and the following con- 
ceptual order: s i tuat ion ,  need for research,  
problem,  solut ion, ent i ty  in t roduct ion ,  
topical  in format ion,  goal of concep- 
tua l  ent i ty,  focus of conceptua l  ent i ty,  
methodo log ica l  aspects,  inferences and 
s t ruc tura l  in format ion.  Templates of the 
same type are grouped together if they ap- 
peared in sequence in the list. The types 
considered in this process are: the  topic, 
sect ion topic and s t ruc tura l  in format ion.  
The sorted templates constitute the text  plan. 
3.1.2 Informativeness 
For each potential: topic and sentence where 
it appears (that information is found on the 
term tree) the system verifies if the sentence 
contains an informative marker (conceptual 
index) and satisfies an informative pattern. If 
so, the potential topic is considered a topic 
of the document and a link will be created be- 
tween the topic and the sentence which will be 
part of the informative abstract. 
4 Content  P resentat ion  
Our approach to text generation is based 
on the regularities observed in the corpus 
of professional abstracts and so, it does not 
implement a general theory of text generation 
by computers. Each element in the text  p lan 
is used to produce a sentence. The structure of 
the sentence depends on the type of template. 
The information about the s i tuat ion,  the  
problem, the need for research,  etc. is 
reported as in the original document with few 
modifications (concept re-expression). Instead 
other types require additional re-generation: 
for the topic of the  document  template the 
generation procedure is as follows: (i) the verb 
form for the predicate in the Pred icate  slot 
is generated in the present  tense (topical 
information is always reported in present 
tense), 3rd person of s ingu lar  in ac t ive  
voice at the beginning of the sentence; 
(ii) the parsed sentence fragment from the N'hat 
slot is generated in the middle of the sentence 
(so the appropriate case for the first element 
I 
I 
I 
i 
I 
Topic of the Document  
topic Type: 
Id: 
Predicate: 
Where: 
Who: 
What: 
Position: 
Topic candidates: 
Weight: 
integer identifier 
instance of make known 
instance of {research paper, study, work, research} 
instance of{research paper, author,  study, work, research,  none} 
parsed sentence fragment 
section and sentence id
list of terms from the What filler 
number 
Topic o f  Sect ion  
Type: 
Id: 
Predicate:  
Section: 
Argument: 
Position: 
Topic candidates: 
Weight: 
sec_desc 
integer identifier 
instance of make known 
instance of paper component 
parsed sentence fragment 
section and sentence id
list of terms from the Argument filler 
number 
Type: 
Id: 
Predicate : 
Structural : 
Argument : 
Po s it ion: 
Topic candidates : 
Weight : 
structure-2 
integer identifier 
instance of show graphical material 
instance of structural element 
parsed sentence fragment 
section and sentence id 
list of terms from the Argument filler 
number 
Signaling 
(indicative) 
Topic  (indica- 
tive) 
SKIP1 + s t ructura l  + SKIP2 + show graphica l ly  + ARGUMENT + eos 
noun group + author + make known + prepos i t ion + research paper + 
DESCRIPT ION + eos 
Author 's  Goal SKIP1 + goal of author + def ine + GOAL + eos 
(indicative) 
Goal of SKIP  + goal + preposi t ion + TOPIC + def ine + GOAL + eos 
TOPIC (in- 
formative) 
Definition SKIP  + TOPIC + def ine + noun group 
of TOPIC 
(informative) 
Table 2: Templates and Patterns. 
has, to be generated); and (iii) a full stop is 
generated. This schema of generation avoids 
the formulation of expressions like "X will be 
presented", "X have been presented" or "We 
have presented here X" which are usually found 
on source documents but which are awkward 
in the context of the abstract ext-type. Note 
that each type of information prescribes its 
own schema of generation. 
Some elements in the parsed sentence frag- 
ment require re-expression while others are 
presented in "the words of the author." If the 
system detects an acronym without expansion 
in the string it would expand it and record that 
situation in order to avoid repetitions. Note 
that as the templates contain parsed sentence 
fragments, the correct punctuation has to 
be re-generated. For merged templates the 
generator implements the following patterns 
of production: if n adjacent emplates are to 
be presented using the same predicate, only 
one verb will be generated whose argument is 
the conjunction of the arguments from the n 
templates. If the sequence of templates have 
no common predicate, the information will 
be presented as a conjunction of propositions. 
These patterns of sentence production are 
exemplified in Table 3. 
The elaboration of the topics is presented 
upon reader's demand. The information is pre- 
sented in the order of the original text. The in- 
formative abstract is the information obtained 
by this process as it is shown in Figure 1. 
5 L imi ta t ions  o f  the  Approach  
Our approach is based on the empirical ex- 
amination of abstracts published by second 
services. In our first study, we examined 100 
abstracts and source documents in order to 
deduce a conceptual and linguistic model for 
the task of summarization of technical articles. 
Then, we expanded the corpus with 100 more 
items in order to validate the model. We 
believe that the concepts, relations and types 
.of information identified account for interesting 
,phenomena appearing in the corpus and con- 
stitute a sound basis for text summarization. 
'Nevertheless, we have identified only a few 
? linguistic expressions used in order to express 
-particular elements of the conceptual model 
(241 domain verbs, 163 domain nouns, 129 
adj.ectives , 174 indicative patterns, 87 informa- 
tive patterns). This is because we are mainly 
concerned with the development of a general 
method of automatic abstracting and the task 
of constructing such linguistic resources i time 
consuming as recent work have shown (Minel 
et al, 2000). 
The implementation f our method relies? on 
State-of-the-art techniques in natural anguage 
processing including noun and verb group iden- 
tification and conceptual tagging. The inter- 
preter relies on the output produced by a shal- 
low text segmenter and on a statistical POS- 
tagger. Our prototype only analyses entences 
for the specific purpose of text summarization 
and implements some patterns of generation ob- 
served in the corpus. Additional analysis could 
be done on the obtained representation to pro- 
duce better esults. 
6 Re la ted  Work  
(Paice and Jones, 1993) have already addressed 
the issue of content identification and expression 
in technical summarization using templates, but 
while they produced indicative abstracts for a 
specific domain, we are producing domain inde- 
pendent indicative-informative abstracts. Being 
designed for one specific domain, their abstracts 
are fixed in structure while our abstracts are 
dynamically constructed. Radev and McKeown 
(1998) also used instantiated templates, but in 
order to produce summaries of multiple docu- 
ments in one specific domain. They focus on 
the generation of the text while we are address- 
ing the overall process of automatic abstracting. 
Our concern regarding the presentation of the 
information is now being addressed by other re- 
searchers as well (Jing and McKeown, 1999). 
7 Eva luat ing  Content  and  Qua l i ty  in  
Text  Summar izat ion  
Abstracts are texts used in tasks such as assess- 
ing the content of the document and deciding 
if the source is worth reading. If text summa- 
rization systems are designed to fulfill those re- 
quirements, the generated texts have to be eval- 
uated according to their intended function and 
its quality. The quality and success of human 
produced abstracts have already been addressed 
in the literature (Grant, 1992; Gibson, 1993) us- 
ing linguistic criteria such as cohesion and co- 
herence, thematic structure, sentence structure 
and lexical density. But in automatic text sum- 
marization, this is an emergent research topic. 
(Minel et al, 1997) have proposed two meth- 
ods of evaluation addressing the content of the 
abstract and its quality. For content evalua- 
tion, they asked human judges to classify sum- 
maries in broad categories and also verify if 
the key ideas of source documents are appropri- 
ately expressed in the Summaries. For text qual- 
ity, they asked human judges to identify prob- 
lems such as dangling anaphora nd broken tex- 
tual segments and also to make subjective judg- 
ments about readability. In the context of the 
TIPSTER program, (Firmin and Chrzanowski, 
6 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
Re-Generated Sentences Sentences from Source Documents 
Illustrates the principle of virtual prototyping and 
the different echniques and models required. 
Presents the mechanical and electronic design o\] 
the robot harvester including all subsystems, 
namely, fruit localisation module, harvesting arm 
and gripper-cutter as well as the integration of 
subsystems and the specific mechanical design of 
the picking arm addressing the reduction of 
undesirable dynamic effects during high velocity 
operation. 
Shows configuration of the robotic fruit harvester 
Agribot and schematic view of the detaching tool. 
PAWS (the programmable automated welding sys- 
tem) was designed to provide an automated means 
of planning, controlling, and performing critical 
welding operations for improving productivity and 
quality. 
Describes HuDL (local autonomy) in greater 
detail; discusses ystem integration and the 1MA 
(the intelligent machine architecture); and also 
gives an example implementation. 
Figure 1 Virtual prototyping models and techniques 
illustrates the principle of virtual prototyping and the 
different techniques and models required. 
After a brief introduction, we present the mechanical 
and electronic design of the robot harvester includ- 
ing all subsystems, namely, fruit localisation module, 
harvesting arm and gripper-cutter aswell as the inte- 
gration of subsystems. 
Throughout this work, we present the specific mechan- 
ical design of the picking arm addressing the reduction 
of undesirable dynamic effects during high velocity op- 
eration. 
The final prototype consists of two jointed harvesting 
arms mounted on a human guided vehicle as shown 
schematically in Figure 1 Configuration ofthe robotic 
. fruit' harvester Agribot. 
Schematic representation f the operations involved in 
the detaching step can be seen in Figure 5 Schematic 
view of the detaching tool and operation. 
PAWS was designed to provide an automated means of 
planning, controlling, and performing critical welding 
operations for improving productivity and quality. 
Section 2 describes HuDL in greater detail and section 
3 discusses system integration and the IMA. 
An example implementation is given in section 4 and 
section 5 contains the conclusions. 
Table 3: Re-Generated Sentences 
1999) and (Mani et al, 1998) also used a cat -  
.egorization task using TREC topics. For text 
quality, they addressed subjective aspects uch 
? as the length of the summary, its intelligibility 
and its usefulness. We have carried out an eval- 
? uation of our summarization method in order to 
assess the function of the abstract and its text 
quality. 
7.1 Exper iment  
We compared abstrac?s produced by our 
method with abstracts produced by Mi- 
crosoft'97 Summarizer and with others 
published with source documents (usually au- 
thor abstracts). We have chosen Microsoft'97 
Summarizer because, even if it only produces 
extracts, it was the only summarizer available 
in order to carry out this evaluation and 
because it has already been used in other eval- 
uations (Marcu, 1997; Barzilay and Elhadad, 
1997). 
In order to evaluate content, we presented 
judges with randomly selected abstracts and 
five lists of keywords (content indicators). The 
judges had to decide to which list of keywords 
the abstract belongs given that different lists 
share some keywords and that they belong 
to the same technical domain. Those. lists 
were obtained from the journals where the 
source documents were published. The idea 
behind this evaluation is to see if the abstract 
convey the very essential content of the source 
document. 
In Order to evaluate the quality of the text, we 
asked the judges to provide an acceptability 
score between 0-5 for the abstract (0 for un- 
acceptable and 5 for acceptable) based on the 
following criteria taken from (Rowley, 1982) 
(they were only suggestions to the evaluators 
and were not enforced): good spelling and 
grammar; clear indication of the topic of 
7 
the  source document; impersonal style; one 
paragraph; conciseness; readable and under- 
standable; acronyms are presented along with 
their expansions; and other criteria that the 
judge considered important as an experienced 
reader of abstracts of technical documents. 
We told the judges that we would consider 
the abstracts with scores above 2.5 as accept- 
able. Some criteria are more important han 
other, for example judges do not care about 
impersonal style but care about readability. 
7.1.1 Mater ia ls  
Source  Documents :  we used twelve source 
documents from the journal Industrial Robots 
found on the Emerald Electronic Library (all 
technical articles). The articles were down- 
loaded in plain text format. These documents 
are quite long texts with an average of 23K 
characters (minimum of l lK  characters and a 
maximum of 41K characters). They contain 
an average of 3472 words (minimum of 1756 
words and a maximum of 6196 words excluding 
punctuation), and an average of 154 sentences 
(with a minimum of 85 and a maximum of 288). 
Abstracts :  we produced twelve abstracts us- 
:ing our method and computed the compression 
,ratio in number of words, then we produced 
twelve abstracts by Microsoft'97 Summarizer 1 
us ing  a compression rate at least as high as 
our (i.e. if our method produced an abstract 
with a compression rate of 3.3% of the source, 
we produced the Microsoft abstract with a 
compression rate of 4% of the source). We 
extracted the twelve abstracts and the twelve 
lists of keywords publ ished with the source 
documents. We thus obtained 36 different 
abstracts and twelve lists of keywords. 
Forms:  we produced 6 different forms each con- 
taining six different abstracts randomly 2 chosen 
out of twelve different documents (for a total of 
36 abstracts). Each abstract was printed in a 
1We had to format the source document in order for 
the Microsoft Summarizer to be able to recognize the 
structure of the document (titles, sections, paragraphs 
and sentences). 
2Random numbers for this evaluation were produced 
using software provided by SICSTus Prolog. 
different page. It included 5 lists of keywords, a
field to be completed with the quality score as- 
sociated to the abstract and a field to be f i l led 
with comments about the abstract. One of the 
lists of keywords was the one published with the 
source document, he other four were randomly 
selected from the set of 11 remaining keyword 
lists, they were printed in the form in random 
order. One page was also available to be com- 
pleted with comments about the task, in partic- 
ular with the time it took to the judges to com- 
plete the evaluation. We produced three copies 
of each form for a total of 18 forms. 
7.1.2 Sub jec ts  
We had a total of 18 human judges or eval- 
uators. Our evaluators were 18 students of 
the M.Sc. program in Information Science at 
McGill Graduate School of Library & Informa- 
tion Studies. All of the subjects had good read- 
ing and comprehension skills in English. This 
group was chosen because they have knowledge 
about what constitutes a good abstract and 
they are educated to become professionals in In- 
formation Science. 
7.1.3 Eva luat ion  Procedure  
The evaluation was performed in one hour ses- 
sion at McGill University. Each human judge 
received a form (so he/she evaluated six dif- 
ferent abstracts) and an instruction booklet. 
No other material was required for the evalu- 
ation (i.e. dictionary). We asked the judges to 
read carefully the abstract. They had to decide 
which was the list of keywords that matched 
the abstract (they could chose more than one 
or none at all) and then, they had to associate 
a numeric score to the abstract representing its 
quality based on the given criteria. This pro- 
cedure produced three different evaluations of 
content and text quality for each of the 36 ab- 
stracts. The overall evaluation was completed 
in a maximum of 40 minutes. 
7.2 Resu l ts  
For each abstract, we computed the average 
quality using the scores given by the judges. 
We considered that the abstract indicated the 
essential content of the source document if two 
or more judges were able to chose the correct 
list of keywords for the abstract. The results for 
individual articles and the average information 
8 
I 
I 
i 
i 
g 
I 
I 
i 
I 
I 
I 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
I 
I 
I 
i 
Microsoft Abstract 
# Art. Indic? Quality 
1 yes 2.66 
2 no 1.36 
3 no 1.16 
4 yes 3.00 
5 no 2.16 
6 yes 2.16 
7 no 0.83 
8 yes 2.33 
9 yes 2.16 
10 yes 2.16 
11 yes 2.40 
12 no 1.16 
Average 70% 1.98 
\] Average l 
Our Method 
Indic? Quality 
yes 2.93 
yes 3.66 
no 3.00 
yes 4.00 
no 1.76 
yes 4.00 
yes 2.50 
yes 3.00 
no 2.66 
yes 4.00 
no 2.70 
no 3.33 
70% 3.15 
S.D. Abstract 
Indic? Quality 
yes 4.16 
yes 4.00 
no 4.06 
yes 4.33 
yes 4.00 
no 4.53 
yes 4.40 
yes 4.00 
yes 3.66 
yes 3.31 
no 4.26 
no 4.00 
80% 4.04 
Results with Different Documents and Subjects 
80% I 1.46\[ 80%\] 3.23\[ 100% \[ 4.25 I 
Table 4: Results of Human Judgment about Indicativeness and Text Quality 
are shown in Table 4. For a given source docu- 
ment and type of abstract, the value in column 
'Indic?' contains the value 'yes' if the majority 
of the evaluator have chosen the source docu- 
ment list of keywords for the abstract and 'no' 
on the contrary. The value in column 'Qual- 
ity' is the average acceptability for the abstract. 
Content:  In 80% of the cases, the abstracts 
published with the source documents were 
correctly classified by the evaluators. Instead, 
the automatic abstracts were correctly classi- 
fied in 70% of the cases. It is worth noting 
"that the automatic systems did not use the 
? journal abstracts nor the lists of keywords or  
the.information about the journal. 
Quality: The figures about text acceptabil- 
ity indicate that the abstracts produced by 
Microsoft'97 Summarizer are below the accept- 
abil!ty level of 2.5, the abstracts produced by 
our method are above the acceptability level of 
2.5 and that the human abstracts are highly 
acceptable. 
In a run of this experiment using 30 ab- 
stracts from a different set of 10 articles and 15 
judges from \]~cole de Biblioth6conomie et des 
Sciences de l'Information (EBSI) at Universit6 
de Montr@al we have obtained similar results 
(last row in Table 4). 
8 Conc lus ions  
In this paper, we have presented a method of 
text summarization which produces indicative- 
informative abstracts. We have described the 
techniques we are using to implement our 
method and some experiments howing the 
viability of the approach. 
Our method was specified for summarization 
of one specific type of text: the scientific and 
technical document. Nevertheless, it is domain 
independent because the concepts, relations 
and types of information we use are common 
across different domains. The question of the 
coverage of the model will be addressed in 
our future work. Our method was designed 
without any particular reader in mind and 
with the assumption that a text does have 
a "main" topic. If readers were known, the 
abstract could be tailored towards their specific 
profiles. User profiles could be used in order to 
produce the informative abstracts elaborating 
those specific aspects the reader is "usually" 
interested in. This aspect will be elaborated in
future work. 
The experiments reported here addressed 
9 
the evaluation of the indicative abstracts using 
a categorization task. Using the automatic 
abstracts reader have chosen the correct 
category for the articles in 70% of the cases 
compared with 80% of the cases when using the 
author abstracts. Readers found the abstracts 
produced by our method of better quality than 
a sentence-extraction based system. 
Acknowledgments  
We would like to thank three anonymous re- 
viewers for their comments which helped us im- 
prove the final version of this paper. We are 
grateful to Professor Mich~le Hudon from Uni- 
versit~ de Montreal for fruitful discussion and to 
Professor John E. Leide from McGill University 
and to Mme Gracia Pagola from Universit~ de 
Montreal for their help in recruiting informants 
for the experiments. 
References 
R. Barzilay and M. Elhadad. 1997. Using 
Lexical Chains for Text Summarization. In
Proceedings of the A CL/EA CL '97 Workshop 
on Intelligent Scalable Text Summarization, 
pages 10-17, Madrid, Spain, July. 
G. DeJong. 1982. An Overview of the FRUMP 
System. In W.G. Lehnert and M.H. Ringle, 
editors, Strategies for Natural Language Pro- 
cessing, pages 149-176. Lawrence Erlbaum 
. Associates, Publishers. 
T. Firmin and M.J. Chrzanowski. 1999. An 
Evaluation of Automatic Text Summariza- 
tion Systems. In I. Mani and M.T. Maybury,. 
editors, Advances in Automatic Text Summa- 
~ization, pages 325-336. 
T.R. Gibson. 1993. Towards a Discourse The- 
ory of Abstracts and Abstracting. Depart- 
ment of English Studies. University of Not- 
tingham. 
P. Grant. 1992. The Integration of Theory and 
Practice in the Development of Summary- 
Writting Strategies. Ph.D. thesis, Universit~ 
de Montreal. Facult~ des ~tudes up~rieures. 
H. Jing and K.R. McKeown. 1999. The Decom- 
position of Human-Written Summary Sen- 
tences. In M. Hearst, Gey. F., and R. Tong, 
editors, Proceedings of SIGIR '99. 22nd Inter- 
national Conference on Research and Devel- 
opment in Information Retrieval, pages 129- 
136, University of California, Beekely, Au- 
gust. 
H.P. Luhn. 1958. The Automatic Creation of " 
Literature Abstracts. IBM Journal of Re- 
search Development, 2(2):159-165. 
I. Mani, D. House, G. Klein, L. Hirshman, 
L. Obrst, T. Firmin, M. Chrzanowski, and 
B. Sundheim. 1998. The TIPSTER SUM- 
MAC Text Summarization Evaluation. Tech- 
nical report, The Mitre Corporation. 
D. Marcu. 1997. From Discourse Structures 
to Text Summaries. In The Proceedings of 
the A CL '97lEA CL '97 Workshop on Intelli- 
gent Scalable Text Summarization, pages 82- 
88, Madrid, Spain, July 11. 
J-L. Minel, S. Nugier, and G. Piat. 1997. Com- 
ment Appr~cier la Qualit~ des R~sum~s Au- 
tomatiques de Textes? Les Exemples des Pro- 
tocoles FAN et MLUCE et leurs R~sultats 
sur SERAPHIN. In ldres Journdes Scientific- 
ques et Techniques du Rdseau Francophone 
de l'Ingdnierie de la Langue de I'AUPELF- 
UREF., pages 227-232, 15-16 avril. 
J-L. Minel, J-P. Descl~s, E. Cartier, 
G. Crispino, S.B. Hazez, and A. Jack- 
iewicz. 2000. R~sum~ automatique par 
filtrage s~mantique d'informations dans des 
textes. TSI, X(X/2000):l-23. 
C.D. Paice and P.A. Jones. 1993. The Iden- 
tification of Important Concepts in Highly 
Structured Technical Papers. In R. Korfhage, 
E. Rasmussen, and P. Willett, editors, Proc. 
of the 16th ACM-SIGIR Conference, pages 
69-78. 
D.R. Radev and K.R. McKeown. 1998. Gen- 
erating Natural Language Summaries from 
Multiple On-Line Sources. Computational 
Linguistics, 24(3):469-500. 
J. Rowley. 1982. Abstracting and Indexing. 
Clive Bingley, London. 
H. Saggion and G. Lapalme. 1998. Where does 
Information come from? Corpus Analysis for 
Automatic Abstracting. In RIFRA'98. Ren- 
contre Internationale sur l'extraction le Fil- 
trage et le Rdsumd Automatique, pages 72-83. 
H. Saggion and G. Lapalme. 2000. Evaluation 
of Content and Text Quality in the Context 
of Technical Text Summarization. In Pro- 
ceedings of RIAO'2000, Paris, France, 12-14 
April, 2000. 
10 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
I 
I 
I! 
I, 
I 
I 
I 
I 
i, 
  
	143
144
145
146
235
236
237
238
239
240
241
242
Experiments on Semantic-based Clustering for Cross-document Coreference
Horacio Saggion
Department of Computer Science
University of Sheffield
211 Portobello Street - Sheffield, England, UK, S1 4DP
Tel: +44-114-222-1947
Fax: +44-114-222-1810
saggion@dcs.shef.ac.uk
Abstract
We describe clustering experiments for
cross-document coreference for the first
Web People Search Evaluation. In our ex-
periments we apply agglomerative cluster-
ing to group together documents potentially
referring to the same individual. The algo-
rithm is informed by the results of two dif-
ferent summarization strategies and an off-
the-shelf named entity recognition compo-
nent. We present different configurations of
the system and show the potential of the ap-
plied techniques. We also present an analy-
sis of the impact that semantic information
and text summarization have in the cluster-
ing process.
1 Introduction
Finding information about people on huge text col-
lections or on-line repositories on the Web is a com-
mon activity. In ad-hoc Internet retrieval, a request
for documents/pages referring to a person name may
return thousand of pages which although containing
the name, do not refer to the same individual. Cross-
document coreference is the task of deciding if two
entity mentions in two sources refer to the same indi-
vidual. Because person names are highly ambiguous
(i.e., names are shared by many individuals), decid-
ing if two documents returned by a search engine
such as Google or Yahoo! refer to the same individ-
ual is a difficult problem.
Automatic techniques for solving this problem are
required not only for better access to information
but also in natural language processing applications
such as multidocument summarization, question an-
swering, and information extraction. Here, we con-
centrate on the Web People Search Task (Artiles et
al., 2007) as defined in the SemEval 2007 Work-
shop: a search engine user types in a person name as
a query. Instead of ranking web pages, an ideal sys-
tem should organise search results in as many clus-
ters as there are different people sharing the same
name in the documents returned by the search en-
gine. The input is, therefore, the results given by
a web search engine using a person name as query.
The output is a number of sets, each containing doc-
uments referring to the same individual. The task is
related to the coreference resolution problem disre-
garding however the linking of mentions of the tar-
get entity inside each single document.
Similarly to (Bagga and Baldwin, 1998; Phan et
al., 2006), we have addressed the task as a document
clustering problem. We have implemented our own
clustering algorithms but rely on available extraction
and summarization technology to produce document
representations used as input for the clustering pro-
cedure. We will shown that our techniques produce
not only very good results but are also very compet-
itive when compared with SemEval 2007 systems.
We will also show that carefully selection of docu-
ment representation is of paramount importance to
achieve good performance. Our system has a sim-
ilar level of performance as the best system in the
recent SemEval 2007 evaluation framework. This
paper extends our previous work on this task (Sag-
gion, 2007).
149
2 Evaluation Framework
The SemEval evaluation has prepared two sets of
data to investigate the cross-document coreference
problem: one for development and one for testing.
The data consists of around 100 Web files per per-
son name, which have been frozen and so, can be
used as an static corpus. Each file in the corpus is
associated with an integer number which indicates
the rank at which the particular page was retrieved
by the search engine. In addition to the files them-
selves, the following information was available: the
page title, the url, and the snippet. In addition to the
data itself, human assessments are provided which
are used for evaluating the output of the automatic
systems. The assessment for each person name is
a file which contains a number of sets where each
set is assumed to contain all (and only those) pages
that refer to one individual. The development data is
a selection of person names from different sources
such as participants of the European Conference on
Digital Libraries (ECDL) 2006 and the on-line en-
cyclop?dia Wikipedia.
The test data to be used by the systems consisted
of 30 person names from different sources: (i) 10
names were selected from Wikipedia; (ii) 10 names
were selected from participants in the ACL 2006
conference; and finally, (iii) 10 further names were
selected from the US Census. One hundred doc-
uments were retrieved using the person name as a
query using the search engine Yahoo!.
Metrics used to measure the performance of
automatic systems against the human output were
borrowed from the clustering literature (Hotho et
al., 2003) and they are defined as follows:
Precision(A,B) = |A ?B|
|A|
Purity(C,L) =
n?
i=1
|Ci|
n
maxjPrecision(Ci, Lj)
Inverse Purity(C,L) =
n?
i=1
|Li|
n
maxjPrecision(Li, Cj)
F-Score?(C,L) =
Purity(C,L) ? Inverse Purity(C,L)
?Purity(C,L) + (1? ?)Inverse Purity(C,L)
where C is the set of clusters to be evaluated and
L is the set of clusters produced by the human. Note
that purity is a kind of precision metric which re-
wards a partition which has less noise. Inverse pu-
rity is a kind of recall metric. ? was set to 0.5 in
the SemEval 2007 evaluation. Two simple baseline
systems were defined in order to measure if the tech-
niques used by participants were able to improve
over them. The all-in-one baseline produces one sin-
gle cluster ? all documents belonging to that cluster.
The one-in-one baseline produces n cluster with one
different document in each cluster.
3 Agglomerative Clustering Algorithm
Clustering is an important technique used in areas
such as information retrieval, text mining, and data
mining (Cutting et al, 1992). Clustering algorithms
combine data points into groups such that: (i) data
points in the same group are similar to each other;
and (ii) data points in one group are ?different? from
data points in a different group or cluster. In infor-
mation retrieval it is assumed that documents that
are similar to each other are likely to be relevant
for the same query, and therefore having the doc-
ument collection organised in clusters can provide
improved document access (van Rijsbergen, 1979).
Different clustering techniques exist (Willett, 1988)
the simplest one being the one-pass clustering al-
gorithm (Rasmussen and Willett, 1987). We have
implemented an agglomerative clustering algorithm
which is relatively simple, has reasonable complex-
ity, and gave us rather good results. Our algorithm
operates in an exclusive way, meaning that a doc-
ument belongs to one and only one cluster ? while
this is our working hypothesis, it might not be valid
in some cases.
The input to the algorithm is a set of document
representations implemented as vectors of terms and
weights. Initially, there are as many clusters as
input documents; as the algorithm proceeds clus-
ters are merged until a certain termination condi-
tion is reached. The algorithm computes the similar-
ity between vector representations in order to decide
whether or not to merge two clusters.
The similarity metric we use is the cosine of the
angle between two vectors. This metric gives value
one for identical vectors and zero for vectors which
are orthogonal (non related). Various options have
been implemented in order to measure how close
150
two clusters are, but for the experiments reported
here we have used the following approach: the sim-
ilarity between two clusters (simC) is equivalent to
the ?document? similarity (simD) between the two
more similar documents in the two clusters ? this is
known as single linkage in the clustering literature;
the following formula is used:
simC (C1,C2) =
maxdi?C1;dj?C2simD(di,dj)
Where Ck are clusters, dl are document represen-
tations (e.g., vectors), and simD is the cosine metric
given by the following formula:
cosine(d1, d2) =
?n
i=1 wi,d1 ? wi,d2??n
i=1(wi,d1)
2 ?
??n
i=1(wi,d2)
2
where wi,d is the weight of term i in document d
and n is the numbers of terms.
If this similarity is greater than a threshold ? ex-
perimentally obtained ? the two clusters are merged
together. At each iteration the most similar pair of
clusters is merged. If this similarity is less than a
certain threshold the algorithm stops. Merging two
clusters consist of a simple step of set union, so there
is no re-computation involved ? such as computing
a cluster centroid.
We estimated the threshold for the clustering al-
gorithm using the ECDL subset of the training data
provided by SemEval. We applied the clustering al-
gorithm where the threshold was set to zero. For
each document set, purity, inverse purity, and F-
score were computed at each iteration of the algo-
rithm, recording the similarity value of each newly
created cluster. The similarity values for the best
clustering results (best F-score) were recorded, and
the maximum and minimum values discarded. The
rest of the values were averaged to obtain an esti-
mate of the optimal threshold. The thresholds used
for the experiments reported here are as follows:
0.10 for word vectors and 0.12 for named entity vec-
tors (see Section 5 for vector representations).
4 Natural Language Processing
Technology
We rely on available extraction and summarization
technology in order to linguistically process the doc-
uments for creating document representations for
clustering. Although the SemEval corpus contains
information other than the retrieved pages them-
selves, we have made no attempt to analyse or use
contextual information given with the input docu-
ment.
Two tools are used: the GATE system (Cunning-
ham et al, 2002) and a summarization toolkit (Sag-
gion, 2002; Saggion and Gaizauskas, 2004) which
is compatible with GATE. The input for analysis is
a set of documents and a person name (first name
and last name). The documents are analysed by the
default GATE1 ANNIE system which creates differ-
ent types of named entity annotations. No adap-
tation of the system was carried out because we
wanted to verify how far we could go using available
tools. Summarization technology was used from
single document summarization modules from our
summarization toolkit.
The core of the toolkit is a set of summariza-
tion modules which compute numeric features for
each sentence in the input document, the value of
the feature indicates how relevant the information
in the sentence is for the feature. The computed
values, which are normalised yielding numbers in
the interval [0..1] ? are combined in a linear for-
mula to obtain a score for each sentence which is
used as the basis for sentence selection. Sentences
are ranked based on their score and top ranked sen-
tences selected to produce an extract. Many fea-
tures implemented in this tool have been suggested
in past research as valuable for the task of identify-
ing sentences for creating summaries. In this work,
summaries are created following two different ap-
proaches as described below.
The text and linguistic processors used in our sys-
tem are: document tokenisation to identify different
kinds of words; sentence splitting to segment the text
into units used by the summariser; parts-of-speech
tagging used for named entity recognition; named
entity recognition using a gazetteer lookup module
and regular expressions grammars; and named entity
coreference module using a rule-based orthographic
name matcher to identify name mentions considered
equivalent (e.g., ?John Smith? and ?Mr. Smith?).
Named entities of type Person, Organization, Ad-
dress, Date, and Location are considered relevant
1http://gate.ac.uk
151
document terms and stored in a special named en-
tity called Mention as an annotation. The perfor-
mance of the named entity recogniser on Web data
(business news from the Web) is around 0.90 F-score
(Maynard et al, 2003).
Coreference chains are created and analysed and
if they contain an entity matching the target person?s
surname, all elements of the chain are marked as a
feature of the annotation.
We have tested two summarization conditions in
this work: In one set of experiments a sentence be-
longs to a summary if it contains a mention which
is coreferent with the target entity. In a second set
of experiments a sentence belongs to a summary if
it contains a ?biographical pattern?. We rely on a
number of patterns that have been proposed in the
past to identify descriptive phrases in text collec-
tions (Joho and Sanderson, 2000). The patterns used
in the experiments described here are shown in Ta-
ble 1. In the patterns, dp is a descriptive phrase that
in (Joho and Sanderson, 2000) is taken as a noun
phrase. These patterns are likely to capture infor-
mation which is relevant to create person profiles, as
used in DUC 2004 and in TREC QA ? to answer
definitional questions.
These patterns are implemented as regular expres-
sions using the JAPE language (Cunningham et al,
2002). Our implementation of the patterns make use
of coreference information so that target is any name
in text which is coreferent with sought person. In or-
der to implement the dp element in the patterns we
use the information provided by a noun phrase chun-
ker. The following is one of the JAPE rules for iden-
tifying key phrases as implemented in our system:
({TargetPerson}
({ Token.string == "is" } |
{Token.string == "was" })
{NounChunk}):annotate --> :annotate.KeyPhrase = {}
where TargetPerson is the sought entity, and
NounChunk is a noun chunk. The rule states that
when the pattern is found, a KeyPhrase should be
created.
Some examples of these patterns in text are shown
in Table 4. A profile-based summarization system
which uses these patterns to create person profiles is
reported in (Saggion and Gaizauskas, 2005).
Patterns
target (is | was |...) (a | an | the) dp
target, (who | whose | ...)
target, (a | the | one ...) dp
target, dp
target?s
target and others
Table 1: Set of patterns for identifying profile infor-
mation.
Dickson?s invention, the Kinetoscope, was simple:
a strip of several images was passed in front of an
illuminated lens and behind a spinning wheel.
James Hamilton, 1st earl of Arran
James Davidson, MD, Sports Medicine Orthope-
dic Surgeon, Phoenix Arizona
As adjutant general, Davidson was chief of the
State Police, qv which he organized quickly.
Table 2: Descriptive phrases in test documents for
different target names.
4.1 Frequency Information
Using language resources creation modules from the
summarization tool, two frequency tables are cre-
ated for each document set (or person) on-the-fly: (i)
an inverted document frequency table for words (no
normalisation is applied); and (ii) an inverted fre-
quency table for Mentions (the full entity string is
used, no normalisation is applied).
Statistics (term frequencies (tf(Term)) and in-
verted document frequencies (idf(Term))) are com-
puted over tokens and Mentions using tools from the
summarization toolkit (see examples in Table 3).
word frequencies Mention frequencies
of (92) Jerry Hobbs (80)
Hobbs (92) Hobbs (56)
Jerry (90) Krystal Tobias (38)
to (89) Texas (37)
in (87) Jerry (36)
and (86) Laura Hobbs (35)
the (85) Monday (34)
a (85) 1990 (31)
Table 3: Examples of top frequent terms (words and
named entities) and their frequencies in the Jerry
Hobbs set.
Using these tables vector representations are cre-
ated for each document (same as in (Bagga and
152
Baldwin, 1998)). We use the following formula to
compute term weight (N is the number of documents
in the input set):
weight(Term) = tf(Term) ? log2( Nidf(Term) )
These vectors are also stored in the GATE doc-
uments. Two types of representations were con-
sidered for these experiments: (i) full document or
summary (terms in the summary are considered for
vector creation); and (ii) words are used as terms or
Mentions are used as terms.
5 Cross-document Coreference Systems
In this section we present results of six different con-
figurations of the clustering algorithm. The config-
urations are composed of two parts one which indi-
cates where the terms are extracted from and the sec-
ond part indicates what type of terms were used. The
text conditions are as follows: Full Document (FD)
condition means that the whole document was used
for extracting terms for vector creation; Person Sum-
mary (PS) means that sentences containing the target
person name were used to extract terms for vector
creation; Descriptive Phrase (DP) means that sen-
tences containing a descriptive patterns were used to
extract terms for vector creation. The term condi-
tions are: Words (W) words were used as terms and
Mentions (M) named entities were used as terms.
Local inverted term frequencies were used to weight
the terms.
6 SemEval 2007 Web People Search
Results
The best system in SemEval 2007 obtained an F-
score of 0.78, the average F-score of all 16 partic-
ipant systems is 0.60. Baseline one-in-one has an
F-score of 0.61 and baseline all-in-one an F-score of
0.40. Results for our system configurations are pre-
sented in Table 4. Our best configuration (FD+W)
obtains an F-score of 0.74 (or a fourth position in the
SemEval ranking). All our configurations obtained
F-scores greater than the average of 0.60 of all par-
ticipant systems. They also perform better than the
two baselines.
Our optimal configurations (FD+W and PS+W)
both perform similarly with respect to F-score.
While the full document condition favours ?inverse
purity?, summary condition favours ?purity?. As
one may expect, the use of descriptive phrases to
create summaries has the effect of increasing purity
to one extreme, these expressions are far too restric-
tive to capture all necessary information for disam-
biguation.
Configuration Purity Inv.Purity F-Score
FD+W 0.68 0.85 0.74
FD+M 0.62 0.85 0.68
PS+W 0.84 0.70 0.74
PS+M 0.65 0.75 0.64
DP+W 0.90 0.62 0.71
DP+M 0.97 0.53 0.66
Table 4: Results for different clustering configura-
tions. These results are those obtained on the whole
set of 30 person names.
7 Semantic-based Experiments
While these results are rather encouraging, they
were not optimal. In particular, we were surprised
that semantic information performed worst than a
simple word-based approach. We decided to inves-
tigate whether some types of semantic information
might be more helpful than others in the cluster-
ing process. We therefore created one vector for
each type of information: Organization, Person,
Location, Date, Address in each document and re-
clustered all test data using one type at a time, with-
out modifying any of the system parameters (e.g.,
without re-training). The results were very encour-
aging.
7.1 Results
Results of semantic-based clustering per informa-
tion type are presented in Tables 5 and 6. Each row
Semantic Type Purity Inv.Purity F-Score +/-
Organization 0.90 0.72 0.78 +0.10
Person 0.81 0.72 0.75 +0.07
Address 0.82 0.64 0.69 +0.01
Date 0.58 0.85 0.67 -0.01
Location 0.55 0.85 0.64 -0.04
Table 5: Results for full document condition and
different semantic information types. Improvements
over FD+M are reported.
153
Semantic Type Purity Inv.Purity F-Score +/-
Person 0.85 0.64 0.70 +0.06
Organization 0.97 0.57 0.69 +0.05
Date 0.87 0.60 0.68 +0.04
Location 0.82 0.63 0.67 +0.03
Address 0.93 0.54 0.65 +0.01
Table 6: Results for summary condition and differ-
ent semantic information types. Improvements over
PS+M are reported.
in the tables reports results for clustering using one
type of information alone. Table 5 reports results for
semantic information with full text condition and it
is therefore compared to our configuration FD+M
which also uses full text condition together with se-
mantic information. The last column in the table
shows improvements over that configuration. Using
Organization type of information in full text condi-
tion, not only outperforms the previous system by
ten points, also exceeds by a fraction of a point the
best system in SemEval 2007 (one point if we con-
sider macro averaged F-score). Statistical tests (t-
test) show that improvement over FD+M is statisti-
cally significant. Other semantic types of informa-
tion also have improved performance, not all of them
however. Location and Date in the full documents
are probably too ambiguous to help disambiguating
the target named entity.
Table 6 reports results for semantic information
with summary text condition (only personal sum-
maries were tried, experiments using descriptive
phrases are underway) and it is therefore compared
to our configuration PS+M which also uses sum-
mary condition together with semantic information.
The last column in the table shows improvements
over that configuration. Here all semantic types of
information taken individually outperform a system
which uses the combination of all types. This is
probably because all types of information in a per-
sonal summary are somehow related to the target
person.
7.2 Results per Person Set
Following (Popescu and Magnini, 2007), we present
purity, inverse purity, and F-score results for all
our configurations per category (ACL, US Census,
Wikipedia) in the test set.
In Tables 7, 8, and 9, results are reported for full
Configuration Set Purity I.Purity F-Score
FD+Address ACL 0.86 0.48 0.57
FD+Address US C. 0.81 0.71 0.75
FD+Address Wikip. 0.78 0.70 0.73
PS+Address ACL 0.96 0.38 0.50
PS+Address US C. 0.94 0.61 0.72
PS+Address Wikip. 0.88 0.62 0.71
FD+Date ACL 0.63 0.82 0.69
FD+Date US C. 0.52 0.87 0.64
FD+Date Wikip. 0.59 0.85 0.68
PS+Date ACL 0.88 0.49 0.59
PS+Date US C. 0.88 0.64 0.72
PS+Date Wikip. 0.84 0.67 0.72
FD+Location ACL 0.63 0.78 0.65
FD+Location US C. 0.52 0.86 0.64
FD+Location Wikip. 0.49 0.91 0.62
PS+Location ACL 0.87 0.47 0.54
PS+Location US C. 0.85 0.66 0.73
PS+Location Wikip. 0.74 0.75 0.72
Table 7: Results for clustering configurations per
person type set (ACL, US Census, and Wikipedia)
- Part I.
Configuration Set Purity I.Purity F-Score
FD+Org. ACL 0.92 0.57 0.69
FD+Org. US C. 0.87 0.78 0.82
FD+Org. Wikip. 0.88 0.79 0.83
PS+Org. ACL 0.98 0.42 0.54
PS+Org. US C. 0.95 0.63 0.74
PS+Org. Wikip. 0.96 0.65 0.77
FD+Person ACL 0.82 0.66 0.72
FD+Person US C. 0.81 0.74 0.76
FD+Person Wikip. 0.77 0.75 0.75
PS+Person ACL 0.86 0.53 0.63
PS+Person US C. 0.85 0.6721 0.73
PS+Person Wikip. 0.82 0.70 0.73
Table 8: Results for clustering configurations per
person type set (ACL, US Census, and Wikipedia)
- Part II.
document condition(FD), summary condition (PS),
word-based representation (W), mention representa-
tion (M) ? i.e. all types of named entities, and five
different mention types: Person, Location, Organi-
zation, Date, and Address.
While the Organization type of entity worked bet-
ter overall, it is not optimal across different cat-
egories of people. Note for example that very
good results are obtained for the Wikipedia and US
Census sets, but rather poor results for the ACL
set, where a technique which relies on using full
documents and words for document representations
works better. These results show that more work is
154
Configuration Set Purity I.Purity F-Score
FD+W ACL 0.73 0.84 0.77
FD+W US C. 0.54 0.91 0.67
FD+W Wikip. 0.57 0.91 0.68
FD+M ACL 0.73 0.76 0.70
FD+M US C. 0.68 0.82 0.71
FD+M Wikip. 0.60 0.86 0.68
PS+W ACL 0.84 0.59 0.65
PS+W US C. 0.80 0.74 0.75
PS+W Wikip. 0.70 0.81 0.73
PS+M ACL 0.75 0.62 0.60
PS+M US C. 0.71 0.74 0.69
PS+M Wikip. 0.58 0.83 0.66
Table 9: Results for clustering configurations per
person type set (ACL, US Census, and Wikipedia)
- Part III.
needed before reaching any conclusions on the best
document representation for our algorithm in this
task.
8 Related Work
The problem of cross-document coreference has
been studied for a number of years now. Bagga
and Baldwin (Bagga and Baldwin, 1998) used the
vector space model together with summarization
techniques to tackle the cross-document coreference
problem. Their approach uses vector representa-
tions following a bag-of-words approach. Terms for
vector representation are obtained from sentences
where the target person appears. They have not pre-
sented an analysis of the impact of full document
versus summary condition and their clustering algo-
rithm is rather under-specified. Here we have pre-
sented a clearer picture of the influence of summary
vs full document condition in the clustering process.
Mann and Yarowsky (Mann and Yarowsky, 2003)
used semantic information extracted from docu-
ments referring to the target person in an hierarchical
agglomerative clustering algorithm. Semantic infor-
mation here refers to factual information about a per-
son such as the date of birth, professional career or
education. Information is extracted using patterns
some of them manually developed and others in-
duced from examples. We differ from this approach
in that our semantic information is more general and
is not particularly related - although it might be - to
the target person.
Phan el al. (Phan et al, 2006) follow Mann and
Yarowsky in their use of a kind of biographical in-
formation about a person. They use a machine learn-
ing algorithm to classify sentences according to par-
ticular information types in order to automatically
construct a person profile. Instead of comparing
biographical information in the person profile alto-
gether as in (Mann and Yarowsky, 2003), they com-
pare each type of information independently of each
other, combining them only to make the final deci-
sion.
Finally, the best SemEval 2007 Web People
Search system (Chen and Martin, 2007) used tech-
niques similar to ours: named entity recognition us-
ing off-the-shelf systems. However in addition to
semantic information and full document condition
they also explore the use of contextual information
such as the url where the document comes from.
They show that this information is of little help. Our
improved system obtained a slightly higher macro-
averaged f-score over their system.
9 Conclusions and Future Work
We have presented experiments on cross-document
coreference of person names in the context of the
first SemEval 2007 Web People Search task. We
have designed and implemented a solution which
uses an in-house clustering algorithm and available
extraction and summarization techniques to produce
representations needed by the clustering algorithm.
We have presented different approaches and com-
pared them with SemEval evaluation?s results. We
have also shown that one system which uses one
specific type of semantic information achieves state-
of-the-art performance. However, more work is
needed, in order to understand variation in perfor-
mance from one data set to another.
Many avenues of improvement are expected.
Where extraction technology is concerned, we have
used an off-the-shelf system which is probably not
the most appropriate for the type of data we are deal-
ing with, and so adaptation is needed here. With re-
spect to the clustering algorithm we plan to carry out
further experiments to test the effect of different sim-
ilarity metrics, different merging criteria including
creation of cluster centroids, and cluster distances;
with respect to the summarization techniques we in-
tend to investigate how the extraction of sentences
155
containing pronouns referring to the target entity af-
fects performance, our current version only exploits
name coreference. Our future work will also explore
how (and if) the use of contextual information avail-
able on the web can lead to better performance.
Acknowledgements
We are indebted to the three anonymous reviewers
for their extensive suggestions that helped improve
this work. This work was partially supported by the
EU-funded MUSING project (IST-2004-027097).
References
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
SemEval-2007 WePS Evaluation: Establishing a
benchmark for Web People Search Task. In Proceed-
ings of Semeval 2007, Association for Computational
Linguistics.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
the 17th International Conference on Computational
Linguistics (COLING-ACL?98), pages 79?85.
Y. Chen and J.H. Martin. 2007. Cu-comsem: Explor-
ing rich features for unsupervised web personal named
disambiguation. In Proceedings of SemEval 2007, As-
socciation for Computational Linguistics, pages 125?
128.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools and
Applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics (ACL?02).
Douglass R. Cutting, Jan O. Pedersen, David Karger, and
John W. Tukey. 1992. Scatter/gather: A cluster-
based approach to browsing large document collec-
tions. In Proceedings of the Fifteenth Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 318?329.
A. Hotho, S. Staab, and G. Stumme. 2003. WordNet im-
proves text document clustering. In Proc. of the SIGIR
2003 Semantic Web Workshop.
H. Joho and M. Sanderson. 2000. Retrieving Descrip-
tive Phrases from Large Amounts of Free Text. In
Proceedings of Conference on Information and Know-
eldge Management (CIKM), pages 180?186. ACM.
G. S. Mann and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In W. Daelemans and
M. Osborne, editors, Proceedings of the 7th Confer-
ence on Natural Language Learning (CoNLL-2003),
pages 33?40. Edmonton, Canada, May.
D. Maynard, K. Bontcheva, and H. Cunningham.
2003. Towards a semantic extraction of named
entities. In G. Angelova, K. Bontcheva, R. Mitkov,
N. Nicolov, and N. Nikolov, editors, Proceedings of
Recent Advances in Natural Language Processing
(RANLP?03), pages 255?261, Borovets, Bulgaria, Sep.
http://gate.ac.uk/sale/ranlp03/ranlp03.pdf.
X.-H. Phan, L.-M. Nguyen, and S. Horiguchi. 2006.
Personal name resolution crossover documents by a
semantics-based approach. IEICE Trans. Inf. & Syst.,
Feb 2006.
Octavian Popescu and Bernardo Magnini. 2007. Irst-bp:
Web people search using name entities. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 195?198, Prague,
Czech Republic, June. Association for Computational
Linguistics.
E. Rasmussen and P. Willett. 1987. Non-hierarchical
document clustering using the icl distribution ar-
ray processor. In SIGIR ?87: Proceedings of the
10th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 132?139, New York, NY, USA. ACM Press.
H. Saggion and R. Gaizauskas. 2004. Multi-document
summarization by cluster/profile relevance and redun-
dancy removal. In Proceedings of the Document Un-
derstanding Conference 2004. NIST.
H. Saggion and R. Gaizauskas. 2005. Experiments on
statistical and pattern-based biographical summariza-
tion. In Proceedings of EPIA 2005, pages 611?621.
H. Saggion. 2002. Shallow-based Robust Summariza-
tion. In Automatic Summarization: Solutions and Per-
spectives, ATALA, December, 14.
H. Saggion. 2007. Shef: Semantic tagging and summa-
rization techniques applied to cross-document corefer-
ence. In Proceedings of SemEval 2007, Assocciation
for Computational Linguistics, pages 292?295.
C.J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London.
P. Willett. 1988. Recent trends in hierarchic document
clustering: A critical review. Information Processing
& Management, 24(5):577?597.
156
Introduction to Text Summarization and Other Information
Access Technologies
Horacio Saggion
Natural Language Processing Group
University of Sheffield
211 Portobello Street - Sheffield - S1 4DP
England - United Kingdom
saggion@dcs.shef.ac.uk
Abstract
In recent years we have witnessed an explosion
of on-line unstructured information in multi-
ple languages, making natural language process-
ing technologies such as automatic text summa-
rization increasingly important for the informa-
tion society. Text Summarization provides users
with condensed descriptions of documents, al-
lowing them to make informed decisions based
on text summaries. Text summarization can be
combined with Information Retrieval (IR) and
Question Answering (QA) to provide users with
focus-based or query-based summaries which
are targeted towards the users? specific needs.
When the information a user looks for is spread
across multiple sources, text summarization can
be used to condense facts and present a non-
redundant account of the most relevant facts
found across a set of documents.
The objective of this IJCNLP 2008 tutorial
is to give an overview of a number of tech-
nologies in natural language processing for in-
formation access including: single and multi-
document summarization, cross-lingual summa-
rization; and summarization in the context of
question answering.
The tutorial will discuss summarization con-
cepts and techniques as well as its relation and
relevance to other technologies such as informa-
tion retrieval and question answering. It will
also include description of available resources for
development, training and evaluation of summa-
rization components. A summarization (multi-
document and multilingual) toolkit will be used
for demonstration purposes. A number of ques-
tion answering components relevant for the cre-
ation of definitional summaries and profiles will
also be demonstrated.
Biography
Dr. Saggion is a research fellow in the Natu-
ral Language Processing group, Department of
Computer Science, University of Sheffield, Eng-
land, UK. His area of expertise is Text Summa-
rization. He works in national and international
projects on information extraction, ontology-
based information extraction, question answer-
ing, and text summarization. He obtained his
PhD. in 2000 from Universite? de Montre?al, De-
partement d?Informatique et de Recherche Op-
erationnelle. He has published over 50 works in
conferences, workshops and journal papers. To-
gether with his research career, he has been an
active teacher, he was assistant professor and
researcher at Universidad de Buenos Aires, and
Universidad Nacional de Quilmes, and teach-
ing assistant at Universite? de Montre?al. He
has participated in a number of summarization
and question answering evaluations including
DUC 2004, DUC 2005, MSE 2005, TREC/QA
2003, TREC/QA 2004, TREC/QA 2005. He
has recently organised the workshops ?Multi-
source Multi-lingual Information Extraction and
Summarization? and ?Crossing Barriers in Text
Summarization Research? in the RANLP Con-
ferences. He has given courses and tutorials
on Text Summarization and other technologies
such as question answering in a number of inter-
national venues such as ESSLLI and LREC.
939
c? 2002 Association for Computational Linguistics
Generating Indicative-Informative
Summaries with SumUM
Horacio Saggion? Guy Lapalme?
University of Sheffield Universite? de Montre?al
We present and evaluate SumUM, a text summarization system that takes a raw technical text
as input and produces an indicative informative summary. The indicative part of the summary
identifies the topics of the document, and the informative part elaborates on some of these topics
according to the reader?s interest. SumUM motivates the topics, describes entities, and defines
concepts. It is a first step for exploring the issue of dynamic summarization. This is accomplished
through a process of shallow syntactic and semantic analysis, concept identification, and text
regeneration. Our method was developed through the study of a corpus of abstracts written
by professional abstractors. Relying on human judgment, we have evaluated indicativeness,
informativeness, and text acceptability of the automatic summaries. The results thus far indicate
good performance when compared with other summarization technologies.
1. Introduction
A summary is a condensed version of a source document having a recognizable genre
and a very specific purpose: to give the reader an exact and concise idea of the contents
of the source. In most cases, summaries are written by humans, but nowadays, the
overwhelming quantity of information,1 and the need to access the essential content
of documents accurately in order to satisfy users? demands calls for the development
of computer programs able to produce text summaries. The process of automatically
producing a summary from a source text consists of the following steps:
1. interpreting the text
2. extracting the relevant information, which ideally includes the ?topics?
of the source
3. condensing the extracted information and constructing a summary
representation
4. presenting the summary representation to the reader in natural language.
Even though some approaches to text summarization produce acceptable summaries
for specific tasks, it is generally agreed that the problem of coherent selection and
expression of information in text summarization is far from being resolved. Sparck
Jones and Endres-Niggemeyer (1995) stated the need for a research program in text
? Department of Computer Science, University of Sheffield, Sheffield, England, United Kingdom, S1 4DP.
E-mail: saggion@dcs.shef.ac.uk
? De?partement d?Informatique et Recherche Ope?rationnelle, Universite? de Montre?al, CP 6128, Succ
Centre-Ville, Montre?al, Que?bec, Canada, H3C 3J7. E-mail: lapalme@iro.umontreal.ca
1 In 1998, the volume of this information was calculated at somewhere between 400 and 500 million
documents (Filman and Pant 1998).
498
Computational Linguistics Volume 28, Number 4
summarization that would study the relation between source document and summary,
the different types of summaries and their functions, the development of new methods
and/or combination of already existing techniques for text summarization, and the
development of evaluation procedures for summaries and systems. Rowley (1982)
proposes the following typology of different types of document condensations:
? the extract, which is a set of passages selected from a source document
to represent the whole document
? the summary, which occurs at the end of the document and is a
restatement of the salient findings of a work
? the abridgment, which is a reduction of the original document that
necessarily omits secondary points
? the precis, which stands for the main points of an argument
? the digest, which is a condensation of a book or news article
? the highlight, which is a comment included in specific parts of a
document to alert a reader
? the synopsis, which in cinematography represents a script of a film.
In our research, we are concerned only with summaries of technical articles, which
are called abstracts. In this context, two main types of abstracts are considered (ANSI
1979; ERIC 1980; Maizell, Smith, and Singer 1971): indicative abstracts, which point to
information alerting the reader about the content of an article in a given domain (these
abstracts will contain sentences like ?The work of Consumer Advice Centres is exam-
ined.?), and informative abstracts, which provide as much quantitative or qualitative
information contained in the source document as possible (these abstracts will contain
sentences like ?Consumer Advice Centres have dealt with preshopping advice, edu-
cation on consumers? rights and complaints about goods and services, advising the
client and often obtaining expert assessments.?). In the course of our research, we have
studied the relation between abstracts and source documents, and as a result, we have
developed SumUM (Summarization at Universite? de Montre?al), a text summarization
system that produces an indicative-informative abstract for technical documents. The
abstracts are produced in two steps: First, the reader is presented with an indicative
abstract that identifies the topics of the document (what the authors present, discuss,
etc.). Then, if the reader is interested in some of the topics, specific information about
them from the source document is presented in an informative abstract.
Figure 1 shows an automatic abstract produced by our system. The abstract was
produced by a process of conceptual identification and text re-generation we call se-
lective analysis. The indicative abstract contains information about the topic of the
document. It describes the topics of sections and introduces relevant entities. The iden-
tified topics are terms either appearing in the indicative abstract or obtained from the
terms and words of the indicative abstract through a process of term expansion. The
one particular feature of these terms is that they can be used to obtain more conceptual
information from the source document, such as definitions or statements of relevance,
usefulness, and development, as can be seen in Figure 2.
This article is organized as follows. In the next section, we describe the analysis
of a corpus of professional abstracts used to specify selective analysis; conceptual and
linguistic information for the task of summarization of technical texts deduced from
this corpus is also presented. An overview of selective analysis and the implementation
499
Saggion and Lapalme Generating Summaries with SumUM
Designing for human-robot symbiosis
Presents the views on the development of intelligent interactive service robots.
The authors have observed that a key research issue in service robotics is the inte-
gration of humans into the system. Discusses some of the technologies with par-
ticular emphasis on human-robot interaction, and system integration; describes
human direct local autonomy (HuDL) in greater detail; and also discusses system
integration and intelligent machine architecture (IMA). Gives an example imple-
mentation; discusses some issues in software development; and also presents the
solution for integration, the IMA. Shows the mobile robot.
Identified Topics: HuDL - IMA - aid systems - architecture - holonic manu-
facturing system - human - human-robot interaction - intelligent interactive
service robots - intelligent machine architecture - intelligent machine software
- interaction - key issue - widely used interaction - novel software architecture
- overall interaction - robot - second issue - service - service robots - software
- system - Technologies
Figure 1
Indicative abstract and identified topics for the text ?Designing for Human-Robot Symbiosis,?
D. M. Wilkes et al, Industrial Robot, 26(1), 1999, 49?58.
Development of a service robot is an extremely challenging task.
In the IRL, we are using HuDL to guide the development of a cooperative service
robot team.
IMA is a two-level software architecture for rapidly integrating these elements,
for an intelligent machine such as a service robot.
A holonic manufacturing system is a manufacturing system having autonomous
but cooperative elements called holons (Koestler, 1971).
Communication between the robot and the human is a key concern for intelligent
service robotics.
Figure 2
Informative abstract elaborating some topics.
of our experimental prototype, SumUM, is then presented in section 3. In section 4, we
discuss the limitations of our approach; then, in section 5, we present an evaluation and
comparison of our method with state-of-the art summarization systems and human
abstracts. Related work on text summarization is discussed in section 6. Finally, in
section 7, we draw our conclusions and discuss prospects for future research.
2. Observations from a Corpus
We have developed our method of text summarization by studying a corpus of profes-
sional abstracts and source documents. Our corpus contains 100 items, each composed
of a professional abstract and its source document. As sources for the abstracts we used
the journals Library & Information Science Abstracts (LISA), Information Science Abstracts
500
Computational Linguistics Volume 28, Number 4
(ISA), and Computer & Control Abstracts. The source documents were found in journals
of computer science (CS) and information science (IS), such as AI Communications, AI
Magazine, American Libraries, Annals of Library Science & Documentation, Artificial Intelli-
gence, Computers in Libraries, and IEEE Expert, among others (a total of 44 publications
were examined). The professional abstracts contained three sentences on the average,
with a maximum of seven and a minimum of one. The source documents covered a va-
riety of subjects from IS and CS. We examined 62 documents in CS and 38 in IS, some
of them containing author-provided abstracts. Most of the documents are structured in
sections; but apart from conceptual sections such as ?Introduction? and ?Conclusion,?
they do not follow any particular style (articles from medicine, for example, usually
have a fixed structure like ?Introduction,? ?Method,? ?Statistical Analysis,? ?Result,?
?Discussion,? ?Previous Work,? ?Limitations,? ?Conclusion,? but this was not the case
in our corpus). The documents were 7 pages on average, with a minimum of 2 and
a maximum of 45. Neither the abstracts nor the source documents were electronically
available, so the information was collected through photocopies. Thus we do not have
information regarding number of sentences and words in the source document.
Our methodological approach consisted of the manual alignment of sentences
from the professional abstract with elements of the source document. This was accom-
plished by looking for a match between the information in the professional abstract
and the information in the source document. The structural parts of the source doc-
ument we examined were the title of the source document, the author abstract, the
first section, the last section, the section headings, and the captions of tables and fig-
ures. When the information was not found, we looked in other parts of the source
document. The information is not always found anywhere in the source document, in
which case we acknowledge that fact. This methodological process was established af-
ter studying procedures for abstract writing (Cremmins 1982; Rowley 1982) and some
initial observations from our corpus. One alignment is shown in Table 1. All align-
ments are available for research purposes at the SumUM Web page ?http://www-
rali.iro.umontreal.ca/sumum.html?.
In this example, the three sentences of the professional abstract were aligned with
four elements of the source document, two in the introduction and two in the author-
provided abstract. The information of the abstract was found ?literally? in the source
document. The differences between the sentences of the professional abstract and those
of the source document are the persons of the verbs (?Presents? vs. ?We present? in
alignment (1)), the verbs (?were discovered? vs. ?We found? in alignment (3)), the
impersonal versus personal styles (?Uses? vs. ?Our experiment used? in alignment
(2)), and the use of markers in the source document (?In this paper? in alignment (1)).
This example shows that the organization of the abstract does not always mirror the
organization of the source document.
2.1 Distributional Results
The 309 sentences of the professional abstracts in our corpus were manually aligned
with 568 elements in the source documents. (We were not able to align six sentences
of the professional abstracts.) Other studies have already investigated the alignment
between sentences in the abstract and sentences in the source document. Kupiec, Ped-
ersen, and Chen (1995) report on the semiautomatic alignment of 79% of sentences of
professional abstracts in a corpus of 188 documents with professional abstracts. Us-
ing automatic means, it is difficult to deal with conceptual alignments that appeared
in our corpus. Teufel and Moens (1998) report on a similar work, but this time on
the alignment of sentences from author-provided abstracts. They use a corpus of 201
articles, obtaining only 31% of alignable sentences by automatic means. No informa-
501
Saggion and Lapalme Generating Summaries with SumUM
Table 1
Item of corpus. Professional abstract: Library & Information Science abstract 3024 and source
document: ?Movement Characteristics Using a Mouse with Tactile and Force Feedback,?
International Journal of Human-Computer Studies, 45(5), October 1996, pages 483?493.
Ex. Professional Abstract Source Document Position/Type
(1) Presents the results of an
empirical study that investigates
the movement characteristics of
a multi-modal mouse?a mouse
that includes tactile and relevance
feedback.
In this paper, we present the
results of an empirical study
that investigates the movement
characteristics of a multi-modal
mouse?a mouse that includes
tactile and force feedback.
1st/Intr.
(2) Uses a simple target selection
task while varying the target
distance, target size, and the
sensory modality.
Our experiment used a simple
target selection task while varying
the target distance, target size, and
the sensory modality.
1st/Intr.
(3) Significant reduction in the overall
movement times and in the time
taken to stop the cursor after en-
tering the target were discovered,
indicating that modifying a mouse
to include tactile feedback, and
to a lesser extent, force feedback,
offers performance advantages in
target selecting tasks.
We found significant reductions in
the overall movement time and in
the time to stop the cursor after
entering the target.
?/Abs.
The results indicate that modi-
fying a mouse to include tac-
tile feedback, and to a lesser ex-
tent, force feedback, offers perfor-
mance advantages in target selec-
tion tasks.
?/Abs.
Table 2
Distribution of information.
Documents With Author Abstract Without Author Abstract Average
# % # % # % %
Title 10 2 6 2 4 1 2
Author abstract 83 15 83 34 20
First section 195 34 61 26 134 42 40
Last section 18 3 6 2 12 4 4
Headlines
and captions 191 33 76 31 115 36 23
Other sections 71 13 13 5 58 17 11
Total 568 100 245 100 323 100 100
tion is given about the distribution of the sentences in structural parts in the source
document.
In Table 2, we present the distribution of the sentences in the source documents
that were aligned with the professional abstracts in our corpus. We consider all the
structured documents of our corpus (97 documents). The first three columns contain
the information for all documents, for documents with author abstracts, and for docu-
ments without author abstracts (the information is given in terms of total elements and
502
Computational Linguistics Volume 28, Number 4
percentage of elements). We also recorded how the types of information are distributed
in the professional abstract. For each abstract, we computed the ratio of the number
of elements of each type contributing to the abstract to the total number of elements
in the abstract (for example, the abstract in Table 1 contains 50% of first section and
50% of author abstract). The last column gives the average of the information over all
abstracts. In this corpus, we found that 72% of the information for the abstracts comes
from the following structural parts of the source documents: the title of the document,
the first section, the last section, and the section headers and captions of tables and
figures (sum of these entries on the first column of Table 2). Sharp (1989) reports on
experiments carried out with abstractors in which it is shown that introductions and
conclusions provide a basis for producing a coherent and informative abstract. In fact
abstractors use a short cut strategy (looking at the introduction and conclusion) prior
to looking at the whole paper. But our results indicate that using just those parts is not
enough to produce a good informative abstract. Important information is also found
in sections other than the introduction and conclusion. Abstractors not only select the
information for the abstract because of its particular position in the source document,
but they also look for specific types of information that happen to be lexically marked.
In Table 1 the information reported is the topic of the document, the method, and the
author?s discovery. This information is lexically marked in the source document by
expressions such as we, paper, present, study, experiment, use, find, and indicate. Based
on these observations we have defined a conceptual and linguistic model for the task
of text summarization of technical articles.
2.2 Conceptual Information for Text Summarization
A scientific and technical article is the result of the complex process of scientific in-
quiry, which starts with the identification of a problem and ends with its solution. It is
a complex linguistic record of knowledge referring to a variety of real and hypothetical
concepts and relations. Some of them are domain dependent (like diseases and treat-
ments in medical science; atoms and fusion in physics; and algorithms and proofs in
computer science), whereas others are generic to the technical literature (authors, the
research article, the problem, the solution, etc.). We have identified 55 concepts and 39
relations that are typical of a technical article and relevant for identifying types of in-
formation for text summarization by collecting domain-independent lexical items and
linguistic constructions from the corpus and classifying them using thesauri (Vianna
1980; Fellbaum 1998). We expanded the initial set with other linguistic constructions
not observed in the corpus.
Concepts. Concepts can be classified in categories referring to the authors (the
authors of the article, their affiliation, researchers, etc.), the work of the authors (work,
study, etc.), the research activity (current situation, need for research, problem, solu-
tion, method, etc.), the research article (the paper, the paper components, etc.), the
objectives (objective, focus, etc.), and the cognitive activities (presentation, introduc-
tion, argument, etc.).
Relations. Relations refer to general activities of the author during the research
and writing of the work: studying (investigate, study, etc.), reporting the work (present,
report, etc.), motivating (objective, focus, etc.), thinking (interest, opinion, etc.), and
identifying (define, describe, etc.).
Types of Information. We have identified 52 types of information for the process
of automatic text summarization referring to the following aspects of the technical
503
Saggion and Lapalme Generating Summaries with SumUM
Table 3
Conceptual information for text summarization.
Domain concepts author, institutions, affiliation, author related, research group, project, research
paper, others? paper, study, research, problem, solution, method, result, experiment,
need, goal, focus, conclusion, recommendation, summary, researcher, work,
hypothesis, research question, future plan, reference, acronym, expansion,
structural, title, caption, quantity, mathematical, paper component, date,
conceptual goal, conceptual focus, topic, introduction, overview, survey,
development, analysis, comparison, discussion, presentation, definition,
explanation, suggestion, discovery, situation, advantage, example
Domain relations make known, show graphical material, study, investigate, summarize, situation,
need, experiment, discover, infer, problem, solution, objective, focus, conclude,
recommend, create, open, close, interest, explain, opinion, argue, comment, suggest,
evidence, relevance, define, describe, elaborate, essential, advantage, use, identify
entity, exemplify, effective, positive, novel, practical
Indicative types topic of document, possible topic, topic of section, conceptual goal, conceptual
focus, author development, development, inference, author interest, interest,
author study, study, opening, closing, problem, solution, topic, entity introduction,
acronym identification, signaling structure, signaling concept, experiments,
methodology, explaining, commenting, giving evidence, need for research,
situation, opinion, discovery, demonstration, investigation, suggestion, conclusion,
summarization
Informative types relevance, goal, focus, essential, positiveness, usefulness, effectiveness, description,
definition, advantage, practicality, novelty, elaboration, exemplification,
introduction, identification, development
article: background information (situation, need, problem, etc.), reporting of informa-
tion (presenting entities, topic, subtopics, objectives, etc.), referring to the work of the
author (study, investigate, method, hypothesis, etc.), cognitive activities (argue, infer,
conclude, etc.), and elaboration of the contents (definitions, advantages, etc.).
The complete list of concepts, relations, and types of information is provided in
Table 3. Concepts and relations are the basis for the classification of types of infor-
mation referring to the essential contents of a technical abstract. Nevertheless, the
presence of a single concept or relation in a sentence is not enough to understand
the type of information it conveys. The co-occurrence of concepts and relations in
appropriate linguistic-conceptual patterns is used in our case as the basis for the clas-
sification of the sentences. The types of information are classified as Indicative or
Informative depending on the type of abstract to which they will contribute. For ex-
ample, Topic of Document and Topic of Section are indicative, whereas Goal of
Entity and Description of Entity are informative. Note that we have identified only a
few linguistic expressions used to express particular elements of the conceptual model,
because we were mainly concerned with the development of a general method of text
summarization and because the task of constructing such linguistic resources is time
consuming.
2.3 From Source to Abstract
According to Cremmins (1982), the last step in the human production of the sum-
mary text is the ?extracting? into ?abstracting? step in which the extracted informa-
tion will be mentally sorted into a preestablished format and will be ?edited? using
cognitive techniques. The editing of the raw material ranges from minor to major
operations. Cremmins gives little indication, however, about the process of editing.
504
Computational Linguistics Volume 28, Number 4
Table 4
Text editing in human abstracting.
Professional Abstract Source Document
Mortality in rats and mice of both sexes was
dose related.
There were significant positive associations
between the concentrations of the substance
administered and mortality in rats and mice
of both sexes.
No treatment related tumors were found in
any of the animals.
There was no convincing evidence to indicate
that endrin ingestion induced any of the
different types of tumors which were found
in the treated animals.
Major transformations are those of the complex process of language understanding
and production, such as deduction, generalization, and paraphrase. Some examples
of editing given by Cremmins are shown in Table 4. In the first example, the con-
cept mortality in rats and mice of both sexes is stated with the wording of the source
document; however, the concept expressed by the concentrations of the substance admin-
istered is stated with the expression dose. In the second example, the relation between
the tumors and endrin ingestion is expressed through the complex nominal treatment
related tumors.
In his rules for abstracting, Bernier (1985) states that redundancy, repetition, and
circumlocutions are to be avoided. He gives a list of linguistic expressions that can be
safely removed from extracted sentences or reexpressed in order to gain conciseness.
These include expressions such as It was concluded that X, to be replaced by X, and It
appears that, to be replaced by Apparently. Also, Mathis and Rush (1985) indicate that
some transformations in the source material are allowed, such as concatenation, trun-
cation, phrase deletion, voice transformation, paraphrase, division, and word deletion.
Rowley (1982) mentions the inclusion of the lead or topical sentence and the use of
active voice and advocates conciseness. But in fact, the issue of editing in text summa-
rization has usually been neglected, notable exceptions being the works by Jing and
McKeown (2000) and Mani, Gates, and Bloedorn (1999). In our work, we partially ad-
dress this issue by enumerating some transformations frequently found in our corpus
that are computationally implementable. The transformations are always conceptual
in nature and not textual (they do not operate on the string level), even if some of
them seem to take the form of simple string deletion or substitution. The rephras-
ing transformations we have identified are outlined below. We also include for each
transformation the number and percentage of times the transformation was used to
produce a sentence of the professional abstract. (Note that the percentages do not add
up to 100, as sentences can be involved in more than one operation.)
Syntactic verb transformation: Some verbs from the source document are
reexpressed in the abstract, usually in order to make the style impersonal.
The person, tense, and voice of the original verb are changed. Also, verbs
that are used to state the topic of the document are generally expressed in
the present tense (in active or passive voice). The same applies to verbs
introducing the objective of the research paper or investigation (according to
convention, objectives are reported in the present tense and results in the
past tense). This transformation was observed 48 times (15%).
505
Saggion and Lapalme Generating Summaries with SumUM
Lexical verb transformation: A verb used to introduce a topic is changed and
restated in the impersonal form. This transformation was observed 13 times
(4%).
Verb selection: The topic or subtopic of the document is introduced by a
domain verb, usually when information from titles is used to create a
sentence. This transformation was observed 70 times (21%).
Conceptual deletion: Domain concepts such as research paper and author are
avoided in the abstract. This transformation was observed 43 times (13%).
Concept reexpression: Domain concepts such as author, research paper, and
author-related entity are stated in the impersonal form. This transformation
was observed 4 times (1%).
Structural deletion: Discourse markers (contrast, structuring, logical
consequence, adding, etc.) such as first, next, finally, however, and although are
deleted. This transformation was observed 7 times (2%).
Clause deletion: One or more clauses (principal or complement) of the
sentence are deleted. This transformation was observed 47 times (14%).
Parenthetical deletion: Some parenthetical expressions are eliminated. This
transformation was observed 10 times (3%).
Acronym expansion: Acronyms introduced for the first time are presented
along with their expansions, or only the expansion is presented. This
transformation was observed 7 times (2%).
Abbreviation: A shorter expression (e.g., acronym or anaphoric expression) is
used to refer to an entity. This transformation was observed 3 times (1%).
Merge: Information from several parts of the source document are merged
into a single sentence. This is the usual case when reporting entities stated in
titles and captions. This transformation was observed 124 times (38%).
Split: Information from one sentence of the source document is presented in
separate sentences in the abstract. This transformation was observed 3 times
(1%).
Complex reformulation: A complex reformulation takes place. This could
involve several cognitive processes, such as generalization and paraphrase.
This transformation was observed 75 times (23%).
Noun transformations: Other transformations take place, such as
nominalization, generalization, restatement of complex nominals, deletion of
complex nominals, expansion of complex nominals (different classes of
aggregation), and change of initial uppercase to lowercase (e.g., when words
from titles or headlines, usually in upper initial, are used for the summary).
This transformation was observed 70 times (21%).
No transformation: The information is reported as in the source. This
transformation was observed 35 times (11%).
We found that transformations involving domain verbs appeared in 40% of the
sentences, noun editing occurred in 38% of the sentences, discourse level editing oc-
curred in 19% of the sentences, merging and splitting of information occurred in 38%
of the sentences, complex reformulation accounts for 23% of the sentences, and finally,
506
Computational Linguistics Volume 28, Number 4
only 11% of the information from the source document is stated without transfor-
mation. Although most approaches to automatic text summarization present the ex-
tracted information in both the order and the form of the original, this is not the case
in human-produced abstracts. Nevertheless, some transformations in the source doc-
ument could be implemented by computers with state-of-the-art techniques in natural
language processing in order to improve the quality of the automatic summaries.
In this section, we have studied relations between abstracts and their source doc-
uments. This study was motivated by the need to answer to the question of content
selection in text summarization (Sparck Jones 1993). We have also addressed here an-
other important research question: how the information is expressed in the summary.
Our study was based on the manual construction of alignments between sentences
of professional abstracts and elements of source documents. In order to obtain an ap-
propriate coverage, abstracts from different secondary sources and source documents
from different journals were used. We have shown that more than 70% of the informa-
tion for abstracts comes from the introduction, conclusion, titles, and captioning of the
source document. This is an empirical verification of what is generally acknowledged
in practical abstract writing in professional settings. We have also identified 15 types of
transformation usually applied to the source document in order to produce a coherent
piece of text. Of the sentences of our corpus, 89% have been edited. In section 3.1, we
detail the specification of patterns of sentence and text production inspired from our
corpus study that were implemented in our automatic system.
Although the linguistic information for our model has been manually collected,
Teufel (1998) has shown how this labor-intensive task can be accomplished in a semi-
automatic fashion. The analysis presented here and the idea of the alignments have
been greatly influenced by the exploration of abstracting manuals (Cremmins 1982).
Our conceptual model comes mainly from the empirical analysis of the corpus but
has also been influenced by work on discourse modeling (Liddy 1991) and in the phi-
losophy of science (Bunge 1967). It is interesting to note that our concerns regarding
the presentation and editing of the information for text summarization are now be-
ing addressed by other researchers as well. Jing and McKeown (2000) and Jing (2000)
propose a cut-and-paste strategy as a computational process of automatic abstracting
and a sentence reduction strategy to produce concise sentences. They have identified
six ?editing? operations in human abstracting that are a subset of the transformation
found in our study. Jing and McKeown?s work on sentence reduction will be dis-
cussed in section 6. Knight and Marcu (2000) propose a noisy-channel model and a
decision-based model for sentence reduction also aiming at conciseness.
3. Selective Analysis and Its Implementation
Selective analysis is a method for text summarization of technical articles whose design
is based on the study of the corpus described in section 2. The method emphasizes
the selection of particular types of information and its elaboration, exploring the is-
sue of dynamic summarization. It is independent of any particular implementation.
Nevertheless, its design was motivated by actual needs for accessing the content of
long documents and the current limitations of natural language processing of domain-
independent texts. Selective analysis is composed of four main steps, which are briefly
motivated here and fully explained in the rest of the section.
? Indicative selection: The function of indicative selection is to identify
potential topics of the document and to instantiate a set of indicative
templates. These templates are instantiated with sentences matching
507
Saggion and Lapalme Generating Summaries with SumUM
specific patterns. A subset of templates is retained based on a matching
process between terms from titles and terms from the indicative
templates. From the selected templates, terms are extracted for further
analysis (i.e., potential topics).
? Informative selection: The information selection process determines the
subset of topics computed by the indicative selection that can be
informatively expanded according to the interest of the reader. This
process considers sentences in which informative markers and
interesting topics co-occur and instantiates a set of informative templates
that elaborate the topics.
? Indicative generation: In indicative generation, the set of templates
detected by the indicative selection are first sorted using a preestablished
conceptual order. Then, the templates are used to generate sentences
according to the style observed in the corpus of professional abstracts
(i.e., verbs in the impersonal and reformulation of some domain
concepts). When possible, information from different templates is
integrated in order to produce a single sentence. A list of topics is also
presented to the reader.
? Informative generation: In informative generation, the reader selects some
of the topics presented as a result of indicative generation, thereby
asking for more information about those topics. Templates instantiated
by the informative selection associated with the selected topics are used
to present additional information to the reader.
Whereas the indicative abstract depends on the structure, content, and to some ex-
tent, on specific types of information generally reported in this kind of summary, the
informative abstract relies on the interests of the reader to determine the topics to
expand.
3.1 Implementing SumUM
The architecture of SumUM is depicted in Figure 3. Our approach to text summariza-
tion is based on a superficial analysis of the source document to extract appropriate
types of information and on the implementation of some text regeneration techniques.
SumUM has been implemented in SICStus Prolog (release 3.7.1) (SICStus 1998) and
Perl (Wall, Christiansen, and Schwartz 1996) running on Sun workstations (5.6) and
Linux machines (RH 6.0). For a complete description of the system and its implemen-
tation, the reader is referred to Saggion (2000).
The sources of information we use for implementing our system are a POS tag-
ger (Foster 1991); linguistic and conceptual patterns specified by regular expressions
combining POS tags, our syntactic categories, domain concepts, and words; and a
conceptual dictionary that implements our conceptual model (241 domain verbs, 163
domain nouns, and 129 adjectives); see Table 5.
3.1.1 Preprocessing and Interpretation. The input article (plain ASCII text in English
without markup) is segmented in main units (title, author information, main sections
and references) using typographic information (i.e., nonblank lines ending with a char-
acter different from punctuation surrounded by blank lines) and some keywords like
?Introduction? and ?References.? Each unit is passed through the statistical tagger
(based on bigrams). A scanning process reads each element of the tagged files and
508
Computational Linguistics Volume 28, Number 4
DATABASE
INDICATIVE
INFORMATIVE DATABASE
INFORMATIVE ABSTRACT
TOPICS
INDICATIVE
INDICATIVE ABSTRACT
SELECTED TOPICS
GENERATION 
GENERATION
USER
INDICATIVE
SELECTION 
INFORMATIVE
INFORMATIVE
SELECTION
RAW TEXT
PREPROCESSING
INTERPRETATION
POTENTIAL TOPICS INDICATIVE CONTENT
TEXT REPRESENTATIONCONCEPTUAL INDEX TOPICAL STRUCTURETERM TREE
ACRONYM
INFORMATION
CONCEPTUAL DICTIONARY
Figure 3
SumUM architecture.
transforms sequences of tagged words into lists of elements, each element being a
list of attribute-value pairs. For instance, the word systems, which is a common noun
is represented with the following attributes (cat,?NomC?), (Nbr,plur), (canon,system)
in addition to the original word. The frequency of each noun (proper or common) is
also computed. SumUM gradually determines the paragraph structure of the docu-
ment, relying on end of paragraph markers. Sentences are interpreted using finite-state
transducers we developed (implementing 334 linguistic and domain-specific patterns)
and the conceptual dictionary. The interpretation process produces a partial represen-
tation that consists of the sentence position (section and sentence numbers) and a list
of syntactic constituents annotated with conceptual information. As title and section
headers are recognized by position (i.e., sentence number 0 of the section), only noun
group identification is carried out in those components. Each sentence constituent is
represented by a list of attribute-value pairs. The parse of each element is as follows:
? Noun group parsing. We identify only nonrecursive, base noun groups.
The parse of a noun group contains information about the original
string, the canonical or citation form, syntactic features, the semantics
509
Saggion and Lapalme Generating Summaries with SumUM
Table 5
Overview of the conceptual dictionary.
Concept/Relation Lexical Item
make known cover, describe, examine, explore, present, report, overview, outline, . . .
create create, construct, ideate, develop, design, implement, produce, project,
. . .
study investigate, compare, analyze, measure, study, estimate, contrast, . . .
interest address, interest, concern, matter, worry, . . .
infer demonstrate, infer, deduce, show, conclude, draw, indicate, . . .
identify entity include, classify, call, contain, categorize, divide, . . .
paper paper, article, report, . . .
paper component section, subsection, appendix, . . .
structural figure, table, picture, graphic, . . .
problem complexity, intricacy, problem, difficulty, lack, . . .
goal goal, objective, . . .
result finding, result, . . .
important important, relevant, outstanding, . . .
necessary needed, necessary, indispensable, mandatory, vital, . . .
novelty innovative, new, novel, original, . . .
(i.e., the head of the group in citation form), adjectives, and information
referring to the conceptual model that is optional.
? Verb group parsing. The parse of a verb group contains information about
the original string, the semantics (i.e., the head of the group in citation
form), the syntactic features, information about adverbs, and the
conceptual information that is optional.
? Adjectives and adverbials. The parse of adjectival and adverbial groups
contains the original string, the citation form, and the optional
information from the conceptual model.
? Other. The rest of the elements (i.e., conjunctions, prepositions, etc.) are
left unanalyzed.
In order to assess the accuracy of the parsing process, we manually extracted base
noun groups and base verb groups from a set of 42 abstracts found on the INSPEC
(2000) service (about 5,000 words). Then, we parsed the abstracts and automatically
extracted noun groups and verb groups with our finite-state machinery and computed
recall and precision measures. Recall measures the ratio of the number of correct
syntactic constructions identified by the algorithm to the number of correct syntactic
constructions. Precision is the ratio of the number of correct syntactic constructions
identified by the algorithm to the total number of constructions identified by the
algorithm. We found the parser to perform at 86% recall and 86% precision for noun
groups and 85% recall and 76% precision for verb groups.
Term extraction. Terms are constructed from the citation form of noun groups.
They are extracted from sentences and stored along with their semantics and position
in the term tree, an AVL tree structure for efficient access from the SICStus Prolog
association lists package. As each term is extracted from a sentence, its frequency is
updated. We also build a conceptual index that specifies the types of information of
each sentence using the concepts and relations identified before. Finally, terms and
510
Computational Linguistics Volume 28, Number 4
Table 6
Specification of indicative templates for the topic of the document and the topic of a section.
Type: topic
Id: integer identifier
Predicate: instance of make known
Where: instance of {research paper, study, work, research}
Who: instance of {research paper, author, study, work, research}
What: parsed sentence fragment
Position: section and sentence id
Topic candidates: list of terms from the What filler
Weight: number
Type: sec desc
Id: integer identifier
Predicate: instance of make known
Section: instance of section(Id)
Argument: parsed sentence fragment
Position: section and sentence id
Topic candidates: list of terms from the Argument filler
Weight: number
words are extracted from titles (identified as those sentences with numeral 0 in the
representation) and stored in a list, the topical structure, and acronyms and their
expansions are identified and recorded.
3.1.2 Indicative Selection. Simple templates are used to represent the types of informa-
tion. We have implemented 21 indicative templates in this version of SumUM. Table 6
presents two of these indicative templates and their slots. The slot Topic candidates is
filled with terms and acronym expansions. Term relevance is the total frequency of all
nominal components of the term divided by the total number of nominal components.
It is computed using the following formula:
relevance(Term) =
?
{N?Term? noun(N)} noun frequency(N)
|N : N ? Term ? noun(N)|
where noun(N) is true if N is a noun, noun frequency(N) is a function computed during
preprocessing and interpretation that gives the word count for noun N, and the nota-
tion |S| stands for the number of elements in the set S. As complex terms have lower
distribution than single terms, this formula gives us an estimate of the distribution of
the term and its components in the document. In doing so, a low-frequency term like
robot architecture is assigned a high degree of relevance because chances are that robot
and architecture occur frequently on their own. Other techniques exist for boosting the
score of longer phrases, such as adjusting the score of the phrase by a fixed factor that
depends on the length of the phrase (Turney 1999). The Weight slot is filled in with
the sum of the relevance of the terms on the Topic candidates slot.
For determining the content of the indicative abstract, SumUM considers only
sentences that have been identified as carrying indicative information; excludes sen-
tences containing problematic anaphoric references (?the first. . . ,? ?the previous. . . ,?
?that. . . ,? quantifiers in sentence initial, etc.), those that are not domain concepts (e.g.,
?These results,? ?The first section,? etc.), and some connectives (?although,? ?how-
ever,? etc.); and checks whether the sentence matches an indicative pattern. Indicative
511
Saggion and Lapalme Generating Summaries with SumUM
Table 7
Indicative pattern specification and sentence fragments matching the patterns (in parentheses).
Signaling structural SKIP1 + GN + Prep + GN + show graphical material + Prep + structural (In our
case, the architecture of the self-tuner is shown in Figure 3 Auto-tuning. . . )
Topic SKIP1 + research paper + SKIP2 + author + make known + ARGUMENT (In this
article, we overview the main techniques used in order. . . )
Author?s Goal SKIP + conceptual goal + SKIP + define + GOAL (Our goals within the HMS project
are to develop a holonic architecture for. . . )
Signaling concept SKIP + development + Prep + GN (Implementation of industrial robots)
Section Topic paper component + make known + ARGUMENT + ConC + paper component
(Section 2 describes HuDL in greater detail and Section 3. . . )
Problem/Solution SKIP + solution (dr) + problem (The proposed methodology overcomes the problems
caused by. . . )
Introduce Entity GN + define + SKIP (Rapid Prototyping (RP) is a technique. . . )
patterns contain variables, syntactic constructions, domain concepts, and relations.
One hundred seventy-four indicative patterns have been implemented; some of them
are shown in Table 7.
For each matched pattern, SumUM verifies some restrictions, such as verb tenses
and voice, extracts information from pattern variables, and instantiates a template of
the appropriate type. All the instantiated templates constitute the indicative database
(IDB). SumUM matches each element of the topical structure with the terms of the
Topic candidate slots of templates in the IDB. Two terms Term1 and Term2 match if
Term1 is a substring of Term2 or if Term2 is a substring of Term1 (e.g., robotic fruit harvester
matches harvester).
Then, SumUM selects the template with the greatest Weight. In case of conflict,
types are selected following the precedence given in Table 8. This order gives prefer-
ence to explicit topical information more usually found in indicative abstracts. Where
there is conflict, the Position and the Id slots are used to decide: If two topic tem-
plates have the same Weight, the template with position closer to the beginning of the
document is selected, and if they are still equal, the template with lower Id is used.
SumUM prioritizes topical information by selecting the topical template with greatest
weight. The selected templates constitute the indicative content (IC), and the terms
and words appearing in the Topic candidate slots and their expansions constitute the
potential topics (PTs) of the document. Expansions are obtained by looking for terms
in the term tree sharing the semantics of any term in the IC.
3.1.3 Informative Selection. For each potential topic PT and sentence in which it
appears, SumUM checks whether the sentence contains an informative marker and
matches a dynamic informative pattern. Dynamic patterns include a TOPIC slot in-
stantiated with the PT before trying a match. They also include concepts, relations,
and linguistic information. Eighty-seven informative patterns have been implemented,
Table 8
Precedence for content selection.
Topic of Document > Topic of Section > Topic Description > Possible Topic > Author
Study > Author Development > Author Interest > Conceptual Goal, Research Goal
> Conceptual Focus, Focus > Entity Introduction > Entity Identification > Signaling
Structural, Signaling Concepts > Other Indicative Types
512
Computational Linguistics Volume 28, Number 4
Table 9
Informative pattern specification and sentence fragments matching the patterns (in
parentheses).
Definition SKIP + TOPIC + define + GN (The RIMHO walking robot is a prototype developed
with the aim of. . . )
Description SKIP + TOPIC + describe (The hardware of the MMI consists of a main pendant (MP),
an operator pendant. . . )
Use SKIP + use + TOPIC (To realize the control using an industrial robot, such as. . . )
Advantage SKIP + advantage + Prep + TOPIC (The biggest advantage of SWERS is the easier and
faster. . . )
Effectiveness SKIP + TOPIC + define + effective (The system is effective in the task of. . . )
some of which are presented in Table 9. If a sentence satisfies an informative pattern,
the PT is considered a topic of the document, and an informative template is instan-
tiated with the sentence. The informative templates contain a Content slot to record
the information from the sentence, a Topic slot to record the topic, and a Position
slot to record positional information. Examples are presented in Tables 10 and 11. The
templates obtained by this process constitute the Informative Data Base (InfoDB), and
the topics are the terms appearing in the slot Topic of the templates in the InfoDB.2
3.1.4 Generation. The process of generation consists of the arrangement of the infor-
mation in a preestablished conceptual order, the merging of some types of information,
and the reformulation of the information in one text paragraph. The IC is sorted us-
ing positional information and the order presented in Table 12, which is typical of
technical articles.
SumUM merges groups of up to three templates of type Topic of Document to
produce more complex sentences (Merge transformation). The same is done for tem-
plates of type Topic of Section, Signaling Concept, and Signaling Structural. The
template Signaling Concept contains information about concepts found on section
headings; SumUM selects an appropriate verb to introduce that information in the
abstract (Verb Selection). In this way, for example, given the section heading ?Ex-
perimental Results,? SumUM is able to produce the sentence ?Presents experimental
results.?
The sorted templates constitute the text plan. Each element in the text plan is used
to produce a sentence the structure of which depends on the template. The schema of
presentation of a text plan composed of n(? 1) templates Tmpli is as follows:
Text =
n
?
i=1
[Tmpli ? ?.?].
The notation A? means the string produced by the generation of A, ? denotes con-
catenation, and
?n
i=1 Ai stands for the concatenation of all Ai. We assume that all
the parameters necessary for the generation are available (i.e., voice, tense, number,
position, etc.).
The schema of presentation of a template Tmpl of type Topic of the Document is:3
Tmpl = Tmpl.Predicate ? Tmpl.What
2 TOPIC = {Term : ?Template ? InfoDB ? Template.Topic = Term}.
3 The notation Tmpl.Slot denotes the content of slot Slot of template Tmpl.
513
Saggion and Lapalme Generating Summaries with SumUM
Table 10
Specification of the templates for the description and definition of a topic.
Type: description
Id: integer identifier
Topic: term
Predicate: instance of describe (i.e., X is composed of Y)
Content: parsed sentence fragment
Position: section and sentence id
Type: definition
Id: integer identifier
Topic: term
Predicate: instance of define (i.e., X is a Y)
Content: parsed sentence fragment
Position: section and sentence id
Table 11
Definition template instantiated with sentence ?REVERSA is a dual viewpoint noncontact
laser scanner which comes complete with scanning software and data manipulation tools.?
Type: definition
Id: 41
Topic: REVERSA
Predicate: be, . . .
Content: REVERSA is a dual viewpoint noncontact laser scanner which. . .
Position: Sentence 1 from Section 2
The predicate is generated in the present tense of the third-person singular (Syntactic
Verb Transformation). So sentences like ?X will be presented? or ?X have been pre-
sented? or ?We have presented here X,? which are usually found in source documents,
will be avoided because they are awkward in an abstract. Arguments are generated by
a procedure that expands/abbreviates acronyms (Acronym Expansion and Abbrevi-
ation), presents author-related entities in the impersonal form (concept reexpression),
uses fixed expressions in order to refer to the authors and the research paper, and
produces correct case and punctuation. Examples of sentences generated by the sys-
tem have been presented in Saggion and Lapalme (2000a). In this way we implement
some of the transformations studied in section 2.3. The schema of presentation of the
Table 12
Conceptual order for content expression.
Problem Solution, Problem Identification, Need and Situation in positional order
Topic of Document sorted in descending order of Weight
Possible Topic sorted in descending order of Weight
Topic Description, Study, Interest, Development, Entity Introduction, Research Goal,
Conceptual Goal, Conceptual Focus and Focus in positional order
Method and Experiment in positional order
Results, Inference, Knowledge and Summarization in positional order
Entity Identification in positional order
Topic of Section in section order
Signaling Structural and Signaling Concepts in positional order
514
Computational Linguistics Volume 28, Number 4
Topic of Section is
Tmpl = Tmpl.Predicate ? Tmpl.Argument.
The schema of generation of a merged template Tmpl is
Tmpl =
(
n?1
?
i=1
[Tmpl.Templatesi ? ?;?]
)
? ?and also? ? Tmpl.Templatesn,
where Tmpl.Templatesi is the ith template in the merge. Note that if n adjacent templates
in the merge share the same predicate, then only one verb is generated, and the
arguments are presented as a conjunction (i.e., ?Presents X and Y.? instead of ?Presents
X and presents Y.?). This is specified with the following schema:
Tmpl = Predicate ? Tmpl1.Arg ?
(
n?1
?
i=2
[Tmpli.Arg ? ?;?]
)
? ?and? ? Tmpln.Arg,
where Predicate is the predicate common to the merged templates.
The indicative abstract is presented along with the list of topics that are obtained
from the list Topics. SumUM presents in alphabetical order the first superficial occur-
rence of the term in the source document (this information is found in the term tree).
For the informative abstract, the system retrieves from the InfoDB those templates
matching the topics selected by the user (using the slot Topic for that purpose) and
presents the information on the Content slots in the order of the original text (using
the Position for that purpose).
4. Limitations of the Approach
Our approach is based on the empirical examination of abstracts published by sec-
ond services and on assumptions about technical text organization (Paice 1991; Bhatia
1993; Jordan 1993, 1996). In our first study, we examined 100 abstracts and source
documents in order to deduce a conceptual and linguistic model for the task of sum-
marization of technical articles. Then we expanded the corpus with 100 more items
in order to validate the model. We believe that the concepts, relations, and types of
information identified account for interesting phenomena appearing in the corpus and
constitute a sound basis for text summarization. The conceptual information has not
been formalized in ontological form, opening an avenue for future developments. All
the knowledge of the system (syntactic and conceptual) was manually acquired dur-
ing specification, implementation, and testing. The coverage and completeness of the
model have not been assessed in this work and will be the subject of future studies.
Nevertheless SumUM has been tested in different technical domains.
The implementation of our method relies on noun and verb group identification,
conceptual tagging, pattern matching, and template instantiation we have developed
for the purpose of this research. The interpreter relies on the output produced by a shal-
low text segmenter and on a statistical part-of-speech tagger. Our prototype analyzes
sentences for the specific purpose of text summarization and implements some pat-
terns of generation observed in the corpus, including the reformulation of verb groups
and noun groups, sentence combination or fusion, and conceptual deletion, among
others. We have not addressed here the question of text understanding: SumUM is
able to produce text summaries, but it is not able to demonstrate intelligent behavior
515
Saggion and Lapalme Generating Summaries with SumUM
(answering questions, paraphrasing, anaphora resolution, etc.). Concerning the prob-
lem of text coherence, we have not properly addressed the problem of identification of
anaphoric expressions in technical documents: SumUM excludes from the content of
the indicative abstract sentences containing expressions considered problematic. The
problem of anaphoric expressions in technical articles has been extensively addressed
in research work carried out under the British Library Automatic Abstracting Project
(BLAB) (Johnson et al 1993; Paice et al 1994). Although some of the exclusion rules
implemented in the BLAB project are considered in SumUM (exclusion of sentences
with quantifier subject, sentences with demonstratives, some initial connectives, and
pronouns), our approach lacks coverage of some important cases dealt with in the
BLAB rules, such as the inclusion of sentences because of dangling anaphora.
This implementation of SumUM ignores some aspects of the structure of textlike
lists and enumerations, and most of the process overlooks the information about para-
graph structure. Nevertheless, in future improvements of SumUM, these will be taken
into consideration to produce better results.
5. Evaluating the Summaries
Abstracts are texts used in tasks such as assessing the content of a source document
and deciding if it is worth reading. If text summarization systems are designed to
fulfill the requirements of those tasks, the quality of the generated texts has to be eval-
uated according to their intended function. The quality of human-produced abstracts
has been examined in the literature (Grant 1992; Kaplan et al 1994; Gibson 1993),
using linguistic criteria such as cohesion and coherence, thematic structure, sentence
structure, and lexical density; in automatic text summarization, however, such detailed
analysis is only just emerging. Content evaluation assesses whether an automatic sys-
tem is able to identify the intended ?topics? of the source document. Text quality
evaluation assesses the readability, grammar, and coherence of a summary. The eval-
uations can be made in intrinsic or extrinsic fashions as defined by Sparck Jones and
Galliers (1995).
An intrinsic evaluation measures the quality of the summary itself by compar-
ing the summary with the source document, by measuring how many ?main? ideas
of the source document are covered by the abstract, or by comparing the content of
the automatic summary with an ideal abstract (gold standard) produced by a human
(Mariani 1995). An extrinsic evaluation measures how helpful a summary is in the
completion of a given task. For example, given a document that contains the answers
to some predefined questions, readers are asked to answer those questions using the
document?s abstract. If the reader correctly answers the questions, the abstract is con-
sidered of good quality for the given question-answering task. Variables measured can
be the number of correct answers and the time to complete the task. Recent experi-
ments (Jing et al 1998) have shown how different parameters such as the length of
the abstract can affect the outcome of the evaluation.
5.1 Evaluation of Indicative Content and Text Quality
Our objective in the evaluation of indicative content is to see whether the abstracts
produced by our method convey the essential content of the source documents in
order to help readers complete a categorization task. In the evaluation of text quality,
we want to determine whether the abstracts produced by our method are acceptable
according to a number of acceptability criteria.
516
Computational Linguistics Volume 28, Number 4
5.1.1 Design. In both evaluations we are interested in comparing our summaries with
summaries produced using other methodologies, including human-written ones. In or-
der to evaluate the content, we presented evaluators with abstracts and five descriptors
(lists of keywords) for each abstract. The evaluators had to find the correct descrip-
tor for the abstract. One of the descriptors was the correct descriptor of the abstract
and the others were descriptors from the same domain, obtained from the journals
in which the source documents were published. In order to evaluate text quality, we
asked the evaluators to provide an acceptability score between 0?5 for the abstract
(0 for unacceptable and 5 for acceptable) based on the following criteria taken from
Rowley (1982): good spelling and grammar, clear indication of the topic of the source
document, conciseness, readability and understandability, and whether acronyms are
presented along with their expansions. We told the evaluators that we would consider
the abstracts with scores above 2.5 acceptable; with this information, they could use
scores below or above that borderline to enforce acceptability. The design of this ex-
periment was validated by three IS specialists. The experiment was run three times
with different data each time and with a different set of summarizers (human or auto-
matic). When we first designed this experiment, only one text summarization system
was available to us, so we performed the experiment comparing automatic abstracts
produced by two summarizers and abstracts published with the source documents.
Later on, we found two other summarizers, and we decided to repeat the experiment
only considering three automatic systems.
Our evaluation mirrors the TIPSTER SUMMAC categorization task (Firmin and
Chrzanowski 1999; Mani et al 1998) in which given a generic summary (or a full
document), the human participant chooses a single category (out of five categories)
to which the document is relevant. The evaluation seeks to determine whether the
summary is effective in capturing whatever information in the document is needed
to correctly categorize the document. In the TIPSTER SUMMAC evaluation, 10 Text
Retrieval Conference (TREC) topics and 100 documents per topic were used, and
16 systems participated. The results for TREC indicate that there are no significant
differences among the systems for the categorization task and that the performance
using the full document is not much better.
5.1.2 Subjects and Materials. All our evaluators were IS students/staff from Uni-
versite? de Montre?al, McGill University, and John Abbott College. They were chosen
because they have knowledge about what constitutes a good indicative abstract. We
used the Latin square experimental design, whereby forms included n abstracts from
n different documents, where n depends on the number of subjects (thus an evaluator
never compared different summaries of the same document). Each abstract was printed
on a different page including the five descriptors, a field to be completed with the qual-
ity score associated with the abstract, and a field to be filled with comments about the
abstract. In order to produce the evaluation forms, we used source documents (all tech-
nical articles) from the journal Industrial Robots, found in the Emerald Electronic Library
?http://www.emerald-library.com?. In addition to the abstracts published with source
documents, we produced automatic abstracts using the following systems: SumUM,
Microsoft?97 Autosummarize, Extractor, and n-STEIN. Microsoft?97 Autosummarize is
distributed with Word?97. Extractor (Turney 1999) is a system that takes a text file
as input (plain ASCII text, HTML, or e-mail) and generates a list of keywords and
keyphrases as output. On average, it generates the number of phrases requested by
the user, but the actual number for any given document may be slightly below or above
the requested number, depending mainly on the length of the input document. Ex-
tractor has 12 parameters relevant for keyphrase extraction that are tuned by a genetic
517
Saggion and Lapalme Generating Summaries with SumUM
algorithm to maximize performance on training data. We used Extractor 5.1, which
is distributed for demonstration (downloaded from ?http://extractor.iit.nrc.ca/?). n-
STEIN is a commercial system that was available for demonstration purposes at the
time we were conducting our research (n-STEIN 2000) (January 2000). The system
is based on a combination of statistical and linguistic processing. Unfortunately no
technical details of the system are given.
5.1.3 Procedure. Each abstract was evaluated by three different evaluators, who were
not aware of the method used to produce the abstracts. In order to measure the out-
come of the categorization task, we considered the abstract to have helped in catego-
rizing the source document if two or more evaluators were able to chose the correct
descriptor for the abstract. In order to measure the quality of the abstract, we computed
the average quality using the scores given by the evaluators.
5.1.4 Results and Discussion. In Table 13 we present the average information for
three runs of this experiment. ?Success? refers to the percentage of cases in which
subjects identified the correct descriptor. ?Quality? refers to subjects? summary quality
score. Note that because of this particular design, we cannot compare numbers across
experiments, we can only discuss results for each experiment.
Overall, for each experiment no significant differences were observed between the
different automatic systems in the categorization task. All automatic methods per-
formed similarly, though we believe that documents and descriptors of narrower do-
mains are needed in order to correctly assess the effectiveness of each summarization
method. Unfortunately, the construction of such resources goes beyond our present
research and will be addressed in future work.
The figures for text acceptability indicate that abstracts produced by Autosum-
marize are below the acceptability level of 2.5. The abstracts produced by SumUM,
Extractor, and n-STEIN are above the acceptability level of 2.5, and the human abstracts
are highly acceptable. In the first experiment, an analysis of the variance (ANOVA) for
text quality (Oakes 1998) showed differences among the three methods at p ? 0.005
(observed F(2, 27) = 9.66). Tukey?s multiple-comparison test (Byrkit 1987) shows statis-
Table 13
Results of human judgment in a categorization task and assessment about text quality.
Experiment Summarization Methods
First Autosummarize SumUM Human
15 evaluators Success Quality Success Quality Success Quality
10 documents 80% 1.46 80% 3.23 100% 4.25
Second Autosummarize SumUM Human
18 evaluators Success Quality Success Quality Success Quality
12 documents 70% 1.98 70% 3.15 80% 4.04
Third n-STEIN SumUM Extractor
20 evaluators Success Quality Success Quality Success Quality
15 documents 67% 2.76 80% 3.13 73% 3.47
518
Computational Linguistics Volume 28, Number 4
tical differences in text quality at p ? 0.01 for the two automatic systems (SumUM and
Autosummarize), but no conclusion can be drawn about differences between the ab-
stracts produced by those systems and the author abstract at levels 0.01 or 0.05. In
the second experiment, the ANOVA showed differences at p ? 0.01 between the
three methods (observed F(2, 33) = 10.35). Tukey?s test shows statistical differences
at p ? 0.01 between the two automatic systems (SumUM and Autosummarize) and
differences with the author abstract at 0.05. In the third experiment, the ANOVA for
text quality did not allow us to draw any conclusions about differences in text quality
(F(2, 42) = 0.83).
5.2 Evaluation of Content in a Coselection Experiment
Our objective in the evaluation of content in a coselection experiment is to measure
coselection between sentences selected by our system and a set of ?correct? extracted
sentences. This method of evaluation has already been used in other summarization
evaluations such as Edmundson (1969) and Marcu (1997). The idea is that if we find
a high degree of overlap between the sentences selected by an automatic method
and the sentences selected by a human, the method can be regarded as effective.
Nevertheless, this method of evaluation has been criticized not only because of the
low rate of agreement between human subjects in this task (Jing et al 1998), but also
because there is no unique ideal or target abstract for a given document. Instead,
there is a set of main ideas that a good abstract should contain (Johnson 1995). In our
coselection experiment, we were also interested in comparing our system with other
summarization technologies.
5.2.1 Materials.
Data used. We used 10 technical articles from two different sources: 5 from the
journal Rapid Prototyping and 5 from the journal Internet Research. The documents were
downloaded from the Emerald Electronic Library. The abstracts and lists of keywords
provided with the documents were deleted before the documents were used in the
evaluation.
Reference extracts. We used 30 automatic abstracts (three for each article) and nine
assessors with a background in dealing with technical articles, on whom we relied to
obtain an assessment of important sentences in the source documents. Eight assessors
read two articles each, and one read four articles, because no other participants were
available when the experiment was conducted. The assessor of each article chose a
number of important sentences from that article (up to a maximum of Ni, the number
of sentences chosen by the summarization methods). Each article was read by two
different assessors; we thus had two sets of sentences for each article. We call these
sets Si,j (i ? [1 . . . 10]? j ? [1 . . . 2]). Most of the assessors found the task quite complex.
Agreement between human assessors was only 37%.
Automatic extracts. We considered three automatic systems in this evaluation:
SumUM, Autosummarize, and Extractor. We produced three abstracts for each doc-
ument. First we produced an abstract using SumUM. We counted the number of
sentences selected by SumUM in order to produce the indicative-informative abstract
(we verified that the number of sentences selected by the system represented between
10% and 25% of source documents). Then, we produced two other automatic abstracts,
one using Autosummarize and another using Extractor. We specified to each system
that it should select the same number of sentences as SumUM selected.
519
Saggion and Lapalme Generating Summaries with SumUM
5.2.2 Procedure. We measure coselection between sentences produced by each method
and the sentences selected by the assessors, computing recall, precision, and F-score
as in Firmin and Chrzanowski (1999). In order to obtain a clear picture, we borrowed
the scoring methodology proposed by Salton et al (1997), additionally considering the
following situations:
? Union scenario: For each document we considered the union of the
sentences selected by the two assessors (Si,1 ? Si,2) and computed recall,
precision, and F-score for each method.
? Intersection scenario: For each document we considered the intersection of
the sentences selected by the two assessors (Si,1 ? Si,2) and computed
recall, precision, and F-score for each method.
? Optimistic scenario: For each document and method we considered the
case in which the method performed the best (highest F-score) and
computed recall, precision, and F-score.
? Pessimistic scenario: For each document and method we considered the
case in which the method performed the worst (lowest F-score) and
computed recall, precision, and F-score.
5.2.3 Results and Discussion. For each scenario we present the average information in
Table 14 (Saggion and Lapalme [2000c] presented detailed results of this experiment).
For the scenario in which we consider the 20 human abstracts, SumUM obtained the
best F-score in 60% of the cases, Extractor in 25% of the cases, and Autosummarize in
15% of the cases. If we assume that the sentences selected by the human assessors rep-
resent the most important or interesting information in the documents, then we can
conclude that on average, SumUM performed better than the other two summarization
technologies, even if these results are not exceptional in individual cases. An ANOVA
showed statistical differences in the F-score measure at p ? 0.01 between the differ-
ent automatic abstracts (observed F(2, 57) = 5.28). Tukey?s tests showed differences
between SumUM and the two other automatic methods at p ? 0.01.
Here, we have compared three different methods of producing abstracts that are
domain independent. Nevertheless, whereas Autosummarize and Extractor are truly
text independent, SumUM is genre dependent: It was designed for the technical arti-
cle and takes advantage of this fact in order to produce abstracts. We think that this
Table 14
Coselection between sentences selected by human assessors and sentences selected by three
automatic summarization methods in recall (R), precision (P) and F-score (F).
SumUM Autosummarize Extractor
R P F R P F R P F
Average .23 .20 .21 .14 .11 .12 .12 .18 .14
Union .21 .31 .25 .16 .19 .17 .11 .26 .15
Intersection .28 .09 .14 .13 .04 .06 .08 .04 .06
Optimistic .26 .23 .25 .16 .14 .15 .14 .25 .18
Pessimistic .19 .17 .18 .11 .08 .09 .08 .11 .09
520
Computational Linguistics Volume 28, Number 4
is the reason for the better performance of SumUM in this evaluation. The results
of this experiment are encouraging considering the limited capacities of the actual
implementation. We expect to improve the results in future versions of SumUM. Ad-
ditional evaluations of SumUM using sentence acceptability criteria and content-based
measures of indicativeness have been presented in Saggion and Lapalme (2000b) and
Saggion (2000).
6. Related Work on Summarization
As a human activity, the production of summaries is directly associated with the
processes of language understanding and production: A source text is read and un-
derstood to recognize its content, which is then compiled in a concise text. In order to
explain this process, several theories have been proposed and tested in text linguistics,
cognitive science, and artificial intelligence, including macro structures (Kintsch and
van Dijk 1975; van Dijk 1977), history grammars (Rumelhart 1975), plot units (Lehn-
ert 1981), and concept/coherence relations (Alterman and Bookman 1990). Computers
have been producing summaries since the original work of Luhn (1958). Since then
several methods and theories have been applied, including the use of term frequency
? inverse document frequency (TF ? IDF) measures, sentence position, and cue and ti-
tle words (Luhn 1958; Edmundson 1969; Kupiec, Pedersen, and Chen 1995; Brandow,
Mitze, and Rau 1995); partial understanding using conceptual structures (DeJong 1982;
Tait 1982); bottom-up understanding, top-down parsing, and automatic linguistic ac-
quisition (Rau, Jacobs, and Zernik 1989); recognition of thematic text structures (Hahn
1990); cohesive properties of texts (Benbrahim and Ahmad 1995; Barzilay and Elhadad
1997); and rhetorical structure theory (Ono, Sumita, and Miike 1994; Marcu 1997).
In the context of the scientific article, Rino and Scott (1996) have addressed the
problem of coherent selection for text summarization, but they depend on the avail-
ability of a complex meaning representation, which in practice is difficult to obtain
from the raw text. Instead, superficial analysis in scientific text summarization using
lexical information was applied by Lehmam (1997) for the French language. Liddy
(1991) produced one of the most complete descriptions of conceptual information for
abstracts of empirical research. In our work, we concentrated instead on conceptual in-
formation that is common across domains. Liddy?s model includes three typical levels
of information. The most representative level, called the prototypical structure, in-
cludes the information categories subjects, purpose, conclusions, methods, references,
and hypotheses. The other two levels are the typical structure and the elaborated
structure, which include information less frequently found in abstracts of empirical
research. To our knowledge Liddy?s model has never been implemented; nevertheless,
it could be used as a starting point for improving our flat-domain model. Relevant
work in rhetorical classification for scientific articles, which is the first step toward
the production of scientific abstracts, is due to Teufel and Moens (1998), who used
statistical approaches borrowed from Kupiec, Pedersen, and Chen (1995).
Our method is close to concept-based abstracting (CBA) (Jones and Paice 1992;
Paice and Jones 1993) but differs from this approach in several aspects. CBA is used to
produce abstracts of technical articles in specific domains, for example, in the domain
of agriculture. Semantic roles such as species, cultivar, high-level property, and low-
level property are first identified by the manual analysis of a corpus, and then patterns
are specified that account for stylistic regularities of expression of the semantic roles
in texts. These patterns are used in an information extraction process that instantiates
the semantic roles. Selective analysis, although genre dependent, was developed as
domain independent and tested in different technical domains without the need to
521
Saggion and Lapalme Generating Summaries with SumUM
adapt the conceptual model, the patterns, or the conceptual dictionary. In order to
adapt CBA to new domains, the semantic roles representing the ?key? information in
the new domain need to be identified, and new templates and patterns need to be con-
structed (Oakes and Paice 2001). Although such adaptation is generally done manually,
recent work has shown how to export CBA to new domains automatically (Oakes and
Paice 1999). CBA uses a fixed canned template for summary generation, whereas our
method allows greater stylistic variability because the main ?content? of the summary
generated is expressed in the words of the authors of the paper. Selective analysis
is used to produce indicative-informative abstracts, whereas CBA is mainly used to
produce indicative abstracts, though some informative content is included in the form
of extracted sentences containing results and conclusions (Paice and Oakes 1999). Our
method can be seen as an extension of CBA that allows for domain independence and
informativeness. We believe that the indicative patterns we have designed are genre
dependent, whereas the informative patterns are general and can be used in any do-
main. Our implementation of patterns for information extraction is similar to Black?s
(1990) implementation of Paice?s (1981) indicative phrases method, but whereas Black
scores sentences based on indicative phrases contained in the sentences, our method
scores the information from the sentences based on term distribution.
Our work in sentence reformulation is different from cut-and-paste summariza-
tion (Jing and McKeown 2000) in many ways. Jing (2000) proposes a novel algorithm
for sentence reduction that takes into account different sources of information to de-
cide whether or not to remove a particular component from a sentence to be included
in a summary. The decision is made based on (1) the relation of the component to
its context, (2) the probability of deleting such a component (estimated from a cor-
pus of reduced sentences), and (3) linguistic knowledge about the essentiality of the
component in the syntactic structure. Sentence reduction is concerned only with the
removal of sentence components, so it cannot explain transformations observed in our
corpus and in summarization in general, such as the reexpression of domain concepts
and verbs. We achieve sentence reduction through a process of information extraction
that extracts verbs and arguments, sometimes considering only sentence fragments
(for example, initial prepositional phrases, parenthetical expressions, and some adver-
bials are ignored for some templates). The process removes domain concepts, avoids
unnecessary grammatical subjects, and generates coordinate structures, avoiding verb
repetition. Whereas our algorithm is genre dependent, requiring only shallow parsing,
Jing?s algorithm is genre and domain independent and requires full syntactic parsing
and disambiguation and extensive linguistic resources.
Regarding the fusion of information, we have concentrated only on the fusion
of explicit topical information (document topic, section topic, and signaling struc-
tural and conceptual elements). Jing and McKeown (2000) have proposed a rule-based
algorithm for sentence combination, but no results have been reported. Radev and
McKeown (1998) have already addressed the issue of information fusion in the con-
text of multidocument summarization in one specific domain (i.e., terrorism): The
fusion of information is achieved through the implementation of summary operators
that integrate the information of different templates from different documents refer-
ring to the same event. Although those operators are dependent on the specific task
of multidocument summarization, and to some extent on the particular domain they
deal with, it is interesting to observe that some of Radev and McKeown?s ideas could
be applied in order to improve our texts. For example, their ?refinement? operator
could be used to improve the descriptions of the entities of the indicative abstract.
The entities from the indicative abstract could be refined with definitions or descrip-
tions from the InfoDB in order to obtain a better and more compact text. The idea of
522
Computational Linguistics Volume 28, Number 4
elaborating topics has also been addressed by Mani, Gates, and Bloedorn (1999). They
have proposed a number of rules for summary revision aiming at conciseness; their
elimination rule discards parenthetical and initial prepositional phrases, as does our
approach. Their aggregation operation combines two constituents on the basis of ref-
erential identity and so is more general than our combination of topical information.
Although their approach is domain independent, it requires full syntactic analysis and
coreference resolution.
7. Conclusions
SumUM has been fully implemented to take a raw text as input and produce a sum-
mary. This involves the following successive steps: text segmentation, part-of-speech
tagging, partial syntactic and semantic analysis, sentence classification, template in-
stantiation, content selection, text regeneration, and topic elaboration. Our research
was based on the intensive study of manual alignments between sentences of pro-
fessional abstracts and elements of source documents and on the exploration of the
essential differences between indicative and informative abstracts.
Although our method was deeply influenced by the results of our corpus study,
it nevertheless has many points in common with recent theoretical and program-
matic directions in automatic text summarization. For example, Sparck Jones (1997)
argues in favor of a kind of ?indicative, skeletal summary? and the need to explore dy-
namic, context-sensitive summarization in interactive situations in which the summary
changes according to the user needs. Hutchins (1995) advocates indicative summaries,
produced from parts of a document in which the topics are likely to be stated. These
abstracts are well suited for situations in which the actual user is unknown (i.e., a
general reader), since the abstract will provide the reader with good entry points
for retrieving more detailed information. If the users are known, the abstract can be
tailored to their specific profiles; such profiles might specify the reader?s interest in
various types of information, such as conclusions, definitions, methods, or user needs
expressed in a ?query? to an information retrieval system (Tombros, Sanderson, and
Gray 1998). Our method, however, was designed without any particular reader in
mind and with the assumption that a text does have a ?main? topic.
In this article, we have presented an evaluation of automatically generated
indicative-informative abstracts in terms of content and text quality. In the evalua-
tion of the indicative content in a categorization task, no differences were observed
among the different automatic systems. The automatic abstracts generated by SumUM
were considered more acceptable than other systems? abstracts. In the evaluation of
the informative content, SumUM selected sentences that were evaluated as more rele-
vant by human assessors than sentences selected by other summarization technologies;
statistical tests showed significant differences between the automatic methods in that
evaluation. In the future, we plan to address several issues, including the study of
robust automatic text classification techniques, anaphora resolution, and lexical cohe-
sion for improving elaboration of topics as well as the incorporation of local discourse
analysis to improve the coherence of abstracts.
Acknowledgments
We would like specially to thank Eduard
Hovy for his valuable comments and
suggestions, which helped improve and
clarify the present work. We are indebted to
the three anonymous reviewers for their
extensive suggestions, which also helped
improve this work. We are grateful to
Professor Miche`le Hudon from Universite?
de Montre?al for fruitful discussion and to
Professor John E. Leide from McGill
University, to Mme. Gracia Pagola from
523
Saggion and Lapalme Generating Summaries with SumUM
Universite? de Montre?al, and to Christine
Jacobs from John Abbott College for their
help in recruiting assessors for the
experiments. We thank also Elliott
Macklovitch and Diana Maynard, who
helped us improve the quality of our article,
and the members of the Laboratoire de
Recherche Applique?e en Linguistique
Informatique (RALI) for their participation
in our experiments. The first author was
supported by Agence Canadienne de
De?veloppement International (ACDI)
during his Ph.D. research. He also received
support from Fundacio?n Antorchas
(A-13671/1-47), Ministerio de Educacio?n de
la Nacio?n de la Repu?blica Argentina
(Resolucio?n 1041/96) and Departamento de
Computacio?n, Facultad de Ciencias Exactas
y Naturales, Universidad de Buenos Aires,
Argentina.
References
Alterman, Richard and
Lawrence A. Bookman. 1990. Some
computational experiments in
summarization. Discourse Processes,
13:143?174.
American National Standards Institute.
(ANSI). 1979. Writing Abstracts.
Barzilay, Regina and Michael Elhadad. 1997.
Using lexical chains for text
summarization. In Proceedings of the
ACL/EACL?97 Workshop on Intelligent
Scalable Text Summarization, pages 10?17,
Madrid, July.
Benbrahim, Mohamed and Kurshid Ahmad.
1995. Text summarisation: The role of
lexical cohesion analysis. New Review of
Document & Text Management, 1:321?335.
Bernier, Charles L. 1985. Abstracts and
abstracting. In E. D. Dym, editor, Subject
and Information Analysis, volume 47 of
Books in Library and Information Science.
Marcel Dekker, Inc., pages 423?444.
Bhatia, Vijay K. 1993. Analysing Genre:
Language Use in Professional Settings.
Longman.
Black, William J. 1990. Knowledge based
abstracting. Online Review, 14(5):327?340.
Brandow, Ronald, K. Mitze, and Lisa F. Rau.
1995. Automatic condensation of
electronic publications by sentence
selection. Information Processing &
Management, 31(5):675?685.
Bunge, Mario. 1967. Scientific Research I. The
Search for System. Springer-Verlag, New
York.
Byrkit, Donald R. 1987. Statistics Today: A
Comprehensive Introduction.
Benjamin/Cummings.
Cremmins, Eduard T. 1982. The Art of
Abstracting. ISI Press.
DeJong, Gerald. 1982. An overview of the
FRUMP system. In W. G. Lehnert and
M. H. Ringle, editors, Strategies for Natural
Language Processing. Lawrence Erlbaum,
pages 149?176.
Edmundson, H. P. 1969. New methods in
automatic extracting. Journal of the
Association for Computing Machinery,
16(2):264?285.
Educational Resources Information Center
(ERIC). 1980. Processing Manual: Rules and
Guidelines for the Acquisition, Selection, and
Technical Processing of Documents and
Journal Articles by the Various Components of
the ERIC Network. ERIC.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press.
Filman, Roger E. and Sangan Pant. 1998.
Searching the Internet. IEEE Internet
Computing, 2(4):21?23.
Firmin, The?re`se and
Michael J. Chrzanowski. 1999. An
evaluation of automatic text
summarization systems. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
pages 325?336.
Foster, George. 1991. Statistical lexical
disambiguation. Master?s thesis, School of
Computer Science, McGill University,
Montre?al, Que?bec, Canada.
Gibson, Timothy R. 1993. Towards a Discourse
Theory of Abstracts and Abstracting.
Department of English Studies,
University of Nottingham.
Grant, Pamela. 1992. The Integration of Theory
and Practice in the Development of
Summary-Writing Strategies. Ph.D. thesis,
Faculte? des E?tudes Supe?rieures,
Universite? de Montre?al.
Hahn, Udo. 1990. Topic parsing: Accounting
for text macro structures in full-text
analysis. Information Processing &
Management, 26(1):135?170.
Hutchins, John. 1995. Introduction to text
summarization workshop. In
B. Engres-Niggemeyer, J. Hobbs, and
K. Sparck Jones, editors, Summarising Text
for Intelligent Communication, Dagstuhl
Seminar Report 79. IBFI, Schloss
Dagstuhl, Wadern, Germany,
INSPEC. 2000. INSPEC database for physics,
electronics and computing.
http://www.iee.org.uk/publish/inspec/
Jing, Hongyan. 2000. Sentence reduction for
automatic text summarization. In
Proceedings of the Sixth Applied Natural
Language Processing Conference, pages
310?315, Seattle, April 29?May 4.
524
Computational Linguistics Volume 28, Number 4
Jing, Hongyan and Kathleen McKeown.
2000. Cut and paste based text
summarization. In Proceedings of the First
Meeting of the North American Chapter of the
Association for Computational Linguistics,
pages 178?185, Seattle, April 29?May 4.
Jing, Hongyan, Kathleen McKeown, Regina
Barzilay, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In Intelligent
Text Summarization: Papers from the 1998
AAAI Spring Symposium. Stanford, March
23?25. Technical Report SS-98-06, AAAI
Press, pages 60?68.
Johnson, Frances. 1995. Automatic
abstracting research. Library Review,
44(8):28?36.
Johnson, Frances C., Chris D. Paice,
William J. Black, and A. P. Neal. 1993. The
application of linguistic processing to
automatic abstract generation. Journal of
Document & Text Management, 1(3):215?241.
Jones, Paul A. and Chris D. Paice. 1992. A
?select and generate? approach to
automatic abstracting. In A. M. McEnry
and C. D. Paice, editors, Proceedings of the
14th British Computer Society Information
Retrieval Colloquium. Springer Verlag,
pages 151?154.
Jordan, Michael P. 1993. Openings in very
formal technical texts. Technostyle,
11(1):1?26.
Jordan, Michael P. 1996. The Language of
Technical Communication: A Practical Guide
for Engineers, Technologists and Technicians.
Quarry.
Kaplan, Robert B., Selena Cantor, Cynthia
Hagstrom, Lia D. Kamhi-Stein, Yumiko
Shiotani, and Cheryl B. Zimmerman. 1994.
On abstract writing. Text, 14(3):401?426.
Kintsch, Walter and Teun A. van Dijk. 1975.
Comment on se rappelle et on re?sume des
histoires. Langages, 40:98?116.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?Step one:
Sentence compression. In Proceedings of the
17th National Conference of the American
Association for Artificial Intelligence (AAAI),
July 30?August 3.
Kupiec, Julian, Jan Pedersen, and Francine
Chen. 1995. A trainable document
summarizer. In Proceedings of the 18th
ACM-SIGIR Conference, pages 68?73.
Lehmam, Abderrafih. 1997. Une
structuration de texte conduisant a` la
construction d?un syste`me de re?sume?
automatique. In Actes des journe?es
scientifiques et techniques du re?seau
francophone de l?inge?nierie de la langue de
l?AUPELF-UREF, pages 175?182, 15?16
April.
Lehnert, Wendy. 1981. Plot units and
narrative summarization. Cognitive
Science, 5:293?331.
Liddy, Elizabeth D. 1991. The
discourse-level structure of empirical
abstracts: An exploratory study.
Information Processing & Management,
27(1):55?81.
Luhn, Hans P. 1958. The automatic creation
of literature abstracts. IBM Journal of
Research Development, 2(2):159?165.
Maizell, Robert E., Julian F. Smith, and
Tibor E. R. Singer. 1971. Abstracting
Scientific and Technical Literature.
Wiley-Interscience.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 558?565,
College Park, MD, 20?26 June.
Mani, Inderjeet, David House, Gary Klein,
Lynette Hirshman, Leo Obrst, The?re`se
Firmin, Michael Chrzanowski, and Beth
Sundheim. 1998. The TIPSTER SUMMAC
text summarization evaluation. Technical
Report, Mitre Corporation.
Marcu, Daniel. 1997. From discourse
structures to text summaries. In
Proceedings of the ACL?97/EACL?97 Workshop
on Intelligent Scalable Text Summarization,
pages 82?88, Madrid, July 11.
Mariani, Joseph. 1995. Evaluation. In
Ronald E. Cole, editor, Survey of the State of
the Art in Human Language Technology.
Cambridge University Press, chapter 13,
pages 475?518.
Mathis, Betty A. and James E. Rush. 1985.
Abstracting. In E. D. Dym, editor, Subject
and Information Analysis, volume 47 of
Books in Library and Information Science.
Marcel Dekker, pages 445?484.
n-STEIN. 2000. n-STEIN Web page.
http://www.gespro.com.
Oakes, Michael P. 1998. Statistics for Corpus
Linguistics. Edinburgh University Press.
Oakes, Michael P. and Chris D. Paice. 1999.
The automatic generation of templates for
automatic abstracting. In 21st BCS IRSG
Colloquium on IR, Glasgow.
Oakes, Michael P. and Chris D. Paice. 2001.
Term extraction for automatic abstracting.
In D. Bourigault, C. Jacquemin, and
M.-C. L?Homme, editors, Recent Advances
in Computational Terminology, volume 2 of
Natural Language Processing. John
Benjamins, chapter 17, pages 353?370.
Ono, Kenji, Kazuo Sumita, and Seiji Miike.
1994. Abstract generation based on
rhetorical structure extraction. In
Proceedings of the International Conference on
525
Saggion and Lapalme Generating Summaries with SumUM
Computational Linguistics, pages 344?348.
Paice, Chris D. 1981. The automatic
generation of literary abtracts: An
approach based on identification of
self-indicating phrases. In O. R. Norman,
S. E. Robertson, C. J. van Rijsbergen, and
P. W. Williams, editors, Information
Retrieval Research. London: Butterworth,
pages 172?191.
Paice, Chris D. 1991. The rhetorical
structure of expository text. In K. P. Jones,
editor, Informatics 11: The Structuring of
Information, University of York, 20?22
March. Aslib.
Paice, Chris D., William J. Black, Frances C.
Johnson, and A. P. Neal. 1994. Automatic
abstracting. R&D Report 6166, British
Library.
Paice, Chris D. and Paul A. Jones. 1993. The
identification of important concepts in
highly structured technical papers. In
R. Korfhage, E. Rasmussen, and P. Willett,
editors, Proceedings of the 16th ACM-SIGIR
Conference, pages 69?78.
Paice, Chris D. and Michael P. Oakes. 1999.
A concept-based method for automatic
abstracting. Research Report 27, Library
and Information Commission.
Radev, Dragomir R. and Kathleen R.
McKeown. 1998. Generating natural
language summaries from multiple
on-line sources. Computational Linguistics,
24(3):469?500.
Rau, Lisa F., Paul S. Jacobs, and Uri Zernik.
1989. Information extraction and text
summarization using linguistic
knowledge acquisition. Information
Processing & Management, 25(4):419?428.
Rino, Lucia H. M. and Donia Scott. 1996. A
discourse model for gist preservation. In
D. L. Borges and C. A. A. Kaestner,
editors, Proceedings of the 13th Brazilian
Symposium on Artificial Intelligence
(SBIA?96): Advances in Artificial Intelligence,
October 23?25, Curitiba, Brazil. Springer,
pages 131?140.
Rowley, Jennifer. 1982. Abstracting and
Indexing. Clive Bingley, London.
Rumelhart, David E. 1975. Notes on a
schema for stories. In Language, Thought,
and Culture: Advances in the Study of
Cognition. Academic Press.
Saggion, Horacio. 2000. Ge?ne?ration
automatique de re?sume?s par analyse se?lective.
Ph.D. thesis, De?partement d?informatique
et de recherche ope?rationnelle, Faculte?
des arts et des sciences, Universite? de
Montre?al, Montre?al.
Saggion, Horacio and Guy Lapalme. 2000a.
Concept identification and presentation in
the context of technical text
summarization. In Proceedings of the
Workshop on Automatic Summarization
(ANLP-NAACL2000), Seattle, 30 April.
Association for Computational
Linguistics.
Saggion, Horacio and Guy Lapalme. 2000b.
Selective analysis for automatic
abstracting: Evaluating indicativeness and
acceptability. In Proceedings of the
Computer-Assisted Information Searching on
Internet Conference (RIAO?2000), Paris,
12?14 April.
Saggion, Horacio and Guy Lapalme. 2000c.
Summary generation and evaluation in
SumUM. In Advances in Artificial
Intelligence. International Joint Conference:
Seventh Ibero-American Conference on
Artificial Intelligence and 15th Brazilian
Symposium on Artificial Intelligence
(IBERAMIA-SBIA 2000), volume 1952 of
Lecture Notes in Artificial Intelligence.
Springer-Verlag, pages 329?338.
Salton, Gerald, Amit Singhal, Mandar Mitra,
and Chris Buckley. 1997. Automatic text
structuring and summarization.
Information Processing & Management,
33(2):193?207.
Sharp, Bernadette. 1998. Elaboration and
Testing of New Methodologies for Automatic
Abstracting. Ph.D. thesis, University of
Aston in Birmingham.
SICStus. 1998. SICStus Prolog User?s Manual.
SICStus.
Sparck Jones, Karen. 1993. What might be in
a summary? In K. Knorz and
C. Womser-Hacker, editors, Information
Retrieval 93: Von der Modellierung zur
Anwendung.
Sparck Jones, Karen. 1997. Document
processing: Summarization. In R. Cole,
editor, Survey of the State of the Art in
Human Language Technology. Cambridge
University Press, chapter 7, pages
266?269.
Sparck Jones, Karen and Brigitte
Endres-Niggemeyer. 1995. Automatic
summarizing. Information Processing &
Management, 31(5):625?630.
Sparck Jones, Karen and Julia R. Galliers.
1995. Evaluating Natural Language
Processing Systems: An Analysis and Review,
number 1083 in Lecture Notes in Artificial
Intelligence. Springer.
Tait, John I. 1982. Automatic Summarising of
English Texts. Ph.D. thesis, Computer
Laboratory, Cambridge University,
Cambridge.
Teufel, S. 1998. Meta-discourse markers and
problem-structuring in scientific texts. In
M. Stede, L. Wanner, and E. Hovy,
editors, Proceedings of the Workshop on
526
Computational Linguistics Volume 28, Number 4
Discourse Relations and Discourse Markers
(COLING-ACL?98), pages 43?49, 15
August.
Teufel, S. and M. Moens. 1998. Sentence
extraction and rhetorical classification for
flexible abstracts. In Intelligent Text
Summarization: Papers from the 1998 AAAI
Spring Symposium, Stanford, March 23?25.
Technical Report SS-98-06, AAAI Press,
pages 16?25.
Tombros, Anastasios, Mark Sanderson, and
Phil Gray. 1998. Advantages of query
biased summaries in information
retrieval. In Intelligent Text Summarization:
Papers from the 1998 AAAI Spring
Symposium, Stanford, March 23?25.
Technical Report SS-98-06, AAAI Press,
pages 34?43.
Turney, Peter D. 1999. Learning to extract
keyphrases from text. Technical Report
ERB-1051, National Research Council of
Canada.
van Dijk, Teun A. 1977. Recalling and
summarizing complex discourse. In
W. Burghardt and K. Holzer, editors, Text
Processing. De Gruyter, New York and
Berlin, pages 49?118.
Vianna, Fernando de Melo, editor. 1980.
Roget?s II: The New Thesaurus. Houghton
Mifflin, Boston.
Wall, Larry, Tom Christiansen, and
Randal L. Schwartz. 1996. Programming
Perl. O?Reilly & Associates, second
edition.
Evaluation challenges in large-scale document summarization
Dragomir R. Radev
U. of Michigan
radev@umich.edu
Wai Lam
Chinese U. of Hong Kong
wlam@se.cuhk.edu.hk
Arda C?elebi
USC/ISI
ardax@isi.edu
Simone Teufel
U. of Cambridge
simone.teufel@cl.cam.ac.uk
John Blitzer
U. of Pennsylvania
blitzer@seas.upenn.edu
Danyu Liu
U. of Alabama
liudy@cis.uab.edu
Horacio Saggion
U. of Sheffield
h.saggion@dcs.shef.ac.uk
Hong Qi
U. of Michigan
hqi@umich.edu
Elliott Drabek
Johns Hopkins U.
edrabek@cs.jhu.edu
Abstract
We present a large-scale meta evaluation
of eight evaluation measures for both
single-document and multi-document
summarizers. To this end we built a
corpus consisting of (a) 100 Million auto-
matic summaries using six summarizers
and baselines at ten summary lengths in
both English and Chinese, (b) more than
10,000 manual abstracts and extracts, and
(c) 200 Million automatic document and
summary retrievals using 20 queries. We
present both qualitative and quantitative
results showing the strengths and draw-
backs of all evaluation methods and how
they rank the different summarizers.
1 Introduction
Automatic document summarization is a field that
has seen increasing attention from the NLP commu-
nity in recent years. In part, this is because sum-
marization incorporates many important aspects of
both natural language understanding and natural lan-
guage generation. In part it is because effective auto-
matic summarization would be useful in a variety of
areas. Unfortunately, evaluating automatic summa-
rization in a standard and inexpensive way is a diffi-
cult task (Mani et al, 2001). Traditional large-scale
evaluations are either too simplistic (using measures
like precision, recall, and percent agreement which
(1) don?t take chance agreement into account and (2)
don?t account for the fact that human judges don?t
agree which sentences should be in a summary) or
too expensive (an approach using manual judge-
ments can scale up to a few hundred summaries but
not to tens or hundreds of thousands).
In this paper, we present a comparison of six
summarizers as well as a meta-evaluation including
eight measures: Precision/Recall, Percent Agree-
ment, Kappa, Relative Utility, Relevance Correla-
tion, and three types of Content-Based measures
(cosine, longest common subsequence, and word
overlap). We found that while all measures tend
to rank summarizers in different orders, measures
like Kappa, Relative Utility, Relevance Correlation
and Content-Based each offer significant advantages
over the more simplistic methods.
2 Data, Annotation, and Experimental
Design
We performed our experiments on the Hong Kong
News corpus provided by the Hong Kong SAR of
the People?s Republic of China (LDC catalog num-
ber LDC2000T46). It contains 18,146 pairs of par-
allel documents in English and Chinese. The texts
are not typical news articles. The Hong Kong News-
paper mainly publishes announcements of the local
administration and descriptions of municipal events,
such as an anniversary of the fire department, or sea-
sonal festivals. We tokenized the corpus to iden-
tify headlines and sentence boundaries. For the En-
glish text, we used a lemmatizer for nouns and verbs.
We also segmented the Chinese documents using the
tool provided at http://www.mandarintools.com.
Several steps of the meta evaluation that we per-
formed involved human annotator support. First, we
Cluster 2 Meetings with foreign leaders
Cluster 46 Improving Employment Opportunities
Cluster 54 Illegal immigrants
Cluster 60 Customs staff doing good job.
Cluster 61 Permits for charitable fund raising
Cluster 62 Y2K readiness
Cluster 112 Autumn and sports carnivals
Cluster 125 Narcotics Rehabilitation
Cluster 199 Intellectual Property Rights
Cluster 241 Fire safety, building management concerns
Cluster 323 Battle against disc piracy
Cluster 398 Flu results in Health Controls
Cluster 447 Housing (Amendment) Bill Brings Assorted Improvements
Cluster 551 Natural disaster victims aided
Cluster 827 Health education for youngsters
Cluster 885 Customs combats contraband/dutiable cigarette operations
Cluster 883 Public health concerns cause food-business closings
Cluster 1014 Traffic Safety Enforcement
Cluster 1018 Flower shows
Cluster 1197 Museums: exhibits/hours
Figure 1: Twenty queries created by the LDC for
this experiment.
asked LDC to build a set of queries (Figure 1). Each
of these queries produced a cluster of relevant doc-
uments. Twenty of these clusters were used in the
experiments in this paper.
Additionally, we needed manual summaries or ex-
tracts for reference. The LDC annotators produced
summaries for each document in all clusters. In or-
der to produce human extracts, our judges also la-
beled sentences with ?relevance judgements?, which
indicate the relevance of sentence to the topic of the
document. The relevance judgements for sentences
range from 0 (irrelevant) to 10 (essential). As in
(Radev et al, 2000), in order to create an extract of
a certain length, we simply extract the top scoring
sentences that add up to that length.
For each target summary length, we produce an
extract using a summarizer or baseline. Then we
compare the output of the summarizer or baseline
with the extract produced from the human relevance
judgements. Both the summarizers and the evalua-
tion measures are described in greater detail in the
next two sections.
2.1 Summarizers and baselines
This section briefly describes the summarizers we
used in the evaluation. All summarizers take as input
a target length (n%) and a document (or cluster) split
into sentences. Their output is an n% extract of the
document (or cluster).
? MEAD (Radev et al, 2000): MEAD is
a centroid-based extractive summarizer that
scores sentences based on sentence-level and
inter-sentence features which indicate the qual-
ity of the sentence as a summary sentence. It
then chooses the top-ranked sentences for in-
clusion in the output summary. MEAD runs on
both English documents and on BIG5-encoded
Chinese. We tested the summarizer in both lan-
guages.
? WEBS (Websumm (Mani and Bloedorn,
2000)): can be used to produce generic and
query-based summaries. Websumm uses a
graph-connectivity model and operates under
the assumption that nodes which are connected
to many other nodes are likely to carry salient
information.
? SUMM (Summarist (Hovy and Lin, 1999)):
an extractive summarizer based on topic signa-
tures.
? ALGN (alignment-based): We ran a sentence
alignment algorithm (Gale and Church, 1993)
for each pair of English and Chinese stories.
We used it to automatically generate Chinese
?manual? extracts from the English manual ex-
tracts we received from LDC.
? LEAD (lead-based): n% sentences are chosen
from the beginning of the text.
? RAND (random): n% sentences are chosen at
random.
The six summarizers were run at ten different tar-
get lengths to produce more than 100 million sum-
maries (Figure 2). For the purpose of this paper, we
only focus on a small portion of the possible experi-
ments that our corpus can facilitate.
3 Summary Evaluation Techniques
We used three general types of evaluation measures:
co-selection, content-based similarity, and relevance
correlation. Co-selection measures include preci-
sion and recall of co-selected sentences, relative util-
ity (Radev et al, 2000), and Kappa (Siegel and
Castellan, 1988; Carletta, 1996). Co-selection meth-
ods have some restrictions: they only work for ex-
tractive summarizers. Two manual summaries of the
same input do not in general share many identical
sentences. We address this weakness of co-selection
Lengths #dj
05W 05S 10W 10S 20W 20S 30W 30S 40W 40S FD
E-FD - - - - - - - - - - x 40
E-LD X X X X x x X X X X - 440
E-RA X X X X x x X X X X - 440
E-MO x x X x x x X x X x - 540
E-M2 - - - - - X - - - - - 20
E-M3 - - - - - X - - - - - 8
E-S2 - - - - - X - - - - - 8
E-WS - X - X x x - X - X - 160
E-WQ - - - - - X - - - - - 10
E-LC - - - - - - x - - - - 40
E-CY - X - X - x - X - X - 120
E-AL X X X X X X X X X X - 200
E-AR X X X X X X X X X X - 200
E-AM X X X X X X X X X X - 200
C-FD - - - - - - - - - - x 40
C-LD X X X X x x X X X X - 240
C-RA X X X X x x X X X X - 240
C-MO X x X x x x X x X x - 320
C-M2 - - - - - X - - - - - 20
C-CY - X - X - x - X - X - 120
C-AL X X X X X X X X X X - 180
C-AR X X X X X X X X X X - 200
C-AM - X X X X X X X X - 120
X-FD - - - - - - - - - - x 40
X-LD X X X X x x X X X X - 240
X-RA X X X X x x X X X X - 240
X-MO X x X x x x X x X x - 320
X-M2 - - - - - X - - - - - 20
X-CY - X - X - x - X - X - 120
X-AL X X X X X X X X X X - 140
X-AR X X X X X X X X X X - 160
X-AM - X X X X X X X - X - 120
Figure 2: All runs performed (X = 20 clusters, x = 10 clusters). Language: E = English, C = Chinese,
X = cross-lingual; Summarizer: LD=LEAD, RA=RAND, WS=WEBS, WQ=WEBS-query based, etc.; S =
sentence-based, W = word-based; #dj = number of ?docjudges? (ranked lists of documents and summaries).
Target lengths above 50% are not shown in this table for lack of space. Each run is available using two
different retrieval schemes. We report results using the cross-lingual retrievals in a separate paper.
measures with several content-based similarity mea-
sures. The similarity measures we use are word
overlap, longest common subsequence, and cosine.
One advantage of similarity measures is that they
can compare manual and automatic extracts with
manual abstracts. To our knowledge, no system-
atic experiments about agreement on the task of
summary writing have been performed before. We
use similarity measures to measure interjudge agree-
ment among three judges per topic. We also ap-
ply the measures between human extracts and sum-
maries, which answers the question if human ex-
tracts are more similar to automatic extracts or to
human summaries.
The third group of evaluation measures includes
relevance correlation. It shows the relative perfor-
mance of a summary: how much the performance
of document retrieval decreases when indexing sum-
maries rather than full texts.
Task-based evaluations (e.g., SUMMAC (Mani
et al, 2001), DUC (Harman and Marcu, 2001), or
(Tombros et al, 1998) measure human performance
using the summaries for a certain task (after the
summaries are created). Although they can be a
very effective way of measuring summary quality,
task-based evaluations are prohibitively expensive at
large scales. In this project, we didn?t perform any
task-based evaluations as they would not be appro-
priate at the scale of millions of summaries.
3.1 Evaluation by sentence co-selection
For each document and target length we produce
three extracts from the three different judges, which
we label throughout as J1, J2, and J3.
We used the rates 5%, 10%, 20%, 30%, 40% for
most experiments. For some experiments, we also
consider summaries of 50%, 60%, 70%, 80% and
90% of the original length of the documents. Figure
3 shows some abbreviations for co-selection that we
will use throughout this section.
3.1.1 Precision and Recall
Precision and recall are defined as:
PJ2 (J1) =
A
A+ C
,RJ2 (J1) =
A
A+ B
J2
Sentence in
Extract
Sentence not
in Extract
Sentence in
Extract
A B A+ B
J1 Sentence not
in Extract
C D C +D
A+ C B +D N = A +
B+C+D
Figure 3: Contingency table comparing sentences
extracted by the system and the judges.
In our case, each set of documents which is com-
pared has the same number of sentences and also
the same number of sentences are extracted; thus
P = R.
The average precision Pavg(SY STEM) and re-
call Ravg(SY STEM) are calculated by summing
over individual judges and normalizing. The aver-
age interjudge precision and recall is computed by
averaging over all judge pairs.
However, precision and recall do not take chance
agreement into account. The amount of agreement
one would expect two judges to reach by chance de-
pends on the number and relative proportions of the
categories used by the coders. The next section on
Kappa shows that chance agreement is very high in
extractive summarization.
3.1.2 Kappa
Kappa (Siegel and Castellan, 1988) is an evalua-
tion measure which is increasingly used in NLP an-
notation work (Krippendorff, 1980; Carletta, 1996).
Kappa has the following advantages over P and R:
? It factors out random agreement. Random
agreement is defined as the level of agreement
which would be reached by random annotation
using the same distribution of categories as the
real annotators.
? It allows for comparisons between arbitrary
numbers of annotators and items.
? It treats less frequent categories as more im-
portant (in our case: selected sentences), simi-
larly to precision and recall but it also consid-
ers (with a smaller weight) more frequent cate-
gories as well.
The Kappa coefficient controls agreement P (A)
by taking into account agreement by chance P (E) :
K =
P (A)? P (E)
1? P (E)
No matter how many items or annotators, or how
the categories are distributed, K = 0 when there is
no agreement other than what would be expected by
chance, and K = 1 when agreement is perfect. If
two annotators agree less than expected by chance,
Kappa can also be negative.
We report Kappa between three annotators in the
case of human agreement, and between three hu-
mans and a system (i.e. four judges) in the next sec-
tion.
3.1.3 Relative Utility
Relative Utility (RU) (Radev et al, 2000) is tested
on a large corpus for the first time in this project.
RU takes into account chance agreement as a lower
bound and interjudge agreement as an upper bound
of performance. RU allows judges and summarizers
to pick different sentences with similar content in
their summaries without penalizing them for doing
so. Each judge is asked to indicate the importance
of each sentence in a cluster on a scale from 0 to
10. Judges also specify which sentences subsume or
paraphrase each other. In relative utility, the score
of an automatic summary increases with the impor-
tance of the sentences that it includes but goes down
with the inclusion of redundant sentences.
3.2 Content-based Similarity measures
Content-based similarity measures compute the sim-
ilarity between two summaries at a more fine-
grained level than just sentences. For each automatic
extract S and similarity measure M we compute the
following number:
sim(M,S, {J1, J2, J3}) =
M(S, J1) +M(S, J2) +M(S, J3)
3
We used several content-based similarity mea-
sures that take into account different properties of
the text:
Cosine similarity is computed using the follow-
ing formula (Salton, 1988):
cos(X,Y ) =
?
xi ? yi
??
(xi)2 ?
??
(yi)2
where X and Y are text representations based on
the vector space model.
Longest Common Subsequence is computed as
follows:
lcs(X,Y ) = (length(X) + length(Y )? d(X,Y ))/2
where X and Y are representations based on
sequences and where lcs(X,Y ) is the length of
the longest common subsequence between X and
Y , length(X) is the length of the string X , and
d(X,Y ) is the minimum number of deletion and in-
sertions needed to transform X into Y (Crochemore
and Rytter, 1994).
3.3 Relevance Correlation
Relevance correlation (RC) is a new measure for as-
sessing the relative decrease in retrieval performance
when indexing summaries instead of full documents.
The idea behind it is similar to (Sparck-Jones and
Sakai, 2001). In that experiment, Sparck-Jones and
Sakai determine that short summaries are good sub-
stitutes for full documents at the high precision end.
With RC we attempt to rank all documents given a
query.
Suppose that given a queryQ and a corpus of doc-
uments Di, a search engine ranks all documents in
Di according to their relevance to the query Q. If
instead of the corpus Di, the respective summaries
of all documents are substituted for the full docu-
ments and the resulting corpus of summaries Si is
ranked by the same retrieval engine for relevance to
the query, a different ranking will be obtained. If
the summaries are good surrogates for the full docu-
ments, then it can be expected that rankings will be
similar.
There exist several methods for measuring the
similarity of rankings. One such method is Kendall?s
tau and another is Spearman?s rank correlation. Both
methods are quite appropriate for the task that we
want to perform; however, since search engines pro-
duce relevance scores in addition to rankings, we
can use a stronger similarity test, linear correlation
between retrieval scores. When two identical rank-
ings are compared, their correlation is 1. Two com-
pletely independent rankings result in a score of 0
while two rankings that are reverse versions of one
another have a score of -1. Although rank correla-
tion seems to be another valid measure, given the
large number of irrelevant documents per query re-
sulting in a large number of tied ranks, we opted for
linear correlation. Interestingly enough, linear cor-
relation and rank correlation agreed with each other.
Relevance correlation r is defined as the linear
correlation of the relevance scores (x and y) as-
signed by two different IR algorithms on the same
set of documents or by the same IR algorithm on
different data sets:
r =
?
i
(xi ? x)(yi ? y)
??
i
(xi ? x)2
??
i
(yi ? y)2
Here x and y are the means of the relevance scores
for the document sequence.
We preprocess the documents and use Smart to
index and retrieve them. After the retrieval process,
each summary is associated with a score indicating
the relevance of the summary to the query. The
relevance score is actually calculated as the inner
product of the summary vector and the query vec-
tor. Based on the relevance score, we can produce a
full ranking of all the summaries in the corpus.
In contrast to (Brandow et al, 1995) who run 12
Boolean queries on a corpus of 21,000 documents
and compare three types of documents (full docu-
ments, lead extracts, and ANES extracts), we mea-
sure retrieval performance under more than 300 con-
ditions (by language, summary length, retrieval pol-
icy for 8 summarizers or baselines).
4 Results
This section reports results for the summarizers and
baselines described above. We relied directly on the
relevance judgements to create ?manual extracts? to
use as gold standards for evaluating the English sys-
tems. To evaluate Chinese, we made use of a ta-
ble of automatically produced alignments. While
the accuracy of the alignments is quite high, we
have not thoroughly measured the errors produced
when mapping target English summaries into Chi-
nese. This will be done in future work.
4.1 Co-selection results
Co-selection agreement (Section 3.1) is reported in
Figures 4, and 5). The tables assume human perfor-
mance is the upper bound, the next rows compare
the different summarizers.
Figure 4 shows results for precision and recall.
We observe the effect of a dependence of the nu-
merical results on the length of the summary, which
is a well-known fact from information retrieval eval-
uations.
Websumm has an advantage over MEAD for
longer summaries but not for 20% or less. Lead
summaries perform better than all the automatic
summarizers, and better than the human judges.
This result usually occurs when the judges choose
different, but early sentences. Human judgements
overtake the lead baseline for summaries of length
50% or more.
5% 10% 20% 30% 40%
Humans .187 .246 .379 .467 .579
MEAD .160 .231 .351 .420 .519
WEBS .310 .305 .358 .439 .543
LEAD .354 .387 .447 .483 .583
RAND .094 .113 .224 .357 .432
Figure 4: Results in precision=recall (averaged over
20 clusters).
Figure 5 shows results using Kappa. Random
agreement is 0 by definition between a random pro-
cess and a non-random process.
While the results are overall rather low, the num-
bers still show the following trends:
? MEAD outperforms Websumm for all but the
5% target length.
? Lead summaries perform best below 20%,
whereas human agreement is higher after that.
? There is a rather large difference between the
two summarizers and the humans (except for
the 5% case for Websumm). This numerical
difference is relatively higher than for any other
co-selection measure treated here.
? Random is overall the worst performer.
? Agreement improves with summary length.
Figures 6 and 7 summarize the results obtained
through Relative Utility. As the figures indicate,
random performance is quite high although all non-
random methods outperform it significantly. Fur-
ther, and in contrast with other co-selection evalua-
tion criteria, in both the single- and multi-document
5% 10% 20% 30% 40%
Humans .127 .157 .194 .225 .274
MEAD .109 .136 .168 .192 .230
WEBS .138 .128 .146 .159 .192
LEAD .180 .198 .213 .220 .261
RAND .064 .081 .097 .116 .137
Figure 5: Results in kappa, averaged over 20 clus-
ters.
case MEAD outperforms LEAD for shorter sum-
maries (5-30%). The lower bound (R) represents the
average performance of all extracts at the given sum-
mary length while the upper bound (J) is the inter-
judge agreement among the three judges.
5% 10% 20% 30% 40%
R 0.66 0.68 0.71 0.74 0.76
RAND 0.67 0.67 0.71 0.75 0.77
WEBS 0.72 0.73 0.76 0.79 0.82
LEAD 0.72 0.73 0.77 0.80 0.83
MEAD 0.78 0.79 0.79 0.81 0.83
J 0.80 0.81 0.83 0.85 0.87
Figure 6: RU per summarizer and summary length
(Single-document).
5% 10% 20% 30% 40%
R 0.64 0.66 0.69 0.72 0.74
RAND 0.63 0.65 0.71 0.72 0.74
LEAD 0.71 0.71 0.76 0.79 0.82
MEAD 0.73 0.75 0.78 0.79 0.81
J 0.76 0.78 0.81 0.83 0.85
Figure 7: RU per summarizer and summary length
(Multi-document).
4.2 Content-based results
The results obtained for a subset of target lengths
using content-based evaluation can be seen in Fig-
ures 8 and 9. In all our experiments with tf ? idf -
weighted cosine, the lead-based summarizer ob-
tained results close to the judges in most of the target
lengths while MEAD is ranked in second position.
In all our experiments using longest common sub-
sequence, no system obtained better results in the
majority of the cases.
10% 20% 30% 40%
LEAD 0.55 0.65 0.70 0.79
MEAD 0.46 0.61 0.70 0.78
RAND 0.31 0.47 0.60 0.69
WEBS 0.52 0.60 0.68 0.77
Figure 8: Cosine (tf?idf ). Average over 10 clusters.
10% 20% 30% 40%
LEAD 0.47 0.55 0.60 0.70
MEAD 0.37 0.52 0.61 0.70
RAND 0.25 0.38 0.50 0.58
WEBS 0.39 0.45 0.53 0.64
Figure 9: Longest Common Subsequence. Average
over 10 clusters.
The numbers obtained in the evaluation of Chi-
nese summaries for cosine and longest common sub-
sequence can be seen in Figures 10 and 11. Both
measures identify MEAD as the summarizer that
produced results closer to the ideal summaries (these
results also were observed across measures and text
representations).
10% 20% 30% 40%
SUMM 0.44 0.65 0.71 0.78
LEAD 0.54 0.63 0.68 0.77
MEAD 0.49 0.65 0.74 0.82
RAND 0.31 0.50 0.65 0.71
Figure 10: Chinese Summaries. Cosine (tf ? idf ).
Average over 10 clusters. Vector space of Words as
Text Representation.
10% 20% 30% 40%
SUMM 0.32 0.53 0.57 0.65
LEAD 0.42 0.49 0.54 0.64
MEAD 0.35 0.50 0.60 0.70
RAND 0.21 0.35 0.49 0.54
Figure 11: Chinese Summaries. Longest Common
Subsequence. Average over 10 clusters. Chinese
Words as Text Representation.
We have based this evaluation on target sum-
maries produced by LDC assessors, although other
alternatives exist. Content-based similarity mea-
sures do not require the target summary to be a sub-
set of sentences from the source document, thus,
content evaluation based on similarity measures
can be done using summaries published with the
source documents which are in many cases available
(Teufel and Moens, 1997; Saggion, 2000).
4.3 Relevance Correlation results
We present several results using Relevance Correla-
tion. Figures 12 and 13 show how RC changes de-
pending on the summarizer and the language used.
RC is as high as 1.0 when full documents (FD) are
compared to themselves. One can notice that even
random extracts get a relatively high RC score. It is
also worth observing that Chinese summaries score
lower than their corresponding English summaries.
Figure 14 shows the effects of summary length and
summarizers on RC. As one might expect, longer
summaries carry more of the content of the full doc-
ument than shorter ones. At the same time, the rel-
ative performance of the different summarizers re-
mains the same across compression rates.
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.91 0.92 0.93 0.92 0.90 0.903
WEBS 0.88 0.82 0.89 0.91 0.88 0.843
LEAD 0.80 0.80 0.84 0.85 0.81 0.802
RAND 0.80 0.78 0.87 0.85 0.79 0.800
SUMM 0.77 0.79 0.85 0.88 0.81 0.775
Figure 12: RC per summarizer (English 20%).
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.78 0.87 0.93 0.66 0.91 0.850
SUMM 0.76 0.75 0.85 0.84 0.75 0.755
RAND 0.71 0.75 0.85 0.60 0.74 0.744
ALGN 0.74 0.72 0.83 0.95 0.72 0.738
LEAD 0.72 0.71 0.83 0.58 0.75 0.733
Figure 13: RC per summarizer (Chinese, 20%).
5% 10% 20% 30% 40%
FD 1.000 1.000 1.000 1.000 1.000
MEAD 0.724 0.834 0.916 0.946 0.962
WEBS 0.730 0.804 0.876 0.912 0.936
LEAD 0.660 0.730 0.820 0.880 0.906
SUMM 0.622 0.710 0.820 0.848 0.862
RAND 0.554 0.708 0.818 0.884 0.922
Figure 14: RC per summary length and summarizer.
5 Conclusion
This paper describes several contributions to text
summarization:
First, we observed that different measures rank
summaries differently, although most of them
showed that ?intelligent? summarizers outperform
lead-based summaries which is encouraging given
that previous results had cast doubt on the ability of
summarizers to do better than simple baselines.
Second, we found that measures like Kappa, Rel-
ative Utility, Relevance Correlation and Content-
Based, each offer significant advantages over more
simplistic methods like Precision, Recall, and Per-
cent Agreement with respect to scalability, applica-
bility to multidocument summaries, and ability to
include human and chance agreement. Figure 15
Property Prec, recall Kappa Normalized RU Word overlap, cosine, LCS Relevance Correlation
Intrinsic (I)/extrinsic (E) I I I I E
Agreement between human extracts X X X X X
Agreement human extracts and automatic extracts X X X X X
Agreement human abstracts and human extracts X
Non-binary decisions X X
Takes random agreement into account by design X X
Full documents vs. extracts X X
Systems with different sentence segmentation X X
Multidocument extracts X X X X
Full corpus coverage X X
Figure 15: Properties of evaluation measures used in this project.
presents a short comparison of all these evaluation
measures.
Third, we performed extensive experiments using
a new evaluation measure, Relevance Correlation,
which measures how well a summary can be used
to replace a document for retrieval purposes.
Finally, we have packaged the code used for this
project into a summarization evaluation toolkit and
produced what we believe is the largest and most
complete annotated corpus for further research in
text summarization. The corpus and related software
is slated for release by the LDC in mid 2003.
References
Ron Brandow, Karl Mitze, and Lisa F. Rau. 1995. Auto-
matic Condensation of Electronic Publications by Sen-
tence Selection. Information Processing and Manage-
ment, 31(5):675?685.
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic. CL, 22(2):249?254.
Maxime Crochemore and Wojciech Rytter. 1994. Text
Algorithms. Oxford University Press.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Donna Harman and Daniel Marcu, editors. 2001. Pro-
ceedings of the 1st Document Understanding Confer-
ence. New Orleans, LA, September.
Eduard Hovy and Chin Yew Lin. 1999. Automated Text
Summarization in SUMMARIST. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?94. The MIT Press.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications, Bev-
erly Hills, CA.
Inderjeet Mani and Eric Bloedorn. 2000. Summariz-
ing Similarities and Differences Among Related Doc-
uments. Information Retrieval, 1(1).
Inderjeet Mani, The?re`se Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 2001.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. In Natural Language Engineering.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-Based Summarization
of Multiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceedings
of the Workshop on Automatic Summarization at the
6th Applied Natural Language Processing Conference
and the 1st Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Seattle, WA, April.
Horacio Saggion. 2000. Ge?ne?ration automatique
de re?sume?s par analyse se?lective. Ph.D. the-
sis, De?partement d?informatique et de recherche
ope?rationnelle. Faculte? des arts et des sciences. Uni-
versite? de Montre?al, August.
Gerard Salton. 1988. Automatic Text Processing.
Addison-Wesley Publishing Company.
Sidney Siegel and N. John Jr. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Berkeley, CA, 2nd edition.
Karen Sparck-Jones and Tetsuya Sakai. 2001. Generic
Summaries for Indexing in IR. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 190?198, New Orleans, LA, September.
Simone Teufel and Marc Moens. 1997. Sentence Ex-
traction as a Classification Task. In Proceedings of the
Workshop on Intelligent Scalable Text Summarization
at the 35th Meeting of the Association for Computa-
tional Linguistics, and the 8th Conference of the Eu-
ropean Chapter of the Assocation for Computational
Linguistics, Madrid, Spain.
Anastasios Tombros, Mark Sanderson, and Phil Gray.
1998. Advantages of Query Biased Summaries in In-
formation Retrieval. In Eduard Hovy and Dragomir R.
Radev, editors, Proceedings of the AAAI Symposium
on Intelligent Text Summarization, pages 34?43, Stan-
ford, California, USA, March 23?25,. The AAAI
Press.
 
	Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 200?201,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
SUPPLE: A Practical Parser for Natural Language Engineering
Applications
Robert Gaizauskas, Mark Hepple, Horacio Saggion,
Mark A. Greenwood and Kevin Humphreys?
Department of Computer Science
University of Sheffield, Sheffield, UK
{robertg|hepple|saggion|m.greenwood|-}@dcs.shef.ac.uk
Abstract
We describe SUPPLE, a freely-available,
open source natural language parsing sys-
tem, implemented in Prolog, and designed
for practical use in language engineering
(LE) applications. SUPPLE can be run as
a stand-alone application, or as a compo-
nent within the GATE General Architec-
ture for Text Engineering. SUPPLE is dis-
tributed with an example grammar that has
been developed over a number of years
across several LE projects. This paper de-
scribes the key characteristics of the parser
and the distributed grammar.
1 Introduction
In this paper we describe SUPPLE1 ? the Sheffield
University Prolog Parser for Language Engineering
? a general purpose parser that produces both syn-
tactic and semantic representations for input sen-
tences, which is well-suited for a range of LE ap-
plications. SUPPLE is freely available, and is dis-
tributed with an example grammar for English that
was developed across a number of LE projects. We
will describe key characteristics of the parser and the
grammar in turn.
2 The SUPPLE Parser
SUPPLE is a general purpose bottom-up chart parser
for feature-based context free phrase structure gram-
?At Microsoft Corporation since 2000 (Speech and Natural
Language Group). Email: kevinhum@microsoft.com.
1In previous published materials and in the current GATE
release the parser is referred to as buChart. This is name is now
deprecated.
mars (CF-PSGs), written in Prolog, that has a num-
ber of characteristics making it well-suited for use
in LE applications. It is available both as a language
processing resource within the GATE General Ar-
chitecture for Text Engineering (Cunningham et al,
2002) and as a standalone program requiring vari-
ous preprocessing steps to be applied to the input.
We will here list some of its key characteristics.
Firstly, the parser allows multiword units identi-
fied by earlier processing components, e.g. named
entity recognisers (NERs), gazetteers, etc, to be
treated as non-decomposable units for syntactic pro-
cessing. This is important as the identification of
such items is an essential part of analyzing real text
in many domains.
The parser allows a layered parsing process, with
a number of separate grammars being applied in se-
ries, one on top of the other, with a ?best parse? se-
lection process between stages so that only a sub-
set of the constituents constructed at each stage is
passed forward to the next. While this may make
the parsing process incomplete with respect to the
total set of analyses licensed by the grammar rules,
it makes the parsing process much more efficient and
allows a modular development of sub-grammars.
Facilities are provided to simplify handling
feature-based grammars. The grammar representa-
tion uses flat, i.e. non-embedded, feature represen-
tations which are combined used Prolog term uni-
fication for efficiency. Features are predefined and
source grammars compiled into a full form repre-
sentation, allowing grammar writers to include only
relevant features in any rule, and to ignore feature or-
dering. The formalism also permits disjunctive and
optional right-hand-side constituents.
The chart parsing algorithm is simple but very
200
efficient, exploiting the characteristics of Prolog to
avoid the need for active edges or an agenda. In in-
formal testing, this approach was roughly ten times
faster than a related Prolog implementation of stan-
dard bottom-up active chart parsing.
The parser does not fail if full sentential parses
cannot be found, but instead outputs partial anal-
yses as syntactic and semantic fragments for user-
selectable syntactic categories. This makes the
parser robust in applications which deal with large
volumes of real text.
3 The Sample Grammar
The sample grammar distributed with SUPPLE has
been developed over several years, across a number
LE projects. We here list some key characteristics.
The morpho-syntactic and semantic information
required for individual lexical items is minimal ?
inflectional root and word class only, where the word
class inventory is basically the PTB tagset.
A conservative philosophy is adopted regarding
identification of verbal arguments and attachment of
nominal and verbal post-modifiers, such as preposi-
tional phrases and relative clauses. Rather than pro-
ducing all possible analyses or using probabilities to
generate the most likely analysis, the preference is to
offer a single analysis that spans the input sentence
only if it can be relied on to be correct, so that in
many cases only partial analyses are produced. The
philosophy is that it is more useful to produce par-
tial analyses that are correct than full analyses which
may well be wrong or highly disjunctive. Output
from the parser can be passed to further processing
components which may bring additional information
to bear in resolving attachments.
An analysis of verb phrases is adopted in which
a core verb cluster consisting of verbal head plus
auxiliaries and adverbials is identified before any at-
tempt to attach any post-verbal arguments. This con-
trasts with analyses where complements are attached
to the verbal head at a lower level than auxiliaries
and adverbials, e.g. as in the Penn TreeBank. This
decision is again motivated by practical concerns: it
is relatively easy to recognise verbal clusters, much
harder to correctly attach complements.
A semantic analysis, or simplified quasi-logical
form (SQLF), is produced for each phrasal con-
stituent, in which tensed verbs are interpreted as re-
ferring to unique events, and noun phrases as refer-
ring to unique objects. Where relations between syn-
tactic constituents are identified in parsing, semantic
relations between associated objects and events are
asserted in the SQLF.
While linguistically richer grammatical theories
could be implemented in the grammar formalism
of SUPPLE, the emphasis in our work has been on
building robust wide-coverage tools ? hence the re-
quirement for only minimal lexical morphosyntac-
tic and semantic information. As a consequence the
combination of parser and grammars developed to
date results in a tool that, although capable of return-
ing full sentence analyses, more commonly returns
results that include chunks of analysis with some,
but not all, attachment relations determined.
4 Downloading SUPPLE Resources
SUPPLE resources, including source code and the
sample grammar, and also a longer paper providing
a more detailed account of both the parser and gram-
mar, are available from the supple homepage at:
http://nlp.shef.ac.uk/research/supple
5 Conclusion
The SUPPLE parser has served as a component in
numerous LE research projects, and is currently in
use in a Question Answering system which partic-
ipated in recent TREC/QA evaluations. We hope
its availability as a GATE component will facilitate
its broader use by NLP researchers, and by others
building applications exploiting NL technology.
Acknowledgements
The authors would like to acknowledge the sup-
port of the UK EPSRC under grants R91465 and
K25267, and also the contributions of Chris Huyck
and Sam Scott to the parser code and grammars.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. GATE: A framework and graphical devel-
opment environment for robust NLP tools and applica-
tions. Proceedings of the 40th Anniversary Meeting of
the Association for Computational Linguistics, 2002.
201
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 292?295,
Prague, June 2007. c?2007 Association for Computational Linguistics
SHEF: Semantic Tagging and Summarization Techniques Applied to
Cross-document Coreference
Horacio Saggion
Department of Computer Science
University of Sheffield
211 Portobello Street - Sheffield, England, UK, S1 4DP
Tel: +44-114-222-1947
Fax: +44-114-222-1810
saggion@dcs.shef.ac.uk
Abstract
We describe experiments for the cross-
document coreference task in SemEval
2007. Our cross-document coreference sys-
tem uses an in-house agglomerative clus-
tering implementation to group documents
referring to the same entity. Clustering
uses vector representations created by sum-
marization and semantic tagging analysis
components. We present evaluation results
for four system configurations demonstrat-
ing the potential of the applied techniques.
1 Introduction
Cross-document coreference resolution is the task of
identifying if two mentions of the same (or similar)
name in different sources refer to the same individ-
ual. Deciding if two documents refer to the same
individual is a difficult problem because names are
highly ambiguous. Automatic techniques for solv-
ing this problem are required not only for better ac-
cess to information but also in natural language pro-
cessing applications such as multidocument summa-
rization and information extraction. Here, we con-
centrate on the following SemEval 2007 Web Peo-
ple Search Task (Artiles et al, 2007): a search en-
gine user types in a person name as a query. Instead
of ranking web pages, an ideal system should orga-
nize search results in as many clusters as there are
different people sharing the same name in the doc-
uments returned by the search engine. The input is,
therefore, the results given by a web search engine
using a person name as query. The output is a num-
ber of sets, each containing documents referring to
the same individual.
As past and recent research (Bagga and Baldwin,
1998; Phan et al, 2006), we have addressed the
problem as a document clustering problem. For our
first participation in SemEval 2007, we use two ap-
proaches: a lexical or bag-of-words approach and a
semantic based approach. We have implemented our
own clustering algorithms but rely on available ex-
traction and summarization technology developed in
our laboratory to produce document representations
used as input for the clustering procedure.
2 Clustering Algorithm
We have implemented an agglomerative clustering
algorithm. The input to the algorithm is a set of
document representations implemented as vectors of
terms and weights. Initially, there are as many clus-
ters as input documents; as the algorithm proceeds
clusters are merged until a certain termination condi-
tion is reached. The algorithm computes the similar-
ity between vector representations in order to decide
whether or not to merge two clusters. The similar-
ity metric we use is the cosine of the angle between
two vectors. This metric gives value one for identi-
cal vectors and zero for vectors which are orthogo-
nal (non related). Various options have been imple-
mented in order to measure how close two clusters
are, but for the experiments reported here we have
used the following approach: the similarity between
two clusters (sim   ) is equivalent to the ?document?
similarity (sim  ) between the two more similar doc-
uments in the two clusters; the following formula is
used:
292
sim   (C   ,C  ) 
max   	
    sim  (d  ,d )
Where  are clusters,  are document represen-
tations (e.g., vectors), and sim  is the cosine metric.
If this similarity is greater than a threshold ? ex-
perimentally obtained ? the two clusters are merged
together. At each iteration the most similar pair of
clusters is merged. If this similarity is less than a
certain threshold the algorithm stops.
3 Extraction and Summarization
The input for analysis is a set of documents and
a person name (first name and last name). The
documents are analysed by the default GATE1
ANNIE system (Cunningham et al, 2002) and
single document summarization modules (Saggion
and Gaizauskas, 2004b) from our summarization
toolkit2 . No attempt is made to analyse or use con-
textual information given with the input document.
The processing elements include:
 Document tokenisation
 Sentence splitting
 Parts-of-speech tagging
 Named Entity Recognition using a gazetteer
lookup module and regular expressions
 Named entity coreference using an ortho-
graphic name matcher
Named entities of type person, organization, ad-
dress, date, and location are considered relevant
document terms and stored in a special named en-
tity called Mention.
Coreference chains are created and analysed and
if they contain an entity matching the target person?s
surname, all elements of the chain are marked. Ex-
tractive summaries are created for each document,
a sentence belongs to the summary if it contains a
mention which is coreferent with the target entity.
Using language resources creation modules from
the summarization tool, two frequency tables are
1http://gate.ac.uk
2http://www.dcs.shef.ac.uk/?saggion
created for each document set (or person): (i) an in-
verted document frequency table for words (no nor-
malisation is applied); and (ii) an inverted frequency
table for Mentions (the full entity string is used, no
normalisation is applied).
Statistics (term frequencies and tf*idf) are com-
puted over tokens and Mentions using the appropri-
ate tables (these tools are part of the summarization
toolkit) and vector representations created for each
document (same as in (Bagga and Baldwin, 1998)).
Two types of representations were considered for
these experiments: (i) full document or summary
(terms in the summary are considered for vector cre-
ation); and (ii) words or Mentions.
4 System Configurations
Four system configurations were prepared for Se-
mEval:
 System I: vector representations were created
for full documents. Words were used as terms
and local inverted document frequencies used
(word frequencies) for weighting.
 System II: vector representations were created
for full documents. Mentions were used as
terms and local inverted document frequencies
used (Mentions frequencies) for weighting.
 System III: vector representations were created
for person summaries. Words were used as
terms and local inverted document frequencies
used (word frequencies) for weighting.
 System IV: vector representations were created
for person summaries. Mentions were used as
terms and local inverted document frequencies
used (Mentions frequencies) for weighting.
Because only one system configuration was al-
lowed per participant team, we decided to select
System II for official evaluation interested in eval-
uating the effect of semantic information in the clus-
tering process.
5 Parameter Setting and Results
Evaluation of the task was carried out using standard
clustering evaluation measures of ?purity? and ?in-
verse purity? (Hotho et al, 2003), and the harmonic
293
Configuration Purity Inv.Purity F-Score
System I 0.68 0.85 0.74
System II 0.62 0.85 0.68
System III 0.84 0.70 0.74
System IV 0.65 0.75 0.64
Table 1: Results for our configurations omitting one
set. System II was the system we evaluated in Se-
mEval 2007.
mean of purity and inverse purity: F-score. We esti-
mated the threshold for the clustering algorithm us-
ing the ECDL subset of the training data provided
by SemEval. We applied the clustering algorithm to
each document set and computed purity, inverse pu-
rity, and F-score at each iteration of the algorithm,
recording the similarity value of each newly created
cluster. The similarity values for the best clustering
results (best F-score) were recorded, and the max-
imum and minimum values discarded. The rest of
the values were averaged to obtain an estimate of
the optimal threshold. Two different thresholds were
obtained: 0.10 for word vectors and 0.12 for named
entity vectors.
Results for the test set in SemEval are presented
in Table 1 (One set ? ?Jerry Hobbs? ? was ignored
when computing these numbers: due to a failure
during document analysis this set could not be clus-
tered. The error was identified too close to the sub-
mission?s date to allow us to re-process the cluster).
Our official submission System II (SHEF in the offi-
cial results) obtained an F-score of 0.66 positioning
itself in 5th place (out of 16 systems). Our best con-
figuration obtained 0.74 F-score, so a fourth place
would be in theory possible.
Our system obtained an F-score greater than the
average of 0.60 of all participant systems. Our
optimal configurations (System I and System II)
both perform similarly with respect to F-score.
While System I favours ?inverse purity?, System III
favours ?purity?. Results for every individual set are
reported in the Appendix.
6 Conclusions and Future Work
We have presented a system used to participate in
the SemEval 2007 Web People Search task. The
system uses an in-house clustering algorithm and
available extraction and summarization techniques
to produce representations needed by the clustering
algorithm. Although the configuration we submit-
ted was suboptimal, we have obtained good results;
in fact all our system configurations produce results
well above the average of all participants. Our future
work will explore how the use of contextual infor-
mation available on the web can lead to better per-
formance. We will explore if a similar approach to
our method for creating profiles or answering def-
inition questions (Saggion and Gaizauskas, 2004a)
which uses co-occurence information to identify
pieces of information related to a given entity can
be applied here.
Acknowledgements
This work was partially supported by the EU-funded
MUSING project (IST-2004-027097) and the EU-
funded LIRICS project (eContent project 22236).
References
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
SemEval-2007 WePS Evaluation: Establishing a
benchmark for Web People Search Task. In Proceed-
ings of Semeval 2007, Association for Computational
Linguistics.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
the 17th International Conference on Computational
Linguistics (COLING-ACL?98), pages 79?85.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools and
Applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics (ACL?02).
A. Hotho, S. Staab, and G. Stumme. 2003. WordNet im-
proves text document clustering. In Proc. of the SIGIR
2003 Semantic Web Workshop.
X.-H. Phan, L.-M. Nguyen, and S. Horiguchi. 2006.
Personal name resolution crossover documents by a
semantics-based approach. IEICE Trans. Inf. & Syst.,
Feb 2006.
H. Saggion and R. Gaizauskas. 2004a. Mining on-line
sources for definition knowledge. In Proceedings of
the 17th FLAIRS 2004, Miami Bearch, Florida, USA,
May 17-19. AAAI.
294
H. Saggion and R. Gaizauskas. 2004b. Multi-document
summarization by cluster/profile relevance and redun-
dancy removal. In Proceedings of the Document Un-
derstanding Conference 2004. NIST.
Appendix I: Detailed Results
The following tables present purity, inverse purity,
and F-score results for all sets and systems. These
results were computed after re-processing the ?Jerry
Hobbs? missing set.
System I System II
Person Pur. I-Pur. F Pur. I-Pur. F
Alvin Cooper 0.72 0.87 0.79 0.86 0.70 0.77
Arthur Morgan 0.90 0.83 0.86 0.75 0.92 0.83
Chris Brockett 0.87 0.85 0.86 0.94 0.67 0.78
Dekang Lin 1.00 0.63 0.77 1.00 0.66 0.79
Frank Keller 0.68 0.81 0.74 0.65 0.66 0.66
George Foster 0.61 0.83 0.71 0.45 0.88 0.60
Harry Hughes 0.82 0.80 0.81 0.71 0.93 0.80
James Curran 0.76 0.74 0.75 0.53 0.84 0.65
James Davidson 0.74 0.91 0.82 0.59 0.90 0.71
James Hamilton 0.52 0.90 0.66 0.25 0.97 0.39
James Morehead 0.38 0.91 0.54 0.39 0.92 0.55
Jerry Hobbs 0.67 0.86 0.75 0.61 0.85 0.71
John Nelson 0.64 0.93 0.76 0.56 0.90 0.69
Jonathan Brooks 0.70 0.89 0.78 0.54 0.89 0.67
Jude Brown 0.75 0.80 0.78 0.74 0.77 0.75
Karen Peterson 0.60 0.92 0.72 0.19 1.00 0.32
Leon Barrett 0.75 0.84 0.80 0.43 0.96 0.59
Marcy Jackson 0.60 0.91 0.72 0.87 0.85 0.86
Mark Johnson 0.57 0.86 0.68 0.33 0.94 0.49
Martha Edwards 0.49 0.96 0.65 0.43 0.91 0.58
Neil Clark 0.74 0.83 0.78 0.60 0.76 0.67
Patrick Killen 0.83 0.77 0.80 0.82 0.77 0.79
Robert Moore 0.64 0.78 0.71 0.44 0.91 0.60
Sharon Goldwater 1.00 0.80 0.89 1.00 0.80 0.89
Stephan Johnson 0.84 0.87 0.85 0.97 0.69 0.81
Stephen Clark 0.63 0.87 0.73 0.57 0.83 0.67
Thomas Fraser 0.51 0.94 0.66 0.44 0.94 0.60
Thomas Kirk 0.66 0.94 0.78 0.87 0.92 0.90
Violet Howard 0.34 0.96 0.51 0.71 0.90 0.80
William Dickson 0.55 0.94 0.70 0.38 0.95 0.54
AVERAGES 0.68 0.86 0.74 0.62 0.85 0.68
System III System VI
Person Pur. I-Pur. F Pur. I-Pur. F
Alvin Cooper 0.98 0.58 0.73 0.93 0.52 0.67
Arthur Morgan 0.98 0.64 0.78 0.71 0.79 0.75
Chris Brockett 1.00 0.32 0.49 0.95 0.31 0.47
Dekang Lin 1.00 0.40 0.58 1.00 0.34 0.51
Frank Keller 0.85 0.65 0.74 0.50 0.71 0.59
George Foster 0.80 0.80 0.80 0.48 0.86 0.61
Harry Hughes 0.91 0.65 0.76 0.76 0.77 0.77
James Curran 0.92 0.69 0.79 0.64 0.77 0.70
James Davidson 0.82 0.85 0.83 0.48 0.93 0.63
James Hamilton 0.65 0.87 0.74 0.26 0.96 0.41
James Morehead 0.66 0.73 0.70 0.57 0.70 0.63
Jerry Hobbs 0.67 0.82 0.74 0.63 0.86 0.73
John Nelson 0.80 0.78 0.79 0.52 0.92 0.66
Jonathan Brooks 0.84 0.85 0.85 0.55 0.86 0.67
Jude Brown 0.75 0.72 0.74 0.80 0.69 0.74
Karen Peterson 0.80 0.86 0.83 0.26 0.94 0.41
Leon Barrett 0.91 0.52 0.66 0.79 0.62 0.69
Marcy Jackson 0.95 0.58 0.72 0.98 0.57 0.72
Mark Johnson 0.76 0.84 0.80 0.44 0.90 0.60
Martha Edwards 0.78 0.85 0.81 0.57 0.87 0.69
Neil Clark 0.85 0.53 0.65 0.60 0.75 0.67
Patrick Killen 0.99 0.57 0.73 0.90 0.61 0.73
Robert Moore 0.74 0.67 0.71 0.49 0.85 0.62
Sharon Goldwater 1.00 0.15 0.26 1.00 0.23 0.37
Stephan Johnson 0.94 0.71 0.81 0.95 0.71 0.81
Stephen Clark 0.87 0.80 0.83 0.55 0.82 0.66
Thomas Fraser 0.62 0.89 0.73 0.47 0.92 0.62
Thomas Kirk 0.81 0.87 0.84 0.84 0.86 0.85
Violet Howard 0.89 0.78 0.83 0.87 0.75 0.81
William Dickson 0.68 0.88 0.77 0.52 0.88 0.66
AVERAGES 0.84 0.70 0.73 0.67 0.74 0.65
295
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31?38,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
A Classification Algorithm for Predicting the Structure of Summaries
Horacio Saggion
University of Sheffield
211 Portobello Street
Sheffield - S1 4DP
United Kingdom
http://www.dcs.shef.ac.uk/
~
saggion
H.Saggion@dcs.shef.ac.uk
Abstract
We investigate the problem of generating
the structure of short domain independent
abstracts. We apply a supervised machine
learning approach trained over a set of ab-
stracts collected from abstracting services
and automatically annotated with a text
analysis tool. We design a set of features
for learning inspired from past research
in content selection, information order-
ing, and rhetorical analysis for training
an algorithm which then predicts the dis-
course structure of unseen abstracts. The
proposed approach to the problem which
combines local and contextual features is
able to predict the local structure of the ab-
stracts in just over 60% of the cases.
1 Introduction
Mani (2001) defines an abstract as ?a summary
at least some of whose material is not present in
the input?. In a study of professional abstracting,
Endres-Niggemeyer (2000) concluded that profes-
sional abstractors produce abstracts by ?cut-and-
paste? operations, and that standard sentence pat-
terns are used in their production. Examples of
abstracts produced by a professional abstractor are
shown in Figures 1 and 2. They contain fragments
?copied? from the input documents together with
phrases (underlined in the figures) inserted by the
professional abstractors. In a recent study in hu-
man abstracting (restricted to the amendment of
authors abstracts) Montesi and Owen (2007) noted
that professional abstractors prepend third person
singular verbs in present tense and without subject
to the author abstract, a phenomenon related ? yet
different ? from the problem we are investigating
in this paper.
Note that the phrases or predicates prepended to
the selected sentence fragments copied from the
input document have a communicative function:
Presents a model instructional session that was prepared and
taught by librarians to introduce college students, faculty,
and staff to the Internet by teaching them how to join list-
servs and topic- centered discussion groups. Describes the
sessions? audience, learning objectives, facility, and course
design. Presents a checklist for preparing an Internet instruc-
tion session.
Figure 1: Professional Abstracts with Inserted
Predicates from LISA Abstracting Service
Talks about Newsblaster, an experimental software tool that
scans and summarizes electronic news stories, developed by
Columbia University?s Natural Language Processing Group.
Reports that Newsblaster is a cross between a search en-
gine and ... Explains that Newsblaster publishes the sum-
maries in a Web page that divides the day summaries into ....
Mentions that Newsblaster is considered an aid to those who
have to quickly canvas large amounts of information from
many sources.
Figure 2: Professional Abstract with Inserted
Predicates from Internet & Personal Computing
Abstracts
they inform or alert the reader about the content
of the abstracted document by explicitly mark-
ing what the author says or mentions, presents or
introduces, concludes, or includes, in her paper.
Montesi and Owen (2007) observe that the revi-
sion of abstracts is carried out to improve com-
prehensibility and style and to make the abstract
objective.
We investigate how to create the discourse
structure of the abstracts: more specifically we are
interested in predicting the inserted predicates or
phrases and at which positions in the abstract they
should be prepended.
Abstractive techniques in text summarization
include sentence compression (Cohn and Lapata,
2008), headline generation (Soricut and Marcu,
2007), and canned-based generation (Oakes and
Paice, 2001). Close to the problem studied here
is Jing and McKeown?s (Jing and McKeown,
2000) cut-and-paste method founded on Endres-
Niggemeyer?s observations. The cut-and-paste
31
method includes such operations as sentence trun-
cation, aggregation, specialization/generalization,
reference adjustment and rewording. None of
these operations account for the transformations
observed in the abstracts of Figures 1 and 2. The
formulaic expressions or predicates inserted in the
abstract ?glue? together the extracted fragments,
thus creating the abstract?s discourse structure.
To the best of our knowledge, and with the ex-
ception of Saggion and Lapalme (2002) indicative
generation approach which included operations to
add extra linguistic material to generate an indica-
tive abstract, the work presented here is the first
to investigate this relevant operation in the field of
text abstracting and to propose a robust computa-
tional method for its simulation.
In this paper we are interested in the process
of generating the structure of the abstract by au-
tomatic means. In order to study this problem, we
have collected a corpus of abstracts written by ab-
stractors; we have designed an algorithm for pre-
dicting the structure; implemented the algorithm;
and evaluated the structure predicted by the auto-
matic system against the true structure.
2 Problem Specification, Data Collection,
and Annotation
The abstracts we study in this research follow the
pattern:
Abstract?
?
n
i=1
Pred
i
??
i
where Pred
i
is a phrase used to introduce the ?con-
tent? ?
i
of sentence i, n is the number of sentences
in the abstract,
?
indicates multiple concatena-
tion, and X ?Y indicates the concatenation of X
and Y . In this paper we concentrate only on this
?linear? structure, we plan to study more complex
(e.g., tree-like representations) in future work.
The problem we are interested in solving is the
following: given sentence fragments ?
i
extracted
from the document, how to create the Abstract.
Note that if N is the number of different phrases
(Pred
i
) used in the model, then a priori there are
N
n
possible discourse structures to select from for
the abstract, generating all possibilities and select-
ing the most appropriate would be impractical. We
present an algorithm that decides which predicate
or phrase is most suitable for each sentence, do-
ing this by considering the sentence content and
the abstract generated so far. For the experiments
to be reported in this paper, the discourse structure
of the abstracts is created using predicates or ex-
pressions learned from a corpus a subset of which
is shown in Table 1.
We have collected abstracts from various
databases including LISA, ERIC, and Internet
& Personal Computing Abstracts, using our in-
stitutional library?s facilities and the abstracts?
providers? keyword search facilities. Electronic
copies of the abstracted documents can also be
accessed through our institution following a link,
thus allowing us to check abstracts against ab-
stracted document (additional information on the
abstracts is given in the Appendix).
2.1 Document Processing and Annotation
Each electronic version of the abstract was pro-
cessed using the freely available GATE text analy-
sis software (Cunningham et al, 2002). First each
abstract was analyzed by a text structure analy-
sis program to identify meta-data such as title, au-
thor, source document, the text of the abstract, etc.
Each sentence in the abstract was stripped from
the predicate or phrase inserted by the abstractor
(e.g., ?Mentions that?, ?Concludes with?) and a
normalised version of the expression was used to
annotate the sentence, in a way similar to the ab-
stracts in Figures 1 and 2. After this each abstract
and document title was tokenised, sentence split-
ted, part-of-speech tagged, and morphologically
analyzed. A rule-based system was used to carry
out partial, robust syntactic and semantic analy-
sis of the abstracts (Gaizauskas et al, 2005) pro-
ducing predicate-argument representations where
predicates which are used to represent entities are
created from the morphological roots of nouns or
verbs in the text (unary predicates) and predicates
with are used to represent binary relations are a
closed set of names representing grammatical re-
lations such as the verb logical object, or the verb
logical subject or a prepositional attachment, etc.
This predicate-argument structure representation
was further analysed in order to extract ?seman-
tic? triples which are used in the experiments re-
ported here. Output of this analysis is shown in
Figure 3. Note that the representation also con-
tains the tokens of the text, their parts of speech,
lemmas, noun phrases, verb phrases, etc.
3 Proposed Solution
Our algorithm (see Algorithm 1) takes as in-
put an ordered list of sentence fragments obtained
from the source document and decides how to
?paste? the fragments together into an abstract;
32
to address; to add; to advise; to assert; to claim; to comment; to compare; to conclude; to define; to
describe; to discuss; to evaluate; to examine; to explain; to focus; to give; to highlight; to include;
to indicate; to note; to observe; to overview; to point out; to present; to recommend; to report; to
say; to show; to suggest; ...
to report + to indicate + to note + to declare + to include; to provide + to explain + to indicate +
to mention; to point out + to report + to mention + to include; to discuss + to list + to suggest +
to conclude; to present + to say + to add + to conclude + to contain; to discuss + to explain + to
recommend; to discuss + to cite + to say; ...
Table 1: Subset of predicates or expressions used by professional abstractors and some of the discourse
structures used.
Sentence: Features a listing of ten family-oriented pro-
grams, including vendor information, registration fees, and
a short review of each.
Representation: listing-det-a; listing-of-program; family-
oriented-adj-program; fee-qual-registration; information-
qual-vendor; listing-apposed-information; ...
Figure 3: Sentence Representation (partial)
Algorithm 1 Discourse Structure Prediction Al-
gorithm
Given: a list of n sorted text fragments ?
i
begin
Abstract? ??;
Context? START;
for all i : 0? i? n?1; do
Pred? PredictPredicate(Context,?
i
);
Abstract? Abstract?Pred??
i
? ?.?;
Context? ExtractContext(Abstract);
end for
return Abstract
end
at each iteration the algorithm selects the ?best?
available phrase or predicate to prepend to the cur-
rent fragment from a finite vocabulary (induced
from the analysed corpus) based on local and
contextual information. One could rely on ex-
isting trainable sentence selection (Kupiec et al,
1995) or even phrase selection (Banko et al, 2000)
strategies to pick up appropriate ?
i
?s from the doc-
ument to be abstracted and rely on recent informa-
tion ordering techniques to sort the ?
i
fragments
(Lapata, 2003). This is the reason why we only ad-
dress here the discourse structure generation prob-
lem.
3.1 Predicting Discourse Structure as
Classification
There are various possible ways of predicting what
expression to insert at each point in the genera-
tion process (i.e., the PredictPredicate function
in Algorithm 1). In the experiments reported here
we use a classification algorithm based on lexical,
syntactic, and discursive features, which decides
which of the N possible available phrases is most
suitable. The algorithm is trained over the anno-
tated abstracts and used to predict the structure of
unseen test abstracts.
Where the classification algorithm is concerned,
we have decided to use Support Vector Machines
which have recently been used in different tasks
in natural language processing, they have been
shown particularly suitable for text categorization
(Joachims, 1998). We have tried other machine
learning algorithms such as Decision Trees, Naive
Bayes Classification, and Nearest Neighbor from
the Weka toolkit (Witten and Frank, 1999), but the
support vector machines gave us the best classifi-
cation accuracy (a comparison with Naive Bayes
will be presented in Section 4).
The features used for the experiments reported
here are inspired by previous work in text summa-
rization on content selection (Kupiec et al, 1995),
rhetorical classification (Teufel and Moens, 2002),
and information ordering (Lapata, 2003). The
features are extracted from the analyzed abstracts
with specialized programs. In particular we use
positional features (position of the predicate to be
generated in the structure), length features (num-
ber of words in the sentence), title features (e.g.,
presence of title words in sentence), content fea-
tures computed as the syntactic head of noun and
verb phrases, semantic features computed as the
33
to add; to conclude; to contain; to describe; to
discuss; to explain; to feature; to include; to indi-
cate; to mention; to note; to point out; to present;
to provide; to report; to say
Table 2: Predicates in the reduced corpus
arguments of ?semantic? triples (Section 2.1) ex-
tracted from the parsed abstracts. Features occur-
ring less than 4 times in the corpus were removed
for the experiments. For each sentence, a cohe-
sion feature is also computed as the number of
nouns in common with the previous sentence frag-
ment (or title if first sentence). Cohesion infor-
mation has been used in rhetorical-based parsing
for summarization (Marcu, 1997) in order to de-
cide between ?list? or ?elaboration? relations and
also in content selection for summarization (Barzi-
lay and Elhadad, 1997). For some experiments
we also use word-level information (lemmas) and
part-of-speech tags. For some of the experiments
reported here the variable Context at iteration i in
Algorithm 1 is instantiated with the predicates pre-
dicted at iterations i?1 and i?2.
4 Experiments and Results
The experiments reported here correspond to the
use of different features as input for the classifier.
In these experiments we have used a subset of the
collected abstracts, they contain predicates which
appeared at least 5 times in the corpus. With this
restriction in place the original set of predicates
used to create the discourse structure is reduced to
sixteen (See Table 2), however, the number of pos-
sible structures in the reduced corpus is still con-
siderable with a total of 179 different structures.
In the experiments we compare several classi-
fiers:
? Random Generation selects a predicate at
random at each iteration of the algorithm;
? Predicate-based Generation is a SVM classi-
fier which uses the two previous predicates to
generate the current predicate ignoring sen-
tence content;
? Position-based Generation is a SVM classi-
fier which also ignores sentence content but
uses as features for classification the absolute
position of the sentence to be generated;
Configuration Avg.Acc
Random Generation 10%
Predicate-based Generation 35%
Position-based Generation 38%
tf*idf-based Generation 55%
Summarization-based Generation 60%
Table 3: Average accuracy of different classifica-
tion configurations.
? tf*idf-based Generation is a SVM classifier
which uses lemmas of the sentence fragment
to be generated to pick up one predicate (note
that position features and predicates were
added to the mix without improving the clas-
sifier);
? Summarization-based Generation is a SVM
which uses the summarization and discourse
features discussed in the previous section in-
cluding contextual information (Pred
i?2
and
Pred
i?1
? with special values when i = 0 and
i = 1).
We measure the performance of each instance
of the algorithm by comparing the predicted struc-
ture against the true structure. We compute two
metrics: (i) accuracy at the sentence level (as in
classification), which is the proportion of predi-
cates which were correctly generated; and (ii) ac-
curacy at the textual level, which is the proportion
of abstracts correctly generated. For the latter we
compute the proportion of abstracts with zero er-
rors, less than two errors, and less than three er-
rors.
For every instance of the algorithm we perform
a cross-validation experiment, selecting for each
experiment 20 abstracts for testing and the rest of
the abstracts for training. Accuracy measures at
sentence and text levels are averages of the cross-
validation experiments.
Results of the algorithms are presented in Ta-
bles 3 and 4. Random generation has very poor
performance with only 10% local accuracy and
less than 1% of full correct structures. Knowledge
of the predicates selected for previous sentences
improves performance over the random system
(35% local accuracy and 5% of full correct struc-
tures predicted). As in previous summarization
studies, position proved to contribute to the task:
the positional classifier predicts individual predi-
cates with a 38% accuracy; however only 8% of
34
the structures are recalled. Differences between
the accuracies of the two algorithms (predicate-
based and position-based) are significant at 95%
confidence level (a t-test was used). As it is usu-
ally the case in text classification experiments,
the use of word level information (lemmas in our
case) achieves good performance: 55% classifica-
tion accuracy at sentence level, and 18% of full
structures correctly predicted. The use of lex-
ical (noun and verb heads, arguments), syntac-
tic (parts of speech information), and discourse
(predicted predicates, position, cohesion) features
has the better performance with 60% classifica-
tion accuracy at sentence level predicting 21%
of all structures with 73% of the structures con-
taining less than 3 errors. The differences in
accuracy between the word-based classifier and
the summarization-based classifier are statistically
significant at 95% confidence level (a t-test was
used). A Naive Bayes classifier which uses the
summarization features achieves 50% classifica-
tion accuracy.
Conf. 0 errs < 2 errrs < 3 errs
Random 0.3% 4% 20%
Predicate-based 5% 24% 48%
Position-based 8% 33% 50%
tf*idf-based 18% 42% 67%
Summ-based 21% 55% 73%
Table 4: Percent of correct and partially correct
structures predicted. Averaged over all runs.
Table 5 shows a partial confusion table for pred-
icates ?to add?, ?to conclude?, ?to explain?, and
?to present? while and Table 6 reports individual
classification accuracy. All these results are based
on averages of the summarization-based classifier.
5 Discussion
We have presented here a problem which has not
been investigated before in the field of text sum-
marization: the addition of extra linguistic mate-
rial (i.e., not present in the source document) to the
abstract ?informational content? in order to create
the structure of the abstract. We have proposed an
algorithm which uses a classification component
at each iteration to predict predicates or phrases to
be prepended to fragments extracted from a doc-
ument. We have shown that this classifier based
on summarization features including linguistic, se-
mantic, positional, cohesive, and discursive infor-
mation can predict the local discourse structures in
over 60% of the cases. There is a mixed picture on
the prediction of individual predicates, with most
predicates correctly classified in most of the cases
except for predicates such as ?to describe?, ?to
note?, and ?to report? which are confused with
other phrases. Predicates such as ?to present? and
?to include? have the tendency of appearing to-
wards the very beginning or the very end of the ab-
stract been therefore predicted by position-based
features (Edmundson, 1969; Lin and Hovy, 1997).
Note that in this work we have decided to evaluate
the predicted structure against the true structure (a
hard evaluation measure), in future work we will
assess the abstracts with a set of quality questions
similar to those put forward by the Document Un-
derstanding Conference Evaluations (also in a way
similar to (Kan and McKeown, 2002) who eval-
uated their abstracts in a retrieval environment).
We expect to obtain a reasonable evaluation result
given that it appears that some of the predicates or
phrases are ?interchangeable? (e.g., ?to contain?
and ?to include?).
Actual Pred. Predicted Pred. Conf.Freq.
to add to add 32%
to explain 16%
to say 10%
to conclude to conclude 35%
to say 29%
to add 7%
to explain to explain 35%
to say 15%
to add 11%
to present to present 86%
to discuss 7%
to provide 1%
Table 5: Classification Confusion Table for a Sub-
set of Predicates in the Corpus (Average Fre-
quency).
6 Related Work
Liddy (1991) produced a formal model of the in-
formational or conceptual structure of abstracts
of empirical research. This structure was elicited
from abstractors of two organizations ERIC and
PsycINFO through a series of tasks. Lexical clues
which predict the components of the structure
were latter induced by corpus analysis. In the do-
main of indicative summarization, Kan and McK-
35
Predicate Avg. Accuracy
to add 31.40
to conclude 34.78
to contain 10.96
to describe 15.69
to discuss 54.55
to explain 35.63
to feature 34.38
to include 85.86
to indicate 20.69
to mention 26.47
to note 6.78
to point out 91.67
to present 86.19
to provide 40.94
to report 1.59
to say 75.86
Table 6: Predicate Classification Accuracy
eown (2002) studied the problem of generating ab-
stracts for bibliographical data which although in a
restricted domain has some contact points with the
work described here. As in their work we use the
abstracts in our corpus to induce the model. They
rely on a more or less fixed discourse structure to
accommodate the generation process. In our ap-
proach the discourse structure is not fixed but pre-
dicted for each particular abstract. Related to our
classification experiments is work on semantic or
rhetorical classification of ?structured? abstracts
(Saggion, 2008) from the MEDLINE abstracting
database where similar features to those presented
here were used to identify in abstracts semantic
categories such as objective, method, results, and
conclusions. Related to this is the work by Teufel
and Moens (2002) on rhetorical classification for
content selection. In cut-and-paste summarization
(Jing and McKeown, 2000), sentence combina-
tion operations were implemented manually fol-
lowing the study of a set of professionally written
abstracts; however the particular ?pasting? oper-
ation presented here was not implemented. Pre-
vious studies on text-to-text abstracting (Banko et
al., 2000; Knight and Marcu, 2000) have studied
problems such as sentence compression and sen-
tence combination but not the ?pasting? procedure
presented here. The insertion in the abstract of
linguistic material not present in the input docu-
ment has been addressed in paraphrase generation
(Barzilay and Lee, 2004) and canned-based sum-
marization (Oakes and Paice, 2001) in limited do-
mains. Saggion and Lapalme (2002) have studied
and implemented a rule-based ?verb selection? op-
eration in their SumUM system which has been
applied to introduce document topics during in-
dicative summary generation.
Our discourse structure generation procedure is
in principle generic but depends on the availability
of a corpus for training.
7 Conclusions
In text summarization research, most attention
has been paid to the problem of what information
to select for a summary. Here, we have focused
on the problem of how to combine the selected
content with extra linguistic information in order
to create the structure of the summary.
There are several contributions of this work:
? First, we have presented the problem of gen-
erating the discourse structures of an abstract
and proposed a meta algorithm for predicting
it. This problem has not been investigated be-
fore.
? Second, we have proposed ? based on pre-
vious summarization research ? a number of
features to be used for solving this problem;
and
? Finally, we have propose several instantia-
tions of the algorithm to solve the problem
and achieved a reasonable accuracy using the
designed features;
There is however much space for improvement
even though the algorithm recalls some ?partial
structures?, many ?full structures? can not be gen-
erated. We are currently investigating the use
of induced rules to address the problem and will
compare a rule-based approach with our classi-
fier. Less superficial cohesion features are being
investigated and will be tested in this classification
framework.
Acknowledgements
We would like to thank three anonymous review-
ers for their suggestions and comments. We thank
Adam Funk who helped us improve the quality of
our paper. Part of this research was carried out
while the author was working for the EU-funded
MUSING project (IST-2004-027097).
36
References
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In ACL ?00: Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 318?325, Morristown, NJ, USA.
Association for Computational Linguistics.
Regina Barzilay and Michael Elhadad. 1997. Using
Lexical Chains for Text Summarization. In Proceed-
ings of the ACL/EACL?97 Workshop on Intelligent
Scalable Text Summarization, pages 10?17, Madrid,
Spain, July.
R. Barzilay and L. Lee. 2004. Catching the Drift:
Probabilistic Content Models, with Applications to
Generation and Summarization. In Proceedings of
HLT-NAACL 2004.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proceedings of COLING
2008, Manchester.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graph-
ical development environment for robust NLP tools
and applications. In ACL 2002.
H.P. Edmundson. 1969. New Methods in Automatic
Extracting. Journal of the Association for Comput-
ing Machinery, 16(2):264?285, April.
Brigitte Endres-Niggemeyer. 2000. SimSum: an em-
pirically founded simulation of summarizing. Infor-
mation Processing & Management, 36:659?682.
R. Gaizauskas, M. Hepple, H. Saggion, and M. Green-
wood. 2005. SUPPLE: A Practical Parser for Natu-
ral Language Engineering Applications.
Hongyan Jing and Kathleen McKeown. 2000. Cut
and Paste Based Text Summarization. In Proceed-
ings of the 1st Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 178?185, Seattle, Washington, USA, April 29
- May 4.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learn-
ing (ECML), pages 137?142, Berlin. Springer.
Min-Yen Kan and Kathleen R.. McKeown. 2002.
Corpus-trained text generation for summarization.
In Proceedings of the Second International Natu-
ral Language Generation Conference (INLG 2002),
pages 1?8, Harriman, New York, USA.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the 17th National Confer-
ence of the American Association for Artificial In-
telligence. AAAI, July 30 - August 3.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A Trainable Document Summarizer. In Proc. of the
18th ACM-SIGIR Conference, pages 68?73, Seattle,
Washington, United States.
M. Lapata. 2003. Probabilistic Text Structuring: Ex-
periments with Sentence Ordering. In Proceedings
of the 41st Meeting of the Association of Computa-
tional Linguistics, pages 545?552, Sapporo, Japan.
Elizabeth D. Liddy. 1991. The Discourse-Level Struc-
ture of Empirical Abstracts: An Exploratory Study.
Information Processing & Management, 27(1):55?
81.
C. Lin and E. Hovy. 1997. Identifying Topics by Po-
sition. In Fifth Conference on Applied Natural Lan-
guage Processing, pages 283?290. Association for
Computational Linguistics, 31 March-3 April.
Inderjeet Mani. 2001. Automatic Text Summarization.
John Benjamins Publishing Company.
D. Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, Department of Computer Science, Uni-
versity of Toronto.
M. Montesi and J. M. Owen. 2007. Revision of au-
thor abstracts: how it is carried out by LISA editors.
Aslib Proceedings, 59(1):26?45.
M.P. Oakes and C.D. Paice. 2001. Term extrac-
tion for automatic abstracting. In D. Bourigault,
C. Jacquemin, and M-C. L?Homme, editors, Recent
Advances in Computational Terminology, volume 2
of Natural Language Processing, chapter 17, pages
353?370. John Benjamins Publishing Company.
H. Saggion and G. Lapalme. 2002. Generating
Indicative-Informative Summaries with SumUM.
Computational Linguistics, 28(4):497?526.
H. Saggion. 2008. Automatic Summarization: An
Overview. Revue Franc?aise de Linguistique Ap-
pliqu?ee , XIII(1), Juin.
R. Soricut and D. Marcu. 2007. Abstractive headline
generation using WIDL-expressions. Inf. Process.
Manage., 43(6):1536?1548.
S. Teufel and M. Moens. 2002. Summarizing
Scientific Articles: Experiments with Relevance
and Rhetorical Status. Computational Linguistics,
28(4):409?445.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, Oc-
tober.
Appendix I: Corpus Statistics and
Examples
The corpus of abstracts following the specification
given in Section 2 contains 693 abstracts, 10,423
37
sentences, and 305,105 tokens. The reduced
corpus used for the experiments contains 300
abstracts.
Examples
Here we list one example of the use of each of the
predicates in the reduced set of 300 abstracts used
for the experiments.
Adds that it uses search commands and
features that are similar to those of
traditional online commercial database
services, has the ability to do nested
Boolean queries as well as truncation
when needed, and provides detailed doc-
umentation that offers plenty of exam-
ples.
Concludes CNET is a network of sites,
each dealing with a specialized aspect of
computers that are accessible from the
home page and elsewhere around the site.
Contains a step-by-step guide to using
PGP.
Describes smallbizNet, the LEXIS-
NEXIS Small Business Service, Small
Business Administration, Small Business
Advancement National Center, and other
small business-related sites.
Discusses connections and links between
differing electronic mail systems.
Explains DataStar was one of the first on-
line hosts to offer a Web interface, and
was upgraded in 1997.
Features tables showing the number of
relevant, non-relevant, and use retrievals
on both LEXIS and WIN for federal and
for state court queries.
Includes an electronic organizer, an er-
gonomically correct keyboard, an on-
line idle-disconnect, a video capture de-
vice, a color photo scanner, a real-time
Web audio player, laptop speakers, a
personal information manager (PIM), a
mouse with built-in scrolling, and a voice
fax-modem.
Indicates that the University of Califor-
nia, Berkeley, has the School of Informa-
tion Management and Systems, the Uni-
versity of Washington has the Informa-
tion School, and the University of Mary-
land has the College of Information Stud-
ies.
Mentions that overall, the interface is
effective because the menus and sear
screens permit very precise searches with
no knowledge of searching or Dialog
databases.
Notes that Magazine Index was origi-
nally offered on Lyle Priest?s invention,
a unique microfilm reader.
Points out the strong competition that the
Internet has created for the traditional on-
line information services, and the move
of these services to the Internet.
Presents searching tips and techniques.
Provides summaries of African art; Allen
Memorial Art Museum of Oberlin Col-
lege; Art crimes; Asian arts; Da Vinci,
Leonardo; Gallery Walk; and Native
American Art Gallery.
Reports that Dialog has announced ma-
jor enhancements to its alerting system on
the DialogClassic, DialogClassic Web,
and DialogWeb services.
Says that dads from all over the country
share advice on raising children, educa-
tional resources, kids? software, and other
related topics using their favorite online
service provider.
38
Coling 2010: Poster Volume, pages 1059?1067,
Beijing, August 2010
Multilingual Summarization Evaluation without Human Models
Horacio Saggion
TALN - DTIC
Universitat Pompeu Fabra
horacio.saggion@upf.edu
Juan-Manuel Torres-Moreno
LIA/Universite? d?Avignon
E?cole Polytechnique de Montre?al
juan-manuel.torres@univ-avignon.fr
Iria da Cunha
IULA/Universitat Pompeu Fabra
LIA/Universite? d?Avignon
iria.dacunha@upf.edu
Eric SanJuan
LIA/Universite? d?Avignon
eric.sanjuan@univ-avignon.fr
Patricia Vela?zquez-Morales
VM Labs
patricia vazquez@yahoo.com
Abstract
We study correlation of rankings of text
summarization systems using evaluation
methods with and without human mod-
els. We apply our comparison frame-
work to various well-established content-
based evaluation measures in text sum-
marization such as coverage, Responsive-
ness, Pyramids and ROUGE studying their
associations in various text summarization
tasks including generic and focus-based
multi-document summarization in English
and generic single-document summariza-
tion in French and Spanish. The research
is carried out using a new content-based
evaluation framework called FRESA to
compute a variety of divergences among
probability distributions.
1 Introduction
Text summarization evaluation has always been a
complex and controversial issue in computational
linguistics. In the last decade, significant ad-
vances have been made in the summarization eval-
uation field. Various evaluation frameworks have
been established and evaluation measures devel-
oped. SUMMAC (Mani et al, 2002), in 1998,
provided the first system independent framework
for summary evaluation; the Document Under-
standing Conference (DUC) (Over et al, 2007)
was the main evaluation forum from 2000 until
2007; nowadays, the Text Analysis Conference
(TAC)1 provides a forum for assessment of dif-
ferent information access technologies including
text summarization.
Evaluation in text summarization can be extrin-
sic or intrinsic (Spa?rck-Jones and Galliers, 1996).
In an extrinsic evaluation, the summaries are as-
sessed in the context of an specific task a human
or machine has to carry out; in an intrinsic eval-
uation, the summaries are evaluated in reference
to some ideal model. SUMMAC was mainly ex-
trinsic while DUC and TAC followed an intrinsic
evaluation paradigm. In order to intrinsically eval-
uate summaries, the automatic summary (peer)
has to be compared to a model summary or sum-
maries. DUC used an interface called SEE to al-
low human judges compare a peer summary to a
model summary. Using SEE, human judges give a
coverage score to the peer summary representing
the degree of overlap with the model summary.
Summarization systems obtain a final coverage
score which is the average of the coverage?s scores
associated to their summaries. The system?s cov-
erage score can then be used to rank summariza-
tion systems. In the case of query-focused sum-
marization (e.g. when the summary has to re-
spond to a question or set of questions) a Respon-
siveness score is also assigned to each summary
which indicates how responsive the summary is to
the question(s).
Because manual comparison of peer summaries
with model summaries is an arduous and costly
1http://www.nist.gov/tac
1059
process, a body of research has been produced in
the last decade on automatic content-based eval-
uation procedures. Early studies used text simi-
larity measures such as cosine similarity (with or
without weighting schema) to compare peer and
model summaries (Donaway et al, 2000), vari-
ous vocabulary overlap measures such as set of
n-grams overlap or longest common subsequence
between peer and model have also been pro-
posed (Saggion et al, 2002; Radev et al, 2003).
The Bleu machine translation evaluation measure
(Papineni et al, 2002) has also been tested in
summarization (Pastra and Saggion, 2003). The
DUC conferences adopted the ROUGE package
for content-based evaluation (Lin, 2004). It im-
plements a series of recall measures based on n-
gram co-occurrence statistics between a peer sum-
mary and a set of model summaries. ROUGE mea-
sures can be used to produce systems ranks. It
has been shown that system rankings produced
by some ROUGE measures (e.g., ROUGE-2 which
uses bi-grams) correlate with rankings produced
using coverage. In recent years the Pyramids eval-
uation method (Nenkova and Passonneau, 2004)
was introduced. It is based on the distribution
of ?content? in a set of model summaries. Sum-
mary Content Units (SCUs) are first identified in
the model summaries, then each SCU receives
a weight which is the number of models con-
taining or expressing the same unit. Peer SCUs
are identified in the peer, matched against model
SCUs, and weighted accordingly. The Pyramids
score given to the peer is the ratio of the sum
of the weights of its units and the sum of the
weights of the best possible ideal summary with
the same number of SCUs as the peer. The Pyra-
mids scores can be used for ranking summariza-
tion systems. Nenkova and Passonneau (2004)
showed that Pyramids scores produced reliable
system rankings when multiple (4 or more) mod-
els were used and that Pyramids rankings cor-
relate with rankings produced by ROUGE-2 and
ROUGE-SU2 (i.e. ROUGE with skip bi-grams).
Still this method requires the creation of models
and the identification, matching, and weighting of
SCUs in both models and peers.
Donaway et al (2000) put forward the idea of
using directly the full document for comparison
purposes, and argued that content-based measures
which compare the document to the summary may
be acceptable substitutes for those using model
summaries. A method for evaluation of sum-
marization systems without models has been re-
cently proposed (Louis and Nenkova, 2009). It is
based on the direct content-based comparison be-
tween summaries and their corresponding source
documents. Louis and Nenkova (2009) evalu-
ated the effectiveness of the Jensen-Shannon (Lin,
1991b) theoretic measure in predicting systems
ranks in two summarization tasks query-focused
and update summarization. They have shown that
ranks produced by Pyramids and ranks produced
by the Jensen-Shannon measure correlate. How-
ever, they did not investigate the effect of the mea-
sure in past summarization tasks such as generic
multi-document summarization (DUC 2004 Task
2), biographical summarization (DUC 2004 Task
5), opinion summarization (TAC 2008 OS), and
summarization in languages other than English.
We think that, in order to have a better under-
standing of document-summary evaluation mea-
sures, more research is needed. In this paper we
present a series of experiments aimed at a better
understanding of the value of the Jensen-Shannon
divergence for ranking summarization systems.
We have carried out experimentation with the
proposed measure and have verified that in cer-
tain tasks (such as those studied by (Louis and
Nenkova, 2009)) there is a strong correlation
among Pyramids and Responsiveness and the
Jensen-Shannon divergence, but as we will show
in this paper, there are datasets in which the cor-
relation is not so strong. We also present exper-
iments in Spanish and French showing positive
correlation between the Jensen-Shannon measure
and ROUGE.
The rest of the paper is organized in the follow-
ing way: First in Section 2 we introduce related
work in the area of content-based evaluation iden-
tifying the departing point for our inquiry; then in
Section 3 we explain the methodology adopted in
our work and the tools and resources used for ex-
perimentation. In Section 4 we present the experi-
ments carried out together with the results. Sec-
tion 5 discusses the results and Section 6 con-
cludes the paper.
1060
2 Related Work
One of the first works to use content-based mea-
sures in text summarization evaluation is due to
(Donaway et al, 2000) who presented an evalu-
ation framework to compare rankings of summa-
rization systems produced by recall and cosine-
based measures. They showed that there was
weak correlation between rankings produced by
recall, but that content-based measures produce
rankings which were strongly correlated, thus
paving the way for content-based measures in text
summarization evaluation.
Radev et al (2003) also compared various eval-
uation measures based on vocabulary overlap. Al-
though these measures were able to separate ran-
dom from non-random systems, no clear conclu-
sion was reached on the value of each of the mea-
sures studied.
Nowadays, a widespread summarization evalu-
ation framework is ROUGE (Lin and Hovy, 2003)
which, as we have mentioned before, offers a set
of statistics that compare peer summaries with
models. Various statistics exist depending on the
used n-gram and on the type of text processing ap-
plied to the input texts (e.g., lemmatization, stop-
word removal).
Lin et al (2006) proposed a method of evalua-
tion based on the use of ?distances? or divergences
between two probability distributions (the distri-
bution of units in the automatic summary and the
distribution of units in the model summary). They
studied two different Information Theoretic mea-
sures of divergence: the Kullback-Leibler (KL)
(Kullback and Leibler, 1951) and Jensen-Shannon
(JS) (Lin, 1991a) divergences. In this work we
use the Jensen-Shannon (JS) divergence that is
defined as follows:
DJS(P ||Q) = 12
?
w
Pw log2
2Pw
Pw +Qw
+ Qw log2
2Qw
Pw +Qw
(1)
This measure can be applied to the distribu-
tion of units in system summaries P and refer-
ence summaries Q and the value obtained used
as a score for the system summary. The method
has been tested by (Lin et al, 2006) over the
DUC 2002 corpus for single and multi docu-
ment summarization tasks showing good correla-
tion among divergence measures and both cover-
age and ROUGE rankings.
Louis and Nenkova (2009) went even further
and, as in (Donaway et al, 2000), proposed to
directly compare the distribution of words in full
documents with the distribution of words in auto-
matic summaries to derive a content-based eval-
uation measure. They found high correlation
among rankings produced using models and rank-
ings produced without models. This work is the
departing point for our inquiry into the value of
measures that do not rely on human models.
3 Methodology
The methodology of this paper mirrors the one
adopted in past work (Donaway et al, 2000;
Louis and Nenkova, 2009). Given a particular
summarization task T , p data points to be sum-
marized with input material {Ii}p?1i=0 (e.g. doc-
ument(s), questions, topics), s peer summaries
{SUMi,k}s?1k=0 for input i, and m model sum-
maries {MODELi,j}m?1j=0 for input i, we will com-
pare rankings of the s peer summaries produced
by various evaluation measures. Some measures
we use compare summaries with n out of the m
models:
MEASUREM (SUMi,k, {MODELi,j}nj=0) (2)
while other measures compare peers with all or
some of the input material:
MEASUREM (SUMi,k, I ?i) (3)
where I ?i is some subset of input Ii. The val-
ues produced by the measures for each sum-
mary SUMi,k are averaged for each system k =
0, . . . , s ? 1 and these averages are used to pro-
duce a ranking. Rankings are compared using
Spearman Rank correlation (Spiegel and Castel-
lan, 1998) used to measure the degree of associa-
tion between two variables whose values are used
to rank objects. We use this correlation to directly
compare results to those presented in (Louis and
Nenkova, 2009). Computation of correlations is
1061
done using the CPAN Statistics-RankCorrelation-
0.12 package2, which computes the rank correla-
tion between two vectors.
3.1 Tools
We carry out experimentation using a new sum-
marization evaluation framework: FRESA
?FRamework for Evaluating Summaries
Automatically? which includes document-
based summary evaluation measures based on
probabilities distribution. As in the ROUGE
package, FRESA supports different n-grams
and skip n-grams probability distributions.
The FRESA environment can be used in the
evaluation of summaries in English, French,
Spanish and Catalan, and it integrates filtering
and lemmatization in the treatment of summaries
and documents. It is developed in Perl and will be
made publicly available. We also use the ROUGE
package to compute various ROUGE statistics in
new datasets.
3.2 Summarization Tasks and Data Sets
We have conducted our experimentation with the
following summarization tasks and data sets:
Generic multi-document-summarization in En-
glish (i.e. production a short summary of a cluster
of related documents) using data fromDUC 20043
corpus task 2: 50 clusters (10 documents each) ?
294,636 words.
Focused-based summarization in English (i.e.
production a short focused multi-document sum-
mary focused on the question ?who is X??, where
X is a person?s name) using data from the DUC
2004 task 5: 50 clusters ( 10 documents each plus
a target person name) ? 284,440 words.
Update-summarization task that consists of cre-
ating a summary out of a cluster of documents and
a topic. Two sub-tasks are considered here: A)
an initial summary has to be produced based on
an initial set of documents and topic; B) an up-
date summary has to be produced from a differ-
ent (but related) cluster assuming documents used
in A) are known. The English TAC 2008 Update
2http://search.cpan.org/?gene/
Statistics-RankCorrelation-0.12/
3http://www-nlpir.nist.gov/projects/
duc/guidelines/2004.html
Summarization dataset is used which consists of
48 topics with 20 documents each ? 36,911 words.
Opinion summarization where systems have to
analyze a set of blog articles and summarize the
opinions about a target in the articles. The TAC
2008 Opinion Summarization in English4 data set
(taken from the Blogs06 Text Collection) is used:
25 clusters and targets (i.e., target entity and ques-
tions) were used ? 1,167,735 words.
Generic single-document summarization in
Spanish using the ?Spanish Medicina Cl??nica?5
corpus which is composed of 50 biomedical ar-
ticles in Spanish, each one with its corresponding
author abstract ? 124,929 words.
Generic single document summarization in
French using the ?Canadien French Sociologi-
cal Articles? corpus from the journal Perspec-
tives interdisciplinaires sur le travail et la sante?
(PISTES)6. It contains 50 sociological articles in
French with their corresponding author abstracts
? 381,039 words.
3.3 Summarization Systems
For experimentation in the TAC and the DUC
datasets we directly use the peer summaries
produced by systems participating in the eval-
uations. For experimentation in Spanish and
French (single-document summarization) we
have created summaries at the compression rates
of the model summaries using the following
summarization systems:
? CORTEX (Torres-Moreno et al, 2002), a
single-document sentence extraction system
for Spanish and French that combines vari-
ous statistical measures of relevance (angle
between sentence and topic, various Ham-
ming weights for sentences, etc.) and applies
an optimal decision algorithm for sentence
selection;
? ENERTEX (Fernandez et al, 2007), a sum-
marizer based on a theory of textual energy;
4http://www.nist.gov/tac/data/index.
html
5http://www.elsevier.es/revistas/
ctl servlet? f=7032&revistaid=2
6http://www.pistes.uqam.ca/
1062
? SUMMTERM (Vivaldi et al, 2010), a
terminology-based summarizer that is used
for summarization of medical articles and
uses specialized terminology for scoring and
ranking sentences;
? JS summarizer, a summarization system that
scores and ranks sentences according to their
Jensen-Shannon divergence to the source
document;
? a lead-based summarization system that se-
lects the lead sentences of the document;
? a random-based summarization system that
selects sentences at random;
? the multilingual word-frequency Open Text
Summarizer (Yatsko and Vishnyakov, 2007);
? the AutoSummarize program of Microsoft
Word;
? the commercial SSSummarizer7;
? the Pertinence summarizer8;
? the Copernic summarizer9.
3.4 Evaluation Measures
The following measures derived from human
assessment of the content of the summaries are
used in our experiments:
? Coverage is understood as the degree to
which one peer summary conveys the same
information as a model summary (Over et al,
2007). Coverage was used in DUC evalua-
tions.
? Responsiveness ranks summaries in a 5-point
scale indicating how well the summary sat-
isfied a given information need (Over et al,
2007). It is used in focused-based summa-
rization tasks. Responsiveness was used in
DUC-TAC evaluations.
7http://www.kryltech.com/summarizer.
htm
8http://www.pertinence.net
9http://www.copernic.com/en/products/
summarizer
? Pyramids (briefly introduced in Section 1)
(Nenkova and Passonneau, 2004) is a content
assessment measure which compares content
units in a peer summary to weighted content
units in a set of model summaries. Pyramids
is the adopted metric for content-based eval-
uation in the TAC evaluations.
For DUC and TAC datasets the values of these
measures are available and we used them directly.
We used the following automatic evaluation
measures in our experiments:
? We use the Rouge package (Lin, 2004) to
compute various statistics. For the experi-
ments presented here we used uni-grams, bi-
grams, and the skip bi-grams with maximum
skip distance of 4 (ROUGE-1, ROUGE-2 and
ROUGE-SU4). ROUGE is used to compare a
peer summary to a set of model summaries
in our framework.
? Jensen-Shannon divergence formula given in
Equation 1 is implemented in our FRESA
package with the following specification for
the probability distribution of words w.
Pw =
CTw
N (4)
Qw =
{
CSw
NS if w ? S
CTw+?
N+??B elsewhere
(5)
Where P is the probability distribution of
words w in text T and Q is the probabil-
ity distribution of words w in summary S;
N is the number of words in text and sum-
mary N = NT + NS , B = 1.5|V |, CTw is
the number of words in the text and CSw is
the number of words in the summary. For
smoothing the summary?s probabilities we
have used ? = 0.005.
4 Experiments and Results
We first replicated the experiments presented in
(Louis and Nenkova, 2009) to verify that our im-
plementation of JS produced correlation results
compatible with that work. We used the TAC
2008 Update Summarization data set and com-
puted JS and ROUGE measures for each peer
1063
summary. We produced two system rankings (one
for each measure), which were compared to rank-
ings produced using the manual Pyramids and Re-
sponsiveness scores. Spearman correlations were
computed among the different rankings. The re-
sults are presented in Table 1. These results con-
firm a high correlation among Pyramids, Respon-
siveness, and JS. We also verified high corre-
lation between JS and ROUGE-2 (0.83 Spearman
correlation, not shown in the table) in this task and
dataset.
Measure Pyr. p-value Resp. p-value
ROUGE-2 0.96 p < 0.005 0.92 p < 0.005
JS 0.85 p < 0.005 0.74 p < 0.005
Table 1: Spearman system rank correlation of
content-based measures in TAC 2008 Update
Summarization task
Then, we experimented with data from DUC
2004, TAC 2008 Opinion Summarization pilot
and with single document summarization in Span-
ish and French. In spite of the fact that the exper-
iments for French and Spanish corpora use less
data points (i.e., less summarizers per task) than
for English, results are still quite significant.
For DUC 2004, we computed the JS measure
for each peer summary in tasks 2 and 5 and we
used JS and the official ROUGE, coverage, and
Responsiveness scores to produce systems? rank-
ings. The various Spearman?s rank correlation
values for DUC 2004 are presented in Tables 2
(for task 2) and 3 (for task 5). For task 2, we have
verified a strong correlation between JS and cov-
erage. For task 5, the correlation between JS and
coverage is weak, and the correlation between JS
and Responsiveness weak and negative.
Measure Cov. p-value
ROUGE-2 0.79 p < 0.0050
JS 0.68 p < 0.0025
Table 2: Spearman system rank correlation of
content-based measures with coverage in DUC
2004 Task 2
Although the Opinion Summarization task is a
new type of summarization task and its evaluation
is a complicated issue, we have decided to com-
pare JS rankings with those obtained using Pyra-
Measure Cov. p-value Resp. p-value
ROUGE-2 0.78 p < 0.001 0.44 p < 0.05
JS 0.40 p < 0.050 -0.18 p < 0.25
Table 3: Spearman system rank correlation of
content-based measures in DUC 2004 Task 5
mids and Responsiveness in TAC 2008. Spear-
man?s correlation values are listed in Table 4. As
can be seen, there is weak and negative correla-
tion of JS with both Pyramids and Responsive-
ness. Correlation between Pyramids and Respon-
siveness rankings is high for this task (0.71 Spear-
man?s correlation value).
Measure Pyr. p-value Resp. p-value
JS -0.13 p < 0.25 -0.14 p < 0.25
Table 4: Spearman system rank correlation of
content-based measures in TAC 2008 Opinion
Summarization task
For experimentation in Spanish and French, we
have run 11 multi-lingual summarization systems
over each of the documents in the two corpora,
producing summaries at a compression rate close
to the compression rate of the provided authors?
abstracts. We have computed JS and ROUGE
measures for each summary and we have aver-
aged the measure?s values for each system. These
averages were used to produce rankings per each
measure. We computed Spearman?s correlations
for all pairs of rankings. Results are presented in
Tables 5-6. All results show medium to strong
correlation between JS and ROUGE measures.
However the JS measure based on uni-grams has
lower correlation than JSs which use n-grams of
higher order.
5 Discussion
The departing point for our inquiry into text sum-
marization evaluation has been recent work on the
use of content-based evaluation metrics that do
not rely on human models but that compare sum-
mary content to input content directly (Louis and
Nenkova, 2009). We have some positive and some
negative results regarding the direct use of the full
document in content-based evaluation. We have
verified that in both generic muti-document sum-
1064
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-value
JS 0.56 p < 0.100 0.46 p < 0.100 0.45 p < 0.200
JS2 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005
JS4 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005
JSM 0.82 p < 0.005 0.71 p < 0.020 0.71 p < 0.010
Table 5: Spearman system rank correlation of content-based measures with ROUGE in the Medicina
Clinica Corpus (Spanish)
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-2 p-value
JS 0.70 p < 0.050 0.73 p < 0.05 0.73 p < 0.500
JS2 0.93 p < 0.002 0.86 p < 0.01 0.86 p < 0.005
JS4 0.83 p < 0.020 0.76 p < 0.05 0.76 p < 0.050
JSM 0.88 p < 0.010 0.83 p < 0.02 0.83 p < 0.010
Table 6: Spearman system rank correlation of content-based measures with ROUGE in the PISTES
Sociological Articles Corpus (French)
marization and in topic-based multi-document
summarization in English correlation among mea-
sures that use human models (Pyramids, Respon-
siveness, and ROUGE) and a measure that does
not use models (the Jensen Shannon divergence)
is strong. We have found that correlation among
the same measures is weak for summarization of
biographical information and summarization of
opinions in blogs. We believe that in these cases
content-based measures should consider in addi-
tion to the input document, the summarization
task (i.e. its text-based representation) to better
assess the content of the peers, the task being a
determinant factor in the selection of content for
the summary. Our multi-lingual experiments in
generic single-document summarization confirm a
strong correlation among the Jensen-Shannon di-
vergence and ROUGE measures. It is worth not-
ing that ROUGE is in general the chosen frame-
work for presenting content-based evaluation re-
sults in non-English summarization. For the ex-
periments in Spanish, we are conscious that we
only have one model summary to compare with
the peers. Nevertheless, these models are the cor-
responding abstracts written by the authors of the
articles and this is in fact the reason for choosing
this corpus. As the experiments in (da Cunha et
al., 2007) show, the professionals of a specialized
domain (as, for example, the medical domain)
adopt similar strategies to summarize their texts
and they tend to choose roughly the same content
chunks for their summaries. Because of this, the
summary of the author of a medical article can be
taken as reference for summaries evaluation. It is
worth noting that there is still debate on the num-
ber of models to be used in summarization evalu-
ation (Owkzarzak and Dang, 2009). In the French
corpus PISTES, we suspect the situation is similar
to the Spanish case.
6 Conclusions and Future Work
This paper has presented a series of experiments
in content evaluation in text summarization to as-
sess the value of content-based measures that do
not rely on the use of model summaries for com-
parison purposes. We have carried out exten-
sive experimentation with different summariza-
tion tasks drawing a clearer picture of tasks where
the measures could be applied. This paper makes
the following contributions:
? We have shown that if we are only interested
in ranking summarization systems according
to the content of their automatic summaries,
there are tasks where models could be sub-
stituted by the full document in the computa-
tion of the Jensen-Shannon divergence mea-
sure obtaining reliable rankings. However,
we have also found that the substitution of
models by full-documents is not always ad-
visable. We have found weak correlation
among different rankings in complex sum-
marization tasks such as the summarization
of biographical information and the summa-
1065
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-2 p-value
JS 0.83 p < 0.002 0.66 p < 0.05 0.741 p < 0.01
JS2 0.80 p < 0.005 0.59 p < 0.05 0.68 p < 0.02
JS4 0.75 p < 0.010 0.52 p < 0.10 0.62 p < 0.05
JSM 0.85 p < 0.002 0.64 p < 0.05 0.74 p < 0.01
Table 7: Spearman system rank correlation of content-based measures with ROUGE in the RPM2 Cor-
pus (French)
rization of opinions about an ?entity?.
? We have also carried out large-scale exper-
iments in Spanish and French which show
positive medium to strong correlation among
system?s ranks produced by ROUGE and di-
vergence measures that do not use the model
summaries.
? We have also presented a new framework,
FRESA, for the computation of measures
based on Jensen-Shannon divergence. Fol-
lowing the ROUGE approach, FRESA imple-
ments word uni-grams, bi-grams and skip n-
grams for the computation of divergences.
The framework is being made available to the
community for research purposes.
Although we have made a number of contribu-
tions, this paper leaves many questions open that
need to be addressed. In order to verify correlation
between ROUGE and JS, in the short term we in-
tend to extend our investigation to other languages
and datasets such as Portuguese and Chinese for
which we have access to data and summarization
technology. We also plan to apply our evaluation
framework to the rest of the DUC and TAC sum-
marization tasks to have a full picture of the corre-
lations among measures with and without human
models. In the long term we plan to incorporate a
representation of the task/topic in the computation
of the measures.
Acknowledgements
We thank three anonymous reviewers for their
valuable and enthusiastic comments. Horacio
Saggion is grateful to the Programa Ramo?n y Ca-
jal from the Ministerio de Ciencia e Innovacio?n,
Spain and to a Comenc?a grant from Universitat
Pompeu Fabra (COMENC?A10.004). This work
is partially supported by a postdoctoral grant (Na-
tional Program for Mobility of Research Human
Resources; National Plan of Scientific Research,
Development and Innovation 2008-2011) given to
Iria da Cunha by the Ministerio de Ciencia e In-
novacio?n, Spain.
References
da Cunha, Iria, Leo Wanner, and M. Teresa Cabre?.
2007. Summarization of specialized discourse: The
case of medical articles in spanish. Terminology,
13(2):249?286.
Donaway, Robert L., Kevin W. Drummey, and
Laura A. Mather. 2000. A comparison of rank-
ings produced by summarization evaluation mea-
sures. In NAACL-ANLP 2000 Workshop on Au-
tomatic Summarization, pages 69?78, Morristown,
NJ, USA. ACL.
Fernandez, Silvia, Eric SanJuan, and Juan-Manuel
Torres-Moreno. 2007. Textual Energy of Associa-
tive Memories: performants applications of Enertex
algorithm in text summarization and topic segmen-
tation. In MICAI?07, pages 861?871.
Kullback, S. and R.A. Leibler. 1951. On information
and sufficiency. Annals of Mathematical Statistics,
22(1):79?86.
Lin, C.-Y. and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL 2003, pages 71?78,
Morristown, NJ, USA. ACL.
Lin, Chin-Yew, Guihong Cao, Jianfeng Gao, and Jian-
Yun Nie. 2006. An information-theoretic approach
to automatic evaluation of summaries. In Confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics, pages 463?470, Mor-
ristown, NJ, USA. ACL.
Lin, J. 1991a. Divergence measures based on the
shannon entropy. IEEE Transactions on Informa-
tion Theory, 37(145-151).
1066
Lin, Jianhua. 1991b. Divergence measures based on
the shannon entropy. IEEE Transactions on Infor-
mation theory, 37:145?151.
Lin, Chin-Yew. 2004. ROUGE: A Package for
Automatic Evaluation of Summaries. In Marie-
Francine Moens, Stan Szpakowicz, editor, Text
Summarization Branches Out: ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July.
Louis, Annie and Ani Nenkova. 2009. Automati-
cally Evaluating Content Selection in Summariza-
tion without Human Models. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 306?314, Singapore, August. ACL.
Mani, I., G. Klein, D. House, L. Hirschman, T. Firmin,
and B. Sundheim. 2002. Summac: a text summa-
rization evaluation. Natural Language Engineering,
8(1):43?68.
Nenkova, Ani and Rebecca Passonneau. 2004. Eval-
uating Content Selection in Summarization: The
Pyramid Method. In Proceedings of NAACL-HLT
2004.
Over, Paul, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing & Management,
43(6):1506?1520.
Owkzarzak, Karolina and Hoa Trang Dang. 2009.
Evaluation of automatic summaries: Metrics under
varying data conditions. In Proceedings of the 2009
Workshop on Language Generation and Summari-
sation (UCNLG+Sum 2009), pages 23?30, Suntec,
Singapore, August. ACL.
Papineni, K., S. Roukos, T. Ward, , and W. J. Zhu.
2002. BLEU: a method for automatic evaluation
of machine translation. In ACL?02: 40th Annual
meeting of the Association for Computational Lin-
guistics, pages 311?318.
Pastra, K. and H. Saggion. 2003. Colouring sum-
maries Bleu. In Proceedings of Evaluation Initia-
tives in Natural Language Processing, Budapest,
Hungary, 14 April. EACL.
Radev, Dragomir R., Simone Teufel, Horacio Sag-
gion, Wai Lam, John Blitzer, Hong Qi, Arda C?elebi,
Danyu Liu, and Elliott Dra?bek. 2003. Evaluation
challenges in large-scale document summarization.
In ACL, pages 375?382.
Saggion, H., D. Radev, S. Teufel, and W. Lam. 2002.
Meta-evaluation of Summaries in a Cross-lingual
Environment using Content-based Metrics. In Pro-
ceedings of COLING 2002, pages 849?855, Taipei,
Taiwan, August 24-September 1.
Spa?rck-Jones, Karen and Julia Rose Galliers, editors.
1996. Evaluating Natural Language Processing
Systems, An Analysis and Review, volume 1083 of
Lecture Notes in Computer Science. Springer.
Spiegel, S. and N.J. Castellan, Jr. 1998. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill International.
Torres-Moreno, Juan-Manuel, Patricia Velz?quez-
Morales, and Jean-Guy Meunier. 2002. Condenss?
de textes par des me?thodes numr?iques. In JADT?02,
volume 2, pages 723?734, St Malo, France.
Vivaldi, Jorge, Iria da Cunha, Juan-Manuel Torres-
Moreno, and Patricia Vela?zquez-Morales. 2010.
Automatic summarization using terminological and
semantic resources. In LREC?10, volume 2,
page 10, Malta.
Yatsko, V.A. and T.N. Vishnyakov. 2007. A method
for evaluating modern systems of automatic text
summarization. Automatic Documentation and
Mathematical Linguistics, 41(3):93?103.
1067
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 56?64,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Modelling Irony in Twitter
Francesco Barbieri
Pompeu Fabra University
Barcelona, Spain
francesco.barbieri@upf.edu
Horacio Saggion
Pompeu Fabra University
Barcelona, Spain
horacio.saggion@upf.edu
Abstract
Computational creativity is one of the
central research topics of Artificial Intel-
ligence and Natural Language Process-
ing today. Irony, a creative use of
language, has received very little atten-
tion from the computational linguistics
research point of view. In this study
we investigate the automatic detection of
irony casting it as a classification prob-
lem. We propose a model capable of de-
tecting irony in the social network Twit-
ter. In cross-domain classification experi-
ments our model based on lexical features
outperforms a word-based baseline previ-
ously used in opinion mining and achieves
state-of-the-art performance. Our features
are simple to implement making the ap-
proach easily replicable.
1 Introduction
Irony, a creative use of language, has received very
little attention from the computational linguistics
research point of view. It is however considered an
important aspect of language which deserves spe-
cial attention given its relevance in fields such as
sentiment analysis and opinion mining (Pang and
Lee, 2008). Irony detection appears as a difficult
problem since ironic statements are used to ex-
press the contrary of what is being said (Quintilien
and Butler, 1953), therefore being a tough nut to
crack by current systems. Being a creative form of
language, there is no consensual agreement in the
literature on how verbal irony should be defined.
Only recently irony detection has been approached
from a computational perspective. Reyes et al.
(2013) cast the problem as one of classification
training machine learning algorithms to sepatare
ironic from non-ironic statements. In a similar
vein, we propose and evaluate a new model to
detect irony, using seven sets of lexical features,
most of them based on our intuitions about ?un-
expectedness?, a key component of ironic state-
ments. Indeed, Lucariello (1994) claims that irony
is strictly connected to surprise, showing that un-
expectedness is the feature most related to situa-
tional ironies.
In this paper we reduce the complexity of the
problem by studying irony detection in the micro-
blogging service Twitter
1
that allows users to send
and read text messages (shorter than 140 charac-
ters) called tweets.
We do not adopt any formal definition of irony,
instead we rely on a dataset created for the study
of irony detection which allows us to compare our
findings with recent state-of-the-art approaches
(Reyes et al., 2013).
The contributions of this paper are as follows:
? a novel set of linguistically motivated, easy-
to-compute features
? a comparison of our model with the state-of-
the-art; and
? a novel set of experiments to demonstrate
cross-domain adaptation.
The paper will show that our model outperforms
a baseline, achieves state-of-the-art performance,
and can be applied to different domains.
The rest of the paper is organised as follows: in
the next Section we describe related work. In Sec-
tion 3 we described the corpus and text process-
ing tools used and in Section 4 we present our ap-
proach to tackle the irony detection problem. Sec-
tion 5 describes the experiments while Section 6
interprets the results. Finally we close the paper in
Section 7 with conclusions and future work.
1
https://twitter.com/
56
2 Related Work
Verbal irony has been defined in several ways over
the years but there is no consensual agreement
on the definition. The standard definition is con-
sidered ?saying the opposite of what you mean?
(Quintilien and Butler, 1953) where the opposi-
tion of literal and intended meanings is very clear.
Grice (1975) believes that irony is a rhetorical fig-
ure that violates the maxim of quality: ?Do not
say what you believe to be false?. Irony is also de-
fined (Giora, 1995) as any form of negation with
no negation markers (as most of the ironic utter-
ances are affirmative, and ironic speakers use in-
direct negation). Wilson and Sperber (2002) de-
fined it as echoic utterance that shows a negative
aspect of someone?s else opinion. For example if
someone states ?the weather will be great tomor-
row? and the following day it rains, someone with
ironic intents may repeat the sentence ?the weather
will be great tomorrow? in order to show the state-
ments was incorrect. Finally irony has been de-
fined as form of pretence by Utsumi (2000) and
Veale and Hao (2010b). Veale states that ?ironic
speakers usually craft their utterances in spite of
what has just happened, not because of it. The
pretence alludes to, or echoes, an expectation that
has been violated?.
Past computational approaches to irony detec-
tion are scarce. Carvalho et. al (2009) created
an automatic system for detecting irony relying on
emoticons and special punctuation. They focused
on detection of ironic style in newspaper articles.
Veale and Hao (2010a) proposed an algorithm for
separating ironic from non-ironic similes, detect-
ing common terms used in this ironic comparison.
Reyes et. al (2013) have recently proposed a model
to detect irony in Twitter, which is based on four
groups of features: signatures, unexpectedness,
style, and emotional scenarios. Their classification
results support the idea that textual features can
capture patterns used by people to convey irony.
Among the proposed features, skip-grams (part of
the style group) which captures word sequences
that contain (or skip over) arbitrary gaps, seems to
be the best one.
There are also a few computational model that
detect sarcasm ((Davidov et al., 2010); (Gonz?alez-
Ib?a?nez et al., 2011); (Liebrecht et al., 2013)) on
Twitter and Amazon, but even if one may argue
that sarcasm and irony are the same linguistic phe-
nomena, the latter is more similar to mocking or
making jokes (sometimes about ourselves) in a
sharp and non-offensive manner. On the other
hand, sarcasm is a meaner form of irony as it tends
to be offensive and directed towards other people
(or products like in Amazon reviews). Textual ex-
amples of sarcasm lack the sharp tone of an ag-
gressive speaker, so for textual purposes we think
irony and sarcasm should be considered as differ-
ent phenomena and studied separately (Reyes et
al., 2013).
3 Data and Text Processing
The dataset used for the experiments reported
in this paper has been prepared by Reyes et al.
(2013). It is a corpus of 40.000 tweets equally di-
vided into four different topics: Irony, Education,
Humour, and Politics where the last three topics
are considered non-ironic. The tweets were au-
tomatically selected by looking at Twitter hash-
tags (#irony, #education, #humour, and #politics)
added by users in order to link their contribution to
a particular subject and community. The hashtags
are removed from the tweets for the experiments.
According to Reyes et. al (2013), these hashtags
were selected for three main reasons: (i) to avoid
manual selection of tweets, (ii) to allow irony anal-
ysis beyond literary uses, and because (iii) irony
hashtag may ?reflect a tacit belief about what con-
stitutes irony.?
Another corpora is employed in our approach to
measure the frequency of word usage. We adopted
the Second Release of the American National Cor-
pus Frequency Data
2
(Ide and Suderman, 2004),
which provides the number of occurrences of a
word in the written and spoken ANC. From now
on, we will mean with ?frequency of a term? the
absolute frequency the term has in the ANC.
3.1 Text Processing
In order to process the tweets we use the freely
available vinhkhuc Twitter Tokenizer
3
which al-
lows us to recognise words in each tweet. To part-
of-speech tag the words, we rely on the Rita Word-
Net API (Howe, 2009) that associates to a word
with its most frequently used part of speech. We
also adopted the Java API for WordNet Searching
2
The American National Corpus (http://www.anc.org/) is,
as we read in the web site, a massive electronic collection of
American English words (15 million)
3
https://github.com/vinhkhuc/Twitter-
Tokenizer/blob/master/src/Twokenizer.java
57
(Spell, 2009) to perform some operation on Word-
Net synsets. It is worth noting that although our
approach to text processing is rather superficial for
the moment, other tools are available to perform
deeper tweet linguistic analysis (Bontcheva et al.,
2013; Derczynski et al., 2013).
4 Methodology
We approach the detection of irony as a classifica-
tion problem applying supervised machine learn-
ing methods to the Twitter corpus described in
Section 3. When choosing the classifiers we had
avoided those requiring features to be independent
(e.g. Naive Bayes) as some of our features are not.
Since we approach the problem as a binary deci-
sion (deciding if a tweet is ironic or not) we picked
two tree-based classifiers: Random Forest and De-
cision tree (the latter allows us to compare our
findings directly to Reyes et. al (2013)). We use
the implementations available in the Weka toolkit
(Witten and Frank, 2005).
To represent each tweet we use six groups of
features. Some of them are designed to detect im-
balance and unexpectedness, others to detect com-
mon patterns in the structure of the ironic tweets
(like type of punctuation, length, emoticons). Be-
low is an overview of the group of features in our
model:
? Frequency (gap between rare and common
words)
? Written-Spoken (written-spoken style uses)
? Intensity (intensity of adverbs and adjectives)
? Structure (length, punctuation, emoticons)
? Sentiments (gap between positive and nega-
tive terms)
? Synonyms (common vs. rare synonyms use)
? Ambiguity (measure of possible ambiguities)
In our knowledge Frequency, Written Spoken, In-
tensity and Synonyms groups have not been used
before in similar studies. The other groups have
been used already (for example by Carvalho et. al
(2009) or Reyes et al. (2013)) yet our implemen-
tation is different in most of the cases.
In the following sections we describe the the-
oretical motivations behind the features and how
them have been implemented.
4.1 Frequency
As said previously unexpectedness can be a sig-
nal of irony and in this first group of features we
try to detect it. We explore the frequency imbal-
ance between words, i.e. register inconsistencies
between terms of the same tweet. The idea is that
the use of many words commonly used in English
(i.e. high frequency in ANC) and only a few terms
rarely used in English (i.e. low frequency in ANC)
in the same sentence creates imbalance that may
cause unexpectedness, since within a single tweet
only one kind of register is expected. We are able
to explore this aspect using the ANC Frequency
Data corpus.
Three features belong to this group: frequency
mean, rarest word, frequency gap. The first one
is the arithmetic average of all the frequencies of
the words in a tweet, and it is used to detect the
frequency style of a tweet. The second one, rarest
word, is the frequency value of the rarest word,
designed to capture the word that may create im-
balance. The assumption is that very rare words
may be a sign of irony. The third one is the abso-
lute difference between the first two and it is used
to measure the imbalance between them, and cap-
ture a possible intention of surprise. We have ver-
ified that the mean of this gap in each tweet of the
irony corpus is higher than in the other corpora.
4.2 Written-Spoken
Twitter is composed of written text, but an infor-
mal spoken English style is often used. We de-
signed this set of features to explore the unexpect-
edness created by using spoken style words in a
mainly written style tweet or vice versa (formal
words usually adopted in written text employed in
a spoken style context). We can analyse this aspect
with ANC written and spoken, as we can see us-
ing this corpora whether a word is more often used
in written or spoken English. There are three fea-
tures in this group: written mean, spoken mean,
written spoken gap. The first and second ones are
the means of the frequency values, respectively, in
written and spoken ANC corpora of all the words
in the tweet. The third one, written spoken gap,
is the absolute value of the difference between the
first two, designed to see if ironic writers use both
styles (creating imbalance) or only one of them. A
low difference between written and spoken styles
means that both styles are used.
58
4.3 Structure
With this group of features we want to study the
structure of the tweet: if it is long or short (length),
if it contains long or short words (mean of word
length), and also what kind of punctuation is used
(exclamation marks, emoticons, etc.). This is a
powerful feature, as ironic tweets in our corpora
present specific structures: for example they are
often longer than the tweets in the other corpora,
they contain certain kind of punctuation and they
use only specific emoticons. This group includes
several features that we describe below.
The length feature consists of the number of
characters that compose the tweet, n. words is
the number of words, and words length mean is
the mean of the words length. Moreover, we use
the number of verbs, nouns, adjectives and adverbs
as features, naming them n. verbs, n. nouns, n.
adjectives and n. adverbs. With these last four
features we also computed the ratio of each part
of speech to the number of words in the tweet; we
called them verb ratio, noun ratio, adjective ra-
tio, and adverb ratio. All these features have the
purpose of capturing the style of the writer. Some
of them seem to be significant; for example the
average length of an ironic tweet is 94.8 charac-
ters and the average length of education, humour,
and politics tweets are respectively 82.0, 86.6, and
86.5. The words used in the irony corpus are usu-
ally shorter than in the other corpora, but they
amount to more.
The punctuation feature is the sum of the num-
ber of commas, full stops, ellipsis and exclama-
tion that a tweet presents. We also added a feature
called laughing which is the sum of all the internet
laughs, denoted with hahah, lol, rofl, and lmao that
we consider as a new form of punctuation: instead
of using many exclamation marks internet users
may use the sequence lol (i.e. laughing out loud) or
just type hahaha. As the previous features, punc-
tuation and laughing occur more frequently in the
ironic tweets than in the other topics.
The emoticon feature is the sum of the emoti-
cons :), :D, :( and ;) in a tweet. This feature works
well in the humour corpus because is the one that
presents a very different number of them, it has
four times more emoticons than the other corpora.
The ironic corpus is the one with the least emoti-
cons (there are only 360 emoticons in the Irony
corpus, while in Humour, Education, and Poli-
tics tweets they are 2065, 492, 397 respectively).
In the light of these statistics we can argue that
ironic authors avoid emoticons and leave words to
be the central thing: the audience has to under-
stand the irony without explicit signs, like emoti-
cons. Another detail is the number of winks ;). In
the irony corpus one in every five emoticon is a
wink, whereas in the Humour, Education and Pol-
itics corpora the number of winks are 1 in every
30, 22 and 18 respectively. Even if the wink is not
a usual emoticon, ironic authors use it more of-
ten because they mean something else when writ-
ing their tweets, and a wink is used to suggest that
something is hidden behind the words.
4.4 Intensity
A technique ironic authors may employ is saying
the opposite of what they mean (Quintilien and
Butler, 1953) using adjectives and adverbs to, for
example, describe something very big to denote
something very small (e.g. saying ?Do we hike
that tiny hill now?? before going on top of a very
high mountain). In order to produce an ironic ef-
fect some authors might use an expression which
is antonymic to what they are trying to describe,
we believe that in the case the word being an ad-
jective or adverb its intensity (more or less exag-
gerated) may well play a role in producing the in-
tended effect. We adopted the intensity scores of
Potts (2011) who uses naturally occurring meta-
data (star ratings on service and product reviews)
to construct adjectives and adverbs scales. An ex-
ample of adjective scale (and relative scores in
brackets) could be the following: horrible (-1.9)
? bad (-1.1)? good (0.2)? nice (0.3)? great
(0.8).
With these scores we evaluate four features for
adjective intensity and four for adverb intensity
(implemented in the same way): adj (adv) tot,
adj (adv) mean, adj (adv) max, and adj (adv)
gap. The sum of the AdjScale scores of all the ad-
jectives in the tweet is called adj tot. adj mean is
adj tot divided by the number of adjectives in the
tweet. The maximum AdjScale score within a sin-
gle tweet is adj max. Finally, adj gap is the differ-
ence between adj max and adj mean, designed to
see ?how much? the most intense adjective is out
of context.
4.5 Synonyms
Ironic authors send two messages to the audience
at the same time, the literal and the figurative one
(Veale, 2004). It follows that the choice of a term
59
(rather than one of its synonyms) is very impor-
tant in order to send the second, not obvious, mes-
sage. For example if the sky is grey and it is
about to rain, someone with ironic intents may say
?sublime weather today?, choosing sublime over
many different, more common, synonyms (like
nice, good, very good and so on, that according to
ANC are more used in English) to advise the lis-
tener that the literal meaning may not be the only
meaning present. A listener will grasp this hid-
den information when he asks himself why a rare
word like sublime was used in that context where
other more common synonyms were available to
express the same literal meaning.
For each word of a tweet we get its synonyms
with WordNet (Miller, 1995), then we calculate
their ANC frequencies and sort them into a de-
creasing ranked list (the actual word is part of this
ranking as well). We use these rankings to define
the four features which belong to this group. The
first one is syno lower which is the number of syn-
onyms of the word w
i
with frequency lower than
the frequency of w
i
. It is defined as in Equation 1:
sl
w
i
= |syn
i,k
: f(syn
i,k
) < f(w
i
)| (1)
where syn
i,k
is the synonym of w
i
with rank k,
and f(x) the ANC frequency of x. Then we also
defined syno lower mean as mean of sl
w
i
(i.e. the
arithmetic average of sl
w
i
over all the words of a
tweet).
We also designed two more features: syno
lower gap and syno greater gap, but to define
them we need two more parameters. The first one
is word lowest syno that is the maximum sl
w
i
in a
tweet. It is formally defined as:
wls
t
= max
w
i
{|syn
i,k
: f(syn
i,k
) < f(w
i
)|}
(2)
The second one is word greatest syno defined as:
wgs
t
= max
w
i
{|syn
i,k
: f(syn
i,k
) > f(w
i
)|}
(3)
We are now able to describe syno lower gap
which detects the imbalance that creates a com-
mon synonym in a context of rare synonyms. It is
the difference between word lowest syno and syno
lower mean. Finally, we detect the gap of very
rare synonyms in a context of common ones with
syno greater gap. It is the difference between
word greatest syno and syno greater mean, where
syno greater mean is the following:
sgm
t
=
|syn
i,k
: f(syn
i,k
) > f(w
i
)|
n. words of t
(4)
The arithmetic averages of syno greater gap
and of syno lower gap in the irony corpus are
higher than in the other corpora, suggesting that a
very common (or very rare) synonym is often used
out of context i.e. a very rare synonym when most
of the words are common (have a high rank in our
model) and vice versa.
4.6 Ambiguity
Another interesting aspect of irony is ambiguity.
We noticed that the arithmetic average of the num-
ber of WordNet synsets in the irony corpus is
greater than in all the other corpora; this indi-
cates that ironic tweets presents words with more
meanings. Our assumption is that if a word has
many meanings the possibility of ?saying some-
thing else? with this word is higher than in a term
that has only a few meanings, then higher possibil-
ity of sending more then one message (literal and
intended) at the same time.
There are three features that aim to capture
these aspects: synset mean, max synset, and
synset gap. The first one is the mean of the num-
ber of synsets of each word of the tweet, to see if
words with many meanings are often used in the
tweet. The second one is the greatest number of
synsets that a single word has; we consider this
word the one with the highest possibility of being
used ironically (as multiple meanings are available
to say different things). In addition, we calculate
synset gap as the difference between the number
of synsets of this word (max synset) and the av-
erage number of synsets (synset mean), assuming
that if this gap is high the author may have used
that inconsistent word intentionally.
4.7 Sentiments
We think that sign of irony could also be found
using sentiment analysis. The SentiWordNet sen-
timent lexicon (Esuli and Sebastiani, 2006) as-
signs to each synset of WordNet sentiment scores
of positivity and negativity. We used these scores
to examine what kind of sentiments characterises
irony. We explore ironic sentiments with two dif-
ferent views: the first one is the simple analysis
of sentiments (to identify the main sentiment that
arises from ironic tweets) and the second one con-
cerns sentiment imbalances between words, de-
60
Training Set
Education Humour Politics
Test set P R F1 P R F1 P R F1
Education .85/.73 .84/.73 .84/.73 .57/.61 .53/.61 .46/.61 .61/.67 .56/.67 .51/.67
Humour .64/.62 .51/.62 .58/.62 .85/.75 .85/.75 .85/.75 .65/.61 .59/.61 .55/.60
Politics .61/.67 .58/.67 .55/.67 .55/.61 .60/.60 .56/.60 .87/.75 .87/.75 .87/.75
Table 1: Precision, Recall and F-Measure of each topic combination for word based algorithm and our
algorithm in the form ?Word Based / Ours?. Decision Tree has been used as classifier for both algorithms.
We marked in bold the results that, according to the t-test, are significantly better.
signed to explore unexpectedness from a senti-
ment prospective.
There are six features in the Sentiments group.
The first one is named positive sum and it is the
sum of all the positive scores in a tweet, the sec-
ond one is negative sum, defined as sum of all the
negative scores. The arithmetic average of the pre-
vious ones is another feature, named positive neg-
ative mean, designed to reveal the sentiment that
better describe the whole tweet. Moreover, there
is positive-negative gap that is the difference be-
tween the first two features, as we wanted also to
detect the positive/negative imbalance within the
same tweet.
The imbalance may be created using only one
single very positive (or negative) word in the
tweet, and the previous features will not be able
to detect it, thus we needed to add two more. For
this purpose the model includes positive single
gap defined as the difference between most posi-
tive word and the mean of all the sentiment scores
of all the words of the tweet and negative single
gap defined in the same way, but with the most
negative one.
4.8 Bag of Words Baseline
Based on previous work on sentiment analysis and
opinon classification (see (Pang et al., 2002; Dave
et al., 2003) for example) we also investigate the
value of using bag of words representations for
irony classification. In this case, each tweet is rep-
resented as a set of word features. Because of the
brevity of tweets, we are only considering pres-
ence/absence of terms instead of frequency-based
representations based on tf ? idf .
5 Experiments and Results
In order to carry out experimentation and to be
able to compare our approach to that of (Reyes et
al., 2013) we use three datasets derived from the
corpus in Section 3. Irony vs Education, Irony
vs Humour and Irony vs Politics. Each topic
combination was balanced with 10.000 ironic
and 10.000 of non-ironic examples. The task at
hand it to train a classifier to identify ironic and
non-ironic tweets.
Figure 1: Information gain value of each group
(mean of the features belonged to each group) over
the three balanced corpus.
We perform two types of experiments:
? we run in each of the datasets a 10-fold cross-
validation classification;
? across datasets, we train the classifier in one
dataset and apply it to the other two datasets.
To perform these experiments, we create
three balanced datasets containing each one
third of the original 10.000 ironic tweets (so
that the datasets are disjoint) and one third of
the original domain tweets.
The experimental framework is executed for the
word-based baseline model and our model. In Ta-
ble 1 we present precision, recall, and F-measure
61
Figure 2: Information gain of each feature of the model. Irony corpus is compared to Education, Humor,
and Politics corpora. High values of information gain help to better discriminate ironic from non-ironic
tweets.
figures for the different runs of the experiments.
Table 3 shows precision, recall, and F-measure
figures for our approach compared to (Reyes et
al., 2013). Table 2 compares two different algo-
rithms: Decision Tree and Random Forest using
our model.
In order to have a clear understanding about the
contribution of each set of features in our model,
we also studied the behaviour of information gain
in each dataset. We compute information gain
experiments over the three balanced corpora and
present the results in Figure 1. The graphic shows
the mean information gain for each group of fea-
tures. We also report in Figure 2 the information
gain of each single feature, where one can under-
stand if a feature will be important to distinguish
ironic from non-ironic tweets.
6 Discussion
The results obtained with the bag-of-words base-
line seem to indicate that this approach is work-
ing as a topic-based classifier and not as an irony
detection procedure. Indeed, within each domain
using a 10 fold cross-validation setting, the bag-
of-words approach seems to overtake our model.
However, a clear picture emerges when a cross-
domain experiment is performed. In a setting
where different topics are used for training and
testing our model performs significantly better
than the baseline. t-tests were run for each ex-
periment and differences between baseline and our
model were observed for each cross-domain con-
dition (with a 99% confidence level). This could
be an indication that our model is more able to cap-
ture ironic style disregarding domain.
Analysing the data on Figure 2, we observe that
features which are more discriminative of ironic
style are rarest value, synonym lower, synonym
greater gap, and punctuation, suggesting that
Frequency, Structure and choice of the Synonym
are important aspects to consider for irony detec-
tion in tweets (this latter statement can be appre-
ciated in Figure 1 as well). Note, however, that
there is a topic or theme effect since features be-
have differently depending on the dataset used:
the Humour corpus seems to be the least consis-
tent. For instance punctuation well distinguishes
ironic from educational tweets, but behaves poorly
in the Humour corpus. This imbalance may cause
issues in a not controlled environment (e.g. no
preselected topics, only random generic tweets).
In spite of this, information gain values are fairly
high with four features having information gain
values over 0.1. Finding features that are signif-
icant for any non-ironic topic is hard, this is why
our system includes several feature sets: they aim
to distinguish irony from as many different topics
as possible.
62
Training Set
Education Humour Politics
Test set P R F1 P R F1 P R F1
Education .78/.73 .78/.73 .78/.73 .65/.61 .63/.61 .62/.61 .71/.67 .71/.67 .70/.67
Humour .64/.62 .61/.62 .60/.62 .80/.75 .80/.75 .80/.75 .64/.61 .62/.61 .60/.60
Politics .71/.67 .70/.67 .69/.67 .63/.61 .51/.60 .59/.60 .79/.75 .79/.75 .79/.75
Table 2: Precision, Recall and F-Measure for each topic combination of our model when Decision Tree
and Random Forest are used. Data are in the format ?Random Forest / Decision Tree?. We marked in
bold the F-Measures that are better.
Education Humour Politics
Model P R F1 P R F1 P R F1
Reyes et. al .76 .66 .70 .78 .74 .76 .75 .71 .73
Our model .73 .73 .73 .75 .75 .75 .75 .75 .75
Table 3: Precision, Recall, and F-Measure over the three corpora Education, Humour, and Politics. Both
our and Reyes et al. results are shown; the classifier used is Decision Tree for both models. We marked
in bold the F-Measures that are better compared to the other model.
With respect to results for two different classi-
fiers trained with our model (Random Forest (RF)
and Decision Trees (DT)) we observe that (see Ta-
ble 2) RF is better in cross-validation but across-
domains both algorithms are comparable.
Turning now to the state of the art we compare
our approach to (Reyes et al., 2013), the num-
bers presented in Table 3 seem to indicate that (i)
our approach is more balanced in terms of preci-
sion and recall and that (ii) our approach performs
slightly better in terms of F-Measure in two out of
three domains.
7 Conclusion and Future Work
In this article we have proposed a novel linguisti-
cally motivated set of features to detect irony in the
social network Twitter. The features take into ac-
count frequency, written/spoken differences, senti-
ments, ambiguity, intensity, synonymy and struc-
ture. We have designed many of them to be able
to model ?unexpectedness?, a key characteristic of
irony.
We have performed controlled experiments with
an available corpus of ironic and non-ironic tweets
using classifiers trained with bag-of-words fea-
tures and with our irony specific features. We have
shown that our model performs better than a bag-
of-words approach across-domains. We have also
shown that our model achieves state-of-the-art per-
formance.
There is however much space for improve-
ments. The ambiguity aspect is still weak in this
research, and it needs to be improved. Also exper-
iments adopting different corpora (Filatova, 2012)
and different negative topics may be useful in or-
der to explore the system behaviour in a real situa-
tion. Finally, we have relied on very basic tools for
linguistic analysis of the tweets, so in the near fu-
ture we intend to incorporate better linguistic pro-
cessors. A final aspect we want to investigate is
the use of n-grams from huge collections to model
?unexpected? word usage.
Acknowledgments
We are greatful to three anonymous reviewers for
their comments and suggestions that help improve
our paper. The research described in this paper is
partially funded by fellowship RYC-2009-04291
from Programa Ram?on y Cajal 2009 and project
number TIN2012-38584-C06-03 (SKATER-UPF-
TALN) from Ministerio de Econom??a y Competi-
tividad, Secretar??a de Estado de Investigaci?on, De-
sarrollo e Innovac??on, Spain.
References
Kalina Bontcheva, Leon Derczynski, Adam Funk,
Mark A. Greenwood, Diana Maynard, and Niraj
Aswani. 2013. TwitIE: An Open-Source Informa-
tion Extraction Pipeline for Microblog Text. In Pro-
ceedings of Recent Advances in Natural Language
Processing Conferemce.
Paula Carvalho, Lu??s Sarmento, M?ario J Silva, and
63
Eug?enio de Oliveira. 2009. Clues for detect-
ing irony in user-generated contents: oh...!! it?s
so easy;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion, pages 53?56. ACM.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th International Conference on
World Wide Web, WWW ?03, pages 519?528, New
York, NY, USA. ACM.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107?116. Association for
Computational Linguistics.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In Proceedings of Language
Resources and Evaluation Conference, volume 6,
pages 417?422.
Elena Filatova. 2012. Irony and sarcasm: Corpus
generation and analysis using crowdsourcing. In
Proceedings of Language Resources and Evaluation
Conference, pages 392?398.
Rachel Giora. 1995. On irony and negation. Discourse
processes, 19(2):239?264.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: A closer look. In ACL (Short Papers), pages
581?586. Citeseer.
H Paul Grice. 1975. Logic and conversation. 1975,
pages 41?58.
Daniel C Howe. 2009. Rita wordnet. java based api to
access wordnet.
Nancy Ide and Keith Suderman. 2004. The Ameri-
can National Corpus First Release. In Proceedings
of the Language Resources and Evaluation Confer-
ence.
Christine Liebrecht, Florian Kunneman, and Antal
van den Bosch. 2013. The perfect solution for
detecting sarcasm in tweets# not. WASSA 2013,
page 29.
Joan Lucariello. 1994. Situational irony: A concept of
events gone awry. Journal of Experimental Psychol-
ogy: General, 123(2):129.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natu-
ral Language Processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christopher Potts. 2011. Developing adjective scales
from user-supplied textual metadata. NSF Work-
shop on Restructuring Adjectives in WordNet. Ar-
lington,VA.
Quintilien and Harold Edgeworth Butler. 1953. The
Institutio Oratoria of Quintilian. With an English
Translation by HE Butler. W. Heinemann.
Antonio Reyes, Paolo Rosso, and Tony Veale. 2013.
A multidimensional approach for detecting irony in
twitter. Language Resources and Evaluation, pages
1?30.
Brett Spell. 2009. Java api for wordnet searching
(jaws).
Akira Utsumi. 2000. Verbal irony as implicit dis-
play of ironic environment: Distinguishing ironic
utterances from nonirony. Journal of Pragmatics,
32(12):1777?1806.
Tony Veale and Yanfen Hao. 2010a. Detecting ironic
intent in creative comparisons. In ECAI, volume
215, pages 765?770.
Tony Veale and Yanfen Hao. 2010b. An ironic fist
in a velvet glove: Creative mis-representation in the
construction of ironic similes. Minds and Machines,
20(4):635?650.
Tony Veale. 2004. The challenge of creative informa-
tion retrieval. In Computational Linguistics and In-
telligent Text Processing, pages 457?467. Springer.
Deirdre Wilson and Dan Sperber. 2002. Relevance
theory. Handbook of pragmatics.
Ian H Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.
64
Proceedings of NAACL-HLT 2013, pages 270?279,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Unsupervised Learning Summarization Templates from Concise Summaries
Horacio Saggion?
Universitat Pompeu Fabra
Department of Information and Communication Technologies
TALN Group
C/Tanger 122 - Campus de la Comunicacio?n
Barcelona - 08018
Spain
http://www.dtic.upf.edu/?hsaggion/
Abstract
We here present and compare two unsuper-
vised approaches for inducing the main con-
ceptual information in rather stereotypical
summaries in two different languages. We
evaluate the two approaches in two differ-
ent information extraction settings: mono-
lingual and cross-lingual information extrac-
tion. The extraction systems are trained on
auto-annotated summaries (containing the in-
duced concepts) and evaluated on human-
annotated documents. Extraction results are
promising, being close in performance to
those achieved when the system is trained on
human-annotated summaries.
1 Introduction
Information Extraction (Piskorski and Yangarber,
2013) and Automatic Text Summarization (Saggion
and Poibeau, 2013) are two Natural Language Pro-
cessing tasks which require domain and language
adaptation. For over two decades (Riloff, 1993;
Riloff, 1996) the natural language processing com-
munity has been interested in automatic or semi-
automatic methods which could be used to port sys-
tems from one domain or task to another, aiming at
reducing at least in part the cost associated with the
creation of human annotated datasets. Automatic
system adaptation can take different forms: if high
?This work is partially supported by Ministerio de Econom??a
y Competitividad, Secretar??a de Estado de Investigacio?n, De-
sarrollo e Innovacio?n, Spain under project number TIN2012-
38584-C06-03 and Advanced Research Fellowship RYC-2009-
04291. We thank Biljana Drndarevic? for proofreading the paper.
quality human annotated data is available, then rule-
based or statistical systems can be trained on this
data (Brill, 1994), reducing the efforts of writing
rules and handcrafting dictionaries. If high quality
human annotated data is unavailable, a large non-
annotated corpus and a bootstrapping procedure can
be used to produce annotated data (Ciravegna and
Wilks, 2003; Yangarber, 2003). Here, we concen-
trate on developing and evaluating automatic proce-
dures to learn the main concepts of a domain and
at the same time auto-annotate texts so that they be-
come available for training information extraction or
text summarization applications. However, it would
be naive to think that in the current state of the art we
would be able to learn all knowledge from text au-
tomatically (Poon and Domingos, 2010; Biemann,
2005; Buitelaar and Magnini, 2005). We therefore
here concentrate on learning template-like represen-
tations from concise event summaries which should
contain the key information of an event.
18 de julio de 1994DateOfAttack . Un atentado
contra la sede de la Asociacio?n Mutual Israelita
ArgentinaTarget de Buenos AiresPlaceOfAttack causa la
muerte de 86NumberOfVictims personas.
(18th July 1994. An attack against the headquarters
of the Jewish Mutual Association in Buenos Aires, Ar-
gentina, kills 86 people.)
Figure 1: Sample of Human Annotated Summary in
Spanish
An example of the summaries we want to learn
from is presented in Figure 1. It is a summary in
the terrorist attack domain in Spanish. It has been
270
manually annotated with concepts such as DateO-
fAttack, Target, PlaceOfAttack, and NumberOfVic-
tims, which are key in the domain. Our task is to
discover from this kind of summary what the con-
cepts are and how to recognise them automatically.
As will be shown in this paper and unlike current
approaches (Chambers and Jurafsky, 2011; Leung et
al., 2011), the methods to be presented here do not
require parsing or semantic dictionaries to work or
specification of the underlying number of concepts
in the domain to be learn. The approach we take
learns concepts in the set of domain summaries, re-
lying on noun phrase contextual information. They
are able to generate reasonable domain conceptual-
izations from relatively small datasets and in differ-
ent languages.
The rest of the paper is structured as follows: In
Section 2 we overview related work in the area of
concept induction from text. Next, in Section 3 we
describe the dataset used and how we have processed
it while in Section 4 we outline the two unsuper-
vised learning algorithms we compare in this paper
for template induction from text. Then, in Section 5,
we describe the experiments on template induction
indicating how we have instantiated the algorithms
and in Section 6 we explain how we have extrinsi-
cally evaluated the induction process. In Section 7
we discuss the obtained results and in Section 8 we
summarize our findings and close the paper.
2 Related Work
A long standing issue in natural language process-
ing is how to learn conceptualizations from text in
automatic or semi-automatic ways. The availabil-
ity of redundant data has been used, for example,
to discover template-like representations (Barzilay
and Lee, 2003) or sentence-level paraphrases which
could be used for extraction or generation. Vari-
ous approaches to concept learning use clustering
techniques. (Leung et al, 2011) apply various clus-
tering procedures to learn a small number of slots
in three typical information extraction domains, us-
ing manually annotated data and fixing the num-
ber of concepts to be learnt. (Li et al, 2010) gen-
erate templates and extraction patterns for specific
entity types (actors, companies, etc.). (Chambers
and Jurafsky, 2011) learn the structure of MUC tem-
plates from raw data in English, an approach that
needs both full parsing and semantic interpretation
using WordNet (Fellbaum, 1998) in order to extract
verb arguments and measure the similarity betweern
verbs. In (Saggion, 2012) an iterative learning pro-
cedure is used to discover core domain conceptual
information from short summaries in two languages.
However, the obtained results were not assessed in a
real information extraction scenario. There are ap-
proaches which do not need any human interven-
tion or sophisticated text processing, but learn based
on redundancy of the input dataset and some well
grounded linguistic intuitions (Banko and Etzioni,
2008; Etzioni et al, 2004). Related to the work pre-
sented here are approaches that aim at generating
short stereotypical summaries (DeJong, 1982; Paice
and Jones, 1993; Ratnaparkhi, 2000; Saggion and
Lapalme, 2002; Konstas and Lapata, 2012).
3 Dataset and Text Processing Steps
For the experiments reported here we rely on the
CONCISUS corpus1 (Saggion and Szasz, 2012)
which is distributed free of charge. It is a corpus
of Web summaries in Spanish and English in four
different application domains: Aviation Accidents
(32 English, 32 Spanish), Earthquakes (44 English,
56 Spanish), Train Accidents (36 English, 43 Span-
ish), and Terrorist Attacks (42 English, 53 Spanish).
The dataset contains original and comparable sum-
mary pairs, automatic translations of Spanish sum-
maries into English, automatic translation of English
summaries into Spanish, and associated original full
documents in Spanish and English for two of the do-
mains (Aviation Accidents and Earthquakes). The
dataset comes with human annotations representing
the key information in each domain. In Table 1
we detail the concepts used in each of the domains.
Note that not all concepts are represented in each
of the summaries. Creation of such a dataset can
take up to 500 hours for a human annotator, con-
sidering data collection, cleansing, and annotation
proper. Only one human annotator and one curator
were responsible for the annotation process.
1http://www.taln.upf.edu/pages/concisus/.
271
Aviation Accident Airline, Cause, DateOfAccident, Destination, FlightNumber, NumberOfVictims, Origin, Passengers, Place, Sur-
vivors, Crew, TypeOfAccident, TypeOfAircraft, Year
Earthquake City, Country, DateOfEarthquake, Depth, Duration, Epicentre, Fatalities, Homeless, Injured, Magnitude, Other-
PlacesAffected, Province, Region, Survivors, TimeOfEarthquake
Terrorist Attack City, Country, DateOfAccident, Fatalities, Injured, Target, Perpetrator, Place, NumberOfVictims, TypeOfAttack
Train Accident Cause, DateOfAccident, Destination, NumberOfVictims, Origin, Passenger, Place, Survivors, TypeOfAccident,
TypeOfTrain
Table 1: Conceptual Information in Summaries
3.1 Text Processing
In order to carry out experimentation we adopt the
GATE infrastructure for document representation
and annotation (Maynard et al, 2002). All doc-
uments in the dataset are processed with available
natural language processors to compute shallow lin-
gustic information. Documents in English are pro-
cessed with the ANNIE system, a morphological an-
alyzer, and a noun chunker, all three from GATE.
The documents in Spanish are analyzed with Tree-
Tagger (Schmid, 1995), a rule-base noun chunker,
and an SVM-based named entity recognition and
classification system.
4 Concept Induction Algorithms
Two algorithms are used to induce conceptual in-
formation in a domain from a set of textual sum-
maries. The algorithms form concepts based on tar-
get strings (or chunks) in the set of summaries us-
ing token-level linguistic information. The chunks
are represented with different features which are ex-
plained later in Section 5.1. One algorithm we use
is based on clustering, while the other is based on
iterative learning.
4.1 Clustering-based Induction
The procedure for learning conceptual information
by clustering is straithforward: the chunks in the set
of summaries are represented as instances consider-
ing both internal and sourrounding linguistic infor-
mation. These instances are the input to a clustering
procedure which returns a list of clusters each con-
taining a set of chunks. We consider each cluster as
a key concept in the set of domain summaries and
the chunks in each cluster as the concept extension.
4.2 Iterative Induction
We use the iterative learning algorithm described in
(Saggion, 2012) which learns from a set of sum-
maries S, also annotated with target strings (e.g.
chunks) and shallow linguistic information. In a nut-
shell the algorithm is as follows:
(1) Choose a document D from the set of summaries S
and add it to a training set TRAIN. Set REST to
S ? TRAIN.
(2) Choose an available target concept T from D, i.e. a
target concept not tried before by the algorithm.
(3) Train a classifier on TRAIN to learn instances of
the target concept using the available linguistic fea-
tures; the classifier uses the linguistic information
provided.
(4) Apply the classifier to REST (all summaries minus
those in TRAIN) to annotate all instances of the tar-
get concept T .
(5) Select a document BEST in REST, where there is an
instance of the concept recognised with the highest
probability in the REST set.
(6) Remove BEST from REST and add BEST to the
training set, remove all identified instances of T
from REST, and go to step 3.
The algorithm is executed a number of times (see
Section 5.1 for parametrization of the algorithms)
to learn all concepts in the set of summaries, and
at each iteration a single concept is formed. There
are two circumstances when a concept being formed
is discarded and their associated initial target con-
cept removed from the learning process: one case is
when there are not enough occurrences of the con-
cept across a set of summaries; another case is when
too many identical strings are proposed as instances
for the concept in the set of summaries. This latter
restriction is only valid if we consider sets of non-
redundant documents, which is the case to which we
restrict our experiments.
4.3 Text Chunks
Given that the algorithms presented above try to in-
duce a concept from the chunks in the summaries,
272
we are interested in assessing how the type of chunk
influences the learning process. Also, given that our
objective is to test methods which learn with mini-
mal human intervention, we are interested in inves-
tigating differences between the use of manual and
automatic chunks. We therefore use the following
chunk types in this work: gold chunks (gold) are the
human produced annotations (as in Figure 1); named
entity chunks (ne) are named entities computed by an
off-the-shelf named entity recognizer; noun chunks
(nc) are text chunks identified by rule-based off-the-
shelf NP chunkers and finally, wiki chunks (wiki) are
strings of text in the summaries which happen to be
Wikipedia titles.
In order to automatically compute these chunk
types, different levels of knowledge are needed.
For example, NP chunks require syntactic infor-
mation, while named entities and wiki chunks re-
quire some external form of knowledge, such as
precompiled gazetteer lists or access to an ency-
clop?dia or a semantic dictionary. Named enti-
ties and noun chunks are computed as described in
Section 3, while wiki chunks are computed as fol-
lows: string n-grams w1w2...wn are computed in
each summary and strings w1 w2 ... wn are checked
against the Wikipedia on-line encyclop?dia, if a
hit occurs (i.e. if for an English n-gram the page
en.wikipedia.org/wiki/w1... wn exists or for a Span-
ish n-gram the page es.wikipedia.org/wiki/w1... wn
exists), the n-gram is annotated in the summary as a
wiki chunk. Wiki chunks are cached to speed up the
automatic annotation process.
Spanish
P R F
Terrorist Attack 0.47 0.10 0.17
Aviation Accident 0.52 0.08 0.14
Earthquake 0.24 0.06 0.10
Train Accident 0.59 0.15 0.24
English
P R F
Terrorist Attack 0.46 0.39 0.42
Aviation Accident 0.40 0.27 0.32
Earthquake 0.27 0.22 0.24
Train Accident 0.57 0.27 0.36
Table 2: Baseline Induction Performance
4.4 Mapping the Induced Concepts onto
Human Concepts
For evaluation purposes, each induced concept is
mapped onto one human concept applying the fol-
lowing procedure: let HCi be the set of summary
offsets where human concept i occurs, and let ICi be
the set of summary offsets where automatic concept
i occurs, then the induced concept j is mapped onto
concept k such that: k = argmaxi(|HCi ? ICj |),
where |X| is the size of set X . That is, the in-
duced concept is mapped onto the label it gives it
a best match. As an example, one induced concept
in the terrorist attack domain containing the follow-
ing string instances: two bombs, car bomb, pair of
bombs, 10 coordinated shooting and bombing, two
car bombs, suicide bomb, the attack, guerrilla war-
fare, the coca-growing regions, etc. This induced
concept is mapped onto the TypeOfAttack human
concept in that domain.
4.5 Baseline Concept Induction
A baseline induction mechanism is designed for
comparison with the two learning procedures pro-
posed here. It is based on the mapping of named en-
tity chunks onto concepts in a straightforward way:
each named entity type is considered a different con-
cept and therefore mapped onto human concepts as
in Section 4.4. For example, in the terrorist attack
domain, Organization named entity type is mapped
by this procedure onto the human concept Target
(i.e. churches, government buildings, etc., are com-
mon targets in terrorist attacks) while in the Avia-
tion Accident domain the Organization named en-
tity type is mapped onto TypeOfAircraft (i.e. Boe-
ing, Airbus, etc. are names of organizations).
5 Experimental Setting and Results of the
Induction Process
In this section we detail the different parameters
used by the algorithms and report the performance
of the induction process with different inputs.
5.1 Settings
The features used by the induction procedure are ex-
tracted from the text tokens. We extract the POS tag,
root, and string of each token. The clustering-based
algorithm uses a standard Expectation Maximization
273
Spanish
Iterative Clustering
P R F P R F
Terrorist Attack 0.25 0.59 0.35 0.59 0.59 0.59?
Aviation Accident 0.50 0.62 0.55 0.66 0.66 0.66?
Earthquake 0.34 0.51 0.41 0.56 0.53 0.55?
Train Accident 0.41 0.69 0.52 0.58 0.58 0.58
English
Iterative Clustering
P R F P R F
Terrorist Attack 0.23 0.39 0.29 0.50 0.50 0.50?
Aviation Accident 0.57 0.68 0.62 0.79 0.79 0.79?
Earthquake 0.26 0.53 0.34 0.39 0.39 0.39
Train Accident 0.50 0.59 0.54 0.61 0.61 0.61?
Table 3: Conceptual induction (Spanish and English) Using Gold
Chunks for Learning
implementation from the Weka machine learning li-
brary (Witten and Frank, 1999). We instruct the al-
gorithm to decide on the number of clusters based on
the data, instead of setting the number of clusters by
hand. The instances to cluster are representations of
the input chunks; these representations contain the
internal features of the chunks, as well as the infor-
mation of 5 tokens to the left of the beginning of
the chunk and 5 tokens to the right of the end of the
chunk. The transformation from GATE documents
into arff Weka files and the mapping from Weka onto
the GATE documents, is carried out using specific
programs. The classification algorithm used for the
iterative learning process is an SVM classifier dis-
tributed with the GATE system and tuned to per-
form chunk learning using the same features as the
clustering procedure (Li et al, 2004). This classifier
outputs a probability which we use for selecting the
best document at step (5) of the iterative procedure.
The document selected to start the process is the one
with more target strings, and the target string chosen
is the next available in textual order. The iterative
learning procedure is set to stop when the number
of concepts induced reaches the average number of
chunks in the corpus. Induced concepts not covering
at least 10% of the number of documents are dis-
carded, as are concepts with strings repeated at least
10% of the concept extension.
5.2 Experiments and Results
We carry out a number of experiments per domain
where we run the algorithms using as input the sum-
maries annotated with a different chunk type each
time. After each experiment all concepts induced are
Terrorist Attacks
Iterative Clustering
P R F P R F
nc 0.22 0.53 0.31? 0.15 0.51 0.23
ne 0.27 0.14 0.18 0.12 0.42 0.18
wiki 0.15 0.26 0.19 0.22 0.18 0.20
all 0.25 0.53 0.34? 0.12 0.51 0.20
Aviation Accidents
Iterative Clustering
P R F P R F
nc 0.30 0.50 0.38? 0.21 0.51 0.30
ne 0.84 0.07 0.14 0.57 0.07 0.13
wiki 0.29 0.28 0.28? 0.27 0.17 0.21
all 0.39 0.62 0.48? 0.16 0.31 0.21
Earthquakes
Iterative Clustering
P R F P R F
nc 0.29 0.42 0.34? 0.14 0.42 0.21
ne 0.20 0.19 0.20? 0.38 0.02 0.05
wiki 0.16 0.16 0.16 0.24 0.11 0.15
all 0.28 0.50 0.36? 0.12 0.46 0.19
Train Accidents
Iterative Clustering
P R F P R F
nc 0.36 0.66 0.47? 0.23 0.51 0.32
ne 0.33 0.66 0.44? 0.65 0.12 0.20
wiki 0.25 0.25 0.25 0.51 0.13 0.21
all 0.33 0.62 0.44? 0.16 0.50 0.24
Table 4: Comparison of conceptual induction in Spanish
mapped onto the human concepts (see Section 4.4)
producing auto-annotated summaries. The auto-
matic annotations are then compared with the gold
annotations, and precision, recall, and f-score fig-
ures are computed to observe the performance of the
two algorithms, the baseline, and the effect of type
of chunk on the learning process.
In Table 2 we report baseline performance on the
entire dataset. As can be appreciated by the obtained
numbers, directly mapping named entity types onto
concepts does not provide a very good performance,
especially for Spanish; we expected the learning
procedures to produce better results. In Table 3 we
present the results of inducing concepts from the
gold chunks by the two algorithms. In almost all
cases, using gold chunks improves over the baseline
procedure, except for the Terrorist Attack domain
in English, where the iterative learning procedure
underperforms the baseline. In all tested domains,
the clustering-based induction procedure has a very
competitive performance. A t-test is run to verify
differences in performance between the two systems
in terms of f-score. In all tested domains in Span-
ish, except the Train Accident domain, there are sta-
274
Terrorist Attacks
Iterative Clustering
P R F P R F
nc 0.43 0.50 0.46? 0.23 0.42 0.30
ne 0.28 0.44 0.34 0.42 0.29 0.34
wiki 0.24 0.33 0.28? 0.15 0.25 0.19
all 0.31 0.49 0.38? 0.09 0.39 0.15
Aviation Accidents
Iterative Clustering
P R F P R F
nc 0.48 0.31 0.38 0.33 0.34 0.34
ne 0.53 0.38 0.44? 0.63 0.27 0.38
wiki 0.31 0.44 0.36? 0.28 0.37 0.32
all 0.50 0.67 0.58? 0.15 0.47 0.23
Earthquakes
Iterative Clustering
P R F P R F
nc 0.29 0.48 0.36? 0.06 0.40 0.10
ne 0.28 0.34 0.30 0.30 0.25 0.28
wiki 0.21 0.30 0.25? 0.16 0.23 0.19
all 0.31 0.44 0.37? 0.08 0.40 0.13
Train Accidents
Iterative Clustering
P R F P R F
nc 0.45 0.54 0.49? 0.32 0.50 0.39
ne 0.47 0.29 0.36 0.58 0.27 0.36
wiki 0.51 0.32 0.39? 0.30 0.29 0.29
all 0.50 0.58 0.53? 0.16 0.49 0.24
Table 5: Comparison of conceptual induction in English
Spanish
P R F
Aviation Accident 0.83 0.60 0.70
Earthquake 0.61 0.48 0.53
Train Accident 0.77 0.54 0.64
English
P R F
Aviation Accident 0.88 0.38 0.53
Earthquake 0.86 0.56 0.68
Train Accident 0.84 0.43 0.57
Table 6: Cross-lingual Information Extraction. System Trained with
Gold Summaries.
tistically significant differences between the cluster-
ing procedure and the iterative learning procedure
(p = 0.01). In all tested domains in English, except
for the Earthquake domain, there are statistically
significant differences between the performance of
clustering and iterative learning (p = 0.01).
Now we turn to the results of both algorithms
when automatic chunks are used, that is, when no
human annotation is provided to the learners. Re-
sults are reported in Tables 4 (Spanish) and 5 (En-
glish). The results are presented by the chunk type
used during the learning procedure. In addition
to the chunk types specified above, we include a
type all, which represents the use of all automat-
Aviation Accidents
Iterative Clustering
P R F P R F
gold 0.85 0.52 0.65? 0.84 0.41 0.55
all 0.88 0.49 0.63? 0.87 0.19 0.32
nc 0.87 0.46 0.60 0.88 0.46 0.60
Earthquakes
Iterative Clustering
P R F P R F
gold 0.65 0.41 0.50? 0.66 0.31 0.43
all 0.64 0.36 0.46 0.62 0.40 0.49
nc 0.63 0.33 0.43 0.67 0.38 0.49
Train Accidents
Iterative Clustering
P R F P R F
gold 0.81 0.54 0.65 0.82 0.52 0.64
all 0.81 0.52 0.64? 0.72 0.31 0.43
nc 0.79 0.54 0.64? 0.79 0.42 0.55
Table 7: Cross-lingual Information Extraction Results in Spanish
Translations. System trained with auto-annotated summaries in Span-
ish.
Aviation Accidents
Iterative Clustering
P R F P R F
gold 0.87 0.35 0.50 0.87 0.37 0.52
all 0.87 0.37 0.52? 0.82 0.18 0.29
nc 0.90 0.21 0.34? 0.90 0.17 0.29
Earthquakes
Iterative Clustering
P R F P R F
gold 0.87 0.53 0.66? 0.87 0.36 0.51
all 0.88 0.51 0.64? 0.87 0.30 0.45
nc 0.88 0.51 0.65? 0.93 0.43 0.59
Train Accidents
Iterative Clustering
P R F P R F
gold 0.82 0.30 0.44 0.87 0.32 0.47
all 0.84 0.39 0.53? 0.91 0.24 0.38
nc 0.89 0.36 0.51? 0.46 0.25 0.32
Table 8: Cross-lingual Information Extraction Results in English
Translations. System trained with auto-annotated summaries in English.
ically computed chunks (i.e. nc, ne, wiki). We
observe that, in general, when presented with au-
tomatic chuks, the iterative learning procedure is
able to induce concepts with a better f-score than
the clustering-based algorithm. A t-test is run to
verify differences between the two induction pro-
cedures within each chunk condition (differences
shown with a ? in the tables). In 11 out of 16 cases
in Spanish and in 12 out of 16 cases in English,
statistically significant differences are observed. In
three out of four domains the combination of au-
tomatic chunks outperforms the use of individual
chunk types. Generally, named entity chunks and
wiki chunks have the lowest performance. This is
275
Spanish
P R F
Aviation Accident 0.56 0.47 0.51
Earthquake 0.64 0.41 0.50
English
P R F
Aviation Accident 0.61 0.35 0.44
Earthquake 0.78 0.41 0.54
Table 9: Extraction from Full Documents. System Trained on Gold
Summaries.
Aviation Accidents
Iterative Clustering
P R F P R F
gold 0.55 0.37 0.44 0.54 0.31 0.39
all 0.55 0.36 0.43? 0.69 0.17 0.27
nc 0.45 0.22 0.30? 0.52 0.26 0.35
Earthquake
Iterative Clustering
P R F P R F
gold 0.62 0.31 0.41? 0.63 0.22 0.33
all 0.61 0.26 0.37 0.63 0.31 0.41?
nc 0.60 0.24 0.35 0.70 0.28 0.40?
Table 10: Full-text Information Extraction Results in Spanish. Sys-
tem trained with auto-annotated summaries in Spanish.
not an unexpected result since named entities, for
example, cover much fewer strings which may form
part of a concept extension. Additionally, off-the-
shelf entity recogizers only identify a limited num-
ber of entity types.
6 Information Extraction Evaluation
Framework
The numbers above are interesting because they pro-
vide intrinsic evaluation of the concept induction
procedure, but they do not tell us much about their
usability. Therefore, and in order to better assess
the value of the discovered concepts, we decided to
carry out two extrinsic evaluations using an informa-
tion extraction task. Once the conceps are induced
and, as a result, the summaries are auto-annotated
with domain specific concepts, we decide to train
an off-the-shelf SVM token classification procedure
and apply it to unseen human annotated documents.
The SVM classifier uses the same linguistic infor-
mation as the induction procedures: token level in-
formation and a window size of 5 around each token
to be classified.
Aviation Accidents
Iterative Clustering
P R F P R F
gold 0.60 0.28 0.39 0.62 0.31 0.41?
all 0.62 0.30 0.41? 0.54 0.14 0.23
nc 0.53 0.15 0.23? 0.46 0.10 0.16
Earthquake
Iterative Clustering
P R F P R F
gold 0.70 0.35 0.47? 0.72 0.32 0.44
all 0.74 0.37 0.49 0.70 0.22 0.34
nc 0.73 0.36 0.48? 0.73 0.30 0.42
Table 11: Full-text Information Extraction Results in English. Sys-
tem trained with auto-annotated summaries in English.
6.1 Extraction from Automatic Translations
The first task we carry out is cross-lingual informa-
tion extraction where the input documents are auto-
matic translations of summaries in Spanish and En-
glish2. Note that the expriment is performed in three
domains for which such translations are manually
annotated. We first run an experiment to assess the
extraction performance of the SVM when trained on
human annotated data. Results of the experiment
are reported in Table 6 and they should be taken
as an upperbound of the performance of a system
trained on auto-annotated summaries. We then train
the SVM on the different auto-annotated datasets,
but note that due to space restrictions, we here only
report the three most revealing experiments per lan-
guage: concepts induced with gold chunks, noun
chunks, and all automatic chunks. Results are re-
ported in Table 7 (Spanish) and in Table 8 (English).
In most cases the SVM trained with auto-annotated
summaries produced by the iterative learning proce-
dure outperforms the clustering-based method with
statistically significant differences (? shown in the
tables) (p = 0.01).
6.2 Extraction from Full Documents
The second and the last evaluation consists in the ap-
plication of the SVM extraction system to full doc-
uments. In this case, the experiment can be run only
in two domains for which full documents have been
provided and manually annotated. We first test the
performance of the system when trained on human
annotated summaries and present the results in Ta-
ble 9. Results of the experiments when the system
is trained on auto-annotated datasets are shown in
2The translations were produced by Google translator.
276
Tables 10 (Spanish) and 11 (English). Results are
lower than when training on clean human annotated
summaries. It is unclear which approach is more
competitive when training with auto-annotated sum-
maries. What is clear is that the performance of
the iterative learning algorithm when training with
concepts induced from gold chunks is not statisti-
cally different (according to a t-test and p = 0.01)
from the performance of the algorithm when training
with concepts induced from automatically computed
chunks. We consider this to be a positive outcome of
the experiments.
7 Discussion
The two methods presented here are able to produce
partial domain conceptualizations from a relatively
small set of domain summaries3. We have found
that the clustering-based procedure is very competi-
tive when presented with gold chunks. On the other
hand, the iterative learning procedure performs very
well when presented with automatic chunks in all
tested domains and the two languages. We have also
found that the performance of the iterative induction
system is not much affected by the use of automati-
cally computed chunks. We have run a t-test to ver-
ify the differences in induction performance when
learning with gold and automatic chunks (all con-
dition) and have found statistically significant dif-
ferences in only one domain out of four in Spanish
(Terrorist Attack) and in two domains out of four
in English (Aviation Accident and Train Accident)
(p = 0.01). The applicability of the induction pro-
cess, that is, if the auto-annotated data could be used
for specific tasks, has been tested in two information
extraction experiments. In a cross-lingual informa-
tion extraction setting (Riloff et al, 2002; Saggion
and Szasz, 2011) we have observed that a system
trained on automatically computed chunks has a per-
formance close to one trained on concepts induced
from gold chunks. No statistically significant differ-
ences exist (p = 0.01) between the use of automatic
chunks and gold chunks, except for the Train Acci-
dent domain in English, where the system trained
on fully automatically annotated summaries has a
better performance. In a full document information
3Depending on the language and domain, between 50% and
77% of all concepts are generated.
extraction task, although the best system trained on
auto-annotated summaries in Spanish has a big dif-
ference with respect to a system trained on human-
annotated summaries, in English the differences are
slight. We belive that this is due to the differences
in performance between the underlying text process-
ing components. Our methods work by grouping to-
gether sets of chunks, unlike (Chambers and Juraf-
sky, 2011), whose approach is centered around verb
arguments and clustering, and relies on the avail-
ability of considerable amounts of data. Ontology
learning approaches such as OntoUSP (Poon and
Domingos, 2010) are also clustering-based but fo-
cus on learning is-a relations only. Unlike (Leung et
al., 2011) whose approach is based on gold-standard
humman annotations, we here test the performance
of the induction process using automatically com-
puted candidate strings, and we additionally learn
the number of concepts automatically.
8 Conclusions and Future Work
In this paper we have concentrated on the prob-
lem of knowledge induction from text summaries.
The approaches we have presented are fully unsu-
pervised and are able to produce reasonable con-
ceptualizations (close to human concepts) without
relying on annotated data. Unlike previous work,
our approach does not require full syntactic parsing
or a semantic dictionary. In fact, it only requires
a process of text chunking and named entity recog-
nition, which we have carefully assessed here. We
believe our work contributes with a viable method-
ology to induce conceptual information from texts,
and at the same time with an auto-annotation mech-
anism which could be used to train information ex-
traction systems. Since our procedure requires very
little linguistic information, we believe it can be suc-
cessfully applied to a number of languages. We also
believe that there is much work to be carried out and
that induction from summaries should be comple-
mented with a process that explores full event re-
ports, in order to reinforce some induced concepts,
discard others, and discover additional ones.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
277
Proceedings of ACL-08, pages 28?36. Association for
Computational Linguistics, June.
R. Barzilay and L. Lee. 2003. Learning to paraphrase: an
unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, NAACL ?03, pages 16?23, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Chris Biemann. 2005. Ontology Learning from Text: A
Survey of Methods. LDV Forum, 20(2):75?93.
E. Brill. 1994. Some Advances in Transformation-Based
Part of Speech Tagging. In Proceedings of the Twelfth
National Conference on AI (AAAI-94), Seattle, Wash-
ington.
P. Buitelaar and B. Magnini. 2005. Ontology learning
from text: An overview. In In Paul Buitelaar, P., Cimi-
ano, P., Magnini B. (Eds.), Ontology Learning from
Text: Methods, Applications and Evaluation, pages 3?
12. IOS Press.
N. Chambers and D. Jurafsky. 2011. Template-Based In-
formation Extraction without the Templates. In ACL,
pages 976?986.
Fabio Ciravegna and Yorick Wilks. 2003. Designing
adaptive information extraction for the semantic web
in amilcare. In Annotation for the Semantic Web,
Frontiers in Artificial Intelligence and Applications.
IOS. Press.
Gerald DeJong. 1982. An Overview of the FRUMP Sys-
tem. In W.G. Lehnert and M.H. Ringle, editors, Strate-
gies for Natural Language Processing, pages 149?
176. Lawrence Erlbaum Associates, Publishers.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2004. Methods for Domain-Indepedent Information
Extraction from the Web: An Experimental Compari-
son. In Proceedings of AAAI-2004.
Christiane Fellbaum, editor. 1998. WordNet - An Elec-
tronic Lexical Database. MIT Press.
I. Konstas and M. Lapata. 2012. Concept-to-text gener-
ation via discriminative reranking. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, pages 369?378, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Cane Wing-ki Leung, Jing Jiang, Kian Ming A. Chai,
Hai Leong Chieu, and Loo-Nin Teow. 2011. Unsuper-
vised information extraction with distributional prior
knowledge. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 814?824, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Y. Li, K. Bontcheva, and H. Cunningham. 2004. An
SVM Based Learning Algorithm for Information Ex-
traction. Machine Learning Workshop, Sheffield.
P. Li, J. Jiang, and Y. Wang. 2010. Generating Templates
of Entity Summaries with an Entity-Aspect Model and
Pattern Mining. In Proceedings of ACL, Uppsala.
ACL.
D. Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Sag-
gion, K. Bontcheva, and Y. Wilks. 2002. Architectural
Elements of Language Engineering Robustness. Jour-
nal of Natural Language Engineering ? Special Issue
on Robust Methods in Analysis of Natural Language
Data, 8(2/3):257?274.
Chris D. Paice and Paul A. Jones. 1993. The Identi-
fication of Important Concepts in Highly Structured
Technical Papers. In R. Korfhage, E. Rasmussen, and
P. Willett, editors, Proc. of the 16th ACM-SIGIR Con-
ference, pages 69?78.
J. Piskorski and R. Yangarber. 2013. Information ex-
traction: Past, present and future. In Thierry Poibeau,
Horacio Saggion, Jakub Piskorski, and Roman Yan-
garber, editors, Multi-source, Multilingual Informa-
tion Extraction and Summarization, Theory and Ap-
plications of Natural Language Processing, pages 23?
49. Springer Berlin Heidelberg.
H. Poon and P. Domingos. 2010. Unsupervised ontology
induction from text. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 296?305, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, NAACL 2000,
pages 194?201, Stroudsburg, PA, USA. Association
for Computational Linguistics.
E. Riloff, C. Schafer, and D. Yarowsky. 2002. Induc-
ing information extraction systems for new languages
via cross-language projection. In Proceedings of the
19th international conference on Computational lin-
guistics, pages 1?7, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
E. Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. Proceedings of the
Eleventh Annual Conference on Artificial Intelligence,
pages 811?816.
E. Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. Proceedings of the Thir-
teenth Annual Conference on Artificial Intelligence,
pages 1044?1049.
H. Saggion and G. Lapalme. 2002. Generat-
ing Indicative-Informative Summaries with SumUM.
Computational Linguistics.
278
H. Saggion and T. Poibeau. 2013. Automatic text
summarization: Past, present and future. In Thierry
Poibeau, Horacio Saggion, Jakub Piskorski, and Ro-
man Yangarber, editors, Multi-source, Multilingual In-
formation Extraction and Summarization, Theory and
Applications of Natural Language Processing, pages
3?21. Springer Berlin Heidelberg.
H. Saggion and S. Szasz. 2011. Multi-domain Cross-
lingual Information Extraction from Clean and Noisy
Texts. In Proceedings of the 8th Brazilian Sympo-
sium in Information and Human Language Technol-
ogy, Cuiaba?, Brazil. BCS.
H. Saggion and S. Szasz. 2012. The CONCISUS Corpus
of Event Summaries. In Proceedings of the 8th Lan-
guage Resources and Evaluation Conference (LREC),
Istanbul, Turkey. ELDA.
H. Saggion. 2012. Unsupervised content discovery
from concise summaries. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction, AKBC-
WEKEX ?12, pages 13?18, Stroudsburg, PA, USA.
Association for Computational Linguistics.
H. Schmid. 1995. Improvements In Part-of-Speech Tag-
ging With an Application To German. In In Proceed-
ings of the ACL SIGDAT-Workshop, pages 47?50.
I. H. Witten and E. Frank. 1999. Data Mining: Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann.
R. Yangarber. 2003. Counter-Training in Discovery of
Semantic Patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL?03).
279
Colouring Summaries BLEU
Katerina Pastra
Department of Computer Science
University of Sheffield
katerina@dcs.shef.ac.uk
Horacio Saggion
Department of Computing Science
University of Sheffield
saggion@dcs.shef.ac.uk
Abstract
In this paper we attempt to apply the
IBM algorithm, BLEU, to the output of
four different summarizers in order to
perform an intrinsic evaluation of their
output. The objective of this experiment
is to explore whether a metric, originally
developed for the evaluation of machine
translation output, could be used for as-
sessing another type of output reliably.
Changing the type of text to be evalu-
ated by BLEU into automatically gener-
ated extracts and setting the conditions
and parameters of the evaluation exper-
iment according to the idiosyncrasies
of the task, we put the feasibility of
porting BLEU in different Natural Lan-
guage Processing research areas under
test. Furthermore, some important con-
clusions relevant to the resources needed
for evaluating summaries have come up
as a side-effect of running the whole ex-
periment.
1 Introduction
Machine Translation and Automatic Summariza-
tion are two very different Natural Language Pro-
cessing (NLP) tasks with -among others- differ-
ent implementation needs and goals. They both
aim at generating text; however, the properties and
characteristics of these target texts vary consider-
ably. Simply put, in Machine Translation, the gen-
erated document should be an accurate and fluent
translation of the original document, in the target
language. In Summarization, the generated text
should be an informative, reduced version of the
original document (single-document summary), or
sets of documents (multi-document summary) in
the form of an abstract, or an extract. Abstracts
present an overview of the main points expressed
in the original document, while extracts consist of
a number of informative sentences taken directly
from the source document. The fact that, by their
very nature, automatically generated extracts carry
the single sentence qualities of the source docu-
ments1, may lead one to the conclusion that eval-
uating this type of text is trivial, as compared to
the evaluation of abstracts or even machine trans-
lation, since in the latter, one needs to be able to
evaluate the content of the generated translation in
terms of grammaticality, semantic equivalence to
the source document and other quality character-
istics (Hovy et al, 2002).
Though the evaluation of generated extracts is
not as demanding as the evaluation of Machine
Translation, it does have two critical idiosyncratic
aspects that render the evaluation task difficult:
? the compression level (word or sentence
level) and the compression rate of the source
document must be determined for the selec-
tion of the contents of the extract ; the val-
ues of these variables may greatly affect the
whole evaluation setup and the results ob-
tained
1Even if coherence issues may arise beyond the sentence
boundaries i.e. at the text level
? the very low agreement among human eval-
uators on what is considered to be ?impor-
tant information? for inclusion in the extract,
reaching sometimes the point of total dis-
agreement on the focus of the extract (Mani,
2001; Mani et al, 2001). The nature of this
disagreement on the adequacy of the extracts
is such that - by definition - cannot manifest
itself in Machine Translation; this is because
it refers to the adequacy of the contents cho-
sen to form the extract, rather than what con-
stitutes an adequate way of expressing all the
contents of the source document in a target
language.
The difference on the parameters to be taken
into consideration when performing evaluation
within these two NLP tasks presents a challenge
for porting evaluation metrics from the one re-
search area to the other. Given the relatively re-
cent success in achieving high correlations with
human judgement for Machine Translation evalua-
tion, using the IBM content-based evaluation met-
ric, BLEU (Papineni et al, 2001), we attempt to
run this same metric on system generated extracts;
this way we explore whether BLEU can be used
reliably in this research area and if so, which test-
ing parameters need to be taken into considera-
tion. First, we refer briefly to BLEU and its use
across different NLP areas, then we locate our ex-
periments relatively to this related work and we
describe the resources we used, the tools we de-
veloped and the parameters we set for running the
experiments. The description of these experiments
and the interpretation of the results follows. The
paper concludes with some preliminary observa-
tions we make as a result of this restricted, first
experimentation.
2 Using BLEU in NLP
Being an intrinsic evaluation measure
(Sparck Jones and Galliers, 1995), BLEU
compares the content of a machine translation
against an ?ideal? translation. It is based on
a ?weighted average of similar length phrase
matches? (n-grams), it is sensitive to longer
n-grams (the baseline being the use of up to 4-
grams) and it also includes a brevity penalty factor
for penalising shorter than the ?gold standard?
translations (Papineni et al, 2001; Doddington,
2002). The metric has been found to highly
correlate with human judgement, being at the
same time reliable even when run on different
documents and against different number of model
references. Experiments run by NIST (Dodding-
ton, 2002), checking the metric for consistency
and sensitivity, verified these findings and showed
that the metric distinguishes, indeed, between
quite similar systems. A slightly different version
of BLEU has been suggested by the same people,
which still needs to be put into comparative testing
with BLEU before any claims for its performance
are made.
BLEU has been used for evaluating different
types of NLP output to a small extent. In (Za-
jic et al, 2002), the algorithm has been used in
a specific Natural Language Generation applica-
tion: headline generation. The purpose of this
work was to use an automated metric for evalu-
ating a system generated headline against a hu-
man generated one, in order to draw conclusions
on the parameters that affect the performance of
a system and improve scoring similarity. In (Lin
and Hovy, 2002) BLEU has been applied on sum-
marization. The authors argue on the unstable
and unreliable nature of manual evaluation and
the low agreement among humans on the con-
tents of a reference summary. Lin and Hovy make
the case that automated metrics are necessary and
test their own modified recall metric, along with
BLEU itself, on single and multi-document sum-
maries and compare the results with human judge-
ment. Modified recall seems to reach very high
correlation scores, though direct comparative ex-
perimentation is needed for drawing conclusions
on its performance in relation to BLEU. The lat-
ter, has been shown to achieve 0.66 correlation
in single-document summaries at 100 words com-
pression rate and against a single reference sum-
mary. The correlation achieved by BLEU climbs
up to 0.82 when BLEU is run over and compared
against multiply judged document units, that could
be thought of as a sort of multiple reference sum-
maries. The correlation scores for multi-document
summaries are similar. Therefore, BLEU has been
found to correlate quite highly with human judge-
ment for the summarization task when multiple
judgement is involved, while -as Lin and Hovy
indicate- using a single reference is not adequate
for getting reliable results with high correlation
with the human evaluators.
It is this conclusion that Lin and Hovy have
drawn, that contradicts findings by the IBM and
NIST people for the importance of using multiple
references when using BLEU in Machine Trans-
lation. The use of either multiple references or
just a single reference has been proved not to af-
fect the reliability of the results provided by BLEU
(Papineni et al, 2001; Doddington, 2002), which
seems not to be the case in summarization. This
is not a surprise; comparisons of content-based
metrics for summarization in (Donaway et al,
2000) have led the authors to the conclusion that
such metrics correlate highly with human judge-
ment when the humans do not disagree substan-
tially. The fact that more than one reference sum-
maries are needed because of the low agreement
between human evaluators has been repeatedly
indicated in automatic summarization evaluation
(Mani, 2001).
We attempt to test BLEU?s reliability when
changing various evaluation parameters such as
the source documents, the reference summaries
used and even parameters unique to the evaluation
of summaries, such as the compression rate of the
extract. In doing so, we explore whether the met-
ric is indeed reliable only when using more than
a single reference and whether any other testing
parameter could compensate for lack of multiple
references, if used appropriately.
3 Evaluation Experiment
In this section, we will present a description of the
experiments themselves, along with the results ob-
tained and their analysis, preceded by information
on the corpus we used for our experiments and the
tools we developed for setting their parameters and
running them automatically.
3.1 Testing corpus
We make use of part of the language resources
(HKNews Corpus) developed during the 2001
Workshop on Automatic Summarization of Mul-
tiple (Multilingual) Documents (Saggion et al,
2002).
The documents of each cluster are all relevant
to a specific topic-query, so that they form, in fact,
thematic clusters. The texts are marked up on the
paragraph, sentence and word level. Annotations
with linguistic information (Part of speech tags
and morphological information), though marked
up on the documents have not been used in our
experiments at all. Three judges have assessed
the sentences in each cluster and have provided a
score on a scale from 0 to 10 (i.e. utility judge-
ment), expressing how important the sentence is
for the topic of the cluster (Radev et al, 2000).
In our experiments, we have used three document
clusters, each consisting of ten documents in En-
glish.
3.2 Summarizers
It is important to note, that our objective is not
to demonstrate how a particular summarization
methodology performs, but to analyse an evalua-
tion metric. The summaries used for the evalu-
ation were produced as extracts at different ?sen-
tence? (and not word) compression rates2. In or-
der to produce summarizers for our evaluation,
we use a robust summarisation system (Saggion,
2002) that makes use of components for seman-
tic tagging and coreference resolution developed
within the GATE architecture (Cunningham et al,
2002). The system combines GATE components
with well established statistical techniques devel-
oped for the purpose of text summarisation re-
search. The system supports ?generic? and query-
based summarisation addressing the need for user
adaptation3. For each sentence, the system com-
putes values for a number of ?shallow? summariza-
tion features: position of the sentence, term distri-
bution analysis, similarity of the sentence with the
document, similarity with the sentence at the lead-
ing part of the document, similarity of the sentence
with the query, named entity distribution analysis,
statistic cohesion, etc. The values of these features
are linearly combined to produce the sentence fi-
2We have to note that the level of compression i.e sentence
or word level, affects probably the evaluation of the summa-
rizers? output. Comparative testing could indicate whether
this is a crucial parameter for system evaluation.
3The software can be obtained from http://www.
dcs.shef.ac.uk/?saggion
nal score. Top-ranked sentences are annotated un-
til the target n% compression is achieved (an an-
notation set is produced for each summary that is
generated). Different summarization systems can
be deployed by setting-up the weights that par-
ticipate in the scoring formula. Note that as the
summarization components are not aware of the
compression parameter, one would expect specific
configurations to produce good extracts at differ-
ent compression rates and across documents.
We have configured four different summariz-
ers, namely, the ?query-based system? that com-
putes the similarity of each sentence of the source
document with the documents topic-query, in or-
der to decide whether to include a sentence in the
generated extract or not. We also have the ?Sim-
ple 1 system?, whose main feature is that it com-
putes the similarity of a sentence with the whole
document, the ?Simple 2 system? which is a lead
based summarizer and the ?Simple 3 system? that
blindly extracts the last part of the source docu-
ment.
3.3 Judge-based Summaries
Following the same methodology used in (Saggion
et al, 2002), we implemented a judge-based sum-
marization system that given a judge number (1,
2, 3, or all), it scores sentences based on a combi-
nation of the utility that the sentence has accord-
ing to the judge (or the sum of the utilities if ?all?)
and the position of the sentence (leading sentences
are preferred). These ?extracts? represent our gold-
standards for evaluation in our experiments. In
order to use the documents in a stand-alone way,
we have enriched the initial corpus mark-up and
added to each document information about cluster
number, cluster topic (or query) and all the infor-
mation about utility judgement (that information
was kept in separate files in the original HKNews
corpus).
3.4 Evaluation Software
We have developed a number of software compo-
nents to facilitate the evaluation and we make use
of the GATE development environment for testing
and processing. The evaluation package allows the
user to specify different reference extracts (judge-
based summarizers) and summarization systems to
be compared.
Co-selection comparison (i.e., precision and re-
call) is being done with modules obtained from
the GATE library (AnnotationDiff components).
Content-based comparison by the Bleu algorithm
was implemented as a Java class. The exact for-
mula provided by the developers of BLEU has
been implemented following the baseline config-
urations i.e use of 4-grams and uniform weights
summing to 1:
Bleu(S,R) = K(S,R) ? eBleu1(S,R)
Bleu1(S,R) =
?
i=1,2,...n
wi ? lg( |(Si
?Ri)|
|Si| )
K(S,R) =
{
1 if |S| > |R|
e(1?
|R|
|S| ) otherwise
wi = i?
j=1,2,...n j
for i = 1, 2, ..., n
where S and R are the system and reference
sets. Si and Ri are the ?bags? of i-grams for sys-
tem and reference. n is a parameter of our imple-
mentation, but for the purpose of our experiments
we have set n to 4.
3.5 Experiments
In our experiments we have treated compression
rates and clusters as variables each one being a
condition for the other and both dependent to a
third variable, the gold standard summary. We
ran BLEU in all different combinations in order to
see the main effects of each combination and the
interactions among them. In particular, we have
used three different text clusters, consisting of
texts that refer to the same topic: cluster 1197 on
?Museum exhibits and hours?, cluster 125 which
deals with ?Narcotics and rehabilitation? and clus-
ter 241 which refers to ?Fire safety and building
management?. For the texts of each cluster we
have three different reference summaries (created
according to the utility judgement score assigned
by human evaluators cf. 3.1 and 3.2). We will
refer to these as Reference1, Reference2 and Ref-
erence3. The judges behind these references are
all the same for the three text clusters with one ex-
ception: Reference1 in cluster 241 has not been
created by the same human evaluator as the Refer-
ence 1 summaries for the other two clusters. Last,
we ran the experiments at five different compres-
sion rates 4: 10%, 20%, 30%, 40% and 50%.
We first ran BLEU on the reference summaries
in order to check whether BLEU is consistent
in the data it produces concerning the agreement
among human evaluators. We tried all possible
combinations for comparing the reference sum-
maries; using at first Reference 1 as the gold stan-
dard, we ran BLEU over References 2 and 3 and
we did this for two clusters (since the third?s -241-
Reference 1 set of summaries had been created by
another judge - a fourth one). We did this for all
five compression rates separately. We repeated the
experiment changing the gold standard and the ref-
erences to be scored accordingly (i.e Reference 1
and 3 against 2, Reference 1 and 2 against 3). The
results we got were consistent neither across clus-
ters, nor within clusters across compression rates;
however the latter, did show a general tendency for
consistency which allows for some observations
to be made. In cluster 1197, References 1 and 2
are generally in higher agreement than with 3, a
fact verified regardless the reference chosen as a
gold standard. The fact that References 1 and 2 are
very close was also evident when both compared
against Reference 3; though the latter is generally
closer to Reference 2, the scores assigned to Ref-
erence 1 and 2 are extremely close. In cluster 125,
Reference 1 is consistently closer to 3, while 2 is
closer to 1 at some compression rates and closer
to 3 at others. These very close scores indicate
that all three references are similarly ?distant? one
from another, and no groupings of agreement can
actually be made. Agreement between reference
summaries augments as the compression rate also
increases, with the higher similarity scores always
found at the 50% compression rate and the lower
ones consistently found at 10%. Table 1 shows
a consistent ranking across compression rates in
cluster 1197 and an inconsistent one in cluster 125,
using in both cases Reference 2 as the gold stan-
dard. From this first experiment, the rankings of
4In our experiments compression is always performed at
the sentence level
the reference summaries seem to depend on the
different values of the variables used. If that is
the case, then one should use BLEU in summa-
rization only when determining specific values for
the evaluation experiment, that will guarantee re-
liable results; but how could one determine which
value(s) should be chosen? To explore things fur-
ther we decided to proceed with a second experi-
ment set up in a similar way.
In our second experiment we try to compare
the system generated extracts (and therefore the
performance of the four summarizers) against the
different human references. Again, the differ-
ent rounds of the experiment involve multiple pa-
rameters; the generated extracts of all three text
clusters are compared against each reference sum-
mary, against all reference summaries (integrated
summary) and at all five compression rates. Going
through the different stages of this experiment we
observe that:
? For Reference X within Cluster Y across
Compressions, the ranking of the systems is
not consistent
One does not get the same system ranking at dif-
ferent compression rates. The similarity of a gen-
erated extract to a specific reference summary is
the same at some compression rates, similar at oth-
ers (e.g the order of two of the systems swaps)
and totally different at other rates. No patterns
arise in the way that rankings are similar at spe-
cific compression rates; for example, in table 2,
there seems to be a prevailing ranking common in
four compression rates; however, the ranking pro-
vided at 10% is totally different, and no apparent
reason seems to justify this deviation (e.g. very
close scores). Furthermore, this agreement among
the four highest compression rates does not form
a pattern i.e it does not appear as such across clus-
ters or references.
? For Reference X at Compression Y across
Clusters, the ranking of the systems is not
consistent
In our experiments we were able to observe 15 dif-
ferent realisations of these testing configurations
and hardly did a case of consistency at a compres-
sion rate across clusters appeared.
Ref 2 - 1197 10% 20% 30% 40% 50%
Reference 1 0.50 - 1 0.67 - 1 0.73 - 1 0.73 - 1 0.79 - 1
Reference 3 0.34 - 2 0.51 - 2 0.52 - 2 0.63 - 2 0.69 - 2
Ref 2 - 125 10% 20% 30% 40% 50%
Reference 1 0.36 - 1 0.41 - 1 0.59 - 2 0.67 - 2 0.78 - 1
Reference 3 0.20 - 2 0.46 - 2 0.66 - 1 0.73 - 1 0.73 - 2
Table 1: Reference summary similarity scores and rankings across clusters and compression rates
Reference 3 10% 20% 30% 40% 50%
Query-based 0.44 - 2 0.50 - 1 0.58 - 1 0.66 - 1 0.71 - 1
Simple 1 0.10 - 3 0.23 - 3 0.48 - 3 0.57 - 3 0.64 - 3
Simple 2 0.52 - 1 0.45 - 2 0.53 - 2 0.62 - 2 0.68 - 2
Simple 3 0.03 - 4 0.07 - 4 0.08 - 4 0.11 - 4 0.11 - 4
Table 2: System scores and rankings for cluster 241, against Reference 3, at different compression rates
? For Reference All across Clusters at multiple
Compressions, the ranking of the systems is
consistent
Estimating similarity scores against Reference
All (use of multiple references cf. 3.2), proves to
provide reliable, consistent results across clusters
and compression rates. Table 3 presents the scores
and corresponding system rankings for two differ-
ent clusters and at the five different compression
rates. The prevailing system ranking is [1324],
which is what we would intuitively expect accord-
ing to the features of the summarizers we compare.
Some deviations from this ranking are due to very
small differences in the similarity scores assigned
to the systems5, which indicates the need for using
a larger testing corpus for the experiments.
So, the need for multiple references is evident;
BLEU is a consistent, reliable metric, but when
used in summarization, one has to apply it to mul-
tiple references in order to get reliable results.
This is not just a way to improve correlation with
human judgement (Lin and Hovy, 2002); it is a
crucial evaluation parameter that affects the qual-
ity of the automatic evaluation results. In our case
we had a balanced set of reference summaries to
work with, i.e none of them was too similar to an-
other. The more reference summaries one has and
the larger one?s testing corpus, the safer the con-
clusions drawn will be. However, what happens
when there is lack of such resources and especially
5For example, at the 10% compression rate, cluster 1197,
systems Simple 1 and Simple 2 swap places in the final rank-
ing with a 0.005 difference in their similarity scores
of multiple reference summaries? Is there a way
to use BLEU with a single reference summary and
still get reliable results back?
Looking at the results of our experiments, when
using each reference summary separately as a gold
standard, we realised that estimating the average
ranking of each system across multiple compres-
sion rates might lead to consistent rankings. Fol-
lowing the average rank aggregation techique (Ra-
jman and Hartley, 2001), we transfered the aver-
age scores each system got per text cluster at each
compression rate into ranks and computed the av-
erage rank of each system across all five compres-
sion rates per text cluster and against each refer-
ence summary. Table 4, shows the average system
rankings we got for each system at clusters 1197
and 125, using Reference 1, 2, and 3 separately.
[1324] is the average system ranking that is clearly
indicated in the vast majority of cases. The two
exceptions to this are due to extremely small dif-
ferences in average scores at specific compression
rates and indicate the need for scaling up our ex-
periment, a fact that has already been indicated by
the results of our experiment using multiple refer-
ences (Reference All).
4 Conclusions and Future Work
BLEU has been developed for measuring con-
tent similarity in terms of length and wording
between texts. For the evaluation of automati-
cally generated extracts, the metric is expected to
capture similarities between sentences not shared
by both the generated text and the model sum-
Ref All - 1197 10% 20% 30% 40% 50%
Query based 0.55 - 1 0.47 - 1 0.49 - 1 0.62 - 1 0.63 - 2
Simple 1 0.3184 - 2 0.32 - 3 0.40 - 3 0.49 - 3 0.62 - 3
Simple 2 0.3134 - 3 0.39 - 2 0.44 - 2 0.56 - 2 0.67 - 1
Simple 3 0.02 - 4 0.03 - 4 0.07 - 4 0.11 - 4 0.13 - 4
Ref All - 125 10% 20% 30% 40% 50%
Query based 0.44 - 1 0.43 - 1 0.57 - 1 0.72 - 1 0.7641 - 2
Simple 1 0.18 - 3 0.3684 - 2 0.54 - 2 0.60 - 3 0.68 - 3
Simple 2 0.32 - 2 0.3673 - 3 0.44 - 3 0.66 - 2 0.7691 - 1
Simple 3 0.03 - 4 0.06 - 4 0.07 - 4 0.10 - 4 0.14 - 4
Table 3: Systems? similarity scores and rankings using Reference All as gold standard
10% 20% 30% 40% 50% Average Rank
Ref 1 - 125 1324 1234 2134 1324 1234 1234
Ref 2 - 125 1324 1324 1324 1324 2314 1324
Ref 3 - 125 2314 2314 1324 1324 2314 2314
Ref 1 - 1197 1324 2314 1324 1324 2314 1324
Ref 2 - 1197 1324 1324 1324 1324 2314 1324
Ref 3 - 1197 1324 1324 1324 1324 2314 1324
Table 4: Systems? average rankings resulting from ranks at multiple compression rates in clusters 125 and
1197. (Systems assumed to be listed in alphabetical order: Query-based, Simple1, Simple2, Simple3)
mary. Going through the texts scored in the above
experiments, we found cases in which BLEU
does not actually capture content similarity to
such a granularity that a human would. Some-
times, this is because the order of the words
forming n-grams differs slightly but still conveys
the same meaning (e.g. ?...abusers reported...?
vs. ?...reported abusers...?) and most of the
times because there is no way to capture cases
of synonymy, paraphrasing (e.g. ?downward
tendency?/?falling trend?/?decrease?) and other
deeper semantic equivalence (e.g. ?number of X?
vs. ?9,000 of X?). Such phenomena are -of course-
expected from a statistical metric which involves
no linguistic knowledge at all. Our aim in this pa-
per was to shed some light on the conditions under
which the metric performs reliably within summa-
rization, given the different parameters that affect
evaluation in this NLP research area. From the re-
sults obtained by our preliminary experiments, we
have generally concluded that:
? Running BLEU over system generated sum-
maries using a single reference affects the re-
liability of the results provided by the metric.
The use of multiple references is a sine qua
non for reliable results
? Running BLEU over system generated sum-
maries at multiple compression rates and esti-
mating the average rank of each system might
yield consistent and reliable results even with
a single reference summary and therefore
compensate for lack of multiple reference
summaries
In order to draw more safe conclusions, we need
to scale our experiments considerably, and this is
already in progress. Many research questions need
still to be answered, such as how BLEU scores
correlate with results produced by other content-
based metrics used in summarization and else-
where. We hope that this preliminary, experimen-
tal work on porting evaluation metrics across dif-
ferent NLP research areas will function as a stim-
ulus for extensive and thorough research in this di-
rection.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graph-
ical development environment for robust NLP tools
and applications. In ACL 2002.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurence
statistics. In Proceedings of HLT 2002, Human Lan-
guage Technology Conference, San Diego, CA.
R. Donaway, K. Drummey, and L. Mather. 2000. A
comparison of rankings produced by summariza-
tion evaluation measures. In Proceedings of the
ANLP-NAACL 2000 Workshop on Automatic Sum-
marization, Advanced Natural Language Processing
- North American of the Association for Computa-
tional Linguistics Conference, Seattle, DC.
E. Hovy, M. King, and A. Popescu-Belis. 2002. An in-
troduction to machine translation evaluation. In Pro-
ceedings of the LREC 2002 Workshop on Machine
Translation Evaluation: Human Evaluators Meet
Automated Metrics, Language Resources and Eval-
uation Conference. European Language Resources
Association (ELRA).
Ch. Lin and E. Hovy. 2002. Manual and automatic
evaluation of summariess. In Proceedings of the
ACL 2002 Workshop on Automatic Summarization,
Association for Computation Linguistics, Philadel-
phia, PA.
I. Mani, T. Firmin, and B. Sundheim. 2001. Summac:
A text summarization evaluation. Natural Language
Engineering.
I. Mani. 2001. Summarization evaluation: an
overview. In Proceedings of the NAACL 2001 Work-
shop on Automatic Summarization, North Chapter
of the Association for Computational Linguistics,
Pittsburgh, PA.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0 109-
022), IBM Research Division.
Dr. Radev, J. Hongyan, and M. Budzikowska. 2000.
Centroid-based summarization of multiple docu-
ments: sentence extraction, utility-based evaluation,
and user studies. In ANLP/NAACL Workshop on
Summarization, Seattle, WA, April.
M. Rajman and A. Hartley. 2001. Automatically pre-
dicting mt system rankings compatible with fluency,
adequacy or informativeness scores. In Proceed-
ings of the MT Summit 2001 Workshop on Machine
Translation Evaluation: Who did what to whom, Eu-
ropean Association for Machine Translation, Santi-
ago de Compostella, Spain.
H. Saggion, D. Radev., S. Teufel, L. Wai, and
S. Strassel. 2002. Developing infrastructure for
the evaluation of single and multi-document sum-
marization systems in a cross-lingual environment.
In 3rd International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 747?
754, Las Palmas, Gran Canaria, Spain.
H. Saggion. 2002. Shallow-based Robust Summariza-
tion. In Automatic Summarization: Solutions and
Perspectives, ATALA, December, 14.
Karen Sparck Jones and Julia R. Galliers. 1995. Eval-
uating Natural Language Processing Systems: An
Analysis and Review. Number 1083 in Lecture
Notes in Artificial Intelligence. Springer.
D. Zajic, B. Dorr, and R. Schwartz. 2002. Automatic
headline generation for newspaper stories. In Pro-
ceedings of the ACL 2002 Workshop on Automatic
Summarization, Association for Computation Lin-
guistics, Philadelphia, PA.
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107?115,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Experiments on Summary-based Opinion Classification
Elena Lloret
Department of Software
and Computing Systems
University of Alicante
Apdo. de Correos 99
E-03080, Alicante, Spain
elloret@dlsi.ua.es
Horacio Saggion
Department of Infomation and
Communication Technologies
Grupo TALN
Universitat Pompeu Fabra
C/Ta?nger, 122-134, 2nd floor
08018 Barcelona, Spain
horacio.saggion@upf.edu
Manuel Palomar
Department of Software
and Computing Systems
University of Alicante
Apdo. de Correos 99
E-03080, Alicante, Spain
mpalomar@dlsi.ua.es
Abstract
We investigate the effect of text summarisa-
tion in the problem of rating-inference ? the
task of associating a fine-grained numerical
rating to an opinionated document. We set-up
a comparison framework to study the effect of
different summarisation algorithms of various
compression rates in this task and compare the
classification accuracy of summaries and doc-
uments for associating documents to classes.
We make use of SVM algorithms to associate
numerical ratings to opinionated documents.
The algorithms are informed by linguistic and
sentiment-based features computed from full
documents and summaries. Preliminary re-
sults show that some types of summaries could
be as effective or better as full documents in
this problem.
1 Introduction
Public opinion has a great impact on company and
government decision making. In particular, compa-
nies have to constantly monitor public perception of
their products, services, and key company represen-
tatives to ensure that good reputation is maintained.
Recent cases of public figures making headlines for
the wrong reasons have shown how companies take
into account public opinion to distance themselves
from figures which can damage their public image.
The Web has become an important source for find-
ing information, in the field of business intelligence,
business analysts are turning their eyes to the Web
in order to monitor public perception on products,
services, policies, and managers. The field of senti-
ment analysis has recently emerged (Pang and Lee,
2008) as an important area of research in Natural
Language Processing (NLP) which can provide vi-
able solutions for monitoring public perception on
a number of issues; with evaluation programs such
as the Text REtrieval Conference track on blog min-
ing 1, the Text Analysis Conference 2 track on opin-
ion summarisation, and the DEfi Fouille de Textes
program (Grouin et al, 2009) advances in the state
of the art have been produced. Although sentiment
analysis involves various different problems such as
identifying subjective sentences or identifying posi-
tive and negative opinions in text, here we concen-
trate on the opinion classification task; and more
specifically on rating-inference, the task of identify-
ing the author?s evaluation of an entity with respect
to an ordinal-scale based on the author?s textual eval-
uation of the entity (Pang and Lee, 2005). The spe-
cific problem we study in this paper is that of as-
sociating a fine-grained rating (1=worst,...5=best)
to a review. This is in general considered a dif-
ficult problem because of the fuzziness inherent of
mid-range ratings (Mukras et al, 2007). A consid-
erable body of research has recently been produced
to tackle this problem (Chakraborti et al, 2007; Fer-
rari et al, 2009) and reported figures showing accu-
racies ranging from 30% to 50% for such complex
task; most approaches derive features for the classi-
fication task from the full document. In this research
we ask whether extracting features from document
summaries could help a classification system. Since
text summaries are meant to contain the essential
content of a document (Mani, 2001), we investigate
whether filtering noise through text summarisation
is of any help in the rating-inference task. In re-
1http:trec.nist.gov/
2http://www.nist.gov/tac/
107
cent years, text summarisation has been used to sup-
port both manual and automatic tasks; in the SUM-
MAC evaluation (Mani et al, 1998), text summaries
were tested in document classification and ques-
tion answering tasks where summaries were consid-
ered suitable surrogates for full documents; Bagga
and Baldwin (1998) studied summarisation in the
context of a cross-document coreference task and
found that summaries improved the performance of
a clustering-based coreference mechanism; more re-
cently Latif and McGee (2009) have proposed text
summarisation as a preprocessing step for student
essay assessment finding that summaries could be
used instead of full essays to group ?similar? qual-
ity essays. Summarisation has been studied in the
field of sentiment analysis with the objective of pro-
ducing opinion summaries, however, to the best of
our knowlegde there has been little research on the
study of document summarisation as a text pro-
cessing step for opinion classification. This paper
presents a framework and extensive experiments on
text summarisation for opinion classification, and in
particular, for the rating-inference problem. We will
present results indicating that some types of sum-
maries could be as effective or better than the full
documents in this task.
The remainder of the paper is organised as fol-
lows: Section 2 will compile the existing work with
respect to the inference-rating problem; Section 3
and Section 4 will describe the corpus and the NLP
tools used for all the experimental set-up. Next, the
text summarisation approaches will be described in
Section 5, and then Section 6 will show the exper-
iments conducted and the results obtained together
with a discussion. Finally, we will draw some con-
clusions and address further work in Section 7.
2 Related Work
Most of the literature regarding sentiment analysis
addresses the problem either by detecting and clas-
sifying opinions at a sentence level (Wilson et al,
2005; Du and Tan, 2009), or by attempting to cap-
ture the overall sentiment of a document (McDonald
et al, 2007; Hu et al, 2008). Traditional approaches
tackle the task as binary classification, where text
units (e.g. words, sentences, fragments) are classi-
fied into positive vs. negative, or subjective vs. ob-
jective, according to their polarity and subjectivity
degree, respectively. However, sentiment classifica-
tion taking into account a finer granularity has been
less considered. Rating-inference is a particular task
within sentiment analysis, which aims at inferring
the author?s numerical rating for a review. For in-
stance, given a review and 5-star-rating scale (rang-
ing from 1 -the worst- to 5 -the best), this task should
correctly predict the review?s rating, based on the
language and sentiment expressed in its content.
In (Pang and Lee, 2005), the rating-inference
problem is analysed for the movies domain. In
particular, the utility of employing label and item
similarity is shown by analysing the performance
of three different methods based on SVM (one vs.
all, regression and metric labeling), in order to infer
the author?s implied numerical rating, which ranges
from 1 up to 4 stars, depending on the degree the au-
thor of the review liked or not the film. The approach
described in (Leung et al, 2006) suggests the use of
collaborative filtering algorithms together with sen-
timent analysis techniques to obtain user preferences
expressed in textual reviews, focusing also on movie
reviews. Once the opinion words from user reviews
have been identified, the polarity of those opinion
words together with their strength need to be com-
puted and mapped to the rating scales to be further
input to the collaborative input algorithms.
Apart from these approaches, this problem is
stated from a different point of view in (Shimada
and Endo, 2008). Here it is approached from the
perspective of rating different details of a product
under the same review. Consequently, they rename
the problem as ?seeing several stars? instead of only
one, corresponding to the overall sentiment of the
review. Also, in (Baccianella et al, 2009) the rating
of different features regarding hotel reviews (cleanli-
ness, location, staff, etc.) is addressed by analysing
several aspects involved in the generation of prod-
uct review?s representations, such as part-of-speech
and lexicons. Other approaches (Devitt and Ahmad,
2007), (Turney, 2002) face this problem by group-
ing documents with closer stars under the same cat-
egory, i.e. positive or negative, simplifying the task
into a binary classification problem.
Recently, due to the vast amount of on-line infor-
mation and the subjectivity appearing in documents,
the combination of sentiment analysis and summari-
108
sation task in tandem can result in great benefits
for stand-alone applications of sentiment analysis,
as well as for the potential uses of sentiment analy-
sis as part of other NLP applications (Stoyanov and
Cardie, 2006). Whilst there is much literature com-
bining sentiment analysis and text summarisation
focusing on generating opinion-oriented summaries
for the new textual genres, such as blogs (Lloret
et al, 2009), or reviews (Zhuang et al, 2006), the
use of summaries as substitutes of full documents in
tasks such as rating-inference has been not yet ex-
plored to the best of our knowledge. In contrast to
the existing literature, this paper uses summaries in-
stead of full reviews to tackle the rating-inference
task in the financial domain, and we carry out a pre-
liminary analysis concerning the potential benefits
of text summaries for this task.
3 Dataset for the Rating-inference Task
Since there is no standard dataset for carrying out
the rating-inference task, the corpus used for our ex-
periments was one associated to a current project on
business intelligence we are working on. These data
consisted of 89 reviews of several English banks
(Abbey, Barcalys, Halifax, HSBC, Lloyds TSB, and
National Westminster) gathered from the Internet. In
particular the documents were collected from Ciao3,
a Website where users can write reviews about dif-
ferent products and services, depending on their own
experience.
Table 1 lists some of the statistical properties of
the data. It is worth stressing upon the fact that
the reviews have on average 2,603 words, which
means that we are dealing with long documents
rather than short ones, making the rating-inference
task even more challenging. The shortest document
contains 1,491 words, whereas the longest document
has more than 5,000 words.
# Reviews Avg length Max length Min length
89 2,603 5,730 1,491
Table 1: Corpus Statistics
Since the aim of the task we are pursuing focuses
on classifying correctly the star for a review (rang-
ing from 1 to 5 stars), it is necessary to study how
3http://www.ciao.co.uk/
many reviews we have for each class, in order to see
whether we have a balanced distribution or not. Ta-
ble 2 shows this numbers for each star-rating. It is
worth mentioning that one-third of the reviews be-
long to the 4-star class. In contrast, we have only 9
reviews that have been rated as 3-star, consisting of
the 10% of the corpus, which is a very low number.
Star-rating # reviews %
1-star 17 19
2-star 11 12
3-star 9 10
4-star 28 32
5-star 24 27
Table 2: Class Distribution
4 Natural Language Processing Tools
Linguistic analysis of textual input is carried out
using the General Architecture for Text Engineer-
ing (GATE) ? a framework for the development and
deployment of language processing technology in
large scale (Cunningham et al, 2002). We make use
of typical GATE components: tokenisation, parts of
speech tagging, and morphological analysis to pro-
duce document annotations. From the annotations
we produce a number of features for document rep-
resentation. Features produced from the annotations
are: string ? the original, unmodified text of each
token; root ? the lemmatised, lower-case form of
the token; category ? the part-of-speech (POS) tag, a
symbol that represents a grammatical category such
as determiner, present-tense verb, past-tense verb,
singular noun, etc.; orth ? a code representing the to-
ken?s combination of upper- and lower-case letters.
In addition to these basic features, ?sentiment? fea-
tures based on a lexical resource are computed as
explained below.
4.1 Sentiment Features
SentiWordNet (Esuli and Sebastiani, 2006) is a lexi-
cal resource in which each synset (set of synonyms)
of WordNet (Fellbaum, 1998) is associated with
three numerical scores obj (how objective the word
is), pos (how positive the word is), and neg (how
negative the word is). Each of the scores ranges
from 0 to 1, and their sum equals 1. SentiWord-
Net word values have been semi-automatically com-
puted based on the use of weakly supervised classi-
109
fication algorithms. In this work we compute the
?general sentiment? of a word in the following way:
given a word w we compute the number of times the
word w is more positive than negative (positive >
negative), the number of times is more negative than
positive (positive < negative) and the total number
of entries of word w in SentiWordNet, therefore we
can consider the overall positivity or negativity a
particular word has in SentiWordNet. We are in-
terested in words that are generally ?positive?, gen-
erally ?negative? or generally ?neutral? (not much
variation between positive and negative). For exam-
ple a word such as ?good? has many more entries
where the positive score is greater than the nega-
tivity score while a word such as ?unhelpful? has
more negative occurrences than positive. We use this
aggregated scores in our classification experiments.
Note that we do not apply any word sense disam-
biguation procedure here.
4.2 Machine Learning Tool
For the experiments reported here, we adopt a Sup-
port Vector Machine (SVM) learning paradigm not
only because it has recently been used with suc-
cess in different tasks in natural language processing
(Isozaki and Kazawa, 2002), but it has been shown
particularly suitable for text categorization (Kumar
and Gopal, 2009) where the feature space is huge, as
it is in our case. We rely on the support vector ma-
chines implementation distributed with the GATE
system (Li et al, 2009) which hides from the user
the complexities of feature extraction and conver-
sion from documents to the machine learning imple-
mentation. The tool has been applied with success
to a number of datasets for opinion classification and
rating-inference (Saggion and Funk, 2009).
5 Text Summarisation Approach
In this Section, three approaches for carrying out the
summarisation process are explained in detail. First,
a generic approach is taken as a basis, and then, it is
adapted into a query-focused and a opinion-oriented
approach, respectively.
5.1 Generic Summarisation
A generic text summarisation approach is first taken
as a core, in which three main stages can be distin-
guished: i) document preprocessing; ii) relevance
detection; and ii) summary generation. Since we
work with Web documents, an initial preprocessing
step is essential to remove all unnecessary tags and
noisy information. Therefore, in the first stage the
body of the review out of the whole Web page is
automatically delimitated by means of patterns, and
only this text is used as the input for the next sum-
marisation stages. Further on, a sentence relevance
detection process is carried out employing different
combinations of various techniques. In particular,
the techniques employed are:
Term frequency (tf ): this technique has been
widely used in different summarisation approaches,
showing the the most frequent words in a document
contain relevant information and can be indicative of
the document?s topic (Nenkova et al, 2006)
Textual entailment (te): a te module (Ferra?ndez
et al, 2007) is used to detect redundant information
in the document, by computing the entailment be-
tween two consecutive sentences and discarding the
entailed ones. The identification of these entailment
relations helps to avoid incorporating redundant in-
formation in summaries.
Code quantity principle (cqp): this is a linguis-
tic principle which proves the existence of a propor-
tional relation between how important the informa-
tion is, and the number of coding elements it has
(Givo?n, 1990). In this approach we assume that sen-
tences containing longer noun-phrases are more rel-
evant.
The aforementioned techniques are combined
together taking always into account the term-
frequency, leading to different summarisation strate-
gies (tf, te+tf, cqp+tf, te+cqp+tf ). Finally, the re-
sulting summary is produced by extracting the high-
est scored sentences up to the desired length, accord-
ing the techniques explained.
5.2 Query-focused Summarisation
Through adapting the generic summarisation ap-
proach into a query-focused one, we could benefit
from obtaining more specific sentences with regard
to the topic of the review. As a preliminary work, we
are going to assume that a review is about a bank,
and as a consequence, the name of the bank is con-
sidered to be the topic. It is worth mentioning that a
person can refer to a specific bank in different ways.
For example, in the case of ?The National Westmin-
110
ster Bank?, it can be referred to as ?National West-
minster? or ?NatWest?. Such different denomina-
tions were manually identified and they were used
to biased the content of the generated summaries,
employing the same techniques of tf, te and the cqp
combined together. One limitation of this approach
is that we do not directly deal with the coreference
problem, so for example, sentences containing pro-
nouns referring also to the bank, will not be taken
into consideration in the summarisation process. We
are aware of this limitation and for future work it
would be necessary to run a coreference algorithm
to identify all occurrences of a bank within a review.
However, since the main goal of this paper is to carry
out a preliminary analysis of the usefulness of sum-
maries in contrast to whole reviews in the rating-
inference problem, we did not take this problem into
account at this stage of the research. In addition,
when we do query-focused summarisation only we
rely on the SUMMA toolkit (Saggion, 2008) to pro-
duce a query similarity value for each sentence in the
review which in turn is used to rank sentences for an
extractive summary (qf ). This similarity value is the
cosine similarity between a sentence vector (terms
and weights) and a query vector (terms and weigths)
and where the query is the name of the entity being
reviewed (e.g. National Westminster).
5.3 Opinion-oriented Summarisation
Since reviews are written by people who want to
express their opinion and experience with regard
to a bank, in this particular case, either generic or
query-focused summaries can miss including some
important information concerning their sentiments
and feelings towards this particular entity. There-
fore, a sentiment classification system similar to the
one used in (Balahur-Dobrescu et al, 2009) is used
together with the summarisation approach, in order
to generate opinion-oriented summaries. First of all,
the sentences containing opinions are identified, as-
signing each of them a polarity (positive and neg-
ative) and a numerical value corresponding to the
polarity strength (the higher the negative score, the
more negative the sentence and similarly, the higher
the positive score, the more positive the sentence).
Sentences containing a polarity value of 0 are con-
sidered neutral and are not taken into account. Once
the sentences are classified into positives, negatives
and neutrals, they are grouped together according
to its type. Further on, the same combination of
techniques as for previously explained summarisa-
tion approaches are then used.
Additionally, a summary containing only the most
positive and negative sentences is also generated (we
have called this type of summaries sent) in order to
check whether the polarity strength on its own could
be a relevant feature for the summarisation process.
6 Evaluation Environment
In this Section we are going to describe in detail all
the experimental set-up. Firstly, we will explain the
corpus we used together with some figures regard-
ing some statistics computed. Secondly, we will de-
scribe in-depth all the experiments we ran and the re-
sults obtained. Finally, an extensive discussion will
be given in order to analyse all the results and draw
some conclusions.
6.1 Experiments and Results
The main objective of the paper is to investigate the
influence of summaries in contrast to full reviews for
the rating-inference problem.
The purpose of the experiments is to analyse the
performance of the different suggested text sum-
marisation approaches and compare them to the per-
formance of the full review. Therefore, the experi-
ments conducted were the following: for each pro-
posed summarisation approach, we experimented
with five different types of compression rates for
summaries (ranging from 10% to 50%). Apart from
the full review, we dealt with 14 different sum-
marisation approaches (4 for generic, 5 for query-
focused and 5 for opinion-oriented summarisation),
as well as 2 baselines (lead and final, taking the first
or the last sentences according to a specific compres-
sion rate, respectively). Each experiment consisted
of predicting the correct star of a review, either with
the review as a whole or with one of the summari-
sation approaches. As we previously said in Sec-
tion 4, for predicting the correct star-rating, we used
machine learning techniques. In particular, differ-
ent features were used to train a SVM classifier with
10-fold cross validation4 , using the whole review:
4The classifier used was the one integrated within the GATE
framework: http://gate.ac.uk/
111
the root of each word, its category, and the calcu-
lated value employing the SentiWordNet lexicon, as
well as their combinations. As a baseline for the full
document we took into account a totally uninformed
approach with respect to the class with higher num-
ber of reviews, i.e. considering all documents as if
they were scored with 4 stars. The different results
according different features can be seen in Table 3.
Feature F?=1
baseline 0.300
root 0.378
category 0.367
sentiWN 0.333
root+category 0.356
root+sentiWN 0.333
category+sentiWN 0.389
root+category+sentiWN 0.413
Table 3: F-measure results using the full review for clas-
sification
Regarding the features for training the summaries,
it is worth mentioning that the best performing fea-
ture when no sentiment-based features are taken into
account is the one using the root of the words. Con-
sequently, this feature was used to train the sum-
maries. Moreover, since the best results using the
full review were obtained using the combination of
the all the features (root+category+sentiWN), we
also selected this combination to train the SVM
classifier with our summaries. Conducting both
experiments, we could analyse to what extent the
sentiment-based feature benefit the classification
process.
The results obtained are shown in Table 4 and
Table 5, respectively. These tables show the F-
measure value obtained for the classification task,
when features extracted from summaries are used
instead from the full review. On the one hand,
results using the root feature extracted from sum-
maries can be seen in Table 4. On the other hand,
Table 5 shows the results when the combination
of all the linguistic and sentiment-based features
(root+category+sentiWN), that has been extracted
from summaries, are used for training the SVM clas-
sifier.
We also performed two statistical tests in order
to measure the significance for the results obtained.
The tests we performed were the one-way Analy-
sis of Variance (ANOVA) and the t-test (Spiegel and
Castellan, 1998). Given a group of experiments, we
first run ANOVA for analysing the difference be-
tween their means. In case some differences are
found, we run the t-test between those pairs.
6.2 Discussion
A first analysis derived from the results obtained in
Table 3 makes us be aware of the difficulty associ-
ated to the rating-inference task. As can be seen,
a baseline without any information from the docu-
ment at all, is performing around 30%, which com-
pared to the remaining approaches is not a very bad
number. However, we assumed that dealing with
some information contained in documents, the clas-
sification algorithm will do better in finding the cor-
rect star associated to a review. This was the rea-
son why we experimented with different features
alone or in combination. From these experiments,
we obtained that the combination of linguistic and
semantic-based features leads to the best results, ob-
taining a F-measure value of 41%. If sentiment-
based features are not taken into account, the best
feature is the root of the word on its own. Further-
more, in order to analyse further combinations, we
ran some experiments with bigrams. However, the
results obtained did not improve the ones we already
had, so they are not reported in this paper.
As far as the results is concerned comparing the
use of summaries to the full document, it is worth
mentioning that when using specific summarisation
approaches, such as query-focused summaries com-
bined with term-frequency, we get better results than
using the full document with a 90% confidence in-
terval, according to a t-test. In particular, qf for 10%
is significant with respect to the full document, us-
ing only root as feature for training. For the results
regarding the combination of root, category and Sen-
tiWordNet, qf for 10% and qf+tf for 10% and 20%
are significant with respect to the full document.
Concerning the different summarisation ap-
proaches, it cannot be claimed a general tendency
about which ones may lead to the best results. We
also performed some significance tests between dif-
ferent strategies, and in most of the cases, the t-
test and the ANOVA did not report significance
over 95%. Only a few approaches were significant
at a 95% confidence level, for instance, te+cqp+tf
and sent+te+cqp+tf with respect to sent+cqp+tf
112
Approach Compression Rate
Summarisation method 10% 20% 30% 40% 50%
lead F?=1 0.411 0.378 0.367 0.311 0.322
final F?=1 0.322 0.389 0.300 0.467 0.456
tf F?=1 0.400 0.344 0.400 0.367 0.367
te+tf F?=1 0.367 0.422 0.411 0.389 0.322
cqp+tf F?=1 0.300 0.344 0.311 0.300 0.256
te+cqp+tf F?=1 0.422 0.356 0.333 0.300 0.322
qf F?=1 0.513 0.388 0.375 0.363 0.363
qf+tf F?=1 0.567 0.467 0.311 0.367 0.389
qf+te+tf F?=1 0.389 0.367 0.411 0.378 0.333
qf+cqp+tf F?=1 0.300 0.356 0.378 0.378 0.333
qf+te+cqp+tf F?=1 0.322 0.322 0.367 0.367 0.356
sent F?=1 0.344 0.380 0.391 0.290 0.336
sent+tf F?=1 0.378 0.425 0.446 0.303 0.337
sent+te+tf F?=1 0.278 0.424 0.313 0.369 0.347
sent+cqp+tf F?=1 0.333 0.300 0.358 0.358 0.324
sent+te+cqp+tf F?=1 0.446 0.334 0.358 0.292 0.369
Table 4: Classification results (F-measure) for summaries using root (lead = first sentences; final = last sentences;
tf = term frequency; te = textual entailment; cqp = code quantity principle with noun-phrases; qf = query-focused
summaries; and sent = opinion-oriented summaries)
for 10%; sent+tf in comparison to sent+cqp+tf
for 20%; or sent with respect to cqp+tf for 40%
and 50% compression rates. Other examples of
the approaches that were significant at a 90%
level of confidence are qf for 10% with respect to
sent+te+cqp+tf. Due to the wide range of summari-
sation strategies tested in the experiments, the results
obtained vary a lot and, due to the space limitations,
it is not possible to report all the tables. What it
seems to be clear from the results is that the code
quantity principle (see Section 5) is not contributing
much to the summarisation process, thus obtaining
poor results when it is employed. Intuitively, this
can be due to the fact that after the first mention of
the bank, there is a predominant use of pronouns,
and as a consequence, the accuracy of the tool that
identifies noun-phrases could be affected. The same
reason could be affecting the term-frequency calcu-
lus, as it is computed based on the lemmas of the
words, not taking into account the pronouns that re-
fer also to them.
7 Conclusion and Future Work
This paper presented a preliminary study of
inference-rating task. We have proposed here a new
framework for comparison and extrinsic evaluation
of summaries in a text-based classification task. In
our research, text summaries generated using differ-
ent strategies were used for training a SVM classifier
instead of full reviews. The aim of this task was to
correctly predict the category of a review within a 1
to 5 star-scale. For the experiments, we gathered 89
bank reviews from the Internet and we generated 16
summaries of 5 different compression rates for each
of them (80 different summaries for each review,
having generated in total 7,120 summaries). We also
experimented with several linguistic and sentiment-
based features for the classifier. Although the re-
sults obtained are not significant enough to state
that summaries really help the rating-inference task,
we have shown that in some cases the use of sum-
maries (e.g. query/entity-focused summaries) could
offer competitive advantage over the use of full doc-
uments and we have also shown that some summari-
sation techniques do not degrade the performance of
a rating-inference algorithm when compared to the
use of full documents. We strongly believe that this
preliminary study could serve as a starting point for
future developments.
Although we have carried out extensive experi-
mentation with different summarisation techniques,
compression rates, and document/summary features,
there are many issues that we have not explored. In
the future, we plan to investigate whether the re-
sults could be affected by the class distribution of
the reviews, and in this line we would like to see the
distribution of the documents using clustering tech-
113
Approach Compression Rate
Summarisation method 10% 20% 30% 40% 50%
lead F?=1 0.275 0.422 0.422 0.378 0.322
final F?=1 0.275 0.378 0.333 0.344 0.400
tf F?=1 0.411 0.422 0.411 0.378 0.378
te+tf F?=1 0.411 0.344 0.344 0.344 0.378
cqp+tf F?=1 0.358 0.267 0.333 0.222 0.289
te+cqp+tf F?=1 0.444 0.411 0.411 0.311 0.322
qf F?=1 0.563 0.488 0.400 0.375 0.350
qf+tf F?=1 0.444 0.411 0.433 0.367 0.356
qf+te+tf F?=1 0.322 0.367 0.356 0.344 0.344
qf+cqp+tf F?=1 0.292 0.322 0.367 0.333 0.356
qf+te+cqp+tf F?=1 0.356 0.378 0.356 0.367 0.356
sent F?=1 0.322 0.370 0.379 0.412 0.414
sent+tf F?=1 0.378 0.446 0.359 0.380 0.402
sent+te+tf F?=1 0.333 0.414 0.404 0.380 0.381
sent+cqp+tf F?=1 0.300 0.333 0.347 0.358 0.296
sent+te+cqp+tf F?=1 0.436 0.413 0.425 0.359 0.324
Table 5: Classification results (F-measure) for summaries using root, category and SentiWordNet (lead = first sen-
tences; final = last sentences; tf = term frequency; te = textual entailment; cqp = code quantity principle with
noun-phrases; qf = query-focused summaries; and sent = opinion-oriented summaries)
niques. Moreover, we would also like to investigate
what it would happen if we consider the values of the
star-rating scale as ordinal numbers, and not only as
labels for categories. We will replicate the exper-
iments presented here using as evaluation measure
the ?mean square error? which has been pinpointed
as a more appropriate measure for categorisation in
an ordinal scale. Finally, in the medium to long-
term we plan to extent the experiments and analy-
sis to other available datasets in different domains,
such as movie or book reviews, in order to see if
the results could be influenced by the nature of the
corpus, allowing also further results for comparison
with other approaches and assessing the difficulty of
the task from a perspective of different domains.
Acknowledgments
This research has been supported by the project PROM-
ETEO ?Desarrollo de Te?cnicas Inteligentes e Interacti-
vas de Miner??a de Textos? (2009/119) from the Valencian
Government. Moreover, Elena Lloret is funded by the
FPI program (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation under the project TEXT-
MESS (TIN2006-15265-C06-01), and Horacio Saggion
is supported by a Ramo?n y Cajal Fellowship from the
Ministry of Science and Innovation, Spain. The authors
would also like to thank Alexandra Balahur for helping to
process the dataset with her Opinion Mining approach.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2009. Multi-
facet Rating of Product Reviews. In Proceedings of
the 31th European Conference on IR Research on Ad-
vances in Information Retrieval, pages 461?472.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. In Proceedings of the COLING-ACL, pages
79?85.
A. Balahur-Dobrescu, M. Kabadjov, J. Steinberger,
R. Steinberger, and A. Montoyo. 2009. Summarizing
Opinions in Blog Threads. In Proceedings of the Pa-
cific Asia Conference on Language, INformation and
Computation Conference, pages 606?613.
S. Chakraborti, R. Mukras, R. Lothian, N. Wiratunga,
S. Watt, and D Harper. 2007. Supervised Latent Se-
mantic Indexing using Adaptive Sprinkling. In Pro-
ceedings of IJCAI-07, pages 1582?1587.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphi-
cal Development Environment for Robust NLP Tools
and Applications. In Proceedings of the ACL.
A. Devitt and K. Ahmad. 2007. Sentiment Polarity Iden-
tification in Financial News: A Cohesion-based Ap-
proach. In Proceedings of the ACL, pages 984?991.
W. Du and S. Tan. 2009. An Iterative Reinforcement
Approach for Fine-Grained Opinion Mining. In Pro-
ceedings of the NAACL, pages 486?493.
A. Esuli and F. Sebastiani. 2006. SENTIWORDNET: A
Publicly Available Lexical Resource for Opinion Min-
ing. In Proceedings of LREC, pages 417?422.
114
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
O. Ferra?ndez, D. Micol, R. Mun?oz, and M. Palomar.
2007. A Perspective-Based Approach for Solving Tex-
tual Entailment Recognition. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 66?71, June.
S. Ferrari, T. Charnois, Y. Mathet, F. Rioult, and
D. Legallois. 2009. Analyse de Discours ?Evaluatif,
Mode`le Linguistique et Applications. In Fouille de
donne?es d?opinion, volume E-17, pages 71?93.
T. Givo?n, 1990. Syntax: A functional-typological intro-
duction, II. John Benjamins.
C. Grouin, M. Hurault-Plantet, P. Paroubek, and J. B.
Berthelin. 2009. DEFT?07 : Une Campagne
d?Avaluation en Fouille d?Opinion. In Fouille de
donne?es d?opinion, volume E-17, pages 1?24.
Y. Hu, W. Li, and Q. Lu. 2008. Developing Evalua-
tion Model of Topical Term for Document-Level Sen-
timent Classification. In Proceedings of the 10th Pa-
cific Rim International Conference on Artificial Intel-
ligence, pages 175?186.
H. Isozaki and H. Kazawa. 2002. Efficient Support
Vector Classifiers for Named Entity Recognition. In
Proceedings of the 19th International Conference on
Computational Linguistics, pages 390?396.
M. A. Kumar and M. Gopal. 2009. Text Categorization
Using Fuzzy Proximal SVM and Distributional Clus-
tering of Words. In Proceedings of the 13th Pacific-
Asia Conference on Advances in Knowledge Discovery
and Data Mining, pages 52?61.
S. Latif and M. McGee Wood. 2009. A Novel Technique
for Automated Linguistic Quality Assessment of Stu-
dents? Essays Using Automatic Summarizers. Com-
puter Science and Information Engineering, World
Congress on, 5:144?148.
C. W. K. Leung, S. C. F. Chan, and F. L. Chung.
2006. Integrating Collaborative Filtering and Sen-
timent Analysis: A Rating Inference Approach. In
Proceedings of The ECAI 2006 Workshop on Recom-
mender Systems, pages 62?66.
Y. Li, K. Bontcheva, and H. Cunningham. 2009. Adapt-
ing SVM for Data Sparseness and Imbalance: A Case
Study in Information Extraction. Natural Language
Engineering, 15(2):241?271.
E. Lloret, A. Balahur, M. Palomar, and A. Montoyo.
2009. Towards Building a Competitive Opinion Sum-
marization System: Challenges and Keys. In Proceed-
ings of the NAACL. Student Research Workshop and
Doctoral Consortium, pages 72?77.
I. Mani, D. House, G. Klein, L. Hirshman, L. Obrst,
T. Firmin, M. Chrzanowski, and B. Sundheim. 1998.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. Technical report, The Mitre Corporation.
I. Mani. 2001. Automatic Text Summarization. John
Benjamins Publishing Company.
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured Models for Fine-to-
Coarse Sentiment Analysis. In Proceedings of the
ACL, pages 432?439.
R. Mukras, N. Wiratunga, R. Lothian, S. Chakraborti, and
D. Harper. 2007. Information Gain Feature Selection
for Ordinal Text Classification using Probability Re-
distribution. In Proceedings of the Textlink workshop
at IJCAI-07.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A Compositional Context Sensitive Multi-document
Summarizer: Exploring the Factors that Influence
Summarization. In Proceedings of the ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 573?580.
B. Pang and L. Lee. 2005. Seeing Stars: Exploiting
Class Relationships for Sentiment Categorization with
Respect to Rating Scales. In Proceedings of the ACL,
pages 115?124.
B. Pang and L. Lee. 2008. Opinion Mining and Senti-
ment Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
H. Saggion and A. Funk. 2009. Extracting Opinions and
Facts for Business Intelligence. RNTI, E-17:119?146.
H. Saggion. 2008. SUMMA: A Robust and Adapt-
able Summarization Tool. Traitement Automatique
des Languages, 49:103?125.
K. Shimada and T. Endo. 2008. Seeing Several Stars: A
Rating Inference Task for a Document Containing Sev-
eral Evaluation Criteria. In Proceedings of the 12th
Pacific-Asia Conference on Advances in Knowledge
Discovery and Data Mining, pages 1006?1014.
S. Spiegel and N. J. Castellan, Jr. 1998. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill
International.
V. Stoyanov and C. Cardie. 2006. Toward Opinion Sum-
marization: Linking the Sources. In Proceedings of
the Workshop on Sentiment and Subjectivity in Text,
pages 9?14.
P. D. Turney. 2002. Thumbs Up or Thumbs Down?: Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. In Proceedings of the ACL, pages
417?424.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of the EMNLP, pages 347?
354.
L. Zhuang, F. Jing, and X. Y. Zhu. 2006. Movie Re-
view Mining and Summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, pages 43?50.
115
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 32?39, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Human Language Technology for Text-based Analysis of Psychotherapy
Sessions in the Spanish Language
Horacio Saggion?, Elena Stein-Sparvieri? , David Maldavsky? , Sandra Szasz?
?DTIC - Universitat Pompeu Fabra
Calle Tanger 122-140, Poble Nou
Barcelona - Spain
H.Saggion@dcs.shef.ac.uk
?IAEPCS - Universidad de Ciencias Sociales y Empresariales
Paraguay 1401, PB, Bs. As. Argentina
estein@solutiion.com.ar;dmaldavsky@elsitio.net
?University of Sheffield - Department of Computer Sciences
211 Portobello Street - Sheffield - UK
S.Szasz@sheffield.ac.uk
Abstract
We present work in progress in the application
of Natural Language Processing (NLP) tech-
nology to the analysis of textual transcriptions
of psychotherapy sessions in the Spanish Lan-
guage. We are developing a set of NLP tools
as well as adapting an existing dictionary for
the analysis of interviews framed on a psycho-
analytic theory. We investigate the applica-
tion of NLP techniques, including dictionary-
based interpretation, and speech act identifica-
tion and classification for the (semi) automatic
identification in text of a set of psychoanalyt-
ical variables. The objective of the work is to
provide a set of tools and resources to assist
therapist during discourse analysis.
1 Introduction
Computer-based textual analysis in psychology is
not new; in psychotherapy, electronic dictionaries
and other lexical resources are widely used to anal-
yse both therapist?s and patient?s discourses pro-
duced during psychotherapy sessions. In this pa-
per we present work in progress in the applica-
tion of Natural Language Processing (NLP) tech-
nology to the analysis of psychotherapy sessions in
the Spanish Language. Based on a psychoanalytic
theory, we are developing a set of NLP tools as
well as adapting an existing dictionary for the anal-
ysis of interviews. We investigate the application
of NLP techniques, including dictionary-based in-
terpretation, and speech act identification and clas-
sification for the automatic analysis of spoken tran-
scriptions in Spanish of psychoanalysis sessions be-
tween therapists and patients. In Figure 1 we show
a fragment of a manually transcribed interview in
Spanish (and its translation to English) from our de-
velopment corpus.
The automatic analysis of the sessions, which is
used as a tool for assessment and interpretation of
the transcribed psychotherapy sessions is based on
a theory developed by Liberman and extended by
Maldavsky (Liberman and Maldavsky, 1975) and
framed on Freudian theory (Freud, 1925). The au-
tomatic tools to be presented here aim at recogniz-
ing a subset of Freudian drives manifested in both
patient?s and therapist?s discourse.
The objective of the analysis is not to provide a
full automated solution to discourse interpretation
in this area, but a set of tools and resources to as-
sist therapists during discourse analysis. Although
work in text-based interpretation in psychology is
not new, researchers in our project have identified
limitations in current practices due to the fact that
current text-based systems do not tackle ambiguity
problems at lexical, syntactic, or semantic levels:
for example systems that consider out-of-context su-
perficial forms would are unable to distinguish be-
tween different used of the same lexical item (?para?
as a preposition vs. ?para? as a form of the verb
?parar? (to stop); ?rio? as a common noun vs. ?rio?
as a contextual clue for the identification of a geo-
graphical name; etc.). The use of advanced natural
language processing techniques could help produce
32
Transcribed Session (Spanish/English Version)
T: ?con que te cortaste? (T: What did you cut
yourself with?)
L: con un vidrio que encontre? en el patio (L: With
a glass I found in the patio.)
T: ?donde lo ten??as? (T: Where did you have it?)
L: en el locker, en la puertita del locker, y despue?s
lo puse en la jabonera cuando baje a ban?arme (L:
In the locker, in the locker?s small door, and then
I put it in the soap box when I went down to have
a bath.)
T: o sea, ya ten??as un vidrio escondido (T: so, you
already had the glass hidden.)
L: s???, ayer lo encontre? (L: Yes, I found it yester-
day.)
T: ?ayer a la tarde? (T: Yesterday afternoon?)
L: s??, s??, de ayer a la tarde (L: Yes, yes, yesterday
afternoon.)
Figure 1: Transcription of a small fragment of a therapy session in Spanish and its translation to English. T indicates
therapist and L indicates patient.
better analysis of the input material and therefore
be used for a better diagnosis and follow-up. It is
worth mentioning that full interpretation of therapy
sessions is not only based on textual analysis, but
also in other elements of the session such as the ac-
tual speech (e.g. pitch), para-verbal elements such
as patient movement, etc. This work addresses only
text interpretation issues.
The rest of the paper is organized as follows: Sec-
tion 2 describes related work in the area of com-
putational tools for text analysis in psychology. In
Section 3, the theoretical framework for our work is
briefly introduced. Section 4 describes the imple-
mentation of NLP tools for the analysis of the in-
terviews and Section 5 closes the paper describing
current and future work.
2 Related Work
There are a number of well-established compu-
tational tools for the analysis and extraction of
meaning from text in the social sciences (See
(Alexa and Zuell, 2000) for an overview of tools
and resources). Some tools are bound to particular
theoretical principles, for example the LWIC dic-
tionary (Pennebaker et al, 2001) encodes specific
categories to be identified in text while others follow
a theory-free approach (Iker and Klein, 1974) where
the theory emerges from the analysis of the data.
There has been substantial research in the de-
velopment of methods to analyze linguistic input
in the field of psychotherapy in order to measure a
number of psychological variables such as emotion,
abstraction, referential activity, etc. among them
Bucci?s Referential Activity (RA) non-weighted
(Bucci, 2002) and weighted dictionaries (Bucci and
Maskit, 2006) for the English language, or Ho?ltzer
and others? affective dictionary (Ho?lzer et al, 1997)
for the German language. The LIWC tool has been
used to detect different types of personalities in
written self-descriptions (Chung and Pennebaker,
2008). This program counts meaningful words
that express emotion, abstraction, verbal behavior,
demographic variables, traditional personality mea-
sures, formal and informal settings, deception and
honesty, emotional upheavals, social interaction,
use of cognitive and emotion words, word analysis
in psychotherapy, references to self and others.
For Spanish (Roussos and O?Connell, 2005) have
developed a dictionary in the area of psychotherapy
33
to measure referential activity.
Early work on dictionaries in the area of psy-
chology include the General Inquirer psycho-
sociological dictionary (Stone and Hunt, 1963)
which can be used in various applications; current
work on lexical resources for identifying particu-
lar text variables ? such as measuring strong/weak
opinions, sentiments, subjective/objective language,
etc. ? include the SentiWordnet resource (Esuli and
Sebastiani, 2006) derived from WordNet which has
been used in various opinion mining works (Devitt
and Ahmad, 2007); other lines of research include
the derivation of word-lists (semi) automatically for
opinion classification (Turney, 2002). To the best of
our knowledge, little research has been carried out
on natural language processing for discourse inter-
pretation in psychology.
3 Theoretical Framework Overview
Liberman?s theory identifies 7 drives (i.e., a sub-
set of Freud?s drives) which are introduced in Ta-
ble 1 we may associate these drives with emotional
or affective states such as: strong emotions asso-
ciated with IL; ecstasy or trance with O1; sadness
with O2; anger with A1; concrete language with
A2; warnings, suspense, and premonition with UPH
; and congratulation, adulation, and promises with
GPH. In diagnosis these variables are associated to
pathologies such as addiction, schizophrenia, de-
pression, paranoia, obsession, phobia, and hysteria;
so their manifestation in text is of paramount impor-
tance for diagnosis.
Abbreviation Drive Name
IL Intra-somatic libido
O1 Primary oral
O2 Secondary oral sadistic
A1 Primary anal sadistic
A2 Secondary anal sadistic
UPH Urethrae phallic
GPH Genital phallic
Table 1: Drives in Liberman and Maldavsky theory
The theory also associates lexicalizations to each
of the drives (Maldavsky, 2003), thus creating a se-
mantic dictionary with 7 categories, the main work-
Drive Lexicalisation
IL verbs: to throw up, to break; nouns: hos-
pital, throat; adjectives: sick, fat; ad-
verbs: fatally, greedily
O1 verbs: to sip, to suck; nouns: enigma,
research; adjectives: mystical, enlighten-
ing; adverbs: elliptically, enigmatically
O2 verbs: to feel, to feel like; nouns: feel-
ing, victim; adjectives: sensitive, happy,
sad; adverbs: fondly, obediently
A1 verbs: to bother, to kick; nouns: vio-
lence, transgression; adjectives: angry,
locked; adverbs: angrily,boldly, crossly
A2 verbs: must, to know; nouns: vice,
doubt; adjectives: good, bad; adverbs:
but, although, however
UPH verbs: to be able, to dare; nouns: scar,
precipice, wound; adjectives: coward,
scared; adverbs: almost, a bit
GPH verbs: to promise, to give; nouns:
beauty, ugliness; adjectives: wavy,
pretty; adverbs: more, even
Table 2: Sample of drives and associated lexicalisation
ing hypothesis is that drives manifest through lin-
guistic style, present at word level, phrase, and nar-
rative. Lexicalisations for each drive have been care-
fully selected following a variety of methods includ-
ing manual derivation of words from concepts, study
of texts where a scene is clearly present (e.g., every-
day activities), use of thesaurus, etc. Ambiguity is
preserved and a lexicalisation can signal more than
one drive. We show some lexicalisations in Table 2.
In addition to word-level analysis, the theory pro-
vides methods for analysis at narrative and speech
act level.
Speech acts are actions performed when making
an utterance (Searle, 1969) and they include (Searle,
1976) illocutionary (e.g. assert, suggest), perlocu-
tionary (e.g. convince, insult), and propositional
(e.g. making a reference) types. There has been sub-
stantial work on speech act segmentation and classi-
fication. Different authors adopt different classifica-
tions or theories of speech acts in order to restrict
the categories to those relevant for the purpose of
analysis. For example, in dialogue systems (Allen et
34
Drive Speech Acts
IL references to the state of things; reference
to body and body processes; etc.
O1 abstract deduction; negation; reference to
physical disomfort; etc.
O2 lamentation; complain; beg; etc.
A1 verbally abuse; provoke; confront; etc.
A2 judge; clarify; confirm; etc.
UPH forewarning; warning; inquest; counsel;
etc.
GPH congratulate; thank; promise; exaggerate;
etc.
Table 3: Drives and Speech Acts
al., 1996; Henry Prakken, 2000), the list of speech
acts may vary from 4 to 10 categories and it may
include acts such as assertion, WH-question, direc-
tives, greeting, direct/indicrect request, etc.
The pychoanalytic framework we are following has
its own inventory of speech acts. The objective is
also to link scenes in narratives and speech acts to
the 7 drives (in Table 1). There is a variety of speech
acts in the adopted framework, in Table 3 we present
a sample of speech acts associated to each of the
drives. The objective of the semi-automatic analysis
is to help their identification to facilitate the work of
the psychotherapist.
4 Text Analysis of Interviews
We have implemented a series of programs, lexical
resources, and grammars to process interviews and
other types of textual data in Spanish. We are us-
ing the GATE system (Maynard et al, 2002) as an
infrastructure or development framework; most de-
velopments are new, not included in the GATE sys-
tem, and they are packaged in a plug-in which can
be accessed through the GATE system or used stand-
alone. We have developed various programs to auto-
matically annotate the interviews including segmen-
tation of the transcription, word-based thematic seg-
mentation, tagging, and dictionary-based interpreta-
tion and analysis.
4.1 Dictionary
One of the main components of the system is a
dictionary which is taken as the basis for text inter-
pretation. This is being implemented as a language
resource in GATE. It is based on lists of word forms
which have been created for each of the drives.
The lists are organized according to their parts
of speech. The available dictionary (Maldavsky,
2003) contains all inflected forms of verbs, nouns,
adjectives, and adverbs which we are transforming
into a dictionary which will contain only roots. An
instance of the dictionary is created from the set of
lists and kept on-line for processing. The current
version of the dictionary (inflected forms) contains
over 298 thousand verb forms, over 22 thousand
noun forms, over 137 thousand adjectives, and over
9 thousand adverbs. An annotation tool has been
implemented based on a schema for our dictionary,
we use the graphical user interface functionalities
provided by the GATE infrastructure allowing a
researcher annotate words she may want to included
in the dictionary or segment the text in units for
further analysis.
4.2 Programs for Interviews? Interpretation
The following programs used for the automatic
analysis of the interviews.
? A wrapper to the TreeTagger parts of speech
package (Schmid, 1995) (See http://www.
clarin.eu/tools/treetagger) has
been implemented in order to call it from
the GATE system and an alignment program
has been developed to associate the output of
the tagger to the actual text of the interview,
therefore creating word annotations containing
features from the TreeTagger and additional
features computed by our programs. Note that
the TreeTagger distributed with GATE was
innapropriate for our purposes because it does
require tokenisation of the input performed
before invoking the tagger, this is the reason
why we had to create our own wrapper.
? A segmentation program is used to identify pa-
tient and therapist interventions.
? Text chuncking and named entity recognition
is being developed using Support Vector
Machines and training data from the CoNLL
35
evaluation program. We have created a train-
able system using machine learning resources
provided by the GATE framework. The
CoNLL 2002 Spanish dataset which provides
information on named entities such as Loca-
tion, Organization, Person, and Miscellaneous
was analyzed using parts-of-speech tagging,
morphological analysis, and gazetteer lookup
in order to derive a set of features for learn-
ing. A supports vector machine was trained
that uses gazetteer information, word level
information, orthography, parts-of-speech, and
lemmatization. We have collected a number
of lists to assist the identification of names
of organization, persons, locations, time ex-
pressions, etc. The performance of the current
system is at 68% F-score. Note that named
entity recognition is particularly important
to track names in longitudinal analysis of
interviews, but also to disambiguate names
which in Spanish are ambiguous (e.g. ?amado?
can be a person name in addition to a form of
the verb ?amar?; ?quito? can be the name of a
place in addition to a form of the verb ?quitar?,
etc.)
? A program uses the dictionary and interprets
each word or complex term according to the
drives in the dictionary taking into account
parts of speech information and named entity
recognition.
? A topic segmentation program has been
implemented to break the interview in frag-
ments which can be selected for fine-grained
interpretation. This module is based on tf*idf
similarity between candidate segments. A
second module we are implementing aims
at the recognition of segments referring to
prototypical scenes a patient may refer to:
family, work, love, health, money, etc. Further
gazetteer list information has been collected
from Spanish sources to create lexicons for
assisting the automatic identification of the
above categories. We are in the process of
manually annotating a set of transcriptions as
the basis for training a classification system for
this task. Conceptual information will be used
for this purpose.
? A processing resource has been implemented to
generate an interpretation of the different lan-
guages or drives? variables for different seg-
ments chosen by the human analyst (thera-
pist or patient or any other segment of inter-
est) and statistics are computed for each of the
segments; these can be exported for the ther-
apist to carry out additional analysis and in-
terpretation. Note that the current tool con-
siderably improves the previous practises in
dictionary-based interpretation, since the im-
plemented tool takes into account syntactic and
semantic information as a filter for interpreta-
tion.
4.3 Rule-based Speech Acts? Detection
We are carrying out induction sessions with psy-
chotherapits in order to capture ways in which
speech acts in the adopted framework are expressed.
The induction sessions provided valuable material
to start implementation of a rule-based speech act
detection program (with regular expressions and a
dictionary) based on use use of syntactic and lexical
information. These procedures allow us to collect a
set of expressions and lexical/syntactic patterns for
objective identification of a subset of speech acts.
We are also annotating the development corpus of
interviews (a total of 30 will be annotated with a
minimum of 2 annotators per interview) with speech
acts categories. Each speech segment is annotated
with one main speech act and a number (possibly
zero) subordinate speech acts. We are using the
GATE environment to provide appropriate support
for the annotation process. In Figure 2 we show a
fragment of interview in the annotation tool anno-
tated according to the interpretation of one of our
judges (the annotation window shows a ?complaint?
speech act associated to the fragment ?no me estaba
tratando de entender como e?l siempre hace? (?he did
not understand as he always does?)). We expect the
annotated corpus to be a valuable resource for the
development of a trainable speech act recognition
program based on lexical clues and syntactic infor-
36
mation. This trainable system will extend the rule-
based approach or incorporate the rule-based analy-
sis into it.
A sample of expressions we have identified and
implemented for a subset of speech acts is presented
in Table 4. The analysis of speech acts will provide
an additional level for drive?s identification.
5 Perspectives and Current Work
We have described our initial work on a set of tools
being developed for the analysis of psychotherapy
interviews in the Spanish language. The tools ex-
tend work on dictionary-based text interpretation by
incorporating NLP tools such as tagging, topic/scene
segmentation, speech act detection, and named en-
tity recognition. One main contribution of our re-
search is the implementation of a dictionary for the
Spanish language which can be used not only for the
identification of Freudian variables but also for work
on affective language and sentiment analysis. We
are currently working on the development of a full
module for speech-act recognition and on the cre-
ation of a corpus of annotated interviews which will
serve for further training and evaluation purposes.
The set of resources developed in the project will
be made available to the computational linguistics
community for research purposes. We think that al-
though this is work in progress it is worth mention-
ing evaluation. Where evaluation of the tools is con-
cerned, we are carrying out intrinsic evaluation com-
paring annotated categories against predicted cate-
gories currently for named entity recognition and
discourse segmentation and in the future for speech
act recognition and classification. Where more ex-
trinsic evaluation is concerned, we will evaluate how
the tools presented here can help theraphist in bet-
ter interpretation of clinical data. The implemented
tools will also be used to compare word-level based
interpretation produced by the dictionary to interpre-
tation produced by the analysis at speech act level.
Acknowledgements
We thank the reviewers for their very useful com-
ments. This work was partially supported by a grant
from the Royal Society (JP090069), UK. The first
author is grateful to Programa Ramo?n y Cajal 2009
from the Ministerio de Ciencia e Innovacio?n, Spain.
References
M. Alexa and C. Zuell. 2000. Text analysis software:
Commonalities, differences and limitations: The re-
sults of a review. Quality & Quantity, 34:299?231.
J. F. Allen, B. W. Miller, E. K. Ringger, and T. Sikorski.
1996. A robust system for natural spoken dialogue.
In Proceedings of the 34th annual meeting on Asso-
ciation for Computational Linguistics, pages 62?70,
Morristown, NJ, USA. Association for Computational
Linguistics.
W. Bucci and B. Maskit. 2006. A Weighted Referential
Activity Dictionary. In Computing Attitude and Affect
in Text: Theory and Applications, volume 20 of The
Information Retrieval Series, pages 49?60. Springer
Verlag.
W. Bucci. 2002. Referential Activity (RA): Scales and
computer procedures. In An Open Door Review of
Outcome Studies in Psychoanalysis. International Psy-
choanalytical Association.
C.K. Chung and J.W. Pennebaker. 2008. Revealing di-
mensions of thinking in open-ended self-descriptions:
An automated meaning extraction method for natural
language. Journal of Research in Personality, 42:96?
132.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 984?991, Prague, Czech Republic, June.
Association for Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource for
opinion mining. In Proceedings of LREC-06, 5th Con-
ference on Language Resources and Evaluation, pages
417?422, Genova, IT.
S. Freud. 1925. Obras Completas. Amorrortu (Eds.),
Madrid, Spain.
H. Henry Prakken. 2000. On dialogue systems
with speech acts, arguments, and counterarguments.
In Logics in Artificial Intelligence, pages 224?238.
Springer Verlag.
M. Ho?lzer, D. Pokorny, H. Ka?chele, and L. Luborsky.
1997. The Verbalization of Emotions in the Thera-
peutic Dialogue-A Correlate of Therapeutic Outcome?
Psychotherapy Research, 7(3):261?273.
H.P. Iker and R. Klein. 1974. WORDS: A computer
system for the analysis of content. Behavior Research
Methods and Instrumentation, 6:430?438.
D. Liberman and D. Maldavsky. 1975. Psicoanlisis y
semitica. Paidos, Buenos Aires, Argentina.
D. Maldavsky. 2003. La investigacin psicoanaltica del
lenguaje: algoritmo David Liberman. Editorial Lugar,
Buenos Aires, Argentina.
37
Speech Act Pattern or Expression
beg PPX + rogar | implorar | suplicar
demand PPX + exhortar | exigir | demandar | perdir
demand recognition decir que esta bien | correcto | perfecto |
bueno; esta? bien, no?
demand forgiveness PPX + perdonar
justify por que; por eso; debido a que; por esa razo?n
permission con PPO permiso; pedir; PPX + dejar
interrupt para... para; espera...; ah me olvide...
cite como dijo NP | PPX ; segu?n NP | PPX ; de
acuerdo con NP | PPX
sinthesis en resumen; para concluir; en s??ntesis
doubt no PPX quedar | ser | estar claro; quien sabe
trust/distrust no confiar | desconfiar; confiar | desconfiar
submission tener razo?n; no + PPX + enojar
appeal decime que me quere?s; ...
compassion/self-compassion me da pena; pobre; pobrecito;...
sacrifice yo que hice todo esto; yo que te di todo; si no
fuera por mi; ...
Table 4: Speech Acts and Lexical/Syntactic Patterns (PPX = pronouns; NP = proper nouns; PPO = possessive)
D. Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Sag-
gion, K. Bontcheva, and Y.Wilks. 2002. Architectural
Elements of Language Engineering Robustness. Jour-
nal of Natural Language Engineering ? Special Issue
on Robust Methods in Analysis of Natural Language
Data, 8(2/3):257?274.
J.W. Pennebaker, M.E. Francis, and R.j. Both. 2001.
Linguistic Inquiry and Word Count (LIWC). Erlbaum
Publishers.
A. Roussos and M. O?Connell. 2005. Construccio?n de
un diccionario ponderado en espan?ol para medir la Ac-
tividad Referencial. Revista del Instituto de Investiga-
ciones de la Facultad de Psicolog??a - UBA, 10(2):99?
119.
H. Schmid. 1995. Improvements in part-of-speech tag-
ging with an application to german. In In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50.
J. Searle. 1969. Speech acts: An essay in the philosophy
of language. Cambridge University Press.
John R. Searle. 1976. A classification of illocutionary
acts. Language in Society, 5(1):1?23.
P. J. Stone and E. B. Hunt. 1963. A Computer Ap-
proach to Content Analysis: Studies using the General
Inquirer System. In Proceedings of the Spring Joint
Computer Conference, pages 241?256, NewYork, NY,
USA. ACM.
P. D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics (ACL ?02), pages 417?424, Morristown, NJ, USA,
July. Association for Computational Linguistics.
38
Figure 2: Speech Acts Segmentation and Interpretation
39
Workshop on Monolingual Text-To-Text Generation, pages 20?26,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 20?26,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
An Unsupervised Alignment Algorithm for Text Simplification Corpus
Construction
Stefan Bott
TALN Research Group
Universitat Pompeu Fabra
C/Tanger 122 - Barcelona - 08018
Spain
stefan.bott@upf.edu
Horacio Saggion
TALN Research Group
Universitat Pompeu Fabra
C/Tanger 122 - Barcelona - 08018
Spain
horacio.saggion@upf.edu
Abstract
We present a method for the sentence-level
alignment of short simplified text to the orig-
inal text from which they were adapted. Our
goal is to align a medium-sized corpus of par-
allel text, consisting of short news texts in
Spanish with their simplified counterpart. No
training data is available for this task, so we
have to rely on unsupervised learning. In con-
trast to bilingual sentence alignment, in this
task we can exploit the fact that the probability
of sentence correspondence can be estimated
from lexical similarity between sentences. We
show that the algoithm employed performs
better than a baseline which approaches the
problem with a TF*IDF sentence similarity
metric. The alignment algorithm is being used
for the creation of a corpus for the study of
text simplification in the Spanish language.
1 Introduction
Text simplification is the process of transforming a
text into an equivalent which is more understand-
able for a target user. This simplification is bene-
ficial for many groups of readers, such as language
learners, elderly persons and people with other spe-
cial reading and comprehension necessities. Simpli-
fied texts are characterized by a simple and direct
style and a smaller vocabulary which substitutes in-
frequent and otherwise difficult words (such as long
composite nouns, technical terms, neologisms and
abstract concepts) by simpler corresponding expres-
sions. Usually unnecessary details are omitted. An-
other characteristic trait of simplified texts is that
usually only one main idea is expressed by a single
sentence. This also means that in the simplification
process complex sentences are often split into sev-
eral smaller sentences.
The availability of a sentence-aligned corpus
of original texts and their simplifications is of
paramount importance for the study of simplifica-
tion and for developing an automatic text simplifi-
cation system. The different strategies that human
editors employ to simplify texts are varied and have
the effect that individual parts of the resulting text
may either become shorter or longer than the orig-
inal text. An editor may, for example, delete de-
tailed information, making the text shorter. Or she
may split complex sentences into various smaller
sentences. As a result, simplified texts tend to be-
come shorter than the source, but often the number
of sentences increases. Not all of the information
presented in the original needs to be preserved but in
general all of the information in the simplified text
stems from the source text.
The need to align parallel texts arises from a larger
need to create a medium size corpus which will al-
low the study of the editing process of simplifying
text, as well as to serve as a gold standard to evalu-
ate a text simplification system.
Sentence alignment for simplified texts is related
to, but different from, the alignment of bilingual text
and also from the alignment of summaries to an orig-
inal text. Since the alignment of simplified sentences
is a case of monolingual alignment the lexical sim-
ilarity between two corresponding sentences can be
taken as an indicator of correspondence.
This paper is organized as follows: Section 2
briefly introduces text simplification which contex-
20
tualises this piece of research and Section 3 dis-
cusses some related work. In Section 4 we briefly
describe the texts we are working with and in Sec-
tion 5 we present the alignment algorithm. Section 6
presents the details of the experiment and its results.
Finally, section 7 gives a concluding discussion and
an outlook on future work.
2 Text Simplification
The simplification of written documents by humans
has the objective of making texts more accessible to
people with a linguistic handicap, however manual
simplification of written documents is very expen-
sive. If one considers people who cannot read doc-
uments with heavy information load or documents
from authorities or governmental sources the percent
of need for simplification is estimated at around 25%
of the population, it is therefore of great importance
to develop methods and tools to tackle this problem.
Automatic text simplification, the task of transform-
ing a given text into an ?equivalent? which is less
complex in vocabulary and form, aims at reducing
the efforts and costs associated with human simpli-
fication. In addition to transforming texts into their
simplification for human consumption, text simpli-
fication has other advantages since simpler texts can
be processed more efficiently by different natural
language processing processors such as parsers and
used in applications such as machine translation, in-
formation extraction, question answering, and text
summarization.
Early attempts to text simplification were based
on rule-based methods where rules were designed
following linguistic intuitions (Chandrasekar et al,
1996). Steps in the process included linguistic text
analysis (including parsing) and pattern matching
and transformation steps. Other computational mod-
els of text simplification included processes of anal-
ysis, transformation, and phrase re-generation (Sid-
dharthan, 2002) also using rule-based techniques.
In the PSET project (Carroll et al, 1998) the pro-
posal is for a news simplification system for aphasic
readers and particular attention is paid to linguistic
phenomena such as passive constructions and coref-
erence which are difficult to deal with by people
with disabilities. The PorSimples project (Alu??sio et
al., 2008) has looked into simplification of the Por-
tuguese language. The methodology consisted in the
creation of a corpus of simplification at two different
levels and on the use of the corpus to train a deci-
sion procedure for simplification based on linguistic
features. Simplification decisions about whether to
simplify a text or sentence have been studied fol-
lowing rule-based paradigms (Chandrasekar et al,
1996) or trainable systems (Petersen and Ostendorf,
2007) where a corpus of texts and their simplifica-
tions becomes necessary. Some resources are avail-
able for the English language such as parallel cor-
pora created or studied in various projects (Barzilay
and Elhadad, 2003; Feng et al, 2009; Petersen and
Ostendorf, 2007; Quirk et al, 2004); however there
is no parallel Spanish corpus available for research
into text simplification. The algorithms to be pre-
sented here will be used to create such resource.
3 Related Work
The problem of sentence alignment was first tack-
led in the context of statistical machine translation.
Gale and Church (1993) proposed a dynamic pro-
gramming algorithm for the sentence-level align-
ment of translations that exploited two facts: the
length of translated sentences roughly corresponds
to the length of the original sentences and the se-
quence of sentences in translated text largely corre-
sponds to the original order of sentences. With this
simple approach they reached a high degree of accu-
racy.
Within the field of monolingual sentence align-
ment a large part of the work has concentrated on the
alignment between text summaries and the source
texts they summarize. Jing (2002) present an al-
gorithm which aligns strings of words to pieces of
text in an original document using a Hidden Markov
Model. This approach is very specific to summary
texts, concretely such summaries which have been
produced by a ?cut and paste? process. A work
which is more closely related to our task is pre-
sented in Barzilay and Elhadad (2003). They carried
out an experiment on two different versions of the
Encyclopedia Britannica (the regular version and
the Britannica Elementary) and aligned sentences
in a four-step procedure: They clustered paragraphs
into ?topic? groups, then they trained a binary clas-
sifier (aligned or not aligned) for paragraph pairs
21
on a handcrafted set of sentence alignments. Af-
ter that they grouped all paragraphs of unseen text
pairs into the same topic clusters as in the first step
and aligned the texts on the paragraph level, al-
lowing for multiple matches. Finally they aligned
the sentences within the already aligned paragraphs.
Their similarity measure, both for paragraphs and
sentences, was based on cosine distance of word
overlap. Nelken and Shieber (2006) improve over
Barzilay and Elhadad?s work: They use the same
data set, but they base their similarity measure for
aligning sentences on a TF*IDF score. Although
this score can be obtained without any training, they
apply logistic regression on these scores and train
two parameters of this regression model on the train-
ing data. Both of these approaches can be tuned by
parameter settings, which results in a trade-off be-
tween precision and recall. Barzilay and Elhadad
report a precision of 76.9% when the recall reaches
55.8%. Nelken and Shieber raise this value to 83.1%
with the same recall level and show that TF*IDF is
a much better sentence similarity measure. Zhu et
al. (2010) even report a precision of 91.3% (at the
same fixed recall value of 55.8%) for the alignment
of simple English Wikipedia articles to the English
Wikipedia counterparts using Nelken and Shieber?s
TF*IDF score, but their alignment was part of a
larger problem setting and they do not discuss fur-
ther details.
We consider that our task is not directly compara-
ble to this previous work: the texts we are working
with are direct simplifications of the source texts. So
we can assume that all information in the simplified
text must stem from the original text. In addition we
can make the simplifying assumption that there are
one-to-many, but no many-to-one relations between
source sentences and simplified sentences, a simpli-
fication which largely holds for our corpus. This
means that all target sentences must find at least one
alignment to a source sentence, but not vice versa.
Nelken and Shieber make the interesting observa-
tion that dynamic programming, as used by Gale
and Church (1991) fails to work in the monolingual
case. Their test data consisted of pairs of encyclo-
pedia articles which presented a large intersection
of factual information, but which was not necessar-
ily presented in the same order. The corpus we are
working with, however, largely preserves the order
in which information is presented.
4 Dataset
We are working with a corpus of 200 news arti-
cles in Spanish covering the following topics: Na-
tional News, Society, International News and Cul-
ture. Each of the texts is being adapted by the DILES
Research Group from Universidad Auto?noma de
Madrid (Anula, 2007). Original and adapted ex-
amples of texts in Spanish can be seen in Figure 1
(the texts are adaptations carried out by DILES for
Revista ?La Plaza?). The texts are being processed
using part-of-speech tagging, named entity recogni-
tion, and parsing in order to create an automatically
annotated corpus. The bi-texts are first aligned us-
ing the tools to be described in this paper and then
post-edited with the help of a bi-text editor provided
in the GATE framework (Cunningham et al, 2002).
Figure 2 shows the texts in the alignment editor.
This tool is however insufficient for our purposes
since it does not provide mechanisms for uploading
the alignments produced outside the GATE frame-
work and for producing stand-alone versions of the
bi-texts; we have therefore extended the functionali-
ties of the tool for the purpose of corpus creation.
5 Algorithm
Our algorithm is based on two intuitions about sim-
plified texts (as found in our corpus): As repeatedly
observed sentences in simplified texts use similar
words to those in the original sentences that they
stem from (even if some of the words may have
undergone lexical simplification). The second ob-
servation is very specific to our data: the order in
which information is presented in simplified texts
roughly corresponds to the order of the information
in the source text. So sentences which are close to
each other in simplified texts correspond to original
sentences which are also close to each other in the
source text. In many cases, two adjacent simplified
sentences even correspond to one single sentence in
the source text. This leads us to apply a simple Hid-
den Markov Model which allows for a sequential
classification.
Firstly, we define an alignment as a pair of sen-
tences as
?source senti, target sentj?,
22
Original Text Adapted Text
Un Plan Global desde tu hogar
El Programa GAP (Global Action Plan) es una iniciativa
que se desarrolla en distintos pa??ses y que pretende dis-
minuir las emisiones de CO2, principales causantes del
cambio clima?tico y avanzar hacia ha?bitos ma?s sostenibles
en aspectos como el consumo de agua y energ??a, la
movilidad o la gestio?n de los residuos dome?sticos.
San Sebastia?n de los Reyes se ha adherido a este Pro-
grama.
Toda la informacio?n disponible para actuar desde el
hogar en la construccio?n de un mundo ma?s sostenible se
puede encontrar en ssreyes.org o programagap.es.
Un Plan Global desde tu hogar
San Sebastia?n de los Reyes se ha unido al Plan de Accio?n
Global (GAP).
El Plan es una iniciativa para luchar contra el cambio
clima?tico desde tu casa.
Los objetivos del Plan son:
Disminuir nuestros gastos dome?sticos de agua y energ??a.
Reducir los efectos dan?inos que producimos en el planeta
con nuestros residuos.
Mejorar la calidad de vida de nuestra ciudad.
Tienes ma?s informacio?n en ssreyes.org y en programa-
gap.es.
Apu?ntate al programa GAP y desca?rgate los manuales
con las propuestas para conservar el planeta.
Figure 1: Original Full Document and Easy-to-Read Version
Figure 2: The Alignment Editor with Text and Adaptation
where a target sentence belongs to the simplified
text and the source sentence belongs to the original
sentence. Applying standard Bayesian decomposi-
tion, the probability of an alignment to a given target
text can be calculated as follows:
P (alignn1 |target sent
m
1 ) =
P (alignn1 )P (target sent
m
1 |align
n
1 )
P (target sentm1 )
Since P (target sentm1 ) is constant we can calcu-
late the most probable alignment sequence a?lign as
follows:
a?lign =
arg maxP (alignn1 ) P (target sent
m
1 |align
n
1 ) =
arg max
? n
i=1P (aligni,j)P (target sentj |aligni,j)
This leaves us with two measures: a measure
of sentence similarity (the probability of alignment
proper) and a measure of consistency, under the as-
sumption that a consistent simplified text presents
the information in the same order as it is presented
in the source text. In order to determine a?lign, we
apply the Viterbi algorithm (Viterbi, 1967).
Sentence similarity can be calculated as follows:
P (wordl1|target sentj) =
? l
k=1
P (target sentj |wordk)P (target sentj)
P (wordk)
where wordl1 is the sequence of words in the
source sentence i and l is the length of sentence i.
This similarity measure is different from both
word overlap cosine distance and TF*IDF. It is,
however, similar to TF*IDF in that it penalizes
23
words which are frequent in the source text and
boosts the score for less frequent words. In addi-
tion we eliminated a short list of stopwords from the
calculation, but this has no significant effect on the
general performance.
Note that P (wordk) may correspond to a MLE
of 0 since simplified texts often use different (and
simpler) words and add connectors, conjunctions
and the like. For this reason we have to recalcu-
late P (wordk) according to a distortion probability
?. Distortion is taken here as the process of word
insertion or lexical changes. ? is a small constant,
which could be determined empirically, but since no
training data is available we estimated ? for our ex-
periment and set it by hand to a value of 0.0075.
Even if we had to estimate ? we found that the per-
formance of the system is robust regarding its value:
even for unrealistic values like 0.0001 and 0.1 the
performance only drops by two percent points.
P (wordk|distortion) =
(1 ? ?)P (wordk) + ?(1 ? P (wordk))
For the consistency measure we made the
Markov assumption that each alignment aligni,j
only depends on the proceeding alignment
aligni?1,j? . We assume that this is the proba-
bility of a distance d between the corresponding
sentences of source senti?1 and source senti, i.e.
P (source senti|aligni?1,j?k) for each possible
jump distance k. Since long jumps are relatively
rare, we used a normalized even probability dis-
tribution for all jump lengths greater than 2 and
smaller than -1.
Since we have no training data, we have to ini-
tially set these probabilities by hand. We do this
by assuming that all jump distances k in the range
between -1 and 2 are distributed evenly and larger
jump distances have an accumulated probability
mass corresponding to one of the local jump dis-
tances. Although this model for sentence transitions
is apparently oversimplistic and gives a very bad es-
timate for each P (source senti|aligni?1,j?k), the
probabilities for P (alignn1 ) give a counterweight to
these bad estimates. What we can expect is, that af-
ter running the aligner once, using very unreliable
transitions probability estimates, the output of the
aligner is a set of alignments with an implicit align-
ment sequence. Taking this alignment sequence, we
can calculate newmaximum likelihood estimates for
each jump distance P (source senti|aligni?1,j?k)
again, and we can expect that these new estimates
are much better than the original ones.
For this reason we apply the Viterbi classifier it-
eratively: The first iteration employs the hand set
values. Then we run the classifier and determine
the values for P (source senti|aligni?1,j?k) on its
output. Then we run the classifier again, with the
new model and so on. Interestingly values for
P (source senti|aligni?1,j?k) emerge after as little
as two iterations. After the first iteration, precision
already lies only 1.2 percent points and recall 1.3
points below the stable values. We will comment on
this finding in Section 7.
6 Experiment and Results
Our goal is to align a larger corpus of Spanish short
news texts with their simplified counterparts. At the
moment, however, we only have a small sample of
this corpus available. The size of this corpus sam-
ple is 1840 words of simplified text (145 sentences)
which correspond to 2456 (110 sentences) of source
text. We manually created a gold standard which in-
cludes all the correct alignments between simplified
and source sentences. The results of the classifier
were calculated against this gold standard.
As a baseline we used a TF*IDF score based
method which chooses for each sentence in the sim-
plified text the sentence with the minimal word vec-
tor distance. The procedure is as follows: each sen-
tence in the original and simplified document is rep-
resented in the vector space model using a term vec-
tor (Saggion, 2008). Each term (e.g. token) is wei-
thed using as TF the frequency of the term in the
document and IDF = log(N +1/Mt +1) where Mt
is the number of sentences 1 containing t and N is
the number of sentences in the corpus (counts are
obtained from the set of documents to align). As
similarity metric between vectors we use the cosine
of the angle between the two vectors given in the
following formula:
1The relevant unit for the calculation of IDF (the D in IDF)
here is the sentence, not the document as in information re-
trieval.
24
cosine(s1, s2) =
?n
i=1 wi,s1 ? wi,s2??n
i=1(wi,s1)
2 ?
??n
i=1(wi,s2)
2
Here s1 and s2 are the sentence vectors and wi,sk
is the weight of term i in sentence sk. We align all
simplified sentences (i.e. for the time being no cut-
off has been used to identify newmaterial in the sim-
plified text).
For the calculation of the first baseline we calcu-
late IDF over the sentences in whole corpus. Nelken
and Shieber (2006) argue that that the relevant unit
for this calculation should be each document for the
following reason: Some words are much more fre-
quent in some texts than they are in others. For ex-
ample the word unicorn is relatively infrequent in
English and it it may also be infrequent in a given
colletion of texts. So this word is highly discrimina-
tive and it?s IDF will be relatively high. In a specific
text about imagenary creatures, however, the same
word unicornmay be much more frequent and hence
it?s discrimiative power is much lower. For this rea-
son we calcuated a second baseline, where we cal-
culate the IDF only on the sentences of the relevanct
texts.
Results of aligning all sentences in our sample
corpus using both the baseline and the HMM algo-
rithms are given in Table 6.
precision recall
HMM aligner 82.4% 80.9%
alignment only 81.13% 79.63%
TF*IDF + transitions 76.1% 73.5%
TF*IDF (document) 75.47% 74.07%
TF*IDF (full corpus) 62.2% 61.1%
If we compare these results to those presented by
Nelken and Shieber (2006), we can observe that we
obtain a comparable precision, but the recall im-
proves dramatically from 55.8% (with their specific
feature setting) to 82.4%. Our TF*IDF baselines
are not directly comparable comparable to Nelken
and Shieber?s results. The reason why we can-
not compare our results directly is that Nelken and
Shieber use supervised learning in order to optimize
the transformation of TF*IDF scores into probabili-
ties and we had no training data available.
We included the additional scores for our system,
when no transition probabilities are included in the
calculation of the optimal alignment sequence and
the score comes only from the probabilies of our
clalculation of lexical similarity between sentences
(alignment only). These scores show that a large part
of the good performance comes from lexical similar-
ity and sequencial classification only give an addi-
tional final boost, a fact which was already observed
by Nelken and Shieber. We also attribute the fact
that the system alrives at stable values after two it-
erations to the same efect: lexical similarity seems
to have a much bigger effect on the general perfor-
mance. Still our probability-based similarity meas-
sure clearly outperforms the TF*IDF baselines.
7 Discussion and Outlook
We have argued above that our task is not directly
comparable to Nelken and Shieber?s alignment of
two versions of Encyclopedia articles. First of all,
the texts we are working with are simplified texts in
a much stricter sense: they are the result of an edit-
ing process which turns a source text into a simpli-
fied version. This allows us to use sequential classi-
fication which is usually not successful for mono-
lingual sentence alignment. This helps especially
in the case of simplified sentences which have been
largely re-written with simpler vocabulary. These
cases would normally be hard to align correctly. Al-
though it could be argued that the characteristics of
such genuinely simplified text makes the alignment
task somewhat easier, we would like to stress that
the alignment method we present makes no use of
any kind of training data, in contrast to Barzilay and
Elhadad (2003) and, to a minor extent, Nelken and
Shieber (2006).
Although we started out from a very specific need
to align a corpus with reliably simplified news arti-
cles, we are confident that our approach can be ap-
plied in other circumstances. For future work we
are planning to apply this algorithm in combina-
tion of a version of Barzilay and Elhadad?s macro-
alignment and use sequential classification only for
the alignment of sentences within already aligned
paragraphs. This would make our work directly
comparable. We are also planning to test our algo-
rithm, especially the sentence similarity measure it
uses, on data which is similar the data Barzilay and
Elhadad (and also Nelken and Shieber) used in their
25
experiment.
Finally, the alignment tool will be used to
sentence-align a medium-sized parallel Spanish cor-
pus of news and their adaptations that will be a much
needed resource for the study of text simplification
and other natural language processing applications.
Since the size of the corpus we have available at
the moment is relatively modest, we are also investi-
gating alternative resources which could allow us to
create a larger parallel corpus.
Acknowledgments
We thank three anonymous reviewers for their com-
ments and suggestions which help improve the fi-
nal version of this paper. The research described
in this paper arises from a Spanish research project
called Simplext: An automatic system for text sim-
plification (http://www.simplext.es). Sim-
plext is led by Technosite and partially funded by
the Ministry of Industry, Tourism and Trade of the
Government of Spain, by means of the National
Plan of Scientific Research, Development and Tech-
nological Innovation (I+D+i), within strategic Ac-
tion of Telecommunications and Information Soci-
ety (Avanza Competitiveness, with file number TSI-
020302-2010-84). We thanks the Department of In-
formation and Communication Technologies at UPF
for their support. We are grateful to Programa
Ramo?n y Cajal from Ministerio de Ciencia e Inno-
vacio?n, Spain.
References
Sandra M. Alu??sio, Lucia Specia, Thiago Alexan-
dre Salgueiro Pardo, Erick Galani Maziero, and Re-
nata Pontin de Mattos Fortes. 2008. Towards brazil-
ian portuguese automatic text simplification systems.
In ACM Symposium on Document Engineering, pages
240?248.
A. Anula. 2007. Tipos de textos, complejidad lingu???stica
y facilicitacio?n lectora. In Actas del Sexto Congreso
de Hispanistas de Asia, pages 45?61.
Regina Barzilay and Noemi Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
In Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?32.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
In Proc. of AAAI-98 Workshop on Integrating Artificial
Intelligence and Assistive Technology, pages 7?10.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text sim-
plification. In COLING, pages 1041?1044.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics.
Lijun Feng, Noemie Elhadad, and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In EACL, pages 229?237.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics.
Hongyan Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Comput. Lin-
guist., 28:527?543, December.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In In 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Sarah E. Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis. In
In Proc. of Workshop on Speech and Language Tech-
nology for Education.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
H. Saggion. 2008. SUMMA: A Robust and Adapt-
able Summarization Tool. Traitement Automatique
des Langues, 49(2):103?125.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In In LEC 02: Proceedings of
the Language Engineering Conference (LEC02, pages
64?71.
A. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Transactions on Information Theory, 13:260?
269.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of The 23rd
International Conference on Computational Linguis-
tics, pages 1353?1361, Beijing, China, Aug.
26
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 8?16,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Towards Automatic Lexical Simplification in Spanish: An Empirical Study
Biljana Drndarevic? and Horacio Saggion
Universitat Pompeu Fabra
Department of Information and Communication Technologies
C/ Tanger, 122-140
08018 Barcelona, Spain
{biljana.drndarevic,horacio.saggion}@upf.edu
Abstract
In this paper we present the results of the anal-
ysis of a parallel corpus of original and simpli-
fied texts in Spanish, gathered for the purpose
of developing an automatic simplification sys-
tem for this language. The system is intended
for individuals with cognitive disabilities who
experience difficulties reading and interpret-
ing informative texts. We here concentrate
on lexical simplification operations applied by
human editors on the basis of which we derive
a set of rules to be implemented automatically.
We have so far addressed the issue of lexical
units substitution, with special attention to re-
porting verbs and adjectives of nationality; in-
sertion of definitions; simplification of numer-
ical expressions; and simplification of named
entities.
1 Introduction
In the highly digitalized 21st century sharing infor-
mation via Internet has become not only common-
place but also essential. Yet, there are still a large
number of people who are denied this fundamental
human right ? access to information. In 2006 the UN
conducted an audit with the aim of testing the state
of accessibility of the leading websites around the
world. The results were rather disappointing, with
only three out of 100 tested web pages achieving
basic accessibility status. It is therefore clear that
one of the priorities for the future is working on en-
abling inclusion of all the groups that are currently
marginalised and denied equal access to information
as the rest of the population.
Written information available online is far too of-
ten presented in a way that is perceived as incom-
prehensible to individuals with cognitive disabili-
ties. It is therefore necessary to simplify the com-
plex textual content in order to make it more acces-
sible. However, manual simplification is too time-
consuming and little cost-effective so as to yield suf-
ficient amount of simplified reading material in a
satisfactory time frame. Hence, the need and in-
terest arise to develop automatic or semi-automatic
simplification tools that would (partially) substitute
humans in carrying out this laborious task.
Our project is one such aspiration. Our goal is to
offer an automated text simplification tool for Span-
ish, targeted at readers with cognitive disabilities.
We delimit our research to simplification of infor-
mative texts and news articles. So far we have fo-
cused primarily on syntactic simplification, with an
already implemented module currently in the test
stage (Bott and Saggion, 2012b). The present work,
however, deals with lexical simplification and is cen-
tred around a corpus analysis, a preparatory stage for
the development of a separate lexical module in the
future.
Earlier work already establishes the importance of
lexical changes for text simplification (Carroll et al,
1998; Caseli et al, 2009; De Belder et al, 2010).
Upon examining a parallel corpus consisting of orig-
inal and manually simplified newspaper articles in
Spanish, we have found that by far the most com-
mon type of changes applied by human editors are
precisely lexical changes, accounting for 17.48% of
all annotated operations (Bott and Saggion, 2012a).
Words perceived as more complicated are replaced
8
by their simpler synonyms. A recurring example
is that of reporting verbs. Corpus analysis shows a
clear tendency towards replacing all reporting verbs
such as advertir (warn), afirmar (declare), explicar
(explain), etc. with the ubiquitous decir (say). Sen-
tences 1 (original) and 2 (simplified) illustrate the
said phenomenon (translated into English):
1. It is important that we continue working on the
means that promote the access of the disabled
to cultural content, she explained.
2. The Minister of Culture said that she is work-
ing towards granting the disabled access to cul-
tural content.
We therefore document all cases of lexical change
observed in the corpus and try to extract rules for
their automatic implementation. The remainder of
this paper is organized as follows: Section 2 ad-
dresses the related work in the field; in Section 3 we
describe the experimental setting and the process of
obtaining the parallel corpus used in the study, while
Section 4 provides a more detailed insight into the
kind of lexical simplifications observed. We con-
clude in Section 5 and outline our future work.
2 Related Work
Text simplification has so far been approached with
two different aims. One is to offer simplified ver-
sions of original text to human readers, such as
foreign language learners (Petersen and Ostendorf,
2007; Medero and Ostendorf, 2011); aphasic peo-
ple (Devlin and Unthank, 2006); low literacy in-
dividuals (Specia, 2010) and others. On the other
hand, simplified text is seen as input for further nat-
ural language processing to enhance its proficiency,
e.g. in machine translation or information retrieval
tools (Klebanov et al, 2004). The earliest simplifi-
cation systems employed a rule-based approach and
focused on syntactic structure of the text (Chan-
drasekar et al, 1996). The PSET project (Carroll et
al., 1998) dealt with simplification of news articles
in English for aphasic readers. Together with syn-
tactic analysis and transformations similar to those
of Chandrasekar et al (1996), they employed lexi-
cal simplification based on looking up synonyms in
WordNet and extracting Kucera-Francis frequency
from the Oxford Psycholinguistic Database (Quin-
lan, 1992). Therefore, the most frequent of a set of
synonyms for every content word of the input text
was chosen to appear in its simplified version.
The above approach to lexical simplification has
been repeated in a number of works (Lal and Ruger,
2002; Burstein et al, 2007). Bautista et al (2009)
also rely on a dictionary of synonyms, but their crite-
rion for choosing the most appropriate one is word-
length rather than frequency. Caseli et al (2009)
analyse lexical operations on a parallel corpus of
original and manually simplified texts in Portuguese,
using lists of simple words and discourse markers as
resources. Bautista et al (2011) focused on numeri-
cal expressions as one particular problem of lexical
simplification and suggested the use of hedges as a
means of dealing with complex numerical content.
Given the fact that many words tend to be poly-
semic, attempts have been made to address this is-
sue so as to provide more accurate, context-aware
lexical substitution. De Belder et al (2010) were
the first to employ word sense disambiguation tech-
niques in order to capture contextual information,
while Biran et al (2011) apply an unsupervised
method for learning pairs of complex and simple
synonyms based on an unaligned corpus of texts
from the original Wikipedia and Simple English
Wikipedia.
3 Experimental Setting
We have gathered a corpus consisting of 200 in-
formative texts in Spanish, obtained from the news
agency Servimedia. The articles have been clas-
sified into four categories: national news, interna-
tional news, society and culture. We then obtained
simplified versions of the said texts, courtesy of the
DILES (Discurso y Lengua Espan?ola) group of the
Autonomous University of Madrid. Simplifications
have been applied manually, by trained human ed-
itors, following easy-to-read guidelines suggested
by Anula (2009), (2008). We are interested to see
how these guidelines are applied in practice, as well
as how human editors naturally deal with cases not
treated by the guidelines in sufficient detail.
The corpus has been automatically annotated us-
ing part-of-speech tagging, named entity recognition
and parsing (Padro? et al, 2010). Furthermore, a text
9
aligning algorithm based on Hidden Markov Mod-
els (Bott and Saggion, 2011) has been applied to ob-
tain sentence-level alignments. The automatic align-
ments have then been manually corrected through a
graphical editing tool within the GATE framework
(Cunningham et al, 2002). A total of 570 sentences
have been aligned (246 in original and 324 in sim-
ple texts), with the following correlations between
them: one to one, one to many or many to one, as
well as cases where there is no correlation (cases of
content reduction through summarisation or infor-
mation expansion through the introduction of defini-
tions). The alignments facilitate the observation of
the corpus, particularly cases where entire sentences
have been eliminated or inserted.
A parallel corpus thus aligned enables us to en-
gage in data analysis as well as possibly carry out
machine learning experiments to treat specific prob-
lems we have so far detected. We have documented
all simplification operations used by human editors
and placed them in eight major categories applied at
various linguistic levels (individual words, phrases
or sentences). The operations are change, delete, in-
sert, split, proximization, re-order, select and join,
listed in the decreasing order of their relative fre-
quency in the corpus. Among these are the changes
that are either rather idiosyncratic or involve com-
plex inferential processes proper to humans but not
machines. Sentence 1 (original) and paragraph 2
(simplified) are an example (translated into English):
1. Around 390,000 people have returned to their
homes after being forced to evacuate due to
floods caused by monsoon rains last summer in
Pakistan.
2. Last summer it rained a lot in Pakistan. The
rain flooded the fields and the houses. That is
to say, the water covered the houses and the
fields. For this reason a lot of people left their
homes in Pakistan. Now these people return to
their homes.
Sentences in bold are examples of information ex-
pansion which is difficult to implement automati-
cally. The concept of flood is obviously perceived
as complicated. However, instead of offering a defi-
nition taken out of a dictionary and applicable to any
context (as in the example further below), the writer
explains what happened in this particular instance,
relying on their common knowledge and inferential
thinking. It is obvious that such conclusions cannot
be drawn by computers. What can be done is insert
a definition of a difficult term, as in the following
example:
1. The Red Cross asks for almost one million eu-
ros for the 500,000 Vietnamese affected by the
floods.
2. The Red Cross asks for one million euros for
Vietnam. The Red Cross is an organization
that helps people and countries around the
world.
After documenting all the operations and analysing
their nature and frequency, we have finally decided
to focus on the automatic treatment of the following:
lexical simplification, deletions, split operations, in-
version of direct speech and the insertion of defini-
tions. In the next section, we concentrate on oper-
ations applied at the lexical level, with the aim of
drawing conclusions about the nature of lexical sim-
plification carried out by trained editors and the pos-
sibility of their automatic implementation in the fu-
ture.
4 Data Analysis
We have so far obtained forty simplifications and our
goal is to shortly acquire simplified versions of all
200 texts. A variety of lexical operations have been
observed in the corpus, which go far beyond sim-
ple substitution of one lexical unit with its simpler
equivalent. In order to describe the nature of these
changes, we have categorized them as follows:
? substitutions of one lexical unit with its simpler
synonym;
? insertion of definitions of difficult terms and
concepts;
? simplification of numerical expressions;
? simplification of named entities;
? elimination of nominalisation;
? rewording of idioms and collocations; and
? rewording of metaphorically used expressions.
10
4.1 Lexical substitution
We have documented 84 cases where one lexical
unit has been substituted with its simpler synonym.
These words make up our lexical substitution ta-
ble (LST), gathered for the purpose of data analy-
sis. The table contains the lemma of the original
(O) word, its simple (S) equivalent and additional in-
formation about either the original word, the simple
word or the nature of the simplification, such as pol-
ysemy, hyponym ? hypernym, metaphor, etc. Ta-
ble 1 is an excerpt.
Original Simple Commentary
impartir pronunciar polysemy
informar decir reporting verb
inmigrante extranjero hyponym? hypernym
letras literatura polysemy
Table 1: An excerpt from the Lexical Substitution Table
To analyse the relationship between the sets of O-
S words, we have concentrated on their frequency of
use and length (both in characters and syllables).
4.1.1 Word frequency
For every word in the LST, we consulted its fre-
quency in a dictionary developed for the purposes of
our project by the DILES group and based on the
Referential Corpus of Contemporary Spanish (Cor-
pus de Referencia del Espan?ol Actual, CREA)1. We
have found that for 54.76% of the words, the fre-
quency of the simple word is higher than the fre-
quency of its original equivalent; in 30.95% of the
cases, the frequency is the same; only 3.57% of the
simple words have lower frequency than the corre-
sponding original ones; and in 10.71% of the cases
it was impossible to analyse the frequency since the
original word was a multi-word expression not in-
cluded in the dictionary, as is the case with complex
conjunctions like sin embargo (however) or pese a
(despite).
As can be appreciated, in a high number of cases
O and S words have the same frequency of use ac-
cording to CREA. In an intent to rationalise this
phenomenon, we have counted the number of times
each of these words appears in the totality of orig-
inal and simple texts. In more than half of the O-
1http://corpus.rae.es/creanet.html
S pairs the simple word is more common than its
original equivalent, not only in the simplified texts,
where it is expected to abound, but also in the orig-
inal ones. This difference in the frequency of use
in actual texts and the CREA database could be ex-
plained by the specificity of the genre of the texts in
our corpus, where certain words are expected to be
recurrent, and the genre-neutral language of CREA
on the other hand. Out of the remaining 44.5% of
the cases, where O words are more abundant than S
words, five out of fourteen may have been used for
stylistic purposes. One good example is the use of
varied reporting verbs, such as afirmar (confirm) or
anunciar (announce), instead of uniformly using de-
cir (say). Six in fourteen of the same group are pol-
ysemic words possibly used in contexts other than
the one where the simplification was recorded. Such
is the example of the word art??culo, substituted with
cosa where it meant thing. However, it also occurs
with its second meaning (article: a piece of writing)
where it cannot be substituted with cosa.
What can be concluded so far is that frequency is
a relatively good indicator of the word difficulty, al-
beit not the only one, as seen by a large number of
cases when the pairs of O-S words have the same
frequency. For that reason we analyse word length
in Section 4.1.2. Polysemy and style are also seen
as important factors at the time of deciding on the
choice of the synonym to replace a difficult word.
Whereas style is a factor we currently do not intend
to treat computationally, we cannot but recognize
the impact that polysemy has on the quality and ac-
curacy of the output text. Consider the example of
another pair of words in our lexical substitution ta-
ble: impresio?n ? influencia, in the following pair
of original (1) and simplified (2) sentences:
1. Su propia sede ya da testimonio de la ?im-
presio?n profunda? que la ciudad andaluza dejo?
en el pintor.
Its very setting testifies to the profound influ-
ence of the Andalusian town on the painter.
2. En esa casa tambie?n se ve la influencia de
Granada.
The influence of Granada is also visible in that
house.
In the given context, the two words are perfect syn-
11
onyms. However, in expressions such as tengo la
impresio?n que (I am under the impression that), the
word impresio?n cannot be substituted with influen-
cia. We have found that around 35% of all the orig-
inal words in the LST are polysemic. We therefore
believe it is necessary to include a word sense dis-
ambiguation approach as part of the lexical simplifi-
cation component of our system in the future.
4.1.2 Word Length
Table 2 summarizes the findings relative to the
word length of the original and simple words in the
LST, where syll. stands for syllable and char. for
character.
Type of relationship Percentage
S has fewer syll. than O 57.85%
S has more syll. than O 17.85%
S has the same number of syll. as O 25%
S has fewer char. than O 66.66%
S has more char. than O 23.8%
S has the same number of char. as O 9.52%
Table 2: Word length of original and simple words
The average word length in the totality of origi-
nal texts is 4.81 characters, while the simplified texts
contain words of average length of 4.76 characters.
We have also found that the original and simplified
texts have roughly the same number of short words
(up to 5 characters) and medium length words (6-10
characters), while the original texts are more satu-
rated in long words (more than 11 characters) than
the simplified ones (5.91% in original and 3.64%
in simplified texts). Going back to the words from
the LST which had the same frequency according
to CREA, we found that around 80% of these were
pairs where the simple word had fewer syllables than
the original one. This leads us to the conclusion that
there is a strong preference for shorter words and
that word length is to be combined with frequency
when deciding among a set of possible synonyms to
replace a difficult word.
4.2 Transformation rules
Upon close observation of our data, we have derived
a set of preliminary simplification rules that apply
to lexical units substitution. These rules concern re-
porting verbs and adjectives of nationality, and will
be addressed in that order.
In the twenty pairs of aligned texts nine differ-
ent reporting verbs are used. All nine of them
have been substituted with decir (say) at least once,
amounting to eleven instances of such substitutions.
Three verbs from the same set appear in simplified
texts without change. On the whole, we perceive
a strong tendency towards using a simple verb like
say when reporting direct speech. Our intention is
to build a lexicon of reporting verbs in Spanish and
complement it with grammatical rules so as to en-
able accurate lexical substitution of these items of
vocabulary. Simple substitution of one lexical unit
with another is not always possible due to syntactic
constraints, as illustrated in the following example:
1. El juez advirtio? al duque que podr??a provocar la
citacio?n de la Infanta.
The judge warned the Duke that he might cause
the Princess to be subpoenaed.
2. Murio? cient??fico que advirtio? sobre deterioro de
la capa de ozono.
The scientist who warned about the deteriora-
tion of the ozone layer died.
In the first case the verb advertir is used as part of the
structure [advertir a X que], in English [warn some-
body that]. The verb decir easily fits this structure
without disturbing the grammaticality of the sen-
tence. In the second instance, however, the reporting
verb is used with the preposition and an indirect ob-
ject, a structure where the insertion of decir would
be fatal for the grammaticality of the output. We be-
lieve that the implementation of this rule would be
a worthwhile effort, given that informative texts of-
ten abound in direct speech that could be relatively
easily simplified so as to enhance readability.
As for adjectives of nationality, we have no-
ticed a strong preference for the use of periphrastic
structure instead of denominal adjective denoting
nationality. Thus, a simple adjective is replaced
with the construction [de < COUNTRY >], e.g.
el gobierno pakistan?? (the Pakistani government)
is replaced by el gobierno de Pakista?n (the gov-
ernment of Pakistan). The same rule is applied
to instances of nominalised nationality adjectives.
In these cases the structure [ArtDef + Adj]2 be-
2ArtDef: definite article, Adj: adjective
12
comes [ArtDef + persona + de + < COUNTRY >],
e.g: los pakistan??es ? las personas de Pakista?n
(the Pakistani? the people from Pakistan). In only
five instances the adjective was preferred. Twice it
was espan?ol (Spanish), which were the only two in-
stances of the expression of this nationality. This
leads us to the conclusion that espan?ol is sufficiently
widespread and therefore simple enough and would
not need to be substituted with its periphrastic equiv-
alent. Norteamericano (North American) was used
twice, therefore being slightly more acceptable than
estadounidense (of/from the United States), which
is always replaced by de Estados Unidos. The re-
maining is the one instance of egipcio (Egyptian),
otherwise replaced by de Egipto.
Based on the observations, our hypothesis is that
more common nationality adjectives, such as Span-
ish, and possibly also English or French need not be
modified. Norteamericano or estadounidense how-
ever common are possibly perceived as complicated
due to their length. In order to derive a definite rule,
we would need to carry out a more detailed analy-
sis on a richer corpus to determine how frequency of
use and length of these adjectives correlate.
4.3 Insertion of definitions
Definitions of difficult terms are found in 57.5% of
all texts we have analysed. Around 70% of these
are definitions of named entities, such as El Greco,
Amnesty International, Guantanamo and others. In
addition to these, difficult lexical units, and even ex-
pressions, are explained by means of a definition.
Thus, a (prison) cell is defined as a room in a prison,
and the prisoner of conscience as a person put in
prison for his ideas. In order to deal with named
entity definitions, we intend to investigate the meth-
ods for the look-up of such definitions in the future.
To solve the problem of defining difficult individual
lexical units, one solution is to target those words
with the lowest frequency rate and in the absence
of an adequate simpler synonym insert a definition
from a monolingual dictionary, given the availabil-
ity of such resources (the definition itself might need
to be simplified).
4.4 Numerical expressions
Our analysis shows that the treatment of numerical
expressions should have a significant place in our
simplification system, given their abundance in the
kind of texts our system is mainly intended for, and
a wide variety of simplification solutions observed
by examining the parallel corpus. Even though by
far the most common operation is elimination (in
the process of summarization), there are a number
of other recurrent operations. The most common of
these are explained below for the purpose of illus-
tration, given that the totality of the rules is beyond
the scope of this paper. We separately address nu-
merical expressions forming part of a date and other
instances of using numbers and numerals.
The following are the rules concerning numerical
expressions in dates:
1. en < YEAR >? en el an?o < YEAR >
en 2010? en el an?o 2010
2. Years in parenthesis are eliminated (this opera-
tion has been applied in 100% of the cases dur-
ing manual simplification):
El Greco (1541?1614)? El Greco
3. In expressions containing the name and/or the
day of the month, irrespective of whether it is
followed by a year, the information relative to
the month (i.e. name or name and day) is elim-
inated (applied in around 85% of the cases):
en septiembre 2010 ? en el an?o 2010
el 3 de mayo? ?
As for other numerical expressions, the most
common rules and most uniformly applied are the
following:
1. Replacing a word with a figure:
cinco d??as? 5 d??as
2. Rounding of big numbers:
ma?s de 540.000 personas ? medio millo?n de
personas
3. Rounding by elimination of decimal points:
Cerca de 1,9 millones de casas ? 2 millones
de casas
4. Simplification of noun phrases containing two
numerals in plural and the preposition of by
eliminating the first numeral:
cientos de miles de personas ? miles de per-
sonas
13
5. Substitution of words denoting a certain num-
ber of years (such as decade or centenary) by
the corresponding number:
IV centenario de su nacimiento? 400 an?os de
su nacimiento
6. The thousands and millions in big numbers are
expressed by means of a word, rather than a
figure:
17.000 casas? 17 mil casas
We are currently working on implementing a nu-
merical expression simplification module based on
rounding and rewording rules derived from our cor-
pus and previous study in the field (Bautista et al,
2011).
4.5 Named Entities
As with numerical expressions, the majority of
named entities are eliminated as a result of sum-
marization. Only those names that are relative to
the theme of the text in question and which tend to
appear throughout the article are kept. In the case
of these examples, we have observed the follow-
ing operations: abbreviation; disabbreviation; using
full name instead of the surname alone, customary
in newspaper articles; expanding the noun phrase
[ArtDef + NCom]3 with the name of the referent; re-
placing the noun phrase [ArtDef + NCom] with the
name of the referent; inversion of the constituents
in the structures where a professional title is fol-
lowed by the name of its holder in apposition; and
a handful of other, less frequent changes. Table 3
summarizes the most common operations and illus-
trates them with examples from the corpus. As can
be observed, some NE are written as acronyms while
others are disabbreviated. It would be interesting to
analyse in the future whether the length and the rel-
ative frequency of the words that make up these ex-
pressions are a factor, or these are simply examples
of arbitrary choices made by human editors lacking
more specific guidelines.
While to decide how to deal with names of organ-
isations that may possibly be abbreviated we would
need a larger corpus more saturated in these exam-
ples, there are a number of rules ready to be imple-
mented. Such is the case of personal names, where
3NCom: common noun
almost 90% of the names appearing in simplified
texts contain both name and surname as opposed to
first name alone. The same is true of the order of
name and title, where in 100% of such examples the
name is preferred in the initial position. As for ex-
panding the named entity with a common noun (the
painter Pablo Picasso), we have recorded this op-
eration in only 15% of the personal names used in
S texts. We do, however, notice a pattern ? this
kind of operation is applied at the first mention of the
name, where the common noun acts as an additional
defining element. It is an interesting phenomenon to
be further researched.
4.6 Other simplification tendencies
Human editors have opted for a number of other sim-
plification solutions which are either difficult or im-
possible to implement computationally. The elimi-
nation of nominalisations is an example of the for-
mer. Whereas common in the journalistic genre, hu-
man simplifications show a very strong tendency to-
wards substituting the combination of the support
verb and a deverbal noun with the corresponding
verb alone, as in the example:
1. La financiacio?n ha sido realizada por la Gener-
alitat Valenciana.
The funding has been provided by the Valencian
Government.
2. La Generalitat Valenciana ha financiado la in-
vestigacio?n.
The Valencian Government has financed the re-
search.
The expression realizar una financiacio?n (provide
funding) from the original sentence (1) has been sub-
stituted by the verb financiar (to fund) in the simpli-
fied version (2). Twenty other instances of this kind
of operation have been recorded, thus making it an
issue to be readdressed in the future.
What is also to be addressed is the treatment of
set expressions such as idioms and collocations. Al-
though not excessively abundant in the current ver-
sion of our corpus, we hypothesise that the simpli-
fication of such expressions could considerably en-
hance the readability of the text and the research of
the issue could, therefore, prove beneficial, provided
14
Original Simple Operation Type
Comite? Espan?ol de Representates
de Personas con Discapacidad
CERMI abbreviation
el PSOE el Partido Socialista Obrero
Espan?ol
disabbreviation
Gonzales-Sinde Angeles Gonzales-Sinde full name
el artista el artista Pablo Picasso NCom+NE
la ciudad andaluza Granada NCom ? NE
La ministra de Defensa, Carme
Chaco?n
Carme Chaco?n, ministra de De-
fensa
NCom,NE ? NE,NCom
Table 3: Named Entities Substitution Examples
the availability of the necessary resources for Span-
ish.
On the other hand, an example of common hu-
man simplification tactics which is out of reach for
a computational system is rewording of metaphori-
cally used expressions. Thus, un gigante de la es-
cena (a giant on stage) is changed into un actor ex-
traordinario (an extraordinary actor). Such exam-
ples point out to the limitations automatic simplifi-
cation systems are bound to possess.
5 Conclusions and future work
In the present paper we have concentrated on the
analysis of lexical changes observed in a parallel
corpus of original and simplified texts in Spanish.
We have categorized all the operations into substitu-
tion of lexical units; insertion of definitions of diffi-
cult terms and concepts; simplification of numerical
expressions; simplification of named entities; and
different cases of rewording. Analysis suggests that
frequency in combination with word length is the
necessary combination of factors to consider when
deciding on the choice among a set of synonyms to
replace a difficult input word. On the other hand, a
high number of polysemic input words underline the
importance of including word sense disambiguation
as part of the lexical substitution module.
Based on the available data, we have so far de-
rived a set of rules concerning reporting verbs, ad-
jectives of nationality, numerical expressions and
named entities, all of which are to be further de-
veloped and implemented in the future. Numeri-
cal expressions in particular are given an important
place in our system and more in-depth analysis is
being carried out. We are working on rounding of
big numbers and the use of modifiers in the simplifi-
cation of these expressions. A number of issues are
still to be tackled, such as elimination of nominali-
sation and simplification of multi-word expressions.
The ultimate goal is to implement the lexical mod-
ule as part of a larger architecture of the system for
automatic text simplification for Spanish.
Acknowledgements
The present study is part of a Spanish research
project entitled Simplext: An automatic system for
text simplification (http://www.simplext.
es). Simplext is led by Technosite and partially
funded by the Ministry of Industry, Tourism and
Trade of the Government of Spain, through the Na-
tional Plan for Scientific Research, Development
and Technological Innovation (I+D+i), within the
strategic Action of Telecommunications and Infor-
mation Society (Avanza Competitiveness, with the
file number TSI-020302-2010-84). We are grate-
ful to the fellowship RYC-2009-04291 from Pro-
grama Ramo?n y Cajal 2009, Ministerio de Econom??a
y Competitividad, Secretar??a de Estado de Investi-
gacio?n, Desarrollo e Innovacio?n, Spain.
References
A. Anula. 2008. Lecturas adaptadas a la ensen?anza del
espan?ol como l2: variables lingu???sticas para la deter-
minacio?n del nivel de legibilidad. In La evaluacio?n en
el aprendizaje y la ensen?anza del espan?ol como LE/L2.
A. Anula. 2009. Tipos de textos, complejidad lingu???stica
y facilicitacio?n lectora. In Actas del Sexto Congreso
de Hispanistas de Asia, pages 45?61.
S. Bautista, P. Gerva?s, and R.I. Madrid. 2009. Feasi-
bility analysis for semiautomatic conversion of text to
improve readability. In The Second International Con-
ference on Information and Communication Technolo-
gies and Accessibility.
15
S. Bautista, R. Herva?s, P. Gerva?s, R. Power, and
S. Williams. 2011. How to make numerical in-
formation accessible: Experimental identification of
simplification strategies. In Conference on Human-
Computer Interaction, Lisbon, Portugal.
O. Biran, S. Brody, and N. Elhadad. 2011. Putting it
simply: a context-aware approach to lexical simpli-
fication. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 496?501, Port-
land, Oregon, USA. Association for Computational
Linguistics.
S. Bott and H. Saggion. 2011. An unsupervised align-
ment algorithm for text simplification corpus construc-
tion. In ACL Workshop on Monolingual Text-to-Text
Generation, Portland, USA, June 2011. ACL, ACL.
Stefan Bott and H. Saggion. 2012a. Text simplifica-
tion tools for spanish. In Proceedings of Language
Resources and Evaluation Conference, 2012.
Stefan Bott and Horacio Saggion. 2012b. A hybrid sys-
tem for spanish text simplification. In Third Work-
shop on Speech and Language Processing for Assistive
Technologies (SLPAT), Montreal, Canada.
J. Burstein, J. Shore, J. Sabatini, Yong-Won Lee, and
M. Ventura. 2007. The automated text adaptation tool.
In HLT-NAACL (Demonstrations), pages 3?4.
J. Carroll, G. Minnen, Y. Canning, S. Devlin, and J. Tait.
1998. Practical simplification of english newspaper
text to assist aphasic readers. In Proc. of AAAI-98
Workshop on Integrating Artificial Intelligence and As-
sistive Technology, pages 7?10.
H. M. Caseli, T. F. Pereira, L. Specia, Thiago A. S. Pardo,
C. Gasperin, and S. M. Alu??sio. 2009. Building a
brazilian portuguese parallel corpus of original and
simplified texts. In 10th Conference on Intelligent Text
PRocessing and Computational Linguistics (CICLing
2009).
R. Chandrasekar, D. Doran, and B. Srinivas. 1996. Mo-
tivations and methods for text simplification. In COL-
ING, pages 1041?1044.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics.
J. De Belder, K. Deschacht, and Marie-Francine Moens.
2010. Lexical simplification. In Proceedings of
Itec2010 : 1st International Conference on Interdisci-
plinary Research on Technology, Education and Com-
munication.
S. Devlin and G. Unthank. 2006. Helping aphasic people
process online information. In Proceedings of the 8th
international ACM SIGACCESS conference on Com-
puters and accessibility, Assets ?06, pages 225?226,
New York, NY, USA.
B. B. Klebanov, K. Knight, and D. Marcu. 2004. Text
simplification for information-seeking applications. In
On the Move to Meaningful Internet Systems, Lecture
Notes in Computer Science, pages 735?747.
P. Lal and S. Ruger. 2002. Extract-based summarization
with simplification. In Proceedings of the ACL 2002
Automatic Summarization / DUC 2002 Workshop.
J. Medero and M. Ostendorf. 2011. Identifying targets
for syntactic simplification.
Ll. Padro?, M. Collado, S. Reese, M. Lloberes, and
I. Castello?n. 2010. Freeling 2.1: Five years of open-
source language processing tools. In Proceedings of
the Seventh International Conference on Language
Resources and Evaluation (LREC?10), Valletta, Malta.
S. E. Petersen and M. Ostendorf. 2007. Text simplifica-
tion for language learners: a corpus analysis. In Proc.
of Workshop on Speech and Language Technology for
Education.
P. Quinlan. 1992. The Oxford Psycholinguistic
Database. Oxford University Press.
Lucia Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th international
conference on Computational Processing of the Por-
tuguese Language, pages 30?39, Berlin, Heidelberg.
16
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 25?32,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Graphical Schemes May Improve Readability but
Not Understandability for People with Dyslexia
Luz Rello,1,2 Horacio Saggion2
1 Web Research Group
2 NLP Research Group
Universitat Pompeu Fabra, Barcelona
luzrello@acm.org
horacio.saggion@upf.edu
Ricardo Baeza-Yates, Eduardo Graells
Web Research Group
Universitat Pompeu Fabra
Yahoo! Research, Barcelona
rbaeza@acm.org
eduardo.graells@upf.edu
Abstract
Generally, people with dyslexia are poor read-
ers but strong visual thinkers. The use of
graphical schemes for helping text compre-
hension is recommended in education man-
uals. This study explores the relation be-
tween text readability and the visual concep-
tual schemes which aim to make the text more
clear for these specific target readers. Our re-
sults are based on a user study for Spanish na-
tive speakers through a group of twenty three
dyslexic users and a control group of similar
size. The data collected from our study com-
bines qualitative data from questionnaires and
quantitative data from tests carried out using
eye tracking. The findings suggest that graph-
ical schemes may help to improve readability
for dyslexics but are, unexpectedly, counter-
productive for understandability.
1 Introduction
Readability refers to the legibility of a text, that
is, the ease with which text can be read. On the
other hand, understandability refers to comprehen-
sibility, the ease with which text can be understood.
Since readability strongly affects text comprehen-
sion (Barzilay et al, 2002), sometimes both terms
have been used interchangeably (Inui et al, 2003).
However, previous research with dyslexic people
have shown that both concepts need to be taken
into consideration separately. For instance, while
in dyslexic population reading, comprehension has
been found to be independent of the spelling errors
of the text; lexical quality can be used as an indicator
of understandability for the non-dyslexic population
(Rello and Baeza-Yates, 2012).
Dyslexia has been defined both as a specific read-
ing disability (Vellutino et al, 2004) and as a learn-
ing disability (International Dyslexia Association,
2011). It is neurological in origin and it is char-
acterized by difficulties with accurate and/or fluent
word recognition and by poor spelling and decoding
abilities. Secondary consequences include problems
in reading comprehension and reduced reading ex-
perience that can impede growth of vocabulary and
background knowledge (International Dyslexia As-
sociation, 2011).
On the other hand, the role of visual thinking
is crucial in dyslexics and its development may be
helpful for a number of tasks such as visual anal-
ysis and pattern recognition (West, 2009). Par-
tially related to the importance of visual thinking in
dyslexics, the use of graphical schemes has been an
extensively recommended pedagogical strategy for
dyslexic students (Ram??rez Sa?nchez, 2011; Chalk-
ley et al, 2001) as well as for students with reading
disabilities (Lo?pez Castro, 2010).
The inclusion of semantic maps was found to
be beneficial for reading comprehension of gen-
eral disabled readers in (Sinatra et al, 1984) and
the inclusion of graphical schemes to improve com-
prehension for dyslexic readers has been proposed
in (Weaver, 1978). However, to the best of our
knowledge, no estimation of the effect of graphical
schemes on the readability for dyslexics using eye
tracking together with their effect in understandabil-
ity has been done. Therefore, this paper presents the
following three main contributions for Spanish na-
25
tive speakers:
? An estimation of the effect of graphical
schemes in the readability of dyslexic readers
based on the analysis of an eye tracking user
study.
? The relationship between readability and un-
derstandability in dyslexic readers using com-
prehension questionnaires.
? A survey conducted among dyslexics on the
helpfulness of including graphical schemes.
The rest of the paper is organized as follows. Sec-
tion 2 covers related work and Section 3 details the
experimental methodology. Section 4 presents our
results and in Section 5 conclusions and future chal-
lenges are drawn.
2 Related Work
We divide the related work in: (1) strategies used in
discourse simplification for dyslexics, and (2) how
these strategies were measured in relationship with
readability and understandability.
Since dyslexics represent a target population of
poor readers, different strategies have been applied
for improving readability: the use of different text
formats (Rello et al, 2012) and environments (Gre-
gor and Newell, 2000), the use of multi-modal infor-
mation (Kiraly and Ridge, 2001) and text to speech
technologies (Elkind et al, 1993), among others.
The closest work to ours is the incorporation of
summaries and graphical schemes in texts. Previ-
ous work has shown that the readability of dyslexic
students could be improved by using text summa-
rization (Nandhini and Balasundaram, 2011) and se-
mantic maps (Sinatra et al, 1984).
Various factors have been applied to measure
readability in dyslexics. Classic readability mea-
sures are useful to find appropriate reading material
for dyslexics (Kotula, 2003) and to measure com-
prehension. For instance, the Flesch-Kincaid read-
ability degree was applied to access comprehension
speeds and accuracy in dyslexic readers (Kurniawan
and Conroy, 2006). Other specific readability mea-
sures for dyslexic readers have been proposed in
other domains such as information retrieval (Sitbon
and Bellot, 2008).
In the case of the use of summaries, the evaluation
of comprehension was carried out using question-
naires (Nandhini and Balasundaram, 2011). Multi-
ple choice questions were applied to measure the in-
corporation of semantic maps among disable readers
(Sinatra et al, 1984) and eye tracking measures have
been used to explore various characteristics related
to dyslexic reading (Eden et al, 1994).
Although the creation of graphical schemes is ex-
tensively recommended in literature (Weaver, 1978;
Ram??rez Sa?nchez, 2011; Lo?pez Castro, 2010), we
found no formal evaluation of their impact in read-
ability and comprehension combining data from eye
tracking, questionnaires, and a survey.
3 Experimental Methodology
3.1 Participants
Twenty three native Spanish speakers with a con-
firmed diagnosis of dyslexia took part in the study,
twelve of whom were female and eleven male. All
the participants were asked to bring their diagnoses
to the experiment. Their ages ranged from 13 to 37,
with a mean age of 20.74. There were three par-
ticipants with attention deficit disorder. All partic-
ipants were frequent readers; eleven read less than
four hours per day, nine read between four and eight
hours per day, and three participants read more than
eight hours daily. Ten people were studying or al-
ready finished university degrees, eleven were at-
tending school or high school and two had no higher
education. A control group of 23 participants with-
out dyslexia and similar age average (20.91) also
participated in the experiment.
3.2 Design
The experiment was composed of four parts: (1) an
initial interview designed to collect demographic in-
formation, (2) a reading test, (3) two questionnaires
designed to control the comprehension, and (4) a
survey to know the impressions of each person re-
garding the inclusion of graphical schemes.
Along the reading test we collected the quantita-
tive data to measure readability, with the compre-
hension questionnaires we measure understandabil-
ity, while with the survey we gather information
about the participant views.
We used two different variants (A and B) of the
26
Test A
Comprenhesion 
Questionnarie
Text 1: 
Star
Scheme
Text 2: 
Fish
Scheme
Text 2: 
Fish
Text 1: 
Star
Comprehension 
Questionnaire
Comprenhesion 
Questionnarie
Comprehension 
Questionnaire
Comprehension 
Questionnaire
Comprehension 
Questionnaire
Participant Preferences Survey
Test B
Comprenhesion 
Questionnarie
Sunday, May 6, 12
Figure 1: Variants of the experiment.
test (see Figure 1). Each test was composed of two
texts: one text that included a graphical scheme
in the top and another text without the graphical
scheme. We extracted the most similar texts we
could find from the Spanish Simplex corpus (Bott
and Saggion, 2012). The chosen texts share the fol-
lowing characteristics:
(a) They both have the same genre: science news.
(b) They are about similar topics: Text 1 (called
Star) is about the discovery of a supernova and
text 2 (Fish) is about the discovery of a new
species of fish.
(c) They contain the same number of sentences: 4
sentences in addition to the title.
(d) They have the same number of words (136).
(e) They have a similar average word length: 5.06
letters per word in Star and 5.12 letters per
word in Fish.
(f) They contain the same number of unique
named entities (7).
(g) They contain one foreign word: Science in Star
and Jean Gaudant in Fish.
(h) They contain one number: 6.300 an?os luz
(?6,300 light years?) in Star and 10 millones de
an?os (?10 millions of years?) in Fish.
As seen in Figure 1, in variant A, text 2 includes
a graphical scheme while text 1 was presented with-
out the graphical scheme. Variant B is reversed: text
1 appeared with a graphical scheme and text 2 with-
out it. The order of the experiments was counterbal-
anced using the variants A and B to guarantee that
the participant never reads the same text twice.
For the layout of the texts and graphical schemes
we chose a recommended font type for dyslexics,
sans serif arial (Al-Wabil et al, 2007), unjustified
text (Pedley, 2006), and recommended color and
brightness contrast using a black font with creme
background1 (British Dyslexia Association, 2012).
For the creation of the graphical schemes2 we
took into account the pedagogical recommendations
for dyslexics (Ram??rez Sa?nchez, 2011; Chalkley et
al., 2001), and the cognitive principles of inductive
learning in concept acquisition from scheme theory
(Anderson et al, 1979; Anderson and Robert, 2000).
Since the tests were going to be read by dyslex-
ics, the graphical schemes were manually created by
a dyslexic adult and supervised by a psychologist.
The graphical schemes simplify the discourse and
highlight the most important information from the
title and the content. Each of the graphical schemes
shares the following pattern: the first line of the
graphical scheme encloses the main words of the ti-
tle connected by arrows and then, starting from the
title, there is a node for each of the sentences of the
text. These nodes summarize the most relevant in-
formation of the text, as the example translated to
English shown in Figure 2. We present the original
text and its translation in the Appendix.
To control the comprehension, after each text we
designed a maximum performance questionnaire in-
cluding inferential items related to the main idea.
We did not include items related to details, be-
cause they involve memory more than comprehen-
sion (Sinatra et al, 1984). Each of the items had
1The CYMK are creme (FAFAC8) and black (000000).
Color difference: 700, brightness difference: 244.
2Notice that we distinguish graphical schemes from con-
ceptual graphs (Sowa, 1983) or semantic maps (Sinatra et al,
1984).
27
Figure 2: Example of a graphical scheme (Fish).
three answers, a correct one, another partially incor-
rect (normally containing details), and an incorrect
one. We gave 100, 50, and 0 points for each type
of answer, respectively. For instance (translated into
English):
? What is the text about?
(a) About the National Museum of Natural His-
tory in Paris (0 points).
(b) About the discovery of a prehistoric fish in Va-
lencia (100 points).
(c) About the content of the fish feces (50 points).
The test finishes with one survey to learn the par-
ticipant preferences. The survey is composed of
three items about how helpful was the graphical
scheme for (1) reading, (2) understanding, and (3)
remembering the text. Each item uses a Likert scale
with 5 levels, from strongly disagree (1) to strongly
agree (5). An example of an item follows:
? Without the graphical scheme, my understand-
ing of the text would have been:
1. Much more easier because I did not under-
stand anything about the graphical scheme.
2. Easier because the graphical scheme is com-
plicated.
3. Neither easier nor more difficult.
4. More difficult because the graphical scheme
has helped me.
5. Much more difficult because the graphical
scheme has shed light about the content.
3.3 Equipment
The eye tracker used was a Tobii T50 (Tobii Tech-
nology, 2005) with a 17-inch TFT monitor. The eye
tracker was calibrated for each participant and the
light focus was always in the same position. The
distance between the participant and the eye tracker
was constant (approximately 60 cm. or 24 in.) and
controlled by using a fixed chair.
3.4 Procedure
The sessions were conducted at Pompeu Fabra Uni-
versity and they took around 30 minutes, depending
on the amount of information given by the partici-
pant. In each session the participant was alone with
the interviewer (first author) in the quiet room pre-
pared for the study.
The first part began with an interview designed to
collect demographic information. Second, we pro-
ceeded with the recordings of the passages using eye
tracking. Half of the participants made variant A of
the test and the other half variant B. The participant
was asked to read the texts in silence and completing
each comprehension questionnaire. The text ends by
answering the survey.
3.5 Data Analysis
The software used for analyzing the eye tracking
data was Tobii Studio 3.0 and the R 2.14.1 statistical
software. The measures used for the comparison of
the text passages were the means of the fixation du-
ration and the total duration of reading. Differences
between groups and parameter values were tested by
means of a one-way analysis of variance (ANOVA).
4 Results
In this section we present first the analyses of the
data from the eye tracking and comprehension ques-
tionnaires (Section 4.1), followed by the analysis of
the survey (Section 4.2).
4.1 Readability and Understandability
To measure the impact of graphical schemes in
readability we analyzed the means of the fixation
time and the total reading duration of the pas-
sages. Shorter fixations are preferred to longer
ones because according to previous studies (Just and
Carpenter, 1980), readers make longer fixations at
points where processing loads are greater. Also,
shorter reading durations are preferred to longer
ones since faster reading is related to more read-
able texts (Williams et al, 2003). We compare read-
ability with understandability through the inferential
items of the comprehension questionnaire.
First, we studied the differences between the
dyslexic participants and the control group. Then,
28
Table 1: Experimental results of the eye-tracking and the
comprehension user study.
Measure (sec., %) Scheme + Text Text
(ave. ? std.dev.) Group D
Fixations Duration 0.224? 0.046 0.248? 0.057
Visit Duration 64.747? 22.469 78.493? 34.639
Correct Answers 86.93% 97.73%
Group N
Fixations Duration 0.205? 0.033 0.198? 0.030
Visit Duration 43.771? 14.790 45.124? 13.353
Correct Answers 89.58% 95.83%
we analyzed the influence of the graphical schemes
in the readability and understandability.
In (Kurniawan and Conroy, 2006) it was found
that students with dyslexia are not slower in read-
ing than students without dyslexia when the articles
are presented in a dyslexia friendly colour scheme.
However, we found statistical significance among
the dyslexic and non-dyslexic groups when reading
both texts without graphical schemes taking into ac-
count the mean of fixation time (p < 0.0008) and
the total reading duration for the texts with graph-
ical schemes (p < 0.0007) and without graphi-
cal schemes (p < 0.0001) (see Table 1). On the
other hand, our results are consistent with other eye-
tracking studies to diagnose dyslexia that found sta-
tistical differences among the two populations (Eden
et al, 1994).
The presence of graphical schemes improves the
readability of the text for people with dyslexia be-
cause the fixation time and the reading duration de-
creases for all texts with a graphical scheme (see Ta-
bles 1, 2, and 3). Notice that these positive results
are given for the comparison of the texts alone (see
the text areas in Figure 1). If we compare the to-
tal reading duration of the text alone with the text
plus the graphical scheme, it takes in average 18.6%
more time to read the whole slide than the text alone.
However, we found no statistically significant
results among texts with and without graphical
schemes using such measures. The greatest differ-
ence in readability among texts with and without
graphical schemes was found taking into account the
fixation times for both texts (p = 0.146) among the
dyslexic participants.
Comparing both Fish and Star texts (see Tables
2 and 3), we observe that Fish was more difficult
to read and understand since it presents longer fixa-
tions and a lower rate of correct answers. In dyslex-
ics the fixation time decreases more (from 0.258
seconds without graphical scheme to 0.227 with a
graphical scheme, p < 0.228) in Fish that in Star
(0.237 to 0.222, p < 0.405), meaning that graphi-
cal schemes have a higher impact in readability for
complex texts.
Considering the similarity of the texts, it is sur-
prising how Fish seems to be easier to read than Star.
One possible explanation is that the scientific piece
of news contained in Star was more present in the
media than the other news contained in Fish.
However, graphical schemes have not helped our
participants to increase their rate of correct answers
for the inferential items. For all the cases except one
(non-dyslexic participants in Star, Table 2) the rate
of correct answers decreased when the text was ac-
companied by a scheme.
Dyslexic participants have a higher percentage of
correct answers than non-dyslexics when the text is
presented with the graphical scheme, and lower rate
if the text is presented without the graphical scheme.
These results are consistent with some of the opin-
ions that the participants expressed after the session.
A few dyslexic participants explained that the graph-
ical scheme actually distracted them from the text
content. Another dyslexic participant exposed that
the graphical schemes helped her to remember and
study texts but not to understand them. The diverse
opinions of the participants towards the graphical
schemes suggest that normally graphical schemes
are highly customized by the person that creates
them and therefore a non-customized schema could
complicate understandability.
4.2 Survey
Through the user survey we infer how the partici-
pants were influenced by the graphical schemes in:
(1) the text?s readability, (2) the understandability
of the text, and (3) remembering the text content.
In Figure 3 we present the results for each of the
items comparing dyslexic and non-dyslexic partici-
pants (N = 23).
In terms of readability, dyslexic and non-dyslexic
participants have opposite opinions. While dyslexic
participants agree in finding graphical schemes help-
29
Figure 3: Survey results for understandability, readability and remembering.
Table 2: Experimental results of the eye-tracking and
comprehension user study for text 1, Star.
Measure (sec., %) Scheme + Text Text
(ave. ? std.dev.) Group D
Fixations Duration 0.222? 0.061 0.237? 0.023
Visit Duration 63.633? 0.00 83.918? 18.606
Correct Answers 87.5% 95.45%
Group N
Fixations Duration 0.205? 0.023 0.199? 0.041
Visit Duration 39.552? 14.850 47.351? 15.580
Correct Answers 91.67% 91.67%
Table 3: Experimental results of the eye-tracking and
comprehension user study for text 2, Fish.
Measure (sec., %) Scheme + Text Text
(ave. ? std.dev.) Group D
Fixations Duration 0.227? 0.026 0.258? 0.078
Visit Duration 60.073? 20.684 69.058? 29.910
Correct Answers 86.36% 100%
Group N
Fixations Duration 0.205? 0.042 0.214? 0.036
Visit Duration 47.990? 14.130 42.896? 10.991
Correct Answers 87.5% 100%
ful for reading (12 participants, 52.17%), non-
dyslexic participants said that graphical schemes
were unhelpful. Some participants explained that
the graphical schemes mislead them because they
were placed at the beginning of the slide when they
did not know the topic of the text. However, a few
participants claimed that they found the graphical
schemes very helpful.
Participants with dyslexia mostly agree (10 partic-
ipants, 43.48%) in finding graphical schemes helpful
for textual comprehension while most of the non-
dyslexic participants (14 participants, 60.87%) did
not find graphical schemes neither helpful nor un-
helpful for understandability. On the other hand,
both populations agree in finding graphical schemes
helpful for remembering data from the text.
5 Conclusions and Future Work
The addition of informational elements to a text im-
pacts its readability. Since dyslexics are strong vi-
sual thinkers this study relates the use of graphical
schemes to readability and understandability, con-
tributing to predict their impact.
In general terms, we can affirm that adding a
graphical scheme in a text improves its readability,
since we observed a decrease in the fixation time
and an increase of reading speed in texts containing
graphical schemes. On the contrary to the expected
result, understandability does not improve with the
presence of graphical schemes.
Even though dyslexia presents heterogenous man-
ifestations among subjects, we found patterns re-
lated to readability and understandability using
quantitative and qualitative data.
However, our results shall be taken with care since
readability, specially in dyslexic users, depends on
many factors which are very challenging to control
in an experimental setup. These factor include the
30
vocabulary of the participants, their working mem-
ory or the different strategies they use to overcome
dyslexia.
Further work is needed such as the inclusion of
more types of graphical schemes in the experiments,
the addition of a delayed post-test to address the ef-
fect of supplemental graphical schemes on robust-
ness of learning, and the exploration of more factors
related to readability.
Acknowledgments
We thank Mari-Carmen Marcos for her assistance
with the eye tracker hardware and Joaquim Llisterri
for his invaluable help distributing our experiments
announcement among experts. Special thanks go
for the psychologists Daniela Alarco?n Sa?nchez and
Clara Mar??a Pavo?n Gonzalo for their help when de-
signing the questionnaires and graphical schemes.
For their altruistic help in contacting dyslexic people
we are indebted to Anna Eva Jarabo, Mar??a Fuentes
Fort, Beatriu Pasarin, Du?nia Pe`rdrix i Sola`s, Toni
Mart??, Vero?nica Moreno, Emma Rodero, Olga Soler,
Berta Torres, Mar??a Vilaseca, and Silvia Zaragoza.
Special thanks go to all the anonymous dyslexic par-
ticipants and their families. Finally, we thank the
anonymous reviewers for their excellent comments.
References
A. Al-Wabil, P. Zaphiris, and S. Wilson. 2007. Web nav-
igation for individuals with dyslexia: an exploratory
study. Universal Acess in Human Computer Interac-
tion. Coping with Diversity, pages 593?602.
J.R. Anderson and J. Robert. 2000. Learning and mem-
ory. John Wiley New York.
J.R. Anderson, P.J. Kline, and C.M. Beasley. 1979. A
general learning theory and its application to schema
abstraction. Psychology of learning and motivation,
13:277?318.
R. Barzilay, N. Elhadad, and K. R. McKeown. 2002. In-
ferring strategies for sentence ordering in multidocu-
ment news summarization. Journal of Artificial Intel-
ligence Research, 17:35?55.
Stefan Bott and Horacio Saggion. 2012. Text simplifi-
cation tools for spanish. In Proceedings of the eighth
international conference on Language Resources and
Evaluation (LREC 2012), Instanbul, Turkey, May.
ELRA.
British Dyslexia Association. 2012. Dyslexia style
guide, January. http://www.bdadyslexia.
org.uk/.
B. Chalkley, J. Waterfield, and Geography Discipline
Network. 2001. Providing learning support for stu-
dents with hidden disabilities and dyslexia undertak-
ing fieldwork and related activities. University of
Gloucestershire, Geography Discipline Network.
GF Eden, JF Stein, HM Wood, and FB Wood. 1994.
Differences in eye movements and reading problems
in dyslexic and normal children. Vision Research,
34(10):1345?1358.
J. Elkind, K. Cohen, and C. Murray. 1993. Using
computer-based readers to improve reading compre-
hension of students with dyslexia. Annals of Dyslexia,
43(1):238?259.
Peter Gregor and Alan F. Newell. 2000. An empirical in-
vestigation of ways in which some of the problems en-
countered by some dyslexics may be alleviated using
computer techniques. In Proceedings of the fourth in-
ternational ACM conference on Assistive technologies,
ASSETS 2000, pages 85?91, New York, NY, USA.
ACM.
International Dyslexia Association. 2011. Def-
inition of dyslexia: http://interdys.org/
DyslexiaDefinition.htm. Based in the ini-
tial definition of the Research Committee of the Or-
ton Dyslexia Society, former name of the IDA, done in
1994.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the second interna-
tional workshop on Paraphrasing-Volume 16, pages 9?
16. Association for Computational Linguistics.
M.A. Just and P.A. Carpenter. 1980. A theory of reading:
From eye fixations to comprehension. Psychological
review, 87:329?354.
J. Kiraly and P.M. Ridge. 2001. Method of and apparatus
for multi-modal information presentation to computer
users with dyslexia, reading disabilities or visual im-
pairment. US Patent 6,324,511.
A.W. Kotula. 2003. Matching readers to instructional
materials: The use of classic readability measures
for students with language learning disabilities and
dyslexia. Topics in Language Disorders, 23(3):190.
S. Kurniawan and G. Conroy. 2006. Comparing compre-
hension speed and accuracy of online information in
students with and without dyslexia. Advances in Uni-
versal Web Design and Evaluation: Research, Trends
and Opportunities, Idea Group Publishing, Hershey,
PA, pages 257?70.
M.R. Lo?pez Castro. 2010. Intervencio?n educativa en
un caso real de problemas de comprensio?n lectora.
Hekademos: revista educativa digital, (6):27?48.
31
K. Nandhini and SR Balasundaram. 2011. Improv-
ing readability of dyslexic learners through document
summarization. In IEEE International Conference
on Technology for Education (T4E), pages 246?249.
IEEE.
M. Pedley. 2006. Designing for dyslexics: Part 3 of 3.
http://accessites.org/site/2006/11/
designing-for-dyslexics-part-3-of-3.
D. M. Ram??rez Sa?nchez. 2011. Estrategias de inter-
vencio?n educativa con el alumnado con dislexia. In-
novacio?n y experiencias educativas, 49.
L. Rello and R. Baeza-Yates. 2012. Lexical quality as a
proxy for web text understandability (poster). In The
21st International World Wide Web Conference (WWW
2012), April.
L. Rello, G. Kanvinde, and R. Baeza-Yates. 2012. Lay-
out guidelines for web text and a web service to im-
prove accessibility for dyslexics. In International
Cross Disciplinary Conference on Web Accessibility
(W4A 2014), Lyon, France, April. ACM Press.
R.C. Sinatra, J. Stahl-Gemake, and D.N. Berg. 1984.
Improving reading comprehension of disabled read-
ers through semantic mapping. The Reading Teacher,
38(1):22?29.
L. Sitbon and P. Bellot. 2008. A readability measure for
an information retrieval process adapted to dyslexics.
In Second international workshop on Adaptive Infor-
mation Retrieval (AIR 2008 in conjunction with IIiX
2008), pages 52?57.
J.F. Sowa. 1983. Conceptual structures: information
processing in mind and machine. Addison-Wesley
Pub., Reading, MA.
Tobii Technology. 2005. Product description Tobii 50
Series.
F.R. Vellutino, J.M. Fletcher, M.J. Snowling, and D.M.
Scanlon. 2004. Specific reading disability (dyslexia):
What have we learned in the past four decades? Jour-
nal of child psychology and psychiatry, 45(1):2?40.
P.A. Weaver. 1978. Comprehension, recall, and dyslexia:
A proposal for the application of schema theory. An-
nals of Dyslexia, 28(1):92?113.
T.G. West. 2009. In the Mind?s Eye: Creative Vi-
sual Thinkers, Gifted Dyslexics, and the Rise of Visual
Technologies. Prometheus Books.
S. Williams, E. Reiter, and L. Osman. 2003. Experi-
ments with discourse-level choices and readability. In
Proceedings of the 9th European Workshop on Natural
Language Generation (ENLG-2003), Budapest, Hun-
gary.
A Appendix
Below we present Text 2 (Fish) and its translation to
English.
Descubren en Valencia una nueva especie de
pez prehisto?rico
El estudio de un lago salino que existio? hace 10 mil-
lones de an?os en Bicorb (Valencia) ha permitido descubrir
el fo?sil de una nueva especie de pez prehisto?rico y de sus
heces. Segu?n informo? este martes el Instituto Geolo?gico
y Minero de Espan?a, este pez depredador ha sido bau-
tizado por los investigadores como ?Aphanius bicorben-
sis?, en honor a la poblacio?n de Bicorb donde ha sido
encontrado. La investigacin ha sido realizada por En-
rique Pen?alver, experto en insectos fo?siles del Instituto
Geolo?gico y Minero, y por Jean Gaudant, especialista en
peces fo?siles del Museo Nacional de Historia Natural de
Par??s, gracias a la financiacio?n de la Consejer??a de Cultura
de la Generalitat Valenciana. El estudio del contenido
de las heces de estos peces, que tambie?n quedaron fos-
ilizadas en la roca, ha permitido a los investigadores saber
que este depredador se alimentaba de los foramin??feros y
de las larvas de mosquito, especialmente abundantes en
el lago.
A new species of a prehistoric fish is discovered
in Valencia
The study of a saline lake that existed 10 million years
ago in Bicorb (Valencia) has uncovered the fossil of a new
species of prehistoric fish and their feces. The Geological
and Mining Institute of Spain informed last Tuesday that
this predatory fish has been named by the researchers as
?Aphanius bicorbensis? in honor of the town of Bicorb
where was found. The research was conducted by En-
rique Pen?alver, an expert on insect fossils of the Geologi-
cal and Mining Institute, and Jean Gaudant, a specialist in
fossil fishes of the National Museum of Natural History
in Paris, thanks to funding from the Council of Culture of
the Government of Valencia. The study of the content of
the feces of these fishes, which were also fossilized in the
rock, has allowed researchers to know that this predator
was feeding on foraminifera and mosquito larvae, espe-
cially abundant in the lake.
32
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 75?84,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
A Hybrid System for Spanish Text Simplification
Stefan Bott
Universitat Pompeu Fabra
C/ Tanger, 122-140
Barcelona, Spain
Horacio Saggion
Universitat Pompeu Fabra
C/ Tanger, 122-140
Barcelona, Spain
David Figueroa
Asi-soft
C/ Albasanz 76
Madrid, Spain
Abstract
This paper addresses the problem of automatic
text simplification. Automatic text simplifica-
tions aims at reducing the reading difficulty
for people with cognitive disability, among
other target groups. We describe an automatic
text simplification system for Spanish which
combines a rule based core module with a sta-
tistical support module that controls the ap-
plication of rules in the wrong contexts. Our
system is integrated in a service architecture
which includes a web service and mobile ap-
plications.
1 Introduction
According to the Easy-to-Read Foundation at least
5% of the world population is functional illiterate
due to disability or language deficiencies. Easy
access to digital content for the intellectual dis-
abled community or people with difficulty in lan-
guage comprehension constitutes a fundamental hu-
man right (United Nations, 2007); however it is far
from being a reality. Nowadays there are several
methodologies that are used to make texts easy to
read in such ways that they enable their reading by
a target group of people. These adapted or simpli-
fied texts are currently being created manually fol-
lowing specific guidelines developed by organiza-
tions, such as the Asociaci?n Facil Lectura,1 among
others. Conventional text simplification requires a
heavy load of human resources, a fact that not only
limits the number of simplified digital content ac-
1http://www.lecturafacil.net
cessible today but also makes practically impossi-
ble easy access to already available (legacy) mate-
rial. This barrier is especially important in contexts
where information is generated in real time ? news
? because it would be very expensive to manually
simplify this type of ?ephemeral? content.
Some people have no problem reading compli-
cated official documents, regulations, scientific lit-
erature etc. while others find it difficult to under-
stand short texts in popular newspapers or maga-
zines. Even if the concept of "easy-to-read" is not
universal, it is possible in a number of specific con-
texts to write a text that will suit the abilities of most
people with literacy and comprehension problems.
This easy-to-read material is generally characterized
by the following features:
? The text is usually shorter than a standard text
and redundant content and details which do not
contribute to the general understanding of the
topic are eliminated.2 It is written in varied
but fairly short sentences, with ordinary words,
without too many subordinate clauses.
? Previous knowledge is not taken for granted.
Backgrounds, difficult words and context are
explained but in such a way that it does not dis-
turb the flow of the text.
? Easy-to-read is always easier than standard lan-
guage. There are differences of level in differ-
2Other providers, for example the Simple English Wikipedia
(http://simple.wikipedia.org) explicitly oppose to
content reduction. The writing guidelines for the Simple En-
glish Wikipedia include the lemma "Simple does not mean
short".
75
ent texts, all depending on the target group in
mind.
Access to information about culture, literature,
laws, local and national policies, etc. is of
paramount importance in order to take part in so-
ciety, it is also a fundamental right. The United Na-
tions (2007) "Convention on the Rights of Persons
with Disabilities" (Article 21) calls on governments
to make all public information services and docu-
mentation accessible for different groups of people
with disabilities and to encourage the media - tele-
vision, radio, newspapers and the internet - to make
their services easily available to everyone. Only a
few systematic efforts have been made to address
this issue. Some governments or organisations for
people with cognitive disability have translated doc-
uments into a language that is "easy to read", how-
ever, in most countries little has been done and orga-
nizations and people such as editors, writers, teach-
ers and translators seldom have guidelines on how to
produce texts and summaries which are easy to read
and understand.
1.1 Automatic Text Simplification
Automatic text simplification is the process by
which a computer transforms a text for a particular
readership into an adapted version which is easier to
read than the original. It is a technology which can
assist in the effort of making information more ac-
cessible and at the same time reduce the cost associ-
ated with the mass production of easy texts. Our re-
search is embedded within the broader context of the
Simplext project (Saggion et al, 2011).3 It is con-
cerned with the development of assistive text simpli-
fication technology in Spanish and for people with
cognitive disabilities. The simplification system is
currently under development. Some of the compo-
nents for text simplification are operational, while
other parts are in a development stage. The sys-
tem is integrated in a larger service hierarchy which
makes it available to the users. This paper concen-
trates on syntactic simplification, as one specific as-
pect, which is a central, but not the only aspect of
automatic text simplification. More concretely, we
present a syntactic simplification module, which is
3http://www.simplext.es
based on a hybrid technique: The core of the sys-
tem is a hand-written computational grammar which
reduces syntactic complexity and the application of
the rules in this grammar is controlled by a statisti-
cal support system, which acts as a filter to prevent
the grammar from manipulating wrong target struc-
tures. Section 2 describes related work, in the con-
text of which our research has been carried out. Sec-
tion 3 justifies the hybrid approach we have taken
and section 4 describes our syntactic simplification
module, including an evaluation of the grammar and
the statistical component. Finally, in section 5 we
show how our simplification system is integrated in
a larger architecture of applications and services.
2 Related Work
As it has happened with other NLP tasks, the first
attempts to tackle the problem of text simplifica-
tion were rule-based (Chandrasekar et al, 1996;
Siddharthan, 2002). In the last decade the focus
has been gradually shifting to more data driven ap-
proaches (Petersen and Ostendorf, 2007) and hybrid
solutions. The PorSimples (Alu?sio et al, 2008;
Gasperin et al, 2010) project used a methodology
where a parallel corpus was created and this cor-
pus was used to train a decision process for sim-
plification based on linguistic features. Siddharthan
(2011) compares a rule-based simplification system
with a simplification system based on a general pur-
pose generator.
Some approaches have concentrated on specific
constructions which are especially hard to under-
stand for readers with disabilities (Carroll et al,
1998; Canning et al, 2000), others focused on text
simplification as a help for other linguistic tasks
such as the simplification of patent texts (Mille and
Wanner, 2008; Bouayad-Agha et al, 2009). Re-
cently the availability of larger parallel or quasi-
parallel corpora, most notably the combination of
the English and the Simple English Wikipedia,
has opened up new possibilities for the use of
more purely data-driven approaches. Zhu et al
(2010), for example, use a tree-based simplification
model which uses techniques from statistical ma-
chine translation (SMT) with this data set.
A recent work, which is interesting because of
its purely data-driven setup, is Coster and Kauchak
76
(2011). They use standard software from the field
of statistical machine translation (SMT) and apply
these to the problem of text simplification. They
complement these with a deletion component which
was created for the task. They concentrate on four
text simplification operations: deletion, rewording
(lexical simplification), reordering and insertions.
Text simplification is explicitly treated in a simi-
lar way to sentence compression. They use stan-
dard SMT software, Moses (Koehn et al, 2007) and
GIZA++ (Och and Ney, 2000), and define the prob-
lem as translating from English (represented by the
English Wikipedia) to Simple English (represented
by the Simple English Wikipedia). The translation
process can then imply any of the four mentioned
operations. They compared their approach to var-
ious other systems, including a dedicated sentence
compression system (Knight and Marcu, 2002) and
show that their system outperforms the others when
evaluated on automatic metrics which use human
created reference text, including BLEU (Papineni et
al., 2002). Their problem setting does, however, not
include sentence splitting (as we will describe be-
low). Another potential problem is that the met-
rics they use for evaluation compare to human ref-
erences, but they do not necessarily reflect human
acceptability or grammaticality.
Woodsend and Lapata (2011) use quasi-
synchronous grammars as a more sophisticated
formalism and integer programming to learn to
translate from English to Simple English. This
system can handle sentence splitting operations
and the authors use both automatic and human
evaluation and show an improvement over the
results of Zhu et al (2010) on the same data set, but
they have to admit that learning from parallel bi-text
is not as efficient as learning from revision histories
of the Wiki-pages. Text simplification can also be
seen as a type of paraphrasing problem. There are
various data-driven approaches to this NLP-task
(Madnani and Dorr, 2010), but they usually focus on
lexical paraphrases and do not address the problem
of sentence splitting, either.
Such data-driven methods are very attractive, es-
pecially because they are in principle language in-
dependent, but they do depend on a large amount of
data, which are not available for the majority of lan-
guages.
3 A Hybrid Approach to Text
Simplification
There are several considerations which lead us to
take a hybrid approach to text simplification. First
of all there is a lack of parallel data in the case of
Spanish. Within our project we are preparing a cor-
pus of Spanish news texts (from the domain of na-
tional news, international news, society and culture),
consisting of 200 news text and their manually sim-
plified versions. The manual simplification is time
consuming and requires work from specially trained
experts, so the resulting corpus is not very big, even
if the quality is controlled and the type of data is very
specific for our needs. It is also very hard to find
large amounts of parallel text from other sources. In
order to use data driven techniques we would require
amounts of bi-text comparable to those used for sta-
tistical machine translation (SMT) and this makes it
nearly impossible to approach the problem from this
direction, at least for the time being.
But there are also theoretic considerations which
make us believe that a rule based approach is a good
starting point for automatic text simplification. We
consider that there are at least four separate NLP
tasks which may be combined in a text simplifica-
tion setting and which may help to reduce the read-
ing difficulty of a text. They all have a different na-
ture and require different solutions.
? Lexical simplification: technical terms, for-
eign words or infrequent lexical items make a
text more difficult to understand and the task
consists in substituting them with counterparts
which are easier to understand.
? Reduction of syntactic complexity: long sen-
tences, subordinate structure and especially re-
cursive subordination make a text harder to un-
derstand. The task consists in splitting long
sentences in a series of shorter ones.
? Content reduction: redundant information
make a text harder to read. The task consists
in identifying linguistic structures which can be
deleted without harming the text grammatical-
ity and informativeness in general. This task is
similar to the tasks of automatic summarization
and sentence compression.
77
? Clarification: Explaining difficult concepts re-
duces the difficulty of text understanding. The
task consists in identifying words which need
further clarification, selecting an appropriate
place for the insertion of a clarification or a
definition and finding an appropriate text unit
which actually clarifies the concept.
There is at least one task of the mentioned which
does not fully correspond to an established machine
learning paradigm in NLP, namely the reduction of
syntactic complexity. Consider the example (1), an
example from our corpus; (2) is the simplification
which was produced by our system.
(1) Se trata de un proyecto novedoso y pionero
que coordina el trabajo de seis concejal?as,
destacando las delegaciones municipales de
Educaci?n y Seguridad . . .
"This is a new and pioneering project that
coordinates the work of six councillors,
highlighting the municipal delegations Ed-
ucation and Safety . . . "
(2) Se trata de un proyecto novedoso y pionero ,
destacando las delegaciones municipales de
Educaci?n y Seguridad . . .
Este proyecto coordina el trabajo de seis
concejal?as.
"This is a new and pioneering project, high-
lighting the municipal delegations Educa-
tion and Safety . . .
This project coordinates the work of six
councillors."
What we can observe here is a split operation
which identifies a relative clause, cuts it out of the
matrix clause and converts it into a sentence of its
own. In the process the relative pronoun is deleted
and a subject phrase (este proyecto / this project) has
been added, whose head noun is copied from the ma-
trix clause. It is tempting to think that converting a
source sentence A in a series of simplified sentences
{b1, . . . , bn} is a sort of translation task, and a very
trivial one. In part this is true: most words translate
to a word which is identical in its form and they hap-
pen to appear largely in the same order. The difficult
part of the problem is that translation is usually an
operation from sentence to sentence, while here the
problem setting is explicitly one in which one input
unit produces several output units. This also affects
word alignment: in order to find the alignment for
the word proyecto in (1) the alignment learner has
to identify the word proyecto in two sentences in
(2). The linear distance between the two instances of
this noun is considerable and the sentences in which
two alignment targets occur are not even necessarily
adjacent. In addition, there may be multiple occur-
rences of the same word in the simplified text which
are not correct targets; the most apparent case are
functional words, but even words which are gener-
ally infrequent may be used repeatedly in a small
stretch of text if the topic requires it (in this para-
graph, for example, the word translation occurs 4
times and the word sentence 5 times). While a ma-
chine can probably learn the one-to-may translations
which are needed here, a non-trivial extension of the
machine-translation setting is needed and the learn-
ing problem needs to be carefully reformulated. Ap-
plying standard SMT machinery does not seem to
truly address the problem of syntactic simplification.
In fact, some approaches to SMT try use text simpli-
fication as a pre-process for translation; for exam-
ple Poornima et al (2011) apply a sentence splitting
module in order to improve translation quality.
On the other hand, other sub-task mentioned
above can be treated with data driven methods. Lex-
ical simplification requires the measurement of lex-
ical similarity, combined with word sense disam-
biguation. Content reduction is very similar to ex-
tractive summarization or sentence compression and
the insertion of clarifications can be broken down
into three learnable steps: identification of difficult
words, finding an insertion site and choosing a suit-
able definition for the target word.
4 Syntactic Simplification
We are developing a text simplification system
which will integrate different simplification mod-
ules, such as syntactic simplification, lexical simpli-
fication (Drndarevic and Saggion, 2012) and content
reduction. At the moment the most advanced mod-
ule of this system is the one for syntactic simplifica-
tion. In (Bott et al, 2012) we describe the function-
ing of the simplification grammar in more detail.
For the representation of syntactic structures we
78
use dependency trees. The trees are produced by the
Mate-tools parser (Bohnet, 2009) and the syntactic
simplification rules are developed within the MATE
framework (Bohnet et al, 2000). MATE is a graph
transducer which uses hand written grammars. For
grammar development we used a development cor-
pus of 282 sentences.
The grammar mainly focuses on syntactic simpli-
fication and, in particular, sentence splitting. The
types of sentence splitting operations we treat at the
moment are the following ones:
? Relative clauses: we distinguish between sim-
ple relative clauses which are only introduced
by a bare relative pronoun (e.g. a question
which is hard to answer) and complex relative
clauses which are introduced by a preposition
and a relative pronoun (e.g. a question to which
there is no answer)
? Gerundive constructions and participle con-
structions (e.g. the elections scheduled for next
November)
? Coordinations of clauses (e.g.[the problem is
difficult] and [there is probably no right an-
swer]) and verb phrases (e.g. The problem [is
difficult] and [has no easy solution]).
? Coordinations of objects clauses (e.g. . . . to get
close to [the fauna], [the plant life] and [the
culture of this immense American jungle re-
gion])
We carried out a evaluation of this grammar,
which is resumed in Table 1. This evaluation looked
at the correctness of the output. Many of the er-
rors were due to wrong parse trees and and the
grammar produced an incorrect output because the
parsed input was already faulty. In the case of rel-
ative clauses nearly 10% occurred because of this
and in the case of gerundive construction 37% of
the errors belonged into that category. We also
found that many of the syntactic trees are ambigu-
ous and cannot be disambiguated only on the basis
of morphosyntactic information. A particular case
of such ambiguity is the distinction between restric-
tive and non-restrictive relative clauses. Only non-
restrictive clauses can be turned into separate sen-
tences and the distinction between the two types is
usually not marked by syntax in Spanish4. Error
analysis showed us that 57.58% of all the errors re-
lated to relative clauses were due to this distinction.
A further 18.18% of the error occurred because the
grammar wrongly identified complement clauses as
relative clauses (in part because of previous parsing
errors).
For this reason, and according to our general phi-
losophy to apply data-driven approaches whenever
possible, we decided to apply a statistical filter in
order to filter out cases where the applications of
the simplification rules lead to incorrect results. Fig-
ure 1 shows the general architecture of the automatic
simplification system, including the statistical filter.
The nucleus of the system in its current state is the
syntactic simplification system, implemented as a
MATE grammar, which consists of various layers.
Origina
l Text Parser
Markin
g of 
Target 
Structu
re
Statisitc
al
Filterin
g
Mate-T
ools
Simplifi
ed Text
Applica
tion of Structu
ral Change
s MATE
Mate-T
ools
Figure 1: The architecture of the simplification system
Syntactic simplification is carried out in three
steps: first a grammar looks for suitable target struc-
tures which could be simplified. Such structures are
then marked with an attribute that informs subse-
quent levels of the grammar. After that the statistical
filter applies and classifies the marked target struc-
tures according to whether they should be changed
or not. In a third step the syntactic manipulations
themselves are carried out. This can combine dele-
tions, insertions and copying of syntactic nodes or
subtrees.
4In English it is mandatory to place non-restrictive relative
clauses between commas, even if many writers do not respect
this rule, but in Spanish comma-placement is only a stylistic
recommendation.
79
Operation Precision Recall Frequency
Relative Clauses (all types) 39.34% 0.80% 20.65%
Gerundive Constructions 63.64% 20.59% 2.48%
Object coordination 42.03% 58.33% 7.79%
VP and clause coordination 64.81% 50% 6.09%
Table 1: Percentage of right rule application and frequency of application (percentage of sentences affected) per rule
type
4.1 Statistical Filtering
Since the training of such filters requires a certain
amount of hand-annotated data, so far we only im-
plemented filters for simple and complex relative
clauses. These filters are implemented as binary
classifiers. For each structure which the grammar
could manipulate, the classifier decides if the sim-
plification operation should be carried out or not.
In this way, restrictive relative clauses, comple-
ment clauses and other non-relative clause construc-
tions should be retained by the filter and only non-
restrictive relative clauses are allowed to pass.
For the training of the filters we hand annotated
a selection of sentences which contained the rele-
vant type of relative clauses (150 cases for simple
and 116 for complex). The training examples were
taken from news texts published in the on-line edi-
tion of an established Spanish newspaper. The style
in which these news were written was notably differ-
ent from the news texts of the corpus we are devel-
oping in within our project, in that they were much
more complex and contained more cases of recursive
subordination. The annotators reported that some of
the sentences had to be re-read in order to fully un-
derstand them; this is not uncommon in this type of
news which may contain opinion columns and in-
depth comments.
In our classification framework we consider one
set of contextual features arising from tokens sur-
rounding the target structure to be classified5 ? the
relative pronoun marked by the simplification iden-
tification rules. This set is composed of, among oth-
ers, the position of the target structure in the sen-
tence; the parts of speech tags of neighbour token;
the depth of the target in a dependency tree; the de-
pendency information to neighbour tokens, etc.
Linguistic intuitions such as specific construc-
5A 5 words window to the left and to the right.
tions which, according to the Spanish grammar,
could be considered as indicating that the simplifi-
cation can or cannot take place. These features are
for example: the presence of a definite or indefinite
article; the presence of a comma in the vicinity of
the pronoun; specific constructions such as ya que
(since), como que (as), etc. where que is not relative
pronoun; context where que is used as a comparative
such as in m?s....que (more... than); contexts where
que is introducing a subordinate complement as in
quiero que (I want that ...); etc. While some of these
features should be implemented relying on syntactic
analysis we have relied for the experiments reported
here on finite state approximations implementing all
features in regular grammars using the GATE JAPE
language (Cunningham et al, 2000; Maynard et al,
2002). For other learning tasks such as deciding
for the splitting of coordinations or the separation
of participle clauses we design and implement spe-
cific features based on intuitions; contextual features
remain the same for all problems.
The classification framework is implemented in
the GATE system, using the machine learning li-
braries it provides (Li et al, 2005). In particular,
we have used the Support Vector Machines learn-
ing libraries (Li and Shawe-Taylor, 2003) which
have given acceptable classification results in other
NLP tasks. The framework allows us to run cross-
validation experiments as well as training and test-
ing.
Table 2 shows the performance of the statistical
filter in isolation, i.e. the capacity of the filter alone
to distinguish between good and bad target struc-
tures for simplification operations. The in-domain
performance was obtained by a ten-fold cross clas-
sification of the training data. The out-of-domain
evaluation was carried out over news texts from our
own corpus, the same collection we used for the
80
Figure 2: A simplified news text produced by the service
on a tablet computer running Android
evaluation of the grammar and the combination of
the grammar with the statistical filter. The perfor-
mance is given here as the overall classification re-
sult. Table 3 shows the performance of the grammar
with and without application of the filter.6
4.2 Discussion
We can observe that the statistical filters have a
quite different performance when they are applied
in-domain and out-of-domain (cf. Table 2), espe-
cially in the case of simple relative clauses. We
attribute this to the fact that the style of the texts
which we used for training is much more compli-
cated than the texts which we find in our own cor-
pus. The annotators commented that many relative
clauses could not turned into separate sentences be-
cause of the overall complexity of the sentence. This
problem seems to propagate into the performance
of the combination of the grammar with the filter
(cf. Table 3). The precision improves with filter-
ing, but the recall drops even more. Again, we sus-
pect that the filter is very restrictive because in the
training data many relative clauses were not separa-
ble, due to the overall sentence complexity which is
much lesser in the corpus from which the test data
was taken. For the near future we plan to repeat
6The results here are not fully comparable to Table 1, be-
cause in order to evaluate the filter, we did not consider parse
errors, as we did in the previous evaluation.
Este mi?rcoles las personas con Sindrome de Down celebran
si d?a mundial . En Espa?a , hay m?s de 34 .000 personas con
esta discapacidad . esta discapacidad ocurre en uno de cada
800 nacimientos .
El S?drome de Down es un trastorno gen?tico . este trastorno
causa la presencia de una copia extra del cromosoma 21 en
vez de los dos habituales ( trisom?a del par 21 ) . La
consecuencia es un grado variable de discapacidad cognitiva y
unos r?sgos f?sicos particulares y reconocibles .
Se trata de la causa m?s frecuente de discapacidad cognitiva
ps?quica cong?nita y debe su nombre a John Langdon Haydon
Down . este Landgdon fue el primero en describir esta
alteraci?n gen?tica en 1866 . Siegue sin conocerse con
exactitud las causas . estas causas provocan el exceso
cromos?mico , a?nque se relaciona estad?stica mente con
madres de m?s de 35 a?os .
Table 4: The simplified text shown in figure2
the experiment with annotated data which is more
similar to the test set. The performance in the case
of complex relative clauses is much better. We at-
tribute the difference between simple and complex
relative clauses to the fact that the complex construc-
tions cannot be confounded with other, non-relative,
constructions, while in the case of the simple type
this danger is considerable. The somewhat unre-
alistic value of 100% is a consequence of the fact
that in the part of the corpus we annotated complex
relative clauses were not very frequent. We took
some additional cases from our corpus into consider-
ation, evaluating more cases from the corpus where
the corresponding rule was applied7 and the value
dropped to slightly over 90%.
5 Integration of the Simplification System
in Applications
As we have mentioned in the introduction, our text
simplification system is integrated in a larger service
and application setting. Even if some modules of the
system must still be integrated, we have an operative
prototype which includes a mobile application and a
web service.
In the context of the Simplext project two mo-
bile applications have been developed. The first one
runs on iOS (developed by Apple Inc. for its de-
vices: Iphone, Ipad and Ipod touch), and the other
one on Android (developed by Google, included in
many different devices). These applications allow
7For these cases we could not calculate recall because this
would have implied a more extensive annotation of all the sen-
tences of the part of the corpus from which they were taken.
81
Operation Precision Recall F-score
Simple Relative Clauses (in domain) 85.41% 86.77% 86.06%
Complex Relative Clauses (in domain) 70.88% 71.33% 71.10 %
Simple Relative Clauses (out of domain) 76.35% 76.35% 76.35%
Complex Relative Clauses (out of domain) 90.48% 85.71% 88.10%
Table 2: The performance of the statistical filter in isolation
Operation Precision Recall F-score
Simple Relative Clauses (Grammar) 47.61% 95.24% 71.43%
Complex Relative Clauses (Grammar) 62.50% 55.56% 59.02%
Simple Relative Clauses (Grammar + Filter) 59.57% 66.67% 63.12%
Complex Relative Clauses (Grammar + Filter) 100% 55.56% 77.78%
Table 3: The performance of grammar and the statistical filter together
to read news feeds (RSS / Atom) from different
sources through a proxy that provide the language
simplification mechanism. The mobile applications
are basically RSS/Atom feed readers, with simpli-
fication capabilities (provided by the service layer).
Both applications work the same way and allow to
the user functionalities as keeping a list of favourite
feeds, adding and removing feeds, marking content
as favourite and showing the simplified and origi-
nal versions of the content. Also a web service was
created, which works in a similar way for RSS and
Atom feeds and allows to simplify the text portion
of other publicly available websites.
Figure 2 shows a screen capture of the mobile ap-
plication running in a Android tablet, displaying a
simplification example of a text taken from a news
website. The display text of this image is reproduced
in Table 4 for better readability. The text itself is too
long for us to provide a translation, but it can be seen
that many sentences have been split. Also a series of
minor problems can be seen, which we will resolve
in the near future: The first word of a sentence is still
in lower case and the head noun of the named en-
tity John Langdon Haydon Down was not correctly
identified.
6 Conclusions
Automatic text simplification is an Assistive Tech-
nology which help people with cognitive disabilities
to gain access to textual information. In this paper
we have presented a syntactic simplification module
of a automatic text simplification system which is
under development. We have presented arguments
for the decision of using a hybrid strategy which
combines a rule-based grammar with a statistical
support component, we have described the imple-
mentation of this idea and have given a contrastive
evaluation of the grammar with and without statisti-
cal support. The simplification system we described
here is integrated in a user-oriented service architec-
ture with mobile applications and web services. In
future work we will further enhance the system and
integrate new components dedicated to other simpli-
fication aspects, such as lexical simplification and
content reduction.
Acknowledgements
The research described in this paper arises from
a Spanish research project called Simplext: An au-
tomatic system for text simplification (http://
www.simplext.es). Simplext is led by Tech-
nosite and partially funded by the Ministry of In-
dustry, Tourism and Trade of the Government of
Spain, by means of the National Plan of Scientific
Research, Development and Technological Innova-
tion (I+D+i), within strategic Action of Telecom-
munications and Information Society (Avanza Com-
petitiveness, with file number TSI-020302-2010-
84). We are grateful to fellowship RYC-2009-04291
from Programa Ram?n y Cajal 2009, Ministerio de
Econom?a y Competitividad, Secretar?a de Estado de
Investigaci?n, Desarrollo e Innovaci?n, Spain.
82
References
Sandra M. Alu?sio, Lucia Specia, Thiago Alexan-
dre Salgueiro Pardo, Erick Galani Maziero, and Re-
nata Pontin de Mattos Fortes. 2008. Towards brazil-
ian portuguese automatic text simplification systems.
In ACM Symposium on Document Engineering, pages
240?248.
Bernd Bohnet, Andreas Langjahr, and Leo Wanner.
2000. A development environment for MTT-based
sentence generators. Revista de la Sociedad Espa?ola
para el Procesamiento del Lenguaje Natural.
Bernd Bohnet. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceed-
ings of the Conference on Natural Language Learning
(CoNLL), pages 67?72, Boulder, Colorado. Associa-
tion for Computational Linguistics.
Stefan Bott, Horacio Saggion, and Simon Mille. 2012.
Text simplification tools for spanish. In Proceedings
of the LREC-2012, Estambul, Turkey.
Nadjet Bouayad-Agha, Gerard Casamayor, Gabriela Fer-
raro, and Leo Wanner. 2009. Simplification of patent
claim sentences for their paraphrasing and summariza-
tion. In FLAIRS Conference.
Yvonne Canning, John Tait, Jackie Archibald, and Ros
Crawley. 2000. Cohesive generation of syntactically
simplified newspaper text. In TSD, pages 145?150.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
In Proc. of AAAI-98 Workshop on Integrating Artificial
Intelligence and Assistive Technology, pages 7?10.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text sim-
plification. In COLING, pages 1041?1044.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of Text-To-Text Generation, Portland, Oregon. Associ-
ation for Computational Linguistics.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS?00?10, Department
of Computer Science, University of Sheffield, Novem-
ber.
Biljana Drndarevic and Horacio Saggion. 2012. Towards
automatic lexical simplification in spanish: an empir-
ical study. In NAACL 2012 Workshop on Predicting
and Improving Text Readability for Target Reader Pop-
ulations, Montreal, Canada.
Caroline Gasperin, Erick Galani Maziero, and Sandra M.
Alu?sio. 2010. Challenging choices for text simplifi-
cation. In PROPOR, pages 40?50.
Kevin Knight and Daniel Marcu. 2002. Summarization
beyond sentence extraction: a probabilistic approach
to sentence compression. Artif. Intell., 139(1):91?107,
July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Y. Li and J. Shawe-Taylor. 2003. The SVM with
Uneven Margins and Chinese Document Categoriza-
tion. In Proceedings of The 17th Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC17), Singapore, Oct.
Yaoyong Li, Katalina Bontcheva, and Hamish Cunning-
ham. 2005. Using Uneven Margins SVM and Per-
ceptron for Information Extraction. In Proceedings
of Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005).
N. Madnani and B.J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341?387.
Diana Maynard, Valentin Tablan, Hamish Cunningham,
Cristian Ursu, Horacio Saggion, Katalina Bontcheva,
and Yorik Wilks. 2002. Architectural Elements of
Language Engineering Robustness. Journal of Nat-
ural Language Engineering ? Special Issue on Ro-
bust Methods in Analysis of Natural Language Data,
8(2/3):257?274.
Simon Mille and Leo Wanner. 2008. Making text re-
sources accessible to the reader: The case of patent
claims. Marrakech (Marocco), 05/2008.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440?447, Hongkong, China, Oc-
tober.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sarah E. Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis. In
In Proc. of Workshop on Speech and Language Tech-
nology for Education.
C. Poornima, V. Dhanalakshmi, K.M. Anand, and KP So-
man. 2011. Rule based sentence simplification for
english to tamil machine translation system. Interna-
tional Journal of Computer Applications, 25(8):38?42.
83
H. Saggion, E. G?mez Mart?nez, E. Etayo, A. Anula, and
L. Bourg. 2011. Text simplification in simplext. mak-
ing text more accessible. Procesamiento de Lenguaje
Natural, 47(0):341?342.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In In LREC?02: Proceedings of
the Language Engineering Conference, pages 64?71.
Advaith Siddharthan. 2011. Text simplification using
typed dependencies: A comparison of the robustness
of different generation strategies. In Proceedings of
the 13th European Workshop on Natural Language
Generation (ENLG), pages 2?11, September.
United Nations. 2007. Convention on the
rights of persons with disabilities. http:
//www2.ohchr.org/english/law/
disabilities-convention.htm.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 409?420.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of The 23rd
International Conference on Computational Linguis-
tics, pages 1353?1361, Beijing, China, Aug.
84
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 1?10,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
One Step Closer to Automatic Evaluation
of Text Simplification Systems
Sanja
?
Stajner
1
and Ruslan Mitkov
1
and Horacio Saggion
2
1
Research Group in Computational Linguistics, University of Wolverhampton, UK
2
TALN Research Group, Universitat Pompeu Fabra, Spain
S.Stajner@wlv.ac.uk, R.Mitkov@wlv.ac.uk, horacio.saggion@upf.edu
Abstract
This study explores the possibility of re-
placing the costly and time-consuming
human evaluation of the grammaticality
and meaning preservation of the output
of text simplification (TS) systems with
some automatic measures. The focus is on
six widely used machine translation (MT)
evaluation metrics and their correlation
with human judgements of grammatical-
ity and meaning preservation in text snip-
pets. As the results show a significant cor-
relation between them, we go further and
try to classify simplified sentences into:
(1) those which are acceptable; (2) those
which need minimal post-editing; and (3)
those which should be discarded. The pre-
liminary results, reported in this paper, are
promising.
1 Introduction
Lexically and syntactically complex sentences can
be difficult to understand for non-native speak-
ers (Petersen and Ostendorf, 2007; Alu??sio et
al., 2008b), and for people with language impair-
ments, e.g. people diagnosed with aphasia (Car-
roll et al., 1999; Devlin, 1999), autism spectrum
disorder (
?
Stajner et al., 2012; Martos et al., 2012),
dyslexia (Rello, 2012), congenital deafness (Inui
et al., 2003), and intellectual disability (Feng,
2009). At the same time, long and complex sen-
tences are also a stumbling block for many NLP
tasks and applications such as parsing, machine
translation, information retrieval, and summarisa-
tion (Chandrasekar et al., 1996). This justifies the
need for Text Simplification (TS) systems which
would convert such sentences into their simpler
and easier-to-read variants, while at the same time
preserving the original meaning.
So far, TS systems have been developed for En-
glish (Siddharthan, 2006; Zhu et al., 2010; Wood-
send and Lapata, 2011a; Coster and Kauchak,
2011; Wubben et al., 2012), Spanish (Saggion et
al., 2011), and Portuguese (Alu??sio et al., 2008a),
with recent attempts at Basque (Aranzabe et al.,
2012), Swedish (Rybing et al., 2010), Dutch
(Ruiter et al., 2010), and Italian (Barlacchi and
Tonelli, 2013).
Usually, TS systems are either evaluated for: (1)
the quality of the generated output, or (2) the effec-
tiveness/usefulness of such simplification on read-
ing speed and comprehension of the target popula-
tion. For the purpose of this study we focused only
on the former. The quality of the output generated
by TS systems is commonly evaluated by using
a combination of readability metrics (measuring
the degree of simplification) and human assess-
ment (measuring the grammaticality and meaning
preservation). Despite the noticeable similarity
between evaluation of the fluency and adequacy of
a machine translation (MT) output, and evaluation
of grammaticality and meaning preservation of a
TS system output, there have been no works ex-
ploring whether any of the MT evaluation metrics
are well correlated with the latter, and could thus
replace the time-consuming human assessment.
The contributions of the present work are the
following:
? It is the first study to explore the possibility of
replacing human assessment of the quality of
TS system output with automatic evaluation.
? It is the first study to investigate the correla-
tion of human assessment of TS system out-
put with MT evaluation metrics.
? It proposes a decision-making procedure for
the classification of simplified sentences into:
(1) those which are acceptable; (2) those
which need further post-editing; and (3) those
which should be discarded.
1
2 Related Work
The output of the TS system proposed by Sid-
dharthan (2006) was rated for grammaticality and
meaning preservation by three human evaluators.
Similarly, Drndarevic et al. (2013) evaluated the
grammaticality and the meaning preservation of
automatically simplified Spanish sentences on a
Likert scale with the help of twenty-five human
annotators. Additionally, the authors used seven
readability metrics to assess the degree of simplifi-
cation. Woodsend and Lapata (2011b), and Glava?s
and
?
Stajner (2013) used human annotators? rat-
ings for evaluating simplification, meaning preser-
vation, and grammaticality, while additionally ap-
plying several readability metrics for evaluating
complexity reduction in entire texts.
Another set of studies approached TS as an MT
task translating from ?original? to ?simplified?
language, e.g. (Specia, 2010; Woodsend and Lap-
ata, 2011a; Zhu et al., 2010). In this case, the qual-
ity of the output generated by the system was eval-
uated using several standard MT evaluation met-
rics: BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), and TERp (Snover et al., 2009).
3 Methodology
All experiments were conducted on a freely avail-
able sentence-level dataset
1
, fully described in
(Glava?s and
?
Stajner, 2013), and the two datasets
we derived from it. The original dataset and the
instructions for the human assessment are given in
the next two subsections. Section 3.3 explains how
we derived two additional datasets from the origi-
nal one, and to what end. Section 3.4 describes the
automatic MT evaluation metrics used as features
in correlation and classification experiments; Sec-
tion 3.5 presents the main goals of the study; and
Section 3.6 describes the conducted experiments.
3.1 Original dataset
The dataset contains 280 pairs of original sen-
tences and their corresponding simplified versions
annotated by humans for grammaticality, meaning
preservation, and simplicity of the simplified ver-
sion. We used all sentence pairs, focusing only on
four out of eight available features: (1) the original
text, (2) the simplified text, (3) the grammaticality
score, and (4) the score for meaning preservation.
2
1
http://takelab.fer.hr/data/evsimplify/
2
The other four features contain the pairID, groupID, the
method with which the simplification was obtained, and the
Category weighted ? Pearson MAE
Grammaticality 0.68 0.77 0.18
Meaning 0.53 0.67 0.37
Simplicity 0.54 0.60 0.28
Table 1: IAA from (Glava?s and
?
Stajner, 2013)
The simplified versions of original sentences
were obtained by using four different simplifi-
cation methods: baseline, sentence-wise, event-
wise, and pronominal anaphora. The baseline re-
tains only the main clause of a sentence, and dis-
cards all subordinate clauses, based on the out-
put of the Stanford constituency parser (Klein and
Manning, 2003). Sentence-wise simplification
eliminates all those tokens in the original sentence
that do not belong to any of the extracted factual
event mentions, while the event-wise simplifica-
tion transforms each factual event mention into a
separate sentence of the output. The last simplifi-
cation scheme (pronominal anaphora) additionally
employs pronominal anaphora resolution on top of
the event-wise simplification scheme.
3
3.2 Human Assessment
Human assessors were asked to score the given
sentence pairs (or text snippets in the case of split
sentences) on a 1?3 scale based on three crite-
ria: Grammaticality (1 ? ungrammatical, 2 ? mi-
nor problems with grammaticality, 3 ? grammati-
cal), Meaning (1 ? meaning is seriously changed
or most of the relevant information lost, 2 ? some
of the relevant information is lost but the meaning
of the remaining information is unchanged, 3 ? all
relevant information is kept without any change in
meaning), and Simplicity (1 ? a lot of irrelevant in-
formation is retained, 2 ? some of irrelevant infor-
mation is retained, 3 ? all irrelevant information is
eliminated). The inter-annotator agreement (IAA)
was calculated using weighted Kappa (weighted
?), Pearson?s correlation (Pearson), and mean av-
erage error (MAE), and the obtained results are
presented in Table 1. A few examples of assigned
scores are given in Table 2, where G, M, and S
denote human scores for grammaticality, meaning
preservation and simplicity respectively.
score for simplicity, which are not relevant here.
3
For more detailed explanation of simplification schemes
and the dataset see (Glava?s and
?
Stajner, 2013).
2
Ex. Original Simplified G M S SM
(a) ?It is understood the dead girl had
been living at her family home, in
a neighbouring housing estate, and
was visiting her older sister at the
time of the shooting.?
?The dead girl had been living at
her family home, in a neighbouring
housing estate and was visiting her
older sister.?
3 3 3 S
(b) ?On Facebook, more than 10,000
people signed up to a page an-
nouncing an opposition rally for
Saturday.?
?On Facebook, more than 10,000
people signed to a page announcing
an opposition rally for Saturday.?
2 3 3 S
(c) ?Joel Elliott, also 22, of North
Road, Brighton, was charged on
May 3 with murder. He appeared at
Lewes Crown Court on May 8 but
did not enter a plea.?
?Joel Elliott was charged on May 3
with murder. He appeared at Lewes
Crown Court on May 8.?
3 2 3 S
(d) ?For years the former Bosnia Serb
army commander Ratko Mladic had
evaded capture and was one of the
world?s most wanted men, but his
time on the run finally ended last
year when he was arrested near
Belgrade.?
?For years the former Bosnia Serb
army commander Ratko Mladic had
evaded but his time the run ended
last year he was arrested near Bel-
grade.?
1 2 3 S
(e) ?Police have examined the scene at
a house at William Court in Bel-
laghy, near Magherafelt for clues to
the incident which has stunned the
community.?
?Police have examined the scene
at William Court near Magherafelt.
The incident has stunned the com-
munity.?
3 1 3 P
(f) ?Rastan, 25 km (15 miles) north
of Homs city, has slipped in and
out of government control several
times since the uprising against As-
sad erupted in March 2011.?
?Rastan has slipped government
control several times. The uprising
erupted in March 2011.?
2 1 3 P
(g) ?But opposition parties and inter-
national observers said the vote was
marred by vote-rigging, including
alleged ballot-box stuffing and false
voter rolls.?
?But opposition parties and inter-
national observers said .?
1 1 3 B
(h) ?Foreign Affairs Secretary Albert
del Rosario was seeking a diplo-
matic solution with Chinese Ambas-
sador Ma Keqing, the TV network
said.?
?Foreign Affairs Secretary Albert
del Rosario was seeking a diplo-
matic solution with Chinese Ambas-
sador Ma Keqing, the TV network
said.?
3 3 1 B
(h) ? On Wednesday, two video jour-
nalists working for the state-owned
RIA Novosti news agency were
briefly detained outside the Election
Commission building where Putin
was handing in his application to
run.?
?On Wednesday two video journal-
ists were briefly detained outside
the Election Commission building.
Two video journalists worked for
the state-owned RIA Novosti news
agency. Putin was handing in his
application.?
3 2 2 E
Table 2: Human evaluation examples (G, M, and S correspond to the human scores for grammaticality,
meaning preservation and simplicity, and SM denotes the simplification method used: B ? baseline, S ?
sentence-wise, E ? event-wise, and P ? pronominal anaphora)
3
3.3 Derived Datasets
The original dataset (Original) contains separate
scores for grammaticality (G), meaning preserva-
tion (M), and simplicity (S), each of them on a 1?3
scale. From this dataset we derived two additional
ones: Total3 and Total2.
The Total3 dataset contains three marks (OK ?
use as it is, PE ? post-editing required, and Dis
? discard) derived from G and M in the Original
dataset. Those simplified sentences which scored
?3? for both meaning preservation (M) and gram-
maticality (G) are placed in the OK class as they
do not need any kind of post-editing. A closer
look at the remaining sentences suggests that any
simplified sentence which got a score ?2? or ?3?
for meaning preservation (M) could be easily post-
edited, i.e. it requires minimal changes which are
obvious from its comparison to the corresponding
original. For instance, in the sentence (b) in Ta-
ble 2 the only change that needs to be made is
adding the word ?up? after ?signed?. Those sen-
tences which scored ?2? for meaning need slightly
more, albeit simple modification. The simplified
text snippet (c) in Table 2 would need ?but did
not enter a plea? added at the end of the last
sentence. The next sentence (d) in the same ta-
ble needs a few more changes, but still very mi-
nor ones: adding the word ?capture? after ?had
evaded?, adding the preposition ?on? before ?the
run?, and adding ?when? after ?last year?. There-
fore, we grouped all those sentences into one class
? PE (sentences which require a minimal post-
editing effort). Those sentences which scored ?1?
for meaning need to either be left in their original
form or simplified from scratch. We thus classify
them as Dis. This newly created dataset (Total3)
allows us to investigate whether we could auto-
matically classify simplified sentences into those
three categories, taking into account both gram-
maticality and meaning preservation at the same
time.
The Total2 dataset contains only two marks (?0?
and ?1?) which correspond to the sentences which
should be discarded (?0?) and those which should
be retained (?1?), where ?0? corresponds to Dis in
Total3, and ?1? corresponds to the union of OK and
PE in Total3. The derivation procedure for both
datasets is presented in Table 3. We wanted to in-
vestigate whether the classification task would be
simpler (better performed) if there were only two
classes instead of three. In the case that such clas-
sification could be performed with satisfactory ac-
curacy, all sentences classified as ?0? would be left
in their original form or simplified with some dif-
ferent simplification strategy, while those classi-
fied as ?1? would be sent for a quick human post-
editing procedure.
Original
Total3 Total2
G M
3 3 OK 1
2 3 PE 1
1 3 PE 1
3 2 PE 1
2 2 PE 1
1 2 PE 1
3 1 Dis 0
2 1 Dis 0
1 1 Dis 0
Table 3: Datasets
Here it is important to mention that we decided
not to use human scores for simplicity (S) for sev-
eral reasons. First, simplicity was defined as the
amount of irrelevant information which was elim-
inated. Therefore, we cannot expect that any of
the six MT evaluation metrics would have a sig-
nificant correlation with this score (except maybe
TERp and, in particular, one of its parts ? ?number
of deletions?. However, none of the two demon-
strated any significant correlation with the sim-
plicity score, and those results are thus not re-
ported in this paper). Second, the output sentences
with a low simplicity score are not as detrimental
for the TS system as those with a low grammat-
icality or meaning preservation score. The sen-
tences with a low simplicity score would simply
not help the target user read faster or understand
better, but would not do any harm either. Alter-
natively, if the target ?user? is an MT or infor-
mation extraction (IE) system, or a parser for ex-
ample, such sentences would not lower the perfor-
mance of the system; they would just not improve
it. Low scores for G and M, however, would lead
to a worse performance for such NLP systems,
longer reading time, and a worse or erroneous un-
derstanding of the text. Third, the simplicity of
the output (or complexity reduction performed by
a TS system) could be evaluated separately, in a
fully automatic manner ? using some readability
measures or average sentence length as features
(as in (Drndarevi?c et al., 2013; Glava?s and
?
Stajner,
4
2013) for example).
3.4 Features: MT Evaluation Metrics
In all experiments, we focused on six commonly
used MT evaluation metrics. These are cosine
similarity (using the bag-of-words representation),
METEOR (Denkowski and Lavie, 2011), TERp
(Snover et al., 2009), TINE (Rios et al., 2011), and
two components of TINE: T-BLEU (which differs
from the standard BLEU (Papineni et al., 2002) by
using 3-grams, 2-grams, and 1-grams when there
are no 4-grams found, where the ?original? BLEU
would give score ?0?) and SRL (which is the com-
ponent of TINE based on semantic role labeling
using SENNA
4
). Although these two components
contribute equally to TINE (thus being linearly
correlated with TINE), we wanted to investigate
which one of them contributes more to the cor-
relation of TINE with human judgements. Given
their different natures, we expect T-BLEU to con-
tribute more to the correlation of TINE with hu-
man judgements of grammaticality, and SRL to
contribute more to the correlation of TINE with
human judgements of meaning preservation.
As we do not have the reference for the simpli-
fied sentence, all metrics are applied in a slightly
different way than in MT. Instead of evaluating the
translation hypothesis (output of the automatic TS
system in our case) with the corresponding ref-
erence translation (which would be a ?gold stan-
dard? simplified sentence), we apply the metrics
to the output of the automatic TS system com-
paring it with the corresponding original sentence.
Given that the simplified sentences in the used
dataset are usually shorter than the original ones
(due to the elimination of irrelevant content which
was the main focus of the TS system proposed by
Glava?s and
?
Stajner (2013)), we expect low scores
of T-BLEU and METEOR which apply a brevity
penalty. However, our dataset does not contain any
kind of lexical simplification, but rather copies all
relevant information from the original sentence
5
.
Therefore, we expect the exact matches of word
forms and semantic role labels (which are compo-
nents of the MT evaluation metrics) to have a good
correlation to human judgements of grammatical-
ity and meaning preservation.
4
http://ml.nec-labs.com/senna/
5
The exceptions being changes of gerundive forms into
past tense, and anaphoric pronoun resolution in some simpli-
fication schemes. See Section 3.1 and (Glava?s and
?
Stajner,
2013) for more details.
3.5 Goal
After we obtained the six automatic metrics (co-
sine, METEOR, TERp, TINE, T-BLEU, and
SRL), we performed two sets of experiments, try-
ing to answer two main questions:
1. Are the chosen MT evaluation metrics cor-
related with the human judgements of gram-
maticality and meaning preservation of the
TS system output?
2. Could we automatically classify the simpli-
fied sentences into those which are: (1) cor-
rect, (2) require a minimal post-editing, (3)
incorrect and need to be discarded?
A positive answer to the first question would
mean that there is a possibility of finding an au-
tomatic metric (or a combination of several au-
tomatic metrics) which could successfully replace
the time consuming human evaluation. The search
for that ?ideal? combination of automatic metrics
could be performed by using various classification
algorithms and carefully designed features. If we
manage to classify simplified sentences into the
three aforementioned categories with a satisfying
accuracy, the benefits would be two-fold. Firstly,
such a classification system could be used for an
automatic evaluation of TS systems and an easy
comparison of their performances. Secondly, it
could be used inside a TS system to mark those
sentences of low quality which need to be checked
further, or those sentences whose original mean-
ing changed significantly. The latter could then be
left in their original form or simplified using some
different technique.
3.6 Experiments
The six experiments conducted in this study are
presented in Table 4. The first two experiments
had the aim of answering the first question (Sec-
tion 3.5) as to whether the chosen MT metrics cor-
relate with the human judgements of grammatical-
ity (G) and meaning preservation (M) of the TS
system output. The results were obtained in terms
of Pearson?s, Kendall?s and Spearman?s correla-
tion coefficients. The third and the fourth exper-
iments (Table 4) could be seen as the intermediate
experiments exploring the possibility of automatic
classification of simplified sentences according to
their grammaticality, and meaning preservation.
The main experiment was the fifth experiment, try-
ing to answer the second question (Section 3.5)
5
Exp. Description
1. Correlation of the six automatic MT metrics with the human scores for Grammaticality
2. Correlation of the six automatic MT metrics with the human scores for Meaning preservation
3. Classification of the simplified sentences into 3 classes (?1? ? Bad, ?2? ? Medium, and ?3? ? Good) according to
their Grammaticality
4. Classification of the simplified sentences into 3 classes (?1? ? Bad, ?2? ? Medium, and ?3? ? Good) according to
their Meaning preservation
5. Classification of the simplified sentences into 3 classes (OK, PE, Dis) according to their Total3 score
6. Classification of the simplified sentences into 2 classes (?1? ? Retain, ?0? ? Discard) according to their Total2 score
Table 4: Experiments
as to whether we could automatically classify the
simplified sentences into those which are: (1) cor-
rect (OK), (2) require minimal post-editing (PE),
and (3) incorrect and need to be discarded (Dis).
The last experiment (Table 4) was conducted with
the aim of exploring whether the classification of
simplified sentences into only two classes ? Retain
(for further post-editing) and Discard ? would lead
to better results than the classification into three
classes (OK, PE, and Dis) in the fifth experiment.
All classification experiments were performed
in Weka workbench (Witten and Frank, 2005; Hall
et al., 2009), using seven classification algorithms
in a 10-fold cross-validation setup:
? NB ? NaiveBayes (John and Langley, 1995),
? SMO ? Weka implementation of Support
Vector Machines (Keerthi et al., 2001) with
normalisation (n) or with standardisation (s),
? Logistic (le Cessie and van Houwelingen,
1992),
? Lazy.IBk ? K-nearest neighbours (Aha and
Kibler, 1991),
? JRip ? a propositional rule learner (Cohen,
1995),
? J48 ? Weka implementation of C4.5 (Quin-
lan, 1993).
As a baseline we use the classifier which assigns
the most frequent (majority) class to all instances.
4 Results and Discussion
The results of the first two experiments (correla-
tion experiments in Table 4) are presented in Sec-
tion 4.1, while the results of the other four exper-
iments (classification experiments in Table 4) can
be found in Section 4.2. When interpreting the re-
sults of all experiments, it is important to keep in
mind that human agreements for meaning preser-
vation (M) and grammaticality (G) were accept-
able but far from perfect (Section 3.2), and thus
it would be unrealistic to expect the correlation
between the MT evaluation metrics and human
judgements or the agreement of the classification
system with human assessments to be higher than
the reported IAA agreement.
4.1 Correlation of Automatic Metrics with
Human Judgements
The correlations of automatic metrics with hu-
man judgements of grammaticality and meaning
preservation are given in Tables 5 and 6 respec-
tively. Statistically significant correlations (at a
0.01 level of significance) are presented in bold.
Metric Pearson Kendall Spearman
cosine 0.097 0.092 0.115
METEOR 0.176 0.141 0.178
T-BLEU 0.226 0.185 0.234
SRL 0.097 0.076 0.095
TINE 0.175 0.145 0.181
TERp -0.208 -0.158 -0.198
Table 5: Correlation between automatic evaluation
metrics and human scores for grammaticality
Metric Pearson Kendall Spearman
cosine 0.293 0.262 0.334
METEOR 0.386 0.322 0.405
T-BLEU 0.442 0.382 0.475
SRL 0.348 0.285 0.356
TINE 0.427 0.385 0.447
TERp -0.414 -0.336 -0.416
Table 6: Correlation between automatic evaluation
metrics and human scores for meaning preserva-
tion
It can be noted that human perception of gram-
maticality is positively correlated with three auto-
6
Algorithm
Grammaticality Meaning Total3 Total2
P R F P R F P R F P R F
NB 0.53 0.46 0.48 0.54 0.54 0.54 0.54 0.53 0.53 0.74 0.69 0.71
SMO(n) 0.39 0.63 0.48 0.52 0.49 0.45 0.43 0.53 0.44 0.55 0.74 0.63
SMO(s) 0.39 0.63 0.48 0.57 0.56 0.55 0.57 0.55 0.51 0.60 0.73 0.63
Logistic 0.45 0.61 0.49 0.57 0.57 0.56 0.61 0.60 0.59 0.75 0.77 0.74
Lazy.IBk 0.57 0.58 0.57 0.50 0.50 0.50 0.54 0.54 0.54 0.73 0.73 0.73
JRip 0.41 0.59 0.48 0.53 0.50 0.48 0.57 0.56 0.55 0.72 0.75 0.73
J48 0.45 0.61 0.49 0.48 0.47 0.47 0.59 0.57 0.54 0.68 0.71 0.69
baseline 0.39 0.63 0.48 0.17 0.41 0.24 0.21 0.46 0.29 0.55 0.74 0.63
Table 7: Classification results (the best performances are shown in bold; baseline uses the majority class)
Actual
Grammaticality Meaning
Good Med. Bad Good Med. Bad
Good 127 21 23 50 31 7
Med. 29 19 10 24 73 16
Bad 24 9 10 9 31 31
Table 8: Confusion matrices for the best classifications according to Grammaticality (Lazy.IBk) and
Meaning (Logistic). The number of ?severe? classification mistakes (classifying Good as Bad or vice
versa) are presented in bold.
matic measures ? METEOR, T-BLEU, and TINE,
while it is negatively correlated with TERp (TERp
measures the number of edits necessary to perform
on the simplified sentence to transform it into its
original one, i.e. the higher the value of TERp,
the less similar the original and its corresponding
simplified sentence are. The other five MT metrics
measure the similarity between the original and its
corresponding simplified version, i.e. the higher
their value is, the more similar are the sentences
are). All the MT metrics appear to be even bet-
ter correlated with the human scores for meaning
preservation (Table 6), demonstrating six positive
and one (TERp) negative statistically significant
correlation with M. The correlation is the highest
for T-BLEU, TINE, and TERp, though closely fol-
lowed by all others.
4.2 Sentence Classification
The results of the four classification experiments
(Section 3.6) are given in Table 7.
At first glance, the performance of the classifi-
cation algorithms seems similar for the first two
tasks (classification of the simplified sentences
according to their Grammaticality and Meaning
preservation). However, one needs to take into ac-
count that the baseline for the first task was much
much higher than for the second task (Table 7).
Furthermore, it can be noted that for the first task,
recall was significantly higher than precision for
most classification algorithms (all except NB and
Logistic), while for the second task they were very
similar in all cases. More importantly, a closer
look at the confusion matrices reveals that most of
the incorrectly classified sentences were assigned
to the nearest class (Medium into Bad or Good;
Bad into Medium; and Good into Medium
6
) in the
second task, while it was not the case in the first
task (Table 8).
Classification performed on the Total3 dataset
outperformed both previous classifications ? that
based on Grammaticality and that based on Mean-
ing ? on four different algorithms (NB, Logis-
tic, JRip, and J48). Classification conducted on
Total3 using Logistic outperformed all results of
classifications on either Grammaticality or Mean-
ing separately (Table 7). It reached a 0.61, 0.60,
and 0.59 score for the weighted precision (P), re-
call (R), and F-measure (F), respectively, thus out-
performing the baseline significantly. More im-
portantly, classification on the Total3 dataset led
to significantly fewer mis-classifications between
Good and Bad (Table 9) than the classification
based on Grammaticality, and slightly less than
6
Bad, Medium, and Good correspond to marks ?1?, ?2?,
and ?3? given by human evaluators.
7
Actual
Total3
OK PE Dis.
OK 41 32 4
PE 17 85 12
Dis. 6 31 28
Table 9: Confusion matrix for the best classifica-
tion according to Total3 (Logistic). The number of
?severe? classification mistakes (classifying Good
as Bad or vice versa) are presented in bold.
Actual
Total2
Retain Discard
Retain 21 50
Discard 12 189
Table 10: Confusion matrix for the best classifi-
cation according to Total2 (Logistic). The num-
ber of ?severe? classification mistakes (classifying
Retain as Discard or vice versa) are presented in
bold.
the classification based only on Meaning (Table 8).
Therefore, it seems that simplified sentences are
better classified into three classes giving a unique
score for both grammaticality and preservation of
meaning together.
The binary classification experiments based on
the Total2 led to results which significantly out-
performed the baseline in terms of precision and
F-measure (Table 7). However, they resulted in
a great number of sentences which should be re-
tained (Retain) being classified into those which
should be discarded (Discard) and vice versa (Ta-
ble 10). Therefore, it seems that it would be better
to opt for classification into three classes (Total3)
than for classification into two classes (Total2).
Additionally, we used CfsSubsetEval attribute
selection algorithm (Hall and Smith, 1998) in or-
der to identify the ?best? subset of features. The
?best? subsets of features for each of the four clas-
sification tasks returned by the algorithm are listed
in Table 11. However, the classification perfor-
mances achieved (P, R, and F) when using only
the ?best? features did not differ significantly from
those when using all initially selected features, and
thus are not presented in this paper.
5 Limitations
The used dataset does not contain any kind of
lexical simplification (Glava?s and
?
Stajner, 2013).
Classification ?Best? features
Meaning {TERp, T-BLEU, SRL, TINE}
Grammaticality {TERp, T-BLEU}
New3 {TERp, T-BLEU, SRL, TINE}
New2 {TERp, T-BLEU, SRL}
Table 11: The ?best? features (CfsSubsetEval)
Therefore, one should consider the limitation of
this TS system which performs only syntactic sim-
plification and content reduction. On the other
hand, the dataset used contains a significant con-
tent reduction in most of the sentences. If the same
experiments were conducted on a dataset which
performs only syntactic simplification, we would
expect much higher correlation of MT evaluation
metrics to human judgements, due to the lesser im-
pact of the brevity penalty in that case.
If we were to apply the same MT evaluation
metrics to a TS system which additionally per-
forms some kind of lexical simplification (either
a simple lexical substitution or paraphrasing), the
correlation results for T-BLEU and cosine similar-
ity would be lower (due to the lower number of
exact matches), but not for METEOR, TERp and
SRL (and thus TINE as well). As a similar prob-
lem is also present in the evaluation of MT sys-
tems where the obtained output could differ from
the reference translation (while still being equally
good), METEOR, TERp, and SRL in TINE ad-
ditionally use inexact matching. The first two use
the stem, synonym, and paraphrase matches, while
SRL uses ontologies and thesaurus.
6 Conclusions and Future Work
While the results reported are preliminary and
their universality needs to be validated on different
TS datasets, the experiments and results presented
can be regarded as a promising step towards an au-
tomatic assessment of grammaticality and mean-
ing preservation for the output of TS systems. In
addition and to the best of our knowledge, there
are no such datasets publicly available other than
the one used. Nevertheless, we hope that these re-
sults would initiate an interesting discussion in the
TS community and start a new direction of studies
towards automatic evaluation of text simplification
systems.
8
Acknowledgements
The research described in this paper was par-
tially funded by the European Commission un-
der the Seventh (FP7-2007-2013) Framework Pro-
gramme for Research and Technological Develop-
ment (FP7-ICT-2011.5.5 FIRST 287607).
References
D. Aha and D. Kibler. 1991. Instance-based learning
algorithms. Machine Learning, 6:37?66.
S. M. Alu??sio, L. Specia, T. A. S. Pardo, E. G. Maziero,
H. M. Caseli, and R. P. M. Fortes. 2008a. A cor-
pus analysis of simple account texts and the pro-
posal of simplification strategies: first steps towards
text simplification systems. In Proceedings of the
26th annual ACM international conference on De-
sign of communication, SIGDOC ?08, pages 15?22,
New York, NY, USA. ACM.
S. M. Alu??sio, L. Specia, T. A.S. Pardo, E. G. Maziero,
and R. P.M. Fortes. 2008b. Towards brazilian por-
tuguese automatic text simplification systems. In
Proceedings of the eighth ACM symposium on Doc-
ument engineering, DocEng ?08, pages 240?248,
New York, NY, USA. ACM.
M. J. Aranzabe, A. D??az De Ilarraza, and I. Gonz?alez.
2012. First Approach to Automatic Text Simplifica-
tion in Basque. In Proceedings of the first Natural
Language Processing for Improving Textual Acces-
sibility Workshop (NLP4ITA).
G. Barlacchi and S. Tonelli. 2013. ERNESTA: A sen-
tence simplification tool for childrens stories in ital-
ian. In Computational Linguistics and Intelligent
Text Processing.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings of the
9th Conference of the European Chapter of the ACL
(EACL?99), pages 269?270.
R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifi-
cation. In In Proceedings of the Sixteenth Inter-
national Conference on Computational Linguistics
(COLING ?96, pages 1041?1044.
W. Cohen. 1995. Fast Effective Rule Induction. In
Proceedings of the Twelfth International Conference
on Machine Learning, pages 115?123.
W. Coster and D. Kauchak. 2011. Learning to Sim-
plify Sentences Using Wikipedia. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics, pages 1?9.
M. Denkowski and A. Lavie. 2011. Meteor 1.3: Au-
tomatic Metric for Reliable Optimization and Evalu-
ation of Machine Translation Systems. In Proceed-
ings of the EMNLP Workshop on Statistical Machine
Translation.
S. Devlin. 1999. Simplifying natural language text for
aphasic readers. Ph.D. thesis, University of Sunder-
land, UK.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram coocurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138?145. Morgan Kaufmann Pub-
lishers Inc.
B. Drndarevi?c, S.
?
Stajner, S. Bott, S. Bautista, and
H. Saggion. 2013. Automatic Text Simplication
in Spanish: A Comparative Evaluation of Com-
plementing Components. In Proceedings of the
12th International Conference on Intelligent Text
Processing and Computational Linguistics. Lecture
Notes in Computer Science. Samos, Greece, 24-30
March, 2013., pages 488?500.
L. Feng. 2009. Automatic readability assessment for
people with intellectual disabilities. In SIGACCESS
Access. Comput., number 93, pages 84?91. ACM,
New York, NY, USA, jan.
G. Glava?s and S.
?
Stajner. 2013. Event-Centered Sim-
plication of News Stories. In Proceedings of the
Student Workshop held in conjunction with RANLP
2013, Hissar, Bulgaria, pages 71?78.
M. A. Hall and L. A. Smith. 1998. Practical feature
subset selection for machine learning. In C. Mc-
Donald, editor, Computer Science ?98 Proceedings
of the 21st Australasian Computer Science Confer-
ence ACSC?98, pages 181?191. Berlin: Springer.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The weka data min-
ing software: an update. SIGKDD Explor. Newsl.,
11:10?18, November.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the second inter-
national workshop on Paraphrasing - Volume 16,
PARAPHRASE ?03, pages 9?16, Stroudsburg, PA,
USA. Association for Computational Linguistics.
G. H. John and P. Langley. 1995. Estimating Contin-
uous Distributions in Bayesian Classifiers. In Pro-
ceedings of the Eleventh Conference on Uncertainty
in Artificial Intelligence, pages 338?345.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 2001. Improvements to Platt?s
SMO Algorithm for SVM Classifier Design. Neural
Computation, 13(3):637?649.
9
D. Klein and C.D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, volume 1, pages 423?430. Association for
Computational Linguistics.
S. le Cessie and J.C. van Houwelingen. 1992. Ridge
Estimators in Logistic Regression. Applied Statis-
tics, 41(1):191?201.
J. Martos, S. Freire, A. Gonz?alez, D. Gil, and M. Se-
bastian. 2012. D2.1: Functional requirements spec-
ifications and user preference survey. Technical re-
port, FIRST technical report.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
S. E. Petersen and M. Ostendorf. 2007. Text Sim-
plification for Language Learners: A Corpus Anal-
ysis. In Proceedings of Workshop on Speech and
Language Technology for Education.
R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
L. Rello. 2012. Dyswebxia: a model to improve ac-
cessibility of the textual web for dyslexic users. In
SIGACCESS Access. Comput., number 102, pages
41?44. ACM, New York, NY, USA, January.
M. Rios, W. Aziz, and L. Specia. 2011. TINE: A met-
ric to assess MT adequacy. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT-2011), Edinburgh, UK.
M. B. Ruiter, T. C. M. Rietveld, Cucchiarini C., Krah-
mer E. J., and H. Strik. 2010. Human Language
Technology and communicative disabilities: Re-
quirements and possibilities for the future. In Pro-
ceedings of the the seventh international conference
on Language Resources and Evaluation (LREC).
J. Rybing, C. Smithr, and A. Silvervarg. 2010. To-
wards a Rule Based System for Automatic Simpli-
fication of Texts. In The Third Swedish Language
Technology Conference.
H. Saggion, E. G?omez Mart??nez, E. Etayo, A. An-
ula, and L. Bourg. 2011. Text Simplification in
Simplext: Making Text More Accessible. Revista
de la Sociedad Espa?nola para el Procesamiento del
Lenguaje Natural, 47:341?342.
A. Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language & Computa-
tion, 4(1):77?109.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, Adequacy, or HTER? Exploring Dif-
ferent Human Judgments with a Tunable MT Metric.
In Proceedings of the Fourth Workshop on Statisti-
cal Machine Translation at the 12th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2009), Athens, Greece.
L. Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language, pages 30?39, Berlin, Hei-
delberg.
S.
?
Stajner, R. Evans, C. Orasan, and R. Mitkov. 2012.
What Can Readability Measures Really Tell Us
About Text Complexity? In Proceedings of the
LREC?12 Workshop: Natural Language Processing
for Improving Textual Accessibility (NLP4ITA), Is-
tanbul, Turkey.
I. H. Witten and E. Frank. 2005. Data mining: practi-
cal machine learning tools and techniques. Morgan
Kaufmann Publishers.
K. Woodsend and M. Lapata. 2011a. Learning to Sim-
plify Sentences with Quasi-Synchronous Grammar
and Integer Programming. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
K. Woodsend and M. Lapata. 2011b. WikiSimple:
Automatic Simplification of Wikipedia Articles. In
Proceedings of the 25th AAI Coference on Artificial
Intelligence.
S. Wubben, A. van den Bosch, and E. Krahmer. 2012.
Sentence simplification by monolingual machine
translation. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 1015?
1024, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Mono-
lingual Tree-based Translation Model for Sentence
Simplification. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 1353?1361.
10
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 30?37,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Keyword Highlighting Improves Comprehension for People with Dyslexia
Luz Rello
NLP & Web Research Groups
Universitat Pompeu Fabra
Barcelona, Spain
luzrello@acm.org
Horacio Saggion
NLP Research Group
Universitat Pompeu Fabra
Barcelona, Spain
horacio.saggion@upf.edu
Ricardo Baeza-Yates
Yahoo Labs Barcelona &
Web Research Group, UPF
Barcelona, Spain
rbaeza@acm.org
Abstract
The use of certain font types and sizes im-
prove the reading performance of people
with dyslexia. However, the impact of
combining such features with the seman-
tics of the text has not yet been studied. In
this eye-tracking study with 62 people (31
with dyslexia), we explore whether high-
lighting the main ideas of the text in bold-
face has an impact on readability and com-
prehensibility. We found that highlighting
keywords improved the comprehension of
participants with dyslexia. To the best of
our knowledge, this is the first result of this
kind for people with dyslexia.
1 Introduction
Dyslexia is a neurological reading disability which
is characterized by difficulties with accurate
and/or fluent word recognition as well as by poor
spelling and decoding abilities. These difficulties
typically result from a deficit in the phonologi-
cal component of language that is often unrelated
to other cognitive disabilities. Secondary conse-
quences include problems in reading comprehen-
sion and reduced reading experience that can im-
pede vocabulary growth and background knowl-
edge (International Dyslexia Association, 2011).
From 10 to 17.5% of the population in the
U.S.A. (Interagency Commission on Learning
Disabilities, 1987) and from 8.6 to 11% of the
Spanish speaking population (Carrillo et al., 2011;
Jim?enez et al., 2009) have dyslexia. Even if
dyslexia is also popularly identified with brilliant
famous people, the most frequent way to detect
a child with dyslexia is by low-performance in
school (Carrillo et al., 2011). In Spain, it is es-
timated that four out of six cases of school fail-
ure are related to dyslexia.
1
The prevalence of
1
The percentage of school failure is calculated by the
dyslexia and its impact in school failure are the
main motivations of our work.
Previous eye-tracking studies with people with
dyslexia have shown that their reading perfor-
mance can improve when the presentation of the
text contains certain font types (Rello and Baeza-
Yates, 2013) or font sizes (O?Brien et al., 2005;
Dickinson et al., 2002; Rello et al., 2013c).
Keywords ? or key-phrases
2
? are words that
capture the main ideas of a text. Highlighting key-
words in the text is a well known strategy to sup-
port reading tasks (Weinstein and Mayer, 1986).
In fact, highlighting keywords is recommended to
students with dyslexia (Hargreaves, 2007), as well
as to teachers for making texts more accessible for
this target group (Peer and Reid, 2001).
Here, we present the first study which explores
the modification of the text presentation in rela-
tionship with its semantics, by highlighting key-
words. We measure the impact of highlighting the
text on the reading performance (readability and
comprehensibility) of people with dyslexia using
eye-tracking. Our hypotheses are:
? H1: The presence of highlighted keywords in
the text increases readability for people with
dyslexia.
? H2: The presence of highlighted keywords in
the text increases comprehensibility for peo-
ple with dyslexia.
Next section reviews related work, while Sec-
tion 3 explains the experimental methodology.
Section 4 presents the results, which are discussed
in Section 5. In Section 6 we draw the conclusions
and we mention future lines of research.
number or students who drop school before finishing sec-
ondary education (high school). While the average of school
failure in the European Union is around 15%, Spain has
around 25-30% of school failure, 31% in 2010 (Enguita et
al., 2010).
2
We use ?keywords?, meaning also ?key-phrase?, to refer
to both single words or phrases that are highlighted.
30
2 Related Work
Related work to ours can be found in: (1) natural
language processing (NLP) literature about key-
phrase and keyword extraction (Section 2.1), and
(2) accessibility literature about dyslexia and key-
words (Section 2.2).
2.1 Key-phrase and Keyword Extraction
There is a vast amount of NLP literature on key-
phrase extraction (Kim et al., 2010; Witten et al.,
1999; Frank et al., 1999).
The semantic data provided by key-phrase ex-
traction can be used as metadata for refining NLP
applications, such as summarization (D?Avanzo
and Magnini, 2005; Lawrie et al., 2001), text
ranking (Mihalcea and Tarau, 2004), indexing
(Medelyan and Witten, 2006), query expansion
(Song et al., 2006), or document management and
topic search (Gutwin et al., 1999).
The closest work to ours is (Turney, 1999) be-
cause they highlight key-phrases in the text to fa-
cilitate its skimming. They compare the highlight-
ing outputs of two different systems, Search 97
and GenEx, using six corpora belonging to differ-
ent genre.
2.2 Accessibility
In accessibility and education literature, highlight-
ing keywords is a broadly recommended learning
strategy (Weinstein and Mayer, 1986). Regarding
students with dyslexia, teachers are encouraged to
highlight keywords to make texts more accessible
(Peer and Reid, 2001; Hargreaves, 2007). These
recommendations are based on qualitative analy-
sis and direct observations with students.
In the applications for people with dyslexia
highlighting is used not for keywords or main
ideas but to help users for tracking their position
when reading such as in ScreenRuler (ClaroSoft-
ware, 2012). Sometimes highlighting is used
simultaneously with text-to-speech technology
(Kanvinde et al., 2012; ClaroSoftware, 2012). In
the SeeWord tool for MS Word (Dickinson et al.,
2002; Gregor et al., 2003), highlighting is used
on the letters where people with dyslexia normally
make mistakes in order to attract the user?s atten-
tion.
Previous studies similar to ours have used
eye-tracking to show how people with dyslexia
can read significantly faster as using certain font
types (Rello and Baeza-Yates, 2013) or font sizes
(O?Brien et al., 2005; Dickinson et al., 2002; Rello
et al., 2013c).
2.3 What is Missing?
First, we did not find any study that measured ob-
jectively the impact of highlighting keywords in a
text on the readability and comprehensibility for
people with dyslexia. Second, to the best of our
knowledge, there are no studies in assistive tech-
nology that uses an NLP based engine to high-
light keywords for people with dyslexia. In this
work we address the first issue, taking the sec-
ond one into consideration. Hence, we emulated
in the experiment the output that a potential NLP
tool would give for highlighting the main ideas in
the text.
3 Methodology
To study the effect of keywords on readability and
comprehensibility of texts on the screen, we con-
ducted an experiment where 62 participants (31
with dyslexia) had to read two texts on a screen,
where one of them had the main ideas highlighted
using boldface. Readability and comprehensibility
were measured via eye-tracking and comprehen-
sion tests, respectively. The participants? prefer-
ences were gathered via a subjective ratings ques-
tionnaire.
3.1 Design
In the experiment there was one condition, Key-
words, with two levels: [+keywords] denotes the
condition where main ideas of the text were high-
lighted in boldface and [?keywords] denotes the
condition where the presentation of the text was
not modified.
The experiments followed a within-subjects de-
sign, so every participant contributed to each of
the levels of the condition. The order of the condi-
tions was counter-balanced to cancel out sequence
effects.
When measuring the reading performance of
people with dyslexia we need to separate readabil-
ity
3
from comprehensibility
4
because they are not
necessarily related. In the case of dyslexia, texts
that might seen not readable for the general popu-
lation, such as texts with errors, can be better un-
derstood by people with dyslexia, and vice versa,
3
The ease with which a text can be read.
4
The ease with which a text can be understood.
31
people with dyslexia find difficulties with standard
texts (Rello and Baeza-Yates, 2012).
To measure readability we consider two depen-
dent variables derived from the eye-tracking data:
Reading Time and Fixation Duration. To measure
comprehensibility we used a comprehension score
as dependent variable.
? Fixation Duration. When reading a text, the
eye does not move contiguously over the text,
but alternates saccades and visual fixations,
that is, jumps in short steps and rests on parts
of the text. Fixation duration denotes how
long the eye rests on a single place of the
text. Fixation duration has been shown to be
a valid indicator of readability. According to
(Rayner and Duffy, 1986; Hy?on?a and Olson,
1995), shorter fixations are associated with
better readability, while longer fixations can
indicate that the processing load is greater.
On the other hand, it is not directly propor-
tional to reading time as some people may
fixate more often in or near the same piece
of text (re-reading). Hence, we used fixation
duration average as an objective approxima-
tion of readability.
? Reading Time. The total time it takes a par-
ticipant to completely read one text. Shorter
reading durations are preferred to longer
ones, since faster reading is related to more
readable texts (Williams et al., 2003). There-
fore, we use Reading Time, that is, the time
it takes a participant to completely read one
text, as a measure of readability, in addition
to Fixation Duration.
? Comprehension Score. To measure text com-
prehensibility we used inferential items, that
is, questions that require a deep understand-
ing of the content of the text. We used
multiple-choice questions with three possi-
ble choices, one correct, and two wrong. We
compute the text comprehension score as the
number of correct answers divided by the to-
tal number of questions.
? Subjective Ratings. In addition, we asked
the participants to rate on a five-point Likert
scale their personal preferences and percep-
tion about how helpful the highlighted key-
words were.
3.2 Participants
We had 62 native Spanish speakers, 31 with a con-
firmed diagnosis of dyslexia.
5
The ages of the par-
ticipants with dyslexia ranged from 13 to 37, with
a mean age of 21.09 years (s = 8.18). The ages
of the control group ranged from 13 to 40, with a
mean age of 23.03 years (s = 7.10).
Regarding the group with dyslexia, three of
them were also diagnosed with attention deficit
disorder. Fifteen people were studying or already
finished university degrees, fourteen were attend-
ing school or high school, and two had no higher
education. All participants were frequent readers
and the level of education was similar for the con-
trol group.
3.3 Materials
In this section we describe how we designed the
texts and keywords that were used as study mate-
rial, as well as the comprehension and subjective
ratings questionnaires.
Base Texts. We picked two similar texts from
the Spanish corpus Simplext (Bott and Saggion,
2012). To meet the comparability requirements
among the texts belonging to the same experiment,
we adapted the texts maintaining as much as pos-
sible the original text. We matched the readabil-
ity of the texts by making sure that the parameters
commonly used to compute readability (Drndare-
vic and Saggion, 2012), had the same or similar
values. Both texts:
(a) are written in the same genre (news);
(b) are about similar topics (culture);
(c) have the same number of words (158 words):
(d) have a similar word length average (4.83 and
5.61 letters);
(e) are accessible news, readable for the general
public so they contained no rare or technical
words, which present an extra difficulty for
people with dyslexia (Rello et al., 2013a).
(f) have the same number of proper names (one
per text);
5
All of them presented official clinical results to prove
that dyslexia was diagnosed in an authorized center or hos-
pital. The Catalonian protocol of dyslexia diagnosis (Speech
Therapy Association of Catalonia, 2011) does not consider
different kinds of dyslexia.
32
The Museo Picasso M?laga includes new works of the artist in its permanent collection
The Andalusian Minister of Culture, Paulino Plata, presented a new reorganization of the 
permanent collection of the Picasso Museum that, coinciding with the birth anniversary 
of the painter, incorporates a wide selection of works by Pablo Picasso provided by the 
Almine and Bernard Ruiz-Picasso Foundation for Art. Paintings, sculptures and ceramics 
from different periods and styles compose this set of 43 pieces given for 15 years by the 
mentioned foundation. The incorporation of these creations assumes, according to the 
Andalusian Council, a valuable contribution to the permanent collection of the 
Museum Picasso M?laga. In this way, a visitor can now contemplate paintings and 
sculptures that, for the first time, are exposed in the gallery.
Siguiente 
Figure 1: Example slide used in the experiment.
(g) have the same number of sentences (five per
text) and similar sentence complexity (three
sentences per text contain relative clauses);
(h) one text has two numerical expressions
(Rello et al., 2013b) and the other has two
foreign words (Cuetos and Valle, 1988), both
being elements of similar difficulty; and
(i) have the same number of highlighted key-
phrases.
An example of a text used (translation from Span-
ish
6
) is given in Figure 1.
Keywords. For creating the keywords we high-
lighted using boldface the words which contained
the main semantic meaning (focus) of the sen-
tence. This focus normally corresponds with the
direct object and contains the new and most rele-
vant information of the sentence (Sperber and Wil-
son, 1986). We only focused on the main sen-
tences; subordinate or relative clauses were dis-
missed. For the syntactic analysis of the sen-
tences we used Connexor?s Machinese Syntax
(Connexor Oy, 2006), a statistical syntactic parser
that employes a functional dependency grammar
(Tapanainen and J?arvinen, 1997). We took direct
objects parsed by Connexor without correcting the
output.
Comprehension Questionnaires. For each text
we manually create three inferential items. The
order of the correct answer was counterbalanced
and all questions have similar difficulty. An exam-
ple question is given in Figure 2.
Subjective Questionnaire. The participants rated
how much did the keywords helped their reading,
6
www.luzrello.com/picasso
El texto habla de: ?The text is about:?
? Sobre la obra del pintor y escultor Picasso.
?The work of the painter and sculptor Picasso.?
? Sobre la Fundaci?on Almine y Bernard Ruiz-Picasso
para el Arte. ?The Almine and Bernard Ruiz-Picasso
Foundation for Arts.?
? Sobre incorporaci?on de nuevas obras en el museo
Picasso de M?alaga. ?About incorporation of new
works in the Picasso Museum of Malaga.?
Figure 2: Comprehension questionnaire item.
their ease to remember the text, and to which ex-
tent would they like to find keywords in texts.
Text Presentation. The presentation of the text
has an effect on reading speed of people with
dyslexia (Kurniawan and Conroy, 2006; Gregor
and Newell, 2000). Therefore, we used a text
layout that follows the recommendations of pre-
vious research. As font type, we chose Arial,
sans serif, as recommended in (Rello and Baeza-
Yates, 2013). The text was left-justified, as rec-
ommended by the British Association of Dyslexia
(British Dyslexia Association, 2012). Each line
did not exceeded 62 characters/column, the font
size was 20 point, and the colors used were black
font with creme background,
7
as recommended in
(Rello et al., 2012).
3.4 Equipment
The eye-tracker used was the Tobii T50 that
has a 17-inch TFT monitor with a resolution of
1024?768 pixels. It was calibrated for each par-
ticipant and the light focus was always in the
same position. The time measurements of the eye-
tracker have a precision of 0.02 seconds. The dis-
7
The CYMK are creme (FAFAC8) and black (000000).
Color difference: 700. Brightness difference: 244.
33
tance between the participant and the eye-tracker
was constant (approximately 60 cm. or 24 in.) and
controlled by using a fixed chair.
3.5 Procedure
The sessions were conducted at Universitat Pom-
peu Fabra in a quiet room and lasted from 20 to 30
minutes. First, we began with a questionnaire to
collect demographic information. Then, we con-
ducted the experiment using eye-tracking. The
participants were asked to read the texts in silence
and to complete the comprehension tests after each
text read. Finally, we carried out the subjective rat-
ings questionnaire.
4 Results
None of the datasets were normally distributed
(Shapiro-Wilk test) and neither of them had an
homogeneous variance (Levene test). Hence, to
study the effect of Keywords on readability and
comprehensibility we used the Wilcoxon non-
parametric test for repeated measures.
4.1 Differences between Groups
We found a significant difference between
the groups regarding Reading Time (W =
2578.5, p < 0.001), Fixation Duration (W =
2953, p < 0.001) and Comprehension Score
(W = 1544, p = 0.040).
Participants with dyslexia had lower compre-
hension scores and longer reading times and fixa-
tions than participants from the control group (see
Table 1).
4.2 Readability
We did not find a significant effect of Keywords
on Reading Time for the participants with dyslexia
(W = 210, p = 0.688) and for the participants
without dyslexia (W = 702.5, p = 0.351).
Similarly, there were found no significant ef-
fects of Keywords on Fixation Duration for the
participants with dyslexia (W = 259.5, p =
0.688) or without dyslexia (W = 862, p = 0.552).
4.3 Comprehension
For the participants with dyslexia, we found a sig-
nificant effect on the Comprehension Score (W =
178.5, p = 0.022). Text with highlighted key-
words led to significantly higher comprehension
scores in this target group.
For the control group we did not find an ef-
fect on the Comprehension Score (W = 740, p =
0.155).
4.4 Subjective Ratings
The debate of what analyses are admissible for
Likert scales ? parametric or non-parametric tests?
is pretty contentious (Carifio and Perla, 2008). A
Shapiro-Wilk test showed that the datasets were
not normally distributed. Hence, we also used the
Wilcoxon non-parametric test.
? Readability. We found no significant dif-
ferences between the groups regarding how
much highlighting keywords helped them
reading the text (W = 504.5, p = 0.316).
Both groups found that keywords can slightly
help their reading (x? = 3, x? = 3.0, s =
1.155)
8
for the participants with dyslexia,
and (x? = 3, x? = 2.8, s = 0.966) for the
control group.
? Memorability. We found no significant differ-
ences between the groups regarding if high-
lighting keywords help to memorize the text
(W = 484, p = 0.493).
Both agree that keywords help them to re-
member the text moderately (x? = 4, x? =
3.636, s = 1.002) for the participants with
dyslexia and (x? = 4, x? = 3.450, s = 1.085)
for the control group.
? Preferences. Also, no differences between
groups were found regarding their prefer-
ences in finding highlighted keywords in the
texts (W = 463, p = 0.727).
Participants with dyslexia would like to find
texts including highlighted keywords (x? = 4,
x? = 3.636, s = 1.136), as well as in the con-
trol group (x? = 4, x? = 3.600, s = 1.057).
5 Discussion
Regarding the differences between the groups, our
results are consistent with other eye-tracking stud-
ies to diagnose dyslexia that found statistical dif-
ferences (Eden et al., 1994).
8
We use x? for the median, and s for the standard devia-
tion.
34
Dependent Variable [+Keywords] [?Keywords ]
(? ? s) Group with Dyslexia
Reading Time (s) 59.98 ? 25.32 53.71 ? 18.42
Fixation Duration (s) 0.22 ? 0.06 0.23 ? 0.060
Comprehension Score (%) 100 ? 0 77.27 ? 42.89
Control Group
Reading Time (s) 36.31 ? 15.17 33.81 ? 12.82
Fixation Duration (s) 0.18 ? 0.04 0.19 ? 0.04
Comprehension Score (%) 100 ? 0 94.87 ? 22.35
Table 1: Results of the Keywords experiment.
5.1 Hypothesis 1
Shorter reading times and fixation durations are
associated with better readability (Just and Car-
penter, 1980). Since Keywords had no significant
effect on readability, we cannot confirm H.1: The
presence of highlighted keywords in the text in-
creases readability for people with dyslexia.
One possible reason for this is that text presen-
tation might only have an impact on readability
when the whole text is modified, not only portions
of it. Most probably, if one text was presented all
in boldface or italics and the other one in roman,
significant differences could have been found as in
(Rello and Baeza-Yates, 2013) where the effect of
different font styles was evaluated. Another expla-
nation could be that the text might look different to
what the participants were used to see and partic-
ipants might need some time to get used to high-
lighted keywords in the text before testing read-
ability effects.
From the content point of view, the fact that the
readability did not change as expected, since the
content of the text is not modified in any of the
conditions.
5.2 Hypothesis 2
Because participants with dyslexia had a signifi-
cantly increase in text comprehension with texts
having highlighted keywords, our findings support
H.2: The presence of highlighted keywords in the
text increases comprehensibility for people with
dyslexia.
This improvement might be due to the possibil-
ity that keywords might help to remember the text
better. This is consistent with the pedagogic lit-
erature that recommends this strategy for learning
and retaining text content (Weinstein and Mayer,
1986).
5.3 Subjective Perception of Keywords
The fact that using keywords for learning is a
shared strategy for both groups (Weinstein and
Mayer, 1986), may explain that no significant
differences between groups were found regard-
ing their preference and perception of keywords
on readability and memorability. Also, high-
lighted keywords in bold are found in general
school books, not only in materials for people with
dyslexia, so both groups were familiar with the
conditions.
5.4 Limitations
This study has at least two limitations. First, the
study was performed with a manually annotated
dataset. These annotations were based on the out-
put of the Connexor parser. We have not found any
evaluation of Connexor?s accuracy when parsing
syntactic constituents. Nevertheless, it has been
observed that the accuracy for direct objects in
Spanish achieves results that varies from 85.7%
to 93.1%, depending on the test set (Padr?o et al.,
2013). Second, the participants read only two texts
because we did not wanted to fatigue participants
with dyslexia. Now that we have observed that
they could have read more texts, we will carry out
further studies with more texts that will incorpo-
rate automatic keyword extraction.
6 Conclusions and Future Work
Our main conclusion is that highlighted keywords
in the text increases the comprehension by peo-
ple with dyslexia. For the control group no effects
were found. Our results support previous educa-
tional recommendations by adding the analysis of
the impact of highlighting keywords using objec-
tive measures.
35
These results can have impact on systems that
rely on text as the main information medium.
By applying keyword extraction automatically and
highlighting them, digital texts could become eas-
ier to understand by people with dyslexia.
Future work include the integration of auto-
matic keyword extraction and its evaluation using
a larger number of texts. Also, different strategies
to select keywords will be explored and the com-
prehension questionnaires will be enriched com-
bining inferential and literal questions. Future
work also includes testing memorability using ob-
jective measures in addition to the subjective re-
sponses of the participants.
Acknowledgements
We are grateful to the three anonymous review-
ers for their comments and suggestions. We
acknowledge the support from grant TIN2012-
38584-C06-03 (SKATER-UPF-TALN) from Min-
isterio de Econom??a y Competitividad, Secretar??a
de Estado de Investigaci?on, Desarrollo e Inno-
vaci?on, Spain.
References
S. Bott and H. Saggion. 2012. Text simplification tools
for Spanish. In Proc. LREC?12, Istanbul, Turkey,
May. ELRA.
British Dyslexia Association. 2012. Dyslexia style
guide, January. www.bdadyslexia.org.uk/.
J. Carifio and R. Perla. 2008. Resolving the 50-
year debate around using and misusing Likert scales.
Medical education, 42(12):1150?1152.
M. S. Carrillo, J. Alegr??a, P. Miranda, and N. S?anchez
P?erez. 2011. Evaluaci?on de la dislexia en la es-
cuela primaria: Prevalencia en espa?nol (Evaluation
of dyslexia in primary school: The prevalence in
Spanish). Escritos de Psicolog??a (Psychology Writ-
ings), 4(2):35?44.
ClaroSoftware. 2012. Screenruler. www.
clarosoftware.com/index.php?cPath=348.
Connexor Oy, 2006. Machinese language model. Con-
nexor Oy, Helsinki, Finland.
F. Cuetos and F. Valle. 1988. Modelos de lectura y
dislexias (Reading models and dyslexias). Infancia
y Aprendizaje (Infancy and Learning), 44:3?19.
E. D?Avanzo and B.Magnini. 2005. A keyphrase-
based approach to summarization: the lake system
at duc-2005. In Proceedings of DUC.
A. Dickinson, P. Gregor, and A.F. Newell. 2002. On-
going investigation of the ways in which some of
the problems encountered by some dyslexics can be
alleviated using computer techniques. In Proc. AS-
SETS?02, pages 97?103, Edinburgh, Scotland.
B. Drndarevic and H. Saggion. 2012. Towards auto-
matic lexical simplification in Spanish: an empirical
study. In Proc. NAACL HLT?12 Workshop PITR?12,
Montreal, Canada.
G.F. Eden, J.F. Stein, H.M. Wood, and F.B. Wood.
1994. Differences in eye movements and reading
problems in dyslexic and normal children. Vision
Research, 34(10):1345?1358.
M. Fern?andez Enguita, L. Mena Mart??nez, and J. Riv-
iere G?omez. 2010. Fracaso y abandono escolar en
Espa?na (School Failure in Spain). Obra Social, Fun-
daci?on la Caixa.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proc. Sixteenth International Joint
Conference on Artificial Intelligence (IJCAI 1999).
Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA.
P. Gregor and A. F. Newell. 2000. An empirical in-
vestigation of ways in which some of the problems
encountered by some dyslexics may be alleviated
using computer techniques. In Proc. ASSETS?00,
ASSETS 2000, pages 85?91, New York, NY, USA.
ACM Press.
P. Gregor, A. Dickinson, A. Macaffer, and P. An-
dreasen. 2003. Seeword: a personal word
processing environment for dyslexic computer
users. British Journal of Educational Technology,
34(3):341?355.
C. Gutwin, G. Paynter, I. Witten, C. Nevill-Manning,
and E. Frank. 1999. Improving browsing in digital
libraries with keyphrase indexes. Decision Support
Systems, 27(1):81?104.
S. Hargreaves. 2007. Study skills for dyslexic students.
Sage.
J. Hy?on?a and R.K. Olson. 1995. Eye fixation patterns
among dyslexic and normal readers: Effects of word
length and word frequency. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
21(6):1430.
Interagency Commission on Learning Disabilities.
1987. Learning Disabilities: A Report to the U.S.
Congress. Government Printing Office, Washington
DC, U.S.
International Dyslexia Association. 2011.
Definition of dyslexia: interdys.org/
DyslexiaDefinition.htm. Based in the
initial definition of the Research Committee of the
Orton Dyslexia Society, former name of the IDA,
done in 1994.
36
J. E. Jim?enez, R. Guzm?an, C. Rodr??guez, and C. Ar-
tiles. 2009. Prevalencia de las dificultades es-
pec??ficas de aprendizaje: La dislexia en espa?nol (the
prevalence of specific learning difficulties: Dyslexia
in Spanish). Anales de Psicolog??a (Annals of Psy-
chology), 25(1):78?85.
M.A. Just and P.A. Carpenter. 1980. A theory of read-
ing: From eye fixations to comprehension. Psycho-
logical review, 87:329?354.
G. Kanvinde, L. Rello, and R. Baeza-Yates. 2012.
IDEAL: a dyslexic-friendly e-book reader (poster).
In Proc. ASSETS?12, pages 205?206, Boulder, USA,
October.
S.N. Kim, O. Medelyan, M.Y. Kan, and T. Baldwin.
2010. Semeval-2010 task 5: Automatic keyphrase
extraction from scientific articles. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 21?26. Association for Computational
Linguistics.
S. Kurniawan and G. Conroy. 2006. Comparing com-
prehension speed and accuracy of online informa-
tion in students with and without dyslexia. Advances
in Universal Web Design and Evaluation: Research,
Trends and Opportunities, Idea Group Publishing,
Hershey, PA, pages 257?70.
D. Lawrie, W.B. Croft, and A. Rosenberg. 2001. Find-
ing topic words for hierarchical summarization. In
Proceedings of the 24th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 349?357. ACM Press.
O. Medelyan and I.H. Witten. 2006. Thesaurus based
automatic keyphrase indexing. In Proceedings of
the 6th ACM/IEEE-CS joint conference on Digital
libraries, pages 296?297. ACM Press.
Rada M. and P. Tarau. 2004. Textrank: Bringing or-
der into texts. In Proceedings of EMNLP, volume 4.
Barcelona, Spain.
B.A. O?Brien, J.S. Mansfield, and G.E. Legge. 2005.
The effect of print size on reading speed in dyslexia.
Journal of Research in Reading, 28(3):332?349.
M. Padr?o, M. Ballesteros, H. Mart??nez, and B. Bohnet.
2013. Finding dependency parsing limits over a
large spanish corpus. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, Nagoya, Japan, October.
L. Peer and G. Reid. 2001. Dyslexia: Successful inclu-
sion in the secondary school. Routledge.
K. Rayner and S.A. Duffy. 1986. Lexical complex-
ity and fixation times in reading: Effects of word
frequency, verb complexity, and lexical ambiguity.
Memory & Cognition, 14(3):191?201.
L. Rello and R. Baeza-Yates. 2012. Lexical quality as
a proxy for web text understandability (poster). In
Proc. WWW ?12, pages 591?592, Lyon, France.
L. Rello and R. Baeza-Yates. 2013. Good fonts for
dyslexia. In Proc. ASSETS?13, Bellevue, Washing-
ton, USA. ACM Press.
L. Rello, G. Kanvinde, and R. Baeza-Yates. 2012.
Layout guidelines for web text and a web service
to improve accessibility for dyslexics. In Proc. W4A
?12, Lyon, France. ACM Press.
L. Rello, R. Baeza-Yates, L. Dempere, and H. Sag-
gion. 2013a. Frequent words improve readability
and short words improve understandability for peo-
ple with dyslexia. In Proc. INTERACT ?13, Cape
Town, South Africa.
L. Rello, S. Bautista, R. Baeza-Yates, P. Gerv?as,
R. Herv?as, and H. Saggion. 2013b. One half or
50%? An eye-tracking study of number represen-
tation readability. In Proc. INTERACT ?13, Cape
Town, South Africa.
L. Rello, M. Pielot, M. C. Marcos, and R. Carlini.
2013c. Size matters (spacing not): 18 points for a
dyslexic-friendly Wikipedia. In Proc. W4A ?13, Rio
de Janeiro, Brazil.
M. Song, I. Y. Song, R. B. Allen, and Z. Obradovic.
2006. Keyphrase extraction-based query expan-
sion in digital libraries. In Proceedings of the 6th
ACM/IEEE-CS joint conference on Digital libraries,
pages 202?209. ACM Press.
Speech Therapy Association of Catalonia. 2011.
PRODISCAT: Protocol de detecci?o i actuaci?o en la
disl`exia.
`
Ambit Educativo (Protocol for detection
and management of dyslexia. Educational scope.).
Education Department of Catalonia.
D. Sperber and D. Wilson. 1986. Relevance: Commu-
nication and cognition, volume 142. Harvard Uni-
versity Press Cambridge, MA.
P. Tapanainen and T. J?arvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Con-
ference on Applied Natural Language Processing
(ANLP-97), pages 64?71.
P. Turney. 1999. Learning to extract keyphrases from
text. In National Research Council, Institute for In-
formation Technology, Technical Report ERB-1057.
C.E. Weinstein and R.E. Mayer. 1986. The teaching of
learning strategies. Handbook of research on teach-
ing, 3:315?327.
S. Williams, E. Reiter, and L. Osman. 2003. Exper-
iments with discourse-level choices and readability.
In Proc. ENLG ?03), Budapest, Hungary.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
D.G. Nevill-Manning. 1999. Kea: Practical auto-
matic keyphrase extraction. In Proceedings of the
fourth ACM conference on Digital libraries, pages
254?255. ACM Press.
37
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 50?58,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Modelling Sarcasm in Twitter, a Novel Approach
Francesco Barbieri and Horacio Saggion and Francesco Ronzano
Pompeu Fabra University, Barcelona, Spain
<firstName>.<lastName>@upf.edu
Abstract
Automatic detection of figurative language
is a challenging task in computational lin-
guistics. Recognising both literal and fig-
urative meaning is not trivial for a ma-
chine and in some cases it is hard even
for humans. For this reason novel and
accurate systems able to recognise figura-
tive languages are necessary. We present
in this paper a novel computational model
capable to detect sarcasm in the social
network Twitter (a popular microblogging
service which allows users to post short
messages). Our model is easy to imple-
ment and, unlike previous systems, it does
not include patterns of words as features.
Our seven sets of lexical features aim to
detect sarcasm by its inner structure (for
example unexpectedness, intensity of the
terms or imbalance between registers), ab-
stracting from the use of specific terms.
1 Introduction
Sarcasm is a mode of communication where literal
and intended meanings are in opposition. Sarcasm
is often used to express a negative message using
positive words. Automatic detection of sarcasm is
then very important in the sentiment analysis field,
as a sarcastic phrase that includes positive words
conveys a negative message and can be easily mis-
understood by an automatic system.
A number of systems with the objective of de-
tecting sarcasm have been designed in the past
years (Davidov et al., 2010; Gonz?alez-Ib?a?nez et
al., 2011; Riloff et al., 2013). All these computa-
tional models have in common the use of frequent
and typical sarcastic expressions as features. This
is of course a good approach as some words are
used sarcastically more often than others.
Our research seeks to avoid the use of words as
features, for two reasons. Firstly, we want to re-
duce the complexity of the computational model,
decreasing drastically the number of features re-
quired for classification. Secondly, typical sarcas-
tic expressions are often culturally specific (an ex-
pression that is considered sarcastic in British En-
glish is not necessary sarcastic in American En-
glish and vice-versa). For these reasons we have
designed a system that aims to detect sarcasm
without the use of words and patterns of words.
We use simple features such as punctuation (Car-
valho et al., 2009) and more sophisticated features,
that for example detect imbalance between regis-
ters (the use of an ?out of context? word may sug-
gest sarcastic intentions) or the use of very intense
terms.
We study sarcasm detection in the micro-
blogging platform Twitter
1
that allows users to
send and read text messages (shorter than 140
characters) called tweets, which often do not fol-
low the expected rules of the grammar. The dataset
we adopted contains positive examples tagged as
sarcastic by the users (using the hashtag #sarcasm)
and negative examples (tagged with a different
hashtag). This methodology has been previously
used in similar studies (Reyes et al., 2013; Lukin
and Walker, 2013; Liebrecht et al., 2013).
We presented in Barbieri and Saggion (2014) a
model capable of detecting irony, in this paper we
add important features to this model and evaluate
a new corpus to determine if our system is capa-
ble of detecting tweets marked as sarcastic (#sar-
casm). The contributions of this paper are the fol-
lowing:
? Novel set of features to improve the perfor-
mances of our model
? A new set of experiments to test our model?s
ability to detect sarcasm
? A corpus to study sarcasm in twitter
1
https://twitter.com/
50
We will show in the paper that results are posi-
tive and the system recognises sarcasm with good
accuracy in comparison with the state-of-the-art.
The rest of the paper is organised as follows: in
the next Section we describe related work. In
Section 3 we describes the corpus and text pro-
cessing tools used and in Section 4 we present
our approach to tackle the sarcasm detection prob-
lem. Section 5 describes the experiments while
Section 6 interprets the results. Finally, we close
the paper in Section 7 with conclusions and future
work.
2 Related Work
A standard definition for sarcasm seems not to ex-
ist. Sarcasm is often identified as irony or verbal
irony (?). Irony has been defined in several ways
over the years as for example ?saying the opposite
of what you mean? (Quintilien and Butler, 1953),
or by Grice (1975) as a rhetorical figure that vio-
lates the maxim of quality: ?Do not say what you
believe to be false?, or as any form of negation
with no negation markers (Giora, 1995). Other
definitions are the ones of Wilson and Sperber
(2002) who states irony is an echoic utterance that
shows a negative aspect of someone?s else opinion,
and as form of pretence by Utsumi (2000) and by
Veale and Hao (2010a). Veale states that ?ironic
speakers usually craft their utterances in spite of
what has just happened, not because of it. The
pretence alludes to, or echoes, an expectation that
has been violated?.
Irony and sarcasm has been approached as
computation problem recently by Carvalho et al.
(2009) who created an automatic system for de-
tecting irony relying on emoticons and special
punctuation. They focused on detection of ironic
style in newspaper articles. Veale and Hao (2010b)
proposed an algorithm for separating ironic from
non-ironic similes, detecting common terms used
in this ironic comparison. Reyes et al. (2013) and
also Barbieri and Saggion (2014) have recently
proposed two approaches to detect irony in Twit-
ter. There are also some computational model to
detect sarcasm in Twitter. The systems of Gon-
zalez et al. (2011) and Davidov et al. (2010) de-
tect sarcasm with good accuracy in English tweets
(the latter model is also studied in the Amazon
review context). Lukin and Walker (2013) used
bootstrapping to improve the performance of sar-
casm and nastiness classifiers for Online Dialogue,
and Liebrecht et al. (2013) designed a model to de-
tect sarcasm in Duch tweets. Finally Riloff (2013)
built a model to detect sarcasm with a bootstrap-
ping algorithm that automatically learn lists of
positive sentiments phrases and negative situation
phrases from sarcastic tweet, in order to detect the
characteristic of sarcasm of being a contrast be-
tween positive sentiment and negative situation.
One may argue that sarcasm and irony are the
same linguistic phenomena, but in our opinion the
latter is more similar to mocking or making jokes
(sometimes about ourselves) in a sharp and non-
offensive manner. On the other hand, sarcasm is
a meaner form of irony as it tends to be offensive
and directed towards other people (or products like
in Amazon reviews). Textual examples of sarcasm
lack the sharp tone of an aggressive speaker, so
for textual purposes we think irony and sarcasm
should be considered as different phenomena and
studied separately (Reyes et al., 2013).
Some datasets exist for the study of sarcasm and
irony. Filatova (2012) designed a corpus genera-
tion experiment where regular and sarcastic Ama-
zon product reviews were collected. Also Bosco
et. al (2013) collected and annotate a set of ironic
examples (in Italian) for the study of sentiment
analysis and opinion mining.
3 Data and Text Processing
We adopted a corpus of 60,000 tweets equally
divided into six different topics: Sarcasm, Edu-
cation, Humour, Irony, Politics and Newspaper.
The Newspaper set includes 10,000 tweets from
three popular newspapers (New York Times, The
Economist and The Guardian). The rest of the
tweets (50,000) were automatically selected by
looking at Twitter hashtags #education, #humour,
#irony, #politics and #sarcasm) added by users in
order to link their contribution to a particular sub-
ject and community. These hashtags are removed
from the tweets for the experiments. According to
Reyes et al. (2013), these hashtags were selected
for three main reasons: (i) to avoid manual se-
lection of tweets, (ii) to allow irony analysis be-
yond literary uses, and because (iii) irony hash-
tag may ?reflect a tacit belief about what consti-
tutes irony? (and sarcasm in the case of the hash-
tag #sarcasm). Education, Humour and Politics
tweets were prepared by Reyes et al. (2013), we
51
added Irony, Newspaper and Sarcasm tweets
2
. We
obtained these data using the Twitter API.
Examples of tweets tagged with #sarcasm are:
? This script is superb, honestly.
? First run in almost two months. I think I did
really well.
? Jeez I just love when I?m trying to eat lunch
and someone?s blowing smoke in my face.
Yum. I love ingesting cigarette smoke.
Another corpora is employed in our approach to
measure the frequency of word usage. We adopted
the Second Release of the American National Cor-
pus Frequency Data
3
(Ide and Suderman, 2004),
which provides the number of occurrences of a
word in the written and spoken ANC. From now
on, we will mean with ?frequency of a term? the
absolute frequency the term has in the ANC.
Processing microblog text is not easy because
they are noisy, with little context, and often En-
glish grammar rules are violated. For these rea-
sons, in order to process the tweets, we use the
GATE Plugin TwitIE (Bontcheva et al., 2013) as
tokeniser and Part of Speech Tagger. The POS
tagger (adapted version of the Stanford tagger
(Toutanova et al., 2003)) achieves 90.54% token
accuracy, which is a very good results knowing
the difficulty of the task in the microblogging con-
text. This POS tagger is more accurate and reliable
than the method we used in the previous research,
where the POS of a term was defined by the most
commonly used (provided by WordNet). TwitIE
also includes the best Named Entity Recognitions
for Twitter (F1=0.8).
We adopted also Rita WordNet API (Howe,
2009) and Java API for WordNet Searching (Spell,
2009) to perform operations on WordNet synsets.
4 Methodology
We approach the detection of sarcasm as a clas-
sification problem applying supervised machine
learning methods to the Twitter corpus described
in Section 3. When choosing the classifiers we had
avoided those requiring features to be independent
2
To make possible comparisons with our sys-
tem we published the IDs of these tweets at
http://sempub.taln.upf.edu/tw/wassa2014/
3
The American National Corpus (http://www.anc.org/) is,
as we read in the web site, a massive electronic collection of
American English words (15 million)
(e.g. Naive Bayes) as some of our features are not.
Since we approach the problem as a binary deci-
sion we picked a tree-based classifiers: Decision
Tree. We already studied the performance of an-
other classifier (Random Forest) but even if Ran-
dom Forest performed better in cross validation
experiments, Decision Tree resulted better in cross
domain experiments, suggesting that it would be
more reliable in a real situation (where the nega-
tive topics are several). We use the Decision Tree
implementation of the Weka toolkit (Witten and
Frank, 2005).
Our model uses seven groups of features to rep-
resent each tweet. Some of them are designed
to detect imbalance and unexpectedness, others
to detect common patterns in the structure of the
sarcastic tweets (like type of punctuation, length,
emoticons), and some others to recognise senti-
ments and intensity of the terms used. Below is
an overview of the group of features in our model:
? Frequency (gap between rare and common
words)
? Written-Spoken (written-spoken style uses)
? Intensity (intensity of adverbs and adjectives)
? Structure (length, punctuation, emoticons)
? Sentiments (gap between positive and nega-
tive terms)
? Synonyms (common vs. rare synonyms use)
? Ambiguity (measure of possible ambiguities)
To the best of our knowledge Frequency, Written
Spoken, Intensity and Synonyms groups have not
been used before in similar studies. The other
groups have been used already (for example by
Carvalho et al. (2009) or Reyes et al. (2013)) yet
our implementation is different.
In the following sections we quickly describe all
the features we used.
4.1 Frequency
Unexpectedness can be a signal of verbal irony,
Lucariello (1994) claims that irony is strictly con-
nected to surprise, showing that unexpectedness is
the feature most related to situational ironies. In
this first group of features we try to detect it. We
explore the frequency imbalance between words,
i.e. register inconsistencies between terms of the
52
same tweet. The idea is that the use of many words
commonly used in English (i.e. high frequency in
ANC) and only a few terms rarely used in English
(i.e. low frequency in ANC) in the same sentence
creates imbalance that may cause unexpectedness,
since within a single tweet only one kind of regis-
ter is expected.
Three features belong to this group: frequency
mean, rarest word, frequency gap. The first one
is the arithmetic average of all the frequencies of
the words in a tweet, and it is used to detect the
frequency style of a tweet. The second one, rarest
word, is the frequency value of the rarest word,
designed to capture the word that may create im-
balance. The assumption is that very rare words
may be a sign of irony. The third one is the abso-
lute difference between the first two and it is used
to measure the imbalance between them, and cap-
ture a possible intention of surprise.
4.2 Written-Spoken
Twitter is composed of written text, but an infor-
mal spoken English style is often used. We de-
signed this set of features to explore the unexpect-
edness created by using spoken style words in a
mainly written style tweet or vice versa (formal
words usually adopted in written text employed in
a spoken style context). We can analyse this aspect
with ANC written and spoken, as we can see us-
ing this corpora whether a word is more often used
in written or spoken English. There are three fea-
tures in this group: written mean, spoken mean,
written spoken gap. The first and second ones are
the means of the frequency values, respectively, in
written and spoken ANC corpora of all the words
in the tweet. The third one, written spoken gap,
is the absolute value of the difference between the
first two, designed to see if ironic writers use both
styles (creating imbalance) or only one of them. A
low difference between written and spoken styles
means that both styles are used.
4.3 Structure
With this group of features we want to study the
structure of the tweet: if it is long or short (length),
if it contains long or short words (mean of word
length), and also what kind of punctuation is used
(exclamation marks, emoticons, etc.).
The length feature consists of the number of
characters that compose the tweet, n. words is
the number of words, and words length mean is
the mean of the words length. Moreover, we use
the number of verbs, nouns, adjectives and adverbs
as features, naming them n. verbs, n. nouns, n.
adjectives and n. adverbs. With these last four
features we also computed the ratio of each part
of speech to the number of words in the tweet; we
called them verb ratio, noun ratio, adjective ra-
tio, and adverb ratio. All these features have the
purpose of capturing the style of the writer.
The punctuation feature is the sum of the num-
ber of commas, full stops, ellipsis and exclama-
tion that a tweet presents. We also added a feature
called laughing which is the sum of all the inter-
net laughs, denoted with hahah, lol, rofl, and lmao
that we consider as a new form of punctuation: in-
stead of using many exclamation marks internet
users may use the sequence lol (i.e. laughing out
loud) or just type hahaha.
Inspired by Davidov et al. (2010) and Carvalho
(2009) we designed features related to punctua-
tion. These features are: number of commas, full
stops, ellipsis, exclamation and quotation marks
that a tweet contain.
The emoticon feature is the sum of the emoti-
cons :), :D, :( and ;) in a tweet.
The new features we included are http that sim-
ply says if a tweet includes or not an Internet
link, and the entities features provided by TwitIE
(Bontcheva et al., 2013). These features check if a
tweet contains the following entities: n. organisa-
tion, n. location, n. person, n. first person, n. title,
n job title, n. date. These last seven features were
not available in the previous model, and some of
them work very well when distinguishing sarcasm
from newspaper tweets.
4.4 Intensity
In order to produce a sarcastic effect some authors
might use an expression which is antonymic to
what they are trying to describe (saying the op-
posite of what they mean (Quintilien and Butler,
1953)). In the case the word being an adjective
or adverb its intensity (more or less exaggerated)
may well play a role in producing the intended ef-
fect (Riloff et al., 2013). We adopted the intensity
scores of Potts (2011) who uses naturally occur-
ring metadata (star ratings on service and prod-
uct reviews) to construct adjectives and adverbs
scales. An example of adjective scale (and relative
scores in brackets) could be the following: horri-
ble (-1.9)? bad (-1.1)? good (0.2)? nice (0.3)
? great (0.8).
53
With these scores we evaluate four features for
adjective intensity and four for adverb intensity
(implemented in the same way): adj (adv) tot,
adj (adv) mean, adj (adv) max, and adj (adv)
gap. The sum of the AdjScale scores of all the ad-
jectives in the tweet is called adj tot. adj mean is
adj tot divided by the number of adjectives in the
tweet. The maximum AdjScale score within a sin-
gle tweet is adj max. Finally, adj gap is the differ-
ence between adj max and adj mean, designed to
see ?how much? the most intense adjective is out
of context.
4.5 Synonyms
As previously said, sarcasm convey two messages
to the audience at the same time. It follows that the
choice of a term (rather than one of its synonyms)
is very important in order to send the second, not
obvious, message.
For each word of a tweet we get its synonyms
with WordNet (Miller, 1995), then we calculate
their ANC frequencies and sort them into a de-
creasing ranked list (the actual word is part of this
ranking as well). We use these rankings to define
the four features which belong to this group. The
first one is syno lower which is the number of syn-
onyms of the word w
i
with frequency lower than
the frequency of w
i
. It is defined as in Equation 1:
sl
w
i
= |syn
i,k
: f(syn
i,k
) < f(w
i
)| (1)
where syn
i,k
is the synonym of w
i
with rank k,
and f(x) the ANC frequency of x. Then we also
defined syno lower mean as mean of sl
w
i
(i.e. the
arithmetic average of sl
w
i
over all the words of a
tweet).
We also designed two more features: syno
lower gap and syno greater gap, but to define
them we need two more parameters. The first one
is word lowest syno that is the maximum sl
w
i
in a
tweet. It is formally defined as:
wls
t
= max
w
i
{|syn
i,k
: f(syn
i,k
) < f(w
i
)|}
(2)
The second one is word greatest syno defined as:
wgs
t
= max
w
i
{|syn
i,k
: f(syn
i,k
) > f(w
i
)|}
(3)
We are now able to describe syno lower gap
which detects the imbalance that creates a com-
mon synonym in a context of rare synonyms. It is
the difference between word lowest syno and syno
lower mean. Finally, we detect the gap of very
rare synonyms in a context of common ones with
syno greater gap. It is the difference between
word greatest syno and syno greater mean, where
syno greater mean is the following:
sgm
t
=
|syn
i,k
: f(syn
i,k
) > f(w
i
)|
n. words of t
(4)
The arithmetic averages of syno greater gap
and of syno lower gap in the Sarcasm corpus are
higher than in the other topics, suggesting that a
very common (or very rare) synonym is often used
out of context i.e. a very rare synonym when most
of the words are common (have a high rank in our
model) and vice versa.
4.6 Ambiguity
Another interesting aspect of sarcasm is ambi-
guity. We noticed that sarcastic tweets presents
words with more meanings (more WordNet
synsets). Our assumption is that if a word has
many meanings the possibility of ?saying some-
thing else? with this word is higher than in a term
that has only a few meanings, then higher possibil-
ity of sending more then one message (literal and
intended) at the same time.
There are three features that aim to capture
these aspects: synset mean, max synset, and
synset gap. The first one is the mean of the num-
ber of synsets of each word of the tweet, to see if
words with many meanings are often used in the
tweet. The second one is the greatest number of
synsets that a single word has; we consider this
word the one with the highest possibility of being
used ironically (as multiple meanings are available
to say different things). In addition, we calculate
synset gap as the difference between the number
of synsets of this word (max synset) and the av-
erage number of synsets (synset mean), assuming
that if this gap is high the author may have used
that inconsistent word intentionally.
4.7 Sentiments
We also evaluate the sentiment of the sarcas-
tic tweets. The SentiWordNet sentiment lexicon
(Esuli and Sebastiani, 2006) assigns to each synset
of WordNet sentiment scores of positivity and neg-
ativity. We used these scores to examine what kind
of sentiments characterises sarcasm. We explore
ironic sentiments with two different views: the
first one is the simple analysis of sentiments (to
54
Figure 1: Information gain of each feature of the model. Sarcasm is compared to Education, Humor,
Irony, Newspaper and Politics. High values of information gain help to better discriminate sarcastic
from non-sarcastic tweets.
identify the main sentiment of a tweet) and the sec-
ond one concerns sentiment imbalances between
words.
There are six features in the Sentiments group.
The first one is named positive sum and it is the
sum of all the positive scores in a tweet, the sec-
ond one is negative sum, defined as sum of all the
negative scores. The arithmetic average of the pre-
vious ones is another feature, named positive neg-
ative mean, designed to reveal the sentiment that
better describe the whole tweet. Moreover, there
is positive-negative gap that is the difference be-
tween the first two features, as we wanted also to
detect the positive/negative imbalance within the
same tweet.
The imbalance may be created using only one
single very positive (or negative) word in the
tweet, and the previous features will not be able
to detect it, thus we needed to add two more. For
this purpose the model includes positive single
gap defined as the difference between most posi-
tive word and the mean of all the sentiment scores
of all the words of the tweet and negative single
gap defined in the same way, but with the most
negative one.
5 Experiments and Results
In order to evaluate our system we use five
datasets, subsets of the corpus in Section 3: Sar-
casm vs Education, Sarcasm vs Humour, Sarcasm
vs Irony, Sarcasm vs Newspaper and Sarcasm
vs Politics. Each combination is balanced with
10.000 sarcastic and 10.000 of non-sarcastic ex-
amples. We run the following two types of exper-
iments:
1. We run in each datasets a 10-fold cross-
validation classification experiment.
2. We train the classifier on 75% of positive ex-
amples and 75% of negative examples of the
same dataset, then we use as test set the rest
25% positive and 25% negative. We perform
this experiment for the five datasets.
In Figure 1 and Figure 2 we show the values of
information gain of the five combinations of topics
(Sarcasm versus each not-sarcastic topic). Note
that, in the first figure the scale we chose to bet-
ter visualise all the features truncates the scores
of the feature http of Education, Newspaper, and
Politics. These three values are respectively 0.4,
0.7 and 0.4. Table 1 and Table 2 includes Preci-
sion, Recall, and F-Measure results of Experiment
1 and Experiment 2.
6 Discussion
The best results are obtained when our model has
to distinguish Sarcasm from Newspaper tweets.
This was expected as the task was simpler than the
others. In Newspaper tweets nine out of ten times
present an internet link, and this aspect can be used
to well distinguish sarcasm as internet links are not
used often. Moreover the Newspaper tweets use a
formal language easily distinguishable from sar-
casm. In Newspaper tweets there are more nouns
(average ratio of 0.5) than in sarcastic tweets (ratio
55
Figure 2: Information gain of each feature of the model. Sarcasm is compared to Education, Humor,
Irony, Newspaper and Politics. High values of information gain help to better discriminate sarcastic
from non-sarcastic tweets.
Prec. Recall F1
Education .87 .90 .88
Humour .88 .87 .88
Irony .62 .62 .62
Newspaper .98 .96 .97
Politics .90 .90 .90
Table 1: Precision, Recall and F-Measure of each
topic combination for Experiment 1 (10 cross val-
idation). Sarcasm corpus is compared to Educa-
tion, Humour, Irony, Newspaper, and Politics cor-
pora. The classifier used is Decision Tree
0.3), and Newspaper uses less punctuation marks
than sarcasm. Overall Newspaper results are very
good, the F1 is over 0.95.
Education and Politics results are very good as
well, F1 of 0.90 and 0.92. Also in these topics the
internet link is a good feature. Other powerful fea-
tures in these two topics are noun ratio (as News-
paper they present more number of nouns than sar-
casm), question, rarest val. (sarcasm includes
less frequently used words) and syno lower.
Results regarding sarcasm versus Humour are
positive, F-Measure is above 0.87. The most
marked differences between Humour and sar-
casm are the following. Humour includes more
links (http), more question marks are used to
mark jokes like: ?Do you know the difference
between...??, ?What is an elephant doing...??
(question), sarcasm includes rarer terms and more
intense adverbs than Humour (rarest val., adv.
max).
Our model struggles to detect tweets marked as
sarcastic from the ones marked as ironic. Even
if not very powerful, relevant features to detect
sarcasm against irony are two: use of adverbs
(sarcasm uses less but more intense adverbs) and
sentiment scores (as expected sarcastic tweets are
denoted by more positive sentiments than irony).
Poor results in this topic indicate that irony and
sarcasm have similar structures in our model,
and that new features are necessary to distinguish
them.
Prec. Recall F1
Education .87 .88 .87
Humour .87 .86 .86
Irony .60 .61 .60
Newspaper .95 .96 .95
Politics .89 .89 .89
Table 2: Precision, Recall and F-Measure of each
topic combination for Experiment 2 (Test set).
Sarcasm corpus is compared to Education, Hu-
mour, Irony, Newspaper, and Politics corpora.The
classifier used is Decision Tree
The comparison with other similar systems is
not easy. We obtain better results than Reyes et
al. (2013) and than Barbieri and Saggion (2014),
but the positive class in their experiments is irony.
The system of Davidov et al. (2010) to detect sar-
casm seems to be powerful as well, and their re-
sults can compete with ours, but in the mentioned
study there is no negative topic distinction, the not-
sarcastic topic is not a fixed domain (and our con-
56
trolled experiments results show that depending on
the negative example the task can be more or less
difficult).
7 Conclusion and Future Work
In this study we evaluate our system to detect sar-
casm in the social network Twitter. We tackle this
problem as binary classification, where the nega-
tive topics are Education, Humour, Irony, News-
paper and Politics. The originality of our system
is avoiding the use of pattern of words as feature to
detect sarcasm. In spite of the good results, there
is much space for improvement. We can still en-
hance our results by including additional features
such as language models. We will also run new ex-
periments with different negative topics and differ-
ent kind of text, for example on Amazon reviews
as Davidov et al. (2010). Finally, a very interesting
but challenging issue will be distinguishing with
better accuracy sarcasm from irony.
Acknowledgments
We are grateful to two anonymous reviewers for
their comments and suggestions that help improve
our paper. The research described in this paper is
partially funded by fellowship RYC-2009-04291
from Programa Ram?on y Cajal 2009 and project
number TIN2012-38584-C06-03 (SKATER-UPF-
TALN) from Ministerio de Econom??a y Compet-
itividad, Secretar??a de Estado de Investigaci?on,
Desarrollo e Innovaci?on, Spain. We also ac-
knowledge partial support from the EU project
Dr. Inventor (FP7-ICT-2013.8.1 project number
611383).
References
Francesco Barbieri and Horacio Saggion. 2014. Mod-
elling Irony in Twitter. In Proceedings of the Student
Research Workshop at the 14th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 56?64, Gothenburg, Swe-
den, April. Association for Computational Linguis-
tics.
Kalina Bontcheva, Leon Derczynski, Adam Funk,
Mark A. Greenwood, Diana Maynard, and Niraj
Aswani. 2013. TwitIE: An Open-Source Informa-
tion Extraction Pipeline for Microblog Text. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing. Associ-
ation for Computational Linguistics.
Cristina Bosco, Viviana Patti, and Andrea Bolioli.
2013. Developing corpora for sentiment analysis
and opinion mining: the case of irony and senti-tut.
Intelligent Systems, IEEE.
Paula Carvalho, Lu??s Sarmento, M?ario J Silva, and
Eug?enio de Oliveira. 2009. Clues for detect-
ing irony in user-generated contents: oh...!! it?s
so easy;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion, pages 53?56. ACM.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107?116. Association for
Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In Proceedings of Language
Resources and Evaluation Conference, volume 6,
pages 417?422.
Elena Filatova. 2012. Irony and Sarcasm: Corpus
Generation and Analysis Using Crowdsourcing. In
Proceedings of Language Resources and Evaluation
Conference, pages 392?398.
Rachel Giora. 1995. On irony and negation. Discourse
processes, 19(2):239?264.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying Sarcasm in
Twitter: A Closer Look. In ACL (Short Papers),
pages 581?586. Citeseer.
H Paul Grice. 1975. Logic and conversation. 1975,
pages 41?58.
Daniel C Howe. 2009. Rita wordnet. Java based API
to access Wordnet.
Nancy Ide and Keith Suderman. 2004. The Ameri-
can National Corpus First Release. In Proceedings
of the Language Resources and Evaluation Confer-
ence.
Christine Liebrecht, Florian Kunneman, and Antal
van den Bosch. 2013. The perfect solution for
detecting sarcasm in tweets# not. WASSA 2013,
page 29.
Joan Lucariello. 1994. Situational irony: A concept of
events gone awry. Journal of Experimental Psychol-
ogy: General, 123(2):129.
Stephanie Lukin and Marilyn Walker. 2013. Really?
well. apparently bootstrapping improves the perfor-
mance of sarcasm and nastiness classifiers for online
dialogue. NAACL 2013, page 30.
George A Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
57
Christopher Potts. 2011. Developing adjective scales
from user-supplied textual metadata. NSF Work-
shop on Restructuring Adjectives in WordNet. Ar-
lington,VA.
Quintilien and Harold Edgeworth Butler. 1953. The
Institutio Oratoria of Quintilian. With an English
Translation by HE Butler. W. Heinemann.
Antonio Reyes, Paolo Rosso, and Tony Veale. 2013.
A multidimensional approach for detecting irony in
Twitter. Language Resources and Evaluation, pages
1?30.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive senti-
ment and negative situation.
Brett Spell. 2009. Java API for WordNet Searching
(JAWS).
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Akira Utsumi. 2000. Verbal irony as implicit dis-
play of ironic environment: Distinguishing ironic
utterances from nonirony. Journal of Pragmatics,
32(12):1777?1806.
Tony Veale and Yanfen Hao. 2010a. An ironic fist
in a velvet glove: Creative mis-representation in the
construction of ironic similes. Minds and Machines,
20(4):635?650.
Tony Veale and Yanfen Hao. 2010b. Detecting Ironic
Intent in Creative Comparisons. In ECAI, volume
215, pages 765?770.
Deirdre Wilson and Dan Sperber. 2002. Relevance
theory. Handbook of pragmatics.
Ian H Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.
58
