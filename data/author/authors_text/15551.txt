Proceedings of the 6th Workshop on Statistical Machine Translation, pages 92?98,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Approximating a Deep-Syntactic Metric for MT Evaluation and Tuning?
Matous? Macha?c?ek and Ondr?ej Bojar
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{bojar,machacek}@ufal.mff.cuni.cz
Abstract
SemPOS is an automatic metric of machine
translation quality for Czech and English fo-
cused on content words. It correlates well
with human judgments but it is computation-
ally costly and hard to adapt to other lan-
guages because it relies on a deep-syntactic
analysis of the system output and the refer-
ence. To remedy this, we attempt at approxi-
mating SemPOS using only tagger output and
a few heuristics. At a little expense in corre-
lation to human judgments, we can evaluate
MT systems much faster. Additionally, we de-
scribe our submission to the Tunable Metrics
Task in WMT11.
1 Introduction
SemPOS metric for machine translation quality was
introduced by Kos and Bojar (2009). It is inspired
by a set of metrics relying on various linguistic fea-
tures on syntactic and semantic level introduced by
Gime?nez and Ma?rquez (2007). One of their best
performing metrics was Semantic role overlapping:
the candidate and the reference translation are rep-
resented as bags of words and their semantic roles.
The similarity between the candidate and the refer-
ence is calculated using a general similarity measure
called Overlapping. The formal definition may be
found in Section 4.
? This work has been supported by the grants EuroMa-
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of
the Czech Republic), P406/10/P259, P406/11/1499, and MSM
0021620838.
Instead of semantic role labels (not available for
Czech), Kos and Bojar (2009) use TectoMT frame-
work (Z?abokrtsky? et al, 2008) to assign a seman-
tic part of speech defined by Sgall et al (1986). In
addition they use t-lemmas (deep-syntactic lemmas)
instead of surface word forms, which most impor-
tantly means that the metric considers content words
only. In the following, we will use ?sempos? to de-
note the semantic part of speech and ?SemPOS? to
denote the whole metric by Kos and Bojar (2009).
SemPOS correlates well with human judgments
on system level, see Section 2 for a brief summary
of how the correlation is computed. The main draw-
back of SemPOS is its computational cost because
it requires full parsing up to the deep syntactic level
to obtain t-lemmas and semposes. In Section 3 we
propose four methods which approximate t-lemmas
and semposes without the deep syntactic analysis.
These methods require only part-of-speech tagging
and therefore they are not only faster but also eas-
ier to adapt for other languages, not requiring more
advanced linguistic tools.
Gime?nez and Ma?rquez (2007) and Bojar et al
(2010) used different formulas to calculate the final
overlapping.1 In Section 4, we examine variations
of the formula, adding one version of our own.
By combining one of the approximation tech-
niques with one of the overlapping formulas, we ob-
1In fact, Gime?nez and Ma?rquez (2007) released two versions
of the paper. Both of them are nearly identical except for the
formula for overlapping, so we asked the authors which of the
two versions is correct. It turns out that Bojar et al (2010),
unaware of the second version of the paper, used the wrong one
but still obtained good results. We therefore (re-)examine both
versions.
92
Workshop Filename Sentences To English from To Czech from
WMT08 test2008 2000 de, es, fr ?
WMT08 nc-test2008 2028 cs en
WMT08 newstest2008 2051 cs, de, es, fr en
WMT09 newstest2009 2525 cs, de, es, fr en
WMT10 newssyscombtest2010 2034 cs, de, es, fr en
Table 1: Datasets used to evaluate the correlation with human judgments. For example: the testset ?test2008? was
used for translation to English from German, Spanish and French and it was not used for any translation to Czech.
tain a variant of our metric. The performance of the
individual variants is reported in Section 5.
Section 6 is devoted to our submission to the Tun-
able Metrics shared task of the Sixth Workshop on
Statistical Machine Translation (WMT11).
2 Method of Evaluation
Our primary objective is to create a good metric
for automatic MT evaluation and possibly also tun-
ing. We are not interested much in how close is our
proposed approximation to the (automatic or man-
ual) semposes and t-lemmas. Therefore, we evaluate
only how well do our metrics (the pair of a chosen
approximation and a chosen formula for the overlap-
ping) correlate with human judgments.
2.1 Test Data
We use the data collected during three Workshops on
Statistical Machine Translation: WMT08 (Callison-
Burch et al, 2008), WMT09 (Callison-Burch et al,
2009) and WMT10 (Callison-Burch et al, 2010). So
far, we study only Czech and English as the target
languages. Our test sets are summarized in Table 1:
we have four sets with Czech as the target language
and 16 sets with English as the target language.
Each testset in each translation direction gives us
for each sentence one hypothesis for each participat-
ing MT system. Human judges (repeatedly) ranked
subsets of these hypotheses comparing at most 5 hy-
potheses at once and indicating some ordering of
the hypotheses. The ordering may include ties. In
WMT, these 5-fold rankings are interpreted as ?sim-
ulated pairwise comparisons?: all pairwise compar-
isons are extracted from each ranking. The HUMAN
SCORE for each system is then the percentage of
pairs where the system was ranked better or equal
to its competitor.
2.2 Correlation with Human Judgments
For each metric we examine, the correlation to hu-
man judgments is calculated as follows: given one
of the test sets (the hypotheses and reference transla-
tions), the examined metric provides a single-figure
score for each system. We use Spearman?s rank cor-
relation coefficient between the human scores and
the scores of the given metric to see how well the
metric matches human judgments. Because tied
ranks do not exist, the correlation coefficient is given
by:
? = 1?
6
?
i
(pi ? qi)2
n(n2 ? 1)
(1)
Human scores across different test sets are not
comparable, so we compute correlations for each
test set separately and average them.
3 Approximations of SemPOS
We would like to obtain t-lemmas and semantic parts
of speech without deep syntactic analysis, assuming
only automatic tagging and lemmatization.
Except for one option (Section 3.4), we approxi-
mate t-lemmas simply by surface lemmas. For the
majority of content words, this works perfectly, but
there are several regular classes of words where the
t-lemma differs. In such cases, the t-lemma usu-
ally consists of the lemma of the main content word
and an auxiliary word that significantly changes the
meaning of the content word. These are e.g. English
phrasal verbs (?blow up? should have the t-lemma
?blow up?) and Czech reflexive verbs (?sma?t se?).
The approximation of semantic part of speech de-
serves at least some minimal treatment. The follow-
ing sections describe four variations of the approxi-
mation.
93
Morph. Tag Sempos Rel. Freq.
NN n.denot 0.989
VBZ v 0.766
VBN v 0.953
JJ adj.denot 0.975
NNP n.denot 0.999
PRP n.pron.def.pers 0.999
VB v 0.875
VBP v 0.663
VBD v 0.810
WP n.pron.indef 1.000
NNS n.denot 0.996
JJR adj.denot 0.813
Table 2: A sample of the mapping from English morpho-
logical tags to semposes, including the relative frequency,
e.g. count(NN,n.denot)count(NN) .
3.1 Sempos from Tag
We noticed that the morphological tag determines
almost uniquely the semantic part of speech. We use
the Czech-English sentence-parallel corpus CzEng
(Bojar and Z?abokrtsky?, 2009) to create a simple dic-
tionary which maps morphological tags to most fre-
quent semantic parts of speech. Some morpholog-
ical tags belong almost always to auxiliary words
which do not have a corresponding deep-syntactic
node at all, so the t-lemma and sempos are not de-
fined for them. We include these morphological tags
in the dictionary and map them to a special sempos
value ?-?. Ultimately, words with such sempos are
not included in the overlapping at all.
Table 2 shows a sample of this dictionary. The
high relative frequencies indicate that we are not los-
ing too much of the accuracy: overall 93.6 % for
English and 88.4 % for Czech on CzEng e-test.
The first approximation relies just on this
(language-specific) dictionary. The input text is au-
tomatically tagged, the morphological tags are de-
terministically mapped to semposes using the dictio-
nary and words where the mapping led to the special
value of ?-? are removed.
In the following, we label this method as APPROX.
3.2 Exclude Stop-Words
By definition, the deep syntactic layer we use repre-
sents more or less only content words. Most aux-
iliary words become only attributes of the deep-
syntactic nodes and play no role in the overlapping
between the hypothesis and the reference.
Our first approximation technique (Section 3.1)
identifies auxiliary words only on the basis of the
morphological tag. We attempt to refine the re-
call by excluding a certain number of most frequent
words in each language. The frequency list was ob-
tained from the Czech and English sides of the cor-
pus CzEng. We choose the exact cut-off for stop-
words in each language separately: 100 words in
English and 220 words in Czech. See Section 5.1
below.
In the following, the method is called APPROX-
STOPWORDS.
3.3 Restricting the Set of Examined Semposes
We noticed that the contribution of each sempos to
the overall performance of the metric in terms of cor-
relation to human judgments can differ a lot. One
of the underlying reasons may be e.g. greater or
lower tagging accuracy of certain word classes, an-
other reason may be that translation errors in certain
word classes may be more relevant for human judges
of MT quality.
Tables 3 and 4 report the correlation to human
judgments if only words in a given sempos are con-
sidered in the overlapping. Based on these obser-
vations, we assume that some sempos types raise
the correlation of the overlapping with human judg-
ments and some lower it. We therefore try one more
variant of the approximation which considers only
(language-specific) subset of semposes.
The approximation called APPROX-RESTR con-
siders only these sempos tags in Czech: v, n.denot,
adj.denot, n.pron.def.pers, n.pron.def.demon, adv.-
denot.ngrad.nneg, adv.denot.grad.nneg. The consid-
ered sempos tags for English are: v, n.denot, adj.-
denot, n.pron.indef.
3.4 T-lemma and Sempos Tagging
Our last approximation method differs a lot from the
previous three approximations. We use the sequence
labeling algorithm (Collins, 2002) as implemented
in Featurama2 to choose the t-lemma and sempos
tag. The CzEng corpus (Bojar and Z?abokrtsky?,
2009) serves to train two taggers: one for Czech and
2http://sourceforge.net/projects/
featurama/
94
Tag R. Fr. Min. Max. Avg.
v 0.236 0.403 1.000 0.735
n.denot 0.506 0.189 1.000 0.728
adj.denot 0.124 0.264 0.964 0.720
n.pron.indef 0.019 0.224 1.000 0.639
n.quant.def 0.039 -0.084 0.893 0.495
n.pron.def.pers 0.068 -0.500 0.975 0.493
adv.pron.indef 0.005 -0.382 1.000 0.432
adv.denot.grad.neg 0.003 -1.000 0.904 0.413
Table 3: English semposes and their performance in
terms of correlation with human judgments if only words
of the given sempos in APPROX are checked for match
with the reference. Averaged across all testsets. Over-
lapping CAP is used, see Section 4 below. Column R. Fr.
reports relative frequency of each sempos in the testsets.
Tag R. Fr. Min. Max. Avg.
n.pron.def.pers 0.030 0.406 0.800 0.680
n.pron.def.demon 0.026 0.308 1.000 0.651
adj.denot 0.156 0.143 0.874 0.554
adv.denot.ngrad.nneg 0.047 0.291 0.800 0.451
adv.denot.grad.nneg 0.001 0.219 0.632 0.445
adj.quant.def 0.004 -0.029 0.800 0.393
n.denot.neg 0.037 0.029 0.736 0.391
adv.denot.grad.neg 0.018 -0.371 0.800 0.313
n.denot 0.432 -0.200 0.720 0.280
adv.pron.def 0.000 -0.185 0.894 0.262
adj.pron.def.demon 0.000 0.018 0.632 0.241
n.pron.indef 0.027 -0.200 0.423 0.112
adj.quant.grad 0.006 -0.225 0.316 0.079
v 0.180 -0.600 0.706 0.076
adj.quant.indef 0.002 -0.105 0.200 0.052
adv.denot.ngrad.neg 0.000 -0.883 0.775 0.000
n.quant.def 0.000 -0.800 0.713 -0.085
Table 4: Czech semposes. See Table 3 for explanation.
one for English. At each token, each of the taggers
uses the word form, morphological tag and surface
lemma (of the current and the previous two tokens)
to choose one pair of t-lemma and sempos tag from
a given set.
The set of possible t-lemma and sempos pairs is
created as follows. At first the sempos set is ob-
tained. We simply use all semposes being seen with
the given morphological tag in the corpus. Then we
find possible t-lemmas for each sempos. For most
semposes we consider surface lemma as the only
t-lemma. For the sempos tag ?v? we also add t-
lemmas composed of the surface lemma and some
auxiliary word present in the sentence (?blow up?,
?sma?t se?). For some other sempos tags we add spe-
cial t-lemmas for negation and personal pronouns
(?#Neg?, ?#PersPron?).
The overall accuracy of the tagger on the e-test is
97.9 % for English and 94.9 % for Czech, a better re-
sult on a harder task (t-lemmas also predicted) than
the deterministic tagging in Section 3.1.
We call this approximation method TAGGER.
4 Variations of Overlapping
The original Overlapping defined by Gime?nez and
Ma?rquez (2007) is given in Equations 2 and 3:
O(t) =
?
w?ri
cnt(w, t, ci)
?
w?ri?ci
max(cnt(w, t, ri), cnt(w, t, ci))
(2)
where ci and ri denotes the candidate and refer-
ence translation of sentence i and cnt(w, t, s) de-
notes number of times t-lemma w of type (sempos)
t appears in sentence s. For each sempos type t,
Overlapping O(t) calculates the proportion of cor-
rectly translated items of type t. In this paper we
will call this overlapping BOOST.
Equation 3 describes Overlapping of all types:
O(?) =
?
t?T
?
w?ri
cnt(w, t, ci)
?
t?T
?
w?ri?ci
max(cnt(w, t, ri), cnt(w, t, ci))
(3)
where T denotes the set of all sempos types. We
will call this Overlapping BOOST-MICRO because it
micro-averages the overlappings of individual sem-
pos types.
Kos and Bojar (2009) used a slightly different
Overlapping formula, denoted CAP in this paper:
O(t) =
?
w?ri
min(cnt(w, t, ri), cnt(w, t, ci))
?
w?ri
cnt(w, t, ri)
(4)
To calculate Overlapping of all types, Kos and
Bojar (2009) used ordinary macro-averaging. We
call the method CAP-MACRO:
O(?) =
1
|T |
?
t?T
O(t) (5)
The difference between micro- and macro-
average is that in macro-average all types have
95
Reduction Overlapping Min. Max. Avg.
approx cap-micro 0.409 1.000 0.804
orig cap-macro 0.536 1.000 0.801
approx cap-macro 0.420 1.000 0.799
approx-restr cap-macro 0.476 1.000 0.798
tagger cap-micro 0.409 1.000 0.790
orig cap-micro 0.391 1.000 0.784
approx-restr cap-micro 0.391 1.000 0.782
approx-stopwords cap-micro 0.391 1.000 0.754
sempos-bleu 0.374 1.000 0.754
approx-stopwords cap-macro 0.280 1.000 0.724
tagger boost-micro 0.306 1.000 0.717
orig boost-micro 0.324 1.000 0.711
approx-stopwords boost-micro 0.133 1.000 0.697
approx-restr boost-micro 0.126 1.000 0.688
approx boost-micro 0.224 1.000 0.686
tagger cap-macro 0.118 1.000 0.669
bleu -0.143 1.000 0.628
Table 5: Metric correlations for English as a target lan-
guage
the same weight regardless of count. For exam-
ple O(n.denot) and O(adv.denot.grad.nneg) would
have the same weight, however there are many
more items of type n.denot than items of type
adv.denot.grad.nneg (see Tables 3 and 4). We con-
sider this unnatural and we suggest a new Overlap-
ping formula CAP-MICRO:
O(?) =
?
t?T
?
w?ri
min(cnt(w, t, ri), cnt(w, t, ci))
?
t?T
?
w?ri
cnt(w, t, ri)
(6)
In sum, we have three Overlappings which should
be evaluated: BOOST-MICRO (Equation 3), CAP-
MACRO (Equation 5), and CAP-MICRO (Equation 6).
5 Experiments
Table 5 shows the results for English as the target
language. The first two columns denote the combi-
nation of an approximation method and an overlap-
ping formula. For conciseness, we report only the
minimum, maximum and average value among cor-
relations of all test sets.
To compare metrics to original SemPOS, the ta-
ble includes non-approximated variant ORIG where
the t-lemmas and semposes are assigned by the Tec-
toMT framework. For the purposes of compari-
son, we also report the correlations of BLEU (Pa-
pineni et al, 2002) and a linear combination of AP-
Reduction Overlapping Min. Max. Avg.
approx-restr cap-macro 0.400 0.800 0.608
tagger cap-macro 0.143 0.800 0.428
orig cap-macro 0.143 0.800 0.423
approx-restr cap-micro 0.086 0.769 0.413
tagger cap-micro 0.086 0.769 0.413
orig cap-micro 0.086 0.741 0.406
approx-stopwords cap-micro 0.086 0.790 0.368
approx cap-micro 0.086 0.734 0.354
approx-stopwords cap-macro 0.086 0.503 0.347
sempos-bleu 0.086 0.676 0.340
approx cap-macro 0.086 0.469 0.338
tagger boost-micro 0.086 0.664 0.337
bleu 0.029 0.490 0.279
orig boost-micro -0.200 0.692 0.273
approx-stopwords boost-micro -0.200 0.685 0.271
approx boost-micro -0.200 0.664 0.266
approx-restr boost-micro -0.200 0.664 0.266
Table 6: Metric correlations for Czech as a target lan-
guage
PROX+CAP-MICRO and BLEU (even weights) under
the name SEMPOS-BLEU since this metric was used
in Tunable Metric Task (Section 6).
The best performing metric is the combination
of approximation APPROX and overlapping CAP-
MICRO. It actually slightly outperforms all non-
approximated metrics. In general, the reductions
APPROX and ORIG combined with CAP-MICRO
or CAP-MACRO perform very well. Reductions
APPROX-STOPWORDS and APPROX-RESTR do not
improve on APPROX.
The TAGGER approximation correlates similarly
to ORIG when micro-average is used.
Table 6 contains the results for Czech as the target
language. The best performing metric for Czech is
APPROX-RESTR together with CAP-MACRO. In gen-
eral approximation APPROX-RESTR is better than
APPROX-STOPWORDS which is slightly better than
APPROX.
The success of overlapping CAP-MACRO in Czech
is due to the higher contribution of less frequent
semposes to the overall correlation. While in En-
glish the best correlating semposes are also very fre-
quent (Table 3), this does not hold for Czech (Ta-
ble 4). The underlying reasons have yet to be ex-
plained.
In both languages, the overlapping BOOST-
MICRO has a very low correlation. We therefore
consider this overlapping not suitable for any met-
96
 
0.62
 
0.64
 
0.66
 
0.68 0.7
 
0.72
 
0.74
 
0.76
 
0.78 0.8
 
0.82
 
0
 
50
 
100
 
150
 
200
 
250
 
300
cap-
micr
o
cap-
mac
ro
boo
st-m
icro
Figure 1: Correlation vs. the number of most frequent
words which are thrown away for English. The big drop
for lengths 109 and 110 is caused by the words ?who? and
?how?.
ric based on semposes.
On the other hand, most of the examined com-
binations are on average better than the baseline
BLEU, sometimes by a very wide margin.
5.1 Dependency of Correlation on Stopwords
List Length
We tried various stopwords list lengths for the
approximation APPROX-STOPWORDS. Figure 5.1
shows the dependency of the correlation on stop-
words list length for all overlappings in English. We
see that the best correlation arises when no words
are thrown away. One possible explanation is that
auxiliary words are recognized by the morphologi-
cal tag well enough anyway and stopwords lists re-
move also important content words, decreasing the
overall accuracy of the overlapping.
6 Tunable Metric WMT11 Shared Task
The goal of the tunable metric task in WMT11 was
to use the custom metric in MERT optimization
(Och, 2003). The target language was English. We
choose APPROX + CAP-MICRO since this combina-
tion correlates best with human judgments.
Based on the experience of Bojar and Kos (2010),
we combine this metric with BLEU. In our opin-
ion, the SemPOS metric and its variants alone are
are good at comparing systems? outputs where sen-
tence fluency has been already ensured. On the other
hand, they fail in ranking sentences in n-best lists
Weights Devset scores
BLEU APPROX BLEU APPROX
1 0 0.246 0.546
0.75 0.25 0.242 0.584
0.5 0.5 0.229 0.594
0.25 0.75 0.215 0.602
0 1 0.025 0.631
Table 7: Results of MERT optimization. The last two
columns contain metric scores of the last iteration of the
MERT process with given combination weights.
in MERT optimization because they observe only
t-lemmas and don?t penalize wrong morphological
forms of words. We thus use BLEU to establish
sentence fluency and our metrics to prefer sentences
with correctly translated content words.
We have tried several weights for the linear com-
bination of BLEU and the chosen approximation.
See Table 7 for details. We have submitted the vari-
ant with equal weights.
The preliminary results of manual evaluation (see
the WMT11 overview paper) indicate that our sys-
tem is fairly distinct from others: we won under the
?> others? metric but we were the fifth of 8 systems
in the official ?? others? (the percentage of pairs
where the system was ranked better or equal to its
competitor).
7 Conclusions
We have introduced and evaluated several approx-
imations of a deep-syntactic MT evaluation metric
SEMPOS. This allows us to reduce the computa-
tional load by far, use only shallow tagging and still
reach reasonable correlation scores.
For English, our combination of APPROX and
CAP-MICRO performs even marginally better than
the original SEMPOS. For Czech, it is APPROX-
RESTR and TAGGER approximations with CAP-
MACRO that outperform the original SEMPOS.
The applicability of these metrics (in link with
BLEU) in model optimization was confirmed by
the manual judgments for the Tunable Metrics Task.
Our submission was surprisingly different from oth-
ers: the best one in the score excluding ties and
mediocre in the score where ties are rewarded.
97
References
Ondrej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92. in
print.
Ondr?ej Bojar, Kamil Kos, and David Marec?ek. 2010.
Tackling Sparse Data Issue in Machine Translation
Evaluation. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 86?91, Uppsala, Sweden,
July. Association for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogenous
MT Systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 256?
264, Prague, June. Association for Computational Lin-
guistics.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of Ma-
chine Translation Metrics for Czech as the Target Lan-
guage. Prague Bulletin of Mathematical Linguistics,
92.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of the Asso-
ciation for Computational Linguistics, Sapporo, Japan,
July 6-7.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia/Reidel Publishing
Company, Prague, Czech Republic/Dordrecht, Nether-
lands.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly modular MT system with tectogram-
matics used as transfer layer. In ACL 2008 WMT: Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 167?170, Columbus, OH, USA.
Association for Computational Linguistics.
98
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45?51,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Results of the WMT13 Metrics Shared Task
Matous? Macha?c?ek and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
machacekmatous@gmail.com and bojar@ufal.mff.cuni.cz
Abstract
This paper presents the results of the
WMT13 Metrics Shared Task. We asked
participants of this task to score the
outputs of the MT systems involved in
WMT13 Shared Translation Task. We
collected scores of 16 metrics from 8 re-
search groups. In addition to that we com-
puted scores of 5 standard metrics such as
BLEU, WER, PER as baselines. Collected
scores were evaluated in terms of system
level correlation (how well each metric?s
scores correlate with WMT13 official hu-
man scores) and in terms of segment level
correlation (how often a metric agrees with
humans in comparing two translations of a
particular sentence).
1 Introduction
Automatic machine translation metrics play a very
important role in the development of MT systems
and their evaluation. There are many different
metrics of diverse nature and one would like to
assess their quality. For this reason, the Metrics
Shared Task is held annually at the Workshop of
Statistical Machine Translation (Callison-Burch et
al., 2012). This year, the Metrics Task was run
by different organizers but the only visible change
is hopefully that the results of the task are pre-
sented in a separate paper instead of the main
WMT overview paper.
In this task, we asked metrics developers to
score the outputs of WMT13 Shared Translation
Task (Bojar et al, 2013). We have collected the
computed metrics? scores and use them to evalu-
ate quality of the metrics.
The systems? outputs, human judgements and
evaluated metrics are described in Section 2. The
quality of the metrics in terms of system level cor-
relation is reported in Section 3. Segment level
correlation is reported in Section 4.
2 Data
We used the translations of MT systems involved
in WMT13 Shared Translation Task together with
reference translations as the test set for the Metrics
Task. This dataset consists of 135 systems? out-
puts and 6 reference translations in 10 translation
directions (5 into English and 5 out of English).
Each system?s output and the reference translation
contain 3000 sentences. For more details please
see the WMT13 main overview paper (Bojar et al,
2013).
2.1 Manual MT Quality Judgements
During the WMT13 Translation Task a large scale
manual annotation was conducted to compare the
systems. We used these collected human judge-
ments for evaluating the automatic metrics.
The participants in the manual annotation were
asked to evaluate system outputs by ranking trans-
lated sentences relative to each other. For each
source segment that was included in the procedure,
the annotator was shown the outputs of five sys-
tems to which he or she was supposed to assign
ranks. Ties were allowed. Only sentences with 30
or less words were ranked by humans.
These collected rank labels were then used to
assign each system a score that reflects how high
that system was usually ranked by the annotators.
Please see the WMT13 main overview paper for
details on how this score is computed. You can
also find inter- and intra-annotator agreement esti-
mates there.
2.2 Participants of the Shared Task
Table 1 lists the participants of WMT13 Shared
Metrics Task, along with their metrics. We have
collected 16 metrics from a total of 8 research
groups.
In addition to that we have computed the fol-
lowing two groups of standard metrics as base-
lines:
45
Metrics Participant
METEOR Carnegie Mellon University (Denkowski and Lavie, 2011)
LEPOR, NLEPOR University of Macau (Han et al, 2013)
ACTA, ACTA5+6 Idiap Research Institute (Hajlaoui, 2013) (Hajlaoui and Popescu-Belis, 2013)
DEPREF-{ALIGN,EXACT} Dublin City University (Wu et al, 2013)
SIMPBLEU-{RECALL,PREC} University of Shefield (Song et al, 2013)
MEANT, UMEANT Hong Kong University of Science and Technology (Lo and Wu, 2013)
TERRORCAT German Research Center for Artificial Intelligence (Fishel, 2013)
LOGREGFSS, LOGREGNORM DFKI (Avramidis and Popovic?, 2013)
Table 1: Participants of WMT13 Metrics Shared Task
? Moses Scorer. Metrics BLEU (Papineni et
al., 2002), TER (Snover et al, 2006), WER,
PER and CDER (Leusch et al, 2006) were
computed using the Moses scorer which is
used in Moses model optimization. To tok-
enize the sentences we used the standard tok-
enizer script as available in Moses Toolkit. In
this paper we use the suffix *-MOSES to label
these metrics.
? Mteval. Metrics BLEU (Papineni et
al., 2002) and NIST (Doddington,
2002) were computed using the script
mteval-v13a.pl 1 which is used in
OpenMT Evaluation Campaign and includes
its own tokenization. We use *-MTEVAL
suffix to label these metrics. By default,
mteval assumes the text is in ASCII,
causing poor tokenization around curly
quotes. We run mteval in both the
default setting as well as with the flag
--international-tokenization
(marked *-INTL).
We have normalized all metrics? scores such
that better translations get higher scores.
3 System-Level Metric Analysis
We measured the quality of system-level metrics?
scores using the Spearman?s rank correlation coef-
ficient ?. For each direction of translation we con-
verted the official human scores into ranks. For
each metric, we converted the metric?s scores of
systems in a given direction into ranks. Since there
were no ties in the rankings, we used the simplified
formula to compute the Spearman?s ?:
? = 1? 6
?
d2i
n(n2 ? 1) (1)
1http://www.itl.nist.gov/iad/mig/
/tools/
where di is the difference between the human rank
and metric?s rank for system i and n is number of
systems. The possible values of ? range between
1 (where all systems are ranked in the same order)
and -1 (where the systems are ranked in the re-
verse order). A good metric produces rankings of
systems similar to human rankings. Since we have
normalized all metrics such that better translations
get higher score we consider metrics with values
of Spearman?s ? closer to 1 as better.
We also computed empirical confidences of
Spearman?s ? using bootstrap resampling. Since
we did not have direct access to participants? met-
rics (we received only metrics? scores for the com-
plete test sets without the ability to run them on
new sampled test sets), we varied the ?golden
truth? by sampling from human judgments. We
have bootstrapped 1000 new sets and used 95 %
confidence level to compute confidence intervals.
The Spearman?s ? correlation coefficient is
sometimes too harsh: If a metric disagrees with
humans in ranking two systems of a very similar
quality, the ? coefficient penalizes this equally as
if the systems were very distant in their quality.
Aware of how uncertain the golden ranks are in
general, we do not find the method very fair. We
thus also computed three following correlation co-
efficients besides the Spearman?s ?:
? Pearson?s correlation coefficient. This co-
efficient measures the strength of the linear
relationship between metric?s scores and hu-
man scores. In fact, Spearman?s ? is Pear-
son?s correlation coefficient applied to ranks.
? Correlation with systems? clusters. In the
Translation Task (Bojar et al, 2013), the
manual scores are also presented as clus-
ters of systems that can no longer be signifi-
cantly distinguished from one another given
the available judgements. (Please see the
WMT13 Overview paper for more details).
46
We take this cluster information as a ?rank
with ties? for each system and calculate its
Pearson?s correlation coefficient with each
metric?s scores.
? Correlation with systems? fuzzy ranks. For
a given system the fuzzy rank is computed
as an average of ranks of all systems which
are not significantly better or worse than the
given system. The Pearson?s correlation co-
efficient of a metric?s scores and systems?
fuzzy ranks is then computed.
You can find the system-level correlations for
translations into English in Table 2 and for transla-
tions out of English in Table 3. Each row in the ta-
bles contains correlations of a metric in each of the
examined translation directions. The metrics are
sorted by average Spearman?s ? correlation across
translation directions. The best results in each di-
rection are in bold.
As in previous years, a lot of metrics outper-
formed BLEU in system level correlation. The
metric which has on average the strongest corre-
lation in directions into English is METEOR. For
the out of English direction, SIMPBLEU-RECALL
has the highest system-level correlation. TER-
RORCAT achieved even a higher average corre-
lation but it did not participate in all language
pairs. The implementation of BLEU in mteval
is slightly better than the one in Moses scorer
(BLEU-MOSES). This confirms the known truth
that tokenization and other minor implementation
details can considerably influence a metric perfor-
mance.
4 Segment-Level Metric Analysis
We measured the quality of metrics? segment-
level scores using Kendall?s ? rank correlation
coefficient. For this we did not use the official
WMT13 human scores but we worked with raw
human judgements: For each translation direction
we extracted all pairwise comparisons where one
system?s translation of a particular segment was
judged to be (strictly) better than the other sys-
tem?s translation. Formally, this is a list of pairs
(a, b) where a segment translation a was ranked
better than translation b:
Pairs := {(a, b) | r(a) < r(b)} (2)
where r(?) is human rank. For a given metricm(?),
we then counted all concordant pairwise compar-
isons and all discordant pairwise comparisons. A
concordant pair is a pair of two translations of
the same segment in which the comparison of hu-
man ranks agree with the comparison of the met-
ric?s scores. A discordant pair is a pair in which
the comparison of human ranks disagrees with the
metric?s comparison. Note that we totally ignore
pairs where human ranks or metric?s scores are
tied. Formally:
Con := {(a, b) ? Pairs | m(a) > m(b)} (3)
Dis := {(a, b) ? Pairs | m(a) < m(b)} (4)
Finally the Kendall?s ? is computed using the fol-
lowing formula:
? = |Con| ? |Dis||Con|+ |Dis| (5)
The possible values of ? range between -1 (a met-
ric always predicted a different order than humans
did) and 1 (a metric always predicted the same or-
der as humans). Metrics with higher ? are better.
The final Kendall?s ?s are shown in Table 4
for directions into English and in Table 5 for di-
rections out of English. Each row in the tables
contains correlations of a metric in given direc-
tions. The metrics are sorted by average corre-
lation across the translation directions. Metrics
which did not compute scores for systems in all
directions are at the bottom of the tables.
You can see that in both categories, into and out
of English, the strongest correlated segment-level
metric is SIMPBLEU-RECALL.
4.1 Details on Kendall?s ?
The computation of Kendall?s ? has slightly
changed this year. In WMT12 Metrics Task
(Callison-Burch et al, 2012), the concordant pairs
were defined exactly as we do (Equation 3) but the
discordant pairs were defined differently: pairs in
which one system was ranked better by the human
annotator but in which the metric predicted a tie
were considered also as discordant:
Dis := {(a, b) ? Pairs | m(a) ? m(b)} (6)
We feel that for two translations a and b of a seg-
ment, where a is ranked better by humans, a metric
which produces equal scores for both translations
should not be penalized as much as a metric which
47
Co
rre
lat
ion
coe
ffic
ien
t
Sp
ear
ma
n?s
?
Co
rre
lat
ion
Co
effi
cie
nt
Pe
ars
on
?s
Cl
ust
ers
Fu
zzy
Ra
nk
s
Di
rec
tio
ns
fr-
en
de
-en
es-
en
cs-
en
ru
-en
Av
era
ge
Av
era
ge
Av
era
ge
Av
era
ge
Co
nsi
de
red
sys
tem
s
12
22
11
10
17
M
ET
EO
R
.9
84
?
.0
14
.9
61
?
.0
20
.97
9?
.0
24
.9
64
?
.0
27
.7
89
?
.0
40
.93
5?
.0
12
.95
0
.92
4
.93
6
DE
PR
EF
-A
LI
GN
.99
5?
.0
11
.96
6?
.0
18
.9
65
?
.0
31
.9
64
?
.0
23
.7
68
?
.0
41
.9
31
?
.0
12
.92
6
.90
9
.92
4
UM
EA
NT
.9
89
?
.0
11
.9
46
?
.0
18
.9
58
?
.0
28
.97
3?
.0
32
.7
75
?
.0
37
.9
28
?
.0
12
.90
9
.90
3
o.
93
0
M
EA
NT
.9
73
?
.0
14
.9
26
?
.0
21
.9
44
?
.0
38
.97
3?
.0
32
.7
65
?
.0
38
.9
16
?
.0
13
.90
1
.89
1
.91
8
SE
M
PO
S
.9
38
?
.0
14
.9
19
?
.0
28
.9
30
?
.0
31
.9
55
?
.0
18
.82
3?
.0
37
.9
13
?
.0
12
o.
93
4
o.
89
4
.90
1
DE
PR
EF
-EX
AC
T
.9
84
?
.0
11
.9
61
?
.0
17
.9
37
?
.0
38
.9
36
?
.0
27
.7
44
?
.0
46
.9
12
?
.0
15
o.
92
4
o.
89
2
.90
1
SIM
PB
LE
U-
RE
CA
LL
.9
78
?
.0
14
.9
36
?
.0
20
.9
23
?
.0
52
.9
09
?
.0
27
.7
98
?
.0
43
.9
09
?
.0
17
o.
92
3
.87
4
.88
6
BL
EU
-M
TE
VA
L-I
NT
L
.9
89
?
.0
14
.9
02
?
.0
17
.8
95
?
.0
49
.9
36
?
.0
32
.6
95
?
.0
42
.8
83
?
.0
15
.86
6
.84
3
.87
4
BL
EU
-M
TE
VA
L
.9
89
?
.0
14
.8
95
?
.0
20
.8
88
?
.0
45
.9
36
?
.0
32
.6
70
?
.0
41
.8
76
?
.0
15
.85
4
.83
5
.86
5
BL
EU
-M
OS
ES
.9
93
?
.0
14
.9
02
?
.0
17
.8
79
?
.0
51
.9
36
?
.0
36
.6
51
?
.0
41
.8
72
?
.0
16
o.
85
6
.82
6
.86
1
CD
ER
-M
OS
ES
.99
5?
.0
14
.8
77
?
.0
17
.8
88
?
.0
49
.9
27
?
.0
36
.6
59
?
.0
45
.8
69
?
.0
17
o.
87
7
o.
83
1
.85
9
SIM
PB
LE
U-
PR
EC
.9
89
?
.0
08
.8
46
?
.0
20
.8
32
?
.0
59
.9
18
?
.0
23
.7
04
?
.0
42
.8
58
?
.0
17
o.
87
1
.81
5
.84
7
NL
EP
OR
.9
45
?
.0
22
.9
49
?
.0
25
.8
25
?
.0
56
.8
45
?
.0
41
.7
05
?
.0
43
.8
54
?
.0
18
o.
86
7
.80
4
o.
85
3
LE
PO
R
V3
.10
0
.9
45
?
.0
19
.9
34
?
.0
27
.7
48
?
.0
77
.8
00
?
.0
36
.7
79
?
.0
41
.8
41
?
.0
20
o.
86
9
.78
0
o.
85
0
NI
ST
-M
TE
VA
L
.9
51
?
.0
19
.8
75
?
.0
22
.7
69
?
.0
77
.8
91
?
.0
27
.6
49
?
.0
45
.8
27
?
.0
20
.85
2
.77
4
.82
4
NI
ST
-M
TE
VA
L-I
NT
L
.9
51
?
.0
19
.8
75
?
.0
22
.7
62
?
.0
77
.8
82
?
.0
32
.6
58
?
.0
45
.8
26
?
.0
21
o.
85
6
.77
4
o.
82
6
TE
R-
M
OS
ES
.9
51
?
.0
19
.8
33
?
.0
23
.8
25
?
.0
77
.8
00
?
.0
36
.5
81
?
.0
45
.7
98
?
.0
21
.80
3
.73
3
.79
7
W
ER
-M
OS
ES
.9
51
?
.0
19
.6
72
?
.0
26
.7
97
?
.0
70
.7
55
?
.0
41
.5
91
?
.0
42
.7
53
?
.0
20
.78
5
.68
2
.74
9
PE
R-
M
OS
ES
.8
52
?
.0
27
.8
58
?
.0
25
.3
57
?
.0
91
.6
97
?
.0
43
.6
77
?
.0
40
.6
88
?
.0
24
.75
7
.63
7
.70
6
TE
RR
OR
CA
T
.9
84
?
.0
11
.9
61
?
.0
23
.9
72
?
.0
28
n/a
n/a
.97
2?
.0
12
.97
7
.95
8
.95
9
Ta
ble
2:
Sy
ste
m-
lev
el
co
rre
lat
ion
so
fa
uto
ma
tic
eva
lua
tio
nm
etr
ics
an
dt
he
offi
cia
lW
MT
hu
ma
ns
co
res
wh
en
tra
nsl
ati
ng
int
oE
ng
lis
h.
Th
es
ym
bo
l?
o?
ind
ica
tes
wh
ere
the
oth
er
ave
rag
es
are
ou
to
fs
eq
ue
nc
ec
om
pa
red
to
the
ma
in
Sp
ear
ma
n?s
?
ave
rag
e.
48
Co
rre
lat
ion
coe
ffic
ien
t
Sp
ear
ma
n?s
?
Co
rre
lat
ion
Co
effi
cie
nt
Pe
ars
on
?s
Cl
ust
ers
Fu
zzy
Ra
nk
s
Di
rec
tio
ns
en
-fr
en
-de
en
-es
en
-cs
en
-ru
Av
era
ge
Av
era
ge
Av
era
ge
Av
era
ge
Co
nsi
de
red
sys
tem
s
14
14
12
11
12
SIM
PB
LE
U-
RE
CA
LL
.9
24
?
.0
22
.92
5?
.0
20
.8
30
?
.0
47
.8
67
?
.0
31
.7
10
?
.0
53
.85
1?
.0
18
.84
4
.85
6
.84
9
LE
PO
R
V3
.10
0
.9
04
?
.0
34
.9
00
?
.0
27
.8
41
?
.0
49
.7
48
?
.0
56
.85
5?
.0
48
.8
50
?
.0
20
o.8
54
.83
3
.84
4
NI
ST
-M
TE
VA
L-I
NT
L
.92
9?
.0
32
.8
46
?
.0
29
.7
97
?
.0
60
.9
02
?
.0
45
.7
71
?
.0
48
.8
49
?
.0
20
.80
8
o.8
63
o.
84
5
CD
ER
-M
OS
ES
.9
21
?
.0
29
.8
67
?
.0
29
.85
7?
.0
58
.8
88
?
.0
24
.7
01
?
.0
59
.8
47
?
.0
19
.79
6
o.
86
1
.84
3
NL
EP
OR
.9
19
?
.0
28
.9
04
?
.0
27
.8
52
?
.0
49
.8
18
?
.0
45
.7
27
?
.0
64
.8
44
?
.0
21
o.
84
9
o.
84
6
.84
0
NI
ST
-M
TE
VA
L
.9
14
?
.0
34
.8
25
?
.0
30
.7
80
?
.0
66
.9
16
?
.0
31
.7
23
?
.0
48
.8
32
?
.0
21
.79
4
o.
85
1
.82
8
SIM
PB
LE
U-
PR
EC
.9
09
?
.0
26
.8
79
?
.0
25
.7
80
?
.0
71
.8
81
?
.0
35
.6
97
?
.0
51
.8
29
?
.0
20
o.
84
0
o.
85
2
.82
7
M
ET
EO
R
.9
24
?
.0
27
.8
79
?
.0
30
.7
80
?
.0
60
.93
7?
.0
24
.5
69
?
.0
66
.8
18
?
.0
22
o.
80
6
.82
5
.81
4
BL
EU
-M
TE
VA
L-I
NT
L
.9
17
?
.0
33
.8
32
?
.0
30
.7
64
?
.0
71
.8
95
?
.0
28
.6
57
?
.0
62
.8
13
?
.0
22
o.
80
2
.82
1
.80
8
BL
EU
-M
TE
VA
L
.8
95
?
.0
37
.7
86
?
.0
34
.7
64
?
.0
71
.8
95
?
.0
28
.6
31
?
.0
53
.7
94
?
.0
22
o.
79
9
.80
9
.79
0
TE
R-
M
OS
ES
.9
12
?
.0
38
.8
54
?
.0
32
.7
53
?
.0
66
.8
60
?
.0
59
.5
38
?
.0
68
.7
83
?
.0
23
.74
6
.80
6
.77
8
BL
EU
-M
OS
ES
.8
97
?
.0
34
.7
86
?
.0
34
.7
59
?
.0
78
.8
95
?
.0
28
.5
74
?
.0
57
.7
82
?
.0
22
o.
80
2
.79
2
o.
77
9
W
ER
-M
OS
ES
.9
14
?
.0
34
.8
25
?
.0
34
.7
14
?
.0
77
.8
60
?
.0
56
.5
52
?
.0
66
.7
73
?
.0
24
.73
7
o.
79
6
.76
6
PE
R-
M
OS
ES
.8
73
?
.0
40
.6
86
?
.0
45
.7
75
?
.0
47
.7
97
?
.0
49
.5
91
?
.0
62
.7
44
?
.0
24
o.
75
8
.74
7
.73
9
TE
RR
OR
CA
T
.92
9?
.0
22
.94
6?
.0
18
.91
2?
.0
41
n/a
n/a
.92
9?
.0
17
.95
2
.93
3
.92
3
SE
M
PO
S
n/a
n/a
n/a
.6
99
?
.0
45
n/a
.6
99
?
.0
45
.71
7
.61
5
.69
6
AC
TA
5?
6
.8
09
?
.0
46
-.5
26
?
.0
34
n/a
n/a
n/a
.1
41
?
.0
29
.16
6
.19
6
.17
6
AC
TA
.8
09
?
.0
46
-.5
26
?
.0
34
n/a
n/a
n/a
.1
41
?
.0
29
.16
6
.19
6
.17
6
Ta
ble
3:
Sy
ste
m-
lev
el
co
rre
lat
ion
so
fa
uto
ma
tic
eva
lua
tio
nm
etr
ics
an
dt
he
offi
cia
lW
MT
hu
ma
ns
co
res
wh
en
tra
nsl
ati
ng
ou
to
fE
ng
lis
h.
Th
es
ym
bo
l?
o?
ind
ica
tes
wh
ere
the
oth
er
ave
rag
es
are
ou
to
fs
eq
ue
nc
ec
om
pa
red
to
the
ma
in
Sp
ear
ma
n?s
?
ave
rag
e.
49
Directions fr-en de-en es-en cs-en ru-en Average
Extracted pairs 80741 128668 67832 85469 151422
SIMPBLEU-RECALL .193 .318 .279 .260 .234 .257
METEOR .178 .293 .236 .265 .239 .242
DEPREF-ALIGN .161 .267 .234 .228 .200 .218
DEPREF-EXACT .167 .263 .228 .227 .195 .216
SIMPBLEU-PREC .154 .236 .214 .208 .174 .197
NLEPOR .149 .240 .204 .176 .172 .188
SENTBLEU-MOSES .150 .218 .198 .197 .170 .187
LEPOR V3.100 .149 .221 .161 .187 .177 .179
UMEANT .101 .166 .144 .160 .108 .136
MEANT .101 .160 .145 .164 .109 .136
LOGREGFSS-33 n/a .272 n/a n/a n/a .272
LOGREGFSS-24 n/a .270 n/a n/a n/a .270
TERRORCAT .161 .298 .230 n/a n/a .230
Table 4: Segment-level Kendall?s ? correlations of automatic evaluation metrics and the official WMT
human judgements when translating into English.
Directions en-fr en-de en-es en-cs en-ru Average
Extracted pairs 100783 77286 60464 102842 87323
SIMPBLEU-RECALL .158 .085 .231 .065 .126 .133
SIMPBLEU-PREC .138 .065 .187 .055 .095 .108
METEOR .147 .049 .175 .058 .111 .108
SENTBLEU-MOSES .133 .047 .171 .052 .095 .100
LEPOR V3.100 .126 .058 .178 .023 .109 .099
NLEPOR .124 .048 .163 .048 .097 .096
LOGREGNORM-411 n/a n/a .136 n/a n/a .136
TERRORCAT .116 .074 .186 n/a n/a .125
LOGREGNORMSOFT-431 n/a n/a .033 n/a n/a .033
Table 5: Segment-level Kendall?s ? correlations of automatic evaluation metrics and the official WMT
human judgements when translating out of English.
50
strongly disagrees with humans. The method we
used this year does not harm metrics which often
estimate two segments as equally good.
5 Conclusion
We carried out WMT13 Metrics Shared Task in
which we assessed the quality of various au-
tomatic machine translation metrics. We used
the human judgements as collected for WMT13
Translation Task to compute system-level and
segment-level correlations with human scores.
While most of the metrics correlate very well
on the system-level, the segment-level correlations
are still rather poor. It was shown again this year
that a lot of metrics outperform BLEU, hopefully
one of them will attract a wider use at last.
Acknowledgements
This work was supported by the grants
P406/11/1499 of the Grant Agency of the
Czech Republic and FP7-ICT-2011-7-288487
(MosesCore) of the European Union.
References
Eleftherios Avramidis and Maja Popovic?. 2013. Ma-
chine learning methods for comparative and time-
oriented Quality Estimation of Machine Translation
output. In Proceedings of the Eight Workshop on
Statistical Machine Translation.
Ondr?ej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Mark Fishel. 2013. Ranking Translations using Error
Analysis and Quality Estimation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Najeh Hajlaoui and Andrei Popescu-Belis. 2013. As-
sessing the accuracy of discourse connective transla-
tions: Validation of an automatic metric. In 14th In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, page 12. Uni-
versity of the Aegean, Springer, March.
Najeh Hajlaoui. 2013. Are ACT?s scores increasing
with better translation quality. In Proceedings of the
Eight Workshop on Statistical Machine Translation.
Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao,
Yi Lu, Liangye He, Yiming Wang, and Jiaji Zhou.
2013. A Description of Tunable Machine Transla-
tion Evaluation Systems in WMT13 Metrics Task.
In Proceedings of the Eight Workshop on Statistical
Machine Translation.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. Cder: Efficient mt evaluation using block
movements. In In Proceedings of EACL, pages 241?
248.
Chi-Kiu Lo and Dekai Wu. 2013. MEANT @
WMT2013 metrics evaluation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Xingyi Song, Trevor Cohn, and Lucia Specia. 2013.
BLEU deconstructed: Designing a better MT evalu-
ation metric. March.
Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. DCU Par-
ticipation in WMT2013 Metrics Task. In Proceed-
ings of the Eight Workshop on Statistical Machine
Translation.
51
