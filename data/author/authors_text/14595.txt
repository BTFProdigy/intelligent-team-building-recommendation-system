Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352?357,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning to Prune: Context-Sensitive Pruning for Syntactic MT
Wenduan Xu
Computer Laboratory
University of Cambridge
wenduan.xu@cl.cam.ac.uk
Yue Zhang
Singapore University of
Technology and Design
yue zhang@sutd.edu.sg
Philip Williams and Philipp Koehn
School of Informatics
University of Edinburgh
p.j.williams-2@sms.ed.ac.uk
pkoehn@inf.ed.ac.uk
Abstract
We present a context-sensitive chart prun-
ing method for CKY-style MT decoding.
Source phrases that are unlikely to have
aligned target constituents are identified
using sequence labellers learned from the
parallel corpus, and speed-up is obtained
by pruning corresponding chart cells. The
proposed method is easy to implement, or-
thogonal to cube pruning and additive to
its pruning power. On a full-scale English-
to-German experiment with a string-to-
tree model, we obtain a speed-up of more
than 60% over a strong baseline, with no
loss in BLEU.
1 Introduction
Syntactic MT models suffer from decoding effi-
ciency bottlenecks introduced by online n-gram
language model integration and high grammar
complexity. Various efforts have been devoted to
improving decoding efficiency, including hyper-
graph rescoring (Heafield et al, 2013; Huang and
Chiang, 2007), coarse-to-fine processing (Petrov
et al, 2008; Zhang and Gildea, 2008) and gram-
mar transformations (Zhang et al, 2006). For
more expressive, linguistically-motivated syntac-
tic MT models (Galley et al, 2004; Galley et
al., 2006), the grammar complexity has grown
considerably over hierarchical phrase-based mod-
els (Chiang, 2007), and decoding still suffers from
efficiency issues (DeNero et al, 2009).
In this paper, we study a chart pruning method
for CKY-style MT decoding that is orthogonal to
cube pruning (Chiang, 2007) and additive to its
pruning power. The main intuition of our method
is to find those source phrases (i.e. any sequence
of consecutive words) that are unlikely to have any
consistently aligned target counterparts according
to the source context and grammar constraints. We
show that by using highly-efficient sequence la-
belling models learned from the bitext used for
translation model training, such phrases can be ef-
fectively identified prior to MT decoding, and cor-
responding chart cells can be excluded for decod-
ing without affecting translation quality.
We call our method context-sensitive pruning
(CSP); it can be viewed as a bilingual adap-
tation of similar methods in monolingual pars-
ing (Roark and Hollingshead, 2008; Zhang et al,
2010) which improve parsing efficiency by ?clos-
ing? chart cells using binary classifiers. Our con-
tribution is that we demonstrate such methods can
be applied to synchronous-grammar parsing by la-
belling the source-side alone. This is achieved
through a novel training scheme where the la-
belling models are trained over the word-aligned
bitext and gold-standard pruning labels are ob-
tained by projecting target-side constituents to the
source words. To our knowledge, this is the first
work to apply this technique to MT decoding.
The proposed method is easy to implement
and effective in practice. Results on a full-scale
English-to-German experiment show that it gives
more than 60% speed-up over a strong cube prun-
ing baseline, with no loss in BLEU. While we use
a string-to-tree model in this paper, the approach
can be adapted to other syntax-based models.
352
but we need that reform process .
TOP
S-TOP
KON
denn
NP-OA
PDAT
diesen
NN
Reformproze?
VVFIN
brauchen
NP-SB
PPER
wir
PUNC.
.
r1 KON ? ? but, denn ?
r2 NP-SB ? ? we, wir ?
r3 NP-OA ? ? that reform process,
diesen Reformproze? ?
r4 TOP ? ? X1 . , S-TOP1 . ?
r5 S-TOP ? ? but X1 need X2,
denn NP-OA2 brauchen NP-SB1 ?
Figure 1: A selection of grammar rules extractable
from an example word-aligned sentence pair.
2 The Baseline String-to-Tree Model
Our baseline translation model uses the rule ex-
traction algorithm of Chiang (2007) adapted to a
string-to-tree grammar. After extracting phrasal
pairs using the standard approach of Koehn et al
(2003), all pairs whose target phrases are not ex-
haustively dominated by a constituent of the parse
tree are removed and each remaining pair, ?f, e?,
together with its constituent label, C, forms a lex-
ical grammar rule: C ? ?f, e?. The rules r1, r2,
and r3 in Figure 1 are lexical rules. Non-lexical
rules are generated by eliminating one or more
pairs of terminal substrings from an existing rule
and substituting non-terminals. This process pro-
duces the example rules r4 and r5.
Our decoding algorithm is a variant of CKY
and is similar to other algorithms tailored for spe-
cific syntactic translation grammars (DeNero et
al., 2009; Hopkins and Langmead, 2010). By tak-
ing the source-side of each rule, projecting onto it
the non-terminal labels from the target-side, and
weighting the grammar according to the model?s
local scoring features, decoding is a straightfor-
ward extension of monolingual weighted chart
parsing. Non-local features, such as n-gram lan-
guage model scores, are incorporated through
cube pruning (Chiang, 2007).
3 Chart Pruning
3.1 Motivations
The abstract rules and large non-terminal sets of
many syntactic MT grammars cause translation
productstheofvalue
NP-TOP
NP-AG
NN
Produkte
ART
der
NN
Wert
(a) en-de
productstheofvalue
NP
NN
??
NP
DEG
?
NN
??
(b) en-jp
Figure 2: Two example alignments. In (a) ?the
products? does not have a consistent alignment on
the target side, while it does in (b).
overgeneration at the span level and render decod-
ing inefficient. Prior work on monolingual syn-
tactic parsing has demonstrated that by exclud-
ing chart cells that are likely to violate constituent
constraints, decoding efficiency can be improved
with no loss in accuracy (Roark and Hollingshead,
2008). We consider a similar mechanism for syn-
tactic MT decoding by prohibiting subtranslation
generation for chart cells violating synchronous-
grammar constraints.
A motivating example is shown in Figure 2a,
where a segment of an English-German sentence
pair from the training data, along with its word
alignment and target-side parse tree is depicted.
The English phrases ?value of? and ?the products?
do not have corresponding German translations in
this example. Although the grammar may have
rules to translate these two phrases, they can be
safely pruned for this particular sentence pair.
In contrast to chart pruning for monolingual
parsing, our pruning decisions are based on the
source context, its target translation and the map-
ping between the two. This distinction is impor-
tant since the syntactic correspondence between
different language pairs is different. Suppose that
we were to translate the same English sentence
into Japanese (Figure 2a); unlike the English to
German example, the English phrase ?the prod-
ucts? will be a valid phrase that has a Japanese
translation under a target constituent, since it is
syntactically aligned to ???? (Figure 2b).
The key question to consider is how to inject
target syntax and word alignment information into
our labelling models, so that pruning decisions can
be based on the source alone, we address this in the
following two sections.
3.2 Pruning by Labelling
We use binary tags to indicate whether a source
word can start or end a multi-word phrase that has
353
1 	
 0 	
 1 	
 1 	
 1 	

(a) b-tags
1 	
 1 	
 1 	
 0 	
 1 	

(b) e-tags
Figure 3: The pruning effects of two types of bi-
nary tags. The shaded cells are pruned and two
types of tags are assigned independently.
a consistently aligned target constituent. We call
these two types the b-tag and the e-tag, respec-
tively, and use the set of values {0, 1} for both.
Under this scheme, a b-tag value of 1 indi-
cates that a source word can be the start of a
source phrase that has a consistently aligned target
phrase; similarly an e-tag of 0 indicates that a word
cannot end a source phrase. If either the b-tag or
the e-tag of an input phrase is 0, the correspond-
ing chart cells will be pruned. The pruning effects
of the two types of tags are illustrated in Figure 3.
In general, 0-valued b-tags prune a whole column
of chart cells and 0-valued e-tags prune a whole
diagonal of cells; and the chart cells on the first
row and the top-most cell are always kept so that
complete translations can always be found.
We build a separate labeller for each tag type us-
ing gold-standard b- and e-tags, respectively. We
train the labellers with maximum-entropy models
(Curran and Clark, 2003; Ratnaparkhi, 1996), us-
ing features similar to those used for suppertag-
ging for CCG parsing (Clark and Curran, 2004).
In each case, features for a pruning tag consist
of word and POS uni-grams extracted from the 5-
word window with the current word in the middle,
POS trigrams ending with the current word, as well
as two previous tags as a bigram and two separate
uni-grams. Our pruning labellers are highly effi-
cient, run in linear time and add little overhead to
decoding. During testing, in order to prevent over-
pruning, a probability cutoff value ? is used. A tag
value of 0 is assigned to a word only if its marginal
probability is greater than ?.
3.3 Gold-standard Pruning Tags
Gold-standard tags are extracted from the word-
aligned bitext used for translation model train-
ing, respecting rule extraction constraints, which
is crucial for the success of our method.
For each training sentence pair, gold-standard
b-tags and e-tags are assigned separately to the
Algorithm 1 Gold-standard Labelling Algorithm
Input forward alignment Ae?f , backward align-
ment A?f?e and 1-best parse tree ? for f
Output Tag sequences b and e for e
1: procedure TAG(e, f , ?,A, A?)
2: l? |e|
3: for i? 0 to l ? 1 do
4: b[i]? 0, e[i]? 0
5: for f [i?, j?] in ? do
6: s? {A?[k] | k ? [i?, j?]}
7: if |s| ? 1 then continue
8: i? min(s), j ? max(s)
9: if CONSISTENT(i, j, i?, j?) then
10: b[i?]? 1, e[j?]? 1
11: procedure CONSISTENT(i, j, i?, j?)
12: t? {A[k] | k ? [i, j]}
13: return min(t) ? i? and max(t) ? j?
source words. First, we initialize both tags of each
source word to 0s. Then, we iterate through all tar-
get constituent spans, and for each span, we find
its corresponding source phrase, as determined by
the word alignment. If a constituent exists for the
phrase pair, the b-tag of the first word and the e-tag
of the last word in the source phrase are set to 1s,
respectively. Pseudocode is shown in Algorithm 1.
Note that our definition of the gold-standard al-
lows source-side labels to integrate bilingual in-
formation. On line 6, the target-side syntax is
projected to the source; on line 9, consistency is
checked against word alignment.
Consider again the alignment in Figure 2a. Tak-
ing the target constituent span covering ?der Pro-
dukte? as an example, the source phrase under a
consistent word alignment is ?of the products?.
Thus, the b-tag of ?of? and the e-tag of ?prod-
ucts? are set to 1s. After considering all target
constituent spans, the complete b- and e-tag se-
quences for the source-side phrase in Figure 2a
are [1, 1, 0, 0] and [0, 0, 1, 1], respectively. Note
that, since we never prune single-word spans, we
ignore source phrases under consistent one-to-one
or one-to-many alignments.
From the gold standard data, we found 73.69%
of the 54M words do not begin a multi-word
aligned phrase and 77.71% do not end a multi-
word aligned phrase; the 1-best accuracies of the
two labellers tested on a held-out 20K sentences
are 82.50% and 88.78% respectively.
354
 0.146
 0.1465
 0.147
 0.1475
 0.148
 0.1485
 0.149
 0  2  4  6  8  10  12  14  16  18  20
BLE
U
CPU seconds/sentence
csp
cube pruning
(a) time vs. BLEU
 0.146
 0.1465
 0.147
 0.1475
 0.148
 0.1485
 0.149
 0  0.5  1  1.5  2  2.5
BLE
U
Hypothesis Count (x106)
csp
cube pruning
(b) hypo count vs. BLEU
Figure 4: Translation quality comparison with the cube pruning baseline.
4 Experiments
4.1 Setup
A Moses (Koehn et al, 2007) string-to-tree sys-
tem is used as our baseline. The training cor-
pus consists of the English-German sections of
the Europarl (Koehn, 2005) and the News Com-
mentary corpus. Discarding pairs without target-
side parses, the final training data has 2M sen-
tence pairs, with 54M and 52M words on the
English and German sides, respectively. Word-
alignments are obtained by running GIZA++ (Och
and Ney, 2000) in both directions and refined
with ?grow-diag-final-and? (Koehn et al, 2003).
For all experiments, a 5-gram language model
with Kneser-Ney smoothing (Chen and Goodman,
1996) built with the SRILM Toolkit (Stolcke and
others, 2002) is used.
The development and test sets are the 2008
WMT newstest (2,051 sentences) and 2009 WMT
newstest (2,525 sentences) respectively. Feature
weights are tuned with MERT (Och, 2003) on
the development set and output is evaluated us-
ing case-sensitive BLEU (Papineni et al, 2002).
For both rule extraction and decoding, up to seven
terminal/non-terminal symbols on the source-side
are allowed. For decoding, the maximum span-
length is restricted to 15, and the grammar is pre-
filtered to match the entire test set for both the
baseline system and the chart pruning decoder.
We use two labellers to perform b- and e-tag la-
belling independently prior to decoding. Training
of the labelling models is able to complete in un-
der 2.5 hours and the whole test set is labelled in
under 2 seconds. A standard perceptron POS tag-
ger (Collins, 2002) trained on Wall Street Journal
sections 2-21 of the Penn Treebank is used to as-
sign POS tags for both our training and test data.
4.2 Results
Figures 4a and 4b compare CSP with the cube
pruning baseline in terms of BLEU. Decoding
speed is measured by the average decoding time
and average number of hypotheses generated per
sentence. We first run the baseline decoder un-
der various beam settings (b = 100 - 2500) un-
til no further increase in BLEU is observed. We
then run the CSP decoder with a range of ? val-
ues (? = 0.91 ? 0.99), at the default beam size
of 1000 of the baseline decoder. The CSP de-
coder, which considers far fewer chart cells and
generates significantly fewer subtranslations, con-
sistently outperforms the slower baseline. It ulti-
mately achieves a BLEU score of 14.86 at a proba-
bility cutoff value of 0.98, slightly higher than the
highest score of the baseline.
At all levels of comparable translation quality,
our decoder is faster than the baseline. On aver-
age, the speed-up gained is 63.58% as measured
by average decoding time, and comparing on a
point-by-point basis, our decoder always runs over
60% faster. At the ? value of 0.98, it yields a
speed-up of 57.30%, compared with a beam size
of 400 for the baseline, where both achieved the
highest BLEU.
Figures 5a and 5b demonstrate the pruning
power of CSP (? = 0.95) in comparison with the
baseline (beam size = 300); across all the cutoff
values and beam sizes, the CSP decoder considers
54.92% fewer translation hypotheses on average
and the minimal reduction achieved is 46.56%.
Figure 6 shows the percentage of spans of dif-
ferent lengths pruned by CSP (? = 0.98). As ex-
355
 0
 1
 2
 3
 4
 5
 6
 7
 0  20  40  60  80  100  120  140
Hyp
oth
esis
 Co
unt
 (x1
06 )
Sentence Length
csp
cube pruning
(a) sentence length vs. hypo count
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 0  20  40  60  80  100  120  140
Cha
rt C
ell C
oun
t
Sentence Length
csp
cube pruning
(b) sentence length vs. cell count
Figure 5: Search space comparison with the cube pruning baseline.
 0
 10
 20
 30
 40
 50
 60
 0  20  40  60  80  100  120
Pe
rce
nta
ge
 Pr
un
ed
 (%
)
Span Length
Figure 6: Percentage of spans of different lengths pruned at ? = 0.98.
pected, longer spans are pruned more often, as
they are more likely to be at the intersections of
cells pruned by the two types of pruning labels,
thus can be pruned by either type.
We also find CSP does not improve search qual-
ity and it leads to slightly lower model scores,
which shows that some higher scored translation
hypotheses are pruned. This, however, is perfectly
desirable. Since our pruning decisions are based
on independent labellers using contextual infor-
mation, with the objective of eliminating unlikely
subtranslations and rule applications. It may even
offset defects of the translation model (i.e. high-
scored bad translations). The fact that the output
BLEU did not decrease supports this reasoning.
Finally, it is worth noting that our string-to-tree
model does not force complete target parses to be
built during decoding, which is not required in our
pruning method either. We do not use any other
heuristics (other than keeping singleton and the
top-most cells) to make complete translation al-
ways possible. The hypothesis here is that good
labelling models should not affect the derivation
of complete target translations.
5 Conclusion
We presented a novel sequence labelling based,
context-sensitive pruning method for a string-to-
tree MT model. Our method achieves more than
60% speed-up over a state-of-the-art baseline on
a full-scale translation task. In future work, we
plan to adapt our method to models with differ-
ent rule extraction algorithms, such as Hiero and
forest-based translation (Mi and Huang, 2008).
Acknowledgements
We thank the anonymous reviewers for comments.
The first author is fully supported by the Carnegie
Trust and receives additional support from the
Cambridge Trusts. Yue Zhang is supported by
SUTD under the grant SRG ISTD 2012-038.
Philip Williams and Philipp Koehn are supported
under EU-FP7-287658 (EU BRIDGE).
356
References
S.F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proc. ACL, pages 310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
S. Clark and J.R. Curran. 2004. The importance of su-
pertagging for wide-coverage ccg parsing. In Proc.
COLING, page 282.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
J.R. Curran and S. Clark. 2003. Investigating gis and
smoothing for maximum entropy taggers. In Proc.
EACL, pages 91?98.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In Proc. NAACL-HLT, pages 227?235.
M. Galley, M. Hopkins, K. Knight, and D. Marcu.
2004. What?s in a translation rule. In Proc. HLT-
NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
COLING and ACL, pages 961?968.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words to
speed k?best extraction from hypergraphs. In Proc.
NAACL.
Mark Hopkins and Greg Langmead. 2010. SCFG
decoding without binarization. In Proc. EMNLP,
pages 646?655, October.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. ACL, volume 45, page 144.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL-HLT, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL Demo Sessions, pages 177?180.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit, volume 5.
H. Mi and L. Huang. 2008. Forest-based translation
rule extraction. In Proc. EMNLP, pages 206?214.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. ACL, pages 440?447,
Hongkong, China, October.
F.J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. ACL, pages 160?
167.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311?318.
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-
to-fine syntactic machine translation using language
projections. In Proc. ACL, pages 108?116.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP, volume 1,
pages 133?142.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Proc. COLING, pages 745?751.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proc. NAACL, pages 647?655.
A. Stolcke et al 2002. Srilm-an extensible language
modeling toolkit. In Proc. ICSLP, volume 2, pages
901?904.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proc. ACL.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proc. NAACL, pages 256?263.
Y. Zhang, B.G. Ahn, S. Clark, C. Van Wyk, J.R. Cur-
ran, and L. Rimell. 2010. Chart pruning for fast
lexicalised-grammar parsing. In Proc. COLING,
pages 1471?1479.
357
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 115?120,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
More Linguistic Annotation for Statistical Machine Translation
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu Hoang
University of Edinburgh
Edinburgh, United Kingdom
{pkoehn,bhaddow,p.j.williams-2,h.hoang}@inf.ed.ac.uk
Abstract
We report on efforts to build large-scale
translation systems for eight European
language pairs. We achieve most gains
from the use of larger training corpora and
basic modeling, but also show promising
results from integrating more linguistic an-
notation.
1 Introduction
We participated in the shared translation task of
the ACL Workshop for Statistical Machine Trans-
lation 2010 in all language pairs. We continued
our efforts to integrate linguistic annotation into
the translation process, using factored and tree-
based translation models. On average we out-
performed our submission from last year by 2.16
BLEU points on the same newstest2009 test set.
While the submitted system follows the factored
phrase-based approach, we also built hierarchical
and syntax-based models for the English?German
language pair and report on its performance on the
development test sets. All our systems are based
on the Moses toolkit (Koehn et al, 2007).
We achieved gains over the systems from last
year by consistently exploiting all available train-
ing data, using large-scale domain-interpolated,
and consistent use of the factored translation
model to integrate n-gram models over speech
tags. We also experimented with novel domain
adaptation methods, with mixed results.
2 Baseline System
The baseline system uses all available training
data, except for the large UN and 109 corpora, as
well as the optional LDC Gigaword corpus. It uses
a straight-forward setup of the Moses decoder.
Some relevant parameter settings are:
? maximum sentence length 80 words
? tokenization with hyphen splitting
? truecasing
? grow-diag-final-and alignment heuristic
? msd-bidirectional-fe lexicalized reordering
? interpolated 5-gram language model
? tuning on newsdev2009
? testing during development on newstest2009
? MBR decoding
? no reordering over punctuation
? cube pruning
We used most of these setting in our submission
last year (Koehn and Haddow, 2009).
The main difference to our baseline system
from the submission from last year is the use of ad-
ditional training data: larger releases of the News
Commentary, Europarl, Czeng, and monolingual
news corpora. The first two parallel corpora in-
creased roughly 10-20% in size, while the Czeng
parallel corpus and the monolingual news corpora
are five times and twice as big, respectively.
We also handled some of the corpus preparation
steps with more care to avoid some data incon-
sistency problems from last year (affecting mostly
the French language pairs).
An overview of the results is given in Table 1.
The baseline outperforms our submission from
last year by an average of +1.25 points. The gains
for the individual language pairs track the increase
in training data (most significantly for the Czech?
English pairs), and the French?English data pro-
cessing issue.
Note that last year?s submission used special
handling of the German?English language pair,
which we did not replicate in the baseline system,
but report on below.
The table also contains results on the extensions
discussed in the next section.
115
Language Pair ?09 Baseline GT Smooth. UN Data Factored Beam
Spanish-English 24.41 25.25 (+0.76) 25.48 (+0.23) 26.03 (+0.55) 26.20 (+0.17) 26.22 (+0.02)
French-English 23.88 25.23 (+1.35) 25.37 (+0.14) 25.92 (+0.55) 26.13 (+0.21) 26.07 (?0.08)
German-English 18.51 19.47 (+0.96) 19.51 (+0.04) - 21.09 (+0.24) 21.10 (+0.01)
Czech-English 18.49 20.74 (+2.25) 21.19 (+0.45) - 21.33 (+0.14) 21.32 (?0.01)
English-Spanish 23.27 24.20 (+0.93) 24.65 (+0.45) 24.65 (+0.30) 24.37 (?0.28) 24.42 (+0.05)
English-French 22.50 23.83 (+1.33) 23.72 (?0.11) 24.70 (+0.98) 24.74 (+0.04) 24.92 (+0.18)
English-German 14.22 14.68 (+0.46) 14.81 (+0.13) - 15.28 (+0.47) 15.34 (+0.06)
English-Czech 12.64 14.63 (+1.99) 14.68 (+0.05) - - -
avg +1.25 +0.17 +0.60 +0.14 +0.03
Table 1: Overview of results: baseline system and extensions. On average we outperformed our sub-
mission from last year by 1.87 BLEU points on the same newstest2009 test set. For additional gains for
French?English and German?English, please see Tables 7 and 8.
Czech?English
Corpus Num. Tokens Pplx. Weight
EU 29,238,799 582 0.054
Fiction 15,441,105 429 0.028
Navajo 561,144 671 0.002
News (czeng) 2,909,322 288 0.127
News (mono) 1,148,480,525 175 0.599
Subtitles 23,914,244 526 0.019
Techdoc 8,322,958 851 0.099
Web 4,469,177 441 0.073
French?English
Corpus Num. Tokens Pplx. Weight
Europarl 50,132,615 352 0.105
News Com. 2,101,921 311 0.204
UN 216,052,412 383 0.089
News 1,148,480,525 175 0.601
Table 2: English LM interpolation: number of to-
kens, perplexity, and interpolation weight for the
different corpora
2.1 Interpolated Language Model
The WMT training data exhibits an increasing di-
versity of corpora: Europarl, News Commentary,
UN, 109, News ? and seven different sources
within the Czeng corpus.
It is well known that domain adaptation is an
important step in optimizing machine translation
systems. A relatively simple and straight-forward
method is the linear interpolation of the language
model, as we explored previously (Koehn and
Schroeder, 2007; Schwenk and Koehn, 2008).
We trained domain-specific language models
separately and then linearly interpolated them us-
ing SRILM toolkit (Stolke, 2002) with weights op-
Language Pair Cased Uncased
Spanish-English 25.25 26.36 (+1.11)
French-English 25.23 26.29 (+1.06)
German-English 19.47 20.63 (+1.16)
Czech-English 20.74 21.76 (+1.02)
English-Spanish 24.20 25.47 (+1.27)
English-French 23.83 25.02 (+1.19)
English-German 14.68 15.18 (+0.50)
English-Czech 14.63 15.13 (+0.50)
avg +0.98
Table 3: Effect of truecasing: cased and uncased
BLEU scores
timized on the development set newsdev2009.
See Table 2 for numbers on perplexity, corpus
sizes, and interpolation weights. Note, for in-
stance, the relatively high weight for the News
Commentary corpus (0.204) compared to the Eu-
roparl corpus (0.105) in the English language
model for the French-English system, despite the
latter being about 25 times bigger.
2.2 Truecasing
As last year, we deal with uppercase and lowercase
forms of the same words by truecasing the corpus.
This means that we change each surface word oc-
currence of a word to its natural case, e.g., the, Eu-
rope. During truecasing, we change the first word
of a sentence to its most frequent casing. During
de-truecasing, we uppercase the first letter of the
first word of a sentence.
See Table 3 for the performance of this method.
In this table, we compare the cased and uncased
BLEU scores, and observe that we lose on average
roughly one BLEU point due to wrong casing.
116
Count Count of Count Discount Count*
1 357,929,182 0.140 0.140
2 24,966,751 0.487 0.975
3 8,112,930 0.671 2.014
4 4,084,365 0.714 2.858
5 2,334,274 0.817 4.088
Table 4: Good Turing smoothing, as in the
French?English model: counts, counts of counts,
discounting factor and discounted count
3 Extensions
In this section, we describe extensions over the
baseline system. On average, these give us im-
provements of about 1 BLEU point over the base-
line.
3.1 Good Turing Smoothing
Traditionally, we use raw counts to estimate con-
ditional probabilities for phrase translation. How-
ever, this method gives dubious results for rare
counts. The most blatant case is the single oc-
currence of a foreign phrase, whose sole English
translation will receive the translation probability
1
1 = 1.
Foster et al (2006) applied ideas from language
model smoothing to the translation model. Good
Turing smoothing (Good, 1953) uses counts of
counts statistics to assess how likely we will see
a word (or, in our case, a phrase) again, if we have
seen it n times in the training corpus. Instead of
using the raw counts, adapted (lower) counts are
used in the estimation of the conditional probabil-
ity distribution.
The count of counts are collected for the phrase
pairs. See Table 4 for details on how this ef-
fects the French?English model. For instance,
we find singleton 357,929,182 phrase pairs and
24,966,751 phrase pairs that occur twice. The
Good Turing formula tells us to adapt singleton
counts to 24,966,751357,929,182 = 0.14. This means for our
degenerate example of a single occurrence of a
single French phrase that its single English transla-
tion has probability 0.141 = 0.14 (we do not adjust
the denominator).
Good Turing smoothing of the translation table
gives us a gain of +0.17 BLEU points on average,
and improvements for 7 out of 8 language pairs.
For details refer back to Table 1.
Model BLEU
Baseline 14.81
Part-of-Speech 15.03 (+0.22)
Morphogical 15.28 (+0.47)
Table 5: English?German: use of morphological
and part-of-speech n-gram models
3.2 UN Data
While we already used the UN data in the lan-
guage model for the Spanish?English and French?
English language pairs, we now also add it to the
translation model.
The corpus is very large, four times bigger than
the already used training data, but relatively out
of domain, as indicated by the high perplexity and
low interpolation weight during language model
interpolation (recall Table 2).
Adding the corpus to the four systems gives im-
provements of +0.60 BLEU points on average.
For details refer back to Table 1.
3.3 POS n-gram Model
The factored model approach (Koehn and Hoang,
2007) allows us to integrate 7-gram models over
part-of-speech tags. The part-of-speech tags are
produced during decoding by the phrase mapping
of surface words on the source side to a factored
representation of surface words and their part-of-
speech tags on the target side in one translation
step.
We previously used this additional scoring com-
ponent for the German?English language pairs
with success. Thus we now applied to it all other
language pairs (except for English?Czech due to
the lack of a Czech part-of-speech tagger).
We used the following part-of-speech taggers:
? English: mxpost1
? German: LoPar2
? French: TreeTagger3
? Spanish: TreeTagger
For English?German, we also used morpholog-
ical tags, which give better performance than just
basic part-of-speech tags (+0.46 vs. +0.22, see Ta-
ble 5). We observe gains for all language pairs
except for English?Spanish, possibly due to the
1www.inf.ed.ac.uk/resources/nlp/local doc/MXPOST.html
2www.ims.uni-stuttgart.de/projekte/gramotron/SOFTWARE/
LoPar.html
3www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
117
Model BLEU
Baseline 14.81
Part-of-Speech 15.03 (+0.22)
Morphogical 15.28 (+0.47)
Table 6: English?German: use of morphological
and part-of-speech n-gram models
Language Pair Baseline with 109
French?English 25.92 27.15 (+1.23)
English?French 24.70 24.80 (+0.10)
Table 7: Use of large French?English corpus
faulty use of the Spanish part-of-speech tagger.
We gain +0.14 BLEU points on average (includ-
ing the ?0.28 drop for Spanish). For details refer
back to Table 1.
3.4 Bigger Beam Sizes
As a final general improvement, we adjusted the
beam settings during decoding. We increased the
pop-limit from 5,000 to 20,000 and the translation
table limit from the default 20 to 50.
The decoder is quite fast, partly due to multi-
threaded decoding using 4 cores machines (Had-
dow, 2010). Increasing the beam sizes slowed
down decoding speed from about 2 seconds per
sentence to about 8 sec/sentence.
However, this resulted only in minimal gains,
on average +0.03 BLEU. For details refer back to
Table 1.
3.5 109 Corpus
Last year, due to time constraints, we were not
able to use the billion word 109 corpus for the
French?English language pairs. This is largest
publicly available parallel corpus, and it does
strain computing resources, for instance forcing
us to use multi-threaded GIZA++ (Gao and Vogel,
2008).
Table 7 shows the gains obtained from us-
ing this corpus in both the translation model and
the language model opposed to a baseline sys-
tem trained with otherwise the same settings. For
French?English we see large gains (+1.23), but not
for English?French (+0.10).
Our official submission for the French?English
language pairs used these models. They did not in-
clude a part-of-speech language model and bigger
beam sizes.
Model BLEU
Baseline 19.51
+ compound splitting 20.09 (+0.58)
+ pre-reordering 20.03 (+0.52)
+ both 20.85 (+1.34)
Table 8: Special handling of German?English
Language Pair Baseline Weighted TM
Spanish-English 26.20 26.15 (?0.05)
French-English 26.11 26.30 (+0.19)
German-English 21.09 20.81 (?0.28)
Czech-English 21.33 21.21 (?0.12)
English-German 15.28 15.01 (?0.27)
avg. ?0.11
Table 9: Interpolating the translation model with
language model weights
3.6 German?English
For the German?English language direction, we
used two additional processing steps that have
shown to be successful in the past, and again re-
sulted in significant gains.
We split large words based on word frequen-
cies to tackle the problem of word compounds in
German (Koehn and Knight, 2003). Secondly, we
re-order the German input to the decoder (and the
German side of the training data) to align more
closely to the English target language (Collins
et al, 2005).
The two methods improve +0.58 and +0.52 over
the baseline individually, and +1.34 when com-
bined. See also Table 8.
3.7 Translation Model Interpolation
Finally, we explored a novel domain adaption
method for the translation model. Since the in-
terpolation of language models is very success-
ful, we want to interpolate translation models sim-
ilarly. Given interpolation weights, the resulting
translation table is a weighted linear interpolation
of the individual translation models trained sepa-
rately for each domain.
However, while for language models we have a
effective method to find the interpolation weights
(optimizing perplexity on a development set), we
do not have such a method for the translation
model. Thus, we simply recycle the weights we
obtained from language model interpolation (ex-
cluding the weighting for monolingual corpora).
118
Model BLEU
phrase-based 14.81
factored phrase-based 15.28
hierarchical 14.86
target syntax 14.66
Table 10: Tree-based models for English?German
Over the Spanish?English baseline system, we
obtained gains of +0.39 BLEU points. Unfortu-
nately, we did not see comparable gains on the sys-
tems optimized by the preceding steps. In fact, in
4 out of 5 language pairs, we observed lower BLEU
scores. See Table 9 for details.
We did not use this method in our submission.
4 Tree-Based Models
A major extension of the capabilities of the Moses
system is the accommodation of tree-based mod-
els (Hoang et al, 2009). While we have not yet
carried out sufficient experimentation and opti-
mization of the implementation, we took the occa-
sion of the shared translation task as a opportunity
to build large-scale systems using such models.
We build two translation systems: One using
tree-based models without additional linguistic an-
notation, which are known as hierarchical phrase-
based models (Chiang, 2005), and another sys-
tem that uses linguistic annotation on the target
side, which are known under many names such as
string-to-tree models or syntactified target models
(Marcu et al, 2006).
Both models are trained using a very similar
pipeline as for the phrase model. The main dif-
ference is that the translation rules do not have to
be contiguous phrases, but may contain gaps with
are labeled and co-ordinated by non-terminal sym-
bols. Decoding with such models requires a very
different algorithm, which is related to syntactic
chart parsing.
In the target syntax model, the target gaps and
the entire target phrase must map to constituents
in the parse tree. This restriction may be relaxed
by adding constituent labels such as DET+ADJ or
NP\DET to group neighboring constituents or indi-
cate constituents that lack an initial child, respec-
tively (Zollmann and Venugopal, 2006).
We applied these models to the English?
German language direction, which is of particu-
lar interest to us due to the rich target side mor-
phology and large degree of reordering, resulting
in relatively poor performance. See Table 10 for
experimental results with the two traditional mod-
els (phrase-based model and a factored model that
includes a 7-gram morphological tag model) and
the two newer models (hierarchical and target syn-
tax). The performance of the phrase-based, hierar-
chical, and target syntax model are close in terms
of BLEU.
5 Conclusions
We obtained substantial gains over our systems
from last year for all language pairs. To a large
part, these gains are due to additional training data
and our ability to exploit them.
We also saw gains from adding linguistic an-
notation (in form of 7-gram models over part-of-
speech tags) and promising results for tree-based
models. At this point, we are quite satisfied be-
ing able to build competitive systems with these
new models, which opens up major new research
directions.
Everything we described here is part of the open
source Moses toolkit. Thus, all our experiments
should be replicable with publicly available re-
sources.
Acknowledgement
This work was supported by the EuroMatrixPlus
project funded by the European Commission (7th
Framework Programme).
References
Chiang, D. (2005). A hierarchical phrase-based
model for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL?05), pages 263?270, Ann Arbor, Michi-
gan. Association for Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
Foster, G., Kuhn, R., and Johnson, H. (2006).
Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 53?61, Sydney, Aus-
119
tralia. Association for Computational Linguis-
tics.
Gao, Q. and Vogel, S. (2008). Parallel implemen-
tations of word alignment tool. In ACL Work-
shop on Software Engineering, Testing, and
Quality Assurance for Natural Language Pro-
cessing, pages 49?57.
Good, I. J. (1953). The population frequency of
species and the estimation of population param-
eters. Biometrika, 40:237?264.
Haddow, B. (2010). Adding multi-threaded de-
coding to moses. The Prague Bulletin of Math-
ematical Linguistics, (93):57?66.
Hoang, H., Koehn, P., and Lopez, A. (2009). A
unified framework for phrase-based, hierarchi-
cal, and syntax-based statistical machine trans-
lation. In Proceedings of IWSLT.
Koehn, P. and Haddow, B. (2009). Edinburgh?s
submission to all tracks of the WMT2009
shared task with reordering and speed improve-
ments to Moses. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 160?164, Athens, Greece. Association
for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored trans-
lation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868?876.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B., Shen,
W., Moran, C., Zens, R., Dyer, C. J., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Koehn, P. and Knight, K. (2003). Empirical meth-
ods for compound splitting. In Proceedings of
Meeting of the European Chapter of the Associ-
ation of Computational Linguistics (EACL).
Koehn, P. and Schroeder, J. (2007). Experiments
in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages
224?227, Prague, Czech Republic. Association
for Computational Linguistics.
Marcu, D., Wang, W., Echihabi, A., and Knight,
K. (2006). Spmt: Statistical machine transla-
tion with syntactified target language phrases.
In Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 44?52, Sydney, Australia. Associa-
tion for Computational Linguistics.
Schwenk, H. and Koehn, P. (2008). Large and
diverse language models for statistical machine
translation. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language
Processing (IJCNLP).
Stolke, A. (2002). SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the
International Conference on Spoken Language
Processing.
Zollmann, A. and Venugopal, A. (2006). Syntax
augmented machine translation via chart pars-
ing. In Proceedings on the Workshop on Statis-
tical Machine Translation, pages 138?141, New
York City. Association for Computational Lin-
guistics.
120
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 217?226,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Agreement Constraints for Statistical Machine Translation into German
Philip Williams and Philipp Koehn
School of Informatics
University of Edinburgh
10 Crichton Street
EH8 9AB, UK
p.j.williams-2@sms.ed.ac.uk
pkoehn@inf.ed.ac.uk
Abstract
Languages with rich inflectional morphology
pose a difficult challenge for statistical ma-
chine translation. To address the problem of
morphologically inconsistent output, we add
unification-based constraints to the target-side
of a string-to-tree model. By integrating con-
straint evaluation into the decoding process,
implausible hypotheses can be penalised or
filtered out during search. We use a sim-
ple heuristic process to extract agreement con-
straints for German and test our approach on
an English-German system trained on WMT
data, achieving a small improvement in trans-
lation accuracy as measured by BLEU.
1 Introduction
Historically, most work in statistical machine trans-
lation (SMT) has focused on translation into En-
glish. Languages with richer inflectional mor-
phologies pose additional challenges for translation
and conventional SMT approaches tend to perform
poorly when either source or target language has rich
morphology (Koehn, 2005).
For complex source inflection, a successful ap-
proach has been to cluster inflectional variants into
equivalence classes. This removes information that
is redundant for translation and can be performed as
a preprocessing step for input to a conventional sur-
face form based translation model (Nie?en and Ney,
2001; Goldwater and McClosky, 2005; Talbot and
Osborne, 2006).
For complex target inflection,
Minkov et al (2007) investigate how post-
processing can be used to generate inflection for a
system that produces uninflected output. Their ap-
proach is successfully applied to English-Arabic and
English-Russian systems by Toutanova et al (2008).
Another promising line of research involves the
direct integration of linguistic information into SMT
models. Koehn and Hoang (2007) generalise the
phrase-based model?s representation of the word
from a string to a vector, allowing additional features
such as part-of-speech and morphology to be asso-
ciated with, or even to replace, surface forms dur-
ing search. Luong et al (2010) decompose words
into morphemes and use this extended represen-
tation throughout the training, tuning, and testing
pipeline.
Departing further from traditional SMT mod-
els, the transfer-based systems of Riezler and
Maxwell (2006), Bojar and Hajic? (2008), and Gra-
ham et al (2009) employ rich feature structure
representations for linguistic attributes, but have
so far been limited by their dependence on non-
stochastic parsers with limited coverage. The Stat-
XFER transfer-based framework (Lavie, 2008) is
neutral with regard to the rule acquisition method
and the author describes a manually developed
Hebrew-English transfer grammar, which includes a
small number of constraints between agreement fea-
tures. In Hanneman et al (2009) the framework is
used with a large automatically-extracted grammar,
though this does not use feature constraints.
In this paper we propose a model that retains the
use of surface forms during decoding whilst also
checking linguistic constraints defined over asso-
ciated feature structures. Specifically, we extend
a string-to-tree model by adding unification-based
217
constraints to the target-side of the synchronous
grammar. We suggest that such a constraint system
can:
? improve the model by enforcing inflectional
consistency in combinations unseen by the lan-
guage model
? improve search by allowing the early elimina-
tion of morphologically-inconsistent hypothe-
ses
To evaluate the approach, we develop a system for
English-German with constraints to enforce intra-
NP/PP and subject-verb agreement, and with a sim-
ple probabilistic model for NP case.
2 Preliminaries
There is an extensive literature on constraint-based
approaches to grammar, employing a rich variety
of terminology and linguistic devices. We use only
a few of the core ideas, which we briefly describe
in this section. We borrow the terminology and
notation of PATR-II (Shieber, 1984), a minimal
constraint-based formalism that extends context-free
grammar.
Central to our model are the concepts of feature
structures and unification. Feature structures are of
two kinds:
? atomic feature structures are untyped, indivisi-
ble values, such as NP, nom, or sg
? complex feature structures are partial functions
mapping features to values, the values them-
selves being feature structures.
Complex feature structures are conventionally writ-
ten as attribute-value matrices. For example, the fol-
lowing might represent lexical entries for the Ger-
man definite article, die, and the German noun,
Katze, meaning cat:
die ? ?
?
?
?
?
?
?
?
POS ART
AGR
?
?
?
?
?
CASE acc
DECL weak
GENDER fem
NUMBER sg
?
?
?
?
?
?
?
?
?
?
?
?
?
Katze ? ?
?
?
?
?
POS NN
AGR
?
?
?
CASE acc
GENDER fem
NUMBER sg
?
?
?
?
?
?
?
?
An equivalent representation, and the one we use
for implementation, is that of a rooted, labelled, di-
rected acyclic graph.
A value belonging to a complex feature structure
can be specified using a path notation that describes
the chain of features in enclosing feature structures.
In the examples above, the path ? AGR GENDER ?
specifies the atomic value fem.
Informally, unification is a merging operation that
given two feature structures, yields the minimal fea-
ture structure containing all information from both
inputs. A unification failure results if the input
feature structures have mutually-conflicting values.
The subject of unification, both in the context of nat-
ural language processing and more generally, is sur-
veyed in Knight (1989). In this work, we use de-
structive graph-based unification, which results in
the source feature structures sharing values upon
unification.
For example, the result of unifying the agreement
values for the feature structures above would be:
die ? ?
?
?
?
?
?
?
?
POS ART
AGR 1
?
?
?
?
?
CASE acc
DECL weak
GENDER fem
NUMBER sg
?
?
?
?
?
?
?
?
?
?
?
?
?
Katze ? [POS NN
AGR 1
]
The index boxes are used to indicate that a value is
shared.
3 Grammar
In this section we describe the synchronous gram-
mar used in our string-to-tree model. Rule extraction
is similar to the syntax-augmented model of Zoll-
mann and Venugopal (2006), though we do not use
extended categories in this work. We then describe
how we extend the grammar with target-side con-
straints.
3.1 Synchronous Grammar
Our translation model is based on a synchronous
context-free grammar (SCFG) learned from a par-
allel corpus. Rule extraction follows the hierarchi-
cal phrase-based algorithm of Chiang (2005; 2007).
Source non-terminals are given the undistinguished
label X, whereas the target non-terminals are given
part-of-speech and constituent labels obtained from
218
a parse of the target-side of the parallel corpus.
Rules in which the target span is not covered by a
parse tree constituent are discarded.
Compared with the hierarchical phrase-based
model, the restriction to constituent target phrases
reduces the total grammar size and the addition of
linguistic labels reduces the problem of spurious am-
biguity. We therefore relax Chiang?s (2007) rule fil-
tering in the following ways:
1. Up to seven source-side terminal / non-terminal
elements are allowed.
2. Rules with scope greater than three are filtered
out (Hopkins and Langmead, 2010).
3. Consecutive source non-terminals are permit-
ted.
4. Single-word lexical phrases are allowed for hi-
erarchical subphrase subtraction.
3.2 Constraint Grammar
We extend the synchronous grammar by adding con-
straints to the target-side. A constraint is an identity
between either:
i) feature structure values belonging to two rule
elements,
ii) a feature structure value belonging to a rule el-
ement and a constant value, or
iii) a feature structure value belonging to a rule ele-
ment and a random variable with an associated
probability function
For example, the following synchronous rule:
NP-SB ? the X1 cat | die AP1 Katze
might have the target constraint rule shown in Fig-
ure 1.
The first three constraints ensure that any AP has
agreement values consistent with the lexical items
die and Katze. The next provides a probability based
on the resulting case value. The final two are used to
disambiguate between possible parts-of-speech.
Constraints are evaluated by attempting to unify
the specified feature structures. A rule element may
have more than one associated feature structure, so
NP-SB ? die AP Katze
? NP-SB AGR? = ? die AGR?
? NP-SB AGR? = ? AP AGR?
? NP-SB AGR? = ? Katze AGR?
? NP-SB AGR CASE? = C
? die POS? = ART
? Katze POS? = NN
P (C = c) =
?
?
?
?
?
?
?
0.990, c = NOM
0.005, c = DAT
0.004, c = GEN
0.001, c = ACC
Figure 1: Example target constraint rule
unification is attempted between all combinations. If
no combination can be successfully unified then the
constraint fails.
Ultimately, all feature structures originate in the
lexicon, which maps a surface form word to a set of
zero or more complex feature structures.
3.3 Some Constraints for German
We now describe the German constraints that we use
in this paper. Whilst the constraint model described
above is language-independent, the actual form of
the constraints will largely be language- and corpus-
specific.
In this work, the linguistic annotation is obtained
from a statistical parser and a morphological anal-
yser. We use the BitPar parser (Schmid, 2004)
trained on the TIGER treebank (Brants et al, 2002)
and the Morphisto morphological analyser (Zielin-
ski and Simon, 2009). We find that we can extract
useful constraints for German based on a minimal
set of simple manually-developed heuristics.
Base NP/PP Agreement
German determiners and adjectives are inflected
to agree in gender and number with the nouns that
they modify. As in English, a distinction is made be-
tween singular and plural number, with most nouns
having separate forms for each. Grammatical gender
has three values: masculine, feminine, and neuter.
A noun phrase?s case is usually determined by its
219
{ADJA, ART, NN, PDAT,
PIAT, PPOSAT, PWAT
}
? {NP, PP}
{APPR, APPRART} ? {PP}
{ADJA} ? {AP, CAP}
{AP} ? {CAP}
{AP, CAP} ? {NP, PP}
Figure 2: Propagation rules used to capture NP/PP agree-
ment relations
role in the clause. For example, nominative case
usually indicates the subject of a verb. The case of
a prepositional phrase is usually determined by the
choice of preposition.
We model these grammatical properties by i) as-
sociating, via the lexicon, a set of possible agree-
ment values with each preposition, determiner, ad-
jective, and noun, and ii) enforcing agreement rela-
tions through pairwise identities between rule ele-
ments (as in the example in Figure 1).
For constraint extraction, we first group parse tree
nodes into agreement relations. We use the parse
tree labels to determine whether a parent shares
agreement information with a child. Figure 2 shows
the rules that we used in experiments. These should
be read as saying that if a child node has a label that
appears on the left-hand side of a rule, r, and its par-
ent node has a label that appears on the right-hand
side of r then the parent and child share agreement
information.
These rules are applied bottom-up from the
preterminal nodes of the training data trees. Agree-
ment relations are merged if they share a common
parent. Finally, relations are extended to include
child words. Figure 3 shows a sentence pair in which
the target-side tree has been annotated to show two
NP agreement relations found according to the rules
of Figure 2.
Of course, this process is not perfect and finds
many spurious relations. We guard against the most
frequent errors by:
i) Filtering out relations based on label-patterns
found during error analysis (for example, rela-
tions containing multiple NN nodes)
ii) Attempting to unify the agreement feature
structures of the words and rejecting relations
for which this fails
Having annotated the training data trees with
agreement relations, rule extraction is extended to
accept annotated trees and to generate constraint
rules of the form shown in Figure 1. Constraints are
produced where any two target-side rule elements
belong to a common agreement relation. The result-
ing constraints are grouped by relation into distinct
constraint sets.
Subject-Verb Agreement
We add limited subject-verb agreement in a sim-
ilar manner. The additional propagation rules are
given in Figure 4. To determine the subject we rely
upon the TIGER treebank?s grammatical function
labels, which the parser affixes to constituent labels.
These are otherwise ignored in all propagation rules.
Probabilistic Constraints for NP Case
We make further use of the treebank?s grammat-
ical function labels in order to define probabilistic
constraints for noun phrase case. Many of the func-
tion labels are strongly biased towards a particu-
lar case (NP-TOP uses nominative case in 91.5% of
unambiguous occurrences, for example). We esti-
mate probabilities by evaluating NP agreement rela-
tions in the training data and counting case-label co-
occurrences. Ambiguous case values are ignored.
The training data uses only 23 distinct NP labels,
most of which occur very frequently, so no smooth-
ing is applied. Table 1 shows the 10 most common
labels and their case frequencies.
4 Model
As is standard, we frame the decoding problem as a
search for the most probable target language tree t?
given a source language string s:
t? = argmaxt p(t|s)
The function p(t|s) is modelled by a log-linear
sum of weighted feature functions:
p(t|s) = 1Z
n
?
i=1
?ihi(s, t)
220
TOP
S-TOP
NP-SB
PIAT
beide
NN
Versa?umnisse
VAFIN
haben
VP-OC
NP-OA
ADJA
terroristische
NN
Gruppen
PP-MNR
APPR
in
NE
Pakistan
VVPP
gesta?rkt
PUNC.
.
both failures have strengthened domestic terrorist groups .
Figure 3: Sentence pair from training data. The two NP agreement relations used for constraint extraction are indicated
by the rectangular and elliptical node borders.
{VAFIN, VMFIN, VVFIN} ? {S}
{NP-SB} ? {S}
Figure 4: Propagation rules used to capture subject-verb
agreement relations
Label Nom Acc Gen Dat Freq
AG 0.1 0.0 99.9 0.0 308156
CJ 10.9 10.3 32.4 46.4 77198
OA 1.6 91.5 0.7 6.2 67686
SB 99.0 0.1 0.4 0.5 60245
DA 1.9 0.2 1.4 96.5 41624
PD 98.2 0.2 1.4 0.3 19736
APP 39.4 7.3 8.7 44.6 7739
MO 18.6 17.3 56.9 7.2 7591
PNC 30.6 0.0 47.4 22.0 4888
OG 0.1 0.0 97.9 2.0 2060
Table 1: The 10 most freqently occurring NP labels with
their case frequencies (shown as percentages)
4.1 String-to-Tree Features
Our feature functions include the n-gram language
model probability of t?s yield, a count of the words
in t?s yield, and various scores for the synchronous
derivation. We score grammar rules according to the
following functions:
? p(RHSs|RHSt,LHS), the noisy-channel trans-
lation probability.
? p(RHSt|RHSs,LHS), the direct translation
probability, which we further condition on the
root label of the target tree fragment.
? plex (RHSt|RHSs) and plex (RHSs|RHSt), the
direct and indirect lexical weights (Koehn et al,
2003).
? ppcfg(FRAGt), the monolingual PCFG proba-
bility of the tree fragment from which the rule
was extracted. This is defined as
?n
i=1 p(ri),
where r1 . . . rn are the constituent CFG rules
of the fragment. The PCFG parameters are esti-
mated from the parse of the target-side training
data. All lexical rules are given the probabil-
ity 1. This is similar to the pcfg feature used in
Marcu et al (2006) and is intended to encour-
age the production of syntactically well-formed
derivations.
? exp(1), a rule penalty.
221
4.2 Constraint Model Features
In addition to the string-to-tree features, we add two
features related to constraint evaluation:
? exp(f), where f is the derivation?s constraint
set failure count. This serves as a penalty fea-
ture in a soft constraint variant of the model:
for each constraint set in which a unification
failure occurs, this count is increased and an
empty feature structure is produced, permitting
decoding to continue.
?
?
n pcase(cn), the product of the derivation?s
case model probabilities. Where the case value
is ambiguous we take the highest possible prob-
ability.
5 Decoding
We use the Moses (Koehn et al, 2007) decoder, a
bottom-up synchronous parser that implements the
CYK+ algorithm (Chappelier and Rajman, 1998)
with cube pruning (Chiang, 2007).
The constraint model requires some changes to
decoding, which we briefly describe here:
5.1 Hypothesis State
Bottom-up constraint evaluation requires a feature
structure set for every rule element that participates
in a constraint. For lexical rule elements these are
obtained from the lexicon. For non-lexical rule ele-
ments these are obtained from predecessor hypothe-
ses. After constraint evaluation, each hypothesis
therefore stores the resulting, possibly empty, set of
feature structures corresponding to its root rule ele-
ment.
Hypothesis recombination must take these feature
structure states into account. We take the simplest
approach of requiring sets to be equal for recombi-
nation.
5.2 Cube Pruning
At each chart cell, the decoder determines which
rules can be applied to the span and which com-
binations of subspans they can cover (the applica-
tion contexts). An n-dimensional cube is created for
each application context of a rule, where n?1 is the
rank of the rule. Each cube has one dimension per
subspan and one for target-side translation options.
Cube pruning begins with these cubes being placed
into a priority queue ordered according to the model
score of their corner hypotheses.
With the introduction of the constraint model, the
cube pruning algorithm must also allow for con-
straint failure. For the hard constraint model, we
make the following modifications:
1. Since the corner hypothesis might fail the con-
straint check, rule cube ordering is based on
the score of the nearest hypothesis to the corner
that satisifies its constraints (if any exists). This
hypothesis is found by exploring neighbours in
order of estimated score (that is, without calcu-
lating the full language model score) starting at
the corner.
2. When a hypothesis is popped from a cube and
its neighbours created, constraint-failing neigh-
bours are added to a ?bad neighbours? queue.
3. If a cube cannot produce a new hypothesis be-
cause all of the neighbours fail constraints, it
starts exploring neighbours of the bad neigh-
bours.
We place an arbitrary limit of 10 on the number
of consecutive constraint-failing hypotheses to con-
sider before discarding the cube.
We anticipate that decoding for a highly in-
flected target language will result in a less mono-
tonic search space due to the increased formation of
inflectionally-inconsistent combinations.
6 Experiments
6.1 Baseline Setup
We trained a baseline system using the English-
German Europarl and News Commentary data from
the ACL 2010 Joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR1.
The German-side of the parallel corpus was
parsed using the BitPar2 parser. Where a parse failed
the pair was discarded, leaving a total of 1,516,961
sentence pairs. These were aligned using GIZA++
1http://www.statmt.org/wmt10/
translation-task.html
2http://www.ims.uni-stuttgart.de/tcl/
SOFTWARE/BitPar.html
222
and SCFG rules were extracted as described in sec-
tion 3.1 using the Moses toolkit. The resulting gram-
mar contained just under 140 million synchronous
rules.
We used all of the available monolingual Ger-
man data to train three 5-gram language models (one
each for the Europarl, News Commentary, and News
data sets). These were interpolated using weights
optimised against the development set and the re-
sulting language model was used in experiments.
We used the SRILM toolkit (Stolcke, 2002) with
Kneser-Ney smoothing (Chen and Goodman, 1998).
The baseline system?s feature weights were tuned
on the news-test2008 dev set (2,051 sentence pairs)
using minimum error rate training (Och, 2003).
6.2 Constraint Model Setup
A feature structure lexicon was generated by run-
ning the Morphisto3 morphological analyser over
the training vocabulary and then extracting feature
values from the output.
The constraint rules were extracted using the
agreement relation identification and filtering meth-
ods described in section 3.3.
We tested two constraint model systems, one us-
ing the rules as hard constraints and the other as soft
constraints. The former discarded all hypotheses
that failed constraints and used the modified cube
pruning search algorithm. The latter allowed con-
straint failure but used the failure count feature as a
penalty. Both systems used the NP case probabil-
ity feature. The weights for these two features were
optimised using MERT (with all baseline weights
fixed). The systems were otherwise identical to the
baseline.
6.3 Evaluation
The systems were evaluated against constrained ver-
sions of the newstest2009, newstest2010, and new-
stest2011 test sets. We used a maximum rule span
of 20 tokens for decoding. In order that the input
could be covered without the use of glue rules (ex-
cept for unknown words), we used sentences of 20
or fewer tokens, giving test sets of 1,025, 1,054, and
1,317 sentences, respectively. We evaluated transla-
tion quality using case-sensitive BLEU-4 (Papineni
3http://code.google.com/p/morphisto/
(NP-AG der (ADJA regelma??igen) (ADJA ta?glichen) (NN Handel))
(PP-MO nach Angaben der (ADJA o?rtlichen) (NN Index))
(NP-CJ die (ADJA amerikanischen) (NN Blutbad))
(PP-MNR fu?r die (ADJA asiatischen) (NN Handel))
(TOP (NP-SB der (NN Vorsprung) des (NN razor))
(VVFIN ka?mpfen)
(CNP-OA : (NN MP3-Player) (KON und) (NN Mobiltelefone))
.)
Figure 5: Tree fragments containing the first five con-
straint failures found on the baseline 1-best output
et al, 2002) with a single reference.
Table 2 shows the results for the three constrained
test tests. The p-values were calculated using paired
bootstrap resampling (Koehn, 2004). We suspect
that the substantially lower baseline scores on the
newstest2011 test set are largely due to recency ef-
fects (since we use 2010 data for training).
To gauge the frequency of agreement violations
in the baseline output we matched constraint rules
to the 1-best baseline derivations and performed a
bottom-up evaluation for each target-side tree. For
the three constrained test sets, newstest2009, new-
stest2010, and newstest2011, we found that 15.5%,
14.4%, and 15.6% of sentences, respectively, con-
tained one or more constraint failures. Figure 5
shows the tree fragments for the first five failures
found in newstest2009.
In order to explore the interaction of the constraint
model with search we then repeated the experiments
for varying cube pruning pop limits. Figure 6 shows
how the mean test set BLEU score varies against pop
limit. Except at very low pop limits, the soft con-
straint system outperforms the hard constraint sys-
tem. Together with the high p-values for the hard
constraint system, this suggests that, despite filter-
ing, our simple constraint extraction heuristics may
be introducing significant numbers of spurious con-
straints. Alternatively, enforcing the hard constraint
may eliminate too many hypotheses that cannot be
satisifactorily substituted ? constraint-satisfying al-
ternatives frequently differ in more than just inflec-
tion. Either way, the soft constraint model is able to
overcome some of these deficiencies by permitting
some constraint failures in the 1-best output.
223
newstest2009-20 newstest2010-20 newstest2011-20
Experiment BLEU p-value BLEU p-value BLEU p-value
baseline 15.34 - 15.65 - 12.90 -
hard constraint 15.49 0.164 15.95 0.065 12.87 0.318
soft constraint 15.67 0.006 15.98 0.009 13.11 0.053
Table 2: BLEU scores and p-values for the three test sets
14.4
14.5
14.6
14.7
14.8
14.9
15
0 500 1000 1500 2000 2500
Av
g
B
LE
U
Pop limit
baseline
soft constraint
hard constraint
Figure 6: Cube pruning pop limit vs average BLEU score
7 Conclusion
In this paper we have presented an SMT model that
allows the addition of linguistic constraints to the
target-side of a conventional string-to-tree model.
We have developed a simple heuristic method to ex-
tract constraints for German and demonstrated the
approach on a constrained translation task, achiev-
ing a small improvement in translation accuracy.
In future work we intend to investigate the de-
velopment of constraint models for target languages
with more complex inflection. Besides the require-
ment for suitable language processing tools, this re-
quires the development of reliable language-specific
constraint extraction techniques.
We also plan to investigate how the model could
be extended to generate inflection during decoding:
a complementary constraint system could curb the
overgeneration of surface form combinations that
has limited previous approaches.
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. This
work was supported by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme) and made use of the resources
provided by the Edinburgh Compute and Data Facil-
ity.4 The ECDF is partially supported by the eDIKT
initiative.5 This work was also supported in part un-
der the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-C-0022. The first author was supported by an
EPSRC Studentship.
References
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-based
and deep syntactic english-to-czech statistical machine
translation. In StatMT ?08: Proceedings of the Third
Workshop on Statistical Machine Translation, pages
143?146, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the workshop on treebanks
and linguistic theories, pages 24?41.
J.-C. Chappelier and M. Rajman. 1998. A generalized
cyk algorithm for parsing stochastic cfg. In Proceed-
ings of the First Workshop on Tabulation in Parsing
and Deduction, pages 133?137.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
4http://www.ecdf.ed.ac.uk
5http://www.edikt.org.uk
224
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical mt through morphological analysis. In
HLT ?05: Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, pages 676?683, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Yvette Graham, Anton Bryl, and Josef van Genabith.
2009. F-structure transfer-based statistical machine
translation. In In Proceedings of Lexical Functional
Grammar Conference 2009.
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,
Alok Parlikar, and Alon Lavie. 2009. An im-
proved statistical transfer system for french?english
machine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 140?144, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Kevin Knight. 1989. Unification: a multidisciplinary
survey. ACM Comput. Surv., 21(1):93?124.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In In Proceedings of EMNLP, 2007.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Alon Lavie. 2008. Stat-xfer: a general search-based
syntax-driven framework for machine translation. In
Proceedings of the 9th international conference on
Computational linguistics and intelligent text process-
ing, CICLing?08, pages 362?375, Berlin, Heidelberg.
Springer-Verlag.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich lan-
guages. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 148?157, Cambridge, MA, October. Associa-
tion for Computational Linguistics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: statistical machine trans-
lation with syntactified target language phrases. In
EMNLP ?06: Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 44?52, Morristown, NJ, USA. Association for
Computational Linguistics.
Einat Minkov, Kristina Toutanova, and Suzuki Hisami.
2007. Generating complex morphology for machine
translation. In Proceedings of the ACL.
Sonja Nie?en and Hermann Ney. 2001. Toward hier-
archical models for statistical machine translation of
inflected languages. In Proceedings of the workshop
on Data-driven methods in machine translation, pages
1?8, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Stefan Riezler and John T. Maxwell, III. 2006. Gram-
matical machine translation. In Proceedings of the
main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 248?
255, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proceedings of the 20th international conference on
Computational Linguistics, COLING ?04, Strouds-
225
burg, PA, USA. Association for Computational Lin-
guistics.
Stuart M. Shieber. 1984. The design of a computer lan-
guage for linguistic information. In Proceedings of the
10th international conference on Computational lin-
guistics, COLING ?84, pages 362?366, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, Denver, Colorado, September 2002.
David Talbot and Miles Osborne. 2006. Modelling lex-
ical redundancy for machine translation. In ACL-44:
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 969?976, Morristown, NJ, USA. Association for
Computational Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL, Associ-
ation for Computational Linguistics, June 2008.
Andrea Zielinski and Christian Simon. 2009. Mor-
phisto ?an open source morphological analyzer for
german. In Proceeding of the 2009 conference on
Finite-State Methods and Natural Language Process-
ing: Post-proceedings of the 7th International Work-
shop FSMNLP 2008, pages 224?231, Amsterdam, The
Netherlands, The Netherlands. IOS Press.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
StatMT ?06: Proceedings of the Workshop on Statisti-
cal Machine Translation, pages 138?141, Morristown,
NJ, USA. Association for Computational Linguistics.
226
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 388?394,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
GHKM Rule Extraction and Scope-3 Parsing in Moses
Philip Williams and Philipp Koehn
School of Informatics
University of Edinburgh
10 Crichton Street
EH8 9AB, UK
p.j.williams-2@sms.ed.ac.uk
pkoehn@inf.ed.ac.uk
Abstract
We developed a string-to-tree system for
English?German, achieving competitive re-
sults against a hierarchical model baseline.
We provide details of our implementation of
GHKM rule extraction and scope-3 parsing
in the Moses toolkit. We compare systems
trained on the same data using different gram-
mar extraction methods.
1 Introduction
Over the last few years, syntax-based rule extraction
has largely developed along two lines, one originat-
ing in hierarchical phrase-based translation (Chiang,
2005; Chiang, 2007) and the other in GHKM (Gal-
ley et al, 2004; Galley et al, 2006).
Hierarchical rule extraction generalizes the estab-
lished phrase-based extraction method to produce
formally-syntactic synchronous context-free gram-
mar rules without any requirement for linguistic an-
notation of the training data. In subsequent work, the
approach has been extended to incorporate linguis-
tic annotation on the target side (as in SAMT (Zoll-
mann and Venugopal, 2006)) or on both sides (Chi-
ang, 2010).
In contrast, GHKM places target-side syntactic
structure at the heart of the rule extraction process,
producing extended tree transducer rules that map
between strings and tree fragments.
Ultimately, both methods define rules according
to a sentence pair?s word-alignments. Without any
restriction on rule size they will produce an expo-
nentially large set of rules and so in practice only
a subgrammar can be extracted. It is the differing
rule selection heuristics that distinguish these two
approaches, with hierarchical approaches being mo-
tivated by phrasal coverage and GHKM by target-
side tree coverage.
The Moses toolkit (Koehn et al, 2007) has in-
cluded support for hierarchical phrase-based rule ex-
traction since the decoder was first extended to sup-
port syntax-based translation (Hoang et al, 2009).
In this paper we provide some implementation de-
tails for the recently-added GHKM rule extractor
and for the related scope-3 decoding algorithm. We
then describe the University of Edinburgh?s GHKM-
based English-German submission to the WMT
translation task and present comparisons with hier-
archical systems trained on the same data. To our
knowledge, these are the first GHKM results pre-
sented for English-German, a language pair with a
high degree of reordering and rich target-side mor-
phology.
2 GHKM Rule Extraction in Moses
A basic GHKM rule extractor was first developed
for Moses during the fourth Machine Translation
Marathon1 in 2010. We have recently extended it
to support several key features that are described in
the literature, namely: composition of rules (Gal-
ley et al, 2006), attachment of unaligned source
words (Galley et al, 2004), and elimination of fully
non-lexical unary rules (Chung et al, 2011).
We provide some basic implementation details in
the remainder of this section. In section 4 we present
1http://www.mtmarathon2010.info
388
TOP
S-TOP
PDS
das
VAFIN
ist
NP-PD
ART
der
NN
Fall
PP-MNR
APPR
von
PN-NK
NE
Alexander
NE
Nikitin
PUNC.
.
it is the case of Alexander Nikitin .
Figure 1: Sentence pair from training data.
experimental results comparing performance against
Moses? alternative rule extraction methods.
2.1 Composed Rules
Composition of minimal GHKM rules into larger,
contextually-richer rules has been found to signif-
icantly improve translation quality (Galley et al,
2006). Allowing any combination of adjacent min-
imal rules without restriction is unfeasible and so
in practice various constraints are imposed on com-
position. Our implementation includes three con-
figurable parameters for this purpose, which we
describe with reference to the example alignment
graph shown in Figure 1. All three are defined in
terms of the target tree fragment.
Rule depth is defined as the maximum distance
from the composed rule?s root node to any other
node within the fragment, not counting preterminal
expansions (such as NE ? Nikitin). By default, the
rule depth is limited to three. If we consider the
composition of rules rooted at the S-TOP node in
Figure 1 then, among many other possibilities, this
setting permits the formation of a rule with the target
side:
S-TOP ? das ist der Fall von PN-NK
since the maximum distance from the rule?s root
node to another node is three (to APPR or to PN-NK).
However, a rule with the target side:
S-TOP ? das ist der Fall von NE Nikitin
is not permitted since it has a rule depth of four
(from S-TOP to either of the NE nodes).
Node count is defined as the number of target tree
nodes in the composed rule, excluding target words.
The default limit is 15, which for the example is
large enough to permit any possible composed rule
(the full tree has a node count of 13).
Rule size is the measure defined in De-
Neefe et al (2007): the number of non-part-of-
speech, non-leaf constituent labels in the target tree.
The default rule size limit is three.
2.2 Unaligned Source Words
Unaligned source words are attached to the tree
using the following heuristic: if there are aligned
source words to both the left and the right of an un-
aligned source word then it is attached to the lowest
common ancestor of its nearest such left and right
neighbours. Otherwise, it is attached to the root of
the parse tree.
2.3 Unary Rule Elimination
Moses? chart decoder does not currently support
the use of grammars containing fully non-lexical
unary rules (such as NP ? X1 | NN1). Unless the
--AllowUnary option is given, the rule extractor
eliminates these rules using the method described in
Chung et al (2011).
2.4 Scope Pruning
Unlike hierarchical phrase-based rule extraction,
GHKM places no restriction on the rank of the re-
sulting rules. In order that the grammar can be
parsed efficiently, one of two approaches is usually
taken: (i) synchronous binarization (Zhang et al,
2006), which transforms the original grammar to a
weakly equivalent form in which no rule has rank
greater than two. This makes the grammar amenable
to decoding with a standard chart-parsing algorithm
such as CYK, and (ii) scope pruning (Hopkins and
Langmead, 2010), which eliminates rules in order to
produce a subgrammar that can be parsed in cubic
time.
Of these two approaches, Moses currently sup-
ports only the latter. Both rule extractors prune
the extracted grammar to remove rules with scope
greater than three. The next section describes the
parsing algorithm that is used for scope-3 grammars.
389
3 Scope-3 Parsing in Moses
Hopkins and Langmead (2010) show that a sentence
of length n can be parsed using a scope-k grammar
in O(nk) chart updates. In this section, we describe
some details of Moses? implementation of their chart
parsing method.
3.1 The Grammar Trie
The grammar is stored in a trie-based data structure.
Each edge is labelled with either a symbol from the
source terminal vocabulary or a generic gap sym-
bol, and the trie is constructed such that for any path
originating at the root vertex, the sequence of edge
labels represents the prefix of a rule?s source right-
hand-side (RHSs, also referred to as a rule pattern).
Wherever a path corresponds to a complete RHSs,
the vertex stores an associative array holding the set
of grammar rules that share that RHSs. The asso-
ciative array maps a rule?s sequence of target non-
terminal symbols to the subset of grammar rules that
share those symbols.
Figure 2 shows a sample of the grammar rules that
can be extracted from the example alignment graph
of Figure 1, and Figure 3 shows the corresponding
grammar trie.
3.2 Initialization
The first step is to construct a secondary trie that
records all possible applications of rule patterns
from the grammar to the sentence under consider-
ation. This trie is built during a single depth-first
traversal of the grammar trie in which the terminal
edge labels are searched for in the input sentence. If
a matching input word is found then the secondary
trie is extended by one vertex for each sentence posi-
tion at which the word occurs and trie traversal con-
tinues along that path. A search for a gap label al-
ways results in a match. Edges in the secondary trie
are labelled with the matching symbol and the posi-
tion of the word in the input sentence (or a null po-
sition for gap labels). Each vertex in the secondary
trie stores a pointer to the corresponding grammar
trie vertex.
Once the secondary trie has been built, it is easy
to determine the set of subspans to which each rule
pattern applies. A set of pairs is recorded against
each subspan, each pair holding a pointer to a gram-
mar trie vertex and a record of the sentence positions
covered by the symbols (which will be ambiguous if
the pattern contains a sequence of k > 1 adjacent
gap symbols covering more than k sentence posi-
tions).
After this initialization step, the secondary trie is
discarded.
3.3 Subspan Processing
The parsing algorithm proceeds by processing chart
cells in order of increasing span width (i.e. bottom-
up). At each cell, a stack lattice is constructed for
each rule pattern that was found during initialization.
The stack lattice compactly represents all possible
applications of that pattern over the span, together
with pointers to the underlying hypothesis stacks for
every gap. A full path through the lattice corre-
sponds to a single application context. By selecting
a derivation class (i.e. target-side non-terminal la-
bel) at each arc, the path can be bound to a set of
grammar rules that differ only in the choice of target
words or LHS label.
Recall that for every rule pattern found during
initialization, the corresponding grammar trie ver-
tex was recorded and that the vertex holds an as-
sociative array in which the keys are sequences of
target-side non-terminal labels and the mapped val-
ues are grammar rules (together with associated fea-
ture model scores). The algorithm now loops over
the associated array?s key sequences, searching the
lattice for matching paths. Where found, the gram-
mar rule is bound with a sequence of underlying
stack pointers. The cell?s stacks are then populated
by applying cube pruning (Chiang, 2007) to the set
of bound grammar rules.
4 Experiments
This section describes the GHKM-based English-
German system submitted by the University of Ed-
inburgh. Subsequent to submission, a further set of
comparative experiments were run using a hierarchi-
cal phrase-based system and a hierarchical system
with target side syntactic annotation.
4.1 Data
We made use of all available English-German Eu-
ropean and News Commentary data. For the hi-
erarchical phrase-based experiments, this totalled
390
1. NP-PD ? the case of Alexander Nikitin | der Fall von Alexander Nikitin
2. NP-PD ? the case X1 | der Fall PP-MNR1
3. NP-PD ? X1 case X2 | ART1 Fall PP-MNR2
4. PP-MNR ? of X1 | von PN-NK1
5. PP-MNR ? of X1 X2 | von NE1 NE2
Figure 2: A sample of the rules extractable from the alignment graph in Figure 1. Rules are written in the form
LHS ? RHSs | RHSt .
Nikitin
Alexander
of ?
case
?
?
?
case
the of ?
Figure 3: Example grammar trie. The filled vertices hold
associative array values.
2,043,914 sentence pairs. For the target syntax ex-
periments, the German-side of the parallel corpus
was parsed using the BitPar2 parser. If a parse
failed then the sentence pair was discarded, leav-
ing a total of 2,028,556 pairs. The parallel corpus
was then word-aligned using MGIZA++ (Gao and
Vogel, 2008), a multi-threaded implementation of
GIZA++ (Och and Ney, 2003).
We used all available monolingual German data
to train seven 5-gram language models (one each
for Europarl, News Commentary, and the five News
data sets). These were interpolated using weights
optimised against the development set and the re-
sulting language model was used in experiments.
We used the SRILM toolkit (Stolcke, 2002) with
Kneser-Ney smoothing (Chen and Goodman, 1998).
The baseline system?s feature weights were tuned
on the news-test2008 dev set (2,051 sentence pairs)
using Moses? implementation of minimum error rate
training (Och, 2003).
2http://www.ims.uni-stuttgart.de/tcl/
SOFTWARE/BitPar.html
4.2 Rule Extraction
For the hierarchical phrase-based model we used
the default Moses rule extraction settings, which
are taken from Chiang (2007). For target-annotated
models, the syntactic constraints imposed by the
parse trees reduce the grammar size significantly.
This allows us to relax the rule extraction settings,
which we have previously found to benefit transla-
tion quality, without producing an unusably large
grammar. We use identical settings to those used in
WMT?s 2010 translation task (Koehn et al, 2010).
Specifically, we relax the hierarchical phrase-based
extraction settings in the following ways:
? Up to seven source-side symbols are allowed.
? Consecutive source non-terminals are permit-
ted.
? Single-word lexical phrases are allowed for hi-
erarchical subphrase subtraction.
? Initial phrases are limited to 15 source words
(instead of 10).
By using the scope-3 parser we can also relax the
restriction on grammar rank. For comparison, we
extract two target-annotated grammars, one with a
maximum rank of two, and one with an unlimited
rank but subject to scope-3 pruning.
GHKM rule extraction uses the default settings3
as described in section 2.
Table 1 shows the sizes of the extracted grammars
after filtering for the newstest2011 test set. Fil-
tering removes any rule in which the source right-
hand-side contains a sequence of terminals and gaps
that does not appear in any test set sentence.
3GHKM rule extraction is now fully integrated into Moses?
Experiment Management System (EMS) and can be enabled for
string-to-tree pipelines using the TRAINING:use-ghkm pa-
rameter.
391
Experiment Grammar Size
Hierarchical 118,649,771
Target Syntax 12,748,259
Target Syntax (scope-3) 40,661,639
GHKM 27,002,733
Table 1: Grammar sizes (distinct rule counts) after filter-
ing for the newstest-2011 test set
4.3 Features
Our feature functions include the n-gram language
model probability of the derivation?s target yield, its
word count, and various scores for the synchronous
derivation. We score grammar rules according to the
following functions:
? p(RHSs|RHSt,LHS), the noisy-channel trans-
lation probability.
? p(LHS,RHSt|RHSs), the direct translation
probability.
? plex (RHSt|RHSs) and plex (RHSs|RHSt), the
direct and indirect lexical weights (Koehn et al,
2003).
? ppcfg(FRAGt), the monolingual PCFG proba-
bility of the tree fragment from which the rule
was extracted (GHKM and target-annotated
systems only). This is defined as ?ni=1 p(ri),
where r1 . . . rn are the constituent CFG rules
of the fragment. The PCFG parameters are es-
timated from the parse of the target-side train-
ing data. All lexical CFG rules are given the
probability 1. This is similar to the pcfg feature
used in Marcu et al (2006) and is intended to
encourage the production of syntactically well-
formed derivations.
? exp(?1/count(r)), a rule rareness penalty.
? exp(1), a rule penalty. The main grammar and
glue grammars have distinct penalty features.
4.4 Decoder Settings
For the submitted GHKM system we used a max-
imum chart span setting of 25. For the other sys-
tems we used settings that matched the rule extrac-
tion spans: 10 for hierarchical phrase-based, 15 for
target syntax, and unlimited for GHKM.
We used the scope-3 parsing algorithm (enabled
using the option -parsing-algorithm 1) for
all systems except the hierarchical system, which
used the CYK+ algorithm (Chappelier and Rajman,
1998).
For all systems we set the ttable-limit pa-
rameter to 50 (increased from the default value of
20). This setting controls the level of grammar prun-
ing that is performed after loading: only the top scor-
ing translations are retained for a given source RHS.
4.5 Results
Following the recommendation of
Clark et al (2011), we ran the optimization
three times and repeated evaluation with each set
of feature weights. Table 2 presents the averaged
single-reference BLEU scores. To give a rough
indication of how much use the systems make of
syntactic information for reordering, we also report
glue rule statistics taken from the 1-best derivations.
There is a huge variation in decoding time be-
tween the systems, much of which can be at-
tributed to the differing chart span limits. To give
a comparison of system performance we selected an
80-sentence subset of newstest2011, randomly
choosing ten sentences of length 1-10, ten of length
11-20, and so on. We decoded the test set four times
for each system, discarding the first set of results (to
allow for filesystem cache priming) and then aver-
aging the remaining three. Table 3 shows the total
decoding times for each system and the peak virtual
memory usage4. Figure 4 shows a plot of sentence
length against decoding time for the two GHKM
systems.
5 Conclusion
We developed a GHKM-based string-to-tree system
for English to German, achieving competitive results
compared to a hierarchical model baseline. We ex-
tended the Moses toolkit to include a GHKM rule
extractor and scope-3 parsing algorithm and pro-
vided details of our implementation. We intend to
further improve this system in future work.
4The server has 142GB physical memory. The decoder was
run single-threaded in performance tests. For the hierarchical
system we used an on-disk rule table, which reduces memory
requirements at the cost of increased rule lookup time. For all
other systems we used in-memory rule tables.
392
newstest2009 newstest2010 newstest2011 Glue Rule Apps
Experiment BLEU s.d. BLEU s.d. BLEU s.d. Mean s.d.
GHKM (max span 25) 15.2 0.1 16.7 0.1 15.4 0.1 3.1 0.3
Hierarchical 15.2 0.0 16.4 0.1 15.5 0.0 13.9 0.5
Target 14.6 0.1 16.0 0.1 14.9 0.1 8.4 5.0
Target (scope-3) 14.7 0.0 16.4 0.2 15.0 0.0 9.7 1.2
GHKM (no span limit) 15.0 0.3 16.6 0.1 15.2 0.2 1.9 1.3
Table 2: Average BLEU scores and standard deviations over three optimization runs. GHKM (max span 25) is the
submitted system. Also shown is the average number of rule applications per sentence for the 1-best output of the
three test sets, averaged over the three optimization runs.
System Max Time (s) VM (MB)
span
Hierarchical 10 122 5,345
Target 15 367 8,688
Target (scope-3) 15 1,539 19,761
GHKM 25 3,529 17,424
GHKM None 11,196 18,060
Table 3: Total decoding time and peak virtual memory
usage for the 80-sentence subset of newstest2011.
0
100
200
300
400
500
600
700
800
0 10 20 30 40 50 60 70 80
D
ec
o
di
ng
Ti
m
e
(se
co
n
ds
)
Sentence Length
Max span 25
No span limit
Figure 4: Sentence length vs decoding time for the
GHKM (max span 25) and GHKM (no limit) systems
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. This
work was supported by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme) and made use of the resources
provided by the Edinburgh Compute and Data Facil-
ity.5 The ECDF is partially supported by the eDIKT
initiative.6 This work was also supported in part un-
der the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-C-0022. The first author was supported by an
EPSRC Studentship.
References
J.-C. Chappelier and M. Rajman. 1998. A generalized
CYK algorithm for parsing stochastic CFG. In Pro-
ceedings of the First Workshop on Tabulation in Pars-
ing and Deduction, pages 133?137.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
5http://www.ecdf.ed.ac.uk
6http://www.edikt.org.uk
393
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
413?417, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176?181, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007). June 28-30,
2007. Prague, Czech Republic.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL ?04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL-44:
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 961?968, Morristown, NJ, USA. Association for
Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?57,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A
unified framework for phrase-based, hierarchical, and
syntax-based statistical machine translation. In In Pro-
ceedings of IWSLT, December 2009.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu
Hoang. 2010. More linguistic annotation for sta-
tistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 115?120, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: statistical machine trans-
lation with syntactified target language phrases. In
EMNLP ?06: Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 44?52, Morristown, NJ, USA. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, Denver, Colorado, September 2002.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Compu-
tational Linguistics, pages 256?263, Morristown, NJ,
USA. Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
StatMT ?06: Proceedings of the Workshop on Statisti-
cal Machine Translation, pages 138?141, Morristown,
NJ, USA. Association for Computational Linguistics.
394
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 170?176,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Edinburgh?s Syntax-Based Machine Translation Systems
Maria Nadejde, Philip Williams, and Philipp Koehn
School of Informatics, University of Edinburgh, Scotland, United Kingdom
maria.nadejde@gmail.com, P.J.Williams-2@sms.ed.ac.uk, pkoehn@inf.ed.ac.uk
Abstract
We present the syntax-based string-to-
tree statistical machine translation systems
built for the WMT 2013 shared transla-
tion task. Systems were developed for
four language pairs. We report on adapting
parameters, targeted reduction of the tun-
ing set, and post-evaluation experiments
on rule binarization and preventing drop-
ping of verbs.
1 Overview
Syntax-based machine translation models hold
the promise to overcome some of the fundamen-
tal problems of the currently dominating phrase-
based approach, most importantly handling re-
ordering for syntactically divergent language pairs
and grammatical coherence of the output.
We are especially interested in string-to-tree
models that focus syntactic annotation on the tar-
get side, especially for morphologically rich target
languages (Williams and Koehn, 2011).
We have trained syntax-based systems for the
language pairs
? English-German,
? German-English,
? Czech-English, and
? Russian-English.
We have also tried building systems for French-
English and Spanish-English but the data size
proved to be problematic given the time con-
straints. We give a brief description of the syntax-
based model and its implementation within the
Moses system. Some of the available features are
described as well as some of the pre-processing
steps. Several experiments are described and final
results are presented for each language pair.
2 System Description
The syntax-based system used in all experiments
is the Moses string-to-tree toolkit implementing
GHKM rule extraction and Scope-3 parsing previ-
ously described in by Williams and Koehn (2012)
2.1 Grammar
Our translation grammar is a synchronous context-
free grammar (SCFG) with phrase-structure labels
on the target side and the generic non-terminal la-
bel X on the source side. In this paper, we write
these rules in the form
LHS ? RHSs | RHSt
where LHS is a target-side non-terminal label and
RHSs and RHSt are strings of terminals and non-
terminals for the source and target sides, respec-
tively. We use subscripted indices to indicate the
correspondences between source and target non-
terminals.
For example, a translation rule to translate the
German Haus into the English house is
NN ? Haus | house
If our grammar also contains the translation rule
S ? das ist ein X1 | this is a NN1
then we can apply the two rules to an input das ist
ein Haus to produce the output this is a house.
2.2 Rule Extraction
The GHKM rule extractor (Galley et al, 2004,
2006) learns translation rules from a word-aligned
parallel corpora for which the target sentences are
syntactically annotated. Given a string-tree pair,
the set of minimally-sized translation rules is ex-
tracted that can explain the example and is consis-
tent with the alignment. The resulting rules can be
composed in a non-overlapping fashion in order to
cover the string-tree pair.
Two or more minimal rules that are in a parent-
child relationship can be composed together to ob-
tain larger rules with more syntactic context. To
avoid generating an exponential number of com-
posed rules, several limitation have to be imposed.
One such limitation is on the size of the com-
posed rules, which is defined as the number of
non-part-of-speech, non-leaf constituent labels in
the target tree (DeNeefe et al, 2007). The corre-
sponding parameter in the Moses implementation
is MaxRuleSize and its default value is 3.
170
Another limitation is on the depth of the rules?
target subtree. The rule depth is computed as the
maximum distance from its root node to any of its
children, not counting pre-terminal nodes (param-
eter MaxRuleDepth, default 3).
The third limitation considered is the number of
nodes in the composed rule, not counting target
words (parameter MaxNodes, default 15).
These parameters are language-dependent and
should be set to values that best represent the char-
acteristics of the target trees on which the rule ex-
tractor is trained on. Therefore the style of the
treebanks used for training the syntactic parsers
will also influence these numbers. The default
values have been set based on experiments on
the English-German language pair (Williams and
Koehn, 2012). It is worth noting that the Ger-
man parse trees (Skut et al, 1997) tend to be
broader and shallower than those for English. In
Section 3 we present some experiments where we
choose different settings of these parameters for
the German-English language pair. We use those
settings for all language pairs where the target lan-
guage is English.
2.3 Tree Restructuring
The coverage of the extracted grammar depends
partly on the structure of the target trees. If the
target trees have flat constructions such as long
noun phrases with many sibling nodes, the rules
extracted will not generalize well to unseen data
since there will be many constraints given by the
types of different sibling nodes.
In order to improve the grammar coverage to
generalize over such cases, the target tree can be
restructured. One restructuring strategy is tree
binarization. Wang et al (2010) give an exten-
sive overview of different tree binarization strate-
gies applied for the Chinese-English language
pair. Moses currently supports left binarization
and right binarization.
By left binarization all the left-most children
of a parent node n except the right most child
are grouped under a new node. This node is in-
serted as the left child of n and receives the la-
bel n?. Left binarization is then applied recursively
on all newly inserted nodes until the leaves are
reached. Right binarization implies a similar pro-
cedure but in this case the right-most children of
the parent node are grouped together except the
left most child.
Another binarization strategy that is not cur-
rently integrated in Moses, but is worth investigat-
ing for different language pairs, is parallel head
binarization.
The result of parallel binarization of a parse
tree is a binarization forest. To generate a bina-
rization forest node, both right binarization and
left binarization are applied recursively to a parent
node with more than two children. Parallel head
binarization is a case of parallel binarization with
the additional constraint that the head constituent
is part of all the new nodes inserted by either left
or right binarization steps.
In Section 3 we give example of some initial ex-
periments carried out for the German-English lan-
guage pair.
2.4 Pruning The Grammar
Decoding for syntax-based model relies on a
bottom-up chart parsing algorithm. Therefore de-
coding efficiency is influenced by the following
combinatorial problem: given an input sentence
of length n and a context-free grammar rule with
s consecutive non-terminals, there are (n+1s
) ways
to choose subspans, or application contexts (Hop-
kins and Langmead, 2010), that the rule can ap-
plied to. The asymptotic running time of chart
parsing is linear in this number O(ns).
Hopkins and Langmead (2010) maintain cubic
decoding time by pruning the grammar to remove
rules for which the number of potential applica-
tion contexts is too large. Their key observation is
that a rule can have any number of non-terminals
and terminals as long as the number of consecutive
non-terminal pairs is bounded. Terminals act to
anchor the rule, restricting the number of potential
application contexts. An example is the rule X ?
WyY Zz for which there are at most O(n2) appli-
cation contexts, given that the terminals will have
a fixed position and will play the role of anchors
in the sentence for the non-terminal spans. The
number of consecutive non-terminal pairs plus the
number of non-terminals at the edge of a rule is
referred to as the scope of the rule. The scope of a
grammar is the maximum scope of any of its rules.
Moses implements scope-3 pruning and therefore
the resulting grammar can be parsed in cubic time.
2.5 Feature Functions
Our feature functions are unchanged from last
year. They include the n-gram language model
probability of the derivation?s target yield, its word
171
count, and various scores for the synchronous
derivation. Our grammar rules are scored accord-
ing to the following functions:
? p(RHSs|RHSt,LHS), the noisy-channel
translation probability.
? p(LHS,RHSt|RHSs), the direct translation
probability.
? plex (RHSt|RHSs) and plex (RHSs|RHSt),
the direct and indirect lexical weights (Koehn
et al, 2003).
? ppcfg(FRAGt), the monolingual PCFG prob-
ability of the tree fragment from which
the rule was extracted. This is defined
as ?ni=1 p(ri), where r1 . . . rn are the con-
stituent CFG rules of the fragment. The
PCFG parameters are estimated from the
parse of the target-side training data. All lex-
ical CFG rules are given the probability 1.
This is similar to the pcfg feature proposed
by Marcu et al (2006) and is intended to en-
courage the production of syntactically well-
formed derivations.
? exp(?1/count(r)), a rule rareness penalty.
? exp(1), a rule penalty. The main grammar
and glue grammars have distinct penalty fea-
tures.
3 Experiments
This section describes details for the syntax-based
systems submitted by the University of Edinburgh.
Additional post-evaluation experiments were car-
ried out for the German-English language pair.
3.1 Data
We made use of all available data for each lan-
guage pair except for the Russian-English where
the Commoncrawl corpus was not used. Table 1
shows the size of the parallel corpus used for each
language pair. The English side of the paral-
lel corpus was parsed using the Berkeley parser
(Petrov et al, 2006) and the German side of the
parallel corpus was parsed using the BitPar parser
(Schmid, 2004). For German-English, German
compounds were split using the script provided
with Moses. The parallel corpus was word-aligned
using MGIZA++ (Gao and Vogel, 2008).
All available monolingual data was used for
training the language models for each language
Lang. pair Sentences Grammar Size
en-de 4,411,792 31,568,480
de-en 4,434,060 55,310,162
cs-en 14,425,564 209,841,388
ru-en 1,140,359 7,946,502
Table 1: Corpus statistics for parallel data.
pair. 5-gram language models were trained us-
ing SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) and then interpolated using weights tuned
on the newstest2011 development set.
The feature weights for each system were tuned
on development sets using the Moses implementa-
tion of minimum error rate training (Och, 2003).
The size of the tuning data varied for different lan-
guages depending on the amount of available data.
In the case of the the German-English pair a filter-
ing criteria based on sentence level BLEU score
was applied which is briefly described in Section
3.5. Table 2 shows the size of the tuning set for
each language pair.
Lang. pair Sentences
en-de 7,065
de-en 2,400
cs-en 10,068
ru-en 1,501
Table 2: Corpus statistics for tuning data.
3.2 Pre-processing
Some attention was given to pre-processing of the
English side of the corpus prior to parsing. This
was done to avoid propagating parser errors to the
rule-extraction step. These particular errors arise
from a mismatch in punctuation and tokenization
between the corpus used to train the parser, the
PennTree bank, and the corpus which is being
parsed and passed on to the rule extractor. There-
fore we changed the quotation marks, which ap-
pear quite often in the parallel corpora, to opening
and closing quotation marks. We also added some
PennTree bank style tokenization rules1. These
rules split contractions such as I?ll, It?s, Don?t,
Gonna, Commissioner?s in order to correctly sep-
arate the verbs, negation and possessives that are
1The PennTree bank tokenization rules considered were
taken from http://www.cis.upenn.edu/?treebank/
tokenizer.sed. Further examples of contractions were
added.
172
Grammar Size BLEU
Parameters Full Filtered 2009-40 2010-40 2011-40 Average
Depth=3, Nodes=15, Size=3 2,572,222 751,355 18.57 20.43 18.51 19.17
Depth=4, Nodes=20, Size=4 3,188,970 901,710 18.88 20.38 18.63 19.30
Depth=5, Nodes=20, Size=5 3,668,205 980,057 19.04 20.47 18.75 19.42
Depth=5, Nodes=30, Size=5 3,776,961 980,061 18.90 20.59 18.77 19.42
Depth=5, Nodes=30, Size=6 4,340,716 1,006,174 18.98 20.52 18.80 19.43
Table 3: Cased BLEU scores for various rule extraction parameter settings for German-English language
pair. The parameters considered are MaxRuleDepth, MaxRuleSize, MaxNodes. Grammar sizes are given
for the full extracted grammar and after filtering for the newstest2008 dev set.
newstest2012 newstest2013
System Sentences BLEU Glue Rule Tree Depth BLEU Glue Rule Tree Depth
Baseline 5,771 23.21 5.42 4.03 26.27 4.23 3.80
Big tuning set 10,068 23.52 3.41 4.34 26.33 2.49 4.03
Filtered tuning set 2,400 23.54 3.21 4.37 26.30 2.37 4.05
Table 4: Cased BLEU scores for German-English systems tuned on different data. Scores are emphasized
for the system submitted to the shared translation task.
parsed as separate constituents.
For German?English, we carried out the usual
compound splitting (Koehn and Knight, 2003), but
not pre-reordering (Collins et al, 2005).
3.3 Rule Extraction
Some preliminary experiments were carried out
for the German-English language pair to deter-
mine the parameters for the rule extraction step:
MaxRuleDepth, MaxRuleSize, MaxNodes. Table 3
shows the BLEU score on different test sets for
various parameter settings. For efficiency rea-
sons less training data was used, therefore the
grammar sizes, measured as the total number of
extracted rules, are smaller than the final sys-
tems (Table 1). The parameters on the third line
Depth=5, Nodes=20, Size=4 were chosen as the
average BLEU score did not increase although the
size of the extracted grammar kept growing. Com-
paring the rate of growth of the full grammar and
the grammar after filtering for the dev set (the
columns headed ?Full? and ?Filtered?) suggests
that beyond this point not many more usable rules
are extracted, even while the total number of rules
stills increases.
3.4 Decoder Settings
We used the following non-default decoder param-
eters:
max-chart-span=25: This limits sub deriva-
tions to a maximum span of 25 source words. Glue
rules are used to combine sub derivations allowing
the full sentence to be covered.
ttable-limit=200: Moses prunes the translation
grammar on loading, removing low scoring rules.
This option increases the number of translation
rules that are retained for any given source side
RHSs.
cube-pruning-pop-limit=1000: Number of hy-
potheses created for each chart span.
3.5 Tuning sets
One major limitation for the syntax-based systems
is that decoding becomes inefficient for long sen-
tences. Therefore using large tuning sets will slow
down considerably the development cycle. We
carried out some preliminary experiments to de-
termine how the size of the tuning set affects the
quality and speed of the system.
Three tuning sets were considered. The tun-
ing set that was used for training the baseline sys-
tem was built using the data from newstest2008-
2010 filtering out sentences longer than 30 words.
The second tuning set was built using all data
from newstest2008-2011. The final tuning set
was also built using the concatenation of the sets
newstest2008-2011. All sentences in this set were
decoded with a baseline system and the output was
scored according to sentence-BLEU scores. We se-
173
lected examples with high sentence-BLEU score in
a way that penalizes excessively short examples2.
Results of these experiments are shown in Table 4.
Results show that there is some gain in BLEU
score when providing longer sentences during tun-
ing. Further experiments should consider tuning
the baseline with the newstest2008-2011 data, to
eliminate variance caused by having different data
sources. Although the size of the third tuning set is
much smaller than that of the other tuning sets, the
BLEU score remains the same as when using the
largest tuning set. The glue rule number, which
shows how many times the glue rule was applied,
is lowest when tuning with the third data set. The
tree depth number, which shows the depth of the
resulting target parse tree, is higher for the third
tuning set as compared to the baseline and similar
to that resulted from using the largest tuning set.
These numbers are all indicators of better utilisa-
tion of the syntactic structure.
Regarding efficiency, the baseline tuning set and
the filtered tuning set took about a third of the time
needed to decode the larger tuning set.
Therefore we could draw some initial conclu-
sions that providing longer sentences is useful,
but sentences for which some baseline system per-
forms very poorly in terms of BLEU score can be
eliminated from the tuning set.
3.6 Results
Table 5 summarizes the results for the systems
submitted to the shared task. The BLEU scores for
the phrase-based system submitted by the Univer-
sity of Edinburgh are also shown for comparison.
The syntax-based system had BLEU scores similar
to those of the phrase-based system for German-
English and English-German language pairs. For
the Czech-English and Russian-English language
pairs the syntax-based system was 2 BLEU points
behind the phrase-based system.
However, in the manual evaluation, the
German?English and English?German syntax
based systems were ranked higher than the phrase-
based systems. For Czech?English, the syntax
systems also came much closer than the BLEU
score would have indicated.
The Russian-English system performed worse
because we used much less of the available data
for training (leaving out Commoncrawl) and there-
2Ongoing work by Eva Hasler. Filtered data set was pro-
vided in order to speed up experiment cycles.
phrase-based syntax-based
BLEU manual BLEU manual
en-de 20.1 0.571 19.4 0.614
de-en 26.6 0.586 26.3 0.608
cs-en 26.2 0.562 24.4 0.542
ru-en 24.3 0.507 22.5 0.416
Table 5: Cased BLEU scores and manual evalua-
tion scores (?expected wins?) on the newstest2013
evaluation set for the phrase-based and syntax-
based systems submitted by the University of Ed-
inburgh.
fore the extracted grammar is less reliable. An-
other reason was the mismatch in data format-
ting for the Russian-English parallel corpus. All
the training data was lowercased which resulted in
more parsing errors.
3.7 Post-Submission Experiments
Table 6 shows results for some preliminary ex-
periments carried out for the German-English lan-
guage pair that were not included in the final sub-
mission. The baseline system is trained on all
available parallel data and tuned on data from
newstest2008-2010 filtered for sentences up to 30
words.
Tree restructuring ? In one experiment the
parse trees were restructured before training by
left binarization. Tree restructuring is need to im-
prove generalization power of rules extracted from
flat structures such as base noun phrases with sev-
eral children. The second raw in Table 6 shows
that the BLEU score did not improve and more
glue rules were applied when using left binariza-
tion. One reason for this result is that the rule ex-
traction parameters MaxRuleDepth, MaxRuleSize,
MaxNodes had the same values as in the baseline.
Increasing this parameters should improve the ex-
tracted grammar since binarizing the trees will in-
crease these three dimensions.
Verb dropping ? A serious problem of
German?English machine translation is the ten-
dency to drop verbs, which shatters sentence struc-
ture. One cause of this problem is the failure of the
IBM Models to properly align the German verb to
its English equivalent, since it is often dislocated
with respect to English word order. Further prob-
lems appear when the main verb is not reordered in
the target sentence, which can result in lower lan-
174
newstest2012 newstest2013
System Grammar size BLEU glue rule tree depth BLEU glue rule tree depth
Baseline 55,310,162 23.21 5.42 4.03 26.27 4.23 3.80
Left binarized 57,151,032 23.17 7.79 4.09 26.13 6.57 3.85
Realigned vb 53,894,112 23.26 4.88 4.19 26.26 3.73 3.96
Table 6: Cased BLEU scores for various German-English systems.
System Vb droprules
Vb Count
nt2012
Vb Count
nt2013
Baseline 1,038,597 9,216 8,418
Realigned
verbs 391,231 9,471 8,614
Reference
translation - 9,992 9,207
Table 7: Statistics about verb dropping.
guage model scores and BLEU scores. However
the syntax models handle the reordering of verbs
better than phrase-based models.
In an experiment we investigated how the num-
ber of verbs dropped by the translation rules can
be reduced. In order to reduce the number of
verb dropping rules we looked at unaligned verbs
and realigned them before rule extraction. An un-
aligned verb in the source sentence was aligned
to the verb in the target sentence for which IBM
model 1 predicted the highest translation probabil-
ity. The third row in Table 6 shows the results of
this experiment. While there is no change in BLEU
score the number of glue rules applied is lower.
Further analysis shows in Table 7 that the number
of verb dropping rules in the grammar is almost
three times lower and that there are more trans-
lated verbs in the output when realigning verbs.
4 Conclusion
We describe in detail the syntax-based machine
translation systems that we developed for four Eu-
ropean language pairs. We achieved competitive
results, especially for the language pairs involving
German.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement 287658 (EU BRIDGE) and
grant agreement 288487 (MosesCore).
References
Chen, S. F. and Goodman, J. (1998). An empiri-
cal study of smoothing techniques for language
modeling. Technical report, Harvard University.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
DeNeefe, S., Knight, K., Wang, W., and Marcu,
D. (2007). What can syntax-based MT learn
from phrase-based MT? In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-
CoNLL 2007). June 28-30, 2007. Prague, Czech
Republic.
Galley, M., Graehl, J., Knight, K., Marcu, D., De-
Neefe, S., Wang, W., and Thayer, I. (2006).
Scalable inference and training of context-rich
syntactic translation models. In ACL-44: Pro-
ceedings of the 21st International Conference
on Computational Linguistics and the 44th an-
nual meeting of the Association for Computa-
tional Linguistics, pages 961?968, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Galley, M., Hopkins, M., Knight, K., and Marcu,
D. (2004). What?s in a translation rule? In HLT-
NAACL ?04.
Gao, Q. and Vogel, S. (2008). Parallel implemen-
tations of word alignment tool. In Software En-
gineering, Testing, and Quality Assurance for
Natural Language Processing, SETQA-NLP
?08, pages 49?57, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Hopkins, M. and Langmead, G. (2010). SCFG
decoding without binarization. In Proceedings
of the 2010 Conference on Empirical Methods
175
in Natural Language Processing, pages 646?
655, Cambridge, MA. Association for Compu-
tational Linguistics.
Koehn, P. and Knight, K. (2003). Empirical meth-
ods for compound splitting. In Proceedings of
Meeting of the European Chapter of the Associ-
ation of Computational Linguistics (EACL).
Koehn, P., Och, F. J., and Marcu, D. (2003). Sta-
tistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of
the North American Chapter of the Association
for Computational Linguistics on Human Lan-
guage Technology, pages 48?54, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Marcu, D., Wang, W., Echihabi, A., and Knight,
K. (2006). SPMT: statistical machine transla-
tion with syntactified target language phrases.
In EMNLP ?06: Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 44?52, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Och, F. J. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL
?03, pages 160?167, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Petrov, S., Barrett, L., Thibaux, R., and Klein, D.
(2006). Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
of the Association for Computational Linguis-
tics, ACL-44, pages 433?440, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Schmid, H. (2004). Efficient parsing of highly am-
biguous context-free grammars with bit vectors.
In Proceedings of the 20th international con-
ference on Computational Linguistics, COL-
ING ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Skut, W., Krenn, B., Brants, T., and Uszkoreit, H.
(1997). An annotation scheme for free word or-
der languages. In Proceedings of the Fifth Con-
ference on Applied Natural Language Process-
ing (ANLP-97).
Stolcke, A. (2002). SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. Spo-
ken Language Processing, Denver, Colorado,
September 2002.
Wang, W., May, J., Knight, K., and Marcu, D.
(2010). Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation.
Comput. Linguist., 36(2):247?277.
Williams, P. and Koehn, P. (2011). Agreement
constraints for statistical machine translation
into german. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages
217?226, Edinburgh, Scotland. Association for
Computational Linguistics.
Williams, P. and Koehn, P. (2012). Ghkm rule
extraction and scope-3 parsing in moses. In
Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, pages 388?394,
Montre?al, Canada. Association for Computa-
tional Linguistics.
176
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 21?29,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
Using Feature Structures to Improve Verb Translation in
English-to-German Statistical MT
Philip Williams
?
p.j.williams-2@sms.ed.ac.uk
School of Informatics
?
University of Edinburgh
Philipp Koehn
??
pkoehn@inf.ed.ac.uk
Center for Speech and Language Processing
?
The Johns Hopkins University
Abstract
SCFG-based statistical MT models have
proven effective for modelling syntactic
aspects of translation, but still suffer prob-
lems of overgeneration. The production
of German verbal complexes is particu-
larly challenging since highly discontigu-
ous constructions must be formed con-
sistently, often from multiple independent
rules. We extend a strong SCFG-based
string-to-tree model to incorporate a rich
feature-structure based representation of
German verbal complex types and com-
pare verbal complex production against
that of the reference translations, finding a
high baseline rate of error. By developing
model features that use source-side infor-
mation to influence the production of ver-
bal complexes we are able to substantially
improve the type accuracy as compared to
the reference.
1 Introduction
Syntax-based models of statistical machine trans-
lation (SMT) are becoming increasingly compet-
itive against state-of-the-art phrase-based mod-
els, even surpassing them for some language
pairs. The incorporation of syntactic structure
has proven effective for modelling reordering phe-
nomena and improving the fluency of target out-
put, but these models still suffer from problems of
overgeneration.
One example is the production of German ver-
bal constructions. This is particularly challenging
for SMT models since highly discontiguous con-
structions must be formed consistently, often from
multiple independent rules. Whilst the model?s
.failedhaspolicythisyet
TOP-S
PUNC.
.
S-TOP
VP-OC
VVPP
fehlgeschlagen
NP-SB
NN
Strategie
PDAT
diese
VAFIN
ist
KON
doch
Figure 1: Alignment graph for a sentence pair
from the training data. The boxes indicate the
components of the target-side verbal complex: a
main verb, fehlgeschlagen, and an auxiliary, ist.
grammar may contain rules in which a complete
multi-word verb translation is captured in a single
discontiguous rule, in practice many verb transla-
tions are incompletely or inconsistently produced.
There are many routes by which ill-formed con-
structions come to be licensed by the model, none
of which is easy to address. For instance, Figure 1
shows an example from our training data in which
a missing alignment link (between has and ist) al-
lows the extraction of rules that translate has failed
to the incomplete fehlgeschlagen.
Even with perfect word alignments, the ex-
tracted rules may not include sufficient context to
ensure the overall grammaticality of a derivation.
The extent of this problem will depend partly on
the original treebank annotation style, which typi-
cally will not have been designed with translation
in mind. The problem may be further exacerbated
by errors during automatic parsing.
In this paper, we address the problem by fo-
cusing on the derivation process. We extend a
strong SCFG-based string-to-tree model to incor-
porate a rich feature-structure based representation
21
of German verbal complex types. During decod-
ing, our model composes type values for every
clause. When we compare these values against
those of the reference translations, we find a high
baseline rate of error (either incomplete or mis-
matching values). By developing model features
that use source-side information to influence the
production of verbal complexes we are able to sub-
stantially improve the type accuracy as compared
to the reference.
2 Verbal Complex Structures
Adopting the terminology of Gojun and
Fraser (2012), we use the term ?verbal com-
plex? to mean a main verb and any associated
auxiliaries within a single clause.
2.1 Feature Structures
We use feature structures to represent the under-
lying grammatical properties of German verbal
complexes. The feature structures serve two main
functions: the first is to specify a type for the
verbal complex. The types describe clause-level
properties and are defined along four dimensions:
1. tense (present, past, perfect, pluperfect, future,
future perfect), 2. voice (active, werden-passive,
sein-passive), 3. mood (indicative, subjunctive I,
subjunctive II), and 4. auxiliary modality (modal,
non-modal).
The second function is to restrict the choice of
individual word forms that are allowed to com-
bine within a given type. For example, a fea-
ture structure value for the verbal complex hat
. . . gespielt belongs to the perfect, active, indica-
tive, non-modal type. Additionally, it specifies
that for this type, the verbal complex comprises
exactly two verbs: one is a finite, indicative form
of the auxiliary haben or sein, the other is a past-
participle.
2.2 The Lexicon
Our model uses a lexicon that maps each German
verb in the target-side terminal vocabulary to a set
of features structures. Each feature structure con-
tains two top-level features: POS, a part-of-speech
feature, and VC, a verbal complex feature of the
form described above.
Since a verbal complex can comprise multiple
individual verbs, the lexicon entries include partial
VC structures. The full feature structure values are
composed through unification during decoding.
VP-OC ? ? rebuilt , wieder aufgebaut ?
? VP-OC VC ? = ? aufgebaut VC ?
? aufgebaut POS ? = VVPP
S-TOP ? ? X
1
have X
2
been X
3
,
PP-MO
1
wurde NP-SB
2
VP-OC
3
?
? S-TOP VC ? = ? wurde VC ?
? S-TOP VC ? = ? VP-OC VC ?
? wurde POS ? = VAFIN
Figure 2: SCFG rules with constraints
The lexicon?s POS values are derived from the
parse trees on the target-side of the training data.
The VC values are assigned according to POS value
from a small set of hand-written feature struc-
tures. Every main verb is assigned VC values from
one of three possible groups, selected according to
whether the verb is finite, a past-participle, or an
infinitive. For the closed class of modal and non-
modal auxiliary verbs, VC values were manually
assigned.
3 The Grammar
Our baseline translation model is learned from a
parallel corpus with automatically-derived word
alignments. In the literature, string-to-tree trans-
lation models are typically based on either syn-
chronous context-free grammars (SCFGs) (as in
Chiang et al. (2007)) or tree transducers (as in Gal-
ley et al. (2004)). In this work, we use an SCFG-
based model but our extensions are applicable in
both cases.
Following Williams and Koehn (2011), each
rule of our grammar is supplemented with
a (possibly-empty) set of PATR-II-style identi-
ties (Shieber, 1984). Figure 2 shows two example
rules with identities. The identities should be in-
terpreted as constraints that the feature structures
of the corresponding rule elements are compatible
under unification. During decoding, this imposes
a hard constraint on rule application.
3.1 Identity Extraction
The identities are learned using the following pro-
cedure:
1. The syntax of the German parse trees is used
to identify verbal complexes and label the
participating verb and clause nodes.
22
rebuiltbeenthesehaverecentlyonly
S-TOP
VP-OC
VVPP
aufgebaut
ADV
wieder
NP-SB
PDS
diese
VAFIN
wurde
PP-MO
NN
Zeit
ADJA
j?ungster
APPR
in
ADV
erst
Figure 3: Alignment graph for a sentence pair
from the training data. The target sentence has
a single verbal complex. Participating nodes are
indicated by the boxes.
2. Grammar rule extraction is extended to gen-
erate identities between VC values when an
SCFG rule contains two or more nodes from
a common verbal complex.
3. POS identities are added for terminals that ap-
pear in VC identities.
Figure 3 shows a sentence-pair from the train-
ing data with the verbal complex highlighted.
The rules in Figure 2 were extracted from this
sentence-pair.
Crucially, in step 2 of the extraction procedure
the identities can be added to SCFG rules that
cover only part of a verbal complex. For example,
the first rule of Figure 2 includes the main verb but
not the auxiliary. On application of this rule, the
partial VC value is propagated from the main verb
to the root. The second rule in Figure 2 identifies
the VC value of an auxiliary with the VC value of
a VP-OC subderivation (such as the subderivation
produced by applying the first rule).
4 Source-side Features
Since Och and Ney (2002), most SMT models
have been defined as a log-linear sum of weighted
feature functions. In this section, we define two
verbal-complex-specific feature functions. In or-
der to do so, we first describe ?clause projection,?
a simple source-syntactic restriction on decoding.
We then describe our heuristic method of obtain-
ing probability estimates for a target verbal com-
plex value given the source clause.
4.1 Clause Projection
Our feature functions assume that we have an
alignment from source-side clauses to target
clauses. In order to satisfy this requirement, we
adopt a simple restriction that declarative clauses
(both main and embedded) on the source-side
must be translated as clauses on the target-side.
This is clearly an over-simplification from a lin-
guistic perspective but it appears not to harm trans-
lation quality in practice. Table 1 shows small
gains in BLEU score over our baseline system
with this restriction.
Test Set Baseline Clause Proj.
newstest2008 15.7 15.8 (+0.1)
newstest2009 14.9 15.0 (+0.1)
newstest2010 16.5 16.8 (+0.3)
newstest2011 15.4 15.5 (+0.1)
Table 1: Results with and without clause projec-
tion (baseline tuning weights are used for clause
projection)
Clause projection is implemented as follows:
1. The input sentence is parsed and a set
of clause spans is extracted according to
the 1-best parse. We use the Berkeley
parser (Petrov and Klein, 2007), which is
trained on the Penn Treebank and so we base
our definition of a declarative clause on the
treebank annotation guidelines.
2. We modify the decoder to produce deriva-
tions in chart cells only if the cell span is
consistent with the set of clause spans (i.e.
if source span [i,j] is a clause span then no
derivation is built over span [m,n] where i <
m ? j and n > j, etc.)
3. We modify the decoder so that grammar rules
can only be applied over clause spans if they
have a clause label (?S? or ?CS?, since the
parser we use is trained on the Tiger tree-
bank).
4.2 Verbal Complex Probabilities
When translating a clause, the source-side verbal
complex will often provide sufficient information
to select a reasonable type for the target verbal
complex, or to give preferences to a few candi-
dates. By matching up source-side and target-side
verbal complexes we estimate co-occurrence fre-
quencies in the training data. To do this for all
pairs in the training data, we would need to align
clauses between the source and target training sen-
tences. However, it is not crucial that we identify
23
every last verbal complex and so we simplify the
task by restricting training data to sentence pairs in
which both source and target sentences are declar-
ative sentences, making the assumption that the
main clause of the source sentence aligns with the
main clause of the target.
We represent source-side verbal complexes
with a label that is the string of verbs and
particles and their POS tags in the order that
they occur in the clause, e.g. plays VBZ,
is addressing VBZ VBG. The target-side
feature structures are generated by identifying
verbal complex nodes in the training data parse
trees (as in Section 3.1) and then unifying the
corresponding feature structures from the lexicon.
Many source verbal complex labels exhibit a
strong co-occurrence preference for a particular
target type. For example, Table 2 shows the
three most frequent feature structure values for
the target-side clause when the source label is
is closed VBZ VBN. The most frequent value
corresponds to a non-modal, sein-passive con-
struction in the present tense and indicative mood.
RF F-Structure
0.841
?
?
?
?
?
?
FIN
[
AUX
[
LEMMA sein
MOOD indicative
TENSE present
]]
NON-FIN
[
PP/SP
[
PP
[
LEMMA
*
]
]
]
?
?
?
?
?
?
0.045
[
FIN
[
FULL
[
LEMMA sein
]
]
NON-FIN none
]
0.034
?
?
?
?
?
?
?
?
?
FIN
[
AUX
[
LEMMA werden
MOOD indicative
TENSE present
]]
NON-FIN
?
?
?
WPP
?
?
?
PP
[
LEMMA
*
]
WERDEN none
WORDEN none
SEIN none
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
. . . . . .
Table 2: Observed values and relative frequencies
(RF) for is closed, which was observed 44 times in
the training data.
4.3 Feature Functions
As with the baseline features, our verbal complex-
specific feature functions are evaluated for every
rule application r
i
of the synchronous derivation.
Like the language model feature, they are non-
local features and so cannot be pre-computed. Un-
like the baseline features, their value depends on
whether the source span that the rule is applied to
is a declarative clause or not.
Both features are defined in terms of X , the
verbal complex feature structure value of the sub-
derivation at rule application r
i
.
The first feature function, f(r
i
), uses the source
verb label, l, and the probability estimate, P (X|l),
learned from the training data:
f(r
i
) =
?
?
?
?
?
?
?
?
?
P (X|l) if r
i
covers a clause span
with verb label l
and c
l
? c
min
1 otherwise
The probability estimates are not used for scoring
if the number of training observations falls below
a threshold, c
min
. We use a threshold of 10 in ex-
periments.
The second feature function, g(r
i
), is simpler:
it penalizes the absence of a target-side finite verb
when translating a source declarative clause:
g(r
i
) =
?
?
?
?
?
exp(1) if r
i
covers a clause span
and X has no finite verb
1 otherwise
Unlike f , which requires the verb label to have
been observed a number of times during training,
g is applied to all source spans that cover a declar-
ative clause.
Dropped finite verbs are a frequent problem in
our baseline model and this feature was motivated
by an early version of the analysis presented in
Section 5.3.
5 Experiments and Analysis
In preliminary experiments, we found that changes
in translation quality resulting from our verb trans-
lation features were difficult to measure using
BLEU. In the following experiments, we mea-
sure accuracy by comparing verbal complex val-
ues against feature structures derived from the ref-
erence sentences.
5.1 Setup
Our experiments use the GHKM-based string-to-
tree pipeline implemented in Moses (Koehn et al.,
2007; Williams and Koehn, 2012). We extend a
conventional baseline model using the constraints
and feature functions described earlier.
24
Data Set Reference Baseline Hard Constraint
(MC count) F E Total F E Total F E Total
Dev 95.6% 4.4% 100.0% 86.1% 13.9% 100.0% 87.6% 12.4% 100.0%
(633) 637 29 666 545 88 633 559 79 638
Test 92.2% 7.8% 100.0% 83.5% 16.5% 100.0% 85.4% 14.6% 100.0%
(2445) 2439 206 2645 2034 403 2437 2096 359 2455
Table 3: Counts of main clause VC structures that are present and contain at least a finite verb (F) versus
those that are empty or absent (E). Declarative main clause counts (MC count) are given for each input
set. Counts for the three test sets are aggregated.
We extracted a translation grammar using all
English-German parallel data from the WMT
2012 translation task (Callison-Burch et al., 2012),
a total of 2.0M sentence pairs. We used all of the
WMT 2012 monolingual German data to train a
5-gram language model.
The baseline system uses the feature functions
described in Williams and Koehn (2012). The
feature weights were tuned on the WMT new-
stest2008 development set using MERT (Och,
2003). We use the newstest2009, newstest2010,
and newstest2011 test sets for evaluation. The de-
velopment and test sets all use a single reference.
5.2 Main Clause Verb Errors
When translating a declarative main clause, the
translation should usually also be a declarative
main clause ? that is, it should usually contain at
least a finite verb. From manually inspecting the
output it is clear that verb dropping is a common
source of translation error in our baseline system.
By making the assumption that a declarative main
clause should always be translated to a declara-
tive main clause, we can use the absence of a finite
verb as a test for translation error.
By evaluating identities, our decoder now gen-
erates a trace of verbal complex feature structures.
We obtain a reference trace by applying the same
process of verbal complex identification and fea-
ture structure unification to a parse of our refer-
ence data. Given these two traces, we compare the
presence or absence of main clause finite-verbs in
the baseline and reference.
Since we do not have alignments between the
clause nodes of the test and reference trees, we re-
strict our analysis to a simpler version of this task:
the translation of declarative input sentences that
contain only a single clause. To select test sen-
tences, we first parse the source-side of the tuning
and test sets. Filtering out sentences that are not
declarative or that contain multiple clauses leaves
633, 699, 793, and 953 input sentences for new-
stest2008, 2009, 2010, and 2011, respectively.
Our baseline system evaluates constraints in or-
der to generate a trace of feature structures but
constraint failures are allowed and hypotheses are
retained. Our hard constraint system discards all
hypotheses for which the constraints fail. The f
and g feature functions are not used in these ex-
periments.
For all main clause nodes in the output tree,
we count the number of feature structure values
that contain finite verbs and are complete versus
the number that are either incomplete or absent.
Since constraint failure results in the production
of empty feature structures, incompatible verbal
combinations do not contribute to the finite verb
total even if a finite verb is produced. We com-
pare the counts of clause nodes with empty fea-
ture structures for these two systems against those
of the reference set.
Table 3 shows total clause counts for the ref-
erence, baseline, and hard constraint system (the
?total? columns). For each system, we record how
frequently a complete feature structure containing
at least a finite verb is present (the F columns) or
not (E).
As expected, the finite verb counts for the refer-
ence translations closely match the counts for the
source sentences. The reference sets also contain
verb-less clauses (accounting for 4.4% and 7.8%
of the total clause counts for the dev and test sets).
Verb-less clauses are common in the training data
and so it is not surprising to find them in the refer-
ence sets.
Our baseline and hard constraint systems both
fail to produce complete feature structures for a
high proportion of test sentences. Table 4 shows
the proportion of single-clause declarative source
sentences for which the translation trace does not
25
include a complete feature structure. As well as
suggesting a high level of baseline failure, these
results suggest that using constraints alone is in-
sufficient.
Test set Ref. Baseline HC
newstest2008 0.0% 13.9% 11.7%
newstest2009 0.6% 18.6% 16.0%
newstest2010 0.0% 14.5% 12.5%
newstest2011 1.4% 17.4% 14.4%
Table 4: Proportion of declarative single-clause
sentences for which there is not a complete feature
structure for the translation. Ref. is the reference
and HC is our hard constraint system.
5.3 Error Classification
In order to verify that the incomplete feature struc-
tures indicate genuine translation errors and to un-
derstand the types of errors that occur, we manu-
ally check 100 sentences from our baseline system
and classify the errors. We check the verb con-
structions of the sentences containing the first 50
failures in newstest2009 and the first 50 failures in
newstest2011.
Invalid Combination (27) An ungrammatical
combination of auxiliary and main verbs.
Example: im Jahr 2007 hatte es bereits um
zwei Drittel reduziert worden .
Perfect missing aux (25) There is a past-
participle in sentence-final position, but no
auxiliary verb.
Example: der Dow Jones etwas sp?ater
wieder bereitgestellt .
False positive (14) Output is OK. In the sample
this happens either because the output string
is well-formed in terms of verb structure, but
the tree is wrong, or because the parse of the
source is wrong and the input does not actu-
ally contain a verb.
No verb (13) The input contains at least one verb
that should be translated but the output con-
tains none.
Example: der universelle Charakter der
Handy auch Nachteile .
Invalid sentence structure (13) Verbs are
present and make sense, but sentence struc-
ture is wrong
Example: die rund hunderttausend Men-
schen in Besitz von ihren eigenen Chipcard
Opencard in dieser Zeit , diese Kupon
bekommen kann .
Inf missing aux (5) There is an infinitive in
sentence-final position, but no auxiliary
verb or the main verb is erroneously in final
position (the output is likely to be ambiguous
for this error type).
Example: die Preislisten dieser Un-
ternehmen in der Regel nur ausgew?ahlte
Personen erreichen .
Unknown verb (2) The input verb is untrans-
lated.
Example: dann scurried ich auf meinem
Platz .
Werden-passive missing aux (1) There is a
werden-passive non-finite part, but no finite
auxiliary verb.
Example: die meisten ger?aumigen und
luxuri?osesten Wohnung im ersten Stock f?ur
die
?
Offentlichkeit ge?offnet worden .
In our classification, the most common individ-
ual error type in the baseline is the ungrammatical
combination of verbs, at 27 out of 100. However,
there are multiple categories that can be character-
ized as the absence of a required verb and com-
bined these total 44 out of 100 errors. There are
also some false positives and potentially mislead-
ing results in which wider syntactic errors result
in the failure to produce a feature structure, but the
majority are genuine errors. However, this method
fails to identify instances where the verbal com-
plex is grammatical but has the wrong features.
For that, we compare accuracy against reference
values.
5.4 Feature Structure Accuracy
If we had gold-standard feature structures for our
reference sets and alignments between test and ref-
erence clauses then we could evaluate accuracy by
counting the number of matches and reporting pre-
cision, recall, and F-measure values for this task.
In the absence of gold reference values, we rely
on values generated automatically from our refer-
ence sets. This requires accepting some level of
error from parsing and verb labelling (we perform
a manual analysis to estimate the degree of this
problem). We also require alignments between
26
Data Set Experiment F E g m Prec. Recall F1
Dev Baseline 545 88 637 253 46.4 39.7 42.8
f 610 48 637 312 51.1 49.0 50.0
g 600 58 637 289 48.2 45.4 46.7
f + g 627 29 637 317 50.6 49.8 50.2
Test Baseline 2034 403 2439 993 48.8 40.7 44.4
f 2370 224 2439 1214 51.2 49.8 50.5
g 2307 278 2439 1072 46.5 44.0 45.2
f + g 2437 145 2439 1225 50.3 50.2 50.2
Table 5: Feature structure accuracy for the development and test sets. As in Table 3, counts are given for
main clause VC structures that are present and contain at least a finite verb (F) versus those that are absent
or empty (E). The VC values of the output are compared against the reference values giving the number
of matches (m). The counts F, m, and g, (the number of gold reference values) are used to compute
precision, recall, and F1 values.
Input Bangladesh ex-PM is denied bail
Reference Ehemaliger Premierministerin von Bangladesch wird Kaution verwehrt
Baseline Bangladesch ex-PM ist keine Kaution
f + g Bangladesch ex-PM wird die Kaution verweigert
Input the stock exchange in Taiwan dropped by 3.6 percent according to the local index .
Reference Die B?orse in Taiwan sank nach dem dortigen Index um 3,6 Prozent .
Baseline die B?orse in Taiwan die lokalen Index entsprechend um 3,6 Prozent gesunken .
f + g die B?orse in Taiwan fiel nach Angaben der ?ortlichen Index um 3,6 Prozent .
Input the commission had been assembled at the request of Minister of Sport Miroslav Drzeviecki .
Reference Die Kommission war auf Anfrage von Sportminister Miroslaw Drzewiecki zusammengekommen.
Baseline die Kommission hatte auf Antrag der Minister f?ur Sport Miroslav Drzeviecki montiert worden .
f + g die Kommission war auf Antrag der Minister f?ur Sport Miroslav Drzeviecki versammelt .
Figure 4: Example translations where the baseline verbal complex type does not match the reference but
the f + g system does.
test and reference clauses. Here we make the same
simplification as in Section 5.2 and restrict evalu-
ation to single-clause declarative sentences.
We test the effect of the f and g features on
feature structure accuracy. Their log-linear model
weights were tuned by running a line search to
optimize the F1 score on a subset of the new-
stest2008 dev set containing sentences up to 30
tokens in length (all baseline weights were fixed).
For the experiments in which both features are
used, we first tune the weight for f and then tune
g with the f weight fixed.
Table 5 reports feature structure accuracy for the
development and test sets. On the test set, the indi-
vidual f and g features both improve the F1 score.
f is effective in terms of both precision and recall,
but the g feature degrades precision compared to
the baseline. Using both features appears to offer
little benefit beyond using f alone.
Compared with the baseline or using hard con-
straints alone (Table 3), the proportion of sen-
tences with incomplete or inconsistent verbal
complex values (column E) is substantially re-
duced by the f and g feature functions.
To estimate the false match rate, we manually
checked the first 50 sentences from the 2009 test
set in which one system was reported to agree with
reference and the other not:
37/50 Verb constructions are grammatical. We
agree with comparisons against the reference
value.
9/50 Verb constructions are grammatical. We
agree with the comparison for the test system but
not the baseline.
4/50 Verb constructions are ungrammatical or
difficult to interpret in both baseline and test.
Figure 4 shows some example translations from
our system.
27
5.5 BLEU
Finally, we report BLEU scores for two versions
of our dev and test sets: in addition to the full
data sets (Table 6), we use sub-sets that contain
all source sentences up to 30 tokens in length (Ta-
ble 7). There are two reasons for this: first, we
expect shorter sentences to use simpler sentence
structure with less coordination and fewer relative
and subordinate clauses. All else being equal, we
expect to see a greater degree of high-level struc-
tural divergence between complex source and tar-
get sentence structures than between simple ones.
We therefore anticipate that our naive clause pro-
jection strategy is more likely to break down on
long sentences. Second, we expect the effects on
BLEU score to become diluted as sentence length
increases, for the simple reason that verbs are
likely to account for a smaller proportion of the
total number of words (though this effect seems to
be small: in a parse of the newstest2009-30 subset,
verbs account for 14.2% of tokens; in the full set
they account for 13.1%). We find that the change
in BLEU is larger for the constrained test sets, but
only slightly.
Experiment 2008 2009 2010 2011
baseline 15.7 14.9 16.5 15.4
f 15.8 15.0 16.9 15.5
g 15.9 15.1 16.9 15.6
f + g 15.8 15.0 16.9 15.6
Table 6: BLEU scores for full dev/test sets
Experiment 2008 2009 2010 2011
baseline 16.1 15.7 16.3 15.1
f 16.2 15.8 16.9 15.3
g 16.4 15.9 16.9 15.4
f + g 16.3 15.9 16.9 15.4
Table 7: BLEU scores for constrained dev/test sets
(max. 30 tokens)
6 Related Work
The problem of verbal complex translation in
English-to-German is tackled by Gojun and
Fraser (2012) in the context of phrase-based
SMT. They overcome the reordering limitation of
phrase-based SMT by preprocessing the source-
side of the training and test data to move En-
glish verbs within clauses into more ?German-like?
positions. In contrast, our SCFG-based baseline
model does not place any restriction on reordering
distance.
Arora and Mahesh (2012) address a similar
problem in English-Hindi translation. They im-
prove a phrase-based model by merging verbs and
associated particles into single tokens, thus simpli-
fying the task of word alignment and phrase-pair
extraction. Their approach relies upon the mostly-
contiguous nature of English and Hindi verbal
complexes. The discontiguity of verbal complexes
rules out this approach for translation into Ger-
man.
Our model adopts a similar constraint-based ex-
tension of SCFG to that described in Williams and
Koehn (2011). In that work, constraints are used to
enforce target-side agreement between nouns and
modifiers and between subjects and verbs. Whilst
that constraint model operates purely on the target-
side, our verbal complex feature functions also
take source-side information into account.
7 Conclusion
We have presented a model in which a conven-
tional SCFG-based string-to-tree system is ex-
tended with a rich feature-structure based repre-
sentation of German verbal complexes, a gram-
matical construction that is difficult for an SMT
model to produce correctly. Our feature struc-
ture representation enabled us to easily identify
where our baseline model made errors and pro-
vided a means to measure accuracy against the ref-
erence translations. By developing feature func-
tions that use source-side information to influence
verbal complex formation we were able to im-
prove translation quality, measured both in terms
of BLEU score where there were small, consis-
tent gains across the test sets, and in terms of task-
specific accuracy.
In future work we intend to explore the use
of richer models for predicting target-side verbal
complex types. For example, discriminative mod-
els that include non-verbal source features.
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment 287658 (EU-BRIDGE).
28
References
Karunesh Kumar Arora and R.Mahesh K. Sinha. 2012.
Improving statistical machine translation through
co-joining parts of verbal constructs in english-hindi
translation. In Proceedings of the Sixth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 95?101, Jeju, Republic of Korea, July.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL ?04.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of german verbs in english?to?
german smt. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, pages 726?735, Avi-
gnon, France, April. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Morristown, NJ, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 295?302, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Stuart M. Shieber. 1984. The design of a computer lan-
guage for linguistic information. In Proceedings of
the 10th international conference on Computational
linguistics, COLING ?84, pages 362?366, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Philip Williams and Philipp Koehn. 2011. Agree-
ment constraints for statistical machine translation
into german. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 217?226,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Philip Williams and Philipp Koehn. 2012. Ghkm
rule extraction and scope-3 parsing in moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 388?394, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
29
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207?214,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Edinburgh?s Syntax-Based Systems at WMT 2014
Philip Williams
1
, Rico Sennrich
1
, Maria Nadejde
1
,
Matthias Huck
1
, Eva Hasler
1
, Philipp Koehn
1,2
1
School of Informatics, University of Edinburgh
2
Center for Speech and Language Processing, The Johns Hopkins University
Abstract
This paper describes the string-to-tree sys-
tems built at the University of Edin-
burgh for the WMT 2014 shared trans-
lation task. We developed systems for
English-German, Czech-English, French-
English, German-English, Hindi-English,
and Russian-English. This year we
improved our English-German system
through target-side compound splitting,
morphosyntactic constraints, and refine-
ments to parse tree annotation; we ad-
dressed the out-of-vocabulary problem us-
ing transliteration for Hindi and Rus-
sian and using morphological reduction
for Russian; we improved our German-
English system through tree binarization;
and we reduced system development time
by filtering the tuning sets.
1 Introduction
For this year?s WMT shared translation task we
built syntax-based systems for six language pairs:
? English-German ? German-English
? Czech-English ? Hindi-English
? French-English ? Russian-English
As last year (Nadejde et al., 2013), our systems are
based on the string-to-tree pipeline implemented
in the Moses toolkit (Koehn et al., 2007).
We paid particular attention to the production of
grammatical German, trying various parsers and
incorporating target-side compound splitting and
morphosyntactic constraints; for Hindi and Rus-
sian, we employed the new Moses transliteration
model to handle out-of-vocabulary words; and for
German to English, we experimented with tree bi-
narization, obtaining good results from right bina-
rization.
We also present our first syntax-based results
for French-English, the scale of which defeated us
last year. This year we were able to train a sys-
tem using all available training data, a task that
was made considerably easier through principled
filtering of the tuning set. Although our system
was not ready in time for human evaluation, we
present BLEU scores in this paper.
In addition to the five single-system submis-
sions described here, we also contributed our
English-German and German-English systems for
use in the collaborative EU-BRIDGE system com-
bination effort (Freitag et al., 2014).
This paper is organised as follows. In Sec-
tion 2 we describe the core setup that is com-
mon to all systems. In subsequent sections we de-
scribe language-pair specific variations and exten-
sions. For each language pair, we present results
for both the development test set (newstest2013
in most cases) and for the filtered test set (new-
stest2014) that was provided after the system sub-
mission deadline. We refer to these as ?devtest?
and ?test?, respectively.
2 System Overview
2.1 Pre-processing
The training data was normalized using the WMT
normalize-punctuation.perl script then
tokenized and truecased. Where the target lan-
guage was English, we used the Moses tokenizer?s
-penn option, which uses a tokenization scheme
that more closely matches that of the parser. For
the English-German system we used the default
Moses tokenization scheme, which is similar to
that of the German parsers.
For the systems that translate into English, we
used the Berkeley parser (Petrov et al., 2006;
Petrov and Klein, 2007) to parse the target-side of
the training corpus. As we will describe in Sec-
tion 3, we tried a variety of parsers for German.
We did not perform any corpus filtering other
than the standard Moses method, which removes
207
sentence pairs with dubious length ratios and sen-
tence pairs where parsing fails for the target-side
sentence.
2.2 Translation Model
Our translation grammar is a synchronous context-
free grammar (SCFG) with phrase-structure labels
on the target side and the generic non-terminal la-
bel X on the source side.
The grammar was extracted from the word-
aligned parallel data using the Moses implemen-
tation (Williams and Koehn, 2012) of the GHKM
algorithm (Galley et al., 2004; Galley et al., 2006).
For word alignment we used MGIZA++ (Gao and
Vogel, 2008), a multi-threaded implementation of
GIZA++ (Och and Ney, 2003).
Minimal GHKM rules were composed into
larger rules subject to parameterized restrictions
on size defined in terms of the resulting target tree
fragment. A good choice of parameter settings
depends on the annotation style of the target-side
parse trees. We used the settings shown in Table 1,
which were chosen empirically during the devel-
opment of last years? systems:
Parameter Value
Rule depth 5
Node count 20
Rule size 5
Table 1: Parameter settings for rule composition.
Further to the restrictions on rule composition,
fully non-lexical unary rules were eliminated us-
ing the method described in Chung et al. (2011)
and rules with scope greater than 3 (Hopkins and
Langmead, 2010) were pruned from the trans-
lation grammar. Scope pruning makes parsing
tractable without the need for grammar binariza-
tion.
2.3 Language Model
We used all available monolingual data to train
5-gram language models. Language models
for each monolingual corpus were trained using
the SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) and then interpolated using weights tuned to
minimize perplexity on the development set.
2.4 Feature Functions
Our feature functions are unchanged from the pre-
vious two years. They include the n-gram lan-
guage model probability of the derivation?s target
yield, its word count, and various scores for the
synchronous derivation.
Each grammar rule has a number of pre-
computed scores. For a grammar rule r of the form
C ? ??, ?,??
where C is a target-side non-terminal label, ? is a
string of source terminals and non-terminals, ? is
a string of target terminals and non-terminals, and
? is a one-to-one correspondence between source
and target non-terminals, we score the rule accord-
ing to the following functions:
? p (C, ? | ?,?) and p (? | C, ?,?), the direct
and indirect translation probabilities.
? p
lex
(? | ?) and p
lex
(? | ?), the direct and
indirect lexical weights (Koehn et al., 2003).
? p
pcfg
(pi), the monolingual PCFG probability
of the tree fragment pi from which the rule
was extracted.
? exp(?1/count(r)), a rule rareness penalty.
? exp(1), a rule penalty. The main grammar
and glue grammars have distinct penalty fea-
tures.
2.5 Tuning
The feature weights were tuned using the Moses
implementation of MERT (Och, 2003) for all sys-
tems except English-to-German, for which we
used k-best MIRA (Cherry and Foster, 2012) due
to the larger number of features.
We used tuning sentences drawn from all of
the previous years? test sets (except newstest2013,
which was used as the development test set). In
order to speed up the tuning process, we used sub-
sets of the full tuning sets with sentence pairs up
to length 30 (Max-30) and further applied a fil-
tering technique to reduce the tuning set size to
2,000 sentence pairs for the language pairs involv-
ing German, French and Czech
1
. We also experi-
mented with random subsets of size 2,000.
For the filtering technique, we make the as-
sumption that finding suitable weights for all the
feature functions requires the optimizer to see a
range of feature values and to see hypotheses that
can partially match the reference translations in
order to rank the hypotheses. For example, if a
1
For Russian and Hindi, the development sets are smaller
and no filtering was applied.
208
tuning example contains many out-of-vocabulary
words or is difficult to translate for other reasons,
this will result in low quality translation hypothe-
ses and provide the system with little evidence for
which features are useful to produce good transla-
tions. Therefore, we select high quality examples
using a smooth version of sentence-BLEU com-
puted on the 1-best output of a single decoder run
on the development set. Standard sentence-BLEU
tends to select short examples because they are
more likely to have perfect n-gram matches with
the reference translation. Very short sentence pairs
are less informative for tuning but also tend to have
more extreme source-target length ratios which
can affect the weight of the word penalty. Thus,
we penalize short examples by padding the de-
coder output with a fixed number of non-matching
tokens
2
to the left and right before computing
sentence-BLEU. This has the effect of reducing
the precision of short sentences against the refer-
ence translation while affecting longer sentences
proportionally less. Experiments on phrase-based
systems have shown that the resulting tuning sets
are of comparable diversity as randomly selected
sets in terms of their feature vectors and maintain
BLEU scores in comparison with tuning on the en-
tire development set.
Table 2 shows the size of the full tuning sets
and the size of the subsets with up to length 30,
Table 3 shows the results of tuning with different
sets. Reducing the tuning sets to Max-30 results
in a speed-up in tuning time but affects the per-
formance on some of the devtest/test sets (mostly
for Czech-English). However, tuning on the full
set took more than 18 days using 12 cores for
German-English which is not feasible when try-
ing out several model variations. Further filter-
ing these subsets to a size of 2,000 sentence pairs
as described above maintains the BLEU scores in
most cases and even improves the scores in some
cases. This indicates that the quality of the se-
lected examples is more important than the total
number of tuning examples. However, the exper-
iments with random subsets from Max-30 show
that random selection also yields results which im-
prove over the results with Max-30 in most cases,
though are not always as good as with the filtered
sets.
3
The filtered tuning sets yield reasonable per-
2
These can be arbitrary tokens that do not match any ref-
erence token.
3
For random subsets from the full tuning set the perfor-
mance was similar but resulted in standard deviations of up
formance compared to the full tuning sets except
for the German-English devtest set where perfor-
mance drops by 0.5 BLEU
4
.
Tuning set Cs-En En-De De-En
Full 13,055 13,071 13,071
Max-30 10,392 9,151 10,610
Table 2: Size of full tuning sets and with sentence
length up to 30.
devtest
Tuning set Cs-En En-De De-En
Full 25.1 19.9 26.7
Max-30 24.7 19.8 26.2
Filtered 24.9 19.8 26.2
Random 24.8 19.7 26.4
test
Tuning set Cs-En En-De De-En
Full 27.5 19.2 26.9
Max-30 27.2 19.2 27.0
Filtered 27.5 19.1 27.2
Random 27.3 19.4 27.0
Table 3: BLEU results on devtest and test sets with
different tuning sets: Full, Max-30, filtered subsets
of Max-30 and average of three random subsets of
Max-30 (size of filtered/random subsets: 2,000).
3 English to German
We use the projective output of the dependency
parser ParZu (Sennrich et al., 2013) for the syn-
tactic annotation of our primary submission. Con-
trastive systems were built with other parsers: Bit-
Par (Schmid, 2004), the German Stanford Parser
(Rafferty and Manning, 2008), and the German
Berkeley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
The set of syntactic labels provided by ParZu
has been refined to reduce overgeneralization phe-
nomena. Specifically, we disambiguate the labels
ROOT (used for the root of a sentence, but also
commas, punctuation marks, and sentence frag-
ments), KON and CJ (coordinations of different
constituents), and GMOD (pre- or postmodifying
genitive modifier).
to 0.36 across three random sets.
4
Note however that due to the long tuning times, we are
reporting single tuning runs.
209
NN
SEGMENT
gericht
COMP
JUNC
@s@
SEGMENT
berufung
COMP
JUNC
@es@
SEGMENT
Bund
Figure 1: Syntactic representation of split com-
pound Bundesberufungsgericht (Engl: federal ap-
peals court).
We discriminatively learn non-terminal labels
for unknown words using sparse features, rather
than estimating a probability distribution of non-
terminal labels from singleton statistics in the
training corpus.
We perform target-side compound splitting, us-
ing a hybrid method described by Fritzinger and
Fraser (2010) that combines a finite-state mor-
phology and corpus statistics. As finite-state mor-
phology analyzer, we use Zmorge (Sennrich and
Kunz, 2014). An original contribution of our
experiments is a syntactic representation of split
compounds which eliminates typical problems
with target-side compound splitting, namely er-
roneous reorderings and compound merging. We
represent split compounds as a syntactic tree with
the last segment as head, preceded by a modifier.
A modifier consists of an optional modifier, a seg-
ment and a (possibly empty) joining element. An
example is shown in Figure 1. This hierarchical
representation ensures that compounds can be eas-
ily merged in post-processing (by removing the
spaces and special characters around joining ele-
ments), and that no segments are placed outside of
a compound in the translation.
We use unification-based constraints to model
morphological agreement within German noun
phrases, and between subjects and verbs (Williams
and Koehn, 2011). Additionally, we add con-
straints that operate on the internal tree structure of
the translation hypotheses, to enforce several syn-
tactic constraints that were frequently violated in
the baseline system:
? correct subcategorization of auxiliary/modal
verbs in regards to the inflection of the full
verb.
? passive clauses are not allowed to have ac-
cusative objects.
system
BLEU
devtest test
Stanford Parser 19.0 18.3
Berkeley Parser 19.3 18.6
BitPar 19.5 18.6
ParZu 19.6 19.1
+ modified label set 19.8 19.1
+ discriminative UNK weights 19.9 19.2
+ German compound splitting 20.0 19.8
+ grammatical constraints 20.2 20.1
Table 4: English to German translation results
on devtest (newstest2013) and test (newstest2014)
sets.
? relative clauses must contain a relative (or in-
terrogative) pronoun in their first constituent.
Table 4 shows BLEU scores with systems
trained with different parsers, and for our exten-
sions of the baseline system.
4 Czech to English
For Czech to English we used the core setup de-
scribed in Section 2 without modification. Table 5
shows the BLEU scores.
BLEU
system devtest test
baseline 24.8 27.0
Table 5: Czech to English results on the devtest
(newstest2013) and test (newstest2014) sets.
5 French to English
For French to English, alignment of the parallel
corpus was performed using fast_align (Dyer et
al., 2013) instead of MGIZA++ due to the large
volume of parallel data.
Table 6 shows BLEU scores for the system and
Table 7 shows the resulting grammar sizes after
filtering for the evaluation sets.
BLEU
system devtest test
baseline 29.4 32.3
Table 6: French to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
210
system devtest test
baseline 86,341,766 88,657,327
Table 7: Grammar sizes of the French to En-
glish system after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
6 German to English
German compounds were split using the script
provided with Moses.
For training the primary system, the target parse
trees were restructured before rule extraction by
right binarization. Since binarization strategies
increase the tree depth and number of nodes by
adding virtual non-terminals, we increased the ex-
traction parameters to: Rule Depth = 7, Node
Count = 100, Rule Size = 7. A thorough in-
vestigation of binarization methods for restructur-
ing Penn Treebank style trees was carried out by
Wang et al. (2007).
Table 8 shows BLEU scores for the baseline
system and two systems employing different bi-
narization strategies. Table 9 shows the result-
ing grammar sizes after filtering for the evaluation
sets. Results on the development set showed no
improvement when left binarization was used for
restructuring the trees, although the grammar size
increased significantly.
BLEU
system devtest test
baseline 26.2 27.2
+ right binarization (primary) 26.8 28.2
+ left binarization 26.3 -
Table 8: German to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
system devtest test
baseline 11,462,976 13,811,304
+ right binarization 24,851,982 29,133,910
+ left binarization 21,387,976 -
Table 9: Grammar sizes of the German to En-
glish systems after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
7 Hindi to English
English-Hindi has the least parallel training data
of this year?s language pairs. Out-of-vocabulary
(OOV) input words are therefore a comparatively
large source of translation error: in the devtest set
(newsdev2014) and filtered test set (newstest2014)
the average OOV rates are 1.08 and 1.16 unknown
words per sentence, respectively.
Assuming a significant fraction of OOV words
to be named entities and thus amenable to translit-
eration, we applied the post-processing translitera-
tion method described in Durrani et al. (2014) and
implemented in Moses. In brief, this is an unsuper-
vised method that i) uses EM to induce a corpus of
transliteration examples from the parallel training
data; ii) learns a monotone character-level phrase-
based SMT model from the transliteration corpus;
and iii) substitutes transliterations for OOVs in the
system output by using the monolingual language
model and other features to select between translit-
eration candidates.
5
Table 10 shows BLEU scores with and without
transliteration on the devtest and filtered test sets.
Due to a bug in the submitted system, the language
model trained on the HindEnCorp corpus was used
for transliteration candidate selection rather than
the full interpolated language model. This was
fixed subsequent to submission.
BLEU
system devtest test
baseline 12.9 14.7
+ transliteration (submission) 13.3 15.1
+ transliteration (fixed) 13.6 15.5
Table 10: Hindi to English results with and with-
out transliteration on the devtest (newsdev2014)
and test (newstest2014) sets.
Transliteration increased 1-gram precision from
48.1% to 49.4% for devtest and from 49.1% to
50.6% for test. Of the 2,913 OOV words in test,
938 (32.2%) of transliterations exactly match the
reference. Manual inspection reveals that there are
also many near matches. For instance, translitera-
tion produces Bernat Jackie where the reference is
Jacqui Barnat.
8 Russian to English
Compared to Hindi-English, the Russian-English
language pair has over six times as much parallel
data. Nonetheless, OOVs remain a problem: the
average OOV rates are approximately half those
5
This is the variant referred to as Method 2 in Dur-
rani et al. (2014).
211
of Hindi-English, at 0.47 and 0.51 unknown words
per sentence for the devtest (newstest2013) and fil-
tered test (newstest2014) sets, respectively. We
address this in part using the same transliteration
method as for Hindi-English.
Data sparsity issues for this language pair are
exacerbated by the rich inflectional morphology of
Russian. Many Russian word forms express gram-
matical distinctions that are either absent from En-
glish translations (like grammatical gender) or are
expressed by different means (like grammatical
function being expressed through syntactic config-
uration rather than case). We adopt the widely-
used approach of simplifying morphologically-
complex source forms to remove distinctions that
we believe to be redundant. Our method is simi-
lar to that of Weller et al. (2013) except that ours
is much more conservative (in their experiments,
Weller et al. (2013) found morphological reduc-
tion to harm translation indicating that useful in-
formation was likely to have been discarded).
We used TreeTagger (Schmid, 1994) to obtain
a lemma-tag pair for each Russian word. The tag
specifies the word class and various morphosyn-
tactic feature values. For example, the adjective
??????????????? (?republican?) gets the lemma-
tag pair ??????????????? + Afpfsnf, where
the code A indicates the word class and the re-
maining codes indicate values for the type, degree,
gender, number, case, and definiteness features.
Like Weller et al. (2013), we selectively re-
placed surface forms with their lemmas and re-
duced tags, reducing tags through feature dele-
tion. We restricted morphological reduction to ad-
jectives and verbs, leaving all other word forms
unchanged. Table 11 shows the features that
were deleted. We focused on contextual inflec-
tion, making the assumption that inflectional dis-
tinctions required by agreement alone were the
least likely to be useful for translation (since the
same information was marked elsewhere in the
sentence) and also the most likely to be the source
of ?spurious? variation.
Table 12 shows the BLEU scores for Russian-
English with transliteration and morphological re-
duction. The effect of transliteration was smaller
than for Hindi-English, as might be expected from
the lower baseline OOV rate. 1-gram precision in-
creased from 57.1% to 57.6% for devtest and from
62.9% to 63.6% for test. Morphological reduction
decreased the initial OOV rates by 3.5% and 4.1%
Adjective Verb
Type 7 Type 7
Degree 3 VForm 3
Gender 7 Tense 3
Number 7 Person 3
Case 7 Number 3
Definiteness 7 Gender 7
Voice 3
Definiteness 7
Aspect 3
Case 3
Table 11: Feature values that are retained (3)
or deleted (7) during morphological reduction of
Russian.
BLEU
system devtest test
baseline 23.3 29.7
+ transliteration 23.7 30.3
+ morphological reduction 23.8 30.3
Table 12: Russian to English results on the devtest
(newstest2013) and test (newstest2014) sets.
on the devtest and filtered test sets. After both
morphological and transliteration the 1-gram pre-
cisions for devtest and test were 57.7% and 63.8%.
9 Conclusion
We have described Edinburgh?s syntax-based sys-
tems in the WMT 2014 shared translation task.
Building upon the already-strong string-to-tree
systems developed for previous years? shared
translation tasks, we have achieved substantial im-
provements over our baseline setup: we improved
translation into German through target-side com-
pound splitting, morphosyntactic constraints, and
refinements to parse tree annotation; we have ad-
dressed unknown words using transliteration (for
Hindi and Russian) and morphological reduction
(for Russian); and we have improved our German-
English system through tree binarization.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658 (EU-BRIDGE).
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1_148717.
212
References
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montr?al, Canada, June. Association for
Computational Linguistics.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 413?417, Portland, Oregon, USA, June.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an Unsupervised Translit-
eration Model into Statistical Machine Translation.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the ACL (EACL 2014), Gothenburg,
Sweden, April. To appear.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In In Proc. NAACL/HLT 2013,
pages 644?648.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, Teresa Herrmann, Eunah Cho, and Alex
Waibel. 2014. EU-BRIDGE MT: Combined Ma-
chine Translation. In Proceedings of the ACL 2014
Ninth Workshop on Statistical Machine Translation,
Baltimore, MD, USA, June.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 224?234, Uppsala,
Sweden.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In HLT-NAACL ?04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL-
44: Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational
Linguistics, pages 961?968, Morristown, NJ, USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646?655, Cambridge,
MA, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Morristown, NJ, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh?s Syntax-Based Machine Transla-
tion Systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170?
176, Sofia, Bulgaria, August.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, ACL ?03, pages
160?167, Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
213
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich and Beat Kunz. 2014. Zmorge: A Ger-
man Morphological Lexicon Extracted from Wik-
tionary. In Proceedings of the 9th International
Conference on Language Resources and Evaluation
(LREC 2014), Reykjavik, Iceland, May.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Intl. Conf. Spoken
Language Processing, Denver, Colorado, September
2002.
Wei Wang, Kevin Knight, Daniel Marcu, and Marina
Rey. 2007. Binarizing Syntax Trees to Improve
Syntax-Based Machine Translation Accuracy. In
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 746?754.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Rich?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions at WMT13: Mor-
phological and syntactic processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 232?239, Sofia, Bul-
garia, August.
Philip Williams and Philipp Koehn. 2011. Agreement
Constraints for Statistical Machine Translation into
German. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 217?226, Ed-
inburgh, Scotland, July.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 388?394, Montr?al,
Canada, June.
214
