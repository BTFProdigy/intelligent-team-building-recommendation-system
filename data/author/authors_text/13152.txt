Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 206?214,
Beijing, August 2010
Simplicity is Better: Revisiting Single Kernel PPI Extraction 
Sung-Pil Choi 
Information Technology Laboratory 
Korea Institute of Science and Technol-
ogy Information 
spchoi@kisti.re.kr 
Sung-Hyon Myaeng 
Department of Computer Science 
Korea Advanced Institute of Science and 
Technology 
myaeng@kaist.ac.kr 
 
Abstract 
It has been known that a combination of 
multiple kernels and addition of various 
resources are the best options for im-
proving effectiveness of kernel-based 
PPI extraction methods. These supple-
ments, however, involve extensive ker-
nel adaptation and feature selection 
processes, which attenuate the original 
benefits of the kernel methods. This pa-
per shows that we are able to achieve 
the best performance among the state-
of-the-art methods by using only a sin-
gle kernel, convolution parse tree kernel. 
In-depth analyses of the kernel reveal 
that the keys to the improvement are the 
tree pruning method and consideration 
of tree kernel decay factors. It is note-
worthy that we obtained the perfor-
mance without having to use any addi-
tional features, kernels or corpora. 
1 Introduction 
Protein-Protein Interaction (PPI) Extraction 
refers to an automatic extraction of the interac-
tions between multiple protein names from nat-
ural language sentences using linguistic features 
such as lexical clues and syntactic structures. A 
sentence may contain multiple protein names 
and relations, i.e., multiple PPIs. For example, 
the sentence in Fig.1 contains a total of six pro-
tein names of varying word lengths and three 
explicit interactions (relations). The interaction 
type between phosphoprotein and the acronym 
P in the parentheses is ?EQUAL.? A longer pro-
tein name phosphoprotein of vesicular stomati-
tis virus is related to nucleocapsid protein via 
?INTERACT? relation. Like the first PPI, nuc-
leocapsid protein is equivalent to the abbre-
viated term N.  
It is not straightforward to extract PPIs from 
a sentence or textual segment. There may be 
multiple protein names and their relationships, 
which are intertwined in a sentence. An interac-
tion type may be expressed in a number of dif-
ferent ways.  
 
Figure 1. An example sentence containing mul-
tiple PPIs involving different names of varying 
scopes and relations1  
 
A significant amount of efforts have been 
devoted to kernel-based approaches to PPI ex-
tractions (PPIE) as well as relation extractions2 
(Zhang et al, 2006; Pyysalo et al, 2008; Guo-
Dong et al, 2007; Zhang et al, 2008; Airola et 
al., 2008; Miwa et al, 2009). They include 
word feature kernels, parse tree kernels, and 
graph kernels. One of the benefits of using a 
kernel method is that it can keep the original 
                                                 
1 BioInfer, Sentence ID:BioInfer.d10.s0 
2 Relation extraction has been studied massively with the 
help of the ACE (www.nist.gov/tac) competition work-
shop and its corpora. The ACE corpora contain valuable 
information showing the traits of target entities (e.g., ent-
ity types, roles) for relation extraction in single sentences. 
Since all target entities are of the same type, protein 
name, in PPIE, however, we cannot use relational infor-
mation that exists among entity types. This makes PPIE 
more challenging.  
206
formation of target objects such as parse trees, 
not requiring extensive feature engineering for 
learning algorithms (Zelenko et al, 2003).  
In an effort to improve the performance of 
PPIE, researchers have developed not only new 
kernels but also methods for combining them 
(GuoDong et al, 2007; Zhang et al, 2008; Air-
ola et al, 2008; Miwa et al, 2009a; Miwa et al, 
2009b). While the intricate ways of combing 
various kernels and using extra resources have 
played the role of establishing strong baseline 
performance for PPIE, however, they are 
viewed as another form of engineering efforts. 
After all, one of the reasons the kernel methods 
have become popular is to avoid such engineer-
ing efforts. 
Instead, we focus on a state-of-the-art kernel 
and investigate how it can be best utilized for 
enhanced performance. We show that even with 
a single kernel, convolution parse tree kernel in 
this case, we can achieve superior performance 
in PPIE by devising an appropriate preprocess-
ing and factor adjustment method. The keys to 
the improvement are tree pruning and consider-
ation of a tree kernel decay factor, which are 
independent of the machine learning model 
used in this paper. The main contribution of our 
work is the extension and application of the 
particular convolution tree kernel method for 
PPIE, which gives a lesson that a deep analysis 
and a subsequent extension of a kernel for max-
imal performance can override the gains ob-
tained from engineering additional features or 
combining other kernels. 
The remaining part of the paper is organized 
as follows. In section 2, we survey the existing 
approaches. Section 3 introduces the parse tree 
kernel model and its algorithm. Section 4 ex-
plains the performance improving factors ap-
plied to the parse tree kernel. The architecture 
of our system is introduced in section 5. Section 
6 shows the improvements in effectiveness in 
multiple PPI corpora and finally we conclude 
our work in section 7. 
2 Related Work 
In recent years, numerous studies have at-
tempted to extract PPI automatically from text. 
Zhou and He (2008) classified various PPIE 
approaches into three categories: linguistic, 
rule-based and machine learning and statistical 
methods. 
Linguistic approaches involve constructing 
special grammars capable of syntactically ex-
pressing the interactions in sentences and then 
applying them to the language analyzers such as 
part-of-speech taggers, chunkers and parsers to 
extract PPIs. Based on the level of linguistic 
analyses, we can divide the linguistic approach-
es into two categories: shallow parsing (Seki-
mizu et al, 1998; Gondy et al, 2003) and full 
parsing methods (Temkin & Gilder, 2003; Ni-
kolai et al, 2004). 
Rule-based approaches use manually defined 
sets of lexical patterns and find text segments 
that match the patterns. Blaschke et al (1996) 
built a set of lexical rules based on clue words 
denoting interactions. Ono et al (2001) defined 
a group of lexical and syntactic interaction pat-
terns, embracing negative expressions, and ap-
plied them to extract PPIs from documents 
about ?Saccharomyces cerevisiae? and ?Esche-
richia coli?. Recently, Fundel et al (2007) pro-
posed a PPI extraction model based on more 
systematic rules using a dependency parser.  
Machine learning and statistical approaches 
have been around for a while but have recently 
become a dominant approach for PPI extraction. 
These methods involve building supervised or 
semi-supervised models based on training sets 
and various feature extraction methods (An-
drade & Valencia, 1998; Marcotte et al, 2001; 
Craven & Kumlien, 1999). Among them, ker-
nel-based methods have been studied extensive-
ly in recent years. Airola et al (2008) attempted 
to extract PPIs using a graph kernel by convert-
ing dependency parse trees into the correspond-
ing dependency graphs.  
Miwa et al (2009a) utilized multiple kernels 
such as word feature kernels, parse tree kernels, 
and even graph kernels in order to improve the 
performance of PPI extraction. Their experi-
ments based on five PPI corpora, however, 
showed that combining multiple kernels gave 
only minor improvements compared to other 
methods. To further improve the performance 
of the multiple kernel system, the same group 
combined multiple corpora to exploit additional 
features for a modified SVM model (Miwa et 
al., 2009b). While they achieved the best per-
formance in PPI extraction, it was possible only 
207
with additional kernels and corpora from which 
additional features were extracted.  
Unlike the aforementioned approaches trying 
to use all possible resources for performance 
enhancement, this paper aims at maximizing the 
performance of PPIE using only a single kernel 
without any additional resources. Without lo-
wering the performance, we attempt to stick to 
the initial benefits of the kernel methods: sim-
plicity and modularity (Shawe-Taylor & Cris-
tianini, 2004).  
3 Convolution Parse Tree Kernel 
Model for PPIE 
The main idea of a convolution parse tree ker-
nel is to sever a parse tree into its sub-trees and 
transfer it as a point in a vector space in which 
each axis denotes a particular sub-tree in the 
entire set of parse trees. If this set contains M 
unique sub-trees, the vector space becomes M-
dimensional. The similarity between two parse 
trees can be obtained by computing the inner 
product of the two corresponding vectors, 
which is the output of the parse tree kernel. 
There are two types of parse tree kernels of 
different forms of sub-trees: one is SubTree 
Kernel (STK) proposed by Vishwanathan and 
Smola (2003), and the other is SubSet Tree 
Kernel (SSTK) developed by Collins and Duffy 
(2001). In STK, each sub-tree should be a com-
plete tree rooted by a specific node in the entire 
tree and ended with leaf nodes. All the sub-trees 
must obey the production rules of the syntactic 
grammar. Meanwhile, SSTK can have any 
forms of sub-trees in the entire parse tree given 
that they should obey the production rules. It 
was shown that SSTK is much superior to STK 
in many tasks (Moschitti, 2006). He also intro-
duced a fast algorithm for computing a parse 
tree kernel and showed its beneficial effects on 
the semantic role labeling problem.  
A parse tree kernel can be computed by the 
following equation: 
             
                                             (1) 
where Ti is i
th parse tree and n1 and n2 are nodes 
in NT, the set of the entire nodes of T. ? 
represents a tree kernel decay factor, which will 
be explained later, and ? decides the way the 
tree is severed. Finally ?(n1, n2, ?, ?) counts the 
number of the common sub-trees of the two 
parse trees rooted by n1 and n2. Figure 2 shows 
the algorithm. 
In this algorithm, the get_children_number 
function returns the number of the direct child 
nodes of the current node in a tree. The function 
named get_node_value gives the value of a 
node such as part-of-speeches, phrase tags and 
words. The get_production_rule function finds 
the grammatical rule of the current node and its 
children by inspecting their relationship. 
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
FUNCTION delta(TreeNode n1, TreeNode n2, ?, ?) 
n1 = one node of T1;  n2 = one node of T2; 
? = tree kernel decay factor;  ? = tree division me-
thod; 
BEGIN 
nc1 = get_children_number(n1);   
nc2 = get_children_number(n2); 
IF nc1 EQUAL 0 AND nc2 EQUAL 0 THEN     
nv1 = get_node_value(n1);   
nv2 = get_node_value(n2);  
IF nv1 EQUAL nv2 THEN RETURN 1; 
ENDIF 
np1 = get_production_rule(n1);   
np2 = get_production_rule(n2); 
IF np1 NOT EQUAL np2 THEN RETURN 0; 
 
IF np1 EQUAL np2 AND nc1 EQUAL 1  
AND nc2 EQUAL 1 THEN 
        RETURN ?; 
END IF 
 
mult_delta = 1; 
FOR I = 1 TO nc1 
nch1 = I
th child of n1;   nch2 = I
th child of n2; 
mult_delta = mult_delta ?  
(? + delta(nch1, nch2, ?, ?)); 
END FOR 
RETURN ? ? mult_delta; 
END 
Figure 2. ? (n1, n2, ?, ?) algorithm 
4 Performance Improving Factors 
4.1 Tree Pruning Methods 
Tree pruning for relation extraction was firstly 
introduced by Zhang et al (2006) and also re-
ferred to as ?tree shrinking task? for removing 
less related contexts. They suggested five types 
of the pruning methods and later invented two 
more in Zhang et al (2008). Among them, the 
path-enclosed tree (PT) method was shown to 
give the best result in the relation extraction 
task based on ACE corpus. We opted for this 
pruning method in our work.  
208
Figure 3 shows how the PT method prunes a 
tree. To focus on the pivotal context, it pre-
serves only the syntactic structure encompass-
ing the two proteins at hand and the words in 
between them (the part enclosed by the dotted 
lines). Without pruning, all the words like addi-
tion, increased and activity would intricately 
participate in deciding the interaction type of 
this sentence. 
 
Figure 3. Path-enclosed Tree (PT) Method 
 
Another important effect of the tree pruning 
is its ability to separate features when two or 
more interactions exist in a sentence. As in Fig-
ure 1, each interaction involves its unique con-
text even though a sentence has multiple inte-
ractions. With tree pruning, it is likely to extract 
context-sensitive features by ignoring external 
features. 
4.2 Tree Kernel Decay Factor 
Collins and Duffy (2001) addressed two prob-
lems of the parse tree kernel. The first one is 
that its kernel value tends to be largely domi-
nated by the size of two input trees. If they are 
large in size, it is highly probable for the kernel 
to accumulate a large number of overlapping 
counts in computing their similarity. Secondly, 
the kernel value of two identical parse trees can 
become overly large while the value of two dif-
ferent parse trees is much tiny in general. These 
two aspects can cause a trouble during a train-
ing phase because pairs of large parse trees that 
are similar to each other are disproportionately 
dominant. Consequently, the resulting models 
could act like nearest neighbor models (Collins 
and Duffy, 2001). 
To alleviate the problems, Collins and Duffy 
(2001) introduced a scalability parameter called 
decay factor, 0 < ? ? 1 which scales the relative 
importance of tree fragments with their sizes as 
in line 33 of Fig. 2. Based on the algorithm, a 
decay factor decreases the degree of contribu-
tion of a large sub-tree exponentially in kernel 
computation. Figure 4 illustrates both the way a 
tree kernel is computed and the effect of a de-
cay factor. In the figure, T1 and T2 share four 
common sub-trees (S1, S2, S3, S5). Let us assume 
that there are only two trees in a training set and 
only five unique sub-trees exist. Then each tree 
can be expressed by a vector whose elements 
are the number of particular sub-trees. Kernel 
value is obtained by computing the inner prod-
uct of the two vectors. As shown in the figure, 
S1 is a large sub-sub-trees, S1, S2 S3, and S4, two 
of which (S2, and S3) are duplicated in the inner 
product computation. It is highly probable for 
large sub-trees to contain many smaller sub-
trees, which lead to an over-estimated similarity 
value between two parse trees. As mentioned 
above, therefore, it is necessary to rein those 
large sub-trees with respect to their sizes in 
computing kernel values by using decay factors. 
In this paper, we treat the decay factor as one of 
the important optimization parameters for a PPI 
extraction task. 
Figure 4. The effect of decaying in comparing two trees. n(?) denotes #unique subtrees in a tree. 
209
5 Experimental Results 
In order to show the superiority of the simple 
kernel based method using the two factors used 
in this paper, compared to the resent results for 
PPIE using additional resources, we ran a series 
of experiments using the same PPI corpora 
cited in the literature. In addition, we show that 
the method is robust especially for cross-corpus 
experiments where a classifier is trained and 
tested with entirely different corpora.  
5.1 Evaluation Corpora 
To evaluate our approach for PPIE, we used 
?Five PPI Corpora3? organized by Pyysalo et al 
(2008). It contains five different PPI corpora: 
AImed, BioInfer, HPRD50, IEPA and LLL. 
They have been combined in a unified XML 
format and ?binarized? in case of involving 
multiple interaction types.  
Table 1. Five PPI Corpora 
 
Table 1 shows the size of each corpus in 
?Five PPI Corpora.? As mentioned before, a 
sentence can have multiple interactions, which 
results in the gaps between the number of sen-
tences and the sum of the number of instances. 
Negative instances have been automatically 
generated by enumerating sentences with mul-
tiple proteins but not having interactions be-
tween them (Pyysalo et al, 2008).  
5.2 Evaluation Settings 
In order to parse each sentence, we used Char-
niak Parser4. For kernel-based learning, we ex-
panded the original libsvm 2.895 (Chang & Lin, 
2001) so that it has two additional kernels in-
cluding parse tree kernel and composite kernel6 
along with four built-in kernels7 
Our experiment uses both macro-averaged 
and micro-averaged F-scores. Macro-averaging 
                                                 
3 http://mars.cs.utu.fi/PPICorpora/eval-standard.html 
4 http://www.cs.brown.edu/people/ec/#software 
5 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
6 A kernel combining built-in kernels and parse tree kernel 
7 Linear, polynomial, radial basis function, sigmoid ker-
nels 
computes F-scores for all the classes indivi-
dually and takes average of the scores. On the 
other hand, micro-averaging enumerates both 
positive results and negative results on the 
whole without considering the score of each 
class and computes total F-score.  
In 10-fold cross validation, we apply the 
same split used in Airola et al, (2008), Miwa et 
al., (2009a) and Miwa et al, (2009b) for com-
parisons. Also, we empirically estimate the re-
gularization parameters of SVM (C-values) by 
conducting 10-fold cross validation on each 
training data. We do not adjust the SVM thre-
sholds to the optimal value as in Airola et al, 
(2008) and Miwa et al, (2009a).  
5.3 PPI Extraction Performance 
Table 2 shows the best scores of our system. 
The optimal decay factor varies with each cor-
pus. In LLL, the optimal decay factor is 0.28 
indicating that the shortage of data has forced 
our system to normalize parse trees more inten-
sively with a strong decay factor in kernel com-
putation in order to cover various syntactic 
structures.  
 
 
DF AC ma-P ma-R ma-F ?ma-F 
A 0.6 83.6 
72.8 
(55.0) 
62.1 
(68.8) 
67.0 
(60.8) 
4.5 
(6.6) 
B 0.5 79.8 
74.5 
(65.7) 
70.9 
(71.1) 
72.6 
(68.1) 
2.7 
(3.2) 
H 0.7 74.5 
75.3 
(68.5) 
71.0 
(76.1) 
73.1 
(70.9) 
10.2 
(10.3) 
I 0.6 74.2 
74.1 
(67.5) 
72.2 
(78.6) 
73.1 
(71.7) 
6.0 
(7.8) 
L 0.2 82.2 
83.2 
(77.6) 
81.2 
(86.0) 
82.1 
(80.1) 
10.4 
(14.1) 
 
Table 2. The highest results of the proposed 
system w.r.t. decay factors. DF: Decay Factor, 
AC: accuracy, ma-F: macro-averaged F1, ?ma-F: 
standard deviation of F-scores in CV. A:AIMed, 
B:BioInfer, H:HPRD50, I:IEPA, L:LLL. The 
numbers in parentheses refer to the scores of 
Miwa et al, (2009a).  
 
Our system outperforms the previous results 
as in Table 2. Even using rich feature vectors 
including Bag-Of-Words and shortest path trees 
                                                 
8 It was determined by increasing it by 0.1 progressively 
through 10-fold cross validation. 
 
AIMed BioInfer HPRD50 IEPA LLL 
#Sentence 1,955 1,100 145 486 77 
#Positive  1,000 2,534 163 335 164 
#Negative  4,834 7,132 270 482 166 
210
generated from multiple corpora, Miwa et al, 
(2009b) reported 64.0% and 66.7% in AIMed 
and BioInfer, respectively. Our system, howev-
er, produced 67.0% in AIMed and 72.6% in 
BioInfer with a single parse tree kernel. We did 
not have to perform any intensive feature gen-
eration tasks using various linguistic analyzers 
and more importantly, did not use any addition-
al corpora for training as done in Miwa et al, 
(2009b). While the performance differences are 
not very big, we argue that obtaining higher 
performance values is significant because the 
proposed system did not use any of the addi-
tional efforts and resources.  
To investigate the effect of the scaling para-
meter of the parse tree kernel in PPI extraction, 
we measure how the performance changes as 
the decay factor varies (Figure 5). It is obvious 
that the decay factor influences the overall per-
formance of PPI extraction. Especially, the F-
scores of the small-scale corpora such as 
HPRD50 and LLL are influenced by the decay 
factor. The gaps between the best and worst 
scores in LLL and HPRD50 are 19.1% and 
5.2%, respectively. The fluctuation in F-scores 
of the large-scale corpora (AIMed, BioInfer, 
IEPA) is not so extreme, which seems to stem 
from the abundance in syntactic and lexical 
forms that reduce the normalizing effect of the 
decay factor. The increase in the decay factor 
leads to the increase in the precision values of 
all the corpora except for LLL. The phenome-
non is fairly plausible because the decreased 
normalization power causes the system to com-
pute the tree similarities more intensively and 
therefore it classifies each instance in a strict 
and detailed manner. On the contrary, the recall 
values slightly decrease with respect to the de-
cay factor, which indicates that the tree pruning 
(PT) has already conducted the normalization 
process to reduce the sparseness problem in 
each corpus. 
Most importantly, along with tree pruning, 
decay factor could boost the performance of our 
system by controlling the rigidness of the parse 
tree kernel in PPI extraction. 
Table 3 shows the results of the cross-corpus 
evaluation to measure the generalization power 
of our system as conducted in Airola et al, 
(2008) and Miwa et al, (2009a). Miwa et al, 
(2009b) executed a set of combinatorial expe-
riments by mixing multiple corpora and pre-
sented their results. Therefore, it is not reasona-
ble to compare our results with them due to the 
size discrepancy between training corpora. 
Nevertheless, we will compare our results with 
their approaches in later based on AIMed cor-
pus. 
As seen in Table 3, our system outperforms 
the existing approaches in almost all pairs of 
corpora. In particular, in the multiple corpora-
based evaluations aimed at AIMed which has 
been frequently used as a standard set in PPI 
extraction, our approach shows prominent re-
sults compared with others. While other ap-
proaches showed the performance ranging from 
33.3% to 60.8%, our approach achieved much 
higher scores between 55.9% and 67.0%. More 
specific observations are: 
(1) Our PPIE method trained on any corpus ex-
cept for IEPA outperforms the other approaches 
regardless of the test corpus only with a few 
exceptions with IEPA and LLL. 
(2) Even when using LLL or HPRD50, two 
smallest corpora, as training sets, our system 
performs well with every other corpus for test-
ing. It indicates that our approach is much less 
vulnerable to the sizes of training corpora than 
other methods. 
(3) The degree of score fluctuation of our sys-
tem across different testing corpora is much 
smaller than other regardless of the training da-
ta set. When trained on LLL, for example, the 
range for our system (55.9% ~ 82.1%) is small-
er than the others (38.6% ~ 83.2% and 33.3% ~ 
76.8%). 
(4) The cross-corpus evaluation reveals that our 
method outperforms the others significantly. 
This is more visibly shown especially when the 
large-scale corpora (AIMed and BioInfer) are 
used.  
(5) PPI extraction model trained on AIMed 
shows lower scores in IEPA and LLL as com-
pared with other methods, which could trigger 
further investigation. 
In order to convince ourselves further the su-
periority of the proposed method, we compare 
it with other previously reported approaches.  
Table 4 lists the macro-averaged precision, re-
call and F-scores of the nine approaches tested 
on AIMed. While the experimental settings are 
different as reported in the literature, they are 
quite close in terms of the numbers of positive 
and negative documents. 
211
As seen in the table, the proposed method is 
superior to all the others in F-scores. The im-
provement in precision (12.8%) is most signifi-
cant, especially in comparison with the work of 
Miwa et al, (2009b), which used multiple cor-
pora (AIMed + IEPA) for training and com-
bined various kernels such as bag-of-words, 
parse trees and graphs. It is natural that the re-
call value is lower since a less number of pat-
terns (features) must have been learned. What?s 
important is that the proposed method has a 
higher or at least comparable overall perfor-
mance without additional resources.  
Our approach is significantly better than that 
of Airola et al, (2008), which employed two 
different forms of graph kernels to improve the 
initial model. Since they did not use multiple 
corpora for training, the comparison shows the 
direct benefit of using the extension of the ker-
nel. 
6 Conclusion and Future Works 
To improve the performance of PPIE, recent 
research activities have had a tendency of in-
creasing the complexity of the systems by com-
bining various methods and resources. In this 
paper, however, we argue that by paying more  
Training 
corpora 
Systems 
F-Scores in the test corpora 
AIMed BioInfer HPRD50 IEPA LLL 
AIMed 
Our System 67.0  64.2  72.9  59.0  62.7  
(Miwa et al, 2009a) 60.8  53.1  68.3  68.1  73.5  
(Airola et al, 2008) 56.4  47.1  69.0  67.4  74.5  
BioInfer 
Our System 65.2  72.6  71.9  72.9  78.4  
(Miwa et al, 2009a) 49.6  68.1  68.3  71.4  76.9  
(Airola et al, 2008) 47.2  61.3  63.9  68.0  78.0  
HPRD50 
Our System 63.1  65.5  73.1  69.3  73.7  
(Miwa et al, 2009a) 43.9  48.6  70.9  67.8  72.2  
(Airola et al, 2008) 42.2  42.5  63.4  65.1  67.9  
IEPA 
Our System 57.8  66.1  66.3  73.1  78.4  
(Miwa et al, 2009a) 40.4  55.8  66.5  71.7  83.2  
(Airola et al, 2008) 39.1  51.7  67.5  75.1  77.6  
LLL 
Our System 55.9  64.4  69.4  71.4  82.1  
(Miwa et al, 2009a) 38.6  48.9  64.0  65.6  83.2  
(Airola et al, 2008) 33.3  42.5  59.8  64.9  76.8  
Table 3. Macro-averaged F1 scores in cross-corpora evaluation. Rows and columns correspond to 
the training and test corpora, respectively. We parallel our results with other recently reported re-
sults. All the split methods in 10-fold CV are the same for fair comparisons. 
    
Figure 5. Performance variation with respect to decay factor in Five PPI Corpora. Macro-
averaged F1 (left), Precision (middle), Recall (right) evaluated by 10-fold CV 
212
attention to a single model and adjusting para-
meters more carefully, we can obtain at least 
comparable performance if not better. 
This paper indicates that a well-tuned parse 
tree kernel based on decay factor can achieve 
the superior performance in PPIE when it is 
preprocessed by the path-enclosed tree pruning 
method. It was shown in a series of experiments 
that our system produced the best scores in sin-
gle corpus evaluation as well as cross-corpora 
validation in comparison with other state-of-
the-art methods. Contribution points of this pa-
per are as follows: 
(1) We have shown that the benefits of using 
additional resources including richer features 
can be obtained by tuning a single tree kernel 
method with tree pruning and decaying factors. 
(2) We have newly found that the decay factor 
influences precision enhancement of PPIE and 
hence its overall performance as well. 
(3) We have also revealed that the parse tree 
kernel method equipped with decay factors 
shows superior generalization power even with 
small corpora while presenting significant per-
formance increase on cross-corpora experi-
ments. 
As a future study, we leave experiments with 
training the classifier with multiple corpora and 
deeper analysis of what aspects of the corpora 
gave different magnitudes of the improvements. 
Acknowledgment 
We want to thank the anonymous reviewers 
for their valuable comments. This work has 
been supported in part by KRCF Grant, the Ko-
rean government. 
Reference 
Airola, A., Pyysalo, S., Bjorne, J., Pahikkala, T., 
Ginter, F. & Salakoski, T. (2008). All-paths graph 
kernel for protein-protein interaction extraction 
with evaluation of cross-corpus learning. BMC 
Bioinformatics, 9(S2), doi:10.1186/1471-2105-9-
S11-S2. 
Andrade, M. A. & Valencia, A. (1998). Automatic 
extraction of keywords from scientific text: appli-
cation to the knowledge domain of protein fami-
lies. Bioinformatics, 14(7), 600-607. 
Blaschke, C., Andrade, M., Ouzounis, C. & Valencia, 
A. (1999). Automatic extraction of biological in-
formation from scientific text: protein-protein in-
teractions. Proc. Int. Conf. Intell. Syst. Mol. Biol., 
(pp. 60-67). 
Bunescu, R., Ge, R., Kate, R., Marcotte, E., Mooney, 
R., Ramani, A. & Wong, Y. (2005).  Comparative 
Experiments on Learning Information Extractors 
for Proteins and their Interactions. Artif. Intell. 
Med., Summarization and Information Extraction 
from Medical Documents, 33, 139-155 
Collins, M. & Duffy, N. (2001). Convolution Ker-
nels for Natural Language. NIPS-2001, (pp. 625-
632). 
Craven, M. & Kumlien, J. (1999). Constructing bio-
logical knowledge bases by extracting informa-
tion from text sources. Proceedings of the 7th In-
ternational conference on intelligent systems for 
molecular biology, (pp.77-86), Heidelberg, Ger-
many. 
Ding, J., Berleant, D., Nettleton, D. & Wurtele, E. 
(2002). Mining MEDLINE: abstracts, sentences, 
or phrases?. Proceedings of PSB'02, (pp. 326-337) 
Erkan, G., Ozgur, A., & Radev, D. R., (2007). Semi-
supervised classification for extracting protein in-
  POS NEG ma-P ma-R ma-F ?F 
Our System 1,000 4,834 72.8 62.1 67.0 4.5 
(Miwa et al, 2009b) 1,000 4,834 60.0 71.9 65.2 
 
(Miwa et al, 2009a) 1,000 4,834 58.7 66.1 61.9 7.4 
(Miwa et al, 2008) 1,005 4,643 60.4 69.3 61.5 
 
(Miyao et al, 2008) 1,059 4,589 54.9 65.5 59.5 
 
(Giuliano et al, 2006) - - 60.9 57.2 59.0 
 
(Airola et al, 2008) 1,000 4,834 52.9 61.8 56.4 5.0 
(S? tre et al, 2007) 1,068 4,563 64.3 44.1 52.0 
 
(Erkan et al, 2007) 951 4,020 59.6 60.7 60.0 
 
(Bunescu & Mooney, 2005) - - 65.0 46.4 54.2 
 
Table 4. Comparative results in AIMed. The number of positive instances (POS) and negative in-
stances (NEG) and macro-averaged precision (ma-P), recall (ma-R) and F1-score (ma-F) are shown.  
213
teraction sentences using dependency parsing. In 
EMNLP 2007. 
Fundel, K., K?ffner, R. & Zimmer, R. (2007). RelEx 
? Relation extraction using dependency parse 
trees. Bioinformatics, 23, 365-371. 
Giuliano, C., Lavelli, A., Romano, L., (2006). Ex-
ploiting Shallow Linguistic Information for Rela-
tion Extraction From Biomedical Literature. Pro-
ceedings of the 11th Conference of the European 
Chapter of the Association for Computational 
Linguistics. 
Gondy, L., Hsinchun C. & Martinez Jesse D. (2003). 
A shallow parser based on closed-class words to 
capture relations in biomedical text. J. Biomed. 
Informatics. 36(3), 145-158. 
GuoDong, Z., Min, Z., Dong, H. J. & QiaoMing, Z. 
(2007). Tree Kernel-based Relation Extraction 
with Context-Sensitive Structured Parse Tree In-
formation. Proceedings of the 2007 Joint Confe-
rence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, Prague, (pp. 728?736) 
Marcotte, E. M., Xenarios, I. & Eisenberg D. (2001). 
Mining literature for protein-protein interactions. 
Bioinformatics, 17(4), 359-363. 
Miwa, M., S? tre, R., Miyao, Y. & Tsujii J. (2009a). 
Protein-protein interaction extraction by leverag-
ing multiple kernels and parsers. International 
Journal of Medical Informatics, 78(12), e39-e46. 
Miwa, M., S? tre, R., Miyao, Y. & Tsujii J. (2009b). 
A Rich Feature Vector for Protein-Protein Inte-
raction Extraction from Multiple Corpora. Pro-
ceedings of the 2009 Conference on Empirical 
Methods in Natural Language Processing, (pp. 
121-130) 
Miwa, M., S? tre, R., Miyao, Y., Ohta,  T., & Tsujii, 
J. (2008). Combining multiple layers of syntactic 
information for protein-protein interaction extrac-
tion. In Proceedings of the Third International 
Symposium on Semantic Mining in Biomedicine 
(SMBM 2008), (pp. 101?108) 
Miyao, Y., S? tre, R., Sagae, K., Matsuzaki, T., & 
Tsujii, J. (2008). Task-oriented evaluation of syn-
tactic parsers and their representations. Proceed-
ings of the 45th Meeting of the Association for 
Computational Linguistics (ACL?08:HLT). 
Moschitti, A. (2006). Making tree kernels practical 
for natural language learning.  Proceedings of 
EACL?06, Trento, Italy. 
 
Nikolai, D., Anton, Y., Sergei, E., Svetalana, N., 
Alexander, N. & llya, M. (2004). Extracting hu-
man protein interactions from MEDLINE using a 
full-sentence parser.  Bioinformatics, 20(5), 604-
611. 
Ono, T., Hishigaki, H., Tanigam, A. & Takagi, T. 
(2001). Automated extraction of information on 
protein-protein interactions from the biological li-
terature. Bioinformatics, 17(2), 155-161. 
Pyysalo, S., Airola, A., Heimonen, J., Bj?rne, J., 
Ginter, F. & Salakoski, T. (2008).  Comparative 
analysis of five protein-protein interaction corpo-
ra. BMC Bioinformatics, 9(S6), 
doi:10.1186/1471-2105-9-S3-S6. 
S? tre, R., Sagae, K., & Tsujii, J. (2007). Syntactic 
features for protein-protein interaction extraction. 
In LBM 2007 short papers. 
Sekimizu, T., Park H. S. & Tsujii J. (1998). Identify-
ing the interaction between genes and gene prod-
ucts based on frequently seen verbs in MEDLINE 
abstracts. Workshop on genome informatics, vol. 
9, (pp. 62-71). 
Shawe-Taylor, J., Cristianini, N., (2004). Kernel 
Methods for Pattern Analysis, Cambridge Univer-
sity Press. 
Temkin, J. M. & Gilder, M. R. (2003). Extraction of 
protein interaction information from unstructured 
text using a context-free grammar. Bioinformatics, 
19(16), 2046-2053. 
Vishwanathan, S. V. N., Smola, A. J. (2003). Fast 
Kernels for String and Tree Matching.  Advances 
in Neural Information Processing Systems, 15, 
569-576, MIT Press. 
Zhang, M., GuoDong, Z. & Aiti, A. (2008). Explor-
ing syntactic structured features over parse trees 
for relation extraction using kernel methods. In-
formation Processing and Management, 44, 687-
701 
Zhang, M., Zhang, J., Su, J. & Zhou, G. (2006). A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL, 
(pp.825-832). 
Zhou, D. & He, Y. (2008). Extracting interactions 
between proteins from the literature. Journal of 
Biomedical Informatics, 41, 393-407. 
214
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 67?75,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Overview of the Pathway Curation (PC) task of BioNLP Shared Task 2013
Tomoko Ohta 1, Sampo Pyysalo 1, Rafal Rak 1, Andrew Rowley1, Hong-Woo Chun2,
Sung-Jae Jung 2,3, Chang-Hoo Jeong 2 Sung-Pil Choi 2,3, Jun?ichi Tsujii 4,Sophia Ananiadou 1
1National Centre for Text Mining and School of Computer Science, University of Manchester
2Software Research Center, Korea Institute of Science and Technology Information (KISTI)
3Department of Applied Information Science, University of Science and Technology (UST)
4Microsoft Research Asia, Beijing, China
Abstract
We present the Pathway Curation (PC)
task, a main event extraction task of
the BioNLP shared task (ST) 2013.
The PC task concerns the automatic ex-
traction of biomolecular reactions from
text. The task setting, representation
and semantics are defined with respect
to pathway model standards and ontolo-
gies (SBML, BioPAX, SBO) and docu-
ments selected by relevance to specific
model reactions. Two BioNLP ST 2013
participants successfully completed the
PC task. The highest achieved F-
score, 52.8%, indicates that event extrac-
tion is a promising approach to support-
ing pathway curation efforts. The PC
task continues as an open challenge with
data, resources and tools available from
http://2013.bionlp-st.org/
1 Introduction
Following developments in molecular biology, bi-
ological phenomena are increasingly understood
on the molecular level, as the products of complex
systems of molecular reactions. Pathway mod-
els formalizing biomolecules and their reactions
in machine readable representations are a key way
of sharing and communicating human understand-
ing of these phenomena and of developing com-
putational models of biological systems (Kitano,
2002). Many pathway models integrate knowl-
edge from hundreds or thousands of scientific pub-
lications, and their curation requires substantial
manual effort. To support this effort, we have de-
veloped PathText (Kemper et al, 2010) which pro-
vides a seamless environment integrating a path-
way visualizer, text mining systems and annota-
tion tools. Furthermore, automatic processing of
the domain literature could thus potentially play
pyruvate kinase catalyzes the conversion of PEP to pyruvate.
GGP +Regulation Conversion Chem ChemicalThemeCause Theme Product
Figure 1: Event representation for a conversion re-
action.
an important role in the support of pathway cura-
tion.
Information extraction targeting biomolecular
reactions has been a major focus of efforts in
biomedical natural language processing, with sev-
eral tasks, resources, and tools addressing in par-
ticular protein-protein interactions (Krallinger et
al., 2007; Pyysalo et al, 2008; Tikk et al, 2010).
However, most such efforts have employed sim-
ple representations, such as entity pairs, that are
not sufficient for capturing molecular reactions to
the level of detail required to support the curation
of pathway models. Additionally, previous efforts
have not directly involved the semantics (e.g. re-
action type definitions) of such models. Perhaps
in part due to these reasons, natural language pro-
cessing and information extraction methods have
not been widely embraced by biomedical pathway
curation communities (Ohta et al, 2011c; Ohta et
al., 2011a).
We believe that the extraction of structured
event representations (Figure 1) pursued in the
BioNLP Shared Tasks offers many opportuni-
ties to make significant contributions to support
the development, evaluation and maintenance of
biomolecular pathways. The Pathway Curation
(PC) task, a main task of the BioNLP Shared Task
2013, is proposed as a step toward realizing these
opportunities. The PC task aims to evaluate the ap-
plicability of event extraction systems to pathway
curation and to encourage the further development
of methods for related tasks. The design of the
task aims to address current issues in information
extraction for pathway curation by explicitly bas-
ing its representation and extraction targets on ma-
67
GTP GDP
GAPs
re1
re1 Protein Molecule MoleculeReactantModifier ProductConversion GAPs catalyze the hydrolysis of GTP to GDP.GGP +Reg Conversion Chem Chem
Cause ThemeTheme Product
(a) CONVERSION
p38 gamma Pp38 gamma
MKK6
re1
re1
MKK6 phosphorylates p38 gamma.Protein Protein
Protein
Modifier Reactant
Product
Phosphorylation MKK6 phosphorylates p38 gamma.GGP Phosphorylation GGP
Cause Theme
(b) PHOSPHORYLATION
NF-kappaB
p65
p50
p65
p50re1
p65 binds to p50.
GGP Bind GGPTheme Theme2
p65-p50 complex formation.
Complex BindingProduct
p65 and p50 form p65-p50 complex.
Protein Protein NC binding ComplexReactant2 Product
Reactant
(c) BINDING
Figure 2: Illustration of pathway reaction (left), matching representation as an idealized text-bound event
structure (middle) and applied event representation for statements actually appearing in text (right).
jor standards developed in the biomolecular path-
way curation community, such as SBML (Hucka
et al, 2003) and BioPAX (Mi et al, 2011), and
ontologies such as the Systems Biology Ontology1
(SBO) (Courtot et al, 2011). Further, The corpus
texts are selected on the basis of relevance to a se-
lection of pathway models from PANTHER Path-
way DB2 (Mi and Thomas, 2009) and BioMod-
els3 (Li et al, 2010) repositories. The PC task set-
ting and its document selection protocol aim to ac-
count for both signalling and metabolic pathways,
the latter of which has received comparatively lit-
tle attention in recent domain IE efforts (Li et al,
2013).
2 Task setting
The PC task is formulated as an event extraction
task (Ananiadou et al, 2010) following the general
representation and task setting first introduced in
the BioNLP ST 2009 (Kim et al, 2011). The pri-
mary aim is the extraction of event structures, or
events, each of which can involve any number of
physical entities or other events in specific roles.
The event representation is sufficiently expres-
sive to allow the definition of event structures that
closely parallel the definition of reactions in path-
way representations such as SBML and BioPAX.
These pathway representations differentiate be-
tween three primary groups of reaction partici-
pants: reactants (?inputs?), products (?outputs?),
and modifiers, where the specific roles of modi-
fiers can be further identified to differentiate e.g.
1http://www.ebi.ac.uk/sbo/main/
2http://www.pantherdb.org/pathway/
3http://www.ebi.ac.uk/biomodels-main/
reaction catalysts from inhibitors. Correspond-
ingly, the PC task applies the Theme role defined
in previous BioNLP ST tasks to capture reactants,
introduces a new Product role for products, and
applies the previously defined Cause role and reg-
ulatory events to capture modifiers (Figure 2; see
also Section 2.3).
It is important to note that while the event repre-
sentation allows a one-to-one mapping to reactions
in principle, an annotation scheme cannot guar-
antee that actual statements in text map to fully
specified reactions: in free-form text, authors fre-
quently omit mention of some entities taking part
in reactions, perhaps most typically to avoid re-
dundancies such as in ?p38? is phosphorylated
into phospho-p38?? (Figure 2b). Representations
extracted from explicit statements in text will thus
in some cases omit aspects of the corresponding
complete reactions in pathway models.
Systems addressing the PC task are expected to
extract events of specific types given 1) free-form
text and 2) gold standard annotation for mentions
of physical entities in that text. The task annota-
tions also include equivalence relations and event
modifications, a secondary extraction target. The
annotation types are detailed below.
2.1 Entities
The entity annotation marks mentions of physical
entities using start and end offsets in text (contigu-
ous span) and a type selected from a fixed set. The
following four entity types are marked in the PC
task: SIMPLE CHEMICAL, annotated with refer-
ence to the Chemical Entities of Biological Inter-
est (ChEBI) resource (Degtyarenko et al, 2008);
68
Entity type Scope Reference Ontology ID
SIMPLE CHEMICAL simple, non-repetitive chemical entities ChEBI SBO:0000247
GENE OR GENE PRODUCT genes, RNA and proteins gene/protein DBs SBO:0000246
COMPLEX entities of non-covalently linked components complex DBs SBO:0000253
CELLULAR COMPONENT parts of cell and extracellular environment GO-CC SBO:0000290
Table 1: Entity types, definitions, and reference resources.
Event type Core arguments Additional arguments Ontology ID
CONVERSION Theme:Molecule, Product:Molecule SBO:0000182
PHOSPHORYLATION Theme:Molecule, Cause:Molecule Site:SIMPLE CHEMICAL SBO:0000216
DEPHOSPHORYLATION Theme:Molecule, Cause:Molecule Site:SIMPLE CHEMICAL SBO:0000330
(Other modifications, such as ACETYLATION, defined similarly.)
LOCALIZATION Theme:Molecule At/From/ToLoc:CELL. COMP. GO:0051179
TRANSPORT Theme:Molecule From/ToLoc:CELL. COMP. SBO:0000185
GENE EXPRESSION Theme:GENE OR GENE PRODUCT GO:0010467
TRANSCRIPTION Theme:GENE OR GENE PRODUCT SBO:0000183
TRANSLATION Theme:GENE OR GENE PRODUCT SBO:0000184
DEGRADATION Theme:Molecule SBO:0000179
BINDING Theme:Molecule, Product:COMPLEX SBO:0000177
DISSOCIATION Theme:COMPLEX, Product:Molecule SBO:0000180
REGULATION Theme:ANY, Cause:ANY GO:0065007
POSITIVE REGULATION Theme:ANY, Cause:ANY
GO:0048518,
GO:0044093
ACTIVATION Theme:Molecule, Cause:ANY SBO:0000412
NEGATIVE REGULATION Theme:ANY, Cause:ANY
GO:0048519,
GO:0044092
INACTIVATION Theme:Molecule, Cause:ANY SBO:0000412
PATHWAY Participant:Molecule SBO:0000375
Table 2: Event types and arguments. ?Molecule? refers to an entity annotation of any of the types
SIMPLE CHEMICAL, GENE OR GENE PRODUCT, or COMPLEX, and ?ANY? refers to an annotation of
any type, either entity or event. The indentation corresponds to ontological relationships between the
event types: for example, PHOSPHORYLATION is-a CONVERSION and TRANSCRIPTION part-of
GENE EXPRESSION.
GENE OR GENE PRODUCT, annotated with refer-
ence to gene and protein databases such as UniProt
(Consortium, 2011), Entrez Gene (Maglott et al,
2005) and PFam (Finn et al, 2010); COMPLEX,
annotated with reference to database resources
covering complexes; and CELLULAR COMPO-
NENT, annotated following the scope of the Gene
Ontology cellular component subontology
(Ashburner et al, 2000) (Table 1). For discussion
of the relation between these types and the repre-
sentations applied in pathway models, we refer to
Ohta et al (2011c).
In terms of mention types in text, the annotation
for SIMPLE CHEMICAL, GENE OR GENE PROD-
UCT and COMPLEX covers entity name mentions
only, while the annotation for CELLULAR COM-
PONENT covers entity name mentions, nominal
mentions, and adjectival references (e.g. ?mito-
chondrial?).
2.2 Relations
The PC task defines one relation type, Equiv
(equivalence), which can hold between entity
mitogen-activated protein kinase (MAPK, also known as ERK)
Gene or gene product GGP GGPEquivEquiv
Figure 3: Example Equiv annotation.
mentions of the same type and specifies that they
refer to the same real-world entity (Figure 3).
These relations are only applied to determine if
two events match during evaluation, where entities
connected by an Equiv relation are considered in-
terchangeable. Gold standard Equiv relations are
applied also for test data, and systems participat-
ing in the task are not expected to extract these
relations.
2.3 Events
The event annotation marks references to reac-
tions, processes and comparable associations in
scope of the annotation using the event represen-
tation. For the definition and scope of the event
annotation, we rely primarily on the Systems Biol-
ogy Ontology (SBO), drawing some general types
not in scope of this ontology from the Gene Ontol-
ogy (GO). Table 2 presents the event types anno-
69
Pathway Repository ID Publication
mTOR BioModels MODEL1012220002 (Caron et al, 2010)
mTORC1 upstream regulators BioModels MODEL1012220003 (Caron et al, 2010)
TLR BioModels MODEL2463683119 (Oda and Kitano, 2006)
Yeast Cell Cycle BioModels MODEL1011020000 (Kaizu et al, 2010)
Rb BioModels MODEL4132046015 (Calzone et al, 2008)
EGFR BioModels MODEL2463576061 (Oda et al, 2005)
Human Metabolic Network BioModels MODEL6399676120 (Duarte et al, 2007)
NF-kappaB pathway - - (Oda et al, 2008)
p38 MAPK PANTHER DB P05918 -
p53 PANTHER DB P00059 -
p53 feedback loop pathway PANTHER DB P04392 -
Wnt signaling pathway PANTHER DB P00057 -
Table 3: Pathway models used to select documents for the task, with pathway repository model identifiers
and publications presenting each model (when applicable).
tated in the PC task and their arguments. We refer
again to Ohta et al (2011c) for detailed discussion
of the relation between these types and other rep-
resentations applied in pathway models.
The role in which each event argument (entity
or other event) participates in an event is specified
as one of the following:
Theme entity/event that undergoes the effects of
the event. For example, the entity that is tran-
scribed in a TRANSCRIPTION event or transported
in a TRANSPORT event.
Cause entity/event that is causally active in the
event. Marks, for example, ?P1? in ?P1 inhibits P2
expression?.
AtLoc,FromLoc,ToLoc : location in which the
Theme entity of a LOCALIZATION event is local-
ized (At) in LOCALIZATION events not involving
movement or is transported (or moves) from/to
(From/To) in LOCALIZATION and TRANSPORT
events involving movement.
Site site on the Theme entity that is modified in
the event. Can be specified for modification events
such as PHOSPHORYLATION.
Participant general role type identifying an en-
tity that participates in some underspecified way in
a high-level process. Only applied for the PATH-
WAY type.
2.4 Event modifications
In addition to events, the PC task defines a sec-
ondary extraction target, event modifications. Two
modification types are defined: NEGATION and
SPECULATION. Both are binary flags that mod-
ify events, the former marking an event as be-
ing explicitly stated as not occurring (e.g. ?P is
not phosphorylated?) and the latter as being stated
in a speculative context (?P may be phosphory-
lated.?). Both are defined in terms of annotation
scope and semantics identically as in the BioNLP
ST?09 (Kim et al, 2009).
2.5 Evaluation
The PC task evaluation applies the standard evalu-
ation criteria established in the BioNLP ST 2009.
These criteria relax exact matching between gold
and predicted events in two aspects: approximate
trigger boundary matching, and approximate re-
cursive event matching. The former allows pre-
dicted event triggers to differ from gold triggers
by one word, and the latter requires recursively re-
ferred events to only match in their core arguments
(see Table 2). We refer to Kim et al (2011) for a
detailed definition of these criteria.
3 Corpus
This section presents the PC task corpus and its
annotation process.
3.1 Document selection
To assure that the documents annotated for the PC
task corpus are relevant to pathway reactions, we
applied two complementary approaches, both se-
lecting documents on the basis of relevance to a
specific pathway reaction. First, we selected from
the BioModels repository those pathway models
with the largest numbers of manually created an-
notations referencing a specific PubMed document
identifier. For each of these models, we extracted
literature references, selected a random subset,
downloaded the documents, and manually filtered
to select abstracts that explicitly discuss relevant
molecular reactions. Second, as only a small sub-
set of models include explicit references to the
70
literature providing evidence for specific pathway
reactions, we applied an alternative strategy where
reactions from a selection of PANTHER DB mod-
els were entered into the PathText system (Kem-
per et al, 2010),4 which is capable of suggest-
ing documents relevant to given reactions based
on an SBML model. We then selected a random
set of reactions to query the system, and manually
evaluated the highest-ranking documents to iden-
tify those whose abstracts explicitly discuss the se-
lected reaction. We refer to Miwa et al (2013a)
for a detailed description of this approach. Table 3
presents the pathway models on which the docu-
ment selection was based.
3.2 Annotation process
The base entity annotation for the PC corpus was
created automatically using state-of-the-art entity
mention taggers for each of the targeted entity
types. For SIMPLE CHEMICAL tagging, the OS-
CAR4 system (Jessop et al, 2011) trained on
the chemical named entity recognition corpus of
Corbett and Copestake (2008) was applied. For
GENE OR GENE PRODUCT mention detection, the
NERsuite5 system trained on the BioCreative 2
Gene Mention task (Wilbur et al, 2007) corpus
was used. NERsuite was also applied for CEL-
LULAR COMPONENT mention detection, for this
task trained on the Anatomical Entity Mention
(AnEM) corpus (Ohta et al, 2012). Finally, COM-
PLEX annotations were created using a combi-
nation of a dictionary and heuristics making use
of the GENE OR GENE PRODUCT annotation (for
mentions such as ?cyclin E/CDK2 complex?). To
support the curation process, these tools were in-
tegrated into the NaCTeM text-analysis workflow
system Argo (Rak et al, 2012).
Based on the evaluations of each of these tools
in the studies presenting them, we expected initial
automatic tagging performance to be in the range
80-90% in both precision and recall. Following
initial automatic annotation, the entity mention an-
notation was manually revised to improve quality
and consistency. As the entity annotation is not
itself a target of extraction in the shared task, we
did not separately evaluate the consistency of the
revised entity mention annotation.
To assure that the quality and consistency of
the event annotation are as high as possible, ini-
4http://nactem.ac.uk/pathtext/
5http://nersuite.nlplab.org/
Item Train Devel Test Total
Documents 260 90 175 525
Words 53811 18579 35966 108356
Entities 7855 2734 5312 15901
Events 5992 2129 4004 12125
Modifications 317 80 174 571
Table 4: PC corpus statistics
tial event annotation was created entirely man-
ually, without automatic support. This annota-
tion effort was carried out using the BRAT anno-
tation tool (Stenetorp et al, 2012) by a group of
biologists in collaboration between NaCTeM and
KISTI. Following initial annotator training and re-
finement of guidelines based on the event type def-
initions provided by the reference ontologies, the
primary event annotation was created by three bi-
ologists. To evaluate and maintain annotation con-
sistency, a random 20% of documents were an-
notated redundantly by all annotators, and these
overlapping annotations were periodically evalu-
ated and differences in annotation were discussed
between the annotators and annotation coordina-
tors. Following initial annotation, a round of semi-
automatic consistency checks were applied using
BRAT. Evaluation of the redundantly annotated
documents using the primary task evaluation cri-
teria gave an inter-annotator agreement of 61.0%
in F-score. For the final corpus, the redundantly
annotated documents were evaluated separately by
an annotation coordinator to select the best of each
set.6
The overall statistics of the corpus are summa-
rized in Table 4. We note that the among the
previous BioNLP ST corpora, only the GENIA
(GE) task corpus has a larger number of annotated
events than the PC corpus.
4 Results
4.1 Participation
Two groups submitted final results to the PC
task, one from the National Centre for Text Min-
ing (NaCTeM) and one from the University of
Turku BioNLP group (TEES-2.1) (Table 5). Both
participants applied their well-established, state-
of-the-art event extraction systems, EventMine7
(Miwa et al, 2012) (NaCTeM) and the Turku
6This selection implies that the consistency of the event
annotation of the final corpus is expected to exceed the 61%
F-score of the IAA experiment. Consistency after selection
was not separately evaluated.
7http://nactem.ac.uk/EventMine/
71
NLP Events Other resources
Rank Team Org Word Parse Trig. Arg. Group. Modif. Corpora Other
1 NaCTeM 1NLP Snowball Enju, GDep SVM SVM SVM SVM (see text) triggers
2 TEES-2.1 1BI Porter McCCJ + SD SVM SVM SVM SVM GE hedge words
Table 5: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician,
NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Char-
niak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus.
Team recall prec. F-score
NaCTeM 52.23 53.48 52.84
TEES-2.1 47.15 55.78 51.10
Table 6: Primary evaluation results
Event Extraction System8 (Bjo?rne et al, 2011)
(TEES). The two systems share the same over-
all architecture, a one-best pipeline with SVM-
based stages for event trigger detection, trigger-
argument relation detection, argument grouping
into event structures, and modification prediction.
The feature representations of both systems draw
on substructures of dependency-like representa-
tions of sentence syntax, derived from full parses
of input sentences. TEES applies the Charniak
and Johnson (2005) parser with the McClosky
(2009) biomedical model, converting the phrase-
structure parses into dependencies using the Stan-
ford tools (de Marneffe et al, 2006). By contrast,
EventMine uses a combination of the predicate-
argument structure analyses created by the deep
parser Enju (Miyao and Tsujii, 2008) and the out-
put of the the GDep best-first shift-reduce depen-
dency parser (Sagae and Tsujii, 2007). All three
parsers have models trained in part on the biomed-
ical domain GENIA treebank (Tateisi et al, 2005).
Interestingly, both systems make use of the GE
task data, but the application of EventMine ex-
tends on this considerably by applying a stacked
model (Miwa et al, 2013b) with predictions also
from models trained on the BioNLP ST 2011 EPI
and ID tasks (Pyysalo et al, 2012) as well as from
four corpora introduced outside of the shared tasks
by Thompson et al (2011), Pyysalo et al (2011),
Ohta et al (2011b) and Ohta et al (2011c).
4.2 Evaluation results
Table 6 summarizes the primary evaluation results.
The two systems demonstrate broadly similar per-
formance in terms of F-scores, with NaCTeM
achieving an 1.7% point higher overall result.
8http://jbjorne.github.io/TEES/
However, the systems show quite different per-
formance in terms of the precision/recall balance:
while the NaCTeM system has little difference
between precision and recall, TEES-2.1 shows a
clear preference for precision, with 8.6% lower re-
call than precision.
Results are shown separately for each event type
in Table 7. The results largely mirror the over-
all performance, with the NaCTeM system show-
ing better performance for 13 out of the 21 event
types present in the test data and more balanced
precision and recall than TEES-2.1, which em-
phasizes precision over recall for almost all event
types. Although the results do not include evalu-
ation of EventMine with a reduced set of stacked
models in training, the modest difference in per-
formance suggests that comprehensive use of pre-
viously released event resources in EventMine did
not confer a decisive advantage, perhaps in part
due to differences in the event definitions between
the PC task and previous resources.
Overall, the two systems appear quite similar
not only in architecture but also performance, with
the clearest systematic difference observed being
the different emphases on precision vs. recall. As
both systems are based on machine learning meth-
ods with real-valued outputs, it would be relatively
straightforward to use prediction confidences to
analyse performance over the entire precision-
recall curve instead of a single fixed point. Such
analysis could provide further insight into the rel-
ative strengths and weaknesses of these two sys-
tems.
5 Discussion
Although participation in this initial run of the PC
task was somewhat limited, the two participating
systems have been applied to a large variety of
event extraction tasks over the last years and have
shown consistently competitive performance with
the state of the art (Bjo?rne and Salakoski, 2011;
Miwa et al, 2012). It is thus reasonable to as-
sume that the higher performance achieved by the
72
NaCTeM TEES-2.1
Event recall prec. F-score recall prec. F-score
CONVERSION 34.33 35.48 34.90 35.82 42.86 39.02
PHOSPHORYLATION 62.46 55.94 59.02 53.40 66.00 59.03
DEPHOSPHORYLATION 45.00 56.25 50.00 35.00 77.78 48.28
ACETYLATION 69.57 72.73 71.11 82.61 76.00 79.17
DEACETYLATION 33.33 33.33 33.33 0.00 0.00 0.00
METHYLATION 42.86 60.00 50.00 57.14 80.00 66.67
DEMETHYLATION 100.00 100.00 100.00 100.00 100.00 100.00
UBIQUITINATION 52.94 64.29 58.06 58.82 76.92 66.67
DEUBIQUITINATION 100.00 100.00 100.00 100.00 100.00 100.00
LOCALIZATION 42.25 61.22 50.00 43.66 54.39 48.44
TRANSPORT 65.52 61.29 63.33 56.55 59.85 58.16
GENE EXPRESSION 90.65 83.15 86.74 84.55 79.39 81.89
TRANSCRIPTION 71.15 82.22 76.29 57.69 73.17 64.52
TRANSLATION 0.00 0.00 0.00 50.00 100.00 66.67
Simple-total 66.42 64.80 65.60 60.40 67.87 63.92
DEGRADATION 78.57 89.19 83.54 78.57 78.57 78.57
ACTIVATION 78.54 70.96 74.56 72.06 72.06 72.06
INACTIVATION 44.62 55.77 49.57 38.46 45.45 41.67
BINDING 64.96 47.30 54.74 53.96 53.96 53.96
DISSOCIATION 38.46 46.88 42.25 35.90 45.16 40.00
PATHWAY 84.91 75.50 79.93 70.94 75.50 73.15
General-total 69.07 62.69 65.72 61.16 65.74 63.37
REGULATION 33.33 33.97 33.65 29.73 39.51 33.93
POSITIVE REGULATION 35.49 42.81 38.81 34.51 45.45 39.23
NEGATIVE REGULATION 45.75 50.64 48.07 41.02 47.37 43.97
Regulation-total 37.73 42.79 40.10 35.17 44.76 39.39
Sub-total 53.47 53.96 53.72 48.23 56.22 51.92
NEGATION 24.52 35.87 29.13 25.16 41.30 31.27
SPECULATION 15.79 22.22 18.46 0.00 0.00 0.00
Modification-total 23.56 34.65 28.05 22.41 40.00 28.73
Total 52.23 53.48 52.84 47.15 55.78 51.10
Table 7: Primary evaluation results by event type.
task participants, a balanced F-score of 52.8%, is
a good estimate of the performance level that can
be attained for this task by present event extraction
technology.
The results achieved by the two systems are
broadly comparable to the best results achieved by
any system in similar previously introduced event
extraction tasks (Kim et al, 2012; Pyysalo et al,
2012). Given the novelty of the task domain and
reference resource and the broad selection of doc-
uments, we find the results highly encouraging re-
garding the applicability of event extraction tech-
nology to supporting the development, evaluation,
and maintenance of pathway models.
6 Conclusions
This paper presented the Pathway Curation (PC)
task, a main event extraction task of the BioNLP
ST 2013. The task was organized in collaboration
between groups with an interest in pathway cura-
tion with the aim of evaluating and advancing the
state of the art in event extraction toward methods
for developing, evaluating and maintaining formal
pathway models in representations such as SBML
and BioPAX. We introduced an event extraction
task setting with reference to pathway model stan-
dards and the Systems Biology Ontology, selected
a set of 525 publication abstracts relevant to spe-
cific model reactions, and created fully manual
73
event annotation marking over 12,000 event struc-
tures in the corpus.
Two participants in the BioNLP ST 2013 sub-
mitted final predictions to the PC task, applying
established, state-of-the-art event extraction sys-
tems, EventMine and the Turku Event Extrac-
tion System. Both systems achieved F-scores
over 50%, with the EventMine system achiev-
ing the best overall result of 52.8%. This level
of performance is broadly comparable with re-
sults achieved in comparable previously proposed
tasks, indicating that current event extraction tech-
nology is applicable to the projected pathway cu-
ration support tasks.
To allow the further development and evalua-
tion of event extraction methods for the task, the
PC task continues as an open challenge to all inter-
ested participants, with the annotated corpus data,
supporting resources, and evaluation tools avail-
able under open licenses from the task homepage,
http://2013.bionlp-st.org/
Acknowledgments
We would like to thank Yonghwa Jo, Hyeyeon
Choi, Jeong-Ik Lee and Ssang-Goo Cho of
Konkuk University for their contribution to the de-
velopment of the relevance judgment annotation
criteria. We also wish to thank Hyun Uk Kim,
Jinki Kim and Kyusang Hwang of KAIST for
their efforts in producing the PC task annotation.
This work is a part of joint research of KISTI and
NaCTeM, and partially supported by the Biotech-
nology and Biological Sciences Research Council
(BBSRC) [BB/G53025X/1].
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and Dou-
glas B. Kell. 2010. Event extraction for systems biology
by text mining the literature. Trends in Biotechnology,
28(7):381?390.
Michael Ashburner, Catherine A. Ball, Judith A. Blake,
David Botstein, Heather Butler, J. Michael Cherry, Al-
lan P. Davis, Kara Dolinski, et al 2000. Gene ontology:
tool for the unification of biology. Nature genetics, 25:25?
29.
Jari Bjo?rne and Tapio Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 183?191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio
Pahikkala, and Tapio Salakoski. 2011. Extracting contex-
tualized complex biological events with rich graph-based
feature sets. Computational Intelligence, 27(4):541?557.
Laurence Calzone, Ame?lie Gelay, Andrei Zinovyev, Franc?ois
Radvanyl, and Emmanuel Barillot. 2008. A comprehen-
sive modular map of molecular interactions in rb/e2f path-
way. Molecular systems biology, 4(1).
Etienne Caron, Samik Ghosh, Yukiko Matsuoka, Dariel
Ashton-Beaucage, Marc Therrien, Se?bastien Lemieux,
Claude Perreault, Philippe P Roux, and Hiroaki Kitano.
2010. A comprehensive map of the mtor signaling net-
work. Molecular systems biology, 6(1).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine
n-Best Parsing and MaxEnt Discriminative Reranking. In
Proceedings of ACL?05, pages 173?180.
The UniProt Consortium. 2011. Ongoing and future devel-
opments at the universal protein resource. Nucleic Acids
Research, 39(suppl 1):D214?D219.
Peter Corbett and Ann Copestake. 2008. Cascaded classifiers
for confidence-based chemical named entity recognition.
BMC Bioinformatics, 9(Suppl 11):S4.
Me?lanie Courtot, Nick Juty, Christian Knu?pfer, Dagmar Wal-
temath, Anna Zhukova, Andreas Dra?ger, Michel Dumon-
tier, Andrew Finney, Martin Golebiewski, Janna Hastings,
et al 2011. Controlled vocabularies and semantics in sys-
tems biology. Molecular systems biology, 7(1).
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of
LREC, volume 6, pages 449?454.
Kirill Degtyarenko, Paula De Matos, Marcus Ennis, Janna
Hastings, Martin Zbinden, Alan Mcnaught, Rafael
Alca?ntara, Michael Darsow, Mickae?l Guedj, and Michael
Ashburner. 2008. Chebi: a database and ontology for
chemical entities of biological interest. Nucleic acids re-
search, 36(suppl 1):D344?D350.
Natalie C Duarte, Scott A Becker, Neema Jamshidi, Ines
Thiele, Monica L Mo, Thuy D Vo, Rohith Srivas, and
Bernhard ? Palsson. 2007. Global reconstruction of
the human metabolic network based on genomic and bib-
liomic data. Proceedings of the National Academy of Sci-
ences, 104(6):1777?1782.
Robert D. Finn, Jaina Mistry, John Tate, Penny Coggill, An-
dreas Heger, Joanne E. Pollington, O. Luke Gavin, Prasad
Gunasekaran, et al 2010. The Pfam protein families
database. Nucleic Acids Research, 38(suppl 1):D211?
D222.
Michael Hucka, Andrew Finney, Herbert M Sauro, Hamid
Bolouri, John C Doyle, Hiroaki Kitano, Adam P Arkin,
Benjamin J Bornstein, et al 2003. The systems biology
markup language (SBML): a medium for representation
and exchange of biochemical network models. Bioinfor-
matics, 19(4):524?531.
David M. Jessop, Sam Adams, Egon L. Willighagen, Lezan
Hawizy, and Peter Murray-Rust. 2011. Oscar4: a flexible
architecture for chemical text-mining. Journal of chemin-
formatics, 3(1):1?12.
Kazunari Kaizu, Samik Ghosh, Yukiko Matsuoka, Hisao
Moriya, Yuki Shimizu-Yoshida, and Hiroaki Kitano.
2010. A comprehensive molecular interaction map of the
budding yeast cell cycle. Molecular systems biology, 6(1).
Brian Kemper, Takuya Matsuzaki, Yukiko Matsuoka, Yoshi-
masa Tsuruoka, Hiroaki Kitano, Sophia Ananiadou, and
Jun?ichi Tsujii. 2010. Pathtext: a text mining integra-
tor for biological pathway visualizations. Bioinformatics,
26(12):i374?i381.
74
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Jun?ichi Tsujii. 2009. Overview of BioNLP?09
Shared Task on Event Extraction. In Proceedings of
BioNLP?09.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Junichi Tsujii. 2011. Extracting bio-molecular
events from literature ? the bionlp?09 shared task. Com-
putational Intelligence, 27(4):513?540.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsujii,
Toshihisa Takagi, and Akinori Yonezawa. 2012. The
genia event and protein coreference tasks of the bionlp
shared task 2011. BMC bioinformatics, 13(Suppl 11):S1.
Hiroaki Kitano. 2002. Systems biology: a brief overview.
Science, 295(5560):1662?1664.
Martin Krallinger, Florian Leitner, and Alfonso Valencia.
2007. Assessment of the Second BioCreative PPI task:
Automatic Extraction of Protein-Protein Interactions. In
L. Hirschman, M. Krallinger, and A. Valencia, editors,
Proceedings of BioCreative II, pages 29?39.
Chen Li, Marco Donizelli, Nicolas Rodriguez, Harish
Dharuri, Lukas Endler, Vijayalakshmi Chelliah, Lu Li,
Enuo He, et al 2010. BioModels Database: An enhanced,
curated and annotated resource for published quantitative
kinetic models. BMC Systems Biology, 4:92.
Chen Li, Maria Liakata, and Dietrich Rebholz-Schuhmann.
2013. Biological network extraction from scientific litera-
ture: state of the art and challenges. Briefings in bioinfor-
matics.
Donna Maglott, Jim Ostell, Kim D. Pruitt, and Tatiana
Tatusova. 2005. Entrez gene: gene-centered information
at ncbi. Nucleic Acids Research, 33(suppl 1):D54.
David McClosky. 2009. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing. Ph.D.
thesis, Brown University.
Huaiyu Mi and Paul Thomas. 2009. PANTHER pathway: an
ontology-based pathway database coupled with data anal-
ysis tools. In Protein Networks and Pathway Analysis,
pages 123?140. Springer.
Huaiyu Mi, Anushya Muruganujan, Emek Demir, Yukiko
Matsuoka, Akira Funahashi, Hiroaki Kitano, and Paul D
Thomas. 2011. Biopax support in celldesigner. Bioinfor-
matics, 27(24):3437?3438.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the liter-
ature using domain adaptation and coreference resolution.
Bioinformatics, 28(13):1759?1765.
Makoto Miwa, Tomoko Ohta, Rafal Rak, Andrew Rowley,
Douglas B. Kell, Sampo Pyysalo, and Sophia Ananiadou.
2013a. A method for integrating and ranking the evidence
for biochemical pathways by mining reactions from text.
Bioinformatics. in press.
Makoto Miwa, Sampo Pyysalo, Tomoko Ohta, and Sophia
Ananiadou. 2013b. Wide coverage biomedical event
extraction using multiple partially overlapping corpora.
BMC bioinformatics, 14(1):175.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest mod-
els for probabilistic HPSG parsing. Computational Lin-
guistics, 34(1):35?80.
Kanae Oda and Hiroaki Kitano. 2006. A comprehensive
map of the toll-like receptor signaling network. Molecular
Systems Biology, 2(1).
Kanae Oda, Yukiko Matsuoka, Akira Funahashi, and Hiroaki
Kitano. 2005. A comprehensive pathway map of epider-
mal growth factor receptor signaling. Molecular systems
biology, 1(1).
Kanae Oda, Jin-Dong Kim, Tomoko Ohta, Daisuke
Okanohara, Takuya Matsuzaki, Yuka Tateisi, and Jun?ichi
Tsujii. 2008. New challenges for text mining: mapping
between text and manually curated pathways. BMC bioin-
formatics, 9(Suppl 3):S5.
Tomoko Ohta, Sampo Pyysalo, Sophia Ananiadou, and Ju-
nichi Tsujii. 2011a. Pathway curation support as an infor-
mation extraction task. Proceedings of LBM?11.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and Jun?ichi
Tsujii. 2011b. Event extraction for dna methylation.
Journal of Biomedical Semantics, 2(Suppl 5):S2.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011c.
From pathways to biomolecular events: opportunities and
challenges. In Proceedings of BioNLP?11, pages 105?
113.
Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and Sophia
Ananiadou. 2012. Open-domain anatomical entity men-
tion detection. In Proceedings of DSSD?12, pages 27?36.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari Bjo?rne,
Filip Ginter, and Tapio Salakoski. 2008. Comparative
analysis of five protein-protein interaction corpora. BMC
Bioinformatics, 9(Suppl 3):S6.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and Jun?ichi
Tsujii. 2011. Towards exhaustive event extraction for pro-
tein modifications. In Proceedings of BioNLP?11, pages
114?123.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sullivan,
Chunhong Mao, Chunxia Wang, Bruno Sobral, Jun?ichi
Tsujii, and Sophia Ananiadou. 2012. Overview of the id,
epi and rel tasks of bionlp shared task 2011. BMC bioin-
formatics, 13(Suppl 11):S2.
Rafal Rak, Andrew Rowley, William Black, and Sophia Ana-
niadou. 2012. Argo: an integrative, interactive, text
mining-based workbench supporting curation. Database:
The Journal of Biological Databases and Curation, 2012.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing
and domain adaptation with lr models and parser ensem-
bles. In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 1044?1050.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. Brat:
a web-based tool for nlp-assisted text annotation. In Pro-
ceedings of EACL?12, pages 102?107.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Junichi
Tsujii. 2005. Syntax annotation for the genia corpus. In
Proceedings of IJCNLP, volume 5, pages 222?227.
Paul Thompson, Raheel Nawaz, John McNaught, and Sophia
Ananiadou. 2011. Enriching a biomedical event corpus
with meta-knowledge annotation. BMC Bioinformatics,
12(1):393.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg Haken-
berg, and Ulf Leser. 2010. A comprehensive benchmark
of kernel methods to extract protein-protein interactions
from literature. PLoS Comput Biol, 6(7):e1000837, 07.
John Wilbur, Lawrence Smith, and Lorraine Tanabe. 2007.
BioCreative 2. Gene Mention Task. In L. Hirschman,
M. Krallinger, and A. Valencia, editors, Proceedings of
BioCreative II, pages 7?16.
75
