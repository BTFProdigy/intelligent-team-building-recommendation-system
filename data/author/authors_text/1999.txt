BioNLP 2007: Biological, translational, and clinical language processing, pages 97?104,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Shared Task Involving Multi-label Classification of Clinical Free Text
John P. Pestian1, Christopher Brew2, Pawe? Matykiewicz1,4,
DJ Hovermale2, Neil Johnson1, K. Bretonnel Cohen3,
W?odzis?aw Duch4
1Cincinnati Children?s Hospital Medical Center, University of Cincinnati,
2Ohio State University, Department of Linguistics,
3University of Colorado School of Medicine,
4Nicolaus Copernicus University, Torun?, Poland.
Abstract
This paper reports on a shared task involving
the assignment of ICD-9-CM codes to radi-
ology reports. Two features distinguished
this task from previous shared tasks in the
biomedical domain. One is that it resulted in
the first freely distributable corpus of fully
anonymized clinical text. This resource is
permanently available and will (we hope) fa-
cilitate future research. The other key fea-
ture of the task is that it required catego-
rization with respect to a large and commer-
cially significant set of labels. The number
of participants was larger than in any pre-
vious biomedical challenge task. We de-
scribe the data production process and the
evaluation measures, and give a preliminary
analysis of the results. Many systems per-
formed at levels approaching the inter-coder
agreement, suggesting that human-like per-
formance on this task is within the reach of
currently available technologies.
1 Introduction
Clinical free text (primary data about patients, as op-
posed to journal articles) poses significant technical
challenges for natural language processing (NLP).
In addition, there are ethical and social demands
when working with such data, which is intended for
use by trained medical practitioners who appreciate
the constraints that patient confidentiality imposes.
State-of-the-art NLP systems handle carefully edited
text better than fragmentary notes, and clinical lan-
guage is known to exhibit unique sublanguage char-
acteristics (Hirschman and Sager, 1982; Friedman
et al, 2002; Stetson et al, 2002) (e.g. verbless
sentences, domain-specific punctuation semantics,
and unusual metonomies) that may limit the perfor-
mance of general NLP tools. More importantly, the
confidentiality requirements take time and effort to
address, so it is not surprising that much work in
the biomedical domain has focused on edited jour-
nal articles (and the genomics domain) rather than
clinical free text in medical records. The fact re-
mains, however, that the automation of healthcare
workflows can bring important benefits to treatment
(Hurtado et al, 2001) and reduce administrative bur-
den, and that free text is a critical component of
these workflows. There are economic motivations
for the task, as well. The cost of adding labels like
ICD-9-CM to clinical free text and the cost of re-
pairing associated errors is approximately $25 bil-
lion per year in the US (Lang, 2007). For these
(and many other) reasons, there have been consis-
tent attempts to overcome the obstacles which hin-
der the processing of clinical text (Uzuner et al,
2006). This paper discusses one such attempt?
The 2007 Computational Medicine Challenge, here-
after referred to as ?the Challenge?. There were two
main reasons for conducting the Challenge. One
is to facilitate advances in mining clinical free text;
shared tasks in other biomedical domains have been
shown to drive progress in the field in multiple ways
(see (Hirschman and Blaschke, 2006; Hersh et al,
2005; Uzuner et al, 2006; Hersh et al, 2006) for a
comprehensive review of biomedical challenge tasks
and their contributions). The other is a ground-
97
breaking distribution of useful, reusable, carefully
anonymized clinical data to the research commu-
nity, whose data use agreement is simply to cite the
source. The remaining sections of this paper de-
scribe how the data were prepared, the methods for
scoring, preliminary results [to be updated if sub-
mission is accepted?results are currently still under
analysis], and some lessons learned.
2 Corpus collection and coding process
Supervised methods for machine learning require
training data. Yet, due to confidentiality require-
ments, spotty electronic availability, and variance in
recording standards, the requisite clinical training
data are difficult to obtain. One goal of the chal-
lenge was to create a publicly available ?gold stan-
dard? that could serve as the seed for a larger, open-
source clinical corpus. For this we used the follow-
ing guiding principles: individual identity must be
expunged to meet United States HIPAA standards,
(U.S. Health, 2002) and approved for release by the
local Institutional Review Board (IRB); the sample
must represent problems that medical records coders
actually face; the sample must have enough data for
machine-learning-based systems to do well; and the
sample must include proportionate representations
of very low-frequency classes.
Data for the corpus were collected from the
Cincinnati Children?s Hospital Medical Center?s
(CCHMC) Department of Radiology. CCHMC?s
Institutional Review Board approved release of the
data. Sampling of all outpatient chest x-ray and re-
nal procedures for a one-year period was done us-
ing a bootstrap method (Walters, 2004). These data
are among those most commonly used, and are de-
signed to provide enough codes to cover a substan-
tial proportion of pediatric radiology activity. Ex-
punging patient identity to meet HIPAA standards
included three steps: disambiguation, anonymiza-
tion, and data scrubbing (Pestian et al, 2005).
Ambiguity and Anonymization. Not surprisingly,
some degree of disambiguation is needed to carry
out effective anonymization (Uzuner et al, 2006;
Sibanda and Uzuner, 2006). The reason is that clini-
cal text is dense with medical jargon, abbreviations,
and acronyms, many of which turn out to be ambigu-
ous between a sense that needs anonymization and a
different sense that does not. For example, in a clin-
ical setting, FT can be an abbreviation for full-term,
fort (as in Fort Bragg), feet, foot, field test, full-time
or family therapy. Fort Bragg, being a place name,
and a possible component of an address, could indi-
rectly lead to identification of the patient. Until such
occurrences are disambiguated, it is not possible to
be certain that other steps to anonymize data are ad-
equate. To resolve the relevant ambiguities found in
this free text, we relied on previous efforts that used
expert input to develop clinical disambiguation rules
(Pestian et al, 2004).
Anonymization. To assure patient privacy, clin-
ical text that is used for non-clinical reasons must
be anonymized. However, to be maximally useful
for machine-learning, this must be done in a par-
ticular way. Replacing personal names with some
unspecific value such as ?*? would lose potentially
useful information. Our goal is to replace the sensi-
tive fields with like values that obscure the identity
of the individual (Cho et al, 2002). We found that
the amount of sensitive information routinely found
in unstructured free text data is limited. In our case,
these data included patient and physician names and
sometimes dates or geographic locations, but little or
no other sensitive information turned up in the rele-
vant database fields. Using our internally developed
encryption broker software, we replaced all female
names with ?Jane?, all male names with ?John?, and
all surnames with ?Johnson?. Dates were randomly
shifted.
Manual Inspection. Once the data were disam-
biguated and anonymized, they were manually re-
viewed for the presence of any Protected Health In-
formation (PHI). If a specific token was perceived to
potentially violate PHI regulations, the entire record
was deleted from the dataset. In some case, how-
ever, a general geographic area was changed and
not deleted. For example if the data read ?patient
lived near Mr. Roger?s neighborhood? it would be
deleted, because it may be traceable. On the other
hand, if the data read ?patient was from Cincinnati?
it may have been changed to read ?patient was from
the Covington? After this process, a corpus of 2,216
records was obtained (See Table 2 for details).
ICD-9-CM Assignment. A radiology report has
multiple components. Two parts in particular are
essential for the assignment of ICD-9-CM codes:
98
clinical history?provided by an ordering physician
before a radiological procedure, and impression?
reported by a radiologist after the procedure. In the
case of radiology reports, ICD-9-CM codes serve as
justification to have a certain procedure performed.
There are official guidelines for radiology ICD-9-
CM coding (Moisio, 2000). These guidelines note
that every disease code requires a minimum num-
ber of digits before reimbursement will occur; that
a definite diagnosis should always be coded when
possible; that an uncertain diagnosis should never
be coded; and that symptoms must never be coded
when a definite diagnosis is available. Particular
hospitals and insurance companies typically aug-
ment these principles with more specific internal
guidelines and practices for coding. For these rea-
sons of policy, and because of natural variation in
human judgment, it is not uncommon for multiple
annotators to assign different codes to the same text.
Understanding the sources of this variation is impor-
tant; so too is the need to create a definite gold stan-
dard for use in the challenge. To do so, data were
annotated by the coding staff of CCHMC and two
independent coding companies: COMPANY Y and
COMPANY Z.
Majority annotation. A single gold standard was
created from these three sets of annotations. There
was no reason to adopt any a priori preference for
one annotator over another, so the democratic princi-
ple of assigning a majority annotation was used. The
majority annotation consists of those codes assigned
to the document by two or more of the annotators.
There are, however, several possible problems with
this approach. For example, it could be that the ma-
jority annotation will be empty. This will be rare
(126 records out of 2,216 in our case), because it
only happens when the codes assigned by the three
annotators form disjoint sets. In most hospital sys-
tems, including our own, the coders are required to
indicate a primary code. By convention, the primary
code is listed as the record?s first code, and has an
especially strong impact on the billing process. For
simplicity?s sake, the majority annotation process ig-
nores the distinction between primary and secondary
codes. There is space for a better solution here, but
we have not seriously explored it. We have, how-
ever, conducted an analysis of agreement statistics
(not further discussed here) that suggests that the
overall effect of the majority method is to create a
coding that shares many statistical properties with
the originals, except that it reduces the effect of the
annotators? individual idiosyncrasies. The majority
annotation is illustrated in Table 1.
Our evaluation strategy makes the simplistic as-
sumption that the majority annotation is a true gold
standard and a worthwhile target for emulation. This
is debatable, and is discussed below, but for the sake
of definiteness we simply stipulate that submissions
will be compared against the majority annotation,
and that the best possible performance is to exactly
replicate said majority annotation.
3 Evaluation
Micro- and macro-averaging. Although we rank
systems for purposes of determining the top three
performers on the basis of micro-averaged F1, we
report a variety of performance data, including the
micro-average, macro-average, and a cost-sensitive
measure of loss. Jackson and Moulinier comment
(for general text classification) that: ?No agree-
ment has been reached...on whether one should pre-
fer micro- or macro-averages in reporting results.
Macro-averaging may be preferred if a classification
system is required to perform consistently across all
classes regardless of how densely populated these
are. On the other hand, micro-averaging may be
preferred if the density of a class reflects its impor-
tance in the end-user system? (Jackson and Moulin-
ier, 2002):160-161. For the present medical ap-
plication, we are more interested in the number of
patients whose cases are correctly documented and
billed than in ensuring good coverage over the full
range of diagnostic codes. We therefore emphasize
the micro-average.
A cost-sensitive accuracy measure. While F-
measure is well-established as a method for ranking,
there are reasons for wanting to augment this with
a cost-sensitive measure. An approach that allows
penalties for over-coding (a false positive) and
under-coding (a false negative) to be manipulated
has important implications. The penalty for under-
coding is simple?the hospital loses the amount of
revenue that it would have earned if it had assigned
the code. The regulations under which coding is
done enforce an automatic over-coding penalty of
99
Table 1: Majority Annotation
Hospital Company Y Company Z Majority
Document 1 AB BC AB AB
Document 2 BC ABD CDE BCD
Document 3 EF EF E EF
Document 4 ABEF ACEF CDEF ACEF
three times what is earned from the erroneous code,
with the additional risk of possible prosecution
for fraud. This motivates a generalized version of
Jaccard?s similarity metric (Gower and Legendre,
1986), which was introduced by Boutell, Shen, Luo
and Brown (Boutell et al, 2003).
Suppose that Yx is the set of correct labels for a test
set and Px is the set of labels predicted by some
participating system. Define Fx = Px ? Yx and
Mx = Yx ? Px , i.e. Fx is the set of false positives,
and Mx is the set of missed labels or false negatives.
The score is given by
score(Px) =
(
1?
?|Mx|+ ?|Fx|
|Yx ? Px|
)?
(1)
As noted in (Boutell et al, 2003), if ? = ? = 1 this
formula reduces to the simpler case of
score(Px) =
(
1?
|Yx ? Px|
|Yx ? Px|
)?
(2)
The discussion in (Boutell et al, 2003) points out
that constraints are necessary on ? and ? to ensure
that the inner term of the expression is non-negative.
We do not understand the way that they formulate
these constraints, but note that non-negativity will be
ensured if 0 ? ? ? 1 and 0 ? ? ? 1 . Since over-
coding is three times as bad as undercoding, we use
? = 1.0 , ? = 0.33 . Varying the value of ? would
affect the range of the scores, but does not alter the
rankings of individual systems. We therefore used
? = 1 . This measure does not represent the pos-
sibility of prosecution for fraud, because the costs
involved are incommensurate with the ones that are
represented. With these parameter settings, the cost-
sensitive measure produces rankings that differ con-
siderably from those produced by macro-averaged
balanced F-measure. For example, we shall see that
the system ranked third in the competition by macro-
averaged F-measure assigns a total of 1167 labels,
where the second-ranked assigns 1232, and the cost-
sensitive measure rewards this conservatism in as-
signing labels by reversing the ranking of the two
systems. In either case, the difference between the
systems is small (0.86% difference in F-measure,
0.53% difference in the cost-sensitive measure).
4 The Data
We selected for the challenge a subset of the com-
prehensive data set described above. The subset was
created by stratified sampling, such that it contains
20% of the documents in each category. Thus, the
proportion of categories in the sample is the same as
the proportion of categories in the full data set. We
included in the initial sample only those categories
to which 100 or more documents from the compre-
hensive data set were assigned. After the process
summarized in Table 2, the data were divided into
two partitions: a training set with 978 documents,
and a testing set with 976. Forty-five ICD-9-CM
labels (e.g 780.6) are observed in these data sets.
These labels form 94 distinct combinations (e.g. the
combination 780.6, 786.2). We required that any
combination have at least two exemplars in the data,
and we split each combination between the train-
ing and the test sets. So, there may be labels and
combinations of labels that occur only one time in
the training data, but participants can be sure that
no combination will occur in the test data that has
not previously occurred at least once in the train-
ing data. Our policy here has the unintended con-
sequence that any combination that appears exactly
once in the training data is highly likely to appear
exactly once in the test data. This gives unnecessary
information to the participants. In future challenges
we will drop the requirement for two occurrences in
the data, but ensure that single-occurrence combina-
tions are allocated to the training set rather than the
100
test set. This maintains the guarantee that there will
be no unseen combinations in the test data. The full
data set may be downloaded from the official chal-
lenge web-site.
5 Results
Notice of the Challenge was distributed using elec-
tronic mailing lists supplied by the Association of
Computational Linguistics, IEEE Computer Intelli-
gence and Data Mining, and American Medical In-
formatics Association?s Natural Language Process-
ing special interest group. Interested participants
were asked to register at the official challenge web-
site. Registration began February 1, 2007 and ended
February 28, 2007. Approximately 150 individu-
als registered from 22 countries and six continents.
Upon completing registration, an automated e-mail
was sent with the location of the training data. On
March 1, 2007 participants received notice of the
location of the testing data. Participants were en-
couraged to use the data for other purposes as long
as it was non-commercial and the appropriate cita-
tion was made. There were no other data use re-
strictions. Participants had until March 18, 2007
to submit their results and an explanation of their
model. Approximately 33% (50) of the partici-
pants submitted results. During the course of the
Challenge participants asked a range of questions.
These were posted to the official challenge web-site
- www.computationalmedicine.org/challenge.
The figure below is a scatterplot relating micro-
averaged F1 to the cost-sensitive measure described
above. Each point represents a system. The top-
performing systems achieved 0.8908, the minimum
was 0.1541, and the mean was 0.7670, with a SD
of 0.1340. There are 21 systems with a micro-
averaged F1 between 0.81 and 0.90. Another 14
have F1 > 0.70 . It is noticeable that the systems
are not ranked identically by the cost-sensitive and
the micro-averaged measure, but the differences are
small in each case.
A preliminary screening using a two-factor ANOVA
with system identity and diagnostic code as predic-
tive factors for balanced F-measure revealed a sig-
nificant main effect of both system and code. Pair-
wise t-tests using Holm?s correction for multiple
comparisons revealed no statistically significant dif-
Figure 1: Scatter plot of evaluation measures
ferences between the systems performing at F=0.70
or higher. Differences between the top system and a
system with a microaveraged F-measure of 0.66 do
come out significant on this measure.
We have also calculated (Table 3) the agreement
figures for the three individual annotations that
went into the majority gold standard. We see
that CCHMC outranks COMPANY Y on the cost-
sensitive measure, but the reverse is true for micro-
and macro-averaged F1, with the agreement be-
tween the hospital and the gold standard being espe-
cially low for the macro-averaged version. To under-
stand these figures, it is necessary to recall that the
gold standard is a majority annotation that is formed
from the the three component annotations. It appears
that for rare codes, which have a disproportionate
effect on the macro-averaged F, the majority anno-
tation is dominated by cases where company Y and
company Z assign the same code, one that CCHMC
did not assign.
The agreement figures are comparable to those of
the best automatic systems. If submitted to the
competition, the components of the majority anno-
tation would not have outranked the best systems,
even though the components contributed to the ma-
jority opinion. It is tempting to conclude that the
automated systems are close to human-level perfor-
mance. Recall, however, that while the hospital and
the companies did not have the luxury of exposure
to the majority annotation, the systems did have that
access, which allowed them to explicitly model the
properties of that majority annotation. A more mod-
erate conclusion is that the hospital and the compa-
nies might be able to improve (or at least adjust)
their annotation practices by studying the majority
101
Table 2: Characteristics of the data set through the development process.
Step Removed Total documents
One-year collection of documents 20,275
20 percent sample of one-year collection 4,055
Manual inspection for anonymization problems 1,839 2,216
Removal of records with no majority code 126 2,090
Removal of records with a code occurring only once 136 1,954
Table 3: Comparison of human annotators against majority.
Annotator Cost-sensitive Micro-averaged F1 Macro-averaged F1
HOSPITAL 0.9056 0.8264 0.6124
COMPANY Y 0.8997 0.8963 0.8973
COMPANY Z 0.8621 0.8454 0.8829
annotation and adapting as appropriate.
6 Discussion
Compared to other recent text classification shared
tasks in the biomedical domain (Uzuner et al, 2006;
Hersh et al, 2004; Hersh et al, 2005), this task re-
quired categorization with respect to a set of labels
more than an order of magnitude larger than previ-
ous evaluations. This increase in the size of the set
of labels is an important step forward for the field?
systems that perform well on smaller sets of cate-
gories do not necessarily perform well with larger
sets of categories (Jackson and Moulinier, 2002), so
the data set will allow for more thorough text cat-
egorization system evaluations than have been pos-
sible in the past. Another important contribution of
the work reported here may be the distribution of
the data?the first fully distributable, freely usable
data set of clinical text. The high number of partici-
pants and final submissions was a pleasant surprise;
we attribute this, among other things, to the fact that
this was an applied challenge, that real data were
supplied, and that participants were free to use these
data in other venues.
Participants utilized a diverse range of approaches.
These system descriptions are based on brief com-
ments entered into the submission box, and are ob-
viously subject to revision. The three highest scor-
ers all mentioned ?negation,? all seemed to be us-
ing the structure of UMLS in a serious way. The
better systems frequently mentioned ?hypernyms?
or ?synonyms,? and were apparently doing signifi-
cant amounts of symbolic processing. Two of the
top three had machine-learning components, while
one of the top three used purely symbolic methods.
The most common approach seems to be thought-
ful and medically-informed feature engineering fol-
lowed by some variety of machine learning. The
top-performing system used C4.5, suggesting that
use of the latest algorithms is not a pre-requisite for
success. SVMs and related large-margin approaches
to machine learning were strongly represented, but
did not seem to be reliably predictive of high rank-
ing.
6.1 Observations on running the task and the
evaluation
The most frequently viewed question of the FAQ
was related to a script to calculate the evaluation
score. This was supplied both as a downloadable
script and as an interactive web-page with a form for
submission. In retrospect, we realize that we had not
fully thought through what would happen as people
began to use this script. If we run a similar contest
in the future, we will be better prepared for the con-
fusion that this can cause.
A novel aspect of this task was that although we only
scored a single run on the test data, we allowed par-
ticipants to submit their ?final? run up to 10 times,
and to see their score each time. Note that although
102
participants could see how their score varied on suc-
cessive submissions, they did not have access to the
actual test data or to the correct answers, and so there
were no opportunities for special-purpose hacks to
handle special cases in the test data. The average
participant tried 5.27 (SD 3.17) submissions against
the test data. About halfway through the submis-
sion period we began to realize that in a competi-
tive situation, there are risks in providing the type
of feedback given on the submission form. In fu-
ture challenges, we will be judicious in selecting the
number of attempts allowed and the provision of any
type of feedback. As far as we can tell our general
assumption that the scientific integrity of the partic-
ipants was greater than the need to game the system
is true. It is good policy for those administering the
contest, however, to keep temptations to a minimum.
Our current preference would be to provide only the
web-page interface with no more than five attempts,
and to tell participants only whether their submis-
sion had been accepted, and if so, how many items
and how many codes were recognized.
We provided an XML schema as a precise and pub-
licly visible description of the submission format.
Although we should not have been, we were sur-
prised when changes to the schema were required
in order to accommodate small but unexpected vari-
ations in participant submissions. An even simpler
submission format would have been good. The ad-
vantage of the approach that we took was that XML
validation gave us a degree of sanity-checking at lit-
tle cost. The disadvantage was that some of the nec-
essary sanity-checking went beyond what we could
see how to do in a schema.
The fact that numerous participants generated sys-
tems with high performance indicates that the task
was reasonable, and that sufficient information
about the coding task was either provided by us or
inferred by the participants to allow them to do their
work. Since this is a first attempt, it is not yet clear
what the upper limits on performance are for this
task, but preliminary indications are that automated
systems are or will soon be viable as a component of
deployed systems for this kind of application.
7 Acknowledgements
The authors thank Aaron Cohen of the Oregon
Health and Science University for observations on
the inter-rater agreement between the three sources
and its relationship to the majority assignments, and
also for his input on testing for statistically signif-
icant differences between systems. We also thank
PERSON of ORGANIZATION for helpful com-
ments on the manuscript. Most importantly we
thank all the participants for their on-going commit-
ment, professional feedback and scientific integrity.
References
[Boutell et al, 2003] Boutell M., Shen X., Luo J. and
Brown C. 2003. Multi-label Semantic Scene Clas-
sification, Technical Report 813. Department of Com-
puter Science, University of Rochester September.
[Cho et al, 2002] Cho P. S., Taira R. K., and Kangarloo
H. 2002 Text boundary detection of medical reports.
Proceedings of the Annual Symposium of the American
Medical Informatics Association, 998.
[Friedman et al, 2002] Friedman C., Kra P., and Rzhetsky
A. 2002. Two biomedical sublanguages: a descrip-
tion based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35:222?235.
[Gower and Legendre, 1986] Gower J. C. and Legendre P.
1986. Metric and euclidean properties of dissimilarity
coefficient. Journal of Classification, 3:5?48.
[Hersh et al, 2004] Hersh W., Bhupatiraju R. T., Ross L.,
Roberts P., Cohen A. M., and Kraemer D. F. 2004.
TREC 2004 Genomics track overview. Proceedings of
the 13th Annual Text Retrieval Conference. National
Institute of Standards and Technology.
[Hersh et al, 2006] Hersh W., Cohen A. M., Roberts P.,
and Rekapalli H. K. 2006. TREC 2006 Genomics
track overview. Proceedings of the 15th Annual Text
Retrieval Conference National Institute of Standards
and Technology.
[Hersh et al, 2005] Hersh W., Cohen A. M., Yang J.,
Bhupatiraju R. T., Roberts P., and Hearst M. 2005.
TREC 2005 Genomics track overview. Proceedings of
the 14th Annual Text Retrieval Conference. National
Institute of Standards and Technology.
[Hirschman and Blaschke, 2006] Hirschman L. and
Blaschke C. 2006. Evaluation of text mining in
biology. Text mining for biology and biomedicine,
Chapter 9. Ananiadou S. and McNaught J., editors.
Artech House.
103
[Hirschman and Sager, 1982] Hirschman L. and Sager S.
1982. Automatic information formatting of a medi-
cal sublanguage. Sublanguage: studies of language in
restricted semantic domains, Chapter 2. Kittredge R.
and Lehrberger J., editors. Walter de Gruyter.
[Hurtado et al, 2001] Hurtado M. P, Swift E. K., and Cor-
rigan J. M. 2001. Crossing the Quality Chasm: A
New Health System for the 21st Century. Institute of
Medicine, National Academy of Sciences.
[Jackson and Moulinier, 2002] Jackson P. and Moulinier
I. 2002. Natural language processing for online appli-
cations: text retrieval, extraction, and categorization.
John Benjamins Publishing Co.
[Lang, 2007] Lang, D. 2007. CONSULTANT REPORT
- Natural Language Processing in the Health Care In-
dustry. Cincinnati Children?s Hospital Medical Cen-
ter, Winter 2007.
[Moisio, 2000] Moisio M. 2000. A Guide to Health Care
Insurance Billing. Thomson Delmar Learning, Clifton
Park.
[Pestian et al, 2005] Pestian J. P., Itert L., Andersen C. L.,
and Duch W. 2005. Preparing Clinical Text for Use in
Biomedical Research. Journal of Database Manage-
ment, 17(2):1-12.
[Pestian et al, 2004] Pestian J. P., Itert L., and Duch W.
2004. Development of a Pediatric Text-Corpus for
Part-of-Speech Tagging. Intelligent Information Pro-
cessing and Web Mining, Advances in Soft Computing,
219?226 New York, Springer Verlag.
[Sammuelsson and Wiren, 2000] Sammuelsson C. and
Wiren M. 2000. Parsing Techniques. Handbook of
Natural Language Processing, 59?93. Dale R., Moisl
H., Somers H., editors. New York, Marcel Deker.
[Sibanda and Uzuner, 2006] Sibanda T. and Uzuner O.
2006. Role of local context in automatic deidentifica-
tion of ungrammatical, fragmented text. Proceedings
of the Human Language Technology conference of the
North American chapter of the Association for Com-
putational Linguistics, 65?73.
[Stetson et al, 2002] Stetson P. D., Johnson S. B., Scotch
M., and Hripcsak G. 2002. The sublanguage of cross-
coverage. Proceedings of the Annual Symposium of
the American Medical Informatics Association, 742?
746.
[U.S. Health, 2002] U.S. Heath & Human Services.
2002. 45 CFR Parts 160 and 164 Standards for Privacy
of Individually Identifiable Health Information Final
Rule Federal Register, 67(157):53181?53273.
[Uzuner et al, 2006] Uzuner O., Szolovits P., and Kohane
I. 2006. i2b2 workshop on natural language process-
ing challenges for clinical records. Proceedings of the
Fall Symposium of the American Medical Informatics
Association.
[Walters, 2004] Walters S. J. 2004. Sample size and
power estimation for studies with health related quality
of life outcomes: a comparison of four methods using
the SF-36 Health and Quality of Life Outcomes, 2:26.
104
Proceedings of the Workshop on BioNLP, pages 179?184,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Clustering semantic spaces of suicide notes and newsgroups articles
P. Matykiewicz1,2, W. Duch2, J. Pestian1
1Cincinnati Children?s Hospital Medical Center, University of Cincinnati,
2Nicolaus Copernicus University, Torun?, Poland.
Abstract
Historically, suicide risk assessment has re-
lied on question-and-answer type tools. These
tools, built on psychometric advances, are
widely used because of availability. Yet there
is no known tool based on biologic and cogni-
tive evidence. This absence often cause a vex-
ing clinical problem for clinicians who ques-
tion the value of the result as time passes. The
purpose of this paper is to describe one exper-
iment in a series of experiments to develop a
tool that combines Biological Markers (Bm)
with Thought Markers (Tm), and use machine
learning to compute a real-time index for as-
sessing the likelihood repeated suicide attempt
in the next six-months. For this study we fo-
cus using unsupervised machine learning to
distinguish between actual suicide notes and
newsgroups. This is important because it gives
us insight into how well these methods dis-
criminate between real notes and general con-
versation.
1 Introduction
It is estimated that each year 800,000 die by suicide
worldwide (World Health Organization, 2001). In
the United States, suicide ranks second as the lead-
ing cause of death among 25-34 year-olds and the
third leading cause of death among 15-25 year-olds
(Kung et al, 2008). The challenge for those who
care for suicide attempters, such as an Emergency
Medicine clinicians, is to assess the likelihood of an-
other attempt, a more lethal one. We believe to fully
asses this risk a tool must be developed that mea-
sures both the biological and cognitive state of the
patient. Such a tool will include Biological Mark-
ers (Bm): measured by the concentration of cer-
tain biochemical markers, Thought Markers (Tm):
measured by artifacts of thought that have been re-
duced to writing or transcribe speech, and Clini-
cal Markers (Cm): measured by traditional clinical
risk factors. In this study we focus on the Tm be-
cause of BioNLP?s important role. Here, we employ
machine-learning analysis to examine suicide notes
and how these notes compare to newsgroups. This
is one experiment in a series of experiments that are
intended to provide insight into how best to apply
linguistic tools when responding to suicidal patients.
To gain insight into the suicidal mind, researchers
have suggested empirically analyzing national mor-
tality statistics, psychological autopsies, nonfatal
suicide attempts and documents such as suicide
notes (Shneidman and Farberow, 1957; Maris,
1981). Most suicide notes analysis has focused
on classification and theoretical-conceptual analysis.
Content analysis has been limited to extracting ex-
plicit information from a suicide note, e.g., length of
the message, words, and parts of speech (Ogilvie et
al., 1969). Classification analysis uses data such as
age, sex, marital status, educational level, employ-
ment status and mental disorder (Ho et al, 1998;
Girdhar et al, 2004; Chavez et al, 2006; Demirel
et al, 2007). Only a very few studies have utilized
theoretical-conceptual analysis , despite the asser-
tion in the first formal study of suicide notes (Shnei-
dman and Farberow, 1957) that such an analysis has
much promise. So, the inconclusive nature of the
methods of analysis has limited their application to
patient care.
179
Our own research has taken a different approach.
In particular we first wanted to determine if mod-
ern machine learning methods could be applied to
free-text from those who committed suicide. Our
first experiment focused on the the ability of ma-
chine learning to distinguish between real suicide
notes and elicited suicide notes as well as mental
health professionals. This is an important question
since all current care is based on a mental health pro-
fession?s interpretation. Our findings showed that
mental health professionals accurately selected gen-
uine suicide notes 50% of the time and the super-
vised machine learning methods were accurate 78%
(Pestian et al, 2008). In this study we shift from
supervised to unsupervised machine learning meth-
ods. Even though these methods have rich history
we know of no research that has applied them to
suicide notes. Our rationale for this study, then, is
that since our ultimate goal is to create a Suicide
Risk Index that incorporates biological and thought
markers it is important to determine if unsupervised
methods can distinguish between suicidal and non-
suicidal writings. To conduct this research we de-
veloped a corpus of over 800 suicide notes from in-
dividuals who had committed suicide, as opposed to
those who attempted or ideated about suicide. This
is an important contribution and, as far as we know,
it is the largest ever developed. It spans 70 years of
notes, and now includes multiple languages. Details
of this corpus are described below. We also created
a corpus of data from various newsgroups that acted
as non-suicidal writings. These corpora were used
to conduct the analysis. The sections below describe
the cluster analysis process and results.
2 Data
Suicide Notes Corpus
Data for the suicide note database were collected
from around the United States. They were either
in a hand written or typed written form. Once the
note was acquired it was scanned into the database.
Optical character recognition was attempted on the
typed written notes, but not accurate, so the notes
were read from the scanned version and type into the
database exactly as seen. A second person reviewed
what was typed. There were limitation in collecting
deceased demographics. The table 1 provides vari-
ous descriptive statistics.
Newsgroup Corpus
Newsgroup data was selected because it was conve-
nient and as close to normal discourse as we could
find. We understood that and ideal comparison
group would be composed of Internet blogs or e-
mails that were written by suicide ideators. True,
a Google query of ?suicide blog? yields millions
of response, a review of many of these responses
shows that the data are of little use for this analy-
sis. In our opinion, the next suitable corpora was
found in a 20 newsgroup collection from the Uni-
versity of California in Irvine (UCI) machine learn-
ing repository1 . Most of the newsgroups have no
relevance to suicide notes. Since our hypothesis
is that unsupervised learning methods can tell the
difference between suicidal and non-suicidal writ-
ing we selected discussions that we believed may
have some similarity to suicide writings. This se-
lection was based on reviewing the newsgroups
with experts. We had conjectured that if an unsu-
pervised method could distinguish between similar
clusters those methods could distinguish between
dissimilar clusters. The newsgroups ultimately se-
lected were talk.politics.guns, talk.politics.mideast,
talk.politics.misc, talk.religion.misc. Each news-
group contains 1000 articles (newsgroup postings).
Headers and quotes from other postings were re-
moved.
3 Methods
Basic statistics are calculated using variables ex-
tracted by Linguistic Inquiry and Word Count ver-
sion 2007 software (LIWC2007) (Chung and Pen-
nebaker, 2007). J. W. Pennebaker, C. K. Chung, M.
Ireland, A. Gonzales, and R. J. Booth created an an-
notated dictionary. Each word in the dictionary is
assigned to at least one of the following high level
category: linguistic process, psychological process,
personal concern, or spoken category. These cat-
egories provide an efficient and effective method
for studying the various emotional, cognitive, and
structural components present in individuals? verbal
and written speech samples (Chung and Pennebaker,
2007; Pennebaker et al, 2001). Here it is used to
analyze differences between suicide notes and news-
1http://archive.ics.uci.edu/ml/
180
group articles.
Feature space was prepared using open source al-
gorithms available in Perl language2 . First, Brian
Duggan spell checking software that uses aspell li-
brary was used (Text::SpellChecker module3). Then,
tokenizer created by Aaron Coburn was used (Lin-
gua::EN::Tagger module2) to extract words was ap-
plied. After that, words were filtered with 319 ele-
ment stop word list 4. Next, the Richardson/Franz
English stemmer was included in the pre-processing
stage (Lingua::Stem module2). Features that ap-
peared in less than 10 documents or in more than 500
documents were removed. Documents that had less
than 10 features or more than 500 were removed.
Finally, columns and rows were normalized to have
unitary lengths. These last steps of pre-processing
are used to reduce outliers.
Calculations are done using open source software
called R5. Clustering is done with the following al-
gorithms: expectation maximization (EM) (Witten
and Frank, 2000), simple k-means with euclidean
distance (SKM) (Witten and Frank, 2000), and
sequential information bottleneck algorithm (sIB)
(Slonim et al, 2002). The last approach has been
shown to work well work well when clustering doc-
uments. Specificity, sensitivity and F1 measure are
used as performance measures (Rijsbergen, 1979).
Multidimensional scaling with euclidean distance
measures is used for visualization purposes (Cox
and Cox, 1994).
To extract features that represent each cluster,
Pearson correlation coefficient is used. The correla-
tion coefficient r is calculated between each feature
and each cluster separately r(wi, cj) where wi is ith
word and cj is jth cluster. N best features with the
highest values for each cluster are selected as most
representative.
4 Results
Descriptive statistics for the data sets are listed in
table 1. It shows syntactic differences between lan-
guage use in suicide notes and newsgroups when
Lingua::EN::Tagger is used.
2http://www.perl.org
3http://search.cpan.org
4http://www.dcs.gla.ac.uk/idom/ir resources/
/linguistic utils/stop words
5http://www.r-project.org
Table 1: Descriptive statistics of suicide note corpus and
newsgroups.
suicide
corpus
newsgroups
Sample Size 866 4000 (1000
per group)
Collection Years 1945-2009 1992-1993
Avg tokens per record
(SD)
105 (154) 243 (582)
Range of tokens per
record
1-1837 0-11024
Average (SD) nouns 25.21 (34.81) 77.19
(181.63)
Average (SD) pronouns 16.58 (26.69) 18.05 (63.18)
Average (SD) verbs 21.07 (32.82) 41.31
(109.23)
Average (SD) adjec-
tives
6.43 (9.81) 16.92 (36.45)
Table 2 summarizes information about the lin-
guistic and psychological processes of the data.
The idea of ?process? is derived from the Lin-
guistic Inquiry and Word Count (LIWC2007) soft-
ware (Chung and Pennebaker, 2007). This software
conducts traditional natural language processing by
placing various word into categories. For example,
sixltrs includes words that are at least six letters in
length. A full description of this software, dictio-
naries, reliability and validity tests can be found on
LIWC?s website. 6. Table 2 shows that suicide notes
are, in many ways, different than normal text. For
our study this provides inspiration for continued re-
search.
Table 2: Mean and standard deviation in linguistic and
psychological processes. Selected categories with small-
est p-values (<0.0001) are shown.
suicide guns mideast politics religion
artcl 3.31 (2.79) 7.80 (3.52) 7.37 (3.34) 7.21 (3.40) 7.07 (3.51)
sixltrs 14.20 (7.34) 21.22 (6.32) 23.24 (7.03) 22.41 (7.13) 21.37 (7.87)
prnoun 16.75 (6.82) 11.96 (5.15) 10.64 (4.92) 11.77 (5.18) 13.21 (5.76)
prepos 10.61 (4.35) 12.13 (3.97) 12.89 (3.89) 12.21 (3.97) 11.75 (4.07)
verb 14.69 (5.99) 12.75 (4.72) 11.54 (4.74) 12.72 (4.63) 13.54 (4.97)
biolog 2.70 (3.04) 0.93 (1.27) 0.85 (1.50) 1.59 (2.08) 1.10 (1.75)
affctiv 7.71 (5.39) 4.83 (2.87) 4.77 (3.45) 4.90 (3.18) 5.10 (3.93)
cognitv 12.68 (5.76) 16.14 (5.93) 14.72 (5.62) 16.00 (5.49) 17.14 (6.17)
social 10.45 (5.86) 8.10 (4.20) 8.43 (4.71) 8.76 (4.37) 9.06 (5.17)
The four newsgroup data sets are combined
as follows: talk.politics.guns + suicide notes
= guns, talk.politics.mideast + suicide notes =
mideast, talk.politics.misc + suicide notes = politics,
6http://www.liwc.net/liwcdescription.php#index1
181
talk.religion.misc + suicide notes = religion. Each
data set contained 1866 documents before document
and feature selection is applied. Table 3 has final
number of features while table 4 has final number of
documents. In general sIB clustering algorithm per-
formed best for all data sets with respect to F1 mea-
sure (mean = 0.976, sd = 0.008). The average score
also did not change when the number of clusters var-
ied from two to six (mean = 0.973, sd = 0.012). Per-
formance of k-means and expectation maximization
algorithm was much worse. If number of clusters
was varied between two and six for different data
sets the algorithms achieved F1 measure 0.146 lower
than sIB (SKM mean = 0.831, sd = 0.279, EM mean
= 0.824, sd = 0.219). Table 3 summarizes perfor-
mance of best algorithms for each data set if two
clusters are chosen.
Table 3: Best clustering algorithms for each newsgroup
when clustered with suicide notes in case of two clus-
ters (alg = clustering algorithm, sens = sensitivity, spec
= specificity, F1 = F1 measure, #f = number of features,
sIB = sequential information bottleneck, SKM = simple
k-means).
dataset al sens spec F1 #f
guns sIB .9689 .9834 .9721 1658
mideast sIB .9837 .9942 .9877 2023
politics SKM .9705 .9889 .9769 1694
religion sIB .9787 .9700 .9692 1553
If the desired number of clusters is increased to
four then two major sub-groups are discovered in
suicide notes: emotional (represented by words like:
love, forgive, hope, and want) and non-emotional
(represented by words like: check, bank, and no-
tify). Example of the first type of note might be
(suicide note was annonymized and misspellings left
unchanged):
Jane I am bitterly sorry for what I have done to
you. Please try to forgive me. I can?t live with-
out you and you don?t want me. I can?t blame you
though. But I love you very much. I didn?t act like it
but I did and still do. Please try to be happy, Jane.
That is all I ask. I try hope for the best for you and
I guess that is all there is for me to say. Good by.
John Johnson. Please mail this to Mom. Mrs. Jane
Johnson. Cincinnati, OH.
Example of a non-emotional suicide note might be:
There is no use living in pains. That arthritis and
hardening of the arteries are too much for me. There
are two hundred and five dollars in the bank, and
here are fifty- five dollars and eight cents. I hope that
will be enough for my funeral. You have to notify the
Old Age Assistance Board. Phone - 99999.
Table 4 shows best five ranked features for each clus-
ter for each data set according to correlation coeffi-
cient CC . Features are in the order of rank so that
feature with the highest CC is first. Even though
that we use different newsgroups as control groups
same sub-groups of suicide notes are discovered.
sIB is the most stable and best performing algorithm
in this experiment so it was used to discover those
clusters. Stemmed word that appear in best five
ranked features in at least three data sets are marked
bold.
Figures 1, 2, 3, and 4 show high-dimensional doc-
ument/stemmed word feature space projected on a
two dimensional plane using multidimensional scal-
ing (MDS) initialized by principal component analy-
sis. Each figure has different rotation but the shapes
are similar. In addition MDS shows very little mix-
ing of suicide notes and newsgroups which is also
explained by results in the table 3.
Figure 1: MDS showing suicide notes and
talk.politics.guns articles (s character in the figure
means suicide note while a character depicts newsgroup
article, colors are used as cluster numbers).
182
Table 4: Best five features when four clusters are created
by the sIB algorithm (#c = cluster number, #a = number
of newsgroup articles in a cluster, #s = number of suicide
notes in a cluster). Stemmed word that appear in best five
ranked features in at least three data sets are marked bold.
dataset #c stemmed words #a #s
guns 1 address, bank, bond, notifi,
testam
28 204
guns 2 clinton, fbi, foreign, jim,
spea
318 2
guns 3 forgiv, god, hope, love,
want
4 381
guns 4 crime, firearm, gun, law,
weapon
541 8
mideast 1 appressian, armenia, arme-
nian, ohanu, proceed
464 5
mideast 2 arab, congress, isra, israel,
jew
379 4
mideast 3 bank, check, funer, insur,
testam
10 233
mideast 4 forgiv, good, hope, love,
want
2 355
politics 1 compound, disclaim, fbi,
govern, major
593 12
politics 2 clayton, cramer, optilink,
relat, uunet
274 1
politics 3 bank, box, check, funer,
notifi
11 258
politics 4 forgiv, good, hope, life,
love
11 330
religion 1 bank, bond, check, notifi,
paper
36 192
religion 2 frank, object, observ, the-
ori, valu
279 0
religion 3 activ, christian, jesu, ko-
resh, net
502 10
religion 4 forgiv, hope, love, sorri,
want
12 395
5 Conclusions
Our findings suggest that unsupervised methods can
distinguish between suicide notes and newsgroups,
our proxy for general discussion. This is important
because it is helpful in determining if NLP can be
useful when integrating thought markers with bio-
logical and clinical markers (f(Bm, Tm, Cm)). In
other words, can an NLP tools accurately distin-
guish between suicidal and normal thought markers
(T Sm 6= TNm )? Moreover these unsupervised meth-
ods have shown an ability to find sub-groups of sui-
cide notes even when other types of newsgroups are
present. In our analysis, one subgroup showed no
Figure 2: MDS showing suicide notes and
talk.politics.mideast articles (s character in the fig-
ure means suicide notes while a character depicts
newsgroup article, colors are used as cluster numbers).
Figure 3: MDS showing suicide notes and
talk.politics.misc articles (s character in the figure
means suicide note while a character depicts newsgroup
article, colors are used as cluster numbers).
emotional content while the other was emotionally
charged. This finding is consistent with Tuckman?s,
1959 work that showed suicide notes fall into six
emotional categories: emotionally neutral, emotion-
ally positive, emotionally negative directed inward,
emotionally negative directed outward, emotionally
negative directed inward and outward (Tuckman et
al., 1959). The next step in developing a Suicide
Risk Index is to conduct a clinical trail in the Emer-
gency Department that will collect Bm, Tm, Cm
and test multiple methods for computing the Suicide
183
Figure 4: MDS showing suicide notes and
talk.religion.misc articles (s character in the figure
means suicide note while a character depicts newsgroup
article, colors are used as cluster numbers).
Risk Index.
References
A. Chavez, D. Paramo-Castillo, A. Leenaars, and
L. Leenaars. 2006. Suicide notes in mexico: What do
they tell us? Suicide and Life-Threatening Behavior,
36:709?715.
C.K. Chung and J.W. Pennebaker, 2007. The
psychological functions of function words, pages 343?
359. New York: Psychology Press.
T. F. Cox and M. A. A. Cox. 1994. Multidimensional
Scaling. Chapman and Hall.
B. Demirel, T. Akar, A. Sayin, S. Candansayar, and
A Leenaars. 2007. Farewell to the world: Sui-
cide notes from turkey. Suicide and Life-Threatening
Behavior, 38:123?128.
S. Girdhar, A. Leenaars, T.D. Dogra, L. Leenaars, and
G. Kumar. 2004. Suicide notes in india: what do they
tell us? Archives of Suicide Research, 8:179?185.
T. Ho, P. Yip, C. Chiu, and P. Halliday. 1998. Sui-
cide notes: what do they tell us? Acta Psychiatrica
Scandinavica, 98:467?473.
Hsiang-Ching Kung, Donna L. Hoyert, Jiaquan Xu, and
Sherry L. Murphy. 2008. Deaths: Final data for 2005.
National Vital Statistics Report, 56:1?121.
R. Maris. 1981. Pathways to suicide. John Hopkins
University Press, Baltimore, MD.
D Ogilvie, P. Stone, and E. Shneidman. 1969. Some
characteristics of genuine versus simulated suicide
notes. Bulletin of Suicidology, 1:17?26.
J. W. Pennebaker, M. E. Francis, and R. J. Booth. 2001.
Linguistic Inquiry and Word Count: LIWC. Lawrence
Erlbaum Associates, Mahwah, NJ, 2nd edition.
J. P. Pestian, P. Matykiewicz, J. Grupp-Phelan,
S. Arszman-Lavanier, J. Combs, and Robert Kowatch.
2008. Using natural language processing to clas-
sify suicide notes. In AMIA Annual Symposium
Proceedings, volume 2008. American Medical Infor-
matics Association.
C. J. Van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann, Newton, MA, USA.
E Shneidman and N Farberow. 1957. Clues to Suicide.
McGraw Hill Paperbacks.
Noam Slonim, Nir Friedman, and Naftali Tishby. 2002.
Unsupervised document classification using sequential
information maximization. In Proceedings of the 25th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 129?
136.
Jacob Tuckman, Robert J. Kleiner, and Martha Lavell.
1959. Emotional content of suicide notes. Am J
Psychiatry, 116(1):59?63.
Ian H. Witten and Eibe Frank. 2000. Data mining:
practical machine learning tools and techniques with
Java implementations. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA.
World Health Organization, 2001. Burden of mental and
behavioral disorders, pages 19?45. World Health Or-
ganization, Geneva.
184
