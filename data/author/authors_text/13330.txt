Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 430?438, Prague, June 2007. c?2007 Association for Computational Linguistics
Smooth Bilingual N-gram Translation
Holger Schwenk Marta R. Costa-jussa` and Jose? A.R. Fonollosa
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
schwenk@lismi.fr
UPC - TALP
Barcelona 08034, Spain
{mruiz,adrian}@gps.tsc.upc.edu
Abstract
We address the problem of smoothing trans-
lation probabilities in a bilingual N-gram-
based statistical machine translation system.
It is proposed to project the bilingual tuples
onto a continuous space and to estimate the
translation probabilities in this representa-
tion. A neural network is used to perform the
projection and the probability estimation.
Smoothing probabilities is most important
for tasks with a limited amount of training
material. We consider here the BTEC task
of the 2006 IWSLT evaluation. Improve-
ments in all official automatic measures are
reported when translating from Italian to En-
glish. Using a continuous space model for
the translation model and the target language
model, an improvement of 1.5 BLEU on the
test data is observed.
1 Introduction
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Among all possible target language sen-
tences the one with the highest probability is chosen:
e? = arg max
e
Pr(e|f) = arg max
e
Pr(f |e) Pr(e)
where Pr(f |e) is the translation model and Pr(e)
is the target language model. This approach is
usually referred to as the noisy source-channel ap-
proach in statistical machine translation (Brown et
al., 1993).
During the last few years, the use of context
in SMT systems has provided great improvements
in translation. SMT has evolved from the origi-
nal word-based approach to phrase-based translation
systems (Och et al, 1999; Koehn et al, 2003). A
phrase is defined as a group of source words f? that
should be translated together into a group of target
words e?. The translation model in phrase-based sys-
tems includes the phrase translation probabilities in
both directions, i.e. P (e?|f?) and P (f? |e?).
The use of a maximum entropy approach simpli-
fies the introduction of several additional models ex-
plaining the translation process :
e? = arg max p(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models and
the ?i weights are typically optimized to maximize
a scoring function on a development set (Och and
Ney, 2002).
The phrase translation probabilities P (e?|f? ) and
P (f? |e?) are usually obtained using relative frequency
estimates. Statistical learning theory, however, tells
us that relative frequency estimates have several
drawbacks, in particular high variance and low bias.
Phrase tables may contain several millions of en-
tries, most of which appear only once or twice,
which means that we are confronted with a data
sparseness problem. Surprisingly, there seems to be
little work addressing the issue of smoothing of the
phrase table probabilities.
On the other hand, smoothing of relative fre-
quency estimates was extensively investigated in the
430
area of language modeling. A systematic compari-
son can be for instance found in (Chen and Good-
man, 1999). Language models and phrase tables
have in common that the probabilities of rare events
may be overestimated. However, in language mod-
eling probability mass must be redistributed in order
to account for the unseen n-grams. Generalization
to unseen events is less important in phrase-based
SMT systems since the system searches only for the
best segmentation and the best matching phrase pair
among the existing ones.
We are only aware of one work that performs a
systematic comparison of smoothing techniques in
phrase-based machine translation systems (Foster et
al., 2006). Two types of phrase-table smoothing
were compared: black-box and glass-box methods.
Black-methods do not look inside phrases but in-
stead treat them as atomic objects. By these means,
all the methods developed for language modeling
can be used. Glass-box methods decompose P (e?|f?)
into a set of lexical distributions P (e|f? ). For in-
stance, it was suggested to use IBM-1 probabili-
ties (Och et al, 2004), or other lexical translation
probabilities (Koehn et al, 2003; Zens and Ney,
2004). Some form of glass-box smoothing is now
used in all state-of-the-art statistical machine trans-
lation systems.
Another approach related to phrase table smooth-
ing is the so-called N-gram translation model
(Marin?o et al, 2006). In this model, bilingual tu-
ples are used instead of the phrase pairs and n-gram
probabilities are considered rather than relative fre-
quencies. Therefore, smoothing is obtained us-
ing the standard techniques developed for language
modeling. In addition, a context dependence of the
phrases is introduced. On the other hand, some
restrictions on the segmentation of the source sen-
tence must be used. N-gram-based translation mod-
els were extensively compared to phrase-based sys-
tems on several tasks and typically achieve compa-
rable performance.
In this paper we propose to investigate improved
smoothing techniques in the framework of the N-
gram translation model. Despite the undeniable suc-
cess of n-graam back-off models, these techniques
have several drawbacks from a theoretical point of
view: the words are represented in a discrete space,
the vocabulary. This prevents ?true interpolation? of
the probabilities of unseen n-grams since a change
in this word space can result in an arbitrary change
of the n-gram probability. An alternative approach
is based on a continuous representation of the words
(Bengio et al, 2003). The basic idea is to convert
the word indices to a continuous representation and
to use a probability estimator operating in this space.
Since the resulting distributions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. Prob-
ability estimation and interpolation in a continuous
space is mathematically well understood and numer-
ous powerful algorithms are available that can per-
form meaningful interpolations even when only a
limited amount of training material is available. This
approach was successfully applied to language mod-
eling in large vocabulary continuous speech recogni-
tion (Schwenk, 2007) and to language modeling in
phrase-based SMT systems (Schwenk et al, 2006).
In this paper, we investigate whether this ap-
proach is useful to smooth the probabilities involved
in the bilingual tuple translation model. Reliable es-
timation of unseen n-grams is very important in this
translation model. Most of the trigram tuples en-
countered in the development or test data were never
seen in the training data. N-gram hit rates are re-
ported in the results section of this paper. We report
experimental results for the BTEC corpus as used
in the 2006 evaluations of the international work-
shop on spoken language translation IWSLT (Paul,
2006). This task provides a very limited amount
of resources in comparison to other tasks like the
translation of journal texts (NIST evaluations) or of
parliament speeches (TC-STAR evaluations). There-
fore, new techniques must be deployed to take the
best advantage of the limited resources. Among the
language pairs tested in this years evaluation, Ital-
ian to English gave the best BLEU results in this
year evaluation. The better the translation quality is,
the more it is challenging to outperform it without
adding more data. We show that a new smoothing
technique for the translation model achieves a sig-
nificant improvement in the BLEU score for a state-
of-the-art statistical translation system.
This paper is organized as follows. In the next
section we first describe the baseline statistical ma-
chine translation systems. Section 3 presents the ar-
chitecture and training algorithms of the continuous
431
space translation model and section 4 summarizes
the experimental evaluation. The paper concludes
with a discussion of future research directions.
2 N-gram-based Translation Model
The N -gram-based translation model has been de-
rived from the finite-state perspective; more specif-
ically, from the work of Casacuberta (2001). How-
ever, different from it, where the translation model
is implemented by using a finite-state transducer,
the N -gram-based system implements a bilingual
N -gram model. It actually constitutes a language
model of bilingual units, referred to as tuples, which
approximates the joint probability between source
and target languages by using N -grams, such as de-
scribed by the following equation:
p(e, f) ?
K
?
k=1
p((e, f)k|(e, f)k?1, . . . , (e, f)k?4)
(2)
where e refers to target, f to source and (e, f)k to
the kth tuple of a given bilingual sentence pair.
Bilingual units (tuples) are extracted from any
word-to-word alignment according to the following
constraints:
? a monotonic segmentation of each bilingual
sentence pairs is produced,
? no word inside the tuple is aligned to words
outside the tuple, and
? no smaller tuples can be extracted without vio-
lating the previous constraints.
As a consequence of these constraints, only one
segmentation is possible for a given sentence pair.
Two important issues regarding this translation
model must be considered. First, it often occurs that
a large number of single-word translation probabil-
ities are left out of the model. This happens for all
words that are always embedded in tuples contain-
ing two or more words, then no translation probabil-
ity for an independent occurrence of these embed-
ded words will exist. To overcome this problem, the
tuple trigram model is enhanced by incorporating
1-gram translation probabilities for all the embed-
ded words detected during the tuple extraction step.
These 1-gram translation probabilities are computed
from the intersection of both the source-to-target and
the target-to-source alignments.
The second issue has to do with the fact that some
words linked to NULL end up producing tuples with
NULL source sides. Since no NULL is actually ex-
pected to occur in translation inputs, this type of tu-
ple is not allowed. Any target word that is linked to
NULL is attached either to the word that precedes
or the word that follows it. To determine this, an ap-
proach based on the IBM1 probabilities was used, as
described in (Marin?o et al, 2006).
2.1 Additional features
The following feature functions were used in the N-
gram-based translation system:
? A target language model. In the baseline sys-
tem, this feature consists of a 4-gram back-off
model of words, which is trained from the tar-
get side of the bilingual corpus.
? A source-to-target lexicon model and a
target-to-source lexicon model. These fea-
ture, which are based on the lexical parameters
of the IBM Model 1, provide a complementary
probability for each tuple in the translation ta-
ble.
? A word bonus function. This feature intro-
duces a bonus based on the number of target
words contained in the partial-translation hy-
pothesis. It is used to compensate for the sys-
tem?s preference for short output sentences.
All these models are combined in the de-
coder. Additionally, the decoder allows for a
non-monotonic search with the following distorsion
model.
? A word distance-based distorsion model.
P (tK1 ) = exp(?
K
?
k=1
dk)
where dk is the distance between the first word
of the kth tuple (unit), and the last word+1 of
the (k ? 1)th tuple.
432
Figure 1: Comparing regular and unfolded tuples.
Distance are measured in words referring to the units
source side.
To reduce the computational cost we place lim-
its on the search using two parameters: the distor-
tion limit (the maximum distance measured in words
that a tuple is allowed to be reordered, m) and the
reordering limit (the maximum number of reorder-
ing jumps in a sentence, j). Tuples need to be ex-
tracted by an unfolding technique (Marin?o et al,
2006). This means that the tuples are broken into
smaller tuples, and these are sequenced in the order
of the target words. In order not to lose the infor-
mation on the correct order, the decoder performs a
non-monotonic search. Figure 1 shows an example
of tuple unfolding compared to the monotonic ex-
traction. The unfolding technique produces a differ-
ent bilingual n-gram language model with reordered
source words.
In order to combine the models in the decoder
suitably, an optimization tool based on the Simplex
algorithm is used to compute log-linear weights for
each model.
3 Continuous Space N-gram Models
The architecture of the neural network n-gram
model is shown in Figure 2. A standard
fully-connected multi-layer perceptron is
used. The inputs to the neural network are
the indices of the n?1 previous units (words
or tuples) in the vocabulary hj=wj?n+1,
. . . , wj?2, wj?1 and the outputs are the poste-
rior probabilities of all units of the vocabulary:
projection
layer hidden
layer
output
layerinput
projections
shared
LM probabilities
for all words
probability estimation
Neural Network
discrete
representation:
indices in wordlist
continuous
representation:
P dimensional vectors
N
wj?1 P
H
N
P (wj =1|hj)
wj?n+1
wj?n+2
P (wj =i|hj)
P (wj =N|hj)
cl
oiM
Vdj
p1 =
pN =
pi =
Figure 2: Architecture of the continuous space LM.
hj denotes the context wj?n+1, . . . , wj?1. P is the
size of one projection and H ,N is the size of the
hidden and output layer respectively. When short-
lists are used the size of the output layer is much
smaller than the size of the vocabulary.
P (wj = i|hj) ?i ? [1,N ] (3)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith unit of
the vocabulary is coded by setting the ith element of
the vector to 1 and all the other elements to 0. The
ith line of the N ?P dimensional projection matrix
corresponds to the continuous representation of the
ith unit. Let us denote cl these projections, dj the
hidden layer activities, oi the outputs, pi their soft-
max normalization, and mjl, bj , vij and ki the hid-
den and output layer weights and the corresponding
biases. Using these notations, the neural network
performs the following operations:
dj = tanh
(
?
l
mjl cl + bj
)
(4)
oi =
?
j
vij dj + ki (5)
pi = eoi /
N
?
r=1
eor (6)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj).
433
Training is performed with the standard back-
propagation algorithm minimizing the following er-
ror function:
E =
N
?
i=1
ti log pi + ?
?
?
?
jl
m2jl +
?
ij
v2ij
?
? (7)
where ti denotes the desired output, i.e., the proba-
bility should be 1.0 for the next unit in the training
sentence and 0.0 for all the other ones. The first part
of this equation is the cross-entropy between the out-
put and the target probability distributions, and the
second part is a regularization term that aims to pre-
vent the neural network from over-fitting the train-
ing data (weight decay). The parameter ? has to be
determined experimentally. Training is done using
a re-sampling algorithm as described in (Schwenk,
2007).
It can be shown that the outputs of a neural net-
work trained in this manner converge to the posterior
probabilities. Therefore, the neural network directly
minimizes the perplexity on the training data. Note
also that the gradient is back-propagated through the
projection-layer, which means that the neural net-
work learns the projection of the units onto the con-
tinuous space that is best for the probability estima-
tion task.
In general, the complexity to calculate one prob-
ability with this basic version of the neural network
n-gram model is dominated by the dimension of the
output layer since the size of the vocabulary (10k
to 64k) is usually much larger than the dimension of
the hidden layer (200 to 500). Therefore, in previous
applications of the continuous space n-gram model,
the output was limited to the s most frequent units, s
ranging between 2k and 12k (Schwenk, 2007). This
is called a short-list.
Sents Words
Train (bitexts) 20k 155.4/166.3k
Dev 489 5.2k
Eval 500 6k
Table 1: Available data in the supplied resources of
the 2006 IWSLT evaluation.
4 Experimental Evaluation
In this work we report results on the Basic Travel-
ing Expression Corpus (BTEC) as used in the 2006
evaluations of the international workshop on spoken
language translation (IWSLT). This corpus consists
of typical sentences from phrase books for tourists in
several languages (Takezawa et al, 2002). We report
results on the supplied development corpus of 489
sentences and the official test set of the IWSLT?06
evaluation. The main measure is the BLEU score,
using seven reference translations. The scoring is
case insensitive and punctuations are ignored. De-
tails on the available data are summarized in Table 1.
We concentrated first on the translation from Ital-
ian to English. All participants in the IWSLT evalua-
tion achieved much better performances for this lan-
guage pair than for the other considered translation
directions. This makes it more difficult to achieve
additional improvements.
A non-monotonic search was performed follow-
ing a local reordering named in Section 2, setting
m = 5 and j = 3. Also we used histogram prun-
ing in the decoder, i.e. the maximum number of hy-
potheses in a stack is limited to 50.
4.1 Language-dependent preprocessing
Italian contracted prepositions have been separated
into preposition + article, such as ?alla???a la?,
?degli???di gli? or ?dallo???da lo?, among others.
4.2 Model training
The training and development data for the bilingual
back-off and neural network translation model were
created as follows. Given the alignment of the train-
ing parallel corpus, we perform a unique segmenta-
tion of each parallel sentence following the criterion
of unfolded segmentation seen in Section 2. This
segmentation is used in a sequence as training text
for building the language model. As an example,
given the alignment and the unfold extraction of Fig-
ure 1, we obtain the following training sentence:
<s> how long#cua?nto does#NULL last#dura
the#el flight#vuelo </s>
The reference bilingual trigram back-off transla-
tion model was trained on these bilingual tuples us-
434
ing the SRI LM toolkit (Stolcke, 2002). Different
smoothing techniques were tried, and best results
were obtained using Good-Turing discounting.
The neural network approach was trained on ex-
actly the same data. A context of two tuples was
used (trigram model). The training corpus contains
about 21,500 different bilingual tuples. We decided
to limit the output of the neural network to the 8k
most frequent tuples (short-list). This covers about
90% of the requested tuple n-grams in the training
data.
Similar to previous applications, the neural net-
work is not used alone but interpolation is performed
to combine several n-gram models. First of all, the
neural network and the reference back-off model are
interpolated together - this always improved perfor-
mance since both seem to be complementary. Sec-
ond, four neural networks with different sizes of the
continuous representation were trained and interpo-
lated together. This usually achieves better general-
ization behavior than training one larger neural net-
work. The interpolation coefficients were calculated
by optimizing perplexity on the development data,
using an EM procedure. The obtained values are
0.33 for the back-off translation model and about
0.16 for each neural network model respectively.
This interpolation is used in all our experiments. For
the sake of simplicity we will still call this the con-
tinuous space translation model.
Each network was trained independently using
early stopping on the development data. Conver-
gence was achieved after about 10 iterations through
the training data (less than 20 minutes of processing
on a standard Linux machine). The other parameters
are as follows:
? Context of two tuples (trigram)
? The dimension of the continuous representation
of the tuples were c =120,140,150 and 200,
? The dimension of the hidden layer was set to
P = 200,
? The initial learning rate was 0.005 with an ex-
ponential decay,
? The weight decay coefficient was set to ? =
0.00005.
N-gram models are usually evaluated using per-
plexity on some development data. In our case, i.e.
using bilingual tuples as basic units (?words?), it is
less obvious if perplexity is a useful measure. Nev-
ertheless, we provide these numbers for complete-
ness. The perplexity on the development data of the
trigram back-off translation model is 227.0. This
could be reduced to 170.4 using the neural network.
It is also very informative to analyze the n-gram
hit-rates of the back-off model on the development
data: 10% of the probability requests are actually a
true trigram, 40% a bigram and about 49% are fi-
nally estimated using unigram probabilities. This
means that only a limited amount of phrase con-
text is used in the standard N-gram-based translation
model. This makes this an ideal candidate to ap-
ply the continuous space model since probabilities
are interpolated for all possible contexts and never
backed-up to shorter contexts.
4.3 Results and analysis
The incorporation of the neural translation model
is done using n-best list. Each hypothesis is com-
posed of a sequence of bilingual tuples and the cor-
responding scores of all the feature functions. Fig-
ure 3 shows an example of such an n-best list. The
neural trigram translation model is used to replace
the scores of the trigram back-off translation model.
This is followed by a re-optimization of the coef-
ficients of all feature functions, i.e. maximization
of the BLEU score on the development data using
the numerical optimization tool CONDOR (Berghen
and Bersini, 2005). An alternative would be to add
a feature function and to combine both translation
models under the log-linear model framework, us-
ing maximum BLEU training.
Another open question is whether it might by
better to already use the continuous space transla-
tion model during decoding. The continuous space
model has a much higher complexity than a back-
off n-gram. However, this can be heavily optimized
when rescoring n-best lists, i.e. by grouping to-
gether all calls in the whole n-best list with the same
context, resulting in only one forward pass through
the neural network. This is more difficult to per-
form when the continuous space translation model
is used during decoding. Therefore, this was not in-
vestigated in this work.
435
spiacente#sorry tutto occupato#it ?s full
spiacente#i ?m sorry tutto occupato#it ?s full
spiacente#i ?m afraid tutto occupato#it ?s full
spiacente#sorry tutto#all occupato#busy
spiacente#sorry tutto#all occupato#taken
Figure 3: Example of sentences in the n-best list of
bilingual tuples. The special character ?#? is used to
separate the source and target sentence words. Sev-
eral words in one tuple a grouped together using ? .?
In all our experiments 1000-best lists were used.
In order to evaluate the quality of these n-best lists,
an oracle trigram back-off translation model was
build on the development data. Rescoring the n-
best lists with this translation model resulted in an
increase of the BLEU score of about 10 points (see
Table 2). While there is an decrease of about 6%
for the position dependent word error rate (mWER),
a smaller change in the position independent word
error rate was observed (mPER). This suggests that
most of the alternative translation hypothesis re-
sult in word reorderings and not in many alternative
word choices. This is one of the major drawbacks
of phrase- and N-gram-based translation systems:
only translations observed in the training data can
be used. There is no generalization to new phrase
pairs.
Back-off Oracle Neural
BLEU 42.34 52.45 43.87
mWER 41.6% 35.6% 40.3%
mPER 31.5% 28.2% 30.7%
Table 2: Comparison of different N-gram-
translation models on the development data.
When the 1000-best lists are rescored with the
neural network translation model the BLEU score
increases by 1.5 points (42.34 to 43.87). Similar im-
provements were observed in the word error rates
(see Table 2). For comparison, a 4-gram back-off
translation model was also built, but no change of
the BLEU score was observed. This suggests that
careful smoothing is more important than increasing
the context when estimating the translation probabil-
ities in an N-gram-based statistical machine transla-
tion system.
In previous work, we have investigated the use of
the neural network approach to modeling the target
language for the IWSLT task (Schwenk et al, 2006).
We also applied this technique to this improved N-
gram-based translation system. In our implemen-
tation, the neural network target 4-gram language
model gives an improvement of 1.3 points BLEU
on the development data (42.34 to 43.66), in com-
parison to 1.5 points for the neural translation model
(see Table 3).
Back-off neural neural neural
TM+LM TM LM TM+LM
BLEU 42.34 43.87 43.66 44.83
Table 3: Combination of a neural translation model
(TM) and a neural language model (LM). BLEU
scores on the development data.
The neural translation and target language model
were also applied to the test data, using of course the
same feature function coefficients as for the devel-
opment data. The results are given in Table 4 for all
the official measures of the IWSLT evaluation. The
new smoothing method of the translation probabili-
ties achieves improvement in all measures. It gives
also an additional gain (again in all measures) when
used together with a neural target language model.
Surprisingly, neural TM and neural LM improve-
ments almost add up: when both techniques are used
together, the BLEU scores increases by 1.5 points
(36.97 ? 38.50). Remember that the reference N-
gram-based translation system already uses a local
reordering approach.
Back-off neural neural neural
TM+LM TM LM TM+LM
BLEU 36.97 37.21 38.04 38.50
mWER 48.10 47.42 47.83 47.61
mPER 38.21 38.07 37.26 37.12
NIST 8.3 8.3 8.6 8.7
Meteor 63.16 63.40 64.70 65.20
Table 4: Test set scores for the combination of a
neural translation model (TM) and a neural language
model (LM).
436
5 Discussion
Phrase-based approaches are the de-facto standard
in statistical machine translation. The phrases are
extracted automatically from the word alignments
of parallel texts, and the different possible transla-
tions of a phrase are weighted using relative fre-
quency. This can be problematic when the data is
sparse. However, there seems to be little work on
possible improvements of the relative frequency es-
timates by some smoothing techniques. It is today
common practice to use additional feature functions
like IBM-1 scores to obtain some kind of smoothing
(Och et al, 2004; Koehn et al, 2003; Zens and Ney,
2004), but better estimation of the phrase probabili-
ties is usually not addressed.
An alternative way to represent phrases is to de-
fine bilingual tuples. Smoothing, and context de-
pendency, is obtained by using an n-gram model on
these tuples. In this work, we have extended this
approach by using a new smoothing technique that
operates on a continuous representation of the tu-
ples. Our method is distinguished by two charac-
teristics: better estimation of the numerous unseen
n-grams, and a discriminative estimation of the tu-
ple probabilities. Results are provided on the BTEC
task of the 2006 IWSLT evaluation for the translation
direction Italian to English. This task provides very
limited amount of resources in comparison to other
tasks. Therefore, new techniques must be deployed
to take the best advantage of the limited resources.
We have chosen the Italian to English task because it
is challenging to enhance a good quality translation
task (over 40 BLEU percentage). Using the continu-
ous space model for the translation and target lan-
guage model, an improvement of 2.5 BLEU on the
development data and 1.5 BLEU on the test data was
observed.
Despite these encouraging results, we believe that
additional research on improved estimation of prob-
abilities in N-gram- or phrase-based statistical ma-
chine translation systems is needed. In particu-
lar, the problem of generalization to new trans-
lations seems to be promising to us. This could
be addressed by the so-called factored phrase-based
model as implemented in the Moses decoder (Koehn
et al, 2007). In this approach words are decom-
posed into several factors. These factors are trans-
lated and a target phrase is generated. This model
could be complemented by a factored continuous
tuple N-gram. Factored word language models
were already successfully used in speech recogni-
tion (Bilmes and Kirchhoff, 2003; Alexandrescu and
Kirchhoff, 2006) and an extension to machine trans-
lation seems to be promising.
The described smoothing method was explicitly
developed to tackle the data sparseness problem in
tasks like the BTEC corpus. It is well known from
language modeling that careful smoothing is less im-
portant when large amounts of data are available.
We plan to investigate whether this also holds for
smoothing of the probabilities in phrase- or tuple-
based statistical machine translation systems.
6 Acknowledgments
This work has been partially funded by the European
Union under the integrated project TC-STAR (IST-
2002-FP6-506738), by the French Government un-
der the project INSTAR (ANR JCJC06 143038) and
the the Spanish government under a FPU grant and
the project AVIVAVOZ (TEC2006-13964-C03).
References
A. Alexandrescu and K. Kirchhoff. 2006. Factored neu-
ral language models. In HLT-NAACL.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3(2):1137?1155.
F. Vanden Berghen and H. Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175.
J. A. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized backoff. In HLT-NAACL.
P. Brown, S. Della Pietra, V. J. Della Pietra, and R: Mer-
cer. 1993. The mathematics of statistical machine
translation. Computational Linguistics, 19(2):263?
311.
F. Casacuberta, D. Llorens, C. Mart??nez, S. Molau,
F. Nevado, H. Ney, M. Pastor, D. Pico?, A. Sanchis,
E. Vidal, and J.M. Vilar. 2001. Speech-to-speech
translation based on finite-state transducers. Interna-
tional Conference on Acoustic, Speech and Signal Pro-
cessing, 1.
437
S. F. Chen and J. T. Goodman. 1999. An empirical study
of smoothing techniques for language modeling. CSL,
13(4):359?394.
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
smoothing for statistical machine translation. In
EMNLP06, pages 53?61.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrased-based machine translation. In Human Lan-
guage Technology Conference (HLT-NAACL), pages
127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, demonstration session.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M. R. Costa-jussa`.
2006. Bilingual n-gram statistical machine transla-
tion. Computational Linguistics, 32(4):527?549, De-
cember.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL, pages 295?302.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Copora,
pages 20?28.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL, pages 161?168.
M. Paul. 2006. Overview of the IWSLT 2006 campaign.
In IWSLT, pages 1?15.
H. Schwenk, M. R. Costa-jussa`, and J. A. R. Fonollosa.
2006. Continuous space language models for the iwslt
2006 task. IWSLT, pages 166?173.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In ICSLP, pages II: 901?904.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a borad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In LREC, pages 147?152.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In HLT/NACL,
pages 257?264.
438
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 424?432,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
N-gram-based Statistical Machine Translation versus Syntax Augmented
Machine Translation: comparison and system combination
Maxim Khalilov and Jos? A.R. Fonollosa
Universitat Polit?cnica de Catalunya
Campus Nord UPC, 08034
Barcelona, Spain
{khalilov,adrian}@talp.upc.edu
Abstract
In this paper we compare and contrast
two approaches to Machine Translation
(MT): the CMU-UKA Syntax Augmented
Machine Translation system (SAMT) and
UPC-TALP N-gram-based Statistical Ma-
chine Translation (SMT). SAMT is a hier-
archical syntax-driven translation system
underlain by a phrase-based model and a
target part parse tree. In N-gram-based
SMT, the translation process is based on
bilingual units related to word-to-word
alignment and statistical modeling of the
bilingual context following a maximum-
entropy framework. We provide a step-
by-step comparison of the systems and re-
port results in terms of automatic evalu-
ation metrics and required computational
resources for a smaller Arabic-to-English
translation task (1.5M tokens in the train-
ing corpus). Human error analysis clari-
fies advantages and disadvantages of the
systems under consideration. Finally, we
combine the output of both systems to
yield significant improvements in transla-
tion quality.
1 Introduction
There is an ongoing controversy regarding
whether or not information about the syntax of
language can benefit MT or contribute to a hybrid
system.
Classical IBM word-based models were re-
cently augmented with a phrase translation ca-
pability, as shown in Koehn et al (2003), or in
more recent implementation, the MOSES MT sys-
tem1 (Koehn et al, 2007). In parallel to the phrase-
based approach, the N -gram-based approach ap-
peared (Mari?o et al, 2006). It stemms from
1www.statmt.org/moses/
the Finite-State Transducers paradigm, and is ex-
tended to the log-linear modeling framework, as
shown in (Mari?o et al, 2006). A system follow-
ing this approach deals with bilingual units, called
tuples, which are composed of one or more words
from the source language and zero or more words
from the target one. The N -gram-based systems
allow for linguistically motivated word reordering
by implementing word order monotonization.
Prior to the SMT revolution, a major part
of MT systems was developed using rule-based
algorithms; however, starting from the 1990?s,
syntax-driven systems based on phrase hierar-
chy have gained popularity. A representative
sample of modern syntax-based systems includes
models based on bilingual synchronous grammar
(Melamed, 2004), parse tree-to-string translation
models (Yamada and Knight, 2001) and non-
isomorphic tree-to-tree mappings (Eisner, 2003).
The orthodox phrase-based model was en-
hanced in Chiang (2005), where a hierarchical
phrase model allowing for multiple generaliza-
tions within each phrase was introduced. The
open-source toolkit SAMT2 (Zollmann and Venu-
gopal, 2006) is a further evolution of this ap-
proach, in which syntactic categories extracted
from the target side parse tree are directly assigned
to the hierarchically structured phrases.
Several publications discovering similarities
and differences between distinct translation mod-
els have been written over the last few years. In
Crego et al (2005b), the N -gram-based system
is contrasted with a state-of-the-art phrase-based
framework, while in DeNeefe et al (2007), the
authors seek to estimate the advantages, weak-
est points and possible overlap between syntax-
based MT and phrase-based SMT. In Zollmann et
al. (2008) the comparison of phrase-based , "Chi-
ang?s style" hirearchical system and SAMT is pro-
2www.cs.cmu.edu/?zollmann/samt
424
vided.
In this study, we intend to compare the differ-
ences and similarities of the statistical N -gram-
based SMT approach and the SAMT system. The
comparison is performed on a small Arabic-to-
English translation task from the news domain.
2 SAMT system
A criticism of phrase-based models is data sparse-
ness. This problem is even more serious when the
source, the target, or both languages are inflec-
tional and rich in morphology. Moreover, phrase-
based models are unable to cope with global re-
ordering because the distortion model is based
on movement distance, which may face computa-
tional resource limitations (Och and Ney, 2004).
This problem was successfully addressed when
the MT system based on generalized hierarchi-
cally structured phrases was introduced and dis-
cussed in Chiang (2005). It operates with only two
markers (a substantial phrase category and "a glue
marker"). Moreover, a recent work (Zollmann and
Venugopal, 2006) reports significant improvement
in terms of translation quality if complete or par-
tial syntactic categories (derived from the target
side parse tree) are assigned to the phrases.
2.1 Modeling
A formalism for Syntax Augmented Translation
is probabilistic synchronous context-free grammar
(PSynCFG), which is defined in terms of source
and target terminal sets and a set of non-terminals:
X ?? ??, ?,?, ??
where X is a non-terminal, ? is a sequence of
source-side terminals and non-terminals, ? is a se-
quence of target-side terminals and non-terminals,
? is a one-to-one mapping from non-terminal to-
kens space in ? to non-terminal space in ?, and ?
is a non-negative weight assigned to the rule.
The non-terminal set is generated from the syn-
tactic categories corresponding to the target-side
Penn Treebank set, a set of glue rules and a spe-
cial marker representing the "Chiang-style" rules,
which do not span the parse tree. Consequently, all
lexical mapping rules are covered by the phrases
mapping table.
2.2 Rules annotation, generalization and
pruning
The SAMT system is based on a purely lexi-
cal phrase table, which is identified as shown in
Koehn et al (2003), and word alignment, which is
generated by the grow-diag-final-and method (ex-
panding the alignment by adding directly neigh-
boring alignment points and alignment points in
the diagonal neighborhood) (Och and Ney, 2003).
Meanwhile, the target of the training corpus is
parsed with Charniak?s parser (Charniak, 2000),
and each phrase is annotated with the constituent
that spans the target side of the rules. The set of
non-terminals is extended by means of conditional
and additive categories according to Combinatory
Categorical Grammar (CCG) (Steedman, 1999).
Under this approach, new rules can be formed. For
example, RB+VB, can represent an additive con-
stituent consisting of two synthetically generated
adjacent categories 3, i.e., an adverb and a verb.
Furthermore, DT\NP can indicate an incomplete
noun phrase with a missing determiner to the left.
The rule recursive generalization procedure co-
incides with the one proposed in Chiang (2005),
but violates the restrictions introduced for single-
category grammar; for example, rules that contain
adjacent generalized elements are not discarded.
Thus, each rule
N ?? f1 . . . fm/e1 . . . en
can be extended by another existing rule
M ?? fi . . . fu/ej . . . ev
where 1 ? i < u ? m and 1 ? j < v ? n, to
obtain a new rule
N ?? f1 . . . fi?1Mkfu+1 . . . fm/
e1 . . . ej?1Mkev+1 . . . en
where k is an index for the non-terminal M that in-
dicates a one-to-one correspondence between the
new M tokens on the two sides.
Figure 1 shows an example of initial rules ex-
traction, which can be further extended using the
hierarchical model, as shown in Figure 2 (conse-
quently involving more general elements in rule
description).
Rules pruning is necessary because the set of
generalized rules can be huge. Pruning is per-
formed according to the relative frequency and
the nature of the rules: non-lexical rules that
have been seen only once are discarded; source-
conditioned rules with a relative frequency of ap-
pearance below a threshold are also eliminated.
3Adjacent generalized elements are not allowed in Chi-
ang?s work because of over-generation. However, over-
generation is not an issue within the SAMT framework due
to restrictions introduced by target-side syntax
425
Rules that do not contain non-terminals are not
pruned.
2.3 Decoding and feature functions
The decoding process is accomplished using a top-
down log-linear model. The source sentence is de-
coded and enriched with the PSynCFG in such a
way that translation quality is represented by a set
of feature functions for each rule, i.e.:
? rule conditional probabilities, given a source,
a target or a left-hand-side category;
? lexical weights features, as described in
Koehn et al (2003);
? counters of target words and rule applica-
tions;
? binary features reflecting rule context (purely
lexical and purely abstract, among others);
? rule rareness and unbalancedness penalties.
The decoding process can be represented as
a search through the space of neg log probabil-
ity of the target language terminals. The set of
feature functions is combined with a finite-state
target-side n-gram language model (LM), which
is used to derive the target language sequence dur-
ing a parsing decoding. The feature weights are
optimized according to the highest BLEU score.
For more details refer to Zollmann and Venu-
gopal (2006).
3 UPC n-gram SMT system
A description of the UPC-TALP N -gram transla-
tion system can be found in Mari?o et al (2006).
SMT is based on the principle of translating a
source sentence (f ) into a sentence in the target
language (e). The problem is formulated in terms
of source and target languages; it is defined ac-
cording to equation (1) and can be reformulated as
selecting a translation with the highest probability
from a set of target sentences (2):
Figure 1: Example of SAMT and N-gram elements extraction.
Figure 2: Example of SAMT generalized rules.
426
e?I1 = argmaxeI1
{
p(eI1 | fJ1 )
}
= (1)
= argmax
eI1
{
p(fJ1 | eI1) ? p(eI1)
}
(2)
where I and J represent the number of words in
the target and source languages, respectively.
Modern state-of-the-art SMT systems operate
with the bilingual units extracted from the parallel
corpus based on word-to-word alignment. They
are enhanced by the maximum entropy approach
and the posterior probability is calculated as a log-
linear combination of a set of feature functions
(Och and Ney, 2002). Using this technique, the
additional models are combined to determine the
translation hypothesis, as shown in (3):
e?I1 = argmaxeI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
(3)
where the feature functions hm refer to the system
models and the set of ?m refers to the weights cor-
responding to these models.
3.1 N-gram-based translation system
The N -gram approach to SMT is considered to
be an alternative to the phrase-based translation,
where a given source word sequence is decom-
posed into monolingual phrases that are then trans-
lated one by one (Marcu and Wong, 2002).
The N -gram-based approach regards transla-
tion as a stochastic process that maximizes the
joint probability p(f, e), leading to a decomposi-
tion based on bilingual n-grams. The core part of
the system constructed in this way is a translation
model (TM), which is based on bilingual units,
called tuples, that are extracted from a word align-
ment (performed with GIZA++ tool4) according to
certain constraints. A bilingual TM actually con-
stitutes an n-gram LM of tuples, which approxi-
mates the joint probability between the languages
under consideration and can be seen here as a LM,
where the language is composed of tuples.
3.2 Additional features
The N -gram translation system implements a log-
linear combination of five additional models:
? an n-gram target LM;
4http://code.google.com/p/giza-pp/
? a target LM of Part-of-Speech tags;
? a word penalty model that is used to compen-
sate for the system?s preference for short out-
put sentences;
? source-to-target and target-to-source lexicon
models as shown in Och and Ney (2004)).
3.3 Extended word reordering
An extended monotone distortion model based
on the automatically learned reordering rules was
implemented as described in Crego and Mari?o
(2006). Based on the word-to-word alignment, tu-
ples were extracted by an unfolding technique. As
a result, the tuples were broken into smaller tuples,
and these were sequenced in the order of the target
words. An example of unfolding tuple extraction,
contrasted with the SAMT chunk-based rules con-
struction, is presented in Figure 1.
The reordering strategy is additionally sup-
ported by a 4-gram LM of reordered source POS
tags. In training, POS tags are reordered according
to the extracted reordering patterns and word-to-
word links. The resulting sequence of source POS
tags is used to train the n-gram LM.
3.4 Decoding and optimization
The open-source MARIE5 decoder was used as a
search engine for the translation system. Details
can be found in Crego et al (2005a). The de-
coder implements a beam-search algorithm with
pruning capabilities. All the additional fea-
ture models were taken into account during the
decoding process. Given the development set
and references, the log-linear combination of
weights was adjusted using a simplex optimization
method and an n-best re-ranking as described in
http://www.statmt.org/jhuws/.
4 Experiments
4.1 Evaluation framework
As training corpus, we used the 50K first-lines ex-
traction from the Arabic-English corpus that was
provided to the NIST?086 evaluation campaign
and belongs to the news domain. The corpus
statistics can be found in Table 1. The develop-
ment and test sets were provided with 4 reference
translations, belong to the same domain and con-
tain 663 and 500 sentences, respectively.
5http://gps-tsc.upc.es/veu/soft/soft/marie/
6www.nist.gov/speech/tests/mt/2008/
427
Arabic English
Sentences 50 K 50 K
Words 1.41 M 1.57 K
Average sentence length 28.15 31.22
Vocabulary 51.10 K 31.51 K
Table 1: Basic statistics of the training corpus.
Evaluation conditions were case-insensitive and
sensitive to tokenization. The word alignment is
automatically computed by using GIZA++ (Och
and Ney, 2004) in both directions, which are made
symmetric by using the grow-diag-final-and oper-
ation.
The experiments were done on a dual-processor
Pentium IV Intel Xeon Quad Core X5355 2.66
GHz machine with 24 G of RAM. All computa-
tional times and memory size results are approxi-
mated.
4.2 Arabic data preprocessing
Arabic is a VSO (SVO in some cases) pro-
drop language with rich templatic morphology,
where words are made up of roots and affixes
and clitics agglutinate to words. For prepro-
cessing, a similar approach to that shown in
Habash and Sadat (2006) was employed, and the
MADA+TOKAN system for disambiguation and
tokenization was used. For disambiguation, only
diacritic unigram statistics were employed. For to-
kenization, the D3 scheme with -TAGBIES option
was used. The scheme splits the following set of
clitics: w+, f+, b+, k+, l+, Al+ and pronominal cl-
itics. The -TAGBIES option produces Bies POS
tags on all taggable tokens.
4.3 SAMT experiments
The SAMT guideline was used to perform
the experiments and is available on-line:
http://www.cs.cmu.edu/?zollmann/samt/.
Moses MT script was used to create the
grow ? diag ? final word alignment and
extract purely lexical phrases, which are then used
to induce the SAMT grammar. The target side
(English) of the training corpus was parsed with
the Charniak?s parser (Charniak, 2000).
Rule extraction and filtering procedures were
restricted to the concatenation of the development
and test sets, allowing for rules with a maximal
length of 12 elements in the source side and with a
zero minimum occurrence criterion for both non-
lexical and purely lexical rules.
Moses-style phrases extracted with a phrase-
based system were 4.8M , while a number of gen-
eralized rules representing the hierarchical model
grew dramatically to 22.9M . 10.8M of them were
pruned out on the filtering step.
The vocabulary of the English Penn Treebank
elementary non-terminals is 72, while a number of
generalized elements, including additive and trun-
cated categories, is 35.7K.
The FastTranslateChart beam-search de-
coder was used as an engine of MER training aim-
ing to tune the feature weight coefficients and pro-
duce final n-best and 1-best translations by com-
bining the intensive search with a standard 4-gram
LM as shown in Venugopal et al (2007). The it-
eration limit was set to 10 with 1000-best list and
the highest BLEU score as optimization criteria.
We did not use completely abstract rules (with-
out any source-side lexical utterance), since these
rules significantly slow down the decoding process
(noAllowAbstractRules option).
Table 2 shows a summary of computational time
and RAM needed at each step of the translation.
Step Time Memory
Parsing 1.5h 80Mb
Rules extraction 10h 3.5Gb
Filtering&merging 3h 4.0Gb
Weights tuning 40h 3Gb
Testing 2h 3Gb
Table 2: SAMT: Computational resources.
Evaluation scores including results of system
combination (see subsection 4.6) are reported in
Table 3.
4.4 N-gram system experiments
The core model of the N -gram-based system is a
4-gram LM of bilingual units containing: 184.345
1-grams7, 552.838 2-grams, 179.466 3-grams and
176.221 4-grams.
Along with this model, an N -gram SMT sys-
tem implements a log-linear combination of a 5-
gram target LM estimated on the English portion
of the parallel corpus, as well as supporting 4-
gram source and target models of POS tags. Bies
7This number also corresponds to the bilingual model vo-
cabulary.
428
BLEU NIST mPER mWER METEOR
SAMT 43.20 9.26 36.89 49.45 58.50
N-gram-based SMT 46.39 10.06 32.98 48.47 62.36
System combination 48.00 10.15 33.20 47.54 62.27
MOSES Factored System 44.73 9.62 33.92 47.23 59.84
Oracle 61.90 11.41 28.84 41.52 66.19
Table 3: Test set evaluation results
POS tags were used for the Arabic portion, as
shown in subsection 4.2; a TnT tool was used for
English POS tagging (Brants, 2000).
The number of non-unique initially extracted
tuples is 1.1M , which were pruned according to
the maximum number of translation options per
tuple on the source side (30). Tuples with a NULL
on the source side were attached to either the pre-
vious or the next unit (Mari?o et al, 2006). The
feature models weights were optimized according
to the same optimization criteria as in the SAMT
experiments (the highest BLEU score).
Stage-by-stage RAM and time requirements are
presented in Table 4, while translation quality
evaluation results can be found in Table 3.
Step Time Memory
Models estimation 0.2h 1.9Gb
Reordering 1h ?
Weights tuning 15h 120Mb
Testing 2h 120Mb
Table 4: Tuple-based SMT: Computational re-
sources.
4.5 Statistical significance
A statistical significance test based on a bootstrap
resampling method, as shown in Koehn (2004),
was performed. For the 98% confidence interval
and 1000 set resamples, translations generated by
SAMT and N -gram system are significantly dif-
ferent according to BLEU (43.20?1.69 for SAMT
vs. 46.42? 1.61 for tuple-based system).
4.6 System combination
Many MT systems generate very different trans-
lations of similar quality, even if the models
involved into translation process are analogous.
Thus, the outputs of syntax-driven and purely sta-
tistical MT systems were combined at the sentence
level using 1000-best lists of the most probable
translations produced by the both systems.
For system combination, we followed a Mini-
mum Bayes-risk algorithm, as introduced in Ku-
mar and Byrne (2004). Table 3 shows the results
of the system combination experiments on the test
set, which are contrasted with the oracle transla-
tion results, performed as a selection of the transla-
tions with the highest BLEU score from the union
of two 1000-best lists generated by SAMT and N -
gram SMT.
We also analyzed the percentage contribution of
each system to the system combination: 55-60%
of best translations come from the tuples-based
system 1000-best list, both for system combina-
tion and oracle experiments on the test set.
4.7 Phrase-based reference system
In order to understand the obtained results com-
pared to the state-of-the-art SMT, a reference
phrase-based factored SMT system was trained
and tested on the same data using the MOSES
toolkit. Surface forms of words (factor ?0?), POS
(factor ?1?) and canonical forms of the words
(lemmata) (factor ?2?) were used as English fac-
tors, and surface forms and POS were the Arabic
factors.
Word alignment was performed according to
the grow-diag-final algorithm with the GIZA++
tool, a msd-bidirectional-fe conditional reordering
model was trained; the system had access to the
target-side 4-gram LMs of words and POS. The 0-
0,1+0-1,2+0-1 scheme was used on the translation
step and 1,2-0,1+1-0,1 to create generation tables.
A detailed description of the model training can
be found on the MOSES tutorial web-page8. The
results may be seen in Table 3.
5 Error analysis
To understand the strong and weak points of both
systems under consideration, a human analysis of
8http://www.statmt.org/moses/
429
the typical translation errors generated by each
system was performed following the framework
proposed in Vilar et al (2006) and contrasting the
systems output with four reference translations.
Human evaluation of translation output is a time-
consuming process, thus a set of 100 randomly
chosen sentences was picked out from the corre-
sponding system output and was considered as a
representative sample of the automatically gener-
ated translation of the test corpus. According to
the proposed error topology, some classes of errors
can overlap (for example, an unknown word can
lead to a reordering problem), but it allows finding
the most prominent source of errors in a reliable
way (Vilar et al, 2006; Povovic et al, 2006). Ta-
ble 5 presents the comparative statistics of errors
generated by the SAMT and the N -gram-based
SMT systems. The average length of the generated
translations is 32.09 words for the SAMT transla-
tion and 35.30 for the N -gram-based system.
Apart from unknown words, the most important
sources of errors of the SAMT system are missing
content words and extra words generated by the
translation system, causing 17.22 % and 10.60 %
of errors, respectively. A high number of missing
content words is a serious problem affecting the
translation accuracy. In some cases, the system
is able to construct a grammatically correct
translation, but omitting an important content
word leads to a significant reduction in translation
accuracy:
SAMT translation: the ministers of arab
environment for the closure of the Israeli dymwnp
reactor .
Ref 1: arab environment ministers demand the
closure of the Israeli daemona nuclear reactor .
Ref 2: arab environment ministers demand the
closure of Israeli dimona reactor .
Ref 3: arab environment ministers call for Israeli
nuclear reactor at dimona to be shut down .
Ref 4: arab environmental ministers call for the
shutdown of the Israeli dimona reactor .
Extra words embedded into the correctly trans-
lated phrases are a well-known problem of MT
systems based on hierarchical models operating on
the small corpora. For example, in many cases
the Arabic expression AlbHr Almyt is trans-
lated into English as dead sea side and not
as dead sea, since the bilingual instances con-
tain only the whole English phrase, like following:
AlbHr Almyt#the dead sea side#@NP
The N -gram-based system handles miss-
ing words more correctly ? only 9.40 % of
the errors come from the missing content
Type Sub-type SAMT N-gram
Missing words 152 (25.17 %) 92 (15.44 %)
Content words 104 (17.22 %) 56 (9.40 %)
Filler words 48 (7.95 %) 36 (6.04 %)
Word order 96 (15.89 %) 140 (23.49 %)
Local word order 20 (3.31 %) 68 (11.41 %)
Local phrase order 20 (3.31 %) 20 (3.36 %)
Long range word order 32 (5.30 %) 48 (8.05 %)
Long range phrase order 24 (3.97 %) 4 (0.67 %)
Incorrect words 164 (27.15 %) 204 (34.23 %)
Sense: wrong lexical choice 24 (3.97 %) 60 (10.07 %)
Sense: incorrect disambiguation 16 (2.65 %) 8 (1.34 %)
Incorrect form 24 (3.97 %) 56 (9.40 %)
Extra words 64 (10.60 %) 56 (9.40 %)
Style 28 (4.64 %) 20 (3.36 %)
Idioms 4 (0.07 %) 4 (0.67 %)
Unknown words 132 (21.85 %) 104 (17.45 %)
Punctuation 60 (9.93 %) 56 (9.40 %)
Total 604 596
Table 5: Human made error statistics for a representative test set.
430
words; however, it does not handle local and
long-term reordering, thus the main problem
is phrase reordering (11.41 % and 8.05 %
of errors). In the example below, the un-
derlined block (Circumstantial Complement:
from local officials in the tour-
ism sector) is embedded between the verb
and the direct object, while in correct translation
it must be placed in the end of the sentence.
N-gram translation: the winner received
from local officials in the tourism sector three
gold medals .
Ref 1: the winner received three gold medals
from local officials from the tourism sector .
Ref 2: the winner received three gold medals
from the local tourism officials .
Ref 3: the winner received his prize of 3 gold
medals from local officials in the tourist industry .
Ref 4: the winner received three gold medals
from local officials in the tourist sector .
Along with inserting extra words and wrong
lexical choice, another prominent source of
incorrect translation, generated by the N -
gram system, is an erroneous grammatical
form selection, i.e., a situation when the sys-
tem is able to find the correct translation but
cannot choose the correct form. For example,
arab environment minister call for
closing dymwnp Israeli reactor,
where the verb-preposition combination
call for was correctly translated on the
stem level, but the system was not able to generate
a third person conjugation calls for. In spite
of the fact that English is a language with nearly
no inflection, 9.40 % of errors stem from poor
word form modeling. This is an example of the
weakest point of the SMT systems having access
to a small training material; the decoder does not
use syntactic information about the subject of
the sentence (singular) and makes a choice only
concerning the tuple probability.
The difference in total number of errors is neg-
ligible, however a subjective evaluation of the sys-
tems output shows that the translation generated
by the N -gram system is more understandable
than the SAMT one, since more content words are
translated correctly and the meaning of the sen-
tence is still preserved.
6 Discussion and conclusions
In this study two systems are compared: the UPC-
TALP N -gram-based and the CMU-UKA SAMT
systems, originating from the ideas of Finite-State
Transducers and hierarchical phrase translation,
respectively. The comparison was created to be as
fair as possible, using the same training material
and the same tools on the preprocessing, word-
to-word alignment and language modeling steps.
The obtained results were also contrasted with the
state-of-the-art phrase-based SMT.
Analyzing the automatic evaluation scores, the
N -gram-based approach shows good performance
for the small Arabic-to-English task and signifi-
cantly outperforms the SAMT system. The results
shown by the modern phrase-based SMT (factored
MOSES) lie between the two systems under con-
sideration. Considering memory size and compu-
tational time, the tuple-based system has obtained
significantly better results than SAMT, primarily
because of its smaller search space.
Interesting results were obtained for the PER
and WER metrics: according to the PER,
the UPC-TALP system outperforms the SAMT
by 10%, while the WER improvement hardly
achieves a 2% difference. The N -gram-based
SMT can translate the context better, but pro-
duces more reordering errors than SAMT. This
may be explained by the fact that Arabic and En-
glish are languages with high disparity in word
order, and the N -gram system deals worse with
long-distance reordering because it attempts to use
shorter units. However, by means of introducing
the word context into the TM, short-distance bilin-
gual dependencies can be captured effectively.
The main conclusion that can be made from
the human evaluation analysis is that the systems
commit a comparable number of errors, but they
are distributed dissimilarly. In case of the SAMT
system, the frequent errors are caused by missing
or incorrectly inserted extra words, while the N -
gram-based system suffers from reordering prob-
lems and wrong words/word form choice
Significant improvement in translation quality
was achieved by combining the outputs of the two
systems based on different translating principles.
7 Acknowledgments
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project).
431
References
T. Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proceedings of the 6th Applied Natural Lan-
guage Processing (ANLP-2000).
E. Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
J. M. Crego and J. B. Mari?o. 2006. Improving statis-
tical MT by coupling reordering and decoding. Ma-
chine Translation, 20(3):199?215.
J. M. Crego, J. Mari?o, and A. de Gispert. 2005a. An
Ngram-based Statistical Machine Translation De-
coder. In Proceedings of INTERSPEECH05, pages
3185?3188.
J.M. Crego, M.R. Costa-juss?, J.B. Mari?o, and J.A.R.
Fonollosa. 2005b. Ngram-based versus phrase-
based statistical machine translation. In Proc. of the
IWSLT 2005, pages 177?184.
S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based MT learn from phrase-based
MT? In Proceedings of EMNLP-CoNLL 2007,
pages 755?763.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of
ACL 2003 (companion volume), pages 205?208.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of HLT/NAACL 2006, pages 49?52.
Ph. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based machine translation. In Proceedings of
HLT-NAACL 2003, pages 48?54.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open-source toolkit
for statistical machine translation. In Proceedings
of ACL 2007, pages 177?180.
P. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of HLT/NAACL 2004.
D. Marcu and W. Wong. 2002. A Phrase-based, Joint
Probability Model for Statistical Machine Transla-
tion. In Proceedings of EMNLP02, pages 133?139.
J. B. Mari?o, R. E. Banchs, J. M. Crego, A. de Gispert,
P. Lambert, J. A. R. Fonollosa, and M. R. Costa-
juss?. 2006. N-gram based machine translation.
Computational Linguistics, 32(4):527?549, Decem-
ber.
I.D. Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL 2004, pages 111?
114.
F. J. Och and H. Ney. 2002. Discriminative Train-
ing and Maximum Entropy Models for Statistical
Machine Translation. In Proceedings of ACL 2002,
pages 295?302.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
M. Povovic, A. de Gispert, D. Gupta, P. Lambert, J.B.
Mari?o, M. Federico, H. Ney, and R. Banchs. 2006.
Morpho-syntactic information for automatic error
analysis of statistic machine translation output. In
In Proceeding of the HLT-NAACL Workshop on Sta-
tistical Machine Translation, pages 1?6.
M. Steedman. 1999. Alternative quantifier scope in
ccg. In Proceedings of ACL 1999, pages 301?308.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An Efficient Two-Pass Approach to Synchronous-
CFG Driven Statistical MT. In Proceedings of
HLT/NAACL 2007, pages 500?507.
D. Vilar, J. Xu, L. F. D?Haro, and H. Ney. 2006. Error
Analysis of Machine Translation Output. In Pro-
ceedings of LREC?06, pages 697?702.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of ACL
2001, pages 523?530.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proceedings of NAACL 2006.
A. Zollmann, A. Venugopal, F. Och, and J. Ponte.
2008. Systematic comparison of Phrase-based, Hi-
erarchical and Syntax-Augmented Statistical mt. In
Proceedings of Coling 2008, pages 1145?1152.
432
N-gram-based Machine Translation
Jose? B. Marin?o?
Rafael E. Banchs?
Josep M. Crego?
Adria` de Gispert?
Patrik Lambert?
Jose? A. R. Fonollosa?
Marta R. Costa-jussa`?
Universitat Polite`cnica de Catalunya
This article describes in detail an n-gram approach to statistical machine translation. This ap-
proach consists of a log-linear combination of a translation model based on n-grams of bilingual
units, which are referred to as tuples, along with four specific feature functions. Translation
performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English
and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).
1. Introduction
The beginnings of statistical machine translation (SMT) can be traced back to the early
fifties, closely related to the ideas from which information theory arose (Shannon and
Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during
World War II. According to this view, machine translation was conceived as the problem
of finding a sentence by decoding a given ?encrypted? version of it (Weaver 1955).
Although the idea seemed very feasible, enthusiasm faded shortly afterward because of
the computational limitations of the time (Hutchins 1986). Finally, during the nineties,
two factors made it possible for SMT to become an actual and practical technology:
first, significant increment in both the computational power and storage capacity of
computers, and second, the availability of large volumes of bilingual data.
The first SMT systems were developed in the early nineties (Brown et al 1990, 1993).
These systems were based on the so-called noisy channel approach, which models the
probability of a target language sentence T given a source language sentence S as the
product of a translation-model probability p(S|T), which accounts for adequacy of trans-
lation contents, times a target language probability p(T), which accounts for fluency
of target constructions. For these first SMT systems, translation-model probabilities at
the sentence level were approximated from word-based translation models that were
trained by using bilingual corpora (Brown et al 1993). In the case of target language
probabilities, these were generally trained from monolingual data by using n-grams.
Present SMT systems have evolved from the original ones in such a way that
mainly differ from them in two respects: first, word-based translation models have been
? Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain.
Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for
publication: 5 July 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och,
and Marcu 2003) which are directly estimated from aligned bilingual corpora by consid-
ering relative frequencies, and second, the noisy channel approach has been expanded
to a more general maximum entropy approach in which a log-linear combination of
multiple feature functions is implemented (Och and Ney 2002).
As an extension of the machine translation problem, technological advances in the
fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it
possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron,
and Norvig 1992). According to this, SMT has also been approached from a finite-state
point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini,
and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi
2000). In this SMT approach, translation models are implemented by means of finite-
state transducers for which transition probabilities are learned from bilingual data.
As opposed to phrase-based translation models, which consider probabilities between
target and source units referred to as phrases, finite-state translation models rely on
probabilities among sequences of bilingual units, which are defined by the transitions
of the transducer.
The translation system described in this article implements a translation model that
has been derived from the finite-state perspective?more specifically, from the work of
Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier
work the translation model is implemented by using a finite-state transducer, in the sys-
tem presented here the translation model is implemented by using n-grams. In this way,
the proposed translation system can take full advantage of the smoothing and consist-
ency provided by standard back-off n-gram models. The translation model presented
here actually constitutes a language model of a sort of ?bilanguage? composed of bilin-
gual units, which will be referred to as tuples (de Gispert and Marin?o 2002). An alterna-
tive approach, which relies on bilingual-unit unigram probabilities, was developed by
Tillmann and Xia (2003); in contrast, the approach presented here considers bilingual-
unit n-gram probabilities. In addition to the tuple n-gram translation model, the
translation system presented here implements four specific feature functions that are
log-linearly combined along with the translation model for performing the decoding
(Marin?o et al 2005).
This article is intended to provide a detailed description of the n-gram-based
translation system, as well as to demonstrate the system performance in a wide-
domain, large-vocabulary translation task. The article is structured as follows. First,
Section 2 presents a complete description of the n-gram-based translation model. Then,
Section 3 describes in detail the additional feature functions that, along with the trans-
lation model, compose the n-gram-based SMT system implemented. Section 4 describes
the European Parliament Plenary Session (EPPS) data, as well as the most relevant
details about the translation tasks considered. Section 5 presents and discusses the
translation experiments and their results. Finally, Section 6 presents some conclusions
and intended further work.
2. The Tuple N-gram Model
This section describes in detail the tuple n-gram translation model, which constitutes
the core model implemented by the n-gram-based SMT system. First, the bilingual unit
definition and model computation are presented in Section 2.1. Then, some important
refinements to the basic translation model are provided and discussed in Section 2.2.
Finally, Section 2.3 discusses issues related to n-gram-based decoding.
528
Marin?o et al N-gram-based Machine Translation
2.1 Tuple Extraction and Model Computation
As already mentioned, the translation model implemented by the described SMT sys-
tem is based on bilingual n-grams. This model actually constitutes a language model of
a particular bilanguage composed of bilingual units that are referred to as tuples. In this
way, the translation model probabilities at the sentence level are approximated by using
n-grams of tuples, such as described by the following equation:
p(T, S) ?
K
?
k=1
p((t, s)k|(t, s)k?1, (t, s)k?2, . . . , (t, s)k?n+1) (1)
where t refers to target, s to source, and (t, s)k to the kth tuple of a given bilingual
sentence pair. It is important to note that since both languages are linked up in tuples,
the context information provided by this translation model is bilingual.
Tuples are extracted from a word-to-word aligned corpus in such a way that a
unique segmentation of the bilingual corpus is achieved. Although in principle any
Viterbi alignment should allow for tuple extraction, the resulting tuple vocabulary
depends highly on the particular alignment set considered, and this impacts the trans-
lation results. According to our experience, the best performance is achieved when
the union of the source-to-target and target-to-source alignment sets (IBM models;
Brown et al [1993]) is used for tuple extraction (some experimental results regarding
this issue are presented in Section 4.2.2). Additionally, the use of the union can also
be justified from a theoretical point of view by considering that the union set typically
exhibits higher recall values than do other alignment sets such as the intersection and
source-to-target.
In this way, as opposed to other implementations, where one-to-one (Bangalore
and Riccardi 2000) or one-to-many (Casacuberta and Vidal 2004) alignments are used,
tuples are extracted from many-to-many alignments. This implementation produces
a monotonic segmentation of bilingual sentence pairs, which allows for simulta-
neously capturing contextual and reordering information into the bilingual translation
unit structures. This segmentation also allows for estimating the n-gram probabil-
ities appearing in (1). In order to guarantee a unique segmentation of the corpus,
tuple extraction is performed according to the following constraints (Crego, Marin?o,
and de Gispert 2004):
 a monotonic segmentation of each bilingual sentence pair is produced,
 no word inside the tuple is aligned to words outside the tuple, and
 no smaller tuples can be extracted without violating the previous
constraints.
Notice that, according to this, tuples can be formally defined as the set of shortest
phrases that provides a monotonic segmentation of the bilingual corpus. Figure 1
presents a simple example illustrating the unique tuple segmentation for a given pair of
sentences, as well as the complete phrase set.
The first important observation from Figure 1 is related to the possible occurrence
of tuples containing unaligned elements on the target side. This is the case for tuple 1.
Tuples of this kind should be handled in an alternative way for the system to be able
to provide appropriate translations for such unaligned elements. The problem of how
529
Computational Linguistics Volume 32, Number 4
Figure 1
Example of tuple extraction. Tuples are extracted from Viterbi alignments in such a way that the
set of shortest bilingual units that provide a monotonous segmentation of the bilingual sentence
pair is achieved.
to handle this kind of situation, which we refer to as involving source-nulled tuples, is
discussed in detail in Section 2.2.2.
Also, as observed from Figure 1, the total number of tuples is significantly lower
than the total number of phrases, and, in most of the cases, longer phrases can be
constructed by considering tuple n-grams, which is the case for phrases 2, 6, 7, 9, 10,
and 11. However, phrases 4 and 5 cannot be generated from tuples. In general, the tuple
representation is not able to provide translations for individual words that appear tied
to other words unless they occur alone in some other tuple. This problem, which we
refer to as embedded words, is discussed in detail in Section 2.2.1.
Another important observation from Figure 1 is that each tuple length is implicitly
defined by the word links in the alignment. As opposed to phrase-extraction proce-
dures, for which a maximum phrase length should be defined to avoid a vocabulary
explosion, tuple extraction procedures do not have any control over tuple lengths.
According to this, the tuple approach will strongly benefit from the structural similarity
between the languages under consideration. Then, for close language pairs, tuples are
expected to successfully handle those short reordering patterns that are included in
the tuple structure, as in the case of ?traducciones perfectas : perfect translations?
presented in Figure 1. On the other hand, in the case of distant pairs of languages, for
which a large number of long tuples are expected to occur, the approach will more easily
fail to provide a good translation model due to tuple sparseness.
2.2 Translation Model Refinements
The basic n-gram translation model, as defined in the previous section, exhibits some
important limitations that can be easily overcome by incorporating specific changes in
530
Marin?o et al N-gram-based Machine Translation
either the tuple vocabulary or the n-gram model. This section describes such limitations
and provides a detailed description of the implemented refinements.
2.2.1 Embedded Words. The first issue regarding the n-gram translation model is related
to the already mentioned problem of embedded words, which refers to the fact that
the tuple representation is not able to provide translations for individual words all the
time. Embedded words can become a serious drawback when they occur in relatively
significant numbers in the tuple vocabulary.
Consider for example the word translations in Figure 1. As seen from the figure, this
word appears embedded into tuple ?traducciones perfectas : perfect translations.? If a
similar situation is encountered for all other occurrences of that word in the training
corpus, then no translation probability for an independent occurrence of that word
will exist. A more relevant example would be the case of the embedded word perfect
since this adjective always moves relative to the noun it is modifying. In this case,
providing the translation system with a word-to-word translation probability for ?per-
fectas : perfect? only guarantees that the decoder will have a translation option for an
isolated occurrence of such words but does not guarantee anything about word order.
So, certainly, any adjective?noun combination including the word perfect, which has not
been seen during the training stage, will be translated in the wrong order. Accordingly,
the problem resulting from embedded words can be partially solved by incorporating a
bilingual dictionary able to provide word-to-word translation when required by the
translation system. A more complete treatment for this problem must consider the
implementation of a word-reordering strategy for the proposed SMT approach (as will
be discussed in Section 6, this constitutes one of the main concerns for our further
research).
In our n-gram-based SMT implementation, the following strategy for handling em-
bedded words is considered. First, one-word tuples for each detected embedded word
are extracted from the training data and their corresponding word-to-word translation
probabilities are computed by using relative frequencies. Then, the tuple n-gram model
is enhanced by including all embedded-word tuples as unigrams into the model. Since
a high-precision alignment set is desirable for extracting such one-word tuples and
estimating their probabilities, the intersection of both alignments, source to target and
target-to-source, is used instead of the union.
In the particular case of the EPPS tasks considered in this work, embedded words
do not constitute a real problem because of the great amount of training material and
the reduced size of the test data set (see Section 4.1 for a detailed description of the
EPPS data set). On the contrary, in other translation tasks with less available training
material, the embedded-word handling strategy described above has been very useful
(de Gispert, Marin?o, and Crego 2004).
2.2.2 Tuples with Empty Source Sides. The second important issue regarding the
n-gram translation model is related to tuples with empty source sides, hereinafter
referred to as source-nulled tuples. In the tuple n-gram model implementation, it fre-
quently happens that some target words linked to NULL end up producing tuples with
NULL source sides. Consider, for example, the first tuple of the example presented in
Figure 1. In this example, ?NULL : we? is a source-nulled tuple if Spanish is considered
to be the source language. Notice that tuples of this kind cannot be allowed since no
NULL is expected to occur in a translation input.
The classical solution to this problem in the finite-state transducer framework is
the inclusion of epsilon arcs (Knight and Al-Onaizan 1998; Bangalore and Riccardi
531
Computational Linguistics Volume 32, Number 4
2000). However, epsilon arcs significantly increase decoding complexity. In our n-gram
system implementation, this problem is easily solved by preprocessing the union set of
alignments before extracting tuples, in such a way that any target word that is linked
to NULL is attached to either its preceding word or its following word. In this way, no
target word remains linked to NULL, and source-nulled tuples will not occur during
tuple extraction.
Some different strategies for handling target words aligned to NULL have been
considered. In the simplest strategy, which will be referred to as the attach-to-right strat-
egy, target words aligned to NULL are always attached to their following word. This
simple strategy happens to provide better results, for English-to-Spanish and Spanish-
to-English translations, than the opposite one (attachment to the previous word), and
also better than a more sophisticated strategy that considers bigram probabilities for
deciding whether a given word should be attached to the following or to the pre-
vious one.
Notice that in the particular cases of Spanish and English, the attach-to-right strat-
egy can be justified heuristically. Indeed, when translating from Spanish to English,
most of the source-nulled tuples result from omitted verbal subjects, which is a very
common situation in Spanish. This is the case for the first tuple in Figure 1. Suppose,
for instance, that the attach-to-right strategy is used in Figure 1; in such a case, the
tuple ?quisie?ramos : would like? will be replaced by the new tuple ?quisie?ramos : we
would like,? which actually makes a better translation unit, at least from a grammatical
point of view. Similarly, some common situations can be identified for translations in
the English-to-Spanish direction, such as omitted determiners (e.g., ?I want information
about European countries : quiero informacio?n sobre los pa??ses Europeos?). Again,
the attach-to-right strategy for the unaligned Spanish determiner los seems to be the
best one.
Experimental results comparing the attach-to-right strategy to an additional strat-
egy based on a statistical translation lexicon are provided in Section 5.1.3.
2.2.3 Tuple Vocabulary Pruning. The third and last issue regarding the n-gram transla-
tion model is related to the computational costs resulting from the tuple vocabulary size
during decoding. The idea behind this refinement is to reduce both computation time
and storage requirements without degrading translation performance. In our n-gram-
based SMT system implementation, the tuple vocabulary is pruned by using histogram
counts. This pruning is performed by keeping the N most frequent tuples with common
source sides.
Notice that such a pruning, because it is performed before computing tuple n-gram
probabilities, has a direct impact on the translation model probabilities and then on
the overall system performance. For this reason, the pruning parameter N is critical
for efficient usage of the translation system. While a low value of N will significantly
decrease translation quality, on the other hand, a large value of N will provide the
same translation quality than a more adequate N, but with a significant increment in
computational costs. The optimal value for this parameter depends on data and should
be adjusted empirically for each considered translation task.
2.3 N-gram-based Decoding
Decoding for the n-gram-based translation model is slightly different from phrase-
based decoding. For this reason, a specific decoding tool had to be implemented. This
532
Marin?o et al N-gram-based Machine Translation
section briefly describes MARIE, the n-gram based search engine developed for our
SMT system (Crego, Marin?o, and de Gispert 2005a).
MARIE implements a beam-search strategy based on dynamic programming. The
decoding is performed monotonically and is guided by the source. During decoding,
partial-translation hypotheses are arranged into different stacks according to the total
number of source words they cover. In this way, a given hypothesis only competes with
those hypotheses that provide the same source-word coverage. At every translation
step, stacks are pruned to keep decoding tractable. MARIE allows for two different
pruning methods:
 Threshold pruning: for which all partial-translation hypotheses scoring
below a predetermined threshold value are eliminated.
 Histogram pruning: for which the maximum number of partial-translation
hypotheses to be considered is limited to the K-best ranked ones.
Additionally, MARIE allows for hypothesis recombination, which provides a more
efficient search. In the implemented algorithm, partial-translation hypotheses are re-
combined if they coincide exactly in both the present tuple and the tuple trigram history.
MARIE also allows for considering additional feature functions during decoding.
All these models are taken into account simultaneously, along with the n-gram trans-
lation model. In our SMT system implementation, four additional feature functions are
considered. These functions are described in detail in Section 3.2.
3. Feature Functions for the N-gram-based SMT System
This section describes in detail some feature functions that are implemented along with
the n-gram translation model for the complete translation system. First, in subsection
3.1, the log-linear combination framework and the implemented optimization proce-
dure are discussed. Then, four specific feature functions that constitute our SMT system
are detailed in Section 3.2.
3.1 Log-linear Combination Framework
As mentioned in the Introduction, in recent translation systems the noisy channel ap-
proach has been replaced by a more general approach, which is founded on the princi-
ples of maximum entropy (Berger, Della Pietra, and Della Pietra 1996). In this approach,
the corresponding translation for a given source language sentence S is defined by the
target language sentence that maximizes a log-linear combination of multiple feature
functions hi(S, T) (Och and Ney 2002), such as described by the following equation:
argmax
T
?
m
?mhm(S, T) (2)
where ?m represents the coefficient of the mth feature function hm(S, T), which ac-
tually corresponds to a log-scaled version of the mth-model probabilities. Optimal
values for the ?m coefficients are estimated via an optimization procedure by using a
development data set.
533
Computational Linguistics Volume 32, Number 4
3.2 Translation System Features
In addition to the tuple n-gram translation model, our n-gram-based SMT system
implements four feature functions: a target-language model, a word-bonus model, and
two lexicon models. These system features are described next.
3.2.1 Target-language Model. This feature provides information about the target lan-
guage structure and fluency. It favors those partial-translation hypotheses that are more
likely to constitute correctly structured target sentences over those that are not. The
model is implemented by using a word n-gram model of the target language, which is
computed according to the following expression:
hTL(T, S) = hTL(T) = log
K
?
k=1
p(wk|wk?1, wk?2, . . . , wk?n+1) (3)
where wk refers to the kth word in the considered partial-translation hypothesis. Notice
that this model only depends on the target side of the data, and can in fact be trained by
including additional information from other available monolingual corpora.
3.2.2 Word-bonus Model. This feature introduces a bonus that depends on the partial-
translation hypothesis length. This is done to compensate for the system preference for
short translations over large ones. The model is implemented through a bonus factor
that directly depends on the total number of words contained in the partial-translation
hypothesis, and it is computed as follows:
hWP(T, S) = hWP(T) = M (4)
where M is the number of words contained in the partial-translation hypothesis.
3.2.3 Source-to-Target Lexicon Model. This feature actually constitutes a complemen-
tary translation model. This model provides, for a given tuple, a translation probability
estimate between its source and target sides. This feature is implemented by using the
IBM-1 lexical parameters (Brown et al 1993; Och et al 2004). Accordingly, the source-
to-target lexicon probability is computed for each tuple according to the following
equation:
hLF(T, S) = log 1(I + 1)J
J
?
j=1
I
?
i=0
q(tnj |sni ) (5)
where sni and t
n
j are the ith and jth words in the source and target sides of tuple (t, s)n,
with I and J the corresponding total number of words in each side. In the equation,
q(.) refers to IBM-1 lexical parameters, which are estimated from alignments computed
in the source-to-target direction.
3.2.4 Target-to-Source Lexicon Model. Similar to the previous feature, this feature
function constitutes a complementary translation model too. It is computed in ex-
534
Marin?o et al N-gram-based Machine Translation
actly the same way the previous model is, with the only difference that IBM-1 lexical
parameters are estimated from alignments computed in the target-to-source direction
instead.
4. EPPS Translation Task
This section describes in detail the most relevant issues about the translation tasks con-
sidered. Section 4.1 describes the EPPS data set that is used, and Section 4.2 presents the
overall implementation details in regard to preprocessing, training, and optimization.
4.1 Corpus Description
The EPPS data set is composed of the official plenary session transcriptions of the Eu-
ropean Parliament, which are currently available in eleven different languages (Koehn
2002). However, in the case of the results presented here, we have used the Spanish and
English versions of the EPPS data that have been prepared by RWTH Aachen University
in the context of the European Project TC-STAR. The training, development, and test
data used include session transcriptions from April 1996 until September 2004, from
October 21 until October 28, 2004, and from November 15 until November 18, 2004,
respectively.
Table 1 presents the basic statistics for the training, development, and test data sets
for each considered language. More specifically, the statistics shown in Table 1 are the
number of sentences, the number of words, the vocabulary size (or number of distinct
words), the average sentence length in number of words, and the number of available
translation references.
As seen from Table 1, although the total number of words in the training set is
very similar for both languages, vocabulary sizes are substantially different. Indeed,
the Spanish vocabulary is approximately 60% larger than the English vocabulary. This
can be explained by the more inflected nature of Spanish, which is particularly evident
in the case of nouns, adjectives, and verbs, which may have many different forms de-
pending on gender, number, tense, and mode. As will be seen from results presented in
Section 5, this difference in vocabulary size has important consequences in translation
quality for the English-to-Spanish direction.
Regarding the development data set, only 1, 008 sentences were considered. Notice
from Table 1 that in this case, the Spanish vocabulary is 20% larger than the English
Table 1
Basic statistics for the training, development, and test data sets (M and k stand for millions and
thousands, respectively; Lmean refers to the average sentence length in number of words, and
Ref. to the number of available translation references).
Set Language Sentences Words Vocabulary Lmean Ref.
Train English 1.22 M 33.4 M 105 k 23.7 1
Spanish 1.22 M 34.8 M 169 k 28.4 1
Dev. English 1008 26.0 k 3.2 k 25.8 3
Spanish 1008 25.7 k 3.9 k 25.5 3
Test English 1094 26.8 k 3.9 k 24.5 2
Spanish 840 22.7 k 4.0 k 27.0 2
535
Computational Linguistics Volume 32, Number 4
vocabulary. Another important issue regarding the development data set is the number
of unseen words, that is, those words present in the development data that are not
present in the training data. In this case, 35 words (0.13%) out of the total number of
words in the English development set did not occur in the training data. From these 35
words, only 30 corresponded to different words. Similarly, 61 words (0.24%) out of the
total number of words in the Spanish development set were not in the training data. In
this case, 57 different words occurred.
Notice also in Table 1 that a different test set was used for each translation direction,
and although a different number of sentences is considered in each case, vocabulary
sizes are almost equivalent. Regarding unseen words, in this case, 112 words (0.42%) out
of the total number of words in the English test set did not occur in the training data.
From these 112 words, only 81 corresponded to different words. Similarly, 46 words
(0.20%) out of the total number of words in the Spanish test were not in the training
data. In this case, 40 different words occurred.
4.2 Preprocessing, Training, and System Optimization
This section presents the overall implementation details in regard to preprocessing,
training, and optimization of the translation system. Two languages, English and Span-
ish, and both translation directions between them are considered for several different
system configurations.
4.2.1 Preprocessing and Alignment. The training data are preprocessed by using stan-
dard tools for tokenizing and filtering. In the filtering stage, some sentence pairs are
removed from the training data to allow for a better performance of the alignment tool.
Sentence pairs are removed according to the following two criteria:
 Fertility filtering: removes sentence pairs with a word ratio larger than a
predefined threshold value.
 Length filtering: removes sentence pairs with at least one sentence of more
than 100 words in length. This helps to maintain bounded alignment
computational times.
After preprocessing, word-to-word alignments are performed in both directions,
source-to-target and target-to-source. In our system implementation, GIZA++ (Och and
Ney 2000) is used for computing the alignments. A total of five iterations for models
IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed.
Then, the obtained alignment sets are used for computing the intersection and the
union of alignments from which tuples and embedded-word tuples are extracted,
respectively.
4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is ex-
tracted from the union set of alignments while avoiding source-nulled tuples by using
the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are
pruned according to the procedure described in Section 2.2.3. In the case of the EPPS
data under consideration, pruning parameter values of N = 20 and N = 30 are used for
Spanish-to-English and English-to-Spanish, respectively.
In order to better justify such alignment set and pruning parameter selections,
Tables 2 and 3 present model sizes and translation accuracies for the tuple n-gram model
536
Marin?o et al N-gram-based Machine Translation
Table 2
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy when tuples are extracted from different alignment sets. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model alone.
Direction Alignment set Tuple voc. Bigrams Trigrams BLEU
ES ? EN Source-to-target 1.920 6.426 2.353 0.4424
union 2.040 6.009 1.798 0.4745
refined 2.111 6.851 2.398 0.4594
EN ? ES Source-to-target 1.813 6.263 2.268 0.4152
union 2.023 6.092 1.747 0.4276
refined 2.081 6.920 2.323 0.4193
when tuples are extracted from different alignment sets and when different pruning
parameters are used, respectively. Translation accuracy is measured in terms of the
BLEU score (Papineni et al 2002), which is computed here for translations generated
by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2,
in the case of Table 3. Both translation directions, Spanish to English (ES ? EN) and
English to Spanish (EN ? ES), are considered in each table.
In the case of Table 2, model size and translation accuracy are evaluated against
the type of alignment set used for extracting tuples. Three different alignment sets are
considered: source-to-target, the union of source-to-target and target-to-source, and the
?refined? alignment method described by Och and Ney (2003). For the results presented
in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English
direction, while a value of N = 30 was used for the English-to-Spanish direction.
As can be clearly seen in Table 2, the union alignment set happens to be the most
favorable one for extracting tuples in both translation directions since it provides a
significantly better translation accuracy, in terms of BLEU score, than the other two
alignment sets considered. Notice also in Table 2 that the union set is the one providing
the smallest model sizes according to the number of bigrams and trigrams. This might
explain the improvement observed in translation accuracy, with respect to the other two
cases, in terms of model sparseness.
Table 3
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy for different pruning values and both translation directions. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2.
Direction Pruning Tuple voc. Bigrams Trigrams BLEU
ES ? EN N = 30 2.109 6.233 1.805 0.5440
N = 20 2.040 6.009 1.798 0.5434
N = 10 1.921 5.567 1.759 0.5399
EN ? ES N = 30 2.023 6.092 1.747 0.4688
N = 20 1.956 5.840 1.733 0.4671
N = 10 1.843 5.342 1.677 0.4595
537
Computational Linguistics Volume 32, Number 4
In the case of Table 3, model size and translation accuracy are compared for three
different pruning conditions: N = 30, N = 20, and N = 10. For all the cases presented in
the table, tuples were extracted from the union set of alignments.
Notice in Table 3 how translation accuracy is clearly affected by pruning. In the
case of Spanish to English, values of N = 20 and N = 10, while providing tuple vo-
cabulary reductions of 3.27% and 8.91% with respect to N = 30, respectively, produce
a translation BLEU score reductions of 0.11% and 0.75%. On the other hand, in the
case of English to Spanish, values of N = 20 and N = 10 provide tuple vocabulary
reductions of 3.31% and 8.89% and a translation BLEU score reductions of 0.36% and
1.98% with respect to N = 30, respectively. According to these results, a similar tuple
vocabulary reduction seems to affect English-to-Spanish translations more than it af-
fects Spanish-to-English translations. For this reason, we finally adopted N = 20 and
N = 30 as the pruning parameter values for Spanish to English and English to Spanish,
respectively.
Another important observation derived from Table 3 is the higher BLEU score
values with respect to the ones presented in Table 2. This is because, as mentioned
above, the results presented in Table 3 were obtained by considering a full translation
system that implements the tuple n-gram model along with the additional four feature
functions described in Section 3.2. The relative impact of the described feature functions
on translation accuracy is studied in detail in Section 5.1.1.
4.2.3 Translation Model and Feature Function Training. After pruning, a tuple n-gram
model is trained for each translation direction by using the SRI Language Modeling
toolkit (Stolcke 2002). The options for Kneser?Ney smoothing (Kneser and Ney 1995)
and interpolation of higher and lower n-grams are used in these trainings. Then, each
tuple n-gram translation model is finally enhanced by including the unigram probabil-
ities for the embedded-word tuples such as described in Section 2.2.2.
Similarly, a word n-gram target language model is trained for each translation
direction by using the SRI Language Modeling toolkit. Again, as in the case of the
tuple n-gram model, Kneser?Ney smoothing and interpolation of higher and lower
n-grams are used. Extended target language models might also be obtained by adding
additional information from other available monolingual corpora. However, in the
translation tasks described here, target language models are estimated by using only
the information contained in the target side of the training data set.
In our SMT system implementation, trigram models are considered for both the
tuple translation model and the target language model. This selection is based on
perplexity measurements (over the development data set) obtained for n-gram models
computed from the EPPS training data by using different n-gram sizes. Table 4 presents
Table 4
Perplexity measurements for translation and target language models of different n-gram sizes.
Type of model Language Bigram Trigram 4-gram 5-gram
Translation ES ? EN 201.75 161.26 156.88 157.24
Translation EN ? ES 223.94 179.12 174.10 174.49
Language Spanish 81.98 52.49 48.03 47.54
Language English 78.91 50.59 46.22 45.59
538
Marin?o et al N-gram-based Machine Translation
perplexity values obtained for translation and target language models with different
n-gram sizes.
Although our system implements trigram models, the performance of translation
systems using different n-gram sized models is also evaluated. These results are pre-
sented and discussed in Section 5.1.2.
Finally, the source-to-target and target-to-source lexicon models are computed for
each translation direction according to the procedure described in Section 3.2.3. For each
considered lexicon model, either the alignment set in the source-to-target direction or
the alignment set in the target-to-source direction is used, accordingly.
4.2.4 System Optimization. Once the models are computed, a set of optimal log-linear
coefficients is estimated for each translation direction and system configuration via
an optimization procedure, which is described as follows. First, a development data
set that does not overlap either the training set or the test set is required. Then, trans-
lation quality over the development set is maximized by iteratively varying the set of
coefficients. In our SMT system implementation, this optimization procedure is per-
formed by using a tool developed in-house, which is based on a simplex method (Press
et al 2002), and the BLEU score (Papineni et al 2002) is used as a translation quality
measurement.
As will be described in the next section, several different system configurations
are considered in the experiments. For all these optimizations, the development data
described in Table 1 are used. As presented in the table, the development data included
three translation references for both English and Spanish, which are used to compute
the BLEU score at each iteration of the optimization procedures.
The same decoder settings are used for all system optimizations. These settings are
the following:
 decoding is performed monotonically, that is, no reordering capabilities
are used,
 decoding is guided by the source sentence to be translated,
 although available in the decoder, threshold pruning is not used, and
 a value of K = 50 for during-decoding histogram pruning is used.
5. Translation Experiments and Error Analysis
This section presents all translation experiments performed and a brief error analysis
of the obtained results. In order to evaluate the relative contributions of different
system elements to the overall performance of the n-gram-based translation system,
three different experimental settings are considered. The experiments and their re-
sults are described in Section 5.1, and a brief error analysis of results is presented in
Section 5.2. Finally, a comparison between n-gram-based SMT and state-of-the-art
phrase-based translation systems is presented in Section 5.3.
5.1 Translation Experiments and Results
As already mentioned, three experimental settings are considered. For each setting,
the impact on translation quality of a different system parameter is evaluated, namely,
539
Computational Linguistics Volume 32, Number 4
feature function, n-gram size, and the source-nulled tuple strategy. Evaluations in all
three experimental settings are performed with respect to the same standard system
configuration, which is defined in terms of the following parameters:
 Alignment set used for tuple extraction: UNION
 Tuple vocabulary pruning parameter: N = 20 for Spanish to English, and
N = 30 for English to Spanish
 N-gram size used in translation model: 3
 N-gram size used in target language model: 3
 Expanded translation model with embedded-word tuples: YES
 Source-nulled tuple handling strategy: attach-to-right
 Feature functions considered: target language, word-bonus,
source-to-target lexicon, and target-to-source lexicon
In the three experimental settings considered, which are presented in the following
subsections, a total of seven different system configurations are evaluated in both
translation directions, English to Spanish and Spanish to English. Thus, a total of 14
different translation experiments are performed. For each of these cases, the corre-
sponding test set is translated by using the corresponding estimated models and set
of optimal coefficients. The same decoder settings (which were previously described in
Section 4.2.4) that were used during the optimizations are used for all translation
experiments. Translation results are evaluated in terms of mWER and BLEU by using
the two references available for each language test set.
5.1.1 Feature Function Contributions. This experiment is designed to evaluate the
relative contribution of feature functions to the overall system performance. In this
section, four different systems are evaluated. These systems are:
 System A. This constitutes the basic n-gram translation system, which
implements the tuple trigram translation model alone, that is, no
additional feature function is used.
 System B. This is a target-reinforced system. In this system, the translation
model is used along with the target-language and word-bonus models.
 System C. This is a lexicon-reinforced system. In this system, the
translation model is used along with the source-to-target and
target-to-source lexicon models.
 System D. This constitutes the full system, that is, the translation model is
used along with all four additional feature functions. This system
corresponds to the standard system configuration that was defined at the
beginning of Section 5.1.
Table 5 summarizes the results of this evaluation, in terms of BLEU and mWER, for
the four systems considered. As can be seen from the table, both translation directions,
540
Marin?o et al N-gram-based Machine Translation
Table 5
Evaluation results for experiments on feature function contribution.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN A ? ? ? ? 39.71 0.4745
B 0.29 0.31 ? ? 39.51 0.4856
C ? ? 0.77 0.08 35.77 0.5356
D 0.49 0.30 0.94 0.25 34.94 0.5434
EN ? ES A ? ? ? ? 44.46 0.4276
B 0.33 0.27 ? ? 44.67 0.4367
C ? ? 0.29 0.15 41.69 0.4482
D 0.66 0.73 0.32 0.47 40.34 0.4688
Spanish to English and English to Spanish, are considered. Table 5 also presents the
optimized log-linear coefficients associated with the features considered in each system
configuration (the log-linear weight of the translation model has been omitted from the
table because its value is fixed to 1 in all cases).
As can be observed in Table 5, the inclusion of the four feature functions into
the translation system definitively produces a significant improvement in translation
quality in both translation directions. In particular, it becomes evident that the features
with the most impact on translation quality are the lexicon models. The target language
model and the word bonus also contribute to improving translation quality, but to a
lesser degree.
Also, although it is more evident in the English-to-Spanish direction than in the
opposite one, it can be noticed from the presented results that the contribution of
target-language and word-bonus models is more relevant when the lexicon mod-
els are used (full system). In fact, as seen from the ?lm values in Table 5, when
the lexicon models are not included, the target-language model contribution to the
overall translation system becomes much less significant. A comparative analysis of
the resulting translations suggests that including the lexicon models tends to favor
short tuples over long ones, so the target-language model becomes more important
for providing target context information when the lexicon models are used. How-
ever, more experimentation and research are required for fully understanding this
interesting result.
Another important observation, which follows from comparing results between
both translation directions, is that in all cases the Spanish-to-English translations are
consistently and significantly better than the English-to-Spanish translations. This is
clearly due to the more inflected nature of Spanish vocabulary. For example, the single
English word the can generate any of the four Spanish words el, la, los, and las. Similar
situations occur with nouns, adjectives, and verbs that may have many different forms
in Spanish. This would suggest that the English-to-Spanish translation task is more
difficult than the Spanish-to-English task.
5.1.2 Translation and Language N-gram Size. This experiment is designed to evaluate
the impact of translation- and language-model n-gram sizes on overall system perform-
ance. In this section, the full system (System D in the previous experiment) is com-
pared with two similar systems for which 4-grams are used for training the translation
541
Computational Linguistics Volume 32, Number 4
model and/or the target language model. More specifically, the three systems compared
in this experiment are:
 System D, which implements a tuple trigram translation model and a word
trigram target language model. This system corresponds to the standard
system configuration that was defined at the beginning of Section 5.1.
 System E, which implements a tuple trigram translation model and a word
4-gram target language model.
 System F, which implements a tuple 4-gram translation model and a word
4-gram target language model.
Table 6 summarizes the results of this evaluation for Systems E, F, and D. Again, both
translation directions are considered and the optimized coefficients associated with the
four feature functions are also presented for each system configuration.
As can be seen in Table 6, the use of 4-grams for model computation does not
provide a clear improvement in translation quality. This is more evident in the English-
to-Spanish direction for which System F happens to be the worst ranked one, while
System D is the one obtaining the best mWER score and system E is the one obtaining
the best BLEU score. On the other hand, in the Spanish-to-English direction, it seems
that a little improvement with respect to System D is achieved by using 4-grams.
However, it is not clear which system performs the best since System E obtains the
best BLEU score while System F obtains the best mWER score.
According to these results, more experimentation and research are required to fully
understand the interaction between the n-gram sizes of translation and target language
models. Notice that in the particular case of the n-gram SMT system described here,
such an interaction is not evident at all since the n-gram-based translation model itself
contains some of the target language model information.
5.1.3 Source-nulled Tuple Strategy Comparison. This experiment is designed to eval-
uate a different strategy for handling source-nulled tuples. In this section, the standard
system configuration (System D) presented at the beginning of Section 5.1, which imple-
ments the attach-to-right strategy described in Section 2.2.2, is compared with a similar
system (referred to as System G) implementing a more complex strategy for handling
those tuples with NULL source sides. More specifically, the latter system uses the
IBM-1 lexical parameters (Brown et al 1993) for computing the translation probabilities
of two possible new tuples: the one resulting when the null-aligned-word is attached to
Table 6
Evaluation results for experiments on n-gram size incidence.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
E 0.50 0.54 0.66 0.45 34.66 0.5483
F 0.66 0.50 1.01 0.57 34.59 0.5464
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
E 0.57 0.45 0.51 0.26 40.55 0.4714
F 1.24 1.07 0.99 0.57 40.91 0.4688
542
Marin?o et al N-gram-based Machine Translation
the previous word and the one resulting when it is attached to the following one. Then,
the attachment direction is selected according to the tuple with the highest translation
probability.
Table 7 summarizes the results of evaluation Systems D and G. Again, both trans-
lation directions are considered and the optimized coefficients associated with the four
feature functions are also presented for each system configuration.
As can be seen in Table 7, consistently better results are obtained in both translation
tasks when using IBM-1 lexicon probabilities to handle tuples with a NULL source
side. Even though slight improvements are achieved in both cases, especially with
the English-to-Spanish translation task, the results show how the initial attach-to-right
strategy is easily improved by making use of some bilingual knowledge.
5.2 Error Analysis
In this last section, we present a brief description of an error analysis performed
on some of the outputs provided by the standard system configuration that was de-
scribed in Section 5.1 (system D). More specifically, a detailed review of 100 trans-
lated sentences and their corresponding source sentences, in each direction, was
conducted. This analysis was very useful since it allowed us to identify the most com-
mon errors and problems related to our n-gram based SMT system in each translation
direction.
A detailed analysis of all the reviewed translations reveals that most translation
problems encountered are typically related to four basic different types of errors:
 Verbal forms: A significant number of wrong verbal tenses and auxiliary
forms were detected. This problem turned out to be the most common
one, reflecting the difficulty of the current statistical approach to capture
the linguistic phenomena that shape head verbs, auxiliary verbs, and
pronouns into full verbal forms in each language, especially given the
inflected nature of the Spanish language.
 Omitted translations: A large number of translations involving tuples with
NULL target sides were detected. Although in some cases these situations
corresponded to correct translations, most of the time they resulted in
omitted-word errors.
 Reordering problems: The two specific situations that most commonly
occurred were problems related to adjective?noun and subject?verb
structures.
Table 7
Evaluation results for experiments on strategies for handling source-nulled tuples.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
G 0.49 0.45 0.78 0.39 34.15 0.5451
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
G 0.96 0.93 0.53 0.44 40.12 0.4694
543
Computational Linguistics Volume 32, Number 4
 Concordance problems: Inconsistencies related to gender and number
were the most commonly found.
Table 8 presents the relative number of occurrences for each of the four types of errors
identified in both translation directions.
Notice in Table 8 that the most common errors in both translation directions are
those related to verbal forms. However, it is important to mention that 29.5% of verbal-
form errors in the English-to-Spanish direction actually correspond to verbal omissions.
Similarly, 12.8% of verbal-form errors in the Spanish-to-English direction are verbal
omissions. According to this, if errors due to omitted translations and to omitted verbal
forms are considered together, it is evident that errors involving omissions constitute
the most important group, especially in the case of English-to-Spanish translations. It
is also interesting to note that the Spanish-to-English direction exhibits more omitted-
translation errors that are not related to verbal forms than the English-to-Spanish
direction.
Also in Table 8, it can be seen that concordance errors affect more than twice as many
English-to-Spanish translations as Spanish-to-English ones. This result can be explained
by the more inflected nature of Spanish.
Finally, as an illustrative example, three Spanish-to-English translation outputs are
presented below. For each presented example, errors have been boldfaced and correct
translations are provided in brackets:
Example 1
The policy of the European Union on Cuba NULL must [must not] change.
Example 2
To achieve these purposes, it is necessary NULL for the governments to be allocated
[to allocate], at least, 60,000 million NULL dollars a year . . .
Example 3
In the UK we have NULL [already] laws enough [enough laws], but we want to encourage
NULL other States . . .
5.3 N-gram-based SMT Compared with Phrase-Based SMT
The n-gram-based translation system here described has been also evaluated and com-
pared to other phrase-based translation systems in the context of the European Project
Table 8
Percentage of occurrence for each type of error in English-to-Spanish and Spanish-to-English
translations that were studied.
Type of error English-to-Spanish Spanish-to-English
Verbal forms 31.3% 29.9%
Omitted translations 22.0% 26.1%
Reordering problems 15.9% 19.7%
Concordance problems 10.8% 4.6%
Other errors 20.0% 19.7%
544
Marin?o et al N-gram-based Machine Translation
TC-STAR. A detailed description of the first evaluation campaign (including the main
characteristics of every system) is available through the consortium?s Web site as a
progress report (Ney et al 2005).
Table 9 presents the four best BLEU results for the EPPS translation task in the
first TC-STAR?s evaluation campaign, where the results corresponding to our n-gram-
based translation system are provided in brackets. A total of six systems were evaluated
in this evaluation campaign. The task consisted of two translation directions: English
to Spanish and Spanish to English, and three different evaluation conditions: final
text edition, verbatim, and ASR output. The final text edition condition corresponds
to the official transcripts of the EPPS, so it is actually a written-language translation
condition. On the other hand, the other two conditions are spoken-language transla-
tion conditions. More specifically, the verbatim condition corresponds to literal tran-
scriptions of parliamentary speeches, which include hesitations, repeated words, and
other spontaneous speech effects; and the ASR output condition corresponds to the
output of an automatic speech recognition system, so it additionally includes speech-
recognition errors.
As can be seen in Table 9, performance of the n-gram-based translation system is
among the three best systems for the translation directions and conditions considered
in the first TC-STAR evaluation campaign.
Another independent comparison of the translation system proposed here with
other phrase-based translation systems is available through the results of the second
shared task of the ACL 2005 workshop on ?Building and using parallel texts: Data-
driven machine translation and beyond.? In this shared task, which was entitled ?Ex-
ploiting Parallel Texts for Statistical Machine Translation,? our n-gram-based translation
system was evaluated in four different translation directions: Spanish to English, French
to English, German to English, and Finish to English (Banchs et al 2005). The domain
of this task was also the European Parliament; however, the data set considered in this
evaluation was different from the one used in TC-STAR?s evaluation campaign. The
final text edition condition (official transcripts) was the only one considered here. A total
of twelve different systems participated in this shared task. Table 10 presents the four
best BLEU results for each of the four translation directions considered in the shared
task. Again, results corresponding to our n-gram-based translation system are provided
in brackets.
As can be seen in Table 10, the performance of the n-gram-based translation system
is among the three best systems for the four translation directions considered in the
ACL 2005 workshop shared task. The third system in Table 10 for ES to EN translation
Table 9
The four best BLEU results for the EPPS translation task in TC-STAR?s first evaluation campaign.
N-gram based system results are provided in brackets. All BLEU values presented here have
been taken from TC-STAR?s SLT Progress Report, available at: http://www.tc-star.org/.
Direction Condition First Second Third Fourth
ES ? EN Final text edition [53.3] 53.1 47.5 46.1
Verbatim 45.9 44.1 [42.1] 38.1
ASR output 41.5 39.7 [37.7] 34.7
EN ? ES Final text edition [46.2] 45.2 38.9 37.6
Verbatim 42.5 [38.1] 36.8 33.4
ASR output 38.7 34.3 [33.8] 33.0
545
Computational Linguistics Volume 32, Number 4
Table 10
The four best BLEU results for the four translation directions considered in the shared task
?Exploiting Parallel Texts for Statistical Machine Translation? (ACL 2005 workshop on
?Building and using parallel texts: Data-driven machine translation and beyond?). N-gram-
based system results are provided in brackets. All BLEU values presented here have been
taken from the shared task?s Web site: http://www.statmt.org/wpt05/mt-shared-task/.
Direction Condition First Second Third Fourth
FR ? EN Final text edition 30.27 [30.20] 29.53 28.89
ES ? EN Final text edition 30.95 [30.07] 29.84 29.08
DE ? EN Final text edition 24.77 [24.26] 23.21 22.91
FI ? EN Final text edition 22.01 20.95 [20.31] 18.87
deserves some comment. This system is a conventional phrase-based system sharing
the same decoder MARIE, IBM features, word bonus, and target-language model as the
n-gram-based system. The specific characteristics of the phrase-based system are direct
and inverse phrase conditional probabilities and phrase penalty. Additional compar-
isons between an n-gram system and a phrase-based system sharing a common decoder
and training and test framework can be found in Crego et al (2005c).
6. Conclusions and Further Work
As can be concluded from the results presented, the tuple n-gram translation model,
when used along with additional feature functions, provides state-of-the-art transla-
tions for the considered translation directions.
Another important result is that the quality of Spanish-to-English translations is
significantly and consistently better than those obtained in English-to-Spanish transla-
tions. Consequently, significant efforts should be dedicated towards properly exploiting
morphological analysis and synthesis methods for improving English-to-Spanish trans-
lation quality.
Additionally, four commonly occurring types of translation errors were identified
by reviewing a significant number of translated sentence pairs. This analysis has pro-
vided us with useful hints for future research and improvement of our SMT system.
However, more evaluation and discussion are required in this area in order to fully
understand these common translation failures and then implementing appropriate
solutions.
All the experiments presented in this work were performed using monotone de-
coding, and no reordering strategies were implemented. Although this system con-
figuration proved to provide state-of-the-art translations for the tasks presented, this
may not hold for tasks involving more distant language pairs for which reordering
capabilities must be implemented. Accordingly, along with other results obtained in
the present work, we consider that further research on n-gram SMT should focus on the
following issues:
 Reordering strategies, as well as non-monotonous decoding schemes, for
the proposed SMT system must be developed and tested. As mentioned
before, reordering problems specifically related to adjective?noun and
subject?verb structures occur very often in Spanish-to-English and
546
Marin?o et al N-gram-based Machine Translation
English-to-Spanish translations. Preliminary results concerning the use of
word class deterministic reordering and POS-tag-based reordering
patterns can be found in Costa-jussa`, Fonollosa, and Monte (2006) and
Crego and Marin?o (2006), respectively.
 An effective long-tuple unfolding strategy must be developed to avoid
the occurrence of long tuples resulting from long alignment links, which
happens to be a common situation when dealing with translations
between distant pairs of languages. This problem is closely related to
reordering, and some preliminary results have been presented by Crego,
Marin?o, and de Gispert (2005b).
 The definition of the tuple as a bilingual pair will be revised in order to
better handle unaligned words in both the source and the target sides. As
mentioned above, a better strategy for dealing with target words aligned
to NULL is required. Similarly, a better handling of NULLs in the target
side will result in fewer omitted-translation errors.
 The extension of the embedded-word concept to the more general idea of
embedded n-grams should be evaluated and implemented. Accordingly, a
translation probability should be estimated for those groups of words
that always occur embedded in tuples. This would guarantee that the
decoder will always have a translation option for any given word or word
combination previously seen in the training data. Further work is required
to determine the relative impact of these embedded n-grams on the
translation model, and the most appropriate strategy for handling them.
 Linguistic information must be used to cope with the observed
morphological problems in the English-to-Spanish translation direction,
as well as the more general problem of incorrect verbal form translations.
In this regard, ongoing research on linguistic tuples classification is
being done in order to improve translation results. Preliminary results
on detecting and classifying verb forms have been presented by
de Gispert (2005).
 A more detailed error analysis than the one presented in Section 5.2 is
required to fully understand the n-gram SMT system behavior and the
specific causes of each resulting type of error. It would be very useful for
improving our translation system performance to clearly identify whether
these errors are due to unseen information while training, to modeling
problems, or to decoding errors.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-STAR (Technology and Corpora for
Speech to Speech Translation) (IST-2002-
FP6-506738, http://www.tc-star.org), the
Spanish Department of Education and
Science (MEC), the Department of
Universities, Research and Information
Society (Generalitat de Catalunya), and
the Universitat Polite`cnica de Catalunya.
References
Banchs, Rachel E., Josep Maria Crego,
Adria` de Gispert, Patrik Lambert, and
Jose? Bernardo Marin?o. 2005. Statistical
machine translation of Euparl data by
using bilingual n-grams. In ACL Workshop
on Data-Driven Machine Translation and
Beyond, pages 133?136, Ann Arbor, MI.
Bangalore, Srinivas and Giuseppe Riccardi.
2000. Stochastic finite-state models for
spoken language machine translation.
547
Computational Linguistics Volume 32, Number 4
In Proceedings of the Workshop on Embedded
Machine Translation Systems, pages 52?59,
Seattle, WA.
Berger, Adam, Stephen Della Pietra, and
Vincent Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Brown, Peter, John Cocke, Stephen Della
Pietra, Vincent Della Pietra, Frederick
Jelinek, John Lafferty, Robert Mercer, and
Paul S. Roossin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter, Stephen Della Pietra, Vincent
Della Pietra, and Robert Mercer. 1993.
The mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Casacuberta, Francisco. 2001. Finite-state
transducers for speech input translation. In
Proceedings IEEE ASRU, pages 375?380,
Madonna di Campiglio, Italy.
Casacuberta, Francisco and Enrique Vidal.
2004. Machine translation with inferred
stochastic finite-state transducers.
Computational Linguistics, 30(2):205?225.
Costa-jussa`, Marta Ruiz, Jose? Adria?n
Rodriguez Fonollosa, and Enric Monte.
2006. Using reordering in statistical
machine translation based on alignment
block classification. Internal Report.
http://gps-tsc.upc.es/veu/personal/
mruiz/docs/br06.pdf.
Crego, Josep Maria, Jose? Bernardo
Marin?o, and Adria` de Gispert. 2004.
Finite-state-based and phrase-based
statistical machine translation. In
Proceedings of the 8th International
Conference on Spoken Language
Processing, pages 37?40, Jeju, Korea.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005a. An
Ngram-based statistical machine
translation decoder. In INTERSPEECH
2005, pages 3185?3188, Lisbon, Portugal.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005b. Reordered
search and tuple unfolding for Ngram-
based SMT. Proceedings of the Tenth
Machine Translation Summit, pages 283?289,
Phuket, Thailand.
Crego, Josep Maria, Marta Ruiz Costa-jussa`,
Jose? Bernardo Marin?o, and Jose? Adria?n
Rodriguez Fonollosa. 2005c. Ngram-
based versus phrase-based statistical
machine translation. In Proceedings of the
International Workshop on Spoken Language
Translation, pages 177?184, Pittsburgh, PA.
Crego, Josep Maria and Jose? Bernardo
Marin?o. 2006. Integration of POStag-based
source reordering into SMT decoding by
an extended search graph. In Proceedings of
the 7th Biennial Conference of the Association
for Machine Translation in the Americas,
Boston, MA.
de Gispert, Adria` and Jose? Bernardo Marin?o.
2002. Using X-grams for speech-to-
speech translation. In Proceedings of the
7th International Conference on Spoken
Language Processing, pages 1885?1888,
Denver, CO.
de Gispert, Adria`, Jose? Bernardo Marin?o, and
Josep Maria Crego. 2004. TALP:
Xgram-based spoken language translation
system. In Proceedings of the International
Workshop on Spoken Language Translation,
pages 85?90, Kyoto, Japan.
de Gispert, Adria`. 2005. Phrase linguistic
classification and generalization for
improving statistical machine translation.
In ACL?05 Student Workshop, pages 67?72,
Ann Arbor, MI.
Hutchins, John. 1986. Machine Translation:
Past, Present and Future. Ellis Horwood,
Chichester, England.
Kay, Martin, Jean Mark Gawron, and Peter
Norvig. 1992. Verbmobil: A Translation
System for Face-to-Face Dialog. CSLI.
Kneser, Reinhard and Hermann Ney. 1995.
Improved backing-off for m-gram
language modeling. In IEEE International
Conference on Acoustics, Speech and Signal
Processing, pages 49?52, Detroit, MI.
Knight, Kevin and Yaser Al-Onaizan.
1998. Translation with finite-state
devices. In AI Lecture Notes in Artificial
Intelligence, volume 1529, Springer-Verlag,
pages 421?437.
Koehn, Philippe, Franz Joseph Och,
and Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
chapter of the ACL, pages 48?54, Edmonton,
Alberta, Canada.
Koehn, Philippe. 2002. Europarl: A
multilingual corpus for evaluation
of machine translation. Available
online at: http://people.csail.mit.edu/
people/koehn/publications/europarl/.
Marin?o, Jose? Bernardo, Rafael E. Banchs,
Josep Maria Crego, Adria` de Gispert,
Patrik Lambert, Jose? Adria?n Rodriguez
Fonollosa, and Marta Ruiz. 2005. Bilingual
N-gram statistical machine translation.
In Proceedings of the Tenth Machine
Translation Summit, pages 275?282,
Phuket, Thailand.
548
Marin?o et al N-gram-based Machine Translation
Ney, Hermann, Volker Steinbiss, Richard
Zens, Evgeny Matusov, Jorge Gonza?lez,
Young-suk Lee, Salim Roukos, Marcello
Federico, Muntsin Kolss, and Rafael
Banchs. 2005. SLT progress report.
TC-STAR Deliverable D5, European
Community project no. FP6-506738.
Available online at: http://www.
tc-star.org/pages/f documents.htm.
Och, Franz Joseph and Hermann Ney.
2000. Improved statistical alignment
models. In Proceedings of the 38th Annual
Meeting of the ACL, pages 440?447,
Hong Kong, China.
Och, Franz Joseph and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295?302,
Philadelphia, PA.
Och, Franz Joseph and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Och, Franz Joseph, Daniel Gildea, Sanjeev
Khudanpur, Anoop Sarkar, Kenji Yamada,
Alexander Fraser, Shankar Kumar, Libin
Shen, David Smith, Katharine Eng, Viren
Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical
machine translation. In Proceedings of the
Human Language Technology Conference
NAACL, pages 161?168, Boston, MA, May.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the ACL,
pages 311?318, Philadelphia, PA.
Press, William H., Saul Teukolsky, William
Vetterling, and Brian P. Flannery.
2002. Numerical Recipes in C++: The
Art of Scientific Computing, Cambridge
University Press.
Riccardi, Giuseppe, Roberto Pieraccini, and
Enrico Bocchieri. 1996. Stochastic automata
for language modeling. Computer Speech
and Language, 10(4):265?293.
Shannon, Claude E. 1949. Communication
theory of secrecy systems. Bell System
Technical Journal, 28:656?715.
Shannon, Claude E. 1951. Prediction and
entropy of printed English. Bell System
Technical Journal, 30:50?64.
Shannon, Claude E. and Warren Weaver.
1949. The Mathematical Theory of
Communication, University of Illinois
Press, Urbana, IL.
Stolcke, Andreas 2002. SRLIM: An extensible
language modeling toolkit. In Proceedings
of the International Conference on Spoken
Language Processing, pages 901?904,
Denver, CO.
Tillmann, Christoph and Fei Xia. 2003. A
phrase-based unigram model for statistical
machine translation. In Proceedings of
HLT-NAACL - Short Papers, pages 106?108,
Edmonton, Alberta, Canada.
Vidal, Enrique. 1997. Finite-state speech-to-
speech translation. In Proceedings of 1997
IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 111?114,
Munich, Germany.
Weaver, Warren. 1955. Translation. In
William Locke and A. Donald Booth,
editors, Machine Translation of Languages:
Fourteen Essays. John Wiley & Sons, New
York, pages 15?23.
Zens, Richard, Franz Joseph Och, and
Hermann Ney. 2002. Phrase-based
statistical machine translation. In
25th German Conference on Artificial
Intelligence, pages 18?32, September.
Aachen, Springer Verlag.
549

Proceedings of NAACL HLT 2007, Companion Volume, pages 137?140,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Analysis and System Combination of Phrase- and N -gram-based
Statistical Machine Translation Systems
Marta R. Costa-jussa`1, Josep M. Crego1, David Vilar2
Jose? A. R. Fonollosa1, Jose? B. Marin?o1 and Hermann Ney2
1TALP Research Center (UPC), Barcelona 08034, Spain
{mruiz,jmcrego,adrian,canton}@gps.tsc.upc.edu
2RWTH Aachen University, Aachen D-52056, Germany
{vilar,ney}@i6.informatik.rwth-aachen.de
Abstract
In the framework of the Tc-Star project,
we analyze and propose a combination of
two Statistical Machine Translation sys-
tems: a phrase-based and an N -gram-based
one. The exhaustive analysis includes a
comparison of the translation models in
terms of efficiency (number of translation
units used in the search and computational
time) and an examination of the errors in
each system?s output. Additionally, we
combine both systems, showing accuracy
improvements.
1 Introduction
Statistical machine translation (SMT) has evolved
from the initial word-based translation models to
more advanced models that take the context sur-
rounding the words into account. The so-called
phrase-based and N -gram-based models are two ex-
amples of these approaches (Zens and Ney, 2004;
Marin?o et al, 2006).
In current state-of-the-art SMT systems, the
phrase-based or the N -gram-based models are usu-
ally the main features in a log-linear framework, rem-
iniscent of the maximum entropy modeling approach.
Two basic issues differentiate the N -gram-based
system from the phrase-based one: the training data
is sequentially segmented into bilingual units; and
the probability of these units is estimated as a bilin-
gual N -gram language model. In the phrase-based
model, no monotonicity restriction is imposed on the
segmentation and the probabilities are normally es-
timated simply by relative frequencies.
This paper extends the analysis of both systems
performed in (Crego et al, 2005a) by additionally
performing a manual error analysis of both systems,
which were the ones used by UPC and RWTH in the
last Tc-Star evaluation.
Furthermore, we will propose a way to combine
both systems in order to improve the quality of trans-
lations.
Experiments combining several kinds of MT sys-
tems have been presented in (Matusov et al, 2006),
based only on the single best output of each system.
Recently, a more straightforward approach of both
systems has been performed in (Costa-jussa` et al,
2006) which simply selects, for each sentence, one of
the provided hypotheses.
This paper is organized as follows. In section 2,
we briefly describe the phrase and the N -gram-based
baseline systems. In the next section we present the
evaluation framework. In Section 4 we report a struc-
tural comparison performed for both systems and, af-
terwards, in Section 5, we analyze the errors of both
systems. Finally, in the last two sections we rescore
and combine both systems, and the obtained results
are discussed.
2 Baseline Systems
2.1 Phrase-based System
The basic idea of phrase-based translation is to seg-
ment the given source sentence into units (here called
phrases), then translate each phrase and finally com-
pose the target sentence from these phrase transla-
tions.
In order to train these phrase-based models, an
alignment between the source and target training
sentences is found by using the standard IBM mod-
els in both directions (source-to-target and target-
to-source) and combining the two obtained align-
ments. Given this alignment an extraction of con-
tiguous phrases is carried out, specifically we extract
all phrases that fulfill the following restrictions: all
source (target) words within the phrase are aligned
only to target (source) words within the phrase.
The probability of these phrases is normally esti-
mated by relative frequencies, normally in both di-
rections, which are then combined in a log-linear way.
137
2.2 N-gram-based System
In contrast with standard phrase-based approaches,
the N -gram translation model uses tuples as bilin-
gual units whose probabilities are estimated as an
N -gram language model (Marin?o et al, 2006). This
model approximates the joint probability between
the source and target languages by using N -grams.
Given a word alignment, tuples define a unique
and monotonic segmentation of each bilingual sen-
tence, building up a much smaller set of units
than with phrases and allowing N -gram estimation
to account for the history of the translation pro-
cess (Marin?o et al, 2006).
2.3 Feature functions
Both baseline systems are combined in a log-linear
way with several additional feature functions: a tar-
get language model, a forward and a backward lex-
icon model and a word bonus are common features
for both systems. The phrase-based system also in-
troduces a phrase bonus model.
3 Evaluation framework
The translation models presented so far were the ones
used by UPC and RWTH in the second evaluation
campaign of the Tc-Star project. The goal of this
project is to build a speech-to-speech translation sys-
tem that can deal with real life data.
The corpus consists of the official version of the
speeches held in the European Parliament Plenary
Sessions (EPPS), as available on the web page of the
European Parliament. Table 1 shows some statistics.
The following tools have been used for building
both systems: Word alignments were computed us-
ing GIZA++ (Och, 2003), language models were es-
timated using the SRILM toolkit (Stolcke, 2002), de-
coding was carried out by the free available MARIE
decoder (Crego et al, 2005b) and the optimization
was performed through an in-house implementation
of the simplex method (Nelder and Mead, 1965).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Dev Sentences 1 122 699
Words 26K 21K
Test Sentences 1 117 894
Words 26K 26K
Table 1: Statistics of the EPPS Corpora.
4 Structural comparison
Both approaches aim at improving accuracy by in-
cluding word context in the model. However, the
implementation of the models are quite different and
may produce variations in several aspects.
Table 2 shows the effect on decoding time intro-
duced through different settings of the beam size.
Additionally, the number of available translation
units is shown, corresponding to number of avail-
able phrases for the phrase-based system and 1gram,
2gram and 3gram entries for the N -gram-based sys-
tem. Results are computed on the development set.
Task Beam Time(s) Units
50 2,677
es?en 10 852 537k
5 311
50 2,689
en?es 10 903 594k
5 329
50 1,264
es?en 10 281 104k 288k 145k
5 138
50 1,508
en?es 10 302 118k 355k 178k
5 155
Table 2: Impact on efficiency of the beam size in PB
(top) and NB system (bottom).
As it can be seen, the number of translation units
is similar in both tasks for both systems (537k ?
537k for Spanish to English and 594k ? 651k for
English to Spanish) while the time consumed in de-
coding is clearly higher for the phrase-based system.
This can be explained by the fact that in the phrase-
based approach, the same translation can be hypoth-
esized following several segmentations of the input
sentence, as phrases appear (and are collected) from
multiple segmentations of the training sentence pairs.
In other words, the search graph seems to be over-
populated under the phrase-based approach.
Table 3 shows the effect on translation accuracy
regarding the size of the beam in the search. Results
are computed on the test set for the phrase-based
and N -gram-based systems.
Results of the N -gram-based system show that de-
creasing the beam size produces a clear reduction
of the accuracy results. The phrase-based system
shows that accuracy results remain very similar un-
der the different settings. The reason is found on
how translation models are used in the search. In
the phrase-based approach, every partial hypothesis
138
Task Beam BLEU NIST mWER
50 51.90 10.53 37.54
es?en 10 51.93 10.54 37.49
5 51.87 10.55 37.47
50 47.75 9.94 41.20
en?es 10 47.77 9.96 41.09
5 47.86 10.00 40.74
50 51.63 10.46 37.88
es?en 10 51.50 10.45 37.83
5 51.39 10.45 37.85
50 47.73 10.08 40.50
en?es 10 46.82 9.97 41.04
5 45.59 9.83 41.04
Table 3: Impact on accuracy of the beam size in PB
(top) and NB system (bottom).
is scored uncontextualized, hence, a single score is
used for a given partial hypothesis (phrase). In the
N -gram-based approach, the model is intrinsically
contextualized, which means that each partial hy-
pothesis (tuple) depends on the preceding sequence
of tuples. Thus, if a bad sequence of tuples (bad
scored) is composed of a good initial sequence (well
scored), it is placed on top of the first stacks (beam)
and may cause the pruning of the rest of hypotheses.
5 Error analysis
In order to better asses the quality and the differ-
ences between the two systems, a human error anal-
ysis was carried out. The guidelines for this error
analysis can be found in (Vilar et al, 2006). We
randomly selected 100 sentences, which were evalu-
ated by bilingual judges.
This analysis reveals that both systems produce
the same kind of errors in general. However some dif-
ferences were identified. For the English to Spanish
direction the greatest problem is the correct genera-
tion of the right tense for verbs, with around 20% of
all translation errors being of this kind. Reordering
also poses an important problem for both phrase and
N-gram-based systems, with 18% or 15% (respec-
tively) of the errors falling into this category. Miss-
ing words is also an important problem. However,
most of them (approximately two thirds for both sys-
tems) are filler words (i.e. words which do not con-
vey meaning), that is, the meaning of the sentence
is preserved. The most remarkable difference when
comparing both systems is that the N -gram based
system produces a relatively large amount of extra
words (approximately 10%), while for the phrase-
based system, this is only a minor problem (2% of
the errors). In contrast the phrase-based system has
more problems with incorrect translations, that is
words for which a human can find a correspondence
in the source text, but the translation is incorrect.
Similar conclusions can be drawn for the inverse di-
rection. The verb generating problem is not so acute
in this translation direction due to the much simpli-
fied morphology of English. An important problem
is the generation of the right preposition.
The N -gram based system seems to be able to pro-
duce more accurate translations (reflected by a lower
percentage of translation errors). However, it gener-
ates too many additional (and incorrect words) in
the process. The phrase-based system, in contrast,
counteracts this effect by producing a more direct
correspondence with the words present in the source
sentence at the cost of sometimes not being able to
find the exact translation.
6 System Rescoring and
Combination
Integration of both output translations in the search
procedure is a complex task. Translation units of
both models are quite different and generation his-
tories pose severe implementation difficulties. We
propose a method for combining the two systems at
the level of N -best lists.
Some features that are useful for SMT are too com-
plex for including them directly in the search pro-
cess. A clear example are the features that require
the entire target sentence to be evaluated, as this is
not compatible with the pruning and recombination
procedures that are necessary for keeping the target
sentence generation process manageable. A possible
solution for this problem is to apply sentence level
re-ranking by using N -best lists.
6.1 Rescoring Criteria
The aim of the rescoring procedure is to choose the
best translation candidate out of a given set of N
possible translations. In our approach this transla-
tion candidates are produced independently by both
of the systems and then combined by a simple con-
catenation1. In order for the hypothesis to have a
comparable set of scores, we perform an additional
?cross-rescoring? of the lists.
Given an N -best list of the phrase-based (N -gram-
based) system, we compute the cost of each target
sentence of this N -best list for the N -gram-based
(phrase-based) system. However this computation
is not possible in all cases. Table 4 shows the per-
centage of target sentences that the N -gram-based
1With removal of duplicates.
139
(phrase-based) system is able to produce given an N -
best list of target sentences computed by the phrase-
based (N -gram-based) system. This percentage is
calculated on the development set.
The vocabulary of phrases is bigger than the vo-
cabulary of tuples, due to the fact that phrases are
extracted from multiple segmentations of the train-
ing sentence pairs. Hence, the number of sentences
reproduced by the N -gram-based system is smaller
than the number of sentences reproduced by the
phrase-based system. Whenever a sentence can not
be reproduced by a given system, the cost of the
worst sentence in the N -best list is assigned to it.
Task N -best % NB % PB
es?en 1000 37.5 57.5
en?es 1000 37.2 48.6
Table 4: Sentences (%) produced by each system.
6.2 Results
Table 5 shows results of the rescoring and system
combination experiments on the test set. The first
two rows include results of systems non-rescored and
PB (NB) rescored by NB (PB). The third row corre-
sponds to the system combination. Here, PB (NB)
rescored by NB (PB) are simply merged and ranked
by rescored score.
System N -best BLEU NIST mWER
Spanish-to-English
PB 1 51.90 10.54 37.50
PB 1000 52.55 10.61 37.12
NB 1 51.63 10.46 37.88
NB 1000 52.25 10.55 37.43
PB+NB 2 51.77 10.49 37.68
PB+NB 2000 52.31 10.56 37.32
English-to-Spanish
PB 1 47.75 9.94 41.2
PB 1000 48.46 10.13 39.98
NB 1 47.73 10.09 40.50
NB 1000 48.33 10.15 40.13
PB+NB 2 48.26 10.05 40.61
PB+NB 2000 48.54 10.16 40.00
Table 5: Rescoring and system combination results.
7 Discussion
The structural comparison has shown on the one
hand that the N -gram-based system outperforms
the phrase-based in terms of search time efficiency
by avoiding the overpopulation problem presented
in the phrase-based approach. On the other hand
the phrase-based system shows a better performance
when decoding under a highly constrained search.
A detailed error analysis has also been carried out
in order to better determine the differences in per-
formance of both systems. The N -gram based sys-
tem produced more accurate translations, but also a
larger amount of extra (incorrect) words when com-
pare to the phrase-based translation system.
In section 6 we have presented a system combina-
tion method using a rescoring feature for each SMT
system, i.e. the N -gram-based feature for the phrase-
based system and vice-versa. For both systems, con-
sidering the feature of the opposite system leads to
an improvement of BLEU score.
References
M.R. Costa-jussa`, J.M. Crego, A. de Gispert,
P. Lambert, M. Khalilov J.A.R. Fonollosa, J.B.
Marin?o, and R. Banchs. 2006. Talp phrase-based
statistical machine translation and talp system
combination the iwslt 2006. IWSLT06.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005a. N-gram-based versus phrase-
based statistical machine translation. IWSLT05,
October.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005b.
An Ngram-based statistical machine translation
decoder. ICSLP05, April.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M.R.
Costa-jussa`. 2006. N-gram based machine trans-
lation. Computational Linguistics, 32(4):527?549.
E. Matusov, N. Ueffing, and H. Ney. 2006. Com-
puting consensus translation from multiple ma-
chine translation systems using enhanced hypothe-
ses alignment. EACL06, pages 33?40.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language
modeling toolkit. Proc. of the 7th Int. Conf. on
Spoken Language Processing, ICSLP?02, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and
Hermann Ney. 2006. Error Analysis of Machine
Translation Output. In LREC06, pages 697?702,
Genoa, Italy, May.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In HLT04, pages 257?264, Boston, MA, May.
140
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149?154,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Improving Phrase-Based Statistical Translation by modifying
phrase extraction and including several features
Marta Ruiz Costa-jussa` and Jose? A. R. Fonollosa
TALP Research Center
Universitat Polite`cnica de Catalunya
{mruiz,adrian}@gps.tsc.upc.edu
Abstract
Nowadays, most of the statistical translation sys-
tems are based on phrases (i.e. groups of words).
In this paper we study different improvements to
the standard phrase-based translation system. We
describe a modified method for the phrase extrac-
tion which deals with larger phrases while keeping
a reasonable number of phrases. We also propose
additional features which lead to a clear improve-
ment in the performance of the translation. We
present results with the EuroParl task in the direc-
tion Spanish to English and results from the evalu-
ation of the shared task ?Exploiting Parallel Texts
for Statistical Machine Translation? (ACL Work-
shop on Parallel Texts 2005).
1 Introduction
Statistical Machine Translation (SMT) is based on
the assumption that every sentence e in the target
language is a possible translation of a given sen-
tence f in the source language. The main difference
between two possible translations of a given sen-
tence is a probability assigned to each, which has
to be learned from a bilingual text corpus. Thus,
the translation of a source sentence f can be for-
mulated as the search of the target sentence e that
maximizes the translation probability P (e|f),
e? = argmax
e
P (e|f) (1)
0This work has been supported by the European Union
under grant FP6-506738 (TC-STAR project).
If we use Bayes rule to reformulate the transla-
tion probability, we obtain,
e? = argmax
e
P (f |e)P (e) (2)
This translation model is known as the source-
channel approach [1] and it consists on a lan-
guage model P (e) and a separate translation model
P (f |e) [5].
In the last few years, new systems tend to use
sequences of words, commonly called phrases [8],
aiming at introducing word context in the transla-
tion model. As alternative to the source-channel
approach the decision rule can be modeled through
a log-linear maximum entropy framework.
e? = argmax
e
{ M
?
m=1
?mhm(e, f)
}
(3)
The features functions, hm, are the system mod-
els (translation model, language model and others)
and weigths, ?i, are typically optimized to max-
imize a scoring function. It is derived from the
Maximum Entropy approach suggested by [13] [14]
for a natural language understanding task. It has
the advantatge that additional features functions
can be easily integrated in the overall system.
This paper addresses a modification of the
phrase-extraction algorythm in [11]. It also com-
bines several interesting features and it reports an
important improvement from the baseline. It is or-
ganized as follows. Section 2 introduces the base-
line; the following section explains the modification
in the phrase extraction; section 4 shows the differ-
ent features which have been taken into account;
section 5 presents the evaluation framework; and
149
the final section shows some conclusions on the ex-
periments in the paper and on the results in the
shared task.
2 Baseline
The baseline is based on the source-channel ap-
proach, and it is composed of the following models
which later will be combined in the decoder.
The Translation Model. It is based on bilin-
gual phrases, where a bilingual phrase (BP ) is
simply two monolingual phrases (MP ) in which
each one is supposed to be the translation of each
other. A monolingual phrase is a sequence of words.
Therefore, the basic idea of phrase-based transla-
tion is to segment the given source sentence into
phrases, then translate each phrase and finally com-
pose the target sentence from these phrase transla-
tions [17].
During training, the system has to learn a dictio-
nary of phrases. We begin by aligning the training
corpus using GIZA++ [6], which is done in both
translation directions. We take the union of both
alignments to obtain a symmetrized word align-
ment matrix. This alignment matrix is the starting
point for the phrase based extraction.
Next, we define the criterion to extract the set of
BP of the sentence pair (f j2j1 ; e
i2
i1) and the alignment
matrix A ? J?I , which is identical to the alignment
criterion described in [11].
BP (fJ1 , eI1, A) = {(f j2j1 , e
i2
i1) :
?(j, i)?A : j1 ? j ? j2 ? i1 ? i ? i2
??(j, i)?A : j1 ? j ? j2 ? i1 ? i ? i2}
The set of BP is consistent with the alignment
and consists of all BP pairs where all words within
the foreign language phrase are only aligned to the
words of the English language phrase and viceversa.
At least one word in the foreign language phrase has
to be aligned with at least one word of the English
language. Finally, the algorithm takes into account
possibly unaligned words at the boundaries of the
foreign or English language phrases.
The target language model. It is combined
with the translation probability as showed in equa-
tion (2). It gives coherence to the target text ob-
tained by the concatenated phrases.
3 Phrase Extraction
Motivation. The length of a MP is defined as
its number of words. The length of a BP is the
greatest of the lengths of its MP .
As we are working with a huge amount of data
(see corpus statistics), it is unfeasible to build a
dictionary with all the phrases longer than length
4. Moreover, the huge increase in computational
and storage cost of including longer phrases does
not provide a significant improve in quality [8].
X-length In our system we considered two length
limits. We first extract all the phrases of length 3
or less. Then, we also add phrases up to length
5 if they cannot be generated by smaller phrases.
Empirically, we chose 5, as the probability of reap-
pearence of larger phrases decreases.
Basically, we select additional phrases with
source words that otherwise would be missed be-
cause of cross or long alignments. For example,
from the following sentence,
Cuando el Parlamento Europeo , que tan fre-
cuentemente insiste en los derechos de los traba-
jadores y en la debida proteccio?n social , (...)
NULL ( ) When ( 1 ) the ( 2 ) European ( 4
) Parliament ( 3 4 ) , ( 5 ) that ( 6 ) so ( 7 )
frequently ( 8 ) insists ( 9 ) on ( 10 ) workers ( 11
15 ) ? ( 14 ) rights ( 12 ) and ( 16 ) proper ( 19 )
social ( 21 ) protection ( 20 ) , ( 22 ) (...)
where the number inside the clauses is the
aligned word(s). And the phrase that we are look-
ing for is the following one.
los derechos de los trabajadores # workers ?
rights
which only could appear in the case the maximum
length was 5.
150
4 Phrase ranking
4.1 Conditional probability P (f |e)
Given the collected phrase pairs, we estimated the
phrase translation probability distribution by rela-
tive frecuency.
P (f |e) = N(f, e)N(e) (4)
where N(f,e) means the number of times the phrase
f is translated by e. If a phrase e has N > 1
possible translations, then each one contributes as
1/N [17].
Note that no smoothing is performed, which may
cause an overestimation of the probability of rare
phrases. This is specially harmful given a BP
where the source part has a big frecuency of ap-
pearence but the target part appears rarely. For
example, from our database we can extract the fol-
lowing BP : ?you # la que no?, where the English
is the source language and the Spanish, the tar-
get language. Clearly, ?la que no? is not a good
translation of ?you?, so this phrase should have a
low probability. However, from our aligned training
database we obtain,
P (f |e) = P (you|la que no) = 0.23
This BP is clearly overestimated due to sparse-
ness. On the other, note that ?la que no? can-
not be considered an unusual trigram in Spanish.
Hence, the language model does not penalise this
target sequence either. So, the total probability
(P (f |e)P (e)) would be higher than desired.
In order to somehow compensate these unreili-
able probabilities we have studied the inclusion of
the posterior [12] and lexical probabilities [1] [10]
as additional features.
4.2 Feature P (e|f)
In order to estimate the posterior phrase probabil-
ity, we compute again the relative frequency but re-
placing the count of the target phrase by the count
of the source phrase.
P (e|f) = N
?(f, e)
N(f) (5)
where N?(f,e) means the number of times the
phrase e is translated by f. If a phrase f has N > 1
possible translations, then each one contributes as
1/N.
Adding this feature function we reduce the num-
ber of cases in which the overall probability is over-
estimated. This results in an important improve-
ment in translation quality.
4.3 IBM Model 1
We used IBM Model 1 to estimate the probability
of a BP . As IBM Model 1 is a word translation and
it gives the sum of all possible alignment probabil-
ities, a lexical co-ocurrence effect is expected. This
captures a sort of semantic coherence in transla-
tions.
Therefore, the probability of a sentence pair is
given by the following equation.
P (f |e; M1) = 1(I + 1)J
J
?
j=1
I
?
i=0
p(fj |ei) (6)
The p(fj |ei) are the source-target IBM Model 1
word probabilities trained by GIZA++. Because
the phrases are formed from the union of source-to-
target and target-to-source alignments, there can
be words that are not in the P (fj |ei) table. In this
case, the probability was taken to be 10?40.
In addition, we have calculated the IBM?1 Model
1.
P (e|f ; M1) = 1(J + 1)I
I
?
I=1
J
?
j=0
p(ei|fj) (7)
4.4 Language Model
The English language model plays an important
role in the source channel model, see equation (2),
and also in its modification, see equation (3). The
English language model should give an idea of the
sentence quality that is generated.
As default language model feature, we use a stan-
dard word-based trigram language model generated
with smoothing Kneser-Ney and interpolation (by
using SRILM [16]).
4.5 Word and Phrase Penalty
To compensate the preference of the target lan-
guage model for shorter sentences, we added two
151
Spanish English
Train Sentences 1223443 1223443
Words 34794006 33379333
Vocabulary 168685 104975
Dev Sentences 504 504
Words 15353 15335
OOV 25 16
Test Sentences 504 504
Words 10305 10667
OOV 36 19
Table 1: Statistics of training and test corpus
simple features which are widely used [17] [7]. The
word penalty provides means to ensure that the
translations do not get too long or too short. Neg-
ative values for the word penalty favor longer out-
put, positive values favor shorter output [7].
The phrase penalty is a constant cost per pro-
duced phrase. Here, a negative weight, which
means reducing the costs per phrase, results in a
preference for adding phrases. Alternatively, by us-
ing a positive scaling factors, the system will favor
less phrases.
5 Evaluation framework
5.1 Corpus Statistics
Experiments were performed to study the effect
of our modifications in the phrases. The training
material covers the transcriptions from April 1996
to September 2004. This material has been dis-
tributed by the European Parlament. In our ex-
periments, we have used the distribution of RWTH
of Aachen under the project of TC-STAR 1. The
test material was used in the first evaluation of the
project in March 2005. In our case, we have used
the development divided in two sets. This mate-
rial corresponds to the transcriptions of the sessions
from October the 21st to October the 28th. It has
been distributed by ELDA2. Results are reported
for Spanish-to-English translations.
1http://www.tcstar.org/
2http://www.elda.org/
5.2 Experiments
The decoder used for the presented translation sys-
tem is reported in [2]. This decoder is called
MARIE and it takes into account simultaneously
all the 7 features functions described above. It im-
plements a beam-search strategy.
As evaluation criteria we use: the Word Error
Rate (WER), the BLEU score [15] and the NIST
score [3].
As follows we report the results for several ex-
periments that show the performance of: the base-
line, adding the posterior probability, IBM Model
1 and IBM1?1, and, finally, the modification of the
phrases extraction.
Optimisation. Significant improvements can be
obtained by tuning the parameters of the features
adequately. In the complet system we have 7 pa-
rameters to tune: the relatives frecuencies P (f |e)
and P (e|f), IBM Model 1 and its inverse, the word
penalty, the phrase penalty and the weight of the
language model. We applied the widely used algo-
rithm SIMPLEX to optimise [9]. In Table 2 (line
5th), we see the final results.
Baseline. We report the results of the baseline.
We use the union alignment and we extract the
BP of length 3. As default language model fea-
ture, we use the standard trigram with smoothing
Kneser-Ney and interpolation. Also we tune the
parameters (only two parameters) with the SIM-
PLEX algorithm (see Table 2).
Posterior probability. Table 2 shows the effect
of using the posterior probability: P (e|f). We use
all the features but the P (e|f) and we optimise the
parameters. We see the results without this feature
decrease around 1.1 points both in BLEU and WER
(see line 2rd and 5th in Table 2).
IBM Model 1. We do the same as in the para-
graph above, we do not consider the IBM Model
1 and the IBM1?1. Under these conditions, the
translation?s quality decreases around 1.3 points
both in BLEU and WER (see line 3th and 5th in
Table 2).
152
Modification of the Phrase Extraction. Fi-
nally, we made an experiment without modification
of the phrases? length. We can see the comparison
between: (1) the phrases of fixed maximum length
of 3; and (2) including phrases with a maximum
length of 5 which can not be generated by smaller
phrases. We can see it in Table 2 (lines 4th and
5th). We observe that there is no much difference
between the number of phrases, so this approach
does not require more resources. However, we get
slightly better scores.
5.3 Shared Task
This section explains the participation of ?Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion?. We used the EuroParl data provided for this
shared task [4]. A word-to-word alignment was per-
formed in both directions as explained in section
2. The phrase-based translation system which has
been considered implements a total of 7 features
(already explained in section 4). Notice that the
language model has been trained with the training
provided in the shared task. However, the opti-
mization in the parameters has not been repeated,
and we used the parameters obtained in the sub-
section above. We have obtained the results in the
Table 3.
6 Conclusions
We reported a new method to extract longer
phrases without increasing the quantity of phrases
(less than 0.5%).
We also reported several features as P (e|f)
which in combination with the functions of the
source-channel model provides significant improve-
ment. Also, the feature IBM1 in combination with
IBM1?1 provides improved scores, too.
Finally, we have optimized the parameters, and
we provided the final results which have been pre-
sented in the Shared Task: Exploiting Parallel
Texts for Statistical Machine Translation (June 30,
2005) in conjunction with ACL 2005 in Ann Arbor,
Michigan.
7 Acknowledgements
The authors want to thank Jose? B. Marin?o, Adria`
de Gispert, Josep M. Crego, Patrik Lambert and
Rafael E. Banchs (members of the TALP Research
Center) for their contribution to this work.
References
[1] P.F. Brown, J. Cocke, S.A. Della Pietra,
V.J. Della Pietra, F. Jelinek, J.D. Lafferty,
R.L. Mercer, and P.S. Rossin. A statistical ap-
proach to machine translation. Computational
Linguistics, 16(2):79?85, June 1990.
[2] Josep M. Crego, Jose? B. Marin?o, and Adria`
de Gispert. An Ngram-based Statistical Ma-
chine Translation Decoder. In Draft, 2005.
[3] G. Doddington. Automatic evaluation ma-
chine translation quality using n-gram co-
ocurrence statistics. In Proc. ARPA Workshop
on Human Language Technology, 2002.
[4] EuroParl: European Parliament Proceed-
ings Parallel Corpus. Available on-line at:
http://people.csail.mit.edu/koehn/publica-
tions/europarl/. 1996-2003.
[5] I. Garc??a-Varea. Traduccio?n Automa?tica es-
tad??stica: Modelos de Traduccio?n basados en
Ma?xima Entrop??a y Algoritmos de Bu?squeda .
UPV, Diciembre 2003.
[6] Giza++. http://www-i6.informatik.rwth-
aachen.de/?och/software/giza++.html/,
1999.
[7] P. Koehn. A Beam Search Decoder for Phrase-
Based Statistical Machine Translation Models.
2003.
[8] P. Koehn, F. J. Och, and D. Marcu. Statisti-
cal phrase-based translation. In Proceedings of
the Human Language Technology Conference
(HLT-NAACL), pages 127?133, May 2003.
[9] J.A. Nelder and R. Mead. A simplex method
for function minimization. The Computer
Journal, 7:308?313, 1965.
153
Phr Length ?LM ?p(f |e) ?p(e|f) ?IBM1 ?IBM1?1 ?PP ?WP WER BLEU NIST # frases
3 0.788 0.906 0 0 0 0 0 33.98 57.44 10.11 67.7M
3+5length 0.788 0.941 0 0.771 0.200 3.227 0.448 28.97 64.71 11.07 68M
3+5length 0.788 0.824 0.820 0 0 3.430 -0.083 29.17 64.59 10.99 68M
3 0.746 0.515 0.979 0.514 0.390 1.537 -1.264 27.94 65.70 11.18 67.7M
3+5length 0.788 0.617 0.810 0.635 0.101 1.995 -0.296 27.88 65.82 11.23 68M
Table 2: Results for the different experiments with optimized parameters in the direction SPA->ENG
Phr Length ?LM ?p(f |e) ?p(e|f) ?IBM1 ?IBM1?1 ?PP ?WP BLEU # frases
3+5length 0.788 0.617 0.810 0.635 0.101 1.995 -0.296 29.84 34.8M
Table 3: Results for the ACL training and ACL test (SPA->ENG)
[10] F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar,
K. Yamada, A. Fraser, S. Kumar, L. Shen,
D. Smith, K. Eng, V. Jain, Z. Jin, and
D. Radev. A Smorgasbord of Features for Sta-
tistical Machine Translation. In Proceedings of
the Human Language Technology Conference
(HLT-NAACL), May 2004.
[11] F. J. Och and H. Ney. The Alignment Tem-
plate Approach to Statistical Machine Trans-
lation. Computational linguistics, 30:417?449,
December 2004.
[12] Franz Josef Och and Hermann Ney. Discrimi-
native Training and Maximum Entropy Mod-
els for Statistical Machine Translation. In
ACL, pages pages 295?302, July 2002.
[13] Papineni, S.Roukos, and R.T. Ward. Feature-
based language understanding. In European
Conf. on Speech Communication and Technol-
ogy, pages 1435?1438, September 1997.
[14] Papineni, S.Roukos, and R.T. Ward. Maxi-
mum likelihood and discriminative training of
direct translation models. In Proc. Int. Conf.
on Acoustics, Speech, and Signal Proceedings,
pages 189?192, May 1998.
[15] K.A. Papineni, S. Roukos, T. Ward, and W.J.
Zhu. Bleu: a method for automatic evaluation
of machine translation. In Technical Report
RC22176 (W0109-022), IBM Research Divi-
sion, 2001.
[16] A. Stolcke. SRILM - An Extensible Language
Modeling Toolkit. In Proceedings Intl. Confer-
ence Spoken Language Processing, September
2002.
[17] R. Zens and H. Ney. Improvements in Phrase-
Based Statistical Machine Translation. In
Proceedings of the Human Language Technol-
ogy Conference (HLT-NAACL), pages 257?
264, May 2004.
154
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 70?76,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Statistical Machine Reordering
Marta R. Costa-jussa` and Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,adrian)@gps.tsc.upc.edu
Abstract
Reordering is currently one of the most
important problems in statistical machine
translation systems. This paper presents
a novel strategy for dealing with it: sta-
tistical machine reordering (SMR). It con-
sists in using the powerful techniques de-
veloped for statistical machine translation
(SMT) to translate the source language
(S) into a reordered source language (S?),
which allows for an improved translation
into the target language (T). The SMT task
changes from S2T to S?2T which leads to a
monotonized word alignment and shorter
translation units. In addition, the use of
classes in SMR helps to infer new word
reorderings. Experiments are reported in
the EsEn WMT06 tasks and the ZhEn
IWSLT05 task and show significant im-
provement in translation quality.
1 Introduction
During the last few years, SMT systems
have evolved from the original word-based ap-
proach (Brown et al, 1993) to phrase-based trans-
lation systems (Koehn et al, 2003). In parallel
to the phrase-based approach, the use of bilin-
gual n-grams gives comparable results, as shown
by Crego et al (2005a). Two basic issues differ-
entiate the n-gram-based system from the phrase-
based: training data are monotonously segmented
into bilingual units; and, the model considers n-
gram probabilities rather than relative frequencies.
This translation approach is described in detail by
Marin?o et al (2005). The n-gram-based system
follows a maximum entropy approach, in which a
log-linear combination of multiple models is im-
plemented (Och and Ney, 2002), as an alternative
to the source-channel approach.
In both systems, introducing reordering capabil-
ities is of crucial importance for certain language
pairs. Recently, new reordering strategies have
been proposed in the literature on SMT such as the
reordering of each source sentence to match the
word order in the corresponding target sentence,
see Kanthak et al (2005) and Crego et al (2005b).
Similarly, Matusov et al (2006) describe a method
for simultaneously aligning and monotonizing the
training corpus. The main problems of these ap-
proaches are: (1) the fact that the proposed mono-
tonization is based on the alignment and cannot be
applied to the test sets, and (2) the lack of reorder-
ing generalization.
This paper presents a reordering approach
called statistical machine reordering (SMR) which
improves the reordering capabilities of SMT sys-
tems without incurring any of the problems men-
tioned above. SMR is a first-pass translation
performed on the source corpus, which converts
it into an intermediate representation, in which
source-language words are presented in an order
that more closely matches that of the target lan-
guage. SMR and SMT are performed using the
same modeling tools as n-gram-based systems but
using different statistical log-linear models.
In order to be able to infer new reorderings we
use word classes instead of words themselves as
the input to the SMR system. In fact, the use of
classes to help in the reordering is a key difference
between our approach and standard SMT systems.
This paper is organized as follows: Section 2
outlines the baseline system. Section 3 describes
the reordering strategy in detail. Section 4 presents
and discusses the results, and Section 5 presents
our conclusions and suggestions for further work.
70
2 N-gram-based SMT System
This section briefly describes the n-gram-based
SMT which uses a translation model based on
bilingual n-grams. It is actually a language model
of bilingual units, referred to as tuples, which ap-
proximates the joint probability between source
and target languages by using bilingual n-grams
(de Gispert and Marin?o, 2002).
Bilingual units (tuples) are extracted from any
word alignment according to the following con-
straints:
1. a monotonous segmentation of each bilingual
sentence pairs is produced,
2. no word inside the tuple is aligned to words
outside the tuple, and
3. no smaller tuples can be extracted without vi-
olating the previous constraints.
As a result of these constraints, only one seg-
mentation is possible for a given sentence pair.
Figure 1 presents a simple example which illus-
trates the tuple extraction process.
I would like NULL to eat a huge ice-cream
NULL quisiera ir a comer un helado gigante
t1 t2 t3 t4 t5 t6
Figure 1: Example of tuple extraction from an
aligned bilingual sentence pair.
Two important issues regarding this translation
model must be considered. First, it often occurs
that large number of single-word translation prob-
abilities are left out of the model. This happens
for all words that are always embedded in tuples
containing two or more words. Consider for ex-
ample the word ?ice-cream? in Figure 1. As seen
from the Figure, ?ice-cream? is embedded into tu-
ple t6. If a similar situation is encountered for all
occurrences of ?ice-cream? in the training corpus,
then no translation probability for an independent
occurrence of this word will exist.
To overcome this problem, the tuple 4-gram
model is enhanced by incorporating 1-gram trans-
lation probabilities for all the embedded words de-
tected during the tuple extraction step. These 1-
gram translation probabilities are computed from
the intersection of both, the source-to-target and
the target-to-source alignments.
The second issue has to do with the fact that
some words linked to NULL end up producing tu-
ples with NULL source sides. Consider for exam-
ple the tuple t3 in Figure 1. Since no NULL is ac-
tually expected to occur in translation inputs, this
type of tuple is not allowed. Any target word that
is linked to NULL is attached either to the word
that precedes or the word that follows it. To de-
termine this, we use the IBM1 probabilities, see
Crego et al (2005a).
In addition to the bilingual n-gram transla-
tion model, the baseline system implements a
log-linear combination of four feature functions,
which are described as follows:
? A target language model. This feature con-
sists of a 4-gram model of words, which is
trained from the target side of the bilingual
corpus.
? A word bonus function. This feature intro-
duces a bonus based on the number of target
words contained in the partial-translation hy-
pothesis. It is used to compensate for the sys-
tem?s preference for short output sentences.
? A source-to-target lexicon model. This fea-
ture, which is based on the lexical param-
eters of the IBM Model 1 (Brown et al,
1993), provides a complementary probabil-
ity for each tuple in the translation table.
These lexicon parameters are obtained from
the source-to-target algnments.
? A target-to-source lexicon model. Similarly
to the previous feature, this feature is based
on the lexical parameters of the IBM Model
1 but, in this case, these parameters are ob-
tained from target-to-source alignments.
All these models are combined in the de-
coder. Additionally, the decoder allows for a non-
monotonous search with the following distorsion
model.
71
? A word distance-based distorsion model.
P (tK1 ) = exp(?
K
?
k=1
dk)
where dk is the distance between the first
word of the kth tuple (unit), and the last
word+1 of the (k ? 1)th tuple. Distance
are measured in words referring to the units
source side.
To reduce the computational cost we place lim-
its on the search using two parameters: the dis-
tortion limit (the maximum distance measured in
words that a tuple is allowed to be reordered, m)
and the reordering limit (the maximum number of
reordering jumps in a sentence, j). This feature is
independent of the reordering approach presented
in this paper, so they can be used simultaneously.
In order to combine the models in the decoder
suitably, an optimization tool is needed to compute
log-linear weights for each model.
3 Statistical Machine Reordering
As mentioned in the introduction, SMR and SMT
are based on the same principles. Here, we give
a detailed description of the SMR reordering ap-
proach proposed.
3.1 Concept
The aim of SMR consists in using an SMT sys-
tem to deal with reordering problems. Therefore,
the SMR system can be seen as an SMT system
which translates from an original source language
(S) to a reordered source language (S?), given a
target language (T). Then, the translation tasks
changes from S2T to S?2T. The main difference
between the two tasks is that the latter allows for:
(1) monotonized word alignment, and (2) higher
quality monotonized translation.
3.2 Description
Figure 2 shows the SMR block diagram. The in-
put is the initial source sentence (S) and the output
is the reordered source sentence (S?). There three
blocks inside SMR: (1) class replacing ; (2) the de-
coder, which requires the translation model; and,
(3) the block which reorders the original sentence
using the indexes given by the decoder. The fol-
lowing example specifies the input and output of
each block inside the SMR.
Figure 2: SMR block diagram.
1. Source sentence (S):
El compromiso s?olo podr?a mejorar
2. Source sentence classes (S-c):
C38 C43 C49 C42 C22
3. Decoder output (translation, T ):
C38#0 | C43 C49 C42#1 2 0 | C22#0
where | indicates the segmentation into trans-
lation units and # divides the source and tar-
get. The source part is composed of word
classes and the target part is composed of
the new positions of the source word classes,
starting at 0.
4. SMR output (S?). The reordering information
inside each translation unit of the decoder
output (T ) is applied to the original source
sentence (S):
El s?olo podr?a compromiso mejorar
3.3 Training
For the reordering translation, we used an n-gram-
based SMT system (and considered only the trans-
lation model). Figure 3 shows the block diagram
of the training process of the SMR translation
model, which is a bilingual n-gram-based model.
The training process uses the training source and
target corpora and consists of the following steps:
1. Determine source and target word classes.
2. Align parallel training sentences at the word
level in both translation directions. Compute
the union of the two alignments to obtain a
symmetrized many-to-many word alignment.
3. Extract reordering tuples, see Figure 4.
(a) From union word alignment, extract
bilingual S2T tuples (i.e. source and
target fragments) while maintaining the
72
Figure 3: Block diagram of the training process of the SMR translation model.
Figure 4: Example of the extraction of reordering
tuples (step 3).
alignment inside the tuple. As an ex-
ample of a bilingual S2T tuple consider:
only possible compromise # compromiso
s?olo podr?a # 0-1 1-1 1-2 2-0, as shown
in Figure 4, where the different fields are
separated by # and correspond to: (1)
the target fragment; (2) the source frag-
ment; and (3) the word alignment (in
this case, the fields that respectively cor-
respond to a target and source word are
separated by ?).
(b) Modify the many-to-many word align-
ment from each tuple to many-to-one.
If one source word is aligned to two or
more target words, the most probable
link given IBM Model 1 is chosen, while
the other are omitted (i.e. the num-
ber of source words is the same before
and after the reordering translation). In
the above example, the tuple would be
changed to: only possible compromise
# compromiso s?olo podr?a # 0-1 1-2 2-
0, as Pibm1(only, so?lo) is higher than
Pibm1(possible, so?lo).
(c) From bilingual S2T tuples (with many-
to-one inside alignment), extract bilin-
gual S2S? tuples (i.e. the source frag-
ment and its reordering). As in the ex-
ample: compromiso s?olo podr?a # 1 2 0,
where the first field is the source frag-
ment, and the second is the reordering
of these source words.
(d) Eliminate tuples whose source fragment
consists of the NULL word.
(e) Replace the words of each tuple source
fragment with the classes determined in
Step 1.
4. Compute the bilingual language model of the
bilingual S2S? tuple sequence composed of
the source fragment (in classes) and its re-
order.
Once the translation model is built, the origi-
nal source corpus S is translated into the reordered
source corpus S? with the SMR system, see Fig-
ure 2. The reordered training source corpus and
the original training target corpus are used to train
the SMT system (as explained in Section 2). Fi-
nally, with this system, the reordered test source
corpus is translated.
4 Evaluation Framework
In this section, we present experiments carried out
using the EsEn WMT06 and the ZhEn IWSLT05
parallel corpus. We detail the tools which have
been used and the corpus statistics.
73
EuroParl Spanish English
Training Sentences 727.1 k 727.1 k
Words 15.7 M 15.2 M
Vocabulary 108.7 k 72.3 k
Development Sentences 500 500
Words 15.2 k 14.8 k
Vocabulary 3.6 k 3 k
Test Sentences 3064 3064
Words 91.9 k 85.2 k
Vocabulary 11.1 k 9.1 k
Table 1: Spanish to English task. EuroParl cor-
pus: training, development and test data sets.
4.1 Tools
? The word alignments were computed using
the GIZA++ tool (Och, 2003).
? The word classes were determined us-
ing ?mkcls?, a freely-available tool with
GIZA++.
? The language model was estimated using the
SRILM toolkit (Stolcke, 2002).
? We used MARIE as a decoder (Crego et al,
2005b).
? The optimization tool used for computing
log-linear weights (see Section 2) is based
on the simplex method (Nelder and Mead,
1965).
4.2 Corpus Statistics
Experiments were carried out on the Spanish and
English task of the WMT06 evaluation1 (EuroParl
Corpus) and on the Chinese to English task of the
IWSLT05 evaluation2 (BTEC Corpus). The for-
mer is a large corpus, whereas the latter is a small
corpus translation task. Table 1 and 2 show the
main statistics of the data used, namely the number
of sentences, words, vocabulary, and mean sen-
tence lengths for each language.
4.3 Units
In this section different statistics units of both ap-
proaches (S2T and S?2T) are shown (using the
ZhEn task). All the experiments in this section
were carried out using 100 classes in the SMR
step.
1www.statmt.org/wmt06/shared-task/
2www.slt.atr.jp/IWSLT2005
BTEC Chinese English
Training Sentences 20 k 20 k
Words 176.2 k 182.3 k
Vocabulary 8.7 k 7.3 k
Development Sentences 506 506
Words 3.5 k 3.3 k
Vocabulary 870 799
Test Sentences 506 506
Words 4 k 3 k
Vocabulary 916 818
Table 2: Chinese to English task. BTEC corpus:
training, development and test data sets. Develop-
ment and test data sets have 16 references.
Table 3 shows the vocabulary of bilingual n-
grams and embedded words in the translation
model. Once the reordering translation has been
computed, alignment becomes more monotonic. It
is commonly known that non-monotonicity poses
difficulties for word alignments. Therefore, when
the alignment becomes more monotonic, we ex-
pect an improvement in the alignment, and, there-
fore in the translation. Here, we can observe a
significant enlargement of the number of transla-
tion units, which leads to a growth of the transla-
tion vocabulary. We also observe a decrease in the
number of embedded words (around 20%). From
Section 2, we know that the probability of embed-
ded words is estimated independently of the trans-
lation model. Reducing embedded words allows
for a better estimation of the translation model.
Figure 5 shows the histogram of the tuple size in
the two approaches. We observe that the number
of tuples is similar over length 5. However, there
are a greater number of shorter units in the case of
SMR+NB (shorter units lead to a reduction in data
sparseness).
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 0  2  4  6  8  10  12  14
NB
SMR + NB
Figure 5: Comparison of the histogram of the tuple
size in the two approaches (NB and SMR+NB).
74
System 1gr 2gr 3gr 4gr Embedded
NB 34487 57597 3536 1918 5735
SMR + NB 35638 70947 5894 3412 4632
Table 3: Vocabulary of n-grams and embedded words in the translation model.
System Total Vocabulary
NB 4460 959
SMR + NB 4628 1052
Table 4: Tuples used to translate the test set (total
number and vocabulary).
Table 4 shows the tuples used to translate the
test set (total number and vocabulary). Note that
the number of tuples and vocabulary used to trans-
late the test set is significantly greater after the re-
ordering translation.
4.4 Results
Here, we introduce the experiments that were car-
ried out in order to evaluate the influence of the
SMR approach in both tasks EsEn and ZhEn. The
log-linear translation model was optimized with
the simplex algorithm by maximizing over the
BLEU score. The evaluation was carried out us-
ing references and translation in lowercase and, in
the ZhEn task, without punctuation marks.
We studied the influence of the proposed SMR
approach on the n-gram-based SMT system de-
scribed using a monotonous search (NBm or
monotonous baseline configuration) in the two
tasks and a non-monotonous search (NBnm or
non-monotonous baseline configuration) in the
ZhEn task. In allowing for reordering in the SMT
decoder, the distortion limit (m) and reordering
limit (j) (see Section 2) were empirically set to
5 and 3, as they showed a good trade-off between
quality and efficiency. Both systems include the
four features explained in Section 2: the language
model, the word bonus, and the source-to-target
and target-to-source lexicon models.
Tables 5 and 6 show the results in the test set.
The former corresponds to the influence of the
SMR system on the EsEn task (NBm), whereas
the latter corresponds to the influence of the SMR
system on the ZhEn task (NBm and NBnm).
4.5 Discussion
Both BLEU and NIST coherently increase after
the inclusion of the SMR step when 100 classes
are used. The improvement in translation quality
can be explained as follows:
? SMR takes advantage of the use of classes
and correctly captures word reorderings that
are missed in the standard SMT system. In
addition, the use of classes allows new re-
orderings to be inferred.
? The new task S?2T becomes more
monotonous. Therefore, the translation
units tend to be shorter and SMT systems
perform better.
The gain obtained in the SMR+NBnm case indi-
cates that the reordering provided by SMR system
and the non-monotonous search are complemen-
tary. It means that the output of the SMR could
still be further monotonized. Note that the ZhEn
task has complex word reorderings.
These preliminary results also show that SMR
itself provides further improvements to those pro-
vided by the non-monotonous search.
5 Conclusions and Further Research
In this paper we have mainly dealt with the re-
ordering problem for an n-gram-based SMT sys-
tem. However, our approach could be used sim-
ilarly for a phrase-based system. We have ad-
dressed the reordering problem as a translation
from the source sentence to a monotonized source
sentence. The proposed SMR system is applied
before a standard SMT system. The SMR and
SMT systems are based on the same principles and
share the same type of decoder.
In extracting bilingual units, the change of order
performed in the source sentence has allowed the
modeling of the translation units to be improved
(shorter units mean a reduction in data sparse-
ness). Also, note that the SMR approach allows
the coherence between the change of order in the
training and test source corpora to be maintained.
75
System Classes BLEU NIST WER PER
NBm - 27.69 7.31 61.6 45.34
SMR + NBm - 28.60 7.53 59.89 43.53
SMR + NBm 100 30.89 7.75 55.77 42.85
Table 5: Results in the test set of the EsEn task using a monotonous search.
System Classes BLEU NIST WER PER
NBm - 42.42 8.3 42.87 33.44
NBnm - 43.58 8.9 43.89 34.05
SMR + NBm 100 43.75 8.49 42.45 33.85
SMR + NBnm 100 45.97 9.0 40.92 32.32
Table 6: Results in the test set of the ZhEn task using a monotonous and a non-monotonous search.
Performing reordering as a preprocessing step
and independently from the SMT system allows
for a more efficient final system implementation
and a quicker translation. Additionally, using
word classes helps to infer unseen reorderings.
These preliminary results show consistent and sig-
nificant improvements in translation quality.
As further research, we would like to add extra
features to the SMR system, and study new types
of classes for the reordering task.
6 Acknowledgments
This work has been partially funded by the Eu-
ropean Union under the integrated project TC-
STAR - Technology and Corpora for Speech
to Speech Translation - (IST-2002-FP6-506738,
http://www.tc-star.org) and the Spanish govern-
ment under a FPU grant.
References
E. Matusov A. Mauser and H. Ney. 2006. Train-
ing a statistical machine translation system without
giza++. 5th Int. Conf. on Language Resources and
Evaluation, LREC?06, May.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation. Computational Linguistics, 19(2):263?
311.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005a. Ngram-based versus phrase-
based statistical machine translation. Proc. of the
Int. Workshop on Spoken Language Translation,
IWSLT?05, October.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005b.
An Ngram-based statistical machine translation de-
coder. Proc. of the 9th Int. Conf. on Spoken Lan-
guage Processing, ICSLP?05.
A. de Gispert and J. Marin?o. 2002. Using X-grams for
speech-to-speech translation. Proc. of the 7th Int.
Conf. on Spoken Language Processing, ICSLP?02,
September.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H.
Ney. 2005. Novel reordering approaches in phrase-
based statistical machine translation. Proceedings
of the ACL Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. Proc. of the Human Lan-
guage Technology Conference, HLT-NAACL?2003,
May.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M. Ruiz. 2005.
Bilingual n-gram statistical machine translation. In
Proc. of the MT Summit X, pages 275?82, Pukhet
(Thailand), May.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 295?
302, July.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
76
Proceedings of the Workshop on Statistical Machine Translation, pages 142?145,
New York City, June 2006. c?2006 Association for Computational Linguistics
TALP Phrase-based statistical translation system for European language
pairs
Marta R. Costa-jussa`
Patrik Lambert
Jose? B. Marin?o
Josep M. Crego
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,agispert,lambert,khalilov,canton,adrian, rbanchs)@gps.tsc.upc.edu
Adria` de Gispert
Rafael E. Banchs
Abstract
This paper reports translation results for
the ?Exploiting Parallel Texts for Statis-
tical Machine Translation? (HLT-NAACL
Workshop on Parallel Texts 2006). We
have studied different techniques to im-
prove the standard Phrase-Based transla-
tion system. Mainly we introduce two re-
ordering approaches and add morphologi-
cal information.
1 Introduction
Nowadays most Statistical Machine Translation
(SMT) systems use phrases as translation units. In
addition, the decision rule is commonly modelled
through a log-linear maximum entropy framework
which is based on several feature functions (in-
cluding the translation model), hm. Each feature
function models the probability that a sentence e in
the target language is a translation of a given sen-
tence f in the source language. The weights, ?i,
of each feature function are typically optimized to
maximize a scoring function. It has the advantage
that additional features functions can be easily in-
tegrated in the overall system.
This paper describes a Phrase-Based system
whose baseline is similar to the system in Costa-
jussa` and Fonollosa (2005). Here we introduce
two reordering approaches and add morphological
information. Translation results for all six trans-
lation directions proposed in the shared task are
presented and discussed. More specifically, four
different languages are considered: English (en),
Spanish (es), French (fr) and German (de); and
both translation directions are considered for the
pairs: EnEs, EnFr, and EnDe. The paper is orga-
nized as follows: Section 2 describes the system;
0This work has been supported by the European Union
under grant FP6-506738 (TC-STAR project) and the TALP
Research Center (under a TALP-UPC-Recerca grant).
Section 3 presents the shared task results; and, fi-
nally, in Section 4, we conclude.
2 System Description
This section describes the system procedure fol-
lowed for the data provided.
2.1 Alignment
Given a bilingual corpus, we use GIZA++ (Och,
2003) as word alignment core algorithm. During
word alignment, we use 50 classes per language
estimated by ?mkcls?, a freely-available tool along
with GIZA++. Before aligning we work with low-
ercase text (which leads to an Alignment Error
Rate reduction) and we recover truecase after the
alignment is done.
In addition, the alignment (in specific pairs of
languages) was improved using two strategies:
Full verb forms The morphology of the verbs
usually differs in each language. Therefore, it is
interesting to classify the verbs in order to address
the rich variety of verbal forms. Each verb is re-
duced into its base form and reduced POS tag as
explained in (de Gispert, 2005). This transforma-
tion is only done for the alignment, and its goal
is to simplify the work of the word alignment im-
proving its quality.
Block reordering (br) The difference in word
order between two languages is one of the most
significant sources of error in SMT. Related works
either deal with reordering in general as (Kanthak
et al, 2005) or deal with local reordering as (Till-
mann and Ney, 2003). We report a local reorder-
ing technique, which is implemented as a pre-
processing stage, with two applications: (1) to im-
prove only alignment quality, and (2) to improve
alignment quality and to infer reordering in trans-
lation. Here, we present a short explanation of the
algorithm, for further details see Costa-jussa` and
Fonollosa (2006).
142
Figure 1: Example of an Alignment Block, i.e. a
pair of consecutive blocks whose target translation
is swapped
This reordering strategy is intended to infer the
most probable reordering for sequences of words,
which are referred to as blocks, in order to mono-
tonize current data alignments and generalize re-
ordering for unseen pairs of blocks.
Given a word alignment, we identify those pairs
of consecutive source blocks whose translation is
swapped, i.e. those blocks which, if swapped,
generate a correct monotone translation. Figure 1
shows an example of these pairs (hereinafter called
Alignment Blocks).
Then, the list of Alignment Blocks (LAB) is
processed in order to decide whether two consec-
utive blocks have to be reordered or not. By using
the classification algorithm, see the Appendix, we
divide the LAB in groups (Gn, n = 1 . . . N ). In-
side the same group, we allow new internal com-
bination in order to generalize the reordering to
unseen pairs of blocks (i.e. new Alignment Blocks
are created). Based on this information, the source
side of the bilingual corpora are reordered.
In case of applying the reordering technique for
purpose (1), we modify only the source training
corpora to realign and then we recover the origi-
nal order of the training corpora. In case of using
Block Reordering for purpose (2), we modify all
the source corpora (both training and test), and we
use the new training corpora to realign and build
the final translation system.
2.2 Phrase Extraction
Given a sentence pair and a corresponding word
alignment, phrases are extracted following the cri-
terion in Och and Ney (2004). A phrase (or
bilingual phrase) is any pair of m source words
and n target words that satisfies two basic con-
straints: words are consecutive along both sides
of the bilingual phrase, and no word on either side
of the phrase is aligned to a word out of the phrase.
We limit the maximum size of any given phrase to
7. The huge increase in computational and storage
cost of including longer phrases does not provide
a significant improvement in quality (Koehn et al,
2003) as the probability of reappearance of larger
phrases decreases.
2.3 Feature functions
Conditional and posterior probability (cp, pp)
Given the collected phrase pairs, we estimate the
phrase translation probability distribution by rela-
tive frequency in both directions.
The target language model (lm) consists of an
n-gram model, in which the probability of a trans-
lation hypothesis is approximated by the product
of word n-gram probabilities. As default language
model feature, we use a standard word-based 5-
gram language model generated with Kneser-Ney
smoothing and interpolation of higher and lower
order n-grams (Stolcke, 2002).
The POS target language model (tpos) con-
sists of an N-gram language model estimated over
the same target-side of the training corpus but us-
ing POS tags instead of raw words.
The forward and backwards lexicon mod-
els (ibm1, ibm1?1) provide lexicon translation
probabilities for each phrase based on the word
IBM model 1 probabilities. For computing the
forward lexicon model, IBM model 1 probabili-
ties from GIZA++ source-to-target algnments are
used. In the case of the backwards lexicon model,
target-to-source alignments are used instead.
The word bonus model (wb) introduces a sen-
tence length bonus in order to compensate the sys-
tem preference for short output sentences.
The phrase bonus model (pb) introduces a con-
stant bonus per produced phrase.
2.4 Decoding
The search engine for this translation system is de-
scribed in Crego et al (2005) which takes into ac-
count the features described above.
Using reordering in the decoder (rgraph) A
highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
143
extend the monotone search graph with additional
arcs. See the details in Crego et al (2006).
2.5 Optimization
It is based on a simplex method (Nelder and
Mead, 1965). This algorithm adjusts the log-
linear weights in order to maximize a non-linear
combination of translation BLEU and NIST: 10 ?
log10((BLEU ? 100) + 1) + NIST. The max-
imization is done over the provided development
set for each of the six translation directions under
consideration. We have experimented an improve-
ment in the coherence between all the automatic
figures by integrating two of these figures in the
optimization function.
3 Shared Task Results
3.1 Data
The data provided for this shared task corresponds
to a subset of the official transcriptions of the
European Parliament Plenary Sessions, and it
is available through the shared task website at:
http://www.statmt.org/wmt06/shared-task/.
The development set used to tune the system
consists of a subset (500 first sentences) of the
official development set made available for the
Shared Task.
We carried out a morphological analysis of the
data. The English POS-tagging has been carried
out using freely available TNT tagger (Brants,
2000). In the Spanish case, we have used the
Freeling (Carreras et al, 2004) analysis tool
which generates the POS-tagging for each input
word.
3.2 Systems configurations
The baseline system is the same for all tasks and
includes the following features functions: cp, pp,
lm, ibm1, ibm1?1, wb, pb. The POStag target
language model has been used in those tasks for
which the tagger was available. Table 1 shows the
reordering configuration used for each task.
The Block Reordering (application 2) has been
used when the source language belongs to the Ro-
manic family. The length of the block is lim-
ited to 1 (i.e. it allows the swapping of single
words). The main reason is that specific errors are
solved in the tasks from a Romanic language to
a Germanic language (as the common reorder of
Noun + Adjective that turns into Adjective +
Noun). Although the Block Reordering approach
Task Reordering Configuration
Es2En br2
En2Es br1 + rgraph
Fr2En br2
En2Fr br1 + rgraph
De2En -
En2De -
Table 1: Additional reordering models for each
task: br1 (br2) stands for Block Reordering ap-
plication 1 (application 2); and rgraph refers to
the reordering integrated in the decoder
does not depend on the task, we have not done
the corresponding experiments to observe its ef-
ficiency in all the pairs used in this evaluation.
The rgraph has been applied in those cases
where: we do not use br2 (there is no sense in
applying them simultaneously); and we have the
tagger for the source language model available.
In the case of the pair GeEn, we have not exper-
imented any reordering, we left the application of
both reordering approaches as future work.
3.3 Discussion
Table 2 presents the BLEU scores evaluated on the
test set (using TRUECASE) for each configuration.
The official results were slightly better because a
lowercase evaluation was used, see (Koehn and
Monz, 2006).
For both, Es2En and Fr2En tasks, br helps
slightly. The improvement of the approach de-
pends on the quality of the alignment. The better
alignments allow to extract higher quality Align-
ment Blocks (Costa-jussa` and Fonollosa, 2006).
The En2Es task is improved when adding both
br1 and rgraph. Similarly, the En2Fr task seems to
perform fairly well when using the rgraph. In this
case, the improvement of the approach depends on
the quality of the alignment patterns (Crego et al,
2006). However, it has the advantage of delay-
ing the final decision of reordering to the overall
search, where all models are used to take a fully
informed decision.
Finally, the tpos does not help much when trans-
lating to English. It is not surprising because it was
used in order to improve the gender and number
agreement, and in English there is no need. How-
ever, in the direction to Spanish, the tpos added
to the corresponding reordering helps more as the
Spanish language has gender and number agree-
ment.
144
Task Baseline +tpos +rc +tpos+rc
Es2En 29.08 29.08 29.89 29.98
En2Es 27.73 27.66 28.79 28.99
Fr2En 27.05 27.06 27.43 27.23
En2Fr 26.16 - 27.80 -
De2En 21.59 21.33 - -
En2De 15.20 - - -
Table 2: Results evaluated using TRUECASE on
the test set for each conguration: rc stands for
Reordering Conguration and refers to Table 1.
The bold results were the congurations submit-
ted.
4 Conclusions
Reordering is important when using a Phrase-
Based system. Although local reordering is sup-
posed to be included in the phrase structure, per-
forming local reordering improves the translation
quality. In fact, local reordering, provided by the
reordering approaches, allows for those general-
izations which phrases could not achieve. Re-
ordering in the DeEn task is left as further work.
References
T. Brants. 2000. Tnt - a statistical part-of-speech tag-
ger. Proceedings of the Sixth Applied Natural Lan-
guage Processing.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyz-
ers. 4th Int. Conf. on Language Resources and Eval-
uation, LREC?04.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2005. Im-
proving the phrase-based statistical translation by
modifying phrase extraction and including new fea-
tures. Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2006. Using
reordering in statistical machine translation based on
alignment block classification. Internal Report.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005.
An Ngram-based statistical machine translation de-
coder. Proc. of the 9th Int. Conf. on Spoken Lan-
guage Processing, ICSLP?05.
J. M. Crego, A. de Gispert, P. Lambert, M. R.
Costa-jussa`, M. Khalilov, J. Marin?o, J. A. Fonol-
losa, and R. Banchs. 2006. Ngram-based smt
system enhanced with reordering patterns. HLT-
NAACL06 Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, June.
A. de Gispert. 2005. Phrase linguistic classification for
improving statistical machine translation. ACL 2005
Students Workshop, June.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H.
Ney. 2005. Novel reordering approaches in phrase-
based statistical machine translation. Proceedings
of the ACL Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, June.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. Proc. of the Human Lan-
guage Technology Conference, HLT-NAACL?2003,
May.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
C. Tillmann and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133, March.
A Appendix
Here we describe the classification algorithm used
in Section 1.
1. Initialization: set n? 1 and LAB ? ? LAB.
2. Main part: while LAB ? is not empty do
? Gn = {(?k, ?k)} where (?k, ?k) is any
element of LAB ?, i.e. ?k is the first
block and ?k is the second block of the
Alignment Block k of the LAB ?.
? Recursively, move elements (?i, ?i)
from LAB? to Gn if there is an element
(?j , ?j) ? Gn such that ?i = ?j or
?i = ?j
? Increase n (i.e. n? n + 1)
3. Ending: For each Gn, construct the two sets
An and Bn which consists on the first and
second element of the pairs in Gn, respec-
tively.
145
Proceedings of the Workshop on Statistical Machine Translation, pages 162?165,
New York City, June 2006. c?2006 Association for Computational Linguistics
N-gram-based SMT System Enhanced with Reordering Patterns
Josep M. Crego
Marta R. Costa-jussa`
Jose? B. Marin?o
Adria` de Gispert
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
{jmcrego,agispert,lambert,mruiz,khalilov,rbanchs,canton,adrian}@gps.tsc.upc.edu
Patrik Lambert
Rafael E. Banchs
Abstract
This work presents translation results for
the three data sets made available in the
shared task ?Exploiting Parallel Texts for
Statistical Machine Translation? of the
HLT-NAACL 2006 Workshop on Statisti-
cal Machine Translation. All results pre-
sented were generated by using the N-
gram-based statistical machine translation
system which has been enhanced from the
last year?s evaluation with a tagged target
language model (using Part-Of-Speech
tags). For both Spanish-English transla-
tion directions and the English-to-French
translation task, the baseline system al-
lows for linguistically motivated source-
side reorderings.
1 Introduction
The statistical machine translation approach used
in this work implements a log-linear combination
of feature functions along with a translation model
which is based on bilingual n-grams (de Gispert and
Marin?o, 2002).
This translation model differs from the well
known phrase-based translation approach (Koehn
et al, 2003) in two basic issues: first, training data
is monotonously segmented into bilingual units; and
second, the model considers n-gram probabilities in-
stead of relative frequencies. This translation ap-
proach is described in detail in (Marin?o et al, 2005).
For those translation tasks with Spanish or En-
glish as target language, an additional tagged (us-
ing POS information) target language model is used.
Additionally a reordering strategy that includes POS
information is described and evaluated.
Translation results for all six translation directions
proposed in the shared task are presented and dis-
cussed. Both translation directions are considered
for the pairs: English-Spanish, English-French,
and English-German.
The paper is structured as follows: Section 2
briefly outlines the baseline system. Section 3 de-
scribes in detail the implemented POS-based re-
ordering strategy. Section 4 presents and discusses
the shared task results and, finally, section 5 presents
some conclusions and further work.
2 Baseline N-gram-based SMT System
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units, referred
to as tuples, which approximates the joint probabil-
ity between source and target languages by using
bilingual n-grams (de Gispert and Marin?o, 2002).
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, no smaller tuples can be extracted without vi-
olating the previous constraint. See (Crego et al,
2004) for further details.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tu-
ples. In addition to this bilingual n-gram translation
model, the baseline system implements a log linear
combination of five feature functions.
162
These five additional models are:
? A target language model. 5-gram of the target
side of the bilingual corpus.
? A word bonus. Based on the number of tar-
get words in the partial-translation hypothesis,
to compensate the LM preference for short sen-
tences.
? A Source-to-target lexicon model. Based on
IBM Model 1 lexical parameters(Brown et al,
1993), providing a complementary probability
for each tuple in the translation table. These
parameters are obtained from source-to-target
alignments.
? A Target-to-source lexicon model. Analo-
gous to the previous feature, but obtained from
target-to-source alignments.
? A Tagged (POS) target language model. This
feature implements a 5-gram language model
of target POS-tags. In this case, each trans-
lation unit carried the information of its target
side POS-tags, though this is not used for trans-
lation model estimation (only in order to eval-
uate the target POS language model at decod-
ing time). Due to the non-availability of POS-
taggers for French and German, it was not pos-
sible to incorporate this feature in all transla-
tion tasks considered, being only used for those
translation tasks with Spanish and English as
target languages.
The search engine for this translation system is
described in (Crego et al, 2005) and implements
a beam-search strategy based on dynamic program-
ming, taking into account all feature functions de-
scribed above, along with the bilingual n-gram trans-
lation model. Monotone search is performed, in-
cluding histogram and threshold pruning and hy-
pothesis recombination.
An optimization tool, which is based on a down-
hill simplex method was developed and used for
computing log-linear weights for each of the feature
functions. This algorithm adjusts the weights so that
a non-linear combination of BLEU and NIST scores
is maximized over the development set for each of
the six translation directions considered.
This baseline system is actually very similar to
the system used for last year?s shared task ?Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion? of ACL?05 Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Translation
and Beyond (Banchs et al, 2005), whose results
are available at: http://www.statmt.org/wpt05/
mt-shared-task/. A more detailed description of
the system can be found in (2005).
The tools used for POS-tagging were Freel-
ing (Carreras et al, 2004) for Spanish and
TnT (Brants, 2000) for English. All language mod-
els were estimated using the SRI language mod-
eling toolkit. Word-to-word alignments were ex-
tracted with GIZA++. Improvements in word-to-
word alignments were achieved through verb group
classification as described in (de Gispert, 2005).
3 Reordering Framework
In this section we outline the reordering framework
used for the experiments (Crego and Marin?o, 2006).
A highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
extend the monotone search graph with additional
arcs.
To extract patterns, we use the word-to-word
alignments (the union of both alignment directions)
and source-side POS tags. The main procedure con-
sists of identifying all crossings produced in the
Figure 1: Reordering patterns are extracted using
word-to-word alignments. The generalization power
is achieved through the POS tags. Three instances of
different patterns are extracted using the sentences
in the example.
163
word-to-word alignments. Once a crossing has been
detected, its source POS tags and alignments are
used to account for a new instance of pattern. The
target side of a pattern (source-side positions after
reordering), is computed using the original order
of the target words to which the source words are
aligned. See figure 1 for a clarifying example of
pattern extraction.
The monotone search graph is extended with re-
orderings following the patterns found in training.
The procedure identifies first the sequences of words
in the input sentence that match any available pat-
tern. Then, each of the matchings implies the ad-
dition of an arc into the search graph (encoding the
reordering learnt in the pattern). However, this ad-
dition of a new arc is not performed if a translation
unit with the same source-side words already exists
in the training. Figure 2 shows an example of the
procedure.
Figure 2: Three additional arcs have been added
to the original monotone graph (bold arcs) given
the reordering patterns found matching any of the
source POS tags sequence.
Once the search graph is built, the decoder tra-
verses the graph looking for the best translation.
Hence, the winner hypothesis is computed using
all the available information (the whole SMT mod-
els). The reordering strategy is additionally sup-
ported by a 5-gram language model of reordered
source POS-tags. In training, POS-tags are re-
ordered according with the extracted reordering pat-
terns and word-to-word links. The resulting se-
quence of source POS-tags are used to train the n-
gram LM.
Notice that this reordering framework has only
been used for some translation tasks (Spanish-
to-English, English-to-Spanish and English-to-
French). The reason is double: first, because we
did not have available a French POS-tagger. Second,
because the technique used to learn reorderings (de-
tailed below) does not seem to apply for language
pairs like German-English, because the agglutina-
tive characteristic of German (words are formed by
joining morphemes together).
Table 1: BLEU, NIST and mWER scores (com-
puted using two reference translations) obtained for
both translation directions (Spanish-to-English and
English-to-Spanish).
Conf BLEU NIST mWER
Spanish-to-English
base 55.23 10.69 34.40
+rgraph 55.59 10.70 34.23
+pos 56.39 10.75 33.75
English-to-Spanish
base 48.03 9.84 41.18
+rgraph 48.53 9.81 41.15
+pos 48.91 9.91 40.29
Table 1 shows the improvement of the original
baseline system described in section 2 (base), en-
hanced using reordering graphs (+rgraph) and pro-
vided the tagged-source language model (+pos).
The experiments in table 1 were not carried out over
the official corpus of this shared task. The Spanish-
English corpus of the TC-Star 2005 Evaluation was
used. Due to the high similarities between both cor-
pus (this shared task corpus consists of a subset of
the whole corpus used in the TC-Star 2005 Evalua-
tion), it makes sense to think that comparable results
would be obtained.
It is worth mentioning that the official corpus of
the shared task (HLT-NAACL 2006) was used when
building and tuning the present shared task system.
4 Shared Task Results
The data provided for this shared task corresponds
to a subset of the official transcriptions of the Euro-
pean Parliament Plenary Sessions. The development
set used to tune the system consists of a subset (500
first sentences) of the official development set made
available for the Shared Task.
164
Table 2 presents the BLEU, NIST and mWER
scores obtained for the development-test data set.
The last column shows whether the target POS lan-
guage model feature was used or not. Computed
scores are case sensitive and compare to one refer-
ence translation. Tasks in bold were conducted al-
lowing for the reordering framework. For French-
to-English task, block reordering strategy was used,
which is described in (Costa-jussa` et al, 2006). As it
can be seen, for the English-to-German task we did
not use any of the previous enhancements.
Table 2: Translation results
Task BLEU NIST mWER tPOS
en ? es 29.50 7.32 58.95 yes
es ? en 30.29 7.51 57.72 yes
en ? fr 30.23 7.40 59.76 no
fr ? en 30.21 7.61 56.97 yes
en ? de 17.40 5.61 71.18 no
de ? en 23.78 6.70 65.83 yes
Important differences can be observed between
the German-English and the rest of translation tasks.
They result from the greater differences in word
order present in this language pair (the German-
English results are obtained under monotone decod-
ing conditions). Also because the greater vocabulary
of words of German, which increases sparseness in
any task where German is envolved. As expected,
differences in translation accuracy between Spanish-
English and French-English are smaller.
5 Conclusions and Further Work
As it can be concluded from the presented results,
although in principle some language pairs (Spanish-
English-French) seem to have very little need for re-
orderings (due to their similar word order), the use
of linguistically-based reorderings proves to be use-
ful to improve translation accuracy.
Additional work is to be conducted to allow for
reorderings when translating from/to German.
6 Acknowledgments
This work was partly funded by the European Union
under the integrated project TC-STAR1: Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738) and the European Social Fund.
1http://www.tc-star.org
References
R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, and
J. B. Marin?o. 2005. Statistical machine translation of
euparl data by using bilingual n-grams. Proc. of the
ACL Workshop on Building and Using Parallel Texts
(ACL?05/Wkshp), pages 67?72, June.
T. Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proc. of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?311.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyzers.
4th Int. Conf. on Language Resources and Evaluation,
LREC?04, May.
M.R. Costa-jussa`, J.M. Crego, A. de Gispert, P. Lam-
bert, M. Khalilov, R. Banchs, J.B. Marin?o, and J.A.R.
Fonollosa. 2006. Talp phrase-based statistical transla-
tion system for european language pairs. Proc. of the
HLT/NAACL Workshop on Statistical Machine Trans-
lation, June.
J. M. Crego and J. Marin?o. 2006. A reordering frame-
work for statistical machine translation. Internal Re-
port.
J. M. Crego, J. Marin?o, and A. de Gispert. 2004. Finite-
state-based and phrase-based statistical machine trans-
lation. Proc. of the 8th Int. Conf. on Spoken Language
Processing, ICSLP?04, pages 37?40, October.
J. M. Crego, J. Marin?o, and A. Gispert. 2005. An ngram-
based statistical machine translation decoder. Proc. of
the 9th European Conference on Speech Communica-
tion and Technology, Interspeech?05, September.
A. de Gispert and J. Marin?o. 2002. Using X-grams
for speech-to-speech translation. Proc. of the 7th
Int. Conf. on Spoken Language Processing, ICSLP?02,
September.
A. de Gispert. 2005. Phrase linguistic classification and
generalization for improving statistical machine trans-
lation. Proc. of the ACL Student Research Workshop
(ACL?05/SRW), June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. Proc. of the Human
Language Technology Conference, HLT-NAACL?2003,
May.
J.B. Marin?o, R Banchs, J.M. Crego, A. de Gispert,
P. Lambert, M. R. Costa-jussa`, and J.A.R. Fonollosa.
2005. Bilingual n?gram statistical machine transla-
tion. Proc. of the MT Summit X, September.
165
Proceedings of the Second Workshop on Statistical Machine Translation, pages 167?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Ngram-based statistical machine translation enhanced with multiple
weighted reordering hypotheses
Marta R. Costa-jussa`, Josep M. Crego, Patrik Lambert, Maxim Khalilov
Jose? A. R. Fonollosa, Jose? B. Marin?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,lambert,khalilov,adrian,canton,rbanchs)@gps.tsc.upc.edu
Abstract
This paper describes the 2007 Ngram-based sta-
tistical machine translation system developed at
the TALP Research Center of the UPC (Uni-
versitat Polite`cnica de Catalunya) in Barcelona.
Emphasis is put on improvements and extensions
of the previous years system, being highlighted
and empirically compared. Mainly, these include
a novel word ordering strategy based on: (1) sta-
tistically monotonizing the training source cor-
pus and (2) a novel reordering approach based
on weighted reordering graphs. In addition, this
system introduces a target language model based
on statistical classes, a feature for out-of-domain
units and an improved optimization procedure.
The paper provides details of this system par-
ticipation in the ACL 2007 SECOND WORK-
SHOP ON STATISTICAL MACHINE TRANSLA-
TION. Results on three pairs of languages are
reported, namely from Spanish, French and Ger-
man into English (and the other way round) for
both the in-domain and out-of-domain tasks.
1 Introduction
Based on estimating a joint-probability model between
the source and the target languages, Ngram-based SMT
has proved to be a very competitive alternatively to
phrase-based and other state-of-the-art systems in previ-
ous evaluation campaigns, as shown in (Koehn and Monz,
2005; Koehn and Monz, 2006).
Given the challenge of domain adaptation, efforts have
been focused on improving strategies for Ngram-based
SMT which could generalize better. Specifically, a novel
reordering strategy is explored. It is based on extending
the search by using precomputed statistical information.
Results are promising while keeping computational ex-
penses at a similar level as monotonic search. Addition-
ally, a bonus for tuples from the out-of-domain corpus is
introduced, as well as a target language model based on
statistical classes. One of the advantages of working with
statistical classes is that they can easily be used for any
pair of languages.
This paper is organized as follows. Section 2 briefly
reviews last year?s system, including tuple definition and
extraction, translation model and feature functions, de-
coding tool and optimization criterion. Section 3 delves
into the word ordering problem, by contrasting last year
strategy with the novel weighted reordering input graph.
Section 4 focuses on new features: both tuple-domain
bonus and target language model based on classes. Later
on, Section 5 reports on all experiments carried out for
WMT 2007. Finally, Section 6 sums up the main conclu-
sions from the paper and discusses future research lines.
2 Baseline N-gram-based SMT System
The translation model is based on bilingual n-grams. It
actually constitutes a language model of bilingual units,
referred to as tuples, which approximates the joint proba-
bility between source and target languages by using bilin-
gual n-grams.
Tuples are extracted from a word-to-word aligned cor-
pus according to the following two constraints: first, tu-
ple extraction should produce a monotonic segmentation
of bilingual sentence pairs; and second, no smaller tuples
can be extracted without violating the previous constraint.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tuples.
In addition to this bilingual n-gram translation model, the
baseline system implements a log linear combination of
four feature functions. These four additional models are:
a target language model (a 5-gram model of words);
a word bonus; a source-to-target lexicon model and a
target-to-source lexicon model, both features provide a
complementary probability for each tuple in the transla-
tion table.
The decoder (called MARIE) for this translation sys-
167
tem is based on a beam search 1.
This baseline system is actually the same system used
for the first shared task ?Exploiting Parallel Texts for Sta-
tistical Machine Translation? of the ACL 2005 Work-
shop on Building and Using Parallel Texts: Data-Driven
Machine Translation and Beyond. A more detailed de-
scription of the system can be found in (Marin?o et al,
2006).
3 Baseline System Enhanced with a
Weighted Reordering Input Graph
This section briefly describes the statistical machine re-
ordering (SMR) technique. Further details on the archi-
tecture of SMR system can be found on (Costa-jussa` and
Fonollosa, 2006).
3.1 Concept
The SMR system can be seen as a SMT system which
translates from an original source language (S) to a re-
ordered source language (S?), given a target language
(T). The SMR technique works with statistical word
classes (Och, 1999) instead of words themselves (partic-
ularly, we have used 200 classes in all experiments).
Figure 1: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
3.2 Using SMR technique to improve SMT training
The original source corpus S is translated into the re-
ordered source corpus S? with the SMR system. Fig-
ure 1 (A) shows the corresponding block diagram. The
reordered training source corpus and the original training
target corpus are used to build the SMT system.
The main difference here is that the training is com-
puted with the S?2T task instead of the S2T original task.
Figure 2 (A) shows an example of the alignment com-
puted on the original training corpus. Figure 2 (B) shows
the same links but with the source training corpus in a
different order (this training corpus comes from the SMR
output). Although, the quality in alignment is the same,
the tuples that can be extracted change (notice that the
tuple extraction is monotonic). We are able to extract
1http://gps-tsc.upc.es/veu/soft/soft/marie/
smaller tuples which reduces the translation vocabulary
sparseness. These new tuples are used to build the SMT
system.
Figure 2: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
3.3 Using SMR technique to generate multiple
weighted reordering hypotheses
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 1 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear framework. Figure 3 shows the
weighted reordering graph.
The main difference with the reordering technique for
WMT06 (Crego et al, 2006) lies in (1) the tuples are ex-
tracted from the word alignment between the reordered
source training corpus and the given target training cor-
pus and (2) the graph structure: the SMR graph provides
weights for each reordering path.
4 Other features and functionalities
In addition to the novel reordering strategy, we consider
two new features functions.
4.1 Target Language Model based on Statistical
Classes
This feature implements a 5-gram language model of tar-
get statistical classes (Och, 1999). This model is trained
by considering statistical classes, instead of words, for
168
Figure 3: Weighted reordering input graph for SMT sys-
tem.
the target side of the training corpus. Accordingly, the tu-
ple translation unit is redefined in terms of a triplet which
includes: a source string containing the source side of
the tuple, a target string containing the target side of the
tuple, and a class string containing the statistical classes
corresponding to the words in the target strings.
4.2 Bonus for out-of-domain tuples
This feature adds a bonus to those tuples which comes
from the training of the out-of-domain task. This feature
is added when optimizing with the development of the
out-of-domain task.
4.3 Optimization
Finally, a n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/. This procedure
allows for a faster and more efficient adjustment of model
weights by means of a double-loop optimization, which
provides significant reduction of the number of transla-
tions that should be carried out. The current optimization
procedure uses the Simplex algorithm.
5 Shared Task Framework
5.1 Data
The data provided for this shared task corresponds to a
subset of the official transcriptions of the European Par-
liament Plenary Sessions 2. Additionally, there was avail-
able a smaller corpus called News-Commentary. For all
tasks and domains, our training corpus was the catenation
of both.
2http://www.statmt.org/wmt07/shared-task/
5.2 Processing details
Word Alignment. The word alignment is automati-
cally computed by using GIZA++ 3 in both directions,
which are symmetrized by using the union operation. In-
stead of aligning words themselves, stems are used for
aligning. Afterwards case sensitive words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al are splited into de el or a
el. As a post-processing, in the En2Es direction we used
a POS target language model as a feature (instead of the
target language model based on classes) that allowed to
recover the segmentations (de Gispert, 2006).
Language Model Interpolation. In other to better
adapt the system to the out-of-domain condition, the
target language model feature was built by combining
two 5-gram target language models (using SRILM 4).
One was trained from the EuroParl training data set, and
the other from the available, but much smaller, news-
commentary data set. The combination weights for the
EuroParl and news-commentary language models were
empirically adjusted by following a minimum perplexity
criterion. A relative perplexity reduction around 10-15%
respect to original EuroParl language model was achieved
in all the tasks.
5.3 Experiments and Results
The main difference between this year?s and last year?s
systems are: the amount of data provided; the word align-
ment; the Spanish morphology reduction; the reordering
technique; the extra target language model based on sta-
tistical classes (except for the En2Es); and the bonus for
the out-of-domain task (only for the En2Es task).
Among them, the most important is the reordering
technique. That is why we provide a fair comparison be-
tween the reordering patterns (Crego and Marin?o, 2006)
technique and the SMR reordering technique. Table 1
shows the system described above using either reorder-
ing patterns or the SMR technique. The BLEU calcula-
tion was case insensitive and sensitive to tokenization.
Table 2 presents the BLEU score obtained for the 2006
test data set comparing last year?s and this year?s systems.
The computed BLEU scores are case insensitive, sensi-
tive to tokenization and uses one translation reference.
The improvement in BLEU results shown from UPC-jm
3http://www.fjoch.com/GIZA++.html
4http://www.speech.sri.com/projects/srilm/
169
Task Reordering patterns SMR technique
es2en 31.21 33.34
en2es 31.67 32.33
Table 1: BLEU comparison: reordering patterns vs. SMR
technique.
Task UPC-jm 2006 UPC 2007
in-d out-d in-d out-d
es2en 31.01 27.92 33.34 32.85
en2es 30.44 25.59 32.33 33.07
fr2en 30.42 21.79 32.44 26.93
en2fr 31.75 23.30 32.30 27.03
de2en 24.43 17.57 26.54 21.63
en2de 17.73 10.96 19.74 15.06
Table 2: BLEU scores for each of the six translation di-
rections considered (computed over 2006 test set) com-
paring last year?s and this year?s system results (in-
domain and out-domain).
2006 Table 2 and reordering patterns Table 1 in the En-
glish/Spanish in-domain task comes from the combina-
tion of: the additional corpora, the word alignment, the
Spanish morphology reduction and the extra target lan-
guage model based on classes (only in the Es2En direc-
tion).
6 Conclusions and Further Work
This paper describes the UPC system for the WMT07
Evaluation. In the framework of Ngram-based system, a
novel reordering strategy which can be used for any pair
of languages has been presented and it has been showed
to significantly improve translation performance. Ad-
ditionally two features has been added to the log-lineal
scheme: the target language model based on classes and
the bonus for out-of-domain translation units.
7 Acknowledgments
This work has been funded by the European Union un-
der the TC-STAR project (IST-2002-FP6-506738) and
the Spanish Government under grant TEC2006-13964-
C03 (AVIVAVOZ project).
References
M.R. Costa-jussa` and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In EMNLP, pages 71?77, Sydney,
July. ACL.
J.M. Crego and J.B. Marin?o. 2006. Reordering experi-
ments for n-gram-based smt. In SLT, pages 242?245,
Aruba.
Josep M. Crego, Adria` de Gispert, Patrik Lambert,
Marta R. Costa-jussa`, Maxim Khalilov, Rafael Banchs,
Jose? B. Marin?o, and Jose? A. R. Fonollosa. 2006. N-
gram-based smt system enhanced with reordering pat-
terns. In WMT, pages 162?165, New York City, June.
ACL.
Adria` de Gispert. 2006. Introducing Linguistic Knowl-
edge in Statistical Machine Translation. Ph.D. thesis,
Universitat Polite`cnica de Catalunya, December.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between european lan-
guages. In WMT, pages 119?124, Michigan, June.
ACL.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In WMT, pages 102?121, New
York City, June. ACL.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527?549, December.
F.J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In EACL, pages 71?76,
Bergen, Norway, June.
170
Proceedings of the Second Workshop on Statistical Machine Translation, pages 171?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
Analysis of statistical and morphological classes to generate weighted
reordering hypotheses on a Statistical Machine Translation system
Marta R. Costa-jussa` and Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,adrian)@gps.tsc.upc.edu
Abstract
One main challenge of statistical machine trans-
lation (SMT) is dealing with word order. The
main idea of the statistical machine reordering
(SMR) approach is to use the powerful tech-
niques of SMT systems to generate a weighted
reordering graph for SMT systems. This tech-
nique supplies reordering constraints to an SMT
system, using statistical criteria.
In this paper, we experiment with different graph
pruning which guarantees the translation quality
improvement due to reordering at a very low in-
crease of computational cost.
The SMR approach is capable of generalizing re-
orderings, which have been learned during train-
ing, by using word classes instead of words
themselves. We experiment with statistical and
morphological classes in order to choose those
which capture the most probable reorderings.
Satisfactory results are reported in the WMT07
Es/En task. Our system outperforms in terms of
BLEU the WMT07 Official baseline system.
1 Introduction
Nowadays, statistical machine translation is mainly based
on phrases (Koehn et al, 2003). In parallel to this phrase-
based approach, the use of bilingual n-grams gives com-
parable results, as shown by Crego et al (2005). Two
basic issues differentiate the n-gram-based system from
the phrase-based: training data is monotonically seg-
mented into bilingual units; and, the model considers n-
gram probabilities rather than relative frequencies. The
n-gram-based system follows a maximum entropy ap-
proach, in which a log-linear combination of multiple
models is implemented (Marin?o et al, 2006), as an al-
ternative to the source-channel approach.
Introducing reordering capabilities is important in both
systems. Recently, new reordering strategies have been
proposed such as the reordering of each source sentence
to match the word order in the corresponding target sen-
tence, see Kanthak et al (2005) and Marin?o et al (2006).
These approaches are applied in the training set and they
lack of reordering generalization.
Applied both in the training and decoding step, Collins
et al (2005) describe a method for introducing syntac-
tic information for reordering in SMT. This approach is
applied as a pre-processing step.
Differently, Crego et al (2006) presents a reordering
approach based on reordering patterns which is coupled
with decoding. The reordering patterns are learned di-
rectly from word alignment and all reorderings have the
same probability.
In our previous work (Costa-jussa` and Fonollosa,
2006) we presented the SMR approach which is based
on using the powerful SMT techniques to generate a re-
ordered source input for an SMT system both in train-
ing and decoding steps. One step further, (Costa-jussa`
et al, 2007) shows how the SMR system can generate a
weighted reordering graph, allowing the SMT system to
make the final reordering decision.
In this paper, the SMR approach is used to train the
SMT system and to generate a weighted reordering graph
for the decoding step. The SMR system uses word classes
instead of words themselves and we analyze both statisti-
cal and morphological classes. Moreover, we present ex-
periments regarding the reordering graph efficiency: we
analyze different graph pruning and we show the very low
increase in computational cost (compared to a monotonic
translation). Finally, we compare the performance our
system in terms of BLEU with the WMT07 baseline sys-
tem.
This paper is organized as follows. The first two sec-
tions explain the SMT and the SMR baseline systems,
respectively. Section 4 reports the study of statistical and
171
morphological classes. Section 5 describes the experi-
mental framework and discusses the results. Finally, Sec-
tion 6 presents the conclusions and some further work.
2 Ngram-based SMT System
This section briefly describes the Ngram-based SMT (for
further details see (Marin?o et al, 2006)). The Ngram-
based SMT system uses a translation model based on
bilingual n-grams. It is actually a language model of
bilingual units, referred to as tuples, which approxi-
mates the joint probability between source and target lan-
guages by using bilingual n-grams. Tuples are extracted
from any word alignment according to the following con-
straints:
1. a monotonic segmentation of each bilingual sen-
tence pairs is produced,
2. no word inside the tuple is aligned to words outside
the tuple, and
3. no smaller tuples can be extracted without violating
the previous constraints.
As a result of these constraints, only one segmentation
is possible for a given sentence pair.
In addition to the bilingual n-gram translation model,
the baseline system implements a log-linear combination
of feature functions, which are described as follows:
? A target language model. This feature consists of
a 4-gram model of words, which is trained from the
target side of the bilingual corpus.
? A class target language model. This feature con-
sists of a 5-gram model of words classes, which is
trained from the target side of the bilingual corpus
using the statistical classes from (Och, 1999).
? A word bonus function. This feature introduces
a bonus based on the number of target words con-
tained in the partial-translation hypothesis. It is used
to compensate for the system?s preference for short
output sentences.
? A source-to-target lexicon model. This feature,
which is based on the lexical parameters of the IBM
Model 1 (Brown et al, 1993), provides a comple-
mentary probability for each tuple in the translation
table. These lexicon parameters are obtained from
the source-to-target algnments.
? A target-to-source lexicon model. Similarly to the
previous feature, this feature is based on the lexical
parameters of the IBM Model 1 but, in this case,
these parameters are obtained from target-to-source
alignments.
Figure 1: SMR block diagram.
3 SMR Baseline System
As mentioned in the introduction, SMR and SMT are
based on the same principles.
3.1 Concept
The aim of SMR consists in using an SMT system to deal
with reordering problems. Therefore, the SMR system
can be seen as an SMT system which translates from an
original source language (S) to a reordered source lan-
guage (S?), given a target language (T).
3.2 Description
Figure 1 shows the SMR block diagram and an exam-
ple of the input and output of each block inside the
SMR system. The input is the initial source sentence
(S) and the output is the reordered source sentence (S?).
There are three blocks inside SMR: (1) the class replac-
ing block; (2) the decoder, which requires an Ngram
model containing the reordering information; and, (3) the
post-processing block which either reorders the source
sentence given the indexes of the decoder output 1-best
(training step) or transforms the decoder output graph to
an input graph for the SMT system (decoding step).
The decoder in Figure 1 requires a translation model
which is an Ngram model. Given a training parallel cor-
pus this model has been built following the next steps:
1. Select source and target word classes.
2. Align parallel training sentences at the word level in
both translation directions. Compute the union of
the two alignments to obtain a symmetrized many-
to-many word alignment.
3. Use the IBM1 Model to obtain a many-to-one word
alignment from the many-to-many word alignment.
4. Extract translation units from the computed many-
to-one alignment. Replace source words by their
172
Figure 2: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
classes and target words by the index of the linked
source word. An example of a translation unit here
is: C61 C28 C63#2 0 1, where # divides source
(word classes) and target (positions).
5. Compute the sequence of the above units and learn
the language model
For further information about the SMR training proce-
dure see (Costa-jussa` and Fonollosa, 2006).
3.3 Improving SMT training
Figure 2 (A) shows the corresponding block diagram
for the training corpus: first, the given training corpus
S is translated into the reordered training source corpus
S? with the SMR system. Then, this reordered training
source corpus S? and the given training target corpus T
are used to build the SMT system
The main difference here is that the training is com-
puted with the S?2T task instead of the S2T given task.
Figure 3 (A) shows an example of the word alignment
computed on the given training parallel corpus S2T. Fig-
ure 3 (B) shows the same links but with the reordered
source training corpus S?. Although the quality in align-
ment is the same, the tuples that can be extracted change
(notice that tuple extraction is monotonic). We now are
able to extract smaller tuples which reduce the transla-
tion vocabulary sparseness. These new tuples are used to
build the SMT system.
3.4 Generation of multiple weighted reordering
hypotheses
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 2 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
Figure 3: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear model.
4 Morphological vs Statistical Classes
Previous SMR studies (Costa-jussa` and Fonollosa,
2006) (Costa-jussa` et al, 2007) considered only statisti-
cal classes. On the one hand, these statistical classes per-
formed fairly well and had the advantage of being suit-
able for any language. On the other hand, it should be
taken into account the fact of training them in the train-
ing set alows for unknown words in the development or
in the test set. Additionally, they do not have any reorder-
ing information because they are trained on a monolin-
gual set.
The first problem, unknown words which appear in
the development or in the test set, may be solved by us-
ing a disambiguation technique. Unknown words can be
assigned to one class by taking into account their own
context. The second problem, incorporating information
about order, might be solved by training classes in the
reordered training source corpus. In other words, we
monotonized the training corpus with the alignment in-
formation (i.e. reorder the source corpus in the way that
matches the target corpus under the alignment links cri-
terion). After that, we train the statistical classes, here-
inafter, called statistical reordered classes.
In some pair of languages, as for example En-
glish/Spanish, the reordering that may be performed is
related to word?s morphology (i.e. TAGS). Some TAGS
rules (with some lexical exceptions) can be extracted as
in (Popovic and Ney, 2006) where they were applied
with reordering purposes as a preprocessing step. An-
other approach that has related TAGS and reordering was
presented in (Crego and Marin?o, 2006) where instead of
rules, they learned reordering patterns based on TAGS as
named in this paper?s introduction. Hence, the SMR tech-
173
Spanish English
Train Sentences 1,3M
Words 37,9M 35,5M
Vocabulary 138,9k 133k
Dev Sentences 2 000 2 000
Words 60.5k 58.7k
Vocabulary 8.1k 6.5k
Test Sentences 2 000 2 000
Words 60,2k 58k
Vocabulary 8,2k 6,5k
Table 1: Corpus Statistics.
nique may take advantage of the morphological informa-
tion. Notice that an advantage is that there is a TAG for
each word, hence there are not unknown words.
5 Evaluation Framework
5.1 Corpus Statistics
Experiments were carried out using the data in the second
evaluation campaign of the WMT07 1.
This corpus consists in the official version of the
speeches held in the European Parliament Plenary Ses-
sions (EPPS), as available on the web page of the Eu-
ropean Parliament. Additionally, there was available a
smaller corpus (News-Commentary). Our training cor-
pus was the catenation of both. Table 1 shows the corpus
statistics.
5.2 Tools and preprocessing
The system was built similarly to (Costa-jussa` et al,
2007). The SMT baseline system uses the Ngram-
based approach, which has been explained in Section 2.
Tools used are defined as follows: word alignments were
computed using GIZA++ 2; language model was esti-
mated using SRILM 3; decoding was carried out with
MARIE4; an n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/ using the simplex
method (Nelder and Mead, 1965) and BLEU as a loss
function.
The SMT system we use a 4gram translation language
model, a 5gram target language model and a 5gram class
target language model.
Spanish data have been processed so that the pronouns
which are attached to verbs are split up. Additionally,
several article and prepositions words are separated (i.e.
1http://www.statmt.org/wmt07/
2http://www.fjoch.com/GIZA++.html
3http://www.speech.sri.com/projects/srilm/
4http://gps-tsc.upc.es/veu/soft/soft/marie/
Figure 5: Perplexity over the manually aligned test set
given the SMR Ngram length.
del goes into de el). This preprocessing was performed
using Freeling software (Atserias et al, 2006). Training
and evaluation were both true-case.
5.3 Classes and Ngram length Study for the
SMR-Graph generation
This section evaluates several types of classes and n-gram
lengths in the SMR model in order to choose the SMR
configuration which provides the best results in trans-
lation in terms of quality. To accomplish this evalua-
tion, we have designed the following experiment. Given
500 manually aligned parallel sentences of the EPPS cor-
pora (Lambert et al, 2006), we order the source test in
the way that better matches the target set. This ordered
source set is considered our reference as it is based on
manual alignments. On the other hand, the 500 sen-
tences set is translated using the SMR configurations to
be tested. Finally, the Word Error Rate (WER) is used as
quality measure.
Figure 4 shows the WER behavior given different types
of classes. As statistical classes (cl50,cl100,cl200) we
used the Och monolingual classes (Och, 1999), which
can be performed using ?mkcls? (a tool available with
GIZA). Also we used the statistical reordered classes
(cl100mono) which were explained in Section 4. Both
statistical and statistical reordered classes used the dis-
amb tool of SRILM in order to classify unknown words.
As morphological classes we used the TAGS provided by
Freeling. Clearly, statistical classes perform better than
TAGS and best results can be achieved with 100 and 200
classes and an n-gram length of 5.
For the sake of completeness, we have evaluated the
perplexity of the SMR Ngram model over the aligned test
set above and choosing 200 classes. Figure 5 is coherent
with the WER results above and it shows that perplexity
is not reduced for an n-gram length greater than 5.
174
Figure 4: WER over the reference given various sets of classes and Ngram lengths.
5.4 Graph pruning
The more complex is the reordering graph, the less effi-
cient is the decoding. That is why, in this section, we ex-
periment with several ways of graph pruning. Addition-
ally, for each pruning we see the influence of considering
the graph weights (i.e. reordering feature importance).
Given that the reordering graph is the output of a beam
search decoder, we can consider pruning the reordering
graph by limiting the SMR beam, i.e. limiting the size of
hypothesis stacks.
Given a reordering graph, another option is to prune
states and arches only used in paths s times worse than
the best path.
Table 2 gives the results of the proposed pruning. Note
that computational time is given in terms of the mono-
tonic translation time (and it is the same for both direc-
tions). It is shown that graph pruning guarantees the effi-
ciency of the system and even increases the translation?s
quality. Similar results are obtained in terms of BLEU for
both types of pruning. In this task and for both translation
directions, it seems more appropriate to limit directly the
beam search in the SMR step to 5.
As expected, the influence of the reordering feature,
which takes into account the graph weights, tends to be
more important as pruning decreases (i.e. when the graph
has more paths).
Pruning Wr BLEUEn2Es BLEUEs2En TIME
b5 yes 31.32 32.64 2.4Tm
b5 no 31.25 31.82 2.5Tm
b50 yes 30.95 32.28 5.3Tm
b50 no 30.90 27.44 4.8Tm
b50 s10 yes 31.19 32.20 1.5Tm
b50 s10 no 31.07 32.41 1.4Tm
Table 2: Performance in BLEU in the test set of different
graph pruning (b stands for beam and s for states); the
use of reordering feature function (Wr indicates its use);
and the time increase related to Tm (monotonic transla-
tion time).
5.5 Results and discussion
Table 3 shows the performance of our Ngram-
based system using the SMR technique. First
row is the WMT07 baseline system which can
be reproduced following the instructions in
http://www.statmt.org/wmt07/baseline.html. This
baseline system uses a non-monotonic search. Second
row shows the results of the Ngram-based system
presented in section 2 using the weighted reordering
graph trained with the best configuration found in the
above section (200 statistical classes and an Ngram of
length 5).
175
System BLEUes2en BLEUen2es
WMT07 Of. Baseline 31.21 30.74
Ngram-based 32.64 31.32
Table 3: BLEU Results.
6 Conclusions and further work
The proposed SMR technique can be used both in training
and test steps in a SMT system. Applying the SMR tech-
nique in the training step reduces the sparseness in the
translation vocabulary. Applying SMR technique in the
test step allows to generate a weighted reordering graph
for SMT system.
The use of classes plays an important role in the SMR
technique, and experiments have shown that statistical
classes are better than morphological ones.
Moreover, we have experimented with different graph
pruning showing that best translation results can be
achieved at a very low increase of computational cost
when comparing to the monotonic translation computa-
tional cost.
Finally, we have shown that our translation system us-
ing the SMR technique outperforms the WMT07 Official
baseline system (which uses a non-monotonic search) in
terms of BLEU.
As further work, we want to introduce the SMR tech-
nique in a state-of-the-art phrase-based system.
7 Acknowledgments
This work has been funded by the European Union under
the TC-STAR project (IST- 2002-FP6-506738) and the
Spanish Government under grant TEC2006-13964-C03
(AVIVAVOZ project).
References
J. Atserias, B. Casas, E. Comelles, M. Gonza?lez,
L. Padro?, and M. Padro?. 2006. Freeling 1.3: Syntactic
and semantic services in an open-source nlp library. In
5th Int. Conf. on Language Resource and Evaluation
(LREC), pages 184?187.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?311.
M. Collins, P. Koehn, and I. Kucerova?. 2005. Clause
restructuring for statistical machine translation. In 43st
Annual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531 ? 540, Michigan.
M.R. Costa-jussa` and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In Empirical Methods in Natural
Language Processing (EMNLP), pages 71?77, Sydney.
M. R. Costa-jussa`, P. Lambert, J.M. Crego, M. Khalilov,
J.A.R. Fonollosa, J.B. Marin?o, and R. Banchs. 2007.
Ngram-based statistical machine translation enhanced
with multiple weighted reordering hypotheses. In
ACL: Workshop of Statistical Machine Translation
(WMT07), Prague.
J.M. Crego and J.B. Marin?o. 2006. Reordering exper-
iments for n-gram-based smt. Ist IEEE/ACL Inter-
national Workshop on Spoken Language Technology
(SLT?06), pages 242?245.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005. Ngram-based versus phrase-
based statistical machine translation. In Proc. of
the Int. Workshop on Spoken Language Translation,
IWSLT?05, pages 177?184, Pittsburgh, October.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of the
ACL Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond, pages
167?174, Ann Arbor, MI, June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proc. of the Human
Language Technology Conference, HLT-NAACL?2003,
pages 48 ? 54, Edmonton, Canada, May.
P. Lambert, A. de Gispert, R. Banchs, and J. Marin?o.
2006. Guidelines for word alignment and man-
ual alignment. Language Resources and Evaluation,
39(4):267?285.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527?549.
J.A. Nelder and R. Mead. 1965. A simplex method for
function minimization. The Computer Journal, 7:308?
313.
F.J. Och. 1999. An efficient method for determining
bilingual word classes. In 9th Conf. of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 71?76, June.
M. Popovic and H. Ney. 2006. Pos-based word reorder-
ings for statistical machine translation. In 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), pages 1278?1283, Genova, May.
176
Proceedings of the Third Workshop on Statistical Machine Translation, pages 127?130,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The TALP-UPC Ngram-based statistical machine translation system for
ACL-WMT 2008
Maxim Khalilov, Adolfo Hern?ndez H., Marta R. Costa-juss?,
Josep M. Crego, Carlos A. Henr?quez Q., Patrik Lambert,
Jos? A. R. Fonollosa, Jos? B. Mari?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(khalilov, adolfohh, mruiz, jmcrego, carloshq, lambert, adrian, canton, rbanchs)@gps.tsc.upc.edu
Abstract
This paper reports on the participation of the TALP
Research Center of the UPC (Universitat Polit?cnica
de Catalunya) to the ACL WMT 2008 evaluation
campaign.
This year?s system is the evolution of the one we em-
ployed for the 2007 campaign. Main updates and
extensions involve linguistically motivated word re-
ordering based on the reordering patterns technique.
In addition, this system introduces a target language
model, based on linguistic classes (Part-of-Speech),
morphology reduction for an inflectional language
(Spanish) and an improved optimization procedure.
Results obtained over the development and test sets
on Spanish to English (and the other way round)
translations for both the traditional Europarl and
a challenging News stories tasks are analyzed and
commented.
1 Introduction
Over the past few years, the Statistical Machine Transla-
tion (SMT) group of the TALP-UPC has been develop-
ing the Ngram-based SMT system (Mari?o et al, 2006).
In previous evaluation campaigns the Ngram-based ap-
proach has proved to be comparable with the state-of-
the-art phrase-based systems, as shown in Koehn and
Monz(2006), Callison-Burch et al (2007).
We present a summary of the TALP-UPC Ngram-
based SMT system used for this shared task. We dis-
cuss the system configuration and novel features, namely
linguistically motivated reordering technique, which is
applied on the decoding step. Additionally, the reorder-
ing procedure is supported by an Ngram language model
(LM) of reordered source Part-of-Speech tags (POS).
In this year?s evaluation we submitted systems for
Spanish-English and English-Spanish language pairs for
the traditional (Europarl) and challenging (News) tasks.
In each case, we used only the supplied data for each lan-
guage pair for models training and optimization.
This paper is organized as follows. Section 2 briefly
outlines the 2008 system, including tuple definition and
extraction, translation model and additional feature mod-
els, decoding tool and optimization procedure. Section 3
describes the word reordering problem and presents the
proposed technique of reordering patterns learning and
application. Later on, Section 4 reports on the experi-
mental setups of the WMT 2008 evaluation campaign. In
Section 5 we sum up the main conclusions from the pa-
per.
2 Ngram-based SMT System
Our translation system implements a log-linear model in
which a foreign language sentence fJ1 = f1, f2, ..., fJ
is translated into another language eI1 = f1, f2, ..., eI by
searching for the translation hypothesis e?I1 maximizing a
log-linear combination of several feature models (Brown
et al, 1990):
e?I1 = argmax
eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system models
and the set of ?m refers to the weights corresponding to
these models.
The core part of the system constructed in that way
is a translation model, which is based on bilingual n-
grams. It actually constitutes an Ngram-based LM of
bilingual units (called tuples), which approximates the
joint probability between the languages under consider-
ation. The procedure of tuples extraction from a word-
to-word alignment according to certain constraints is ex-
plained in detail in Mari?o et al (2006).
The Ngram-based approach differs from the phrase-
based SMT mainly by distinct representating of the bilin-
gual units defined by word alignment and using a higher
127
order HMM of the translation process. While regular
phrase-based SMT considers context only for phrase re-
ordering but not for translation, the N-gram based ap-
proach conditions translation decisions on previous trans-
lation decisions.
The TALP-UPC 2008 translation system, besides the
bilingual translation model, which consists of a 4-gram
LM of tuples with Kneser-Ney discounting (estimated
with SRI Language Modeling Toolkit1), implements a
log-linear combination of five additional feature models:
? a target language model (a 4-gram model of words,
estimated with Kneser-Ney smoothing);
? a POS target language model (a 4-gram model of
tags with Good-Turing discounting (TPOS));
? a word bonus model, which is used to compensate
the system?s preference for short output sentences;
? a source-to-target lexicon model and a target-to-
source lexicon model, these models use word-to-
word IBM Model 1 probabilities (Och and Ney,
2004) to estimate the lexical weights for each tuple
in the translation table.
Decisions on the particular LM configuration and
smoothing technique were taken on the minimal-
perplexity and maximal-BLEU bases.
The decoder (called MARIE), an open source tool2,
implementing a beam search strategy with distortion ca-
pabilities was used in the translation system.
Given the development set and references, the log-
linear combination of weights was adjusted using a sim-
plex optimization method (with the optimization criteria
of the highest BLEU score ) and an n-best re-ranking
just as described in http://www.statmt.org/jhuws/. This
strategy allows for a faster and more efficient adjustment
of model weights by means of a double-loop optimiza-
tion, which provides significant reduction of the number
of translations that should be carried out.
3 Reordering framework
For a great number of translation tasks a certain reorder-
ing strategy is required. This is especially important
when the translation is performed between pairs of lan-
guages with non-monotonic word order. There are var-
ious types of distortion models, simplifying bilingual
translation. In our system we use an extended monotone
reordering model based on automatically learned reorder-
ing rules. A detailed description can be found in Crego
and Mari?o (2006).
1http://www.speech.sri.com/projects/srilm/
2http://gps-tsc.upc.es/veu/soft/soft/marie/
Apart from that, tuples were extracted by an unfold-
ing technique: this means that the tuples are broken into
smaller tuples, and these are sequenced in the order of the
target words.
3.1 Reordering patterns
Word movements are realized according to the reordering
rewrite rules, which have the form of:
t1, ..., tn 7? i1, ..., in
where t1, ..., tn is a sequence of POS tags (relating a
sequence of source words), and i1, ..., in indicates which
order of the source words generate monotonically the tar-
get words.
Patterns are extracted in training from the crossed links
found in the word alignment, in other words, found in
translation tuples (as no word within a tuple can be linked
to a word out of it (Crego and Mari?o, 2006)).
Having all the instances of rewrite patterns, a score for
each pattern on the basis of relative frequency is calcu-
lated as shown below:
p(t1, ..., tn 7? i1, ..., in) =
N(t1, ..., tn 7? i1, ..., in)
NN(t1, ..., tn)
3.2 Search graph extension and source POS model
The monotone search graph is extended with reorderings
following the patterns found in training. Once the search
graph is built, the decoder traverses the graph looking for
the best translation. Hence, the winning hypothesis is
computed using all the available information (the whole
SMT models).
Figure 1: Search graph extension. NC, CC and AQ stand re-
spectively for name, conjunction and adjective.
The procedure identifies first the sequences of words
in the input sentence that match any available pattern.
Then, each of the matchings implies the addition of an arc
into the search graph (encoding the reordering learned in
the pattern). However, this addition of a new arc is not
128
Task BL BL+SPOS
Europarl News Europarl News
es2en 32.79 36.09 32.88 36.36
en2es 32.05 33.91 32.10 33.63
Table 1: BLEU comparison demonstrating the impact of the
source-side POS tags model.
performed if a translation unit with the same source-side
words already exists in the training. Figure 1 shows how
two rewrite rules applied over an input sentence extend
the search graph given the reordering patterns that match
the source POS tag sequence.
The reordering strategy is additionally supported by
a 4-gram language model (estimated with Good-Turing
smoothing) of reordered source POS tags (SPOS). In
training, POS tags are reordered according with the ex-
tracted reordering patterns and word-to-word links. The
resulting sequence of source POS tags is used to train the
Ngram LM.
Table 1 presents the effect of the source POS LM in-
troduction to the reordering module of the Ngram-based
SMT. As it can be seen, the impactya le h of the source-
side POS LM is minimal, however we decided to consider
the model aiming at improving it in future. The reported
results are related to the Europarl and News Commen-
tary (News) development sets. BLEU calculation is case
insensitive and insensitive to tokenization. BL (baseline)
refers to the presented Ngram-based system considering
all the features, apart from the target and source POS
models.
4 WMT 2008 Evaluation Framework
4.1 Corpus
An extraction of the official transcriptions of the 3rd re-
lease of the European Parliament Plenary Sessions3 was
provided for the ACL WMT 2008 shared translation task.
About 40 times smaller corpus from news domain (called
News Commentary) was also available. For both tasks,
our training corpus was the catenation of the Europarl and
News Commentary corpora.
TALP UPC participated in the constraint to the
provided training data track for Spanish-English and
English-Spanish translation tasks. We used the same
training material for the traditional and challenging tasks,
while the development sets used to tune the system were
distinct (2000 sentences for Europarl task and 1057
for News Commentary, one reference translation for
each of them). A brief training and development corpora
statistics is presented in Table 2.
3http://www.statmt.org/wmt08/shared-task.html
Spanish English
Train
Sentences 1.3 M 1.3 M
Words 38.2 M 35.8 K
Vocabulary 156 K 120 K
Development Europarl
Sentences 2000 2000
Words 61.8 K 58.7 K
Vocabulary 8 K 6.5 K
Development News Commentary
Sentences 1057 1057
Words 29.8 K 25.8 K
Vocabulary 5.4 K 4.9 K
Table 2: Basic statistics of ACL WMT 2008 corpus.
4.2 Processing details
The training data was preprocessed by using provided
tools for tokenizing and filtering.
POS tagging. POS information for the source and the
target languages was considered for both translation tasks
that we have participated. The software tools available
for performing POS-tagging were Freeling (Carreras et
al., 2004) for Spanish and TnT (Brants, 2000) for En-
glish. The number of classes for English is 44, while
Spanish is considered as a more inflectional language,
and the tag set contains 376 different tags.
Word Alignment. The word alignment is automati-
cally computed by using GIZA++4(Och and Ney, 2000)
in both directions, which are symmetrized by using the
union operation. Instead of aligning words themselves,
stems are used for aligning. Afterwards case sensitive
words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al were splitted into de el or
a el. As a post-processing, in the En2Es direction we
used a POS target LM as a feature (instead of the target
language model based on classes) that allowed to recover
the segmentations (de Gispert, 2006).
4.3 Experiments and Results
In contrast to the last year?s system where statistical
classes were used to train the target-side tags LM, this
year we used linguistically motivated word classes
4http://code.google.com/p/giza-pp/
129
Task BL+SPOS BL+SPOS+TPOS
(UPC 2008)
Europarl News Europarl News
es2en 32.88 36.36 32.89 36.31
en2es 31.52 34.13 30.72 32.72
en2es "clean"5 32.10 33.63 32.09 35.04
Table 3: BLEU scores for Spanish-English and English-Spanish
2008 development corpora (Europarl and News Commentary).
Task UPC 2008
Europarl News
es2en 32.80 19.61
en2es 31.31 19.28
en2es "clean"5 32.34 20.05
Table 4: BLEU scores for official tests 2008.
(POS) which were considered to train the POS target LM
and extract the reordering patterns. Other characteristics
of this year?s system are:
? reordering patterns technique;
? source POS model, supporting word reordering;
? no LM interpolation. For this year?s evaluation, we
trained two separate LMs for each domain-specific
corpus (i.e., Europarl and News Commentary tasks).
It is important to mention that 2008 training material is
identical to the one provided for the 2007 shared transla-
tion task.
Table 3 presents the BLEU score obtained for the 2008
development data sets and shows the impact of the target-
side POS LM introduction, which can be characterized as
highly corpus- and language-dependent feature. BL refers
to the same system configuration as described in subsec-
tion 3.2. The computed BLEU scores are case insensitive,
insensitive to tokenization and use one translation refer-
ence.
After submitting the systems we discovered a bug re-
lated to incorrect implementation of the target LMs of
words and tags for Spanish, it caused serious reduction
of translation quality (1.4 BLEU points for development
set in case of English-to-Spanish Europarl task and 2.3
points in case of the corresponding News Commentary
task). The last raw of table 3 (en2es "clean") repre-
sents the results corresponding to the UPC 2008 post-
evaluation system, while the previous one (en2es) refers
to the "bugged" system submitted to the evaluation.
The experiments presented in Table 4 correspond to the
2008 test evaluation sets.
5Corrected post-evaluation results (see subsection 4.3.)
5 Conclusions
In this paper we introduced the TALP UPC Ngram-based
SMT system participating in the WMT08 evaluation.
Apart from briefly summarizing the decoding and opti-
mization processes, we have presented the feature mod-
els that were taken into account, along with the bilingual
Ngram translation model. A reordering strategy based on
linguistically-motivated reordering patterns to harmonize
the source and target word order has been presented in
the framework of the Ngram-based system.
6 Acknowledgments
This work has been funded by the Spanish Government
under grant TEC2006-13964-C03 (AVIVAVOZ project).
The authors want to thank Adri? de Gispert (Cambridge
University) for his contribution to this work.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger. In
Proceedings of the 6th Applied Natural Language Processing
(ANLP-2000).
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek,
J. D. Lafferty, R. Mercer, and P. S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational Lin-
guistics, 16(2):79?85.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine trans-
lation. In Proceedings of the ACL 2007 Workshop on Statis-
tical and Hybrid methods for Machine Translation (WMT),
pages 136?158.
X. Carreras, I. Chao, L. Padr?, and M. Padr?. 2004. Freeling:
An open-source suite of language analyzers. In Proceedings
of the 4th Int. Conf. on Language Resources and Evaluation
(LREC?04).
J. M. Crego and J. B. Mari?o. 2006. Improving statistical MT
by coupling reordering and decoding. Machine Translation,
20(3):199?215.
A. de Gispert. 2006. Introducing linguistic knowledge into
statistical machine translation. Ph.D. thesis, Universitat
Polit?cnica de Catalunya, December.
P. Koehn and C. Monz. 2006. Manual and automatic eval-
uation of machine translation between european languages.
In Proceedings of the ACL 2006 Workshop on Statistical and
Hybrid methods for Machine Translation (WMT), pages 102?
121.
J. B. Mari?o, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lam-
bert, J. A. R. Fonollosa, and M. R. Costa-juss?. 2006. N-
gram based machine translation. Computational Linguistics,
32(4):527?549, December.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the the 38th Annual Meeting
on Association for Computational Linguistics (ACL), pages
440?447.
F. Och and H. Ney. 2004. The alignment template approach to
statistical machine translation. 30(4):417 ? 449, December.
130
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85?89,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The TALP-UPC phrase-based translation system for EACL-WMT 2009
Jos? A.R. Fonollosa and Maxim Khalilov and Marta R. Costa-juss? and
Jos? B. Mari?o and Carlos A. Henr?quez Q. and Adolfo Hern?ndez H. and
Rafael E. Banchs
TALP Research Center
Universitat Polit?cnica de Catalunya, Barcelona 08034
{adrian,khalilov,mruiz,canton,carloshq,adolfohh,rbanchs}@talp.upc.edu
Abstract
This study presents the TALP-UPC sub-
mission to the EACL Fourth Worskhop
on Statistical Machine Translation 2009
evaluation campaign. It outlines the ar-
chitecture and configuration of the 2009
phrase-based statistical machine transla-
tion (SMT) system, putting emphasis on
the major novelty of this year: combina-
tion of SMT systems implementing differ-
ent word reordering algorithms.
Traditionally, we have concentrated on
the Spanish-to-English and English-to-
Spanish News Commentary translation
tasks.
1 Introduction
TALP-UPC (Center of Speech and Language
Applications and Technology at the Universitat
Polit?cnica de Catalunya) is a permanent par-
ticipant of the ACL WMT shared translations
tasks, traditionally concentrating on the Spanish-
to-English and vice versa language pairs. In this
paper, we describe the 2009 system?s architecture
and design describing individual components and
distinguishing features of our model.
This year?s system stands aside from the
previous years? configurations which were per-
formed following an N -gram-based (tuple-based)
approach to SMT. By contrast to them, this
year we investigate the translation models (TMs)
interpolation for a state-of-the-art phrase-based
translation system. Inspired by the work pre-
sented in (Schwenk and Est?ve, 2008), we attack
this challenge using the coefficients obtained for
the corresponding monolingual language models
(LMs) for TMs interpolation.
On the second step, we have performed
additional word reordering experiments, com-
paring the results obtained with a statisti-
cal method (R. Costa-juss? and R. Fonollosa,
2009) and syntax-based algorithm (Khalilov and
R. Fonollosa, 2008). Further the outputs of
the systems were combined selecting the trans-
lation with the Minimum Bayes Risk (MBR) al-
gorithm (Kumar, 2004) that allowed significantly
outperforming the baseline configuration.
The remainder of this paper is organized as
follows: Section 2 presents the TALP-UPC?09
phrase-based system, along with the translation
models interpolation procedure and other minor
novelties of this year. Section 3 reports on the ex-
perimental setups and outlines the results of the
participation in the EACL WMT 2009 evaluation
campaign. Section 4 concludes the paper with dis-
cussions.
2 TALP-UPC phrase-based SMT
The system developed for this year?s shared
task is based on a state-of-the-art SMT sys-
tem implemented within the open-source MOSES
toolkit (Koehn et al, 2007). A phrase-based trans-
lation is considered as a three step algorithm:
(1) the source sequence of words is segmented
in phrases, (2) each phrase is translated into tar-
get language using translation table, (3) the target
phrases are reordered to be inherent in the target
language.
A bilingual phrase (which in the context of SMT
do not necessarily coincide with their linguistic
analogies) is any pair of m source words and n
target words that satisfies two basic constraints:
(1) words are consecutive along both sides of the
bilingual phrase and (2) no word on either side of
the phrase is aligned to a word outside the phrase.
Given a sentence pair and a corresponding word-
to-word alignment, phrases are extracted follow-
ing the criterion in (Och and Ney, 2004). The
probability of the phrases is estimated by relative
frequencies of their appearance in the training cor-
pus.
85
Classically, a phrase-based translation system
implements a log-linear model in which a foreign
language sentence fJ1 = f1, f2, ..., fJ is trans-
lated into another language eI1 = e1, e2, ..., eI by
searching for the translation hypothesis e?I1 maxi-
mizing a log-linear combination of several feature
models (Brown et al, 1990):
e?I1 = argmaxeI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system
models and the set of ?m refers to the weights cor-
responding to these models.
2.1 Translation models interpolation
We implemented a TM interpolation strategy fol-
lowing the ideas proposed in (Schwenk and Es-
t?ve, 2008), where the authors present a promis-
ing technique of target LMs linear interpolation;
in (Koehn and Schroeder, 2007) where a log-linear
combination of TMs is performed; and specifi-
cally in (Foster and Kuhn, 2007) where the authors
present various ways of TM combination and ana-
lyze in detail the TM domain adaptation.
In the framework of the evaluation campaign,
there were two Spanish-to-English parallel train-
ing corpora available: Europarl v.4 corpus (about
50M tokens) and News Commentary (NC) corpus
(about 2M tokens). The test dataset provided by
the organizers this year was from the news do-
main, so we considered the Europarl training cor-
pus as "out-of-domain" data and the News Com-
mentary as "in-domain" training material. Unfor-
tunately, the in-domain corpus is much smaller in
size, however the Europarl corpus can be also used
to increase the final translation and reordering ta-
bles in spite of its different nature.
A straightforward approach to the TM interpo-
lation would be an iterative TM reconstruction ad-
justing scale coefficients on each step of the loop
with use of the highest BLEU score as a maxi-
mization criterion.
However, we did not expect a significant gain
from this time-consumption strategy and we de-
cided to follow a simpler approach. In the pre-
sented results, we obtained the best interpola-
tion weight following the standard entropy-based
optimization of the target-side LM. We adjust
the weight coefficient ?Europarl (?NC = 1 ?
?Europarl) of the linear interpolation of the target-
side LMs:
P (w) = ?Europarl ? PwEuroparl + ?NC ? PwNC (1)
where PwEuroparl and PwNC are probabilities as-
signed to the word sequence w by the LM esti-
mated on Europarl and NC data, respectively.
The scale factor values are automatically opti-
mized to obtain the lowest perplexity ppl(w) pro-
duced by the interpolated LM P (w). We used the
standard script compute ? best ? mix from the
SRI LM package (Stolcke, 2002) for optimization.
On the next step, the optimized coefficients
?Europarl and ?NC are generalized on the interpo-
lated translation and reordering models. In other
words, reordering and translation models are in-
terpolated using the same weights which yield the
lowest perplexity for LM interpolation.
The word-to-word alignment was obtained from
the joint (merged) database (Europarl + NC).
Then, we separately computed the translation and
reordering tables corresponding to the in- and out-
of-domain parts of the joint alignment. The final
tables, as well as the final target LM were obtained
using linear interpolation. The weights were se-
lected using a minimum perplexity criterion esti-
mated on the corresponding interpolated combina-
tion of the target-side LMs.
The optimized coefficient values are: for Span-
ish: NC weight = 0.526, Europarl weight = 0.474;
for English: NC weight = 0.503, Europarl weight
= 0.497. The perplexity results obtained using
monolingual LMs and the 2009 development set
(English and Spanish references) can be found in
Table 1, while the corresponding improvement in
BLEU score is presented in Section 3.3 and sum-
mary of the obtained results (Table 4).
Europarl NC Interpolated
English 463.439 489.915 353.305
Spanish 308.802 347.092 246.573
Table 1: Perplexity results obtained on the Dev
2009 corpus and the monolingual LMs.
Note that the corresponding reordering models
are interpolated with the same weights.
2.2 Statistical Machine Reordering
The idea of the Statistical Machine Reordering
(SMR) stems from the idea of using the power-
ful techniques developed for SMT and to translate
86
the source language (S) into a reordered source
language (S?), which more closely matches the
order of the target language. To infer more re-
orderings, it makes use of word classes. To cor-
rectly integrate the SMT and SMR systems, both
are concatenated by using a word graph which of-
fers weighted reordering hypotheses to the SMT
system. The details are described in (?).
2.3 Syntax-based Reordering
Syntax-based Reordering (SBR) approach deals
with the word reordering problem and is based on
non-isomorphic parse subtree transfer as described
in details in (Khalilov and R. Fonollosa, 2008).
Local and long-range word reorderings are
driven by automatically extracted permutation pat-
terns operating with source language constituents.
Once the reordering patterns are extracted, they
are further applied to monotonize the bilingual
corpus in the same way as shown in the previ-
ous subsection. The target-side parse tree is con-
sidered as a filter constraining reordering rules to
the set of patterns covered both by the source- and
target-side subtrees.
2.4 System Combination
Over the past few years the MBR algorithm uti-
lization to find the best consensus outputs of dif-
ferent translation systems has proved to improve
the translation accuracy (Kumar, 2004). The sys-
tem combination is performed on the 200-best
lists which are generated by the three systems:
(1) MOSES-based system without pre-translation
monotonization (baseline), (2) MOSES-based
SMT enhanced with SMR monotonization and (3)
MOSES-based SMT augmented with SBR mono-
tonization. The results presented in Table 4 show
that the combined output significantly outperforms
the baseline system configuration.
3 Experiments and results
We followed the evaluation baseline instructions 1
to train the MOSES-based translation system.
In some experiments we used MBR decod-
ing (Kumar and Byrne, 2004) with the smoothed
BLEU score as a similarity criteria, that al-
lowed gaining 0.2 BLEU points comparing to the
standard procedure of outputting the translation
with the highest probability (HP). We applied the
Moses implementation of this algorithm to the list
1http://www.statmt.org/wmt09/baseline.html
of 200 best translations generated by the TALP-
UPC system. The results obtained over the official
2009 Test dataset can be found in Table 2.
Task HP MBR
EsEn 24.48 24.62
EnEs 23.46 23.64
Table 2: MBR versus MERT decoding.
The "recase" script provided within the base-
line was supplemented with and additional mod-
ule, which restore the original case for unknown
words (many of them are proper names and loos-
ing of case information leads to a significant per-
formance degradation).
3.1 Language models
The target-side language models were estimated
using the SRILM toolkit (Stolcke, 2002). We tried
to use all the available in-domain training mate-
rial: apart from the corresponding portions of the
bilingual NC corpora we involved the following
monolingual corpora:
? News monolingual corpus (49M tokens for
English and 49M for Spanish)
? Europarl monolingual corpus (about 504M
tokens for English and 463M for Spanish)
? A collection of News development and test
sets from previous evaluations (151K tokens
for English and 175K for Spanish)
? A collection of Europarl development and
test sets from previous evaluations (295K to-
kens for English and 311K for Spanish)
Five LMs per language were estimated on the
corresponding datasets and interpolated follow-
ing the maximum perplexity criteria. Hence, the
larger LMs incorporating in- and out-of-domain
data were used in decoding.
3.2 Spanish enclitics separation
For the Spanish portion of the corpus we imple-
mented an enclitics separation procedure on the
preprocessing step, i.e. the pronouns attached to
the verb were separated and contractions as del
or al were splitted into de el or a el. Conse-
quently, training data sparseness due to Spanish
morphology was reduced improving the perfor-
mance of the overall translation system. As a
87
post-processing, the segmentation was recovered
in the English-to-Spanish direction using target-
side Part-of-Speech tags (de Gispert, 2006).
3.3 Results
The automatic scores provided by the WMT?09
organizers for TALP-UPC submissions calculated
over the News 2009 dataset can be found in Ta-
ble 3. BLEU and NIST case-insensitive (CI) and
case-sensitive (CS) metrics are considered.
Task Bleu CI Bleu CS NIST CI NIST CS
EsEn 25.93 24.54 7.275 7.017
EnEs 24.85 23.37 6.963 6.689
Table 3: BLEU and NIST scores for preliminary
official test dataset 2009 (primary submission)
with 500 sentences excluded.
The TALP-UPC primary submission was
ranked the 3rd among 28 presented translations
for the Spanish-to-English task and the 4th for the
English-to-Spanish task among 9 systems.
The following system configurations and the in-
ternal results obtained are reported:
? Baseline: Moses-based SMT, as proposed
on the web-page of the evaluation campaign
with Spanish enclitics separation and modi-
fied version of ?recase? tool,
? Baseline+TMI: Baseline enhanced with TM
interpolation as described in subsection 2.1,
? Baseline+TMI+MBR: the same as the latter
but with MBR decoding,
? Baseline+TMI+SMR: the same as Base-
line+TMI but with SMR technique applied to
monotonize the source portion of the corpus,
as described in subsection 2.2,
? Baseline+SBR: the same as Baseline but with
SBR algorithm applied to monotonize the
source portion of the corpus, as described in
subsection 2.3,
? System Combination: a combined output of
the 3 previous systems done with the MBR
algorithm, as described in subsection 2.4.
Impact of TM interpolation and MBR decod-
ing is more significant for the English-to-Spanish
translation task, for which the target-side mono-
lingual corpus is smaller than for the Spanish-to-
English translation.
We did not have time to meet the evalua-
tion deadline for providing the system combi-
nation output. Nevertheless, during the post-
evaluation period we performed the experiments
reported in the last three lines of Table 4 (Base-
line+TMI+SMR, Baseline+SBR and System com-
bination).
Note that the results presented in Table 4 differ
from the ones which can be found the Table 3 due
to selective conditions of preliminary evaluation
done by the Shared Task organizers.
System News 2009 Test CI News 2009 Test CS
Spanish-to-English
Baseline 25.82 24.37
Baseline+TMI 25.84 24.47
Baseline+TMI+MBR (Primary) 26.04 24.62
Baseline+SMR 24.95 23.62
Baseline+SBR 24.24 22.89
System combination 26.44 25.00
English-to-Spanish
Baseline 24.56 23.05
Baseline+TMI 25.01 23.41
Baseline+TMI+MBR (Primary) 25.16 23.64
Baseline+SMR 24.09 22.65
Baseline+SBR 23.52 22.05
System combination 25.39 23.86
Table 4: Experiments summary.
88
4 Conclusions
In this paper, we present the TALP-UPC phrase-
based translation system developed for the EACL-
WMT 2009 evaluation campaign. The major nov-
elties of this year are translation models interpola-
tion done in linear way and combination of SMT
systems implementing different word reordering
algorithms. The system was ranked pretty well for
both translation tasks in which our institution has
participated.
Unfortunately, the promising reordering tech-
niques and the combination of their outputs were
not applied within the evaluation deadline, how-
ever we report the obtained results in the paper.
5 Acknowledgments
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project).
References
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J.D. Lafferty, R. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?
85.
A. de Gispert. 2006. Introducing linguistic knowledge
into Statistical Machine Translation. Ph.D. thesis,
Universitat Polit?cnica de Catalunya, December.
G. Foster and R. Kuhn. 2007. Mixture-model adap-
tation for SMT. In In Annual Meeting of the Asso-
ciation for Computational Linguistics: Proc. of the
Second Workshop on Statistical Machine Transla-
tion (WMT), pages 128?135, Prague, Czech Repub-
lic, June.
M. Khalilov and J. R. Fonollosa. 2008. A new subtree-
transfer approach to syntax-based reordering for sta-
tistical machine translation. Technical report, Uni-
versitat Polit?cnica de Catalunya.
Ph. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In In Annual Meeting of the Association for Compu-
tational Linguistics: Proc. of the Second Workshop
on Statistical Machine Translation (WMT), pages
224?227, Prague, Czech Republic, June.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open-source
toolkit for statistical machine translation. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL) 2007, pages 177?180.
Sh. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In In
HLTNAACL?04, pages 169?176.
Sh. Kumar. 2004. Minimum Bayes-Risk Techniques in
Automatic Speech Recognition and Statistical Ma-
chine Translation. Ph.D. thesis, Johns Hopkins Uni-
versity.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 3(4):417?449, December.
M. R. Costa-juss? and J. R. Fonollosa. 2009. An
Ngram reordering model. Computer Speech and
Language. ISSN 0885-2308, accepted for publica-
tion.
H. Schwenk and Y. Est?ve. 2008. Data selection and
smoothing in an open-source system for the 2008
nist machine translation evaluation. In Proceedings
of the Interspeech?08, pages 2727?2730, Brisbane,
Australia, September.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proceedings of the Int. Conf.
on Spoken Language Processing, pages 901?904.
89
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78?86,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Coupling hierarchical word reordering and decoding in phrase-based
statistical machine translation
Maxim Khalilov and Jos? A.R. Fonollosa
Universitat Polit?cnica de Catalunya
Campus Nord UPC, 08034,
Barcelona, Spain
{khalilov,adrian}@gps.tsc.upc.edu
Mark Dras
Macquarie University
North Ryde NSW 2109,
Sydney, Australia
madras@ics.mq.edu.au
Abstract
In this paper, we start with the existing idea of
taking reordering rules automatically derived
from syntactic representations, and applying
them in a preprocessing step before translation
to make the source sentence structurally more
like the target; and we propose a new approach
to hierarchically extracting these rules. We
evaluate this, combined with a lattice-based
decoding, and show improvements over state-
of-the-art distortion models.
1 Introduction
One of the big challenges for the MT community is
the problem of placing translated words in a natural
order. This issue originates from the fact that dif-
ferent languages are characterized by different word
order requirements. The problem is especially im-
portant if the distance between words which should
be reordered is high (global reordering); in this case
the reordering decision is very difficult to take based
on statistical information due to dramatic expansion
of the search space with the increase in number of
words involved in the search process.
Classically, statistical machine translation (SMT)
systems do not incorporate any linguistic analysis
and work at the surface level of word forms. How-
ever, more recently MT systems are moving towards
including additional linguistic and syntactic infor-
mative sources (for example, source- and/or target-
side syntax) into word reordering process. In this pa-
per we propose using a syntactic reordering system
operating with fully, partially and non- lexicalized
reordering patterns, which are applied on the step
prior to translation; the novel idea in this paper is in
the derivation of these rules in a hierarchical manner,
inspired by Imamura et al(2005). Furthermore, we
propose generating a word lattice from the bilingual
corpus with the reordered source side, extending the
search space on the decoding step. A thorough study
of the combination of syntactical and word lattice re-
ordering approaches is another novelty of the paper.
2 Related work
Many reordering algorithms have appeared over the
past few years. Word class-based reordering was a
part of Och?s Alignment Template system (Och et
al., 2004); the main criticism of this approach is that
it shows bad performance for the pair of languages
with very distinct word order. The state-of-the-art
SMT system Moses implements a distance-based re-
ordering model (Koehn et al, 2003) and a distor-
tion model, operating with rewrite patterns extracted
from a phrase alignment table (Tillman, 2004).
Many SMT models implement the brute force ap-
proach, introducing several constrains for the re-
ordering search as described in Kanthak et al (2005)
and Crego et al (2005). The main criticism of such
systems is that the constraints are not lexicalized.
Recently there has been interest in SMT exploiting
non-monotonic decoding which allow for extension
of the search space and linguistic information in-
volvement. The variety of such models includes a
constrained distance-based reordering (Costa-juss?
et al, 2006); and a constrained version of distortion
model where the reordering search problem is tack-
led through a set of linguistically motivated rules
used during decoding (Crego and Mari?o, 2007).
78
A quite popular class of reordering algorithms is
a monotonization of the source part of the parallel
corpus prior to translation. The first work on this
approach is described in Nie?en and Ney (2004),
where morpho-syntactic information was used to ac-
count for the reorderings needed. A representative
set of similar systems includes: a set of hand-crafted
reordering patterns for German-to-English (Collins
et al, 2005) and Chinese-English (Wang et al,
2007) translations, emphasizing the distinction be-
tween German/Chinese and English clause struc-
ture; and statistical machine reordering (SMR) tech-
nique where a monotonization of the source words
sequence is performed by translating them into the
reordered one using well established SMT mecha-
nism (Costa-juss? and Fonollosa, 2006). Coupling
of SMR algorithm and the search space extension
via generating a set of weighted reordering hypothe-
ses has demonstrated a significant improvement, as
shown in Costa-juss? and Fonollosa (2008).
The technique proposed in this study is most
similar to the one proposed for French-to-English
translation task in Xia and McCord (2004), where
the authors present a hybrid system for French-
English translation based on the principle of auto-
matic rewrite patterns extraction using a parse tree
and phrase alignments. We propose using a word
distortion model not only to monotonize the source
part of the corpus (using a different approach to
rewrite rule organization from Xia and McCord), but
also to extend the search space during decoding.
3 Baseline phrase-based SMT systems
The reference system which was used as a transla-
tion mechanism is the state-of-the-art Moses-based
SMT (Koehn et al, 2007). The training and weights
tuning procedures can be found on the Moses web
page1.
Classical phrase-based translation is considered
as a three step algorithm: (1) the source sequence
of words is segmented into phrases, (2) each phrase
is translated into the target language using a transla-
tion table, (3) the target phrases are reordered to fit
the target language. The probabilities of the phrases
are estimated by relative frequencies of their appear-
ance in the training corpus.
1http://www.statmt.org/moses/
In baseline experiments we used a phrase depen-
dent lexicalized reordering model, as proposed in
Tillmann (2004). According to this model, mono-
tonic or reordered local orientations enriched with
probabilities are learned from training data. During
decoding, translation is viewed as a monotone block
sequence generation process with the possibility to
swap a pair of neighbor blocks.
4 Syntax-based reordering coupled with
word graph
Our syntax-based reordering system requires access
to source and target language parse trees and word
alignments intersections.
4.1 Notation
Syntax-based reordering (SBR) operates with source
and target parse trees that represent the syntactic
structure of a string in source and target languages
according to a Context-Free Grammar (CFG).
We call this representation "CFG form". We
formally define a CFG in the usual way as G =
?N,T,R, S?, where N is a set of nonterminal sym-
bols (corresponding to source-side phrase and part-
of-speech tags); T is a set of source-side terminals
(the lexicon), R is a set of production rules of the
form ? ? ?, with ? ? N and ?, which is a sequence
of terminal and nonterminal symbols; and S ? N is
the distinguished symbol.
The reordering rules then have the form
?0@0 . . . ?k@k ?
?d0@d0 . . . ?dk@dk|Lexicon|p1 (1)
where ?i ? N for all 0 ? i ? k; (do . . . dk) is
a permutation of (0 . . . k); Lexicon comes from the
source-side set of words for each ?i; and p1 is a prob-
ability associated with the rule. Figure 1 gives two
examples of the rule format.
4.2 Rules extraction
Concept. Inspired by the ideas presented in Imamura
et al (2005), where monolingual correspondences of
syntactic nodes are used during decoding, we extract
a set of bilingual patterns allowing for reordering as
described below:
79
(1) align the monotone bilingual corpus with
GIZA++ (Och and Ney, 2003) and find
the intersection of direct and inverse word
alignments, resulting in the construction
of the projection matrix P (see below));
(2) parse the source and the target parts of the
parallel corpus;
(3) extract reordering patterns from the par-
allel non-isomorphic CFG-trees based on
the word alignment intersection.
Step 2 is straightforward; we explain aspects of
Steps 1 and 3 in more detail below. Figures 1 and 2
show an example of the extraction of two lexicalized
rules for a parallel Arabic-English sentence:
Arabic:
English:
h*A
this
hW
is
fndq
your
+k
hotel
We use this below in our explanations.
Figure 2: Example of subtree transfer and reordering
rules extraction.
Projection matrix. Bilingual content can be rep-
resented in the form of words or sequences of words
depending on the syntactic role of the corresponding
grammatical element (constituent or POS).
Given two parse trees and a word alignment in-
tersection, a projection matrix P is defined as an
M ?N matrix such that M is the number of words
in the target phrase; N is the number of words in
the source phrase; and a cell (i, j) has a value based
on the alignment intersection ? this value is zero
if word i and word j do not align, and is a unique
non-zero link number if they do.
For the trees in Figure 2,
P =
?
???
1 0 0 0
0 2 0 0
0 0 0 3
0 0 4 0
?
???
Unary chains. Given an unary chain of the form
X ? Y , rules are extracted for each level in this
chain. For example given a rule
NP@0ADV P@1 ? ADV P@1NP@0
and a unary chain "ADV P ? AD", a following
equivalent rule will be generated
NP@0AD@1 ? AD@1NP@0.
The role of target-side parse tree. Although re-
ordering is performed on the source side only, the
target-side tree is of great importance: the reorder-
ing rules can be only extracted if the words covered
by the rule are entirely covered by both a node in
the source and in the target trees. It allows the more
accurate determination of the covering and limits of
the extracted rules.
4.3 Rules organization
Once the list of fully lexicalized reordering patterns
is extracted, all the rules are progressively processed
reducing the amount of lexical information. These
initial rules are iteratively expanded such that each
element of the pattern is generalized until all the lex-
ical elements of the rule are represented in the form
of fully unlexicalized categories. Hence, from each
NN@0 NP@1 ? NP@1 NN@0 | NN@0 << fndq >> NP@1 << +k >> | p
NN@0 NNP@1 ? NNP@1 NN@0 | NN@0 << fndq >> NNP@1 << +k >> | p?
Figure 1: Directly extracted rules.
80
initial pattern with N lexical elements, 2N ? 2 par-
tially lexicalized rules and 1 general rule are gener-
ated. An example of the process of delexicalization
can be found in Figure 3.
Thus, finally three types of rules are available: (1)
fully lexicalized (initial) rules, (2) partially lexical-
ized rules and (3) unlexicalized (general) rules.
On the next step, the sets are processed separately:
patterns are pruned and ambiguous rules are re-
moved. All the rules from the fully lexicalized, par-
tially lexicalized and general sets that appear fewer
than k times are directly discarded (k is a shorthand
for kful, kpart and kgener). The probability of a
pattern is estimated based on relative frequency of
their appearance in the training corpus. Only one
the most probable rule is stored. Fully lexicalized
rules are not pruned (kful = 0); partially lexicalized
rules that have been seen only once were discarded
(kpart = 1); the thresholds kgener was set to 3: it
limits the number of general patterns capturing rare
grammatical exceptions which can be easily found
in any language.
Only the one-best reordering is used in other
stages of the algorithm, so the rule output function-
ing as an input to the next rule can lead to situa-
tions reverting the change of word order that the
previously applied rule made. Therefore, the rules
that can be ambiguous when applied sequentially
during decoding are pruned according to the higher
probability principle. For example, for the pair of
patterns with the same lexicon (which is empty for
a general rule leading to a recurring contradiction
NP@0 VP@1 ? VP@1 NP@0 p1, VP@0 NP@1
? NP@1 VP@0 p2 ), the less probable rule is re-
moved.
Finally, there are three resulting parameter tables
analogous to the "r-table" as stated in (Yamada and
Knight, 2001), consisting of POS- and constituent-
based patterns allowing for reordering and mono-
tone distortion (examples can be found in Table 5).
4.4 Source-side monotonization
Rule application is performed as a bottom-up parse
tree traversal following two principles:
(1) the longest possible rule is applied, i.e. among
a set of nested rules, the rule with a longest left-side
covering is selected. For example, in the case of the
appearance of an NN JJ RB sequence and presence
of the two reordering rules
NN@0 JJ@1 ? ... and
NN@0 JJ@1 RB@2 ? ...
the latter pattern will be applied.
(2) the rule containing the maximum lexical infor-
mation is applied, i.e. in case there is more than one
alternative pattern from different groups, the lexical-
ized rules have preference over the partially lexical-
ized, and partially lexicalized over general ones.
Figure 4: Reordered source-side parse tree.
Once the reordering of the training corpus is
ready, it is realigned and new more monotonic align-
ment is passed to the SMT system. In theory, the
word links from the original alignment can be used,
however, due to our experience, running GIZA++
again results in a better word alignment since it is
easier to learn on the modified training example.
Example of correct local reordering done with the
SBR model can be found in Figure 4.
Initial rule: NN@0 NP@1 ? NP@1 NN@0 | NN@0 << fndq >> NP@1 << +k >> | p1
Part. lexic. rules: NN@0 NP@1 ? NP@1 NN@0 | NN@0 << fndq >> NP@1 << - >> | p2
NN@0 NP@1 ? NP@1 NN@0 | NN@0 << - >> NP@1 << +k >> | p3
General rule: NN@0 NP@1 ? NP@1 NN@0 | p4
Figure 3: Example of a lexical rule expansion.
81
4.5 Coupling with decoding
In order to improve reordering power of the transla-
tion system, we implemented an additional reorder-
ing as described in Crego and Mari?o (2006).
Multiple word segmentations is encoded in a lat-
tice, which is then passed to the input of the de-
coder, containing reordering alternatives consistent
with the previously extracted rules. The decoder
takes the n-best reordering of a source sentence
coded in the form of a word lattice. This approach
is in line with recent research tendencies in SMT, as
described for example in (Hildebrand et al, 2008;
Xu et al, 2005). Originally, word lattice algorithms
do not involve syntax into reordering process, there-
fore their reordering power is limited at representing
long-distance reordering. Our approach is designed
in the spirit of hybrid MT, integrating syntax trans-
fer approach and statistical word lattice methods to
achieve better MT performance on the basis of the
standard state-of-the-art models.
During training a set of word permutation patterns
is automatically learned following given word-to-
word alignment. Since the original and monotonized
(reordered) alignments may vary, different sets of
reordering patterns are generated. Note that no in-
formation about the syntax of the sentence is used:
the reordering permutations are motivated by the
crossed links found in the word alignment and, con-
S 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 L
> n
+ h
+ h
> n
m T E m m T E m
* w
E r y q
> n
* w
E r y q
t A r y x
m T E m
E r y q
* w
E r y q
t A r y x
* w
* w
E r y q
m T E m
t A r y x
E r y q
* w
S 1 2 3 4 5 6 7 8 9
> n
+ h
+ h
> n
m T E m m T E m
> n
* w
t A r y x
* w
m T E m
t A r y x
1 0 L
E r y q
m T E m
E r y q
t A r y x
> n  + h  m T E m  * w  t A r y x  E r y q  W o r d  l a t t i c e ,  p l a i n  t e x t :
W o r d  l a t t i c e ,  r e o r d e r e d  t e x t : > n  + h  m T E m  * w  E r y q  t A r y x  ( c )
( b )
S 1 2 3 4 5> n + h m T E m * w Lt A r y x E r y q
> n  + h  m T E m  * w  t A r y x   E r y qM o n o t o n i c  s e a r c h ,  p l a i n  t e x t :( a )
Figure 5: Comparative example of a monotone search (a), word lattice for a plain (b) and reordered (c) source
sentences.
82
sequently, the generalization power of this frame-
work is limited to local permutations.
On the step prior to decoding, the system gen-
erates word reordering graph for every source sen-
tence, expressed in the form of a word lattice. The
decoder processes word lattice instead of only one
input hypothesis, extending the monotonic search
graph with alternative paths.
Original sentence in Arabic, the English gloss and
reference translation are:
Ar.:
Gl.:
>n +h
this
mTEm
restaurant
*w
has
Eryq
history
tAryx
illustrious
Ref: ?this restaurant has an illustrious history?
The monotonic search graph (a) is extended with
a word lattice for the monotonic train set (b) and re-
ordered train sets (c). Figure 5 shows an example
of the input word graph expressed in the form of a
word lattice. Lattice (c) differ from the graph (b) in
number of edges and provides more input options to
the decoder. The decision about final translation is
taken during decoding considering all the possible
paths, provided by the word lattice.
5 Experiments and results
5.1 Data
The experiments were performed on two Arabic-
English corpora: the BTEC?08 corpus from the
tourist domain and the 50K first-lines extraction
from the corpus that was provided to the NIST?08
evaluation campaign and belongs to the news do-
main (NIST50K). The corpora differ mainly in the
average sentence length (ASL), which is the key cor-
pus characteristic in global reordering studies.
A training set statistics can be found in Table 1.
BTEC NIST50K
Ar En Ar En
Sentences 24.9 K 24.9 K 50 K 50 K
Words 225 K 210 K 1.2 M 1.35 M
ASL 9.05 8.46 24.61 26.92
Voc 11.4 K 7.6 K 55.3 36.3
Table 1: Basic statistics of the BTEC training corpus.
The BTEC development dataset consists of 489
sentences and 3.8 K running words, with 6 human-
made reference translations per sentence; the dataset
used to test the translation quality has 500 sentences,
4.1 K words and is also provided with 6 reference
translations.
The NIST50K development set consists of 1353
sentences and 43 K words; the test data contains
1056 sentences and 33 K running words. Both
datasets have 4 reference translations per sentence.
5.2 Arabic data preprocessing
We took a similar approach to that shown in Habash
and Sadat (2006), using the MADA+TOKAN sys-
tem for disambiguation and tokenization. For dis-
ambiguation only diacritic unigram statistics were
employed. For tokenization we used the D3 scheme
with -TAGBIES option. The scheme splits the fol-
lowing set of clitics: w+, f+, b+, k+, l+, Al+ and
pronominal clitics. The -TAGBIES option produces
Bies POS tags on all taggable tokens.
5.3 Experimental setup
We used the Stanford Parser (Klein and Man-
ning, 2003) for both languages, Penn English Tree-
bank (Marcus et al, 1993) and Penn Arabic Tree-
bank set (Kulick et al, 2006). The English Treebank
is provided with 48 POS and 14 syntactic tags, the
Arabic Treebank has 26 POS and 23 syntactic cate-
gories.
As mentioned above, specific rules are not pruned
away due to a limited amount of training material we
set the thresholds kpart and kgener to relatively low
values, 1 and 3, respectively.
Evaluation conditions were case-insensitive and
with punctuation marks considered. The target-
side 4-gram language model was estimated using
the SRILM toolkit (Stolcke, 2002) and modified
Kneser-Ney discounting with interpolation. The
highest BLEU score (Papineni et al, 2002) was cho-
sen as the optimization criterion. Apart from BLEU,
a standard automatic measure METEOR (Banerjee
and Lavie, 2005) was used for evaluation.
5.4 Results
The scores considered are: BLEU scores obtained
for the development set as the final point of the
MERT procedure (Dev), and BLEU and METEOR
scores obtained on test dataset (Test).
We present BTEC results (Tables 2), character-
ized by relatively short sentence length, and the re-
83
sults obtained on the NIST corpus (Tables 3) with
much longer sentences and much need of global re-
ordering.
Dev Test
BLEU BLEU METEOR
Plain 48.31 45.02 65.98
BL 48.46 47.10 68.10
SBR 48.75 47.52 67.33
SBR+lattice 48.90 48.78 68.85
Table 2: Summary of BTEC experimental results.
Dev Test
BLEU BLEU METEOR
Plain 41.83 43.80 62.03
BL 42.68 43.52 62.17
SBR 42.71 44.01 63.29
SBR+lattice 43.05 44.89 63.30
Table 3: Summary of NIST50K experimental results.
Four SMT systems are contrasted: BL refers to
the Moses baseline system: the training data is not
reordered, lexicalized reordering model (Tillman,
2004) is applied; SBR refers to the monotonic sys-
tem configuration with reordered (SBR) source part;
SBR+lattice is the run with reordered source part, on
the translation step the input is represented as a word
lattice.
We also compare the proposed approach with a
monotonic system configuration (Plain). It shows
the effect of source-reordering and lattice input, also
decoded monotonically.
Automatic scores obtained on the test dataset
evolve similarly when the SBR and word lattice rep-
resentation applied to BTEC and NIST50K tasks.
The combined method coupling two reordering
techniques was more effective than the techniques
applied independently and shows an improvement
in terms of BLEU for both corpora. The METEOR
score is only slightly better for the SBR configura-
tions in case of BTEC task; in the case of NIST50K
the METEOR improvement is more evident. The
general trend is that automatic scores evaluated on
the test set increase with the reordering model com-
plexity.
Application of the SBR algorithm only (without
a word lattice decoding) does not allow achieving
statistical significance threshold for a 95% confi-
dence interval and 1000 resamples (Koehn, 2004)
for either of considered corpora. However, the
SBR+lattice system configuration outperforms the
BL by about 1.7 BLEU points (3.5%) for BTEC task
and about 1.4 BLEU point (3.1%) for NIST task.
These differences is statistically significant.
Figure 6 demonstrates how two reordering tech-
niques interact within a sentence with a need for
both global and local word permutations.
5.5 Syntax-based rewrite rules
As mentioned above, the SBR operates with three
groups of reordering rules, which are the product
of complete or partial delexicalization of the origi-
nally extracted patterns. The groups are processed
and pruned independently. Basic rules statistics for
both translation tasks can be found in Table 4.
The major part of reordering rules consists of
two or three elements (for BTEC task there are
no patterns including more than three nodes). For
NIST50K there are a few rules with higher size in
words of the move (up to 8). In addition, there are
some long lexicalized rules (7-8), generating a high
number of partially lexicalized patterns.
Table 5 shows the most frequent reordering rules
with non-monotonic right part from each group.
Ar. plain.:
En. gloss:
AElnt
announced
Ajhzp
press
AlAElAm
release
l
by
bEvp
mission
AlAmm AlmtHdp
nations united
fy
in
syrAlywn
sierra leone
An
that
...
...
En. ref.: ?a press release by the united nations mission to sierra leone announced that ...?
Ar. reord.: Ajhzp AlAElAm l bEvp AlmtHdp AlAmm fy syrAlywn AElnt An ...
Figure 6: Example of SBR application (highlited bold) and local reordering error corrected with word lattice reorder-
ing (underlined).
84
6 Conclusions
In this study we have shown how the translation
quality can be improved, coupling (1) SBR al-
gorithm and (2) word alignment-based reordering
framework applied during decoding. The system
automatically learns a set of syntactic reordering
patterns that exploit systematic differences between
word order of source and target languages.
Translation accuracy is clearly higher when al-
lowing for SBR coupled with word lattice input rep-
resentation than standard Moses SMT with existing
(lexicalized) reordering models within the decoder
and one input hypothesis condition. We have also
compared the reordering model a monotonic system.
The method was tested translating from Arabic to
English. Two corpora and tasks were considered:
the BTEC task with much need of local reordering
and the NIST50K task requiring long-distance per-
mutations caused by longer sentences.
The reordering approach can be expanded for any
other pair of languages with available parse tools.
We also expect that the method scale to a large train-
ing set, and that the improvement will still be kept,
however, we plan to confirm this assumption exper-
imentally in the near future.
Acknowledgments
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project) and under a FPU grant.
Group # of rules Voc 2-element 3-element 4-element [5-8]-element
BTEC experiments
Specific rules 703 413 406 7 0 0
Partially lexicalized rules 1,306 432 382 50 0 0
General rules 259 5 259 0 0 0
NIST50K experiments
Specific rules 517 399 193 109 72 25
Partially lexicalized rules 17,897 14,263 374 638 1,010 12,241
General rules 489 372 180 90 72 30
Table 4: Basic reordering rules statistics.
Specific rules
NN@0 NP@1 -> NP@1 NN@0 | NN@0 ? Asm ? NP@1 ? +y ? | 0.0270
DTNN@0 DTJJ@1 -> DTJJ@1 DTNN@0 | DTNN@0 ? AlAmm ?DTJJ@1 ? AlmtHdp ? | 0.0515
Partially lexicalized rules
DTNN@0 DTJJ@1 -> DTJJ@1 DTNN@0 | DTNN@0 ? NON ?DTJJ@1 ? AlmtHdp ? | 0.0017
NN@0 NNP@1 -> NNP@1 NN@0 | NN@0 ? NON ?NNP@1 ? $rm ? | 0.0017
General rules
PP@0 NP@1 -> PP@0 NP@1 | 0.0432
NN@0 DTNN@1 DTJJ@2 -> NN@0 DTJJ@2 DTNN@1 |0.0259
Table 5: Examples of Arabic-to-English reordering rules.
85
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 65?72.
M. Collins, Ph. Koehn, and I. Kuc?erov?. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on ACL 2005,
pages 531?540.
M.R. Costa-juss? and J.A.R. Fonollosa. 2006. Sta-
tistical machine reordering. In Proceedings of the
HLT/EMNLP 2006.
M.R. Costa-juss? and J.A.R. Fonollosa. 2008. Comput-
ing multiple weighted reordering hypotheses for a sta-
tistical machine translation phrase-based system. In In
Proc. of the AMTA?08, Honolulu, USA, October.
M.R. Costa-juss?, J.M. Crego, A. de Gispert, P. Lambert,
M. Khalilov, J. A. Fonollosa, J.B. Mari no, and R.E.
Banchs. 2006. TALP phrase-based system and TALP
system combination for IWSLT 2006. In Proceedings
of the IWSLT 2006, pages 123?129.
J.M. Crego and J. B Mari?o. 2006. Reordering experi-
ments for N-gram-based SMT. In SLT?06, pages 242?
245.
J.M. Crego and J.B. Mari?o. 2007. Syntax-enhanced N-
gram-based smt. In Proceedings of MT SUMMIT XI.
J.M. Crego, J. B. Mari?o, and A. de Gispert. 2005. Re-
ordered search and tuple unfolding for ngram-based
smt. In In Proc. of MT SUMMIT X, pages 283?289,
September.
S. Nie?en and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic in-
formation. volume 30, pages 181?204.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, pages 49?52.
A.S. Hildebrand, K. Rottmann, M. Noamany, Q. Gao,
S. Hewavitharana, N. Bach, and S. Vogel. 2008. Re-
cent improvements in the cmu large scale chinese-
english smt system. In Proceedings of ACL-08: HLT
(Companion Volume), pages 77?80.
K. Imamura, H. Okuma, and E. Sumita. 2005. Practical
approach to syntax-based statistical machine transla-
tion. In Proceedings of MT Summit X, pages 267?274.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In In Proc. of the ACL
Workshop on Building and Using Parallel Texts, pages
167?174, June.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting of
the ACL 2003, pages 423?430.
Ph. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based machine translation. In Proceedings of
the HLT-NAACL 2003, pages 48?54.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open-source toolkit for
statistical machine translation. In Proceedings of ACL
2007, pages 177?180.
Ph. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
S. Kulick, R. Gabbard, and M. Marcus. 2006. Parsing the
Arabic Treebank: Analysis and improvements. Tree-
banks and Linguistic Theories.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F.J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A Smorgasbord of
Features for Statistical Machine Translation. In Pro-
ceedings of HLT/NAACL04, pages 161?168.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, pages 311?
318.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proceedings of the Int. Conf. on Spo-
ken Language Processing, pages 901?904.
C. Tillman. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL?04.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proceedings of the Joint Conference on EMNLP.
F. Xia and M. McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. In
Proceedings of the COLING 2004.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. In-
tegrated chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of ACL 2001,
pages 523?530.
86
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 275?282,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The TALP-UPC phrase-based translation systems for WMT12: Morphologysimplification and domain adaptationLlu??s Formiga, Carlos A. Henr??quez Q., Adolfo Herna?ndez,Jose? B. Marin?o, Enric Monte and Jose? A. R. Fonollosa
TALP Research Centre
Universitat Polite`cnica de Catalunya
Barcelona, Spain
{lluis.formiga,carlos.henriquez,adolfo.hernandezjose.marino,enric.monte,jose.fonollosa}@upc.eduAbstract
This paper describes the UPC participation in
the WMT 12 evaluation campaign. All sys-
tems presented are based on standard phrase-
based Moses systems. Variations adopted sev-
eral improvement techniques such as mor-
phology simplification and generation and do-
main adaptation. The morphology simpli-
fication overcomes the data sparsity prob-
lem when translating into morphologically-
rich languages such as Spanish by translat-
ing first to a morphology-simplified language
and secondly leave the morphology gener-
ation to an independent classification task.
The domain adaptation approach improves the
SMT system by adding new translation units
learned from MT-output and reference align-
ment. Results depict an improvement on TER,
METEOR, NIST and BLEU scores compared
to our baseline system, obtaining on the of-
ficial test set more benefits from the domain
adaptation approach than from the morpho-
logical generalization method.1 Introduction
TALP-UPC (Center of Speech and Language
Applications and Technology at the Universitat
Polite`cnica de Catalunya) has participated in the
WMT12 shared task translating across two direc-
tions: English to Spanish and Spanish to English
tasks.
For the Spanish to English task we submitted a
baseline system that uses all parallel training data
and a combination of different target language mod-
els (LM) and Part-Of-Speech (POS) language mod-
els. A similar configuration was submitted for the
English to Spanish task as baseline. Our main ap-
proaches enriched the latter baseline in two indepen-
dent ways: morphology simplification and domain
adaptation by deriving new units into the phrase-
table. Furthermore, additional specific strategies
have been addressed on all systems to deal with well
known linguistic phenomena in Spanish such as cli-
tics and contractions.
The paper is presented as follows. Section 2
presents the main rationale for the phrase-based sys-
tem and the main pipeline of our baseline system.
Section 3 presents the approaches taken to improve
the baseline system on the English to Spanish task.
Section 4 presents the obtained results on internal
and official test sets while conclusions and further
work are presented in Section 5.2 Baseline system: Phrase-Based SMT
Classically, a phrase-based translation system im-
plements a log-linear model in which a foreign lan-
guage sentence f j1 = f1, f2, . . . , fj is translated into
another language sentence eI1 = e1, e2, . . . , eI by
searching for the translation hypothesis that max-
imizes a log-linear combination of feature models
(Brown et al, 1990):
e?I1 = argmaxeI1 ( MXm=1 mhm  eI1, fJ1  ) (1)
where the separate feature functions hm refer to
the system models and the set of  m refers to the
weights corresponding to these models. As fea-
ture functions we used the standard models available
275
the$ NATO$ mission$ officially$ ended$
la$ misi?n$ de$ la$ OTAN$ termin?$ oficialmente$
DAFS$ NCFS$ SPS$ DAFS$ NP$ VMIS3S0$ RG$
Figure 1: Factored phrase-based MT based on trans-
lation from surface to surface and Part-of-Speech
on Moses, i.e., relative frequencies, lexical weights,
word and phrase penalty, wbe-msd-bidirectional-fe
reordering models and two language models, one for
surface and one for POS tags. Phrase scoring was
computed using Good-Turing discounting (Foster et
al., 2006).
The tuning process was done using MERT (Och,
2003) with Minimum Bayes-Risk decoding (MBR)
(Kumar and Bryne, 2004) on Moses and focusing on
minimizing the BLEU score (Papineni et al, 2002)
of the development set. Final translations were also
computed using MBR decoding.
Additionally to the settings mentioned before, we
worked with a factored version of the corpus. Fac-
tored corpora augments surface forms with addi-
tional information, such as POS tags or lemmas as
shown in Figure 1. In that case, factors other than
surface (e.g. POS) are usually less sparse, allowing
to build factor-specific language models with higher-
order n-grams. These higher-order language models
usually help to obtain more syntactically correct out-
put. Concretely we map input source surfaces to tar-
get surfaces and POS tags.2.1 Corpus used
The baseline system was trained using all paral-
lel corpora, i.e. the European Parliament (EPPS)
(Koehn, 2005), News Commentary and United Na-
tions. Table 1 shows the statistics of the training data
after the cleaning process described later on Subsec-
tion 2.2.
Regarding the monolingual data, there was also
more News corpora separated by years for Spanish
and English and there was the Gigaword monolin-
gual corpus for English. All data can be found on
the Translation Task?s website1. We used all News
corpora (and Gigaword for English) to build the lan-
1http://www.statmt.org/wmt12/translation-task.html
Corpus Sent. Words Vocab. avg.len.
EPPS
Eng
1.90 M
49.40 M 124.03 k 26.05
Spa 52.66 M 154.67 k 27.28
News.Com
Eng
0.15 M
3.73 M 62.70 k 24.20
Spa 4.33 M 73.97 k 28.09
UN
Eng
8.38 M
205.68 M 575.04 k 24.54
Spa 239.40 M 598.54 k 28.56
Table 1: English-Spanish corpora statistics for
NAACL-WMT 2012 after cleaning process
guage model. Initially, a LM was built for every cor-
pus and then they were combined to produce de final
LM. Table 2 presents the statistics of each corpora,
again after the cleaning process.
Corpus Sent. Words Vocab.
EPPS
Eng 2.22 M 59.88 M 144.03 k
Spa 2.12 M 61.97 M 174.92 k
News.Com.
Eng 0.21 M 5.08 M 72.55 k
Spa 0.18 M 5.24 M 81.56 k
UN
Eng 11.20 M 315.90 M 767.12 k
Spa 11.20 M 372.21 M 725.73 k
News.07
Eng 3.79 M 90.25 M 711.55 k
Spa 0.05 M 1.33 M 64.10 k
News.08
Eng 13.01 M 308.82 M 1555.53 k
Spa 1.71 M 49.97 M 377.56 k
News.09
Eng 14.75 M 348.24 M 1648.05 k
Spa 1.07 M 30.57 M 287.81 k
News.10
Eng 6.81 M 158.15 M 915.14 k
Spa 0.69 M 19.58 M 226.76 k
News.11
Eng 13.46 M 312.50 M 1345.79 k
Spa 5.11 M 151.06 M 668.63 k
Giga Eng 22.52 M 657.88 M 3860.67 k
Table 2: Details of monolingual corpora used for
building language-models.
For internal testing we used the News 2011?s data
and concatenated the remaining three years of News
data as a single parallel corpus for development. Ta-
ble 3 shows the statistics for these two sets and in-
cludes in the last rows the statistics of the official test
set for this year?s translation task.2.2 Corpus processing
All corpora were processed in order to remove or
normalize ambiguous or special characters such as
quotes and spaces. Among other TALP-UPC spe-
cific scripts, we used a modified version of the
normalized-punctuation script provided by the orga-
nizers in order to skip the reordering rules which in-
volved quotes and stop punctuation signs.
276
Corpus Sent. Words Vocab. avg.len.
dev
Eng
7.57 k
189.01 k 18.61 k 24.98
Spa 202.80 k 21.75 k 26.80
test11
Eng
3.00 k
74.73 k 10.82 k 24.88
Spa 81.01 k 12.16 k 26.98
test12
Eng
3.00 k
72.91 k 10.24 k 24.28
Spa 80.38 k 12.02 k 26.77
Table 3: Detail of development and test corpora used
to tune and test the system.
POS-Tagging and tokenization for both Spanish
and English data sets were obtained using FreeLing
(Padro? et al, 2010). Freeling tokenization is able
to deal with contractions (?del? ! ?de el?) and cli-
tics separation (?co?mpramelo? ! ?compra me lo?)
in Spanish and English. Stemming was performed
using Snowball (Porter, 2001).
Surface text was lowercased conditionally based
on the POS tagging: proper nouns and adjectives
were separated from other POS categories to deter-
mine if a string should be fully lowercased (no spe-
cial property), partially lowercased (proper noun or
adjective) or not lowercased at all (acronym).
Bilingual corpora were cleaned with clean-
corpus-n script of Moses (Koehn et al, 2007) re-
moving all sentence pair with more than 70 words
in any language, considering the already tokenized
data. That script also ensures a maximum length
ratio below of nine (9) words between source and
target sentences.
Postprocessing in both languages consisted of a
recasing step using Moses recaser script. Further-
more we built an additional script in order to check
the casing of output names with respect to source
sentence names and case them accordingly, with ex-
ception of names placed at beginning of the sen-
tence. After recasing, a final detokenization step
was performed using standard Moses tools. Span-
ish postprocessing also included two special scripts
to recover contractions and clitics.2.3 Language Model and alignmentconfiguration
Word alignment was performed at stem level with
GIZA++ toolkit (Och and Ney, 2003) and grow-
diag-final-and joint alignment.
Language models were built from the monolin-
gual data provided covering different domains: Eu-
roparl, News and UN. We built them using Kneser-
Ney algorithm (Chen and Goodman, 1999), inter-
polation in order to avoid over-fitting and consider-
ing unknown words. First we built a 5-gram lan-
guage model for each corpus; then, the final LM
was obtained interpolating them all towards the de-
velopment set. We used SRI Language Model (Stol-
cke, 2002) toolkit, which provides compute-best-mix
script for the interpolation.
The POS language model was built analogously
to the surface language with some variants: it was a7-gram LM, without discounting nor interpolation.3 Improvement strategies3.1 Motivations
In order to improve the baseline system we present
two different strategies. First we present an im-
provement strategy based on morphology simplifi-
cation plus generation to deal with the problems
raised by morphological rich languages such as
Spanish. Second we present a domain adaptation
strategy that consists in deriving new units into the
phrase-table.3.2 Morphology simplification
The first improvement strategy is based on morphol-
ogy simplification when translating from English to
Spanish.
The problems raised when translating from a lan-
guage such as English into richer morphology lan-
guages are well known and are a research line of
interest nowadays (Popovic and Ney, 2004; Koehn
and Hoang, 2007; de Gispert and Marin?o, 2008;
Toutanova et al, 2008; Clifton and Sarkar, 2011). In
that direction, inflection causes a very large target-
language lexicon with a significant data sparsity
problem. In addition, system output is limited only
to the inflected phrases available in the parallel train-
ing corpus. Hence, SMT systems cannot gener-
ate proper inflections unless they have learned them
from the appropriate phrases. That would require to
have a parallel corpus containing all possible word
inflections for all phrases available, which it is an
unfeasible task.
The morphology related problems in MT have
been addressed from different approaches and may
277
????????????? ????????????????
??????????????????????? ???????????? ??????
??????????????????????????????
???????
???????
??????????????????????????
????????????????????????????????? ?? ???????????? ?? ??
????!???"#???$??%&%?$?%?'
????????????????????????
??????????????????????????????
????????????????????
%')???')?'??'????????????????? ?$?*
??)???
??????????????????????
?+$?,$???"?'??$?%?'?$??,?$?#
??$%'%'"???? ??????
Figure 2: Above, flow diagram of the training of simplified morphology translation models. Below, Spanish
morphology generation as an independent classification task.Type Text
PLAIN la Comisio?n puede llegar
TARGET: a paralizar el programa
TARGET+PoS la Comisio?n VMIP3S0[poder]
(Gen. Sur.): llegar a paralizar el programa
TARGET+PoS la Comisio?n VMIPpn0[poder]
(Simpl. PoS): llegar a paralizar el programa
Table 4: Example of morphology simplification
steps taken for Spanish verbs.
be summarized in four categories: i) factored mod-
els (Koehn and Hoang, 2007), enriched input mod-
els (Avramidis and Koehn, 2008; Ueffing and Ney,
2003), segmented translation (Virpioja et al, 2007)
and morphology generation (Toutanova et al, 2008;
de Gispert and Marin?o, 2008).
Our strategy for dealing with morphology gener-
ation is based in the latter approach (de Gispert and
Marin?o, 2008) (Figure 2). We center our strategy in
simplifying only verb forms as previous studies in-
dicate that they contribute to the main improvement
(Ueffing and Ney, 2003; de Gispert and Marin?o,
2008). That strategy makes clear the real impact
of morphology simplification by providing an upper
bound oracle for the studied scenarios.
The approach is as follows: First, target verbs
are simplified substituting them with their sim-
plified forms (Table 4). In this example, the
verb form ?puede? (he can) is transformed into
?VMIPpn0[poder]?, indicating simplified POS and
base form (lemma); where ?p? and ?n? represent any
person and number once simplified (from 3rd per-
son singular). Secondly, standard MT models are
obtained from English into simplified morphology
Spanish. Morphology prediction acts as a black box,
with its models estimated over a simplified morphol-
ogy parallel texts (including target language model
and lexicon models).
Generation is implemented by Decision Directed
Acyclic Graphs (DDAG) (Platt et al, 2000) com-
pound of binary SVM classifiers. In detail, a DDAG
combines many two-class classifiers to a multi-
classification task (Herna?ndez et al, 2010).3.3 Domain adaptation
Depending on the available resources, different do-
main adaptation techniques are possible. Usually,
the baseline system is built with a large out-of-
domain corpus (in our case the European Parlia-
ment) and we aim to adapt to another domain that
has limited data, either only monolingual or hope-
fully bilingual as well. The WMT Translation Task
focuses on adapting the system to a news domain,
offering an in-domain parallel corpus to work with.
In case of additional target monolingual data, pre-
vious works have focused on language model inter-
polations (Bulyko et al, 2007; Mohit et al, 2009;
Wu et al, 2008). When parallel in-domain data
is available, the latest researches have focused on
mixture model adaptation of the translation model
(Civera and Juan, 2007; Foster and Kuhn, 2007; Fos-
ter et al, 2010). Our work is closer to the latest ap-
278
proaches. We used the in-domain parallel data to
adapt the translation model, but focusing on the de-
coding errors that the out-of-domain baseline system
made while translating the in-domain corpus. The
idea is to detect where the system made its mistakes
and use the in-domain data to teach it how to correct
them.
Our approach began with a baseline system built
with the Parliament and the United Nations parallel
corpora but without the News parallel corpus. The
rest of the configuration remained the same for the
baseline. With this alternative baseline system, we
translated the source side of the News parallel cor-
pus to obtain a revised corpus of it, as defined in
(Henr??quez Q. et al, 2011). The revised corpus con-
sists of the source side, the output translation and the
target side, also called the target correction. The out-
put translation and its reference are then compare to
detect possible mistakes that the system caused dur-
ing decoding.
The translation was used as a pivot to find a word-
to-word alignment between the source side and the
target correction. The word-to-word alignment be-
tween source side and translation was provided by
Moses during decoding. The word-to-word align-
ment between the output translation and target cor-
rection was obtained following these steps:
1. Translation Edit Rate (Snover et al, 2006) be-
tween each output translation and target correc-
tion sentence pair was computed to obtain its
edit path and detect which words do not change
between sentences. Words that did not change
were directly linked
2. Going from left to right, for each unaligned
word wout on the output translation sentence
and each word wtrg on the target correction
sentence, a similarity function was computed
between them and wout got aligned with the
word wtrg that maximized this similarity.
The similarity function was defined as a linear
combination of features that considered if the words
wout and wtrg were identical, if the previous or fol-
lowing word of any of them were aligned with each
other and a lexical weight between them using the
bilingual lexical features from the baseline as refer-
ences.
With both word-to-word alignments computed for
a sentence pair, we linked source word wsrc with tar-
get word wtrg is and only if exists a output transla-
tion word wout such that there is a link between wsrc
and wout and a link between wout and wtrg.
After aligning the corpus, we built the transla-
tion and reordering model of it, using the baseline
settings. We called these translation and reorder-
ing models, revised models. They include phrases
found in the baseline that were correctly chosen dur-
ing decoding and also new phrases that came from
the differences between the output translation and its
correction.
Finally, the revised translation model features
were linearly combined with their corresponding
baseline features to build the final translation model,
called the derived translation model. The combina-
tion was computed in the following way:
hid(s, t) = ?hib(s, t) + (1  ?)hir(s, t) (2)
where hid(s, t) is the derived feature function i for
the bilingual phrase (s, t), hib(s, t) is the baseline
feature function of and hir(s, t) the revised feature
function. A value of ? = 0.60 was chosen after de-
termining it was the one that maximized the BLEU
score of the development set during tuning. Differ-
ent values for ? were considered, between 0.50 and0.95 with increments of 0.05 between them.
Regarding the reordering model, we added the un-
seen phrases from the revised reordering model into
the baseline reordering model, leaving the remaining
baseline phrase reordering weights intact.4 Results4.1 Language Model perplexities
LM
Perplexity
Surface POS
Baseline 205.36 13.23
Simplified 193.66 12.66
Table 6: Perplexities obtained across baseline and
morphology simplification.
Before evaluating translation performance, we
studied to what extent the morphology simplifica-
279
EN!ES
BLEU NIST TER METEOR
CS CI CS CI CS CI
test11
Baseline 30.7 32.53 7.820 8.120 57.19 55.05
Morph. Oracle 31.56 33.35 7.949 8.233 56.44 ?
Morph. Gen. 31.03 32.85 7.866 8.163 56.95 55.39
Adaptation 31.16 32.93 7.857 8.155 56.88 55.19
test12
Baseline 31.21 32.74 7.981 8.244 55.76 55.48
Morph. Oracle 32 33.41 8.090 8.339 55.15 ?
Morph. Gen. 31.46 32.98 8.010 8.274 55.62 55.66
Adaptation 31.73 33.24 8.037 8.294 55.37 55.82
(a) English!Spanish
ES!EN
BLEU NIST TER METEOR
CS CI CS CI CS CI
test11
Baseline
28.81 30.29 7.670 7.933 59.01 51.09
test12 32.27 33.81 8.014 8.282 56.26 53.96
(b) Spanish!English
Table 5: Automatic scores for English$Spanish translations. CS and CI indicate Case-Sensitive or Case-
Insensitive evaluations.
tion strategy may help decreasing the language mod-
els perplexity.
In table 6 we can see the effects of simplification.
Perplexity is computed from the corresponding in-
ternal test sets to the baseline or simplified language
models.
In general terms, the simplification process is
slightly effective, yielding an averaged improvement
of  5.02%.4.2 Translation performance
Evaluations were performed with different transla-
tion quality measures: BLEU, NIST, TER and ME-
TEOR (Denkowski and Lavie, 2011) which evalu-
ate distinct aspects of the quality of the translations.
First we evaluated the WMT11 test (test11) as an
internal indicator of our systems. Later we did the
same analysis with the WMT12 official test files.
Table 5 presents the obtained results. Experi-
ments began building the baseline system, which
included the special treatment for clitics, contrac-
tions and casing as described in Section 2.2. Once
the baseline was set, we proceeded with two paral-
lel lines, one for morphology simplification and the
other for domain adaptation.
For morphology generation approach (Table 5)
oracles (Morph. Oracle) represent how much gain
we could expect with a perfect generation module
and generation (Morph. Gen.) represent the actual
performance combining simplification and the gen-
eration strategies. Oracles achieve a promising av-
eraged improvement of +1.79% (depending on the
metric or the test set) with respect to the baseline.
However, generation only improves the baseline by
a +0.61%, encouraging us to keep working on that
strategy.
Regarding the domain adaptation approach, we
evaluated the internal test set (test11). As we can
see again on Table 5a the adaptation strategy outper-
forms the baseline on all quality measures starting
with an averaged gain of +0.94%.
Comparing the two approaches, we can see that
the domain adaptation method was better in terms of
BLEU score and TER than the morphology genera-
tion but the latter was better on NIST and METEOR
on our internal test set. This made us decided for the
latter as the primary system submitted, leaving the
domain adaptation approach system as a contrastive
submission. Additionally to the automatic quality
measures, we are particularly interested in the man-
ual evaluation results, as we believe the morphology
generation will be more sensitive to this type of eval-
280
uation than to automatic metrics.
Official results (test12) can be found on Table 5b.
Surprisingly, this time the domain adaptation ap-
proach performed better than the morphology sim-
plification on all metrics: BLEU, NIST, TER and
METEOR, with an averaged gain of +1.04% over
the baseline system, which ranks our submissions
second and third in terms of BLEU scores (con-
trastive and primary respectively) when compared
with all other submissions for the WMT12 transla-
tion task.5 Conclusions and further work
This papers describes the UPC participation during
the 2012 WMT?s Translation Task. We have partici-
pated with a baseline system for Spanish-to-English,
a baseline system for English-to-Spanish and two in-
dependent enhancements to the baseline system for
English-to-Spanish as well.
Our primary submission applied morphology sim-
plification and generation with the objective of ease
the translation process when dealing with rich mor-
phology languages like Spanish, deferring the mor-
phology generation as an external post-process clas-
sification task.
The second approach focused on domain adapta-
tion. Instead of concatenating the training News par-
allel data together with the European Parliament and
United Nations, a preliminary system was built with
the latter two and separated translation and reorder-
ing models were computed using the News parallel
data. These models were then added to the prelimi-
nary models in order to build the adapted system.
Results showed that both approaches performed
better than the baseline system, being the domain
adaptation configuration the one that performed bet-
ter for 2012 test in terms of all automatic quality
indicators: BLEU, NIST, TER and METEOR. We
look forward the the manual evaluation results as we
believe our primary system may be more sensitive to
this type of human evaluation.
Future work should focus on combining the two
approaches, applying first morphological general-
ization to the training data and then using the domain
adaptation technique on the resulting corpora in or-
der to determine the joined benefits of both strate-
gies.
Acknowledgments
This work has been partially funded by the Spanish
Government (Buceador, TEC2009-14094-C04-01)
and the European Community?s FP7 program under
grant agreement number 247762 (FAUST, FP7-ICT-
2009-4-247762).References
E. Avramidis and P. Koehn. 2008. Enriching morpho-
logically poor languages for statistical machine trans-
lation. Proceedings of ACL-08: HLT, pages 763?770.
P.F. Brown, J. Cocke, S.A.D. Pietra, V.J.D. Pietra, F. Je-
linek, J.D. Lafferty, R.L. Mercer, and P.S. Roossin.
1990. A statistical approach to machine translation.
Computational linguistics, 16(2):79?85.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen, and John Makhoul. 2007. Language model
adaptation in machine translation from speech. Test,
4:117?120.
S.F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Com-
puter Speech & Language, 13(4):359?393.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
A. Clifton and A. Sarkar. 2011. Combining morpheme-
based machine translation with post-processing mor-
pheme prediction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Portland, OR,
USA.
Adria` de de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statistical
MT. Speech Communication, 50(11-12):1034?1046.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
George Foster and Roland Kuhn. 2007. Mixture-Model
Adaptation For SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, StatMT
?07, pages 128?135, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
281
EMNLP ?06, pages 53?61, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Carlos A. Henr??quez Q., Jose? B. Marin?o, and Rafael E.
Banchs. 2011. Deriving translation units using small
additional corpora. In Proceedings of the 15th Confer-
ence of the European Association for Machine Trans-
lation.
Adolfo Herna?ndez, Enric Monte, and Jose? B. Marin?o.
2010. Multiclass classification for Morphology gener-
ation in statistical machine translation. In Proceedings
of the VI Jornadas en Tecnolog??a del Habla? and II
Iberian SLTech Workshop, pages 179?182, November.
http://fala2010.uvigo.es.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Transla-
tion Summit.
Shankar Kumar and William Bryne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proceedings of the Human Language Technology
and North American Association for Computational
Linguistics Conference (HLT/NAACL), Boston,MA,
May 27-June 1.
Behrang Mohit, Frank Liberato, and Rebecca Hwa.
2009. Language Model Adaptation for Difficult to
Translate Phrases. In Proceedings of the 13th Annual
Conference of the EAMT.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL).
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling 2.1:
Five years of open-source language processing tools.
In Proceedings of 7th Language Resources and Evalu-
ation Conference (LREC 2010), La Valletta, MALTA,
May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL).
John C. Platt, Nello Cristianini, and John Shawe-taylor.
2000. Large margin DAGs for multiclass classifica-
tion. In Advances in Neural Information Processing
Systems, pages 547?553. MIT Press.
Maja Popovic and Hermann Ney. 2004. Towards the
use of word stems and suffixes for statistical machine
translation. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC?04, pages 1585?1588, May.
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Seventh International Conference on
Spoken Language Processing.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08: HLT,
pages 514?522, Columbus, Ohio, June. Association
for Computational Linguistics.
Nicola Ueffing and Hermann Ney. 2003. Using pos in-
formation for statistical machine translation into mor-
phologically rich languages. In Proceedings of the
tenth conference on European chapter of the Associa-
tion for Computational Linguistics - Volume 1, EACL
?03, pages 347?354, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sadeniemi.
2007. Morphology-aware statistical machine transla-
tion based on morphs induced in an unsupervised man-
ner. Machine Translation Summit XI, 2007:491?498.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 993?1000, Stroudsburg, PA, USA. Association
for Computational Linguistics.
282
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134?140,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Phrase-based Translation Systems for WMT13:
System Combination with Morphology Generation,
Domain Adaptation and Corpus Filtering
Llu??s Formiga?, Marta R. Costa-jussa`?, Jose? B. Marin?o?
Jose? A. R. Fonollosa?, Alberto Barro?n-Ceden?o??, Llu??s Ma`rquez?
?TALP Research Centre ?Facultad de Informa?tica
Universitat Polite`cnica de Catalunya Universidad Polite?cnica de Madrid
Barcelona, Spain Madrid, Spain
{lluis.formiga,marta.ruiz,jose.marino,jose.fonollosa}@upc.edu
{albarron, lluism}@lsi.upc.edu
Abstract
This paper describes the TALP participa-
tion in the WMT13 evaluation campaign.
Our participation is based on the combi-
nation of several statistical machine trans-
lation systems: based on standard phrase-
based Moses systems. Variations include
techniques such as morphology genera-
tion, training sentence filtering, and do-
main adaptation through unit derivation.
The results show a coherent improvement
on TER, METEOR, NIST, and BLEU
scores when compared to our baseline sys-
tem.
1 Introduction
The TALP-UPC center (Center for Language and
Speech Technologies and Applications at Univer-
sitat Polite`cnica de Catalunya) focused on the En-
glish to Spanish translation of the WMT13 shared
task.
Our primary (contrastive) run is an internal
system selection comprised of different train-
ing approaches (without CommonCrawl, unless
stated): (a) Moses Baseline (Koehn et al,
2007b), (b) Moses Baseline + Morphology Gener-
ation (Formiga et al, 2012b), (c) Moses Baseline
+ News Adaptation (Henr??quez Q. et al, 2011),
(d) Moses Baseline + News Adaptation + Mor-
phology Generation , and (e) Moses Baseline +
News Adaptation + Filtered CommonCrawl Adap-
tation (Barro?n-Ceden?o et al, 2013). Our sec-
ondary run includes is the full training strategy
marked as (e) in the previous description.
The main differences with respect to our last
year?s participation (Formiga et al, 2012a) are: i)
the inclusion of the CommonCrawl corpus, using
a sentence filtering technique and the system com-
bination itself, and ii) a system selection scheme
to select the best translation among the different
configurations.
The paper is organized as follows. Section 2
presents the phrase-based system and the main
pipeline of our baseline system. Section 3 de-
scribes the our approaches to improve the baseline
system on the English-to-Spanish task (special at-
tention is given to the approaches that differ from
last year). Section 4 presents the system combi-
nation approach once the best candidate phrase of
the different subsystems are selected. Section 5
discusses the obtained results considering both in-
ternal and official test sets. Section 6 includes con-
clusions and further work.
2 Baseline system: Phrase-Based SMT
Our contribution is a follow up of our last year par-
ticipation (Formiga et al, 2012a), based on a fac-
tored Moses from English to Spanish words plus
their Part-of-Speech (POS). Factored corpora aug-
ments words with additional information, such as
POS tags or lemmas. In that case, factors other
than surface (e.g. POS) are usually less sparse, al-
lowing the construction of factor-specific language
models with higher-order n-grams. Such language
models can help to obtain syntactically more cor-
rect outputs.
We used the standard models available in Moses
as feature functions: relative frequencies, lexi-
cal weights, word and phrase penalties, wbe-msd-
bidirectional-fe reordering models, and two lan-
guage models (one for surface and one for POS
tags). Phrase scoring was computed using Good-
Turing discounting (Foster et al, 2006).
As aforementioned, we developed five factored
Moses-based independent systems with different
134
approaches. We explain them in Section 3. As
a final decision, we applied a system selection
scheme (Formiga et al, 2013; Specia et al, 2010)
to consider the best candidate for each sentence,
according to human trained quality estimation
(QE) models. We set monotone reordering of
the punctuation signs for the decoding using the
Moses wall feature.
We tuned the systems using the Moses
MERT (Och, 2003) implementation. Our focus
was on minimizing the BLEU score (Papineni et
al., 2002) of the development set. Still, for ex-
ploratory purposes, we tuned configuration (c) us-
ing PRO (Hopkins and May, 2011) to set the ini-
tial weights at every iteration of the MERT algo-
rithm. However, it showed no significant differ-
ences compared to the original MERT implemen-
tation.
We trained the baseline system using all
the available parallel corpora, except for
common-crawl. That is, European Parlia-
ment (EPPS) (Koehn, 2005), News Commentary,
and United Nations. Regarding the monolingual
data, there were more News corpora organized
by years for Spanish. The data is available at
the Translation Task?s website1. We used all
the News corpora to busld the language model
(LM). Firstly, a LM was built for every corpus
independently. Afterwards, they were combined
to produce de final LM.
For internal testing we used the News 2011 and
News 2012 data and concatenated the remaining
three years of News data as a single parallel corpus
for development.
We processed the corpora as in our participa-
tion to WMT12 (Formiga et al, 2012a). Tok-
enization and POS-tagging in both Spanish and
English was obtained with FreeLing (Padro? et al,
2010). Stemming was carried out with Snow-
ball (Porter, 2001). Words were conditionally case
folded based on their POS: proper nouns and ad-
jectives were separated from other categories to
determine whether a string should be fully folded
(no special property), partially folded (noun or ad-
jective) or not folded at all in (acronym).
Bilingual corpora was filtered with the clean-
corpus-n script of Moses (Koehn et al, 2007a), re-
moving those pairs in which a sentence was longer
than 70. For the CommonCrawl corpus we used a
more complex filtering step (cf. Section 3.3).
1http://www.statmt.org/wmt13/translation-task.html
Postprocessing included two special scripts to
recover contractions and clitics. Detruecasing was
done forcing the capitals after the punctuation
signs. Furthermore we used an additional script in
order to check the casing of output names with re-
spect to the source. We reused our language mod-
els and alignments (with stems) from WMT12.
3 Improvement strategies
We tried three different strategies to improve the
baseline system. Section 3.1 shows a strategy
based on morphology simplification plus genera-
tion. Its aim is dealing with the problems raised
by morphology-rich languages, such as Spanish.
Section 3.2 presents a domain?adaptation strategy
that consists of deriving new units. Section 3.3
presents an advanced strategy to filter the good bi-
sentences from the CommonCrawl corpus, which
might be useful to perform the domain adaptation.
3.1 Morphology generation
Following the success of our WMT12 participa-
tion (Formiga et al, 2012a), our first improve-
ment is based on the morphology generalization
and generation approach (Formiga et al, 2012b).
We focus our strategy on simplifying verb forms
only.
The approach first translates into Spanish sim-
plified forms (de Gispert and Marin?o, 2008). The
final inflected forms are predicted through a mor-
phology generation step, based on the shallow
and deep-projected linguistic information avail-
able from both source and target language sen-
tences.
Lexical sparseness is a crucial aspect to deal
with for an open-domain robust SMT when trans-
lating to morphology-rich languages (e.g. Span-
ish) . We knew beforehand (Formiga et al, 2012b)
that morphology generalization is a good method
to deal with generic translations and it provides
stability to translations of the training domain.
Our morphology prediction (generation) sys-
tems are trained with the WMT13 corpora (Eu-
roparl, News, and UN) together with noisy data
(OpenSubtitles). This combination helps to obtain
better translations without compromising the qual-
ity of the translation models. These kind of mor-
phology generation systems are trained with a rel-
atively short amount of parallel data compared to
standard SMT training corpora.
Our main enhancement to this strategy is the
135
addition of source-projected deep features to the
target sentence in order to perform the morphol-
ogy prediction. These features are Dependency
Features and Semantic Role Labelling, obtained
from the source sentence through Lund Depen-
dency Parser2. These features are then projected
to the target sentence as explained in (Formiga et
al., 2012b).
Projected deep features are important to pre-
dict the correct verb morphology from clean and
fluent text. However, the projection of deep fea-
tures is sentence-fluency sensitive, making it un-
reliable when the baseline MT output is poor. In
other words, the morphology generation strategy
becomes more relevant with high-quality MT de-
coders, as their output is more fluent, making the
shallow and deep features more reliable classifier
guides.
3.2 Domain Adaptation through pivot
derived units
Usually the WMT Translation Task focuses on
adapting a system to a news domain, offering an
in-domain parallel corpus to work with. How-
ever this corpus is relatively small compared to
the other corpora. In our previous participation
we demonstrated the need of performing a more
aggressive domain adaptation strategy. Our strat-
egy was based on using in-domain parallel data to
adapt the translation model, but focusing on the
decoding errors that the out-of-domain baseline
system makes when translating the in-domain cor-
pus.
The idea is to identify the system mistakes and
use the in-domain data to learn how to correct
them. To that effect, we interpolate the transla-
tion models (phrase and lexical reordering tables)
with a new adapted translation model with derived
units. We obtained the units identifying the mis-
matching parts between the non-adapted transla-
tion and the actual reference (Henr??quez Q. et al,
2011). This derivation approach uses the origi-
nal translation as a pivot to find a word-to-word
alignment between the source side and the target
correction (word-to-word alignment provided by
Moses during decoding).
The word-to-word monolingual alignment be-
tween output translation target correction was ob-
tained combining different probabilities such as
i)lexical identity, ii) TER-based alignment links,
2http://nlp.cs.lth.se/software/
Corpus Sent. Words Vocab. avg.len.
Original EN 1.48M 29.44M 465.1k 19.90ES 31.6M 459.9k 21.45
Filtered EN 0.78M 15.3M 278.0k 19.72ES 16.6M 306.8k 21.37
Table 1: Commoncrawl corpora statistics for
WMT13 before and after filtering.
iii) lexical model probabilities, iv) char-based Lev-
enshtein distance between tokens and v) filtering
out those alignments from NULL to a stop word
(p = ??).
We empirically set the linear interpolation
weight as w = 0.60 for the baseline translation
models and w = 0.40 for the derived units trans-
lations models. We applied the pivot derived units
strategy to the News domain and to the filtered
Commoncrawl corpus (cf. Section 5). The proce-
dure to filter out the Commoncrawl corpus is ex-
plained next.
3.3 CommonCrawl Filtering
We used the CommonCrawl corpus, provided for
the first time by the organization, as an impor-
tant source of information for performing aggres-
sive domain adaptation. To decrease the impact
of the noise in the corpus, we performed an auto-
matic pre-selection of the supposedly more correct
(hence useful) sentence pairs: we applied the au-
tomatic quality estimation filters developed in the
context of the FAUST project3. The filters? pur-
pose is to identify cases in which the post-editions
provided by casual users really improve over auto-
matic translations.
The adaptation to the current framework is as
follows. Example selection is modelled as a bi-
nary classification problem. We consider triples
(src, ref , trans), where src and ref stand for the
source-reference sentences in the CommonCrawl
corpus and trans is an automatic translation of the
source, generated by our baseline SMT system. A
triple is assigned a positive label iff ref is a bet-
ter translation from src than trans. That is, if the
translation example provided by CommonCrawl is
better than the output of our baseline SMT system.
We used four feature sets to characterize the
three sentences and their relationships: sur-
face, back-translation, noise-based and similarity-
based. These features try to capture (a) the simi-
larity between the different texts on the basis of
3http://www.faust-fp7.eu
136
diverse measures, (b) the length of the different
sentences (including ratios), and (c) the likelihood
of a source or target text to include noisy text.4
Most of them are simple, fast-calculation and
language-independent features. However, back-
translation features require that trans and ref are
back-translated into the source language. We did
it by using the TALP es-en system from WMT12.
Considering these features, we trained lin-
ear Support Vector Machines using SVMlight
(Joachims, 1999). Our training collection was the
FFF+ corpus, with +500 hundred manually anno-
tated instances (Barro?n-Ceden?o et al, 2013). No
adaptation to CommonCrawl was performed. To
give an idea, classification accuracy over the test
partition of the FFF+ corpus was only moderately
good (?70%). However, ranking by classification
score a fresh set of over 6,000 new examples, and
selecting the top ranked 50% examples to enrich a
state-of-the-art SMT system, allowed us to signifi-
cantly improve translation quality (Barro?n-Ceden?o
et al, 2013).
For WMT13, we applied these classifiers to
rank the CommonCrawl translation pairs and then
selected the top 53% instances to be processed by
the domain adaptation strategy. Table 1 displays
the corpus statistics before and after filtering.
4 System Combination
We approached system combination as a system
selection task. More concretely, we applied Qual-
ity Estimation (QE) models (Specia et al, 2010;
Formiga et al, 2013) to select the highest qual-
ity translation at sentence level among the trans-
lation candidates obtained by our different strate-
gies. The QE models are trained with human
supervision, making use of no system-dependent
features.
In a previous study (Formiga et al, 2013),
we showed the plausibility of building reliable
system-independent QE models from human an-
notations. This type of task should be addressed
with a pairwise ranking strategy, as it yields bet-
ter results than an absolute quality estimation ap-
proach (i.e., regression) for system selection. We
also found that training the quality estimation
models from human assessments, instead of au-
tomatic reference scores, helped to obtain better
4We refer the interested reader to (Barro?n-Ceden?o et al,
2013) for a detailed description of features, process, and eval-
uation.
models for system selection for both i) mimicking
the behavior of automatic metrics and ii) learning
the human behavior when ranking different trans-
lation candidates.
For training the QE models we used the data
from the WMT13 shared task on quality estima-
tion (System Selection Quality Estimation at Sen-
tence Level task5), which contains the test sets
from other WMT campaigns with human assess-
ments. We used five groups of features, namely:
i) QuestQE: 17 QE features provided by the Quest
toolkit6; ii) AsiyaQE: 26 QE features provided by
the Asiya toolkit for MT evaluation (Gime?nez and
Ma`rquez, 2010a); iii) LM (and LM-PoS) perplex-
ities trained with monolingual data; iv) PR: Clas-
sical lexical-based measures -BLEU (Papineni et
al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Denkowski and Lavie, 2011)- computed
with a pseudo-reference approach, that is, using
the other system candidates as references (Sori-
cut and Echihabi, 2010); and v) PROTHER: Ref-
erence based metrics provided by Asiya, including
GTM, ROUGE, PER, TER (Snover et al, 2008),
and syntax-based evaluation measures also with a
pseudo-reference approach.
We trained a Support Vector Machine ranker by
means of pairwise comparison using the SVMlight
toolkit (Joachims, 1999), but with the ?-z p? pa-
rameter, which can provide system rankings for
all the members of different groups. The learner
algorithm was run according to the following pa-
rameters: linear kernel, expanding the working set
by 9 variables at each iteration, for a maximum of
50,000 iterations and with a cache size of 100 for
kernel evaluations. The trade-off parameter was
empirically set to 0.001.
Table 2 shows the contribution of different fea-
ture groups when training the QE models. For
evaluating performance, we used the Asiya nor-
malized linear combination metric ULC (Gime?nez
and Ma`rquez, 2010b), which combines BLEU,
NIST, and METEOR (with exact, paraphrases and
synonym variants). Within this scenario, it can
be observed that the quality estimation features
(QuestQE and AsiyaQE) did not obtain good re-
sults, perhaps because of the high similarity be-
tween the test candidates (Moses with different
configurations) in contrast to the strong differ-
ence between the candidates in training (Moses,
5http://www.quest.dcs.shef.ac.uk/wmt13 qe.html
6http://www.quest.dcs.shef.ac.uk
137
Features Asiya ULCWMT?11 WMT?12 AVG WMT?13
QuestQE 60.46 60.64 60.55 60.06
AsiyaQE 61.04 60.89 60.97 60.29
QuestQE+AsiyaQE 60.86 61.07 60.96 60.42
LM 60.84 60.63 60.74 60.37
QuestQE+AsiyaQE+LM 60.80 60.55 60.67 60.21
QuestQE+AsiyaQE+PR 60.97 61.12 61.05 60.54
QuestQE+AsiyaQE+PR+PROTHER 61.05 61.19 61.12 60.69
PR 61.24 61.08 61.16 61.04
PR+PROTHER 61.19 61.16 61.18 60.98
PR+PROTHER+LM 61.11 61.29 61.20 61.03
QuestQE+AsiyaQE+PR+PROTHER+LM 60.70 60.88 60.79 60.14
Table 2: System selection scores (ULC) obtained using QE models trained with different groups of
features. Results displayed for WMT11, WMT12 internal tests, their average, and the WMT13 test
EN?ES BLEU TER
wmt13 Primary 29.5 0.586
wmt13 Secondary 29.4 0.586
Table 4: Official automatic scores for the WMT13
English?Spanish translations.
RBMT, Jane, etc.). On the contrary, the pseudo-
reference-based features play a crucial role in the
proper performance of the QE model, confirming
the hypothesis that PR features need a clear dom-
inant system to be used as reference. The PR-
based configurations (with and without LM) had
no big differences between them. We choose the
best AVG result for the final system combination:
PR+PROTHER+LM, which it is consistent with
the actual WMT13 evaluated afterwards.
5 Results
Evaluations were performed considering different
quality measures: BLEU, NIST, TER, and ME-
TEOR in addition to an informal manual analy-
sis. This manifold of metrics evaluates distinct as-
pects of the translation. We evaluated both over
the WMT11 and WMT12 test sets as internal in-
dicators of our systems. We also give our perfor-
mance on the WMT13 test dataset.
Table 3 presents the obtained results for the
different strategies: (a) Moses Baseline (w/o
commoncrawl) (b) Moses Baseline+Morphology
Generation (w/o commoncrawl) (c) Moses Base-
line+News Adaptation through pivot based align-
ment (w/o commoncrawl) (d) Moses Baseline +
News Adaptation (b) + Morphology Generation
(c) (e) Moses Baseline + News Adaptation (b) +
Filtered CommonCrawl Adaptation.
The official results are in Table 4. Our primary
(contrastive) run is the system combination strat-
egy whereas our secondary run is the full training
strategy marked as (e) on the system combination.
Our primary system was ranked in the second clus-
ter out of ten constrained systems in the official
manual evaluation.
Independent analyzes of the improvement
strategies show that the highest improvement
comes from the CommonCrawl Filtering + Adap-
tation strategy (system e). The second best strat-
egy is the combination of the morphology pre-
diction system plus the news adaptation system.
However, for the WMT12 test the News Adap-
tation strategy contributes to main improvement
whereas for the WMT13 this major improvement
is achieved with the morphology strategy. Analyz-
ing the distance betweem each test set with respect
to the News and CommonCrawl domain to further
understand the behavior of each strategy seems an
interesting future work. Specifically, for further
contrasting the difference in the morphology ap-
proach, it would be nice to analyze the variation in
the verb inflection forms. Hypothetically, the per-
son or the number of the verb forms used may have
a higher tendency to be different in the WMT13
test set, implying that our morphology approach is
further exploited.
Regarding the system selection step (internal
WMT12 test), the only automatic metric that has
an improvement is TER. However, TER is one of
138
EN?ES BLEU NIST TER METEOR
wmt12 Baseline 32.97 8.27 49.27 49.91
wmt12 + Morphology Generation 33.03 8.29 49.02 50.01
wmt12 + News Adaptation 33.22 8.31 49.00 50.16
wmt12 + News Adaptation + Morphology Generation 33.29 8.32 48.83 50.29
wmt12 + News Adaptation + Filtered CommonCrawl Adaptation 33.61 8.35 48.82 50.52
wmt12 System Combination 33.43 8.34 48.78 50.44
wmt13 Baseline 29.02 7.72 51.92 46.96
wmt13 Morphology Generation 29.35 7.73 52.04 47.04
wmt13 News Adaptation 29.19 7.74 51.91 47.07
wmt13 News Adaptation + Morphology Generation 29.40 7.74 51.96 47.12
wmt13 News Adaptation + Filtered CommonCrawl Adaptation 29.47 7.77 51.82 47.22
wmt13 System Combination 29.54 7.77 51.76 47.34
Table 3: Automatic scores for English?Spanish translations.
the most reliable metrics according to human eval-
uation. Regarding the actual WMT13 test, the sys-
tem selection step is able to overcome all the auto-
matic metrics.
6 Conclusions and further work
This paper described the TALP-UPC participa-
tion for the English-to-Spanish WMT13 transla-
tion task. We applied the same systems as in last
year, but enhanced with new techniques: sentence
filtering and system combination.
Results showed that both approaches performed
better than the baseline system, being the sentence
filtering technique the one that most improvement
reached in terms of all the automatic quality indi-
cators: BLEU, NIST, TER, and METEOR. The
system combination was able to outperform the
independent systems which used morphological
knowledge and/or domain adaptation techniques.
As further work would like to focus on further
advancing on the morphology-based techniques.
Acknowledgments
This work has been supported in part by
Spanish Ministerio de Econom??a y Competitivi-
dad, contract TEC2012-38939-C03-02 as well
as from the European Regional Development
Fund (ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the fol-
lowing grants: 247762 (FAUST, FP7-ICT-2009-
4-247762), 29951 (the International Outgoing
Fellowship Marie Curie Action ? IMTraP-2011-
29951) and 246016 (ERCIM ?Alain Bensoussan?
Fellowship).
References
Alberto Barro?n-Ceden?o, Llu??s Ma`rquez, Carlos A.
Henr??quez Q, Llu??s Formiga, Enrique Romero, and
Jonathan May. 2013. Identifying Useful Hu-
man Correction Feedback from an On-line Machine
Translation Service. In Proceedings of the Twenty-
Third International Joint Conference on Artificial
Intelligence. AAAI Press.
Adria` de de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication, 50(11-12):1034?
1046.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012a. The TALP-UPC phrase-
based translation systems for WMT12: Morphol-
ogy simplification and domain adaptation. In Pro-
ceedings of the Seventh Workshop on Statistical
Machine Translation, pages 275?282, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Llu??s Formiga, Adolfo Herna?ndez, Jose? B. Marin?, and
Enrique Monte. 2012b. Improving english to
spanish out-of-domain translations by morphology
generalization and generation. In Proceedings of
139
the AMTA Monolingual Machine Translation-2012
Workshop.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 53?61, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
measures for automatic machine translation evalu-
ation. Machine Translation, 24(3-4):209?240, De-
cember.
Carlos A. Henr??quez Q., Jose? B. Marin?o, and Rafael E.
Banchs. 2011. Deriving translation units using
small additional corpora. In Proceedings of the 15th
Conference of the European Association for Ma-
chine Translation.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007a. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007b. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC 2010), La Val-
letta, MALTA, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
140
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 359?364,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Approach to System Selection: ASIYA Features
and Pairwise Classification using Random Forests
Llu??s Formiga1, Meritxell Gonza`lez1, Alberto Barro?n-Ceden?o1,2
Jose? A. R. Fonollosa1 and Llu??s Ma`rquez1
1 TALP Research Center, Universitat Polite`cnica de Catalunya, Spain
2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid, Spain
{lluis.formiga,jose.fonollosa}@upc.edu, {mgonzalez,albarron,lluism}@lsi.upc.edu
Abstract
This paper describes the TALP-UPC par-
ticipation in the WMT?13 Shared Task
on Quality Estimation (QE). Our partic-
ipation is reduced to task 1.2 on System
Selection. We used a broad set of fea-
tures (86 for German-to-English and 97
for English-to-Spanish) ranging from stan-
dard QE features to features based on
pseudo-references and semantic similarity.
We approached system selection by means
of pairwise ranking decisions. For that,
we learned Random Forest classifiers es-
pecially tailored for the problem. Evalua-
tion at development time showed consider-
ably good results in a cross-validation ex-
periment, with Kendall?s ? values around
0.30. The results on the test set dropped
significantly, raising different discussions
to be taken into account.
1 Introduction
In this paper we discuss the TALP-UPC1 partici-
pation in the WMT?13 Shared Task on Quality Es-
timation (QE). Our participation is circumscribed
to task 1.2, which deals with System Selection.
Concretely, we were required to rank up to five al-
ternative translations for the same source sentence
produced by multiple MT systems, in the absence
of any reference translation.
We used a broad set of features; mainly avail-
able through the last version of the ASIYA toolkit
for MT evaluation2 (Gime?nez and Ma`rquez,
2010). Concretely, we derived 86 features for
the German-to-English subtask and 97 features for
English-to-Spanish. These features cover different
approaches and include standard Quality Estima-
tion features, as provided by the above mentioned
1Center for Language and Speech Technologies and Ap-
plications (TALP), Technical University of Catalonia (UPC).
2http://asiya.lsi.upc.edu
ASIYA toolkit and Quest (Specia et al, 2010),
but also a variety of features based on pseudo-
references (Soricut and Echihabi, 2010), explicit
semantic analysis (Gabrilovich and Markovitch,
2007) and specialized language models. See sec-
tion 3 for details.
In order to model the ranking problem associ-
ated to the system selection task, we adapted it
to a classification task of pairwise decisions. We
trained Random Forest classifiers (and compared
them to SVM classifiers), expanding the work of
Formiga et al (2013), from which a full ranking
can be derived and the best system per sentence
identified.
Evaluation at development time, using cross-
validation, showed considerably good and stable
results for both language pairs, with correlation
values around 0.30 (Kendall ? coefficient) classi-
fication accuracies around 52% (pairwise classifi-
cation) and 41% (best translation identification).
Unfortunately, the results on the test set were sig-
nificantly lower. Current research is devoted to ex-
plain the behavior of the system at testing time. On
the one hand, it seems clear that more research re-
garding the assignment of ties is needed in order
to have a robust model. On the other hand, the re-
lease of the gold standard annotations for the test
set will facilitate a deeper analysis and understand-
ing of the current results.
The rest of the paper is organized as follows.
Section 2 describes the ranking models studied for
the system selection problem. Section 3 describes
the features used for learning. Section 4 presents
the setting for parameter optimization and feature
selection and the results obtained. Finally, Sec-
tion 5 summarizes the lessons learned so far and
outlines some lines for further research.
2 Ranking Model
We considered two learning strategies to obtain the
best translation ranking model: SVM and Random
359
Forests. Both strategies were based on predicting
pairwise quality ranking decisions by means of su-
pervised learning. These decision was motivated
from our previous work (Formiga et al, 2013)
were we learned that they were more consistent to
select the best system (according to human and au-
tomatic metrics) compared to absolute regression
approaches. In that work we used only the subset
of features 1, 2, 3 and 8 described in Section 3.
For this shared task we have introduced additional
similarity measures (subsets 4 to 7) that feature se-
mantic analysis and automatic alignments between
the source and the translations.
The rationale for transforming a ranking prob-
lem to a pairwise classification problem has been
described previously in several work (Joachims,
2002; Burges et al, 2005). The main idea is to en-
semble the features of both individuals and assign
a class {-1,1} which tries to predict the pairwise
relation among them. For linear based approach
this adaptation is as simple to compute the differ-
ence between features between all the pairs of the
training data.
We used two different learners to perform that
task. First, we trained a Support Vector Machine
ranker by means of pairwise comparison using
the SVMlight toolkit (Joachims, 1999), but with
the ?-z p? parameter, which can provide system
rankings for all the members of different groups.
The learner algorithm was run according to the
following parameters: RBF-kernel, expanding the
working set by 9 variables at each iteration, for a
maximum of 50,000 iterations and with a cache
size of 100 for kernel evaluations. The trade-off
parameter was empirically set to 0.001. This im-
plementation ignores the ties for the training step
as it only focuses in better than/ worse than rela-
tions.
Secondly, we used Random Forests (Breiman,
2001), the rationale was the same as ranking-to-
pairwise implementation from SVMlight. How-
ever, SVMlight considers two different data pre-
processing methods depending on the kernel of
the classifier: LINEAR and RBF-Kernel. We
used the same data-preprocessing algorithm from
SVMlight in order to train a Random Forest clas-
sifier with ties (three classes: {0,-1,1}) based
upon the pairwise relations. We used the Random
Forests implementation of scikit-learn toolkit (Pe-
dregosa et al, 2011) with 50 estimators.
Once the classes are given by the Random For-
est, we build a graph by means of the adjacency
matrix of the pairwise decision. Once the adja-
cency matrix has been built, we assign the final
ranking through a dominance scheme similar to
Pighin et al (2012). In that case, however, there
are not topological problems as the pairwise rela-
tions are complete across all the edges.
3 Features Sets
We considered a broad set of features: 97 and
86 features for English-to-Spanish (en-es) and
German-to-English (de-en), respectively. We
grouped them into the following categories: base-
line QE metrics, comparison against pseudo-
references, source-translation, and adapted lan-
guage models. We describe them below. Unless
noted otherwise, the features apply to both lan-
guage pairs.
3.1 Baseline Features
The baseline features are composed of well-known
quality estimation metrics:
1. Quest Baseline (QQE)
Seventeen baseline features from Specia et
al. (2010). This set includes token counts
(and their ratio), LM probabilities for source
and target sentences, percentage of n-grams
in different quartiles of a reference corpus,
number of punctuation marks, and fertility
ratios. We used these features in the en-es
partition only.
2. ASIYA?s QE-based features (AQE)
Twenty-six QE features provided by
ASIYA (Gonza`lez et al, 2012), comprising
bilingual dictionary ambiguity and overlap;
ratios concerning chunks, named-entities and
PoS; source and candidate LM perplexities
and inverse perplexities over lexical forms,
chunks and PoS; and out-of-vocabulary word
indicators.
3.2 Pseudo-Reference-based Features
Soricut and Echihabi (2010) introduced the con-
cept of pseudo-reference-based features (PR) for
translation ranking estimation. The principle is
that, in the lack of human-produced references,
automatic ones are still good for differentiating
good from bad translations. One or more sec-
ondary MT systems are required to generate trans-
lations starting from the same input, which are
360
taken as pseudo-references. The similarity to-
wards the pseudo-references can be calculated
with any evaluation measure or text similarity
function, which gives us all feature variants in this
group. We consider the following PR-based fea-
tures:
3. Derived from ASIYA?s metrics (APR)
Twenty-three PR features, including GTM-l
(l?{1,2,3}) to reward different length match-
ing (Melamed et al, 2003), four variants of
ROUGE (-L, -S*, -SU* and -W) (Lin and
Och, 2004), WER (Nie?en et al, 2000),
PER (Tillmann et al, 1997), TER, and
TERbase (i.e., without stemming, synonymy
look-up, nor paraphrase support) (Snover et
al., 2009), and all the shallow and full pars-
ing measures (i.e., constituency and depen-
dency parsing, PoS, chunking and lemmas)
that ASIYA provides either for Spanish or En-
glish as target languages.
4. Lexical similarity (NGM)
Cosine and Jaccard coefficient similarity
measures for both token and character
n-grams considering n ? [2, 5] (i.e., sixteen
features). Additionally, one Jaccard-based
similarity measure for ?pseudo-prefixes?
(considering only up to four initial characters
for every token).
5. Based on semantic information (SEM)
Twelve features calculated with named
entity- and semantic role-based evaluation
measures (again, provided by ASIYA). Sen-
tences are automatically annotated using
SwiRL (Surdeanu and Turmo, 2005) and
BIOS (Surdeanu et al, 2005). We used these
features in the de-en subtask only.
6. Explicit semantic analysis (ESA)
Two versions of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), a
semantic similarity measure, built on top of
Wikipedia (we used the opening paragraphs
of 100k Wikipedia articles as in 2010).
3.3 Source-Translation Extra Features
Source-translation features include explicit com-
parisons between the source sentence and its trans-
lation. They are meant to measure how adequate
the translation is, that is, to what extent the trans-
lation expresses the same meaning as the source.
Note that a considerable amount of the features
described in the baseline group (QQE and AQE)
fall in this category. In this subsection we include
some extra features we devised to capture source?
translation dependencies.
7. Alignment-based features (ALG / ALGPR)
One measure calculated over the aligned
words between a candidate translation and
the source (ALG); and two measures based on
the comparison between these alignments for
two different translations (e.g., candidate and
pseudo-reference) and the source (ALGPR).3
8. Length model (LeM)
A measure to estimate the quality likeli-
hood of a candidate sentence by considering
the ?expected length? of a proper translation
from the source. The measure was introduced
by (Pouliquen et al, 2003) to identify docu-
ment translations. We estimated its param-
eters over standard MT corpora, including
Europarl, Newswire, Newscommentary and
UN.
3.4 Adapted Language-Model Features
We interpolated different language models com-
prising the WMT?12 Monolingual corpora (EPPS,
News, UN and Gigafrench for English). The in-
terpolation weights were computed as to minimize
the perplexity according to the WMT Translation
Task test data (2008-2010)4. The features are as
follow:
9. Language Model Features (LM)
Two log-probabilities of the translation can-
didate with respect to the above described in-
terpolated language models over word forms
and PoS labels.
4 Experiments and Results
In this section we describe the experiments car-
ried out to select the best feature set, learner, and
learner configuration. Additionally, we present
the final performance within the task. The set-
up experiments were addressed doing two separate
10-fold cross validations on the training data and
averaging the final results. We evaluated the re-
sults through three indicators: Kendall?s ? with no
3Alignments were computed with the Berkeley aligner
https://code.google.com/p/berkeleyaligner/
4http://www.statmt.org/wmt13/translation-task.html
361
penalization for the ties, accuracy in determining
the pairwise relationship between candidate trans-
lations, and global accuracy in selecting the best
candidate for each source sentence.
First, we compared our SVM learner against
Random Forests with the two variants of data
preprocessing (LINEAR and RBF). In terms of
Kendall?s ? , we found that the Random Forests
(RF) were clearly better compared to SVM imple-
mentation. Concretely, depending on the final fea-
ture set, we found that RF achieved a ? between
0.23 and 0.29 while SVM achieved a ? between
0.23 and 0.25. With respect to the accuracy mea-
sures we did not find noticeable differences be-
tween methods as their results moved from 49% to
52%. However, considering the accuracy in terms
of selecting only the best system there was a dif-
ference of two points (42.2% vs. 40.0%) between
methods, being RF again the best system. Regard-
ing the pairwise preprocessing the results between
RBF and LINEAR based preprocessing were com-
parable, being RBF slightly better than LINEAR.
Hence, we selected Random Forests with RBF
pairwise preprocessing as our final learner.
de-en ? with ties AccuracyIgnored Penalized All Best
AQE+LeM+ALGPR+LM 33.70 15.72 52.56 41.57
AQE+SEM+LM 32.49 14.61 52.72 40.92
AQE+LeM+ALGPR+ESA+LM 32.08 13.81 52.71 41.37
AQE+ALG+ESA+SEM+LM 32.06 13.96 52.20 40.64
AQE+ALG+LM 31.97 14.29 52.00 40.83
AQE+LeM+ALGPR+SEM+LM 31.93 13.57 52.52 40.98
AQE+ESA+SEM+LM 31.79 13.68 52.50 40.76
AQE+LeM+ALGPR+ESA+SEM+LM 31.72 14.01 52.65 40.83
AQE+ALG+SEM+LM 31.17 12.86 52.18 40.51
AQE+ALG+SEM 30.72 12.58 51.75 39.66
AQE+LeM+ALGPR+ESA+SEM 30.47 11.79 51.85 39.58
AQE+ESA+LM 30.31 12.23 52.60 40.69
AQE+ALG+ESA+LM 30.26 12.40 52.03 40.99
AQE+LeM+ALGPR 30.24 11.83 51.96 40.42
AQE+LeM+ALGPR+SEM 30.23 11.84 52.10 40.32
AQE+LeM+ALGPR+ESA 29.89 11.87 51.83 40.07
AQE+ALG+ESA 29.81 11.30 51.37 39.47
AQE+SEM 29.80 12.06 51.75 39.52
AQE+NGM+APR+ESA+SEM+LM 29.34 10.58 51.33 38.55
AQE+ESA+SEM 29.31 11.46 51.66 39.24
AQE+ESA 29.13 11.12 51.82 39.90
AQE+ALG+ESA+SEM 28.35 10.32 51.37 38.98
AQE+NGM+APR+ESA+SEM 27.55 9.22 51.01 38.12
Table 1: Set-up results for de-en
For the feature selection process, we considered
the most relevant combinations of feature groups.
Table 1 shows the set-up results for the de-en sub-
task and Table 2 shows the results for the en-es
subtask.
In terms of ? we observed similar results be-
tween the two language pairs. However accura-
cies for the de-en subtask were one point above
the ones for en-es. Regarding the features used, we
found that the best feature combination to use was
composed of: i) a baseline QE feature set (Asiya
or Quest) but not both of them, ii) Length Model,
iii) Pseudo-reference aligned based features and
the use of iv) adapted language models. However,
within the de-en subtask, we found that substitut-
ing Length Model and Aligned Pseudo-references
by the features based on Semantic Roles (SEM)
could bring marginally better accuracy. We also
noticed that the learner was sensitive to the fea-
tures used so selecting the appropriate set of fea-
tures was crucial to achieve a good performance.
en-es ? with ties AccuracyIgnored Penalized All Best
QQE+LeM+ALGPR+LM 33.81 15.87 51.66 41.01
AQE+LeM+ALGPR+LM 33.75 16.44 51.56 41.52
QQE+AQE+LM 32.71 14.59 51.18 41.02
QQE+AQE+LM+ESA 32.69 15.30 51.48 41.30
QQE+AQE+LeM+ALGPR+LM+ESA 32.63 13.64 51.39 40.48
QQE+AQE+LeM+ALGPR+LM 32.41 14.06 51.43 40.49
QQE+LeM+ALGPR+LM+ESA 31.66 13.39 51.37 41.05
QQE+AQE+ALG+LM 31.46 13.62 51.28 41.29
AQE+LeM+ALGPR+LM+ESA 31.29 14.10 51.55 41.43
QQE+AQE+ALG+LM+ESA 31.25 13.58 51.64 41.66
QQE+AQE+NGM+APR+LM+ESA 30.58 12.48 50.93 40.66
QQE+AQE+NGM+APR+LM 29.94 12.54 50.95 40.25
QQE+AQE 28.98 10.92 49.97 39.65
QQE+AQE+LeM+ALGPR 28.94 10.48 49.99 39.71
QQE+AQE+NGM+ESA+LM 28.85 11.88 50.90 40.22
AQE+LeM+ALGPR 28.81 10.11 50.06 40.01
QQE+AQE+ESA 28.68 10.31 49.96 39.27
AQE+ESA 28.67 10.81 50.35 39.18
AQE 28.65 10.68 49.76 38.90
QQE+AQE+ALG 28.47 9.63 49.67 39.66
QQE+AQE+NGM+APR+ESA 28.43 9.75 49.67 38.74
QQE+AQE+NGM 27.23 9.10 49.44 38.98
QQE+AQE+ALG+ESA 27.08 7.93 50.26 39.71
QQE+AQE+LeM+ALGPR+ESA 27.03 8.65 50.35 40.49
AQE+LeM+ALGPR+ESA 26.96 8.26 50.30 39.47
QQE+AQE+NGM+ESA 26.59 7.56 49.52 38.62
QQE+AQE+NGM+APR 25.39 6.97 49.90 39.53
Table 2: Setup results for en-es
de-en ? (ties penalized,
ID non-symmetric between [-1,1])
Best 0.31
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
Baseline Random-ranks-with-ties -0.12
Worst -0.49
Table 3: Official results for the de-en subtask (ties
penalized)
en-es ? (ties penalized,
ID non-symmetric between [-1,1])
Best 0.15
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
Baseline Random-ranks-with-ties -0.23
Worst -0.63
Table 4: Official results for the en-es subtask (ties
penalized)
In Tables 3, 4, 5 and 6 we present the official re-
sults for the WMT?13 Quality Estimation Task, in
all evaluation variants. In each table we compare
to the best/worst performing systems and also to
the official baseline.
We can observe that in general the results on
the test sets drop significantly, compared to our
362
de-en ? (ties ignored, Non-ties
ID symmetric /between [-1,1]) (882 dec.)
Best 0.31 882
UPC AQE+SEM+LM 0.27 768
UPC AQE+LeM+ALGPR+LM 0.24 788
Baseline Random-ranks-with-ties 0.08 718
Worst -0.03 558
Table 5: Official results for the de-en subtask (ties
ignored)
en-es ? (ties ignored, Non-ties
ID symmetric /between [-1,1]) (882 dec.)
Best 0.23 192
UPC QQE+LeM+ALGPR+LM 0.11 554
UPC AQE+LeM+ALGPR+LM 0.08 554
Baseline Random-ranks-with-ties 0.03 507
Worst -0.11 633
Table 6: Official results for the en-es subtask (ties
ignored)
set-up experiments. Restricting to the evaluation
setting in which ties are not penalized (i.e., cor-
responding to our setting during system and pa-
rameter tuning), we can see that the results corre-
sponding to de-en (Table 5) are comparable to our
set-up results and close to the best performing sys-
tem. However, in the en-es language pair the final
results are comparatively much lower (Table 6).
We find this behavior strange. In this respect, we
analyzed the inter-annotator agreement within the
gold standard. Concretely we computed the Co-
hen?s ? for all overlapping annotations concerning
at least 4 systems for both language pairs. The re-
sults of our analysis are presented in Table 7 and
therefore it confirms our hypothesis that en-es an-
notations had more noise providing an explanation
for the accuracy decrease of our QE models and
setting the subtask into a more challenging sce-
nario. However, further research will be needed to
analyze other factors such as oracles and improve-
ment on automatic metrics prediction and reliabil-
ity compared to linguistic expert annotators.
Another remaining issue for our research con-
cerns investigating better ways to deal with ties,
as their penalization lowered our results dramati-
cally. In this direction we plan to work further on
# of Lang Cohen?s # ofsystems ? elements
4 en-es 0.210 560de-en 0.369 640
5 en-es 0.211 130de-en 0.375 145
Table 7: Golden standard test set agreement coef-
ficients measured by Cohen?s ?
the adjacency matrix reconstruction heuristics and
presenting the features to the learner in a struc-
tured form.
5 Conclusions
This paper described the TALP-UPC participation
in the WMT?13 Shared Task. We approached the
Quality Estimation task based on system selection,
where different systems have to be ranked accord-
ing to their quality. We derive a full ranking and
identify the best system per sentence on the basis
of Random Forest classifiers.
After the model set-up, we observed consid-
erably good and robust results for both transla-
tion directions, German-to-English and English-
to-Spanish: Kendall?s ? around 0.30 as well as
accuracies around 52% on pairwise classification
and 41% on best translation identification. How-
ever, the results over the official test set were
significantly lower. We have found that the low
inter-annotator agreement between users on that
set might provide an explanation to the poor per-
formance of our QE models.
Our current efforts are centered on explaining
the behavior of our QE models when facing the of-
ficial test sets. We are following two directions: i)
studying the ties? impact to come out with a more
robust model and ii) revise the English-to-Spanish
gold standard annotations in terms of correlation
with automatic metrics to facilitate a deeper un-
derstanding of the results.
Acknowledgments
Acknowledgements
This work has been partially funded by the
Spanish Ministerio de Econom??a y Competitivi-
dad, under contracts TEC2012-38939-C03-02
and TIN2009-14675-C03, as well as from
the European Regional Development Fund
(ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the
following grants: 247762 (FAUST, FP7-ICT-
2009-4-247762) and 246016 (ERCIM ?Alain
Bensoussan? Fellowship).
References
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
363
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd international conference on
Machine learning, pages 89?96. ACM.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of the 20th International Joint Conference on Artifi-
cial Intelligence, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Meritxell Gonza`lez, Jesu?s Gime?nez, and Llu??s
Ma`rquez. 2012. A graphical interface for mt evalu-
ation and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In ACM, editor, Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD).
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 605?612, Barcelona,
Spain, July.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and recall of machine translation.
In HLT-NAACL.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of the 2nd Language Resources and
Evaluation Conference (LREC 2000).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python . Journal of Machine Learning Re-
search, 12:2825?2830.
Daniele Pighin, Llu??s Formiga, and Llu??s Ma`rquez.
2012. A graph-based strategy to streamline trans-
lation quality assessments. In Proceedings of the
Tenth Conference of the Association for Machine
Translation in the Americas (AMTA?2012), San
Diego, USA, October. AMTA.
Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat.
2003. Automatic Identification of Document Trans-
lations in Large Multilingual Document Collections.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), pages 401?408, Borovets, Bulgaria.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: Paraphrase,
Semantic, and Alignment Enhancements to Trans-
lation Edit Rate. Machine Translation, 23(2):117?
127.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic
Role Labeling Using Complete Syntactic Analysis.
In Proceedings of CoNLL Shared Task.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proceedings of the 9th Inter-
national Conference on Speech Communication and
Technology (Interspeech).
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H Sawaf. 1997. Accelerated dp based search for
statistical translation. In Proceedings of European
Conference on Speech Communication and Technol-
ogy.
364
