Improved Cross-Language Retrieval
using Backoff Translation
Philip Resnik,1;2 Douglas Oard,2;3 and Gina Levow2
Department of Linguistics,1
Institute for Advanced Computer Studies,2
College of Information Studies,3
University of Maryland
College Park, MD 20742
fresnik,ginag@umiacs.umd.edu, oard@glue.umd.edu
ABSTRACT
The limited coverage of available translation lexicons can pose a se-
rious challenge in some cross-language information retrieval appli-
cations. We present two techniques for combining evidence from
dictionary-based and corpus-based translation lexicons, and show
that backoff translation outperforms a technique based on merging
lexicons.
1. INTRODUCTION
The effectiveness of a broad class of cross-language information
retrieval (CLIR) techniques that are based on term-by-term transla-
tion depends on the coverage and accuracy of the available trans-
lation lexicon(s). Two types of translation lexicons are commonly
used, one based on translation knowledge extracted from bilingual
dictionaries [1] and the other based on translation knowledge ex-
tracted from bilingual corpora [8]. Dictionaries provide reliable ev-
idence, but often lack translation preference information. Corpora,
by contrast, are often a better source for translations of slang or newly
coined terms, but the statistical analysis through which the trans-
lations are extracted sometimes produces erroneous results. In this
paper we explore the question of how best to combine evidencefrom
these two sources.
2. TRANSLATION LEXICONS
Our term-by-term translation technique (described below) requires
a translation lexicon (henceforth tralex) in which each word f is as-
sociated with a ranked set fe
1
; e
2
; : : : e
n
g of translations. We used
two translation lexicons in our experiments.
2.1 WebDict Tralex
We downloadeda freely available, manually constructedEnglish-
French term list from the Web1 and inverted it to French-English
1http://www.freedict.com
.
format. Since the WebDict translations appear in no particular or-
der, we ranked the e
i
based on target language unigram statistics
calculated over a large comparable corpus, the English portion of
the Cross-LanguageEvaluation Forum (CLEF) collection, smoothed
with statistics from the Brown corpus, a balanced corpus covering
many genres of English. All single-word translations are ordered by
decreasing unigram frequency, followed by all multi-word transla-
tions, and finally by any single-word entries not found in either cor-
pus. This ordering has the effect of minimizing the effect of infre-
quent words in non-standard usages or of misspellings that some-
times appear in bilingual term lists.
2.2 STRAND Tralex
Our second lexical resource is a translation lexicon obtained fully
automatically via analysisof parallel French-Englishdocuments from
the Web. A collection of 3,378 document pairs was obtained using
STRAND, our technique for mining the Web for bilingual text [7].
These document pairs were aligned internally, using their HTML
markup, to produce 63,094 aligned text ?chunks? ranging in length
from 2 to 30 words, 8 words on average per chunk, for a total of
500K words per side. Viterbi word-alignments for these paired
chunks were obtained using the GIZA implementation of the IBM
statistical translation models.2 An ordered set of translation pairs
was obtained by treating each alignment link between words as a
co-occurrence and scoring each word pair according to the likeli-
hood ratio [2]. We then rank the translation alternatives in order of
decreasing likelihood ratio score.
3. CLIR EXPERIMENTS
Ranked tralexes are particularly well suited to a simple ranked
term-by-term translation approach. In our experiments, we use top-
2 balanced document translation, in which we produce exactly two
English terms for each French term. For terms with no known trans-
lation, the untranslated French term is generated twice (often appro-
priate for proper names). For French terms with one translation, that
translation is generated twice. For French terms with two or more
translations, we generate the first two translations in the tralex. Thus
balanced translation has the effect of introducing a uniform weight-
ing over the top n translations for each term (here n = 2).
Benefits of the approachinclude simplicity and modularity ? no-
tice that a lexicon containing ranked translations is the only require-
ment, and in particular that there is no need for access to the in-
ternals of the IR system or to the document collection in order to
2http://www.clsp.jhu.edu/ws99/projects/mt/
perform computations on term frequencies or weights. In addition,
the approach is an effective one: in previous experiments we have
found that this balancedtranslation strategy significantly outperforms
the usual (unbalanced) technique of including all known translations [3].
We have also investigated the relationship between balanced trans-
lation and Pirkola?s structured query formulation method [6].
For our experiments we used the CLEF-2000 French document
collection (approximately 21 million words from articles in Le Monde).
Differences in use of diacritics, case, and punctuation can inhibit
matching between tralex entries and document terms, so we normal-
ize the tralex and the documents by converting characters to low-
ercase and removing all diacritic marks and punctuation. We then
translate the documents using the process described above, index
the translated documentswith the Inquery information retrieval sys-
tem, and perform retrieval using ?long? queries formulated by group-
ing all terms in the title, narrative, and description fields of each
English topic description using Inquery?s #sum operator. We report
mean average precision on the 34 topics for which relevant French
documentsexist, basedon the relevancejudgments provided by CLEF.
We evaluated several strategies for using the WebDict and STRAND
tralexes.
3.1 WebDict Tralex
Since a tralex may contain an eclectic mix of root forms and mor-
phological variants, we use a four-stage backoff strategy to maxi-
mize coverage while limiting spurious translations:
1. Match the surface form of a document term to surface forms
of French terms in the tralex.
2. Match the stem of a document term to surface forms of French
terms in the tralex.
3. Match the surface form of a document term to stems of French
terms in the tralex.
4. Match the stem of a document term to stems of French terms in
the tralex.
We used unsupervisedinduction of stemming rules basedon the French
collection to build the stemmer [5]. The process terminates as soon
as a match is found at any stage, and the known translations for that
match are generated. The process may produce an inappropriate
morphological variant for a correct English translation, so we used
Inquery?s English kstem stemmer at indexing time to minimize the
effect of that factor on retrieval effectiveness.
3.2 STRAND Tralex
One limitation of a statistically derived tralex is that any term has
some probability of aligning with any other term. Merely sorting
translation alternatives in order of decreasing likelihood ratio will
thus find some translation alternatives for every French term that ap-
peared at least once in the set of parallel Web pages. In order to limit
the introduction of spurious translations, we included only transla-
tion pairs with at least N co-occurrences in the set used to build the
tralex. We performed runs with N = 1; 2; 3, using the four-stage
backoff strategy described above.
3.3 WebDict Merging using STRAND
When two sources of evidence with different characteristics are
available, a combination-of-evidence strategy can sometimes out-
perform either source alone. Our initial experiments indicated that
the WebDict tralex was the better of the two (see below), so we adopted
a reranking strategy in which the WebDict tralex was refined ac-
cording a voting strategy to which both the original WebDict and
STRAND tralex rankings contributed.
Condition MAP
STRAND (N = 1) 0.2320
STRAND (N = 2) 0.2440
STRAND (N = 3) 0.2499
Merging 0.2892
WebDict 0.2919
Backoff 0.3282
Table 1: Mean Average Precision (MAP), averaged over 34 top-
ics
For each French term that appeared in both tralexes, we gave the
top-ranked translation in each tralex a score of 100, the next a score
of 99, and so on. We then summed the WebDict and STRAND scores
for each translation, reranked the WebDict translations based on that
sum, and then appendedany STRAND-only translations for that French
term. Thus, although both sourcesof evidence were weighted equally
in the voting, STRAND-only evidence received lower precedence
in the merged ranking. For French terms that appeared in only one
tralex, we included those entries unchangedin the merged tralex. In
this experiment run we used a threshold of N = 1, and applied the
four-stage backoff strategy described above to the merged resource.
3.4 WebDict Backoff to STRAND
A possibleweaknessof our merging strategy is that inflected forms
are more common in our STRAND tralex, while root forms are more
common in our WebDict tralex. STRAND tralex entries that were
copied unchangedinto the merged tralex thus often matched in step
1 of the four-stage backoff strategy, preventing WebDict contribu-
tions from being used. With the WebDict tralex outperforming the
STRAND tralex, this factor could hurt our results. As an alterna-
tive to merging, therefore, we also tried a simple backoff strategy in
which we used the original WebDict tralex with the four-stage back-
off strategy described above, to which we added a fifth stage in the
event that fewer than two WebDict tralex matches were found:
5. Match the surface form of a document term to surface forms
of French terms in the STRAND tralex.
We used a threshold of N = 2 for this experiment run.
4. RESULTS
Table 1 summarizes our results. Increasing thresholds seem to
be helpful with the STRAND tralex, although the differences were
not found to be statistically significant by a paired two-tailed t-test
with p < 0:05. Merging the tralexes provided no improvement
over using the WebDict tralex alone, but our backoff strategy pro-
duced a statistically significant 12% improvement in mean average
precision (at p < 0:01) over the next best tralex (WebDict alone).
As Figure 1 shows, the improvement is remarkably consistent, with
only four of the 34 topics adverselyaffected and only one topic show-
ing a substantial negative impact.
Breaking down the backoff results by stage (Table 2), we find
that the majority of query-to-document hits are obtained in the first
stage, i.e. matches of the term?s surface form in the document to a
translation of the surface form in the dictionary. However, the back-
off process improves by-token coverage of terms in documents by
8%, and gives a 3% relative improvement in retrieval results; it also
contributed additional translations to the top-2 set in approximately
30% of the cases, leading to the statistically significant 12% relative
improvement in mean averageprecision as compared to the baseline
using WebDict alone with 4-stage backoff.
Figure 1: WebDict-to-tralex backoff vs. WebDict alone, by
query
Stage (forms) Lexicon matches
1 (surface-surface) 70.38%
2 (stem-surface) 3.18%
3 (surface-stem) 0.46%
4 (stem-stem) 0.98%
5 (STRAND) 8.34%
No match found 16.66%
Table 2: Term matches in 5-stage backoff
5. CONCLUSIONS
There are many ways of combining evidence from multiple trans-
lation lexicons. We use tralexes similar to those usedby Nie et al [4],
but our work differs in our use of balanced translation and a back-
off translation strategy (which produces a stronger baseline for our
WebDict tralex), and in our comparisonof merging and backoff trans-
lation strategies for combining resources. In future work we plan to
explore other combinations of merging and backoff and other merg-
ing strategies, including post-retrieval merging of the ranked lists.
In addition, parallel corpora can be exploited for more than just
the extraction of a non-contextualized translation lexicon. We are
currently engagedin work on lexical selection methods that take ad-
vantage of contextual information, in the context of our research on
machine translation, and we expect that CLIR results will be im-
proved by contextually-informed scoring of term translations.
6. ACKNOWLEDGMENTS
This research was supported in part by Department of Defense
contract MDA90496C1250 and TIDES DARPA/ITO Cooperative
Agreement N660010028910,
7. REFERENCES
[1] L. Ballesteros and W. B. Croft. Resolving ambiguity for
cross-language retrieval. In W. B. Croft, A. Moffat, and C. V.
Rijsbergen, editors, Proceedings of the 21st Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 64?71. ACM
Press, Aug. 1998.
[2] T. Dunning. Accurate methods for the statistics of surprise and
coincidence. Computational Linguistics, 19(1):61?74, March
1993.
[3] G.-A. Levow and D. W. Oard. Translingual topic tracking
with PRISE. In Working Notes of the Third Topic Detection
and Tracking Workshop, Feb. 2000.
[4] J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand.
Cross-language information retrieval based on parallel texts
and automatic mining of parallel texts from the web. In
M. Hearst, F. Gey, and R. Tong, editors, Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages
74?81, Aug. 1999.
[5] D. W. Oard, G.-A. Levow, and C. I. Cabezas. CLEF
experiments at Maryland: Statistical stemming and backoff
translation. In C. Peters, editor, Proceedings of the First
Cross-Language Evaluation Forum. 2001. To appear.
http://www.glue.umd.edu/oard/research.html.
[6] D. W. Oard and J. Wang. NTCIR-2 ECIR experiments at
Maryland: Comparing structured queries and balanced
translation. In Second National Institute of Informatics (NII)
Test Collection Information Retrieval (NTCIR) workshop.
forthcoming.
[7] P. Resnik. Mining the Web for bilingual text. In 37th Annual
Meeting of the Association for Computational Linguistics
(ACL?99), College Park, Maryland, June 1999.
[8] P. Sheridan and J. P. Ballerini. Experiments in multilingual
information retrieval using the SPIDER system. In
Proceedings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, Aug. 1996.
	

		
	
Rapidly Retargetable Interactive Translingual Retrieval
Gina-Anne Levow
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
gina@umiacs.umd.edu
Douglas W. Oard
College of Information Studies
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
oard@glue.umd.edu
Philip Resnik
Department of Linguistics
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
resnik@umiacs.umd.edu
ABSTRACT
This paper describes a system for rapidly retargetable interactive
translingual retrieval. Basic functionality can be achieved for a new
document language in a single day, and further improvements re-
quire only a relatively modest additional investment. We applied
the techniquesfirst to searchChinese collections using English queries,
and have successfully added French, German, and Italian document
collections. We achievethis capability through separation of language-
dependent and language-independent components and through the
application of asymmetric techniques that leverage an extensiveEn-
glish retrieval infrastructure.
Keywords
Cross-language information retrieval
1. INTRODUCTION
Our goal is to producesystems that allow interactive users to present
English queries and retrieve documents in languages that they can-
not read. In this paper we focus on what we call ?rapid retargetabil-
ity?: extending interactive translingual retrieval functionality for a
new document languagerapidly with few language-specificresources.
Our current system can be retargeted to a new language in one day
with only one language-dependent resource: a bilingual term list.1
Our language-independent architecture consists of two main com-
ponents:
1. Document translation and indexing
2. Interactive retrieval
We describe each of these components, demonstrate their effective-
ness for information retrieval tasks, and then concludeby describing
our experience with adding French, German and Italian document
collections to a system that was originally developed for Chinese.
1For Asian languages we also use a language-specificsegmentation
system.
.
2. DOCUMENT TRANSLATION AND IN-
DEXING
We have adopted a document translation architecture for two rea-
sons. First, we support a single query language (English) but multi-
ple document languages, so indexing English terms simplifies query
processing (where interactive response time can be a concern). Sec-
ond, a document translation architecture simplifies the display of
translated documents by decoupling the translation and display pro-
cesses. Gigabyte collections require machine translation that is or-
ders of magnitude faster than present commercial systems. We ac-
complish this using term-by-term translation, in which the basic data
structure is a simple hash table lookup. Any translation requires
some source of translation knowledge?we use a bilingual term list
containing English translation(s) for eachforeign language term. We
typically construct these term lists by harvesting Internet-available
translation resources, so the foreign language terms for which trans-
lations are known are typically an eclectic mix of root and inflected
forms. We accommodate this limitation using a four-stage backoff
statistical stemming approach to enhance translation coverage.
2.1 Preprocessing.
Differences in use of diacritic-s, case, and punctuation can inhibit
matching between term list entries and document terms, so normal-
ization is important. In order to maximize the probability of match-
ing document words with term list entries, we normalize the bilin-
gual term list and the documents by:
 converting characters in Western languages to lowercase,
 removing all accents and diacritics, and
 segmentation, which for Western languages merely involves
separating punctuation from other text by the addition of white
space.
Our preprocessingalso includes conversion of the bilingual term list
and the document collection into standard formats. The preprocess-
ing typically requires about half a day of programmer time.
2.2 Four-Stage Backoff Translation.
Bilingual term lists found on the Web often contain an eclectic
mix of root forms and morphological variants. We thus developed
a four-stage backoff strategy to maximize coverage while limiting
spurious translations:
1. Match the surface form of a document term to surface forms
of source language terms in the bilingual term list.
2. Match the stem of a document term to surfaceforms of source
language terms in the bilingual term list.
3. Match the surface form of a document term to stems of source
language terms in the bilingual term list.
4. Match the stem of a document term to stems of source lan-
guage terms in the bilingual term list.
The process terminates as soon as a match is found at any stage, and
the known translations for that match are generated. Although this
may produce an inappropriate morphological variant for a correct
English translation, use of English stemming at indexing time mini-
mizes the effect of that factor on retrieval effectiveness. Becausewe
are ultimately interested in processing documents in any language,
we may not have a hand-crafted stemmer available for the document
language. We have thus explored the application of rule induction
to learn stemming rules in an unsupervised fashion from the collec-
tion that is being indexed [2].
2.3 Balanced Top-2 Translation.
We produce exactly two English terms for each foreign-language
term. For terms with no known translation, the untranslated term is
generated twice (often appropriate for proper names in the Latin-
1 character set). For terms with one translation, that translation is
generated twice. For terms with two or more known translations,
we generate the ?best? two translations. In prior experiments we
have found that this balanced translation strategy significantly out-
performs the usual (unbalanced) technique of including all known
translations [1]. We establish the ?best? translations by sorting the
bilingual term list in advanceusing only English resources. All single-
word translations are ordered by decreasing unigram frequency in
the Brown corpus, followed by all multi-word translations, and fi-
nally by any single word entries not found in the Brown corpus.
This ordering has the effect of minimizing the effect of infrequent
words in non-standard usages or of misspellings that sometimes ap-
pear in bilingual term lists. This translation strategy allows balanc-
ing of translations in a modular fashion, even when one does not
have access to the internal parameters of the information retrieval
system. We translate  100 MB per hour using Perl on a SPARC
Ultra 5.
2.4 Post-translation Document Expansion.
We implement post-translation document expansion for the for-
eign language stories after translation into English in order to en-
rich the indexing vocabulary beyond that which was available af-
ter term-by-term translation. This is analogous to the process that
Singhal et al applied to monolingual speech retrieval [4].
Term-by-term translation producesa set of English terms that serve
as a noisy representation of the original source language document.
These terms are then treated as a query to a comparable English col-
lection, typically contemporaneous newswire text, from which we
retrieve the five highest ranked documents. From those five docu-
ments, we extract the most selective terms and use them to enrich
the original translations of the documents. For this expansion pro-
cess we select one instance of every term with an IDF value above
an ad hoc threshold that was tuned to yield approximately 50 new
terms. This optional step is the slowest processing stage, with a
throughput of about 20 MB per hour.
2.5 Indexing
The resulting collection is then indexed using Inquery (version
3.1p1), with the kstem stemmer and default English stopword list.
Indexing is the fastest stage in the process, with throughput exceed-
ing one gigabyte per hour.
3. INTERACTIVE RETRIEVAL
Interactive searches are performed using a Web interface. Sum-
mary information for the top-ranked documents is displayedin groups
of ten per page. Document summaries consist of the date and a gloss
translation of the document title. Users can inspect a gloss transla-
tion of the full text of any document if the title is not sufficiently
informative. For both title and full text, the gloss translations are
generated in advance using the same process as translation for in-
dexing, with the following differences in detail:
 Terms added as a result of document expansion are not dis-
played.
 The number of retained translations is separately selectable
for the title and for full text indexing.
 Translations are not duplicated when fewer than the maximum
allowable number of translations are known.
Our goal is to support the process of finding documents, with the
realization that the process of using documents may need to be sup-
ported in some other way (e.g., by forwarding relevant documents
to someone who is able to read that language). We have therefore
designedour interface to highlight the query terms in translated doc-
uments and to facilitate skimming by emphasizing the most com-
mon translation when multiple translations are displayed. We have
found that such displays can support a classification task, even when
the translation is not easy to read [3]. Documents must be classified
by the user as relevant or not relevant, so our classification results
suggest that this can be an effective user interface design.
4. RESULTS
We present results both for component-level performance of our
language-independentretargeting modules and an assessmentof the
overall retargeting process.
4.1 Component-level Evaluation
We applied our retargeting approach and retrieval enhancement
techniquesdescribedabove in the context of the first Cross-Language
Evaluation Forum?s (CLEF) multilingual task. We used the English
language forms of the queries to retrieve English, French, German,
and Italian documents. Below we present comparative performance
measuresfor two of the main processingcomponentsdescribed above
- statistical stemming backoff translation - applied to the English-
French cross-languagesegment of the CLEF task. The post-translation
document expansion component was applied to the smaller Topic
Detection and Tracking (TDT-3) collection to improve retrieval of
Mandarin documents using English.
4.1.1 Baseline CLEF System Configuration
Our baseline run was conducted as follows. We translated the
 44; 000 documents from the 1994 issues of Le Monde. We used
the English-French bilingual term list downloaded from the Web at
http://www.freedict.com. We then inverted the term list
to form a 35,000 term French-English translation resource. We per-
formed the necessary document and term list normalization; in this
case, removing accentsfrom document surface forms to enable match-
ing with the un-accentedterm list entries, converting case, and split-
ting clitic contractions, such as l?horlage, on punctuation. We trained
the statistical stemming rules on a sample of the bilingual term list
and document collection and applied these rules in stemming back-
off. Our default condition was run with top-2 balanced translation
using the Brown corpus as a source of target language unigram fre-
quency information. Translated documents were then indexed with
Stage 1 Stage 2 Stage 3 Stage 4
Match 70% 3% 0.5% 1%
Table 1: Percentage of document terms translated at each stage
of 4-stage backoff translation with statistical stemming.
the InQuery (version 3.1p1) system, using the kstem stemmer for
English stemming and InQuery?s default English stopword list. Long
queries were formed by concatenatingthe title, description, and nar-
rative fields of the original query specification. The resulting word
sequence was enclosed in an InQuery #sum operator, indicating
unweighted sum.
Our figure of merit for the evaluations below is mean (uninter-
polated) average precision computed using trec eval 2 across the 34
topics in the CLEF evaluation for which relevant French documents
are known.
4.1.2 Backoff Translation with Statistical Stemming
We first contrast the above baseline system with the effectiveness
of an otherwise identical run without the stemming backoff compo-
nent. Terms in the documents are thus only translated if there is an
exact match between the surface form in the document and a surface
form in the bilingual term list. We find that mean average preci-
sion for unstemmed translation is 0.19 as compared with 0.2919 for
our baseline system including stemming backoff based on trained
rules. This difference is significant at p < 0:05, by paired t-test,
two-tailed. The per-query effectiveness is illustrated in Figure 1.
Backoff translation improves translation coverage while retaining
relatively high precision of matching in contrast to unstemmed ef-
fectiveness.
Backoff translation improves cross-languageinformation retrieval
effectiveness by improving translation coverage of the terms in the
document collection. Using the statistical stemmer, by-token cover-
age of document terms increased by 7coverage. The different stages
of the four-stage backoff process contributed as illustrated in 1. The
majority of terms match in the Stage 1 exact match, accounting for
70% of the term instances in the documents. The remaining stages
each accountfor between 0.5% and 3% of the document terms, while
20% of document term instances remain untranslatable. However,
this relatively small increase in coverage results in the highly sig-
nificant improvement in retrieval effectiveness above.
4.1.3 Top-2 Balanced Translation
Here we contrast top-2 balanced translation with top-1 transla-
tion. We retain statistical stemming backoff for the top-1 transla-
tion. We replace each French document term with the highest ranked
English translation by target languageunigram frequencyin the Brown
Corpus as detailed above, retaining the original French term when
no translation is found in the bilingual term list. We achieve a mean
average precision of 0.2532 in contrast with the baseline condition.
This difference is significant at p < 0:01 by paired t-test, two-tailed.
We can effectively incorporate additional translations using top-2
balanced translation without degrading performance by introducing
significant additional noise. A query-by-query contrast is presented
in Figure 2.
4.1.4 Document Expansion
We evaluatedpost-translation documentexpansionusing the Topic
Detection and Tracking (TDT-3) collection. For this evaluation, we
used the TDT-1999 topic detection task evaluation framework, but
2Available at ftp://ftp.cs.cornell.edu/pub/smart/.
because out focus in this paper is on ranked retrieval effectiveness
we report mean uninterpolated averageprecision rather than the topic-
weighted detection cost measure typically reported in TDT. In the
topic detection task, the system is presented with one or more exem-
plar stories from the training epoch?a form of query-by-example?
and must determine whether each story in the evaluation epoch ad-
dresses either the same seminal event or activity or some directly
related event or activity. This is generally thought to be a some-
what narrower formulation than the more widely used notion of top-
ical relevance, but it seems to be well suited to query-by-example
evaluations. The TDT-1999 tracking task was multilingual, search-
ing stories in both English and Mandarin Chinese, and multi-modal,
involving both newswire text and broadcast news audio. We fo-
cus on the cross-language spoken document retrieval component of
the tracking task, using English exemplars to identify on-topic sto-
ries in Mandarin Chinese broadcast news audio. We compare top-1
translation of the Mandarin Chinese stories with and without post-
translation document expansion.3 We used the earlier TDT-2 En-
glish newswire text collection as our side collection for expansion.
We perform topic tracking on 60 topics with 4 exemplarseach. Here,
we report the mean average precision on the 55 topics for which
there are on-topic Mandarin audio stories. The mean uninterpolated
averageprecision for retrieval of unexpandeddocuments is 0.36 while
post-translation document expansion raises this figure to 0.41. This
difference is significant at p < 0:01 by paired t-test, two-tailed. The
contrast is illustrated in Figure 3. Interestingly, when we tried this
with French, we noted that expansion tended to select terms from
the few foreign-language documents that happened to be present in
our expansion collection. We have not yet explored that effect in de-
tail, but this observation suggests that the document expansion may
be sensitive to the characteristics of the expansioncollection that are
not immediately apparent.
4.2 The Learning Curve
We havefound that retargeting can be accomplishedquite quickly
(a day without document expansion, three days for TREC-sized col-
lections with document expansion), but only if the required infras-
tructure is in place. Adapting a system that was developed initially
for Chinese to handle French documents required several weeks,
with most of that effort invested in development of four-stage back-
off translation and statistical stemming. Further adapting the system
to handle German documents revealed the importance of compound
splitting, a problem that we will ultimately need to address by incor-
porating a more general segmentationstrategy than we used initially
for Chinese. In extending the system to Italian we have found that
although our statistical stemmer presently performs poorly in that
language, we can achieve quite credible results even with a fairly
small (17,313 term) bilingual term list using a freely available Mus-
cat stemmer (which exist for ten languages). So although it is pos-
sible in concept to retarget to a new language in just a few days, ex-
tending the system typically takes us between one and three weeks
because we are still climbing the learning curve.
5. CONCLUSION
By building on the lessons learned using the TREC, CLEF, NT-
CIR, and TDT collections, we have sought to build an infrastructure
that can be applied to a broad array of languages. Arabic and Ko-
rean collections are expected to become available in the next year,
and we are now evolving our interface to support user studies. Our
approach is distinguished by support for interactive retrieval even
3Since Mandarin Chinese has little surface morphology, we omit
backoff translation in this case.
Figure 1: Comparison of effectiveness of backoff versus unstemmed translation of French documents: Bars above x-axis indicate
backoff transltion outperforms unstemmed translation.
Figure 2: Comparison of effectiveness of top-2 balanced versus top-1 translation of French documents: Bars above x-axis indicate
?Top-2? outperforms ?Top-1?
Figure 3: Comparison of effectiveness of top-1 post-translation document expansion versus bare top-1 translation of Chinese docu-
ments: Bars above x-axis indicate document expansion outperforms bare translation
in languagesfor which machine translation is presently unavailable,
and our ultimate goal is to characterize how closely we can approx-
imate the retrieval effectiveness users would obtain if they had the
best available machine translations for the retrieved documents.
Acknowledgements
This work was supported in part by DARPA contract N6600197C8540
and DARPA cooperative agreement N660010028910.
6. ADDITIONAL AUTHORS
Clara I. Cabezas ( Department of Linguistics, top University of
Maryland, College Park, email: clarac@umiacs.umd.edu)
7. REFERENCES
[1] G.-A. Levow and D. W. Oard. Translingual topic tracking
with PRISE. In Working Notes of the Third Topic Detection
and Tracking Worksho p, Feb. 2000.
http://www.glue.umd.edu/oard/research.html.
[2] D. W. Oard, G.-A. Levow, and C. I. Cabezas. CLEF
experiments at Maryland: Statistical stemming and backof f
translation. In C. Peters, editor, Proceedings of the First
Cross-Language Evaluation Forum. 2001. To appear.
http://www.glue.umd.edu/oard/research .html.
[3] D. W. Oard and P. Resnik. Support for interactive document
selection in cross-language information retrieval. Information
Processing and Management, 35(3):363?379, July 1999.
[4] A. Singhal, J. Choi, D. Hindle, J. Hirschberg, F. Pereira, and
S. Whittaker. AT&T at TREC-7 SDR Track. In Proceedings of
the DARPA Broadcast News Workshop, 1999.
Mandarin-English Information (MEI): 
Investigating Translingual Speech Retrieval 
Helen Meng, 1 Sanjeev Khudanpur, ~ Gina Levow, 3 Douglas W. Oard, 3 Hsin-Min Wang' 
1The Chinese University of Hong Kong, 2Johns Hopkins University, 
3University of Maryland and 4Academia Sinica (Taiwan) 
{hmmeng@se.cuhk.edu.hk, sanjeev@clsp.jhu.edu, gina@umiacs.umd.edu, 
oard@, glue.umd.edu, whm@ iis.sinica.edu.tw } 
Abstract 
We describe a system which supports 
English text queries searching for 
Mandarin Chinese spoken documents. 
This is one of the first attempts to tightly 
couple speech recognition with machine 
translation technologies for cross-media 
and cross-language retrieval. The 
Mandarin Chinese news audio are indexed 
with word and subword units by speech 
recognition. Translation of these multi- 
scale units can effect cross-language 
information retrieval. The integrated 
technologies will be evaluated based on 
the performance of translingnal speech 
retrieval. 
1. Introduction 
Massive quantities of audio and multimedia 
programs are becoming available. For example, 
in mid-February 2000, www.real.com listed 
1432 radio stations, 381 Internet-only 
broadcasters, and 86 television stations with 
Internet-accessible content, with 529 
broadcasting in languages other than English. 
Monolingual speech retrieval is now practical, as 
evidenced by services such as SpeechBot 
(speechbot.research.compaq.com), and it is clear 
that there is a potential demand for translingual 
speech retrieval if effective techniques can be 
developed. The Mandarin-English Information 
(MEI) project represents one of the first efforts 
in that direction. 
MEI is one of the four projects elected for 
the Johns Hopkins University (JHU) Summer 
Workshop 2000.1 Our research focus is on the 
integration of speech recognition and embedded 
translation technologies in the context of 
translingual speech retrieval. Possible 
applications of this work include audio and 
video browsing, spoken document retrieval, 
automated routing of information, and 
automatically alerting the user when special 
events occur. 
At the time of this writing, most of the MEI 
team members have been identified. This paper 
provides an update beyond our first proposal 
\[Meng et al, 2000\]. We present some ongoing 
work of our current eam members, as well as 
our ideas on an evolving plan for the upcoming 
JHU Summer Workshop 2000. We believe the 
input from the research community will benefit 
us greatly in formulating ourfinal plan. 
2. Background 
2.1 Translingual Information Retrieval 
The earliest work on large-vocabulary cross- 
language information retrieval from free-text 
(i,e., without manual topic indexing) was 
reported in 1990 \[Landauer and Littman, 1990\], 
and the topic has received increasing attention 
over the last five years \[Oard and Diekema, 
1998\]. Work on large-vocabulary retrieval from 
recorded speech is more recent, with some initial 
work reported in 1995 using subword indexing 
\[Wechsler and Schauble, 1995\], followed by the 
first TREC 2 Spoken Document Retrieval (SDR) 
I http://www.clsp,jhu.edu/ws2000/ 
2 Text REtrieval Conference, http://trec.nist.gov 
23 
evaluation \[Garofolo et al, 2000\]. The Topic 
Detection and Tracking (TDT) evaluations, 
which started in 1998, fall within our definition 
of speech retrieval for this purpose, differing 
from other evaluations principally in the nature 
of the criteria that human assessors use when 
assessing the relevance of a news stow to an 
information eed. In TDT, stories are assessed 
for relevance to an event, while in TREC stories 
are assessed for relevance to an explicitly stated 
information eed that is often subject- rather 
than event-oriented. 
The TDT-33 evaluation marked the first 
case of translingual speech retrieval - the task of 
finding information in a collection of recorded 
speech based on evidence of the information 
need that might be expressed (at least partially) 
in a different language. Translingual speech 
retrieval thus merges two lines of research that 
have developed separately until now. In the 
TDT-3 topic tracking evaluation, recognizer 
transcripts which have recognition errors were 
available, and it appears that every team made 
use of them. This provides a valuable point of 
reference for investigation of techniques that 
more tightly couple speech recognition with 
translingual retrieval. We plan to explore one 
way of doing this in the Mandarin-English 
Information (MEI) project. 
2.2 The Chinese Language 
In order to retrieve Mandarin audio documents, 
we should consider a number of linguistic 
characteristics of the Chinese language: 
The Chinese language has many dialects. 
Different dialects are characterized by their 
differences in the phonetics, vocabularies and 
syntax. Mandarin, also known as Putonglma 
("the common language"), is the most widely 
used dialect. Another major dialect is Cantonese, 
predominant in Hong Kong, Macau, South 
China and many overseas Chinese communities. 
Chinese is a syllable-based language, 
where each syllable carries a lexical tone. 
Mandarin has about 400 base syllables and four 
lexical tones, plus a "light" tone for reduced 
syllables. There are about 1,200 distinct, tonal 
syllables for Mandarin. Certain syllable-tone 
3 http://morph.ldc.upenn.edu/Projects/TDT3/ 
combinations are non-existent in the language. 
The acoustic correlates of the lexical tone 
include the syllable's fundamental frequency 
(pitch contour) and duration. However, these 
acoustic features are also highly dependent on 
prosodic variations of spoken utterances. 
The structure of Mandarin (base) syllables 
is (CG)V(X), where (CG) the syllable onset - C 
the initial consonant, G is the optional medial 
glide, V is the nuclear vowel, and X is the coda 
(which may be a glide, alveolar nasal or velar 
nasal). Syllable onsets and codas are optional. 
Generally C is known as the syllable initial, and 
the rest (GVX) syllable final. 4 Mandarin has 
approximately 21 initials and 39 finals. 5 
In its written form, Chinese is a sequence 
of characters. A word may contain one or more 
characters. Each character is pronounced as a 
tonal syllable. The character-syllable mapping is 
degenerate. On one hand, a given character may 
have multiple syllable pronunciations - for 
example, the character/d" may be pronounced as 
/hang2/, 6/hang4/, or/xing2/. On the other hand, 
a given tonal syllable may correspond to 
multiple characters. Consider the two-syllable 
pronunciation/fu4 shu4/, which corresponds toa 
two-character word. Possible homophones 
include ~, ,  (meaning "rich"), ~ ~tR, ("negative 
number"), ~1~1~, ("complex number" or 
"plural"), ~1~ ("repeat"). 7 
Aside from homographs and homophones, 
another source of ambiguity in the Chinese 
language is the definition of a Chinese word. 
The word has no delimiters, and the distinction 
between a word and a phrase is often vague. The 
lexical structure of the Chinese word is very 
different compared to English. Inflectional 
forms are minimal, while morphology and word 
derivations abide by a different set of rules. A 
word may inherit the syntax and semantics of 
(some of) its compositional characters, for 
4 http://m?rph'ldc'upenn'edu/Pr?jects/Chinese/intr?'html 
5 The corresponding linguistic haracteristics of Cantonese 
are very similar. 
6 These are Mandarin pinyin, the number encodes the tone 
of the syllable. 
7 Example drawn from \[Leung, 1999\]. 
24 
example, 8 ~ means red (a noun or an 
adjective), ~., means color (a noun), and ~. ,  
together means "the color red"(a noun) or 
simply "red" (an adjective). Alternatively, a 
word may take on totally different 
characteristics of its own, e.g. ~. means east (a 
noun or an adjective), ~ means west (a noun or 
an adjective), and .~.~ together means thing (a 
noun). Yet another case is where the 
compositional characters of a word do not form 
independent lexical entries in isolation, e.g. D~ 
means fancy (a verb), but its characters do not 
occur individually. Possible ways of deriving 
new words from characters are legion. The 
problem of identifying the words string in a 
character sequence is known as the segmentation 
/ tokenization problem. Consider the syllable 
string: 
/zhe4 yil wan3 hui4 ru2 chang2 ju3 xing2/ 
The corresponding character string has three 
possible segmentations - all are correct, but each 
involves a distinct set of words: 
(Meaning: It will be take place tonight as usual.) 
(Meaning: The evening banquet will take place 
as usual.) 
(Meaning: If this evening banquet akes place 
frequently...) 
The above considerations lead to a number 
of techniques we plan to use for our task. We 
concentrate on three equally critical problems 
related to our theme of translingual speech 
retrieval: (i) indexing Mandarin Chinese audio 
with word and subword units, (ii) translating 
variable-size units for cross-language 
information retrieval, and (iii) devising effective 
retrieval strategies for English text queries and 
Mandarin Chinese news audio. 
3. Multiscale Audio Indexing 
A popular approach to spoken document 
retrieval is to apply Large-Vocabulary 
s Examples drawn from \[Meng and Ip, 1999\]. 
Continuous Speech Recognition (LVCSR) 9 for 
audio indexing, followed by text retrieval 
techniques. Mandarin Chinese presents a 
challenge for word-level indexing by LVCSR, 
because of the ambiguity in tokenizing a 
sentence into words (as mentioned earlier). 
Furthermore, LVCSR with a static vocabulary is
hampered by the out-of-vocabulary (OOV) 
problem, especially when searching sources with 
topical coverage as diverse as that found in 
broadcast news. 
By virtue of the monosyllabic nature of the 
Chinese language and its dialects, the syllable 
inventory can provide a complete phonological 
coverage for spoken documents, and circumvent 
the OOV problem in news audio indexing, 
offering the potential for greater recall in 
subsequent retrieval. The approach thus supports 
searches for previously unknown query terms in 
the indexed audio. 
The pros and cons of subword indexing for 
an English spoken document retrieval task was 
studied in \[Ng, 2000\]. Ng pointed out that the  
exclusion of lexical knowledge when subword 
indexing is performed in isolation may adversely 
impact discrimination power for retrieval, but 
that some of that impact can be mitigated by 
modeling sequential constraints among subword 
units. We plan to investigate the efficacy of 
using both word and subword units for 
Mandarin audio indexing \[Meng et al, 2000\]. 
Although Ng found that such an approach 
produced little gain over words alone for 
English, the structure of Mandarin Chinese may 
produce more useful subword features. 
3.1 Modeling Syllable Sequence Constraints 
We have thus far used overlapping syllable N- 
grams for spoken document retrieval for two 
Chinese dialects - Mandarin and Cantonese. 
Results on a known-item retrieval task with over 
1,800 error-free news transcripts \[Meng et al, 
1999\] indicate that constraints from overlapping 
bigrams can yield significant improvements in 
retrieval performance over syllable unigrams, 
producing retrieval performance competitive 
9 The lexicon size of a typical large-vocabulary 
continuous speech recognizer can range from 10,000 
to 100,000 word forms. 
25 
with that obtained using automatically tokenized 
Chinese words. 
The study in \[Chen, Wang and Lee, 2000\] 
also used syllable pairs with skipped syllables in 
between. This is because many Chinese 
abbreviations are derived from skipping 
characters, e.g. J .~:~.~t:~  ~ National 
Science Council" can be abbreviated as l~r~ 
(including only the first, third and the last 
characters). Moreover, synonyms often differ by 
one or two characters, e.g. both ~ ' /~4~ and 
~.~,,Ag mean "Chinese culture". Inclusion o f  
these "skipped syllable pairs" also contributed to
retrieval performance. 
When modeling sequential syllable 
constraints, lexical constraints on recognized 
words may be helpful. We thus plan to exp\]Iore 
the potential for integrated sequential model\]ling 
of both words and syllables \[Meng et al, 20013\]. 
4. Multiseale Embedded Translation 
Figures 1 and 2 illustrate two translingual 
retrieval strategies. In query translation, English 
text queries are transformed into Mandarin and 
then used to retrieve Mandarin documents. For 
document translation, Mandarin documents are 
translated into English before they are indexed 
and then matched with English queries. 
McCarley has reported improved effectiveness 
from techniques that couple the two techniques 
\[McCarley, 1999\], but time constraints may 
limit us to explonng only the query translation 
strategy dunng the six-week Workshop. 
4,1 Word  Translat ion 
While we make use of sub-word 
transcription tosmooth out-of-vocabulary(OOV) 
problems in speech recognition as described 
above, and to alleviate the OOV problem :for 
translation as we discuss in the next section, 
accurate translation generally relies on the 
additional information available at the word and 
phrase levels. Since the "bag of words" 
information retrieval techniques do not 
incorporate any meaningful degree of language 
understanding to assess similarity between 
queries and documents, a word-for-word (or, 
more generally, term-for-term) embedded 
translation approach can achieve a useful level 
of effectiveness for many translingual retrieval 
applications \[Oard and Diekema, 1998\]. 
We have developed such a technique for the 
TDT-3 topic tracking evaluation \[Levow and 
Oard, 2000\]. For that work we extracted an 
enriched bilingual Mandarin-English term list by 
combining two term lists: (i) A list assembled 
by the Linguistic Data Consortium from freely 
available on-line resources; and (ii) entries from 
the CETA file (sometimes referred to as 
"Optilex"). This is a Chinese to English 
translation resource that was manually compiled 
by a team of linguists from more than 250 text 
sources, including special and general-purpose 
print dictionaries, and other text sources uch as 
newspapers. The CETA file contains over 
250,000 entries, but for our lexical work we 
extracted a subset of those entries drawn from 
contemporary general-purpose sources. We also 
excluded efinitions uch as "particle indicating 
a yes/no question." Our resulting Chinese to 
English merged bilingual term list contains 
translations for almost 200,000 Chinese terms, 
with average of almost two translation 
alternatives per term. We have also used the 
same resources to construct an initial English to 
Chinese bilingual term list that we plan to refine 
before the Workshop. 
Three significant challenges faced by term- 
to-term translation systems are term selection in 
the source language, the source language 
coverage of the bilingual term list, and 
translation selection in the target language when 
more than one alternative translation is known. 
Word segmentation is a natural by-product of 
large vocabulary Mandarin speech recognition, 
and white space provides word boundaries for 
the English queries. We thus plan to choose 
words as our basic term set, perhaps augmenting 
this with the multiword expressions found in the 
bilingual term list. 
Achieving adequate source language 
coverage is challenging in news retrieval 
applications of the type modelled by TDT, 
because proper names and technical terms that 
may not be present in general-purpose lexical 
resources often provide important retrieval cues. 
Parallel (translation equivalent) corpora have 
proven to be a useful source of translation 
26 
equivalent terms, but obtaining appropriate 
domain-specific parallel corpora in electronic 
form may not be practical in some applications. 
We therefore plan to investigate the use of 
comparable corpora to learn translation 
equivalents, based on techniques in \[Fung, 
1998\]. Subword translation, described below, 
provides a complementary way of handling 
terms for which translation equivalents cannot 
be reliably extracted from the available 
comparable corpora. 
One way of dealing with multiple 
translations is to weight the alternative 
translations using either a statistical translation 
model trained on parallel or comparable corpora 
to estimate translation probability conditioned 
on the source language term. When such 
resources are not sufficiently informative, it is 
generally possible to back off to an 
unconditioned preference statistic based on 
usage frequency of each possible translation i  a 
representative monolingual corpus in the target 
language. In retrospective r trieval applications 
the collection being searched can be used for 
this purpose. We have applied simple versions 
of this approach with good results \[Levow and 
Oard, 2000\]. 
We have recently observed that a simpler 
technique introduced by \[Pirkola, 1998\] can 
produce xcellent results. The key idea is to use 
the structure of the lexicon, in which several 
target language terms can represent a single 
source language term, to induce structure in the 
translated query that the retrieval system can 
automatically exploit. In essence, the translated 
query becomes a bag of bags of terms, where 
each smaller bag corresponds to the set of 
possible translations for one source-language 
term. We plan to implement his structured 
query translation approach using the Inquery 
\[Callan, 1992\] "synonym" operator in the same 
manner as \[Pirkola, 1998\], and to the potential to 
extend the technique to accommodate alternative 
recognition hypothesis and subword units as 
well: 
4.2 Subword  Translat ion 
Since Mandarin spoken documents can be 
indexed with both words and subwords, the 
translation (or "phonetic transliteration") of 
subword units is of particular interest. We plan 
to make use of cross-language phonetic 
mappings derived from English and Mandarin 
pronunciation rules for this purpose. This should 
be especially useful for handling named entities 
in the queries, e.g. names of people, places and 
organizations, etc. which are generally important 
for retrieval, but may not be easily translated. 
Chinese translations of English proper nouns 
may involve semantic as well as phonetic 
mappings. For example, "Northern Ireland" is 
translated as :~b~ttlM - -  where the first 
character ~ means 'north', and the remaining 
characters ~tllllll are pronounced as /ai4-er3- 
lan2L Hence the translation is both semantic 
and phonetic. When Chinese translations strive 
to attain phonetic similarity, the mapping may 
be inconsistent. For example, consider the 
translation of "Kosovo" - sampling Chinese 
newspapers in China, Taiwan and Hong Kong 
produces the following translations: 
~-~r~ /kel-suo3-wo4?, ~-~ /kel-suo3-fo2/, 
~'~&/kel-suo3-ful/f l4"~dt/kel-suo3-fu2/, or 
~/ke  1-suo3-fo2/. 
As can be seen, there is no systematic 
mapping to the Chinese character sequences, but 
the translated Chinese pronunciations bear some 
resemblance to the English pronunciation (/k ow 
s ax vow/). In order to support retrieval under 
these circumstances, the approach should 
involve approximate matches between the 
English pronunciation and the Chinese 
pronunciation. The matching algorithm should 
also accommodate phonological variations. 
Pronunciation dictionaries, or pronunciation 
generation tools for both English words and 
Chinese words / characters will be useful for the 
matching algorithm. We can probably leverage 
off of ideas in the development of universal 
speech recognizers \[Cohen et al, 1997\]. 
5. Mulfiscale Retrieval 
5.1 Coupling Words and Subwords 
We intend to use both words and subwords for 
retrieval. Loose coupling would involve separate 
retrieval runs using words and subwords, 
producing two ranked lists, followed by list 
merging using techniques such as those explored 
by \[Voorhees, 1995\]. Tight coupling, by 
27 
contrast, would require creation of a unified 
index containing both word and subword units, 
resulting in a single ranked list. We hope to 
explore both techniques during the Workshop. 
5.2 Imperfect Indexing and Translat ion 
It should be noted that speech recognition 
exacerbates uncertainty when indexing audio, 
and that translation or transliteration exacerbates 
uncertainty when translating queries and/or 
documents. To achieve robustness for retrieval, 
we have tried three techniques that we have 
found useful: (i) Syllable lattices were used in 
\[Wang, 1999\] and \[Chien et al, 2000\] for 
monolingual Chinese retrieval experiments. The 
lattices were pruned to constrain the search 
space, but were able to achieve robust retrieval 
based on imperfect recognized transcripts. (ii) 
Query expansion, in which syllable transcription 
were expanded to include possibly confusable 
syllable sequences based on a syllable confusion 
matrix derived from recognition errors, was used 
in \[Meng et al, 1999\]. (iii) We have expanded 
the document representation using terms 
extracted from similar documents in a 
comparable collection \[Levow and Oard, 2000\], 
and similar techniques are known to work well 
in the case of query translation (Ballesteros and 
Croft, 1997). We hope to add to this set: of 
techniques by exploring the potential for query 
expansion based on cross-language phonetic 
mapping. 
6. Using the TDT-3 Collection 
We plan to use the TDT-2 collection for 
development testing and the TDT-3 collection 
for evaluation. Both collections provide 
documents from two English newswire sources, 
six English broadcast news audio sources, two 
Mandarin Chinese newswire sources, and one 
Mandarin broadcast news source (Voice of 
America). Manually established story 
boundaries are available for all audio 
collections, and we plan to exploit that 
information to simplify our experiment design. 
The TDT-2 collection includes complete 
relevance assessments for 20 topics, and the 
TDT-3 collection provides the same for 60 
additional topics, 56 of which have at least one 
relevant audio story. For each topic, at least four 
English stories and four Chinese stories are 
known. 
We plan to automatically derive text queries 
based on one or more English stories that are 
presented as exemplars, and to use those queries 
to search the Mandarin audio collection. 
Manually constructed queries will provide a 
contrastive condition. Unlike the TDT "topic 
tracking" task in which stories must be declared 
relevant or not relevant in the order of their 
arrival, we plan to perform retrospective 
retrieval experiments in which all documents are 
known when the query is issued. By relaxing 
the temporal ordering of the TDT topic tracking 
task, we can meaningfully search for Mandarin 
Chinese stories that may have arrived before the 
exemplar story or stories. We thus plan to report 
ranked retrieval measures of effectiveness uch 
as average precision in addition to the detection 
statistics (miss and false alarm) typically 
reported in TDT. 
7.  Summary 
This paper presents our current ideas and 
evolving plan for the MEI project, to take place 
at the Johns Hopkins University Summer 
Workshop 2000. Translingual speech retrieval is 
a long-term research direction, and our team 
looks forward to jointly taking an initial step to 
tackle the problem. The authors welcome all 
comments and suggestions, aswe strive to better 
define the problem in preparation for the six- 
week Workshop. 
Acknowledgments 
The authors wish to thank Patrick Schone, Erika 
Grams, Fred Jelinek, Charles Wayne, Kenney 
? Ng, John Garofolo, and the participants in the 
December 1999 WS2000 planning meeting and 
the TDT-3 workshop for their many helpful 
suggestions. The Hopkins Summer Workshop 
series is supported by grants from the National 
Science Foundation. Our results reported in this 
paper eference thesis work in progress of Wai- 
Kit Lo (Ph.D. candidate, The Chinese Unversity 
of Hong Kong) and Berlin Chen (Ph.D. 
candidate, National Taiwan University). 
28 
References 
Ballesteros and W. B. Croft, "Phrasal 
Translation and Query Expansion Techniques 
for Cross-Language Information Retrieval," 
Proceedings ofACM SIGIR, 1997. 
Callan, J. P., W. B. Croft, and S. M. Harding, 
"The INQUERY Retrieval System," 
Proceedings of the 3rd International Conference 
on Database and Expert Systems Applications, 
1992. 
Carbonnell, J., Y. Yang, R. Frederking and R.D. 
Brown, "Translingual Information Retrieval: A 
Comparative Evaluation," Proceedings ofIJCAI, 
1997. 
Chen, B., H.M. Wang, and L.S. Lee, "Retrieval 
of Broadcast News Speech in Mandarin Chinese 
Collected in Taiwan using Syllable-Level 
Statistical Characteristics," Proceedings of 
ICASSP, 2000. 
Chien, L. F., H. M. Wang, B. R. Bai, and S. C. 
Lin, "A Spoken-Access Approach for Chinese 
Text and Speech Information Retrieval," Journal 
of the American Society for Information 
Science, 51 (4), pp. 313-323, 2000. 
Choy, C. Y., "Acoustic Units for Mandarin 
Chinese Speech Recognition," M.Phil. Thesis, 
The Chinese University of Hong Kong, Hong 
Kong SAR, China, 1999. 
Cohen, P., S. Dharanipragada, J. Gros, M. 
Mondowski, C. Neti, S. Roukos and T. Ward, 
"Towards a Universal Speech Recognizer for 
Multiple Languages," Proceedings of ASRU, 
1997. 
Fung, P., "A Statistical View on Bilingual 
Lexicon Extraction: From parallel corpora to 
non-parallel corpora," Proceedings of AMTA, 
1998. 
Garofolo, J.S., Auzanne, G.P., Voorhees, E.M., 
"The TREC Spoken Document Retrieval Track: 
A Success Story," Proceedings of the Recherche 
d'informations A sistre par Ordinateur: Content- 
Based Multimedia Information Access 
Conference, April 12-14, 2000,to be published. 
Knight, K. and J. Graehl, "Machine 
Transliteration," Proceedings ofACL, 1997. 
Landauer, T. K. and M.L. Littman, "Fully 
Automatic Cross-Language Document Retrieval 
Using Latent Semantic Indexing," Proceedings 
of the 6 th Annual Conference of the UW Centre 
for the New Oxford English Dictionary, 1990. 
Leung, R., "Lexical Access for Large 
Vocabulary Chinese Speech Recognition," M. 
Phil. Thesis, The Chinese University of Hong 
Kong, Hong Kong SAR, China 1999. 
Levow, G. and D.W. Oard, "Translingual Topic 
Tracking with PRISE," Working notes of the 
DARPA TDT-3 Workshop, 2000. 
Lin, C. H., L. S. Lee, and P. Y. Ting, "A New 
Framework for Recognition of Mandarin 
Syllables with Tones using Sub-Syllabic Units," 
Proceedings ofICASSP, 1993. 
Liu~ F. H., M. Picheny, P. Srinivasa, M. 
Monkowski and J. Chen, "Speech Recognition 
on Mandarin Call Home: A Large-Vocabulary, 
Conversational, nd Telephone Speech Corpus," 
Proceedings ofICASSP, 1996. 
McCarley, S., "Should we Translate the 
Documents or the Queries in Cross-Language 
Information Retrieval," Proceedings of ACL, 
1999. 
Meng, H. and C. W. Ip, "An Analytical Study o f  
Transformational Tagging of Chinese Text," 
Proceedings of the Research On Computational 
Lingustics (ROCLING) Conference, 1999. 
Meng, H., W. K. Lo, Y. C. Li and P. C. Ching, 
"A Study on the Use of Syllables for Chinese 
Spoken Document Retrieval," Technical Report 
SEEM1999-11, The Chinese University of Hong 
Kong, 1999. 
Meng, H., Khudanpur, S., Oard, D. W. and 
Wang, H. M., "Mandarin-English Information 
(MEI)," Working notes of the DARPA TDT-3 
Workshop, 2000. 
Ng, K., "Subword-based Approaches for Spoken 
Document Retrieval," Ph.D. Thesis, MIT, 
February 2000. 
Oard, D. W. and A.R. Diekema, "Cross- 
Language Information Retrieval," Annual 
Review of Information Science and Technology, 
vol.33, 1998. 
Pirkola, A., "The effects of query structure and 
dictionary setups in dictionary-based cross- 
language information retrieval," Proceedings of 
ACM SIGIR, 1998. 
Sheridan P. and J. P. Ballerini, "Experiments in
Multilingual Information Retrieval using the 
29 
SPIDER System," Proceedings of ACM SIGIR, 
1996. 
Voorhees, E., "Learning Collection Fusion 
Strategies," Proceedings of SIGIR, 1995. 
Wang, H. M., "Retrieval of Mandarin Spoken 
Documents Based on Syllable Lattice 
Matching," Proceedings of the Fourth 
International Workshop on Information 
Retrieval in Asian Languages, 1999. 
Wechsler, M. and P. Schaiible, "Speech 
Retrieval Based on Automatic Indexing," 
Proceedings of MIRO- 1995. 
English Text Queries 
(words) 
Words that are present entities and unknown words 
translation dictionary 
\[ Trans'a~on I I Transliteration 
Mand~Sn Queries (with words and syllables) 
Mandarin Spoken Documents \[ 
(indexed with word and subword units) 
"7 
Information Retrieval 
Engine 
I Evaluate Retrieval 
Performance 
Figure 1. Query translation strategy. 
Mandarin Spoken Documents 
(indexed with word and subword units) 
l 
Translation 
Documents in 
English 
English Text Queries 
(words) 
I Information Retrieval 
Engine 
Evaluate 
Retrieval 
Performance 
Figure 2. Document translation strategy. 
3O 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 351?359, Prague, June 2007. c?2007 Association for Computational Linguistics
Topic Segmentation with Hybrid Document Indexing
Irina Matveeva
Department of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Department of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
We present a domain-independent unsuper-
vised topic segmentation approach based on
hybrid document indexing. Lexical chains
have been successfully employed to evalu-
ate lexical cohesion of text segments and to
predict topic boundaries. Our approach is
based in the notion of semantic cohesion. It
uses spectral embedding to estimate seman-
tic association between content nouns over a
span of multiple text segments. Our method
significantly outperforms the baseline on the
topic segmentation task and achieves perfor-
mance comparable to state-of-the-art meth-
ods that incorporate domain specific infor-
mation.
1 Introduction
The goal of topic segmentation is to discover story
boundaries in the stream of text or audio recordings.
Story is broadly defined as segment of text contain-
ing topically related sentences. In particular, the
task may require segmenting a stream of broadcast
news, addressed by the Topic Detection and Track-
ing (TDT) evaluation project (Wayne, 2000; Allan,
2002). In this case topically related sentences belong
to the same news story. While we are considering
TDT data sets in this paper, we would like to pose
the problem more broadly and consider a domain-
independent approach to topic segmentation.
Previous research on topic segmentation has
shown that lexical coherence is a reliable indicator
of topical relatedness. Therefore, many approaches
have concentrated on different ways of estimating
lexical coherence of text segments, such as seman-
tic similarity between words (Kozima, 1993), sim-
ilarity between blocks of text (Hearst, 1994), and
adaptive language models (Beeferman et al, 1999).
These approaches use word repetitions to evaluate
coherence. Since the sentences covering the same
story represent a coherent discourse segment, they
typically contain the same or related words. Re-
peated words build lexical chains that are conse-
quently used to estimate lexical coherence. This can
be done either by analyzing the number of overlap-
ping lexical chains (Hearst, 1994) or by building a
short-range and long-range language model (Beefer-
man et al, 1999). More recently, topic segmentation
with lexical chains has been successfully applied to
segmentation of news stories, multi-party conversa-
tion and audio recordings (Galley et al, 2003).
When the task is to segment long streams of text
containing stories which may continue at a later
point in time, for example developing news stories,
building of lexical chains becomes intricate. In ad-
dition, the word repetitions do not account for syn-
onymy and semantic relatedness between words and
therefore may not be able to discover coherence of
segments with little word overlap.
Our approach aims at discovering semantic relat-
edness beyond word repetition. It is based on the
notion of semantic cohesion rather than lexical cohe-
sion. We propose to use a similarity metric between
segments of text that takes into account semantic as-
sociations between words spanning a number of seg-
ments. This method approximates lexical chains by
averaging the similarity to a number of previous text
351
segments and accounts for synonymy by using a hy-
brid document indexing scheme. Our text segmen-
tation experiments show a significant performance
improvement over the baseline.
The rest of the paper is organized as follows. Sec-
tion 2 discusses hybrid indexing. Section 3 describes
our segmentation algorithm. Section 5 reports the
experimental results. We conclude in section 6.
2 Hybrid Document Indexing
For the topic segmentation task we would like to de-
fine a similarity measure that accounts for synonymy
and semantic association between words. This simi-
larity measure will be used to evaluate semantic co-
hesion between text units and the decrease in seman-
tic cohesion will be used as an indicator of a story
boundary. First, we develop a document representa-
tion which supports this similarity measure.
Capturing semantic relations between words in
a document representation is difficult. Different
approaches tried to overcome the term indepen-
dence assumption of the bag-of-words representa-
tion (Salton and McGill, 1983) by using distribu-
tional term clusters (Slonim and Tishby, 2000) and
expanding the document vectors with synonyms, see
(Levow et al, 2005). Since content words can be
combined into semantic classes there has been a
considerable interest in low-dimensional representa-
tions. Latent Semantic Analysis (LSA) (Deerwester
et al, 1990) is one of the best known dimension-
ality reduction algorithms. In the LSA space doc-
uments are indexed with latent semantic concepts.
LSA maps all words to low dimensional vectors.
However, the notion of semantic relatedness is de-
fined differently for subsets of the vocabulary. In ad-
dition, the numerical information, abbreviations and
the documents? style may be very good indicators of
their topic. However, this information is no longer
available after the dimensionality reduction.
We use a hybrid approach to document indexing
to address these issues. We keep the notion of la-
tent semantic concepts and also try to preserve the
specifics of the document collection. Therefore, we
divide the vocabulary into two sets: nouns and the
rest of the vocabulary. The set of nouns does not
include proper nouns. We use a method of spec-
tral embedding, as described below and compute a
low-dimensional representation for documents using
only the nouns. We also compute a tf-idf represen-
tation for documents using the other set of words.
Since we can treat each latent semantic concept in
the low-dimensional representation as part of the vo-
cabulary, we combine the two vector representations
for each document by concatenating them.
2.1 Spectral Embedding
A vector space representation for documents and
sentences is convenient and makes the similarity
metrics such as cosine and distance readily avail-
able. However, those metrics will not work if they
don?t have a meaningful linguistic interpretation.
Spectral methods comprise a family of algo-
rithms that embed terms and documents in a low-
dimensional vector space. These methods use pair-
wise relations between the data points encoded in a
similarity matrix. The main step is to find an embed-
ding for the data that preserves the original similari-
ties.
GLSA We use Generalized Latent Semantic Anal-
ysis (GLSA) (Matveeva et al, 2005) to compute
spectral embedding for nouns. GLSA computes
term vectors and since we would like to use spectral
embedding for nouns, it is well-suited for our ap-
proach. GLSA extends the ideas of LSA by defining
different ways to obtain the similarities matrix and
has been shown to outperform LSA on a number of
applications (Matveeva and Levow, 2006).
GLSA begins with a matrix of pair-wise term sim-
ilarities S, computes its eigenvectors U and uses the
first k of them to represent terms and documents, for
details see (Matveeva et al, 2005). The justifica-
tion for this approach is the theorem by Eckart and
Young (Golub and Reinsch, 1971) stating that inner
product similarities between the term vectors based
on the eigenvectors of S represent the best element-
wise approximation to the entries in S. In other
words, the inner product similarity in the GLSA
space preserves the semantic similarities in S.
Since our representation will try to preserve se-
mantic similarities in S it is important to have a ma-
trix of similarities which is linguistically motivated.
352
Word Nearest Neighbors in GLSA Space
witness testify prosecutor trial testimony juror eyewitness
finance fund bank investment economy crisis category
broadcast television TV satellite ABC CBS radio
hearing hearing judge voice chatter sound appeal
surprise announcement disappointment stunning shock reaction astonishment
rest stay remain keep leave portion economy
Table 1: Words? nearest neighbors in the GLSA semantic space.
2.2 Distributional Term Similarity
PMI Following (Turney, 2001; Matveeva et al,
2005), we use point-wise mutual information (PMI)
to compute the matrix S. PMI between random vari-
ables representing the words wi and wj is computed
as
PMI(wi, wj) = log
P (Wi = 1,Wj = 1)
P (Wi = 1)P (Wj = 1)
. (1)
Thus, for GLSA, S(wi, wj) = PMI(wi, wj).
Co-occurrence Proximity An advantage of PMI
is the notion of proximity. The co-occurrence statis-
tics for PMI are typically computed using a sliding
window. Thus, PMI will be large only for words that
co-occur within a small context of fixed size.
Semantic Association vs. Synonymy Although
GLSA was successfully applied to synonymy in-
duction (Matveeva et al, 2005), we would like to
point out that the GLSA discovers semantic associ-
ation in a broad sense. Table 1 shows a few words
from the TDT2 corpus and their nearest neighbors
in the GLSA space. We can see that for ?witness?,
?finance? and ?broadcast? words are grouped into
corresponding semantic classes. The nearest neigh-
bors for ?hearing? and ?stay? represent their differ-
ent senses. Interestingly, even for the abstract noun
?surprise? the nearest neighbors are meaningful.
2.3 Document Indexing
We have two sets of the vocabulary terms: a set of
nouns, N , and the other words, T . We compute tf-idf
document vectors indexed with the words in T :
~di = (?i(w1), ?i(w2), ..., ?i(w|T |)), (2)
where ?i(wt) = tf(wt, di) ? idf(wt).
We also compute a k-dimensional representation
with latent concepts ci as a weighted linear combi-
nation of GLSA term vectors ~wt:
~di = (c1, ..., ck) =
?
t=1:|N |
?i(wt) ? ~wt, (3)
We concatenate these two representations to gener-
ate a hybrid indexing of documents:
~di = (?i(w1), ..., ?i(w|T |), c1, ...ck) (4)
In our experiments, we compute document
and sentence representation using three indexing
schemes: the tf-idf baseline, the GLSA represen-
tation and the hybrid indexing. The GLSA index-
ing computes term vectors for all vocabulary words;
document and sentence vectors are generated as lin-
ear combinations of term vectors, as shown above.
2.4 Document similarity
One can define document similarity at different lev-
els of semantic content. Documents can be similar
because they discuss the same people or events and
because they discuss related subjects and contain se-
mantically related words. Hybrid Indexing allows
us to combine both definitions of similarity. Each
representation supports a different similarity mea-
sure. tf-idf uses term-matching, the GLSA represen-
tation uses semantic association in the latent seman-
tic space computed for all words, and hybrid index-
ing uses a combination of both: term-matching for
named entities and content words other than nouns
combined with semantic association for nouns.
In the GLSA space, the inner product between
document vectors contains all pair-wise inner prod-
uct between their words, which allows one to detect
semantic similarity beyond term matching:
?~di, ~dj? =
?
w?di
?
v?dj
?i(w)?j(v)?~w,~v? (5)
353
If documents contain words which are different but
semantically related, the inner product between the
term vectors will contribute to the document similar-
ity, as illustrated with an example in section 5.
When we compare two documents indexed with
the hybrid indexing scheme, we compute a combi-
nation of similarity measures:
?~di, ~dj? =
?
nk?di
?
nm?dj
?i(nk)?j(nm)? ~nk, ~nm?+
?
t?T
?i(t) ? ?j(t).
(6)
Document similarity contains semantic association
between all pairs of nouns and uses term-matching
for the rest of the vocabulary.
3 Topic Segmentation with Semantic
Cohesion
Our approach to topic segmentation is based on
semantic cohesion supported by the hybrid index-
ing. Topic segmentation approaches use either sen-
tences (Galley et al, 2003) or blocks of words as
text units (Hearst, 1994). We used both variants
in our experiments. When using blocks, we com-
puted blocks of a fixed size (typically 20 words) slid-
ing over the documents in a fixed step size (10 or
5 words). The algorithm predicts a story boundary
when the semantic cohesion between two consecu-
tive units drops. Blocks can cross story boundaries,
thus many predicted boundaries will be displaced
with respect to the actual boundary.
Averaged similarity In our preliminary experi-
ments we used the largest difference in score to pre-
dict story boundary, following the TextTiling ap-
proach (Hearst, 1994). We found, however, that in
our document collection the word overlap between
sentences was often not large and pair-wise similar-
ity could drop to zero even for sentences within the
same story, as will be illustrated below. We could
not obtain satisfactory results with this approach.
Therefore, we used the average similarity by us-
ing a history of fixed size n. The semantic cohesion
score was computed for the position between two
text units, ti and tj as follows:
score(ti, tj) =
1
n
n?1
?
k=0
?ti?k, tj? (7)
Our approach predicts story boundaries at the min-
ima of the semantic cohesion score.
Approximating Lexical Chains One of the mo-
tivations for our cohesion score is that it approxi-
mates lexical chains, as for example in (Galley et al,
2003). Galley et al (Galley et al, 2003) define lex-
ical chains R1, .., RN by considering repetitions of
terms t1, .., tN and assigning larger weights to short
and compact chains. Then the lexical cohesion score
between two text units ti and tj is based on the num-
ber of chains that overlap both of them:
score(ti, tj) =
N
?
k=1
wk(ti)wk(tj), (8)
where wk(ti) = score(Rj) if the chain Rj over-
laps ti and zero otherwise. Our cohesion score takes
into account only the chains for words that occur in
tj and have another occurrence within n previous
sentences. Due to this simplification, we compute
the score based on inner products. Once we make
the transition to inner products, we can use hybrid
indexing and compute semantic cohesion score be-
yond term repetition.
4 Related Approaches
We compare our approach to the LCseg algorithm
which uses lexical chains to estimate topic bound-
aries (Galley et al, 2003). Hybrid indexing allows
us to compute semantic cohesion score rather than
the lexical cohesion score based on word repetitions.
Choi at al. used LSA for segmentation (Choi et
al., 2001). LSA (Deerwester et al, 1990) is a spe-
cial case of spectral embedding and Choi at al. (Choi
et al, 2001) used all vocabulary words to com-
pute low-dimensional document vectors. We use
GLSA (Matveeva et al, 2005) because it computes
term vectors as opposed to the dual document-term
representation with LSA and uses a different ma-
trix of pair-wise similarities. Furthermore, Choi
at al. (Choi et al, 2001) used clustering to predict
boundaries whereas we used the average similarity
scores.
354
s1: The Cuban news agency Prensa Latina called Clinton ?s announcement Friday that Cubans picked up
at sea will be taken to Guantanamo Bay naval base a ? new and dangerous element ? in U S immigration policy.
s2: The Cuban government has not yet publicly reacted to Clinton ?s announcement that Cuban rafters
will be turned away from the United States and taken to the U S base on the southeast tip of Cuba.
s5: The arrival of Cuban emigrants could be an ? extraordinary aggravation ? to the situation , Prensa Latina said.
s6: It noted that Cuba had already denounced the use of the base as a camp for Haitian refugees.
whom it had for many years encouraged to come to the United States.
s8: Cuba considers the land at the naval base , leased to the United States at the turn of the century,
to be illegally occupied.
s10: General Motors Corp said Friday it was recalling 5,600 1993-94 model Chevrolet Lumina, Pontiac
Trans Sport and Oldsmobile Silhouette minivans equipped with a power sliding door and built-in child seats.
s14: If this occurs , the shoulder belt may not properly retract , the carmaker said.
s15: GM is the only company to offer the power-sliding door.
s16: The company said it was not aware of any accidents or injuries related to the defect.
s17: To correct the problem , GM said dealers will install a modified interior trim piece that will reroute the seat belt.
Table 2: TDT. The first 17 sentences in the first file.
Existing approaches to hybrid indexing used dif-
ferent weights for proper nouns, nouns phrase heads
and use WordNet synonyms to expand the docu-
ments, for example (Hatzivassiloglou et al, 2000;
Hatzivassiloglou et al, 2001). Our approach does
not require linguistic resources and learning the
weights. The semantic associations between nouns
are estimated using spectral embedding.
5 Experiments
5.1 Data
The first TDT collection is part of the LCseg
toolkit1 (Galley et al, 2003) and we used it to com-
pare our approach to LCseg. We used the part of this
collection with 50 files with 22 documents each.
We also used the TDT2 collection2 of news arti-
cles from six news agencies in 1998. We used only
9,738 documents that are assigned to one topic and
have length more than 50 words. We used the Lemur
toolkit3 with stemming and stop words list for the
tf-idf indexing; we used Bikel?s parser4 to obtain
the POS-tags and select nouns; we used the PLA-
PACK package (Bientinesi et al, 2003) to compute
the eigenvalue decomposition.
1http://www1.cs.columbia.edu/ galley/tools.html
2http://nist.gov/speech/tests/tdt/tdt98/
3http://www.lemurproject.org/
4http://www.cis.upenn.edu/ dbikel/software.html
Evaluation For the TDT data we use the error
metric pk (Beeferman et al, 1999) and WindowD-
iff (Pevzner and Hearst, 2002) which are imple-
mented in the LCseg toolkit. We also used the
TDT cost metric Cseg5, with the default parameters
P(seg)=0.3, Cmiss=1, Cfa=0.3 and distance of 50
words. All these measures look at two units (words
or sentences) N units apart and evaluate how well
the algorithm can predict whether there is a bound-
ary between them or not. Lower values mean better
performance for all measures.
Global vs. Local GLSA Similarity To obtain the
PMI values we used the TDT2 collection, denoted as
GLSAlocal. Since co-occurrence statistics based on
larger collections give a better approximation to lin-
guistic similarities, we also used 700,000 documents
from the English GigaWord collection, denoted as
GLSA. We used a window of size 8.
5.2 Topic Segmentation
The first set of experiments was designed to evaluate
the advantage of the GLSA representation over the
baseline. We compare our approach to the LCseg
algorithm (Galley et al, 2003) and use sentences as
segmentation unit. To avoid the issue of parameters
setting when the number of boundaries is not known,
we provide each algorithm with the actual numbers
5www.nist.gov/speech/tests/tdt/tdt98/doc/
tdt2.eval.plan.98.v3.7.ps
355
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 2 4 6 8 10 12 14 16 18 20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
GLSA
tfidf
Figure 1: TDT. Pair-wise sentence similarities for tf-idf (left), GLSA (middle); x-axis shows story bound-
aries. Details for the first 20 sentences, table 2 (right).
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
10 21 27 45 52 65 73 89 99
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
10 21 27 45 52 65 73 89 99
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 2: TDT. Pair-wise sentence similarities for tf-idf (left), GLSA (middle) averaged over 10 preceeding
sentences; LCseg lexical cohesion scores (right). X-axis shows story boundaries.
of boundaries.
TDT We use the LCseg approach and our ap-
proach with the baseline tf-idf representation and the
GLSA representation to segment this corpus. Ta-
ble 2 shows a few sentences. Many content words
are repeated, so the lexical chains is definitely a
sound approach. As shown in Table 2, in the first
story the word ?Cuba? or ?Cuban? is repeated in ev-
ery sentence thus generating a lexical chain. On the
topic boundary, the word overlap between sentences
is very small. At the same time, the repetition of
words may also be interrupted within a story: sen-
tence 5, 6 and sentences 14, 15, 16 have little word
overlap. LCseg deals with this by defining several
parameters to control chain length and gaps. This
simple example illustrates the potential benefit of se-
mantic cohesion. Table 2 shows that ?General Mo-
tors? or ?GM? are not repeated in every sentence of
the second story. However, ?GM?, ?carmaker? and
?company? are semantically related. Making this
information available to the segmentation algorithm
allows it to establish a connection between each sen-
tence of the second story.
We computed pair-wise sentence similarities be-
tween pairs of consecutive sentences in the tf-idf and
GLSA representations. Figure 1 shows the similar-
ity values plotted for each sentence break. The pair-
wise similarities based on term-matching are very
spiky and there are many zeros within the story. The
GLSA-based similarity makes the dips in the simi-
larities at the boundaries more prominent. The last
plot gives the details for the sentences in table 2.
In the tf-idf representation sentences without word
overlap receive zero similarity but the GLSA repre-
sentation is able to use the semantic association be-
tween between ?emigrants? and ?refugees? for sen-
tences 5 and 6, and also the semantic association be-
tween ?carmaker? and ?company? for sentences 14
356
Measure tf-idf GLSA LCseg
Pmiss 0.29 0.19 N/A
Pfa 0.14 0.09 N/A
Cseg 0.18 0.08 N/A
pk 0.24 0.17 0.07
wd 0.27 0.21 0.10
Table 3: TDT segmentation results.
and 15.
This effect increases as we use the semantic cohe-
sion score as in equation 7. Figure 2 shows the simi-
larity values for tf-idf and GLSA and also the lexical
cohesion scores computed by LCseg. The GLSA-
based similarities are not quite as smooth as the LC-
seg scores, but they correctly discover the bound-
aries. LCseg parameters are fine-tuned for this doc-
ument collection. We used a general TDT2 GLSA
representation for this collection, and the only seg-
mentation parameter we used is to avoid placing
next boundary within n=3 sentences of the previ-
ous one. For this reason the predicted boundary may
be one sentence off the actual boundary. These re-
sults are summarized in Table 3. The GLSA repre-
sentation performs significantly better than the tf-idf
baseline. Its pk and WindowDiff scores with default
parameters for LCseg are worse than for LCseg. We
attribute it to the fact that we did not fine-tuned our
method to this collection and that boundaries are of-
ten placed one position off the actual boundary.
TDT2 For this collection we used three different
indexing schemes: the tf-idf baseline, the GLSA rep-
resentation and the hybrid indexing. Each represen-
tation supports a different similarity measure. Our
TDT experiments showed that the semantic cohe-
sion score based on the GLSA representation im-
proves the segmentation results. The variant of
the TDT corpus we used is rather small and well-
balanced, see (Galley et al, 2003) for details. In
the second phase of experiments we evaluate our ap-
proach on the larger TDT2 corpus. The experiments
were designed to address the following issues:
? performance comparison between GLSA and
Hybrid indexing representations. As men-
tioned before, GLSA embeds all words in
a low-dimensional space. Whereas semantic
#b known
Method Pmiss Pfa Cseg
tf-idf 0.52 0.14 0.19
GLSA 0.4 0.1 0.14
GLSA local 0.44 0.12 0.16
Hybrid 0.34 0.10 0.12
Hybrid local 0.38 0.09 0.13
LCseg 0.80 0.19 0.28
#b unknown
Method Pmiss Pfa Cseg
tf-idf 0.42 0.2 0.17
GLSA 0.37 0.13 0.14
GLSA local 0.35 0.19 0.14
Hybrid 0.26 0.16 0.11
Hybrid local 0.27 0.18 0.12
Table 4: TDT2 segmentation results. Sliding blocks
with size 20 and stepsize 10; similarity averaged
over 10 preceeding blocks.
classes for nouns have theoretical linguistic jus-
tification, it is harder to motivate a latent space
representation for example for proper nouns.
Therefore, we want to evaluate the advantage
of using spectral embedding only for nouns.
? collection dependence of similarities. The sim-
ilarity matrix S is computed using the TDT2
corpus (GLSAlocal) and using the larger Giga-
Word corpus. The larger corpus provides more
reliable co-occurrence statistics. On the other
hand, word distribution is different from that
in the TDT2 corpus. We wanted to evaluate
whether semantic similarities are collection in-
dependent.
Table 4 shows the performance evaluation. We show
the results computed using blocks containing 20
words (after preprocessing) with step size 10. We
tried other parameter values but did not achieve bet-
ter performance, which is consistent with other re-
search (Hearst, 1994; Galley et al, 2003). We show
the results for two settings: predict a known num-
ber of boundaries, and predict boundaries using a
threshold. In our experiments we used the average
of the smallest N scores as threshold, N = 4000
showing best results.
357
The spectral embedding based representations
(GLSA, Hybrid) significantly outperform the base-
line. This confirms the advantage of the semantic
cohesion score vs. term-matching. Hybrid index-
ing outperforms the GLSA representation support-
ing our intuition that semantic association is best de-
fined for nouns.
We used the GigaWord corpus to obtain the pair-
wise word associations for the GLSA and Hybrid
representations. We also computed GLSAlocal and
Hybridlocal using the TDT2 corpus to obtain the
pair-wise word associations. The co-occurrence
statistics based on the GigaWord corpus provide
more reliable estimations of semantic association
despite the difference in term distribution. The dif-
ference is larger for the GLSA case when we com-
pute the embedding for all words, GLSA performs
better than GLSAlocal. Hybridlocal performs only
slightly worse than Hybrid. This seems to support
the claim that semantic associations between nouns
are largely collection independent. On the other
hand, semantic associations for proper names are
collection dependent at least because the collections
are static but the semantic relations of proper names
may change over time. The semantic space for a
name of a president, for example, is different for the
period of time of his presidency and for the time be-
fore and after that.
Disappointingly, we could not achieve good re-
sults with LCseg. It tends to split stories into short
paragraphs. Hybrid indexing could achieve results
comparable to state-of-the art approaches, see (Fis-
cus et al, 1998) for an overview.
6 Conclusion and Future Work
We presented a topic segmentation approach based
on semantic cohesion scores. Our approach is do-
main independent, does not require training or use
of lexical resources. The scores are computed based
on the hybrid document indexing which uses spec-
tral embedding in the space of latent concepts for
nouns and keeps proper nouns and other specifics of
the documents collections unchanged. We approxi-
mate the lexical chains approach by simplifying the
definition of a chain which allows us to use inner
products as basis for the similarity score. The simi-
larity score takes into account semantic relations be-
tween nouns beyond term matching. This semantic
cohesion approach showed good results on the topic
segmentation task.
We intend to extend the hybrid indexing approach
by considering more vocabulary subsets. Syntactic
similarity is more appropriate for verbs, for exam-
ple, than co-occurrence. As a next step, we intend to
embed verbs using syntactic similarity. It would also
be interesting to use lexical chains for proper names
and learn the weights for different similarity scores.
References
J. Allan, editor. 2002. Topic Detection and Tracking:
Event-based Information Organization. Kluwer Aca-
demic Publishers.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
Paolo Bientinesi, Inderjit S. Dhilon, and Robert A. van de
Geijn. 2003. A parallel eigensolver for dense sym-
metric matrices based on multiple relatively robust
representations. UT CS Technical Report TR-03-26.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text seg-
mentation. In Proceedings of EMNLP, pages 109?
117.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
J. G. Fiscus, George Doddington, John S. Garofolo, and
Alvin Martin. 1998. NIST?s 1998 topic detection and
tracking evaluation (tdt2). In Proceedings of NIST?s
1998 Topic Detection and Tracking Evaluation.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of ACL.
G. Golub and C. Reinsch. 1971. Handbook for Ma-
trix Computation II, Linear Algebra. Springer-Verlag,
New York.
V. Hatzivassiloglou, Luis Gravano, and Ankineedu Mag-
anti. 2000. An investigation of linguistic features and
clustering algorithms for topical document clustering.
In Proceedings of SIGIR, pages 224?231.
V. Hatzivassiloglou, Regina Barzilay Min-Yen Kan Ju-
dith L. Klavans, Melissa L. Holcombe, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible
358
clustering tool for summarization. In Proceedings of
NAACL, pages 41?49.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9?16.
Hideki Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Proceedings of ACL, pages
286?288.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management: Special Issue on Cross-language Infor-
mation Retrieval.
Irina Matveeva and Gina-Anne Levow. 2006. Graph-
based Generalized Latent Semantic Analysis for docu-
ment representation. In Proc. of the TextGraphs Work-
shop at HLT/NAACL.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized Latent Semantic
Analysis for term representation. In Proc. of RANLP.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Comput. Linguist., 28(1):19?36.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Research and Development in Infor-
mation Retrieval, pages 208?215.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI?IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491?502.
C. Wayne. 2000. Multilingual topic detection and track-
ing: Successful research enabled by corpora and eval-
uation. In Proceedings of Language Resources and
Evaluation Conference (LREC), pages 1487?1494.
359
Computing Term Translation Probabilities with Generalized Latent
Semantic Analysis
Irina Matveeva
Department of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Department of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
Term translation probabilities proved an
effective method of semantic smoothing in
the language modelling approach to infor-
mation retrieval tasks. In this paper, we
use Generalized Latent Semantic Analysis
to compute semantically motivated term
and document vectors. The normalized
cosine similarity between the term vec-
tors is used as term translation probabil-
ity in the language modelling framework.
Our experiments demonstrate that GLSA-
based term translation probabilities cap-
ture semantic relations between terms and
improve performance on document classi-
fication.
1 Introduction
Many recent applications such as document sum-
marization, passage retrieval and question answer-
ing require a detailed analysis of semantic rela-
tions between terms since often there is no large
context that could disambiguate words?s meaning.
Many approaches model the semantic similarity
between documents using the relations between
semantic classes of words, such as representing
dimensions of the document vectors with distri-
butional term clusters (Bekkerman et al, 2003)
and expanding the document and query vectors
with synonyms and related terms as discussed
in (Levow et al, 2005). They improve the per-
formance on average, but also introduce some in-
stability and thus increased variance (Levow et al,
2005).
The language modelling approach (Ponte and
Croft, 1998; Berger and Lafferty, 1999) proved
very effective for the information retrieval task.
Berger et. al (Berger and Lafferty, 1999) used
translation probabilities between terms to account
for synonymy and polysemy. However, their
model of such probabilities was computationally
demanding.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. Using a bag-of-words docu-
ment vectors (Salton and McGill, 1983), it com-
putes a dual representation for terms and docu-
ments in a lower dimensional space. The resulting
document vectors reside in the space of latent se-
mantic concepts which can be expressed using dif-
ferent words. The statistical analysis of the seman-
tic relatedness between terms is performed implic-
itly, in the course of a matrix decomposition.
In this project, we propose to use a combi-
nation of dimensionality reduction and language
modelling to compute the similarity between doc-
uments. We compute term vectors using the Gen-
eralized Latent Semantic Analysis (Matveeva et
al., 2005). This method uses co-occurrence based
measures of semantic similarity between terms
to compute low dimensional term vectors in the
space of latent semantic concepts. The normalized
cosine similarity between the term vectors is used
as term translation probability.
2 Term Translation Probabilities in
Language Modelling
The language modelling approach (Ponte and
Croft, 1998) proved very effective for the infor-
mation retrieval task. This method assumes that
every document defines a multinomial probabil-
ity distribution p(w|d) over the vocabulary space.
Thus, given a query q = (q1, ..., qm), the like-
lihood of the query is estimated using the docu-
ment?s distribution: p(q|d) = ?m1 p(qi|d), where
151
qi are query terms. Relevant documents maximize
p(d|q) ? p(q|d)p(d).
Many relevant documents may not contain the
same terms as the query. However, they may
contain terms that are semantically related to the
query terms and thus have high probability of
being ?translations?, i.e. re-formulations for the
query words.
Berger et. al (Berger and Lafferty, 1999) in-
troduced translation probabilities between words
into the document-to-query model as a way of se-
mantic smoothing of the conditional word proba-
bilities. Thus, they query-document similarity is
computed as
p(q|d) =
m
?
i
?
w?d
t(qi|w)p(w|d). (1)
Each document word w is a translation of a query
term qi with probability t(qi|w). This approach
showed improvements over the baseline language
modelling approach (Berger and Lafferty, 1999).
The estimation of the translation probabilities is,
however, a difficult task. Lafferty and Zhai used
a Markov chain on words and documents to es-
timate the translation probabilities (Lafferty and
Zhai, 2001). We use the Generalized Latent Se-
mantic Analysis to compute the translation proba-
bilities.
2.1 Document Similarity
We propose to use low dimensional term vectors
for inducing the translation probabilities between
terms. We postpone the discussion of how the term
vectors are computed to section 2.2. To evaluate
the validity of this approach, we applied it to doc-
ument classification.
We used two methods of computing the sim-
ilarity between documents. First, we computed
the language modelling score using term transla-
tion probabilities. Once the term vectors are com-
puted, the document vectors are generated as lin-
ear combinations of term vectors. Therefore, we
also used the cosine similarity between the docu-
ments to perform classificaiton.
We computed the language modelling score of
a test document d relative to a training document
di as
p(d|di) =
?
v?d
?
w?di
t(v|w)p(w|di). (2)
Appropriately normalized values of the cosine
similarity measure between pairs of term vectors
cos(~v, ~w) are used as the translation probability
between the corresponding terms t(v|w).
In addition, we used the cosine similarity be-
tween the document vectors
?~di, ~dj? =
?
w?di
?
v?dj
?diw ?
dj
v ?~w,~v?, (3)
where ?diw and ?
dj
v represent the weight of the
terms w and v with respect to the documents di
and dj , respectively.
In this case, the inner products between the term
vectors are also used to compute the similarity be-
tween the document vectors. Therefore, the cosine
similarity between the document vectors also de-
pends on the relatedness between pairs of terms.
We compare these two document similarity
scores to the cosine similarity between bag-of-
word document vectors. Our experiments show
that these two methods offer an advantage for doc-
ument classification.
2.2 Generalized Latent Semantic Analysis
We use the Generalized Latent Semantic Analy-
sis (GLSA) (Matveeva et al, 2005) to compute se-
mantically motivated term vectors.
The GLSA algorithm computes the term vectors
for the vocabulary of the document collection C
with vocabulary V using a large corpus W . It has
the following outline:
1. Construct the weighted term document ma-
trix D based on C
2. For the vocabulary words in V , obtain a ma-
trix of pair-wise similarities, S, using the
large corpus W
3. Obtain the matrix UT of low dimensional
vector space representation of terms that pre-
serves the similarities in S, UT ? Rk?|V |
4. Compute document vectors by taking linear
combinations of term vectors D? = UTD
The columns of D? are documents in the k-
dimensional space.
In step 2 we used point-wise mutual informa-
tion (PMI) as the co-occurrence based measure of
semantic associations between pairs of the vocab-
ulary terms. PMI has been successfully applied to
semantic proximity tests for words (Turney, 2001;
Terra and Clarke, 2003) and was also success-
fully used as a measure of term similarity to com-
pute document clusters (Pantel and Lin, 2002). In
152
our preliminary experiments, the GLSA with PMI
showed a better performance than with other co-
occurrence based measures such as the likelihood
ratio, and ?2 test.
PMI between random variables representing
two words, w1 and w2, is computed as
PMI(w1, w2) = log
P (W1 = 1,W2 = 1)
P (W1 = 1)P (W2 = 1)
.
(4)
We used the singular value decomposition
(SVD) in step 3 to compute GLSA term vectors.
LSA (Deerwester et al, 1990) and some other
related dimensionality reduction techniques, e.g.
Locality Preserving Projections (He and Niyogi,
2003) compute a dual document-term representa-
tion. The main advantage of GLSA is that it fo-
cuses on term vectors which allows for a greater
flexibility in the choice of the similarity matrix.
3 Experiments
The goal of the experiments was to understand
whether the GLSA term vectors can be used to
model the term translation probabilities. We used
a simple k-NN classifier and a basic baseline to
evalute the performance. We used the GLSA-
based term translation probabilities within the lan-
guage modelling framework and GLSA document
vectors.
We used the 20 news groups data set because
previous studies showed that the classification per-
formance on this document collection can notice-
ably benefit from additional semantic informa-
tion (Bekkerman et al, 2003). For the GLSA
computations we used the terms that occurred in
at least 15 documents, and had a vocabulary of
9732 terms. We removed documents with fewer
than 5 words. Here we used 2 sets of 6 news
groups. Groupd contained documents from dis-
similar news groups1, with a total of 5300 docu-
ments. Groups contained documents from more
similar news groups2 and had 4578 documents.
3.1 GLSA Computation
To collect the co-occurrence statistics for the sim-
ilarities matrix S we used the English Gigaword
collection (LDC). We used 1,119,364 New York
Times articles labeled ?story? with 771,451 terms.
1os.ms, sports.baseball, rec.autos, sci.space, misc.forsale,
religion-christian
2politics.misc, politics.mideast, politics.guns, reli-
gion.misc, religion.christian, atheism
Groupd Groups
#L tf Glsa LM tf Glsa LM
100 0.58 0.75 0.69 0.42 0.48 0.48
200 0.65 0.78 0.74 0.47 0.52 0.51
400 0.69 0.79 0.76 0.51 0.56 0.55
1000 0.75 0.81 0.80 0.58 0.60 0.59
2000 0.78 0.83 0.83 0.63 0.64 0.63
Table 1: k-NN classification accuracy for 20NG.
Figure 1: k-NN with 400 training documents.
We used the Lemur toolkit3 to tokenize and in-
dex the document; we used stemming and a list of
stop words. Unless stated otherwise, for the GLSA
methods we report the best performance over dif-
ferent numbers of embedding dimensions.
The co-occurrence counts can be obtained using
either term co-occurrence within the same docu-
ment or within a sliding window of certain fixed
size. In our experiments we used the window-
based approach which was shown to give better
results (Terra and Clarke, 2003). We used the win-
dow of size 4.
3.2 Classification Experiments
We ran the k-NN classifier with k=5 on ten ran-
dom splits of training and test sets, with different
numbers of training documents. The baseline was
to use the cosine similarity between the bag-of-
words document vectors weighted with term fre-
quency. Other weighting schemes such as max-
imum likelihood and Laplace smoothing did not
improve results.
Table 1 shows the results. We computed the
score between the training and test documents us-
ing two approaches: cosine similarity between the
GLSA document vectors according to Equation 3
(denoted as GLSA), and the language modelling
score which included the translation probabilities
between the terms as in Equation 2 (denoted as
3http://www.lemurproject.org/
153
LM ). We used the term frequency as an estimate
for p(w|d). To compute the matrix of translation
probabilities P , where P [i][j] = t(tj|ti) for the
LMCLSA approach, we first obtained the matrix
P? [i][j] = cos(~ti, ~tj). We set the negative and zero
entries in P? to a small positive value. Finally, we
normalized the rows of P? to sum up to one.
Table 1 shows that for both settings GLSA and
LM outperform the tf document vectors. As ex-
pected, the classification task was more difficult
for the similar news groups. However, in this
case both GLSA-based approaches outperform the
baseline. In both cases, the advantage is more
significant with smaller sizes of the training set.
GLSA and LM performance usually peaked at
around 300-500 dimensions which is in line with
results for other SVD-based approaches (Deer-
wester et al, 1990). When the highest accuracy
was achieved at higher dimensions, the increase
after 500 dimensions was rather small, as illus-
trated in Figure 1.
These results illustrate that the pair-wise simi-
larities between the GLSA term vectors add im-
portant semantic information which helps to go
beyond term matching and deal with synonymy
and polysemy.
4 Conclusion and Future Work
We used the GLSA to compute term translation
probabilities as a measure of semantic similarity
between documents. We showed that the GLSA
term-based document representation and GLSA-
based term translation probabilities improve per-
formance on document classification.
The GLSA term vectors were computed for all
vocabulary terms. However, different measures of
similarity may be required for different groups of
terms such as content bearing general vocabulary
words and proper names as well as other named
entities. Furthermore, different measures of sim-
ilarity work best for nouns and verbs. To extend
this approach, we will use a combination of sim-
ilarity measures between terms to model the doc-
ument similarity. We will divide the vocabulary
into general vocabulary terms and named entities
and compute a separate similarity score for each
of the group of terms. The overall similarity score
is a function of these two scores. In addition, we
will use the GLSA-based score together with syn-
tactic similarity to compute the similarity between
the general vocabulary terms.
References
Ron Bekkerman, Ran El-Yaniv, and Naftali Tishby.
2003. Distributional word clusters vs. words for text
categorization.
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proc. of the 22rd
ACM SIGIR.
Scott C. Deerwester, Susan T. Dumais, ThomasK. Lan-
dauer, GeorgeW. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Proc. of NIPS.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proc. of the 24th
ACM SIGIR, pages 111?119, New York, NY, USA.
ACM Press.
Gina-Anne Levow, Douglas W. Oard, and Philip
Resnik. 2005. Dictionary-based techniques for
cross-language information retrieval. Information
Processing and Management: Special Issue on
Cross-language Information Retrieval.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat,
and Christian Royer. 2005. Generalized latent se-
mantic analysis for term representation. In Proc. of
RANLP.
Patrick Pantel and Dekang Lin. 2002. Document clus-
tering with committees. In Proc. of the 25th ACM
SIGIR, pages 199?206. ACM Press.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Proc.
of the 21st ACM SIGIR, pages 275?281, New York,
NY, USA. ACM Press.
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. McGraw-
Hill.
Egidio L. Terra and Charles L. A. Clarke. 2003. Fre-
quency estimates for statistical word similarity mea-
sures. In Proc.of HLT-NAACL.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI?IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491?502.
154
Turn-taking in Mandarin Dialogue:
Interactions of Tone and Intonation
Gina-Anne Levow
Computer Science Department
University of Chicago
Chicago, IL 60637 USA
levow@cs.uchicago.edu
Abstract
Fluent dialogue requires that speak-
ers successfully negotiate and signal
turn-taking. While many cues to turn
change have been proposed, especially
in multi-modal frameworks, here we fo-
cus on the use of prosodic cues to these
functions. In particular, we consider
the use of prosodic cues in a tone lan-
guage, Mandarin Chinese, where varia-
tions in pitch height and slope addition-
ally serve to determine word meaning.
Within a corpus of spontaneous Chi-
nese dialogues, we find that turn-unit fi-
nal syllables are significantly lower in
average pitch and intensity than turn-
unit initial syllables in both smooth
turn changes and segments ended by
speaker overlap. Interruptions are char-
acterized by significant prosodic dif-
ferences from smooth turn initiations.
Furthermore, we demonstrate that these
contrasts correspond to an overall low-
ering across all tones in final posi-
tion, which largely preserves the rela-
tive heights of the lexical tones. In clas-
sification tasks, we contrast the use of
text and prosodic features. Finally, we
demonstrate that, on balanced training
and test sets, we can distinguish turn-
unit final words from other words at
? 93% accuracy and interruptions from
smooth turn unit initiations at 62% ac-
curacy.
1 Introduction
Fluent dialogues require effective turn transitions
between speakers. Research in turn-taking, typ-
ified by (Duncan, 1974), posits several key sig-
nals for turn-taking, including a turn-change sig-
nal which offers to cede the floor, speaker-state
signal which indicates taking the floor, within-
turn signal, and continuation signals. This pro-
cess fundamentally requires cooperation between
participants both to produce the contextually ap-
propriate signals and to interpret those of their in-
terlocutor. These analyses have proposed a wide
range of cues to turn status, ranging from gaze and
gesture in a multi-modal context to prosodic cues
including pitch, intensity, and duration as well as
lexical and syntactic cues.
Much of this fundamental research as well
as computational implementations have focused
on English, a language with well-studied intona-
tional sentence and discourse structure. A sub-
stantial body of work has identified sentence-like
units as well as fragments and repairs in con-
versational speech, including (Ostendorf, forth-
coming; Liu et al, 2004; Shriberg et al, 2001)
These approaches have employed lexical and
prosodic cues in diverse frameworks, including
Hidden Markov Models employing decision trees
and hidden state language models, neural net-
works, and maximum entropy models. (Shriberg
et al, 2001) identified jump-in points and jump-
in words in multi-party meeting speech using
prosodic and language model features at accura-
cies of 65 and 77% under equal priors. Further-
more, (Ward and Tsukahara, 2000) demonstrated
that backchannels occurred at predictable points
72
with specific prosodic characteristics in both En-
glish and Japanese.
In the current paper, we consider the interac-
tion of potential prosodic intonational cues re-
lated to turn-taking with the realization of lexical
tone in a tone language, Putonghua or Mandarin
Chinese. Mandarin employs four canonical lexi-
cal tones distinguished by pitch height and pitch
contour: high level, mid-rising, low falling-rising,
and high falling. We explore whether prosodic
features are also employed in turn-taking behav-
ior in this language and whether the forms are
comparable to those employed in languages with
lexical tone. We demonstrate that intonational
cues quite similar to those in English are also
employed in Chinese with lower pitch and in-
tensity at ends of turn units than at the begin-
nings of those turn units. Interruptions likewise
are distinguished from smooth turn transitions by
prosodic means, including greater pitch elevation.
We demonstrate how these changes interact with
lexical tone by substantial lowering of average
pitch height across all tones in final positions and
contrast pitch contours in final and non-final po-
sitions. Finally, these cues in conjunction with
silence and durational features can be employed
to distinguish turn-unit final words from non-final
words in the dialogue and words that initiate in-
terruptions from those which start smoother turn
transitions.
In the remainder of the paper we will briefly de-
scribe the data set employed in these experiments
and the basic extraction of prosodic features (Sec-
tion 2). We then present acoustic analyses con-
trasting turn unit final and turn unit initial sylla-
bles under different turn transition types (Section
3). We will describe the impact of these intona-
tional cues on the realization of lexical tone (Sec-
tion 4). Finally we will apply these prosodic con-
trasts to enable classification of words for finality
and interruption status (Section 5).
2 Experimental Data
The data in this study was drawn from the Tai-
wanese Putonghua Speech Corpus1. The mate-
rials chosen include 5 spontaneous dialogues by
Taiwanese speakers of Mandarin, seven females
1Available from http://www.ldc.upenn.edu
and three males. The dialogues, averaging 20
minutes in duration, were recorded on two chan-
nels, one per speaker, in a quiet room and digi-
tized at 16KHz sampling. The recordings were
later manually transcribed and segmented into
words; turn-beginnings and overlaps were time-
stamped. The manual word segmentation was
based on both syntax and phonology according to
a methodology described in detail in (Duanmu,
1996).
2.1 Prosodic Features
For the subsequent analysis, the conversa-
tions were divided into chunks based on the
turn and overlap time-stamps. Using a Chi-
nese character-to-pinyin dictionary and a hand-
constructed mapping of pinyin sequences to
ARPABET phonemes, the transcribed text was
then force-aligned to the corresponding audio
segments using the language porting mechanism
in the University of Colorado Sonic speech rec-
ognizer (Pellom et al, 2001). The resulting align-
ment provided phone, syllable, and word dura-
tions as well as silence positions and durations.
Pitch and intensity values for voiced regions
were computed using the functions ?To Pitch? and
?To Intensity? in the freely available Praat acous-
tic analysis software package(Boersma, 2001).
We then computed normalized pitch and inten-
sity values based on log-scaled z-score normal-
ization of each conversation side. Based on the
above alignment, we then computed maximum
and mean pitch and intensity values for each syl-
lable and word for all voiced regions. Given the
presence of lexical tone, we extracted five points
evenly distributed across the ?final? region of the
syllable, excluding the initial consonant, if any.
We then estimated the linear syllable slope based
on the latter half of this region in which the effects
of tonal coarticulation are likely to be minimized
under the pitch target approximation model(Xu,
1997).
3 Acoustic Analysis of Turn-taking
Each of the turn units extracted above was tagged
based on its starting and ending conditions as one
of four types: smooth, rough, intersmooth, and
interrough. ?Smooth? indicates a segment-ending
transition from one speaker to another, not caused
73
Position Start Start
?Overlap or +Overlap
?Spkr Change +Spkr Change
End Smooth Intersmooth
?Overlap 1413 289
End Rough Interrough
+Overlap 407 68
Table 1: Types of turn units
by the start of overlap with another speaker. By
contrast, a rough transition indicates the end of a
chunk at the start of overlap with another speaker.
The prefix ?inter? indicates the turn began with
an interruption, identified by overlap with the pre-
vious speaker and change of speaker holding the
floor. In this class, the new speaker continues to
hold the floor after the period of overlap.
We contrast turn unit initial and turn unit final
syllables for each type of transition and across all
turns. We compare mean pitch and mean intensity
in each case. We find in all cases highly signifi-
cant differences between mean pitch of turn unit
initial syllables and mean pitch of final syllables
(p < 0.0001) as illustrated in Figure 1. Syllables
in initial position have much higher log-scaled
mean pitch in all conditions. For intensity, we
find highly significant differences across all con-
ditions (p < 0.005), with initial syllables having
higher amplitude than final syllables. These con-
trasts appear in Figure 2. Furthermore, compar-
ing final intensity of transitions not marked by the
start of overlap with the intensity of the final pre-
overlap syllable in a transition caused by overlap,
we find significantly higher normalized mean in-
tensity in all rough transitions relative to others.
In contrast, comparable differences in pitch do not
reach significance.
Finally we compare smooth turn unit initia-
tions (?smooth?) to successful interruptions (?in-
terrough?,?intersmooth?), contrasting initial syl-
lables in each class. Here we find that both nor-
malized mean pitch (Figure 3) and normalized
mean intensity (Figure 4) in turn unit initial sylla-
bles are significantly higher in interruptions than
in ?smooth? turn transitions.2 Speakers use these
2If one compares both ?smooth? and ?rough? transitions
to ?intersmooth? and ?interrough? transitions, initial syl-
lables are significantly higher in pitch for the interruption
Figure 1: Pitch contrasts between syllables in ini-
tial and final position across turn types. Values for
initial position are in grey, final position in black.
Figure 2: Intensity contrasts between syllables in
initial and final position across turn types. Val-
ues for initial position are in grey, final position
in black.
74
Figure 3: Pitch contrasts between initial syllables
in smooth turn transitions and interruptions. Val-
ues for smooth transitions are in black, interrup-
tions in grey.
Figure 4: Intensity contrasts between initial sylla-
bles in smooth turn transitions and interruptions.
Values for smooth transitions are in black, inter-
ruptions in grey.
prosodic cues to take the floor by interruption.
These descriptive analyses demonstrate that in-
tonational cues to turn-taking do play a role in a
tone language. Not only does intensity play a sig-
nificant role, but pitch also is employed to dis-
tinguish initiation and finality, in spite of its con-
current use in determining lexical identity. In the
following section, we describe the effects on tone
height and tone shape caused by these broader in-
tonational phenomena.
4 Tone and Intonation
We have determined that syllables in turn unit fi-
nal position have dramatically reduced average
pitch relative to those in turn unit initial posi-
tion, and these contrasts can serve to signal turn-
change and speaker change as suggested by (Dun-
classes, but differences for intensity do not reach signifi-
cance (p = 0.053)
Figure 5: Contrasts in average pitch for the four
canonical tones in turn non-final and final posi-
tions. Values for non-final positions are in grey,
final positions in black.
can, 1974). How do these changes interact with
lexical identity and lexical tone? Since tone oper-
ates on a syllable in Chinese, we consider the av-
erage pitch and tone contours of syllables in final
and non-final position. We find that average pitch
for all tones is reduced, and relative tone height
is largely preserved.3 Thus a final high tone is
readily distinguishable from a final low tone, if
the listener can interpret the syllable as turn-final.
The contrasts appear in Figure 5.
Turning to tone contour, we find likewise lit-
tle change between non-final and final contours,
with the contours running parallel, but at a much
lower pitch.4 For illustration, mid-rising and high
falling tones are shown in Figure 6. Compara-
ble behavior has been observed at other discourse
boundaries such as story boundaries in newswire
speech. (Levow, 2004).
5 Recognizing Turn Unit Boundaries
and Interruptions
Based on the salient contrasts in pitch and in-
tensity observed above, we employ prosodic fea-
tures both to identify turn boundaries and to dis-
tinguish between the start of interruptions and
smooth transitions. We further contrast the use
of prosodic features with text n-gram features.
3This analysis excludes exclamatory and interjective par-
ticles.
4It is also true that contours do not always match their
canonical forms even in non-final position. This variation
may be attributed to a combination of tonal coarticulatory
effects and the presence of other turn-internal boundaries.
75
Figure 6: Contrasts in pitch contour for mid-
rising and high falling tones in turn non-final and
final positions. Values for non-final positions are
in heavy lines, final positions in thin lines. Mid-
rising tone is in black, dashed lines, high falling
in solid lines.
5.1 Classifier Features: Prosodic
The features used in the classifier trained to rec-
ognize turn boundaries and turn types fall into two
classes: local and contextual. The local features
describe the words or syllables themselves, while
the contextual features capture contrasts between
adjacent words or syllables. The first set of fea-
tures thus includes the mean pitch and mean in-
tensity for the current word and syllable, the word
duration, and the maximum pitch and intensity
for the syllable. The second set of features in-
clude the length of any following silence and the
differences in mean pitch and mean intensity be-
tween the current word or syllable and the follow-
ing word or syllable.
5.2 Classifier Features: Text
For contrastive purposes, we also consider the use
of textual features for turn boundary and bound-
ary type classification.5 Here we exploit syllable
and word features, as well as syllable n-gram fea-
tures. We use the toneless pinyin representation
of the current word and the final syllable in each
word. Such features aim to capture, for example,
question particles that signal the end of a turn.
In addition, we extracted the five preceding and
five following syllables in the sequence around
the current syllable. We then experimented with
different window widths for n-gram construction,
5All text features are drawn from the ground truth manual
text transcripts.
ranging from one to five, as supported by the clas-
sifier described below.
5.3 Classifiers
We performed experiments with several classi-
fiers: Boostexter (Schapire and Singer, 2000),
a well-known implementation of boosted weak
learners, multi-class Support Vector Machines
with linear kernels (C-C.Cheng and Lin, 2001),
an implementation of a margin-based discrimina-
tive classifier, and decision trees, implemented by
C4.5(Quinlan, 1992). All classifiers yielded com-
parable results on this classification task. Here
we present the results using Boostexter to ex-
ploit its support for text features and automatic n-
gram feature selection as well as its relative inter-
pretability. We used downsampled balanced train-
ing and test sets to enable assessment of the utility
of these features for classification and employed
10-fold cross-validation, presenting the average
over the runs.
5.3.1 Recognizing Turn Unit Ends
Using the features above we created a set of
1610 turn unit final words and 1610 non-final
words. Based on 10-fold cross-validation, using
combined text and prosodic features, we obtain an
accuracy of 93.1% on this task. The key prosodic
features in this classification are silence duration,
which is the first feature selected, and maximum
intensity. The highest lexical features are preced-
ing ?ta?, preceding ?ao?, and following ?dui?. If
silence features are excluded, classification accu-
racy drops substantially to 69%, still better than
the 50% chance baseline for this set. In this case,
syllable mean intensity features become the first
selected for classification.
We also consider the relative effectiveness of
classifiers based on text, with silence, or prosodic
features alone. We find that, when silence
duration features are available, both text- and
prosodic-based classifiers perform comparably at
93.5% and 93.7% accuracy respectively, near the
effectiveness of the combined text and prosodic
classifier. However, when silence features are
excluded, a greater difference is found between
classification based on text features and classifi-
cation based on prosodic features. Specifically,
without silence information, classification based
76
on text features alone reaches only 59.5%. How-
ever, classification based on prosodic features re-
mains somewhat more robust, though still with a
substantial drop in accuracy, at 66.5% for prosody
only. This finding suggests that although the pres-
ence of a longer silence interval is the best cue to
finality, additional prosodic features, such as dif-
ferences in pitch and intensity, concurrently sig-
nal the opportunity for another speaker to start a
turn. Text features, especially in highly disflu-
ent conversational speech, provide less clear ev-
idence. Results appear in Table 5.3.1.
5.3.2 Recognizing Interruptions
In order to create a comparable context for ini-
tial words in interruption and smoothly initiated
turns, we reversed the direction of the contex-
tual comparisons, comparing the preceding word
features to those of the current word and mea-
suring pre-word silences rather than following
silences. Using this configuration, we created
a set of 218 interruption initial words and 218
smooth transition initial words, following smooth
transitions without overlap. Based on 10-fold
cross-validation for this downsampled balanced
case, we obtain an accuracy of 62%, relative to a
50% baseline. The best classifiers employed only
prosodic features with silence duration and nor-
malized mean word pitch. Addition of text fea-
tures degrades test set performance as the classi-
fier rapidly overfits to the training materials. If we
exclude silence related features, accuracy drops to
almost chance.
6 Discussion and Conclusion
We have demonstrated that even in a language
that employs lexical tone, cues to turn and speaker
status are still carried by prosodic means, includ-
ing pitch. Specifically, turn unit initial syllables
have significantly higher mean pitch and inten-
sity than do final syllables in different turn transi-
tion types. The elevation of initial pitch is further
enhanced in interruptions, when a new speaker
seizes the floor beginning with an overlap of the
current speaker. These contrasts are similar to
those observed for English, and consistent with
signals described in the literature. These changes
result in an overall reduction in pitch for sylla-
bles in final position across all tones, but relative
pitch height employed by lexical tone is preserved
in most cases. Finally, we employed these cues to
train classifiers to distinguish turn unit final words
from other words in the dialogue and to distin-
guish interruption initial words from initial words
in smooth transitions with no overlap.
We further contrasted the utility of text and
prosodic features for the identification of turn
boundary position and type. We find that silence
is the most reliable cue both to identify final turn
boundaries and to distinguish types of turn tran-
sitions. In conjunction with silence features, text,
prosodic, and joint text-prosodic features can fare
comparably. However, for turn unit boundaries,
the availability of a variety of prosodic features
proves to be essential for relatively better classifi-
cation in the absence of silence information.
In future work, we plan to enhance tone recog-
nition by better contextual modeling that will
compensate for the effects of a variety of dis-
course boundaries, including turn and topic, on
tone realization. We also plan to embed turn-
based classification in a sequential discriminative
model to further facilitate integration of prosodic
and lexical features as well as sequence con-
straints on turn structure.
Acknowledgments
We would like to thank the developers of the Tai-
wanese Putonghua corpus and the Linguistic Data
Consortium for the provision of these resources.
This work was supported by NSF Grant 0414919.
References
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9?10):341?345.
C-C.Cheng and C-J. Lin. 2001. LIBSVM:a library
for support vector machines. Software available at:
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
San Duanmu. 1996. Notes on word boundaries. Tai-
wanese Putonghua Corpus Documentation.
S. Duncan, 1974. Some signals and rules for taking
speaking turns in conversations, pages 298?311.
Gina-Anne Levow. 2004. Prosody-based topic seg-
mentation for mandarin broadcast news. In Pro-
ceedings of HLT-NAACL 2004, Companion Volume,
pages 137?140.
77
Prosody Only Text Prosody + Text
With silence 93.7% 93.5% 93.1%
Without silence 66.5% 59.5% 69%
Table 2: Recognition of turn unit final vs. non-final words
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2004.
Comparing and combining generative and poste-
rior probability models: Some advances in sentence
boundary detection in speech. In Proceedings of
Conf. on Empirical Methods in Natural Language
Processing.
M. Ostendorf. forthcoming. Prosodic boundary de-
tection. In M. Horne, editor, Prosody: Theory
and Experiment. Studies Presented to Gosta Bruce.
Kluwer.
B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang,
X. Yu, and S. Pradhan. 2001. University of Col-
orado dialog systems for travel and navigation.
J.R. Quinlan. 1992. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Robert E. Schapire and Yoram Singer. 2000. Boost-
exter: A boosting-based system for text categoriza-
tion. Machine Learning, 39(2?3):135?168.
E. Shriberg, A. Stolcke, and D. Baron. 2001. Can
prosody aid the automatic processing of multi-party
meetings? evidence from predicting punctuation,
disfluencies, and overlapping speech. In Proc. of
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding.
N. Ward and W. Tsukahara. 2000. Prosodic features
which cue back-channel responses in English and
Japanese. Journal of Pragmatics, 23:1177?1207.
Yi Xu. 1997. Contextual tonal variations in Mandarin.
Journal of Phonetics, 25:62?83.
78
Automatic Prosodic Labeling with Conditional Random Fields and Rich
Acoustic Features
Gina-Anne Levow
University of Chicago
Department of Computer Science
1100 E. 58th St.
Chicago, IL 60637 USA
levow@cs.uchicago.edu
Abstract
Many acoustic approaches to prosodic la-
beling in English have employed only lo-
cal classifiers, although text-based classifi-
cation has employed some sequential mod-
els. In this paper we employ linear chain and
factorial conditional random fields (CRFs)
in conjunction with rich, contextually-based
prosodic features, to exploit sequential de-
pendencies and to facilitate integration with
lexical features. Integration of lexical and
prosodic features improves pitch accent pre-
diction over either feature set alne, and for
lower accuracy feature sets, factorial CRF
models can improve over linear chain based
prediction of pitch accent.
1 Introduction
Prosody plays a crucial role in language understand-
ing. In addition to the well-known effects in tone
languages such as Chinese, prosody in English also
plays a significant role, where pitch accents can
indicate given/new information status, and bound-
ary tones can distinguish statements from yes-no
questions. However, recognition of such prosodic
features poses significant challenges due to differ-
ences in surface realization from the underlying
form. In particular, context plays a significant role
in prosodic realization. Contextual effects due ar-
ticulatory constraints such maximum speed of pitch
change (Xu and Sun, 2002) from neighboring sylla-
bles and accents can yield co-articulatory effects at
the intonational level, analogous to those at the seg-
mental level. Recent phonetic research (Xu, 1999;
Sun, 2002; Shen, 1990) has demonstrated the im-
portance of coarticulation for tone and pitch accent
recognition. In addition context affects interpreta-
tion of prosodic events; an accent is viewed as high
or low relative to the speaker?s pitch range and also
relative to adjacent speech.
Some recent acoustically focused approaches
(Sun, 2002; Levow, 2005) to tone and pitch accent
recognition have begun to model and exploit these
contextual effects on production. Following the Par-
allel Encoding and Target Approximation (PENTA)
(Xu, 2004), this work assumes that the prosodic tar-
get is exponentially approached during the course of
syllable production, and thus the target is best ap-
proximated in the later portion of the syllable. Other
contextual evidence such as relative pitch height or
band energy between syllables has also been em-
ployed (Levow, 2005; Rosenberg and Hirschberg,
2006). Interestingly, although earlier techniques
(Ross and Ostendorf, 1994; Dusterhoff et al, 1999)
employed Hidden Markov Models, they did not ex-
plicitly model these coarticulatory effects, and re-
cent approaches have primarily employed local clas-
sifiers, such as decision trees (Sun, 2002; Rosenberg
and Hirschberg, 2006) or Support Vector Machines
(Levow, 2005).
Another body of work on pitch accent recog-
nition has focused on exploitation of lexical and
syntactic information to predict ToBI labels, for
example for speech synthesis. These approaches
explored a range of machine learning techniques
from local classifiers such as decision trees (Sun,
2002) and RIPPER (Pan andMcKeown, 1998) to se-
quence models such as Conditional Random Fields
217
(CRFs)(Gregory and Altun, 2004) more recently.
The systems often included features that captured lo-
cal or longer range context, such as n-gram probabil-
ities, neighboring words, or even indicators of prior
mention. (Chen et al, 2004; Rangarajan Sridhar et
al., 2007) explored the integration of based prosodic
and lexico-syntactic evidence in GMM-based and
maximum entropy models respectively.
Here we explore the use of contextual acous-
tic and lexical models within a sequence learning
framework. We analyze the interaction of differ-
ent feature types on prediction of prosodic labels us-
ing linear-chain CRFs. We demonstrate improved
recognition by integration of textual and acoustic
cues, well-supported by the sequence model. Finally
we consider the joint prediction of multiple prosodic
label types, finding improvement for joint modeling
in the case of feature sets with lower initial perfor-
mance.
We begin by describing the ToBI annotation task
and our experimental data. We then discuss the
choice of conditional random fields and the use of
linear chain and factorial models. Section 4 de-
scribes the contextual acoustic model and text-based
features. Section 5 presents the experimental struc-
ture and results. We conclude with a brief discussion
of future work.
2 Data
We employ a subset of the Boston Radio News Cor-
pus (Ostendorf et al, 1995), employing data from
speakers f1a, f2b, m1b, and m2b, for experimen-
tal consistency with (Chen et al, 2004; Rangara-
jan Sridhar et al, 2007). The corpus includes pitch
accent, phrase and boundary tone annotation in the
ToBI framework (Silverman et al, 1992) aligned
with manual transcription and manual and automatic
syllabification of the materials. Each word was
also manually part-of-speech tagged. The data com-
prises over forty thousand syllables, with speaker
f2b accounting for just over half the data. Fol-
lowing earlier research (Ostendorf and Ross, 1997;
Sun, 2002), we collapse the ToBI pitch accent labels
to four classes: unaccented, high, low, and down-
stepped high for experimentation, removing distinc-
tions related to bitonal accents. We also consider the
binary case of distinguishing accented from unac-
cented syllables, (Gregory and Altun, 2004; Rosen-
berg and Hirschberg, 2006; Ananthakrishnan and
Narayanan, 2006). For phrase accents and bound-
ary tones, we consider only the binary distinction
between phrase accent/no phrase accent and bound-
ary tone/no boundary tone.
All experiments evaluate automatic prosodic la-
beling at the syllable level.
3 Modeling with Linear-Chain and
Factorial CRFs
Most prior acoustically based approaches to
prosodic labeling have used local classifiers. How-
ever, on phonological grounds, we expect that cer-
tain label sequences will be much more probable
than others. For example, sequences of multiple
high accents are relatively uncommon in contrast to
the case of an unaccented syllable preceding an ac-
cented one. This characteristic argues for a model
which encodes and exploits inter-label dependen-
cies. Furthermore, under the ToBI labeling guide-
lines, the presence of a boundary tone dictates the
co-occurrence of a phrase accent label. To capture
these relations between labels of different types, we
also consider factorial models.
Conditional Random Fields (Lafferty et al, 2001)
are a class of graphical models which are undirected
and conditionally trained. While they can repre-
sent long term dependencies, most applications have
employed first-order linear chains for language and
speech processing tasks including POS tagging, sen-
tence boundary detection (Liu et al, 2005), and
even text-oriented pitch accent prediction(Gregory
and Altun, 2004). The models capture sequential
label-label relations, but unlike HMMs, the condi-
tionally trained model can more tractably support
larger text-based feature sets. Factorial CRFs (Sut-
ton, 2006; McCallum et al, 2003) augment the lin-
ear sequence model with additional cotemporal la-
bels, so that multiple (factors) labels are predicted
at each time step and dependencies between them
can be modeled. Examples of linear-chain and fac-
torial CRFs appear in Figure 1. In the linear chain
example, the fi items correspond to the features and
the yi to labels to be predicted, for example prosodic
and text features and pitch accent labels respectively.
The vertical lines correspond to the dependencies
218
y1 y2 y3
f1 f2 f3
z1 z2 z3
y1 y2 y3
x1 x2 x3
f1 f2 f3
Figure 1: Linear-chain CRF (top) and Two-level
Factorial CRF (bottom).
between the features and labels; the horizontal lines
indicate the dependencies between the labels in se-
quence. In the factorial CRF example, the fi again
represent the features, while the xi, yi, and zi repre-
sent the boundary tone, phrase accent, and pitch ac-
cent labels that are being predicted. The horizontal
arcs again model the sequential bigram label-label
dependencies between labels of the same class; the
vertical arcs model the dependencies between both
the features and labels, and bigram dependencies be-
tween the labels of each of the different pairs of fac-
tors. Thus, we jointly predict pitch accent, phrase
accent, and boundary tone and, the prediction of
each label depends on the features, the other labels
predicted for the same syllable, and the sequential
label of the same class. So, pitch accent prediction
depends on the features, pitch accent predicted for
the neighboring syllable, and phrase and boundary
tone predictions for the current syllable.
We employ the Graphical Models for Mallet
(GRMM) implementation (Sutton, 2006), adapted
to also support the real-valued acoustic features re-
quired for these experiments; in some additional
contrastive experiments on zero order models, we
also employ the Mallet implementation (McCallum,
2002). We employ both linear chain and three-level
factorial CRFs, as above, to perform prosodic label-
ing.
4 Feature Representation
We exploit both lexical and prosodic features for
prosodic labeling of broadcast news speech. In par-
ticular, in contrast to (Gregory and Altun, 2004), we
employ a rich acoustic feature set, designed to cap-
ture and compensate for coarticulatory influences on
accent realization, in addition to word-based fea-
tures.
4.1 Prosodic Features
Using Praat?s (Boersma, 2001) ?To pitch? and ?To
intensity? functions and the phoneme, syllable, and
word alignments provided in the corpus, we extract
acoustic features for the region of interest. This re-
gion corresponds to the syllable nucleus in English.
For all pitch and intensity features, we compute per-
speaker z-score normalized log-scaled values.
Recent phonetic research (Xu, 1997; Shih and
Kochanski, 2000) has identified significant effects
of carryover coarticulation from preceding adjacent
syllable tones. To minimize these effects consistent
with the pitch target approximation model (Xu et al,
1999), we compute slope features based on the sec-
ond half of this region, where this model predicts
that the underlying pitch height and slope targets of
the syllable will be most accurately approached.
For each syllable, we compute the following local
features:
? pitch values at five points evenly spaced across
the syllable nucleus,
? mean and maximum pitch values,
? slope based on a linear fit to the pitch contour
in the second half of the region, and
? mean and maximum intensity.
We consider two types of contextualized features
as well, to model and compensate for coarticula-
tory effects from neighboring syllables. The first set
of features, referred to as ?extended features?, in-
cludes the maximum and mean pitch from adjacent
219
syllables as well as the nearest pitch points from the
adjacent syllables. These features extend the mod-
eled tone beyond the strict bounds of the syllable
segmentation. A second set of contextual features,
termed ?difference features?, captures the change in
feature values between the current and adjacent syl-
lables. The resulting feature set includes:
? mean, maximum, and last two pitch values
from preceding syllable,
? mean, maximum, and first value from follow-
ing syllable, and
? differences in pitch mean, pitch maximum,
pitch of midpoint, pitch slope, intensity mean,
and intensity maximum between the current
syllable and the preceding syllable, and be-
tween the current syllable and the following
syllable.
Finally, we also employ some positional and du-
rational features. Many prosodic phenomena are af-
fected by phrase or sentence position; for example,
both pitch and intensity tend to decrease across an
utterance, and pitch accent realization may also be
affected by cooccurring phrase accents or bound-
ary tones. As syllable duration typically increases
under both accenting and phrase-final lengthening,
this information can be useful in prosodic labeling.
Finally, pause information is also associated with
prosodic phrasing. Thus, we include following fea-
tures:
? two binary features indicating initial and fi-
nal in a pseudo-phrase, defined as a silence-
delimited interval,
? duration of syllable nucleus, and
? durations of pause preceding and following the
syllable.
In prior experiments using support vector ma-
chines (Levow, 2005), variants of this representa-
tion achieved competitive recognition levels for both
tone and pitch accent recognition.
4.2 Text-based Features
We employ text-based models similar to those em-
ployed by (Sun, 2002; Rangarajan Sridhar et al,
2007). For each syllable, we capture the following
manually annotated features:
? The phonetic form of the current syllable, the
previous two syllables, and the following two
syllables,
? binary values indicating whether each of the
current, previous, and following syllables are
lexically stressed,
? integer values indicating position in a word of
the current, previous, and following syllables,
? the current word, the two previous words, and
the two following words, and
? the POS of the current word, of the two previ-
ous words, and of the two following words.
These features capture information about the current
syllable and its lexico-syntactic context, that have
been employed effectively in prosodic labeling of
pitch accent, phrase accent, and boundary tone.
5 Experiments
We explore a range of issues in the experiments
reported below. We hope to assess the impact
of feature set and acoustic and text-based fea-
ture integration in the Conditional Random Field
models. We compare their individual effective-
ness as well as the effect of combined feature
sets on labeling. In particular, we consider both
the binary accented/unaccented assignment task for
pitch accent and the four way - high/downstepped
high/low/unaccented - contrast to compare effective-
ness in problems of different difficulty. We further
consider the effect of sequence and factorial model-
ing on pitch accent recognition. All experiments are
conducted using a leave-one-out evaluation proce-
dure following (Chen et al, 2004), training on all
but one speaker and then testing on that held-out
speaker, reporting the average across the tests on
held-out data. Because speaker f2b contributes such
a large portion of the data, that speaker is never left
out.
On this split, the best word-based accuracy incor-
porating both prosodic and lexico-syntactic infor-
mation in a maximum entropy framework is 86.0%
for binary pitch accent prediction and 93.1% for
220
recognition of boundary status (Rangarajan Srid-
har et al, 2007). For syllable-level recognition on
this dataset, results for speaker-independent models
reach slightly over 80% for binary pitch accent de-
tection and 88% for boundary detection. Speaker de-
pendent models have achieved very high accuracy;
over 87% on speaker f2b was reported by (Sun,
2002) for the four-class task.
5.1 Explicit Prosodic Context Features and
Sequence Models
We first assess the role of contextual prosodic fea-
tures for pitch accent recognition and their inter-
action with sequence models. To minimize inter-
action effects, we concentrate on recognition with
prosodic features alone on the challenging four-way
pitch accent problem. As described above, we aug-
mented the local syllable-based prosodic features
with contextual features associated with the preced-
ing and following syllables. We ask whether the use
of contextual features improves recognition, and,
if so, which type of context, preceding or follow-
ing, has the greatest impact. We also ask whether
the CRF models provide further improvements or
can partially or fully compensate for the lack of
explicit context features. To evaluate this impact,
we compute four-way pitch accent recognition ac-
curacy with no context features, after adding preced-
ing context, after adding following context, and with
both. We also contrast zero order and first order lin-
ear chain CRFs for these conditions. We find that
modeling preceding context yields the greatest im-
provement. This finding is consistent with findings
in recent phonetic research that argue for a larger
role of carryover coarticulation from preceding syl-
lables than of anticipatory coarticulation with fol-
lowing syllables. Furthermore, sequence modeling
in the CRF also improves results, across the explicit
context feature conditions, with improvements being
most pronounced in cases with less effective explicit
prosodic contextual features. Results for prosodic
features alone appear in Table 1. In a side exper-
iment with these prosodic features, we also briefly
explored higher-order models, but no improvement
was observed.
We also assess the impact of this richer contex-
tualized prosodic feature set both alone and in con-
junction with the full text-based feature set, in the
No Context Full Context
Prosody Two-way 78.9% 80.8%
Only Four-way 74.2% 78.2%
All Two-way 86.2% 86.2%
Features Four-way 79% 79.7%
Table 2: Impact of context prosodic features with
prosody alone and all features
full factorial CRF framework. We compare results
for pitch accent identification in both the two-way
and four-way conditions with no context and with
the full ensemble of prosodic features. We find no
difference for the two-way, all features condition for
which text-based features perform well alone. How-
ever, for the prosody only cases and the more chal-
lenging four-way task with all features, contextual
information yields improvements, demonstrating the
utility of this richer, contextualized prosodic feature
representation. These contrasts appear in Table 2.
5.2 Prosodic and Text-based Features
We continue by contrasting effectiveness of differ-
ent feature sets in the basic linear-chain CRF case
for pitch accent recognition. Table 3 presents the
results for prosodic, word-based, and combined fea-
tures sets in both the two-way and four-way classifi-
cation conditions. Overall accuracy is quite good;
in all cases, results are well above the 65% most
common class assignment level, and the best re-
sults (86.2%) outperform any previously published
speaker independent syllable-based results on this
dataset. Overall results and contrasts are found in
Table 3.
It is clear that the two feature sets combine very
effectively. In the 4-way pitch accent task, the com-
bined model yields a significant 1.5% to 2.5% in-
crease over the strong acoustic-only model. In con-
trast, in the binary task, both the overall effective-
ness of the text-based model and its utility in com-
bination with the acoustic features are enhanced,
yielding a much higher individual and combined ac-
curacy rate. This contrast can be explained by the
fact that the word features, such as part of speech,
identify items that, as a class, are likely to be ac-
cented rather then being strongly associated with a
particular tone category. The type of accent is likely
221
No Context Preceding Following Both
Zero order 70.5% 75.2% 71.8% 76.4%
First order 74.2% 75.5% 73.7% 77.1%
Table 1: Prosodic Context Features and CRFs
Acoustic Text Text&Acoustic
Linear-Chain Two-way 79.48% 84.88% 86.1%
Four-way 77.06% 76.21% 79.65%
Factorial CRF Two-way 80.76% 84.74% 86.2%
Four-way 78.22% 77.46% 79.71%
Table 3: Pitch Accent Classification with Linear-Chain (top) and factorial CRFs (bottom) , using Acoustic-
only, Text-based-only, and Combined Features. Results for two- and four-way pitch accent prediction are
shown.
best determined by acoustic contrast, since accent
type is closely linked to pitch height, and the local
context and acoustic features serve to identify which
accentable words are truly accented. Thus, in the
binary task, the text-based features combine most
effectively with the evidence from the acoustic fea-
tures.
To contrast local classifiers with the linear chain
model with text-based features, we trained a zero or-
der classifier for the pitch accent prediction case and
contrasted it with a comparable first-order linear-
chain CRFs. Here for the binary accent recognition
case, using only text-based information, we reach an
accuracy of 84.3% for the history-free model, con-
trasted with an 85.4% level obtained with a compa-
rable first-order model.1
5.3 Factorial CRF Framework
Finally we consider the effect of joint classification
using the factorial CRF framework. Here, beyond
just pitch accent assignment, we perform simultane-
ous assignment of pitch accent, phrase accent and
boundary tone, where each label type corresponds
to a factor, implementing the desired dependencies.2
1This comparison was computed using the original Mallet
CRF package rather than GRMM, due to simpler zero order
model support. This results in a small difference in the resulting
scores.
2The features have not been tuned specifically for phrase ac-
count and boundary prediction, as explicit punctuation or sen-
tence boundary features would have been useful but obvious
giveaways. However, our goal is to assess the potential impact
of combined classification, without excessive tuning.
The contrasts with the linear-chain model in terms
of pitch accent prediction accuracy appear in Table
3. For the binary pitch accent condition, results are
somewhat mixed. While there is a small but not sig-
nificant decrease in accuracy for the text-only binary
classification condition, the combined case shows
little change and the prosodic case increases mod-
estly. We note in one case that joint accuracy has
risen when the pitch accent accuracy has dropped;
we speculate that some additional compensation is
needed to manage the effects of the severe class
imbalance between the dominant ?no-label? classes
for phrase accent and boundary tone and other la-
bels. For the four-way contrast between pitch accent
types, we see small to modest gains across all feature
sets, with the prosodic case improving significantly
(p < 0.025). The best results for all but the two-
way text-based classification task are found with the
factorial CRF model.
For phrase accent and boundary tone prediction,
phrase accent accuracy reaches 91.14%, and bound-
ary tone accuracy 93.72% for all features. Text-
based evidence is more effective than prosodic evi-
dence in these cases, with text-based features reach-
ing 91.06% for phrase accent and 92.51% and
acoustic features only 86.73% and 92.37% respec-
tively. However, little change is observed with the
factorial CRF relative to a linear chain model trained
on the same instances. The results for phrase accent
and boundary tone recognition appear in Table 4.
222
Phrase Accent Boundary Tone
Prosodic 86.73% 92.37%
Text 91.06% 92.51%
Text+Prosodic 91.14% 93.72%
Table 4: Accuracy for phrase accent and boundary
tone with prosodic, text-based, and combined fea-
tures
6 Conclusion and Future Work
The application of linear-chain and factorial Con-
ditional Random Fields for automatic pitch accent
recognition and other prosodic labeling facilitates
modeling of sequential dependencies as well as inte-
gration of rich acoustic features with text-based ev-
idence. We plan to further investigate the model-
ing of dependencies between prosodic labels and the
sequential modeling for acoustic features. Finally,
we will also integrate prior work on subsyllable seg-
mentation to identify the best approximation of the
prosodic target with the CRF framework to produce
a fine-grained sequence model of prosodic realiza-
tion in context.
7 Acknowledgments
The author would like to thank Charles Sutton for
providing the GRMM implementation, Andrew Mc-
Callum for the Mallet CRF implementation, and Si-
wei Wang and Sonja Waxmonsky for the modifica-
tions supporting real-valued features.
References
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2006. Combining acoustic, lexical,
and syntactic evidence for automatic unsupervised
prosody labeling. In Proceedings of ICSLP 2006.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9?10):341?345.
K. Chen, M. Hasegawa-Johnson, and A. Cohen. 2004.
An automatic prosody labeling system using ANN-
based syntactic-prosodic model and GMM-based
acoustic-prosodic model. In Proceedings of ICASSP.
K. Dusterhoff, A. Black, and P. Taylor. 1999. Using de-
cision trees within the tilt intonation model to predict
f0 contours. In Proc. Of Eurospeech ?99.
Michelle Gregory and Yasemin Altun. 2004. Using con-
ditional random fields to predict pitch accents in con-
versational speech. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 677?683, Barcelona,
Spain, July.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning (ICML-2001).
Gina-Anne Levow. 2005. Context in multi-lingual tone
and pitch accent prediction. In Proc. of Interspeech
2005.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, andMary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 451?458,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Andrew McCallum, Khashayar Rohanimanesh, and
Charles Sutton. 2003. Dynamic conditional ran-
dom fields for jointly labeling multiple sequences. In
NIPS*2003 Workshop on Syntax, Semantics, Statistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
M. Ostendorf and K. Ross. 1997. A multi-level model
for recognition of intonation labels. In Y. Sagisaka,
N. Campbell, and N. Higuchi, editors, Computing
Prosody, pages 291?308.
M. Ostendorf, P. J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University radio news corpus.
Technical Report ECS-95-001, Boston University.
Shimei Pan and Kathleen McKeown. 1998. Learning
intonation rules for concept to speech generation. In
Proceedings of ACL/COLING-98, pages 1003?1009.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth Narayanan. 2007. Exploiting acoustic
and syntactic features for prosody labeling in a maxi-
mum entropy framework. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 1?8,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Andrew Rosenberg and Julia Hirschberg. 2006. On the
correlation between energy and pitch accent in read en-
glish speech. In Proceedings of ICLSP 2006.
223
K. Ross and M. Ostendorf. 1994. A dynamical system
model for generating f0 for synthesis. In Proceed-
ings of the ESCA/IEEE Workshop on Speech Synthesis,
pages 131?134.
Xiao-Nan Shen. 1990. Tonal co-articulation in Man-
darin. Journal of Phonetics, 18:281?295.
C. Shih and G. P. Kochanski. 2000. Chinese tone model-
ing with stem-ml. In Proceedings of the International
Conference on Spoken Language Processing, Volume
2, pages 67?70.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert, and
J. Hirschberg. 1992. ToBI: A standard for labelling
English prosody. In Proceedings of ICSLP, pages
867?870.
Xuejing Sun. 2002. Pitch accent prediction using ensem-
ble machine learning. In Proceedings of ICSLP-2002.
Charles Sutton. 2006. Grmm: A graphical models
toolkit. http://mallet.cs.umass.edu.
Yi Xu and X. Sun. 2002. Maximum speed of pitch
change and how it may relate to speech. Journal of
the Acoustical Society of America, 111.
C.X. Xu, Y. Xu, and L.-S. Luo. 1999. A pitch tar-
get approximation model for f0 contours in Mandarin.
In Proceedings of the 14th International Congress of
Phonetic Sciences, pages 2359?2362.
Yi Xu. 1997. Contextual tonal variations in Mandarin.
Journal of Phonetics, 25:62?83.
Y. Xu. 1999. Effects of tone and focus on the formation
and alignment of f0 contours - evidence from Man-
darin. Journal of Phonetics, 27.
Yi Xu. 2004. Transmitting tone and intonation simulta-
neously - the parallel encoding and target approxima-
tion (PENTA) model. In TAL-2004, pages 215?220.
224
Prosody-based Topic Segmentation for Mandarin Broadcast News
Gina-Anne Levow
University of Chicago
levow@cs.uchicago.edu
Abstract
Automatic topic segmentation, separation of
a discourse stream into its constituent sto-
ries or topics, is a necessary preprocessing
step for applications such as information re-
trieval, anaphora resolution, and summariza-
tion. While significant progress has been made
in this area for text sources and for English au-
dio sources, little work has been done in au-
tomatic, acoustic feature-based segmentation
of other languages. In this paper, we focus
on prosody-based topic segmentation of Man-
darin Chinese. As a tone language, Man-
darin presents special challenges for applica-
bility of intonation-based techniques, since the
pitch contour is also used to establish lexical
identity. We demonstrate that intonational cues
such as reduction in pitch and intensity at topic
boundaries and increase in duration and pause
still provide significant contrasts in Mandarin
Chinese. We also build a decision tree classi-
fier that, based only on word and local context
prosodic information without reference to term
similarity, cue phrase, or sentence-level infor-
mation, achieves boundary classification accu-
racy of 89-95.8% on a large standard test set.
1 Introduction
Natural spoken discourse is composed a sequence of ut-
terances, not independently generated or randomly strung
together, but rather organized according to basic struc-
tural principles. This structure in turn guides the inter-
pretation of individual utterances and the discourse as a
whole. Formal written discourse signals a hierarchical,
tree-based discourse structure explicitly by the division
of the text into chapters, sections, paragraphs, and sen-
tences. This structure, in turn, identifies domains for in-
terpretation; many systems for anaphora resolution rely
on some notion of locality (Grosz and Sidner, 1986).
Similarly, this structure represents topical organization,
and thus would be useful in information retrieval to se-
lect documents where the primary sections are on-topic,
and, for summarization, to select information covering
the different aspects of the topic.
Unfortunately, spoken discourse does not include the
orthographic conventions that signal structural organiza-
tion in written discourse. Instead, one must infer the hi-
erarchical structure of spoken discourse from other cues.
Prior research (Nakatani et al, 1995; Swerts, 1997) has
shown that human labelers can more sharply, consis-
tently, and confidently identify discourse structure in a
word-level transcription when an original audio record-
ing is available than they can on the basis of the tran-
scribed text alone. This finding indicates that substan-
tial additional information about the structure of the dis-
course is encoded in the acoustic-prosodic features of the
utterance. Given the often errorful transcriptions avail-
able for large speech corpora, we choose to focus here on
fully exploiting the prosodic cues to discourse structure
present in the original speech, rather than on the lexical
cues or term frequencies of the transcription.
In the current set of experiments, we concentrate on se-
quential segmentation of news broadcasts into individual
stories. While a richer hierarchical segmentation is ulti-
mately desirable, sequential story segmentation provides
a natural starting point. This level of segmentation can
also be most reliably performed by human labelers and
thus can be considered most robust, and segmented data
sets are publicly available.
Furthermore, we apply prosodic-based segmentation to
Mandarin Chinese. Not only is the use of prosodic cues
to topic segmentation much less well-studied in general
than is the use of text cues, but the use of prosodic cues
has been largely limited to English and other European
languages.
2 Related Work
Most prior research on automatic topic segmentation has
been applied to clean text only and thus used textual fea-
tures. Text-based segmentation approaches have utilized
term-based similarity measures computed across candi-
date segments (Hearst, 1994) and also discourse markers
to identify discourse structure (Marcu, 2000).
The Topic Detection and Tracking (TDT) evaluations
focused on segmentation of both text and speech sources.
This framework introduced new challenges in dealing
with errorful automatic transcriptions as well as new op-
portunities to exploit cues in the original speech. The
most successful approach (Beeferman et al, 1999) pro-
duced automatic segmentations that yielded retrieval re-
sults comparable to those with manual segmentations, us-
ing text and silence features. (Tur et al, 2001) applied
both a prosody-only and a mixed text-prosody model
to segmentation of TDT English broadcast news, with
the best results combining text and prosodic features.
(Hirschberg and Nakatani, 1998) also examined auto-
matic topic segmentation based on prosodic cues, in the
domain of English broadcast news.
Work in discourse analysis (Nakatani et al, 1995;
Swerts, 1997) in both English and Dutch has identified
features such as changes in pitch range, intensity, and
speaking rate associated with segment boundaries and
with boundaries of different strengths. They also demon-
strated that access to acoustic cues improves the ease and
quality of human labeling.
3 Prosody and Mandarin
In this paper we focus on topic segmentation in Mandarin
Chinese broadcast news. Mandarin Chinese is a tone lan-
guage in which lexical identity is determined by a pitch
contour - or tone - associated with each syllable. This
additional use of pitch raises the question of the cross-
linguistic applicability of the prosodic cues, especially
pitch cues, identified for non-tone languages. Specifi-
cally, do we find intonational cues in tone languages? The
fact that emphasis is marked intonationally by expansion
of pitch range even in the presence of Mandarin lexical
tone (Shen, 1989) suggests encouragingly that prosodic,
intonational cues to other aspects of information structure
might also prove robust in tone languages.
4 Prosodic Features
We consider four main classes of prosodic features for
our analysis and classification: pitch, intensity, silence
and duration. Pitch, as represented by f0 in Hertz was
computed by the ?To pitch? function of the Praat sys-
tem (Boersma, 2001). We selected the highest ranked
pitch candidate value in each voiced region. We then ap-
plied a 5-point median filter to smooth out local instabili-
ties in the signal such as vocal fry or small regions of spu-
rious doubling or halving. Analogously, we computed the
intensity in decibels for each 10ms frame with the Praat
?To intensity? function, followed by similar smoothing.
For consistency and to allow comparability, we com-
pute all figures for word-based units, using the ASR tran-
scriptions provided with the TDT Mandarin data. The
words are used to establish time spans for computing
pitch or intensity mean or maximum values, to enable du-
rational normalization and the pairwise comparisons re-
ported below, and to identify silence duration.
It is well-established (Ross and Ostendorf, 1996) that
for robust analysis pitch and intensity should be nor-
malized by speaker, since, for example, average pitch
is largely incomparable for male and female speak-
ers. In the absence of speaker identification software,
we approximate speaker normalization with story-based
normalization, computed as  
	

	
, assuming one
speaker per topic1. For duration, we consider both abso-
lute and normalized word duration, where average word
duration is used as the mean in the calculation above.
5 Data Set
We utilize the Topic Detection and Tracking (TDT)
3 (Wayne, 2000) collection Mandarin Chinese broadcast
news audio corpus as our data set. Story segmentation in
Mandarin and English broadcast news and newswire text
was one of the TDT tasks and also an enabling technol-
ogy for other retrieval tasks. We use the segment bound-
aries provided with the corpus as our gold standard label-
ing. Our collection comprises 3014 stories drawn from
approximately 113 hours over three months (October-
December 1998) of news broadcasts from the Voice of
America (VOA) in Mandarin Chinese. The transcriptions
span approximately 740,000 words. The audio is stored
in NIST Sphere format sampled at 16KHz with 16-bit lin-
ear encoding.
6 Prosodic Analysis
To evaluate the potential applicability of prosodic fea-
tures to story segmentation in Mandarin Chinese, we per-
formed some initial data analysis to determine if words
in story-final position differed from the same words used
throughout the story. This lexical match allows direct
pairwise comparison. We anticipated that since words in
Mandarin varied not only in phoneme sequence but also
in tone sequence, a direct comparison might be particu-
larly important to eliminate sources of variability. Fea-
tures that differed significantly would form the basis of
our classifier feature set.
1This is an imperfect approximation as some stories include
off-site interviews, but seems a reasonable choice in the absence
of automatic speaker identification.
Figure 1: Differences in duration, normalized pitch, and
normalized intensity between words in segment non-final
and segment-final positions.
We found highly significant differences based on
paired t-test two-tailed, (  	

 

 ), for
each of the features we considered. Specifically, word
duration, normalized mean pitch, and normalized mean
intensity all differed significantly for words in topic-final
position relative to occurrences throughout the story (Fig-
ure 1). Word duration increased, while both pitch and
intensity decreased. A small side experiment using 15
hours of English broadcast news from the TDT collec-
tion shows similar trends, though the magnitude of the
change in intensity is smaller than that observed for the
Chinese.
These contrasts are consistent with, though in some
cases stronger than, those identified for English (Nakatani
et al, 1995) and Dutch (Swerts, 1997). The relatively
large size of the corpus enhances the salience of these
effects. We find, importantly, that reduction in pitch as
a signal of topic finality is robust across the typological
contrast of tone and non-tone languages. These findings
demonstrate highly significant intonational effects even
in tone languages and suggest that prosodic cues may be
robust across wide ranges of languages.
7 Classification
7.1 Feature Set
The results above indicate that duration, pitch, and inten-
sity should be useful for automatic prosody-based iden-
tification of topic boundaries. To facilitate cross-speaker
comparisons, we use normalized representations of aver-
age pitch, average intensity, and word duration. We also
include absolute word duration. These features form a
word-level context-independent feature set.
Since segment boundaries and their cues exist to con-
trastively signal the separation between topics, we aug-
ment these features with local context-dependent mea-
sures. Specifically, we add features that measure the
change between the current word and the next word. 2
This contextualization adds four contextual features:
change in normalized average pitch, change in normal-
ized average intensity, change in normalized word dura-
tion, and duration of following silence.
7.2 Classifier Training and Testing Configuration
We employed Quinlan?s C4.5 (Quinlan, 1992) decision
tree classifier to provide a readily interpretable classifier.
Now, the vast majority of word positions in our collec-
tion are non-topic-final. So, in order to focus training and
test on topic boundary identification, we downsample our
corpus to produce training and test sets with a 50/50 split
of topic-final and non-topic-final words. We trained on
2789 topic-final words 3 and 2789 non-topic-final words,
not matched in any way, drawn randomly from the full
corpus. We tested on a held-out set of 200 topic-final and
non-topic-final words.
7.3 Classifier Evaluation
The resulting classifier achieved 95.8% accuracy on the
held-out test set, closely approximately pruned tree per-
formance on the training set. This effectiveness is a sub-
stantial improvement over the sample baseline of 50%. 4
A portion of the decision tree is reproduced in Figure 2.
Inspection of the tree indicates the key role of silence as
well as the use of both contextual and purely local fea-
tures of both pitch and intensity. Durational features play
a lesser role in the classifier. The classifier relies on the
theoretically and empirically grounded features of pitch
and intensity and silence, where it has been suggested that
higher pitch and wider range are associated with topic
initiation and lower pitch or narrower range is associated
with topic finality.
We also performed a contrastive experiment where si-
lence features were excluded, to assess the dependence
on these features. 5 The resulting classifier achieved an
accuracy of 89.4% on the heldout balanced test set, rein-
forcing the utility of pitch and intensity features for clas-
sification.
We performed a second set of contrastive experiments
to explore the impact of different lexical tones on classi-
fication accuracy. We grouped words based on the lexical
2We have posed the task of boundary detection as the task
of finding segment-final words, so the technique incorporates a
single-word lookahead. We could also repose the task as iden-
tification of topic-initial words and avoid the lookahead to have
a more on-line process. This is an area for future research.
3We excluded a small proportion of words for which the
pitch tracker returned no results.
4On a randomly sampled test set, there were no missed
boundaries and a  5% false alarm rate was observed.
5VOA Mandarin has been observed stylistically to make id-
iosyncratically large use of silence at story boundaries. (per-
sonal communication, James Allan).
Figure 2: Decision tree classifier labeling words as segment-final or non-segment-final
tone of the initial syllable into high, rising, low, falling,
and neutral groups. We found no tone-based differences
in classification with all groups achieving 94-96% accu-
racy. Since the magnitude of the difference in pitch based
on discourse position is comparable to that based on lex-
ical tone identity, and the overlap between pitch values in
non-final and final positions is relatively small, we obtain
consistent results.
8 Conclusion and Future Work
We have demonstrated the applicability of intonational
prosodic features, specifically pitch, intensity, pause and
duration, to the identification of topic boundaries in a tone
language, Mandarin Chinese. We find highly significant
decreases in pitch and intensity at topic final positions,
and significant increases in word duration. Furthermore,
these features in both local and contextualized form pro-
vide the basis for an effective decision tree classifier of
boundary positions that does not use term similarity or
cue phrase information, but only prosodic features. We
also find that analogous to (Tur et al, 2001)?s work on
an English story segmentation task, pause and pitch - both
for the individual word and adjacency pair - play a crucial
role; our findings for Chinese, however, identify a greater
role played by intensity and durational contrasts.
There is still substantial work to be done. We would
like to integrate speaker identification for normalization
and speaker change detection. We also plan to explore
the integration of prosodic and textual features and in-
vestigate the identification of more fine-grained sub-topic
structure, to provide more focused units for information
retrieval, summarization, and anaphora resolution.
References
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning,
34((1-3)):177?210.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9?10):341?345.
B. Grosz and C. Sidner. 1986. Attention, intention, and
the structure of discourse. Computational Linguistics,
12(3):175?204.
M. Hearst. 1994. Multi-paragraph segmentation of ex-
pository text. In Proceedings of the 32nd Annual Meet-
ing of the Association for Computational Linguistics.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proceedings on
ICSLP-98.
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
C. H. Nakatani, J. Hirschberg, and B. J. Grosz. 1995.
Discourse structure in spoken language: Studies on
speech corpora. In Working Notes of the AAAI Spring
Symposium on Empirical Methods in Discourse Inter-
pretation and Generation, pages 106?112.
J.R. Quinlan. 1992. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann.
K. Ross and M. Ostendorf. 1996. Prediction of ab-
stract labels for speech synthesis. Computer Speech
and Language, 10:155?185.
X.-N. Shen. 1989. The Prosody of Mandarin Chinese,
volume 118 of University of California Publications
in Linguistics. University of California Press.
Marc Swerts. 1997. Prosodic features at discourse
boundaries of different strength. Journal of the Acous-
tical Society of America, 101(1):514?521.
G. Tur, D. Hakkani-Tur, A. Stolcke, and E. Shriberg.
2001. Integrating prosodic and lexical cues for auto-
matic topic segmentation. Computational Linguistics,
27(1):31?57.
C. Wayne. 2000. Multilingual topic detection and track-
ing: Successful research enabled by corpora and eval-
uation. In Language Resources and Evaluation Con-
ference (LREC) 2000, pages 1487?1494.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 224?231,
New York, June 2006. c?2006 Association for Computational Linguistics
Unsupervised and Semi-supervised Learning of Tone and Pitch Accent
Gina-Anne Levow
University of Chicago
1100 E. 58th St.
Chicago, IL 60637 USA
levow@cs.uchicago.edu
Abstract
Recognition of tone and intonation is es-
sential for speech recognition and lan-
guage understanding. However, most ap-
proaches to this recognition task have re-
lied upon extensive collections of man-
ually tagged data obtained at substantial
time and financial cost. In this paper,
we explore two approaches to tone learn-
ing with substantially reductions in train-
ing data. We employ both unsupervised
clustering and semi-supervised learning
to recognize pitch accent in English and
tones in Mandarin Chinese. In unsu-
pervised Mandarin tone clustering exper-
iments, we achieve 57-87% accuracy on
materials ranging from broadcast news to
clean lab speech. For English pitch accent
in broadcast news materials, results reach
78%. In the semi-supervised framework,
we achieve Mandarin tone recognition ac-
curacies ranging from 70% for broadcast
news speech to 94% for read speech, out-
performing both Support Vector Machines
(SVMs) trained on only the labeled data
and the 25% most common class assign-
ment level. These results indicate that the
intrinsic structure of tone and pitch accent
acoustics can be exploited to reduce the
need for costly labeled training data for
tone learning and recognition.
1 Introduction
Tone and intonation play a crucial role across many
languages. However, the use and structure of tone
varies widely, ranging from lexical tone which de-
termines word identity to pitch accent signalling in-
formation status. Here we consider the recognition
of lexical tones in Mandarin Chinese syllables and
pitch accent in English.
Although intonation is an integral part of lan-
guage and is requisite for understanding, recogni-
tion of tone and pitch accent remains a challeng-
ing problem. The majority of current approaches to
tone recognition in Mandarin and other East Asian
tone languages integrate tone identification with the
general task of speech recognition within a Hid-
den Markov Model framework. In some cases tone
recognition is done only implicitly when a word
or syllable is constrained jointly by the segmental
acoustics and a higher level language model and the
word identity determines tone identity. Other strate-
gies build explicit and distinct models for the syl-
lable final region, the vowel and optionally a final
nasal, for each tone.
Recent research has demonstrated the importance
of contextual and coarticulatory influences on the
surface realization of tones.(Xu, 1997; Shen, 1990)
The overall shape of the tone or accent can be sub-
stantially modified by the local effects of adjacent
tone and intonational elements. Furthermore, broad
scale phenomena such as topic and phrase struc-
ture can affect pitch height, and pitch shape may be
variably affected by the presence of boundary tones.
These findings have led to explicit modeling of tonal
224
context within the HMM framework. In addition
to earlier approaches that employed phrase structure
(Fujisaki, 1983), several recent approaches to tone
recognition in East Asian languages (Wang and Sen-
eff, 2000; Zhou et al, 2004) have incorporated ele-
ments of local and broad range contextual influence
on tone. Many of these techniques create explicit
context-dependent models of the phone, tone, or ac-
cent for each context in which they appear, either
using the tone sequence for left or right context or
using a simplified high-low contrast, as is natural
for integration in a Hidden Markov Model speech
recognition framework. In pitch accent recognition,
recent work by (Hasegawa-Johnson et al, 2004) has
integrated pitch accent and boundary tone recogni-
tion with speech recognition using prosodically con-
ditioned models within an HMM framework, im-
proving both speech and prosodic recognition.
Since these approaches are integrated with HMM
speech recognition models, standard HMM training
procedures which rely upon large labeled training
sets are used for tone recognition as well. Other
tone and pitch accent recognition approaches us-
ing other classification frameworks such as support
vector machines (Thubthong and Kijsirikul, 2001)
and decision trees with boosting and bagging (Sun,
2002) have relied upon large labeled training sets -
thousands of instances - for classifier learning. This
labelled training data is costly to construct, both in
terms of time and money, with estimates for some in-
tonation annotation tasks reaching tens of times real-
time. This annotation bottleneck as well as a theo-
retical interest in the learning of tone motivates the
use of unsupervised or semi-supervised approaches
to tone recognition whereby the reliance on this of-
ten scarce resource can be reduced.
Little research has been done in the application
of unsupervised and semi-supervised techniques for
tone and pitch accent recognition. Some prelimi-
nary work by (Gauthier et al, 2005) employs self-
organizing maps and measures of f0 velocity for
tone learning. In this paper we explore the use
of spectral and standard k-means clustering for un-
supervised acquisition of tone, and the framework
of manifold regularization for semi-supervised tone
learning. We find that in clean read speech, un-
supervised techniques can identify the underlying
Mandarin tone categories with high accuracy, while
even on noisier broadcast news speech, Mandarin
tones can be recognized well above chance levels,
with English pitch accent recognition at near the
levels achieved with fully supervised Support Vec-
tor Machine (SVM) classifiers. Likewise in the
semi-supervised framework, tone classification out-
performs both most common class assignment and
a comparable SVM trained on only the same small
set of labeled instances, without recourse to the un-
labeled instances.
The remainder of paper is organized as fol-
lows. Section 2 describes the data sets on which
English pitch accent and Mandarin tone learning
are performed and the feature extraction process.
Section 3 describes the unsupervised and semi-
supervised techniques employed. Sections 4 and
5 describe the experiments and results in unsuper-
vised and semi-supervised frameworks respectively.
Section 6 presents conclusions and future work.
2 Data Sets
We consider two corpora: one in English for pitch
accent recognition and two in Mandarin for tone
recognition. We introduce each briefly below.
2.1 English Corpus
We employ a subset of the Boston Radio News Cor-
pus (Ostendorf et al, 1995), read by female speaker
F2B, comprising 40 minutes of news material. The
corpus includes pitch accent, phrase and boundary
tone annotation in the ToBI framework (Silverman
et al, 1992) aligned with manual transcription and
syllabification of the materials. Following earlier re-
search (Ostendorf and Ross, 1997; Sun, 2002), we
collapse the ToBI pitch accent labels to four classes:
unaccented, high, low, and downstepped high for ex-
perimentation.
2.2 Mandarin Chinese Tone Data
Mandarin Chinese is a language with lexical tone
in which each syllable carries a tone and the mean-
ing of the syllable is jointly determined by the tone
and segmental information. Mandarin Chinese has
four canonical lexical tones, typically described as
follows: 1) high level, 2) mid-rising, 3) low falling-
rising, and 4) high falling.1 The canonical pitch con-
1For the experiments in this paper, we exclude the neutral
tone, which appears on unstressed syllables, because the clear
225
Figure 1: Contours for canonical Mandarin tones
tours for these tones appear in Figure 1.
We employ data from two distinct sources in the
experiments reported here.
2.2.1 Read Speech
The first data set is very clean speech data drawn
from a collection of read speech collected under lab-
oratory conditions by (Xu, 1999). In these mate-
rials, speakers read a set of short sentences where
syllable tone and position of focus were varied to
assess the effects of focus position on tone realiza-
tion. Focus here corresponds to narrow focus, where
speakers were asked to emphasize a particular word
or syllable. Tones on focussed syllables were found
to conform closely to the canonical shapes described
above, and in previous supervised experiments using
a linear support vector machine classifier trained on
focused syllables, accuracy approached 99%. For
these materials, pitch tracks were manually aligned
to the syllable and automatically smoothed and time-
normalized by the original researcher, resulting in 20
pitch values for each syllable.
2.2.2 Broadcast News Speech
The second data set is drawn from the Voice of
America Mandarin broadcast news, distributed by
the Linguistic Data Consortium2, as part of the Topic
Detection and Tracking (TDT-2) evaluation. Us-
ing the corresponding anchor scripts, automatically
word-segmented, as gold standard transcription, au-
dio from the news stories was force-aligned to the
text transcripts. The forced alignment employed the
language porting functionality of the University of
speech data described below contains no such instances.
2http://www.ldc.upenn.edu
Colorado Sonic speech recognizer (Pellom et al,
2001). A mapping from the transcriptions to English
phone sequences supported by Sonic was created
using a Chinese character-pinyin pronunciation dic-
tionary and a manually constructed mapping from
pinyin sequences to the closest corresponding En-
glish phone sequences.3
2.3 Acoustic Features
Using Praat?s (Boersma, 2001) ?To pitch? and ?To
intensity? functions and the alignments generated
above, we extract acoustic features for the prosodic
region of interest. This region corresponds to the
?final? region of each syllable in Chinese, including
the vowel and any following nasal, and to the sylla-
ble nucleus in English.4 For all pitch and intensity
features in both datasets, we compute per-speaker z-
score normalized log-scaled values. We extract pitch
values from points across valid pitch tracked regions
in the syllable. We also compute mean pitch across
the syllable. Recent phonetic research (Xu, 1997;
Shih and Kochanski, 2000) has identified signifi-
cant effects of carryover coarticulation from preced-
ing adjacent syllable tones. To minimize these ef-
fects consistent with the pitch target approximation
model (Xu et al, 1999), we compute slope features
based on the second half of this final region, where
this model predicts that the underlying pitch height
and slope targets of the syllable will be most accu-
rately approached. We further log-scale and normal-
ize slope values to compensate for greater speeds of
pitch fall than pitch rise(Xu and Sun, 2002).
We consider two types of contextualized features
as well, to model and compensate for coarticula-
tory effects from neighboring syllables. The first set
of features, referred to as ?extended features?, in-
cludes the maximum and mean pitch from adjacent
syllables as well as the nearest pitch point or points
from the preceding and following syllables. These
features extend the modeled tone beyond the strict
bounds of the syllable segmentation. A second set
of contextual features, termed ?difference features?,
captures the change in pitch maximum, mean, mid-
point, and slope as well as intensity maximum be-
3All tone transformations due to third tone sandhi are ap-
plied to create the label set.
4We restrict our experiments to syllables with at least 50 ms
of tracked pitch in this final region.
226
tween the current syllable and the previous or fol-
lowing syllable.
In prior supervised experiments using support
vector machines(Levow, 2005), variants of this rep-
resentation achieved competitive recognition levels
for both tone and pitch accent recognition. Since
many of the experiments for Mandarin Chinese tone
recognition deal with clean, careful lab speech, we
anticipate little coarticulatory influence, and use
a simple pitch-only context-free representation for
our primary Mandarin tone recognition experiments.
For primary experiments in pitch accent recognition,
we employ a high-performing contextualized repre-
sentation in (Levow, 2005), using both ?extended?
and ?difference? features computed only on the pre-
ceding syllable. We will also report some contrastive
experimental results varying the amount of contex-
tual information.
3 Unsupervised and Semi-supervised
Learning
The bottleneck of time and monetary cost asso-
ciated with manual annotation has generated sig-
nificant interest in the development of techniques
for machine learning and classification that reduce
the amount of annotated data required for train-
ing. Likewise, learning from unlabeled data aligns
with the perspective of language acquisition, as
child learners must identify these linguistic cate-
gories without explicit instruction by observation of
natural language interaction. Of particular interest
are techniques in unsupervised and semi-supervised
learning where the structure of unlabeled examples
may be exploited. Here we consider both unsuper-
vised techniques with no labeled training data and
semi-supervised approaches where unlabeled train-
ing data is used in conjunction with small amounts
of labeled data.
A wide variety of unsupervised clustering tech-
niques have been proposed. In addition to classic
clustering techniques such as k-means, recent work
has shown good results for many forms of spec-
tral clustering including those by (Shi and Ma-
lik, 2000; Belkin and Niyogi, 2002; Fischer and
Poland, 2004). In the unsupervised experiments re-
ported here, we employ asymmetric k-lines clus-
tering by (Fischer and Poland, 2004) using code
available at the authors? site, as our primary unsu-
pervised learning approach. Asymmetric clustering
is distinguished from other techniques by the con-
struction and use of context-dependent kernel radii.
Rather than assuming that all clusters are uniform
and spherical, this approach enhances clustering ef-
fectiveness when clusters may not be spherical and
may vary in size and shape. We will see that this
flexibility yields a good match to the structure of
Mandarin tone data where both shape and size of
clusters vary across tones. In additional contrastive
experiments reported below, we also compare k-
means clustering, symmetric k-lines clustering (Fis-
cher and Poland, 2004), and Laplacian Eigenmaps
(Belkin and Niyogi, 2002) with k-lines clustering.
The spectral techniques all perform spectral decom-
position on some representation of the affinity or ad-
jacency graph.
For semi-supervised learning, we employ learn-
ers in the Manifold Regularization framework de-
veloped by (Belkin et al, 2004). This work postu-
lates an underlying intrinsic distribution on a low di-
mensional manifold for data with an observed, am-
bient distribution that may be in a higher dimen-
sional space. It further aims to preserve locality in
that elements that are neighbors in the ambient space
should remain ?close? in the intrinsic space. A semi-
supervised classification algorithm, termed ?Lapla-
cian Support Vector Machines?, allows training and
classification based on both labeled and unlabeled
training examples.
We contrast results under both unsupervised and
semi-supervised learning with most common class
assignment and previous results employing fully su-
pervised approaches, such as SVMs.
4 Unsupervised Clustering Experiments
We executed four sets of experiments in unsu-
pervised clustering using the (Fischer and Poland,
2004) asymmetric clustering algorithm.
4.1 Experiment Configuration
In these experiments, we chose increasingly diffi-
cult and natural test materials. In the first experi-
ment with the cleanest data, we used only focused
syllables from the read Mandarin speech dataset.
In the second, we included both in-focus (focused)
227
and pre-focus syllables from the read Mandarin
speech dataset.5 In the third and fourth experiments,
we chose subsets of broadcast news report data,
from the Voice of America (VOA) in Mandarin and
Boston University Radio News corpus in English.
In all experiments on Mandarin data, we per-
formed clustering on a balanced sampling set of
tones, with 100 instances from each class6, yield-
ing a baseline for assignment of a single class to all
instances of 25%. We then employed a two-stage re-
peated clustering process, creating 2 or 3 clusters at
each stage.
For experiments on English data, we extracted a
set of 1000 instances, sampling pitch accent types
according to their frequency in the collection. We
performed a single clustering phase with 2 to 16
clusters, reporting results at different numbers of
clusters.
For evaluation, we report accuracy based on as-
signing the most frequent class label in each cluster
to all members of the cluster.
4.2 Experimental Results
We find that in all cases, accuracy based on the
asymmetric clustering is significantly better than
most common class assignment and in some cases
approaches labelled classification accuracy. Unsur-
prisingly, the best results, in absolute terms, are
achieved on the clean focused syllables, reaching
87% accuracy. For combined in-focus and pre-focus
syllables, this rate drops to 77%. These rates con-
trast with 99-93% accuracies in supervised classi-
fication using linear SVM classifiers with several
thousand labelled training examples(Surendran et
al., 2005).
On broadcast news audio, accuracy for Mandarin
reaches 57%, still much better than the 25% level,
though below a 72% accuracy achieved using super-
vised linear SVMs with 600 labeled training exam-
ples. Interestingly, for English pitch accent recogni-
tion, accuracy reaches 78.4%, aproaching the 80.1%
5Post-focus syllables typically have decreased pitch height
and range, resulting in particularly poor recognition accuracy.
We chose not to concentrate on this specific tone modeling
problem here.
6Sample sizes were bounded to support rapid repeated ex-
perimentation and for consistency with the relatively small
VOA data set.
Figure 2: Differences for alternative unsupervised
learners across numbers of clusters.
accuracy achieved with SVMs on a comparable data
representation.
4.3 Contrastive Experiments
We further contrast the use of different unsupervised
learners, comparing the three spectral techniques
and k-means with Euclidean distance. All contrasts
are presented for English pitch accent classification,
ranging over different numbers of clusters, with the
best parameter setting of neighborhood size. The re-
sults are illustrated in Figure 2. K-means and the
asymmetric clustering technique are presented for
the clean focal Mandarin speech under the standard
two stage clustering, in Table 1.
The asymmetric k-lines clustering approach con-
sistently outperforms the corresponding symmetric
clustering learner, as well as Laplacian Eigenmaps
with binary weights for pitch accent classification.
Somewhat surprisingly, k-means clustering outper-
forms all of the other approaches when producing 3-
14 clusters. Accuracy for the optimal choice of clus-
ters and parameters is comparable for asymmetric
k-lines clustering and k-means, and somewhat bet-
ter than all other techniques considered. The care-
ful feature selection process for tone and pitch ac-
cent modeling may reduce the difference between
the spectral and k-means approaches. In contrast,
for the four tone classification task in Mandarin us-
ing two stage clustering with 2 or 3 initial clusters,
the best clustering using asymmetric k-lines strongly
outperforms k-means.
We also performed a contrastive experiment in
pitch accent recognition in which we excluded con-
textual information from both types of contextual
features. We find little difference for the majority of
228
Asymm. K-means
Clear speech 87% 74.75%
Table 1: Clustering effectiveness for asymmetric k-lines and k-means on clear focused speech.
Figure 3: Scatterplot of pitch height vs pitch slope.
Open Diamond: High tone (1), Filled black traingle:
Rising tone (2), Filled grey square: Low tone (3), X:
Falling tone (4)
the unsupervised clustering algorithms, with results
from symmetric, asymmetric and k-means cluster-
ing differing by less than 1% in absolute accuracy.
It is, however, worth noting that exclusion of these
features from experiments using supervised learning
led to a 4% absolute reduction in accuracy.
4.4 Discussion
An examination of both the clusters formed and the
structure of the data provides insight into the effec-
tiveness of this process. Figure 3 displays 2 dimen-
sions of the Mandarin four-tone data from the fo-
cused read speech, where normalized pitch mean is
on the x-axis and slope is on the y-axis. The sepa-
ration of classes and their structure is clear. One ob-
serves that rising tone (tone 2) lies above the x-axis,
while high-level (tone 1) lies along the x-axis. Low
(tone 3) and falling (tone 4) tones lie mostly below
the x-axis as they generally have falling slope. Low
tone (3) appears to the left of falling tone (4) in the
figure, corresponding to differences in mean pitch.
In clustering experiments, an initial 2- or 3-way
split separates falling from rising or level tones
based on pitch slope. The second stage of cluster-
ing splits either by slope (tones 1,2, some 3) or by
pitch height (tones 3,4). These clusters capture the
natural structure of the data where tones are charac-
terized by pitch height and slope targets.
5 Semi-supervised Learning
By exploiting a semi-supervised approach, we hope
to enhance classification accuracy over that achiev-
able by unsupervised methods alone by incorporat-
ing small amounts of labeled data while exploiting
the structure of the unlabeled examples.
5.1 Experiment Configuration
We again conduct contrastive experiments using
both the clean focused read speech and the more
challenging broadcast news data. In each Mandarin
case, for each class, we use only a small set (40) of
labeled training instances in conjunction with an ad-
ditional sixty unlabeled instances, testing on 40 in-
stances. For English pitch accent, we restricted the
task to the binary classification of syllables as ac-
cented or unaccented. For the one thousand samples
we proportionally labeled 200 unaccented examples
and 100 accented examples. 7
We configure the Laplacian SVM classification
with binary neighborhood weights, radial basis func-
tion kernel, and cosine distance measure typically
with 6 nearest neighbors. Following (C-C.Cheng
and Lin, 2001), for   -class classification we train
	

 binary classifiers. We then classify each
test instance using all of the classifiers and assign
the most frequent prediction, with ties broken ran-
domly. We contrast these results both with conven-
tional SVM classification with a radial basis func-
tion kernel excluding the unlabeled training exam-
ples and with most common class assignment, which
gives a 25% baseline.
5.2 Experimental Results
For the Mandarin focused read syllables, we achieve
94% accuracy on the four-way classification task.
7The framework is transductive; the test samples are a subset
of the unlabeled training examples.
229
For the noisier broadcast news data, the accuracy is
70% for the comparable task. These results all sub-
stantially outperform the 25% most common class
assignment level. The semi-supervised classifier
also reliably outperforms an SVM classifier with an
RBF kernel trained on the same labeled training in-
stances. This baseline SVM classifier with a very
small training set achieves 81% accuracy on clean
read speech, but only   35% on the broadcast news
speech. Finally, for English pitch accent recogni-
tion in broadcast news data, the classifier achieves
81.5%, relative to 84% accuracy in the fully super-
vised case.
6 Conclusion & Future Work
We have demonstrated the effectiveness of both
unsupervised and semi-supervised techniques for
recognition of Mandarin Chinese syllable tones and
English pitch accents using acoustic features alone
to capture pitch target height and slope. Although
outperformed by fully supervised classification tech-
niques using much larger samples of labelled train-
ing data, these unsupervised and semi-supervised
techniques perform well above most common class
assignment, in the best cases approaching 90%
of supervised levels, and, where comparable, well
above a good discriminative classifier trained on a
comparably small set of labelled data. Unsuper-
vised techniques achieve accuracies of 87% on the
cleanest read speech, reaching 57% on data from a
standard Mandarin broadcast news corpus, and over
78% on pitch accent classification for English broad-
cast news. Semi-supervised classification in the
Mandarin four-class classification task reaches 94%
accuracy on read speech, 70% on broadcast news
data, improving dramatically over both the simple
baseline of 25% and a standard SVM with an RBF
kernel trained only on the labeled examples.
Future work will consider a broader range of tone
and intonation classification, including the richer
tone set of Cantonese as well as Bantu family tone
languages, where annotated data truly is very rare.
We also hope to integrate a richer contextual rep-
resentation of tone and intonation consistent with
phonetic theory within this unsupervised and semi-
supervised learning framework. We will further ex-
plore improvements in classification accuracy based
on increases in labeled and unlabeled training exam-
ples.
Acknowledgements
We would like to thank Yi Xu for granting access
to the read speech data, Vikas Sindhwani, Mikhail
Belkin, and Partha Niyogi for their implementation
of Laplacian SVM, and Igor Fischer and J. Poland
for their implementation of asymmetric clustering.
References
Mikhail Belkin and Partha Niyogi. 2002. Laplacian
eigenmaps and spectral techniques for embedding and
clustering. In Proceeding of NIPS?02.
M. Belkin, P. Niyogi, and V. Sindhwani. 2004. Mani-
fold regularization: a geometric framework for learn-
ing from examples. Technical Report TR-2004-06,
University of Chicago Computer Science.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9?10):341?345.
C-C.Cheng and C-J. Lin. 2001. LIBSVM:a library
for support vector machines. Software available at:
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
I. Fischer and J. Poland. 2004. New methods for spectral
clustering. Technical Report ISDIA-12-04, IDSIA.
H. Fujisaki. 1983. Dynamic characteristics of voice fun-
damental frequency in speech and singing. In The Pro-
duction of Speech, pages 39?55. Springer-Verlag.
Bruno Gauthier, Rushen Shi, Yi Xu, and Robert Proulx.
2005. Neural-network simulation of tonal categoriza-
tion based on f0 velocity profiles. Journal of the
Acoustical Society of America, 117, Pt. 2:2430.
M. Hasegawa-Johnson, Jennifer Cole, Chilin Shih abd
Ken Chen, Aaron Cohen, Sandra Chavarria, Heejin
Kim, Taejin Yoon, Sarah Borys, and Jeung-Yoon Choi.
2004. Speech recognition models of the interdepen-
dence among syntax, prosody, and segmental acous-
tics. In HLT/NAACL-2004.
Gina-Anne Levow. 2005. Context in multi-lingual tone
and pitch accent prediction. In Proc. of Interspeech
2005 (to appear).
M. Ostendorf and K. Ross. 1997. A multi-level model
for recognition of intonation labels. In Y. Sagisaka,
N. Campbell, and N. Higuchi, editors, Computing
Prosody, pages 291?308.
M. Ostendorf, P. J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University radio news corpus.
Technical Report ECS-95-001, Boston University.
230
B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang,
X. Yu, and S. Pradhan. 2001. University of Colorado
dialog systems for travel and navigation.
Xiao-Nan Shen. 1990. Tonal co-articulation in Man-
darin. Journal of Phonetics, 18:281?295.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8).
C. Shih and G. P. Kochanski. 2000. Chinese tone model-
ing with stem-ml. In Proceedings of the International
Conference on Spoken Language Processing, Volume
2, pages 67?70.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert, and
J. Hirschberg. 1992. ToBI: A standard for labelling
English prosody. In Proceedings of ICSLP, pages
867?870.
Xuejing Sun. 2002. Pitch accent prediction using ensem-
ble machine learning. In Proceedings of ICSLP-2002.
D. Surendran, Gina-Anne Levow, and Yi Xu. 2005. Tone
recognition in Mandarin using focus. In Proc. of Inter-
speech 2005 (to appear).
Nuttakorn Thubthong and Boonserm Kijsirikul. 2001.
Support vector machines for Thai phoneme recogni-
tion. International Journal of Uncertainty, Fuzziness
and Knowledge-Based Systems, 9(6):803?813.
C. Wang and S. Seneff. 2000. Improved tone recogni-
tion by normalizing for coarticulation and intonation
effects. In Proceedings of 6th International Confer-
ence on Spoken Language Processing.
Yi Xu and X. Sun. 2002. Maximum speed of pitch
change and how it may relate to speech. Journal of
the Acoustical Society of America, 111.
C.X. Xu, Y. Xu, and L.-S. Luo. 1999. A pitch tar-
get approximation model for f0 contours in Mandarin.
In Proceedings of the 14th International Congress of
Phonetic Sciences, pages 2359?2362.
Yi Xu. 1997. Contextual tonal variations in Mandarin.
Journal of Phonetics, 25:62?83.
Y. Xu. 1999. Effects of tone and focus on the formation
and alignment of f0 contours - evidence from Man-
darin. Journal of Phonetics, 27.
J. L. Zhou, Ye Tian, Yu Shi, Chao Huang, and Eric
Chang. 2004. Tone articulation modeling for Man-
darin spontaneous speech recognition. In Proceedings
of ICASSP 2004.
231
Proceedings of NAACL HLT 2007, Companion Volume, pages 113?116,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Hybrid Document Indexing with Spectral Embedding
Irina Matveeva
Department of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Department of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
Document representation has a large im-
pact on the performance of document re-
trieval and clustering algorithms. We
propose a hybrid document indexing
scheme that combines the traditional bag-
of-words representation with spectral em-
bedding. This method accounts for the
specifics of the document collection and
also uses semantic similarity information
based on a large scale statistical analysis.
Clustering experiments showed improve-
ments over the traditional tf-idf represen-
tation and over the spectral methods based
solely on the document collection.
1 Introduction
Capturing semantic relations between words in a
document representation is a difficult problem. Dif-
ferent approaches tried to overcome the term inde-
pendence assumption of the bag-of-words represen-
tation (Salton and McGill, 1983) for example by us-
ing distributional term clusters (Slonim and Tishby,
2000) and expanding the document vectors with
synonyms, see (Levow et al, 2005). Since content
words can be combined into semantic classes there
has been a considerable interest in low-dimensional
term and document representations.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. In the LSA space documents
are indexed with latent semantic concepts. LSA
showed large performance improvements over the
traditional tf-idf representation on small document
collections (Deerwester et al, 1990) but often does
not perform well on large heterogeneous collections.
LSA maps all words to low dimensional vectors.
However, the notion of semantic relatedness is de-
fined differently for subsets of the vocabulary. In ad-
dition, the numerical information, abbreviations and
the documents? style may be very good indicators of
their topic. However, this information is no longer
available after the dimensionality reduction.
We use a hybrid approach to document indexing
to address these issues. We keep the notion of la-
tent semantic concepts and also try to preserve the
specifics of the document collection. We use a low-
dimensional representation only for nouns and rep-
resent the rest of the document?s content as tf-idf
vectors.
The rest of the paper is organized as follows. Sec-
tion 2 discusses our approach. Section 3 reports the
experimental results. We conclude in section 4.
2 Hybrid Document Indexing
This section gives the general idea of our approach.
We divide the vocabulary into two sets: nouns and
the rest of the vocabulary. We use a method of spec-
tral embedding, as described below and compute a
low-dimensional representation for documents using
only the nouns. We also compute a tf-idf represen-
tation for documents using the other set of words.
Since we can treat each latent semantic concept in
the low-dimensional representation as part of the vo-
cabulary, we combine the two vector representations
for each document by concatenating them.
113
2.1 Spectral Embedding
Spectral methods comprise a family of algorithms
that use a matrix of pair-wise similarities S and per-
form its spectral analysis, such as the eigenvalue de-
composition, to embed terms and documents in a
low-dimensional vector space. S = U?UT , where
the columns of U are its eigenvectors and ? is a di-
agonal matrix with the eigenvalues.
If we have a matrix of pair-wise word similarities
S, its first k eigenvectors Uk will be used to repre-
sent the words in the latent semantic space. Seman-
tically related words will have high association with
the same latent concepts and their corresponding
vectors will be similar. Moreover, the vector similar-
ity between the word vectors will optimally preserve
the original similarities (Cox and Cox, 2001).
We use two approaches to compute spectral em-
bedding for nouns. Latent Semantic Analysis
(LSA) (Deerwester et al, 1990) and Generalized La-
tent Semantic Analysis (GLSA) (Matveeva et al,
2005). For both we used the eigenvalue decomposi-
tion as the embedding step. The difference is in the
similarities matrix which we are trying to preserve.
2.2 Distributional Term Similarity
LSA and GLSA begin with a matrix of pair-wise
term similarities S, compute its eigenvectors U and
use the first k of them to represent terms and doc-
uments, for details see (Deerwester et al, 1990;
Matveeva et al, 2005). The main difference in our
implementation of these algorithms is the matrix of
pair-wise word similarities. Since our representation
will try to preserve them it is important to have a ma-
trix of similarities which is linguistically motivated.
LSA uses the matrix of pair-wise similarities
which is based on document vectors. For two words
wi and wj in the document collection containing n
documents dk, the similarity is computed as
S(wi, wj) =
?
k=1:n
tf(wi, dk)idf(wi) ? tf(wj , dk)idf(wj),
where tf(wi, dk) is the term frequency for wi in
dk and idf(wi) is the inverse document frequency
weight for wi. LSA is a special case of spectral em-
bedding restricted to one type of term similarities
and dimensionality reduction method.
GLSA (Matveeva et al, 2005) generalizes the
idea of latent semantic space. It proposes to use
different types of similarity matrix and spectral em-
bedding methods to compute a latent space which is
closer to true semantic similarities. One way to do
so is to use a more appropriate similarities matrix S.
PMI We use point-wise mutual information (PMI)
to compute the matrix S. PMI between random vari-
ables representing the words wi and wj is computed
as
PMI(wi, wj) = log
P (Wi = 1,Wj = 1)
P (Wi = 1)P (Wj = 1)
.
Thus, for GLSA, S(wi, wj) = PMI(wi, wj).
Co-occurrence Proximity An advantage of PMI
is the notion of proximity. The co-occurrence statis-
tics for PMI are typically computed using a sliding
window. Thus, PMI will be large only for words
that co-occur within a small fixed context. Our ex-
periments show that this is a better approximation to
true semantic similarities.
2.3 Document Indexing
We have two sets of the vocabulary terms: a set of
nouns, N , and the other words, T . We compute tf-idf
document vectors indexed with the words in T :
~di = (?i(w1), ?i(w2), ..., ?i(w|T |)),
where ?i(wt) = tf(wt, di) ? idf(wt).
We also compute a k-dimensional representation
with latent concepts ci as a weighted linear combi-
nation of LSA or GLSA term vectors ~wt:
~di = (c1, ..., ck) =
?
t=1:|T |
?i(wt) ? ~wt,
We concatenate these two representations to gener-
ate a hybrid indexing of documents:
~di = (?i(w1), ..., ?i(w|T |), c1, ...ck)
3 Experiments
We performed document clustering experiments to
validate our approach.
114
Subset m-n #topics min #d max #d av. #d
5-10 19 6 10 8.2
50-150 21 55 150 94.7
500-1000 2 544 844 694.0
1000-5000 3 1367 2083 1792.3
Table 1: TDT2 topic subsets containing between m
and n documents: the number of topics per subset,
the minimum, the maximum and the average number
of documents per topic in each subset.
Indexing
All words Nouns Hybrid
tf-idf, LSA tf-idfN
GLSA, GLSA local GLSAN tf-idf+GLSAN
Table 2: Indexing schemes: with full vocabulary
(All), only nouns (Nouns) and the combination.
Data We used the TDT2 collection1 of news arti-
cles from six news agencies in 1998. We used only
10,329 documents that are assigned to one topic.
TDT2 documents are distributed over topics very
unevenly. We used subsets of the TDT2 topics that
contain between m and n documents, see Table 1.
We used the Lemur toolkit2 with stemming and stop
words list for the tf-idf indexing, Bikel?s parser3 to
obtain the set of nouns and the PLAPACK pack-
age (Bientinesi et al, 2003) to compute the eigen-
value decomposition.
Global vs. Local Similarity To obtain the PMI
values for GLSA we used the TDT2 collection, de-
noted as GLSAlocal. Since co-occurrence statistics
based on larger collections gives a better approxima-
tion to linguistic similarities, we also used 700,000
documents from the English GigaWord collection,
denoted as GLSA and GLSAN . We used a window
of size 8.
Representations For each document we com-
puted 7 representations, see Table 2. The vocabulary
size we used with the tf-idf indexing was 114,127.
For computational reasons we used the set of words
that occurred in at least 20 documents with our spec-
tral methods. We used 17,633 words for index-
1http://nist.gov/speech/tests/tdt/tdt98/
2http://www.lemurproject.org/
3http://www.cis.upenn.edu/ dbikel/software.html
ing with LSA and GLSAlocal and 17,572 words for
GLSA. We also indexed documents using only the
15,325 nouns: tf-idfN and GLSAN . The hybrid rep-
resentation was computed using the tf-idf indexing
without nouns and the GLSAN nouns vectors.
Evaluation We used the minimum squared
residue co-clustering algorithm 4. We report two
evaluation measures: accuracy and the F1-score.
The clustering algorithm assigns each document to
a cluster. We map the cluster id?s to topic labels
using the Munkres assignment algorithm (Munkres,
1957) and compute the accuracy as the ratio of the
correctly assigned labels.
The F1 score for cluster ci labeled with topic ti is
computed using F1 = 2(p?r)(p+r) where p is precision
and r is recall. For clusters C = (c1, ..., cn) and
topics T = (t1, ..., tn) we compute the total score:
F1(C, T ) =
?
t?T
Nt
N maxc?C F1(c, t).
Nt is the number of documents belonging to the
topic t and N is the total number of documents. This
measure accounts for the topic size and also corrects
the topic assignments to clusters by using the max.
4 Results and Conclusion
Table 3 shows that the spectral methods outperform
the tf-idf representations and have smaller variance.
We report the performance for four subsets. The
subset 5?10 has a large number of topics, each with
a similar number of documents. The subset 50?150
has a large number of topics with a less even distri-
bution of documents. 500 ? 1000 and 1000 ? 5000
have a couple of large topics. We ran the clustering
over 30 random initializations. To eliminate the ef-
fect of the initial conditions on the performance we
also used one document per cluster to seed the initial
assignment for the 5? 10 subset.
All methods have the worst performance for the
5?10 subset. The best performance is for the subset
500?1000. LSA and GLSAlocal indexing are com-
puted based on the TDT2 collection. GLSAlocal has
better average performance which confirms that the
co-occurrence proximity is important for distribu-
tional similarity. The GLSA indexing computed us-
ing a large corpus performs significantly worse than
4http://www.cs.utexas.edu/users/dml/Software/cocluster.html
115
All words LSA GLSAlocal GLSA onlyN GLSAN Hybrid
5-10 acc 0.56(0.11) 0.69(0.07) 0.78(0.05) 0.60(0.05) 0.63(0.05) 0.76(0.05) 0.82(0.05)
F1 0.60(0.09) 0.73(0.05) 0.81(0.04) 0.64(0.05) 0.67(0.05) 0.80(0.04) 0.85(0.04)
50-150 acc 0.75(0.05) 0.73(0.06) 0.80(0.05) 0.70(0.04) 0.68(0.04) 0.80(0.04) 0.87(0.04)
F1 0.80(0.04) 0.78(0.05) 0.84(0.04) 0.75(0.04) 0.75(0.03) 0.84(0.04) 0.90(0.03)
500-1000 acc 0.95(0.03) 0.98(0.00) 0.99(0.00) 0.97(0.00) 0.97(0.00) 0.99(0.00) 1.00(0.00)
F1 0.95(0.03) 0.98(0.00) 0.99(0.00) 0.97(0.00) 0.97(0.00) 0.99(0.00) 1.00(0.00)
1000-5000 acc 0.86(0.11) 0.88(0.04) 0.88(0.13) 0.92(0.08) 0.82(0.06) 0.92(0.00) 0.96(0.07)
F1 0.88(0.07) 0.88(0.03) 0.90(0.09) 0.93(0.06) 0.82(0.04) 0.92(0.00) 0.97(0.05)
5-10s acc 0.932 0.919 0.986 0.932 0.980 0.980 0.992
F1 0.933 0.927 0.986 0.932 0.979 0.979 0.992
Table 3: Clustering accuracy (first row) and F1 score (second row) for each indexing scheme. The measures
are averaged over 30 random initiations of the clustering algorithm, the standard deviation is shown in
brackets. For the last experiment, 5-10s, we used one document per cluster as the initial assignment.
GLSAlocal on the heterogeneous 5?10 and 50?150
subsets and performs similarly for the other two. It
supports our intuition that the document?s style and
word distribution within the collection are important
and may get lost, especially if we use a document
collection with a different word distribution to esti-
mate the similarities matrix S.
The tf-idf indexing with nouns only, onlyN , has
good performance compared to the all-words index-
ing. The semantic similarity between nouns seems
to be collection independent. The GLSAN index-
ing is significantly better than onlyN and tf-idf in
most cases and performs similar to GLSAlocal. By
using GLSAN we computed the embedding for
more nouns that we could keep in the GLSAlocal
and GLSA representations. Nouns convey impor-
tant topic membership information and it is advan-
tageous to use as many of them as possible.
We observed the same performance relation when
we used labels to make the initial cluster assign-
ment, see 5? 10s in Table 3. tf-idf, GLSA and LSA
performed similarly, GLSAlocal and GLSAN per-
formed better with the hybrid scheme being the best.
The hybrid indexing significantly outperforms tf-
idf, LSA and GLSA on three subsets. This shows the
benefits of using the spectral embedding to discover
the semantic relations between nouns and keeping
the rest of the document content as tf-idf representa-
tion to preserve other indicators of its topic member-
ship. By combining two representations the hybrid
indexing scheme defines a more complex notion of
similarity between documents. For nouns it uses the
semantic proximity in the space of latent semantic
classes and for other words it uses term-matching.
References
Paolo Bientinesi, Inderjit S. Dhilon, and Robert A. van de
Geijn. 2003. A parallel eigensolver for dense sym-
metric matrices based on multiple relatively robust
representations. UT CS Technical Report TR-03-26.
Trevor F. Cox and Micheal A. Cox. 2001. Multidimen-
sional Scaling. CRC/Chapman and Hall.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management: Special Issue on Cross-language Infor-
mation Retrieval.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized latent semantic
analysis for term representation. In Proc. of RANLP.
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. SIAM, 5(1):32?38.
Gerard Salton and Michael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Research and Development in Infor-
mation Retrieval, pages 208?215.
116
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 269?272,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Investigating Pitch Accent Recognition in Non-native Speech
Gina-Anne Levow
Computer Science Department
University of Chicago
ginalevow@gmail.com
Abstract
Acquisition of prosody, in addition to vo-
cabulary and grammar, is essential for lan-
guage learners. However, it has received
less attention in instruction. To enable
automatic identification and feedback on
learners? prosodic errors, we investigate
automatic pitch accent labeling for non-
native speech. We demonstrate that an
acoustic-based context model can achieve
accuracies over 79% on binary pitch ac-
cent recognition when trained on within-
group data. Furthermore, we demonstrate
that good accuracies are achieved in cross-
group training, where native and near-
native training data result in no significant
loss of accuracy on non-native test speech.
These findings illustrate the potential for
automatic feedback in computer-assisted
prosody learning.
1 Introduction
Acquisition of prosody, in addition to vocabulary
and grammar, is essential for language learners.
However, intonation has been less-emphasized
both in classroom and computer-assisted language
instruction (Chun, 1998). Outside of tone lan-
guages, it can be difficult to characterize the fac-
tors that lead to non-native prosody in learner
speech, and it is difficult for instructors to find time
for the one-on-one interaction that is required to
provide feedback and instruction in prosody.
To address these problems and enable automatic
feedback to learners in a computer-assisted lan-
guage learning setting, we investigate automatic
prosodic labelling of non-native speech. While
many prior systems (Teixeia et al, 2000; Tep-
perman and Narayanan, 2008) aim to assign a
score to the learner speech, we hope to provide
more focused feedback by automatically identify-
ing prosodic units, such as pitch accents in English
or tone in Mandarin, to enable direct comparison
with gold-standard native utterances.
There has been substantial progress in auto-
matic pitch accent recognition for native speech,
achieving accuracies above 80% for acoustic-
feature based recognition in multi-speaker cor-
pora (Sridhar et al, 2007; Levow, 2008). How-
ever, there has been little study of pitch accent
recognition in non-native speech. Given the chal-
lenges posed for automatic speech recognition of
non-native speech, we ask whether recognition of
intonational categories is practical for non-native
speech. To lay the foundations for computer-
assisted intonation tutoring, we ask whether com-
petitive accuracies can be achieved on non-native
speech. We further investigate whether good
recognition accuracy can be achieved using rel-
atively available labeled native or near-native
speech, or whether it will be necessary to col-
lect larger amounts of training or adaptation data
matched for speaker, language background, or lan-
guage proficiency.
We employ a pitch accent recognition approach
that exploits local and coarticulatory context to
achieve competitive pitch accent recognition accu-
racy on native speech. Using a corpus of prosod-
ically labelled native and non-native speech, we
illustrate that similar acoustic contrasts hold for
pitch accents in both native and non-native speech.
These contrasts yield competitive accuracies on
binary pitch accent recognition using within-group
training data. Furthermore, there is no significant
drop in accuracy when models trained on native or
near-native speech are employed for classification
of non-native speech.
The remainder of the paper is organized as fol-
lows. We present the LeaP Corpus used for our
experiments in Section 2. We next describe the
feature sets employed for classification (Section 3)
and contrastive acoustic analysis for these features
in native and non-native speech (Section 4). We
269
ID Description
c1 non-native, before prosody training
c2 non-native, after first prosody training
c3 non-native, after second prosody training
e1 non-native, before going abroad
e2 non-native, after going abroad
sl ?super-learner?, near-native
na native
Table 1: Speaker groups, with ID and description
in the LeaP Corpus
then describe the classifier setting and experimen-
tal results in Section 5 as well as discussion. Fi-
nally, we present some conclusions and plans for
future work.
2 LeaP Corpus and the Dataset
We employ data from the LeaP Corpus (Milde and
Gut, 2002), collected at the University of Biele-
feld as part of the ?Learning Prosody in a For-
eign Language? project. Details of the corpus
(Milde and Gut, 2002), inter-rater reliability mea-
sures (Gut and Bayerl, 2004), and other research
findings (Gut, 2009) have been reported.
Here we focus on the read English segment of
the corpus that has been labelled with modified
EToBI tags
1
, to enable better comparison with
prior results of prosodic labelling accuracy and
also to better model a typical language laboratory
setting where students read or repeat. This yields
a total of 37 recordings of just over 300 syllables
each, from 26 speakers, as in Table 1.
2
This set
allows the evaluation of prosodic labelling across
a range of native and non-native proficiency lev-
els. The modified version of ETobi employed by
the LeaP annotators allows transcription of 14 cat-
egories of pitch accent and 14 categories of bound-
ary tone. However, in our experiments, we will fo-
cus only on pitch accent recognition and will col-
lapse the inventory to the relatively standard, and
more reliably annotated, four-way (high, down-
stepped high, low, and unaccented) and binary (ac-
cented, unaccented) label sets.
1
While the full corpus includes speakers from a range of
languages, the EToBI labels were applied primarily to data
from German speakers.
2
Length of recordings varies due to differences in syllab-
ification and cliticization, as well as disfluencies and reading
errors.
3 Acoustic-Prosodic Features
Recent research has highlighted the importance
of context for both tone and intonation. The
role of context can be seen in the characteriza-
tion of pitch accents such as down-stepped high
and in phenomena such as downdrift across a
phrase. Further, local coarticulation with neigh-
boring tones has been shown to have a signif-
icant impact on the realization of prosodic ele-
ments, due to articulatory constraints (Xu and
Sun, 2002). The use of prosodic and coarticu-
latory context has improved the effectiveness of
tone and pitch accent recognition in a range of lan-
guages (Mandarin (Wang and Seneff, 2000), En-
glish (Sun, 2002)) and learning frameworks (deci-
sion trees (Sun, 2002), HMMs (Wang and Seneff,
2000), and CRFs (Levow, 2008)).
Thus, in this work, we employ a rich contextual
feature set, based on that in (Levow, 2008). We
build on the pitch target approximation model, tak-
ing the syllable as the domain of tone prediction
with a pitch height and contour target approached
exponentially over the course of the syllable, con-
sistent with (Sun, 2002). We employ an acoustic
model at the syllable level, employing pitch, in-
tensity and duration measures. The acoustic mea-
sures are computed using Praat?s (Boersma, 2001)
?To pitch? and ?To intensity.? We log-scaled and
speaker-normalized all pitch and intensity values.
We compute two sets of features: one set de-
scribing features local to the syllable and one set
capturing contextual information.
3.1 Local features
We extract features to represent the pitch height
and pitch contour of the syllable. For pitch fea-
tures, we extract the following information: (a)
pitch values for five evenly spaced points in the
voiced region of the syllable, (b) pitch maximum,
mean, minimum, and range, and (c) pitch slope,
from midpoint to end of syllable. We also ob-
tain the following non-pitch features: (a) intensity
maximum and mean and (b) syllable duration.
3.2 Context Modeling
To capture local contextual influences and cues,
we employ two sets of features. The first set of fea-
tures includes differences between pitch maxima,
pitch means, pitches at the midpoint of the sylla-
bles, pitch slopes, intensity maxima, and intensity
means, between the current and preceding or fol-
270
lowing syllable. The second set of features adds
the last pitch values from the end of the preceding
syllable and the first from the beginning of the fol-
lowing syllable. These features capture both the
relative differences in pitch associated with pitch
accent as well as phenomena such as pitch peak
delay in which the actual pitch target may not be
reached until the following syllable.
4 Acoustic Analysis of Native and
Non-native Tone
To assess the potential effectiveness of tone recog-
nition for non-native speech, we analyze and com-
pare native and non-native speech with respect to
features used for classification that have shown
utility in prior work. Pitch accents are charac-
terized not only by their absolute pitch height,
but also by contrast with neighboring syllables.
Thus, we compare the values for pitch and delta
pitch, the difference between the current and pre-
ceding syllable, both with log-scaled measures for
high-accented and unaccented syllables. We con-
trast these values within speaker group (native: na;
non-native: e1, c1). We also compare the delta
pitch measures between speaker groups (na versus
e1 or c1).
Not only do we find significant differences for
delta pitch between accented and unaccented syl-
lables for native speakers as we expect, but we
find that non-native speakers also exhibit signif-
icant differences for this measure (t-test, two-
tailed,p < 0.001). Accented syllables are reli-
ably higher in pitch than immediately preceding
syllables, while unaccented syllables show no con-
trast. Importantly, we further observe a significant
difference in delta pitch for high accented sylla-
bles between native and non-native speech. Na-
tive speakers employ a markedly larger change in
pitch to indicate accent than do non-native speak-
ers, a fine-grained view consistent with findings
that non-native speakers employ a relatively com-
pressed pitch range (Gut, 2009).
For one non-native group (e1), we find that al-
though these speakers produce reliable contrasts
in delta pitch between neighboring syllables, the
overall pitch height of high accented syllables is
not significantly different from that of unaccented
syllables. For native speakers and the ?c1? non-
native group, though, overall pitch height does
differ significantly between accented and unac-
cented syllables. This finding suggests that while
all speakers in this data set understand the locally
contrastive role of pitch accent, some non-native
speaker groups do not have as reliable global con-
trol of pitch.
The presence of these reliable contrasts between
accented and unaccented syllables in both na-
tive and non-native speech suggests that automatic
pitch accent recognition in learner speech could be
successful.
5 Pitch Accent Recognition Experiments
We assess the effectiveness of pitch accent recog-
nition on the LeaP Corpus speech. We hope to
understand whether pitch accent can be accurately
recognized in non-native speech and whether ac-
curacy rates would be competitive with those on
native speech. In addition, we aim to compare the
impact of different sources of training data. We
assess whether non-native prosody can be recog-
nized using native or near-native training speech or
whether it will be necessary to use matched train-
ing data from non-natives of similar skill level or
language background.
Thus we perform experiments on matched train-
ing and test data, training and testing within
groups of speakers. We also evaluate cross-group
training and testing, training on one group of
speakers (native and near-native) and testing on
another (non-native). We contrast all these results
with assignment of the dominant ?unaccented? la-
bel to all instances (common class).
5.1 Support Vector Machine Classifier
For all supervised experiments reported in this pa-
per, we employ a Support Vector machine (SVM)
with a linear kernel. Support Vector Machines pro-
vide a fast, easily trainable classification frame-
work that has proven effective in a wide range of
application tasks. For example, in the binary clas-
sification case, given a set of training examples
presented as feature vectors of length D, the lin-
ear SVM algorithm learns a vector of weights of
length D which is a linear combination of a sub-
set of the input vectors and performs classification
based on the function f(x) = sign(w
T
x? b). We
employ the publicly available implementation of
SVMs, LIBSVM (C-C.Cheng and Lin, 2001).
5.2 Results
We see that, for within group training, on the
binary pitch accent recognition task, accuracies
271
c1 c2 c3 e1 e2 sl na
Within-group Accuracy 79.1 80.9 80.6 81 82.5 82.4 81.2
Cross-group Accuracy (na) 77.2 79 81.4 80.3 82.5 83.2
Cross-group Accuracy (sl) 77.3 79.9 82 80.5 82.9 81.6
Common Class 56.9 59.6 56.2 70.2 64 65.5 63.6
Table 2: Pitch accent recognition, within-group, cross-group with native and near-native training, and
most common class baseline: Non-native (plain), ?Super-learner? (underline sl), Native (bold na)
range from approximately 79% to 82.5%. These
levels are consistent with syllable-, acoustic-
feature-based prosodic recognition reported in the
literature (Levow, 2008). A summary of these re-
sults appears in Table 2. In the cross-group train-
ing and testing condition, we observe some vari-
ations in accuracy, for some training sets. How-
ever, crucially none of the differences between
native-based or near-native training and within-
group training reach significance for the binary
pitch accent recognition task.
6 Conclusion
We have demonstrated the effectiveness of pitch
accent recognition on both native and non-native
data from the LeaP corpus, based on significant
differences between accented and unaccented syl-
lables in both native and non-native speech. Al-
though these differences are significantly larger in
native speech, recognition remains robust to train-
ing with native speech and testing on non-native
speech, without significant drops in accuracy. This
result argues that binary pitch accent recognition
using native training data may be sufficiently ac-
curate that to avoid collection and labeling of large
amounts of training data matched by speaker or
fluency-level to support prosodic annotation and
feedback. In future work, we plan to incorporate
prosodic recognition and synthesized feedback to
support computer-assisted prosody learning.
Acknowledgments
We thank the creators of the LeaP Corpus as well
as C-C. Cheng and C-J. Lin for LibSVM. This
work was supported by NSF IIS: 0414919.
References
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9?10):341?345.
C-C.Cheng and C-J. Lin. 2001. LIBSVM:a library
for support vector machines. Software available at:
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Dorothy M. Chun. 1998. Signal analysis software for
teaching discourse intonation. Language Learning
& Technology, 2(1):61?77.
U. Gut and P. S. Bayerl. 2004. Measuring the relia-
bility of manual annotations of speech corpora. In
Proceedings of Speech Prosody 2004.
U. Gut. 2009. Non-native speech. A corpus-based
analysis of phonological and phonetic properties of
L2 English and German. Peter Lang, Frankfurt.
G.-A. Levow. 2008. Automatic prosodic labeling with
conditional random fields and rich acoustic features.
In Proceedings of the IJCNLP 2008.
J.-T. Milde and U. Gut. 2002. A prosodic corpus of
non-native speech. In Proceedings of the Speech
Prosody 2002 Conference, pages 503?506.
V. Rangarajan Sridhar, S. Bangalore, and S. Narayanan.
2007. Exploiting acoustic and syntactic features for
prosody labeling in a maximum entropy framework.
In Proceedings of HLT NAACL 2007, pages 1?8.
Xuejing Sun. 2002. Pitch accent prediction using en-
semble machine learning. In Proceedings of ICSLP-
2002.
C. Teixeia, H. Franco, E. Shriberg, K. Precoda, and
K. Somnez. 2000. Prosodic features for automatic
text-independent evaluation of degree of nativeness
for language learners. In Proceedings of ICSLP
2000.
J. Tepperman and S. Narayanan. 2008. Better non-
native intonation scores through prosodic theory. In
Proceedings of Interspeech 2008.
C. Wang and S. Seneff. 2000. Improved tone recogni-
tion by normalizing for coarticulation and intonation
effects. In Proceedings of 6th International Confer-
ence on Spoken Language Processing.
Yi Xu and X. Sun. 2002. Maximum speed of pitch
change and how it may relate to speech. Journal of
the Acoustical Society of America, 111.
272
Issues in Pre- and Post-translation Document Expansion:
Untranslatable Cognates and Missegmented Words
Gina-Anne Levow
University of Chicago
1100 E. 58th St., Chicago, IL 60637, USA
levow@cs.uchicago.edu
Abstract
Query expansion by pseudo-relevance
feedback is a well-established technique
in both mono- and cross- lingual informa-
tion retrieval, enriching and disambiguat-
ing the typically terse queries provided
by searchers. Comparable document-side
expansion is a relatively more recent de-
velopment motivated by error-prone tran-
scription and translation processes in spo-
ken document and cross-language re-
trieval. In the cross-language case, one
can perform expansion before translation,
after translation, and at both points. We
investigate the relative impact of pre- and
post- translation document expansion for
cross-language spoken document retrieval
in Mandarin Chinese. We find that post-
translation expansion yields a highly sig-
nificant improvement in retrieval effec-
tiveness, while improvements due to pre-
translation expansion alone or in combina-
tion do not reach significance. We identify
two key factors of segmentation and trans-
lation in Chinese orthography that limit
the effectiveness of pre-translation expan-
sion in the Chinese-English case, while
post-translation expansion yields its full
benefit.
1 Introduction
Information retrieval aims to match the informa-
tion need expressed by the searcher in the query
with concepts expressed in documents. This match-
ing process is complicated by the variety of dif-
ferent ways - different terms - available to express
these concepts and information needs. In addition,
this matching process is dramatically complicated
in cross-language and spoken document retrieval
by the need to match expressions across languages
and typically using error-prone processes such as
translation and automatic speech recognition tran-
scription. To compensate for this variation in ex-
pression of underlying concepts, researchers have
developed the technique of pseudo-relevance feed-
back whereby the information representation - query
or document - is enriched with highly selective,
topically related terms from a large collection of
comparable documents. Such expansion techniques
have proved useful across the range of information
retrieval applications from mono-lingual to multi-
lingual, from text to speech, and from queries to doc-
uments.
Expansion in the context of cross-language in-
formation retrieval (CLIR) is particularly interesting
as it presents multiple opportunities for improving
retrieval effectiveness. The pseudo-relevance feed-
back process can be applied, depending on the re-
trieval architecture, before translating the query, af-
ter translating the query, before translating the doc-
ument, after translating the document, or at some
subset of these points, though not all combinations
are reasonable. While pre- and post-translation ex-
pansion have been well-studied for a query transla-
tion architecture in European languages, as we de-
scribe in more detail below, these effects are less
well-understood on the document side, especially
for Asian languages.
In this paper, we compare the effects of pre-
translation, post-translation, and combined pre-
and post-translation document expansion for cross-
language retrieval using English queries to retrieve
spoken documents in Mandarin Chinese. We iden-
tify not only significant enhancements to retrieval
effectiveness for post-translation document expan-
sion, but also key contrasts with prior work on query
translation and expansion, caused by certain char-
acteristics of Mandarin Chinese, shared by many
Asian languages, including issues of segmentation
and orthography.
2 Related Work
This work draws on prior research in pseudo-
relevance feedback for both queries and documents.
2.1 Pre- and Post-translation Query Expansion
In pre-translation query expansion, the goal is both
that of monolingual query expansion - providing ad-
ditional terms to refine the query and to enhance
the probability of matching the terminology cho-
sen by the authors of the document - and to pro-
vide additional terms to limit the possibility of fail-
ing to translate a concept in the query simply be-
cause the particular term is not present in the trans-
lation lexicon. (Ballesteros and Croft, 1997) eval-
uated pre- and post-translation query expansion in
a Spanish-English cross-language information re-
trieval task and found that combining pre- and post-
translation query expansion improved both precision
and recall with pre-translation expansion improving
both precision and recall, and post-translation ex-
pansion enhancing precision. (McNamee and May-
field, 2002)?s dictionary ablation experiments on the
effect of translation resource size and pre- and post-
translation query expansion effectiveness demon-
strated the key and dominant role of pre-translation
expansion in providing translatable terms. If too few
terms are translated, post-translation expansion can
provide little improvement.
2.2 Document Expansion
The document expansion approach was first pro-
posed by (Singhal et al, 1999) in the context of
spoken document retrieval. Since spoken document
retrieval involves search of error-prone automatic
speech recognition transcriptions, Singhal et alin-
troduced document expansion as a way of recover-
ing those words that might have been in the original
broadcast but that had been misrecognized. They
speculated that correctly recognized terms would
yield a topically coherent transcript, while the spo-
radic errors would be from a random distribution.
Enriching the documents with highly selective terms
drawn from highly ranked documents retrieved by
using the document itself as a query yielded re-
trieval effectiveness that improved not only over the
original errorful transcription but also over a perfect
manual transcription. (Levow and Oard, 2000) ap-
plied post-translation document expansion to both
spoken documents and newswire text in Mandarin-
English multi-lingual retrieval and found some im-
provements in retrieval effectiveness. (Levow,
2003) evaluated multi-scale units (words and bi-
grams) for post-transcription expansion of Mandarin
spoken documents, finding the significant improve-
ments for expansion with word units using bigram
based indexing.
3 Experimental Configuration
Here we describe the basic experimental configu-
ration under which contrastive document expansion
experiments were carried out.
3.1 Experimental Collection
We used the Topic Detection and Tracking (TDT)
Collection for this work. TDT is an evaluation pro-
gram where participating sites tackle tasks as such
identifying the first time a story is reported on a
given topic or grouping similar topics from audio
and textual streams of newswire date. In recent
years, TDT has focused on performing such tasks
in both English and Mandarin Chinese.1 The task
that we have performed is not a strict part of TDT
because we are performing retrospective retrieval
which permits knowledge of the statistics for the
entire collection. Nevertheless, the TDT collection
serves as a valuable resource for our work. The
TDT multilingual collection includes English and
Mandarin newswire text as well as (audio) broad-
cast news. For most of the Mandarin audio data,
word-level transcriptions produced by the Dragon
1This year Arabic was added to the languages of interest.
automatic speech recognition system are provided.
All news stories are exhaustively tagged with event-
based topic labels, which serve as the relevance
judgments for performance evaluation of our cross-
language spoken document retrieval work. We used
a subset of the TDT-2 corpus for the experiments re-
ported here.
3.2 Query Formulation
TDT frames the retrieval task as query-by-example,
designating 4 exemplar documents to specify the in-
formation need. For query formulation, we con-
structed a vector of the 180 terms that best distin-
guish the query exemplars from other contempora-
neous (and hopefully not relevant) stories. We used
a   test in a manner similar to that used by Schu?tze
et al(Schu?tze et al, 1995) to select these terms.
The pure    statistic is symmetric, assigning equal
value to terms that help to recognize known rele-
vant stories and those that help to reject the other
contemporaneous stories. We limited our choice to
terms that were positively associated with the known
relevant training stories. For the    computation,
we constructed a set of 996 contemporaneous doc-
uments for each topic by removing the four query
examplars from a topic-dependent set of up to 1000
stories working backwards chronologically from the
last English query example. Additional details may
be found in (Levow and Oard, 2000).
3.3 Document Translation
Our translation strategy implemented a word-for-
word translation approach. For our original
spoken documents, we used the word bound-
aries provided in the baseline recognizer tran-
scripts. We next perform dictionary-based word-
for-word translation, using a bilingual term list
produced by merging the entries from the sec-
ond release of the LDC Chinese-English term list
(http://www.ldc.upenn.edu, (Huang, 1999)) and en-
tries from the CETA file, a large human-readable
Chinese-English dictionary. The resulting term list
contains 195,078 unique Mandarin terms, with an
average of 1.9 known English translations per Man-
darin term. We select the translation with the highest
target language unigram frequency, based on a side
collection in the target language.
3.4 Document Expansion
We implemented document expansion for the VOA
Mandarin broadcast news stories in an effort to par-
tially recover terms that may have been mistran-
scribed. Singhal et al used document expansion for
monolingual speech retrieval (Singhal and Pereira,
1999).
The automatic transcriptions of the VOA Man-
darin broadcast news stories and their word-for-
word translations are an often noisy representation
of the underlying stories. For expansion, the text
of these documents was treated as a query to a
comparable collection (in Mandarin before transla-
tion and English after translation), by simply com-
bining all the terms with uniform weighting. This
query was presented to the InQuery retrieval system
version 3.1pl developed at the University of Mas-
sachusetts (Callan et al, 1992).
Figure 1 depicts the document expansion process.
The use of pre- and post-translation document ex-
pansion components was varied as part of the ex-
perimental suite described below. We selected the
five highest ranked documents from the ranked re-
trieval list. From those five documents, we extracted
the most selective terms and used them to enrich the
original translations of the stories. For this expan-
sion process we first created a list of terms from the
documents where each document contributed one in-
stance of a term to the list. We then sorted the terms
by inverse document frequency (IDF). We next aug-
mented the original documents with these terms
until the document had approximately doubled in
length. Doubling was computed in terms of number
of whitespace delimited units. For Chinese audio
documents, words were identified by the Dragon au-
tomatic speech recognizer as part of the transcription
process. For the Chinese newswire text, segmenta-
tion was performed by the NMSU segmenter ( (Jin,
1998)). The expansion factor chosen here followed
Singhal et als original proposal. A proportional
expansion factor is more desirable than some con-
stant additive number of words or some selectivity
threshold, as it provides a more consistent effect on
documents of varying lengths; an IDF-based thresh-
old, for example, adds disproportionately more new
terms to short original documents than long ones,
outweighing the original content. Prior experiments
indicate little sensitivity to the exact expansion fac-
tor chosen, as long as it is proportional.
This process thus relatively increased the weight
of terms that occurred rarely in the document collec-
tion as a whole but frequently in related documents.
The resulting augmented documents were then in-
dexed by InQuery in the usual way.This expanded
document collection formed the basis for retrieval
using the translated exemplar queries.
The intuition behind document expansion is that
terms that are correctly transcribed will tend to be
topically coherent, while mistranscription will intro-
duce spurious terms that lack topical coherence. In
other words, although some ?noise? terms are ran-
domly introduced, some ?signal? terms will survive.
The introduction of spurious terms degrades ranked
retrieval somewhat, but the adverse effect is limited
by the design of ranking algorithms that give high
scores to documents that contain many query terms.
Because topically related terms are far more likely
to appear together in documents than are spurious
terms, the correctly transcribed terms will have a
disproportionately large impact on the ranking pro-
cess. The highest ranked documents are thus likely
to be related to the correctly transcribed terms, and
to contain additional related terms. For example, a
system might fail to accurately transcribe the name
?Yeltsin? in the context of the (former) ?Russian
Prime Minister?. However, in a large contemporane-
ous text corpus, the correct form of the name will ap-
pear in such document contexts, and relatively rarely
outside of such contexts. Thus, it will be a highly
correlated and highly selective term to be added in
the course of document expansion.
4 Document Expansion Experiments
Our goal is to evaluate the effectiveness of pseudo-
relevance feedback expansion applied at different
stages of document processing and determine what
factors contribute to the any differences in final re-
trieval effectiveness. We consider expansion before
translation, after translation, and at both points. The
expansion process aims to (re)introduce terminology
that could have been used by the author to express
the concepts in the documents. Expansion at differ-
ent stages of processing addresses different causes
of loss or absence of terms. At all points, it can ad-
!"#$%&?
()*+,
#$%&?+-%./)&
0%1$2/1
!"#$%&?
3)4*+356"%1%
7%8186&%+3)&*$1
9:";:&6"
<&):;.:1/
7%81
(%&4+=%2%./6)"
>=0
(&:"1.&6*/6)"
(&:"1.&6?%;
@).$4%"/1
()*+,
!"#$%&?
3)4*+A"B2615
7%8186&%+3)&*$1
(%&4+=%2%./6)"
(&:"12:/%;
@).$4%"/1
(&:"12:/6)"
C&%D/&:"12:/6)"
AE*:"16)"
C)1/D/&:"12:/6)"
AE*:"16)"
Figure 1: Document Expansion Process
dress terminological choice by the author.
Since we are working with automatic transcrip-
tions of spoken documents, pre-translation (post-
transcription) expansion directly addresses term loss
due to substitution or deletion errors in automatic
recognition. In addition, as emphasized by (Mc-
Namee and Mayfield, 2002), pre-translation expan-
sion can be crucial to providing translatable terms so
that there is some material for post-translation index-
ing and matching to operate on. In other words, by
including a wider range of expressions of the docu-
ment concepts, pre-translation expansion can avoid
translation gaps by enhancing the possibility that
some term representing a concept that appears in
the original document will have a translation in the
bilingual term list. Addition of terms can also serve
a disambiguating effect as identified by (Ballesteros
and Croft, 1997).
Post-translation expansion provides an opportu-
nity to address translation gaps even more strongly.
Pre-translation expansion requires that there be
some representation of the document language con-
cept in the term list, whereas post-translation expan-
sion can acquire related terms with no representation
in the translation resources from the query language
side collection. This capability is particularly desir-
able given both the important role of named entities
(e.g. person and organization names) in many re-
trieval activities, in conjunction with their poor cov-
erage in most translation resources. Finally, it pro-
vides the opportunity to introduce additional con-
ceptually related terminology in the query language,
even if the document language form of the term was
not introduced by the original author to enhance the
representation.
We evaluate four document processing configura-
tions:
1. No Expansion
Documents are translated directly as de-
scribed above, based on the provided automatic
speech recognition transcriptions.
2. Pre-translation Expansion
Documents are expanded as described
above, using a contemporaneous Mandarin
newswire text collection from Xinhua and Za-
obao news agencies. These collections are
segmented into words using the NMSU seg-
menter. The resulting documents are translated
as usual. Note that translation requires that the
expansion units be words.
3. Post-translation Expansion
The English document forms produced by
item 1 are expanded using a contemporaneous
collection of English newswire text from the
New York Times and Associated Press (also
part of the TDT-2 corpus).
4. Pre- and Post-translation Expansion
The document forms produced by item 2
are translated in the the usual word-for-word
process. The resulting English text is expanded
as in item 3.
After the above processing, the resulting English
documents are indexed.
4.1 Results
The results of these different expansion configura-
tions appear in Figure 2. We observe that both post-
translation expansion and combined pre- and post-
translation document expansion yield highly sig-
nificant improvements (Wilcoxon signed rank test,
two-tailed,  
	 ) in retrieval effectiveness
over the unexpanded case. In contrast, although
pre-translation expansion yields an 18% relative in-
crease in mean average precision, this improvement
does not reach significance. The combination of pre-
and post-translation expansion increases effective-
ness by only 3% relative over post-translation ex-
pansion, but 33% relative over pre-translation ex-
pansion alone. This combination of pre- and post-
translation expansion significantly improves over
pre-translation document expansion alone (  

 ).
5 Discussion
These results clearly demonstrate the significant
utility of post-translation document expansion for
English-Mandarin CLIR with Mandarin spoken doc-
uments, in contrast to pre-translation expansion. Not
only do these results extend our understanding of the
interactions of translation and expansion, but they
contrast dramatically with prior work on translation
Document Expansion
None Pre Post Pre+Post
0.39 0.46 0.59 0.61
Figure 2: Retrieval effectiveness of document ex-
pansion
and query expansion - in particular, with the (Mc-
Namee and Mayfield, 2002) work emphasizing the
primary importance of pre-translation expansion.
Two main factors contribute to this contrast: first,
differences between languages, and second, differ-
ences between documents and queries. The charac-
teristics of the document and query languages play a
crucial role in determining the effectiveness of pre-
and post-translation document expansion. In partic-
ular, the orthography of Mandarin Chinese and the
difference in writing systems between the English
queries and Mandarin documents affect the expan-
sion process. If one examines the terms contributed
by post-translation expansion, one can quickly ob-
serve the utility of the enriching terms. For in-
stance in a document about the Iraqi oil embargo,
one finds the names of Tariq Aziz and Saddam; in an
article about the former Soviet republic of Georgia,
one finds the name of former president Zviad Gam-
sakhurdia. These and many of the other useful ex-
pansion terms do not appear anywhere in the transla-
tion resource. Even if these terms were proposed by
pre-translation expansion or existed in the original
document, they would not be available in the trans-
lated result. These named entities are highly useful
in many information retrieval activities but are no-
toriously absent from translation resources. For lan-
guages with different orthographies, these terms can
not match as cognates but must be explicitly trans-
lated or transliterated. Thus, these terms are only
useful for enrichment when the translation barrier
has already been passed. In contrast, the major-
ity of the query translation experiments that demon-
strate the utility of pre-translation expansion have
been performed on European language pairs that
share a common alphabet, making names found at
any stage of expansion available for matching as
cognates in retrieval even when no explicit transla-
tion is available. Recent side experiments on pre-
and post-translation query expansion on the English-
Chinese pair show a similar pattern of effectiveness
for post-translation expansion over pre-translation
expansion (Levow et al, Under Review).
A further complication is caused by the fact that
Mandarin Chinese is written without white space
separating words. As a result, some segmentation
process must be performed to identify words for
translation, even though indexing and retrieval can
be performed effectively on   -gram units (Meng et
al., 2001). This segmentation process typically re-
lies on a list of terms that may appear in legal seg-
mentations. Just as in the case of translation, these
term lists often lack good coverage of proper names.
Thus, these terms may not be identified for trans-
lation, expansion, or even transcription by an auto-
matic speech recognition system that also depends
on word lists as models. These constraints limit
the effectiveness of pre-translation expansion. In
post-translation expansion, however, these problems
are much less significant. In English, white-space
delimited terms are available and largely sufficient
for retrieval (especially after stemming). Even with
multi-word concepts as in the name examples above,
the cooccurrence of these terms in expansion docu-
ments makes it likely that they will cooccur in the
list of enriching terms as well, though perhaps not in
the same order. In Chinese or other typically unseg-
mented languages, overlapping   -grams can be used
as indexing or expansion units, to bypass segmenta-
tion issues, once translation has been completed.
Finally, (McNamee and Mayfield, 2002) observe
that pre-translation query expansion plays a crucial
role in ensuring that some terms are translatable, and
post-translation expansion would having nothing to
operate on if no query terms translated. This is cer-
tainly true, but this problem is much more likely to
arise in the case of short queries, where only a single
term may represent a topic and there are few terms in
the query. As documents are typically much longer,
there is often more redundancy of representation.
This is analogous to the observation (Krovetz, 1993)
that stemming has less of an impact as documents
become longer because a wider variety of surface
forms are likely to appear. Thus it is more likely
that some translatable form of a concept is likely to
appear in a long document, even without expansion
and even with a poor translation resource. As a re-
sult, pre-translation expansion may be less crucial
for long documents.
6 Conclusion
These factors together explain both the significant
improvement for post-translation document expan-
sion that our experiments illustrate in contrast to the
much weaker effects of pre-translation expansion,
and also the difference observed between the exper-
imental results reported here and prior work on pre-
and post-translation query expansion that has em-
phasized European language pairs. We have iden-
tified a key role for post-translation expansion in
CLIR language pairs where trivial cognate matching
is not possible, but explicit translation or translitera-
tion is required. We have also identified limitations
on pre-translation expansion due to corresponding
gaps in segmentation, translation, and transcription
resources. We believe that these findings will extend
to other CLIR language combinations with com-
parable characteristics, including many other Asian
languages.
References
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In Proceedings of
the 20th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
July.
James P. Callan, W. Bruce Croft, and Stephen M. Hard-
ing. 1992. The INQUERY retrieval system. In
Proceedings of the Third International Conference on
Database and Expert Systems Applications, pages 78?
83. Springer-Verlag.
Shudong Huang. 1999. Evaluation of LDC?s bilingual
dictionaries. Unpublished manuscript.
Wanying Jin. 1998. NMSU Chinese segmenter. In First
Chinese Language Processing Workshop, Philadel-
phia.
Robert Krovetz. 1993. Viewing morphology as an infer-
ence process. In SIGIR-93, pages 191?202.
Gina-Anne Levow and Douglas W. Oard. 2000.
Translingual topic tracking with PRISE. In Working
Notes of the Third Topic Detection and Tracking Work-
shop, February.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
Under Review. Dictionary-based techniques for cross-
language information retrieval.
Gina-Anne Levow. 2003. Multi-scale document ex-
pansion for mandarin chinese. In Proceedings of the
ISCA Workshop on Multi-lingual Spoken Document
Retrieval.
Paul McNamee and James Mayfield. 2002. Comparing
cross-language query expansion techniques by degrad-
ing translation resources. In Proceedings of the 25th
Annual International Conference on Research and De-
velopment in Information Retrieval (SIGIR-2002).
Helen Meng, Berlin Chen, Erika Grams, Wai-Kit Lo,
Gina-Anne Levow, Douglas Oard, Patrick Schone,
Karen Tang, and Jian Qiang Wang. 2001. Mandarin-
English Information (MEI): Investigating translingual
speech retrieval. In Human Language Technology
Conference.
Hinrich Sch u?tze, David A. Hull, and Jan O. Peder-
sen. 1995. A comparison of classifiers and docu-
ment representations for the routing problem. In Ed-
ward A. Fox, Peter Ingwersen, and Raya Fidel, ed-
itors, Proceedings of the 18th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 229?237, July.
ftp://parcftp.xerox.com/pub/qca/schuetze.html.
Amit Singhal and Fernando Pereira. 1999. Document
expansion for speech retrieval. In Proceedings of the
22nd International Conference on Research and De-
velopment in Information Retrieval, pages 34?41, Au-
gust.
Amit Singhal, John Choi, Donald Hindle, Julia
Hirschberg, Fernando Pereira, and Steve Whittaker.
1999. AT&T at TREC-7 SDR Track. In Proceedings
of the DARPA Broadcast News Workshop.
Learning to Speak to a Spoken Language System:
Vocabulary Convergence in Novice Users
Gina-Anne Levow
University of Chicago
levow@cs.uchicago.edu
Abstract
A key challenge for users and designers of
spoken language systems is determining
the form of the commands that the system
can recognize. Using more than 60 hours
of interactions, we quantitatively analyze
the acquisition of system vocabulary by
novice users. We contrast the longitudi-
nal performance of long-term novice users
with both expert system developers and
guest users. We find that novice users
successfully learn the form of system re-
quests, achieving a significant decrease in
ill-formed utterances. However, the work-
ing vocabulary on which novice users con-
verge is significantly smaller than that
of expert users, and their rate of speech
recognition errors remains higher. Finally,
we observe that only 50% of each user?s
small vocabulary is shared with any other,
indicating the importance of the flexibility
of a conversational interface that allows
users to converge to their own preferred
vocabulary.
Keywords Spoken Language System; Novice-
Expert; Lexical Entrainment
1 Introduction
Most currently deployed interactive spoken lan-
guage systems employ a restricted vocabulary and
syntax for system commands. These constraints pro-
vide greater recognition accuracy and faster recogni-
tion times. (Makhoul, 1993) However, they also re-
quire the system developer to provide a command
language that is expressive enough to accomplish
the tasks for which the speech system was designed
and flexible enough to allow use by a wide variety
of users with different levels of experience with the
system. In turn, the users must learn the constrained
language that is understood by the system. A con-
versational interface attempts to step away from a
rigid command language with, for example, a sin-
gle form for any command, to provide a set of well-
formed inputs that have more varied and natural syn-
tax and admit a range of synonymous terms and con-
structions. While it has been demonstrated that even
with substantial synonymy, users will still choose
terms outside the system?s vocabulary some percent-
age of the time (Furnas et al, 1987), it is hoped that
the flexibility of a conversational interface will allow
some natural individual variability and potentially
ease the task for novice users. A key challenge for
the user is thus to produce well-formed input to the
system under these restrictions, and for the system
designer to provide a set of commands that it is easy
for the user to learn. (Brennan, 1998) demonstrate
that users adopt the system?s terminology, most re-
liably with explicit correction, but also with im-
plicit correction, similar to the way in which pairs
of human speakers converge on a lexical referent.
(Walker et al, 1998) observe anecdotally that users
learn system vocabulary over time. (Yankelovich,
1996) and (Kamm et al, 1998) explore techniques
to guide users to produce well-formed queries, with
a variety of strategies and tutorials, respectively. The
above studies have focused on pure novice users
within their first few interactions with the system
and on the goal of task achievement. Here, we ana-
lyze quantitatively the process by which users learn
the language understood by the system, by exploring
natural interactions during the course of a field trial
conducted over a period of months. We analyze not
only task completion or command recognition, but
also the vocabulary acquired itself.
2 Data Collection
2.1 Speech System Description
The speech system utilized in the field trial is a pro-
totype spoken language system that provides a voice
interface to a variety of common desktop and in-
formation feed services, including e-mail, on-line
calendars, weather information, and stock quotes.
Two significant features distinguish this system from
other spoken language systems. First, since it was
designed for use over the telephone to provide ubiq-
uitous access, it is a voice-only system. Almost all
user input is spoken, recognized with BBN?s Hark
speech recognizer, and all output is through synthe-
sized speech, using Centigram?s TruVoice.
Secondly, the spoken language system was de-
signed to provide a ?conversational? interface as de-
scribed above, aiming to provide a more natural,
flexible alternative to a fixed command language.
All new users receive a wallet-sized information
card with examples of common commands, but, as
we will demonstrate later in this paper, users each
rapidly develop their own distinct forms.
The system was deployed for a field trial to a lim-
ited number of participants. All interactions were
recorded yielding approximately sixty hours of in-
teractions conducted over several months. In addi-
tion to the audio, speech recognizer results, natural
language analysis results, and the text of all system
responses were stored.
2.2 Subjects
The subjects participating in the field trial fell into
three distinct classes: 14 Novice Users, with no pre-
vious experience with this spoken language system,
4 Expert Users, long-term members of the system?s
development staff, and Guest Users, one-time users
of a public demonstration system.
There were three female, two novice and one ex-
pert, and fifteen male regular system users, twelve
novice and 3 expert. The users engaged in at least
ten phone conversations with the system. The dis-
tribution of users allows us to examine the develop-
ment of novice users? interaction style, in terms of
vocabulary choice and number of out-of-vocabulary
(OOV) utterances. In addition, we can contrast the
different recognition accuracy rates and vocabulary
distributions of expert and novice users.
2.3 Data Coding
All user utterances were manually transcribed and
paired with their corresponding speech recognizer
output. Each of these pairs was assigned one of
four accuracy codes: Correct, Error minor, Error,
or Rejection. The ?error minor? code assignments
generally resulted from a misrecognition of a non-
content word (e.g. an incorrect article) compensated
for by the robust parser. The ?error? and ?rejec-
tion? codes were assigned in those cases where a
user could identify a failure in the interaction. Utter-
ances coded either as Error or Rejection could also
receive an additional tag, OOV. This tag indicates
that either words not in the recognizer?s vocabulary
or constructions not in the system?s grammar were
used in the utterances. For simplicity, we refer to
both cases as OOV. Two examples appear below:
Unknown Word: Rejection
User Said: Abracadabracadabra
System Heard:   nothing 
Unknown Form: Misrecognition
User Said: Go to message five eight six
System Heard: Go to message fifty six
Grammar knows:Go to message five hundred
eighty six
3 Analysis
In total, there were 7529 recorded user utterances.
Of these, 4865 were correctly recognized, and 702
contained minor recognition errors, but still resulted
in the desired action. There were 1961 complete
recognition failures: 1250 of which were rejection
errors and 706 of which were misrecognition errors.
The remaining errors were due to system crashes or
parsing errors. Overall, this yields 25% error rate.
Figure 1: Distributions of Error Rates (Top)
Distributions of OOV Rates (Bottom)
Novice (Dark) vs Expert (Light)
Excluding errors by guest users, nearly 350 errors
resulted from OOV utterances. More than half of
these cases involved unknown words and one quarter
involved unknown grammatical constructions. The
remainder were valid utterances for a different ap-
plication, but were invalid in the application context
in which they were used.
To understand the users? lexical acquisition, we
will look at three specific features of user vocabu-
lary: error and out-of-vocabulary (OOV) rates over
time, vocabulary size and rate of new words over
time, and degree of vocabulary overlap among users.
3.1 Error and OOV Rates
We conduct a longitudinal examination of error and
out-of-vocabulary utterance rates. Overall rates are
given as averages, and longitudinal rates are in ut-
terances per hundred. Figure 1 compares the dis-
tributions of overall average error rates and out-of-
vocabulary rates for all novice users to that for ex-
pert users. We find significantly higher rates of over-
all recognition (24.86% versus 10.75%) and OOV
(7.39% versus 0.76%) errors for novices than for ex-
pert users.
Do these errors rates, especially the higher novice
user error rates, change over time, and if so, how and
how much? To track these longitudinal changes, or
Figure 2: Rate of errors (top) and OOVs (bottom)
over time
changes over time, we recompute the error and OOV
rates from above in terms of the number of errors per
hundred utterances for the first, second, and third set
of one hundred utterances, and so on.
We observe that neither the expert users (10.75%)
nor the guest users (41%) show any significant
change in error rate over time. However, novices
show a distinct decrease in errors after the first hun-
dred utterances (Figure 2). We can quantify this
contrast by comparing number of errors in the first
hundred utterances to the average number of er-
rors per hundred utterances for the later interac-
tions. This contrast is a significant decrease by t-test,
paired, two-tailed. (    ), showing that novice
users make fewer errors over time, but still at a much
higher rate than expert users.1
This observation comes as no surprise; however,
we would like to know which features of novice
vs. expert user interaction account for this contrast.
Specifically, to what degree do out-of-vocabulary ut-
terances or speech acoustics differentially affect the
error rates of these two subject groups? Can all con-
trasts be related to limited knowledge of the system?s
vocabulary? Experts, naturally, exhibit very few in-
stances of out-of-vocabulary utterances. Here we
1For longitudinal analysis, we consider only those users with
more than 200 turns with the system.
consider the change in rate of OOV?s in novice user
utterances over time and contrast it with that of the
guest user class. There is a significant decrease in
OOV?s over time for longer term users (Figure 2) in
contrast with an almost constant OOV rate for guest
users (20%) and for expert users (   1%). Specifi-
cally there is a significant decrease in the number
of OOVs between the first hundred utterances and
all subsequent interactions. This is clearly a desir-
able trend, indicating the new users? increasing fa-
miliarity with the limited vocabulary understood by
the system.
However, repeating the above error rate analysis
after excluding OOV-related errors, we find that the
decrease in error rates with time is not significant.
The decrease in OOV errors is thus the primary con-
tributor to the perceived improvement in recognition
rate over time. In addition, even with all OOV errors
removed, the error rates of novices are still much
higher than those of expert users (18.25% versus
10.25%), indicating that expert use of a spoken lan-
guage system requires more than just the knowledge
of the utterances understood by the system. This
knowledge is acquired fairly rapidly as we see by
the drop in OOV rates, but the knowledge of proper
speaking style, such as timing and pausing, is more
difficult.
3.2 Vocabulary Size and Rate of New Word
Introduction
Here we will use two measures to try to clarify
the process of OOV reduction: number of words in
working vocabulary (defined as number of discrete
words per hundred words spoken) and rate of intro-
duction of new words into the working vocabulary
(again in words per hundred). Unsurprisingly, the
rate of new word introduction undergoes a signif-
icant decrease over time - for all except the guest
user category - and, like OOVs, drops dramatically
after the first 200-300 words. Analysis of variance
of number of new words to point in time is highly
significant (F=59.27, df=323,        )
The trend for the working vocabulary is quite
interesting and somewhat unexpected. There is a
significant decrease in vocabulary size over time.
Specifically, there is a significant decrease in the
number of unique words per hundred between the
first 200-300 words and all later interactions. (F =
1.00 0.30 0.44 0.48 0.41 0.48 0.30 0.37 0.41
0.21 1.00 0.53 0.34 0.26 0.34 0.34 0.42 0.37
0.19 0.32 1.00 0.22 0.24 0.27 0.21 0.32 0.24
0.33 0.33 0.36 1.00 0.26 0.36 0.36 0.28 0.33
0.42 0.38 0.58 0.38 1.00 0.31 0.31 0.35 0.31
0.41 0.41 0.53 0.44 0.25 1.00 0.38 0.38 0.44
0.33 0.54 0.54 0.58 0.33 0.50 1.00 0.33 0.46
0.33 0.53 0.67 0.37 0.30 0.40 0.27 1.00 0.40
0.37 0.47 0.50 0.43 0.27 0.47 0.37 0.40 1.00
Table 1: Proportion of Two Subjects? Vocabulary
that is Shared
8.738, df = 19,       ) Specifically, novice users
who begin with an average working vocabulary of
54 words, after working with the system, converge
on a surprisingly small working vocabulary of an av-
erage of 35 distinct words per hundred. This small
vocabulary size contrasts strongly with the 50 dis-
tinct words per hundred of the expert users 2. From
this analysis, we can see that the decrease in out-
of-vocabulary utterances arises from a narrowing of
the users? working vocabulary to a fairly small set of
words in which the user has high confidence.
3.3 Vocabulary Overlap
What ramifications does this use of a small work-
ing vocabulary have for conversational speech user
interface design? Is it simply irrelevant since only
a small set of words is needed by any user? An
analysis of cross-user vocabulary will help to answer
these questions. Here we tabulated the percentage
of words shared between any pair of users and the
percentage of a user?s vocabulary that overlaps with
any other?s. We see that, for any pair of users, be-
tween 18 - 57% of vocabulary is held in common,
with an average of 21% of the union of the two vo-
cabularies falling in the intersection (Table 1). 3 This
translates to each user sharing approximately 50% of
their words with any other given user.
This relatively small proportion of overlap be-
tween users attests to the value of the conversa-
tional interface. While the users individually do not
have large vocabularies, the choice of words across
users is highly varied. This supports the notion of
a flexible vocabulary that allows users to gravitate
2The expert users do not, in fact, use more of the system
applications than novices.
3Results shown for the nine novice users with more than 200
turns.
toward lexical usages that come naturally, and sup-
ports wide cross-user utility.
4 Discussion & Conclusion
We observe the significant reduction in recognition
errors, largely through a reduction in ill-formed ut-
terances, of novices over their first two to three hun-
dred utterances. This accomplishment supports the
anecdotal reports that users learn system vocabulary
over time, but most impressively, demonstrates the
speed with which users acquire the necessary vo-
cabulary, even in the absence of explicit guidance
or correction.
Many of these early OOV errors arise from issues
in speech system design. Two design goals often
come into conflict: keeping the active recognition
vocabulary small to improve recognition speed and
accuracy and providing a consistent and wide cover-
age vocabulary to the users to enhance flexibility and
functionality. Stock quotes and weather searches are
limited to a small subset of possible cases: technol-
ogy stocks and major U.S. cities respectively. Errors
arise as users, for instance, try to query Canadian
cities. These limitations could be clarified in the
system prompts. Likewise, only application-specific
vocabulary and a small general vocabulary are ac-
tive at any time. Users, rather naturally, generalize
vocabulary use, and encounter a significant number
of errors due to utterances that would be acceptable
in another portion of the system. For example, ?can-
cel? halts e-mail sending, but was erroneously used
to try to stop other system activities. Thus, focusing
on consistent vocabulary and structure across appli-
cations is desirable. Finally, since the system reads
e-mail headers and bodies, the system inevitably vi-
olates the dictum that it should never say words that
the system can not itself recognize. Users frequently
try to use these terms themselves and learn over only
time that they are not in the recognizer?s vocabulary.
It is necessary to develop a strategy to differenti-
ate this type of content from regular conversational
turns, possibly through a different synthetic voice.
The skilled novice users still differ significantly
from expert users in two respects: overall recogni-
tion accuracy and working vocabulary size. Novice
users gradually remove ill-formed utterances from
their input to the system. They achieve this result, in
part, by converging on a small working vocabulary
in which they have high confidence. Interestingly,
this vocabulary varies substantially among users,
suggesting an advantage to the conversational inter-
face that allows users more flexibility in their choice
of words and constructions. We still find, though,
that even if we exclude all errors resulting from out-
of-vocabulary utterances from consideration, novice
users suffer from significantly worse speech recog-
nition performance than do the expert system de-
velopers. Many of these remaining errors involve
speaking too soon, speaking too slowly, or speak-
ing with lengthy pauses. These limitations in over-
all speech recognition accuracy and restricted vo-
cabulary indicate that additional training that guides
users to a suitable speaking style and full exploita-
tion of the system?s vocabulary and capabilities is
necessary for the competent novice users to become
true experts.
Acknowledgments We thank Nicole Yankelovich
and Sun Microsystems for access to the field trial
data and transcriptions.
References
S. Brennan, 1998. The grounding problem in conver-
sations with and through computers, pages 201?225.
Lawrence Erlbaum.
G. Furnas, T. Landauer, L. Gomez, and S. Dumais. 1987.
The vocabulary problem in human-system communi-
cations. Communications of the ACM, 30:964?971.
C. Kamm, D. Litman, and M. Walker. 1998. From
novice to expert: the effect of tutorials on user exper-
tise with spoken dialogue systems. In Proceedings of
the 5th International Conference on Spoken Language
Processing (ICSLP), pages 1211?1214.
J. Makhoul. 1993. Overview of speech recognition
technology. colloquium presentation, human-machine
communication by voice. National Academy of Sci-
ences, Irvine, CA.
M. Walker, J. Fromer, G. Di Fabbrizio, C. Mestel, and
D. Hindle. 1998. What can i say: Evaluating a spo-
ken language interface to email. In Proceedings of the
Conference on Human Factors in Computing Systems,
CHI98.
N. Yankelovich. 1996. How do users know what to say?
ACM Interactions, 3(6).
Combining Prosodic and Text Features for Segmentation of Mandarin
Broadcast News
Gina-Anne Levow
University of Chicago
levow@cs.uchicago.edu
Abstract
Automatic topic segmentation, separation of a dis-
course stream into its constituent stories or topics,
is a necessary preprocessing step for applications
such as information retrieval, anaphora resolution,
and summarization. While significant progress has
been made in this area for text sources and for En-
glish audio sources, little work has been done in
automatic, acoustic feature-based segmentation of
other languages. In this paper, we consider exploit-
ing both prosodic and text-based features for topic
segmentation of Mandarin Chinese. As a tone lan-
guage, Mandarin presents special challenges for ap-
plicability of intonation-based techniques, since the
pitch contour is also used to establish lexical iden-
tity. We demonstrate that intonational cues such as
reduction in pitch and intensity at topic boundaries
and increase in duration and pause still provide sig-
nificant contrasts in Mandarin Chinese. We build
a decision tree classifier that, based only on word
and local context prosodic information without ref-
erence to term similarity, cue phrase, or sentence-
level information, achieves boundary classification
accuracy of 84.6-95.6% on a balanced test set. We
contrast these results with classification using text-
based features, exploiting both text similarity and
n-gram cues, to achieve accuracies between 77-
95.6%, if silence features are used. Finally we in-
tegrate prosody, text, and silence features using a
voting strategy to combine decision tree classifiers
for each feature subset individually and all subsets
jointly. This voted decision tree classifier yields
an overall classification accuracy of 96.85%, with
2.8% miss and 3.15% false alarm rates on a repre-
sentative corpus sample, demonstrating synergistic
combination of prosodic and text features for topic
segmentation.
1 Introduction
Natural spoken discourse is composed of a sequence
of utterances, not independently generated or ran-
domly strung together, but rather organized accord-
ing to basic structural principles. This structure in
turn guides the interpretation of individual utter-
ances and the discourse as a whole. Formal writ-
ten discourse signals a hierarchical, tree-based dis-
course structure explicitly by the division of the text
into chapters, sections, paragraphs, and sentences.
This structure, in turn, identifies domains for in-
terpretation; many systems for anaphora resolution
rely on some notion of locality (Grosz and Sidner,
1986). Similarly, this structure represents topical
organization, and thus would be useful in informa-
tion retrieval to select documents where the primary
sections are on-topic, and, for summarization, to se-
lect information covering the different aspects of the
topic.
Unfortunately, spoken discourse does not include
the orthographic conventions that signal structural
organization in written discourse. Instead, one must
infer the hierarchical structure of spoken discourse
from other cues. Prior research (Nakatani et al,
1995; Swerts, 1997) has shown that human label-
ers can more sharply, consistently, and confidently
identify discourse structure in a word-level tran-
scription when an original audio recording is avail-
able than they can on the basis of the transcribed
text alone. This finding indicates that substantial
additional information about the structure of the
discourse is encoded in the acoustic-prosodic fea-
tures of the utterance. Given the often errorful tran-
scriptions available for large speech corpora, we
choose to focus here on fully exploiting the prosodic
cues to discourse structure present in the original
speech. We then compare the effectiveness of a pure
prosodic classification to text-based and mixed text
and prosodic based classification.
In the current set of experiments, we concentrate
on sequential segmentation of news broadcasts into
individual stories. While a richer hierarchical seg-
mentation is ultimately desirable, sequential story
segmentation provides a natural starting point. This
level of segmentation can also be most reliably per-
formed by human labelers and thus can be consid-
ered most robust, and segmented data sets are pub-
licly available.
Furthermore, we apply prosodic-based segmenta-
tion to Mandarin Chinese, in addition to textual fea-
tures. Not only is the use of prosodic cues to topic
segmentation much less well-studied in general than
is the use of text cues, but the use of prosodic cues
has been largely limited to English and other Euro-
pean languages.
2 Related Work
Most prior research on automatic topic segmenta-
tion has been applied to clean text only and thus
used textual features. Text-based segmentation ap-
proaches have utilized term-based similarity mea-
sures computed across candidate segments (Hearst,
1994) and also discourse markers to identify dis-
course structure (Marcu, 2000).
The Topic Detection and Tracking (TDT) eval-
uations focused on segmentation of both text and
speech sources. This framework introduced new
challenges in dealing with errorful automatic tran-
scriptions as well as new opportunities to exploit
cues in the original speech. The most successful
approach (Beeferman et al, 1999) produced auto-
matic segmentations that yielded retrieval results
approaching those with manual segmentations, us-
ing text and silence features. (Tur et al, 2001) ap-
plied both a prosody-only and a mixed text-prosody
model to segmentation of TDT English broadcast
news, with the best results combining text and
prosodic features. (Hirschberg and Nakatani, 1998)
also examined automatic topic segmentation based
on prosodic cues, in the domain of English broad-
cast news, while (Hirschberg et al, 2001) applied
similar cues to segmentation of voicemail.
Work in discourse analysis (Nakatani et al, 1995;
Swerts, 1997) in both English and Dutch has iden-
tified features such as changes in pitch range, in-
tensity, and speaking rate associated with seg-
ment boundaries and with boundaries of different
strengths. They also demonstrated that access to
acoustic cues improves the ease and quality of hu-
man labeling.
3 Prosody and Mandarin
In this paper we focus on topic segmentation in
Mandarin Chinese broadcast news. Mandarin Chi-
nese is a tone language in which lexical identity is
determined by a pitch contour - or tone - associ-
ated with each syllable. This additional use of pitch
raises the question of the cross-linguistic applicabil-
ity of the prosodic cues, especially pitch cues, iden-
tified for non-tone languages. Specifically, do we
find intonational cues in tone languages? The fact
that emphasis is marked intonationally by expansion
of pitch range even in the presence of Mandarin lex-
ical tone (Shen, 1989) suggests encouragingly that
prosodic, intonational cues to other aspects of in-
formation structure might also prove robust in tone
languages.
4 Data Set
We utilize the Topic Detection and Tracking (TDT)
3 (Wayne, 2000) collection Mandarin Chinese
broadcast news audio corpus as our data set. Story
segmentation in Mandarin and English broadcast
news and newswire text was one of the TDT tasks
and also an enabling technology for other retrieval
tasks. We use the segment boundaries provided with
the corpus as our gold standard labeling. Our col-
lection comprises 3014 news stories drawn from ap-
proximately 113 hours over three months (October-
December 1998) of news broadcasts from the Voice
of America (VOA) in Mandarin Chinese, with 800
regions of other program material including musi-
cal interludes and teasers. The transcriptions span
approximately 750,000 words. Stories average ap-
proximately 250 words in length to span a full story.
No subtopic segmentation is performed. The audio
is stored in NIST Sphere format sampled at 16KHz
with 16-bit linear encoding.
5 Prosodic Features
We consider four main classes of prosodic features
for our analysis and classification: pitch, intensity,
silence and duration. Pitch, as represented by f0 in
Hertz was computed by the ?To pitch? function of
the Praat system (Boersma, 2001). We selected the
highest ranked pitch candidate value in each voiced
region. We then applied a 5-point median filter to
smooth out local instabilities in the signal such as
vocal fry or small regions of spurious doubling or
halving. Analogously, we computed the intensity
in decibels for each 10ms frame with the Praat ?To
intensity? function, followed by similar smoothing.
For consistency and to allow comparability, we
compute all figures for word-based units, using
the automatic speech recognition transcriptions pro-
vided with the TDT Mandarin data. The words are
used to establish time spans for computing pitch
or intensity mean or maximum values, to enable
durational normalization and the pairwise compar-
isons reported below, and to identify silence or non-
speech duration.
It is well-established (Ross and Ostendorf, 1996)
that for robust analysis pitch and intensity should
be normalized by speaker, since, for example, aver-
age pitch is largely incomparable for male and fe-
male speakers. In the absence of speaker identifica-
tion software, we approximate speaker normaliza-
tion with story-based normalization, computed as
 
	

	 , assuming one speaker per topic1. For du-
ration, we consider both absolute and normalized
word duration, where average word duration is used
as the mean in the calculation above.
6 Prosodic Analysis
To evaluate the potential applicability of prosodic
features to story segmentation in Mandarin Chinese,
we performed some initial data analysis to deter-
mine if words in story-final position differed from
the same words used throughout the story in news
stories. This lexical match allows direct pairwise
comparison. We anticipated that since words in
Mandarin varied not only in phoneme sequence but
also in tone sequence, a direct comparison might be
particularly important to eliminate sources of vari-
ability. Features that differed significantly would
form the basis of our classifier feature set.
We found significant differences for each of the
features we considered. Specifically, word dura-
tion, normalized mean pitch, and normalized mean
intensity all differed significantly for words in topic-
final position relative to occurrences throughout the
story (paired t-test, two-tailed,    
Prosodic Cues to Discourse Segment Boundaries in Human-Computer
Dialogue
Gina-Anne Levow
University of Chicago
levow@cs.uchicago.edu
Abstract
Theories of discourse structure hypothesize a
hierarchical structure of discourse segments,
typically tree-structured. While substantial
work has been done on identifying and auto-
matically recognizing the textual and prosodic
correlates of discourse structure in mono-
logue, comparable cues for dialogue or multi-
party conversation, and in particular human-
computer dialogue remain relatively less stud-
ied. In this paper, we explore prosodic
cues to discourse segmentation in human-
computer dialogue. Using data drawn from
60 hours of interactions with a voice-only
conversational spoken language system, we
identify pitch and intensity features that sig-
nal segment boundaries. Specifically, based
on 473 pairs of segment-final and segment-
initiating utterances, we find significant in-
creases for segment-initial utterances in max-
imum pitch, average pitch, and average inten-
sity, while segment-final utterances show sig-
nificantly lower minimum pitch. These results
suggest that even in the artificial environment
of human-computer dialogue, prosodic cues ro-
bustly signal discourse segment structure, com-
parably to the contrastive uses of pitch and am-
plitude identified in natural monologues.
Keywords Dialogue Systems, Discourse structure,
Prosody in understanding
1 Introduction
Contemporary theories of discourse, both computational
and descriptive, postulate a tree-structured hierarchical
model of discourse. These structures may be viewed as
corresponding to?intentional? structure of discourse seg-
ment purposes in the view of (Grosz and Sidner, 1986),
to plan and subplan structure directly in the view of
(Allen and Litman, 1990) , to nuclei and satellite rhetori-
cal relations in the Rhetorical Structure Theory of (Mann
and Thompson, 1987), or to information structures as in
(Traum and Hinkelman, 1992). Despite this diversity of
views on the sources of structural organization, these the-
ories agree on the decomposition of discourse into seg-
ments and subsegments in a hierarchical structure.
Discourse segments help to establish the domain of in-
terpretation for referents or anaphors. (Grosz, 1977) Dis-
course segmentation can also provide guidance for sum-
marization or retrieval by identifying the topical structure
of extended text spans. As a result, an understanding of
the mechanisms that signal discourse structure is highly
desirable.
While substantial work has been done on identifying
and automatically recognizing the textual and prosodic
correlates of discourse structure in monologue, compa-
rable cues for dialogue or multi-party conversation, and
in particular human-computer dialogue remain relatively
less studied. In this paper, we explore prosodic cues to
discourse segmentation in human-computer dialogue.
Using data from 60 hours of interactions with a voice-
only conversational spoken language system, we identify
pitch and intensity features that signal segment bound-
aries. Specifically, based on 473 pairs of segment-final
and segment-initiating utterances, we find significant in-
creases for segment-initial utterances in maximum and
average pitch and average intensity, with significantly
lower minimum pitch for segment-final utterances. These
results suggest that even in the artificial environment of
human-computer dialogue, prosodic cues robustly sig-
nal discourse segment structure, comparably to the con-
trastive uses of pitch and amplitude identified in natural
monologues.
1.1 Overview
We begin with a discussion of related work on discourse
segmentation and dialogue act identification in mono-
logue and dialogue, primarily in the human-human case.
Then we introduce the system and data collection pro-
cess that produced the human-computer discourse seg-
ment change materials for the current analysis. We de-
scribe the acoustic analyses performed and the features
chosen for comparison. Then we identify the prosodic
cues that distinguish discourse segment boundaries and
discuss the relation to previously identified cues for other
discourse types. Finally we conclude and present some
future work.
2 Related Work
Cues for and automatic segmentation of discourse struc-
ture have been most extensively studied for written and
spoken monologue. For written narrative, discourse seg-
ment boundaries have been identified based on textual
topic similarity with a variety of approaches based on
Hearst?s Textiling(Hearst, 1994). More complex rheto-
rial structure theory trees have also been extracted based
heavily on cue phrases and discourse markers by (Marcu,
2000).
In spoken monologue, prosodic cues to discourse
structure and segmentation have been explored by
(Nakatani et al, 1995; Swerts, 1997). Increases in pitch
range, amplitude, and silence duration appear to signal
discourse segment boundaries across different domains
- voicemail, broadcast news, descriptive narrative - and
across different languages, such as English and Dutch.
Comparable prosodic cues have been applied to the re-
lated task of news story segmentation, in conjunction
with textual cues to topicality, by (Tur et al, 2001), where
large pitch differences between pre- and post- boundary
positions play the most significant role among prosodic
cues.
In spoken dialogue, research has focused on the identi-
fication of dialogue acts and dialogue games. Integration
of textual and prosodic cues, such as particular pitch ac-
cent or contour types, have been found useful for identi-
fying act type(Shriberg et al, 1998; Taylor et al, 1998).
Specific classes of dialogue act, such as corrections (re-
quest repair), have received particular interest in work
by (Levow, 1998; Swerts et al, 2000) in the context of
human-computer error resolution. Recent work on the
ICSI multi-party meeting recorder data has demonstrated
some very preliminary results on multi-party segmenta-
tion (Galley et al, 2003); prosodic information in this
case was limited to silence duration.
With the exception of work on error resolution, most
work on dialogue has focused human-human interaction
and on identification of particular act or game types. Here
we concentrate on the general question of discourse seg-
mentation in voice-only human-computer interaction. We
ask whether the cues to segment structure identified for
monologue are robust to the change in number and type
of conversational participant.
3 Data Collection
3.1 Speech System Description
The utterances used in the analysis of discourse seg-
mentation for human-computer interaction were drawn
from approximately 60 hours of user interactions in a
field trial of the Sun Microsystems SpeechActs system
(Yankelovich et al, 1995). SpeechActs provides a voice-
only interface to common desktop applications such as
e-mail, calendar, weather reports, stock quotes, and time
and currency conversions. The system allows conversa-
tional interactions with speech recognition provided by
BBN?s HARK recognizer and speech synthesis through
Centigram?s TruVoice system. A sample interaction ap-
pears below. 1
SpeechActs: You have 14 new messages and
47 old messages. The first new
message is from Eric Baatz
regarding ?report completed.?
User: Let me hear it.
SpeechActs: ?The first draft is ready for
your comments. Eric.?
User: Reply and include current message.
SpeechActs: Preparing message to Eric Baatz.
Begin recording after the tone.
When finished, pause several seconds.
User: Eric, I?ll get those to you
this afternoon.
SpeechActs: Say cancel, send, or review.
User: Send.
SpeechActs: Message sent. What now?
User: Next message.
SpeechActs: New message two from Stuart
Adams, subject ?Can we meet at
3:00 today??
User: Switch to calendar...
The field trial involved a group of nineteen subjects.
Four of the participants were members of the system de-
velopment staff, fourteen were volunteers drawn from
Sun Microsystems? staff, and a final class of subjects con-
sisted of one-time guest users. There were three female
and fifteen male regular user subjects.
All interactions with the system were recorded and dig-
itized in standard telephone audio quality format at 8kHz
sampling in 8-bit mu-law encoding during the conversa-
tion. In addition, speech recognition results, parser re-
sults, and synthesized responses were logged. A paid as-
sistant then produced a correct verbatim transcript of all
user utterances. Overall there were 7752 user utterances
recorded.
1Designing SpeechActs: Issues in Speech User Interface
Design (Yankelovich et al, 1995) p. 2
3.2 Data Coding and Extraction
Consistent discourse segmentation can be difficult even
for trained experts (Nakatani et al, 1995; Swerts, 1997;
Passoneau and Litman, 1997), and differences in depth
of nesting for discourse structure appear to be the most
problematic. As a result, we chose to examine utterances
whose segment and topic initiating status would be rela-
tively unambiguous. As the SpeechActs system consists
of 6 different applications, we chose to focus on changes
from application to application as reliable indicators of
topic initiation. These commands are either simply the
name of the desirable application, as in ?Mail? or ?Cal-
endar?, possibly with an optional politeness term, or a
switch command, such as ?Switch to? and the name of
the application. Approximately 1400 such utterances oc-
cured during the field trial data collection.
We performed an automatic forced alignment in order
to identify and extract the relevant utterances from the
digitized audio. Using the full sequence of synthesized
computer utterances and manually transcribed user utter-
ances, we applied the align function of the Sonic speech
recognizer provided as part of the University of Colorado
(CU) Communicator system to a 16-bit linear version of
the original audio recording. 473 utterances that were
correctly aligned by this automatic process were used for
the current analysis.
4 Acoustic Feature Extraction
Based on prior results for monologue, we selected pitch
and amplitude features for consideration. Although si-
lence duration is often a good cue to discourse segment
boundary position in narrative, we excluded it from con-
sideration in the current study due to the awkward pace of
the SpeechActs human-computer interactions. Users had
to wait for a tone to speak, and interturn silences were as
long as six seconds.
We used the ?To Pitch...? and ?To intensity? functions
in Praat(Boersma, 2001), a freely available acoustic-
phonetic analysis package, to automatically extract the
pitch (in Hertz) and amplitude (in decibels) for the in-
teraction. To smooth out local jitter and noise in the
pitch and amplitude contours, we applied a 5-point me-
dian filter. Finally, in order to provide overall compara-
bility across male and female subjects and across differ-
ent channel characteristics for different sessions2, we per-
formed per-speaker, per-session normalization of pitch
and amplitude values, computed as
 
	

	
. The re-
sulting pitch and amplitude values within the time re-
gions identified for each utterance by forced alignment
2Since the interface was accessed over a regular analog tele-
phone line from a wide variety of locations - including noisy
international airports, the recording quality and level varied
widely.
Figure 1: Significant differences in normalized pitch
and intensity. Light grey: Segment-initial; Dark grey:
segment-final
were used for subsequent analysis.
5 Prosodic Analysis
For both pitch and amplitude we computed summary
scalar measures for each utterance. Mean pitch and inten-
sity are intended to capture overall increases or decreases.
Maximum and minimum pitch and maximum amplitude
served to describe increases in range that might not af-
fect overall average. We compared the segment-initial
?application change? utterances with their immediately
preceding segment-final utterances.3 We find significant
increases in maximum pitch ( Assessing Prosodic and Text Features for Segmentation of Mandarin
Broadcast News
Gina-Anne Levow
University of Chicago
levow@cs.uchicago.edu
Abstract
Automatic topic segmentation, separation of
a discourse stream into its constituent sto-
ries or topics, is a necessary preprocessing
step for applications such as information re-
trieval, anaphora resolution, and summariza-
tion. While significant progress has been made
in this area for text sources and for English au-
dio sources, little work has been done in au-
tomatic segmentation of other languages us-
ing both text and acoustic information. In
this paper, we focus on exploiting both textual
and prosodic features for topic segmentation of
Mandarin Chinese. As a tone language, Man-
darin presents special challenges for applica-
bility of intonation-based techniques, since the
pitch contour is also used to establish lexical
identity. However, intonational cues such as re-
duction in pitch and intensity at topic bound-
aries and increase in duration and pause still
provide significant contrasts in Mandarin Chi-
nese. We first build a decision tree classi-
fier that based only on prosodic information
achieves boundary classification accuracy of
89-95.8% on a large standard test set. We
then contrast these results with a simple text
similarity-based classification scheme. Finally
we build a merged classifier, finding the best
effectiveness for systems integrating text and
prosodic cues.
1 Introduction
Natural spoken discourse is composed of a sequence
of utterances, not independently generated or randomly
strung together, but rather organized according to basic
structural principles. This structure in turn guides the in-
terpretation of individual utterances and the discourse as
a whole. Formal written discourse signals a hierarchical,
tree-based discourse structure explicitly by the division
of the text into chapters, sections, paragraphs, and sen-
tences. This structure, in turn, identifies domains for in-
terpretation; many systems for anaphora resolution rely
on some notion of locality (Grosz and Sidner, 1986).
Similarly, this structure represents topical organization,
and thus would be useful in information retrieval to se-
lect documents where the primary sections are on-topic,
and, for summarization, to select information covering
the different aspects of the topic.
Unfortunately, spoken discourse does not include the
orthographic conventions that signal structural organiza-
tion in written discourse. Instead, one must infer the hi-
erarchical structure of spoken discourse from other cues.
Prior research (Nakatani et al, 1995; Swerts, 1997) has
shown that human labelers can more sharply, consis-
tently, and confidently identify discourse structure in a
word-level transcription when an original audio record-
ing is available than they can on the basis of the tran-
scribed text alone. This finding indicates that substan-
tial additional information about the structure of the dis-
course is encoded in the acoustic-prosodic features of the
utterance. Given the often errorful transcriptions avail-
able for large speech corpora, we choose to focus here
on fully exploiting the prosodic cues to discourse struc-
ture present in the original speech in addition to possibly
noisy textual cues. We then compare the effectiveness of
a pure prosodic classification to text-based and mixed text
and prosodic based classification.
In the current set of experiments, we concentrate on se-
quential segmentation of news broadcasts into individual
stories. This level of segmentation can be most reliably
performed by human labelers and thus can be considered
most robust, and segmented data sets are publicly avail-
able.
Furthermore, we consider the relative effectiveness
prosodic-based, text-based, and mixed cue-based seg-
mentation for Mandarin Chinese, to assess the relative
utility of the cues for a tone language. Not only is the use
of prosodic cues to topic segmentation much less well-
studied in general than is the use of text cues, but the use
of prosodic cues has been largely limited to English and
other European languages.
2 Related Work
Most prior research on automatic topic segmentation has
been applied to clean text only and thus used textual fea-
tures. Text-based segmentation approaches have utilized
term-based similarity measures computed across candi-
date segments (Hearst, 1994) and also discourse markers
to identify discourse structure (Marcu, 2000).
The Topic Detection and Tracking (TDT) evaluations
focused on segmentation of both text and speech sources.
This framework introduced new challenges in dealing
with errorful automatic transcriptions as well as new op-
portunities to exploit cues in the original speech. The
most successful approach (Beeferman et al, 1999) pro-
duced automatic segmentations that yielded retrieval re-
sults comparable to those with manual segmentations, us-
ing text and silence features. (Tur et al, 2001) applied
both a prosody-only and a mixed text-prosody model
to segmentation of TDT English broadcast news, with
the best results combining text and prosodic features.
(Hirschberg and Nakatani, 1998) also examined auto-
matic topic segmentation based on prosodic cues, in the
domain of English broadcast news.
Work in discourse analysis (Nakatani et al, 1995;
Swerts, 1997) in both English and Dutch has identified
features such as changes in pitch range, intensity, and
speaking rate associated with segment boundaries and
with boundaries of different strengths.
3 Data Set
We utilize the Topic Detection and Tracking (TDT)
3 (Wayne, 2000) collection Mandarin Chinese broadcast
news audio corpus as our data set. Story segmentation in
Mandarin and English broadcast news and newswire text
was one of the TDT tasks and also an enabling technol-
ogy for other retrieval tasks. We use the segment bound-
aries provided with the corpus as our gold standard label-
ing. Our collection comprises 3014 stories drawn from
approximately 113 hours over three months (October-
December 1998) of news broadcasts from the Voice of
America (VOA) in Mandarin Chinese. The transcriptions
span approximately 740,000 words. The audio is stored
in NIST Sphere format sampled at 16KHz with 16-bit lin-
ear encoding.
4 Prosodic Features
We employ four main classes of prosodic features: pitch,
intensity, silence and duration. Pitch, as represented by f0
in Hertz, was computed by the ?To pitch? function of the
Praat system (Boersma, 2001). We then applied a 5-point
median filter to smooth out local instabilities in the signal
such as vocal fry or small regions of spurious doubling or
halving. Analogously, we computed the intensity in deci-
bels for each 10ms frame with the Praat ?To intensity?
function, followed by similar smoothing.
For consistency and to allow comparability, we com-
puted all figures for word-based units, using the ASR
transcriptions provided with the TDT Mandarin data. The
words are used to establish time spans for computing
pitch or intensity mean or maximum values, to enable
durational normalization and pairwise comparison, and
to identify silence duration.
It is well-established (Ross and Ostendorf, 1996) that
for robust analysis pitch and intensity should be nor-
malized by speaker, since, for example, average pitch
is largely incomparable for male and female speak-
ers. In the absence of speaker identification software,
we approximate speaker normalization with story-based
normalization, computed as  
	

	
, assuming one
speaker per topic1. For duration, we consider both abso-
lute and normalized word duration, where average word
duration is used as the mean in the calculation above.
Mandarin Chinese is a tone language in which lexi-
cal identity is determined by a pitch contour - or tone
- associated with each syllable. This additional use of
pitch raises the question of the cross-linguistic applicabil-
ity of the prosodic cues, especially pitch cues, identified
for non-tone languages. Specifically, do we find intona-
tional cues in tone languages?
We have found highly significant differences based
on paired t-test two-tailed, ( ffProceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108?117,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Third International Chinese Language Processing Bakeoff:
Word Segmentation and Named Entity Recognition
Gina-Anne Levow
University of Chicago
1100 E. 58th St.
Chicago, IL 60637 USA
levow@cs.uchicago.edu
Abstract
The Third International Chinese Language
Processing Bakeoff was held in Spring
2006 to assess the state of the art in two
important tasks: word segmentation and
named entity recognition. Twenty-nine
groups submitted result sets in the two
tasks across two tracks and a total of five
corpora. We found strong results in both
tasks as well as continuing challenges.
1 Introduction
Many important natural language processing tasks
ranging from part of speech tagging to parsing
to reference resolution and machine translation
assume the ready availability of a tokenization
into words. While such tokenization is relatively
straight-forward in languages which use whites-
pace to delimit words, Chinese presents a signif-
icant challenge since it is typically written with-
out such separation. Word segmentation has thus
long been the focus of significant research because
of its role as a necessary pre-processing phase for
the tasks above. However, word segmentation re-
mains a significant challenge both for the difficulty
of the task itself and because standards for seg-
mentation vary and human segmenters may often
disagree.
SIGHAN, the Special Interest Group for Chi-
nese Language Processing of the Association
for Computational Linguistics, conducted two
prior word segmentation bakeoffs, in 2003 and
2005(Emerson, 2005), which established bench-
marks for word segmentation against which other
systems are judged. The bakeoff presentations at
SIGHAN workshops highlighted new approaches
in the field as well as the crucial importance of
handling out-of-vocabulary (OOV) words.
A significant class of OOV words is Named En-
tities, such as person, location, and organization
names. These terms are frequently poorly covered
in lexical resources and change over time as new
individuals, institutions, or products appear. These
terms also play a particularly crucial role in infor-
mation retrieval, reference resolution, and ques-
tion answering. As a result of this importance, and
interest in expanding the scope of the bakeoff ex-
pressed at the Fourth SIGHAN Workshop, in the
Winter of 2005 it was decided to hold a new bake-
off to evaluate both continued progress in Word
Segmentation (WS) and the state of the art in Chi-
nese Named Entity Recognition (NER).
2 Details of the Evaluation
2.1 Corpora
Five corpora were provided for the evaluation:
three in Simplified characters and two in tradi-
tional characters. The Simplified character cor-
pora were provided by Microsoft Research Asia
(MSRA) for WS and NER, by University of Penn-
sylvania/University of Colorado (UPUC) for WS,
and by the Linguistic Data Consortium (LDC) for
NER. The Traditional character corpora were pro-
vided by City University of Hong Kong (CITYU)
for WS and NER and by the Chinese Knowl-
edge Information Processing Laboratory (CKIP)
of the Academia Sinica, Taiwan for WS. Each data
provider offered separate training and test corpora.
General information for each corpus appears in
Table 1.
All data providers were requested to supply
the training and test corpora in both the stan-
dard local encoding and in Unicode (UTF-8) in
a standard XML format with sentence and word
tags, and named entity tags if appropriate. For
108
Source Encodings Training (Wds/Types) Test (Wds/Types)
CITYU BIG5HKSCS/Unicode 1.6M/76K 220K/23K
CKIP BIG5/Unicode 5.5M/146K 91K/15K
LDC Unicode 632K (est. wds) 61K (est. wds)
MSRA GB18030/Unicode 1.3M/63K 100K/13K
UPUC GB/Unicode 509K/37K 155K/17K
Table 1: Overall corpus statistics
all providers except the LDC, missing encodings
were transcoded by the organizers using the ap-
propriate Python CJK codecs.
Primary training and truth data for word seg-
mentation were generated by the organizers via a
Python script by replacing sentence end tags with
newlines and word end tags with a single whites-
pace character, deleting all other tags and associ-
ated newlines. For test data, end of sentence tags
were replaced with newlines and all other tags re-
moved. Since the UPUC truth corpus was only
provided in white-space separated form, test data
was created by automatically deleting line-internal
whitespace.
Primary training and truth data for named
entity recognition were converted from the
provided XML format to a two-column format
similar to that used in the CoNLL 2002 NER
task(Sang, 2002) adapted for Chinese, where
the first column is the current character and
the second column the corresponding tag. For-
mat details may be found at the bakeoff website
(http://www.sighan.org/bakeoff2006/).
For consistency, we tagged only ?<NAMEX>?
mentions, of either (PER)SON, (LOC)ATION,
(ORG)ANIZATION, or (G)EO-(P)OLITICAL
(E)NTITY as annotated in the corpora.1 Test was
generated as above.
The LDC required sites to download training
data from their website directly in the ACE2 eval-
uation format, restricted to ?NAM? mentions. The
organizers provided the sites with a Python script
to convert the LDC data to the CoNLL format
above, and the same script was used to create the
truth data. Test data was created by splitting on
newlines or Chinese period characters.
Comparable XML format data was also pro-
vided for all corpora and both tasks.
The segmentation and NER annotation stan-
dard, as appropriate, for each corpus was made
1Only the LDC provided GPE tags.
2http://www.ldc.upenn.edu/projects/ACE
available on the bakeoff website. As observed
in previous evaluations, these documents varied
widely in length, detail, and presentation lan-
guage.
Except as noted above, no additional changes
were made to the data furnished by the providers.
2.2 Rules and Procedures
The Third Bakeoff followed the structure of the
first two word segmentation bakeoffs. Participat-
ing groups (?sites?) registered by email form; only
the primary contact was required to register, iden-
tifying the corpora and tasks of interest. Train-
ing data was released for download from the web-
sites (both SIGHAN and LDC) on April 17, 2006.
Test data was released on May 15, 2006 and re-
sults were due 14:00 GMT on May 17. Scores for
all submitted runs were emailed to the individual
groups by May 19, and were made available to all
groups on a web page a few days later.
Groups could participate in either or both of two
tracks for each task and corpus:
? In the open track, participants could use any
external data they chose in addition to the
provided training data. Such data could in-
clude external lexica, name lists, gazetteers,
part-of-speech taggers, etc. Groups were re-
quired to specify this information in their sys-
tem descriptions.
? In the closed track, participants could only
use information found in the provided train-
ing data. Information such as externally ob-
tained word counts, part of speech informa-
tion, or name lists was excluded.
Groups were required to submit fully automatic
runs and were prohibited from testing on corpora
which they had previously used.
Scoring was performed automatically using a
combination of Python and Perl scripts, facilitated
by stringent file naming conventions. In cases
109
where naming errors or minor divergences from
required file formats arose, a mix of manual inter-
vention and automatic conversion was employed
to enable scoring. The primary scoring scripts
were made available to participants for followup
experiments.
3 Participating Sites
A total of 36 sites registered, and 29 submitted re-
sults for scoring. The greatest number of partici-
pants came from the People?s Republic of China
(11), followed by Taiwan (7), the United States
(5), Japan (2), with one team each from Singapore,
Korea, Hong Kong, and Canada. A summary of
participating groups with task and track informa-
tion appears in Table 2. A total of 144 official runs
were scored: 101 for word segmentation and 43
for named entity recognition.
4 Results & Discussion
We report results below first for word segmenta-
tion and second for named entity recognition.
4.1 Word Segmentation Results
To provide a basis for comparison, we computed
baseline and possible topline scores for each of
the corpora. The baseline was constructed by left-
to-right maximal match implemented by Python
script, using the training corpus vocabulary. The
topline employed the same procedure, but instead
used the test vocabulary. These results are shown
in Tables 3 and 4.
For the WS task, we computed the following
measures using the score(Sproat and Emerson,
2003) program developed for the previous bake-
offs: recall (R), precision (P), equally weighted
F-measure (F = 2PR(P+R) ), the rate of out-of-
vocabulary words (OOV rate) in the test cor-
pus, the recall on OOV (Roov), and recall on in-
vocabulary words (Riv). In and out of vocabu-
lary status are defined relative to the training cor-
pus. Following previous bakeoffs, we employ the
Central Limit Theorem for Bernoulli trials (Grin-
stead and Snell, 1997) to compute 95% confidence
interval as ?2
?
(p(1?p)n ), assuming the binomial
distribution is appropriate. For recall, Cr, we as-
sume that recall represents the probability of cor-
rect word identification. Symmetrically, for pre-
cision, we compute Cp, setting p to the precision
value. One can determine if two systems may then
be viewed as significantly different at a 95% con-
fidence level by computing whether their confi-
dence intervals overlap.
Word segmentation results for all runs grouped
by corpus and track appear in Tables 5-12; all ta-
bles are sorted by F-score.
4.2 Word Segmentation Discussion
Across all corpora, the best F-score was achieved
in the MSRA Open Track at 0.979. Overall, as
would be expected, the best results on Open track
runs had higher F-scores than the best results for
Closed Track runs on the same corpora. Likewise,
the OOV recall rates for the best Open Track sys-
tems exceed those of the best Closed Track runs
on comparable corpora by exploiting outside in-
formation. Unfortunately, few sites submitted runs
in both conditions making strong direct compar-
isons difficult.
Many systems strongly outperformed the base-
line runs, though none achieved the topline. The
closest approach to the topline score was on the
CITYU corpus, with the best performing runs
achieving 99% of the topline F-score.
It is also informative to observe the rather wide
variation in scores across the test corpora. The
maximum scores were achieved on the MSRA cor-
pus closely followed by the CITYU corpus. The
best score achieved on the UPUC Open track con-
dition, however, was lower than all scores but one
on the MSRA Open track. However, a comparison
of the baseline, topline, and especially the OOV
rates may shed some light on this disparity. The
UPUC training corpus was only about one-third
the size of the MSRA corpus, and the OOV rate
for UPUC was more than double that of any of
the other corpora, yielding a challenging task, es-
pecially in the Closed track. This high OOV rate
may also be attributed to a change in register, since
the training data for UPUC had been drawn ex-
clusively from the Chinese Treebank and the test
data also included data from other newswire and
broadcast news sources. In contrast, the MSRA
corpus had both the highest baseline and highest
topline scores, possibly indicating an easier cor-
pus in some sense. The differences in topline also
suggest a greater degree of variance in the UPUC,
and in fact all other corpora, relative the MSRA
corpus. These differences highlight the continuing
challenges of handling out-of-vocabulary words
and performing segmentation across different reg-
110
Site Name Site Country CITYU CKIP MSRA UPUC CITYU LDC MSRA
ID WS WS WS WS NER NER NER
Natural Language Processing Lab,
Northeastern University of China 1 ZH C C C C
Language Technologies Institute,
Carnegie Mellon University 2 US O O O O
National Institute of Information
and Communications Technology, Japan 3 JP C C C
Basis Technology Corp. 4 US C C C C
Pattern Recognition and Intelligent
System Laboratory,Beijing University
of Posts and Telecommunications 5 ZH C C
HKUST, Human Language
Technology Center 6 HK O O O
The University of Tokyo 7 JP O O O O
Institute of Software,
Chinese Academy of Sciences 8 ZH C C C C C C OC
Alias-i, Inc. 9 US C C C C C C
Beijing University of Posts
and Telecommunications 10 ZH O O O
France Telecom R&D Beijing 11 ZH C OC O
NETEASE Information
Technology (Beijing) Co., Ltd. 12 ZH O O
AI Lab., Dept of Information
Management, Huafan University,
Taiwan. 13 TW OC OC
Nanjing University, China 14 ZH O OC
Intelligent Agent Systems Lab (IASL),
Academia Sinica 15 TW C C C
Simon Fraser University 16 CA C C C
Tung Nan Institute of Technology 18 TW C
Institute of Information Science,
Taiwan 19 TW C C
Microsoft Research Asia 20 ZH OC OC OC
Yahoo! 21 US C C
CKIP, Academia Sinica, Taiwan 22 TW O
Kookmin University 23 KO C C C C
Shenyang Institute of
Aeronautical Engineering 24 ZH OC OC
Institute for Infocomm Research,
Singapore 26 SG C C C C C C
National Taiwan University 29 TW C
ITNLP, Harbin Institute of
Technology, China 30 ZH OC O
National Central University
at Taiwan 31 TW C C
National Laboratory on Machine
Perception, Peking University, China 32 ZH OC OC OC OC O
University of Texas at Austin 34 US O O O O
Table 2: Participating Sites by Corpus, Task, and Track
Source Recall Precision F-measure OOV Rate Roov Riv
CITYU 0.930 0.882 0.906 0.040 0.009 0.969
CKIP 0.915 0.870 0.892 0.042 0.030 0.954
MSRA 0.949 0.900 0.924 0.034 0.022 0.981
UPUC 0.869 0.790 0.828 0.088 0.011 0.951
Table 3: Baselines: WS: Maximum match with training vocabulary
111
Source Recall Precision F-measure OOV Rate Roov Riv
CITYU 0.982 0.985 0.984 0.040 0.993 0.981
CKIP 0.980 0.987 0.983 0.042 0.997 0.979
MSRA 0.991 0.993 0.992 0.034 0.999 0.991
UPUC 0.961 0.976 0.968 0.088 0.989 0.958
Table 4: Toplines: WS: Maximum match with testing vocabulary
Site RunID R Cr P Cp F Roov Riv
15 d 0.973 ?0.000691 0.972 ?0.000703 0.972 0.787 0.981
15 b 0.973 ?0.000691 0.972 ?0.000703 0.972 0.787 0.981
20 0.972 ?0.000703 0.971 ?0.000715 0.971 0.792 0.979
32 0.969 ?0.000739 0.970 ?0.000727 0.970 0.773 0.978
1 a 0.971 ?0.000715 0.965 ?0.000783 0.968 0.719 0.981
15 c 0.965 ?0.000783 0.967 ?0.000761 0.966 0.792 0.972
15 a 0.966 ?0.000772 0.967 ?0.000761 0.966 0.786 0.973
26 0.968 ?0.000750 0.961 ?0.000825 0.965 0.633 0.983
11 0.962 ?0.000815 0.962 ?0.000815 0.962 0.722 0.972
16 0.963 ?0.000805 0.958 ?0.000855 0.961 0.689 0.974
9 0.966 ?0.000772 0.957 ?0.000865 0.961 0.555 0.983
1 b 0.958 ?0.000855 0.963 ?0.000805 0.960 0.714 0.968
8 0.952 ?0.000911 0.954 ?0.000893 0.953 0.747 0.960
23 0.950 ?0.000929 0.949 ?0.000938 0.949 0.638 0.963
4 b 0.845 ?0.001543 0.844 ?0.001547 0.844 0.632 0.854
4 a 0.841 ?0.001559 0.844 ?0.001547 0.843 0.506 0.855
13 1 0.589 ?0.002097 0.589 ?0.002097 0.589 0.022 0.613
Table 5: CITYU: Word Segmentation: Closed Track
Site RunID R Cr P Cp F Roov Riv
20 0.978 ?0.000625 0.977 ?0.000639 0.977 0.840 0.984
32 0.979 ?0.000611 0.976 ?0.000652 0.977 0.813 0.985
34 0.971 ?0.000715 0.967 ?0.000761 0.969 0.795 0.978
22 0.970 ?0.000727 0.965 ?0.000783 0.967 0.761 0.979
2 0.964 ?0.000794 0.964 ?0.000794 0.964 0.787 0.971
13 2 0.544 ?0.002123 0.549 ?0.002121 0.547 0.194 0.559
13 3 0.524 ?0.002129 0.503 ?0.002131 0.513 0.195 0.538
13 1 0.497 ?0.002131 0.467 ?0.002127 0.481 0.057 0.516
Table 6: CITYU: Word Segmentation: Open Track
Site RunID R Cr P Cp F Roov Riv
20 0.961 ?0.001280 0.955 ?0.001371 0.958 0.702 0.972
15 a 0.961 ?0.001280 0.953 ?0.001400 0.957 0.658 0.974
15 b 0.961 ?0.001280 0.952 ?0.001414 0.957 0.656 0.974
32 0.958 ?0.001327 0.948 ?0.001468 0.953 0.646 0.972
26 0.958 ?0.001327 0.941 ?0.001558 0.949 0.554 0.976
1 b 0.947 ?0.001482 0.943 ?0.001533 0.945 0.601 0.962
1 a 0.949 ?0.001455 0.940 ?0.001571 0.944 0.694 0.960
9 0.951 ?0.001428 0.935 ?0.001630 0.943 0.389 0.976
23 0.937 ?0.001607 0.933 ?0.001654 0.935 0.547 0.954
8 0.939 ?0.001583 0.929 ?0.001699 0.934 0.606 0.954
4 a 0.836 ?0.002449 0.834 ?0.002461 0.835 0.521 0.849
4 b 0.836 ?0.002449 0.828 ?0.002496 0.832 0.590 0.847
13 1 0.747 ?0.002875 0.677 ?0.003093 0.710 0.036 0.778
Table 7: CKIP: Word Segmentation: Closed Track
112
Site RunID R Cr P Cp F Roov Riv
20 0.964 ?0.001232 0.955 ?0.001371 0.959 0.704 0.975
34 0.959 ?0.001311 0.949 ?0.001455 0.954 0.672 0.972
32 0.958 ?0.001327 0.948 ?0.001468 0.953 0.647 0.972
2 a 0.953 ?0.001400 0.946 ?0.001495 0.949 0.679 0.965
2 b 0.951 ?0.001428 0.944 ?0.001521 0.948 0.676 0.964
13 2 0.724 ?0.002956 0.668 ?0.003115 0.695 0.161 0.749
13 3 0.736 ?0.002915 0.653 ?0.003148 0.692 0.160 0.761
13 1 0.654 ?0.003146 0.573 ?0.003271 0.611 0.057 0.680
Table 8: CKIP: Word Segmentation: Open Track
Site RunID R Cr P Cp F Roov Riv
32 0.964 ?0.001176 0.961 ?0.001222 0.963 0.612 0.976
26 0.961 ?0.001222 0.953 ?0.001336 0.957 0.499 0.977
9 0.959 ?0.001252 0.955 ?0.001309 0.957 0.494 0.975
1 a 0.955 ?0.001309 0.956 ?0.001295 0.956 0.650 0.966
15 d 0.953 ?0.001336 0.956 ?0.001295 0.955 0.574 0.966
11 a 0.955 ?0.001309 0.953 ?0.001336 0.954 0.575 0.969
15 b 0.952 ?0.001350 0.956 ?0.001295 0.954 0.575 0.966
15 c 0.949 ?0.001389 0.957 ?0.001281 0.953 0.673 0.959
15 a 0.949 ?0.001389 0.958 ?0.001266 0.953 0.672 0.959
16 0.952 ?0.001350 0.954 ?0.001323 0.953 0.604 0.964
11 b 0.950 ?0.001376 0.954 ?0.001323 0.952 0.602 0.962
5 0.956 ?0.001295 0.947 ?0.001414 0.951 0.493 0.972
1 b 0.946 ?0.001427 0.952 ?0.001350 0.949 0.568 0.959
18 c 0.950 ?0.001376 0.930 ?0.001611 0.940 0.272 0.974
30 a 0.963 ?0.001192 0.918 ?0.001732 0.940 0.175 0.991
18 b 0.954 ?0.001323 0.921 ?0.001703 0.937 0.163 0.981
8 0.933 ?0.001578 0.942 ?0.001476 0.937 0.640 0.943
23 0.933 ?0.001578 0.939 ?0.001511 0.936 0.526 0.948
24 0.923 ?0.001683 0.929 ?0.001621 0.926 0.554 0.936
18 a 0.949 ?0.001389 0.897 ?0.001919 0.922 0.022 0.982
4 a 0.830 ?0.002371 0.832 ?0.002360 0.831 0.473 0.842
4 b 0.817 ?0.002441 0.821 ?0.002420 0.819 0.491 0.829
Table 9: MSRA: Word Segmentation: Closed Track
Site RunID R Cr P Cp F Roov Riv
11 a 0.980 ?0.000884 0.978 ?0.000926 0.979 0.839 0.985
11 b 0.977 ?0.000946 0.976 ?0.000966 0.977 0.840 0.982
14 0.975 ?0.000986 0.976 ?0.000966 0.975 0.811 0.981
32 0.977 ?0.000946 0.971 ?0.001059 0.974 0.675 0.988
10 0.970 ?0.001077 0.970 ?0.001077 0.970 0.804 0.976
30 a 0.977 ?0.000946 0.960 ?0.001237 0.968 0.624 0.989
34 0.959 ?0.001252 0.961 ?0.001222 0.960 0.711 0.968
2 0.949 ?0.001389 0.954 ?0.001323 0.952 0.692 0.958
7 0.953 ?0.001336 0.940 ?0.001499 0.947 0.503 0.969
24 0.938 ?0.001522 0.946 ?0.001427 0.942 0.706 0.946
Table 10: MSRA: Word Segmentation: Open Track
113
Site RunID R Cr P Cp F Roov Riv
20 0.940 ?0.001207 0.926 ?0.001330 0.933 0.707 0.963
32 0.936 ?0.001244 0.923 ?0.001355 0.930 0.683 0.961
1 a 0.940 ?0.001207 0.914 ?0.001425 0.927 0.634 0.969
26 a 0.936 ?0.001244 0.917 ?0.001402 0.926 0.617 0.966
26 b 0.932 ?0.001279 0.910 ?0.001454 0.921 0.577 0.966
16 0.929 ?0.001305 0.909 ?0.001462 0.919 0.628 0.958
5 0.932 ?0.001279 0.904 ?0.001497 0.918 0.546 0.969
1 b 0.922 ?0.001363 0.914 ?0.001425 0.918 0.637 0.949
8 0.922 ?0.001363 0.912 ?0.001440 0.917 0.680 0.945
31 1 0.917 ?0.001402 0.904 ?0.001497 0.910 0.676 0.940
9 0.919 ?0.001387 0.895 ?0.001558 0.907 0.459 0.964
23 0.915 ?0.001417 0.896 ?0.001551 0.905 0.565 0.949
24 0.902 ?0.001511 0.887 ?0.001609 0.895 0.568 0.934
4 a 0.831 ?0.001905 0.819 ?0.001957 0.825 0.487 0.864
4 b 0.809 ?0.001998 0.827 ?0.001922 0.818 0.637 0.825
Table 11: UPUC: Word Segmentation: Closed Track
Site RunID R Cr P Cp F Roov Riv
34 0.949 ?0.001118 0.939 ?0.001216 0.944 0.768 0.966
2 0.942 ?0.001188 0.928 ?0.001314 0.935 0.711 0.964
20 0.940 ?0.001207 0.927 ?0.001322 0.933 0.741 0.959
7 0.944 ?0.001169 0.922 ?0.001363 0.933 0.680 0.970
12 0.933 ?0.001271 0.916 ?0.001410 0.924 0.656 0.959
32 0.940 ?0.001207 0.907 ?0.001476 0.923 0.561 0.976
24 0.928 ?0.001314 0.906 ?0.001483 0.917 0.660 0.954
10 0.925 ?0.001339 0.897 ?0.001545 0.911 0.593 0.957
Table 12: UPUC: Word Segmentation: Open Track
isters and writing styles.
4.3 Named Entity Results
We employed a slightly modified version of the
CoNLL 2002 scoring script to evaluate NER task
submissions. For each submission, we compute
overall phrase precision (P), recall(R), and bal-
anced F-measure (F), as well as F-measure for
each entity type (PER-F,ORG-F,LOC-F,GPE-F).
For each corpus, we compute a baseline per-
formance level as follows. Based on the training
data, using a left-to-right pass over the test data,
we assign a named entity tag to a span of charac-
ters if it was tagged with a single unique NE tag
(PER/LOC/ORG/GPE) in the training data.3 All
In the case of overlapping spans, we tag the max-
imal span. These scores for all NER corpora are
found in Table 13.
4.4 Named Entity Discussion
Though fewer sites participated in the NER task,
performances overall were very strong, with only
3If the span was a single character and appeared UN-
tagged in the corpus, we exclude it. Longer spans are re-
tained for tagging even if they might appear both tagged and
untagged in the training corpus.
two runs performing below baseline. The best F-
score overall on the MSRA Open Track reached
0.912, with ten other scores for MSRA and
CITYU Open Track above 0.85. Only two sites
submitted runs in both Open and Closed Track
conditions, and few Open Track runs were sub-
mitted at all, again limiting comparability. For
the only corpus with substantial numbers of both
Open and Closed Track runs, MSRA, the top three
runs outperformed all Closed Track runs.
System scores and baselines were much higher
for the CITYU and MSRA corpora than for the
LDC corpus. This disparity can, in part, also be at-
tributed to a substantially smaller training corpus
for the LDC than the other two collections. The
presence of an additional category, Geo-political
entity, which is potentially confused for either
location or organization also enhances the diffi-
culty of this corpus. Training requirements, vari-
ation across corpora, and most extensive tag sets
will continue to raise challenges for named entity
recognition.
Named entity recognition results for all runs
grouped by corpus and track appear in Tables 14-
19; all tables are sorted by F-score.
4This result indicates a rescoring of the run below with all
114
Source P R F PER-F ORG-F LOC-F GPE-F
CITY 0.611 0.467 0.529 0.587 0.516 0.503 N/A
LDC 0.493 0.378 0.428 0.395 0.29 0.259 0.539
MSRA 0.59 0.488 0.534 0.614 0.469 0.531 N/A
Table 13: Baselines: NER: Maximal match with unique tag in training corpus
Site RunID P R F ORG-F LOC-F PER-F
3 0.9143 0.8676 0.8903 0.8046 0.9211 0.9087
19 ccrf 0.9201 0.8545 0.8861 0.8054 0.9251 0.8872
21 a 0.9266 0.8475 0.8853 0.7973 0.9232 0.8937
21 b 0.9242 0.8491 0.8850 0.7976 0.9236 0.8920
19 avdic 0.9079 0.8626 0.8847 0.7984 0.9233 0.8914
8 0.9276 0.8181 0.8694 0.7707 0.9114 0.8769
21 f 0.9188 0.8231 0.8683 0.7852 0.9105 0.8652
21 g 0.9164 0.8246 0.8681 0.7842 0.9114 0.8636
9 0.8690 0.8417 0.8551 0.7541 0.8861 0.8845
19 bme 0.8742 0.8307 0.8519 0.7667 0.9015 0.8395
26 0.8466 0.8061 0.8259 0.7467 0.8863 0.7927
31 1 0.9035 0.6973 0.7871 0.7703 0.8905 0.5974
29 0.7703 0.6447 0.7019 0.4948 0.7613 0.7531
Table 14: CITYU: Named Entity Recognition: Closed Track
Site RunID P R F ORG-F LOC-F PER-F
6 0.8692 0.7498 0.8051 0.6801 0.8604 0.8098
Table 15: CITYU: Named Entity Recognition: Open Track
Site RunID P R F ORG-F LOC-F PER-F GPE-F
3 0.8026 0.7265 0.7627 0.6585 0.3046 0.7884 0.8204
8 0.8143 0.5953 0.6878 0.5855 0.1705 0.6571 0.7727
Table 16: LDC: Named Entity Recognition: Closed Track
Site RunID P R F ORG-F LOC-F PER-F GPE-F
7 0.7616 0.6621 0.7084 0.5209 0.2857 0.7422 0.7930
6 GPE-LOC4 0.6720 0.6554 0.6636 0.4553 0.7078 0.7420
6 0.3058 0.2982 0.3019 0.4553 0.0370 0.7420 0.0
Table 17: LDC: Named Entity Recognition: Open Track
Site RunID P R F ORG-F LOC-F PER-F
14 0.8894 0.8420 0.8651 0.8310 0.8545 0.9009
21 a 0.9122 0.8171 0.8620 0.8196 0.9053 0.8257
21 b 0.8843 0.8288 0.8556 0.7698 0.9013 0.8495
3 0.8814 0.8234 0.8514 0.8150 0.9062 0.7938
21 f 0.8845 0.7931 0.8363 0.8071 0.9003 0.7568
21 g 0.8661 0.8032 0.8335 0.7742 0.8991 0.7753
19 avdic 0.8637 0.7767 0.8179 0.8138 0.8233 0.8126
19 dcrf 0.8623 0.7740 0.8158 0.8141 0.8207 0.8093
9 0.8188 0.8097 0.8142 0.7295 0.8529 0.8196
19 cnoword 0.8670 0.7554 0.8074 0.8100 0.8257 0.7764
19 bvoting 0.8583 0.7606 0.8065 0.8145 0.8133 0.7899
26 0.8105 0.7882 0.7992 0.7491 0.8385 0.7699
21 r 0.8748 0.7168 0.7880 0.7288 0.8604 0.7107
8 0.8164 0.3124 0.4519 0.4591 0.5084 0.3521
Table 18: MSRA: Named Entity Recognition: Closed Track
115
Site RunID P R F ORG-F LOC-F PER-F
10 0.9220 0.9018 0.9118 0.8590 0.9034 0.9604
14 0.9076 0.8922 0.8999 0.8397 0.9099 0.9261
11 b 0.8767 0.8753 0.8760 0.7611 0.8976 0.9225
11 a 0.8645 0.8399 0.8520 0.6945 0.8745 0.9199
32 0.8397 0.8184 0.8289 0.7261 0.8804 0.8207
7 0.8468 0.7822 0.8132 0.6958 0.8552 0.8280
6 0.8195 0.6926 0.7507 0.6443 0.8291 0.6955
30 a 0.8697 0.6556 0.7476 0.5841 0.7029 0.8987
8 0.8320 0.6703 0.7424 0.5651 0.8000 0.7565
12 b 0.7083 0.5464 0.6169 0.4168 0.6154 0.7171
12 a 0.7395 0.5186 0.6096 0.4168 0.6154 0.7074
Table 19: MSRA: Named Entity Recognition: Open Track
5 Conclusions & Future Directions
The Third SIGHAN Chinese Language Process-
ing Bakeoff successfully brought together a col-
lection of 29 strong research groups to assess the
progress of research in two important tasks, word
segmentation and named entity recognition, that in
turn enable other important language processing
technologies. The individual group presentations
at the SIGHAN workshop detail the approaches
that yielded strong performance for both tasks. Is-
sues of out-of-vocabulary word handling, anno-
tation consistency, character encoding and code
mixing of Chinese and non-Chinese text all con-
tinue to challenge system designers and bakeoff
organizers alike.
In future analyses, we hope to develop addi-
tional analysis tools to better assess progress in
these fundamental tasks, in a more corpus inde-
pendent fashion. Microsoft Research Asia has
been pursuing work along these lines focusing on
improvements in F-score and OOV F-score rela-
tive to more intrinsic corpus measures, such as
baselines and toplines.5 Such developments will
guide the planning of future evaluations.
Finally, while word segmentation and named
entity recognition are important in themselves, it
is also important to assess the impact of improve-
ments in these enabling technologies on broader
downstream applications. More tightly coupled
experiments that involve joint word segmentation
and named entity recognition could provide in-
sight. Integration of WS and NER with a higher
level task such as parsing, reference resolution, or
machine translation could allow the development
of more refined, task-oriented metrics to evalu-
GPE tags in the truth data mapped to LOC, since no GPE tags
were present in the results.
5Personal communication, Mu Li, Microsoft Research
Asia.
ate WS and NER and focus attention on improve-
ments to the fundamental techniques which en-
hance performance on higher level tasks.
Acknowledgements
We gratefully acknowledge the generous assis-
tance of the organizations and individuals listed
below who provided the data for this bakeoff;
without their support, it could not have taken
place:
? Chinese Knowledge Information Processing
Group, Academia Sinica, Taiwan: Keh-Jiann
Chen, Henning Chiu
? City University of Hong Kong: Benjamin K.
Tsou, Olivia Oi Yee Kwong
? Linguistic Data Consortium: Stephanie
Strassel
? Microsoft Research Asia: Mu Li
? University of Pennsylvania/University of
Colorado, USA: Martha Palmer, Nianwen
Xue
We also thank Hwee Tou Ng and Olivia Oi Yee
Kwong, the co-organizers of the fifth SIGHAN
workshop, in conjunction with which this bakeoff
takes place. Olivia Kwong merits special thanks
both for her help in co-organizing this bakeoff and
in coordinating publications. Finally, we thank all
the participating sites who enabled the success of
this bakeoff.
References
Thomas Emerson. 2005. The Second International
Chinese Word Segmentation Bakeoff. In Proceed-
ings of the Fourth SIGHAN Workshop on Chinese
Language Processing, Jeju Island, Republic of Ko-
rea.
116
Charles M. Grinstead and J. Laurie Snell. 1997. In-
troduction to Probability. American Mathematical
Society, Providence, RI.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In Proceedings of the
6th Conference on Natural Language Learning 2002
(CoNLL-2002).
Richard Sproat and Thomas Emerson. 2003. The First
International Chinese Word Segmentation Bakeoff.
In Proceedings of the Second SIGHAN Workshop on
Chinese Language Processing, Sapporo, Japan.
117
Workshop on TextGraphs, at HLT-NAACL 2006, pages 61?64,
New York City, June 2006. c?2006 Association for Computational Linguistics
Graph-based Generalized Latent Semantic Analysis
for Document Representation
Irina Matveeva
Dept. of Computer Science
University of Chicago
Chicago, IL 60637
matveeva@cs.uchicago.edu
Gina-Anne Levow
Dept. of Computer Science
University of Chicago
Chicago, IL 60637
levow@cs.uchicago.edu
Abstract
Document indexing and representation of
term-document relations are very impor-
tant for document clustering and retrieval.
In this paper, we combine a graph-based
dimensionality reduction method with a
corpus-based association measure within
the Generalized Latent Semantic Analysis
framework. We evaluate the graph-based
GLSA on the document clustering task.
1 Introduction
Document indexing and representation of term-
document relations are very important issues for
document clustering and retrieval. Although the
vocabulary space is very large, content bearing
words are often combined into semantic classes that
contain synonyms and semantically related words.
Hence there has been a considerable interest in low-
dimensional term and document representations.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. The dimensions of the LSA
vector space can be interpreted as latent semantic
concepts. The cosine similarity between the LSA
document vectors corresponds to documents? sim-
ilarity in the input space. LSA preserves the docu-
ments similarities which are based on the inner prod-
ucts of the input bag-of-word documents and it pre-
serves these similarities globally.
More recently, a number of graph-based dimen-
sionality reduction techniques were successfully ap-
plied to document clustering and retrieval (Belkin
and Niyogi, 2003; He et al, 2004). The main ad-
vantage of the graph-based approaches over LSA is
the notion of locality. Laplacian Eigenmaps Embed-
ding (Belkin and Niyogi, 2003) and Locality Pre-
serving Indexing (LPI) (He et al, 2004) discover the
local structure of the term and document space and
compute a semantic subspace with a stronger dis-
criminative power. Laplacian Eigenmaps Embed-
ding and LPI preserve the input similarities only
locally, because this information is most reliable.
Laplacian Eigenmaps Embedding does not provide
a fold-in procedure for unseen documents. LPI
is a linear approximation to Laplacian Eigenmaps
Embedding that eliminates this problem. Similar
to LSA, the input similarities to LPI are based on
the inner products of the bag-of-word documents.
Laplacian Eigenmaps Embedding can use any kind
of similarity in the original space.
Generalized Latent Semantic Analysis
(GLSA) (Matveeva et al, 2005) is a frame-
work for computing semantically motivated term
and document vectors. It extends the LSA approach
by focusing on term vectors instead of the dual
document-term representation. GLSA requires a
measure of semantic association between terms and
a method of dimensionality reduction.
In this paper, we use GLSA with point-wise mu-
tual information as a term association measure. We
introduce the notion of locality into this framework
and propose to use Laplacian Eigenmaps Embed-
ding as a dimensionality reduction algorithm. We
evaluate the importance of locality for document
representation in document clustering experiments.
The rest of the paper is organized as follows. Sec-
61
tion 2 contains the outline of the graph-based GLSA
algorithm. Section 3 presents our experiments, fol-
lowed by conclusion in section 4.
2 Graph-based GLSA
2.1 GLSA Framework
The GLSA algorithm (Matveeva et al, 2005) has the
following setup. The input is a document collection
C with vocabulary V and a large corpus W .
1. For the vocabulary in V , obtain a matrix of
pair-wise similarities, S, using the corpus W
2. Obtain the matrix UT of a low dimensional
vector space representation of terms that pre-
serves the similarities in S, UT ? Rk?|V |
3. Construct the term document matrix D for C
4. Compute document vectors by taking linear
combinations of term vectors D? = UTD
The columns of D? are documents in the k-
dimensional space.
GLSA approach can combine any kind of simi-
larity measure on the space of terms with any suit-
able method of dimensionality reduction. The inner
product between the term and document vectors in
the GLSA space preserves the semantic association
in the input space. The traditional term-document
matrix is used in the last step to provide the weights
in the linear combination of term vectors. LSA is
a special case of GLSA that uses inner product in
step 1 and singular value decomposition in step 2,
see (Bartell et al, 1992).
2.2 Singular Value Decomposition
Given any matrix S, its singular value decompo-
sition (SVD) is S = U?V T . The matrix Sk =
U?kV T is obtained by setting all but the first k di-
agonal elements in ? to zero. If S is symmetric, as
in the GLSA case, U = V and Sk = U?kUT . The
inner product between the GLSA term vectors com-
puted as U?1/2k optimally preserves the similarities
in S wrt square loss.
The basic GLSA computes the SVD of S and uses
k eigenvectors corresponding to the largest eigenval-
ues as a representation for term vectors. We will re-
fer to this approach as GLSA. As for LSA, the simi-
larities are preserved globally.
2.3 Laplacian Eigenmaps Embedding
We used the Laplacian Embedding algo-
rithm (Belkin and Niyogi, 2003) in step 2 of
the GLSA algorithm to compute low-dimensional
term vectors. Laplacian Eigenmaps Embedding
preserves the similarities in S only locally since
local information is often more reliable. We will
refer to this variant of GLSA as GLSAL.
The Laplacian Eigenmaps Embedding algorithm
computes the low dimensional vectors y to minimize
under certain constraints
?
ij
||yi ? yj||2Wij .
W is the weight matrix based on the graph adjacency
matrix. Wij is large if terms i and j are similar ac-
cording to S. Wij can be interpreted as the penalty
of mapping similar terms far apart in the Laplacian
Embedding space, see (Belkin and Niyogi, 2003)
for details. In our experiments we used a binary ad-
jacency matrix W . Wij = 1 if terms i and j are
among the k nearest neighbors of each other and is
zero otherwise.
2.4 Measure of Semantic Association
Following (Matveeva et al, 2005), we primarily
used point-wise mutual information (PMI) as a mea-
sures of semantic association in step 1 of GLSA.
PMI between random variables representing two
words, w1 and w2, is computed as
PMI(w1, w2) = log
P (W1 = 1,W2 = 1)
P (W1 = 1)P (W2 = 1)
.
2.5 GLSA Space
GLSA offers a greater flexibility in exploring the
notion of semantic relatedness between terms. In
our preliminary experiments, we obtained the matrix
of semantic associations in step 1 of GLSA using
point-wise mutual information (PMI), likelihood ra-
tio and ?2 test. Although PMI showed the best per-
formance, other measures are particularly interest-
ing in combination with the Laplacian Embedding.
Related approaches, such as LSA, the Word Space
Model (WS) (Schu?tze, 1998) and Latent Relational
Analysis (LRA) (Turney, 2004) are limited to only
one measure of semantic association and preserve
the similarities globally.
62
Assuming that the vocabulary space has some un-
derlying low dimensional semantic manifold. Lapla-
cian Embedding algorithm tries to approximate this
manifold by relying only on the local similarity in-
formation. It uses the nearest neighbors graph con-
structed using the pair-wise term similarities. The
computations of the Laplacian Embedding uses the
graph adjacency matrix W . This matrix can be bi-
nary or use weighted similarities. The advantage
of the binary adjacency matrix is that it conveys
the neighborhood information without relying on in-
dividual similarity values. It is important for co-
occurrence based similarity measures, see discus-
sion in (Manning and Schu?tze, 1999).
The Locality Preserving Indexing (He et al,
2004) has a similar notion of locality but has to use
bag-of-words document vectors.
3 Document Clustering Experiments
We conducted a document clustering experiment for
the Reuters-21578 collection. To collect the co-
occurrence statistics for the similarities matrix S
we used a subset of the English Gigaword collec-
tion (LDC), containing New York Times articles la-
beled as ?story?. We had 1,119,364 documents with
771,451 terms. We used the Lemur toolkit1 to tok-
enize and index all document collections used in our
experiments, with stemming and a list of stop words.
Since Locality Preserving Indexing algorithm
(LPI) is most related to the graph-based GLSAL, we
ran experiments similar to those reported in (He et
al., 2004). We computed the GLSA document vec-
tors for the 20 largest categories from the Reuters-
21578 document collection. We had 8564 docu-
ments and 7173 terms. We used the same list of 30
TREC words as in (He et al, 2004) which are listed
in table 12. For each word on this list, we generated
a cluster as a subset of Reuters documents that con-
tained this word. Clusters are not disjoint and con-
tain documents from different Reuters categories.
We computed GLSA, GLSAL, LSA and LPI rep-
resentations. We report the results for k = 5 for
the k nearest neighbors graph for LPI and Laplacian
Embedding, and binary weights for the adjacency
1http://www.lemurproject.org/
2We used 28 words because we used stemming whereas (He
et al, 2004) did not, so that in two cases, two words were re-
duces to the same stem.
matrix. We report results for 300 embedding dimen-
sions for GLSA, LPI and LSA and 500 dimensions
for GLSAL.
We evaluate these representations in terms of how
well the cosine similarity between the document
vectors within each cluster corresponds to the true
semantic similarity. We expect documents from the
same Reuters category to have higher similarity.
For each cluster we computed all pair-wise doc-
ument similarities. All pair-wise similarities were
sorted in decreasing order. The term ?inter-pair? de-
scribes a pair of documents that have the same label.
For the kth inter-pair, we computed precision at k as:
precision(pk) =
#inter ? pairs pj, s.t. j < k
k ,
where pj refers to the jth inter-pair. The average
of the precision values for each of the inter-pairs was
used as the average precision for the particular doc-
ument cluster.
Table 1 summarizes the results. The first column
shows the words according to which document clus-
ters were generated and the entropy of the category
distribution within that cluster. The baseline was to
use the tf document vectors. We report results for
GLSA, GLSAL, LSA and LPI. The LSA and LPI
computations were based solely on the Reuters col-
lection. For GLSA and GLSALwe used the term as-
sociations computed for the Gigaword collection, as
described above. Therefore, the similarities that are
preserved are quite different. For LSA and LPI they
reflect the term distribution specific for the Reuters
collection whereas for GLSA they are more general.
By paired 2-tailed t-test, at p ? 0.05, GLSA outper-
formed all other approaches. There was no signifi-
cant difference in performance of GLSAL, LSA and
the baseline. Disappointingly, we could not achieve
good performance with LPI. Its performance varies
over clusters similar to that of other approaches but
the average is significantly lower. We would like
to stress that the comparison of our results to those
presented in (He et al, 2004) are only suggestive
since (He et al, 2004) applied LPI to each cluster
separately and used PCA as preprocessing. We com-
puted the LPI representation for the full collection
and did not use PCA.
63
word tf glsa glsaL lsa lpi
agreement(1) 0.74 0.73 0.73 0.75 0.46
american(0.8) 0.63 0.72 0.59 0.64 0.36
bank(1.4) 0.45 0.52 0.40 0.48 0.28
control(0.7) 0.78 0.82 0.80 0.80 0.58
domestic(0.8) 0.64 0.68 0.66 0.68 0.35
export(0.8) 0.64 0.65 0.70 0.67 0.37
five(1.3) 0.74 0.77 0.71 0.70 0.40
foreign(1.2) 0.51 0.58 0.55 0.56 0.28
growth(1) 0.51 0.58 0.48 0.54 0.32
income(0.5) 0.84 0.86 0.83 0.80 0.69
increase(1.3) 0.51 0.61 0.53 0.53 0.29
industrial(1.2) 0.59 0.66 0.58 0.61 0.34
internat.(1.1) 0.58 0.59 0.54 0.61 0.34
investment(1) 0.68 0.77 0.70 0.72 0.46
loss(0.3) 0.98 0.99 0.98 0.98 0.88
money(1.1) 0.70 0.62 0.71 0.65 0.38
national(1.3) 0.49 0.58 0.49 0.55 0.27
price(1.2) 0.53 0.63 0.57 0.57 0.29
production(1) 0.56 0.66 0.58 0.59 0.29
public(1.2) 0.58 0.60 0.57 0.57 0.31
rate(1.1) 0.61 0.62 0.64 0.60 0.35
report(1.2) 0.66 0.72 0.62 0.65 0.35
service(0.9) 0.59 0.66 0.56 0.61 0.39
source(1.2) 0.56 0.54 0.59 0.60 0.27
talk(0.9) 0.74 0.67 0.73 0.74 0.39
tax(0.7) 0.91 0.93 0.90 0.89 0.67
trade(1) 0.85 0.74 0.82 0.60 0.33
world(1.1) 0.63 0.65 0.68 0.66 0.33
Av. Acc 0.65 0.68 0.65 0.66 0.40
Table 1: Average inter-pairs accuracy.
The inter-pair accuracy depended on the cate-
gories distribution within clusters. For more homo-
geneous clusters, e.g. ?loss?, all methods (except
LPI) achieve similar precision. For less homoge-
neous clusters, e.g. ?national?, ?industrial?, ?bank?,
GLSA and LSA outperformed the tf document vec-
tors more significantly.
4 Conclusion and Future Work
We introduced a graph-based method of dimension-
ality reduction into the GLSA framework. Lapla-
cian Eigenmaps Embedding preserves the similar-
ities only locally, thus providing a potentially bet-
ter approximation to the low dimensional semantic
space. We explored the role of locality in the GLSA
representation and used binary adjacency matrix as
similarity which was preserved and compared it to
GLSA with unnormalized PMI scores.
Our results did not show an advantage of GLSAL.
GLSAL and LPI seem to be very sensitive to the pa-
rameters of the neighborhood graph. We tried dif-
ferent parameter settings but more experiments are
required for a thorough analysis. We are also plan-
ning to use a different document collection to elimi-
nate the possible effect of the specific term distribu-
tion in the Reuters collection. Further experiments
are needed to make conclusions about the geometry
of the vocabulary space and the appropriateness of
these methods for term and document embedding.
References
Brian T. Bartell, Garrison W. Cottrell, and Richard K.
Belew. 1992. Latent semantic indexing is an optimal
special case of multidimensional scaling. In Proc. of
the 15th ACM SIGIR, pages 161?167. ACM Press.
Mikhail Belkin and Partha Niyogi. 2003. Laplacian
eigenmaps for dimensionality reduction and data rep-
resentation. Neural Computation, 15(6):1373?1396.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying Ma.
2004. Locality preserving indexing for document rep-
resentation. In Proc. of the 27rd ACM SIGIR, pages
96?103. ACM Press.
Chris Manning and Hinrich Schu?tze. 1999. Founda-
tions of Statistical Natural Language Processing. MIT
Press. Cambridge, MA.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christian Royer. 2005. Generalized latent semantic
analysis for term representation. In Proc. of RANLP.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(21):97?124.
Peter D. Turney. 2004. Human-level performance on
word analogy questions by latent relational analysis.
Technical report, Technical Report ERB-1118, NRC-
47422.
64
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 106?113,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Studying Discourse and Dialogue with SIDGrid?
Gina-Anne Levow
Department of Computer Science
University of Chicago
Chicago, IL 60611, USA
levow@cs.uchicago.edu
Abstract
Teaching Computational Linguistics is in-
herently multi-disciplinary and frequently
poses challenges and provides opportunities in
teaching to a student body with diverse ed-
ucational backgrounds and goals. This pa-
per describes the use of a computational en-
vironment (SIDGrid) that facilitates interdis-
ciplinary instruction by providing support for
students with little computational background
as well as extending the scale of projects ac-
cessible to students with more advanced com-
putational skills. The environment facilitates
the use of hands-on exercises and is being ap-
plied to interdisciplinary instruction in Dis-
course and Dialogue.
1 Introduction
Teaching Computational Linguistics poses many
challenges but also provides many opportunities.
Students in Computational Linguistics courses come
from diverse academic backgrounds, including com-
puter science, linguistics, and psychology. The
students enter with differing experience and expo-
sure to programming, computational and mathemat-
ical models, and linguistic, psycholinguistic and so-
ciolinguistic theories that inform the practice and
study of computational linguistics. However, study-
ing in a common class provides students with the op-
portunity to gain exposure to diverse perspectives on
their research problems and to apply computational
?The work is supported by a University of Chicago Aca-
demic Technology Innovation Grant.
tools and techniques to expand the range and scope
of problems they can investigate.
While there are many facets of these instructional
challenges that must be addressed to support a suc-
cessful course with a multi-disciplinary class and
perspective, this paper focuses on the use and de-
velopment of a computational environment to sup-
port laboratory exercises for students from diverse
backgrounds. The framework aims to facilitate col-
laborative projects, reduce barriers of entry for stu-
dents with little prior computational experience, and
to provide access to large-scale distributed process-
ing resources for students with greater computa-
tional expertise to expand the scope and scale of
their projects and exercises.
Specifically, we exploit the Social Informatics
Data Grid (SIDGrid) framework developed as part
of the NSF-funded Cyberinfrastructure project, ?Cy-
berinfrastructure for Collaborative Research in the
Social and Behavioral Sciences (PI: Stevens)?, to
support hands-on annotation and analysis exercises
in a computational linguistics course focused on dis-
course and dialogue. We begin by describing the
SIDGrid framework for annotation, archiving, and
analysis of multi-modal, multi-measure data. We
then describe the course setting and the applica-
tion of SIDGrid functionality to expand exercise and
project possibilities. Finally, we discuss the impact
of this framework for multi-disciplinary instruction
in computational linguistics as well as the limita-
tions of the current implementation of the frame-
work.
106
2 SIDGrid Framework
2.1 Motivation
Recent research programs in multi-modal environ-
ments, including understanding and analysis of
multi-party meeting data and oral history recording
projects, have created an explosion of multi-modal
data sets, including video and audio recordings,
transcripts and other annotations, and increased in-
terest in annotation and analysis of such data. A
number of systems have been developed to man-
age and support annotation of multi-modal data, in-
cluding Annotation Graphs (Bird and Liberman,
2001), Exmeralda (Schmidt, 2004), NITE XML
Toolkit (Carletta et al, 2003), Multitool (Allwood
et al, 2001), Anvil (Kipp, 2001), and Elan (Wit-
tenburg et al, 2006). The Social Informatics Data
Grid (SIDGrid), developed under the NSF Cyberin-
frastructure Program, aims to extend the capabilities
of such systems by focusing on support for large-
scale, extensible distributed data annotation, shar-
ing, and analysis. The system is open-source and
multi-platform and based on existing open-source
software and standards. The system greatly eases the
integration of annotation with analysis though user-
defined functions both on the client-side for data ex-
ploration and on the TeraGrid for large-scale dis-
tributed data processing. A web-accessible repos-
itory supports data search, sharing, and distributed
annotation. While the framework is general, anal-
ysis of spoken and multi-modal discourse and dia-
logue data is a primary application.
The details of the system are presented below.
Sections 2.2, 2.3, and 2.4 describe the annota-
tion client, the web-accessible data repository, and
the portal to the TeraGrid, respectively, as shown in
Figure 1 below.
2.2 The SIDGrid Client
The SIDGrid client provides an interactive multi-
modal annotation interface, building on the open-
source ELAN annotation tool from the Max Planck
Institute1. A screenshot appears in Figure 2. ELAN
supports display and synchronized playback of mul-
tiple video files, audio files, and arbitrarily many
annotation ?tiers? in its ?music-score?-style graph-
ical interface. The annotations are assumed to be
1http://www.mpi.nl/tools/elan.html
Web-interface 
to TeraGrid &  
Repository
Client
Data Repository
TeraGrid
Figure 1: System Architecture
Figure 2: Screenshot of the annotation client interface,
with video, time-aligned textual annotations, and time se-
ries displays.
time-aligned intervals with, typically, text content;
the system leverages Unicode to provide multilin-
gual support. Time series such as pitch tracks or
motion capture data can be displayed synchronously.
The user may interactively add, edit, and do sim-
ple search in annotations. For example, in multi-
modal multi-party spoken data, annotation tiers cor-
responding to aligned text transcriptions, head nods,
pause, gesture, and reference can be created.
The client expands on this functionality by al-
lowing the application of user-defined analysis pro-
grams to media, time series, and annotations asso-
ciated with the current project, such as a conver-
sation, to yield time series files or annotation tiers
displayed in the client interface. Any program with
a command-line or scriptable interface installed on
the user?s system may be added to a pull-down list
for invocation. For example, to support a prosodic
107
analysis of multi-party meeting data, the user can se-
lect a Praat (Boersma, 2001) script to perform pitch
or intensity tracking. Also, the client provides inte-
grated import and export capabilities for the central
repository. New and updated experiments and an-
notations may be uploaded directly to the archive
from within the client interface. Existing experi-
ments may be loaded from local disk or downloaded
from the repository for additional annotation.
2.3 The SIDGrid Repository
The SIDGrid repository provides a web-accessible,
central archive of multi-modal data, annotations, and
analyses. This archive facilitates distributed anno-
tation efforts by multiple researchers working on a
common data set by allowing shared storage and ac-
cess to annotations, while keeping a history of up-
dates to the shared data, annotations, and analysis.
The browser-based interface to the archive allows
the user to browse or search the on-line data col-
lection by media type, tags, project identifier, and
group or owner. Once selected, all or part of any ex-
periment may be downloaded. In addition to lists of
experiment names or thumbnail images, the web in-
terface also provides a streaming preview of the se-
lected media and annotations, allowing verification
prior to download. (Figure 3)
All data is stored in a MySQL database. Anno-
tation tiers are converted to an internal time-span
based representation, while media and time series
files are linked in unanalyzed. This format allows
generation of ELAN format files for download to the
client tool without regard to the original source form
of the annotation file. The database structure further
enables the potential for flexible search of the stored
annotations both within and across multiple annota-
tion types.
2.4 The TeraGrid Portal
The large-scale multimedia data collected for multi-
modal research poses significant computational
challenges. Signal processing of gigabytes of me-
dia files requires processing horsepower that may
strain many local sites, as do approaches such as
multi-dimensional scaling for semantic analysis and
topic segmentation. To enable users to more effec-
tively exploit this data, the SIDGrid provides a por-
tal to the TeraGrid (Pennington, 2002), the largest
distributed cyberinfrastructure for open scientific
research, which uses high-speed network connec-
tions to link high performance computers and large
scale data stores distributed across the United States.
While the TeraGrid has been exploited within the as-
tronomy and physics communities, it has been little
used by the computational linguistics community.
The SIDGrid portal to the TeraGrid allows large-
scale experimentation by providing access to large-
scale distributed processing clusters to enable par-
allel processing on very high capacity servers. The
SIDGrid portal to the TeraGrid allows the user to
specify a set of files in the repository and a program
or programs to run on them on the Grid-based re-
sources. Once a program is installed on the Grid,
the processing can be distributed automatically to
different TeraGrid nodes. Software supports arbi-
trarily complex workflow specifications, but the cur-
rent SIDGrid interface provides simple support for
high degrees of data-parallel processing, as well as a
graphical display indicating the progress of the dis-
tributed program execution, as shown in Figure 4.
The results are then reintegrated with the original
experiments in the on-line repository. Currently in-
stalled programs support distributed acoustic analy-
sis using Praat, statistical analysis using R, and ma-
trix computations using Matlab and Octave.
2.5 Software Availability
The client software is freely available. Ac-
cess to the public portion of the repository
is possible through the project website at
https://sidgrid.ci.uchicago.edu;
full access to the repository to create new experi-
ments may also be requested there.
3 Course Setting and Activities
We explore the use of this framework in a course
which focuses on a subarea of Computational Lin-
guistics, specifically discourse and dialogue, tar-
geted at graduate students interested in research in
this area. This topic is the subject of research not
only in computational speech and language process-
ing, but also in linguistics, psychology, sociology,
anthropology, and philosophy. Research in this area
draws on a growing, large-scale collection of text
and multi-modal interaction data that often relies on
108
Figure 3: Screenshot of the archive download interface, with thumbnails of available video and download and analysis
controls.
Figure 4: Progress of execution of programs on TeraGrid. Table lists file identifiers and status. Graph shows progress.
109
computational tools to support annotation, archiv-
ing, and analysis. However, prior offerings of this
course through the Computer Science Department
had attracted primarily Computer Science gradu-
ate students, even though readings for the course
spanned the range of related fields. In collabora-
tion with researchers in co-verbal gesture in the Psy-
chology department, we hoped to increase the attrac-
tion and accessibility of the course material and ex-
ercises to a more diverse student population. Af-
ter advertising the course to a broader population
through the Linguistics Department mailing list, em-
phasizing the use of computational tools but lack of
requirements for previous programming experience,
the resulting class included members of the Linguis-
tics, Slavic Studies, Psychology, and Computer Sci-
ence Departments, about half of whom had some
prior programming experience, but few were expert.
3.1 Hands-on Exercises
Currently, we have only included a small number of
software tools as proof-of-concept and to enable par-
ticular course exercises in discourse and dialogue.
This first set of exercises explores three main prob-
lems in this area: topic segmentation, dialogue act
tagging, and turn-taking.
The topic segmentation exercise investigates the
impact of segment granularity and automatic speech
recognition errors on topic segmentation of conver-
sational speech. The data is drawn from the Cross-
Language Speech Retrieval Track of the Cross-
language Evaluation Forum (CLEF CL-SR) (Pecina
et al, 2007) collection. This collection includes au-
tomatic transcriptions of interviews from an oral his-
tory project, accompanied by manual segmentation
created as part of the MALACH project (Franz et al,
2003). The exercise employs the web-based portal
to the TeraGrid to perform segmentation of multiple
interviews in parallel on the Grid, followed by eval-
uation in parallel. We perform segmentation using
LCSeg (Galley et al, 2003) and evaluate using the
pk and WindowDiff metrics. Students identify the
best segmentation parameters for these interviews
and perform error analysis to assess the effect of
ASR errors.
The dialogue act tagging exercise involves both
annotation and analysis components. The students
are asked to download and annotate a small portion
of a conversation from the AMI corpus (Carletta et
al., 2005) with dialogue act tags. The AMI cor-
pus of multiparty meetings includes recorded video,
recorded audio, aligned manual transcriptions, and
manually annotated head and hand gesture. Stu-
dents annotate from text alone, with audio, with
video, and with all modalities. Local ?transforma-
tions?, programs or scripts associated with the an-
notation client, can also provide prosodic analysis
of features such as pitch and intensity. Students
are asked to assess the influence of different fea-
tures on their annotation process and to compare to
a gold standard annotation which is later provided.
The automatic analysis phase is performed on the
web-based portal to assess the impact of different
feature sets on automatic tagging. The tagging is
done in the Feature Latent Semantic Analysis frame-
work (Serafin and Di Eugenio, 2004), augmented
with additional prosodic and multi-modal features
drawn from the annotation. Since this analysis re-
quires Singular Value Decomposition of the poten-
tially large Feature-by-Dialogue-Act matrices, it is
often impractical to execute on single personal or
even departmental servers. Furthermore, feature ex-
traction, such as pitch tracking, of the full conver-
sation can itself strain the computational resources
available to students. Grid-based processing over-
comes both of these problems.
Exercises on turn-taking follow similar patterns.
An initial phase requires annotation and assessment
exercises by the students in the ELAN-based client
tool and downloaded from the web-based repository.
Subsequent phases of the exercises include applica-
tion and investigation of automatic techniques us-
ing the web-based environment and computational
resources of the TeraGrid. Clearly, many other exer-
cises could be framed within this general paradigm,
and we plan to extend the options available to stu-
dents as our interests and available software and data
sets permit.
4 Impact on Interdisciplinary Instruction
We designed these hands-on exercises to allow stu-
dents to investigate important problems in discourse
and dialogue through exploration of the data and
application of automatic techniques to recognize
these phenomena. We aimed in addition to exploit
110
the cyberinfrastructure framework to achieve three
main goals: lower barriers of entry to use of com-
putational tools by students with little prior pro-
gramming experience, enable students with greater
computational skills to expand the scale and scope
of their experiments, and to support collaborative
projects and a broader, interdisciplinary perspective
on research in discourse and dialogue.
4.1 Enabling All Users
A key goal in employing this architecture was to en-
able students with little or no programming expe-
rience to exploit advanced computational tools and
techniques. The integration of so-called ?transfor-
mations?, actually arbitrary program applications, in
both the annotation client and the web-based portal
to the TeraGrid, supports this goal. In both cases,
drop-down menus to select programs and text- and
check-boxes to specify parameters provide graphi-
cal user interfaces to what can otherwise be complex
command-line specifications. In particular, the web-
based portal removes requirements for local instal-
lation of software, shielding the user from problems
due to complex installations, variations in platforms
and operating systems, and abstruse command-line
syntax. In addition, the web-based archive provides
simple mechanisms to browse and download a range
of data sources. The students all found the archive,
download, and transformation mechanisms easy to
use, regardless of prior programming experience. It
is important to remember that the goal of this envi-
ronment is not to replace existing software systems
for Natural Language Processing, such the Natural
Language Toolkit (NLTK) (Bird and Loper, 2004),
but rather to provide a simpler interface to such soft-
ware tools and to support their application to poten-
tially large data sets, irrespective of the processing
power of the individual user? system.
4.2 Enabling Large-Scale Experimentation
A second goal is to enable larger-scale experimenta-
tion by both expert and non-expert users. The use of
the web-based portal to the TeraGrid provides such
opportunities. The portal provides access to highly
distributed parallel processing capabilities. For ex-
ample, in the case of the segmentation of the oral
history interviews above, the user can select several
interviews, say 60, to segment by checking the as-
sociated check-boxes in the interface. The portal
software will automatically identify available pro-
cessing nodes and distribute the segmentation jobs
for the corresponding interviews to each of the avail-
able nodes to be executed in parallel. Not only are
there many processing nodes, but these nodes are of
very high capacity in terms of CPU speed, number
of CPUs, and available memory.
The multigigabyte data files associated with the
growing number of multi-modal discourse and dia-
logue corpora, such as the AMI and ICSI Meeting
Recorder collections, make such processing power
highly desirable. For example, pitch tracking for
such corpora is beyond the memory limitations of
any single machine in the department, while such
tasks are quickly processed on the powerful Tera-
Grid machines.
Expert users are also granted privileges to upload
their own user-defined programs to be executed on
the Grid. Finally, web services also enable execu-
tion of arbitrary read-only queries on the underly-
ing database of annotations, media files, and time-
series data through standard Structure Query Lan-
guage (SQL) calls. All these capabilities enhance
the scope of problems that more skilled program-
mers can employ in the study of discourse and dia-
logue phenomena.
4.3 Interdisciplinary Collaboration and
Perspectives
The web-based archive in the SIDGrid framework
also provides support for group distributed collab-
orative projects. The archive provides a Unix-style
permission structure that allows data sharing within
groups. The process of project creation, annota-
tion, and experimentation maintains a version his-
tory. Uploads of new annotations create new ver-
sions; older versions are not deleted or overwritten.
Experimental runs are also archived, providing an
experiment history and shared access to intermedi-
ate and final results. Script and software versions
are also maintained. While the version control is not
nearly as sophisticated as that provided by GForge
or Subversion, this simple model requires no spe-
cial training and facilitates flexible, web-based dis-
tributed access and collaboration.
Finally, the interleaving of annotation and auto-
mated experimented permitted by this integrated ar-
111
chitecture provides the students with additional in-
sight into different aspects of research on discourse
and dialogue. Students from linguistics and psy-
chology gain greater experience in automatic analy-
sis and recognition of discourse phenomena, while
more computationally oriented students develop a
greater appreciation of the challenges of annotation
and theoretical issues in analysis of dialogue data.
5 Challenges and Costs
The capabilities and opportunities for study of com-
putational approaches to discourse and dialogue af-
forded within the SIDGrid framework do require
some significant investment of time and effort. In-
corporating new data sets and software packages
requires programming expertise. The framework
can, in principle, incorporate arbitrary data types:
media, physiological measures, manual and auto-
matic annotations, and even motion tracking. The
data must be converted into the ELAN .eaf for-
mat to be deployed effectively by the annotation
client and interpreted correctly by the archive?s un-
derlying database. Converters have been created
for several established formats2, such as Annota-
tion Graphs (Bird and Liberman, 2001), ANVIL
(Kipp, 2001), and EXMARaLDA(Schmidt, 2004),
and projects are underway to improve interoperabil-
ity between formats. However, new formats such as
the CLEF Cross-language Speech Retrieval SGML
format and NITE XML(Carletta et al, 2003) format
for the AMI data used here, required the implemen-
tation of software to convert the source format to one
suitable for use by SIDGrid.
Incorporating new Grid-based ?transformation?
programs can also range in required effort. For self-
contained programs in supported frameworks - cur-
rently, Perl, Python, Praat, and Octave - adding a
new program requires only a simple browser-based
upload. Compiled programs, such as LCSeg here,
must be compatible with the operating systems and
64-bit architecture on the Grid servers, often requir-
ing recompilation and occasionally addition of li-
braries to existing Grid installations. Finally, soft-
ware with licensing restrictions can only run on a
local cluster rather than on the full TeraGrid. Thus,
public domain programs and systems that rely on
2www.multimodal-annotation.org
such are preferred; for example, Octave-based pro-
grams are preferred to Matlab-based ones.
Finally, one must remember that the SIDGrid
framework is itself an ongoing research project. It
provides many opportunities to enhance interdisci-
plinary instruction in Computational Linguistics, es-
pecially in areas involving multi-modal data. How-
ever, the functionality is still under active develop-
ment, and current system users are beta-testers. The
use of the system, both in coursework and in re-
search, has driven improvements and expansions in
service.
6 Conclusions and Future Directions
We have explored the use of the SIDGrid framework
for annotation, archiving, and analysis of multi-
modal data to enhance hands-on activities in the
study of discourse and dialogue in a highly inter-
disciplinary course setting. Our preliminary efforts
have demonstrated the potential for the framework
to lower barriers of entry for students with less pro-
gramming experience to apply computational tech-
niques while enabling large-scale investigation of
discourse and dialogue phenomena by more expert
users. Annotation, analysis, and automatic recog-
nition exercises relating to topic segmentation, di-
alogue act tagging, and turn-taking give students a
broader perspective on research and issues in dis-
course and dialogue. These exercises also allow
students to contribute to class discussion and col-
laborative projects drawing on their diverse disci-
plinary backgrounds. We plan to extend our current
suite of hands-on exercises to cover other aspects of
discourse and dialogue, both in terms of data sets
and software, including well-known toolkits such as
NLTK. We hope that this expanded framework will
encourage additional interdisciplinary collaborative
projects among students.
Acknowledgments
We would like to thank Susan Duncan and David
McNeill for their participation in this project as well
as the University of Chicago Academic Technology
Innovation program. We would also like to thank
Sonjia Waxmonsky for her assistance in implement-
ing the course exercises, and the entire SIDGRID
team for providing the necessary system infrastruc-
112
ture. We are particularly appreciative of the response
to our bug reports and functionality requests by Tom
Uram and Sarah Kenny.
References
Jens Allwood, Leif Groenqvist, Elisabeth Ahlsen, and
Magnus Gunnarsson. 2001. Annotations and tools for
an activity based spoken language corpus. In Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue, pages 1?10.
S. Bird and M. Liberman. 2001. A formal frame-
work for linguistic annotation. Speech Communica-
tion, 33(1,2):23?60.
Steven Bird and Edward Loper. 2004. Nltk: The natural
language toolkit. In Proceedings of the ACL demon-
stration session, pages 214?217.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9?10):341?345.
J. Carletta, S. Evert, U. Heid, J. Kilgour, J. Robertson,
and H. Voormann. 2003. The NITE XML Toolkit:
flexible annotation for multi-modal language data. Be-
havior Research Methods, Instruments, and Comput-
ers, special issue on Measuring Behavior, 35(3):353?
363.
Jean Carletta, Simone Ashby, Sebastien Bourban, Mike
Flynn, Mael Guillemot, Thomas Hain, Jaroslav
Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa
Kronenthal, Guillaume Lathoud, Mike Lincoln, Agnes
Lisowska, Iain A. McCowan, Wilfried Post, Dennis
Reidsma, and Pierre Wellner. 2005. The AMI meet-
ings corpus. In Proceedings of the Measuring Be-
havior 2005 symposium on Annotating and measuring
Meeting Behavior.
M. Franz, B. Ramabhadran, T. Ward, and M. Picheny.
2003. Automated transcription and topic segmenta-
tion of large spoken archives. In Proceedings of EU-
ROSPEECH.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of ACL?03.
M. Kipp. 2001. Anvil- a generic annotation tool for mul-
timodal dialogue. In Proceedings of the 7th European
Conference on Speech Communication and Technol-
ogy (Eurospeech), pages 1367?1370.
Pavel Pecina, Petra Hoffmannova, Gareth J. F. Jones,
Ying Zhang, and Douglas W. Oard. 2007. Overview
of the clef-2007 cross language speech retrieval track.
In Working Notes for CLEF 2007.
Rob Pennington. 2002. Terascale clusters and the Tera-
Grid. In Proceedings for HPC Asia, pages 407?413.
Invited talk.
T. Schmidt. 2004. Transcribing and annotating spoken
language with EXMARaLDA. In Proceedings of the
LREC-Workshop on XML-based richly annotated cor-
pora.
Riccardo Serafin and Barbara Di Eugenio. 2004. Flsa:
Extending latent semantic analysis with features for
dialogue act classification. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 692?699,
Barcelona, Spain, July.
P. Wittenburg, H. Brugman, A. Russel, A. Klassmann,
and H. Sloetjes. 2006. Elan: a professional framework
for multimodality research. In Proceedings of Lan-
guage Resources and Evaluation Conference (LREC)
2006.
113
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 614?619,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Contrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback for
Rapport
Siwei Wang
Department of Psychology
University of Chicago
Chicago, IL 60637 USA
siweiw@cs.uchicago.edu
Gina-Anne Levow
Department of Linguistics
University of Washington
Seattle, WA 98195 USA
levow@uw.edu
Abstract
Verbal feedback is an important information
source in establishing interactional rapport.
However, predicting verbal feedback across
languages is challenging due to language-
specific differences, inter-speaker variation,
and the relative sparseness and optionality of
verbal feedback. In this paper, we employ an
approach combining classifier weighting and
SMOTE algorithm oversampling to improve
verbal feedback prediction in Arabic, English,
and Spanish dyadic conversations. This ap-
proach improves the prediction of verbal feed-
back, up to 6-fold, while maintaining a high
overall accuracy. Analyzing highly weighted
features highlights widespread use of pitch,
with more varied use of intensity and duration.
1 Introduction
Culture-specific aspects of speech and nonverbal be-
havior enable creation and maintenance of a sense of
rapport. Rapport is important because it is known to
enhance goal-directed interactions and also to pro-
mote learning. Previous work has identified cross-
cultural differences in a variety of behaviors, for
example, nodding (Maynard, 1990), facial expres-
sion (Matsumoto et al, 2005), gaze (Watson, 1970),
cues to vocal back-channel (Ward and Tsukuhara,
2000; Ward and Al Bayyari, 2007; Rivera and
Ward, 2007), nonverbal back-channel (Bertrand et
al., 2007)), and coverbal gesturing (Kendon, 2004).
Here we focus on the automatic prediction of lis-
tener verbal feedback in dyadic unrehearsed story-
telling to elucidate the similarities and differences
in three language/cultural groups: Iraqi Arabic-,
Mexican Spanish-, and American English-speaking
cultures. (Tickle-Degnen and Rosenthal, 1990)
identified coordination, along with positive emo-
tion and mutual attention, as a key element of in-
teractional rapport. In the verbal channel, this co-
ordination manifests in the timing of contributions
from the conversational participants, through turn-
taking and back-channels. (Duncan, 1972) pro-
posed an analysis of turn-taking as rule-governed,
supported by a range of prosodic and non-verbal
cues. Several computational approaches have inves-
tigated prosodic and verbal cues to these phenom-
ena. (Shriberg et al, 2001) found that prosodic cues
could aid in the identification of jump-in points in
multi-party meetings. (Cathcart et al, 2003) em-
ployed features such as pause duration and part-of-
speech (POS) tag sequences for back-channel pre-
diction. (Gravano and Hirschberg, 2009) investi-
gated back-channel-inviting cues in task-oriented di-
alog, identifying increases in pitch and intensity as
well as certain POS patterns as key contributors. In
multi-lingual comparisons, (Ward and Tsukuhara,
2000; Ward and Al Bayyari, 2007; Rivera and Ward,
2007) found pitch patterns, including periods of low
pitch or drops in pitch, to be associated with elic-
iting back-channels across Japanese, English, Ara-
bic, and Spanish. (Herrera et al, 2010) collected a
corpus of multi-party interactions among American
English, Mexican Spanish, and Arabic speakers to
investigate cross-cultural differences in proxemics,
gaze, and turn-taking. (Levow et al, 2010) identi-
fied contrasts in narrative length and rate of verbal
feedback in recordings of American English-, Mexi-
614
can Spanish-, and Iraqi Arabic-speaking dyads. This
work also identified reductions in pitch and intensity
associated with instances of verbal feedback as com-
mon, but not uniform, across these groups.
2 Multi-modal Rapport Corpus
To enable a more controlled comparison of listener
behavior, we collected a multi-modal dyadic corpus
of unrehearsed story-telling. We audio- and video-
recorded pairs of individuals who were close ac-
quaintances or family members with, we assumed,
well-established rapport. One participant viewed a
six minute film, the ?Pear Film? (Chafe, 1975), de-
veloped for language-independent elicitation. In the
role of Speaker, this participant then related the story
to the active and engaged Listener, who understood
that they would need to retell the story themselves
later. We have collected 114 elicitations: 45 Arabic,
32 Mexican Spanish, and 37 American English.
All recordings have been fully transcribed and
time-aligned to the audio using a semi-automated
procedure. We convert an initial manual coarse tran-
scription at the phrase level to a full word and phone
alignment using CUSonic (Pellom et al, 2001), ap-
plying its language porting functionality to Spanish
and Arabic. In addition, word and phrase level En-
glish glosses were manually created for the Span-
ish and Arabic data. Manual annotation of a broad
range of nonverbal cues, including gaze, blink, head
nod and tilt, fidget, and coverbal gestures, is under-
way. For the experiments presented in the remainder
of this paper, we employ a set of 45 vetted dyads, 15
in each language.
Analysis of cross-cultural differences in narrative
length, rate of listener verbal contributions, and the
use of pitch and intensity in eliciting listener vocal-
izations appears in (Levow et al, 2010). That work
found that the American English-speaking dyads
produced significantly longer narratives than the
other language/cultural groups, while Arabic listen-
ers provided a significantly higher rate of verbal con-
tributions than those in the other groups. Finally, all
three groups exhibited significantly lower speaker
pitch preceding listener verbal feedback than in
other contexts, while only English and Spanish ex-
hibited significant reductions in intensity. The cur-
rent paper aims to extend and enhance these find-
ings by exploring automatic recognition of speaker
prosodic contexts associated with listener verbal
feedback.
3 Challenges in Predicting Verbal
Feedback
Predicting verbal feedback in dyadic rapport in di-
verse language/cultural groups presents a number of
challenges. In addition to the cross-linguistic, cross-
cultural differences which are the focus of our study,
it is also clear that there are substantial inter-speaker
differences in verbal feedback, both in frequency
and, we expect, in signalling. Furthermore, while
the rate of verbal feedback differs across language
and speaker, it is, overall, a relatively infrequent
phenomenon, occurring in as little as zero percent
of pausal intervals for some dyads and only at an av-
erage of 13-30% of pausal intervals across the three
languages. As a result, the substantial class imbal-
ance and relative sparsity of listener verbal feedback
present challenges for data-driven machine learn-
ing methods. Finally, as prior researchers have ob-
served, provision of verbal feedback can be viewed
as optional. The presence of feedback, we assume,
indicates the presence of a suitable context; the ab-
sence of feedback, however, does not guarantee that
feedback would have been inappropriate, only that
the conversant did not provide it.
We address each of these issues in our experi-
mental process. We employ a leave-one-dyad-out
cross-validation framework that allows us to deter-
mine overall accuracy while highlighting the differ-
ent characteristics of the dyads. We employ and
evaluate both an oversampling technique (Chawla
et al, 2002) and class weighting to compensate for
class imbalance. Finally, we tune our classification
for the recognition of the feedback class.
4 Experimental Setting
We define a Speaker pausal region as an interval in
the Speaker?s channel annotated with a contiguous
span of silence and/or non-speech sounds. These
Speaker pausal regions are tagged as ?Feedback
(FB)? if the participant in the Listener role initi-
ates verbal feedback during that interval and as ?No
Feedback (NoFB)? if the Listener does not. We aim
to characterize and automatically classify each such
615
Arabic English Spanish
0.30 (0.21) 0.152 (0.10) 0.136 (0.12)
Table 1: Mean and standard deviation of proportion of
pausal regions associated with listener verbal feedback
region. We group the dyads by language/cultural
group to contrast the prosodic characteristics of the
speech that elicit listener feedback and to assess the
effectiveness of these prosodic cues for classifica-
tion. The proportion of regions with listener feed-
back for each language appears in Table 1.
4.1 Feature Extraction
For each Speaker pausal region, we extract fea-
tures from the Speaker?s words immediately preced-
ing and following the non-speech interval, as well
as computing differences between some of these
measures. We extract a set of 39 prosodic fea-
tures motivated by (Shriberg et al, 2001), using
Praat?s (Boersma, 2001) ?To Pitch...? and ?To In-
tensity...?. All durational measures and word posi-
tions are based on the semi-automatic alignment de-
scribed above. All measures are log-scaled and z-
score normalized per speaker. The full feature set
appears in Table 2.
4.2 Classification and Analysis
For classification, we employ Support Vector Ma-
chines (SVM), using the LibSVM implementation
(C-C.Cheng and Lin, 2001) with an RBF kernel. For
each language/cultural group, we perform ?leave-
one-dyad-out? cross-validation based on F-measure
as implemented in that toolkit. For each fold, train-
ing on 14 dyads and testing on the last, we determine
not only accuracy but also the weight-based ranking
of each feature described above.
Managing Class Imbalance Since listener verbal
feedback occurs in only 14-30% of candidate posi-
tions, classification often predicts only the majority
?NoFB? class. To compensate for this imbalance, we
apply two strategies: reweighting and oversampling.
We explore increasing the weight on the minority
class in the classifier by a factor of two or four. We
also apply SMOTE (Chawla et al, 2002) oversam-
pling to double or quadruple the number of minority
class training instances. SMOTE oversampling cre-
ates new synthetic minority class instances by iden-
tifying k = 3 nearest neighbors and inducing a new
instance by taking the difference between a sample
and its neighbor, multiplying by a factor between 0
and 1, and adding that value to the original instance.
5 Results
Table 4 presents the classification accuracy for dis-
tinguishing FB and NoFB contexts. We present the
overall class distribution for each language. We then
contrast the minority FB class and overall accuracy
under each of three weighting and oversampling set-
tings. The second row has no weighting or over-
sampling; the third has no weighting with quadru-
ple oversampling on all folds, a setting in which the
largest number of Arabic dyads achieves their best
performance. The last row indicates the oracle per-
formance when the best weighting and oversampling
setting is chosen for each fold.
We find that the use of reweighting and over-
sampling dramatically improves the recognition of
the minority class, with only small reductions in
overall accuracy of 3-7%. Under a uniform set-
ting of quadruple oversampling and no reweight-
ing, the number of correctly recognized Arabic and
English FB samples nearly triples, while the num-
ber of Spanish FB samples doubles. We further
see that if we can dynamically select the optimal
training settings, we can achieve even greater im-
provements. Here the number of correctly recog-
nized FB examples increases between 3- (Spanish)
and 6-fold (Arabic) with only a reduction of 1-4%
in overall accuracy. These accuracy levels corre-
spond to recognizing between 38% (English, Span-
ish) and 73% (Arabic) of the FB instances. Even un-
der these tuned conditions, the sparseness and vari-
ability of the English and Spanish data continue to
present challenges.
Finally, Table 3 illustrates the impact of the full
range of reweighting and oversampling conditions.
Each cell indicates the number of folds in each of
Arabic, English, and Spanish respectively, for which
that training condition yields the highest accuracy.
We can see that the different dyads achieve optimal
results under a wide range of training conditions.
616
Feature Type Description Feature IDs
Pitch 5 uniform points across word pre 0,pre 0.25,pre 0.5,pre 0.75,pre 1
post 0,post 0.25,post 0.5,post 0.75,post 1
Maximum, minimum, mean pre pmax, pre pmin, pre pmean
post pmax, post pmin, post pmean
Differences in max, min, mean diff pmax, diff pmin, diff pmean
Difference b/t boundaries diff pitch endbeg
Start and end slope pre bslope, pre eslope, post bslope, post eslope
Difference b/t slopes diff slope endbeg
Intensity Maximum, minimum, mean pre imax, pre imin, pre imean
post imax,post imin, post imean
Difference in maxima diff imax
Duration Last rhyme, last vowel, pause pre rdur, pre vdur, post rdur, post vdur, pause dur
Voice Quality Doubling & halving pre doub, pre half,post doub,post half
Table 2: Prosodic features for classification and analysis. Features tagged ?pre? are extracted from the word immedi-
ately preceding the Speaker pausal region; those tagged ?post? are extracted from the word immediatey following.
weight 1 2 4
no SMOTE 1,2,3 2,2,2 1,0,3
SMOTE Double 1,0,2 1,2,0 2,2,1
SMOTE Quad 3,0,0 1,2,2 3,6,2
Table 3: Varying SVM weight and SMOTE ratio. Each
cell shows # dyads in each language (Arabic, English,
Spanish) with their best performance with this setting.
Arabic English Spanish
Overall 478 (1405) 395 (2659) 173 (1226)
Baseline 53 (950) 23 (2167) 23 (1066)
S=2, W=1 145 (878) 67 (2120) 47 (1023)
Oracle 347 (918) 152 (2033) 68 (1059)
Table 4: Row 1: Class distribution: # FB instances (#
total instances). Rows 2-4: Recognition under different
settings: # FB correctly recognized (total # correct)
6 Discussion: Feature Analysis
To investigate the cross-language variation in
speaker cues eliciting listener verbal feedback, we
conduct a feature analysis. Table 5 presents the
features with highest average weight for each lan-
guage assigned by the classifier across folds, as well
as those distinctive features highly ranked for only
one language.
We find that the Arabic dyads make extensive
and distinctive use of pitch in cuing verbal feed-
back, from both preceding and following words,
while placing little weight on other feature types.
In contrast, both English and Spanish dyads exploit
both pitch and intensity features from surrounding
words. Spanish alone makes significant use of both
vocalic and pause duration. We also observe that, al-
though there is substantial variation in feature rank-
ing across speakers, the highly ranked features are
robustly employed across almost all folds.
7 Conclusion
Because of the size of our data set, it may be pre-
mature to draw firm conclusion about differences
between these three language groups based on this
analysis. The SVM weighting and SMOTE over-
sampling strategy discussed here is promising for
improving recognition on imbalanced class data.
This strategy substantially improves the prediction
617
Most Important Features
Arabic English Spanish
pre pmax pre pmean pre min
pre pmean post pmean post 0.5
pre 0.25 post 0.5 post 0.75
pre 0.5 post 0.75 post 1
pre 0.75 post 1 pre imax
pre 1 diff pmin pre imean
post pmin pre imax post imax
post bslope pre imean pause dur
diff pmin post imean pre vdur
Most Distinctive Features
Arabic English Spanish
post pmin post pmean post 0
post bslope post 0.25 post eslope
pre 0.25 pre eslope
pre 0.5 post vdur
pre 1 pre imean
Table 5: Highest ranked and distinctive features for each
language/cultural group
of verbal feedback. The resulting feature ranking
also provides insight into the contrasts in the use of
prosodic cues among these language cultural groups,
while highlighting the widespread, robust use of
pitch features.
In future research, we would like to extend our
work to exploit sequential learning frameworks to
predict verbal feedback. We also plan to explore the
fusion of multi-modal features to enhance recogni-
tion and increase our understanding of multi-modal
rapport behavior. We will also work to analyze how
quickly people can establish rapport, as the short du-
ration of our Spanish dyads poses substantial chal-
lenges.
8 Acknowledgments
We would like to thank our team of annota-
tor/analysts for their efforts in creating this corpus,
and Danial Parvaz for the development of the Arabic
transliteration tool. We are grateful for the insights
of Susan Duncan, David McNeill, and Dan Loehr.
This work was supported by NSF BCS#: 0729515.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
References
R. Bertrand, G. Ferre, P. Blache, R. Espesser, and
S. Rauzy. 2007. Backchannels revisited from a mul-
timodal perspective. In Auditory-visual Speech Pro-
cessing, The Netherlands. Hilvarenbeek.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9?10):341?345.
C-C.Cheng and C-J. Lin. 2001. LIBSVM:a library
for support vector machines. Software available at:
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
N. Cathcart, J. Carletta, and E. Klein. 2003. A shallow
model of backchannel continuers in spoken dialogue.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Linguis-
tics - Volume 1, pages 51?58.
W. Chafe. 1975. The Pear Film.
Nitesh Chawla, Kevin Bowyer, Lawrence O. Hall, and
W. Philip Legelmeyer. 2002. SMOTE: Synthetic mi-
nority over-sampling technique. Journal of Artificial
Intelligence Research, 16:321?357.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283?292.
A. Gravano and J. Hirschberg. 2009. Backchannel-
inviting cues in task-oriented dialogue. In Proceedings
of Interspeech 2009, pages 1019?1022.
David Herrera, David Novick, Dusan Jan, and David
Traum. 2010. The UTEP-ICT cross-cultural mul-
tiparty multimodal dialog corpus. In Proceedings of
the Multimodal Corpora Workshop: Advances in Cap-
turing, Coding and Analyzing Multimodality (MMC
2010).
A. Kendon. 2004. Gesture: Visible Action as Utterance.
Cambridge University Press.
G.-A. Levow, S. Duncan, and E. King. 2010. Cross-
cultural investigation of prosody in verbal feedback in
interactional rapport. In Proceedings of Interspeech
2010.
D. Matsumoto, S. H. Yoo, S. Hirayama, and G. Petrova.
2005. Validation of an individual-level measure of
display rules: The display rule assessment inventory
(DRAI). Emotion, 5:23?40.
S. Maynard. 1990. Conversation management in con-
trast: listener response in Japanese and American En-
glish. Journal of Pragmatics, 14:397?412.
B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang,
X. Yu, and S. Pradhan. 2001. University of Colorado
dialog systems for travel and navigation.
618
A. Rivera and N. Ward. 2007. Three prosodic features
that cue back-channel in Northern Mexican Span-
ish. Technical Report UTEP-CS-07-12, University of
Texas, El Paso.
E. Shriberg, A. Stolcke, and D. Baron. 2001. Can
prosody aid the automatic processing of multi-party
meetings? evidence from predicting punctuation, dis-
fluencies, and overlapping speech. In Proc. of ISCA
Tutorial and Research Workshop on Prosody in Speech
Recognition and Understanding.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
nature of rapport and its nonverbal correlates. Psycho-
logical Inquiry, 1(4):285?293.
N. Ward and Y. Al Bayyari. 2007. A prosodic feature
that invites back-channels in Egyptian Arabic. Per-
spectives in Arabic Linguistics XX.
N. Ward and W. Tsukuhara. 2000. Prosodic fea-
tures which cue back-channel responses in English and
Japanese. Journal of Pragmatics, 32(8):1177?1207.
O. M. Watson. 1970. Proxemic Behavior: A Cross-
cultural Study. Mouton, The Hague.
619
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 21?22,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Bridging Gaps for Spoken Dialog System Frameworks in Instructional
Settings
Gina-Anne Levow
Department of Linguistics
University of Washington
Seattle, WA 98195 USA
levow@u.washington.edu
Abstract
Spoken dialog systems frameworks fill a cru-
cial role in the spoken dialog systems com-
munity by providing resources to lower bar-
riers to entry. However, different user groups
have different requirements and expectations
for such systems. Here, we consider the par-
ticular needs for spoken dialog systems toolk-
its within an instructional setting. We discuss
the challenges for existing systems in meet-
ing these needs and propose strategies to over-
come them.
1 Introduction
A key need in the spoken dialog systems community
is a spoken dialog system development framework.
Such systems fulfill fundamental roles in lowering
barriers to entry for development of spoken dialog
systems, providing baseline systems for comparabil-
ity, and supporting novel experimental extensions.
There are many characteristics that are desirable for
a shared spoken dialog system resource, including:
? Availability: Systems should be provided on
an on-going basis, with continuing support, up-
dates, and maintenance.
? Ease-of-use: Systems should be easy to use and
provide an environment in which systems are
easy to develop.
? Platform-independence: Systems be usable on
a wide variety of architectures, if installed, or
provided on an accessible platform, such as a
website.
? Application access: Systems should provide
a range of exemplar applications within the
framework.
? Flexibility and extensibility: Systems should
enable integration of diverse technology com-
ponents and facilitate a wide range of experi-
mental configurations.
? Robustness: Systems should enable state-of-
the-art performance for diverse applications.
? Affordability: Systems should be free if possi-
ble, or provided at pricing that is not prohibitive
for different user groups.
However, these systems also serve diverse groups
of users, from senior research developers to students
building their first spoken dialog systems. While
these users share many requirements, their relative
importance naturally varies. Research developers
will likely place greater emphasis on system robust-
ness, extensibility, and flexibility, for example to
incorporate alternative speech recognizers, speech
synthesizers, or dialog managers. Those using such
systems in an instructional setting will place greater
importance on ease-of-use, platform portability or
independence, availability, affordability, and access
to reference applications. Below, we will discuss
some of the challenges for systems trying to meet
these needs. Then we will describe two popular cur-
rent solutions and how they satisfy the needs of these
different groups. Lastly we will present some addi-
tional needs for spoken dialog systems frameworks
to bridge gaps in dialog systems for instructional
use.
A variety of systems have been developed that ad-
dress many of these needs, but all suffer from signif-
21
icant limitations. Availability and affordability have
posed some of the knottiest problems. For example,
many of the Galaxy Communicator research sys-
tems, such as those by University of Colorado (Pel-
lom et al, 2001), MIT, and CMU, were made avail-
able to the research community. However, many of
the systems are no longer available, usable, or sup-
ported, as research groups have disbanded and sys-
tems architectures have changed. Maintaining sys-
tems over time requires group and community com-
mitment, facilitated by an open-source framework.
Other toolkits and frameworks have become prob-
lematic due to conflicts between availability and af-
fordability. The long-popular CSLU toolkit (Sutton
and Cole, 1997) has recently shifted to a commercial
footing. Similarly, several industry platforms have
provided free non-commercial VoiceXML hosting,
as a simple spoken dialog development environ-
ment. However, at least one of these systems has
recently shifted to a paid-only status. The environ-
ment changes rapidly. Of three freely available aca-
demic systems and five VoiceXML platforms listed
in a 2009 survey (Jokinen and McTear, 2009), two
have already gone to paid status as of late 2011.
Two frameworks have emerged in recent years as
popular SDS frameworks: the Ravenclaw/Olympus
framework (Bohus et al, 2007) and VoiceXML,
hosted on one of the industrial platforms, such as
Nuance?s Cafe or Voxeo1. However, they do seem
to address the needs of different user groups. Raven-
claw/Olympus has been more widely adopted in the
research community: it is robust, flexible, exten-
sible, open-source, provides diverse use cases, and
has an active support and development community.
In contrast, the VoiceXML platforms have proven
popular in an instructional setting, as attested by the
large number of online homework assignments em-
ploying VoiceXML. These VoiceXML frameworks
offer very simple, easy-to-use environments that are
largely platform-independent, include basic support
and tutorials, and provide simple baseline applica-
tions. Given VoiceXML?s extensive role in indus-
try settings, they also provide an advantage in terms
of direct practical experience for students and in
terms of broad resources and support. In an instruc-
tional setting, Ravenclaw/Olympus? relative com-
1http://cafe.bevocal.com; http://www.voxeo.com
plexity, Windows platform and software dependence
in instructional environments where linux has be-
come predominant, and smaller resource base rep-
resent hurdles. While the VoiceXML platforms ex-
cel in these dimensions, their very simplicity and
ease-of-use are limiting. Students are often look-
ing for existing applications of moderate interesting
complexity as a basis for extension and experimenta-
tion. Most typical example applications are simpler
than those given for Olympus, and the platform is
severely limiting for more advanced users and tasks.
For example, many VoiceXML frameworks do not
even support user-defined pronunciations. Lastly,
these VoiceXML platforms rely on the generosity of
the industrial teams, which can readily evaporate as
has already happened with Tellme Studio.
Ideally, for instructional use, we would like to
bridge the gap between the too-simple, restrictive
VoiceXML frameworks and the more challenging
but more flexible and powerful Ravenclaw/Olympus
framework, to allow students and instructors to
transition more smoothly from one to the other.
On the VoiceXML side, a community-supported
VoiceXML platform would reduce dependence on
industry platforms. Access to VoiceXML applica-
tions of greater complexity, comparable to Let?s Go!
or Communicator tasks, would allow more inter-
esting experiments within a course?s limited span.
Lastly, porting Ravenclaw/Olympus to linux would
allow easier adoption in a wider range of academic
programs.
References
D. Bohus, Antoine Raux, Thomas K. Harris, Maxine Es-
kenazi, and Alexander I. Rudnicky. 2007. Olympus:
an open-source framework for conversational spoken
language interface research. In Bridging the Gap:
Academic and Industrial Research in Dialog Technol-
ogy workshop at HLT/NAACL 2007.
Kristiina Jokinen and Michael F. McTear. 2009. Spoken
Dialogue Systems. Morgan & Claypool Publishers.
B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang,
X. Yu, and S. Pradhan. 2001. University of Colorado
dialog systems for travel and navigation.
Stephen Sutton and Ronald Cole. 1997. The cslu toolkit:
rapid prototyping of spoken language systems. In Pro-
ceedings of the 10th annual ACM symposium on User
interface software and technology, UIST ?97, pages
85?86, New York, NY, USA. ACM.
22
