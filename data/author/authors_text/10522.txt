Bengali, Hindi and Telugu to English Ad-hoc Bilingual task  
 
Sivaji Bandyopadhyay, Tapabrata Mondal, Sudip Kumar Naskar, 
Asif Ekbal, Rejwanul Haque, Srinivasa Rao Godavarthy 
 
Abstract 
 
This paper presents the experiments carried out at Jadavpur University as 
part of participation in the CLEF 2007 ad-hoc bilingual task. This is our first 
participation in the CLEF evaluation task and we have considered Bengali, 
Hindi and Telugu as query languages for the retrieval from English 
document collection. We have discussed our Bengali, Hindi and Telugu to 
English CLIR system as part of the ad-hoc bilingual task, English IR system 
for the ad-hoc monolingual task and the associated experiments at CLEF. 
Query construction was manual for Telugu-English ad-hoc bilingual task, 
while it was automatic for all other tasks. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 80?83,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
English to Hindi Machine Transliteration System at NEWS 2009 
 
Amitava Das, Asif Ekbal, Tapabrata Mandal and Sivaji Bandyopadhyay 
Computer Science and Engineering Department 
Jadavpur University, Kolkata-700032, India 
amitava.research@gmail.com, asif.ekbal@gmail.com, ta-
pabratamondal@gmail.com, sivaji_cse_ju@yahoo.com 
 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2009 Machine Transliteration Shared 
Task held as part of ACL-IJCNLP 2009. We 
submitted one standard run and two non-
standard runs for English to Hindi translitera-
tion. The modified joint source-channel model 
has been used along with a number of alterna-
tives. The system has been trained on the 
NEWS 2009 Machine Transliteration Shared 
Task datasets. For standard run, the system 
demonstrated an accuracy of 0.471 and the 
mean F-Score of 0.861. The non-standard runs 
yielded the accuracy and mean F-scores of 
0.389 and 0.831 respectively in the first one 
and 0.384 and 0.828 respectively in the second 
one. The non-standard runs resulted in sub-
stantially worse performance than the standard 
run. The reasons for this are the ranking algo-
rithm used for the output and the types of to-
kens present in the test set. 
1 Introduction 
Technical terms and named entities (NEs) consti-
tute the bulk of the Out Of Vocabulary (OOV) 
words. Named entities are usually not found in 
bilingual dictionaries and are very generative in 
nature. Proper identification, classification and 
translation of Named entities (NEs) are very im-
portant in many Natural Language Processing 
(NLP) applications. Translation of NEs involves 
both translation and transliteration. Translitera-
tion is the method of translating into another lan-
guage by expressing the original foreign word 
using characters of the target language preserv-
ing the pronunciation in their source language. 
Thus, the central problem in transliteration is 
predicting the pronunciation of the original word. 
Transliteration between two languages that use 
the same set of alphabets is trivial: the word is 
left as it is. However, for languages those use 
different alphabet sets the names must be transli-
terated or rendered in the target language alpha-
bets. Transliteration of NEs is necessary in many 
applications, such as machine translation, corpus 
alignment, cross-language Information Retrieval, 
information extraction and automatic lexicon 
acquisition. In the literature, a number of transli-
teration algorithms are available involving Eng-
lish (Li et al, 2004; Vigra and Khudanpur, 2003; 
Goto et al, 2003), European languages (Marino 
et al, 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). 
 
2 Machine Transliteration Systems  
Three transliteration models have been used that 
can generate the Hindi transliteration from an 
English named entity (NE). An English NE is 
divided into Transliteration Units (TUs) with 
patterns C*V*, where C represents a consonant 
and V represents a vowel. The Hindi NE is di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the lexical units for machine transli-
teration. The system considers the English and 
Hindi contextual information in the form of col-
located TUs simultaneously to calculate the plau-
sibility of transliteration from each English TU 
to various Hindi candidate TUs and chooses the 
one with maximum probability. This is equiva-
lent to choosing the most appropriate sense of a 
word in the source language to identify its repre-
sentation in the target language. The system 
learns the mappings automatically from the bi-
lingual NEWS training set being guided by lin-
80
guistic features/knowledge. The system consid-
ers the linguistic knowledge in the form of con-
juncts and/or diphthongs in English and their 
possible transliteration in Hindi. The output of 
the mapping process is a decision-list classifier 
with collocated TUs in the source language and 
their equivalent TUs in collocation in the target 
language along with the probability of each deci-
sion obtained from the training set. Linguistic 
knowledge is used in order to make the number 
of TUs in both the source and target sides equal. 
A Direct example base has been maintained that 
contains the bilingual training examples that do 
not result in the equal number of TUs in both the 
source and target sides during alignment. The 
Direct example base is checked first during ma-
chine transliteration of the input English word. If 
no match is obtained, the system uses direct or-
thographic mapping by identifying the equivalent 
Hindi TU for each English TU in the input and 
then placing the Hindi TUs in order. The transli-
teration models are described below in which S 
and T denotes the source and the target words 
respectively: 
 
? Model A 
This is essentially the joint source-channel model 
(Hazhiou et al, 2004) where the previous TUs 
with reference to the current TUs in both the 
source (s) and the target sides (t) are considered 
as the context.  
1
1
( | ) ( , | , )k k
k
K
P S T P s t s t
?
=
= < > < >?  
( ) arg max { ( ) ( | )}S T S P T P S TT? = ?  
? Model B 
This is basically the trigram model where the 
previous and the next source TUs are considered 
as the context.  
 1, 1
1
( | ) ( , | )k k k
k
K
P S T P s t s s
? +
=
= < >?  
  ( ) arg max { ( ) ( | )}S T S P T P S TT? = ?  
? Model C 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the  improved 
modified joint source-channel model. 
1, 1
1
( | ) ( , | , )k k k
k
K
P S T P s t s t s
? +
=
= < > < >?   
 ( ) arg max { ( ) ( | )}S T S P T P S TT? = ?               
For NE transliteration, P(T), i.e., the 
probability of transliteration in the target 
language, is calculated from a English-Hindi 
bilingual database of approximately 961,890 
English person names, collected from the web1.  
If, T is not found in the dictionary, then a very 
small value is assigned to P(T). These models 
have been desribed in details in Ekbal et al 
(2007). 
 
? Post-Processing 
Depending upon the nature of errors involved in 
the results, we have devised a set of translitera-
tion rules. A few rules have been devised to pro-
duce more spelling variations. Some examples 
are given below. 
Spelling variation rules 
Badlapur ??????? | ??????? 
Shree | Shri ? 
 
3 Experimental Results   
We have trained our transliteration models using 
the English-Hindi datasets obtained from the 
NEWS 2009 Machine Transliteration Shared 
Task (Li et al, 2009). A brief statistics of the 
datasets are presented in Table 1. Out of 9975 
English-Hindi parallel examples in the training 
set, 4009 are multi-words. During training, we 
have split these multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in 22 multi-words and these 
cases were not considered further. Following are 
some examples:  
Paris Charles de Gaulle ????  
???? ??? ? ?????  
South Arlington Church of 
Christ ???? ???? 
In the training set, some multi-words were partly 
translated and not transliterated. Such examples 
were dropped from the training set. Finally, the 
training set consists of 15905 single word Eng-
lish-Hindi parallel examples.  
                                                 
1http://www.eci.gov.in/DevForum/Fullname.asp  
81
      
Set Number of examples 
Training 9975 
Development 974 
Test 1000 
Table 1. Statistics of Dataset 
 
The output of the modified joint source-
channel model is given more priority during out-
put ranking followed by the trigram and the joint 
source-channel model. During testing, the Direct 
example base is searched first to find the transli-
teration. Experimental results on the develop-
ment set yielded the accuracy of 0.442 and mean 
F-score of 0.829. Depending upon the nature of 
errors involved in the results, we have devised a 
set of transliteration rules. The use of these trans-
literation rules increased the accuracy and mean 
F-score values up to 0.489 and 0.881 respective-
ly.  
The system has been evaluated for the test set 
and the detailed reports are available in Li et al 
(2009). There are 88.88% unknown examples in 
the test set. We submitted one standard run in 
which the outputs are provided for the modified 
joint source-channel model (Model C), trigram 
model (Model B) and joint source-channel model 
(Model A). The same ranking procedure (i.e., 
Model C, Model B and Model A) has been fol-
lowed as that of the development set. The output 
of each transliteration model has been post-
processed with the set of transliteration rules. For 
each word, three different outputs are provided in 
a ranked order. If the outputs of any two models 
are same for any word then only two outputs are 
provided for that particular word. Post-
processing rules generate more number of possi-
ble transliteration output. Evaluation results of 
the standard run are shown in Table 2.  
 
Parameters Accuracy 
Accuracy in top-1 0.471 
Mean F-score 0.861 
Mean Reciprocal Rank 
(MRR) 
0.519 
Mean Average Preci-
sion (MAP)ref 
0.463 
MAP10 0.162 
MAPsys 0.383 
Table 2. Results of the standard run  
 
The results of the two non-standard runs are 
presented in Table 3 and Table 4 respectively.  
Parameters Accuracy 
Accuracy in top-1 0.389 
Mean F-score 0.831 
Mean Reciprocal Rank 
(MRR) 
0.487 
Mean Average Preci-
sion (MAP)ref 
0.385 
MAP10 0.16 
MAPsys 0.328 
  
Table 3. Results of the non-standard run 1 
 
Parameters Accuracy 
Accuracy in top-1 0.384 
Mean F-score 0.823 
Mean Reciprocal Rank 
(MRR) 
0.485 
Mean Average Precision 
(MAP)ref 
0.380 
MAP10 0.16 
MAPsys 0.325 
 
Table 4. Results of the non-standard run2 
 
In both the non-standard runs, we have used 
an English-Hindi bilingual database of approx-
imately 961, 890 examples that have been col-
lected from the web2. This database contains the 
(frequency) of the corresponding English-Hindi 
name pair. Along with the outputs of three mod-
els, the output obtained from this bilingual data-
base has been also provided for each English 
word. In the first non-standard run, only the most 
frequent transliteration has been considered. But, 
in the second non-standard run all the possible 
transliteration have been considered. It is to be 
noted that in these two non-standard runs, the 
transliterations obtained from the bilingual data-
base have been kept first in the ranking. Results 
of the tables show quite similar performance in 
both the runs. But the non-standard runs resulted 
in substantially worse performance than the stan-
dard run. The reasons for this are the ranking 
algorithm used for the output and the types of 
tokens present in the test set. The additional da-
                                                 
2http://www.eci.gov.in/DevForum/Fullname.asp  
82
taset used for the non-standard runs is mainly 
census data consisting of only Indian person 
names. The NEWS 2009 Machine Transliteration 
Shared Task training set is well distributed with 
foreign names (Ex. Sweden, Warren), common 
nouns (Mahfuz, Darshanaa) and a few non 
named entities. Hence the training set for the 
non-standard runs was biased towards the Indian 
person name transliteration pattern. Additional 
training set was quite larger (961, 890) than the 
shared task training set (9,975). Actually outputs 
of non-standard runs have more alternative trans-
literation outputs than the standard set. That 
means non-standard sets are superset of standard 
set. Our observation is that the ranking algorithm 
used for the output and biased training are the 
main reasons for the worse performance of the 
non-standard runs. 
4 Conclusion  
This paper reports about our works as part of the 
NEWS 2009 Machine Transliteration Shared 
Task. We have used the modified joint source-
channel model along with two other alternatives 
to generate the Hindi transliteration from an Eng-
lish word (to generate more spelling variations of 
Hindi names). We have also devised some post-
processing rules to remove the errors. During 
standard run, we have obtained the word accura-
cy of 0.471 and mean F-score of 0.831. In non-
standard rune, we have used a bilingual database 
obtained from the web. The non-standard runs 
yielded the word accuracy and mean F-score 
values of 0.389 and 0.831 respectively in the first 
run and 0.384 and 0.823 respectively in the 
second run. 
 
References  
Al-Onaizan, Y. and Knight, K. 2002a. Named 
Entity Translation: Extended Abstract. In 
Proceedings of the Human Language Tech-
nology Conference, 122? 124. 
Al-Onaizan, Y. and Knight, K. 2002b. Translat-
ing Named Entities using Monolingual and 
Bilingual Resources. In Proceedings of the 
40th Annual Meeting of the ACL, 400?408, 
USA. 
Ekbal, A. Naskar, S. and Bandyopadhyay, S. 
2007. Named Entity Transliteration. Interna-
tional Journal of Computer Processing of 
Oriental Languages (IJCPOL), Volume 
(20:4), 289-310, World Scientific Publishing 
Company, Singapore. 
Ekbal, A., Naskar, S. and Bandyopadhyay, S. 
2006. A Modified Joint Source Channel 
Model for Transliteration. In Proceedings of 
the COLING-ACL 2006, 191-198, Australia. 
Goto, I., Kato, N., Uratani, N. and Ehara, T. 
2003. Transliteration Considering Context 
Information based on the Maximum Entropy 
Method. In Proceeding of the MT-Summit 
IX, 125?132, New Orleans, USA.  
Jung, Sung Young , Sung Lim Hong and Eunok 
Paek. 2000. An English to Korean Translite-
ration Model of Extended Markov Window. 
In Proceedings of International Conference 
on Computational Linguistics (COLING 
2000), 383-389. 
Knight, K. and Graehl, J. 1998. Machine Transli-
teration, Computational Linguistics, Volume 
(24:4), 599?612. 
Kumaran, A. and Tobias Kellner. 2007. A gener-
ic framework for machine transliteration. In 
Proc. of the 30th SIGIR. 
Li, Haizhou, A Kumaran, Min Zhang and Vla-
dimir Pervouchine. 2009. Whitepaper of 
NEWS 2009 Machine Transliteration Shared 
Task. In Proceedings of ACL-IJCNLP 2009 
Named Entities Workshop (NEWS 2009), Sin-
gapore. 
Li, Haizhou, A Kumaran, Vladimir Pervouchine 
and Min Zhang. 2009.  Report on NEWS 2009 
Machine Transliteration Shared Task. In Pro-
ceedings of ACL-IJCNLP 2009  amed Entities 
Workshop (NEWS 2009), Singapore. 
Li, Haizhou, Min Zhang and Su Jian. 2004. A 
Joint Source-Channel Model for Machine 
Transliteration. In Proceedings of the 42nd 
Annual Meeting of the ACL, 159-166. Spain. 
Marino, J. B., R. Banchs, J. M. Crego, A. de 
Gispert, P. Lambert, J. A. Fonollosa and M. 
Ruiz. 2005.  Bilingual n-gram Statistical 
Machine Translation. In Proceedings of the 
MT-Summit X, 275?282. 
Surana, Harshit, and Singh, Anil Kumar. 2008. A 
More Discerning and Adaptable Multilingual 
Transliteration Mechanism for Indian Lan-
guages. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Lan-
guage Processing (IJCNLP-08), 64-71, In-
dia. 
Vigra, Paola and Khudanpur, S. 2003. Translite-
ration of Proper Names in Cross-Lingual In-
formation Retrieval. In Proceedings of the 
ACL 2003 Workshop on Multilingual and 
Mixed-Language Named Entity Recognition, 
57?60. 
83
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 71?75,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
English to Indian Languages Machine Transliteration System at 
NEWS 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Asif Ekbal4, Sivaji Bandyopadhyay5 
Department of Computer Science and Engineering1,2,3,5 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com5  
Department of Computational Linguistics4 
University of Heidelberg 
Im Neuenheimer Feld 325 
69120 Heidelberg, Germany 
ekbal@cl.uni-heidelberg.de 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2010 Shared Task on Transliteration 
Generation held as part of ACL 2010. One 
standard run and two non-standard runs were 
submitted for English to Hindi and Bengali 
transliteration while one standard and one non-
standard run were submitted for Kannada and 
Tamil. The transliteration systems are based 
on Orthographic rules and Phoneme based 
technology. The system has been trained on 
the NEWS 2010 Shared Task on Translitera-
tion Generation datasets. For the standard run, 
the system demonstrated mean F-Score values 
of 0.818 for Bengali, 0.714 for Hindi, 0.663 
for Kannada and 0.563 for Tamil. The reported 
mean F-Score values of non-standard runs are 
0.845 and 0.875 for Bengali non-standard run-
1 and 2, 0.752 and 0.739 for Hindi non-
standard run-1 and 2, 0.662 for Kannada non-
standard run-1 and 0.760 for Tamil non-
standard run-1. Non-Standard Run-2 for Ben-
gali has achieved the highest score among all 
the submitted runs. Hindi Non-Standard Run-1 
and Run-2 runs are ranked as the 5th and 6th 
among all submitted Runs. 
1 Introduction 
Transliteration is the method of translating one 
source language word into another target lan-
guage by expressing and preserving the original 
pronunciation in their source language. Thus, the 
central problem in transliteration is predicting the 
pronunciation of the original word. Translitera-
tion between two languages that use the same set 
of alphabets is trivial: the word is left as it is. 
However, for languages those use different al-
phabet sets the names must be transliterated or 
rendered in the target language alphabets. Trans-
literation of words is necessary in many applica-
tions, such as machine translation, corpus align-
ment, cross-language Information Retrieval, in-
formation extraction and automatic lexicon ac-
quisition. In the literature, a number of translite-
ration algorithms are available involving English 
(Li et al, 2004; Vigra and Khudanpur, 2003; Go-
to et al, 2003), European languages (Marino et 
al., 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). The detailed re-
port of our participation in NEWS 2009 could be 
found in (Das et al, 2009).  
One standard run for Bengali (Bengali 
Standard Run: BSR), Hindi (Hindi Standard 
Run: HSR), Kannada (Kannada Standard Run: 
KSR) and Tamil (Tamil Standard Run: TSR) 
were submitted. Two non-standard runs for Eng-
lish to Hindi (Hindi Non-Standard Run 1 & 2: 
HNSR1 & HNSR2) and Bengali (Bengali Non-
Standard Run 1 & 2: BNSR1 & BNSR1) transli-
teration were submitted. Only one non-standard 
run were submitted for Kannada (Kannada Non-
Standard Run-1: KNSR1) and Tamil (Tamil 
Non-Standard Run-1: TNSR1). 
71
2 Machine Transliteration Systems  
Five different transliteration models have been 
proposed in the present report that can generate 
the transliteration in Indian language from an 
English word. The transliteration models are 
named as Trigram Model (Tri), Joint Source-
Channel Model (JSC), Modified Joint Source-
Channel Model (MJSC), Improved Modified 
Joint Source-Channel Model (IMJSC) and Inter-
national Phonetic Alphabet Based Model (IPA). 
Among all the models the first four are catego-
rized as orthographic model and the last one i.e. 
IPA based model is categorized as phoneme 
based model. 
An English word is divided into Translitera-
tion Units (TUs) with patterns C*V*, where C 
represents a consonant and V represents a vowel. 
The targeted words in Indian languages are di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the basic lexical units for machine 
transliteration. The system considers the English 
and Indian languages contextual information in 
the form of collocated TUs simultaneously to 
calculate the plausibility of transliteration from 
each English TU to various Indian languages 
candidate TUs and chooses the one with maxi-
mum probability. The system learns the map-
pings automatically from the bilingual NEWS 
2010 training set being guided by linguistic fea-
tures/knowledge. The output of the mapping 
process is a decision-list classifier with collo-
cated TUs in the source language and their 
equivalent TUs in collocation in the target lan-
guage along with the probability of each decision 
obtained from the training set. A Direct example 
base has been maintained that contains the bilin-
gual training examples that do not result in the 
equal number of TUs in both the source and tar-
get sides during alignment. The Direct example 
base is checked first during machine translitera-
tion of the input English word. If no match is 
obtained, the system uses direct orthographic 
mapping by identifying the equivalent TU in In-
dian languages for each English TU in the input 
and then placing the target language TUs in or-
der. The IPA based model has been used for 
English dictionary words. Words which are not 
present in the dictionary are handled by other 
orthographic models as Trigram, JSC, MJSC and 
IMJSC. 
The transliteration models are described below 
in which S and T denotes the source and the tar-
get words respectively: 
3 Orthographic Transliteration models 
The orthographic models work on the idea of 
TUs from both source and target languages. The 
orthographic models used in the present system 
are described below. For transliteration, P(T), 
i.e., the probability of transliteration in the target 
language, is calculated from a English-Indian 
languages bilingual database If, T is not found in 
the dictionary, then a very small value is 
assigned to P(T). These models have been 
desribed in details in Ekbal et al (2007). 
3.1 Trigram 
This is basically the Trigram model where the 
previous and the next source TUs are considered 
as the context.  
( | ) ( , | )1, 11
K
P S T P s t s sk k kk
= < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.2  Joint Source-Channel Model (JSC) 
This is essentially the Joint Source-Channel 
model (Hazhiou et al, 2004) where the 
previous TUs with reference to the current TUs 
in both the source (s) and the target sides (t) are 
considered as the context.  
( | ) ( , | , )11
K
P S T P s t s tk kk
= < > < >?
?
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.3 Modified Joint Source-Channel Model 
(MJSC) 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the Modified 
Joint Source-Channel model. 
( | ) ( , | , )1, 11
K
P S T P s t s t sk k kk
= < > < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.4 Improved Modified Joint Source-
Channel Model (IMJSC) 
In this model, the previous two and the next TUs 
in the source and the previous target TU are 
considered as the context. This is the  Improved 
Modified Joint Source-Channel model. 
72
( | ) ( , | , )1 1, 11
K
P S T P s t s s t sk k k kk
= < > < >? + ? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
4 International Phonetic Alphabet 
(IPA) Model 
The NEWS 2010 Shared Task on Transliteration 
Generation challenge addresses general domain 
transliteration problem rather than named entity 
transliteration. Due to large number of dictionary 
words as reported in Table 1 in NEWS 2010 data 
set a phoneme based transliteration algorithm  
has been devised.  
 Train Dev Test 
Bengali 7.77% 5.14% 6.46% 
Hindi 27.82% 15.80% 3.7% 
Kannada 27.60% 14.63% 4.4% 
Tamil 27.87% 17.31% 3.0% 
Table 1: Statistics of Dictionary Words 
The International Phonetic Alphabet (IPA) is a 
system of representing phonetic notations based 
primarily on the Latin alphabet and devised by 
the International Phonetic Association as a 
standardized representation of the sounds of 
spoken language. The machine-readable 
Carnegie Mellon Pronouncing Dictionary 1  has 
been used as an external resource to capture 
source language IPA structure. The dictionary 
contains over 125,000 words and their 
transcriptions with mappings from words to their 
pronunciations in the given phoneme set. The 
current phoneme set contains 39 distinct 
phonemes. As there is no such parallel IPA 
dictionary available for Indian languages, 
English IPA structures have been mapped to TUs 
in Indian languages during training. An example 
of such mapping between phonemes and TUs are 
shown in Table 3, for which the vowels may 
carry lexical stress as reported in Table 2. This 
phone set is based on the ARPAbet2 symbol set 
developed for speech recognition uses.  
Representation Stress level 
0 No 
1 Primary 
2 Secondary 
Table 2: Stress Level on Vowel 
A pre-processing module checks whether a 
targeted source English word is a valid 
dictionary word or not. The dictionary words are 
then handled by phoneme based transliteration 
module. 
                                                 
1
 www.speech.cs.cmu.edu/cgi-bin/cmudict 
2
 http://en.wikipedia.org/wiki/Arpabet 
Phoneme Example Translation TUs 
AA odd AA0-D - 
AH hut HH0-AH-T - 
D dee D-IY1 -?	 
Table 3: Phoneme Map Patterns of English 
Words and TUs 
In the target side we use our TU segregation 
logic to get phoneme wise transliteration pattern. 
We present this problem as a sequence labelling 
problem, because transliteration pattern changes 
depending upon the contextual phonemes in 
source side and TUs in the target side. We use a 
standard machine learning based sequence 
labeller Conditional Random Field (CRF)3 here. 
IPA based model increased the performance 
for Bengali, Hindi and Tamil languages as 
reported in Section 6. The performance has 
decreased for Kannada. 
5 Ranking 
The ranking among the transliterated outputs 
follow the order reported in Table 4: The ranking 
decision is based on the experiments as described 
in (Ekbal et al, 2006) and additionally based on 
the experiments on NEWS 2010 development 
dataset. 
Word Type  Ranking Order 1 2 3 4 5 
Dictionary IPA IMJSC MJSC JSC Tri 
Non-
Dictionary IMJSC MJSC JSC Tri - 
Table 4: Phoneme Patterns of English Words 
In BSR, HSR, KSR and TSR the orthographic 
TU based models such as: IMJSC, MJSC, JSC 
and Tri have been used only trained by NEWS 
2010 dataset. In BNSR1 and HNSR1 all the or-
thographic models have been trained with addi-
tional census dataset as described in Section 6. In 
case of BNSR2, HNSR2, KNSR1 and TNSR1 
the output of the IPA based model has been add-
ed with highest priority. As no census data is 
available for Kannada and Tamil therefore there 
is only one Non-Standard Run was submitted for 
these two languages only with the output of IPA 
based model along with the output of Standard 
Run. 
6 Experimental Results  
We have trained our transliteration models using 
the NEWS 2010 datasets obtained from the 
NEWS 2010 Machine Transliteration Shared 
Task (Li et al, 2010). A brief statistics of the 
                                                 
3
 http://crfpp.sourceforge.net 
73
datasets are presented in Table 5. During train-
ing, we have split multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in various multi-words and 
these cases were not considered further. Follow-
ing are some examples:  
Paris Charles de Gaulle  ???? 
???? ??	?
 ? ?????  
Suven Life Scie  ??? ??
	??
 
Delta Air Lines  ???? 
???	
 
In the training set, some multi-words were 
partly translated and not transliterated. Such ex-
amples were dropped from the training set. In the 
following example the English word ?National? 
is being translated in the target as ??????. 
Australian National Univer-
sity  ????? ???? 
?Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 37?45,
Beijing, August 2010
Automatic Extraction of Complex Predicates in Bengali  
Dipankar Das     Santanu Pal      Tapabrata Mondal       Tanmoy Chakraborty   
 
  Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
dipankar.dipnil2005@gmail.com, 
santanupersonal1@gmail.com, 
tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in, 
sivaji_cse_ju@yahho.com 
 
 
Abstract 
This paper presents the automatic ex-
traction of Complex Predicates (CPs) 
in Bengali with a special focus on 
compound verbs (Verb + Verb) and 
conjunct verbs (Noun /Adjective + 
Verb). The lexical patterns of com-
pound and conjunct verbs are extracted 
based on the information of shallow 
morphology and available seed lists of 
verbs. Lexical scopes of compound and 
conjunct verbs in consecutive sequence 
of Complex Predicates (CPs) have 
been identified. The fine-grained error 
analysis through confusion matrix 
highlights some insufficiencies of lexi-
cal patterns and the impacts of different 
constraints that are used to identify the 
Complex Predicates (CPs). System 
achieves F-Scores of 75.73%, and 
77.92% for compound verbs and 
89.90% and 89.66% for conjunct verbs 
respectively on two types of Bengali 
corpus.      
1 Introduction 
Complex Predicates (CPs) contain [verb] + 
verb (compound verbs) or [noun/ 
adjective/adverb] +verb (conjunct verbs) 
combinations in South Asian languages (Hook, 
1974). To the best of our knowledge, Bengali  
 
 
is not only a language of South Asia but also 
the sixth popular language in the World 1 , 
second in India and the national language of 
Bangladesh. The identification of Complex 
Predicates (CPs) adds values for building 
lexical resources (e.g. WordNet (Miller et al, 
1990; VerbNet (Kipper-Schuler, 2005)), 
parsing strategies and machine translation 
systems.  
Bengali is less computerized compared to 
English due to its morphological enrichment. 
As the identification of Complex Predicates 
(CPs) requires the knowledge of morphology, 
the task of automatically extracting the Com-
plex Predicates (CPs) is a challenge. Complex 
Predicates (CPs) in Bengali consists of two 
types, compound verbs (CompVs) and conjunct 
verbs (ConjVs). 
The compound verbs (CompVs) (e.g. ? ?? 
? ?? mere phela ?kill?, ???? ???? bolte laglo 
?started saying?) consist of two verbs. The first 
verb is termed as Full Verb (FV) that is present 
at surface level either as conjunctive participial 
form -e ?e or the infinitive form -?  ?te. The 
second verb bears the inflection based on 
Tense, Aspect and Person. The second verbs 
that are termed as Light Verbs (LV) are 
polysemous, semantically bleached and 
confined into some definite candidate seeds 
(Paul, 2010).  
On the other hand, each of the Bengali con-
junct verbs (ConjVs) (e.g. ???? ??? bharsha 
                                                 
1http://www.ethnologue.com/ethno_docs/distributio
n.asp?by=size 
37
kara ?to depend?, ???? ??? jhakjhak kara ?to 
glow?) consists of noun or adjective followed 
by a Light Verb (LV). The Light Verbs (LVs) 
bear the appropriate inflections based on 
Tense, Aspect and Person.   
According to the definition of multi-word 
expressions (MWEs)(Baldwin and Kim, 2010), 
the absence of conventional meaning of the 
Light Verbs in Complex Predicates (CPs) 
entails us to consider the Complex Predicates 
(CPs) as MWEs (Sinha, 2009). But, there are 
some typical examples of Complex Predicates 
(CPs), e.g. ? ?? ??? dekha kara ?see-do? that 
bear the similar lexical pattern as Full Verb 
(FV)+ Light Verb (LV) but both of the Full 
Verb (FV) and Light Verb (LV) loose their 
conventional meanings and generate a 
completely different meaning (?to meet? in this 
case).  
In addition to that, other types of predicates 
such as ???? ? ? niye gelo ?take-go? (took and 
went), ???? ? ? diye gelo ?give-go? (gave and 
went) follows the similar lexical patterns 
FV+LV as of Complex Predicates (CPs) but 
they are not mono-clausal. Both the Full Verb 
(FV) and Light Verb (LV) behave like 
independent syntactic entities and they belong 
to non-Complex Predicates (non-CPs). The 
verbs are also termed as Serial Verb (SV) 
(Mukherjee et al, 2006). 
Butt (1993) and Paul (2004) have also 
mentioned the following criteria that are used 
to check the validity of complex predicates 
(CPs) in Bengali. The following cases are the 
invalid criteria of complex predicates (CPs). 
1. Control Construction (CC): ????? ??? 
likhte bollo ?asked to write?, ????? ???? 
??? likhte badhyo korlo ?forced to 
write? 
2. Modal Control Construction (MCC): 
? ?? ??? jete hobe ?have to go? ? ?? ??? 
khete hobe ?have to eat? 
3. Passives (Pass) : ??? ??? dhora porlo 
?was caught?, ???? ?? mara holo ?was 
beaten? 
4. Auxiliary Construction (AC): ??? ??? 
bose ache ?is sitting?, ???? ??? niye chilo 
?had taken?. 
Sometimes, the successive sequence of the 
Complex Predicates (CPs) shows a problem of 
deciding the scopes of individual Complex 
Predicates (CPs) present in that sequence. For 
example the sequence, u?? ??? ? ???? uthe pore 
dekhlam ?rise-wear-see? (rose and saw) seems 
to contain two Complex Predicates (CPs) (u?? 
??? uthe pore ?rose? and ??? ? ???? pore 
dekhlam ?wore and see?). But there is actually 
one Complex Predicate (CP). The first one u?? 
??? uthe pore ?rose? is a compound verb 
(CompV) as well as a Complex Predicate (CP). 
Another one is ? ???? dekhlam ?saw? that is a 
simple verb. As the sequence is not mono-
clausal, the Complex Predicate (CP) u?? ??? 
uthe pore ?rose? associated with ? ???? dekhlam 
?saw? is to be separated by a lexical boundary. 
Thus the determination of lexical scopes of 
Complex Predicates (CPs) from a long con-
secutive sequence is indeed a crucial task.      
 The present task therefore not only aims to 
extract the Complex Predicates (CPs) 
containing compound and conjunct verbs but 
also to resolve the problem of deciding the 
lexical scopes automatically. The compound 
verbs (CompVs) and conjunct verbs (ConjVs) 
are extracted from two separate Bengali 
corpora based on the morphological 
information (e.g. participle forms, infinitive 
forms and inflections) and list of Light Verbs 
(LVs). As the Light Verbs (LVs) in the 
compound verbs (CompVs) are limited in 
number, fifteen predefined verbs (Paul, 2010) 
are chosen as Light Verbs (LVs) for framing 
the compound verbs (CompVs).  A manually 
prepared seed list that is used to frame the 
lexical patterns for conjunct verbs (ConjVs) 
contains frequently used Light Verbs (LVs).  
An automatic method is designed to identify 
the lexical scopes of compound and conjunct 
verbs in the long sequences of Complex 
Predicates (CPs). The identification of lexical 
scope of the Complex Predicates (CPs) 
improves the performance of the system as the 
number of identified Complex Predicates 
(CPs) increases.  
Manual evaluation is carried out on two 
types of Bengali corpus. The experiments are 
carried out on 800 development sentences 
from two corpora but the final evaluation is 
carried out on 1000 sentences. Overall, the 
system achieves F-Scores of 75.73%, and 
77.92% for compound verbs and 89.90% and 
89.66% for conjunct verbs respectively.  
38
The error analysis shows that not only the 
lexical patterns but also the augmentation of 
argument structure agreement (Das, 2009), the 
analysis of Non-MonoClausal Verb (NMCV) or 
Serial Verb, Control Construction (CC), 
Modal Control Construction (MCC), Passives 
(Pass) and Auxiliary Construction (AC) (Butt, 
1993; Paul, 2004) are also necessary to 
identify the Complex Predicates (CPs). The 
error analysis shows that the system suffers in 
distinguishing the Complex Predicates (CPs) 
from the above constraint constructions.  
The rest of the paper is organized as fol-
lows. Section 2 describes the related work 
done in this area. The automatic extraction of 
compound and conjunct verbs is described in 
Section 3. In Section 4, the identification of 
lexical scopes of the Complex Predicates 
(CPs) is mentioned. Section 5 discusses the 
results of evaluation along with error analysis. 
Finally, Section 6 concludes the paper. 
2 Related Work 
The general theory of complex predicate is 
discussed in Alsina (1996). Several attempts 
have been organized to identify complex 
predicates in South Asian languages (Abbi, 
1991; Bashir, 1993; Verma, 1993) with a spe-
cial focus to Hindi (Burton-Page, 1957; Hook, 
1974), Urdu (Butt, 1995), Bengali (Sarkar, 
1975; Paul, 2004), Kashmiri (Kaul, 1985) and 
Oriya (Mohanty, 1992). But the automatic ex-
traction of Complex Predicates (CPs) has been 
carried out for few languages, especially 
Hindi. 
The task described in (Mukherjee et al, 
2006) highlights the development of a database 
based on the hypothesis that an English verb is 
projected onto a multi-word sequence in Hindi. 
The simple idea of projecting POS tags across 
an English-Hindi parallel corpus considers the 
Complex Predicate types, adjective-verb (AV), 
noun-verb (NV), adverb-verb (Adv-V), and 
verb-verb (VV) composites. A similar task 
(Sinha, 2009) presents a simple method for 
detecting Complex Predicates of all kinds us-
ing a Hindi-English parallel corpus. His simple 
strategy exploits the fact that Complex Predi-
cate is a multi-word expression with a meaning 
that is distinct from the meaning of the Light 
Verb. In contrast, the present task carries the 
identification of Complex Predicates (CPs) 
from monolingual Bengali corpus based on 
morphological information and lexical pat-
terns. 
The analysis of V+V complex predicates 
termed as lexical compound verbs (LCpdVs) 
and the linguistic tests for their detection in 
Hindi are described in (Chakrabarti et al, 
2008). In addition to compound verbs, the pre-
sent system also identifies the conjunct verbs 
in Bengali. But, it was observed that the identi-
fication of Hindi conjunct verbs that contain 
noun in the first slot is puzzling and therefore a 
sophisticated solution was proposed in (Das, 
2009) based on the control agreement strategy 
with other overtly case marked noun phrases. 
The present task also agrees with the above 
problem in identifying conjunct verbs in Ben-
gali although the system satisfactorily identi-
fies the conjunct verbs (ConjVs). 
Paul (2003) develops a constraint-based 
mechanism within HPSG framework for com-
posing Indo-Aryan compound verb construc-
tions with special focus on Bangla (Bengali) 
compound verb sequences. Postulating seman-
tic relation of compound verbs, another work 
(Paul, 2009) proposed a solution of providing 
lexical link between the Full verb and Light 
Verb to store the Compound Verbs in Indo 
WordNet without any loss of generalization. 
To the best of our knowledge, ours is the first 
attempt at automatic extraction of Complex 
Predicates (CPs) in Bengali.  
3 Identification of Complex Predi-
cates (CPs) 
The compound verbs (CompVs) and conjunct 
verbs (ConjVs) are identified from the shallow 
parsed result using a lexical pattern matching 
technique. 
3.1 Preparation of Corpora 
Two types of Bengali corpus have been con-
sidered to carry out the present task. One cor-
pus is collected from a travel and tourism do-
main and another from an online web archive 
of Rabindranath Rachanabali 2 . Rabindra 
Rachanabali corpus is a large collection of 
short stories of Rabindranath Tagore. The for-
                                                 
2 www.rabindra-rachanabali.nltr.org 
39
mer EILMT travel and tourism corpus is ob-
tained from the consortium mode project ?De-
velopment of English to Indian Languages 
Machine Translation (EILMT 3) System?. The 
second type of corpus is retrieved from the 
web archive and pre-processed accordingly. 
Each of the Bengali corpora contains 400 and 
500 development and test sentences respec-
tively.   
The sentences are passed through an open 
source Bengali shallow parser 4. The shallow 
parser gives different morphological informa-
tion (root, lexical category of the root, gender, 
number, person, case, vibhakti, tam, suffixes 
etc.) that help in identifying the lexical patterns 
of Complex Predicates (CPs).  
3.2 Extracting Complex Predicates (CPs) 
Manual observation shows that the Complex 
Predicates (CPs) contain the lexical pattern 
{[XXX] (n/adj) [YYY] (v)} in the shallow 
parsed sentences where XXX and YYY repre-
sent any word. But, the lexical category of the 
root word of XXX is either noun (n) or adjec-
tive (adj) and the lexical category of the root 
word of YYY is verb (v). The shallow parsed 
sentences are pre-processed to generate the 
simplified patterns. An example of similar 
lexical pattern of the shallow parsed result and 
its simplified output is shown in Figure 1.  
 
((NP  a????  NN  <fs 
?f='a???? ,n,,sg,,d,?? ? ,?? ? '>  ))              
   
((VGF  ???????      VM       <fs 
?f='?r,v,,,5,,? ,? '>     )) 
a???? |no?n|a???? /NN/NP/ 
(a???? ^n^*^sg^*^d^?? ^?? ? )_ 
???????|v??b|???????/VM/VGF/              
(?r^v^*^*^1^*^? ^? ) 
         
 Figure 1. Example of a pre-processed shallow 
parsed result. 
 
                                                 
3 The EILMT project is funded by the Department of 
Information Technology (DIT), Ministry of Communica-
tions and Information Technology (MCIT), Government 
of India. 
4http://ltrc.iiit.ac.in/showfile.php?filename=downloads/sh
allow_parser.php 
The corresponding lexical categories of the 
root words a???? adhyan ?study? (e.g. noun 
for ?n?) and '?r  kar, ?do? (e.g. verb for ?v?) are 
shown in bold face in Figure 1. The f ollowing 
example is of conjunct verb (ConjV).  
The extraction of Bengali compound verbs 
(CompVs) is straightforward rather than con-
junct verbs (ConjVs). The lexical pattern of 
compound verb is {[XXX](v) [YYY] (v)} where 
the lexical or basic POS categories of the root 
words of  ?XXX? and ?YYY? are only verb. If 
the basic POS tags of the root forms of ?XXX? 
and ?YYY? are verbs (v) in shallow parsed sen-
tences, then only the corresponding lexical 
patterns are considered as the probable candi-
dates of compound verbs (CompVs).  
Example 1: 
??i??|v??b|??i??/VM/VGNF/? ? ^v^*^*^?ny^*^i??^i??)
#??????|v??b|??????/VM/VGF/(??^v^*^*^1^*^?^?) 
Example 1 is a compound verb (CompV) but 
Example 2 is not. In Example 2, the lexical 
category or the basic POS of the Full Verb 
(FV) is noun (n) and hence the pattern is dis-
carded as non-compound verb (non-CompV). 
Example 2: 
?k? |noun|?k? /NN/NP/(?k? ^n^*^*^*^*^*^pos
lcat="NM") #  
?????|verb|?????/VM/VGNF/(?r^v^*^*^any^*^i??
^i??) 
Bengali, like any other Indian languages, is 
morphologically very rich. Different suffixes 
may be attached to a Light Verb (LVs) (in this 
case [YYY]) depending on the various features 
such as Tense, Aspect, and Person.  
In case of extracting compound verbs 
(CompVs), the Light Verbs are identified from 
a seed list (Paul, 2004). The list of Light Verbs 
is specified in Table 1. The dictionary forms of 
the Light Verbs are stored in this list. As the 
Light Verbs contain different suffixes, the pri-
mary task is to identify the root forms of the 
Light Verbs (LVs) from shallow parsed result. 
Another table that stores the root forms and the 
corresponding dictionary forms of the Light 
Verbs is used in the present task. The table 
contains a total number of 378 verb entries 
including Full Verbs (FVs) and Light Verbs 
(LVs). The dictionary forms of the Light Verbs 
(LVs) are retrieved from the Table. 
On the other hand, the conjunctive particip-
ial form -e/i?? -e/iya or the infinitive form -
? /i?? ?te/ite are attached with the Full Verbs 
40
(FVs) (in this case [XXX]) in compound verbs 
(CompVs). i?? / iya and i??/ ite are also used 
for conjunctive participial form -e ?e or the 
infinitive form -?  ?te respectively in litera-
ture. The participial and infinitive forms are 
checked based on the morphological informa-
tion (e.g. suffixes of the verb) given in the 
shallow parsed results. In Example 1, the Full 
Verb (FV) contains -i?? -iya suffix. If the dic-
tionary forms of the Light Verbs (LVs) are pre-
sent in the list of Light Verbs and the Full 
Verbs (FVs) contain the suffixes of -e/i?? -
e/iya or ? /i?? ?te/ite, both verbs are combined 
to frame the patterns of compound verbs 
(CompVs). 
 
aSa ?come?          d?Ra ?stand? 
rakha ?keep?       ana ?bring?  
deoya ?give?        pOra ?fall?  
paTha ?send?       bERano ?roam? 
neoya ?take?        tola ?lift? 
bOSa ?sit?           oTha ?rise? 
jaoya ?go?           chaRa ?leave? 
phEla ?drop?       mOra ?die? 
 
Table 1. List of Light Verbs for compound 
verbs. 
The identification of conjunct verbs 
(ConjVs) requires the lexical pattern (Noun / 
Adjective + Light Verb) where a noun or an 
adjective is followed by a Light Verb (LV). The 
dictionary forms of the Light Verbs (LVs) that 
are frequently used as conjunct verbs (ConjVs) 
are prepared manually. The list of Light Verbs 
(LVs) is given in Table 2. The detection of 
Light Verbs (LVs) for conjunct verbs (ConjVs) 
is similar to the detection of the Light Verbs 
(LVs) for compound verbs (CompVs) as de-
scribed earlier in this section.  If the basic POS 
of the root of the first words ([XXX]) is either 
?noun? or ?adj? (n/adj) and the basic POS of 
the following word ([YYY]) is ?verb? (v), the 
patterns are considered as conjunct verbs 
(ConjVs). The Example 2 is an example of 
conjunct verb (ConjV). 
For example, ???? ??? (jhakjhak kara ?to 
glow?), ???? ??? (taktak ?to glow?), ?? ??? ??? 
(chupchap kara ?to silent?) etc are identified as 
conjunct verbs (ConjVs) where the basic POS 
of the former word is an adjective (adj) fol-
lowed by ??? kara ?to do?, a common Light 
Verb.  
deoya ?give?        kara  ?do?    
neoya ?take?        laga  ?start?            
paoya ?pay?         kata  ?cut?   
  
Table 2. List of Light Verbs for conjunct verbs. 
 
Example 3:  
  ????|?dj|???? /JJ/JJP/(???? ^?dj) # 
????|v??b|????/VM/VGF/(?r^v^*^*^5^*^?^?) 
But, the extraction of conjunct verbs 
(ConjVs) that have a ?noun+verb? construction 
is descriptively and theoretically puzzling 
(Das, 2009). The identification of lexical pat-
terns is not sufficient to recognize the com-
pound verbs (CompVs). For example, ?i ? ??? 
boi deoya ?give book? and ???? ? ??? bharsa 
deyoa ?to assure? both contain similar lexical 
pattern (noun+verb) and same Light Verb ? ??? 
deyoa. But, ???? ? ??? bharsa deyoa ?to assure? 
is a conjunct verb (ConjV) whereas ?i ? ??? boi 
deoya ?give book? is not a conjunct verb 
(ConjV). Linguistic observation shows that the 
inclusion of this typical category into conjunct 
verbs (ConjVs) requires the additional knowl-
edge of syntax and semantics.  
In connection to conjunct verbs (ConjVs), 
(Mohanty, 2010) defines two types of conjunct 
verbs (ConjVs), synthetic and analytic. A syn-
thetic conjunct verb is one in which both the 
constituents form an inseparable whole from 
the semantic point of view or semantically 
non-compositional in nature. On the other 
hand, an analytic conjunct verb is semantically 
compositional. Hence, the identification of 
conjunct verbs requires knowledge of seman-
tics rather than only the lexical patterns. 
It is to be mentioned that sometimes, the 
negative markers (?? no, ??i nai) are attached 
with the Light Verbs u?????  uthona ?do not get 
up? ? ?????  phelona ?do not throw?. Negative 
attachments are also considered in the present 
task while checking the suffixes of Light Verbs 
(LVs). 
4 Identification of Lexical Scope for 
Complex Predicates (CPs) 
The identification of lexical scopes of the 
Complex Predicates (CPs) from their succes-
sive sequences shows that multiple Complex 
41
Predicates (CPs) can occur in a long sequence. 
An automatic method is employed to identify 
the Complex Predicates (CPs) along with their 
lexical scopes. The lexical category or basic 
POS tags are obtained from the parsed sen-
tences. 
If the compound and conjunct verbs occur 
successively in a sequence, the left most two 
successive tokens are chosen to construct the 
Complex Predicate (CP). If successive verbs 
are present in a sequence and the dictionary 
form of the second verb reveals that the verb is 
present in the lists of compound Light Verbs 
(LV), then that Light Verb (LV) may be a part 
of a compound verb (CompV). For that reason, 
the immediate previous word token is chosen 
and tested for its basic POS in the parsed result. 
If the basic POS of the previous word is ?verb 
(v)? and any suffixes of either conjunctive par-
ticipial form -e/i?? -e/iya or the infinitive form 
-? /i?? ?te/ite is attached to the previous verb, 
the two successive verbs are grouped together 
to form a compound verb (CompV) and the 
lexical scope is fixed for the Complex Predi-
cate (CP).  
If the previous verb does not contain -e/i?? 
-e/iya or -? /i?? ?te/ite inflections, no com-
pound verb (CompV) is framed with these two 
verbs. But, the second Light Verb (LV) may be 
a part of another Complex Predicate (CP). This 
Light Verb (LV) is now considered as the Full 
Verb (FV) and its immediate next verb is 
searched in the list of compound Light Verbs 
(LVs) and the formation of compound verbs 
(CompVs) progresses similarly.  If the verb is 
not in the list of compound Light Verbs, the 
search begins by considering the present verb 
as Full Verb (FV) and the search goes in a 
similar way. 
The following examples are given to illus-
trate the formation of compound verbs 
(CompVs) and find the lexical scopes of the 
compound verbs (CompVs). 
 
???      ????       ????     ???      ? ??? 
(ami)       (chalte)      (giye)    (pore)    (gelam). 
I <fell down while walking>. 
 
Here, ?chalte giye pore gelam? is a verb 
group. The two left most verbs ???? ???? chalte 
giye are picked and the dictionary form of the 
second verb is searched in the list of com-
pound Light Verbs. As the dictionary form 
(jaoya ?go?) of the verb ???? giye is present in 
the list of compound Light Verbs (as shown in 
Table 1), the immediate previous verb ???? 
chalte is checked for inflections -e/i?? -e/iya 
or -? /i?? ?te/ite.  As the verb ???? chalte con-
tains the inflection -?  -te , the verb group ???? 
???? chalte giye is a compound verb (CompV) 
where ???? giye is a Light Verb and ???? chalte 
is the Full Verb with inflection (-?  -te).  Next 
verb group, ???   ? ??? pore gelam is identified 
as compound verb (CompV) in a similar way 
(??+ (-e) por+ (-e) + ? ??? gelam (jaoya ?go?)).  
Another example is given as follows.  
 
???    u??      ???       ? ????       ?   
(ami)   (uthe)      (pore)      (dekhlam)    (je)  
?? ??      e????        ? i 
(tumi)     (ekhane)       (nei) 
I <get up and saw> that you are not here 
 
Here, u?? ??? ? ???? uthe pore dekhlam is 
another verb group. The immediate next verb 
of u?? uthe is ??? pore that is chosen and its 
dictionary form is searched in the list of com-
pound Light Verbs (LV) similarly. As the dic-
tionary form (???  pOra) of the verb ??? pore 
is present in the list of Light Verbs and the 
verb u??   uthe contains the inflection -e ?e, 
the consecutive verbs frame a compound verb 
(CompV) u?? ??? where u?? uthe is a Full Verb 
with inflection -e ?e and ??? pore is a Light 
Verb. The final verb ? ????      dekhlam is 
chosen and as there is no other verb present, 
the verb ? ???? dekhlam is excluded from any 
formation of compound verb (CompV) by con-
sidering it as a simple verb.  
Similar technique is adopted for identifying 
the lexical scopes of conjunct verbs (ConjVs). 
The method seems to be a simple pattern 
matching technique in a left-to-right fashion 
but it helps in case of conjunct verbs (ConjVs). 
As the noun or adjective occur in the first slot 
of conjunct verbs (ConjVs) construction, the 
search starts from the point of noun or adjec-
tive. If the basic POS of a current token is ei-
ther ?noun? or ?adjective? and the dictionary 
form of the next token with the basic POS 
?verb (v)? is in the list of conjunct Light Verbs 
(LVs), then the two consecutive tokens are 
42
combined to frame the pattern of a conjunct 
verb (ConjV). 
For example, the identification of lexical 
scope of a conjunct verb (ConjV) from a se-
quence such as u????  ???? ? ??? uparjon korte 
gelam ?earn-do-go? (went to earn) identifies 
the conjunct verb (ConjV) u????  ???? uparjon 
korte. There is another verb group ???? ? ??? 
korte gelam that seems to be a compound verb 
(CompV) but is excluded by considering ?? ??? 
gelam as a simple verb. 
5 Evaluation 
The system is tested on 800 development sen-
tences and finally applied on a collection of 
500 sentences from each of the two Bengali 
corpora. As there is no annotated corpus avail-
able for evaluating Complex Predicates (CPs), 
the manual evaluation of total 1000 sentences 
from the two corpora is carried out in the pre-
sent task.  
The recall, precision and F-Score are con-
sidered as the standard metrics for the present 
evaluation. The extracted Complex Predicates 
(CPs) contain compound verb (CompV) and 
conjunct verbs (ConjVs). Hence, the metrics 
are measured for both types of verbs individu-
ally. The separate results for two separate cor-
pora are shown in Table 3 and Table 4 respec-
tively. The results show that the system identi-
fies the Complex Predicates (CPs) satisfacto-
rily from both of the corpus. In case of Com-
pound Verbs (CompVs), the precision value is 
higher than the recall. The lower recall value 
of Compound Verbs (CompVs) signifies that 
the system fails to capture the other instances 
from overlapping sequences as well as non-
Complex predicates (non-CPs).  
But, it is observed that the identification of 
lexical scopes of compound verbs (CompVs) 
and conjunct verbs (ConjVs) from long se-
quence of successive Complex Predicates 
(CPs) increases the number of Complex Predi-
cates (CPs) entries along with compound verbs 
(CompVs) and conjunct verbs (ConjVs). The 
figures shown in bold face in Table 3 and Ta-
ble 4 for the Travel and Tourism corpus and 
Short Story corpus of Rabindranath Tagore 
indicates the improvement of identifying lexi-
cal scopes of the Complex Predicates (CPs).  
In comparison to other similar language 
such as Hindi (Mukerjee et al, 2006) (the re-
ported precision and recall are 83% and 46% 
respectively), our results (84.66% precision 
and 83.67% recall) are higher in case of ex-
tracting Complex Predicates (CPs). The reason 
may be of resolving the lexical scope and han-
dling the morphosyntactic features using shal-
low parser.  
In addition to Non-MonoClausal Verb 
(NMCV) or Serial Verb, the other criteria 
(Butt, 1993; Paul, 2004) are used in our pre-
sent diagnostic tests to identify the complex 
predicates (CPs). The frequencies of 
Compound Verb (CompV), Conjunct Verb 
(ConjV) and the instances of other constraints 
of non Complex Predicates (non-CPs) are 
shown in Figure 2. It is observed that the num-
bers of instances of Conjunct Verb (ConjV), 
Passives (Pass), Auxiliary Construction (AC) 
and Non-MonoClausal Verb (NMCV) or Serial 
Verb are comparatively high than other in-
stances in both of the corpus. 
 
EILMT  Recall Precision F-
Score 
Compound  
Verb 
(CompV) 
65.92% 
70.31% 
 
80.11% 
82.06% 
72.32% 
75.73%
Conjunct 
Verb 
(ConjV) 
94.65% 
96.96% 
80.44% 
83.82% 
86.96% 
89.90%
 
Table 3. Recall, Precision and F-Score of the 
system for acquiring the CompVs and ConjVs 
from EILMT Travel and Tourism Corpus. 
 
Rabindra 
Rachana-
bali 
Recall Precision F-
Score 
Compound  
Verb 
(CompV) 
68.75% 
72.22% 
 
81.81% 
84.61% 
74.71% 
77.92%
Conjunct 
Verb 
(ConjV) 
94.11% 
95.23% 
83.92% 
84.71% 
88.72% 
89.66%
 
Table 4. Recall, Precision and F-Score of the 
system for acquiring the CompVs and ConjVs 
from Rabindra Rachanabali corpus. 
 
43
 
 CompV ConjV NMCV CC MCC Pass AC 
CompV 0.76 0.00 0.02 0.00 0.00 0.03 0.02 
ConjV 0.04 0.72 0.03 0.01 0.02 0.02 0.02 
NMCV 0.17 0.18 0.65 0.00 0.02 0.02 0.02 
CC 0.01 0.00 0.00 0.56 0.01 0.02 0.02 
MCC 0.00 0.00 0.00 0.07 0.65 0.00 0.02 
Pass 0.12 0.01 0.00 0.00 0.00 0.78 0.00 
AC 0.06 0.07 0.04 0.00 0.00 0.08 0.54 
Table 5. Confusion Matrix for CPs and constraints of non-CPs (in %).  
 
0
50
100
150
200
CompV
ConjV
CC MCC
Pass
AC NMCV
EILMTRabindra
  
Figure 2. The frequencies of Complex Predi-
cates (CPs) and different constrains of non-
Complex Predicates (non-CPs). 
 
The error analysis is conducted on both of 
the corpus. Considering both corpora as a 
whole single corpus, the confusion matrix is 
developed and shown in Table 5. The bold face 
figures in Table 5 indicate that the percentages 
of non-Complex Predicates (non-CPs) such as 
Non-MonoClausal Verbs (NMCV), Passives 
(Pass) and Auxiliary Construction (AC) that 
are identified as compound verbs (CompVs). 
The reason is the frequencies of the non-
Complex Predicates (non-CPs) that are rea-
sonably higher in the corpus.  In case of con-
junct verbs (ConjVs), the Non-MonoClausal 
Verbs (NMCV) and Auxiliary Construction 
(AC) occur as conjunct verbs (ConjVs).  The 
system also suffers from clausal detection that 
is not attempted in the present task. The Pas-
sives (Pass) and Auxiliary Construction (AC) 
requires the knowledge of semantics with ar-
gument structure knowledge. 
6 Conclusion 
In this paper, we have presented a study of 
Bengali Complex Predicates (CPs) with a spe-
cial focus on compound verbs, proposed auto-
matic methods for their extraction from a cor-
pus and diagnostic tests for their evaluation. 
The problem arises in case of distinguishing 
Complex Predicates (CPs) from Non-Mono-
Clausal verbs, as only the lexical patterns are 
insufficient to identify the verbs. In future task, 
the subcategorization frames or argument 
structures of the sentences are to be identified 
for solving the issues related to the errors of 
the present system.  
References 
Abbi, Anvita. 1991. Semantics of Explicator Com-
pound Verbs. In South Asian Languages, Lan-
guage Sciences, 13(2): 161-180. 
Alsina, Alex. 1996. Complex Predicates: Structure 
and Theory. Center for the Study of Language 
and Information Publications, Stanford, CA. 
Bashir, Elena. 1993. Causal chains and compound 
verbs. In M. K. Verma ed. (1993) Complex 
Predicates in South Asian Languages, Manohar 
Publishers and Distributors, New Delhi. 
Burton-Page, John. 1957. Compound and conjunct 
verbs in Hindi. Bulletin of the School of Oriental 
and African Studies, 19: 469-78. 
Butt, Miriam. 1995. The Structure of Complex 
Predicates in Urdu. Doctoral Dissertation, Stan-
ford University. 
Chakrabarti, Debasri, Mandalia Hemang, Priya 
Ritwik, Sarma Vaijayanthi, Bhattacharyya Push-
pak. 2008. Hindi Compound Verbs and their 
Automatic Extraction. International Conference 
on Computational Linguistics ?2008, pp. 27-30. 
44
Das, Pradeep Kumar. 2009. The form and function 
of Conjunct verb construction in Hindi. Global 
Association of Indo-ASEAN Studies, Daejeon, 
South Korea. 
Hook, Peter. 1974. The Compound Verbs in Hindi. 
The Michigan Series in South and South-east 
Asian Language and Linguistics. The University 
of Michigan. 
Kaul, Vijay Kumar. 1985. The Compound Verb in 
Kashmiri. Unpublished Ph.D. dissertation. Ku-
rukshetra University. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad-
coverage,  comprehensive verb lexicon. Ph.D. 
thesis, Computer and Information Science Dept., 
University of Pennsylvania, Philadelphia,PA  
Miller, George, Richard Beckwith, Christiane Fell-
baum, Derek Gross and Katherine Miller. 1990. 
Five Papers on WordNet. CSL Report 43, Cogni-
tive Science Laboratory, Princeton University, 
Princeton. 
Mohanty, Gopabandhu. 1992. The Compound 
Verbs in Oriya. Ph. D. dissertation, Deccan Col-
lege Post-Graduate and Research Institute, Pune. 
Mohanty, Panchanan. 2010. WordNets for Indian 
Languages: Some Issues. Global WordNet Con-
ference-2010, pp. 57-64. 
Mukherjee, Amitabha, Soni Ankit and Raina Achla 
M. 2006. Detecting Complex Predicates in Hindi 
using POS Projection across Parallel Corpora. 
Multiword Expressions: Identifying and Exploit-
ing Underlying Properties Association for Com-
putational Linguistics, pp. 28?35, Sydney.  
Paul, Soma. 2010. Representing Compound Verbs 
in Indo WordNet. Golbal Wordnet Conference-
2010, pp. 84-91. 
Paul, Soma. 2004. An HPSG Account of Bangla 
Compound Verbs with LKB Implementation. 
Ph.D dissertation, University of Hyderabad, Hy-
derabad. 
Paul, Soma. 2003. Composition of Compound 
Verbs in Bangla. Multi-Verb constructions. 
Trondheim  Summer School. 
Sarkar, Pabitra. 1975. Aspects of Compound Verbs 
in Bengali. Unpublished M.A. dissertation, Chi-
cago University. 
Sinha, R. Mahesh, K. 2009. Mining Complex 
Predicates In Hindi Using A Parallel Hindi-
English Corpus. Multiword Expression Work-
shop, Association of Computational Linguistics-
International Joint Conference on Natural Lan-
guage Processing-2009, pp. 40-46, Singapore. 
Timothy, Baldwin, Su Nam Kim. 2010. Multiword 
Expressions. In Nitin Indurkhya and Fred J. 
Damerau (eds.) Handbook of Natural Language 
Processing, Second Edition, Chapman & 
Hall/CRC, London, UK, pp. 267-292. 
Verma, Manindra K.1993. Complex Predicates in 
South Asian Languages. Manohar Publishers and 
Distributors, New Delhi. 
 
45
JU_CSE_GREC10: Named Entity Generation at GREC 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Sivaji Bandyopadhyay4 
Department of Computer Science and Engineering 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com4  
 
Abstract 
 
This paper presents the experiments carried 
out at Jadavpur University as part of the par-
ticipation in the GREC Named Entity Genera-
tion Challenge 2010. The Baseline system is 
based on the SEMCAT, SYNCAT and SYN-
FUNC features of REF and REG08-TYPE and 
CASE features of REFEX elements. The dis-
course level system is based on the additional 
positional features: paragraph number, sen-
tence number, word position in the sentence 
and mention number of a particular named ent-
ity in the document. The inclusion of discourse 
level features has improved the performance 
of the system. 
1 Baseline System 
The baseline system is based on the following 
linguistic features of REF elements: SEMCAT 
(Semantic Category), SYNCAT (Syntactic Cate-
gory) and SYNFUNC (Syntactic Function) (Anja 
Belz, 2010) and the following linguistic features 
of REFEX elements: REG08-TYPE (Entity type) 
and CASE (Case marker). The baseline system 
has been separately trained on the training set 
data for the three domains: chefs, composers and 
inventors. The system has been tested on each 
development set by identifying the most probable 
REFEX element among the possible alternatives 
based on the REF element feature combination. 
The probability assigned to a REFEX element 
corresponding to a certain feature combination of 
REF element is calculated as follows:  
( )
i
i
D
REFEX
v D
REF
Np R
N
=  
where ( )vp R is the probability of the targeted 
REFEX element to be assigned, iDREFN is the total 
number of occurrences of REF element feature 
combinations, iD denotes the domain i.e., Chefs, 
Composers and Inventors and iDREFN denotes the 
total number of occurrences of the REFEX ele-
ment corresponding to the REF feature combina-
tion. 
It has been observed that many times the most 
probable REFEX element as identified from the 
training set is not present among the alternative 
REFEX elements. In these cases the system as-
signs the next highest probable REFEX element 
learnt from the training set that matches with one 
of the REFEX elements among the alternatives. 
In some cases more than one REFEX element get 
same probability in the training set. In these cas-
es, the REFEX element that occurs earlier in the 
alternative set is assigned. The experimental re-
sult of Baseline system is reported in Table 1. 
 Chefs Composers Inventors 
Precision 0.63 0.68 0.70 
Recall 0.69 0.60 0.64 
F-Measure 0.66 0.64 0.68 
Table 1: Result of Baseline System 
2 Discourse Level System 
The discourse level features like paragraph num-
ber, sentence number and position of a particular 
word in a sentence have been added with the fea-
tures considered in the baseline system. As men-
tioned in Section 1, more than one REFEX ele-
ment can have the same probability value. This 
happens as REFEX elements are identified by 
two features only REG08-TYPE and CASE.  
 Nam
e 
Pro-
noun 
Com-
mon 
Emp-
ty 
Chefs 2317 3071 55 646 
Composers 2616 4037 92 858 
Inventors 1959 2826 75 621 
Table 2: Distribution of REFEX Types among 
three domains. 
The above problem occurs mainly for Name 
type. Pronouns are very frequent in all the three 
domains but they have small number of varia-
tions as: he, her, him, himself, his, she, who, 
whom and whose. Common type REFEX ele-
ments are too infrequent in the training set and 
they are very hard to generalize. Empty type has 
only one REFEX value as: ?_?.  The distribution 
of the various REFEX types among the three 
domains in the training set is shown in Table 2. 
2.1 Analysis of Name type entities 
Table 2 shows that name types are very frequent 
in all the three domains. Name type entities are 
further differentiated by adding more features 
derived from the analysis of the name type ele-
ment.  
Firstly, the full name of each named entity has 
been identified by Entity identification number 
(id), maximum length among all occurrences of 
that named entity and case marker as plain. For 
example, in Figure 1, the REFEX element of id 3 
has been chosen as a full name of entity ?0? as it 
has the longest string with case ?plain?. 
After identification of full name of each RE-
FEX entity, the following features are identified 
for each occurrence of an entity:: Complete 
Name Genitive (CNG), Complete Name (CN), 
First Name Genitive (FNG), First Name (FN), 
Last Name Genitive (LNG), Last Name (LN), 
Middle Name Genitive (MNG) and Middle 
Name (MN). These features are binary in nature 
and for each occurrence of an entity only one of 
the above features will be true. 
Pronouns are kept as the REFEX element fea-
ture with its surface level pattern as they have 
only 9 variations. Common types are considered 
with tag level ?common? as they hard to general-
ize. Empty types are tagged as ?empty? as they 
have only one tag value ?_?.  
1 
<REFEX ENTITY="0" REG08-
TYPE="name" CASE="genitive">Alain 
Senderens's</REFEX> 
CNG 
2 
<REFEX ENTITY="0" REG08-
TYPE="name" 
CASE="genitive">Senderens's</REFEX> 
LNG 
3 
<REFEX ENTITY="0" REG08-
TYPE="name" CASE="plain">Alain 
Senderens</REFEX> 
CN 
4 
<REFEX ENTITY="0" REG08-
TYPE="name" 
CASE="plain">Senderens</REFEX> 
LN 
Figure 1: Example of Full Name Identification 
3 Experimental Results 
The experimental results of the discourse level 
system on the development set are reported in the 
Table 3 and Table 4 respectively. Table 3 reports 
the results when the system has been trained sep-
arately with domain specific training set and Ta-
ble 4 reports the results when the training has 
been carried out on the complete training set.  
The comparison of the results of the baseline 
and the discourse level system shows an overall 
improvement. But there are some interesting ob-
servations when comparing the results in Table 3 
and Table 4. Currently detailed analyses of the 
results are being carried out.
 Chefs Composers Inventors 
P R F P R F P R F 
Name 0.69 0.74 0.71 0.78 0.61 0.69 0.77 0.67 0.71 
Pronoun 0.81 0.76 0.79 0.70 0.84 0.76 0.76 0.87 0.81 
Common 0.76 0.87 0.81 0.37 0.44 0.40 0.44 0.65 0.68 
Empty 0.92 0.88 0.90 0.86 0.92 0.89 0.72 0.65 0.68 
 
Table 3: Experimental Results of Discourse Level System on the Development Set (Training with 
Domain Specific Training Set) 
 
 
Reg08 Type 
   String  
Accuracy 
BLEU 
NIST 
String Edit 
Distance 
Precision Recall Mean Mean Normalized 1 2 3 4 
Chefs 0.66 0.70 0.57 0.68 0.70 0.76 0.81 3.70 0.77 0.38 
Composers 0.63 0.67 0.56 0.61 0.57 0.54 0.50 3.34 1.07 0.40 
Inventors 0.60 0.62 0.50 0.55 0.54 0.52 0.49 2.90 1.25 0.47 
Total 0.63 0.66 0.54 0.61 0.58 0.57 0.55 3.83 1.03 0.42 
 
Table 4: Table 4: Experimental Results of Discourse Level System on the Development Set (Training 
with Complete Training Set) 
References 
Anja Belz. 2010. GREC Named Entity Generation Challenge 2010: Participants? Pack. 
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 38?42,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Shared task system description: Measuring the Compositionality of 
Bigrams using Statistical Methodologies 
Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal, Tanik Saikh, 
 Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
its_tanmoy@yahoo.co.in, santanu.pal.ju@gmail.com, 
tapabratamondal@gmail.com, tanik4u@gmail.com, 
sivaji_cse_ju@yahoo.com 
 
Abstract 
The measurement of relative 
compositionality of bigrams is crucial to 
identify Multi-word Expressions 
(MWEs) in Natural Language 
Processing (NLP) tasks. The article 
presents the experiments carried out as 
part of the participation in the shared 
task ?Distributional Semantics and 
Compositionality (DiSCo)? organized as 
part of the DiSCo workshop in ACL-
HLT 2011. The experiments deal with 
various collocation based statistical 
approaches to compute the relative 
compositionality of three types of 
bigram phrases (Adjective-Noun, Verb-
subject and Verb-object combinations). 
The experimental results in terms of both 
fine-grained and coarse-grained 
compositionality scores have been 
evaluated with the human annotated gold 
standard data. Reasonable results have 
been obtained in terms of average point 
difference and coarse precision.  
1 Introduction 
The present work examines the relative 
compositionality of Adjective-Noun (ADJ-NN; 
e.g., blue chip), Verb-subject (V-SUBJ; where 
noun acting as a subject of a verb, e.g., name 
imply) and Verb-object (V-OBJ; where noun 
acting as an object of a verb, e.g., beg question) 
combinations using collocation based statistical 
approaches. Measuring the relative 
compositionality is useful in applications such as 
machine translation where the highly non-
compositional collocations can be handled in a 
special way (Hwang and Sasaki, 2005). 
Multi-word expressions (MWEs) are 
sequences of words that tend to co-occur more 
frequently than chance and are either 
idiosyncratic or decomposable into multiple 
simple words (Baldwin, 2006). Deciding 
idiomaticity of MWEs is highly important for 
machine translation, information retrieval, 
question answering, lexical acquisition, parsing 
and language generation. Compositionality 
refers to the degree to which the meaning of a 
MWE can be predicted by combining the 
meanings of its components. Unlike syntactic 
compositionality (e.g. by and large), semantic 
compositionality is continuous (Baldwin, 2006).   
Several studies have been carried out for 
detecting compositionality of noun-noun MWEs 
using WordNet hypothesis (Baldwin et al, 
2003), verb-particle constructions using 
statistical similarities (Bannard et al, 2003; 
McCarthy et al, 2003) and verb-noun pairs 
using Latent Semantic Analysis (Katz and 
Giesbrecht, 2006).  
Our contributions are two-fold: firstly, we 
experimentally show that collocation based 
statistical compositionality measurement can 
assist in identifying the continuum of 
compositionality of MWEs. Secondly, we show 
that supervised weighted parameter tuning 
results in accuracy that is comparable to the best 
manually selected combination of parameters.  
38
2 Proposed Methodologies 
The present task was to identify the numerical 
judgment of compositionality of individual 
phrase. The statistical co-occurrence features 
used in this experiment are described.     
Frequency:  If two words occur together 
quite frequently, the lexical meaning of the 
composition may be different from the 
combination of their individual meanings. The 
frequency of an individual phrase is directly 
used in the following methods. 
Point-wise Information (PMI): An 
information-theoretic motivated measure for 
discovering interesting collocations is point-wise 
mutual information (Church and Hanks, 1990). 
It is originally defined as the mutual information 
between particular events X and Y and in our 
case the occurrence of particular words, as 
follows: 
   = log ,. ? log ,.   1  
PMI represents the amount of information 
provided by the occurrence of the event 
represented by X about the occurrence of the 
event represented by Y. 
T-test:  T-test has been widely used for 
collocation discovery. This statistical test tells us 
the probability of a certain constellation 
(Nugues, 2006). It looks at the mean and 
variance of a sample of measurements. The null 
hypothesis is that the sample is drawn from a 
distribution with mean. T-score is computed 
using the equation (2): 
,  = 