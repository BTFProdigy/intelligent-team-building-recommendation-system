Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 613?623,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Centering Similarity Measures to Reduce Hubs
Ikumi Suzuki
National Institute of Genetics
Mishima, Shizuoka, Japan
suzuki.ikumi@gmail.com
Kazuo Hara
National Institute of Genetics
Mishima, Shizuoka, Japan
kazuo.hara@gmail.com
Masashi Shimbo
Nara Institute of Science and Technology
Ikoma, Nara, Japan
shimbo@is.naist.jp
Marco Saerens
Universite? catholique de Louvain
Louvain-la-Neuve, Belgium
marco.saerens@uclouvain.be
Kenji Fukumizu
The Institute of Statistical Mathematics
Tachikawa, Tokyo, Japan
fukumizu@ism.ac.jp
Abstract
The performance of nearest neighbor methods
is degraded by the presence of hubs, i.e., ob-
jects in the dataset that are similar to many
other objects. In this paper, we show that the
classical method of centering, the transforma-
tion that shifts the origin of the space to the
data centroid, provides an effective way to re-
duce hubs. We show analytically why hubs
emerge and why they are suppressed by cen-
tering, under a simple probabilistic model of
data. To further reduce hubs, we also move
the origin more aggressively towards hubs,
through weighted centering. Our experimental
results show that (weighted) centering is effec-
tive for natural language data; it improves the
performance of the k-nearest neighbor classi-
fiers considerably in word sense disambigua-
tion and document classification tasks.
1 Introduction
1.1 Background
The k-nearest neighbor (kNN) algorithm is a sim-
ple nonparametric method of classification. It has
been applied to various natural language process-
ing (NLP) tasks such as document classification
(Masand et al, 1992; Yang and Liu, 1999), part-
of-speech tagging (S?gaard, 2011), and word sense
disambiguation (Navigli, 2009).
To apply the kNN algorithm, data is typically rep-
resented as a vector object in a feature space, and
(dis)similarity between data is measured by the dis-
tance between the vectors, their inner product, or co-
sine of the angle between them (Jurafsky and Mar-
tin, 2008). With such a (dis)similarity measure, the
unknown class label of a test object is predicted by
a majority vote of the classes of its k most similar
objects in the labeled training set.
Recent studies (Radovanovic? et al, 2010a;
Radovanovic? et al, 2010b) have shown that if the
feature space is high-dimensional, some objects in
the dataset emerge as hubs; i.e., these objects fre-
quently appear in the k nearest neighbors of other
objects.
The emergence of hubs may deteriorate the per-
formance of kNN classification and nearest neighbor
search in general:
? If hub objects exist in the training set, they have
a strong chance to be a kNN of many test ob-
jects. Because the class of a test object is pre-
dicted by a majority vote from its k nearest
neighbors, prediction is biased toward the la-
bels of the hubs.
? In information retrieval, nearest neighbor
search finds objects in the database that are
most relevant, or similar, to user-provided
queries. If particular objects, such as hubs, are
nearly always returned for any query, the re-
trieved results are probably not very useful.
These drawbacks may hinder application of near-
est neighbor methods in NLP, as typical natural lan-
guage data are extremely high-dimensional (Juraf-
sky and Martin, 2008) and thus prone to produce
hubs.
1.2 Contributions
Centering (Mardia et al, 1979; Fisher and Lenz,
1996; Eriksson et al, 2006) is a standard technique
613
for removing observation bias in the data. It is a
transformation of feature space in a way that the ori-
gin of the space is moved to the data centroid (sam-
ple mean). The distance between data objects is not
changed by centering, but their inner product and co-
sine are affected; see Section 3 for detail.
In this paper, we advocate the use of centering as a
means of reducing hubs. Specifically, we propose to
measure the similarity of objects by the inner prod-
uct (not distance or cosine) in the centered feature
space.
Our approach is motivated by the observation that
the objects similar to the data centroid tend to be-
come hubs (Radovanovic? et al, 2010a). This ob-
servation suggests that the number of hubs may be
reduced if we can define a similarity measure that
makes all objects in a dataset equally similar to the
centroid (Suzuki et al, 2012). The inner product in
the centered space indeed enjoys this property.
In Section 4, we analyze why hubs emerge under
a simple probabilistic model of data, and also give
an account of why they are suppressed by centering.
Using both synthetic and real datasets, we show
that objects similar to the centroid also emerge as
hubs in multi-cluster data (Section 5), so the applica-
tion of centering is wider than expected. To further
reduce hubs, we also propose to move the origin of
the space more aggressively towards hubs, through
weighted centering (Section 6).
In Section 7, we show that centering and weighted
centering are effective for natural language data.
these methods markedly improve the performance
of kNN classifiers in word sense disambiguation and
document classification tasks.
2 Related work
Centering is a classical technique widely used in
many fields of science. For instance, centering
forms a preprocessing step in principal component
analysis and Fisher linear discriminant analysis.
In NLP, however, centering is seldom used; the
use of cosine and inner product similarities is quite
common, but they are nearly always used uncen-
tered. Non-centered cosine is used, for instance, in
word sense disambiguation (Schu?tze, 1998; Navigli,
2009), paraphrasing (Erk and Pado?, 2008; Thater
et al, 2010), and compositional semantics (Mitchell
and Lapata, 2008), to name a few.
There have been several approaches to improv-
ing kNN classification: learning similarity/distance
measures from training data (metric learning)
(Weinberger and Saul, 2009; Qamar et al, 2008),
weighting nearest neighbors for similarity-based
classification (Chen et al, 2009), and neighbor-
hood size selection (Wang et al, 2006; Guo and
Chakraborty, 2010). However, none of these have
addressed the reduction of hubs.
More recently, Schnitzer et al (2012) proposed
the Mutual Proximity transformation that rescales
distance measures to decrease hubs in a dataset.
Suzuki et al (2012) showed that kernels based on
graph Laplacian, such as the commute-time kernels
(Saerens et al, 2004) and the regularized Laplacian
(Chebotarev and Shamis, 1997; Smola and Kondor,
2003), make all objects equally similar to the data
centroid, which in turn reduce hubs.
In Section 7, we evaluate centering, Mutual Prox-
imity, and Laplacian kernels in NLP tasks, and
demonstrate that centering is equally or even more
effective. Section 4 presents a theoretical justifica-
tion for using centering to reduce hubs, but this kind
of analysis is missing for the Laplacian kernels.
Centering is easier to compute as well. For a
dataset of n objects, it takes O(n2) time to com-
pute, whereas computing a Laplacian-based kernel
requires O(n3) time for matrix inversion. Mutual
Proximity also has a time complexity of O(n2).
3 Centering
Consider a dataset of n objects in an m-dimensional
feature space, x1, ? ? ? , xn ? Rm. Throughout this
paper, we use the inner product ?xi, x j? as a measure
of similarity between xi and x j. Let K be the Gram
matrix of the n feature vectors, i.e., the n ? n matrix
whose (i, j) element holds ?xi, x j?. Using m? n data
matrix X = [x1, ? ? ? , xn], we can write K as
K = XTX,
where XT represents the matrix transpose of X.
Centering is a transformation in which the origin
of the feature space is shifted to the data centroid
x? =
1
n
n?
i=1
xi, (1)
614
and object x is mapped to the centered feature vector
xcent = x ? x?. (2)
The similarity between two objects x and x? is now
measured by ?xcent, x?cent? = ?x ? x?, x? ? x??.
After centering, the inner product between any
object and the data centroid (which is a zero vector
because x?cent = x? ? x? = 0) is uniformly 0; in other
words, all objects in the dataset have an equal simi-
larity to the centroid. According to the observation
that the objects similar to the centroid become hubs
(Radovanovic? et al, 2010a), we can expect hubs to
be reduced after centering.
Intuitively, centering reduces hubs because it
makes the length of the feature vector xcent short
for (hub) objects x that lie close to the data centroid
x?; see Eq. (2). And since we measure object simi-
larity by inner product, shorter vectors tend to pro-
duce smaller similarity scores. Hence objects close
to the data centroid become less similar to other ob-
jects after centering, and no longer be hubs. In Sec-
tion 4, we analyze the effect of centering on hubness
in more detail.
3.1 Centered Gram matrix
Let I be an n ? n identity matrix and 1 be an n-
dimensional all-ones vector. The symmetric matrix
H = I?(1/n)11T is called centering matrix, because
the centered data matrix Xcent = [xcent1 , ? ? ? , x
cent
n ]
can be computed by Xcent = XH (Mardia et al,
1979).
The Gram matrix Kcent of the centered feature
vectors, whose (i, j) element holds the inner prod-
uct ?xcenti , x
cent
j ?, can be calculated from the original
Gram matrix K by
Kcent =
(
Xcent
)T (
Xcent
)
= HXTXH = HKH. (3)
Eq. (3) implies that the original data matrix X is
not needed to compute the centered Gram matrix
Kcent, provided that K is given. It is hence possi-
ble to use the so-called kernel trick; i.e., centering
can be applied even if data matrix X is not available
but the similarity of objects can be measured by a
kernel function in an implicit feature space.
4 Theoretical analysis of the effect of
centering on hubness
We now analyze why objects most similar to the
centroid tend to be hubs in the dataset, and give an
explanation as to why centering may suppress the
emergence of hubs.
4.1 Before centering
Consider a dataset of m-dimensional feature vectors,
with each vector x ? Rm generated independently
from a distribution with a finite mean vector ?. In
other words, objects x in this dataset are drawn from
a distribution P(x), i.e.,
x ? P(x),
and
? = E[x] =
?
x dP(x) (4)
where E[?] denotes the expectation of a random vari-
able.
We will use the following elementary lemma on
the distributions of inner product subsequently.
Lemma 1. Let a ? Rm be a fixed vector, and x ? Rm
be an object sampled according to distribution P(x).
Then the inner product ?a, x? follows a distribution
with mean ?a,??.
Proof. From the linearity of the inner product and
Eq. (4), we obtain
E[?a, x?] =
?
?a, x? dP(x)
= ?a,
?
x dP(x)? = ?a,??. 
Now, imagine that we have an object x sam-
pled from P(x), and we want to compute its nearest
neighbor in a dataset. Let h and ` be two fixed ob-
jects in the dataset, such that the inner product to the
true mean ? is higher for h than for `, i.e.,
?h,?? ? ?`,?? > 0. (5)
We are interested in which of h and ` is more similar
to x (in terms of inner product), or in other words,
the difference of two inner products
z = ?h, x? ? ?`, x? = ?h ? `, x?. (6)
615
Because x is a random variable, so is z. Let Q(z) be
the distribution of z; i.e., z ? Q(z).
Using Lemma 1 with a = h ? `, together with
Eq. (5), we have
E[z] = ?h ? `,?? = ?h,?? ? ?`,?? > 0. (7)
Note that the above statement is only concerned
about the mean, so it does not in general assure that
?h, x? > ?`, x? (8)
holds with high probability; there is a chance that
a small number of outliers are inflating the mean.
To assure that inequality (8) holds with probability
greater than 1/2 for instance, the median rather than
the mean of the distribution Q(z) must be greater
than 0.
If the distribution Q(z) is symmetric, the median
occurs at the same point as the mean, and the above
claim holds. Indeed, if the components of x are gen-
erated independently from (possibly non-identical)
normal distributions, we can show that Q(z) also
obeys a normal distribution. Because it is a symmet-
ric distribution, we can safely say that in this case,
Eq. (8) holds with probability greater than 1/2.
For a general non-symmetric distribution with a
finite variance, the median is known to be within the
standard deviation of the mean (Mallows, 1991), so
we could still say that Eq. (8) is likely to hold if ?h?
`,?? is sufficiently large compared to the standard
deviation.
Now, if we let h be the object in a given dataset
with the highest similarity (inner product) to the
mean ?, and let ` be any other object in the set, then
we see from the above discussion that h is likely to
have higher similarity to x, a test sample drawn from
distribution P(x). Because this holds for any ` in
the dataset, the conclusion is that the objects in the
dataset most similar to ? are likely to become hubs.
4.2 After centering
Next let us investigate what happens if the dataset
is centered. Let x? be the sample (empirical) mean
given by Eq. (1). After centering, the similarity of x
with each of the two fixed objects h and ` are evalu-
ated by ?h? x?, x? x?? and ?`? x?, x? x??, respectively.
Their difference zcent is given by
zcent = ?h ? x?, x ? x?? ? ?` ? x?, x ? x??
= ?h ? `, x ? x??
= ?h ? `, x? ? ?h ? `, x??
= z ? ?h ? `, x??.
The last equality follows from Eq. (6). By definition
we have z ? Q(z), and since ?h ? `, x?? is a constant,
zcent = z ? ?h ? `, x?? ? Q(z + ?h ? `, x??).
In other words, the shape of the distribution does not
change, but the mean is shifted to
E[zcent] = E[z] ? ?h ? `, x??
= ?h ? `,?? ? ?h ? `, x??
= ?h ? `,? ? x??,
where E[z] is given by Eq. (7). If the sample mean
x? is close enough to the true mean ?, i.e., x? ? ?, we
have an approximation
E[zcent] = ?h ? `,? ? x?? ? 0. (9)
Thus, if the median and the mean of distribution
Q(z) are again not far apart, Eq. (9) suggests that
h ? x? and ` ? x? are about equally likely to be more
similar to x ? x?; i.e., neither has a greater chance to
become a hub.
5 Hubs in multi-cluster data
In this section, we discuss emergence of hubs when
the data consists of multiple clusters. In fact, the
analysis of Section 4 is distribution-free, and thus
also applies to the case of multi-modal P(x). How-
ever, one might still argue that objects similar to the
data centroid should hardly occur in that case. Us-
ing both synthetic and real datasets, we demonstrate
below that even in multi-cluster data, objects that
are only slightly more similar to the data mean (cen-
troid) may emerge as hubs.
5.1 Synthetic data
5.1.1 Data generation
We generated a high-dimensional multi-cluster
dataset by modeling it as a mixture of ten von Mises-
Fisher distributions (Mardia and Jupp, 2000) in
616
0 50 100 1500.45
0.5
0.55
0.6
N10
Similar
ity with
 centro
id
(a) Before centering: N10 vs. inner
product similarity to the data cen-
troid
200 400 600 800 1000
200
400
600
800
1000
Object ID
Object I
D
 
 
510
1520
2530
3540
4550
(b) Before centering: kNN matrix
200 400 600 800 1000150
100
50
0
50
100
150
Object ID
Freque
ncy
(c) Before centering: Breakdown of
N10 by cluster match/mismatch
between objects and neighbors
0 50 100 150?0.1
?0.05
0
0.05
0.1
N10
Similar
ity with
 centro
id
(d) After centering: N10 vs. inner
product similarity to the data cen-
troid
200 400 600 800 1000
200
400
600
800
1000
Object ID
Object I
D
 
 
510
1520
2530
3540
4550
(e) After centering: kNN matrix
200 400 600 800 1000150
100
50
0
50
100
150
Object ID
Freque
ncy
(f) After centering: Breakdown of
N10 by cluster match/mismatch
between objects and neighbors
Figure 1: 300-dimensional synthetic data. (a), (d): scatter plot of the N10 value of objects and their similarity to
centroid. (b), (e): kNN matrices. The points are colored according to the N10 value of object x; warmer colors indicate
higher N10 values. (c), (f): the number of times (y-axis) an object (whose ID is on the x-axis) appears in the 10 nearest
neighbors of objects of the same cluster (black bars), and those of different clusters (magenta).
R300. The von Mises-Fisher distribution is a distri-
bution of unit vectors (it can roughly be thought of
as a normal distribution on a unit hypersphere), so
for objects (feature vectors) sampled from this dis-
tribution, inner product reduces to cosine similarity.
We sampled1 100 objects from each of the ten dis-
tributions (clusters), and made a dataset of 1,000 ob-
jects in total.
The von Mises-Fisher distribution has two param-
eters, the mean direction vector ?, and the concen-
tration parameter ? characterizing how strongly the
population is concentrated around the direction ?.
We set ? = 500 for all ten distributions, but the mean
directions ? were made distinct; all mean direction
1We used the random sampling code available at http:
//people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html
(Banerjee et al, 2005).
vectors had 30 components set to 0.5 while the re-
maining 270 components were set to 1, but the 30
components with value 0.5 were chosen to be dis-
tinct among the ten clusters. This configuration as-
sures that all ten mean directions have the same an-
gle from the all-ones vector [1, . . . , 1]T, which is the
direction of the mean of the entire data distribution.
Note that even though all sampled objects reside
on the surface of the unit hypersphere, the data cen-
troid lies not on the surface but inside the hyper-
sphere. And after centering, the length of the fea-
ture vectors may vary from one another, but we do
not normalize these vectors; i.e., object similarity is
measured by raw inner product, not by cosine.
617
5.1.2 Correlation between hubness and
centroid similarity
The scatter plot in Figure 1(a) shows the correla-
tion between the degree of hubness (N10) of an ob-
ject and its inner product similarity to the data cen-
troid. The N10 value of an object is defined as the
number of times the object appears in the 10 nearest
neighbors of other objects in the dataset. It was used
in (Radovanovic? et al, 2010a) to measure the degree
of hubness of individual objects.
The plot clearly shows that the hub objects (i.e.,
those with high N10) consist of objects that are simi-
lar to the centroid. Figure 1(d) shows the scatter plot
after the data is centered, created in the same way
as Figure 1(a). The similarity to the centroid is uni-
formly 0 as a result of centering, and no objects have
an N10 value greater than 33.
5.1.3 Influence of hubs on objects in different
clusters
The kNN matrix of Figure 1(b) depicts the kNN
relations with k = 10 among objects before center-
ing. In this matrix, both the x- and y- axes represent
the ID of the objects. If object x is in the 10 nearest
neighbors of object y, a point is plotted at coordi-
nates (x, y). As a result, there are exactly k = 10
points in each row. The color of points indicates the
degree of hubness of object x; warmer color repre-
sents higher N10 value of the object.
In this matrix, object IDs are sorted by the clus-
ter the objects belong to. Hence in the ideal case in
which the k nearest neighbors of every object consist
genuinely of objects from the same cluster, only the
diagonal blocks would be colored, and off-diagonal
areas would be left blank.
As Figure 1(b) shows, the actual situation is far
from ideal, even though ten diagonal blocks are still
identifiable. The presence of many warm colored
vertical lines suggests that many hub objects appear
in the 10 nearest neighbors of other objects that are
not in the same cluster as the hubs. Thus these hubs
may have a strong influence on the kNN prediction
of other objects.
Figure 1(e) shows the kNN matrix after centering.
The warm colored lines have disappeared, and the
diagonal blocks are now more visible.
The bar graphs of Figures 1(c) and (f) plot the N10
value of each object (whose ID is on the x-axis). Re-
call that N10 is the number of times an object appears
in the 10 nearest neighbors of other objects. The
bar for each object is broken down by whether the
object and its neighbors belong to the same cluster
(black bar) or in different clusters (magenta bar). In
terms of kNN classification, having a large number
of nearest neighbors with the same class improves
the classification performance, so longer black bars
and shorter magenta bars are more desirable.
Before centering (Figure 1(c)), hub objects with
large N10 values are similar not only to objects be-
longing to the same cluster (as indicated by black
bars), but also to objects belonging to different clus-
ters (magenta bars). After centering (Figure 1(f)),
the number of tall magenta bars decreases.
Before centering, 22.7% of the 10 nearest neigh-
bors of an object have the same class label as the
object (as indicated by the ratio of the total height of
black bars relative to that of all bars in Figure 1(c)).
After centering, the percentage increases to 31.6%.
5.2 Real dataset
We did the same analysis as Sections 5.1.2?5.1.3
to a real dataset with multiple-cluster structure: the
Reuters Transcribed dataset. This multi-class docu-
ment classification dataset has ten classes, and each
class roughly forms a cluster. We will also use this
dataset in an experiment in Section 7.2.
The results are shown in Figure 2. We can ob-
serve the same trends as we saw in Figure 1 for the
synthetic data: positive correlation between hubness
(N10) and inner product with the data centroid be-
fore centering; hubs appearing in the nearest neigh-
bors of many objects of different classes; and both
are reduced after centering.
The ratio of the height of black bars to that of
all bars in Figure 2(c) is 38.4% before centering,
whereas it improves to 41.0% after centering (Fig-
ure 2(f)).
6 Hubness weighted centering
Centering shifts the origin of the space to the data
centroid, and objects similar to the centroid tend to
become hubs. Thus in a sense, centering can be
interpreted as an operation that shifts the origin to-
wards hubs.
In this section, we extrapolate this interpretation,
618
0 10 20 30 40 500
0.01
0.02
0.03
0.04
0.05
0.06
N10
Simila
rity w
ith ce
ntroid
(a) Before centering: N10 vs. inner
product similarity to the data cen-
troid
50 100 150 200
50
100
150
200
Object ID
Object
 ID
 
 
5
10
15
20
25
30
(b) Before centering: kNN matrix
50 100 150 20040
20
0
20
40
Object ID
Frequ
ency
(c) Before centering: Breakdown of
N10 by class match/mismatch be-
tween objects and neighbors
0 10 20 30 40 50?0.03
?0.02
?0.01
0
0.01
0.02
0.03
N10
Simila
rity w
ith ce
ntroid
(d) After centering: N10 vs. inner
product similarity to the data cen-
troid
50 100 150 200
50
100
150
200
Object ID
Object
 ID
 
 
5
10
15
20
25
30
(e) After centering: kNN matrix
50 100 150 20040
20
0
20
40
Object ID
Frequ
ency
(f) After centering: Breakdown of N10
by class match/mismatch between
objects and neighbors
Figure 2: Reuters Transcribed data.
and move the origin more actively towards hub ob-
jects in the dataset, rather than towards the data cen-
troid. To this end, we consider weighted centering,
a variation of centering in which each object is asso-
ciated with a weight, and the origin is shifted to the
weighted mean of the data. Specifically, we define
the weight of an object as the sum of the similarities
(inner products) between the object and all objects,
regarding this sum as the index of how likely the ob-
ject can be a hub.
6.1 Weighted centering
In weighted centering, we associate weight wi to
each object i in the dataset, and move the origin to
the weighted centroid
x?weighted =
n?
i=1
wixi
where
?n
i=1 wi = 1 and 0 ? wi ? 1 for i = 1, . . . , n.
Thus, object x is mapped to a new feature vector
xweighted = x ? x?weighted = x ?
n?
i=1
wixi.
Notice that the original centering formula (2) is re-
covered by letting wi = 1/n for all i = 1, . . . , n.
Weighted centering can also be kernelized by us-
ing the weighted centering matrix H(w) = I ? 1wT
in place of H in Eq. (3). The resulting Gram matrix
is
Kweighted = H(w)KH(w)T. (10)
6.2 Similarity-dependent weighting
To move the origin towards hubs more aggressively,
we place more weights on objects that are more
likely to become hubs. This likelihood is estimated
by the similarity of individual objects to all objects
in the data set.
619
Let di be the sum of the similarity between object
xi and all objects in the dataset. So,
di =
n?
j=1
?xi, x j? = n ?xi,
1
n
n?
j=1
x j?.
As seen from the last equation, di is proportional to
the similarity (inner product) between object xi and
the data centroid.
Now we define {wi}ni=1 from {di}
n
i=1 by
wi =
d?i
?n
j=1 d
?
j
,
where ? is a parameter controlling how much we
emphasize the effect of di. Setting ? = 0 results in
wi = 1 for every i, and hence is equivalent to normal
centering. When ? > 0, weighted centering moves
the origin closer to the objects with a large di than
normal centering would.
7 Experiments
We evaluated the effect of centering in two natural
language tasks: word sense disambiguation (WSD)
and document classification. We are interested in
whether hubs are actually reduced after centering,
and whether the performance of kNN classification
is improved.
Throughout this section, K denotes cosine simi-
larity matrix; i.e., inner product of feature vectors
normalized to unit length; Kcent denotes the cen-
tered similarity matrix computed by Eq. (3) from K;
Kweighted denotes its hubness weighted variant given
by Eq. (10). Depending on context, these symbols
are also used to denote kNN classifiers using respec-
tive similarity measures.
For comparison, we also tested two recently pro-
posed approaches to hub reduction: transformation
of the base similarity measure (in our case, K) by
Mutual Proximity (Schnitzer et al, 2012)2, and the
one (Suzuki et al, 2012) based on graph Laplacian
kernels. Since the Laplacian kernels are defined for
graph nodes, we computed them by taking the co-
sine similarity matrix K as the weighted adjacency
(affinity) matrix of a graph. For Laplacian kernels,
2We used the Matlab script downloaded from http://www.
ofai.at/?dominik.schnitzer/mp/.
we computed both the regularized Laplacian ker-
nel (Chebotarev and Shamis, 1997; Smola and Kon-
dor, 2003) with several parameter values, as well as
the commute-time kernel (Saerens et al, 2004), but
present only the best results among these kernels.
7.1 Word sense disambiguation
7.1.1 Task and dataset
In the WSD experiment, we used the dataset for
the Senseval-3 English Lexical Sample (ELS) task
(Mihalcea et al, 2004). It is a collection of sen-
tences containing 57 polysemous words, and each
of these sentences is annotated with a gold standard
sense of the target word. The goal of the ELS task
is to build a classifier for each target word, which,
given a context around the word, predicts a sense
from the known set of senses.
We used a basic bag-of-words representation for
the context surrounding a target word (Mihalcea,
2004; Navigli, 2009). A context is thus represented
as a high-dimensional feature vector holding the tf-
idf weighted frequency of words3 in context.
7.1.2 Compared methods
We applied kNN classification using cosine sim-
ilarity K, and its four transformed similarity mea-
sures: centered similarity Kcent, its weighted vari-
ant Kweighted, Mutual Proximity and graph Laplacian
kernels. The sense of a test object was predicted by
voting from the k training objects most similar to the
test object, as measured by the respective similarity
measures.
We used leave-one-out cross validation within the
training data to tune neighborhood size k for the
kNN classification and the voting scheme, i.e., ei-
ther (unweighted) majority vote, or weighted vote in
which votes from individual objects are weighted by
their similarity score to the test objects. We also se-
lected parameter ? in Kweighted and the best graph
Laplacian kernel among the regularized Laplacian
and commute time kernels using the training data.
7.1.3 Evaluation
We computed two indices for each similarity mea-
sure: (i) skewness of the N10 distribution to evaluate
3We removed stop words listed in the on-line appendix of
(Lewis et al, 2004).
620
Method F1 score Skewness
K 60.3 4.55
Kcent 64.0 1.19
Kweighted 64.8 1.02
Mutual Proximity 63.0 1.00
Graph Laplacian 61.2 4.51
GAMBL (Decadt et al, 2004) 64.5 ?
Table 1: WSD results: Macro-averaged F1 score (points)
of the compared methods (larger is better) and empirical
skewness of the N10 distribution for each similarity mea-
sure (smaller is better).
the emergence of hubs, and (ii) macro-averaged F1
score to evaluate the classification performance.
Skewness To evaluate the degree of hub emer-
gence for each similarity measure, we followed
(Radovanovic? et al, 2010a) and counted Nk(x), the
number of times object x occurs in the kNN lists
of other objects in the dataset (we fix k = 10 be-
low). The emergence of hubs in a dataset can then
be quantified with skewness, defined as follows:
S Nk =
E
[(
Nk ? ?Nk
)3
]
?3Nk
.
In this equation, E[ ? ] denotes expectation, and ?Nk
and ?Nk are the mean and the standard deviation of
the Nk distribution, respectively.
When hubs exist in a dataset, the distribution of
Nk is expected to skew to the right, and yields a large
S Nk (Radovanovic? et al, 2010a). In other words,
similarity measures that yield smaller S Nk are more
desirable in terms of hub reduction.
Skewness can only be computed for each dataset,
and in the WSD task, each target word has its own
dataset. Hence we computed the skewness S N10 for
each word and then took average.
Macro-averaged F1 score Classification perfor-
mance was measured by the F1 score macro-
averaged over all the 57 target words in the Senseval-
3 ELS dataset. The standard Senseval-3 ELS scor-
ing method is based on micro average, but we used
macro average to make the evaluation consistent
with skewness computation, which, as mentioned
above, can only be computed for each dataset (i.e.,
word).
Dataset #classes #objects #features
Reuters Transcribed 10 201 2730
Mini Newsgroups 20 2000 8811
Table 2: Document classification datasets: Number of
classes, data size, and number of features.
7.1.4 Result
Table 1 shows the F1 scores and the skewness of
the N10 distributions, macro averaged over the 57
target words. The table also includes the macro-
averaged F1 score4 of the GAMBL system, the best
memory-based system participated in the Senseval-
3 ELS task. Note however that GAMBL uses more
elaborate features (e.g., part-of-speech of words)
than just a plain bag-of-words used by other methods
in this comparison. GAMBL also employs complex
post-processing of the kNN outputs.
After centering (Kcent and Kweighted) skewness
became markedly smaller than that of the non-
centered cosine K. F1 score also improved with the
decrease in skewness. In particular, weighted cen-
tering (Kweighted) slightly outperformed GAMBL,
though the difference was small. Recall however
that Kcent and Kweighted only use naive bag-of-words
features, unlike GAMBL.
7.2 Document classification
7.2.1 Task and dataset
Two multiclass document classification datasets
were used: Reuters Transcribed and Mini News-
groups, distributed at http://archive.ics.uci.edu/ml/.
The properties of the datasets are summarized in Ta-
ble 2.
7.2.2 Evaluation
The performance was evaluated by the F1 score
(equivalent to accuracy in this task) of prediction us-
ing leave-one-out cross validation, due to the limited
number of documents.
7.2.3 Compared methods
We used the cosine similarity as the base sim-
ilarity matrix (K). The centered similarity matrix
(Kcent) and its weighted variant (Kweighted), Mutual
4The macro-averaged F1 of GAMBL was calculated from
the per-word F1 scores listed in Table 1 of (Decadt et al, 2004).
621
Method F1 score Skewness
K 56.7 1.61
Kcent 61.2 0.11
Kweighted 60.2 0.04
Mutual Proximity 60.2 ?0.10
Graph Laplacian 57.2 0.37
(a) Reuters Transcribed
Method F1 score Skewness
K 76.5 4.37
Kcent 79.0 1.56
Kweighted 79.4 1.68
Mutual Proximity 79.0 0.49
Graph Laplacian 77.6 2.13
(b) Mini Newsgroups
Table 3: Document classification results: F1 score (%)
(larger is better) and skewness of the N10 distribution for
each similarity measure (smaller is better).
Proximity, and graph Laplacian based kernels were
computed from K.
kNN classification was done in a standard way:
The class of object x is predicted by the majority
vote from k = 10 objects most similar to x, mea-
sured by a specified similarity measure. The param-
eter k for the kNN classification, the voting scheme
(i.e., either unweighted or weighted majority vote),
? in Kweighted, and the best graph Laplacian kernel
were selected by leave-one-out cross validation.
7.2.4 Result
Table 3 shows the F1 score and the skewness of
the N10 distribution of the respective methods in
document classification. Centered cosine (Kcent)
outperformed uncentered cosine similarity K, and
achieved an F1 score comparable to Mutual Proxim-
ity. Weighted centering (Kweighted) further improved
F1 on the Mini Newsgroups data.
8 Conclusion
We have shown that centering similarity matrices re-
duces the emergence of hubs in the data, and conse-
quently improves the accuracy of nearest neighbor
classification. We have theoretically analyzed why
objects most similar to the mean tend to make hubs,
and also proved that centering cancels the bias in the
distribution of inner products, and thus is expected
to reduce hubs.
In WSD and document classification tasks, kNN
classifiers showed much better performance with
centered similarity measures than non-centered
ones. Weighted centering shifts the origin towards
hubs more aggressively, and further improved the
classification performance in some cases.
In future work, we plan to exploit the class distri-
bution in the dataset to make more effective similar-
ity measures; notice that the hubness weighted cen-
tering of Section 6 is an unsupervised method, in the
sense that class information was not used for deter-
mining weights. We will investigate if more effec-
tive weighting can be done using this information.
Acknowledgments
We thank anonymous reviewers for helpful com-
ments.
References
Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh,
and Suvrit Sra. 2005. Clustering on the unit hyper-
sphere using von Mises-Fisher distributions. Journal
of Machine Learning Research, 6:1345?1382.
P. Yu. Chebotarev and E. V. Shamis. 1997. The matrix-
forest theorem and measuring relations in small social
groups. Automation and Remote Control, 58(9):1505?
1514.
Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi,
and Luca Cazzanti. 2009. Similarity-based classifi-
cation: Concepts and algorithms. Journal of Machine
Learning Research, 10:747?776.
Bart Decadt, Ve?ronique Hoste, Walter Daelemans, and
Antal Van den Bosch. 2004. GAMBL, genetic algo-
rithm optimization of memory-based WSD. In Rada
Mihalcea and Phil Edmonds, editors, Proceedings of
the 3rd International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text (Senseval-
3), pages 108?112.
L. Eriksson, E. Johansson, N. Kettaneh-Wold, J. Trygg,
C. Wikstro?m, and S. Wold. 2006. Multi- and
Megavariate Data Analysis, Part 1, Basic Principles
and Applications. Umetrics, Inc.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?08),
pages 897?906, Honolulu, Hawaii, USA.
Douglas H. Fisher and Hans-Joachim Lenz, editors.
1996. Learning from Data: Artificial Intelligence and
622
Statistics V: Workshop on Artificial Intelligence and
Statistics. Lecture Notes in Statistics 112. Springer.
Ruixin Guo and Sounak Chakraborty. 2010. Bayesian
adaptive nearest neighbor. Statistical Analysis and
Data Mining, 3(2):92?105.
Daniel Jurafsky and James H. Martin. 2008. Speech and
Language Processing. Prentice Hall, 2nd edition.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: a new benchmark collection for text
categorization research. Journal of Machine Learning
Research, 5:361?397.
Colin Mallows. 1991. Another comment on O?Cinneide.
The American Statistician, 45(3):257.
K. V. Mardia and P. Jupp. 2000. Directional Statistics.
John Wiley and Sons, 2nd edition.
K. V. Mardia, J. T. Kent, and J. M. Bibby. 1979. Multi-
variate Analysis. Academic Press.
Brij M. Masand, Gordon Linoff, and David L. Waltz.
1992. Classifying news stories using memory based
reasoning. In Proceedings of the 15th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR ?92), pages
59?65.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Rada Mihalcea and Phil Edmonds, editors,
Proceedings of the 3rd International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text (Senseval-3), pages 25?28, Barcelona, Spain.
Rada Mihalcea. 2004. Co-training and self-training for
word sense disambiguation. In Hwee Tou Ng and
Ellen Riloff, editors, Proceedings of the 8th Confer-
ence on Computational Natural Language Learning
(CoNLL ?04), pages 33?40, Boston, Massachusetts,
USA.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association of Compu-
tational Linguistics: Human Language Technologies
(ACL ?08), pages 236?244, Columbus, Ohio, USA.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:10:1?10:69.
Ali Mustafa Qamar, E?ric Gaussier, Jean-Pierre Cheval-
let, and Joo-Hwee Lim. 2008. Similarity learning for
nearest neighbor classification. In Proceedings of the
8th International Conference on Data Mining (ICDM
?08), pages 983?988, Pisa, Italy.
Milos? Radovanovic?, Alexandros Nanopoulos, and Mir-
jana Ivanovic?. 2010a. Hubs in space: Popular nearest
neighbors in high-dimensional data. Journal of Ma-
chine Learning Research, 11:2487?2531.
Milos? Radovanovic?, Alexandros Nanopoulos, and Mir-
jana Ivanovic?. 2010b. On the existence of obstinate
results in vector space models. In Proceedings of the
33rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR ?10), pages 186?193, Geneva, Switzerland.
Marco Saerens, Franc?ois Fouss, Luh Yen, and Pierr
Dupont. 2004. The principal components analysis
of graph, and its relationships to spectral clustering.
In Proceedings of the 15th European Conference on
Machine Learning (ECML ?04), Lecture Notes in Ar-
tificial Intelligence 3201, pages 371?383, Pisa, Italy.
Springer.
Dominik Schnitzer, Arthur Flexer, Markus Schedl, and
Gerhard Widmer. 2012. Local and global scaling re-
duce hubs in space. Journal of Machine Learning Re-
search, 13:2871?2902.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24:97?123.
Alexander J. Smola and Risi Kondor. 2003. Kernels and
regularization on graphs. In Learning Theory and Ker-
nel Machines: 16th Annual Conference on Learning
Theory and 7th Kernel Workshop, Proceedings, Lec-
ture Notes in Artificial Intelligence 2777, pages 144?
158. Springer.
Anders S?gaard. 2011. Semisupervised condensed near-
est neighbor for part-of-speech tagging. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL ?11), pages 48?
52, Portland, Oregon, USA.
Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji Mat-
sumoto, and Marco Saerens. 2012. Investigating the
effectiveness of Laplacian-based kernels in hub reduc-
tion. In Proceedings of the 26th AAAI Conference on
Artificial Intelligence (AAAI-12), pages 1112?1118,
Toronto, Ontario, Canada.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL ?10), pages 948?957,
Uppsala, Sweden.
Jigang Wang, Predrag Neskovic, and Leon N. Cooper.
2006. Neighborhood size selection in the k-nearest-
neighbor rule using statistical confidence. Pattern
Recognition, 39(3):417?423.
Kilian Q. Weinberger and Lawrence K. Saul. 2009. Dis-
tance metric learning for large margin nearest neighbor
classification. Journal of Machine Learning Research,
10:207?244.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR ?99), pages 42?49, Berkeley, California, USA.
623
Proceedings of the TextGraphs-8 Workshop, pages 79?87,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
A Graph-Based Approach to Skill Extraction from Text?
Ilkka Kivima?ki1, Alexander Panchenko4,2, Adrien Dessy1,2, Dries Verdegem3,
Pascal Francq1, Ce?drick Fairon2, Hugues Bersini3 and Marco Saerens1
1ICTEAM, 2CENTAL, Universite? catholique de Louvain, Belgium
3IRIDIA, Universite? libre de Bruxelles, Belgium
4 Digital Society Laboratory LLC, Russia
Abstract
This paper presents a system that performs
skill extraction from text documents. It out-
puts a list of professional skills that are rele-
vant to a given input text. We argue that the
system can be practical for hiring and man-
agement of personnel in an organization. We
make use of the texts and the hyperlink graph
of Wikipedia, as well as a list of professional
skills obtained from the LinkedIn social net-
work. The system is based on first computing
similarities between an input document and
the texts of Wikipedia pages and then using a
biased, hub-avoiding version of the Spreading
Activation algorithm on the Wikipedia graph
in order to associate the input document with
skills.
1 Introduction
One of the most difficult tasks of an employer can
be the recruitment of a new employee out of a long
list of applicants. Another challenge of the employer
is to keep track of the skills and know-how of their
employees in order to direct the right people to work
on things they know. In the scientific community,
editors of journals and committees of conferences
always face the task of assigning suitable reviewers
for a tall pile of submitted papers. The tasks de-
scribed above are example problems of expertise re-
trieval (Balog et al, 2012). It is a subfield of in-
formation retrieval that focuses on inferring asso-
ciations between people, expertise and information
content, such as text documents.
?Part of this work has been funded by projects with the
?Re?gion wallonne?. We thank this institution for giving us the
opportunity to conduct both fundamental and applied research.
In addition, we thank Laurent Genard and Ste?phane Dessy for
their contributions for the work.
In this paper, we propose a method that makes a
step towards a solution of these problems. We de-
scribe an approach for the extraction of professional
skills associated with a text or its author. The goal of
our system is to automatically extract a set of skills
from an input text, such as a set of articles written
by a person. Such technology can be potentially
useful in various contexts, such as the ones men-
tioned above, along with expertise management in a
company, analysis of professional blogs, automatic
meta-data extraction, etc.
For succeeding in our goal, we exploit Wikipedia,
a list of skills obtained from the LinkedIn social net-
work and the mapping between them. Our method
consists of two phases. First, we analyze a query
document with a vector space model or a topic
model in order to associate it with Wikipedia arti-
cles. Then, using these initial pages, we use the
Spreading Activation algorithm on the hyperlink
graph of Wikipedia in order to find articles that cor-
respond to LinkedIn skills and are related or central
to the initial pages.
One difficulty with this approach is that it of-
ten results in some skills, which can be identified
as hubs of the Wikipedia graph, constantly being
retrieved, regardless of what the input is. In or-
der to avoid this pitfall, we bias the activation to
avoid spreading to general, or popular nodes. We
try different measures of node popularity to redirect
the spreading and perform evaluative experiments
which show that this biasing in fact improves re-
trieval results.
We have built a web service that enables anyone
to test our skill extraction system. The name of the
system is Elisit, an abbreviation from ?Expertise
Localization from Informal Sources and Information
79
Technologies? and conveying the idea of trying to
elicit, i.e. draw forth latent information about exper-
tise in a target text. According to the best of our
knowledge, we are the first to propose such a system
and describe openly the method behind it.
2 Related work
The recent review of Balog et al (2012) gives a
thorough presentation of the problems of expertise
retrieval and of the methodology used for solving
them. They classify these problems in subcategories
of expert retrieval and expert profiling. The former
means the task of providing a name of a person who
is an expert in a field that is presented as a query,
while the latter means assigning expertise to a per-
son, or some other entity based on information that
is available of that entity. Recent expertise retrieval
research has focused on the TREC enterprise track,
which uses the TREC W3C and CERC corpora (Ba-
log et al, 2008). These datasets contain annotated
crawls of websites. The task in the TREC enterprise
challenge is to build a model that performs expert
retrieval and document retrieval based on a set of
query topics, which correspond to expertise areas.
Our approach is quite different from the one used
in the TREC challenge, as we focus on a fixed
list of skills gathered from the LinkedIn website.
Thus, we were not able to directly compare our sys-
tem to the systems participating in the TREC enter-
prise track. Our problem shares some resemblance
with the INEX entity-ranking track (Demartini et al,
2010), where the goal was to rank Wikipedia pages
related to queries about a given topic. Our skill re-
trieval task can also be seen as an entity ranking task,
where the entities are Wikipedia pages that corre-
spond to skills.
LinkedIn has developed methods for defining
skills and for finding relations between them (Sko-
moroch et al, 2012). These techniques are used in
their service, for example, for recommending job
opportunities to the users. The key difference of
our technology is that it allows a user to search
for skills by submitting an arbitrary text, instead of
only searching for skills related to a certain skill.
Although expertise retrieval has been an active re-
search topic for some time, there have not been
many methods for explicitly assigning particular
skills to text content or people producing text con-
tent.
Our method consists of two steps. First, we ap-
ply a text similarity method to detect the relevant
Wikipedia pages. Second, we enrich the results
with graph mining techniques using the hyperlink
graph of Wikipedia. We have not found a simi-
lar combination being applied for skill extraction
before, although both parts have been well studied
in similar contexts before. For instance, Steyvers
et al (2004) proposed the Author-Topic Model, a
graphical model based on LDA (Blei et al, 2003),
that associates authors of texts with topics detected
from those texts.
Wikipedia has been already used in NLP research
both as a corpus and as a semantic network. Its hy-
perlink graph is a collaboratively constructed net-
work, as opposed to manually crafted networks
such as WordNet (Miller, 1995). Gabrilovich and
Markovitch (2007) introduced Explicit Semantic
Analysis (ESA), where the words of a document are
represented as mixtures of concepts, i.e. Wikipedia
pages, according to their occurence in the body texts
of the pages. The experimental results show that this
strategy works very well and outranks, for exam-
ple, LSA (Landauer and Dumais, 1997) in the task
of measuring document similarity. ESA was later
extended by taking into account the graph structure
provided by the links in Wikipedia (Yeh et al, 2009).
The authors of this work used a PageRank-based al-
gorithm on the graph for measuring word and doc-
ument similarity. This approach was coined Wiki-
Walk.
Associating the elements of a text document un-
der analysis with Wikipedia pages involves itself al-
ready many problems often encountered in NLP. The
process where certain words and multiword expres-
sions are associated with a certain Wikipedia page
has been called Wikification (Mihalcea and Csomai,
2007). In our work, we take a more general ap-
proach, and try to associate the full input text to a
set of Wikipedia pages according to different vec-
tor space models. The models and the details of this
strategy are explained in section 3.3.
The Elisit system uses the Spreading Activa-
tion algorithm on the Wikipedia graph to establish
associations between texts and skills. We chose
to use Spreading Activation, as it tries to simulate
80
a cognitive associative memory (Anderson, 1983),
and the Wikipedia hyperlink network can be under-
stood as an associative network. The simulation
works by finding associations in a network of con-
cepts by spreading pulses of activation from con-
cepts into their neighbours. In the context of NLP,
the Spreading Activation algorithm has been tradi-
tionally used for word sense disambiguation (Hirst,
1988) and information retrieval (Crestani, 1997).
Gouws et al (2010) have shown that this algorithm,
applied to the Wikipedia graph, can also be used to
measure conceptual and document similarity.
3 Methodology
In this section, we will explain how the Elisit
skill extraction system works. We will first ex-
plain how the system uses data from Wikipedia and
LinkedIn. Then, we will describe the two main
components of the system, the text2wiki mod-
ule, which associates a query document with related
Wikipedia pages, and the wiki2skill module,
which aims to associate the Wikipedia pages found
by the text2wiki module with Wikipedia pages
that correspond to skills.
3.1 Wikipedia texts and links
Each page in Wikipedia contains a text that may in-
clude hyperlinks to other pages. We make the as-
sumption that there is a meaningful semantic rela-
tionship between the pages that are linked with each
other and that the Wikipedia hyperlink graph can be
exploited as an associative network. The properties
of the hyperlink structure of Wikipedia and the na-
ture of the information contained in the links have
been investigated by Koolen (2011).
In addition to the encyclopedia pages, Wikipedia
also contains, among others, category, discussion
and help pages. In our system, we are only interested
in the encyclopedia pages and the hyperlinks be-
tween them. We are using data downloaded1 on May
2nd 2012. This dump encompasses 3,983,338 pages
with 247,560,469 links, after removal of the redi-
rect pages. The Wikipedia graph consists of a giant
Strongly Connected Component (SCC) of 3,744,419
nodes, 4130 SCC?s of sizes from 61 to 2 nodes and
228,881 nodes that form their own SCC?s.
1http://dumps.wikimedia.org/
3.2 LinkedIn skills
We gathered a list of skills from the LinkedIn social
network2. The list includes skills which the users
can assign to their profiles. This enables the site
to recommend new contacts or open job opportu-
nities to each user. The skills in the list have been
generated by an automated process developed by
LinkedIn (Skomoroch et al, 2012). The process de-
cides, whether a word or a phrase or a skill suggested
by a user is actually a skill through an analysis of the
text contained in the user profile pages.
Each LinkedIn skill has its own webpage that con-
tains information about the skill. One piece of infor-
mation contained in most of these pages is a link
to a Wikipedia article. According to Skomoroch et
al. (2012), LinkedIn automatically builds this map-
ping. However, some links are manually verified
through crowdsourcing. Not all skill pages contain
a link to Wikipedia, but these skills are often ei-
ther very specific or ambiguous. Thus, we decided
to remove these skills from our final list. The list
of skills used in the system was extracted from the
LinkedIn site in September 2012. After removal of
the skills without a link to Wikipedia, the list con-
tained 27,153 skills.
3.3 text2wiki module
The goal of the text2wiki module is to retrieve
Wikipedia articles that are relevant to an input text.
The output of the module is a vector of sim-
ilarities between the input document and all arti-
cles of the English Wikipedia that contain at least
300 characters. There are approximately 3.3 mil-
lion such pages. We only retrieve the 200 Wikipedia
pages that are most similar to the input document.
Thus, each input text is represented as a sparse vec-
tor a(0), which has 200 non-zero elements out of
3,983,338 dimensions corresponding to the full list
of Wikipedia pages. Each non-zero value ai(0) of
this vector is a semantic similarity of the query with
the i-th Wikipedia article. This approach stems from
ESA, mentioned above. The vector a(0) is given as
input to the second module wiki2skill.
The text2wiki module relies on the Gensim
library (R?ehu?r?ek and Sojka, 2010)3. In particular,
2http://www.linkedin.com/skills
3http://radimrehurek.com/gensim
81
we have used four different text similarity func-
tions, based respectively on the classical Vector
Space Models (VSM?s) (Berry et al, 1994), LSA
and LDA:
(a) TF-IDF (300,000 dimensions)
(b) LogEntropy (300,000 dimensions)
(c) LogEntropy + LSA (200 dimensions)
(d) LogEntropy + LDA (200 topics)
First, each text is represented as a vector x in
a space of the 300,000 most frequent terms in the
corpus, each appearing at least in 10% of the docu-
ments (excluding stopwords). We limited the num-
ber of dimensions to 300,000 to reduce computa-
tional complexity. The models (a) and (b) directly
use this representation, while for (c) and (d) this ini-
tial representation is transformed to a vector x? in a
reduced space of 200 dimensions/topics. For LSA
and LDA, the number of dimensions is often empir-
ically selected from the range [100 ? 500] (Foltz,
1996; Bast and Majumdar, 2005). We followed this
practice. From the vector representations (x or x?),
the similarity between the input document and each
Wikipedia article is computed using the cosine sim-
ilarity.
Pairwise comparison of a vector of 300,000 di-
mensions against 3.3 million vectors of the same
size has a prohibitive computational cost. To make
our application practical, we use an inverted index of
Gensim to efficiently retrieve articles semantically
related to an input document.
3.4 wiki2skill module
The wiki2skill module performs the Spread-
ing Activation algorithm using the initial activations
provided by the text2wiki module and returns a
vector of final activations of all the nodes of the net-
work and a vector containing the activations of only
the nodes corresponding to skills.
The basic idea of Spreading Activation is to ini-
tially activate a set of nodes in a network and then
iteratively spread the activation into the neighbour-
ing nodes. This can actually be interpreted in many
ways opening up a wide space of algorithms that can
lead to different results. One attempt for an exact
definition of the Spreading Activation algorithm can
be found in the work of Shrager et al (1987). Their
formulation states that if a(0) is a vector containing
the initial activations of each node of the network,
then after each iteration, or time step, or pulse t, the
vector of activations is
a(t) = ?a(t? 1) + ?WTa(t? 1) + c(t), (1)
where ? ? [0, 1] is a decay factor which controls the
conservation of activation during time, ? ? [0, 1] is
a friction factor, which controls the amount of acti-
vation that nodes can spread to their neighbors, c(t)
is an activation source vector and W is a weighted
adjacency matrix, where the weights control the
amount of activation that flows through each link in
the network. In some cases, iterating eq. (1) leads
to a converged activation state, but often, especially
when dealing with large networks, it is more prac-
tical to set the number of pulses, T , to some fixed,
low number.
As already stated, this formulation of Spread-
ing Activation spans a wide space of different al-
gorithms. In particular, this space contains many
random walk based algorithms. By considering the
case where ? = 0, ? = 1, c(t) = 0 and where
the matrix W is row-stochastic, the Spreading Ac-
tivation model boils down to a random walk model
with a transition probability matrix W, where a(t)
contains the proportion of random walkers at each
node when the initial proportions are given by a(0).
When the situation is changed by choosing c(t) =
(1 ? ?)a(0), we obtain a bounded Random Walk
with Restart model (Pan et al, 2004; Mantrach et
al., 2011).
Early experiments with the first versions of the al-
gorithm revealed an activation bias towards nodes
that correspond to very general Wikipedia pages
(e.g. the page ?ISBN?, which is often linked to in
the References section of Wikipedia pages). These
nodes have a high input degree, but are often not rel-
evant for the given query. This problem is often en-
countered when analysing large graphs with random
walk based measures. It is known that they can be
dominated by the stationary distribution of the cor-
responding Markov Chain (Brand, 2005).
To tackle this problem, we assign link weights
according to preferential transition probabilities,
which define biased random walks that try to avoid
hub nodes. They have been studied e.g. in the con-
text of stochastic routing of packages in scale-free
82
networks (Fronczak and Fronczak, 2009). These
weights are given by
w?ij =
pi?j
?
k:(i,k)?E
pi?k
, (2)
where pij is a popularity index and ? is a biasing
parameter, which controls the amount of activation
that flows from node i to node j based on the pop-
ularity of node j. For the popularity index, we con-
sidered three options. First, we tried simply the in-
put degree of a node. As a second option, we used
the PageRank score of the node (Page et al, 1999)
which corresponds to the node?s weight in the sta-
tionary distribution of a random surfer that surfs
Wikipedia by clicking on hyperlinks randomly. As a
third popularity index, we used a score based on the
HITS algorithm (Kleinberg, 1999), which is simi-
lar to PageRank, but instead assigns two scores, an
authority score and a hub score. In short, a page
has a high authority score, if it is linked to by many
hub pages, and vice versa. In the case of HITS, the
popularity index was defined as the product of the
authority and hub scores of the node. When ? = 0,
wij is equal for all links leaving from node i, but
when ? < 0, activation will flow more to less popu-
lar nodes and less to popular nodes. We included the
selection of a suitable value for ? as a parameter to
be tuned along with the rest of the spreading strat-
egy in quantitative experiments that are presented in
section 5.2. These experiments show that biasing
the activation to avoid spreading to popular nodes
indeed improves retrieval results.
We also decided to investigate whether giving
more weight to links that exist in both directions
would improve results. The Wikipedia hyperlink
graph is directed, but in some cases two pages may
contain a link to each other. We thus adjust the link
weights wij so that wij = ?w?ij if (j, i) ? E and
wij = w?ij otherwise, where ? ? 1 is a bidirectional
link weight. With large values of ?, more activation
will flow through bidirectional links than links that
exist only in one direction. After this weighting,
the final link weight matrix W is obtained by nor-
malizing each element with its corresponding row
sum to make the matrix row-stochastic. This makes
the model easier to interpret by considering random
walks. However, in a traditional Spreading Activa-
tion model the matrix W is not required to be row-
stochastic. We plan to investigate in the future, how
much the normalization affects the results.
The large size of the Wikipedia graph challenges
the use of Spreading Activation. In order to pro-
vide a usable web service, we would need the system
to provide results fast, preferably within fractions of
seconds. So far, we have dealt with this issue within
the wiki2skill module by respresenting the link
weight matrix W of the whole Wikipedia graph us-
ing the sparse matrix library SciPy4. Each itera-
tion of the Spreading Activation is then achieved by
simple matrix arithmetic according to eq. (1). As
a result, the matrix W must be precomputed from
the adjacency matrix for a given value of the bias-
ing parameter ? and the bidirectional link weight ?
when the system is launched. Thus, they cannot be
selected separately for each query from the system.
Currently, the system can perform one iteration of
spreading activation within less than one second, de-
pending on the sparsity of the activation vector. Our
experiments indicate that the results are quite stable
after five spreading iterations, meaning that we nor-
mally get results with the wiki2skill module in
about one to three seconds.
4 The Elisit skill extraction system
The Elisit system integrates the text2wiki
and the wiki2skill modules. We have built a
web application5 which lets everyone try our method
and use it from third-party applications. Due to this
web service, the Elisit technology can be eas-
ily integrated into systems performing skill search,
email or document analysis, HR automatization,
analysis of professional blogs, automatic meta-data
extraction, etc. The web interface presents the user
the result of the skill extraction (a list of skills) as
well as the result of the text2wiki module (a list
of Wikipedia pages). Each retrieved skill also con-
tains a link to the corresponding Wikipedia page.
Figure 1 presents an example of results provided
by the Elisit system. It lists skills extracted
from the abstract of the chapter Support vector ma-
chines and machine learning on documents from
4http://www.scipy.org/
5GUI: http://elisit.cental.be/; RESTful web
service: http://elisit.cental.be:8080/.
83
Figure 1: Skills extracted from a text about text document
categorization.
Introduction to Information Retrieval by Manning
et al (2008). As one can observe, the Wikipedia
pages found by the text2wiki module represent
many low-level topics, such as ?Desicion bound-
ary?, ?Ranking SVM? or ?Least square SVM?. On
the other hand, the skills retrieved after using the
wiki2skill module provide high-level topics rel-
evant to the input text, such as ?SVM?, ?Machine
Learning? or ?Classification?. These general topics
are more useful, since a user, such as an HR man-
ager, may be confused by too low-level skills.
5 Experiments & results
5.1 Evaluation of the text2wiki module
In order to compare the four text similarity func-
tions, we collected p = 200, 000 pairs of semanti-
cally related documents from the ?See also? sections
of Wikipedia articles. A good model is supposed
to assign a high similarity to these pairs. However,
since the distribution of similarity scores depends
on the model, one cannot simply compare the mean
similarity s? over the set of pairs. Thus, we used a
Model z-score
TF-IDF 8459
LogEntropy 4370
LogEntropy + LDA 2317
LogEntropy + LSA 2143
Table 1: Comparison of different text similarity functions
on the Wikipedia ?See also? dataset.
z-score as evaluation metric. The z-scores are com-
puted as
z =
s?? ??
?
??2/p
(3)
where ?? and ?? are sample estimates of mean and
standard deviation of similarity scores for a given
model. These sample estimates have been calculated
from a set of 1,000,000 randomly selected pairs of
articles. Table 1 presents the results of this experi-
ment. It appears that more complex models (LSA,
LDA) are outperformed on this task by the simpler
vector space models (TF-IDF, LogEntropy). This
can be just a special case with this experimental
setting and perhaps another choice of the number
of topics could give better results. Thus, further
meta-parameter optimization of LSA and LDA is
one approach for improving the performance of the
text2wiki module.
5.2 Evaluation of the wiki2skill module
In order to find the optimal strategy of applying
Spreading Activation, we designed an evaluation
protocol relying on related skills listed on each
LinkedIn skill page. These are automatically se-
lected by computing similarities between skills from
user profiles (Skomoroch et al, 2012). Each skill
page contains at most 20 related skills.
For the evaluation procedure, we choose an initial
node i, corresponding to a LinkedIn skill, and acti-
vate it by setting a(0) = ei, that is a vector contain-
ing 1 in its i-th element and zeros elsewhere. Then,
we compute a(T ) with some spreading strategy and
for some number of steps T , filter out the skill nodes
and rank them according to their final activations. To
measure how well the related skills are represented
in this ranked list of skills, we use Precision at 1, 5
and 10, and R-Precision to evaluate the accuracy of
the first ranked results and Recall at 100 to see how
well the algorithm manages to activate all of the re-
84
lated skills.
There are many LinkedIn skills that are not well
represented in the Wikipedia graph, because of am-
biguity issues, for instance. To prevent these anoma-
lies from causing misguiding results, we selected a
fixed set of 16 representative skills for the evalua-
tion. These skills were ?Statistics?, ?Hidden Markov
Models?, ?Telecommunications?, ?MeeGo?, ?Digi-
tal Printing?, ?OCR?, ?Linguistics?, ?Speech Syn-
thesis?, ?Classical?, ?Impressionist?, ?Education?,
?Secondary Education?, ?Cinematography?, ?Exec-
utive producer?, ?Social Sciences?, ?Political Soci-
ology?.
Developing a completely automatic optimisation
scheme for this model selection task would be diffi-
cult because of the number of different parameters,
the size of the Wikipedia graph and the heuristic na-
ture of the whole methodology. Thus, we decided to
rely on a manual evaluation of the results.
Exploring the whole space of algorithms spanned
by eq. (1) would be too demanding as well. That is
why we have so far tested only a few models. In the
preliminary experiments that we conducted with the
system, we observed that using a friction factor ?
smaller than one had little effect on the results, and
thus we decided to always use ? = 1. Otherwise,
we experimented with three models, which we will
simply refer to as models 1, 2 and 3 and which we
define as follows
? model 1: ? = 0 and c(t) = 0;
? model 2: ? = 1 and c(t) = 0;
? model 3: ? = 0 and c(t) = a(0).
In model 1, activation is not conserved in a node
but only depends on the activation it has received
from its neighbors after each pulse. In contrast, the
activation that a node receives is completely con-
served in model 2. Model 3 corresponds to the Ran-
dom Walk with Restart model, where the initial ac-
tivation is fed to the system at each pulse. Models
1 and 2 eventually converge to a stationary distribu-
tion that is independent of the initial activation vec-
tor. This can be beneficial in situations where some
of the initially activated nodes are noisy, or irrele-
vant, because it allows the initial activation to die
out, or at least become lower than the activation of
other, possibly more relevant nodes. With Model 3,
the initially activated nodes remain always among
the most activated nodes, which is not necessarily a
robust choice.
The outcomes of the experiments demonstrated
that model 2 and model 3 perform equally well. In-
deed, these models are very similar, and apparently
their small differences do not affect the results much.
However, model 1 provided constantly worse results
than the two other models. Thus, we decided to use
model 3, corresponding to the Random Walk with
Restart model, in the system and in selecting the rest
of the spreading strategy.
We also evaluated different settings for the link
weighting scheme. Here, we faced a startling result,
namely that increasing the bidirectional link weight
? all the way up to the value ? = 15 kept improving
the results according to almost all evaluation mea-
sures. This would indicate that links that exist in
only one direction do not convey a lot of semantic
relatedness. However, we assume that this is a phe-
nomenon caused by the nature of the experiment and
the small subset of skills used in it, and not necessar-
ily a general phenomenon for the whole Wikipedia
graph. In our experiments, the improvement was
more drastic in the range ? ? [1, 5] after which a
damping effect can be observed. For this reason,
we decided to set the bidirectional link weight in the
Elisit system to ? = 5.
We observed a similar phenomenon for the num-
ber of pulses T . Increasing its value up to T = 8 im-
proved constantly the results. However, again, there
was no substantial change in the results in the range
T ? [5, 8]. In the web service, the number of pulses
of the spreading activation can be determined by the
user.
In addition to the parameters discussed above, the
link weighting involves the popularity index pij and
the biasing parameter ?. An overview of the ef-
fect of these two choices can be seen in Table 2,
which presents the results with the different eval-
uation measures. These results were obtained by
setting parameters as described earlier in this sec-
tion. First, we can see from this table that using
negative values for ? in the weighting improves re-
sults compared to the natural random walk, i.e. the
case ? = 0. This indicates that our strategy of bi-
asing the spreading of activation to avoid popular
nodes indeed improves the results. We can also see
85
Pre@1 Pre@5 Pre@10 R-Pre Rec@100
? din PR HITS din PR HITS din PR HITS din PR HITS din PR HITS
0 0 0 0 0.119 0.119 0.119 0.156 0.156 0.156 0.154 0.154 0.154 0.439 0.439 0.439
-0.2 0 0 0 0.206 0.238 0.206 0.222 0.216 0.213 0.172 0.193 0.185 0.469 0.469 0.494
-0.4 0 0 0 0.225 0.263 0.169 0.203 0.200 0.150 0.185 0.204 0.148 0.503 0.498 0.476
-0.6 0 0 0.063 0.238 0.225 0.119 0.200 0.197 0.141 0.186 0.193 0.119 0.511 0.517 0.418
-0.8 0 0 0 0.213 0.181 0.075 0.191 0.197 0.113 0.171 0.185 0.109 0.515 0.524 0.384
-1 0 0 0 0.169 0.156 0.063 0.178 0.197 0.091 0.154 0.172 0.097 0.493 0.518 0.336
Table 2: The effect of the biasing parameter ? and the choice of popularity index on the results in the evaluation of the
wiki2skill module.
that using Pagerank as the popularity index provided
overall better results than using the input degree,
which again yielded better results than using HITS.
Thus, biasing according to the input connections of
nodes seems more preferable than biasing accord-
ing to co-citation or co-reference connections. The
low scores with Precision@1 are understandable,
because of the low number of positives (at most 20
related skills) in comparison to the total number of
skills (over 27,000). In the Elisit system, we use
the Pagerank score as the popularity index and set
the value of the biasing parameter to ? = ?0.4.
5.3 Evaluation of the whole Elisit system
We adapted the evaluation procedure used for the
wiki2skill module, described in the previous
section, in order to test the whole Elisit sys-
tem. This time, instead of activating the node of
a given skill, we activated the nodes found by the
text2wiki module when fed with the Wikipedia
article corresponding to the skill. We run the Spread-
ing Activation algorithm with the setup presented in
the previous section. To make the evaluation more
realistic, the initial activation of the target skill node
is set to zero (instead of 1, i.e. the cosine of a vector
with itself).
The system allows its user to set the number of
initially activated nodes. We investigated the ef-
fect of this choice by measuring Precision and Re-
call according to the related skills, and by looking
at the average rank of the target skill on the list of
final activations. However, there was no clear trend
in the results when testing with 1-200 initially ac-
tivated nodes. Nevertheless, we have noticed that
using more than 20 initially activated nodes rarely
improves the results. We must also emphasize that
the choice of the number of initially activated nodes
depends on the query, especially its length.
We also wanted to compare the different VSM?s
VSM Pre@1 Pre@5 Pre@10 R-Pre Rec@100
TF-IDF 0.042 0.231 0.214 0.190 0.516
LogEntropy 0.068 0.216 0.212 0.193 0.525
LogEnt + LSA 0.042 0.180 0.181 0.163 0.491
LogEnt + LDA 0.089 0.193 0.174 0.159 0.470
Table 3: Comparison of the different models of the
text2wiki module in the performance of the whole
Elisit system.
of the text2wiki module when using the whole
Elisit system. We did this by comparing Pre-
cision and Recall at different ranks w.r.t. the re-
lated skills of the target skill found on LinkedIn.
Thus, this experiment combines the experiments in-
troduced in sections 5.1, where the evaluation was
based on the ?See also? pages, and 5.2, where we
used a set of 16 target skills and their related skills.
Table 3 reports the Precision and Recall values ob-
tained with the different VSM?s. These values result
from an average over 12 different numbers of ini-
tially activated nodes. They confirm the conclusion
drawn from the experiment in section 5.1, namely
that the LogEntropy and TF-IDF models outperform
LSA and LDA models for this task.
6 Conclusion and future work
We have presented a method for skill extraction
based on Wikipedia articles, their hyperlink graph,
and a set of skills built by LinkedIn. We have also
presented the Elisit system as a reference imple-
mentation of this method. This kind of a system
has many potential applications, such as knowledge
management in a company or recommender systems
of websites. We have demonstrated with examples
and with quantitative evaluations that the system in-
deed extracts relevant skills from text. The evalu-
ation experiments have also allowed us to compare
and finetune different strategies and parameters of
the system. For example, we have shown that using
a bias to avoid the spreading of activation to popular
86
nodes of the graph improves retrieval results.
This work is still in progress, and we have many
goals for improvement. One plan is to compute link
weights based on the contents of linked pages using
their vector space representation in the text2wiki
module. The method and system proposed in the
paper could also be extended to other languages. Fi-
nally, our methodology can potentially be used to
different problems than skill extraction by substitut-
ing the LinkedIn skills with a list of Wikipedia pages
from another domain.
References
John R Anderson. 1983. A spreading activation theory of
memory. Journal Of Verbal Learning And Verbal Behavior,
22(3):261?295.
Krisztian Balog, Paul Thomas, Nick Craswell, Ian Soboroff, Pe-
ter Bailey, and Arjen P De Vries. 2008. Overview of the trec
2008 enterprise track. Technical report, DTIC Document.
Krisztian Balog, Yi Fang, Maarten de Rijke, Pavel Serdyukov,
and Luo Si. 2012. Expertise retrieval. Foundations and
Trends in Information Retrieval, 6(2-3):127?256.
Holger Bast and Debapriyo Majumdar. 2005. Why spectral
retrieval works. In Proceedings of the 28th annual interna-
tional ACM SIGIR conference on Research and development
in information retrieval, pages 11?18. ACM.
Michael W. Berry, Susan T. Dumais, and Gavin W. O?Brien.
1994. Using linear algebra for intelligent information re-
trieval. Technical Report UT-CS-94-270.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learning Re-
search, 3:993?1022, March.
Matthew Brand. 2005. A random walks perspective on max-
imizing satisfaction and profit. Proceedings of the 2005
SIAM International Conference on Data Mining.
Fabio Crestani. 1997. Application of spreading activation tech-
niques in information retrieval. Artificial Intelligence Re-
view, 11(6):453?482.
Gianluca Demartini, Tereza Iofciu, and Arjen P De Vries. 2010.
Overview of the inex 2009 entity ranking track. In Focused
Retrieval and Evaluation, pages 254?264. Springer.
Peter W Foltz. 1996. Latent semantic analysis for text-based
research. Behavior Research Methods, Instruments, & Com-
puters, 28(2):197?202.
Agata Fronczak and Piotr Fronczak. 2009. Biased random
walks in complex networks: The role of local navigation
rules. Physical Review E, 80(1):016107.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based explicit se-
mantic analysis. In IJCAI?07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence, pages
1606?1611, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Stephan Gouws, G-J van Rooyen, and Herman A. Engelbrecht.
2010. Measuring conceptual similarity by spreading acti-
vation over wikipedia?s hyperlink structure. In Proceedings
of the 2nd Workshop on The People?s Web Meets NLP: Col-
laboratively Constructed Semantic Resources, pages 46?54,
Beijing, China, August. Coling 2010 Organizing Committee.
Graeme Hirst. 1988. Semantic interpretation and ambiguity.
Artificial Intelligence, 34(2):131?177.
Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked
environment. Journal of the ACM, 46(5):604?632.
Marijn Koolen. 2011. The Meaning Of Structure: the Value
of Link Evidence for Information Retrieval. Ph.D. thesis,
University of Amsterdam, The Netherlands.
Thomas K. Landauer and Susan T. Dumais. 1997. A solu-
tion to plato?s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowledge.
Psych. review, 104(2):211.
Christopher D Manning, Prabhakar Raghavan, and Hinrich
Schu?tze. 2008. Introduction to information retrieval, vol-
ume 1. Cambridge University Press Cambridge.
Amin Mantrach, Nicolas van Zeebroeck, Pascal Francq,
Masashi Shimbo, Hugues Bersini, and Marco Saerens.
2011. Semi-supervised classification and betweenness com-
putation on large, sparse, directed graphs. Pattern Recogni-
tion, 44(6):1212?1224.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Ma?rio J.
Silva, Alberto H. F. Laender, Ricardo A. Baeza-Yates, Debo-
rah L. McGuinness, Bj?rn Olstad, ?ystein Haug Olsen, and
Andre? O. Falca?o, editors, CIKM, pages 233?242. ACM.
George A Miller. 1995. Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39?41.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Wino-
grad. 1999. The pagerank citation ranking: Bringing order
to the web. Technical Report 1999-0120, Computer Science
Department, Stanford University.
Jia-Yu Pan, Hyung-Jeong Yang, Christos Faloutsos, and Pinar
Duygulu. 2004. Automatic multimedia cross-modal corre-
lation discovery. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and data
mining, pages 653?658. ACM.
Radim R?ehu?r?ek and Petr Sojka. 2010. Software Framework
for Topic Modelling with Large Corpora. In Proceedings
of the LREC 2010 Workshop on New Challenges for NLP
Frameworks, pages 45?50, Valletta, Malta, May. ELRA.
Jeff Shrager, Tad Hogg, and Bernardo A Huberman. 1987.
Observation of phase transitions in spreading activation net-
works. Science, 236(4805):1092?1094.
Peter N Skomoroch, Matthew T Hayes, Abhishek Gupta, and
Dhanurjay AS Patil. 2012. Skill customization system, Jan-
uary 24. US Patent App. 13/357,360.
Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi, and
Thomas Griffiths. 2004. Probabilistic author-topic models
for information discovery. In Proceedings of the 10th ACM
International Conference on Knowledge Discovery and Data
Mining. ACM Press.
Eric Yeh, Daniel Ramage, Christopher D. Manning, Eneko
Agirre, and Aitor Soroa. 2009. Wikiwalk: Random walks
on wikipedia for semantic relatedness. In Graph-based
Methods for Natural Language Processing, pages 41?49.
The Association for Computer Linguistics.
87
