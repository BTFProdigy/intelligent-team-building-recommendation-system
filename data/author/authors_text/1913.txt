Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 25?30,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Language Modeling to Select Useful Annotation Data
Dmitriy Dligach
Department of  
Computer Science 
University of Colorado  
at Boulder 
Dmitriy.Dligach
@colorado.edu
Martha Palmer
Department of Linguistics 
University of Colorado  
at Boulder 
Martha.Palmer
@colorado.edu
 
 
Abstract
An annotation project typically has an abun-
dant supply of unlabeled data that can be 
drawn from some corpus, but because the 
labeling process is expensive, it is helpful to 
pre-screen the pool of the candidate instances 
based on some criterion of future usefulness. 
In many cases, that criterion is to improve the 
presence of the rare classes in the data to be 
annotated. We propose a novel method for 
solving this problem and show that it com-
pares favorably to a random sampling baseline 
and a clustering algorithm. 
1 Introduction
A data set is imbalanced when the distribution 
of classes in it is dominated by a single class. In 
Word Sense Disambiguation (WSD), the classes 
are word senses. The problem of imbalanced data 
is painfully familiar to WSD researchers: word 
senses are particularly well known for their skewed 
distributions that are also highly domain and cor-
pus dependent. Most polysemous words have a 
sense that occurs in a disproportionately high 
number of cases and another sense that is seen very 
infrequently. For example, the OntoNotes (Hovy et 
al., 2006) sense inventory defines two senses for 
the verb to add.  Of all the instances of this verb in 
the OntoNotes sense-tagged corpus, 93% are the 
instances of the predominant sense (not the arith-
metic sense!). Another fact: there are 4,554 total 
senses in the OntoNotes sense inventory for 1,713 
recently released verbs. Only 3,498 of them are 
present in the actual annotated data. More than 
1,000 senses (23%) are so rare that they are miss-
ing from the corpus altogether. More than a third 
of the released verbs are missing representative 
instances of at least one sense. In fact many of the 
verbs are pseudo-monosemous: even though the 
sense inventory defines multiple senses, only the 
most frequent sense is present in the actual anno-
tated data. For example, only 1 out of 8 senses of 
to rip is present in the data. 
The skewed nature of sense distributions is a 
fact of life. At the same time, a large-scale annota-
tion project like OntoNotes, whose goal is the crea-
tion of a comprehensive linguistic resource, cannot 
simply ignore it. That a sense is rare in a corpus 
does not mean that it is less important to annotate a 
sufficient number of instances of that sense: in a 
different domain it can be more common and not 
having enough annotated instances of that sense 
could jeopardize the success of an automatic cross-
domain WSD system. For example, sense 8 of to
rip ("to import an audio file directly from CD") is 
extremely popular on the web but it does not exist 
at all in the OntoNotes data. Only the traditional 
sense of to swap exists in the data but not the com-
puter science sense ("to move a piece of program 
into memory"), while the latter can conceivably be 
significantly more popular in technical domains.  
In general, class imbalance complicates super-
vised learning. This contention certainly holds for 
WSD. As an illustration, consider the verb to call, 
for which the OntoNotes sense inventory defines 
11 senses. Senses 3 and 5 are the most frequent: 
together they constitute 84% of the data. To inves-
tigate which classes are problematic for a classifi-
25
er, we conducted 50 supervised learning experi-
ments. In each experiment one instance of this verb 
was selected at random and used for testing while 
the rest was used for training a maximum entropy 
model. The resulting confusion matrix shows that 
the model correctly classified most of the instances 
of the two predominant senses while misclassify-
ing the other classes. The vast majority of the er-
rors came from confusing other senses with sense 5 
which is the most frequent sense of to call. Clearly, 
the data imbalance problem has a significant nega-
tive effect on performance. 
Let us now envision the following realistic sce-
nario: An annotation project receives funds to 
sense-tag a set of verbs in a corpus. It may be the 
case that some annotated data is already available 
for these verbs and the goal is to improve sense 
coverage, or no annotated data is available at all.  
But it turns out there are only enough funds to an-
notate a portion (e.g. half) of the total instances. 
The question arises how to pre-select the instances 
from the corpus in a way that would ensure that all 
the senses are as well represented as possible. Be-
cause some senses of these verbs are very rare, the 
pool of instances pre-selected for the annotation 
should include as many as possible instances of the 
rare senses. Random sampling ? the simplest ap-
proach ? will clearly not work: the pre-selected 
data will contain roughly the same proportion of 
the rare sense instances as the original set.  
If random sampling is not the answer, the data 
must be selected in some non-uniform way, i.e. 
using selective sampling. Active learning (e.g. 
Chen et al, 2006) is one approach to this problem. 
Some evidence is available (Zhu and Hovy, 2007) 
that active learning outperforms random sampling 
in finding the instances of rare senses. However, 
active learning has several shortcomings: (1) it re-
quires some annotated data to start the process; (2) 
it is problematic when the initial training set only 
contains the data for a single class (e.g. the pseudo-
monosemous verbs); (3) it is not always efficient in 
practice: In the OntoNotes project, the data is an-
notated by two human taggers and the disagree-
ments are adjudicated by the third. In classic active 
learning a single instance is labeled on each itera-
tion  This means the human taggers would have to 
wait on each other to tag the instance, on the adju-
dicator for the resolution of a possible disagree-
ment, and finally on the system which still needs to 
be-retrained to select the next instance to be la-
beled, a time sink much greater than tagging addi-
tional instances; (4) finally, active learning may 
not be an option if the data selected needs to be 
manually pre-processed (e.g. sentence segmented, 
tokenized, and treebanked ? as was the case with 
some of the OntoNotes data). In this setting, on 
each iteration of the algorithm, the taggers have to 
also wait for the selected instance to be manually 
pre-processed before they can label it. 
Thus, it would be significantly more convenient 
if all the data to be annotated could be pre-selected 
in advance. In this paper we turn to two unsuper-
vised methods which have the potential to achieve 
that goal. We propose a simple language modeling-
based sampling method (abbreviated as LMS) that 
increases the likelihood of seeing rare senses in the 
pre-selected data. The basic approach is as follow: 
using language modeling we can rank the instances 
of the ambiguous verb according to their probabili-
ty of occurrence in the corpus. Because the in-
stances of the rare senses are less frequent than the 
instances of the predominant sense, we can expect 
that there will be a higher than usual concentration 
of the rare sense instances among the instances that 
have low probabilities. The method is completely 
unsupervised and the only resource that it requires 
is a Language Modeling toolkit such as SRILM 
(Stolcke, 2002), which we used in our experiments. 
We compare this method with a random sampling 
baseline and semi-supervised clustering, which can 
serve the same purpose. We show that our method 
outperforms both of the competing approaches. We 
review the relevant literature in section 2, explain 
the details of LMS in section 3, evaluate LMS in 
section 4, discuss the results in section 5, and de-
scribe our plans for future work in section 6. 
2 Relevant Work
The problem of imbalanced data has recently re-
ceived much attention in the machine learning 
community. Rare classes can be of higher impor-
tance than frequent classes, as in medical diagnosis 
when one is interested in correctly identifying a 
rare disease. Network intrusion detection faces a 
similar problem: a malicious activity, although of 
crucial importance, is a very rare event compared 
to the large volumes of routine network traffic. At 
the same time, imbalanced data poses difficulties 
for an automatic learner in that rare classes have a 
much higher misclassification rate than common 
26
ones (Weiss, 1995; Japkowicz, 2001). Learning 
from imbalanced sets can also be problematic if the 
data is noisy: given a sufficiently high level of 
background noise, a learner may not distinguish 
between true exceptions (i.e. rare cases) and noise 
(Kubat and Matwin, 1997; Weiss, 2004). 
In the realm of supervised learning, cost-
sensitive learning has been recommended as a so-
lution to the problem of learning from imbalanced 
data (e.g. Weiss, 2004). However, the costs of mis-
classifying the senses are highly domain specific 
and hard to estimate. Several studies recently ap-
peared that attempted to apply active learning prin-
ciples to rare category detection (Pelleg and 
Moore, 2004; He and Carbonell, 2007). In addition 
to the issues with active learning outlined in the 
introduction, the algorithm described in (He and 
Carbonell, 2007) requires the knowledge of the 
priors, which is hard to obtain for word senses.  
WSD has a long history of experiments with 
unsupervised learning (e.g. Schutze, 1998; Puran-
dare and Peterson, 2004). McCarthy et al (2004) 
propose a method for automatically identifying the 
predominant sense in a given domain. Erk (2006) 
describes an application of an outlier detection al-
gorithm to the task of identifying the instances of 
unknown senses. Our task differs from the latter 
two works in that it is aimed at finding the in-
stances of the rare senses. 
Finally, the idea of LMS is similar to the tech-
niques for sentence selection based on rare n-gram 
co-occurrences used in machine translation (Eck et 
al., 2005) and syntactic parsing (Hwa, 2004). 
3 Language Modeling for Data Selection
Our method is outlined in Figure 1: 
 
Input 
A large corpus that contains T candidate instances 
from which S instances are to be selected for anno-
tation 
Basic Steps 
1. Compute the language model for the corpus 
2. Compute the probability distribution over the T 
candidate instances of the target verb  
3. Rank the T candidate instances by their proba-
bilities 
4. Form a cluster by selecting S instances with the 
lowest probability 
 
Figure 1. Basic steps of LMS 
 
Let us now clarify a few practical points. Al-
though an instance of the target verb can be 
represented as the entire sentence containing the 
verb, from the experiments with automatic WSD 
(e.g. Dligach and Palmer, 2008), it is known that 
having access to just a few words in the neighbor-
hood of the target verb is sufficient in many cases 
to predict the sense. For the purpose of LMS we 
represent an instance as the chunk of text centered 
upon the target verb plus the surrounding words on 
both sides within a three-word window. Although 
the size of the window around the target verb is 
fixed, the actual number of words in each chunk 
may vary when the target verb is close to the be-
ginning or the end of sentence. Therefore, we need 
some form of length normalization. We normalize 
the log probability of each chunk by the actual 
number of words to make sure we do not favor 
shorter chunks (SRILM operates in log space). The 
resulting metric is related to perplexity: for a se-
quence of words W = w1w2 ? wN  the perplexity is 
N
NwwwPWPP
1
21 )...()(
?
=  
The log of perplexity is 
)]...(log[1)](log[ 21 NwwwPNWPP ?=
 
Thus, the quantity we use for ranking is nega-
tive perplexity. 
4 Evaluation
For the evaluation, we selected two-sense verbs 
from the OntoNotes data that have at least 100 in-
stances and where the share of the rare sense is less 
than 20%. There were 11 such verbs (2,230 in-
stances total) with the average share of the rare 
sense 11%.  
Our task consists of clustering the instances of a 
verb into two clusters, one of which is expected to 
have a higher concentration of the rare senses than 
the other. Since the rare sense cluster is of primary 
interest to us, we report two metrics: (1) precision: 
the ratio of the number of instances of the rare 
sense in the cluster and the total number of in-
stances in the cluster; (2) recall: the ratio of the 
number of instances of the rare sense in the cluster 
and the total number of the rare sense instances in 
both clusters. Note that precision is not of primary 
importance for this task because the goal is not to 
reliably identify the instances of the rare sense but 
27
rather to group them into a cluster where the rare 
senses will have a higher concentration than in the 
original set of the candidate instances. At the same 
time achieving high recall is important since we 
want to ensure that most, if not all, of the rare 
senses that were present among the candidate in-
stances are captured in the rare sense cluster.  
4.1 Plausibility of LMS
The goal of our first set of experiments is to illu-
strate the plausibility of LMS. Due to space con-
straints, we examine only two verbs: compare and 
add. The remaining experiments will focus on a 
more comprehensive evaluation that will involve 
all 11 verbs. We computed the normalized log 
probability for each instance of a verb. We then 
ordered these candidate instances by their norma-
lized log probability and computed the recall of the 
rare sense at various levels of the size of the rare 
sense cluster. We express the size of the rare sense 
cluster as a share of the total number of instances. 
We depict recall vs. cluster size with a dotted 
curve. The graphs are in Figures 2 and 3.  
 
Figure 2. Rare sense recall for compare 
 
Figure 3. Rare sense recall for add 
The diagonal line on these figures corresponds 
to the random sampling baseline. A successful 
LMS would correspond to the dotted curve lying 
above the random sampling baseline, which hap-
pens to be the case for both of these verbs. For 
compare we can capture all of the rare sense in-
stances in a cluster containing less than half of the 
candidate instances. While verbs like compare re-
flect the best-case scenario, the technique we pro-
posed still works for the other verbs although not 
always as well. For example, for add we can recall 
more than 70% of the rare sense instances in a 
cluster that contains only half of all instances. This 
is more than 20 percentage points better than the 
random sampling baseline where the recall of the 
rare sense instances would be approximately 50%. 
4.2 LMS vs. Random Sampling Baseline
In this experiment we evaluated the performance 
of LMS for all 11 verbs. For each verb, we ranked 
the instances by their normalized log probability 
and placed the bottom half in the rare sense cluster. 
The results are in Table 2. The second column 
shows the share of the rare sense instances in the 
entire corpus for each verb. Thus, it represents the 
precision that would be obtained by random sam-
pling. The recall for random sampling in this set-
ting would be 0.5.   
Ten verbs outperformed the random sampling 
baseline both with respect to precision and recall 
(although recall is much more important for this 
task) and one verb performed as well. On average 
these verbs showed a recall figure that was 22 per-
centage points better than random sampling. Two 
of the 11 verbs (compare and point) were able to 
recall all of the rare sense instances. 
 
Verb Rare Inst Precision Recall 
account 0.12 0.21 0.93 
add 0.07 0.10 0.73 
admit 0.18 0.18 0.50 
allow 0.06 0.07 0.62 
compare 0.08 0.16 1.00 
explain 0.10 0.12 0.60 
maintain 0.11 0.11 0.53 
point 0.15 0.29 1.00 
receive 0.07 0.08 0.60 
remain 0.15 0.20 0.65 
worry 0.15 0.22 0.73 
average 0.11 0.16 0.72 
 
Table 2. LMS results for 11 verbs 
28
4.3 LMS vs. K-means Clustering
Since LMS is a form of clustering one way to eva-
luate its performance is by comparing it with an 
established clustering algorithm such as K-means 
(Hastie et al, 2001). There are several issues re-
lated to this evaluation. First, K-means produces 
clusters and which cluster represents which class is 
a moot question. Since for the purpose of the eval-
uation we need to know which cluster is most 
closely associated with a rare sense, we turn K-
means into a semi-supervised algorithm by seeding 
the clusters. This puts LMS at a slight disadvan-
tage since LMS is a completely unsupervised algo-
rithm, while the new version of K-means will 
require an annotated instance of each sense. How-
ever, this disadvantage is not very significant: in a 
real-world application, the examples from a dictio-
nary can be used to seed the clusters. For the pur-
pose of this experiment, we simulated the 
examples from a dictionary by simply taking the 
seeds from the pool of the annotated instances we 
identified for the evaluation. K-means is known to 
be highly sensitive to the choice of the initial 
seeds. Therefore, to make the comparison fair, we 
perform the clustering ten times and pick the seeds 
at random for each iteration. The results are aver-
aged. 
Second, K-means generates clusters of a fixed 
size while the size of the LMS-produced clusters 
can be easily varied. This advantage of the LMS 
method has to be sacrificed to compare its perfor-
mance to K-means. We compare LMS to K-means 
by counting the number of instances that K-means 
placed in the cluster that represents the rare sense 
and selecting the same number of instances that 
have the lowest normalized probability. Thus, we 
end up with the two methods producing clusters of 
the same size (with k-means dictating the cluster 
size).  
Third, K-means operates on vectors and there-
fore the instances of the target verb need to be 
represented as vectors. We replicate lexical, syn-
tactic, and semantic features from a verb sense dis-
ambiguation system that showed state-of-the-art 
performance on the OntoNotes data (Dligach and 
Palmer, 2008).  
The results of the performance comparison are 
shown in Table 3. The fourth column shows the 
relative size of the K-means cluster that was 
seeded with the rare sense. Therefore it also de-
fines the share of the instances with the lowest 
normalized log probability that are to be included 
in the LMS-produced rare sense clusters. On aver-
age, LMS showed 3% better recall than K-means 
clustering.  
 
K-means LMS
verb precision recall size precision recall
account 0.21 1.00 0.58 0.20 1.00
add 0.06 0.54 0.50 0.10 0.73
admit 0.21 0.31 0.29 0.09 0.15
allow 0.08 0.36 0.31 0.06 0.31
compare 0.22 0.42 0.18 0.19 0.43
explain 0.16 0.61 0.44 0.14 0.60
maintain 0.13 0.91 0.80 0.11 0.82
point 0.27 0.66 0.42 0.31 0.89
receive 0.11 0.68 0.72 0.08 0.80
remain 0.10 0.41 0.44 0.21 0.61
worry 0.81 0.51 0.13 0.38 0.33
average 0.21 0.58 0.44 0.17 0.61
 
Table 3. LMS vs. K-means 
5 Discussion and Conclusion
In this paper we proposed a novel method we 
termed LMS for pre-selecting instances for annota-
tion. This method is based on computing the prob-
ability distribution over the instances and selecting 
the ones that have the lowest probability. The ex-
pectation is that instances selected in this fashion 
will capture more of the instances of the rare 
classes than would have been captured by random 
sampling. We evaluated LMS by comparing it to 
random sampling and showed that LMS outper-
forms it. We also demonstrated that LMS com-
pares favorably to K-means clustering. This is 
despite the fact that the cluster sizes were dictated 
by K-means and that K-means had at its disposal 
much richer linguistic representations and some 
annotated data.  
Thus, we conclude that LMS is a promising me-
thod for data selection. It is simple to use since one 
only needs the basic functionality that any lan-
guage modeling toolkit offers. It is flexible in that 
the number of the instances to be selected can be 
specified by the user, unlike, for example, when 
clustering using k-means. 
29
6 Future Work
First, we would like to investigate the effect of se-
lective sampling methods (including LMS) on the 
performance of WSD models learned from the se-
lected data. Next, we plan to apply LMS for Do-
main adaptation. Unlike the scenario we dealt with 
in this paper, the language model would have to be 
learned from and applied to different corpora: it 
would be trained on the source corpus and used to 
compute probabilities for the instances in the target 
corpus that needs to be adapted. We will also expe-
riment with various outlier detection techniques to 
determine their applicability to data selection. 
Another promising direction is a simplified active 
learning approach in which a classifier is trained 
on the labeled data and applied to unlabeled data; 
the instances with a low classifier's confidence are 
selected for annotation (i.e. this is active learning 
conducted over a single iteration). This approach is 
more practical than the standard active learning for 
the reasons mentioned in Section 1 and should be 
compared to LMS. Finally, we will explore the 
utility of LMS-selected data as the initial training 
set for active learning (especially in the cases of 
the pseudo-monosemous verbs). 
Acknowledgments
 
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078, 
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No. 
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.  Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not 
necessarily reflect the views of the National 
Science Foundation. 
References
 
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha 
Palmer. 2006. An Empirical Study of the Behavior of 
Active Learning for Word Sense Disambiguation. In 
Proceedings of the HLT-NAACL.  
Dmitriy Dligach and Martha Palmer. 2008. Novel Se-
mantic Features for Verb Sense Disambiguation. In 
Proceedings of ACL-HLT.  
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. 
Low Cost Portability for Statistical Machine Transla-
tion Based on N-gram Frequency and TF-IDF. Pro-
ceedings of IWSLT 2005. 
Katrin Erk. Unknown Word Sense Detection as Outlier 
Detection. 2006. In Proceedings of HLT-NAACL. 
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 
The Elements of Statistical Learning. Data Mining, 
Inference, and Prediction. 2001. Springer.  
Jingrui He and Jaime Carbonell. 2007. Nearest-
Neighbor-Based Active Learning for Rare Category 
Detection. NIPS. 
Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L. 
Ramshaw, and R. Weischedel. 2006. OntoNotes: The 
90% Solution. In Proceedings of the HLT-NAACL. 
Eduard Hovy and Jingbo Zhu. 2007. Active Learning 
for Word Sense Disambiguation with Methods for 
Addressing the Class Imbalance Problem. In Pro-
ceedings of EMNLP.  
Rebecca Hwa. 2004. Sample Selection for Statistical 
Parsing. Computational Linguistics. Volume 30. Is-
sue 3. 
Natalie Japkowicz. 2001. Concept Learning in the Pres-
ence of Between-Class and Within-Class Imbalances. 
Proceedings of the Fourteenth Conference of the Ca-
nadian Society for Computational Studies of Intelli-
gence, Springer-Verlag. 
Miroslav Kubat and Stan Matwin. 1997. Addressing the 
curse of imbalanced training sets: one-sided selec-
tion. In Proceedings of the Fourteenth International 
Conference on Machine Learning. 
Diana McCarthy, Rob Koeling, Julie Weeds, and John 
Carroll. 2004. Finding Predominant Word Senses in 
Untagged Text. In Proceedings of 42nd Annual 
Meeting of Association for Computational Linguis-
tics. 
Dan Pelleg and Andrew Moore. 2004. Active Learning 
for Anomaly and Rare-Category Detection. NIPS. 
Amruta Purandare and Ted Pedersen. Word Sense Dis-
crimination by Clustering Contexts in Vector and 
Similarity Spaces. 2004. In Proceedings of the Con-
ference on CoNLL. 
Hinrich Schutze. 1998 Automatic Word Sense Discrim-
ination. Computational Linguistics.  
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. Spoken 
Language Processing, Denver, Colorado. 
Gary M. Weiss. 1995. Learning with Rare Cases and 
Small Disjuncts. Proceedings of the Twelfth Interna-
tional Conference on Machine Learning, Morgan 
Kaufmann. 
Gary M. Weiss. 2004. Mining with Rarity: A Unifying 
Framework. SIGKDD Explorations, special issue on 
learning from imbalanced datasets. 
30
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 29?32,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Novel Semantic Features for Verb Sense Disambiguation 
Dmitriy Dligach 
The Center for Computational 
Language and Education  
Research 
1777 Exposition Drive 
Boulder, Colorado 80301  
Dmitriy.Dligach 
@colorado.edu 
Martha Palmer 
Department of Linguistics 
University of Colorado  
at Boulder 
295 UCB 
Boulder, Colorado 80309 
Martha.Palmer 
@colorado.edu 
 
 
Abstract 
We propose a novel method for extracting 
semantic information about a verb's arguments 
and apply it to Verb Sense Disambiguation 
(VSD). We contrast this method with two 
popular approaches to retrieving this informa-
tion and show that it improves the perform-
ance of our VSD system and outperforms the 
other two approaches  
1 Introduction 
The task of Verb Sense Disambiguation (VSD) 
consists in automatically assigning a sense to a 
verb (target verb) given its context. In a supervised 
setting, a VSD system is usually trained on a set of 
pre-labeled examples; the goal of this system is to 
tag unseen examples with a sense from some sense 
inventory. 
 
An automatic VSD system usually has at its 
disposal a diverse set of features among which the 
semantic features play an important role: verb 
sense distinctions often depend on the distinctions 
in the semantics of the target verb's arguments 
(Hanks, 1996). Therefore, some method of captur-
ing the semantic knowledge about the verb's argu-
ments is crucial to the success of a VSD system.  
 
The approaches to obtaining this kind of 
knowledge can be based on extracting it from elec-
tronic dictionaries such as WordNet (Fellbaum, 
1998), using Named Entity (NE) tags, or a combi-
nation of both (Chen, 2005). In this paper, we pro-
pose a novel method for obtaining semantic 
knowledge about words and show how it can be 
applied to VSD. We contrast this method with the 
other two approaches and compare their perform-
ances in a series of experiments.  
2 Lexical and Syntactic Features 
We view VSD as a supervised learning problem, 
solving which requires three groups of features: 
lexical, syntactic, and semantic. Lexical features 
include all open class words; we extract them from 
the target sentence and the two surrounding sen-
tences. We also use as features two words on the 
right and on the left of the target verb as well as 
their POS tags. We extract syntactic features from 
constituency parses; they indicate whether the tar-
get verb has a subject/object and what their head 
words and POS tags are, whether the target verb is 
in a passive or active form, whether the target verb 
has a subordinate clause, and whether the target 
verb has a PP adjunct. Additionally, we implement 
several new syntactic features, which have not 
been used in VSD before: the path through the 
parse tree from the target verb to the verb's argu-
ments and the subcategorization frame, as used in 
semantic role labeling. 
3 Semantic Features 
Consider the verb prepare for which our sense in-
ventory defines two senses: (1) to put together, 
assemble (e.g. He is going to prepare breakfast for 
the whole crowd ; I haven't prepared my lecture 
29
yet); (2) to make ready (e.g. She prepared the chil-
dren for school every morning). Knowing the se-
mantic class of the objects breakfast, lecture and 
children is the decisive factor in distinguishing the 
two senses and facilitates better generalization 
from the training data. One way to obtain this 
knowledge is from WordNet (WN) or from the 
output of a NE-tagger. However, both approaches 
suffer from the same limitation: they collapse mul-
tiple semantic properties of nouns into a finite 
number of predefined static classes. E.g., the most 
immediate hypernym of breakfast in WN is meal, 
while the most immediate hypernym of lecture is 
address, which makes these two nouns unrelated. 
Yet, breakfast and lecture are both social events 
which share some semantic properties: they both 
can be attended, hosted, delivered, given, held, 
organized etc. To discover these class-like descrip-
tions of nouns, one can observe which verbs take 
these nouns as objects. E.g. breakfast can serve as 
the object of serve, host, attend, and cook  which 
are all indicative of breakfast's semantic proper-
ties. 
 
Given a noun, we can dynamically retrieve 
other verbs that take that noun as an object from a 
dependency-parsed corpus; we call this kind of 
data Dynamic Dependency Neighbors  (DDNs) 
because it is obtained dynamically and based on 
the dependency relations in the neighborhood of 
the noun of interest. The top 501 DDNs can be 
viewed as a reliable inventory of semantic proper-
ties of the noun. To collect this data, we utilized 
two resources: (1) MaltParser (Nivre, 2007) ? a 
high-efficiency dependency parser; (2) English 
Gigaword ? a large corpus of 5.7M news articles. 
We preprocessed Gigaword with MaltParser, ex-
tracted all pairs of nouns and verbs that were 
parsed as participants of the object-verb relation, 
and counted the frequency of occurrence of all the 
unique pa irs. Finally, we indexed the resulting re-
cords of the form <frequency, verb, object> using 
the Lucene2 indexing engine. 
 
As an example, consider four nouns: dinner, 
breakfast, lecture, child. When used as the objects 
of prepare, the first three of them correspond to the 
instances of the sense 1 of prepare; the fourth one 
                                                                 
1 In future, we will try to optimize this parameter 
2 Available at http://lucene.apache.org/ 
corresponds to an instance of the sense 2. With the 
help of our index, we can retrieve their DDNs. 
There is a considerable overlap among the DDNs 
of the first three nouns and a much smaller overlap 
between child  and the first three nouns. E.g., din-
ner and breakfast have 34 DDNs in common, 
while dinner and child  only share 14. 
 
Once we have set up the framework for the ex-
traction of DDNs, the algorithm for applying them 
to VSD is straightforward: (1) find the noun object 
of the ambiguous verb (2) extract the DDNs for 
that noun (3) sort the DDNs by frequency and keep 
the top 50 (4) include these DDNs in the feature 
vector so that each of the extracted verbs becomes 
a separate feature. 
4 Relevant Work 
At the core of our work lies the notion of distrib u-
tional similarity (Harris, 1968), which states that 
similar words occur in similar contexts. In various 
sources, the notion of context ranges from bag-of-
words-like approaches to more structured ones in 
which syntax plays a role. Schutze (1998) used 
bag-of-words contexts for sense discrimination. 
Hindle (1990) grouped nouns into thesaurus-like 
lists based on the similarity of their syntactic con-
texts. Our approach is similar with the difference 
that we do not group noun arguments into finite 
categories, but instead leave the category bounda-
ries blurry and allow overlaps. 
 
The DDNs are essentially a form of world 
knowledge which we extract automatically and 
apply to VSD. Other researches attacked the prob-
lem of unsupervised extraction of world knowl-
edge: Schubert (2003) reports a method for 
extracting general facts about the world from tree-
banked Brown corpus. Lin and Pantel in (2001) 
describe their DIRT system for extraction of para-
phrase-like inference rules. 
5 Evaluation 
We selected a subset of the verbs annotated in the 
OntoNotes project (Chen, 2007) that had at least 
50 instances. The resulting data set consisted of 
46,577 instances of 217 verbs. The predominant 
sense baseline for this data is 68%. We used 
30
libsvm3 for classification. We computed the accu-
racy and error rate using 5-fold cross-validation.  
5.1 Experiments with a limited set of features 
The main objective of this experiment was to iso-
late the effect of the novel semantic features we 
proposed in this paper, i.e. the DDN features. To-
ward that goal, we stripped our system of all the 
features but the most essential ones to investigate 
whether the DDN features would have a clearly 
positive or negative impact on the system perform-
ance. Lexical features are the most essential to our 
system: a model that includes only the lexical fea-
tures achieves an accuracy of 80.22, while the ac-
curacy of our full-blown VSD system is 82.88%4. 
Since the DDN features have no effect when the 
object is not present, we identified 18,930 in-
stances where the target verb had an object (about 
41% of all instances) and used only them in the 
experiment. 
 
We built three models that included (1) the 
lexical features only (2) the lexical and the DDN 
features (3) the lexical and the object features. The 
object features consist of the head word of the NP 
object and the head word's POS tag. The object is 
included since extracting the DDN features re-
quires knowledge of the object; therefore the per-
formance of a model that only includes lexical 
features cannot be considered a fair baseline for 
studying the effect of the DDN features. Results 
are in Table 4. 
 
Features Included in 
Model 
Accuracy, % Error Rate, % 
Lexical 78.95 21.05 
Lexical + Object  79.34 20.66 
Lexical + DDN 82.40 17.60 
 
Table 4. Experiments with object instances 
 
As we see, the model that includes the DDN 
features performs more than 3 percentage points 
better than the model that only includes the object 
features (approximately 15% reduction in error 
rate). Also, based on the comparison of the per-
formance of the "lexical features only" and the 
"lexical + DDN" models, we can claim that the 
                                                                 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
4 Given this high baseline, we include error rate when report-
ing the results of the experiments as it is more informative 
knowledge of the DDNs provides richer semantic 
knowledge than just the knowledge of the object's 
head word. 
5.2 Integrating the DDN features into a full-
fledged VSD system 
The objective of this experiment was to investigate 
whether the DDN features improve the perform-
ance of a full-fledged VSD system. We built two 
models which consisted of (1) the entire set of fea-
tures (2) all the features of the first model exclud-
ing the DDN features. The entire data set (46K 
instances) participated in the experiment. Results 
are in Table 5. 
 
Features Included in 
Model 
Accuracy, % Error Rate, % 
All Features ? DDN 82.38 17.62 
All Features 82.88 17.12 
 
Table 5. Performance of the full-fledged VSD system 
 
The DDN features improved performance by 
0.5% (3% drop in error rate). The difference be-
tween the accuracies is statistically significant 
(p=0.05).   
5.3 Relative Contribution of Various Seman-
tic Fe atures 
The goal of this experiment was to study the rela-
tive contribution of various semantic features to 
the performance of our VSD system. We built five 
models each of which, in addition to the lexical 
and syntactic features, included only certain 
type(s) of semantic feature: (1) WN (2) NE (3) 
WN and NE (4) DDN (5) no semantic features 
(baseline). All 46K instances participated in the 
experiment. The results are shown in Table 6. 
 
Features Included in Model Accuracy, 
% 
Error Rate, 
% 
Lexical + Syntactic 81.82 18.18 
Lexical + Syntactic + WN 82.34 17.60 
Lexical + Syntactic + NE 82.01 17.99 
Lexical + Syntactic + WN + NE 82.38 17.62 
Lexical + Syntactic + DDN 82.97 17.03 
 
Table 6. Relative Contribution of Semantic Features 
 
The DDN features outperform the other two 
types of semantic features used separately and in 
conjunction. The difference in performance is sta-
tistically significant (p=0.05). 
31
6 Discussion and Conclusion 
As we saw, the novel semantic features we pro-
posed are beneficial to the task of VSD: they re-
sulted in a decrease in error rate from 3% to 15%, 
depending on the particular experiment. We also 
discovered that the DDN features contributed twice 
as much as the other two types of semantic features 
combined: adding the WN and NE features to the 
baseline resulted in about a 3% decrease in error 
rate, while adding the DDN features caused a more 
than 6% drop. 
 
Our results suggest that DDNs duplicate the ef-
fect of WN and NE: our system achieved the same 
performance when all three types of semantic fea-
tures were used and when we discarded WN and 
NE features and kept only the DDNs. This finding 
is important because such resources as WN and 
NE-taggers are domain and language specific 
while the DDNs have the advantage of being ob-
tainable from a large collection of texts in the do-
main or language of interest. Thus, the DDNs can 
become a crucial part of building a robust VSD 
system for a resource-poor domain or language, 
given a high-accuracy parser. 
7 Future Work 
In this paper we only experimented with verbs' 
objects, however the concept of DDNs can be eas-
ily extended to other arguments of the target verb. 
Also, we only utilized the object-verb relation in 
the dependency parses, but the range of potentially 
useful relations does not have to be limited only to 
it. Finally, we used as features the 50 most fre-
quent verbs that took the noun argument as an ob-
ject. However, the raw frequency is certainly not 
the only way to rank the verbs; we plan on explor-
ing other metrics such as Mutual Information. 
 
Acknowledgements 
 
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078, 
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No. 
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.  Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not 
necessarily reflect the views of the National Sc i-
ence Foundation.  We also thank our colleagues 
Rodney Nielsen and Philipp Wetzler for parsing 
English Gigaword with MaltParser. 
 
References 
 
Jinying Chen, Dmitriy Dligach and Martha Palmer. 
2007. Towards Large-scale High-Performance Eng-
lish Verb Sense Disambiguation by Using Linguisti-
cally Motivated Features. In International 
Conference on Semantic Computing. Issue , 17-19. 
 
Jinying Chen and Martha Palmer. 2005. Towards Ro-
bust High Performance Word Sense Disambiguation 
of English Verbs Using Rich Linguistic Features. In 
Proceedings of the 2nd International Joint Confer-
ence on Natural Language Processing, Korea. 
 
Christiane Fellbaum. 1998. WordNet - an Electronic 
Lexical Database. The MIT Press, Cambridge, Mas-
sachusetts, London, UK. 
  
Patrick Hanks, 1996. Contextual Dependencies and 
Lexical Sets. In The Int. Journal of Corpus Linguis-
tics, 1:1 
 
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. New York. Wiley. 
 
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In Proceedings of the 28th 
Annual Meeting of Association for Computational 
Linguistics. Pages 268-275 
 
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery 
of Inference Rules from Text. In Proceedings of 
ACM Conference on Knowledge Discovery and Data 
Mining. pp. 323-328. San Francisco, CA. 
 
Joakim Nivre, Johan Hall, Jens Nilsson, et. al. Malt -
Parser: A language-independent system for data-
driven dependency parsing. 2007. In Natural Lan-
guage Engineering, 13(2), 95-135. 
 
Lenhart Schubert and Matthew Tong, Extracting and 
evaluating general world knowledge from the Brown 
corpus. 2003. In Proc. of the HLT/NAACL Workshop 
on Text Meaning, May 31, Edmonton, Alberta, Can-
ada. 
 
Hinrich Schutze. 1998. Automatic Word Sense Dis-
crimination. In Computational Linguistics, 24(1):97-
123 
32
Proceedings of the Linguistic Annotation Workshop, pages 49?52,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
Criteria for the Manual Grouping of Verb Senses  Cecily Jill Duffield, Jena D. Hwang, Susan Windisch Brown,  Dmitriy Dligach, Sarah E.Vieweg, Jenny Davis, Martha Palmer Departments of Linguistics and Computer Science University of Colorado Boulder, C0 80039-0295, USA {cecily.duffield, hwangd, susan.brown, dmitry.dligach,  sarah.vieweg, jennifer.davis, martha.palmer}@colorado.edu  
Abstract  In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance. Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a case-by-case basis rather than applying a strict methodology. Determining appropriate criteria for clustering is based primarily on the needs of annotators.  1  Credits  We gratefully acknowledge the support of the National Science Foundation Grant NSF-0415923, Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.  2  Introduction  Word sense ambiguity poses significant obstacles to accurate and efficient information extraction and automatic translation. Successful disambiguation of polysemous words in NLP applications depends on determining an appropriate level of granularity of sense distinctions, perhaps more so for distinguishing between multiple senses of verbs than for any other grammatical category. WordNet, an important and widely used lexical resource, uses fine-grained distinctions that provide subtle information about the particular usages of various 
lexical items (Felbaum, 1998). When used as a resource for annotation of various genres of text, this fine level of granularity has not been conducive to high rates of inter-annotator agreement (ITA) or high automatic tagging performance. Annotation of verb senses as described by coarse-grained Proposition Bank framesets may result in higher ITA scores, but the blurring of distinctions between verb senses with similar argument structures may fail to alleviate the problems posed by ambiguity. Our goal in this project is to create verb sense distinctions at a middle level of granularity that allow us to capture as much information as possible from a lexical item while still attaining high ITA scores and high system performance in automatic sense disambiguation. We have demonstrated that clear sense distinctions improve annotator productivity and accuracy. System performance typically lags around 10% behind ITA rates. ITA scores of at least 90% for a majority of our sense-groupings result in the expected corresponding improvement in system performance. Training on this new data, Chen et al, (2006) report 86.7% accuracy for verbs using a smoothed maximum entropy model and rich linguistic features. (Also Semeval071) They also report state-of-the-art performance on fine-grained senses, but the results are more than 16% lower. We begin by describing the overall process.  3  The Grouping and Annotation Process  The process for building our database with the appropriate level of verb sense distinctions                                                 1 Task 17,  http://nlp.cs.swarthmore.edu/semeval/.  
49
  
involves two steps: sense grouping and annotation (Figure 1). During our sense grouping process, linguists (henceforth, ?groupers?) cluster fine-grained sense distinctions listed in WordNet 2.1 into more coarse-grained groupings. These rough clusters of WordNet entries are based on speaker intuition. Other resources, including PropBank, VerbNet (based on Levin?s verb classes (Levin, 1993)), and online dictionaries are consulted in further refining the distinctions between senses (Palmer, et. al., 2005, Kipper et al, 2006). To aid annotators in understanding the distinctions, sense groupings are ordered according to saliency and frequency. Detailed information, including syntactic frames and semantic features, is provided as commentary for the groupings. We also provide the annotators with simple example sentences from WordNet as well as syntactically complex and ambiguous attested usages from Google search results. These examples are intended to guide annotators faced with similar challenges in the data to be tagged.  Completed verb sense groupings are sent through sample-annotation and tagged by two annotators. Groupings that receive an ITA score of 90% or above are then used to annotate all instances of that verb in our corpora in actual-annotation. Groupings that receive less than 90% ITA scores are regrouped (Hovy et al, 2006). Revisions are made based on a second grouper?s evaluation of the original grouping, as well as patterns of annotator disagreement. Verb groupings receiving ITA scores of 85% or above are sent through actual-annotation. Verbs scoring below 85% are regrouped by a third grouper, and in some cases, by the entire grouping team. It is sometimes impossible to get ITA scores over 85% for high   
frequency verbs that also have high entropy. These have to be carefully adjudicated to produce a gold standard. Revised verbs are then evaluated and either deemed ready for actual-annotation or are sent for a third and final round of sample-annotation. Verbs subject to the re-annotation process are tagged by different annotators. Data from actual-annotation is examined by an adjudicator who resolves remaining disagreements between annotators. The adjudicated data is then used as the gold standard for automatic annotation. The final versions of the sense groupings are mapped to VerbNet and FrameNet and linked to the Omega Ontology (Philpot et al, 2005).  Verbs are selected based on frequency of appearance in the WSJ corpus. As the most frequent verbs are also the most polysemous, the number of sense distinctions per verb as well as the number of instances to be tagged decreases as the project continues. The 740 most frequent verbs in the WSJ corpus were grouped in order of frequency. They have an average polysemy of 7 senses in WordNet; our sense groups have reduced the polysemy to 3.75 senses. Of these, 307 verb groupings have undergone regrouping to some extent. A total of 670 verbs have completed actual-annotation and adjudication. The next 660 verbs have been divided into rough semantic domains based on VerbNet classes, and grouping will proceed according to these semantic domains rather than by verb frequency. As groupers create sense groupings for new verbs, old verb sense groupings in the same semantic domain are consulted. This organization allows for more consistent grouping methodologies, as well as more efficiency in integrating our sense groupings into the Ontology.  
 Figure 1:  The grouping and annotation process.  
50
  
4  Grouping Methodology  Various criteria are considered when disambiguating senses and creating sense groupings for the verbs, including frequent lexical usages and collocations, syntactic features and alternations, and semantic features, similarly to Senseval2 (Palmer, et. al. 2006). Because these criteria do not apply uniformly to every verb, groupers take various approaches when creating sense groupings. Groupers recognize that there are many alternate ways to cluster senses at this level of granularity; each grouping represents only one possible clustering as a middle ground between PropBank and WordNet senses for each verb. Our highest priority is to then create clear distinctions among sense groupings that will be easily understood by the annotators and consequently result in high ITA scores. Initial clustering is based on groupers? intuitions of the most salient categories. Many verb groupings, such as that for the verb kill, provide little detailed syntactic or semantic analysis and yet have received high ITA scores. The success of these intuitive sense groupings is not due to lack of polysemy; kill has 15 WordNet senses and 2 multi-word expressions clustered into 9 sense groupings, yet it received 94% ITA in first round sample-annotation.  While annotators have little trouble tagging text with verb senses that fall neatly into intuitive categories, many verbs have fine-grained WordNet senses that fall on a continuum between two distinct lexical usages. In such cases, syntactic and semantic aspects of the verb and its arguments help groupers cluster senses in such a way that annotators can make consistent decisions in tagging the text. 
Syntactic criteria: Annotators have found syntactic frames, such as those defining VerbNet classes, to be useful in understanding boundaries between sense groupings. For example, split was originally grouped with consideration for the units resulting from a splitting event (i.e. whether a whole unit had been split into incomplete portions of the whole, or into smaller, but complete, individual units.)  This grouping proved difficult for annotators to distinguish, with an ITA of 42%. Using the causative/inchoative alternation for verbs in the ?break-45.1? class to regroup resulted in higher consistency among annotators, increasing the ITA score to 95%. Semantic criteria: When senses of a verb have similar syntactic frames, and usages fall along a continuum between these senses, semantic features of the arguments, or less often, of the verb itself, can clarify these senses and help groupers draw clear distinctions between them. Argument features that are considered when creating sense groupings include [+/-attribute], [+/-patient], and [+/-locative]. It is most common for groupers to mark these features on nominal arguments, but a prepositional phrase may also be described in semantic terms. Semantic features of the verb that are considered include aspectual features, as illustrated by the use of [+/-punctual] in sense groupings for make (Figure 2). However, it may be argued that this feature is unnecessary for annotators to be able to distinguish between the sense groupings, as the prepositional phrase in sense 9 is a more salient feature for annotators. Other features of the verb that were used earlier in the project include concrete/abstract, continuative, stative, and others. However, these features proved less useful than those Sense group Description and Commentary WordNet 2.1 senses Examples 8 Attain or reach something desired NP1[+agent] MAKE[+punctual] NP2[desired goal, destination, state] This sense implies the goal has been met. Includes: MAKE IT 
make 13, 22, 38 - He made the basketball team. - We barely made the plane. - I made the opening act in plenty of time. - Can you believe it? We made it!  
9 Move toward or away from a location NP1[+agent] MAKE[-punctual] (pronoun+way) PP/INFP  
make 30, 37 make off 1 make way 1  
- As the enemy approached our town, we made for the hills. - He made his way carefully across the icy parking lot. - They made off with the jewels. Figure 2: Sense groupings 8 and 9 for ?make.? Senses are distinguished in part by aspectual features marked on the verb.  
51
 described above, and annotators not familiar with linguistic theory found them to be confusing. Therefore, they are now rarely used to label sense groupings. Such concepts, when used, are more likely to be described in prose commentary for the sake of the annotators. Certain compositional features of verbs have also proven to be confusing for annotators. In several cases, attempts to distinguish sense groupings based on manner and path have resulted in increased annotator disagreement. In the first attempt at grouping roll, syntactic and semantic information, as well as prose commentary, was presented to help annotators distinguish the manner and path sense groupings. Despite this, the admissibility of certain prepositions in both senses (?The baby rolled over,? vs ?She rolled over to the wall,?) may have blurred the distinction. In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for roll, which were then merged in the final version of the sense groupings.  5  Conclusion  Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al, 2004; Palmer et al, 2007), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al, 2005). This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly fine-grained until reaching WordNet (or similar) senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntactic and semantic criteria for their groupings, to be presented to the annotators. Criteria are applied on a case-by-case basis, considering syntactic and semantic features as consistently as possible when grouping verbs in similar semantic domains as defined by VerbNet. By using this approach when creating sense groupings, we are 
able to provide annotators with clear and reliable descriptions of senses, resulting in improved accuracy and performance.  References Chen, J., A. Schein, L. Ungar and M. Palmer. 2006. An Empirical Study of the Behavior of Word Sense Disambiguation. Proceedings of HLT-NAACL 2006. New York, NY. Fellbaum, C. (ed.) 1998. WordNet: An On-line Lexical Database and Some of its Applications. MIT Press, Cambridge, MA. Kipper, K., A. Korhonen, N. Ryant, and M. Palmer. 2006. Extensive Classifications of English Verbs. Proceedings of the 12th EURALEX International Congress. Turin, Italy. Levin, B. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago, IL. OntoNotes, 2006. Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of HLT-NAACL 2006. New York, NY. Palmer, M., O. Babko-Malaya, and H.T. Dang. 2004. Different Sense Granularities for Different Applications. Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems (HLT-NAACL 2004). Boston, MA.  Palmer, M., Dang, H.T., and Fellbaum, C., Making Fine-grained and Coarse-grained sense distinctions, both manually and automatically, Journal of Natural Language Engineering (to appear, 2007). Palmer, M., Gildea, D., Kingsbury, P., The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics Journal, 31:1, 2005. Philpot, A., E.H. Hovy, and P. Pantel. 2005. The Omega Ontology. Proceedings of the ONTOLEX Workshop at the International Conference on Natural Language Processing (IJCNLP). Jeju Island, Korea.  
52
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 87?92,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 17: English Lexical Sample, SRL and All Words
Sameer S. Pradhan
BBN Technologies,
Cambridge, MA 02138
Edward Loper
University of Pennsylvania,
Philadelphia, PA 19104
Dmitriy Dligach and Martha Palmer
University of Colorado,
Boulder, CO 80303
Abstract
This paper describes our experience in
preparing the data and evaluating the results
for three subtasks of SemEval-2007 Task-17
? Lexical Sample, Semantic Role Labeling
(SRL) and All-Words respectively. We tab-
ulate and analyze the results of participating
systems.
1 Introduction
Correctly disambiguating words (WSD), and cor-
rectly identifying the semantic relationships be-
tween those words (SRL), is an important step for
building successful natural language processing ap-
plications, such as text summarization, question an-
swering, and machine translation. SemEval-2007
Task-17 (English Lexical Sample, SRL and All-
Words) focuses on both of these challenges, WSD
and SRL, using annotated English text taken from
the Wall Street Journal and the Brown Corpus.
It includes three subtasks: i) the traditional All-
Words task comprising fine-grained word sense dis-
ambiguation using a 3,500 word section of the Wall
Street Journal, annotated with WordNet 2.1 sense
tags, ii) a Lexical Sample task for coarse-grained
word sense disambiguation on a selected set of lex-
emes, and iii) Semantic Role Labeling, using two
different types of arguments, on the same subset of
lexemes.
2 Word Sense Disambiguation
2.1 English fine-grained All-Words
In this task we measure the ability of systems to
identify the correct fine-grained WordNet 2.1 word
sense for all the verbs and head words of their argu-
ments.
2.1.1 Data Preparation
We began by selecting three articles
wsj 0105.mrg (on homelessness), wsj 0186.mrg
(about a book on corruption), and wsj 0239.mrg
(about hot-air ballooning) from a section of the WSJ
corpus that has been Treebanked and PropBanked.
All instances of verbs were identified using the
Treebank part-of-speech tags, and also the head-
words of their noun arguments (using the PropBank
and standard headword rules). The locations of the
sentences containing them as well as the locations
of the verbs and the nouns within these sentences
were recorded for subsequent sense-annotation. A
total of 465 lemmas were selected from about 3500
words of text.
We use a tool called STAMP written by Ben-
jamin Snyder for sense-annotation of these in-
stances. STAMP accepts a list of pointers to the in-
stances that need to be annotated. These pointers
consist of the name of the file where the instance
is located, the sentence number of the instance, and
finally, the word number of the ambiguous word
within that sentence. These pointers were obtained
as described in the previous paragraph. STAMP also
requires a sense inventory, which must be stored in
XML format. This sense inventory was obtained by
querying WordNet 2.1 and storing the output as a
87
set of XML files (one for each word to be anno-
tated) prior to tagging. STAMP works by displaying
to the user the sentence to be annotated with the tar-
get word highlighted along with the previous and the
following sentences and the senses from the sense
inventory. The user can select one of the senses and
move on to the next instance.
Two linguistics students annotated the words with
WordNet 2.1 senses. Our annotators examined each
instance upon which they disagreed and resolved
their disagreements. Finally, we converted the re-
sulting data to the Senseval format. For this dataset,
we got an inter-annotator agreement (ITA) of 72%
on verbs and 86% for nouns.
2.1.2 Results
A total of 14 systems were evaluated on the All
Words task. These results are shown in Table 1.
We used the standard Senseval scorer ? scorer21
to score the systems. All the F-scores2 in this table
as well as other tables in this paper are accompanied
by a 95% confidence interval calculated using the
bootstrap resampling procedure.
2.2 OntoNotes English Lexical Sample WSD
It is quite well accepted at this point that it is dif-
ficult to achieve high inter-annotator agreement on
the fine-grained WordNet style senses, and with-
out a corpus with high annotator agreement, auto-
matic learning methods cannot perform at a level
that would be acceptable for a downstream applica-
tion. OntoNotes (Hovy et al, 2006) is a project that
has annotated several layers of semantic information
? including word senses, at a high inter-annotator
agreement of over 90%. Therefore we decided to
use this data for the lexical sample task.
2.2.1 Data
All the data for this task comes from the 1M word
WSJ Treebank. For the convenience of the partici-
pants who wanted to use syntactic parse information
as features using an off-the-shelf syntactic parser,
we decided to compose the training data of Sections
02-21. For the test sets, we use data from Sections
1http://www.cse.unt.edu/?rada/senseval/senseval3/scoring/
2
scorer2 reports Precision and Recall scores for each system. For a sys-
tem that attempts all the words, both Precision and Recall are the same. Since a
few systems had missing answers, they got different Precision and Recall scores.
Therefore, for ranking purposes, we consolidated them into an F-score.
Train Test Total
Verb 8988 2292 11280
Noun 13293 2559 15852
Total 22281 4851
Table 2: The number of instances for Verbs and
Nouns in the Train and Test sets for the Lexical Sam-
ple WSD task.
01, 22, 23 and 24. Fortunately, the distribution of
words was amenable to an acceptable number of in-
stances for each lemma in the test set. We selected
a total of 100 lemmas (65 verbs and 35 nouns) con-
sidering the degree of polysemy and total instances
that were annotated. The average ITA for these is
over 90%.
The training and test set composition is described
in Table 2. The distribution across all the verbs and
nouns is displayed in Table 4
2.2.2 Results
A total of 13 systems were evaluated on the Lexi-
cal Sample task. Table 3 shows the Precision/Recall
for all these systems. The same scoring software was
used to score this task as well.
2.2.3 Discussion
For the all words task, the baseline performance
using the most frequent WordNet sense for the lem-
mas is 51.4. The top-performing system was a su-
pervised system that used a Maximum Entropy clas-
sifier, and got a Precision/Recall of 59.1% ? about 8
points higher than the baseline. Since the coarse and
fine-grained disambiguation tasks have been part of
the two previous Senseval competitions, and we hap-
pen to have access to that data, we can take this op-
portunity to look at the disambiguation performance
trend. Although different test sets were used for ev-
ery evaluation, we can get a rough indication of the
trend. For the fine-grained All Words sense tagging
task, which has always used WordNet, the system
performance has ranged from our 59% to 65.2 (Sen-
seval3, (Decadt et al, 2004)) to 69% (Seneval2,
(Chklovski and Mihalcea, 2002)). Because of time
constraints on the data preparation, this year?s task
has proportionally more verbs and fewer nouns than
previous All-Words English tasks, which may ac-
count for the lower scores.
As expected, the Lexical Sample task using coarse
88
Rank Participant System ID Classifier F
1 Stephen Tratz <stephen.tratz@pnl.gov> PNNL MaxEnt 59.1?4.5
2 Hwee Tou Ng <nght@comp.nus.edu.sg> NUS-PT SVM 58.7?4.5
3 Rada Mihalcea <rada@cs.unt.edu> UNT-Yahoo Memory-based 58.3?4.5
4 Cai Junfu <caijunfu@gmail.com> NUS-ML naive Bayes 57.6?4.5
5 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM kNN 54.4?4.5
6 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-2 kNN 54.0?4.5
7 Jonathan Chang <jcone@princeton.edu> PU-BCD Exponential Model 53.9?4.5
8 Radu ION <radu@racai.ro> RACAI Unsupervised 52.7?4.5
9 Most Frequent WordNet Sense Baseline N/A 51.4?4.5
10 Davide Buscaldi <dbuscaldi@dsic.upv.es> UPV-WSD Unsupervised 46.9?4.5
11 Sudip Kumar Naskar <sudip.naskar@gmail.com> JU-SKNSB Unsupervised 40.2?4.5
12 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-1 Unsupervised 39.9?4.5
14 Rafael Berlanga <berlanga@uji.es> tkb-uo Unsupervised 32.5?4.5
15 Jordan Boyd-Graber <jbg@princeton.edu> PUTOP Unsupervised 13.2?4.5
Table 1: System Performance for the All-Words task.
Rank Participant System Classifier F
1 Cai Junfu <caijunfu@gmail.com> NUS-ML SVM 88.7?1.2
2 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM SVD+kNN 86.9?1.2
3 Zheng-Yu Niu <niu zy@hotmail.com> I2R Supervised 86.4?1.2
4 Lucia Specia <lspecia@gmail.com> USP-IBM-2 SVM 85.7?1.2
5 Lucia Specia <lspecia@gmail.com> USP-IBM-1 ILP 85.1?1.2
5 Deniz Yuret <dyuret@ku.edu.tr> KU Semi-supervised 85.1?1.2
6 Saarikoski <harri.saarikoski@helsinki.fi> OE naive Bayes, SVM 83.8?1.2
7 University of Technology Brno VUTBR naive Bayes 80.3?1.2
8 Ana Zelaia <ana.zelaia@ehu.es> UBC-ZAS SVD+kNN 79.9?1.2
9 Carlo Strapparava <strappa@itc.it> ITC-irst SVM 79.6?1.2
10 Most frequent sense in training Baseline N/A 78.0?1.2
11 Toby Hawker <toby@it.usyd.edu.au> USYD SVM 74.3?1.2
12 Siddharth Patwardhan <sidd@cs.utah.edu> UMND1 Unsupervised 53.8?1.2
13 Saif Mohammad <smm@cs.toronto.edu> Tor Unsupervised 52.1?1.2
- Toby Hawker <toby@it.usyd.edu.au> USYD? SVM 89.1?1.2
- Carlo Strapparava <strappa@itc.it> ITC? SVM 89.1?1.2
Table 3: System Performance for the OntoNotes Lexical Sample task. Systems marked with an * were
post-competition bug-fix submissions.
grained senses provides consistently higher per-
formance than previous more fine-grained Lexical
Sample Tasks. The high scores here were foreshad-
owed in an evaluation involving a subset of the data
last summer (Chen et al, 2006). Note that the best
system performance is now closely approaching the
ITA for this data of over 90%. Table 4 shows the
performance of the top 8 systems on all the indi-
vidual verbs and nouns in the test set. Owing to
space constraints we have removed some lemmas
that have perfect or almost perfect accuracies. At the
right are mentioned the average, minimum and max-
imum performances of the teams per lemma, and at
the bottom are the average scores per lemma (with-
out considering the lemma frequencies) and broken
down by verbs and nouns. A gap of about 10 points
between the verb and noun performance seems to
indicate that in general the verbs were more difficult
than the nouns. However, this might just be owing
to this particular test sample having more verbs with
higher perplexities, and maybe even ones that are
indeed difficult to disambiguate ? in spite of high
human agreement. The hope is that better knowl-
edge sources can overcome the gap still existing be-
tween the system performance and human agree-
ment. Overall, however, this data indicates that the
approach suggested by (Palmer, 2000) and that is be-
ing adopted in the ongoing OntoNotes project (Hovy
et al, 2006) does result in higher system perfor-
mance. Whether or not the more coarse-grained
senses are effective in improving natural language
processing applications remains to be seen.
89
Lemma S s T t 1 2 3 4 5 6 7 8 Average Min Max
turn.v 13 8 340 62 58 61 40 55 52 53 27 44 49 27 61
go.v 12 6 244 61 64 69 38 66 43 46 31 39 49 31 69
come.v 10 9 186 43 49 46 56 60 37 23 23 49 43 23 60
set.v 9 5 174 42 62 50 52 57 50 57 36 50 52 36 62
hold.v 8 7 129 24 58 46 50 54 54 38 50 67 52 38 67
raise.v 7 6 147 34 50 44 29 26 44 26 24 12 32 12 50
work.v 7 5 230 43 74 65 65 65 72 67 46 65 65 46 74
keep.v 7 6 260 80 56 54 52 64 56 52 48 51 54 48 64
start.v 6 4 214 38 53 50 47 55 45 42 37 45 47 37 55
lead.v 6 6 165 39 69 69 85 69 51 69 36 46 62 36 85
see.v 6 5 158 54 56 54 46 54 57 52 48 48 52 46 57
ask.v 6 3 348 58 84 72 72 78 76 52 67 66 71 52 84
find.v 5 3 174 28 93 93 86 89 82 82 75 86 86 75 93
fix.v 5 3 32 2 50 50 50 50 50 0 0 50 38 0 50
buy.v 5 3 164 46 83 80 80 83 78 76 70 76 78 70 83
begin.v 4 2 114 48 83 65 75 69 79 56 50 56 67 50 83
kill.v 4 1 111 16 88 88 88 88 88 88 88 81 87 81 88
join.v 4 4 68 18 44 50 50 39 56 57 39 44 47 39 57
end.v 4 3 135 21 90 86 86 90 62 87 86 67 82 62 90
do.v 4 2 207 61 92 90 90 93 93 90 85 84 90 84 93
examine.v 3 2 26 3 100 100 67 100 100 67 100 33 83 33 100
report.v 3 2 128 35 89 91 91 91 91 91 91 86 90 86 91
regard.v 3 3 40 14 93 93 86 86 64 86 57 93 82 57 93
recall.v 3 1 49 15 100 100 87 87 93 87 87 87 91 87 100
prove.v 3 2 49 22 90 88 82 80 90 86 70 74 82 70 90
claim.v 3 2 54 15 67 73 80 80 80 80 80 87 78 67 87
build.v 3 3 119 46 74 67 74 61 54 74 61 72 67 54 74
feel.v 3 3 347 51 71 69 69 74 76 69 61 71 70 61 76
care.v 3 3 69 7 43 43 43 43 100 29 57 57 52 29 100
contribute.v 2 2 35 18 67 72 72 67 50 61 50 67 63 50 72
maintain.v 2 2 61 10 80 80 70 100 80 90 90 80 84 70 100
complain.v 2 1 32 14 93 86 86 86 86 86 86 79 86 79 93
propose.v 2 2 34 14 100 86 100 86 100 93 79 79 90 79 100
promise.v 2 2 50 8 88 88 75 88 75 75 62 88 80 62 88
produce.v 2 2 115 44 82 82 77 73 75 75 77 80 78 73 82
prepare.v 2 2 54 18 94 83 89 89 83 86 83 83 86 83 94
explain.v 2 2 85 18 94 89 94 89 94 89 89 94 92 89 94
believe.v 2 2 202 55 87 78 78 86 84 78 74 80 81 74 87
occur.v 2 2 47 22 86 73 91 96 86 96 86 82 87 73 96
grant.v 2 2 19 5 100 80 80 80 40 80 60 80 75 40 100
enjoy.v 2 2 56 14 50 57 57 50 64 57 50 57 55 50 64
need.v 2 2 195 56 89 82 86 89 86 78 70 70 81 70 89
disclose.v 1 1 55 14 93 93 93 93 93 93 93 93 93 93 93
point.n 9 6 469 150 91 91 89 91 92 87 84 79 88 79 92
position.n 7 6 268 45 78 78 78 53 56 65 58 64 66 53 78
defense.n 7 7 120 21 57 48 52 43 48 29 48 48 46 29 57
carrier.n 7 3 111 21 71 71 71 71 67 71 71 62 70 62 71
order.n 7 4 346 57 93 95 93 91 93 92 90 91 92 90 95
exchange.n 5 3 363 61 92 90 92 85 90 88 82 79 87 79 92
system.n 5 3 450 70 79 73 66 67 59 63 63 61 66 59 79
source.n 5 5 152 35 86 80 80 63 83 68 60 29 69 29 86
space.n 5 2 67 14 93 100 93 93 93 86 86 71 89 71 100
base.n 5 4 92 20 75 80 75 50 65 40 50 75 64 40 80
authority.n 4 3 90 21 86 86 81 62 71 33 71 81 71 33 86
people.n 4 4 754 115 96 96 95 96 95 90 91 91 94 90 96
chance.n 4 3 91 15 60 67 60 60 67 73 20 73 60 20 73
part.n 4 3 481 71 90 90 92 97 90 74 66 66 83 66 97
hour.n 4 2 187 48 83 85 92 83 77 90 58 92 83 58 92
development.n 3 3 180 29 100 79 86 79 76 62 79 62 78 62 100
president.n 3 3 879 177 98 97 98 97 93 96 97 85 95 85 98
network.n 3 3 152 55 91 87 98 89 84 88 87 82 88 82 98
future.n 3 3 350 146 97 96 94 97 83 98 89 85 92 83 98
effect.n 3 2 178 30 97 93 80 93 80 90 77 83 87 77 97
state.n 3 3 617 72 85 86 86 83 82 79 83 82 83 79 86
power.n 3 3 251 47 92 87 87 81 77 77 77 74 81 74 92
bill.n 3 3 404 102 98 99 98 96 90 96 96 22 87 22 99
area.n 3 3 326 37 89 73 65 68 84 70 68 65 73 65 89
job.n 3 3 188 39 85 80 77 90 80 82 69 82 80 69 90
management.n 2 2 284 45 89 78 87 73 98 76 67 64 79 64 98
condition.n 2 2 132 34 91 82 82 56 76 78 74 76 77 56 91
policy.n 2 2 331 39 95 97 97 87 95 97 90 64 90 64 97
rate.n 2 2 1009 145 90 88 92 81 92 89 88 91 89 81 92
drug.n 2 2 205 46 94 94 96 78 94 94 87 78 89 78 96
Average Overall 86 83 83 82 82 79 76 77
Verbs 78 75 73 76 73 70 65 70
Nouns 89 87 86 81 83 80 77 76
Table 4: All Supervised system performance per predicate. (Column legend ? S=number of senses in training; s=number senses appearing more than 3 times;
T=instances in training; t=instances in test.; The numbers indicate system ranks.)
90
3 Semantic Role Labeling
Subtask 2 evaluates Semantic Role Labeling (SRL)
systems, where the goal is to locate the constituents
which are arguments of a given verb, and to assign
them appropriate semantic roles that describe how
they relate to the verb. SRL systems are an impor-
tant building block for many larger semantic sys-
tems. For example, in order to determine that ques-
tion (1a) is answered by sentence (1b), but not by
sentence (1c), we must determine the relationships
between the relevant verbs (eat and feed) and their
arguments.
(1) a. What do lobsters like to eat?
b. Recent studies have shown that lobsters pri-
marily feed on live fish, dig for clams, sea
urchins, and feed on algae and eel-grass.
c. In the early 20th century, Mainers would
only eat lobsters because the fish they
caught was too valuable to eat themselves.
Traditionally, SRL systems have been trained on
either the PropBank corpus (Palmer et al, 2005)
? for two years, the CoNLL workshop (Carreras
and Ma`rquez, 2004; Carreras and Ma`rquez, 2005)
has made this their shared task, or the FrameNet
corpus ? Senseval-3 used this for their shared task
(Litkowski, 2004). However, there is still little con-
sensus in the linguistics and NLP communities about
what set of role labels are most appropriate. The
PropBank corpus avoids this issue by using theory-
agnostic labels (ARG0, ARG1, . . . , ARG5), and
by defining those labels to have only verb-specific
meanings. Under this scheme, PropBank can avoid
making any claims about how any one verb?s ar-
guments relate to other verbs? arguments, or about
general distinctions between verb arguments and ad-
juncts.
However, there are several limitations to this ap-
proach. The first is that it can be difficult to make
inferences and generalizations based on role labels
that are only meaningful with respect to a single
verb. Since each role label is verb-specific, we can
not confidently determine when two different verbs?
arguments have the same role; and since no encoded
meaning is associated with each tag, we can not
make generalizations across verb classes. In con-
trast, the use of a shared set of role labels, such
System Type Precision Recall F
UBC-UPC Open 84.51 82.24 83.36?0.5
UBC-UPC Closed 85.04 82.07 83.52?0.5
RTV Closed 81.82 70.37 75.66?0.6
Without ?say?
UBC-UPC Open 78.57 74.70 76.60?0.8
UBC-UPC Closed 78.67 73.94 76.23?0.8
RTV Closed 74.15 57.85 65.00?0.9
Table 5: System performance on PropBank argu-
ments.
as VerbNet roles, would facilitate both inferencing
and generalization. VerbNet has more traditional la-
bels such as Agent, Patient, Theme, Beneficiary, etc.
(Kipper et al, 2006).
Therefore, we chose to annotate the corpus us-
ing two different role label sets: the PropBank role
set and the VerbNet role set. VerbNet roles were
generated using the SemLink mapping (Loper et al,
2007), which provides a mapping between Prop-
Bank and VerbNet role labels. In a small number of
cases, no VerbNet role was available (e.g., because
VerbNet did not contain the appropriate sense of the
verb). In those cases, the PropBank role label was
used instead.
We proposed two levels of participation in this
task: i) Closed ? the systems could use only the an-
notated data provided and nothing else. ii) Open ?
where systems could use PropBank data from Sec-
tions 02-21, as well as any other resource for training
their labelers.
3.1 Data
We selected 50 verbs from the 65 in the lexical sam-
ple task for the SRL task. The partitioning into train
and test set was done in the same fashion as for the
lexical sample task. Since PropBank does not tag
any noun predicates, none of the 35 nouns from the
lexical sample task were part of this data.
3.2 Results
For each system, we calculated the precision, re-
call, and F-measure for both role label sets. Scores
were calculated using the srl-eval.pl script from
the CoNLL-2005 scoring package (Carreras and
Ma`rquez, 2005). Only two teams chose to perform
the SRL subtask. The performance of these two
teams is shown in Table 5 and Table 6.
91
System Type Precision Recall F
UBC-UPC Open 85.31 82.08 83.66?0.5
UBC-UPC Closed 85.31 82.08 83.66?0.5
RTV Closed 81.58 70.16 75.44?0.6
Without ?say?
UBC-UPC Open 79.23 73.88 76.46?0.8
UBC-UPC Closed 79.23 73.88 76.46?0.8
RTV Closed 73.63 57.44 64.53?0.9
Table 6: System performance on VerbNet roles.
3.3 Discussion
Given that only two systems participated in the task,
it is difficult to form any strong conclusions. It
should be noted that since there was no additional
VerbNet role data to be used by the Open system, the
performance of that on PropBank arguments as well
as VerbNet roles is exactly identical. It can be seen
that there is almost no difference between the perfor-
mance of the Open and Closed systems for tagging
PropBank arguments. The reason for this is the fact
that all the instances of the lemma under consider-
ation was selected from the Propbank corpus, and
probably the number of training instances for each
lemma as well as the fact that the predicate is such
an important feature combine to make the difference
negligible. We also realized that more than half of
the test instances were contributed by the predicate
?say? ? the performance over whose arguments is in
the high 90s. To remove the effect of ?say? we also
computed the performances after excluding exam-
ples of ?say? from the test set. These numbers are
shown in the bottom half of the two tables. These
results are not directly comparable to the CoNLL-
2005 shared task since: i) this test set comprises
Sections 01, 22, 23 and 24 as opposed to just Sec-
tion 23, and ii) this test set comprises data for only
50 predicates as opposed to all the verb predicates in
the CoNLL-2005 shared task.
4 Conclusions
The results in the previous discussion seem to con-
firm the hypothesis that there is a predictable corre-
lation between human annotator agreement and sys-
tem performance. Given high enough ITA rates we
can can hope to build sense disambiguation systems
that perform at a level that might be of use to a con-
suming natural language processing application. It
is also encouraging that the more informative Verb-
Net roles which have better/direct applicability in
downstream systems, can also be predicted with al-
most the same degree of accuracy as the PropBank
arguments from which they are mapped.
5 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022;
National Science Foundation Grant NSF-0415923,
Word Sense Disambiguation; the DTO-AQUAINT
NBCHC040036 grant under the University of
Illinois subcontract to University of Pennsylvania
2003-07911-01; and NSF-ITR-0325646:
Domain-Independent Semantic Interpretation.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL-2004.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In
Proceedings of CoNLL-2005.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer.
2006. An empirical study of the behavior of active learning for
word sense disambiguation. In Proceedings of HLT/NAACL.
Timothy Chklovski and Rada Mihalcea. 2002. Building a
sense tagged corpus with open mind word expert. In
Proceedings of ACL-02 Workshop on WSD.
Bart Decadt, Ve?ronique Hoste, Walter Daelemans, and Antal
Van den Bosch. 2004. GAMBL, genetic algorithm
optimization of memory-based wsd. In Senseval-3.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90%
solution. In Proceedings of HLT/NAACL, June.
Karin Kipper, Anna Korhonen, Neville Ryant, and Martha
Palmer. 2006. Extending VerbNet with novel verb classes. In
LREC-06.
Ken Litkowski. 2004. Senseval-3 task: Automatic labeling of
semantic roles. In Proceedings of Senseval-3.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between propbank and
verbnet. In Proceedings of the IWCS-7.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
proposition bank: A corpus annotated with semantic roles.
Computational Linguistics, 31(1):71?106.
Martha Palmer. 2000. Consistent criteria for sense
distinctions. Computers and the Humanities, 34(1-1):217?222.
92
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 6?10,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Good Seed Makes a Good Crop:
Accelerating Active Learning Using Language Modeling
Dmitriy Dligach
Department of Computer Science
University of Colorado at Boulder
Dmitriy.Dligach@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
Martha.Palmer@colorado.edu
Abstract
Active Learning (AL) is typically initialized
with a small seed of examples selected ran-
domly. However, when the distribution of
classes in the data is skewed, some classes
may be missed, resulting in a slow learning
progress. Our contribution is twofold: (1) we
show that an unsupervised language modeling
based technique is effective in selecting rare
class examples, and (2) we use this technique
for seeding AL and demonstrate that it leads
to a higher learning rate. The evaluation is
conducted in the context of word sense disam-
biguation.
1 Introduction
Active learning (AL) (Settles, 2009) has become a
popular research field due to its potential benefits: it
can lead to drastic reductions in the amount of anno-
tation that is necessary for training a highly accurate
statistical classifier. Unlike in a random sampling
approach, where unlabeled data is selected for anno-
tation randomly, AL delegates the selection of un-
labeled data to the classifier. In a typical AL setup,
a classifier is trained on a small sample of the data
(usually selected randomly), known as the seed ex-
amples. The classifier is subsequently applied to a
pool of unlabeled data with the purpose of selecting
additional examples that the classifier views as infor-
mative. The selected data is annotated and the cycle
is repeated, allowing the learner to quickly refine the
decision boundary between the classes.
Unfortunately, AL is susceptible to a shortcom-
ing known as the missed cluster effect (Schu?tze et
al., 2006) and its special case called the missed class
effect (Tomanek et al, 2009). The missed cluster ef-
fect is a consequence of the fact that seed examples
influence the direction the learner takes in its ex-
ploration of the instance space. Whenever the seed
does not contain the examples of a certain cluster
that is representative of a group of examples in the
data, the learner may become overconfident about
the class membership of this cluster (particularly if it
lies far from the decision boundary). As a result, the
learner spends a lot of time exploring one region of
the instance space at the expense of missing another.
This problem can become especially severe, when
the class distribution in the data is skewed: a ran-
domly selected seed may not adequately represent
all the classes or even miss certain classes altogether.
Consider a binary classification task where rare class
examples constitute 5% of the data (a frequent sce-
nario in e.g. word sense disambiguation). If 10
examples are chosen randomly for seeding AL, the
probability that none of the rare class examples will
make it to the seed is 60% 1. Thus, there is a high
probability that AL would stall, selecting only the
examples of the predominant class over the course
of many iterations. At the same time, if we had a
way to ensure that examples of the rare class were
present in the seed, AL would be able to select the
examples of both classes, efficiently clarifying the
decision boundary and ultimately producing an ac-
curate classifier.
Tomanek et al (2009) simulated these scenarios
using manually constructed seed sets. They demon-
strated that seeding AL with a data set that is artifi-
cially enriched with rare class examples indeed leads
to a higher learning rate comparing to randomly
1Calculated using Binomial distribution
6
sampled and predominant class enriched seeds. In
this paper, we propose a simple automatic approach
for selecting the seeds that are rich in the examples
of the rare class. We then demonstrate that this ap-
proach to seed selection accelerates AL. Finally, we
analyze the mechanism of this acceleration.
2 Approach
Language Model (LM) Sampling is a simple unsu-
pervised technique for selecting unlabeled data that
is enriched with rare class examples. LM sampling
involves training a LM on a corpus of unlabeled can-
didate examples and selecting the examples with low
LM probability. Dligach and Palmer (2009) used
this technique in the context of word sense disam-
biguation and showed that rare sense examples tend
to concentrate among the examples with low prob-
ability. Unfortunately these authors provided a lim-
ited evaluation of this technique: they looked at its
effectiveness only at a single selection size. We pro-
vide a more convincing evaluation in which the ef-
fectiveness of this approach is examined for all sizes
of the selected data.
Seed Selection for AL is typically done ran-
domly. However, for datasets with a skewed dis-
tribution of classes, rare class examples may end
up being underrepresented. We propose to use LM
sampling for seed selection, which captures more
examples of rare classes than random selection, thus
leading to a faster learning progress.
3 Evaluation
3.1 Data
For our evaluation, we needed a dataset that is
characterized by a skewed class distribution. This
phenomenon is pervasive in word sense data. A
large word sense annotated corpus has recently
been released by the OntoNotes (Hovy et al, 2006;
Weischedel et al, 2009) project. For clarity of eval-
uation, we identify a set of verbs that satisfy three
criteria: (1) the number of senses is two, (2) the
number of annotated examples is at least 100, (3) the
proportion of the rare sense is at most 20%. The fol-
lowing 25 verbs satisfy these criteria: account, add,
admit, allow, announce, approve, compare, demand,
exist, expand, expect, explain, focus, include, invest,
issue, point, promote, protect, receive, remain, re-
place, strengthen, wait, wonder. The average num-
ber of examples for these verbs is 232. In supervised
word sense disambiguation, a single model per word
is typically trained and that is the approach we take.
Thus, we conduct our evaluation using 25 different
data sets. We report the averages across these 25
data sets. In our evaluation, we use a state-of-the-
art word sense disambiguation system (Dligach and
Palmer, 2008), that utilizes rich linguistic features to
capture the contexts of ambiguous words.
3.2 Rare Sense Retrieval
The success of our approach to seeding AL hinges
on the ability of LM sampling to discover rare class
examples better than random sampling. In this ex-
periment, we demonstrate that LM sampling outper-
forms random sampling for every selection size. For
each verb we conduct an experiment in which we
select the instances of this verb using both methods.
We measure the recall of the rare sense, which we
calculate as the ratio of the number of selected rare
sense examples to the total number of rare sense ex-
amples for this verb.
We train a LM (Stolcke, 2002) on the corpora
from which OntoNotes data originates: the Wall
Street Journal, English Broadcast News, English
Conversation, and the Brown corpus. For each verb,
we compute the LM probability for each instance of
this verb and sort the instances by probability. In
the course of the experiment, we select one example
with the smallest probability and move it to the set
of selected examples. We then measure the recall of
the rare sense for the selected examples. We con-
tinue in this fashion until all the examples have been
selected. We use random sampling as a baseline,
which is obtained by continuously selecting a single
example randomly. We continue until all the exam-
ples have been selected. At the end of the exper-
iment, we have produced two recall curves, which
measure the recall of the rare sense retrieval for this
verb at various sizes of selected data. Due to the
lack of space, we do not show the plots that display
these curves for individual verbs. Instead, in Figure
1 we display the curves that are averaged across all
verbs. At every selection size, LM sampling results
in a higher recall of the rare sense. The average dif-
ference across all selection sizes is 11%.
7
Figure 1: Average recall of rare sense retrieval for LM
and random sampling by relative size of training set
3.3 Classic and Selectively Seeded AL
In this experiment, we seed AL using LM sampling
and compare how this selectively seeded AL per-
forms in comparison with classic (randomly-seeded)
AL. Our experimental setup is typical for an active
learning study. We split the set of annotated exam-
ples for a verb into 90% and 10% parts. The 90%
part is used as a pool of unlabeled data. The 10%
part is used as a test set. We begin classic AL by
randomly selecting 10% of the examples from the
pool to use as seeds. We train a maximum entropy
model (Le, 2004) using these seeds. We then repeat-
edly apply the model to the remaining examples in
the pool: on each iteration of AL, we draw a sin-
gle most informative example from the pool. The
informativeness is estimated using prediction mar-
gin (Schein and Ungar, 2007), which is computed as
|P (c1|x) ? P (c2|x)|, where c1 and c2 are the two
most probable classes of example x according to the
model. The selected example is moved to the train-
ing set. On each iteration, we also keep track of how
accurately the current model classifies the held out
test set.
In parallel, we conduct a selectively seeded AL
experiment that is identical to the classic one but
with one crucial difference: instead of selecting the
seed examples randomly, we select them using LM
sampling by identifying 10% of the examples from
the pool with the smallest LM probability. We also
produce a random sampling curve to be used as a
baseline. At the end of this experiment we have ob-
tained three learning curves: for classic AL, for se-
lectively seeded AL, and for the random sampling
baseline. The final learning curves for each verb are
produced by averaging the learning curves from ten
different trials.
Figure 2 presents the average accuracy of selec-
tively seeded AL (top curve), classic AL (middle
curve) and the random sampling baseline (bottom
curve) at various fractions of the total size of the
training set. The size of zero corresponds to a train-
ing set consisting only of the seed examples. The
size of one corresponds to a training set consisting
of all the examples in the pool labeled. The accuracy
at a given size was averaged across all 25 verbs.
It is clear that LM-seeded AL accelerates learn-
ing: it reaches the same performance as classic AL
with less training data. LM-seeded AL also reaches
a higher classification accuracy (if stopped at its
peak). We will analyze this somewhat surprising be-
havior in the next section. The difference between
the classic and LM-seeded curves is statistically sig-
nificant (p = 0.0174) 2.
Figure 2: Randomly and LM-seeded AL. Random sam-
pling baseline is also shown.
3.4 Why LM Seeding Produces Better Results
For random sampling, the system achieves its best
accuracy, 94.4%, when the entire pool of unlabeled
examples is labeled. The goal of a typical AL study
is to demonstrate that the same accuracy can be
2We compute the average area under the curve for each type
of AL and use Wilcoxon signed rank test to test whether the
difference between the averages is significant.
8
achieved with less labeled data. For example, in our
case, classic AL reaches the best random sampling
accuracy with only about 5% of the data. However,
it is interesting to notice that LM-seeded AL actually
reaches a higher accuracy, 95%, during early stages
of learning (at 15% of the total training set size). We
believe this phenomenon takes place due to overfit-
ting the predominant class: as the model receives
new data (and therefore more and more examples of
the predominant class), it begins to mislabel more
and more examples of the rare class. A similar idea
has been expressed in literature (Weiss, 1995; Kubat
and Matwin, 1997; Japkowicz, 2001; Weiss, 2004;
Chen et al, 2006), however it has never been veri-
fied in the context of AL.
To verify our hypothesis, we conduct an experi-
ment. The experimental setup is the same as in sec-
tion 3.3. However, instead of measuring the accu-
racy on the test set, we resort to different metrics
that reflect how accurately the classifier labels the in-
stances of the rare class in the held out test set. These
metrics are the recall and precision for the rare class.
Recall is the ratio of the correctly labeled examples
of the rare class and the total number of instances of
the rare class. Precision is the ratio of the correctly
labeled examples of the rare class and the number of
instances labeled as that class. Results are in Figures
3 and 4.
Figure 3: Rare sense classification recall
Observe that for LM-seeded AL, the recall peaks
at first and begins to decline later. Thus the clas-
sifier makes progressively more errors on the rare
class as more labeled examples are being received.
Figure 4: Rare sense classification precision
This is consistent with our hypothesis that the clas-
sifier overfits the predominant class. When all the
data is labeled, the recall decreases from about 13%
to only 7%, an almost 50% drop. The reason that
the system achieved a higher level of recall at first is
due to the fact that AL was seeded with LM selected
data, which has a higher content of rare classes (as
we demonstrated in the first experiment). The avail-
ability of the extra examples of the rare class allows
the classifier to label the instances of this class in
the test set more accurately, which in turn boosts the
overall accuracy.
4 Conclusion and Future Work
We introduced a novel approach to seeding AL, in
which the seeds are selected from the examples with
low LM probability. This approach selects more rare
class examples than random sampling, resulting in
more rapid learning and, more importantly, leading
to a classifier that performs better on rare class ex-
amples. As a consequence of this, the overall classi-
fication accuracy is higher than that for classic AL.
Our plans for future work include improving our
LM by incorporating syntactic information such as
POS tags. This should result in better performance
on the rare classes, which is currently still low.
We also plan to experiment with other unsupervised
techniques, such as clustering and outlier detection,
that can lead to better retrieval of rare classes. Fi-
nally, we plan to investigate the applicability of our
approach to a multi-class scenario.
9
Acknowledgements
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambiguation,
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-C-0022, a subcontract from the BBN-AGILE
Team. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behavior
of active learning for word sense disambiguation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 120?127, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ?08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29?32, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Dmitriy Dligach and Martha. Palmer. 2009. Using lan-
guage modeling to select useful annotation data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Student Research Workshop and
Doctoral Consortium, pages 25?30. Association for
Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57?60, Morristown, NJ, USA. Association for
Computational Linguistics.
Nathalie Japkowicz. 2001. Concept-learning in the pres-
ence of between-class and within-class imbalances. In
AI ?01: Proceedings of the 14th Biennial Conference
of the Canadian Society on Computational Studies
of Intelligence, pages 67?77, London, UK. Springer-
Verlag.
M. Kubat and S. Matwin. 1997. Addressing the curse of
imbalanced training sets: one-sided selection. In Pro-
ceedings of the Fourteenth International Conference
on Machine Learning, pages 179?186. Citeseer.
Zhang Le, 2004. Maximum Entropy Modeling Toolkit for
Python and C++.
A.I. Schein and L.H. Ungar. 2007. Active learning for
logistic regression: an evaluation. Machine Learning,
68(3):235?265.
H. Schu?tze, E. Velipasaoglu, and J.O. Pedersen. 2006.
Performance thresholding in practical text classifica-
tion. In Proceedings of the 15th ACM international
conference on Information and knowledge manage-
ment, pages 662?671. ACM.
Burr Settles. 2009. Active learning literature survey. In
Computer Sciences Technical Report 1648 University
of Wisconsin-Madison.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, Denver, Colorado., pages
901?904.
Katrin Tomanek, Florian Laws, Udo Hahn, and Hinrich
Schu?tze. 2009. On proper unit selection in active
learning: co-selection effects for named entity recog-
nition. In HLT ?09: Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 9?17, Morristown, NJ, USA.
Association for Computational Linguistics.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R Belvin, S Pradan, L. Ramshaw, and N. Xue, 2009.
OntoNotes: A Large Training Corpus for Enhanced
Processing, chapter in Global Automatic Language
Exploitation, pages 54?63. Springer Verglag.
G.M. Weiss. 1995. Learning with rare cases and small
disjuncts. In Proceedings of the Twelfth International
Conference on Machine Learning, pages 558?565.
Citeseer.
G.M. Weiss. 2004. Mining with rarity: a unifying
framework. ACM SIGKDD Explorations Newsletter,
6(1):7?19.
10
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 81?86,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Descending-Path Convolution Kernel for Syntactic Structures
Chen Lin
1
, Timothy Miller
1
, Alvin Kho
1
, Steven Bethard
2
,
Dmitriy Dligach
1
, Sameer Pradhan
1
and Guergana Savova
1
,
1
Children?s Hospital Boston Informatics Program and Harvard Medical School
{firstname.lastname}@childrens.harvard.edu
2
Department of Computer and Information Sciences, University of Alabama at Birmingham
bethard@cis.uab.edu
Abstract
Convolution tree kernels are an efficient
and effective method for comparing syntac-
tic structures in NLP methods. However,
current kernel methods such as subset tree
kernel and partial tree kernel understate the
similarity of very similar tree structures.
Although soft-matching approaches can im-
prove the similarity scores, they are corpus-
dependent and match relaxations may be
task-specific. We propose an alternative ap-
proach called descending path kernel which
gives intuitive similarity scores on compa-
rable structures. This method is evaluated
on two temporal relation extraction tasks
and demonstrates its advantage over rich
syntactic representations.
1 Introduction
Syntactic structure can provide useful features for
many natural language processing (NLP) tasks
such as semantic role labeling, coreference resolu-
tion, temporal relation discovery, and others. How-
ever, the choice of features to be extracted from a
tree for a given task is not always clear. Convolu-
tion kernels over syntactic trees (tree kernels) offer
a potential solution to this problem by providing
relatively efficient algorithms for computing sim-
ilarities between entire discrete structures. These
kernels use tree fragments as features and count
the number of common fragments as a measure of
similarity between any two trees.
However, conventional tree kernels are sensitive
to pattern variations. For example, two trees in Fig-
ure 1(a) sharing the same structure except for one
terminal symbol are deemed at most 67% similar
by the conventional tree kernel (PTK) (Moschitti,
2006). Yet one might expect a higher similarity
given their structural correspondence.
The similarity is further attenuated by trivial
structure changes such as the insertion of an ad-
jective in one of the trees in Figure 1(a), which
would reduce the similarity close to zero. Such
an abrupt attenuation would potentially propel a
model to memorize training instances rather than
generalize from trends, leading towards overfitting.
In this paper, we describe a new kernel over
syntactic trees that operates on descending paths
through the tree rather than production rules as
used in most existing methods. This representation
is reminiscent of Sampson?s (2000) leaf-ancestor
paths for scoring parse similarities, but here it is
generalized over all ancestor paths, not just those
from the root to a leaf. This approach assigns more
robust similarity scores (e.g., 78% similarity in the
above example) than other soft matching tree ker-
nels, is faster than the partial tree kernel (Moschitti,
2006), and is less ad hoc than the grammar-based
convolution kernel (Zhang et al, 2007).
2 Background
2.1 Syntax-based Tree Kernels
Syntax-based tree kernels quantify the similarity
between two constituent parses by counting their
common sub-structures. They differ in their defini-
tion of the sub-structures.
Collins and Duffy (2001) use a subset tree (SST)
representation for their sub-structures. In the SST
representation, a subtree is defined as a subgraph
with more than one node, in which only full pro-
duction rules are expanded. While this approach is
widely used and has been successful in many tasks,
the production rule-matching constraint may be un-
necessarily restrictive, giving zero credit to rules
that have only minor structural differences. For
example, the similarity score between the NPs in
Figure 1(b) would be zero since the production rule
is different (the overall similarity score is above-
zero because of matching pre-terminals).
The partial tree kernel (PTK) relaxes the defi-
nition of subtrees to allow partial production rule
81
a)
NP
DT
a
NN
cat
NP
DT
a
NN
dog
b)
NP
DT
a
NN
cat
NP
DT
a
JJ
fat
NN
cat
c)
S
ADVP
RB
here
NP
PRP
she
VP
VBZ
comes
S
NP
PRP
she
VP
VBZ
comes
ADVP
RB
here
Figure 1: Three example tree pairs.
matching (Moschitti, 2006). In the PTK, a subtree
may or may not expand any child in a production
rule, while maintaining the ordering of the child
nodes. Thus it generates a very large but sparse
feature space. To Figure 1(b), the PTK generates
fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a]
[NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among
others, for the second tree. This allows for partial
matching ? substructure (ii) ? while also generating
some fragments that violate grammatical intuitions.
Zhang et al (2007) address the restrictiveness
of SST by allowing soft matching of production
rules. They allow partial matching of optional
nodes based on the Treebank. For example, the
rule NP ? DT JJ NN indicates a noun phrase
consisting of a determiner, adjective, and common
noun. Zhang et al?s method designates the JJ as
optional, since the Treebank contains instances of
a reduced version of the rule without the JJ node
(NP ? DT NN ). They also allow node match-
ing among similar preterminals such as JJ, JJR, and
JJS, mapping them to one equivalence class.
Other relevant approaches are the spectrum tree
(SpT) (Kuboyama et al, 2007) and the route kernel
(RtT) (Aiolli et al, 2009). SpT uses a q-gram
? a sequence of connected vertices of length q ?
as their sub-structure. It observes grammar rules
by recording the orientation of edges: a?b?c is
different from a?b?c. RtT uses a set of routes as
basic structures, which observes grammar rules by
NP
DT
a
NN
cat
l=0: [NP],[DT],[NN]
l=1: [NP-DT],[NP-NN],
[DT-a],[NN-cat]
l=2: [NP-DT-a],[NP-NN-cat]
Figure 2: A parse tree (left) and its descending
paths according to Definition 1 (l - length).
recording the index of a neighbor node.
2.2 Temporal Relation Extraction
Among NLP tasks that use syntactic informa-
tion, temporal relation extraction has been draw-
ing growing attention because of its wide applica-
tions in multiple domains. As subtasks in Tem-
pEval 2007, 2010 and 2013, multiple systems
were built to create labeled links from events
to events/timestamps by using a variety of fea-
tures (Bethard and Martin, 2007; Llorens et al,
2010; Chambers, 2013). Many methods exist for
synthesizing syntactic information for temporal
relation extraction, and most use traditional tree
kernels with various feature representations. Mir-
roshandel et al (2009) used the path-enclosed tree
(PET) representation to represent syntactic informa-
tion for temporal relation extraction on the Time-
Bank (Pustejovsky et al, 2003) and the AQUAINT
TimeML corpus
1
. The PET is the smallest subtree
that contains both proposed arguments of a relation.
Hovy et al (2012) used bag tree structures to rep-
resent the bag of words (BOW) and bag of part of
speech tags (BOP) between the event and time in
addition to a set of baseline features, and improved
the temporal linking performance on the TempEval
2007 and Machine Reading corpora (Strassel et
al., 2010). Miller at al. (2013) used PET tree, bag
tree, and path tree (PT, which is similar to a PET
tree with the internal nodes removed) to represent
syntactic information and improved the temporal
relation discovery performance on THYME data
2
(Styler et al, 2014). In this paper, we also use
syntactic structure-enriched temporal relation dis-
covery as a vehicle to test our proposed kernel.
3 Methods
Here we decribe the Descending Path Kernel
(DPK).
1
http://www.timeml.org
2
http://thyme.healthnlp.org
82
Definition 1 (Descending Path): Let T be a
parse tree, v any non-terminal node in T , dv a
descendant of v, including terminals. A descending
path is the sequence of indexes of edges connecting
v and dv, denoted by [v ? ? ? ? ? dv]. The length l
of a descending path is the number of connecting
edges. When l = 0, a descending path is the non-
terminal node itself, [v]. Figure 2 illustrates a parse
tree and its descending paths of different lengths.
Suppose that all descending paths of a tree T are
indexed 1, ? ? ? , n, and path
i
(T ) is the frequency
of the i-th descending path in T . We represent T as
a vector of frequencies of all its descending paths:
?(T ) = (path
1
(T ), ? ? ? , path
n
(T )).
The similarity between any two trees T
1
and T
2
can be assessed via the dot product of their respec-
tive descending path frequency vector representa-
tions: K(T
1
, T
2
) = ??(T
1
),?(T
2
)?.
Compared with the previous tree kernels, our
descending path kernel has the following advan-
tages: 1) the sub-structures are simplified so that
they are more likely to be shared among trees,
and therefore the sparse feature issues of previous
kernels could be alleviated by this representation;
2) soft matching between two similar structures
(e.g., NP?DT JJ NN versus NP?DT NN) have
high similarity without reference to any corpus or
grammar rules;
Following Collins and Duffy (2001), we derive
a recursive algorithm to compute the dot product
of the descending path frequency vector represen-
tations of two trees T
1
and T
2
:
K(T
1
, T
2
) = ??(T
1
),?(T
2
)?
=
?
i
path
i
(T
1
) ? path
i
(T
2
)
=
?
n
1
?N
1
?
n
2
?N
2
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
=
?
n
1
?N
1
n
2
?N
2
C(n
1
, n
2
)
(1)
where N
1
and N
2
are the sets of nodes in T
1
and
T
2
respectively, i indexes the set of possible paths,
I
path
i
(n) is an indicator function that is 1 iff the
descending path
i
is rooted at node n or 0 other-
wise. C(n
1
, n
2
) counts the number of common
descending paths rooted at nodes n
1
and n
2
:
C(n
1
, n
2
) =
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
C(n
1
, n
2
) can be computed in polynomial time by
the following recursive rules:
Rule 1: If n
1
and n
2
have different labels (e.g.,
?DT? versus ?NN?), then C(n1, n2) = 0;
Rule 2: Else if n
1
and n
2
have the same labels
and are both pre-terminals (POS tags), then
C(n
1
, n
2
) = 1 +
{
1 if term(n
1
) = term(n
2
)
0 otherwise.
where term(n) is the terminal symbol under n;
Rule 3: Else if n
1
and n
2
have the same labels
and they are not both pre-terminals, then:
C(n
1
, n
2
) = 1 +
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
where children(m) are the child nodes of m.
As in other tree kernel approaches (Collins and
Duffy, 2001; Moschitti, 2006), we use a discount
parameter ? to control for the disproportionately
large similarity values of large tree structures.
Therefore, Rule 2 becomes:
C(n
1
, n
2
) = 1 +
{
? if term(n
1
) = term(n
2
)
0 otherwise.
and Rule 3 becomes:
C(n
1
, n
2
) = 1 + ?
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
Note that Eq. (1) is a convolution kernel under
the kernel closure properties described in Haus-
sler (1999). Rules 1-3 show the equivalence be-
tween the number of common descending paths
rooted at nodes n
1
and n
2
, and the number of
matching nodes below n
1
and n
2
.
In practice, there are many non-matching nodes,
and most matching nodes will have only a few
matching children, so the running time, as in SST,
will be approximated by the number of matching
nodes between trees.
3.1 Relationship with other kernels
For a given tree, DPK will generate significantly
fewer sub-structures than PTK, since it does not
consider all ordered permutations of a production
rule. Moreover, the fragments generated by DPK
are more likely to be shared among different trees.
For the number of corpus-wide fragments, it is
83
Kernel ID #Frag Sim N(Sim)
SST a 9 3 0.50
O
(
?|N
1
||N
2
|
)
b 15 2 0.25
c 63 7 0.20
DPK a 11 7 0.78
O
(
?
2
|N
1
||N
2
|
)
b 13 9 0.83
c 31 22 0.83
PTK a 20 10 0.67
O
(
?
3
|N
1
||N
2
|
)
b 36 15 0.65
c 127 34 0.42
Table 1: Comparison of the worst case computa-
tional complexicity (? - the maximum branching
factor) and kernel performance on the 3 examples
from Figure 1. #Frag is the number of fragments,
N(Sim) is the normalized similarity. Please see
the online supplementary note for detailed frag-
ments of example (a).
possible that DPK? SST? PTK. In Table 1, given
? = 1, we compare the performance of 3 kernels
on the three examples in Figure 1. Note that for
more complicated structures, i.e., examples b and
c, DPK generates fewer fragments than SST and
PTK, with more shared fragments among trees.
The complexity for all three kernels are at least
O
(
|N
1
||N
2
|
)
since they share the pairwise summa-
tion at the end of Equation 1. SST, due to its re-
quirement of exact production rule matching, only
takes one pass in the inner loop which adds a factor
of ? (the maximum branching factor of any pro-
duction rule). DPK does a pairwise summation
of children, which adds a factor of ?
2
to the com-
plexity. Finally, the efficient algorithm for PTK
is proved by Moschitti (2006) to contain a con-
stant factor of ?
3
. Table 1 orders the tree kernels
according by their listed complexity.
It may seem that the value of DPK is strictly in its
ability to evaluate all paths, which is not explicitly
accounted for by other kernels. However, another
view of the DPK is possible by thinking of it as
cheaply calculating rule production similarity by
taking advantage of relatively strict English word
ordering. Like SST and PTK, the DPK requires
the root category of two subtrees to be the same
for the similarity to be greater than zero. Unlike
SST and PTK, once the root category comparison
is successfully completed, DPK looks at all paths
that go through it and accumulates their similarity
scores independent of ordering ? in other words, it
will ignore the ordering of the children in its pro-
duction rule. This means, for example, that if the
rule production NP? NN JJ DT were ever found
in a tree, to DPK it would be indistinguishable from
the common production NP? DT JJ NN, despite
having inverted word order, and thus would have
a maximal similarity score. SST and PTK would
assign this pair a much lower score for having com-
pletely different ordering, but we suggest that cases
such as these are very rare due to the relatively
strict word ordering of English. In most cases, the
determiner of a noun phrase will be at the front, the
nouns will be at the end, and the adjectives in the
middle. So with small differences in production
rules (one or two adjectives, extra nominal modifier,
etc.) the PTK will capture similarity by compar-
ing every possible partial rule completion, but the
DPK can obtain higher and faster scores by just
comparing one child at a time because the ordering
is constrained by the language. This analysis does
lead to a hypothesis for the general viability of the
DPK, suggesting that in languages with freer word
order it may give inflated scores to structures that
are syntactically dissimilar if they have the same
constituent components in different order.
Formally, Moschitti (2006) showed that SST is
a special case of PTK when only the longest child
sequence from each tree is considered. On the other
end of the spectrum, DPK is a special case of PTK
where the similarity between rules only considers
child subsequences of length one.
4 Evaluation
We applied DPK to two published temporal relation
extraction systems: (Miller et al, 2013) in the
clinical domain and Cleartk-TimeML (Bethard,
2013) in the general domain respectively.
4.1 Narrative Container Discovery
The task here as described by Miller et al (2013) is
to identify the CONTAINS relation between a time
expression and a same-sentence event from clinical
notes in the THYME corpus, which has 78 notes
of 26 patients. We obtained this corpus from the
authors and followed their linear composite kernel
setting:
K
C
(s
1
, s
2
) = ?
P
?
p=1
K
T
(t
p
1
, t
p
2
)+K
F
(f
1
, f
2
) (2)
where s
i
is an instance object composed of flat fea-
tures f
i
and a syntactic tree t
i
. A syntactic tree t
i
84
can have multiple representations, as in Bag Tree
(BT), Path-enclosed Tree (PET), and Path Tree
(PT). For the tree kernel K
T
, subset tree (SST) ker-
nel was applied on each tree representation p. The
final similarity score between two instances is the
? -weighted sum of the similarities of all representa-
tions, combined with the flat feature (FF) similarity
as measured by a feature kernel K
F
(linear or poly-
nomial). Here we replaced the SST kernel with
DPK and tested two feature combinations FF+PET
and FF+BT+PET+PT. To fine tune parameters, we
used grid search by testing on the default develop-
ment data. Once the parameters were tuned, we
tested the system performance on the testing data,
which was set up by the original system split.
4.2 Cleartk-TimeML
We tested one sub-task from TempEval-2013 ?
the extraction of temporal relations between an
event and time expression within the same sen-
tence. We obtained the training corpus (Time-
Bank + AQUAINT) and testing data from the au-
thors (Bethard, 2013). Since the original features
didn?t contain syntactic features, we created a PET
tree extractor for this system. The kernel setting
was similar to equation (2), while there was only
one tree representation, PET tree, P=1. A linear
kernel was used as K
F
to evaluate the exact same
flat features as used by the original system. We
used the built-in cross validation to do grid search
for tuning the parameters. The final system was
tested on the testing data for reporting results.
4.3 Results and Discussion
Results are shown in Table 2. The top section
shows THYME results. For these experiments,
the DPK is superior when a syntactically-rich PET
representation is used. Using the full feature set of
Miller et al (2013), SST is superior to DPK and
obtains the best overall performance. The bottom
section shows results on TempEval-2013 data, for
which there is little benefit from either tree kernel.
Our experiments with THYME data show that
DPK can capture something in the linguistically
richer PET representation that the SST kernel can-
not, but adding BT and PT representations decrease
the DPK performance. As a shallow representation,
BT does not have much in the way of descending
paths for DPK to use. PT already ignores the pro-
duction grammar by removing the inner tree nodes.
DPK therefore cannot get useful information and
may even get misleading cues from these two rep-
Features K
T
P R F
THYME
FF+PET DPK 0.756 0.667 0.708
SST 0.698 0.630 0.662
FF+BT+ DPK 0.759 0.626 0.686
PET+PT SST 0.754 0.711 0.732
TempEval
FF+PET DPK 0.328 0.263 0.292
SST 0.325 0.263 0.290
FF - 0.309 0.266 0.286
Table 2: Comparison of tree kernel performance
for temporal relation extraction on THYME and
TempEval-2013 data.
resentations. These results show that, while DPK
should not always replace SST, there are represen-
tations in which it is superior to existing methods.
This suggests an approach in which tree representa-
tions are matched to different convolution kernels,
for example by tuning on held-out data.
For TempEval-2013 data, adding syntactic fea-
tures did not improve the performance significantly
(comparing F-score of 0.290 with 0.286 in Ta-
ble 3). Probably, syntactic information is not a
strong feature for all types of temporal relations on
TempEval-2013 data.
5 Conclusion
In this paper, we developed a novel convolution
tree kernel (DPK) for measuring syntactic similar-
ity. This kernel uses a descending path represen-
tation in trees to allow higher similarity scores on
partially matching structures, while being simpler
and faster than other methods for doing the same.
Future work will explore 1) a composite kernel
which uses DPK for PET trees, SST for BT and PT,
and feature kernel for flat features, so that different
tree kernels can work with their ideal syntactic rep-
resentations; 2) incorporate dependency structures
for tree kernel analysis 3) applying DPK to other
relation extraction tasks on various corpora.
6 Acknowledgements
Thanks to Sean Finan for technically supporting the
experiments. The project described was supported
by R01LM010090 (THYME) from the National
Library Of Medicine.
85
References
Fabio Aiolli, Giovanni Da San Martino, and Alessan-
dro Sperduti. 2009. Route kernels for trees. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 17?24. ACM.
Steven Bethard and James H Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
129?132. Association for Computational Linguis-
tics.
Steven Bethard. 2013. Cleartk-timeml: A minimalist
approach to TempEval 2013. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), volume 2, pages 10?14.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73?77, Atlanta, Georgia, USA, June. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Neural Information
Processing Systems.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia in Santa Cruz.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Pat-
wardhan, and Chris Welty. 2012. When did that
happen?: linking events and relations to timestamps.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 185?193. Association for Compu-
tational Linguistics.
Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima,
Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda.
2007. A spectrum tree kernel. Information and Me-
dia Technologies, 2(1):292?299.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. Tipsem (english and spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284?291. Association for
Computational Linguistics.
Timothy Miller, Steven Bethard, Dmitriy Dligach,
Sameer Pradhan, Chen Lin, and Guergana Savova.
2013. Discovering temporal narrative containers
in clinical text. In Proceedings of the 2013 Work-
shop on Biomedical Natural Language Processing,
pages 18?26, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for
classifying temporal relations between events. Proc.
of the PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The TimeBank corpus. In Cor-
pus linguistics, volume 2003, page 40.
Geoffrey Sampson. 2000. A proposal for improving
the measurement of parse accuracy. International
Journal of Corpus Linguistics, 5(1):53?68.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA machine reading
program-encouraging linguistic and reasoning re-
search with a series of reading tasks. In LREC.
William Styler, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet de Groen, Brad Er-
ickson, Timothy Miller, Lin Chen, Guergana K.
Savova, and James Pustejovsky. 2014. Temporal
annotations in the clinical domain. Transactions
of the Association for Computational Linguistics,
2(2):143?154.
Min Zhang, Wanxiang Che, Ai Ti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for seman-
tic role classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 200?207.
86
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 63?68,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 14: Word Sense Induction & Disambiguation
Suresh Manandhar
Department of Computer Science
University of York, UK
Ioannis P. Klapaftis
Department of Computer Science
University of York, UK
Dmitriy Dligach
Department of Computer Science
University of Colorado, USA
Sameer S. Pradhan
BBN Technologies
Cambridge, USA
Abstract
This paper presents the description and
evaluation framework of SemEval-2010
Word Sense Induction & Disambiguation
task, as well as the evaluation results of 26
participating systems. In this task, partici-
pants were required to induce the senses of
100 target words using a training set, and
then disambiguate unseen instances of the
same words using the induced senses. Sys-
tems? answers were evaluated in: (1) an
unsupervised manner by using two clus-
tering evaluation measures, and (2) a su-
pervised manner in a WSD task.
1 Introduction
Word senses are more beneficial than simple word
forms for a variety of tasks including Information
Retrieval, Machine Translation and others (Pantel
and Lin, 2002). However, word senses are usually
represented as a fixed-list of definitions of a manu-
ally constructed lexical database. Several deficien-
cies are caused by this representation, e.g. lexical
databases miss main domain-specific senses (Pan-
tel and Lin, 2002), they often contain general defi-
nitions and suffer from the lack of explicit seman-
tic or contextual links between concepts (Agirre
et al, 2001). More importantly, the definitions of
hand-crafted lexical databases often do not reflect
the exact meaning of a target word in a given con-
text (V?eronis, 2004).
Unsupervised Word Sense Induction (WSI)
aims to overcome these limitations of hand-
constructed lexicons by learning the senses of a
target word directly from text without relying on
any hand-crafted resources. The primary aim of
SemEval-2010 WSI task is to allow comparison
of unsupervised word sense induction and disam-
biguation systems.
The target word dataset consists of 100 words,
50 nouns and 50 verbs. For each target word, par-
ticipants were provided with a training set in or-
der to learn the senses of that word. In the next
step, participating systems were asked to disam-
biguate unseen instances of the same words using
their learned senses. The answers of the systems
were then sent to organisers for evaluation.
2 Task description
Figure 1 provides an overview of the task. As
can be observed, the task consisted of three
separate phases. In the first phase, train-
ing phase, participating systems were provided
with a training dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to use this training dataset to induce the senses
of the target word. No other resources were al-
lowed with the exception of NLP components for
morphology and syntax. In the second phase,
testing phase, participating systems were pro-
vided with a testing dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to tag (disambiguate) each testing instance with
the senses induced during the training phase. In
the third and final phase, the tagged test instances
were received by the organisers in order to evalu-
ate the answers of the systems in a supervised and
an unsupervised framework. Table 1 shows the to-
tal number of target word instances in the training
and testing set, as well as the average number of
senses in the gold standard.
The main difference of the SemEval-2010 as
compared to the SemEval-2007 sense induction
task is that the training and testing data are treated
separately, i.e the testing data are only used for
sense tagging, while the training data are only used
63
Figure 1: Training, testing and evaluation phases of SemEval-2010 Task 14
Training set Testing set Senses (#)
All 879807 8915 3.79
Nouns 716945 5285 4.46
Verbs 162862 3630 3.12
Table 1: Training & testing set details
for sense induction. Treating the testing data as
new unseen instances ensures a realistic evalua-
tion that allows to evaluate the clustering models
of each participating system.
The evaluation framework of SemEval-2010
WSI task considered two types of evaluation.
In the first one, unsupervised evaluation, sys-
tems? answers were evaluated according to: (1) V-
Measure (Rosenberg and Hirschberg, 2007), and
(2) paired F-Score (Artiles et al, 2009). Nei-
ther of these measures were used in the SemEval-
2007 WSI task. Manandhar & Klapaftis (2009)
provide more details on the choice of this evalu-
ation setting and its differences with the previous
evaluation. The second type of evaluation, super-
vised evaluation, follows the supervised evalua-
tion of the SemEval-2007 WSI task (Agirre and
Soroa, 2007). In this evaluation, induced senses
are mapped to gold standard senses using a map-
ping corpus, and systems are then evaluated in a
standard WSD task.
2.1 Training dataset
The target word dataset consisted of 100 words,
i.e. 50 nouns and 50 verbs. The training dataset
for each target noun or verb was created by follow-
ing a web-based semi-automatic method, similar
to the method for the construction of Topic Signa-
tures (Agirre et al, 2001). Specifically, for each
WordNet (Fellbaum, 1998) sense of a target word,
we created a query of the following form:
<Target Word> AND <Relative Set>
The <Target Word> consisted of the target
word stem. The <Relative Set> consisted of a
disjunctive set of word lemmas that were related
Word Query
Sense
Sense 1 failure AND (loss OR nonconformity OR test
OR surrender OR ?force play? OR ...)
Sense 2 failure AND (ruination OR flop OR bust
OR stall OR ruin OR walloping OR ...)
Table 2: Training set creation: example queries for
target word failure
to the target word sense for which the query was
created. The relations considered were WordNet?s
hypernyms, hyponyms, synonyms, meronyms and
holonyms. Each query was manually checked by
one of the organisers to remove ambiguous words.
The following example shows the query created
for the first
1
and second
2
WordNet sense of the
target noun failure.
The created queries were issued to Yahoo!
search API
3
and for each query a maximum of
1000 pages were downloaded. For each page we
extracted fragments of text that occurred in <p>
</p> html tags and contained the target word
stem. In the final stage, each extracted fragment of
text was POS-tagged using the Genia tagger (Tsu-
ruoka and Tsujii, 2005) and was only retained, if
the POS of the target word in the extracted text
matched the POS of the target word in our dataset.
2.2 Testing dataset
The testing dataset consisted of instances of the
same target words from the training dataset. This
dataset is part of OntoNotes (Hovy et al, 2006).
We used the sense-tagged dataset in which sen-
tences containing target word instances are tagged
with OntoNotes (Hovy et al, 2006) senses. The
texts come from various news sources including
CNN, ABC and others.
1
An act that fails
2
An event that does not accomplish its intended purpose
3
http://developer.yahoo.com/search/ [Access:10/04/2010]
64
G1
G
2
G
3
C
1
10 10 15
C
2
20 50 0
C
3
1 10 60
C
4
5 0 0
Table 3: Clusters & GS senses matrix.
3 Evaluation framework
For the purposes of this section we provide an ex-
ample (Table 3) in which a target word has 181
instances and 3 GS senses. A system has gener-
ated a clustering solution with 4 clusters covering
all instances. Table 3 shows the number of com-
mon instances between clusters and GS senses.
3.1 Unsupervised evaluation
This section presents the measures of unsuper-
vised evaluation, i.e V-Measure (Rosenberg and
Hirschberg, 2007) and (2) paired F-Score (Artiles
et al, 2009).
3.1.1 V-Measure evaluation
Let w be a target word with N instances (data
points) in the testing dataset. Let K = {C
j
|j =
1 . . . n} be a set of automatically generated clus-
ters grouping these instances, and S = {G
i
|i =
1 . . .m} the set of gold standard classes contain-
ing the desirable groupings of w instances.
V-Measure (Rosenberg and Hirschberg, 2007)
assesses the quality of a clustering solution by ex-
plicitly measuring its homogeneity and its com-
pleteness. Homogeneity refers to the degree that
each cluster consists of data points primarily be-
longing to a single GS class, while completeness
refers to the degree that each GS class consists of
data points primarily assigned to a single cluster
(Rosenberg and Hirschberg, 2007). Let h be ho-
mogeneity and c completeness. V-Measure is the
harmonic mean of h and c, i.e. VM =
2?h?c
h+c
.
Homogeneity. The homogeneity, h, of a clus-
tering solution is defined in Formula 1, where
H(S|K) is the conditional entropy of the class
distribution given the proposed clustering and
H(S) is the class entropy.
h =
{
1, if H(S) = 0
1?
H(S|K)
H(S)
, otherwise
(1)
H(S) = ?
|S|
?
i=1
?
|K|
j=1
a
ij
N
log
?
|K|
j=1
a
ij
N
(2)
H(S|K) = ?
|K|
?
j=1
|S|
?
i=1
a
ij
N
log
a
ij
?
|S|
k=1
a
kj
(3)
When H(S|K) is 0, the solution is perfectly
homogeneous, because each cluster only contains
data points that belong to a single class. How-
ever in an imperfect situation, H(S|K) depends
on the size of the dataset and the distribution of
class sizes. Hence, instead of taking the raw con-
ditional entropy, V-Measure normalises it by the
maximum reduction in entropy the clustering in-
formation could provide, i.e. H(S). When there
is only a single class (H(S) = 0), any clustering
would produce a perfectly homogeneous solution.
Completeness. Symmetrically to homogeneity,
the completeness, c, of a clustering solution is de-
fined in Formula 4, where H(K|S) is the condi-
tional entropy of the cluster distribution given the
class distribution and H(K) is the clustering en-
tropy. When H(K|S) is 0, the solution is perfectly
complete, because all data points of a class belong
to the same cluster.
For the clustering example in Table 3, homo-
geneity is equal to 0.404, completeness is equal to
0.37 and V-Measure is equal to 0.386.
c =
{
1, if H(K) = 0
1?
H(K|S)
H(K)
, otherwise
(4)
H(K) = ?
|K|
?
j=1
?
|S|
i=1
a
ij
N
log
?
|S|
i=1
a
ij
N
(5)
H(K|S) = ?
|S|
?
i=1
|K|
?
j=1
a
ij
N
log
a
ij
?
|K|
k=1
a
ik
(6)
3.1.2 Paired F-Score evaluation
In this evaluation, the clustering problem is trans-
formed into a classification problem. For each
cluster C
i
we generate
(
|C
i
|
2
)
instance pairs, where
|C
i
| is the total number of instances that belong to
cluster C
i
. Similarly, for each GS class G
i
we gen-
erate
(
|G
i
|
2
)
instance pairs, where |G
i
| is the total
number of instances that belong to GS class G
i
.
Let F (K) be the set of instance pairs that ex-
ist in the automatically induced clusters and F (S)
be the set of instance pairs that exist in the gold
standard. Precision can be defined as the number
of common instance pairs between the two sets to
the total number of pairs in the clustering solu-
tion (Equation 7), while recall can be defined as
the number of common instance pairs between the
two sets to the total number of pairs in the gold
65
standard (Equation 8). Finally, precision and re-
call are combined to produce the harmonic mean
(FS =
2?P ?R
P+R
).
P =
|F (K) ? F (S)|
|F (K)|
(7)
R =
|F (K) ? F (S)|
|F (S)|
(8)
For example in Table 3, we can generate
(
35
2
)
in-
stance pairs for C
1
,
(
70
2
)
for C
2
,
(
71
2
)
for C
3
and
(
5
2
)
for C
4
, resulting in a total of 5505 instance
pairs. In the same vein, we can generate
(
36
2
)
in-
stance pairs for G
1
,
(
70
2
)
for G
2
and
(
75
2
)
for G
3
. In
total, the GS classes contain 5820 instance pairs.
There are 3435 common instance pairs, hence pre-
cision is equal to 62.39%, recall is equal to 59.09%
and paired F-Score is equal to 60.69%.
3.2 Supervised evaluation
In this evaluation, the testing dataset is split into a
mapping and an evaluation corpus. The first one
is used to map the automatically induced clusters
to GS senses, while the second is used to evaluate
methods in a WSD setting. This evaluation fol-
lows the supervised evaluation of SemEval-2007
WSI task (Agirre and Soroa, 2007), with the dif-
ference that the reported results are an average
of 5 random splits. This repeated random sam-
pling was performed to avoid the problems of the
SemEval-2007 WSI challenge, in which different
splits were providing different system rankings.
Let us consider the example in Table 3 and as-
sume that this matrix has been created by using the
mapping corpus. Table 3 shows that C
1
is more
likely to be associated with G
3
, C
2
is more likely
to be associated with G
2
, C
3
is more likely to be
associated with G
3
and C
4
is more likely to be as-
sociated with G
1
. This information can be utilised
to map the clusters to GS senses.
Particularly, the matrix shown in Table 3 is nor-
malised to produce a matrix M , in which each
entry depicts the estimated conditional probabil-
ity P (G
i
|C
j
). Given an instance I of tw from
the evaluation corpus, a row cluster vector IC is
created, in which each entry k corresponds to the
score assigned to C
k
to be the winning cluster for
instance I . The product of IC and M provides a
row sense vector, IG, in which the highest scor-
ing entry a denotes that G
a
is the winning sense.
For example, if we produce the row cluster vector
[C
1
= 0.8, C
2
= 0.1, C
3
= 0.1, C
4
= 0.0], and
System VM (%) VM (%) VM (%) #Cl
(All) (Nouns) (Verbs)
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
KSU KDD 15.7 18 12.4 17.5
Duluth-WSI 9 11.4 5.7 4.15
Duluth-WSI-SVD 9 11.4 5.7 4.15
Duluth-R-110 8.6 8.6 8.5 9.71
Duluth-WSI-Co 7.9 9.2 6 2.49
KCDC-PCGD 7.8 7.3 8.4 2.9
KCDC-PC 7.5 7.7 7.3 2.92
KCDC-PC-2 7.1 7.7 6.1 2.93
Duluth-Mix-Narrow-Gap 6.9 8 5.1 2.42
KCDC-GD-2 6.9 6.1 8 2.82
KCDC-GD 6.9 5.9 8.5 2.78
Duluth-Mix-Narrow-PK2 6.8 7.8 5.5 2.68
Duluth-MIX-PK2 5.6 5.8 5.2 2.66
Duluth-R-15 5.3 5.4 5.1 4.97
Duluth-WSI-Co-Gap 4.8 5.6 3.6 1.6
Random 4.4 4.2 4.6 4
Duluth-R-13 3.6 3.5 3.7 3
Duluth-WSI-Gap 3.1 4.2 1.5 1.4
Duluth-Mix-Gap 3 2.9 3 1.61
Duluth-Mix-Uni-PK2 2.4 0.8 4.7 2.04
Duluth-R-12 2.3 2.2 2.5 2
KCDC-PT 1.9 1 3.1 1.5
Duluth-Mix-Uni-Gap 1.4 0.2 3 1.39
KCDC-GDC 7 6.2 7.8 2.83
MFS 0 0 0 1
Duluth-WSI-SVD-Gap 0 0 0.1 1.02
Table 4: V-Measure unsupervised evaluation
multiply it with the normalised matrix of Table 3,
then we would get a row sense vector in which G
3
would be the winning sense with a score equal to
0.43.
4 Evaluation results
In this section, we present the results of the 26
systems along with two baselines. The first base-
line, Most Frequent Sense (MFS), groups all test-
ing instances of a target word into one cluster. The
second baseline, Random, randomly assigns an in-
stance to one out of four clusters. The number
of clusters of Random was chosen to be roughly
equal to the average number of senses in the GS.
This baseline is executed five times and the results
are averaged.
4.1 Unsupervised evaluation
Table 4 shows the V-Measure (VM) performance
of the 26 systems participating in the task. The last
column shows the number of induced clusters of
each system in the test set.The MFS baseline has a
V-Measure equal to 0, since by definition its com-
pleteness is 1 and homogeneity is 0. All systems
outperform this baseline, apart from one, whose
V-Measure is equal to 0. Regarding the Random
baseline, we observe that 17 perform better, which
indicates that they have learned useful information
better than chance.
Table 4 also shows that V-Measure tends to
favour systems producing a higher number of clus-
66
System FS (%) FS (%) FS (%) #Cl
(All) (Nouns) (Verbs)
MFS 63.5 57.0 72.7 1
Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02
KCDC-PT 61.8 56.4 69.7 1.5
KCDC-GD 59.2 51.6 70.0 2.78
Duluth-Mix-Gap 59.1 54.5 65.8 1.61
Duluth-Mix-Uni-Gap 58.7 57.0 61.2 1.39
KCDC-GD-2 58.2 50.4 69.3 2.82
KCDC-GDC 57.3 48.5 70.0 2.83
Duluth-Mix-Uni-PK2 56.6 57.1 55.9 2.04
KCDC-PC 55.5 50.4 62.9 2.92
KCDC-PC-2 54.7 49.7 61.7 2.93
Duluth-WSI-Gap 53.7 53.4 53.9 1.4
KCDC-PCGD 53.3 44.8 65.6 2.9
Duluth-WSI-Co-Gap 52.6 53.3 51.5 1.6
Duluth-MIX-PK2 50.4 51.7 48.3 2.66
UoY 49.8 38.2 66.6 11.54
Duluth-Mix-Narrow-Gap 49.7 47.4 51.3 2.42
Duluth-WSI-Co 49.5 50.2 48.2 2.49
Duluth-Mix-Narrow-PK2 47.8 37.1 48.2 2.68
Duluth-R-12 47.8 44.3 52.6 2
Duluth-WSI-SVD 41.1 37.1 46.7 4.15
Duluth-WSI 41.1 37.1 46.7 4.15
Duluth-R-13 38.4 36.2 41.5 3
KSU KDD 36.9 24.6 54.7 17.5
Random 31.9 30.4 34.1 4
Duluth-R-15 27.6 26.7 28.9 4.97
Hermit 26.7 24.4 30.1 10.78
Duluth-R-110 16.1 15.8 16.4 9.71
Table 5: Paired F-Score unsupervised evaluation
ters than the number of GS senses, although V-
Measure does not increase monotonically with the
number of clusters increasing. For that reason,
we introduced the second unsupervised evaluation
measure (paired F-Score) that penalises systems
when they produce: (1) a higher number of clus-
ters (low recall) or (2) a lower number of clusters
(low precision), than the GS number of senses.
Table 5 shows the performance of systems us-
ing the second unsupervised evaluation measure.
In this evaluation, we observe that most of the sys-
tems perform better than Random. Despite that,
none of the systems outperform the MFS baseline.
It seems that systems generating a smaller number
of clusters than the GS number of senses are bi-
ased towards the MFS, hence they are not able to
perform better. On the other hand, systems gen-
erating a higher number of clusters are penalised
by this measure. Systems generating a number of
clusters roughly the same as the GS tend to con-
flate the GS senses lot more than the MFS.
4.2 Supervised evaluation results
Table 6 shows the results of this evaluation for a
80-20 test set split, i.e. 80% for mapping and 20%
for evaluation. The last columns shows the aver-
age number of GS senses identified by each sys-
tem in the five splits of the evaluation datasets.
Overall, 14 systems outperform the MFS, while 17
of them perform better than Random. The ranking
of systems in nouns and verbs is different. For in-
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.4 59.4 66.8 1.51
Duluth-WSI 60.5 54.7 68.9 1.66
Duluth-WSI-SVD 60.5 54.7 68.9 1.66
Duluth-WSI-Co-Gap 60.3 54.1 68.6 1.19
Duluth-WSI-Co 60.8 54.7 67.6 1.51
Duluth-WSI-Gap 59.8 54.4 67.8 1.11
KCDC-PC-2 59.8 54.1 68.0 1.21
KCDC-PC 59.7 54.6 67.3 1.39
KCDC-PCGD 59.5 53.3 68.6 1.47
KCDC-GDC 59.1 53.4 67.4 1.34
KCDC-GD 59.0 53.0 67.9 1.33
KCDC-PT 58.9 53.1 67.4 1.08
KCDC-GD-2 58.7 52.8 67.4 1.33
Duluth-WSI-SVD-Gap 58.7 53.2 66.7 1.01
MFS 58.7 53.2 66.6 1
Duluth-R-12 58.5 53.1 66.4 1.25
Hermit 58.3 53.6 65.3 2.06
Duluth-R-13 58.0 52.3 66.4 1.46
Random 57.3 51.5 65.7 1.53
Duluth-R-15 56.8 50.9 65.3 1.61
Duluth-Mix-Narrow-Gap 56.6 48.1 69.1 1.43
Duluth-Mix-Narrow-PK2 56.1 47.5 68.7 1.41
Duluth-R-110 54.8 48.3 64.2 1.94
KSU KDD 52.2 46.6 60.3 1.69
Duluth-MIX-PK2 51.6 41.1 67.0 1.23
Duluth-Mix-Gap 50.6 40.0 66.0 1.01
Duluth-Mix-Uni-PK2 19.3 1.8 44.8 0.62
Duluth-Mix-Uni-Gap 18.7 1.6 43.8 0.56
Table 6: Supervised recall (SR) (test set split:80%
mapping, 20% evaluation)
stance, the highest ranked system in nouns is UoY,
while in verbs Duluth-Mix-Narrow-Gap. It seems
that depending on the part-of-speech of the target
word, different algorithms, features and parame-
ters? tuning have different impact.
The supervised evaluation changes the distri-
bution of clusters by mapping each cluster to a
weighted vector of senses. Hence, it can poten-
tially favour systems generating a high number of
homogeneous clusters. For that reason, we applied
a second testing set split, where 60% of the testing
corpus was used for mapping and 40% for eval-
uation. Reducing the size of the mapping corpus
allows us to observe, whether the above statement
is correct, since systems with a high number of
clusters would suffer from unreliable mapping.
Table 7 shows the results of the second super-
vised evaluation. The ranking of participants did
not change significantly, i.e. we observe only dif-
ferent rankings among systems belonging to the
same participant. Despite that, Table 7 also shows
that the reduction of the mapping corpus has a dif-
ferent impact on systems generating a larger num-
ber of clusters than the GS number of senses.
For instance, UoY that generates 11.54 clusters
outperformed the MFS by 3.77% in the 80-20 split
and by 3.71% in the 60-40 split. The reduction of
the mapping corpus had a minimal impact on its
performance. In contrast, KSU KDD that gener-
ates 17.5 clusters was below the MFS by 6.49%
67
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.0 58.6 66.8 1.66
Duluth-WSI-Co 60.1 54.6 68.1 1.56
Duluth-WSI-Co-Gap 59.5 53.5 68.3 1.2
Duluth-WSI-SVD 59.5 53.5 68.3 1.73
Duluth-WSI 59.5 53.5 68.3 1.73
Duluth-WSI-Gap 59.3 53.2 68.2 1.11
KCDC-PCGD 59.1 52.6 68.6 1.54
KCDC-PC-2 58.9 53.4 67.0 1.25
KCDC-PC 58.9 53.6 66.6 1.44
KCDC-GDC 58.3 52.1 67.3 1.41
KCDC-GD 58.3 51.9 67.6 1.42
MFS 58.3 52.5 66.7 1
KCDC-PT 58.3 52.2 67.1 1.11
Duluth-WSI-SVD-Gap 58.2 52.5 66.7 1.01
KCDC-GD-2 57.9 51.7 67.0 1.44
Duluth-R-12 57.7 51.7 66.4 1.27
Duluth-R-13 57.6 51.1 67.0 1.48
Hermit 57.3 52.5 64.2 2.27
Duluth-R-15 56.5 50.0 66.1 1.76
Random 56.5 50.2 65.7 1.65
Duluth-Mix-Narrow-Gap 56.2 47.7 68.6 1.51
Duluth-Mix-Narrow-PK2 55.7 46.9 68.5 1.51
Duluth-R-110 53.6 46.7 63.6 2.18
Duluth-MIX-PK2 50.5 39.7 66.1 1.31
KSU KDD 50.4 44.3 59.4 1.92
Duluth-Mix-Gap 49.8 38.9 65.6 1.04
Duluth-Mix-Uni-PK2 19.1 1.8 44.4 0.63
Duluth-Mix-Uni-Gap 18.9 1.5 44.2 0.56
Table 7: Supervised recall (SR) (test set split:60%
mapping, 40% evaluation)
in the 80-20 split and by 7.83% in the 60-40 split.
The reduction of the mapping corpus had a larger
impact in this case. This result indicates that the
performance in this evaluation also depends on the
distribution of instances within the clusters. Sys-
tems generating a skewed distribution, in which a
small number of homogeneous clusters tag the ma-
jority of instances and a larger number of clusters
tag only a few instances, are likely to have a bet-
ter performance than systems that produce a more
uniform distribution.
5 Conclusion
We presented the description, evaluation frame-
work and assessment of systems participating in
the SemEval-2010 sense induction task. The eval-
uation has shown that the current state-of-the-art
lacks unbiased measures that objectively evaluate
clustering.
The results of systems have shown that their
performance in the unsupervised and supervised
evaluation settings depends on cluster granularity
along with the distribution of instances within the
clusters. Our future work will focus on the assess-
ment of sense induction on a task-oriented basis as
well as on clustering evaluation.
Acknowledgements
We gratefully acknowledge the support of the EU
FP7 INDECT project, Grant No. 218086, the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating Word Sense Induction and Dis-
crimination Systems. In Proceedings of SemEval-
2007, pages 7?12, Prague, Czech Republic. ACL.
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching Wordnet Concepts With
Topic Signatures. ArXiv Computer Science e-prints.
Javier Artiles, Enrique Amig?o, and Julio Gonzalo.
2009. The role of named entities in web people
search. In Proceedings of EMNLP, pages 534?542.
ACL.
Christiane Fellbaum. 1998. Wordnet: An Electronic
Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL, Com-
panion Volume: Short Papers on XX, pages 57?60.
ACL.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.
Semeval-2010 Task 14: Evaluation Setting for Word
Sense Induction & Disambiguation Systems. In
DEW ?09: Proceedings of the Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, pages 117?122, Boulder, Colorado,
USA. ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In KDD ?02: Proceedings
of the 8th ACM SIGKDD Conference, pages 613?
619, New York, NY, USA. ACM.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A Conditional Entropy-based External
Cluster Evaluation Measure. In Proceedings of the
2007 EMNLP-CoNLL Joint Conference, pages 410?
420, Prague, Czech Republic.
Yoshimasa Tsuruoka and Jun??chi Tsujii. 2005. Bidi-
rectional Inference With the Easiest-first Strategy
for Tagging Sequence Data. In Proceedings of
the HLT-EMNLP Joint Conference, pages 467?474,
Morristown, NJ, USA.
Jean V?eronis. 2004. Hyperlex: Lexical Cartography
for Information Retrieval. Computer Speech & Lan-
guage, 18(3):223?252.
68
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 64?72,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
To Annotate More Accurately or to Annotate More
Dmitriy Dligach
Department of Computer Science
University of Colorado at Boulder
Dmitriy.Dligach@colorado.edu
Rodney D. Nielsen
The Center for Computational Language
and Education Research
University of Colorado at Boulder
Rodney.Nielsen@colorado.edu
Martha Palmer
Department of Linguistics
Department of Computer Science
University of Colorado at Boulder
Martha.Palmer@colorado.edu
Abstract
The common accepted wisdom is that
blind double annotation followed by adju-
dication of disagreements is necessary to
create training and test corpora that result
in the best possible performance. We pro-
vide evidence that this is unlikely to be the
case. Rather, the greatest value for your
annotation dollar lies in single annotating
more data.
1 Introduction
In recent years, supervised learning has become
the dominant paradigm in Natural Language Pro-
cessing (NLP), thus making the creation of hand-
annotated corpora a critically important task. A
corpus where each instance is annotated by a sin-
gle tagger unavoidably contains errors. To im-
prove the quality of the data, an annotation project
may choose to annotate each instance twice and
adjudicate the disagreements, thus producing the
(largely) error-free gold standard. For example,
OntoNotes (Hovy et al, 2006), a large-scale an-
notation project, chose this option.
However, given a virtually unlimited supply of
unlabeled data and limited funding ? a typical set
of constraints in NLP ? an annotation project must
always face the realization that for the cost of dou-
ble annotation, more than twice as much data can
be single annotated. The philosophy behind this
alternative says that modern machine learning al-
gorithms can still generalize well in the presence
of noise, especially when given larger amounts of
training data.
Currently, the commonly accepted wisdom
sides with the view that says that blind double
annotation followed by adjudication of disagree-
ments is necessary to create annotated corpora that
leads to the best possible performance. We pro-
vide empirical evidence that this is unlikely to be
the case. Rather, the greatest value for your an-
notation dollar lies in single annotating more data.
There may, however, be other considerations that
still argue in favor of double annotation.
In this paper, we also consider the arguments of
Beigman and Klebanov (2009), who suggest that
data should be multiply annotated and then filtered
to discard all of the examples where the annota-
tors do not have perfect agreement. We provide
evidence that single annotating more data for the
same cost is likely to result in better system per-
formance.
This paper proceeds as follows: first, we out-
line our evaluation framework in Section 2. Next,
we compare the single annotation and adjudica-
tion scenarios in Section 3. Then, we compare
the annotation scenario of Beigman and Klebanov
(2009) with the single annotation scenario in Sec-
tion 4. After that, we discuss the results and future
work in section 5. Finally, we draw the conclusion
in Section 6.
2 Evaluation
2.1 Data
For evaluation we utilize the word sense data an-
notated by the OntoNotes project. The OntoNotes
data was chosen because it utilizes full double-
blind annotation by human annotators and the dis-
agreements are adjudicated by a third (more expe-
64
rienced) annotator. This allows us to
? Evaluate single annotation results by using
the labels assigned by the first tagger
? Evaluate double annotation results by using
the labels assigned by the second tagger
? Evaluate adjudication results by using the la-
bels assigned by the the adjudicator to the in-
stances where the two annotators disagreed
? Measure the performance under various sce-
narios against the double annotated and adju-
dicated gold standard data
We selected the 215 most frequent verbs in the
OntoNotes data. To make the size of the dataset
more manageable, we randomly selected 500 ex-
amples of each of the 15 most frequent verbs. For
the remaining 200 verbs, we utilized all the an-
notated examples. The resulting dataset contained
66,228 instances of the 215 most frequent verbs.
Table 1 shows various important characteristics of
this dataset averaged across the 215 verbs.
Inter-tagger agreement 86%
Annotator1-gold standard agreement 93%
Share of the most frequent sense 70%
Number of classes (senses) per verb 4.74
Table 1: Data used in evaluation at a glance
2.2 Cost of Annotation
Because for this set of experiments we care pri-
marily about the cost effectiveness of the annota-
tion dollars, we need to know how much it costs
to blind annotate instances and how much it costs
to adjudicate disagreements in instances. There is
an upfront cost associated with any annotation ef-
fort to organize the project, design an annotation
scheme, set up the environment, create annotation
guidelines, hire and train the annotators, etc. We
will assume, for the sake of this paper, that this
cost is fixed and is the same regardless of whether
the data is single annotated or the data is double
annotated and disagreements adjudicated.
In this paper, we focus on a scenario where there
is essentially no difference in cost to collect ad-
ditional data to be annotated, as is often the case
(e.g., there is virtually no additional cost to down-
load 2.5 versus 1.0 million words of text from the
web). However, this is not always the case (e.g.,
collecting speech can be costly).
To calculate a cost per annotated instance for
blind annotation, we take the total expenses asso-
ciated with the annotators in this group less train-
ing costs and any costs not directly associated with
annotation and divide by the total number of blind
instance annotations. This value, $0.0833, is the
per instance cost used for single annotation. We
calculated the cost for adjudicating instances sim-
ilarly, based on the expenses associated with the
adjudication group. The adjudication cost is an ad-
ditional $0.1000 per instance adjudicated. The per
instance cost for double blind, adjudicated data is
then computed as double the cost for single an-
notation plus the per instance cost of adjudication
multiplied by the percent of disagreement, 14%,
which is $0.1805.
We leave an analysis of the extent to which the
up front costs are truly fixed and whether they can
be altered to result in more value for the dollar to
future work.
2.3 Automatic Word Sense Disambiguation
For the experiments we conduct in this study, we
needed a word sense disambiguation (WSD) sys-
tem. Our WSD system is modeled after the state-
of-the-art verb WSD system described in (Dligach
and Palmer, 2008). We will briefly outline it here.
We view WSD as a supervised learning prob-
lem. Each instance of the target verb is represented
as a vector of binary features that indicate the pres-
ence (or absence) of the corresponding features in
the neighborhood of the target verb. We utilize
all of the linguistic features that were shown to be
useful for disambiguating verb senses in (Chen et
al., 2007).
To extract the lexical features we POS-tag
the sentence containing the target verb and the
two surrounding sentences using MXPost soft-
ware (Ratnaparkhi, 1998). All open class words
(nouns, verbs, adjectives, and adverbs) in these
sentences are included in our feature set. In addi-
tion to that, we use as features two words on each
side of the target verb as well as their POS tags.
To extract the syntactic features we parse the
sentence containing the target verb with Bikel?s
constituency parser and utilize a set of rules to
identify the features in Table 2.
Our semantic features represent the semantic
classes of the target verb?s syntactic arguments
65
Feature Explanation
Subject and object - Presence of subject and
object
- Head word of subject
and object NPs
- POS tag of the head
word of subject and
object NPs
Voice - Passive or Active
PP adjunct - Presence of PP adjunct
- Preposition word
- Head word of the
preposition?s NP
argument
Subordinate clause - Presence of subordinate
clause
Path - Parse tree path from
target verb to neighboring
words
- Parse tree path from
target verb to subject and
object
- Parse tree path from
target verb to subordinate
clause
Subcat frame - Phrase structure rule
expanding the target
verb?s parent node in
parse tree
Table 2: Syntactic features
such as subject and object. The semantic classes
are approximated as
? WordNet (Fellbaum, 1998) hypernyms
? NE tags derived from the output of Identi-
Finder (Bikel et al, 1999)
? Dynamic dependency neighbors (Dligach
and Palmer, 2008), which are extracted in an
unsupervised way from a dependency-parsed
corpus
Our WSD system uses the Libsvm software
package (Chang and Lin, 2001) for classification.
We accepted the default options (C = 1 and lin-
ear kernel) when training our classifiers. As is the
case with most WSD systems, we train a separate
model per verb.
3 Experiment One
The results of experiment one show that in these
circumstances, better performance is achieved by
single annotating more data than by deploing re-
sources towards ensuring that the data is annotated
more accurately through an adjudication process.
3.1 Experimental Design
We conduct a number of experiments to compare
the effect of single annotated versus adjudicated
data on the accuracy of a state of the art WSD sys-
tem. Since OntoNotes does not have a specified
test set, for each word, we used repeated random
partitioning of the data with 10 trials and 10% into
the test set and the remaining 90% comprising the
training set.
We then train an SVM classifier on varying frac-
tions of the data, based on the number of examples
that could be annotated per dollar. Specifically,
in increments of $1.00, we calculate the number
of examples that can be single annotated and the
number that can be double blind annotated and ad-
judicated with that amount of money.
The number of examples computed for single
annotation is selected at random from the train-
ing data. Then the adjudicated examples are se-
lected at random from this subset. Selecting from
the same subset of data approaches pair statisti-
cal testing and results in a more accurate statistical
comparison of the models produced.
Classifiers are trained on this data using the la-
bels from the first round of annotation as the single
annotation labels and the final adjudicated labels
for the smaller subset. This procedure is repeated
ten times and the average results are reported.
For a given verb, each classifier created
throughout this process is tested on the same dou-
ble annotated and adjudicated held-out test set.
3.2 Results
Figure 1 shows a plot of the accuracy of the clas-
sifiers relative to the annotation investment for a
typical verb, to call. As can be seen, the accu-
racy is always higher when training on the larger
amount of single annotated data than when train-
ing on the amount of adjudicated data that had the
equivalent cost of annotation.
Figures 2 and 3 present results averaged over
all 215 verbs in the dataset. First, figure 2 shows
the average accuracy over all verbs by amount in-
vested. These accuracy curves are not smooth be-
66
Figure 1: Performance of single annotated vs. ad-
judicated data by amount invested for to call
cause the verbs all have a different number of total
instances. At various annotation cost values, all of
the instances of one or more verbs will have been
annotated. Hence, the accuracy values might jump
or drop by a larger amount than seen elsewhere in
the graph.
Toward the higher dollar amounts the curve is
dominated by fewer and fewer verbs. We only
display the dollar investments of up to $60 due to
the fact that only five verbs have more than $60?s
worth of instances in the training set.
Figure 2: Average performance of single anno-
tated vs. adjudicated data by amount invested
The average difference in accuracy for Figure 2
across all amounts of investment is 1.64%.
Figure 3 presents the average accuracy relative
to the percent of the total cost to single annotate
all of the instances for a verb. The accuracy at a
given percent of total investment was interpolated
for each verb using linear interpolation and then
averaged over all of the verbs.
Figure 3: Average performance of single anno-
tated vs. adjudicated data by fraction of total in-
vestment
The average difference in accuracy for Figure 3
across each percent of investment is 2.10%.
Figure 4 presents essentially the same informa-
tion as Figure 2, but as a reduction in error rate for
single annotation relative to full adjudication.
Figure 4: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 2
The relative reduction in error rate averaged
over all investment amounts in Figure 2 is 7.77%.
Figure 5 presents the information in Figure 3
as a reduction in error rate for single annotation
relative to full adjudication.
The average relative reduction in error rate over
the fractions of total investment in Figure 5 is
9.32%.
67
Figure 5: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 3
3.3 Discussion
First, it is worth noting that, when the amount of
annotated data is the same for both scenarios, ad-
judicated data leads to slightly better performance
than single annotated data. For example, consider
Figure 3. The accuracy at 100% of the total invest-
ment for the double annotation and adjudication
scenario is 81.13%. The same number of exam-
ples can be single annotated for 0.0833 / 0.1805 =
0.4615 of this dollar investment (using the costs
from Section 2.2). The system trained on that
amount of single annotated data shows a lower ac-
curacy, 80.21%. Thus, in this case, the adjudica-
tion scenario brings about a performance improve-
ment of about 1%.
However, the main thesis of this paper is that in-
stead of double annotating and adjudicating, it is
often better to single annotate more data because
it is a more cost-effective way to achieve a higher
performance. The results of our experiments sup-
port this thesis. At every dollar amount invested,
our supervised WSD system performs better when
trained on single annotated data comparing to dou-
ble annotated and adjudicated data.
The maximum annotation investment amount
for each verb is the cost of single annotating all
of its instances. When the system is trained on
the amount of double annotated data possible at
this investment, its accuracy is 81.13% (Figure 3).
When trained on single annotated data, the system
attains the same accuracy much earlier, at approxi-
mately 60% of the total investment. When trained
on the entire available single annotated data, the
system reaches an accuracy of 82.99%, nearly a
10% relative reduction in error rate over the same
system trained on the adjudicated data obtained for
the same cost.
Averaged over the 215 verbs, the single anno-
tation scenario outperformed adjudication at every
dollar amount investigated.
4 Experiment Two
In this experiment, we consider the arguments of
Beigman and Klebanov (2009). They suggest that
data should be at least double annotated and then
filtered to discard all of the examples where there
were any annotator disagreements.
The main points of their argument are as fol-
lows. They first consider the data to be dividable
into two types, easy (to annotate) cases and hard
cases. Then they correctly note that some anno-
tators could have a systematic bias (i.e., could fa-
vor one label over others in certain types of hard
cases), which would in turn bias the learning of
the classifier. They show that it is theoretically
possible that a band of misclassified hard cases
running parallel to the true separating hyperplane
could mistakenly shift the decision boundary past
up to
?
N easy cases.
We suggest that it is extremely unlikely that a
consequential number of easy cases would exist
nearer to the class boundary than the hard cases.
The hard cases are in fact generally considered to
define the separating hyperplane.
In this experiment, our goal is to determine how
the accuracy of classifiers trained on data labeled
according to Beigman and Klebanov?s discard dis-
agreements strategy compares empirically to the
accuracy resulting from single annotated data. As
in the previous experiment, this analysis is per-
formed relative to the investment in the annotation
effort.
4.1 Experimental Design
We follow essentially the same experimental de-
sign described in section 3.1, using the same state
of the art verb WSD system. We conduct a num-
ber of experiments to compare the effect of single
annotated versus double annotated data. We uti-
lized the same training and test sets as the previous
experiment and similarly trained an SVM on frac-
tions of the data representing increments of $1.00
investments.
As before, the number of examples designated
68
for single annotation is selected at random from
the training data and half of that subset is selected
as the training set for the double annotated data.
Again, selecting from the same subset of data re-
sults in a more accurate statistical comparison of
the models produced.
Classifiers for each annotation scenario are
trained on the labels from the first round of an-
notation, but examples where the second annota-
tor disagreed are thrown out of the double anno-
tated data. This results in slightly less than half as
much data in the double annotation scenario based
on the disagreement rate. Again, the procedure is
repeated ten times and the average results are re-
ported.
For a given verb, each classifier created
throughout this process is tested on the same dou-
ble annotated and adjudicated held-out test set.
4.2 Results
Figure 6 shows a plot of the accuracy of the classi-
fiers relative to the annotation investment for a typ-
ical verb, to call. As can be seen, the accuracy for
a specific investment performing single annotation
is always higher than it is for the same investment
in double annotated data.
Figure 6: Performance of single annotated vs.
double annotated data with disagreements dis-
carded by amount invested for to call
Figures 7 and 8 present results averaged over
all 215 verbs in the dataset. First, figure 7 shows
the average accuracy over all verbs by amount
invested. Again, these accuracy curves are not
smooth because the verbs all have a different num-
ber of total instances. Hence, the accuracy val-
ues might jump or drop by a larger amount at the
points where a given verb is no longer included in
the average.
Toward the higher dollar amounts the curve is
dominated by fewer and fewer verbs. As before,
we only display the results for investments of up
to $60.
The average difference in accuracy for Figure 7
across all amounts of investment is 2.32%.
Figure 8 presents the average accuracy relative
to the percent of the total cost to single annotate
all of the instances for a verb. The accuracy at
a given percent of total investment was interpo-
lated for each verb and then averaged over all of
the verbs.
Figure 7: Average performance of single anno-
tated vs. double annotated data with disagree-
ments discarded by amount invested
Figure 8: Average performance of single anno-
tated vs. adjudicated data by fraction of total in-
vestment
The average difference in accuracy for Figure 8
across all amounts of investment is 2.51%.
69
Figures 9 and 10 present this information as a
reduction in error rate for single annotation rela-
tive to full adjudication.
Figure 9: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 7
The relative reduction in error rate averaged
over all investment amounts in Figure 9 is 10.88%.
Figure 10: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 8
The average relative reduction in error rate over
the fractions of total investment in Figure 10 is
10.97%.
4.3 Discussion
At every amount of investment, our supervised
WSD system performs better when trained on sin-
gle annotated data comparing to double annotated
data with discarded cases of disagreements.
The maximum annotation investment amount
for each verb is the cost of single annotating all
of its instances. When the system is trained on
the amount of double annotated data possible at
this investment, its accuracy is 80.78% (Figure 8).
When trained on single annotated data, the system
reaches the same accuracy much earlier, at approx-
imately 52% of the total investment. When trained
on the entire available single annotated data, the
system attains an accuracy of 82.99%, an 11.5%
relative reduction in error rate compared to the
same system trained on the double annotated data
obtained for the same cost.
The average accuracy of the single annotation
scenario outperforms the double annotated with
disagreements discarded scenario at every dollar
amount investigated.
While this empirical investigation only looked
at verb WSD, it was performed using 215 distinct
verb type datasets. These verbs each have con-
textual features that are essentially unique to that
verb type and consequently, 215 distinct classi-
fiers, one per verb type, are trained. Hence, these
could loosely be considered 215 distinct annota-
tion and classification tasks.
The fact that for the 215 classification tasks the
single annotation scenario on average performed
better than the discard disagreements scenario of
Beigman and Klebanov (2009) strongly suggests
that, while it is theoretically possible for annota-
tion bias to, in turn, bias a classifier?s learning, it
is more likely that you will achieve better results
by training on the single annotated data.
It is still an open issue whether it is generally
best to adjudicate disagreements in the test set or
to throw them out as suggested by (Beigman Kle-
banov and Beigman, 2009).
5 Discussion and Future Work
We investigated 215 WSD classification tasks,
comparing performance under three annotation
scenarios each with the equivalent annotation cost,
single annotation, double annotation with dis-
agreements adjudicated, and double annotation
with disagreements discarded. Averaging over the
215 classification tasks, the system trained on sin-
gle annotated data achieved 10.0% and 11.5% rel-
ative reduction in error rates compared to training
on the equivalent investment in adjudicated and
disagreements discarded data, respectively. While
we believe these results will generalize to other an-
notation tasks, this is still an open question to be
determined by future work.
70
There are probably similar issues in what were
considered fixed costs for the purposes of this pa-
per. For example, it may be possible to train fewer
annotators, and invest the savings into annotating
more data. Perhaps more appropriately, it may be
feasible to simply cut back on the amount of train-
ing provided per annotator and instead annotate
more data.
On the other hand, when the unlabeled data
is not freely obtainable, double annotation may
be more suitable as a route to improving system
performance. There may also be factors other
than cost-effectiveness which make double anno-
tation desirable. Many projects point to their ITA
rates and corresponding kappa values as a mea-
sure of annotation quality, and of the reliability of
the annotators (Artstein and Poesio, 2008). The
OntoNotes project used ITA rates as a way of eval-
uating the clarity of the sense inventory that was
being developed in parallel with the annotation.
Lexical entries that resulted in low ITA rates were
revised, usually improving the ITA rate. Calculat-
ing these rates requires double-blind annotation.
Annotators who consistently produced ITA rates
lower than average were also removed from the
project. Therefore, caution is advised in determin-
ing when to dispense with double annotation in fa-
vor of more cost effective single annotation.
Double annotation can also be used to shed light
on other research questions that, for example, re-
quire knowing which instances are ?hard.? That
knowledge may help with designing additional,
richer annotation layers or with cognitive science
investigations into human representations of lan-
guage.
Our results suggest that systems would likely
benefit more from the larger training datasets that
single annotation makes possible than from the
less noisy datasets resulting from adjudication.
Regardless of whether single or double annota-
tion with adjudication is used, there will always be
noise. Hence, we see the further investigation of
algorithms that generalize despite the presence of
noise to be critical to the future of computational
linguistics. Humans are able to learn in the pres-
ence of noise, and our systems must follow suit.
6 Conclusion
Double annotated data contains less noise than
single annotated data and thus improves the per-
formance of supervised machine learning systems
that are trained on a specific amount of data. How-
ever, double annotation is expensive and the alter-
native of single annotating more data instead is on
the table for many annotation projects.
In this paper we compared the performance of
a supervised machine learning system trained on
double annotated data versus single annotated data
obtainable for the same cost. Our results clearly
demonstrate that single annotating more data can
be a more cost-effective way to improve the sys-
tem performance in the many cases where the un-
labeled data is freely available and there are no
other considerations that necessitate double anno-
tation.
7 Acknowledgements
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In ACL-IJCNLP
?09: Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1, pages 280?287, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
put. Linguist., 35(4):495?503.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3):211?231.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
71
J. Chen and M. Palmer. 2005. Towards robust high
performance word sense disambiguation of english
verbs using rich linguistic features. pages 933?944.
Springer.
Jinying Chen, Dmitriy Dligach, and Martha Palmer.
2007. Towards large-scale high-performance en-
glish verb sense disambiguation by using linguisti-
cally motivated features. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 378?388, Washington, DC, USA.
IEEE Computer Society.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ?08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29?32, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. MIT press Cambridge, MA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57?60, Morristown, NJ, USA. Association for
Computational Linguistics.
A. Ratnaparkhi. 1998. Maximum entropy models for
natural language ambiguity resolution. Ph.D. the-
sis, University of Pennsylvania.
72
	ABBCBBDEFFABAAB

BAFDFDBProceedings of the Fifth Law Workshop (LAW V), pages 65?73,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Reducing the Need for Double Annotation
Dmitriy Dligach
Department of Computer Science
University of Colorado at Boulder
Dmitriy.Dligach@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
Martha.Palmer@colorado.edu
Abstract
The quality of annotated data is crucial for
supervised learning. To eliminate errors in
single annotated data, a second round of an-
notation is often used. However, is it abso-
lutely necessary to double annotate every ex-
ample? We show that it is possible to reduce
the amount of the second round of annotation
by more than half without sacrificing the per-
formance.
1 Introduction
Supervised learning has become the dominant
paradigm in NLP in recent years thus making the
creation of high-quality annotated corpora a top pri-
ority in the field. A corpus where each instance is
annotated by a single annotator unavoidably con-
tains errors. To improve the quality of the data, one
may choose to annotate each instance twice and ad-
judicate the disagreements thus producing the gold
standard. For example, the OntoNotes (Hovy et al,
2006) project opted for this approach.
However, is it absolutely necessary to double an-
notate every example? In this paper, we demonstrate
that it is possible to double annotate only a subset of
the single annotated data and still achieve the same
level of performance as with full double annotation.
We accomplish this task by using the single anno-
tated data to guide the selection of the instances to
be double annotated.
We propose several algorithms that accept sin-
gle annotated data as input. The algorithms select
a subset of this data that they recommend for an-
other round of annotation and adjudication. The sin-
gle annotated data our algorithms work with can po-
tentially come from any source. For example, it can
be the single annotated output of active learning or
the data that had been randomly sampled from some
corpus and single annotated. Our approach is ap-
plicable whenever a second round of annotation is
being considered to improve the quality of the data.
Our approach is similar in spirit to active learn-
ing but more practical in a double annotation multi-
tagger environment. We evaluate this approach on
OntoNotes word sense data. Our best algorithm de-
tects 75% of the errors, while the random sampling
baseline only detects less than a half of that amount.
We also show that this algorithm can lead to a 54%
reduction in the amount of annotation needed for the
second round of annotation.
The rest of this paper is structured as follows: we
discuss the relevant work in section 2, we explain
our approach in section 3, we evaluate our approach
in section 4, we discuss the results and draw a con-
clusion in section 5, and finally, we talk about our
plans for future work in section 6.
2 Related Work
Active Learning (Settles, 2009; Olsson, 2009) has
been the traditional avenue for reducing the amount
of annotation. However, in practice, serial active
learning is difficult in a multi-tagger environment
(Settles, 2009) when many annotators are working
in parallel (e.g. OntoNotes employs tens of tag-
gers). At the same time, several papers recently ap-
peared that used OntoNotes data for active learning
experiments (Chen et al, 2006; Zhu, 2007; Zhong et
al., 2008). These works all utilized OntoNotes gold
standard labels, which were obtained via double an-
notation and adjudication. The implicit assumption,
therefore, was that the same process of double anno-
65
tation and adjudication could be reproduced in the
process of active learning. However, this assumption
is not very realistic and in practice, these approaches
may not bring about the kind of annotation cost re-
duction that they report. For example, an instance
would have to be annotated by two taggers (and each
disagreement adjudicated) on each iteration before
the system can be retrained and the next instance se-
lected. Active learning tends to select ambiguous ex-
amples (especially at early stages), which are likely
to cause an unusually high number of disagreements
between taggers. The necessity of frequent manual
adjudication would slow down the overall process.
Thus, if the scenarios of (Chen et al, 2006; Zhu,
2007; Zhong et al, 2008) were used in practice, the
taggers would have to wait on each other, on the ad-
judicator, and on the retraining, before the system
can select the next example. The cost of annotator
waiting time may undermine the savings in annota-
tion cost.
The rationale for our work arises from these dif-
ficulties: because active learning is not practical
in a double annotation scenario, the data is single
annotated first (with the instances selected via ac-
tive learning, random sampling or some other tech-
nique). After that, our algorithms can be applied to
select a subset of the single annotated data for the
second round of annotation and adjudication. Our
algorithms select the data for repeated labeling in a
single batch, which means the selection can be done
off-line. This should greatly simplify the application
of our approach in a real life annotation project.
Our work also borrows from the error detection
literature. Researchers have explored error detec-
tion for manually tagged corpora in the context
of pos-tagging (Eskin, 2000; Kve?ton? and Oliva,
2002; Nova?k and Raz??mova?, 2009), dependency
parsing (Dickinson, 2009), and text-classification
(Fukumoto and Suzuki, 2004). The approaches to
error detection include anomaly detection (Eskin,
2000), finding inconsistent annotations (van Hal-
teren, 2000; Kve?ton? and Oliva, 2002; Nova?k and
Raz??mova?, 2009), and using the weights assigned
by learning algorithms such as boosting (Abney et
al., 1999; Luo et al, 2005) and SVM (Nakagawa
and Matsumoto, 2002; Fukumoto and Suzuki, 2004)
by exploiting the fact that errors tend to concentrate
among the examples with large weights. Some of
these works eliminate the errors (Luo et al, 2005).
Others correct them automatically (Eskin, 2000;
Kve?ton? and Oliva, 2002; Fukumoto and Suzuki,
2004; Dickinson, 2009) or manually (Kve?ton? and
Oliva, 2002). Several authors also demonstrate en-
suing performance improvements (Fukumoto and
Suzuki, 2004; Luo et al, 2005; Dickinson, 2009).
All of these researchers experimented with single
annotated data such as Penn Treebank (Marcus et al,
1993) and they were often unable to hand-examine
all the data their algorithms marked as errors be-
cause of the large size of their data sets. Instead,
to demonstrate the effectiveness of their approaches,
they examined a selected subset of the detected ex-
amples (e.g. (Abney et al, 1999; Eskin, 2000; Nak-
agawa and Matsumoto, 2002; Nova?k and Raz??mova?,
2009)). In this paper, we experiment with fully dou-
ble annotated and adjudicated data, which allows us
to evaluate the effectiveness of our approach more
precisely. A sizable body of work exists on us-
ing noisy labeling obtained from low-cost annota-
tion services such as Amazon?s Mechanical Turk
(Snow et al, 2008; Sheng et al, 2008; Hsueh et
al., 2009). Hsueh et al (2009) identify several cri-
teria for selecting high-quality annotations such as
noise level, sentiment ambiguity, and lexical uncer-
tainty. (Sheng et al, 2008) address the relationships
between various repeated labeling strategies and the
quality of the resulting models. They also propose
a set of techniques for selective repeated labeling
which are based on the principles of active learn-
ing and an estimate of uncertainty derived from each
example?s label multiset. These authors focus on
the scenario where multiple (greater than two) labels
can be obtained cheaply. This is not the case with the
data we experiment with: OntoNotes data is double
annotated by expensive human experts. Also, unfor-
tunately, Sheng et al simulate multiple labeling (the
noise is introduced randomly). However, human an-
notators may have a non-random annotation bias re-
sulting from misreading or misinterpreting the direc-
tions, or from genuine ambiguities. The data we use
in our experiments is annotated by humans.
3 Algorithms
In the approach to double annotation we are propos-
ing, the reduction in annotation effort is achieved by
66
double annotating only the examples selected by our
algorithms instead of double annotating the entire
data set. If we can find most or all the errors made
during the first round of labeling and show that dou-
ble annotating only these instances does not sacri-
fice performance, we will consider the outcome of
this study positive. We propose three algorithms for
selecting a subset of the single annotated data for the
second round of annotation.
Our machine tagger algorithm draws on error de-
tection research. Single annotated data unavoidably
contains errors. The main assumption this algorithm
makes is that a machine learning classifier can form
a theory about how the data should be labeled from
a portion of the single annotated data. The classifier
can be subsequently applied to the rest of the data to
find the examples that contradict this theory. In other
words, the algorithm is geared toward detecting in-
consistent labeling within the single annotated data.
The machine tagger algorithm can also be viewed as
using a machine learning classifier to simulate the
second human annotator. The machine tagger al-
gorithm accepts single annotated data as input and
returns the instances that it believes are labeled in-
consistently.
Our ambiguity detector algorithm is inspired by
uncertainty sampling (Lewis and Gale, 1994), a kind
of active learning in which the model selects the
instances for which its prediction is least certain.
Some instances in the data are intrinsically ambigu-
ous. The main assumption the ambiguity detector
algorithm makes is that a machine learning classifier
trained using a portion of the single annotated data
can be used to detect ambiguous examples in the
rest of the single annotated data. The algorithm is
geared toward finding hard-to-classify instances that
are likely to cause problems for the human annota-
tor. The ambiguity detector algorithm accepts single
annotated data as input and returns the instances that
are potentially ambiguous and thus are likely to be
controversial among different annotators.
It is important to notice that the machine tagger
and ambiguity detector algorithms target two differ-
ent types of errors in the data: the former detects
inconsistent labeling that may be due to inconsistent
views among taggers (in a case when the single an-
notated data is labeled by more than one person) or
the same tagger tagging inconsistently. The latter
finds the examples that are likely to result in dis-
agreements when labeled multiple times due to their
intrinsic ambiguity. Therefore, our goal is not to
compare the performance of the machine tagger and
ambiguity detector algorithms, but rather to provide
a viable solution for reducing the amount of annota-
tion on the second round by detecting as much noise
in the data as possible. Toward that goal we also
consider a hybrid approach, which is a combination
of the first two.
Still, we expect some amount of overlap in the
examples detected by the two approaches. For ex-
ample, the ambiguous instances selected by the sec-
ond algorithm may also turn out to be the ones that
the first one will identify because they are harder
to classify (both by human annotators and machine
learning classifiers). The three algorithms we exper-
iment with are therefore (1) the machine tagger, (2)
the ambiguity detector, and (3) the hybrid of the two.
We will now provide more details about how each of
them is implemented.
3.1 General Framework
All three algorithms accept single annotated data as
input. They output a subset of this data that they rec-
ommend for repeated labeling. All algorithms be-
gin by splitting the single annotated data into N sets
of equal size. They proceed by training a classifier
on N ? 1 sets and applying it to the remaining set,
which we will call the pool1. The cycle repeats N
times in the style of N -fold cross-validation. Upon
completion, each single annotated instance has been
examined by the algorithm. A subset of the single
annotated data is selected for the second round of an-
notation based on various criteria. These criteria are
what sets the algorithms apart. Because of the time
constraints, for the experiments we describe in this
paper, we set N to 10. A larger value will increase
the running time but may also result in an improved
performance.
1Notice that the term pool in active learning research typi-
cally refers to the collection of unlabeled data from which the
examples to be labeled are selected. In our case, this term ap-
plies to the data that is already labeled and the goal is to select
data for repeated labeling.
67
3.2 Machine Tagger Algorithm
The main goal of the machine tagger algorithm is
finding inconsistent labeling in the data. This al-
gorithm operates by training a discriminative clas-
sifier and making a prediction for each instance in
the pool. Whenever this prediction disagrees with
the human-assigned label, the instance is selected
for repeated labeling.
For classification we choose a support vector ma-
chine (SVM) classifier because we need a high-
accuracy classifier. The state-of-the art system we
use for our experiments is SVM-based (Dligach and
Palmer, 2008). The specific classification software
we utilize is LibSVM (Chang and Lin, 2001). We
accept the default settings (C = 1 and linear ker-
nel).
3.3 Ambiguity Detector Algorithm
The ambiguity detector algorithm trains a proba-
bilistic classifier and makes a prediction for each
instance in the pool. However, unlike the previous
algorithm, the objective in this case is to find the in-
stances that are potentially hard to annotate due to
their ambiguity. The instances that lie close to the
decision boundary are intrinsically ambiguous and
therefore harder to annotate. We hypothesize that a
human tagger is more likely to make a mistake when
annotating these instances.
We can estimate the proximity to the class bound-
ary using a classifier confidence metric such as the
prediction margin, which is a simple metric often
used in active learning (e.g. (Chen et al, 2006)). For
an instance x, we compute the prediction margin as
follows:
Margin(x) = |P (c1|x)? P (c2|x)| (1)
Where c1 and c2 are the two most probable classes
of x according to the model. We rank the single
annotated instances by their prediction margin and
select selectsize instances with the smallest margin.
The selectsize setting can be manipulated to increase
the recall. We experiment with the settings of select-
size of 20% and larger.
While SVM classifiers can be adapted to produce
a calibrated posterior probability (Platt and Platt,
1999), for simplicity, we use a maximum entropy
classifier, which is an intrinsically probabilistic clas-
sifier and thus has the advantage of being able to
output the probability distribution over the class la-
bels right off-the-shelf. The specific classification
software we utilize is the python maximum entropy
modeling toolkit (Le, 2004) with the default options.
3.4 Hybrid Algorithm
We hypothesize that both the machine tagger and
ambiguity detector algorithms we just described se-
lect the instances that are appropriate for the second
round of human annotation. The hybrid algorithm
simply unions the instances selected by these two
algorithms. As a result, the amount of data selected
by this algorithm is expected to be larger than the
amount selected by each individual algorithm.
4 Evaluation
For evaluation we use the word sense data annotated
by the OntoNotes project. The OntoNotes data was
chosen because it is fully double-blind annotated by
human annotators and the disagreements are adjudi-
cated by a third (more experienced) annotator. This
type of data allows us to: (1) Simulate single anno-
tation by using the labels assigned by the first an-
notator, (2) Simulate the second round of annotation
for selected examples by using the labels assigned
by the second annotator, (3) Evaluate how well our
algorithms capture the errors made by the first anno-
tator, and (4) Measure the performance of the cor-
rected data against the performance of the double
annotated and adjudicated gold standard.
We randomly split the gold standard data into ten
parts of equal size. Nine parts are used as a pool
of data from which a subset is selected for repeated
labeling. The rest is used as a test set. Before pass-
ing the pool to the algorithm, we ?single annotate?
it (i.e. relabel with the labels assigned by the first
annotator). The test set alays stays double anno-
tated and adjudicated to make sure the performance
is evaluated against the gold standard labels. The cy-
cle is repeated ten times and the results are averaged.
Since our goal is finding errors in single anno-
tated data, a brief explanation of what we count as
an error is appropriate. In this evaluation, the er-
rors are the disagreements between the first anno-
tator and the gold standard. The fact that our data
68
Sense Definition Sample Context
Accept as true without
verification
I assume his train was
late
Take on a feature, po-
sition, responsibility,
right
When will the new
President assume of-
fice?
Take someone?s soul
into heaven
This is the day when
Mary was assumed
into heaven
Table 1: Senses of to assume
is double annotated allows us to be reasonably sure
that most of the errors made by the first annotator
were caught (as disagreements with the second an-
notator) and resolved. Even though other errors may
still exist in the data (e.g. when the two annotators
made the same mistake), we assume that there are
very few of them and we ignore them for the pur-
pose of this study.
4.1 Task
The task we are using for evaluating our approach
is word sense disambiguation (WSD). Resolution of
lexical ambiguities has for a long time been viewed
as an important problem in natural language pro-
cessing that tests our ability to capture and represent
semantic knowledge and and learn from linguistic
data. More specifically, we experiment with verbs.
There are fewer verbs in English than nouns but the
verbs are more polysemous, which makes the task
of disambiguating verbs harder. As an example, we
list the senses of one of the participating verbs, to
assume, in Table 1.
The goal of WSD is predicting the sense of an am-
biguous word given its context. For example, given
a sentence When will the new President assume of-
fice?, the task consists of determining that the verb
assume in this sentence is used in the Take on a fea-
ture, position, responsibility, right, etc. sense.
4.2 Data
We selected the 215 most frequent verbs in the
OntoNotes data and discarded the 15 most frequent
ones to make the size of the dataset more manage-
able (the 15 most frequent verbs have roughly as
many examples as the next 200 frequent verbs). We
Inter-annotator agreement 86%
Annotator1-gold standard agreement 93%
Share of the most frequent sense 71%
Number of classes (senses) per verb 4.44
Table 2: Evaluation data at a glance
ended up with a dataset containing 58,728 instances
of 200 frequent verbs. Table 2 shows various impor-
tant characteristics of this dataset averaged across
the 200 verbs.
Observe that even though the annotator1-gold
standard agreement is high, it is not perfect: about
7% of the instances are the errors the first annota-
tor made. These are the instances we are target-
ing. OntoNotes double annotated all the instances
to eliminate the errors. Our goal is finding them au-
tomatically.
4.3 System
Our word sense disambiguation system (Dligach and
Palmer, 2008) includes three groups of features.
Lexical features include open class words from the
target sentence and the two surrounding sentences;
two words on both sides of the target verb and their
POS tags. Syntactic features are based on con-
stituency parses of the target sentence and include
the information about whether the target verb has a
subject/object, what their head words and POS tags
are, whether the target verb has a subordinate clause,
and whether the target verb has a PP adjunct. The
semantic features include the information about the
semantic class of the subject and the object of the
target verb. The system uses Libsvm (Chang and
Lin, 2001) software for classification. We train a
single model per verb and average the results across
all 200 verbs.
4.4 Performance Metrics
Our objective is finding errors in single annotated
data. One way to quantify the success of error de-
tection is by means of precision and recall. We com-
pute precision as the ratio of the number of errors
in the data that the algorithm selected and the to-
tal number of instances the algorithm selected. We
compute recall as the ratio of the number of errors
in the data that the algorithm selected to the total
69
number of errors in the data. To compute baseline
precision and recall for an algorithm, we count how
many instances it selected and randomly draw the
same number of instances from the single annotated
data. We then compute precision and recall for the
randomly selected data.
We also evaluate each algorithm in terms of clas-
sification accuracy. For each algorithm, we measure
the accuracy on the test set when the model is trained
on: (1) Single annotated data only, (2) Single anno-
tated data with a random subset of it double anno-
tated2 (of the same size as the data selected by the
algorithm), (3) Single annotated data with the in-
stances selected by the algorithm double annotated,
and (4) Single annotated data with all instances dou-
ble annotated.
4.5 Error Detection Performance
In this experiment we evaluate how well the three
algorithms detect the errors. We split the data for
each word into 90% and 10% parts as described at
the beginning of section 4. We relabel the 90% part
with the labels assigned by the first tagger and use it
as a pool in which we detect the errors. We pass the
pool to each algorithm and compute the precision
and recall of errors in the data the algorithm returns.
We also measure the random baseline performance
by drawing the same number of examples randomly
and computing the precision and recall. The results
are in the top portion of Table 3.
Consider the second column, which shows the
performance of the machine tagger algorithm. The
algorithm identified as errors 16.93% of the total
number of examples that we passed to it. These se-
lected examples contained 60.32% of the total num-
ber of errors found in the data. Of the selected ex-
amples, 23.81% were in fact errors. By drawing the
same number of examples (16.93%) randomly we
recall only 16.79% of the single annotation errors.
The share of errors in the randomly drawn examples
is 6.82%. Thus, the machine tagger outperforms the
random baseline both with respect to precision and
recall.
The ambiguity detector algorithm selected 20% of
the examples with the highest value of the prediction
2Random sampling is often used as a baseline in the active
learning literature (Settles, 2009; Olsson, 2009).
margin and beat the random baseline both with re-
spect to precision and recall. The hybrid algorithm
also beat the random baselines. It recalled 75% of
errors but at the expense of selecting a larger set of
examples, 30.48%. This is the case because it selects
both the data selected by the machine tagger and the
ambiguity detector. The size selected, 30.48%, is
smaller than the sum, 16.93% + 20.01%, because
there is some overlap between the instances selected
by the first two algorithms.
4.6 Model Performance
In this experiment we investigate whether double
annotating and adjudicating selected instances im-
proves the accuracy of the models. We use the same
pool/test split (90%-10%) as was used in the previ-
ous experiment. The results are in the bottom por-
tion of Table 3.
Let us first validate empirically an assumption this
paper makes: we have been assuming that full dou-
ble annotation is justified because it helps to correct
the errors the first annotator made, which in turn
leads to a better performance. If this assumption
does not hold, our task is pointless. In general re-
peated labeling does not always lead to better per-
formance (Sheng et al, 2008), but it does in our
case. We train a model using only the single an-
notated data and test it. We then train a model using
the double annotated and adjudicated version of the
same data and evaluate its performance.
As expected, the models trained on fully double
annotated data perform better. The performance of
the fully double annotated data, 84.15%, is the ceil-
ing performance we can expect to obtain if we detect
all the errors made by the first annotator. The perfor-
mance of the single annotated data, 82.84%, is the
hard baseline. Thus, double annotating is beneficial,
especially if one can avoid double annotating every-
thing by identifying the single annotated instances
where an error is suspected.
All three algorithms beat both the hard and the
random baselines. For example, by double annotat-
ing the examples the hybrid algorithm selected we
achieve an accuracy of 83.82%, which is close to the
full double annotation accuracy, 84.15%. By double
annotating the same number of randomly selected
instances, we reach a lower accuracy, 83.36%. The
differences are statistically significant for all three
70
Metric Machine Tagger, % Ambiguity Detector, % Hybrid, %
Actual size selected 16.93 20.01 30.48
Error detection precision 23.81 10.61 14.70
Error detection recall 60.32 37.94 75.14
Baseline error detection precision 6.82 6.63 6.86
Baseline error detection recall 16.79 19.61 29.06
Single annotation only accuracy 82.84 82.84 82.84
Single + random double accuracy 83.23 83.09 83.36
Single + selected double accuracy 83.58 83.42 83.82
Full double annotation accuracy 84.15 84.15 84.15
Table 3: Results of performance evaluation. Error detection performance is shown at the top part of the table. Model
performance is shown at the bottom.
algorithms (p < 0.05).
Even though the accuracy gains over the random
baseline are modest in absolute terms, the reader
should keep in mind that the maximum possible ac-
curacy gain is 84.15% - 82.84% = 1.31% (when all
the data is double annotated). The hybrid algorithm
came closer to the target accuracy than the other
two algorithms because of a higher recall of errors,
75.14%, but at the expense of selecting almost twice
as much data as, for example, the machine tagger
algorithm.
4.7 Reaching Double Annotation Accuracy
The hybrid algorithm performed better than the
baselines but it still fell short of reaching the accu-
racy our system achieves when trained on fully dou-
ble annotated data. However, we have a simple way
of increasing the recall of error detection. One way
to do it is by increasing the number of instances with
the smallest prediction margin the ambiguity detec-
tor algorithm selects, which in turn will increase the
recall of the hybrid algorithm. In this series of exper-
iments we measure the performance of the hybrid al-
gorithm at various settings of the selection size. The
goal is to keep increasing the recall of errors until the
performance is close to the double annotation accu-
racy.
Again, we split the data for each word into 90%
and 10% parts. We relabel the 90% part with the
labels assigned by the first tagger and pass it to the
hybrid algorithm. We vary the selection size setting
between 20% and 50%. At each setting, we com-
pute the precision and recall of errors in the data
the algorithm returns as well as in the random base-
line. We also measure the performance of the mod-
els trained on on the single annotated data with its
randomly and algorithm-selected subsets double an-
notated. The results are in Table 4.
As we see at the top portion of the Table 4, as we
select more and more examples with a small predic-
tion margin, the recall of errors grows. For exam-
ple, at the 30% setting, the hybrid algorithm selects
37.91% of the total number of single annotated ex-
amples, which contain 80.42% of all errors in the
single annotated data (more than twice as much as
the random baseline).
As can be seen at the bottom portion of the Ta-
ble 4, with increased recall of errors, the accuracy
on the test set alo grows and nears the double an-
notation accuracy. At the 40% setting, the algorithm
selects 45.80% of the single annotated instances and
the accuracy with these instances double annotated
reaches 84.06% which is not statistically different
(p < 0.05) from the double annotation accuracy.
5 Discussion and Conclusion
We proposed several simple algorithms for reducing
the amount of the second round of annotation. The
algorithms operate by detecting annotation errors
along with hard-to-annotate and potentially error-
prone instances in single annotated data. We evalu-
ate the algorithms using OntoNotes word sense data.
Because OntoNotes data is double annotated and ad-
judicated we were able to evaluate the error detec-
tion performance of the algorithms as well as their
accuracy on the gold standard test set. All three al-
71
Metric Selection Size
20% 30% 40% 50%
Actual size selected 30.46 37.91 45.80 54.12
Error detection precision 14.63 12.81 11.40 10.28
Error detection recall 75.65 80.42 83.95 87.37
Baseline error detection precision 6.80 6.71 6.78 6.77
Baseline error detection recall 29.86 36.23 45.63 53.30
Single annotation only accuracy 83.04 83.04 83.04 83.04
Single + random double accuracy 83.47 83.49 83.63 83.81
Single + selected double accuracy 83.95 83.99 84.06 84.10
Full double annotation accuracy 84.18 84.18 84.18 84.18
Table 4: Performance at various sizes of selected data.
gorithms outperformed the random sampling base-
line both with respect to error recall and model per-
formance.
By progressively increasing the recall of errors,
we showed that the hybrid algorithm can be used
to replace full double annotation. The hybrid algo-
rithm reached accuracy that is not statistically dif-
ferent from the full double annotation accuracy with
approximately 46% of data double annotated. Thus,
it can potentially save 54% of the second pass of an-
notation effort without sacrificing performance.
While we evaluated the proposed algorithms only
on word sense data, the evaluation was performed
using 200 distinct word type datasets. These words
each have contextual features that are essentially
unique to that word type and consequently, 200
distinct classifiers, one per word type, are trained.
Hence, these could loosely be considered 200 dis-
tinct annotation and classification tasks. Thus, it is
likely that the proposed algorithms will be widely
applicable whenever a second round of annotation
is being contemplated to improve the quality of the
data.
6 Future Work
Toward the same goal of reducing the cost of the sec-
ond round of double annotation, we will explore sev-
eral research directions. We will investigate the util-
ity of more complex error detection algorithms such
as the ones described in (Eskin, 2000) and (Naka-
gawa and Matsumoto, 2002). Currently our algo-
rithms select the instances to be double annotated
in one batch. However it is possible to frame the
selection more like batch active learning, where the
next batch is selected only after the previous one is
annotated, which may result in further reductions in
annotation costs.
Acknowledgements
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambiguation,
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-C-0022, a subcontract from the BBN-AGILE
Team. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Steven Abney, Robert E. Schapire, and Yoram Singer.
1999. Boosting applied to tagging and pp attachment.
In Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 38?45.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behavior
of active learning for word sense disambiguation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 120?127, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
72
Markus Dickinson. 2009. Correcting dependency anno-
tation errors. In EACL ?09: Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 193?201,
Morristown, NJ, USA. Association for Computational
Linguistics.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ?08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29?32, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Eleazar Eskin. 2000. Detecting errors within a corpus
using anomaly detection. In Proceedings of the 1st
North American chapter of the Association for Com-
putational Linguistics conference, pages 148?153, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Fumiyo Fukumoto and Yoshimi Suzuki. 2004. Correct-
ing category errors in text classification. In COLING
?04: Proceedings of the 20th international conference
on Computational Linguistics, page 868, Morristown,
NJ, USA. Association for Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57?60, Morristown, NJ, USA. Association for
Computational Linguistics.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In HLT ?09: Proceedings
of the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 27?35, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Pavel Kve?ton? and Karel Oliva. 2002. (semi-)automatic
detection of errors in pos-tagged corpora. In Proceed-
ings of the 19th international conference on Compu-
tational linguistics, pages 1?7, Morristown, NJ, USA.
Association for Computational Linguistics.
Zhang Le, 2004. Maximum Entropy Modeling Toolkit for
Python and C++.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In SIGIR ?94:
Proceedings of the 17th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 3?12, New York, NY, USA.
Springer-Verlag New York, Inc.
Dingsheng Luo, Xinhao Wang, Xihong Wu, and
Huisheng Chi. 2005. Learning outliers to refine a cor-
pus for chinese webpage categorization. In ICNC (1),
pages 167?178.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Comput. Linguist.,
19(2):313?330.
Tetsuji Nakagawa and Yuji Matsumoto. 2002. Detect-
ing errors in corpora using support vector machines.
In Proceedings of the 19th international conference
on Computational linguistics, pages 1?7, Morristown,
NJ, USA. Association for Computational Linguistics.
Va?clav Nova?k and Magda Raz??mova?. 2009. Unsu-
pervised detection of annotation inconsistencies using
apriori algorithm. In ACL-IJCNLP ?09: Proceedings
of the Third Linguistic Annotation Workshop, pages
138?141, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. In Technical Report, Swedish Institute of
Computer Science.
John C. Platt and John C. Platt. 1999. Probabilistic out-
puts for support vector machines and comparisons to
regularized likelihood methods. In Advances in Large
Margin Classifiers, pages 61?74. MIT Press.
Burr Settles. 2009. Active learning literature survey. In
Computer Sciences Technical Report 1648 University
of Wisconsin-Madison.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get another label? improving data qual-
ity and data mining using multiple, noisy labelers. In
KDD ?08: Proceeding of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 614?622, New York, NY, USA.
ACM.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Hans van Halteren. 2000. The detection of inconsistency
in manually tagged text. In Proceedings of LINC-00,
Luxembourg.
Z. Zhong, H.T. Ng, and Y.S. Chan. 2008. Word sense
disambiguation using OntoNotes: An empirical study.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1002?
1010. Association for Computational Linguistics.
Jingbo Zhu. 2007. Active learning for word sense disam-
biguation with methods for addressing the class imbal-
ance problem. In In Proceedings of ACL, pages 783?
790.
73
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 73?81,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Active Learning for Coreference Resolution
Timothy A. Miller and Dmitriy Dligach and Guergana K. Savova
Children?s Hospital Boston
and Harvard Medical School
300 Longwood Ave.
Enders 141
Boston, MA 02115, USA
{Timothy.Miller,Dmitriy.Dligach,Guergana.Savova}@childrens.harvard.edu
Abstract
Active learning can lower the cost of anno-
tation for some natural language processing
tasks by using a classifier to select informa-
tive instances to send to human annotators. It
has worked well in cases where the training in-
stances are selected one at a time and require
minimal context for annotation. However,
coreference annotations often require some
context and the traditional active learning ap-
proach may not be feasible. In this work we
explore various active learning methods for
coreference resolution that fit more realisti-
cally into coreference annotation workflows.
1 Introduction
Coreference resolution is the task of deciding which
entity mentions in a text refer to the same entity.
Solving this problem is an important part of the
larger task of natural language understanding in gen-
eral. The clinical domain offers specific tasks where
it is easy to see that correctly resolving coreference
is important. For example, one important task in the
clinical domain is template filling for the Clinical El-
ements Model (CEM).1 This task involves extracting
various pieces of information about an entity and fit-
ting the information into a standard data structure
that can be reasoned about. An example CEM tem-
plate is that for Disease with attributes for Body Lo-
cation, Associated Sign or Symptom, Subject, Nega-
tion, Uncertainty, and Severity. Since a given entity
may have many different attributes and relations, it
1http://intermountainhealthcare.org/cem
may be mentioned multiple times in a text. Coref-
erence resolution is important for this task because
it must be known that all the attributes and relations
apply to the same entity so that a single CEM tem-
plate is filled in for an entity, rather than creating a
new template for each mention of the entity.
2 Background
2.1 Coreference Resolution
Space does not permit a thorough review of coref-
erence resolution, but recent publications covered
the history and current state of the art for both the
general domain and the clinical domain (Ng, 2010;
Pradhan et al, 2011; Zheng et al, 2011).
The system used here (Zheng et al, 2012) is
an end-to-end coreference resolution system, mean-
ing that the algorithm receives no gold standard in-
formation about mentions, named entity types, or
any linguistic information. The coreference res-
olution system is a module of the clinical Tex-
tual Analysis and Knowledge Extraction System
(cTAKES) (Savova et al, 2010) that is trained on
clinical data. It takes advantage of named entity
recognition (NER) and categorization to detect en-
tity mentions, and uses several cTAKES modules
as feature generators, including the NER module,
a constituency parser module, and a part of speech
tagging module.
The system architecture is based on the pairwise
discriminative classification approach to the coref-
erence resolution problem. In that paradigm, pairs
of mentions are classified as coreferent or not, and
then some reconciliation must be done on all of the
73
links so that there are no conflicts in the clusters.
The system uses support vector machines (SVMs)
as the pairwise classifiers, and conflicts are avoided
by only allowing an anaphor to link with one an-
tecedent, specifically that antecedent the classifier
links with the highest probability.
There are separate pairwise classifiers for named
entity and pronominal anaphor types. In the domain
of clinical narratives, person mentions and personal
pronouns in particular are not especially challeng-
ing ? the vast majority of person mentions are the
patient. In addition, pronoun mentions, while im-
portant, are relatively rare. Thus we are primarily
interested in named entity coreference classification,
and we use that classifier as the basis of the work de-
scribed here.
The feature set of this system is similar to that
used by Ng and Cardie (2002). That system in-
cludes features based on surface form of the men-
tions, shallow syntactic information, and lexical se-
mantics from WordNet. The system used here has
a similar feature set but uses Unified Medical Lan-
guage System (UMLS)2 semantic features as it is
intended for clinical text, and also incorporates sev-
eral syntactic features extracted from constituency
parses extracted from cTAKES.
To generate training data for active learning simu-
lations, mention detection is run first (cTAKES con-
tains a rule-based NER system) to find named en-
tities and a constituency parser situates entities in
a syntax tree). For each entity found, the system
works backwards through all other mentions within
a ten sentence window. For each candidate anaphor-
antecedent pair, a feature vector is extracted using
the features briefly described above.
2.2 Active Learning
Active Learning (AL) is a popular approach to se-
lecting unlabeled data for annotation (Settles, 2010)
that can potentially lead to drastic reductions in the
amount of annotation that is necessary for train-
ing an accurate statistical classifier. Unlike passive
learning, where the data is sampled for annotation
randomly, AL delegates data selection to the clas-
sifier. AL is an iterative process that operates by
first training a classifier on a small sample of the
2http://www.nlm.nih.gov/research/umls/
data known as the seed examples. The classifier
is subsequently applied to a pool of unlabeled data
with the purpose of selecting additional examples
the classifier views as informative. The selected data
is annotated and the cycle is repeated, allowing the
learner to quickly refine the decision boundary be-
tween classes. One common approach to assessing
the informativeness is uncertainty sampling (Lewis
and Gale, 1994; Schein and Ungar, 2007), in which
the learner requests a label for the instance it is most
uncertain how to label. In this work, we base our
instance selection on the distance to the SVM de-
cision boundary (Tong and Koller, 2002), assuming
that informative instances tend to concentrate near
the boundary.
Most AL work focuses on instance selection
where the unit of selection is one instance repre-
sented as a feature vector. In this paper we also
attempt document selection, where the unit of se-
lection is a document, typically containing multi-
ple coreference pairs each represented as a feature
vector. The most obvious way to extend a sin-
gle instance informativeness metric to the document
scenario is to aggregate the informativeness scores.
Several uncertainty metrics have been proposed that
follow that route to adapt single instance selection
to multiple instance scenarios (Settles et al, 2008;
Tomanek et al, 2009). We borrow some of these
metrics and propose several new ones.
To the best of our knowledge only one work
exists that explores AL for coreference resolution.
Gasperin (2009) experiments with an instance based
approach in which batches of anaphoric pairs are se-
lected on each iteration of AL. In these experiments,
AL did not outperform the passive learning baseline,
probably due to selecting batches of large size.
3 Active Learning Configurations
3.1 Instance Selection
The first active learning model we considered selects
individual training instances ? putatively coreferent
mention pairs. This method is quite easy to simu-
late, and follows naturally from most of the theo-
retical active learning literature, but it has the draw-
back of being seemingly unrealistic as an annotation
paradigm. That is, since coreference can span across
an entire document, it is probably not practical to
74
have a human expert annotate only a single instance
at a time when a given instance may require many
sentences of reading in order to contextualize the in-
stance and properly label it. Moreover, even if such
an annotation scheme proved viable, it may result
in an annotated corpus that is only valuable for one
type of coreference system architecture.
Nonetheless, active learning for coreference at the
instance level is still useful. First, since this method
most closely follows the successful active learning
literature by using the smallest discrete problems, it
can serve as a proof of concept for active learning
in the coreference task ? if it does not work well at
this level, it probably will not work at the document
level. Previous results (Gasperin, 2009) have shown
that certain multiple instance methods do not work
for coreference resolution, so testing on smaller se-
lection sizes first can ensure that active learning is
even viable at that scale. In addition, though in-
stance selection may not be feasible for real world
annotations, individual instances and metrics for se-
lecting them are usually used as building blocks for
more complex methods. In order for this to be pos-
sible it must be shown that the instances themselves
have some value.
3.2 Document Selection
Active learning with document selection is a much
more realistic representation of conventional anno-
tation methods. Conventionally, a set of documents
is selected, and each document is annotated exhaus-
tively for coreference (Pradhan et al, 2011; Savova
et al, 2011). Document selection fits into this work-
flow very naturally, by selecting the next document
to annotate exhaustively based on some metric of
which document has the best instances. In theory,
this method can save annotation time by only anno-
tating the most valuable documents.
Document selection is somewhat similar to the
concept of batch-mode active learning, wherein
multiple instances are selected at once, though
batch-mode learning is usually intended to solve a
different problem, that of an asymmetry between
classifier training speed and annotation speed (Set-
tles, 2010). A more important difference is that doc-
ument selection requires that all of the instances in
the batch must come from the same document. Thus,
one might expect a priori that document selection
for active learning will not perform as well as in-
stance selection. However, it is possible that even
smaller gains will be valuable for improving annota-
tion time, and the more robust nature of a corpus an-
notated in such a way will make the long term bene-
fits worthwhile.
In this work, we propose several metrics for se-
lecting documents to annotate, all of which are
based on instance level uncertainty. In the fol-
lowing descriptions, D is the set of documents, d
is a single document, d? is the selected document,
Instances(d) is a function which returns the set of
pair instances in document d, i is an instance, dist(i)
is a function which returns the distance of instance i
from the classification boundary, and I is the indica-
tor function, which takes the value 1 if its argument
is true and 0 otherwise. Note that high uncertainty
occurs when Abs(dist(i)) approaches 0.
? Best instance ? This method uses the un-
certainty sampling criteria on instances, and
selects the document containing the in-
stance the classifier is least certain about.
d? = argmin
d?D
[mini?Instances(d)Abs(dist(i))]
? Highest average uncertainty ? This method
computes the average uncertainty of all
instances in a document, and selects the
document with the highest average uncertainty.
d? = argmin
d?D
1
|Instances(d)|
?
i?Instances(d)Abs(dist(i))
? Least bad example ? This method uses
uncertainty sampling criteria to find the
document whose most certain example is
least certain, in other words the document
whose most useless example is least useless.
d? = argmin
d?D
maxi?Instances(d)Abs(dist(i))
? Narrow band ? This method creates an un-
certainty band around the discriminating
boundary and selects the document with
the most examples inside that narrow band.
d? = argmax
d?D
?
i?Instances(d) I(Abs(dist(i) < 0.2))
? Smallest spread ? This method computes the
distance between the least certain and most
certain instances and selects the document
minimizing that distance.
75
d? = argmin
d?D
[maxi?Instances(d)(Abs(dist(i)))?
mini?Instances(d)(Abs(dist(i)))]
? Most positives ? This method totals the
number of positive predicted instances
in each document and selects the doc-
ument with the most positive instances.
d? = argmax
d?D
?
i?Instances(d) I(dist(i) > 0)
? Positive ratio ? This method calculates
the percentage of positive predicted in-
stances in each document and selects the
document with the highest percentage.
d? = argmax
d?D
?
i?Instances(d) I(dist(i)>0)
|Instances(d)|
Many of these are straightforward adaptations of
the instance uncertainty criteria, but others deserve
a bit more explanation. The most positives and pos-
itive ratio metrics are based on the observation that
the corpus is somewhat imbalanced ? for every posi-
tive instance there are roughly 20 negative instances.
These metrics try to account for the possibility that
instance selection focuses on positive instances. The
average uncertainty is an obvious attempt to turn in-
stance metrics into document metrics, but narrow
band and smallest spread metrics attempt to do the
same thing while accounting for skew in the distri-
bution of ?good? and ?bad? instances.
3.3 Document-Inertial Instance Selection
One of the biggest impracticalities of instance se-
lection is that labeling any given instance may re-
quire reading a fair amount of the document, since
the antecedent and anaphor can be quite far apart.
Thus, any time savings accumulated by only anno-
tating an instance is reduced since the reading time
per instance is probably increased.
It is also possible that document selection goes
too far in the other direction, and requires too
many useless instances to be annotated to achieve
gains. Therefore, we propose a hybrid method of
document-inertial instance selection which attempts
to combine aspects of instance selection and docu-
ment selection.
This method uses instance selection criteria to se-
lect new instances, but will look inside the current
document for a new instance within an uncertainty
threshold rather than selecting the most uncertain in-
stance in the entire training set. Sticking with the
same document for several instances in a row can
potentially solve the real world annotation problem
that marking up each instance requires some knowl-
edge of the document context. Instead, the context
learned by selecting one instance can be retained if
useful for annotating the next selected instance from
the same document.
This also preserves one of the biggest advantages
of instance selection, that of re-training the model
after every selected instance. In batch-mode selec-
tion and document selection, many instances are se-
lected according to criteria based on the same model
starting point. As a result, the selected instances
may be redundant and document scores based on
accumulated instance scores may not reflect reality.
Re-training the model between selected instances
prevents redundant instances from being selected.
4 Evaluation
Evaluations of the active learning models described
above took place in a simulation context. In active
learning simulations, a labeled data set is used, and
the unlabeled pool is simulated by ignoring or ?cov-
ering? the labels for part of the data until the selec-
tion algorithm selects a new instance for annotation.
After selection the next data point is simply put into
the training data and its label is uncovered.
The data set used was the Ontology Development
and Information Extraction (ODIE) corpus (Savova
et al, 2011) used in the 2011 i2b2/VA Challenge on
coreference resolution.3 We used a set of 64 docu-
ments from the training set of the Mayo Clinic notes
for our simulations.
Instances were created by using the training
pipeline from the coreference system described in
Section 2.1. As previously mentioned, this work
uses the named entity anaphor classifier as it con-
tains the most data points. This training set resulted
in 6820 instances, with 311 positive instances and
6509 negative instances. Baseline ten-fold cross val-
idation performance on this data set using an SVM
with RBF kernel is an F-score of 0.48.
Simulations are performed using ten fold cross-
validation. First, each data point is assigned to one
3https://www.i2b2.org/NLP/Coreference/
76
of ten folds (this is done randomly to avoid any auto-
correlation issues). Then, for each iteration, one fold
is made the seed data, another fold is the validation
data, and the remainder are the unlabeled pool. Ini-
tially the labeled training data contains only the seed
data set. The model is trained on the labeled train-
ing data, tested on the validation set, then used to
select the next data point from the pool data set. The
selected data point is then removed from the pool
and added to the training data with its gold stan-
dard label(s), and the process repeats until the pool
of unlabeled data is empty. Performance is averaged
across folds to minimize the effects of randomness
in seed and validation set selection. Typically, active
learning is compared to a baseline of passive learn-
ing where the next data point to be labeled is selected
from the unlabeled pool data set randomly.
4.1 Instance Selection Experiments
Instance selection simulations follow the general
template above, with each instance (representing
a putative antecedent-anaphor pair) randomly as-
signed to a fold. After scoring on the validation set,
uncertainty sampling is used to select a single in-
stance from the unlabeled pool, and that instance is
added to the training set.
Figure 1 shows the results of active learning using
uncertainty selection on instances versus using pas-
sive learning (random selection). This makes it clear
that if the classifier is allowed to choose the data, top
performance can be achieved much faster than if the
data is presented in random order. Specifically, the
performance for uncertainty selection levels off at
around 500 instances into the active learning, out of
a pool set of around 5500 instances. In contrast, the
passive learning baseline takes basically the entire
dataset to reach the same performance.
This is essentially a proof of concept that there is
such a thing as a ?better? or ?worse? instance when
it comes to training a classifier for coreference. We
take this as a validation for attempting a document
selection experiment, with many metrics using in-
stance uncertainty as a building block.
4.2 Document Selection Experiments
Document selection follows similarly to the instance
selection above. The main difference is that instead
of assigning pair vectors to folds, we assign docu-
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Active vs. Passive Learning on Pairwise Named Entity Coreference
Number of instances
F?
sc
or
e
 
 
Random (Passive)
Uncertainty Sampling
Figure 1: Instance selection simulation results. The x-
axis is number of instances and the y-axis is ten-fold av-
eraged f-score of the pairwise named entity classifier.
ments to folds. To make a selection, each instance is
labeled according to the model, document level met-
rics described in Section 3.2 are computed per docu-
ment, and the document is selected which optimizes
the metric being evaluated. All of that document?s
instances and labels are added to the training data,
and the process repeats as before.
The results of these experiments are divided into
two plots for visual clarity. Figure 2 shows the
results of these experiments, roughly divided into
those that work as well as a random baseline (left)
and those that seem to work worse than a random
baseline (right). The best performing metrics (on
the left side of the figure) are Positive Ratio, Least
Worst,Highest Average, and Narrow Band, although
none of these performs noticeably better than ran-
dom. The remaining metrics (on the right) seem
to do worse than random, taking more instances to
reach the peak performance near the end.
The performance of document selection suggests
that it may not be a viable means of active learn-
ing. This may be due to a model of data distribution
in which useful instances are distributed very uni-
formly throughout the corpus. In this case, an aver-
age document will only have 8?10 useful instances
and many times as many that are not useful.
This was investigated by follow-up experiments
on the instance selection which kept track of which
77
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Least worst
Highest average
Pos/neg ratio
Narrow Band
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Best example
Most positives
Smallest spread
Figure 2: Two sets of document selection experiments.
document each instance came from. The experi-
ments tracked the first 500 instances only, which is
roughly the number of instances shown in Figure 1
to reach peak performance. Figure 3 (left) shows
a histogram with document indices on the x-axis
and normalized instance counts on the y-axis. The
counts are normalized by total number of document
vectors. In other words, we wanted to show whether
there was a distinction between ?good? documents
containing lots of good instances and ?bad? docu-
ments with few good instances.
The figure shows a few spikes, but most docu-
ments have approximately 10% of their instances
sampled, and all but one document has at least one
instance selected. Further investigation shows that
the spikes in the figure are from shorter documents.
Since shorter documents have few instances overall
but always at least one positive instance, they will be
biased to have a higher ratio of positive to negative
instances. If positive instances are more uncertain
(which may be the case due to the class imbalance),
then shorter documents will have more selected in-
stances per unit length.
We performed another follow-up experiment
along these lines using the histogram as a measure
of document value. In this experiment, we took the
normalized histogram, selected documents from it in
order of normalized number of items selected, and
used that as a document selection technique. Ob-
viously this would be ?cheating? if used as a metric
for document selection, but it can serve as a check on
the viability of document selection. If the results are
better than passive document selection, then there is
some hope that a document level metric based on the
uncertainty of its instances can be successful.
In fact, the right plot on Figure 3 shows that the
?cheating? method of document selection still does
not look any better than random document selection.
4.3 Document-Inertial Instance Selection
Experiments
The experiments for document-inertial instance se-
lection were patterned after the instance selection
paradigm. However, each instance was bundled with
metadata representing the document from which it
came. In the first selection, the algorithm selects the
most uncertain instance, and the document it comes
from is recorded. For subsequent selections, the
document which contained the previously selected
instance is given priority when looking for a new
instance. Specifically, each instance in that docu-
ment is classified, and the confidence is compared
against a threshold. If the document contains in-
stances meeting the threshold, the most uncertain in-
stance was selected. After each instance, the model
is retrained as in normal instance selection, and the
new model is used in the next iteration of the selec-
tion algorithm. For these experiments, the threshold
is set at 0.75, where the distance between the classi-
fication boundary and the margin is 1.0.
Figure 4 shows the performance of this algorithm
compared to passive and uncertainty sampling. Per-
78
0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Normalized document selection counts
Document index
%
 o
f v
ec
to
rs
 s
el
ec
te
d
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Cheating
Figure 3: Left: Percentage of instances selected from each document. Right: Performance of a document selection
algorithm that can ?cheat? and select the document with the highest proportion of good instances.
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Active vs. Passive Learning on Pairwise Named Entity Coreference
Number of instances
F?
sc
or
e
 
 
Random (Passive)
Uncertainty Sampling
Sticky Instance Sampling
Figure 4: Document-inertial instance selection results.
formance using this algorithm is clearly better than
passive learning and is similar to standard uncer-
tainty selection ignoring document constraints.
5 Discussion and Conclusion
The results of these experiments paint a complex
picture of the way active learning works for this do-
main and model combination. The first experiments
with uncertainty selection indicate that the number
of instances required to achieve classifier perfor-
mance can be compressed. Selecting and training
on all the good instances first leads to much faster
convergence to the asymptotic performance of the
classifier given the features and data set.
Attempting to extend this result to document se-
lection met with mediocre results. Even the best per-
forming of seven attempted algorithms seems to be
about the same as random document selection. One
can interpret these results in different ways.
The most pessimistic interpretation is that docu-
ment selection simply requires too many useless in-
stances to be annotated, good instances are spread
too evenly, and so document selection will never be
meaningfully faster than random selection. This in-
terpretation seems to be supported by experiments
showing that even if document selection uses a
?cheating? algorithm to select the documents with
the highest proportion of good instances it still does
not beat a passive baseline.
One can also interpret these results to inspire fur-
ther work, first by noting that all of the selection
techniques attempt to build on the instance selec-
tion metrics. While our document selection metrics
were more sophisticated than simply taking the n-
best instances, Settles (2010) notes that some suc-
cessful batch mode techniques explicitly account for
diversity in the selections, which we do not. In ad-
dition, one could argue that our experiments were
unduly constrained by the small number of docu-
ments available in the unlabeled pool, and that with
a larger unlabeled pool, one would eventually en-
counter documents with many good instances. This
may be true, but may be difficult in practice as clin-
ical notes often need to be manually de-identified
79
before any research use, and so it is not simply a
matter of querying all records in an entire electronic
medical record system.
The document-inertial instance selection showed
that the increase in training speed can be main-
tained without switching documents for every in-
stance. This suggests that while good training in-
stances may be uniformly distributed, it is usually
possible to find multiple good enough instances in
the current document, and they can be found despite
not selecting instances in the exact best order that
plain instance selection would suggest.
Future work is mainly concerned with real world
applicability. Document level active learning can
probably be ruled out as being non-beneficial despite
being the easiest to work into annotation work flows.
Instance level selection is very efficient in achieving
classifier performance but the least practical.
Document-inertial seems to provide some com-
promise. It does not completely solve the prob-
lems of instance selection, however, as annotation
will still not be complete if done exactly as simu-
lated here. In addition, the assumption of savings
is based on a model that each instance takes a con-
stant amount of time to annotate. This assumption is
probably true for tasks like word sense disambigua-
tion, where an annotator can be presented one in-
stance at a time with little context. However, a better
model of annotation for tasks like coreference is that
there is a constant amount of time required for read-
ing and understanding the context of a document,
then a constant amount of time on top of that per
instance.While modeling annotation time may pro-
vide some insight, it will probably be most effective
to undertake empirical annotation experiments to in-
vestigate whether document-inertial instance selec-
tion actually provides a valuable time savings.
The final discussion point is that of producing
complete document annotations. For coreference
systems following the pairwise discriminative ap-
proach as in that described in Section 2.1, a corpus
annotated instance by instance is useful. However,
many recent approaches do some form of document-
level clustering or explicit coreference chain build-
ing, and are not natively able to handle incompletely
annotated documents.4
4Other recent unsupervised graphical model approaches us-
Future work will investigate this issue by quan-
tifying the value of complete gold standard annota-
tions versus the partial annotations that may be pro-
duced using document-inertial instance selection.
One way of doing this is in simulation, by training
a model on the 500 good instances that document-
inertial instance selection selects, and then classify-
ing the rest of the training instances using that model
to create a ?diluted? gold standard. Then, a model
trained on the diluted gold standard will be used
to classify the validation set and performance com-
pared to the version trained on the full gold standard
corpus. Similar experiments can be performed using
other systems. The logic here is that if an instance
was not in the top 10% of difficult instances it can be
classified with high certainty. The fact that positive
instances are rare and tend to be most uncertain is a
point in favor of this approach ? after all, high accu-
racy can be obtained by guessing in favor of negative
once the positive instances are labeled. On the other
hand, if document-inertial instance selection simply
amounts to labeling of positive instances, it may not
result in substantial time savings.
In conclusion, this work has shown that instance
selection works for coreference resolution, intro-
duced several metrics for document selection, and
proposed a hybrid selection approach that preserves
the benefits of instance selection while offering the
potential of being applicable to real annotation. This
work can benefit the natural language processing
community by providing practical methods for in-
creasing the speed of coreference annotation.
Acknowledgments
The project described was supported by award
number NLM RC1LM010608, the Strategic Health
IT Advanced Research Projects (SHARP) Program
(90TR002) administered by the Office of the Na-
tional Coordinator for Health Information Technol-
ogy, and Integrating Informatics and Biology to the
Bedside (i2b2) NCBO U54LM008748. The content
is solely the responsibility of the authors and does
not necessarily represent the official views of the
NLM/NIH/ONC.
ing Gibbs sampling (Haghighi and Klein, 2007) may be able to
incorporate partially annotated documents in semi-supervised
training.
80
References
Caroline Gasperin. 2009. Active learning for anaphora
resolution. In Proceedings of the NAACL HLT Work-
shop on Active Learning for Natural Language Pro-
cessing, pages 1?8.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855.
David D. Lewis andWilliam A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 3?12.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL).
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27.
Guergana K. Savova, James J. Masanz, Philip V. Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-
Schuler, and Christopher G. Chute. 2010. Mayo
clinical text analysis and knowledge extraction sys-
tem (cTAKES): architecture, component evaluation
and applications. J Am Med Inform Assoc, 17(5):507?
513.
Guergana K. Savova, Wendy W. Chapman, Jiaping
Zheng, and Rebecca S. Crowley. 2011. Anaphoric
relations in the clinical narrative: corpus creation. J
Am Med Inform Assoc, 18:459?465.
A.I. Schein and L.H. Ungar. 2007. Active learning for
logistic regression: an evaluation. Machine Learning,
68(3):235?265.
B. Settles, M. Craven, and S. Ray. 2008. Multiple-
instance active learning. Advances in Neural Informa-
tion Processing Systems (NIPS), 20:1289?1296.
Burr Settles. 2010. Active learning literature survey.
Technical report, University of Wisconsin?Madison.
Katrin Tomanek, Florian Laws, Udo Hahn, and Hinrich
Schu?tze. 2009. On proper unit selection in active
learning: co-selection effects for named entity recog-
nition. In HLT ?09: Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 9?17, Morristown, NJ, USA.
Association for Computational Linguistics.
S. Tong and D. Koller. 2002. Support vector machine
active learning with applications to text classification.
The Journal of Machine Learning Research, 2:45?66.
Jiaping Zheng, Wendy Webber Chapman, Rebecca S.
Crowley, and Guergana K. Savova. 2011. Coreference
resolution: A review of general methodologies and ap-
plications in the clinical domain. Journal of Biomedi-
cal Informatics, 44:1113?1122.
Jiaping Zheng, Wendy W Chapman, Timothy A Miller,
Chen Lin, Rebecca S Crowley, and Guergana K
Savova. 2012. A system for coreference resolution for
the clinical narrative. Journal of the American Medi-
cal Informatics Association.
81
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 18?26,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discovering Narrative Containers in Clinical Text
Timothy A. Miller1, Steven Bethard2, Dmitriy Dligach1,
Sameer Pradhan1, Chen Lin1, and Guergana K. Savova1
1 Children?s Hospital Informatics Program, Boston Children?s Hospital and Harvard Medical School
firstname.lastname@childrens.harvard.edu
2 Center for Computational Language and Education Research, University of Colorado Boulder
steven.bethard@colorado.edu
Abstract
The clinical narrative contains a great deal
of valuable information that is only under-
standable in a temporal context. Events,
time expressions, and temporal relations
convey information about the time course
of a patient?s clinical record that must be
understood for many applications of inter-
est. In this paper, we focus on extracting
information about how time expressions
and events are related by narrative con-
tainers. We use support vector machines
with composite kernels, which allows for
integrating standard feature kernels with
tree kernels for representing structured
features such as constituency trees. Our
experiments show that using tree kernels
in addition to standard feature kernels im-
proves F1 classification for this task.
1 Introduction
Clinical narratives are a rich source of unstruc-
tured information that hold great potential for im-
pacting clinical research and clinical care. These
narratives consist of unstructured natural language
descriptions of various stages of clinical care,
which makes them information dense but chal-
lenging to use computationally. Information ex-
tracted from these narratives is already being used
for clinical research tasks such as automatic phe-
notype classification for collecting disease cohorts
retrospectively (Ananthakrishnan et al, 2013),
which can in turn be used for a variety of studies,
including pharmacogenomics (Lin et al, 2012;
Wilke et al, 2011). Future applications may use
information extracted from the clinical narrative at
the point of care to assist physicians in decision-
making in a real time fashion.
One of the most interesting and challenging as-
pects of clinical text is the pervasiveness of tempo-
rally grounded information. This includes a num-
ber of clinical concepts which are events with fi-
nite time spans (e.g., surgery or x-ray), time ex-
pressions (December, postoperatively), and links
that relate events to times or other events. For ex-
ample, surgery last May relates the time last May
with the event surgery via the CONTAINS relation,
while Vicodin after surgery relates the medication
event Vicodin with the procedure event surgery via
the AFTER relation. There are many potential ap-
plications of clinical information extraction that
are only possible with an understanding of the or-
dering and duration of the events in a clinical en-
counter.
In this work we focus on extracting a particu-
lar temporal relation, CONTAINS, that holds be-
tween a time expression and an event expression.
This level of representation is based on the compu-
tational discourse model of narrative containers
(Pustejovsky and Stubbs, 2011), which are time
expressions or events which are central to a sec-
tion of a text, usually manifested by being rela-
tive hubs of temporal relation links. We argue that
containment relations are useful as an intermediate
level of granularity between full temporal relation
extraction and ?coarse? temporal bins (Raghavan
et al, 2012) like before admission, on admission,
and after admission. Correctly extracting CON-
TAINS relations will, for example, allow for more
accurate placement of events on a timeline, to
the resolution possible by the number of time ex-
pressions in the document. We suspect that this
finer grained information will also be more useful
for downstream applications like coreference, for
which coarse information was found to be useful.
The approach we develop is a supervised machine
18
learning approach in which pairs of time expres-
sions and events are classified as CONTAINS or
not. The specific approach is a support vector ma-
chine using both standard feature kernels and tree
kernels, a novel approach to this problem in this
domain that has shown promise on other relation
extraction tasks.
This work makes use of a new corpus we devel-
oped as part of the THYME1 project (Temporal
History of Your Medical Events) focusing on tem-
poral events and relations in clinical text. This cor-
pus consists of clinical and pathology notes on col-
orectal cancer from Mayo Clinic. Gold standard
annotations include Penn Treebank-style phrase
structure in addition to clinically relevant temporal
annotations like clinical events, temporal expres-
sions, and various temporal relations.
2 Background and Related Work
2.1 Annotation Methodology
The THYME annotation guidelines2 detail the ex-
tension of TimeML (Pustejovsky et al, 2003b)
to the annotations of events, temporal expres-
sions and temporal relations in the clinical do-
main. In summary, an EVENT is anything that is
relevant to the clinical timeline. Temporal expres-
sions (TIMEX3s) in the clinical domain are simi-
lar to those in the general domain with two excep-
tions. First, TimeML sets and frequencies occur
much more often in the clinical domain, especially
with regard to medications and treatments (Clar-
itin 30mg twice daily). The second deviation is a
new type of TIMEX3 ? PREPOSTEXP which covers
temporally complex terms like preoperative, post-
operative, and intraoperative.
EVENTs and TIMEX3s are ordered on a timeline
through temporal TLINKs which range from fairly
coarse (the relation to document time creation) to
fairly granular (the explicit pairwise TLINKs be-
tween EVENTs and/or TIMEX3s). Of note for this
work, the CONTAINS relation between a TIMEX3
and an EVENT means that the span of the EVENT
is completely within the span of the TIMEX3. The
interannotator agreement F1-score for CONTAINS
for the set of documents used here was 0.60.
2.2 Narrative Containers
One relatively new concept for marking temporal
relations is that of narrative containers, as in Puste-
1http://clear.colorado.edu/TemporalWiki
2Annotation guidelines are posted on the THYME wiki.
jovsky and Stubbs (2011). Narrative containers
are time spans which are central to the discourse
and often subsume multiple events and time ex-
pressions. They are often anchored by a time ex-
pression, though more abstract events may also act
as anchors. Using the narrative container frame-
work significantly reduces the number of explicit
TLINK annotations yet retains a relevant degree of
granularity enabling inferencing.
Consider the following clinical text example
with DocTime of February 8.
The patient recovered well after her ini-
tial first surgery on December 16th to
remove the adenocarcinoma, although
on the evening of January 3rd she was
admitted with a fever and treated with
antibiotics.
There are three narrative containers in this snip-
pet ? (1) the broad period leading up to the docu-
ment creation time which includes the events of re-
covered and adenocarcinoma, (2) December 16th,
which includes the events of surgery and remove,
and (3) January 3rd, which includes the events of
admitted, fever, and treated.
Using only the relation to the document creation
time would provide too coarse of a timeline result-
ing in collapsing the three narrative containers (the
coarse time bins of Raghavan et al (2012) would
collapse all events into the before admission cat-
egory). On the other hand, marking explicit links
between every pair of events and temporal expres-
sions would be tedious and redundant. In this ex-
ample, there is no need to explicitly mark that, for
instance, fever was AFTER surgery, because we
know that the fever happened on January 3rd and
that the surgery happened on December 16th, and
that January 3rd is AFTER December 16th. With
the grouping of EVENTs in this way, we can infer
the links between them and reduce annotator ef-
fort. Narrative containers strike the right balance
between parsimony and expressiveness.
2.3 Related Work
Of course, the possibility of annotating temporal
containment relations was allowed by even the ear-
liest versions of the TimeML specification using
TLINKs with the relation type INCLUDES. How-
ever, TimeML is a specification not a guideline,
and as such, the way in which temporal relations
have been annotated has varied widely and no
19
corpus has previously been annotated with narra-
tive containers in mind. In the TimeBank corpus
(Pustejovsky et al, 2003a), annotators annotated
only a sparse, mostly disconnected graph of the
temporal relations that seemed salient to them. In
TempEval 2007 and 2010 (Verhagen et al, 2007;
Verhagen et al, 2010), annotators annotated only
relations in specific constructions ? e.g. all pairs
of events and times in a sentence ? and used a re-
stricted set of relation types that excluded the IN-
CLUDES relation. TempEval 2013 (UzZaman et
al., 2013) allowed INCLUDES relations, but again
only in particular constructions or when the rela-
tion seemed salient to the annotators. The 2012
i2b2 Challenge3, which provided TimeML anno-
tations on clinical data, annotated the INCLUDES
relation, but merged it with other relations for the
evaluation due to low inter-annotator agreement.
Since no narrative container-annotated corpora
exist, there are also no existing models for extract-
ing narrative container relations. However, we
can draw on the various methods applied to re-
lated temporal relation tasks. Most relevant is the
work on linking events to timestamps. This was
one of the subtasks in TempEval 2007 and 2010,
and systems used a variety of features including
words, part-of-speech tags, and the syntactic path
between the event and the time (Bethard and Mar-
tin, 2007; Llorens et al, 2010). Syntactic path
features were also used in the 2012 i2b2 Chal-
lenge, where they provided gains especially for
intra-sentential temporal links (Xu et al, 2013).
Recent research has also looked to syntac-
tic tree kernels for temporal relation extraction.
Mirroshandel et al (2009) used a path-enclosed
tree (i.e., selecting only the sub-tree containing
the event and time), and used various weighting
scheme variants of this approach on the Time-
Bank (Pustejovsky et al, 2003a) and Opinion4
corpora. Hovy et al (2012) used a flat tree struc-
ture for each event-time pair, including only token-
based information (words, part of speech tags) be-
tween the event and time, and found that adding
such tree kernels on top of a baseline set of fea-
tures improved event-time linking performance on
the TempEval 2007 and Machine Reading cor-
pora (Strassel et al, 2010). While Mirroshandel et
al. saw improvements using a representation with
syntactic structure, Hovy et al used the flat tree
3http://i2b2.org/NLP/TemporalRelations
4Also known as the AQUAINT TimeML corpus ?
http://www.timeml.org
structure because they found that ?using a full-
parse syntactic tree as input representation did not
help performance.? Thus, it remains an open ques-
tion exactly where and when syntactic tree kernels
will help temporal relation extraction.
3 Methods
Inspired by this prior work, we treat the narrative
container extraction task as a within-sentence rela-
tion extraction task between time and event men-
tions. For each sentence, this approach iterates
over every gold standard annotated EVENT, pair-
ing it with each TIMEX3 in the sentence, and uses
a supervised machine learning algorithm to clas-
sify each pair as related by the CONTAINS relation
or not. Training examples are generated in the
same way, with pairs corresponding to annotated
links marked as positive examples and all others
marked as negative. We investigate a variety of
features for the classifier as well as a variety of
tree kernel combinations.
This straightforward approach does not address
all relation pairs, setting aside event-event rela-
tions and inter-sentential relations, which are both
likely to require different approaches.
3.1 SVM with Tree Kernels
The machine learning approach we use is support
vector machine (SVM) with standard feature ker-
nels, tree kernels, and composite kernels that com-
bine the two. SVMs are used extensively for clas-
sification tasks in natural language processing, due
to robust performance and widely available soft-
ware packages. We take advantage of the ability
in SVMs to represent structured features such as
trees using convolution kernels (Collins and Duffy,
2001), also known as tree kernels. This kernel
computes similarity between two tree structures
by computing the number of common sub-trees,
with a weight parameter to discount the influence
of larger structural similarities. The specific for-
malism we use is sometimes called a subset tree
kernel (Moschitti, 2006), which checks for simi-
larity on subtrees of all sizes, as long as each sub-
tree has its production rule completely expanded.
A useful property of kernels is that a linear com-
bination of two kernels is guaranteed to be a ker-
nel (Cristianini and Shawe-Taylor, 2000). In ad-
dition, the product of two kernels is also a ker-
nel. This means that it is simple to combine tradi-
tional feature-based kernels used in SVMs (linear,
20
polynomial, radial basis function) with tree ker-
nels representing structural information. This ap-
proach of using composite kernels has been widely
used in the task of relation extraction where syn-
tactic information is presumed to be useful, but is
hard to represent as traditional numeric features.
We investigate a few different composite ker-
nels here, including a linear combination:
KC(o1, o2) = ? ?KT (t1, t2) +KF (f1, f2) (1)
where a composite kernel KC operates on objects
oj composed of features fj and tree tj , by adding
a tree kernel KT weighted by ? to a feature kernel
KF . We also use a composite kernel that takes the
product of kernels:
KC(o1, o2) = KT (t1, t2) ?KF (f1, f2) (2)
Sometimes it is beneficial to make use of multi-
ple syntactic ?views? of the same instance. Below
we will describe many different tree representa-
tions, and the tree kernel framework allows them
to all be used simultaneously, by simply summing
the similarities of the different representations and
taking the combined sum as the tree kernel value:
KT ({t
1
1, t
2
1 . . . , t
N
1 }, {t
1
2, t
2
2, . . . , t
N
2 }) =
N?
i=1
KT (t
i
1, t
i
2) (3)
where i indexes the N different tree views. In all
kernel combinations we compute the normalized
version of both the feature and tree kernels so that
they can be combined on an even footing.
The actual implementations we use for train-
ing are the SVM-LIGHT-TK package (Mos-
chitti, 2006), which is a tree kernel extension to
SVMlight (Joachims, 1999). At test time, we
use the SVM-LIGHT-TK bindings of the ClearTK
toolkit (Ogren et al, 2009) in a module built on
top of Apache cTAKES (Savova et al, 2010), to
take advantage of the pre-processing stages.
3.2 Flat Features
The flat features developed for the standard fea-
ture kernel include the text of each argument as a
whole, the tokens of each argument represented as
a bag of words, the first and last word of each ar-
gument, and the preceding and following words of
each argument as bags of words. The token con-
text between arguments is also represented using
the text span as a whole, the first and last words,
the set of words represented as a bag of words, and
the distance between the arguments. In addition,
part of speech (POS) tag features are extracted for
each mention, with separate bag of POS tag fea-
tures for each argument. The POS features are
generated by the cTAKES POS tagger.
We also include semantic features of each argu-
ment. For event mentions, we include a feature
marking the contextual modality, which can take
on the possible values Actual, Hedged, Hypothet-
ical, or Generic, which is part of the gold stan-
dard annotations. This feature was included as it
was presumed that actual events are more likely
to have definite time spans, and thus be related
to times, than hypothetical or generic mentions of
events. For time mentions we include a feature for
the time class, with possible values of Date, Time,
Duration, Quantifier, Set, or Prepostexp. The time
class feature was used as it was hypothesized that
dates and times are more likely to contain events
than sets (e.g., once a month).
3.3 Tree Kernel Representations
We leverage existing tree kernel representations
for this work, using some directly and others as
starting point to a domain-specific representation.
First, we take advantage of the (relatively) flat
structured tree kernel representations of Hovy et
al. (2012). This representation uses lexical items
such as POS tags rather than constituent struc-
ture, but places them into an ordered tree struc-
ture, which allows tree kernels to use them as a
bag of items while also taking advantage of order-
ing structure when it is useful. Figure 1 shows an
example tree for an event-time pair for which a re-
lation exists, where the lexical information used is
POS tag information for each term (the represen-
tation that Hovy et al found most useful). We also
used a version of this representation where the sur-
face form is used instead of the POS tag.
While Hovy et al showed positive results using
this representation over just standard features, it is
still somewhat constrained in its ability to repre-
sent long distance relations. This is because the
subset tree kernel compares only complete rule
productions, and with long distance relations a flat
tree structure will have a production that is too big
to learn. Alternatively, tree kernel representations
can be based on constituent structure, as is com-
mon in the relation extraction literature. This will
21
BOP
Event-Actual
TOK
VBN
TOK
TO
TOK
VB
TOK
NN
Timex-Date
TOK
JJ
TOK
NN
BOW
Event-Actual
TOK
scheduled
TOK
to
TOK
undergo
TOK
surgery
Timex-Date
TOK
next
TOK
week
Figure 1: Two trees indicating the flat tree kernel
representation. Above is the bag of POS tags ver-
sion; below is the bag of words version.
hopefully allow for the representation of longer
distance relations by taking advantage of syntactic
sub-structure with smaller productions. The rep-
resentations used here are known as Feature Trees
(FT), Path Trees (PT) and Path-Enclosed Trees
(PET).
The Feature Tree representation takes the en-
tire syntax tree for the sentence containing both
arguments and inserts semantic information about
those arguments. That information includes the ar-
gument type (EVENT or TIMEX) as an additional
tree node above the constituent enclosing the argu-
ment. We also append semantic class information
to the argument (contextual modality for events,
time class for times), as in the flat features.
The Feature Tree representation is not com-
monly used, as it includes an entire sentence
around the arguments of interest, and that may in-
clude a great deal of unrelated structure that adds
noise to the classifier. Here we include it in an at-
tempt to get to the root of an apparent discrepancy
in the tree kernel literature, as explained in Sec-
tion 2, in which Hovy et al (2012) report a nega-
tive result and Mirroshandel et al (2009) report a
positive result for using constituency structure in
tree kernels for temporal relation extraction.
The Path Tree representation uses a sub-tree of
the whole constituent tree, but removes all nodes
that are not along the path between the two argu-
ments. Path information has been used in standard
feature kernels (Pradhan et al, 2008), with each
individual path being a possible boolean feature.
VP
Arg1-Event-Actual
arg1
S
VP
VP
Arg2-Timex-Date
arg2
Figure 2: Path Tree (PT) representation
Another representation making use of the path tree
takes contiguous subsections of the path tree, or
?path n-grams,? in an attempt to combat the spar-
sity of using the whole path (Zheng et al, 2012).
By using the path representation with a tree ker-
nel, the model should get the benefit of all different
sizes of path n-grams, up to the size of the whole
path. This representation is augmented by adding
in argument nodes with event and time features, as
in the Feature Tree. Unlike the Feature Tree and
the PET below, the Path Tree representation does
not include word nodes, because the important as-
pect of this representation is the labels of the nodes
on the path between arguments. Figure 2 shows an
example of what this representation looks like.
The Path-Enclosed Tree representation is based
on the smallest sub-tree that encloses the two pro-
posed arguments. This is a representation that has
shown value in other work using tree kernels for
relation extraction (Zhang et al, 2006; Mirroshan-
del et al, 2009). The information contained in
the PET representation is a superset of that con-
tained in the Path Tree representation, since it in-
cludes the full path between arguments as well
as the structure between arguments and the ar-
gument text. This means that it can take into
account path information while also considering
constituent structure between arguments that may
play a role in determining whether the two ar-
guments are related. For example, temporal cue
words like after or during may occur between ar-
guments and will not be captured by Path Trees.
Like the PT representation, the PET representa-
tion is augmented with the semantic information
specified above in the Feature Tree representation.
Figure 3 shows an example of this representation.
22
VP
Arg1-Event-Actual
VBN
scheduled
VP
TO
to
VP
VB
undergo
NP
NN
surgery
Arg2-Timex-Date
NP
JJ
next
NN
week
Figure 3: Path-Enclosed Tree representation
4 Evaluation
The corpus we used for evaluations was described
in Section 2. There are 78 total notes in the corpus,
with three notes for each of 26 patients. The data
is split into training (50%), development (25%),
and test (25%) sections based on patient number,
so that each patient?s notes are all in the same
section. The combined training and development
set used for final training consists of 4378 sen-
tences with 49,050 tokens, and 7372 events, 849
time expressions, and 2287 CONTAINS relations.
There were 774 positive instances of CONTAINS
in the training data, with 1513 negative instances.
For constituent structure and features we use the
gold standard treebank and event and time features
from our corpus. Preliminary work suggests that
automatic parses from cTAKES do not harm per-
formance very much, but the focus of this work is
on the relation extraction so we use gold standard
parses. All preliminary experiments were done us-
ing the development set for testing.
We designed a set of experiments to exam-
ine several hypotheses regarding extraction of the
CONTAINS relation and the efficacy of different
tree kernel representations. The first two config-
urations test simple rule-based baseline systems,
CLOSEST-P and CLOSEST-R, for distance-related
decision rule systems meant to optimize precision
and recall, respectively. CLOSEST-P hypothesizes
a CONTAINS link between every TIMEX3 and the
closest annotated EVENT, which will make few
links overall. CLOSEST-R hypothesizes a CON-
TAINS link between every EVENT and the closest
TIMEX3, which will make many more links.
The next configuration, Flat Features, uses the
token and part of speech features along with ar-
gument semantics features, as described in Sec-
tion 3. While this feature set may not seem ex-
haustive, in preliminary work many traditional re-
lation extraction features were tried and found to
not have much effect. This particular configura-
tion was tested because it is most comparable to
the bag of word and bag of POS kernels from
Hovy et al (2012), and should help show whether
the tree kernel is providing anything over an equiv-
alent set of basic features.
We then examine several composite kernels, all
using the same feature kernel, but using different
tree kernel-based representations. First, we use a
composite kernel which uses the bag of word and
bag of POS tree views, as in Hovy et al (2012).
Next, we add in two additional tree views to the
tree kernel, Path-Enclosed Tree and Path Tree,
which are intended to examine the effect of using
traditional syntax, and the long distance features
that they enable. The final experimental config-
uration replaces the PET and PT representations
from the last configuration with the Feature Tree
representation. This tests the hypothesis that the
difference between positive results for tree kernels
in this task (as in, say, Mirroshandel et al (2009))
and negative results reported by Hovy et al (2012)
is the difference between using a full-parse tree
and using standard sub-tree representations.
For the rule-based systems, there are no param-
eters to tune. Our machine-learning systems are
based on support vector machines (SVM), which
require tuning of several parameters, including
kernel type (linear, polynomial, and radial basis
function), the parameters for each kernel, and c,
the cost of misclassification. Tree kernels intro-
duce an additional parameter ? for weighting large
structures, and the use of a composite kernel in-
troduces parameters for which kernel combination
operator to use, and how to weight the different
kernels for the sum operator.
For each machine learning configuration, we
performed a large grid search over the combined
parameter space, where we trained on the train-
ing set and tested on the development set. For
the final experiments, the parameters were chosen
that optimized the F1 score on the development
set. Qualitatively, the parameter tuning strongly
favored configurations which combined the ker-
nels using the sum operator, and recall and pre-
cision were strongly correlated with the SVM pa-
rameter c. Using these parameters, we then trained
23
on the combined training and development sets
and tested on the official test set.
4.1 Evaluation Metrics
The state of evaluating temporal relations has been
evolving over the past decade. This is partially
due to the inferential properties of temporal rela-
tions, because it is possible to define the same set
of relations using different set of axioms. To take
a very simple example, given a gold set of rela-
tions A<B and B<C, and given the system output
A<B, A<C and B<C, if one were to compute a
plain precision/recall metric, then the axiom A<C
would be counted against the system, when one
can easily infer from the gold set of relations that
it is indeed correct. With more relations the infer-
ence process becomes more complex.
Recently there has been some work trying
to address the shortcomings of the plain F1
score (Muller and Tannier, 2004; Setzer et al,
2006; UzZaman and Allen, 2011; Tannier and
Muller, 2008; Tannier and Muller, 2011). How-
ever, the community has not yet come to a consen-
sus on the best evaluation approach. Two recent
evaluations, TempEval-3 (UzZaman et al, 2013)
and the 2012 i2b2 Challenge (Sun et al, 2013),
used an implementation of the proposal by (Uz-
Zaman and Allen, 2011). However, as described
in Cherry et al (2013), this algorithm, which uses
a greedy graph minimization approach, is sensi-
tive to the order in which the temporal relations
are presented to the scorer. In addition, the scorer
is not able to give credit for non-redundant, non-
minimum links (Cherry et al, 2013) as with the
the case of the relation A<C mentioned earlier.
Considering that the measures for evaluating
temporal relations are still evolving, we decided to
use plain F-score, with recall and precision scores
also reported. This score is computed across all
intra-sentential EVENT-TIMEX3 pairs in the gold
standard, where precision = # correct predictions# predictions ,
recall = # correct predictions# gold standard relations , and F1 score =
2?precision?recall
precision+recall .
4.2 Experimental Results
Results are shown in Table 1. Rule-based base-
lines perform reasonably well, but are heavily bi-
ased in terms of precision or recall. The ma-
chine learning baseline cannot even obtain the
same performance as the CLOSEST-R rule-based
system, though it is more balanced in terms of pre-
System Precision Recall F1
CLOSEST-P 0.754 0.537 0.627
CLOSEST-R 0.502 0.947 0.656
Flat Features (FF) 0.705 0.593 0.645
FF+Bag Trees (BT) 0.649 0.728 0.686
FF+BT+PET+PT 0.770 0.707 0.737
FF+BT+FT 0.691 0.691 0.691
Table 1: Table of results of main experiments.
cision and recall. Using a composite kernel which
adds in the flat token-based tree kernels improves
performance over the standard feature kernel by
4.1 points. Adding in the Path Tree and Path-
Enclosed Tree constituency-based trees along with
the flat trees improves F1 score to our best result
of 73.7. Finally, replacing PT and PET representa-
tions with the Feature Tree representation does not
offer any performance improvement over the Flat
Features + Bag Trees configuration.
4.3 Error Analysis
We performed error analysis on the outputs of the
best-performing system (FF+BT+PET+PT in Ta-
ble 1). First, we note that the parameter search
was optimized for F1. This resulted in the highest-
scoring configuration using a composite kernel
with the sum operator, polynomial kernel for the
secondary kernel, ? = 0.5, tree kernel weight (T )
of 0.1, and c = 10.0. This high value of c and low
value of T results in higher precision and lower
recall, but there were configurations with lower c
and higher T which made the opposite tradeoff,
with only marginally worse F1-score. For the pur-
poses of error analysis, however, this configuration
leads to a focus on false negatives.
First, the false positives contained many rela-
tions that were legitimately ambiguous or possible
annotator errors. An example ambiguous case is
She is currently being treated on the Surgical Ser-
vice for..., in which the system generates the re-
lation CONTAINS(currently, treated), but the gold
standard labels as OVERLAP. This example is am-
biguous because it is not clear from just the lin-
guistic context whether the treatment is wholly
contained in the small time window denoted by
currently, or whether it started a while ago or will
continue into the future. There are many similar
cases where the event is a disease/disorder type,
and the specific nature of the disease is impor-
tant to understanding whether this is a CONTAINS
24
or OVERLAP relation, specifically understanding
whether the disease is chronic or more acute.
Another source of false positives were where
the event and time were clearly related, but not
with CONTAINS. In the example reports that she
has been having intermittent bleeding since May
of 1998, the term since clearly indicates that this
is a BEGINS-ON relation between bleeding and
May of 1998. This is a case where having other
temporal relation classifiers may be useful, as they
can compete and the relation can be assigned to
whichever classifier is more confident.
False negatives frequently occurred in contexts
where the event and time were far apart. Syn-
tactic tree kernels were introduced to help im-
prove recall on longer-distance relations, and were
successful up to a limit. However, certain ex-
amples are so far apart that the algorithm may
have had difficulty sorting noise from important
structure. For example, the system did not find
the CONTAINS(October 27, 2010, oophorectomy)
relation in the sentence:
October 27, 2010, Dr. XXX performed
exploratory laparotomy with an trans-
verse colectomy and Dr. YYY performed
a total abdominal hysterectomy with a
bilateral salpingo-oophorectomy.
Here, while the date may be part of the same sen-
tence as the event, the syntactic relation between
the pair is not what makes the relation; the date is
acting as a kind of discourse marker that indicates
that the following events are contained. This sug-
gests that discourse-level features may be useful
even for the intra-sentential classification task.
Other false negatives occurred where there was
syntactic complexity, even on shorter examples.
The subset tree kernel used here matches com-
plete rule productions, and across complex struc-
ture with large productions, the chances of finding
similarity decreases substantially. Thus, events
within coordination or separated from the time by
clause breaks are more difficult to relate to the
time due to the multiple different ways of relating
these different syntactic elements.
Finally, there are some examples where the an-
chor of a narrative container is an event with mul-
tiple sub-events. In these cases, the system per-
forms well at relating a time expression to the an-
chor event, but may miss the sub-events that are
farther away. This is a case where having an event-
event TLINK classifier, then applying determinis-
tic closure rules, would allow a combined system
to link the sub-events to the time expression.
5 Discussion and Conclusion
In this paper we have developed a system for auto-
matically identifying CONTAINS relations in clini-
cal text. The experiments show first that a machine
learning approach that intelligently integrates con-
stituency information can greatly improve perfor-
mance over rule-based baselines. We also show
that the tree kernel approach, which can model se-
quence better than a bag of tokens-style approach,
is beneficial even when it uses the same features.
Finally, the experiments show that choosing the
correct representation is important for tree kernel
approaches, and specifically that using a full parse
tree may give inferior performance compared to
sub-trees focused on the structure of interest.
In general, there is much work to be done in the
area of representing temporal information in clin-
ical records. Many of the inputs to the algorithm
described in this paper need to be extracted auto-
matically, including time expressions and events.
Work on relations will focus on adding features
to represent discourse information and richer rep-
resentation of event semantics. Discourse infor-
mation may help with the longer-distance errors,
where the time expression acts almost as a topic
for an extended description of events. Better un-
derstanding of event semantics, such as whether
a disease is chronic or acute, or typical duration
for a treatment, may help constrain relations. In
addition, we will explore the effectiveness of us-
ing dependency tree structure, which has been use-
ful in the domain of extracting relations from the
biomedical literature (Tikk et al, 2013).
Acknowledgements
The work described was supported by Tempo-
ral History of Your Medical Events (THYME)
NLM R01LM010090 and Integrating Informat-
ics and Biology to the Bedside (i2b2) NCBO
U54LM008748. Thanks to the anonymous re-
viewers for thorough and insightful comments.
References
Naushad UzZaman, Hector Llorens, et al 2013. Semeval-
2013 task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Second Joint Confer-
ence on Lexical and Computational Semantics (*SEM),
25
Volume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Computa-
tional Linguistics.
Ashwin N Ananthakrishnan, Tianxi Cai, et al 2013. Improv-
ing case definition of Crohn?s disease and ulcerative col-
itis in electronic medical records using natural language
processing: a novel informatics approach. Inflammatory
bowel diseases.
Steven Bethard and James H. Martin. 2007. CU-TMP: Tem-
poral relation classification using syntactic and semantic
features. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007), pages
129?132.
Colin Cherry, Xiaodan Zhu, et al 2013. A la recherche du
temps perdu: extracting temporal relations from medical
text in the 2012 i2b2 NLP challenge. Journal of the Amer-
ican Medical Informatics Association, March.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Neural Information Processing
Systems.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Dirk Hovy, James Fan, et al 2012. When did that happen?:
linking events and relations to timestamps. In Proceedings
of the 13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 185?193.
Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learn-
ing. Universita?t Dortmund.
Chen Lin, Helena Canhao, et al 2012. Feature engineering
and selection for rheumatoid arthritis disease activity clas-
sification using electronic medical records. In Proceed-
ings of ICML Workshop on Machine Learning for Clinical
Data.
Hector Llorens, Estela Saquete, and Borja Navarro. 2010.
TIPSem (english and spanish): Evaluating crfs and seman-
tic roles in tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 284?291.
Association for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for clas-
sifying temporal relations between events. Proc. of the
PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Ma-
chine Learning: ECML 2006, pages 318?329. Springer.
Philippe Muller and Xavier Tannier. 2004. Annotating and
measuring temporal relations in texts. In Proceedings of
the 20th international conference on Computational Lin-
guistics, COLING ?04, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Philip V. Ogren, Philipp G. Wetzler, and Steven J. Bethard.
2009. ClearTK: a framework for statistical natural lan-
guage processing. In Unstructured Information Manage-
ment Architecture Workshop at the Conference of the Ger-
man Society for Computational Linguistics and Language
Technology, 9.
Sameer S Pradhan, Wayne Ward, and James H Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics, 34(2):289?310.
James Pustejovsky and Amber Stubbs. 2011. Increasing in-
formativeness in temporal annotation. In Proceedings of
the 5th Linguistic Annotation Workshop, pages 152?160.
James Pustejovsky, Patrick Hanks, et al 2003a. The time-
bank corpus. In Corpus linguistics, volume 2003, page 40.
James Pustejovsky, Jose? Casta no, et al 2003b. Timeml:
Robust specification of event and temporal expressions in
text. In Fifth International Workshop on Computational
Semantics (IWCS-5).
Preethi Raghavan, Eric Fosler-Lussier, and Albert M Lai.
2012. Temporal classification of medical events. In Pro-
ceedings of the 2012 Workshop on Biomedical Natural
Language Processing, pages 29?37. Association for Com-
putational Linguistics.
Guergana K. Savova, James J. Masanz, et al 2010. Mayo
clinical text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and appli-
cations. J Am Med Inform Assoc, 17(5):507?513.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple. 2006.
The role of inference in the temporal annotation and anal-
ysis of text. Language Resources and Evaluation, 39(2-
3):243?265, February.
Stephanie Strassel, Dan Adams, et al 2010. The DARPA
machine reading program - encouraging linguistic and rea-
soning research with a series of reading tasks. In Proceed-
ings of the Seventh International Conference on Language
Resources and Evaluation (LREC?10), Valletta, Malta.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informatics
Association, April.
Xavier Tannier and Philippe Muller. 2008. Evaluation met-
rics for automatic temporal annotation of texts. Proceed-
ings of the Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08).
Xavier Tannier and Philippe Muller. 2011. Evaluating tem-
poral graphs built from texts via transitive reduction. J.
Artif. Int. Res., 40(1):375413, January.
Domonkos Tikk, Ille?s Solt, et al 2013. A detailed error anal-
ysis of 13 kernel methods for protein-protein interaction
extraction. BMC bioinformatics, 14(1):12.
Naushad UzZaman and James Allen. 2011. Temporal eval-
uation. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Lan-
guage Technologies, page 351?356.
Marc Verhagen, Robert Gaizauskas, et al 2007. Semeval-
2007 task 15: Tempeval temporal relation identification.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 75?80.
Marc Verhagen, Roser Sauri, et al 2010. Semeval-2010 task
13: Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Uppsala,
Sweden, July. Association for Computational Linguistics.
RA Wilke, H Xu, et al 2011. The emerging role of electronic
medical records in pharmacogenomics. Clinical Pharma-
cology & Therapeutics, 89(3):379?386.
Yan Xu, Yining Wang, et al 2013. An end-to-end system to
identify temporal relation in discharge summaries: 2012
i2b2 challenge. Journal of the American Medical Infor-
matics Association : JAMIA.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring syn-
tactic features for relation extraction using a convolution
tree kernel. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of the
ACL, pages 288?295.
Jiaping Zheng, Wendy W Chapman, et al 2012. A sys-
tem for coreference resolution for the clinical narrative.
Journal of the American Medical Informatics Association,
19:660?667.
26
