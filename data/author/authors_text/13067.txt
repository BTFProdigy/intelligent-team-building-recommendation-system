Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 116?124,
Beijing, August 2010
	
  		   	
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1119?1128,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Function-based question classification for general QA
Fan Bu, Xingwei Zhu, Yu Hao and Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University
buf08@mails.tsinghua.edu.cn
etzhu192@hotmail.com
haoyu@mail.tsinghua.edu.cn
zxy-dcs@tsinghua.edu.cn
Abstract
In contrast with the booming increase of inter-
net data, state-of-art QA (question answering)
systems, otherwise, concerned data from spe-
cific domains or resources such as search en-
gine snippets, online forums and Wikipedia in
a somewhat isolated way. Users may welcome
a more general QA system for its capability
to answer questions of various sources, inte-
grated from existed specialized sub-QA en-
gines. In this framework, question classifica-
tion is the primary task.
However, the current paradigms of question
classification were focused on some speci-
fied type of questions, i.e. factoid questions,
which are inappropriate for the general QA.
In this paper, we propose a new question clas-
sification paradigm, which includes a ques-
tion taxonomy suitable to the general QA and
a question classifier based on MLN (Markov
logic network), where rule-based methods and
statistical methods are unified into a single
framework in a fuzzy discriminative learning
approach. Experiments show that our method
outperforms traditional question classification
approaches.
1 Introduction
During a long period of time, researches on question
answering are mainly focused on finding short and
concise answers from plain text for factoid questions
driven by annual trackes such as CLEF, TREC and
NTCIR. However, people usually ask more complex
questions in real world which cannot be handled by
these QA systems tailored to factoid questions.
During recent years, social collaborative applica-
tions begin to flourish, such asWikipedia, Facebook,
Yahoo! Answers and etc. A large amount of semi-
structured data, which has been accumulated from
these services, becomes new sources for question
answering. Previous researches show that different
sources are suitable for answering different ques-
tions. For example, the answers for factoid questions
can be extracted from webpages with high accuracy,
definition questions can be answered by correspond-
ing articles in wikipedia(Ye et al, 2009) while com-
munity question answering services provide com-
prehensive answers for complex questions(Jeon et
al., 2005). It will greatly enhance the overall per-
formance if we can classify questions into several
types, distribute each type of questions to suitable
sources and trigger corresponding strategy to sum-
marize returned answers.
Question classification (QC) in factoid QA is to
provide constraints on answer types that allows fur-
ther processing to pinpoint and verify the answer
(Li and Roth, 2004). Usually, questions are classi-
fied into a fine grained content-based taxonomy(e.g.
UIUC taxonomy (Li and Roth, 2002)). We can-
not use these taxonomies directly. To guide ques-
tion distribution and answer summarization, ques-
tions are classified according to their functions in-
stead of contents.
Motivated by related work on user goal classi-
fication(Broder, 2002; Rose and Levinson, 2004) ,
we propose a function-based question classification
category tailored to general QA. The category con-
tain six types, namely Fact, List, Reason, Solution,
Definition and Navigation. We will introduced this
1119
category in detail in Section 2.
To classify questions effectively, we unify rule-
based methods and statistical methods into a single
framework. Each question is splited into functional
words and content words. We generate strict pat-
terns from functional words and soft patterns from
content words. Each strict pattern is a regular ex-
pression while each soft pattern is a bi-gram clus-
ter. Given a question, we will evaluate its matching
degree to each patterns. The matching degree is ei-
ther 0 or 1 for strict pattern and between 0 and 1 for
soft pattern. Finally, Markov logic network (MLN)
(Richardson and Domingos, 2006) is used to com-
bine and evaluate all the patterns.
The classical MLN maximize the probability of
an assignment of truth values by evaluating the
weights of each formula. However, the real world
is full of uncertainty and is unnatural to be repre-
sented by a set of boolean values. In this paper,
we propose fuzzy discriminative weight learning of
Markov logic network. This method takes degrees
of confidence of each evidence predicates into ac-
count thus can model the matching degrees between
questions and soft patterns.
The remainder of this paper is organized as fol-
lows: In the next section we review related work
on question classification, query classification and
Markov logic network. Section 2 gives a detailed
introduction to our new taxonomy for general QA.
Section 4 introduces fuzzy discriminative weight
learning of MLN and our methodology to extract
strict and soft patterns. In Section 5 we compare our
method with previous methods on Chinese question
data from Baidu Zhidao and Sina iAsk. In the last
section we conclude this work.
Although we build patterns and do experiments
on Chinese questions, our method does not take ad-
vantage of the particularity of Chinese language and
thus can be easily implemented on other languages.
2 Related Work
Many question taxonomies have been proposed in
QA community. Lehnert (1977) developed the sys-
tem QUALM based on thirteen conceptual cate-
gories which are based on a theory of memory repre-
sentation. On the contrary, the taxonomy proposed
by Graesser et al (1992) has foundations both in the-
ory and in empirical research. Both of these tax-
onomies are for open-domain question answering.
With the booming of internet, researches on
question answering are becoming more practical.
Most taxonomies proposed are focused on factoid
questions, such as UIUC taxonomy (Li and Roth,
2002). UIUC taxonomy contains 6 coarse classes
(Abbreviation, Entity, Description, Human, Lo-
cation and Numeric Value) and 50 fine classes.
All coarse classes are factoid oriented except De-
scription. To classify questions effectively, Re-
searchers have proposed features of different levels,
such as lexical features, syntactic features (Nguyen
et al, 2007; Moschitti et al, 2007) and semantic fea-
tures (Moschitti et al, 2007; Li and Roth, 2004).
Zhang and Lee (2003) compared five machine learn-
ing methods and found SVM outperformed the oth-
ers.
In information retrieval community, researchers
have described frameworks for understanding goals
of user searches. Generally, web queries are classi-
fied into four types: Navigational, Informational,
Transactional (Broder, 2002) and Resource (Rose
and Levinson, 2004). Lee et al (2005) automatically
classify Navigational and Informational queries
based on past user-click behavior and anchor-link
distribution. Jansen and Booth (2010) investigate
the correspondence between three user intents and
eighteen topics. The result shows that user intents
distributed unevenly among different topics.
Inspired by Rose and Levinson (2004)?s work in
user goals classification, Liu et al (2008) describe
a three-layers cQA oriented question taxonomy and
use it to determine the expected best answer types
and summarize answers. Other than Navigational,
Informational and Transactional, the first layer
contains a new Social category which represents the
questions that do not intend to get an answer but to
elicit interaction with other people. Informational
contains two subcategories Constant and Dynamic.
Dynamic is further divided into Opinion, Context-
Dependent and Open.
Markov logic network (MLN) (Richardson and
Domingos, 2006) is a general model combining
first-order logic and probabilistic graphical models
in a single representation. Illustratively, MLN is a
first-order knowledge base with a weight attached
to each formula. The weights can be learnt ei-
1120
TYPE DESCRIPTION EXAMPLES
1. Fact People ask these questions for general facts.
The expected answer will be a short phrase.
Who is the president
of United States?
2. List People ask these questions for a list of an-
swers. Each answer will be a single phrase
or a phrase with explanations or comments.
List Nobel price
winners in 1990s.
Which movie star do
you like best?
3. Reason People ask these questions for opinions or ex-
planations. A good answer summary should
contain a variety of opinions or comprehen-
sive explanations. Sentence-level summariza-
tion can be employed.
Is it good to drink
milk while fasting?
What do you think of
Avatar?
4. Solution People ask these questions for problem shoot-
ing. The sentences in an answer usually have
logical order thus the summary task cannot be
performed on sentence level.
What should I do
during an earthquake?
How to make pizzas?
5. Definition People ask these questions for description of
concepts. Usually these information can be
found in Wikipedia. If the answer is a too
long, we should summarize it into a shorter
one.
Who is Lady Gaga?
What does the Matrix
tell about?
6. Navigation People ask these questions for finding web-
sites or resources. Sometimes the websites are
given by name and the resources are given di-
rectly.
Where can I download
the beta version of
StarCraft 2?
Table 1: Question Taxonomy for general QA
ther generatively (Richardson and Domingos, 2006)
or discriminatively (Singla and Domingos, 2005).
Huynh and Mooney (2008) applies ?
1
-norm regu-
larized MLE to select candidate formulas generated
by a first-order logic induction system and prevent
overfitting. MLN has been introduced to NLP and
IE tasks such as semantic parsing (Poon et al, 2009)
and entity relation extraction (Zhu et al, 2009).
3 A Question Taxonomy
We suggest a function-based taxonomy tailored to
general QA systems by two principles. First, ques-
tions can be distributed into suitable QA subsys-
tems according to their types. Second, we can
employ suitable answer summarization strategy for
each question type. The taxonomy is shown in Tab.
1.
At first glance, classifying questions onto this tax-
onomy seems a solved problem for English ques-
tions because of interrogative words. In most cases,
a question starting with ?Why? is for reason and
?How? is for solution. But it is not always the case
for other languages. From table 2 we can see two
questions in Chinese share same function word ??
??? but have different types.
In fact, even in English, only using interroga-
tive words is not enough for function-based ques-
tion classification. Sometimes the question content
is crucial. For example, for question ?Who is the
current president of U.S. ??, the answer is ?Barak
Obama? and the type is Fact. But for question ?Who
is Barak Obama??, it will be better if we return the
first paragraph from the corresponding Wiki article
instead of a short phrase ?current president of U.S.?.
Therefore the question type will be Definition.
Compared to Wendy Lehnert?s or Arthur
Graesser?s taxonomy, our taxonomy is more prac-
tical on providing useful information for question
1121
Question ?????????
How to cook Kung Pao Chicken?
Type Solution
Question ???????????
What do you think of Avatar?
Type Reason
Table 2: Two Chinese questions share same function
words but have different types
extraction and summarization. Compared to ours,
The UIUC taxonomy is too much focused on factoid
questions. Apart from Description, all coarse types
in UIUC can be mapped into Fact. The cQA
taxonomy proposed in Liu et al (2008) has similar
goal with ours. But it is hard to automatically
classify questions into that taxonomy, especially for
types Constant, Dynamic and Social. Actually the
author did not give implementation in the paper as
well. To examine reasonableness of our taxonomy,
we select and manually annotate 5800 frequent
asked questions from Baidu Zhidao (see Section
5.1). The distribution of six types is shown in Fig.
1. 98.5 percent of questions can be categorized
into our taxonomy. The proportion of each type is
between 7.5% and 23.8%.
The type Navigation was originally proposed in
IR community and did not cause too much concerns
in previous QA researches. But from Fig. 1 we
can see that navigational questions take a substan-
tial proportion in cQA data.
Moreover, we can further develop subtypes for
each type. For example, most categories in UIUC
Reason18.1%
Fact14.4%Solution19.7%Navigation14.8%
List23.8% Definition7.5% Other1.5%
Figure 1: Distribution of six types in Baidu Zhidao data
taxonomy can be regarded as refinement to Fact and
Navigation can be refined into Resource and Web-
site. We will not have further discussion on this is-
sue.
4 Methodology
Many efforts have been made to take advantage of
grammatical , semantic and lexical features in ques-
tion classification. Zhang and Lee (2003) proposed
a SVM based system which used tree kernel to in-
corporate syntactic features.
In this section, we propose a new question clas-
sification methodology which combines rule-based
methods and statistical methods by Markov logic
network. We do not use semantic and syntactic fea-
tures for two reasons. First, the questions posted on
online communities are casually written which can-
not be accurately parsed by NLP tools, especially for
Chinese. Second, the semantic and syntactic pars-
ing are time consuming thus unpractical to be used
in real systems.
We will briefly introduce MLN and fuzzy dis-
criminative learning in section 4.1. The construction
of strict patterns and soft patterns will be shown in
4.2 and 4.3. In section 4.4 we will give details on
MLN construction, inference and learning.
4.1 Markov Logic Network
A first-order knowledge base contains a set of for-
mulas constructed from logic operators and symbols
for predicates, constants, variables and functions.
An atomic formula or atom is a predicate symbol.
Formulas are recursively constructed from atomic
formulas using logical operators. The grounding
of a predicate (formula) is a replacement of all of
its arguments (variables) by constants. A possible
world is an assignment of truth values to all possible
groundings of all predicates.
In first-order KB, if a possible world violates
even one formula, it has zero probability. Markov
logic is a probabilistic extension and softens the hard
constraints by assigning a weight to each formula.
When a possible world violates one formula in the
KB, it is less probable. The higher the weight, the
greater the difference in log probability between a
world that satisfies the formula and a world does
not. Formally, Markov logic network is defined as
1122
follows:
Definition 1 (Richardson & Domingos 2004) A
Markov logic network L is a set of pairs (?
?
, ?
?
),
where ?
?
is a formula in first-order logic and ?
?
is a
real number. Together with a finite set of constants
C = {?
1
, ?
2
, ..., ?
???
}, it defines a Markov network
?
?,?
as follows:
1. ?
?,?
contains one binary node for each pos-
sible grounding of each predicate appearing in
L. The value of the node is 1 if the ground pred-
icate is true, and 0 otherwise.
2. ?
?,?
contains one feature for each possible
grounding of each formula ?
?
in L. The value
of this feature is 1 if the ground formula is true,
and 0 otherwise. The weight of the feature is
the ?
?
associated with ?
?
in L.
There is an edge between two nodes of ?
?,?
iff
the corresponding grounding predicates appear to-
gether in at least one grounding of one formula in
?. An MLN can be regarded as a template for con-
structing Markov networks. From Definition 1 and
the definition of Markov networks, the probability
distribution over possible worlds ? specified by the
ground Markov network ?
?,?
is given by
? (? = ?) =
1
?
exp
(
?
?
?=1
?
?
?
?
(?)
)
MLN weights can be learnt genera-
tively(Richardson and Domingos, 2006) or
discriminatively(Singla and Domingos, 2005). In
discriminative weight learning, ground atom set ?
is partitioned into a set of evidence atoms ? and
a set of query atoms ? . The goal is to correctly
predict the latter given the former. In this paper, we
propose fuzzy discriminative weight learning which
can take the prior confidence of each evidence atom
into account.
Formally, we denote the ground formula set by
? . Suppose each evidence atom ? is given with a
prior confidence ?
?
? [0, 1], we define a confidence
function ? : ? ? [0, 1] as follows. For each ground
atom ?, if ? ? ? then we have ?(?) = ?
?
, else
?(?) = 1. For each ground non-atomic formulas, ?
is defined on standard fuzzy operators, which are
?(??) = 1? ?(?)
?(?
1
? ?
2
) = min(?(?
1
), ?(?
2
))
?(?
1
? ?
2
) = max(?(?
1
), ?(?
2
))
We redefined the conditional likelihood of ?
given ? as
? (???) =
1
?
?
exp
?
?
?
???
?
?
?
?
?
(?, ?)
?
?
=
1
?
?
exp
?
?
?
???
?
?
?
?
?
?
(?, ?)
?
?
Where ?
?
is the set of ground formulas involving
query atoms, ?
?
is the set of formulas with at least
one grounding involving a query atom and ??
?
(?, ?)
is the sum of confidence of the groundings of the i th
formula involving query atoms. The gradient of the
conditional log-likelihood (CLL) is
?
??
?
log?
?
(???)
= ?
?
?
(?, ?)?
?
?
?
?
?
(?
?
??)?
?
?
(?, ?
?
)
= ?
?
?
(?, ?)? ?
?
[?
?
?
(?, ?)] (1)
By fuzzy discriminative learning we can incorpo-
rate evidences of different confidence levels into one
learning framework. Fuzzy discriminative learn-
ing will reduce to traditional discriminative learning
when all prior confidences equal to 1.
4.2 Strict Patterns
In our question classification task, we find function
words are much more discriminative and less sparse
than content words. Therefore, we extract strict pat-
terns from function words and soft patterns from
content words. The definition of content and func-
tion words may vary with languages. In this paper,
nouns, verbs, adjectives, adverbs, numerals and pro-
nouns are regarded as content words and the rest are
function words.
The outline of strict pattern extraction is shown
in Alg. 1. In line 3, we build template ??? by re-
moving punctuations and replacing each character
in each content word by a single dot. In line 4, we
generate patterns from the template as follows. First
we generate n-grams(n is between 2 and ? ) from
1123
Algorithm 1: Strict Pattern Extraction
Input: Question Set ? = {?
1
, ?
2
...?
?
},
Parameters ? and ?
Output: Pattern Set ?
Initialize Pattern Set ? ;1
for each Question ?
?
do2
String ???=ReplaceContentWords(?
?
,?.?);3
Pattern Set ?
?
?
=GeneratePatterns(???,? );4
for each Pattern ? in ?
?
?
do5
if ? in ? then6
UpdateTypeFreq(?,? );7
else8
Add ? to ? ;9
Merge similar patterns in ? ;10
Sort ? by Information Gain on type11
frequencies;
return top ? Patterns in ? ;12
??? during which each dot is treated as a character
of zero length. For coverage concern, if a gener-
ated n-gram ? is not start(end) with dot, we build
another n-gram ?? by adding a dot before(behind) ?
and add both ? and ?? into n-gram set. Then for each
n-gram, we replace each consecutive dot sequence
by ?.*? and the n-gram is transformed into a regular
expression. A example is shown in Tab. 3. Although
generated without exhaustively enumerating all pos-
sible word combinations, these regular expressions
can capture most long range dependencies between
function words.
Each pattern consists of a regular expression as
well as its frequency in each type of questions. Still
Question ???????????
Can I launch online banking services
on internet?
Template ?..??....?
Patterns .*?.*? .*?.*?.*
(?=4) .*??.* .*??.*?
.*??.*?.* .*?.*??.*
.*?.*??.*?.* ?.*?.*
?.*??.* ?.*??.*?
.*?.*?.*
Table 3: Strict patterns generated from a question
from Alg. 1, in line 5-9, if a pattern ? in question ?
?
with type ? is found in ? , we just update the fre-
quency of ? in ?, else ? is added to ? with only
freq. ? equals to 1. In line 10, we merge similar
patterns in ? . two patterns ?
1
and ?
2
are similar iff
?q?QmatchP(q,p1) ? matchP(q,p2), in which
matchP is defined in Section 4.4.
Since a large number of patterns are generated,
it is unpractical to evaluate all of them by Markov
logic network. We sort patterns by information gain
and only choose top? ?good? patterns in line 11-12
of Alg. 1. A ?good? pattern should be discriminative
and of wide coverage. The information gain IG of a
pattern ? is defined as
IG(?) = ? (?)
?
?
?=1
? (?
?
??) log? (?
?
??)+
? (?)
?
?
?=1
? (?
?
??) log? (?
?
??)?
?
?
?=1
? (?
?
) log? (?
?
)
in which ? is the number of question types, ? (?
?
) is
the probability of a question having type ?
?
, ? (?)(or
? (?)) is the probability of a question matching(or
not matching) pattern ?. ? (?
?
??)(or ? (?
?
??)) is
the probability of a question having type ?
?
given
the condition that the question matches(or does not
match) pattern ?. These probabilities can be approx-
imately calculated by type and pattern frequencies
on training data. From the definition we can see
that information gain is suitable for pattern selec-
tion. The more questions a pattern ? matches and
the more unevenly the matched questions distribute
among questions types, the higher IG(?) will be.
4.3 Soft Patterns
Apart from function words, content words are also
important in function-based question classification.
Content words usually contain topic information
which can be a good complement to function words.
Previous research on query classification(Jansen and
Booth, 2010) shows that user intents distribute un-
evenly among topics. Moreover, questions given by
users may be incomplete and contain not function
words. For these questions, we can only predict the
question types from topic information.
Compared with function words, content words
distribute much more sparsely among questions.
1124
When we represent topic information by content
words (or bi-grams), since the training set are small
and less frequent words (or bi-grams) are filtered
to prevent over-fitting, those features would be too
sparse to predict further unseen questions.
To solve this problem, we build soft patterns on
question set. Each question is represented by a
weighted vector of content bi-grams in which the
weight is bi-gram frequency. Cosine similarity is
used to compute the similarity between vectors.
Then we cluster question vectors using a simple
single-pass clustering algorithm(Frakes and Yates,
1992). That is, for each question, we compute its
similarity with each centroid of existing cluster. If
the similarity with nearest cluster is greater than
a minimum similarity threshold ?
1
, we assign this
question to that cluster, else a new cluster is created
for this question.
Each cluster is defined as a soft pattern. Unlike
strict patterns, a question can match a soft pattern
to some extent. In this paper, the degree of match-
ing is defined as the cosine similarity between ques-
tion and centroid of cluster. Soft patterns are flexible
and could alleviate the sparseness of content words.
Also, soft patterns can be pre-filtered by information
gain described in 4.2 if necessary.
4.4 Implementation
Currently, we model patterns into MLN as follows.
The main query predicate is Type(q,t), which
is true iff question q has type t. For strict pat-
terns, the evidence predicate MatchP(q,p) is true
iff question q is matched by strict pattern p. The
confidence of MatchP(q,p) is 1 for each pair of
(q,p). For soft patterns, the evidence predicate
MatchC(q,c) is true iff the similarity of question
q and the cluster c is greater than a minimum simi-
larity requirement ?
2
. If MatchC(q,c) is false, its
confidence is 1, else is the similarity between q and
c.
We represent the relationship between patterns
and types by a group of formulas below.
MatchP(q,+p)?Type(q,+t)
?
?
?
?=?
?Type(q,t?)
The ?+p, +t? notation signifies that the MLN con-
tains an instance of this formula for each (pattern,
type) pair. For the sake of efficacy, for each pattern-
type pair (p,t), if the proportion of type t in ques-
tions matching p is less than a minimum require-
ment ?, we remove corresponding formula from
MLN.
Similarly, we incorporate soft patterns by
MatchC(q,+c)?Type(q,+t)
?
?
?
?=?
?Type(q,t?)
Our weight learner use ?
1
-regularization (Huynh
and Mooney, 2008) to select formulas and prevent
overfitting. A good property of ?
1
-regularization is
its tendency to force parameters to exact zero by
strongly penalizing small terms (Lee et al, 2006).
After training, we can simply remove the formulas
with zero weights.
Formally, to learn weight for each formula, we
iteratively solve ?
1
-norm regularized optimization
problem:
? : ?
?
= argmax
?
log?
?
(???)? ????
1
where ?.?
1
is ?
1
-norm and parameter ? controls the
penalization of non-zero weights. We implement the
Orthant-Wise Limited-memory Quasi-Newton algo-
rithm(Andrew and Gao, 2007) to solve this opti-
mization.
Since we do not model relations among questions,
the derived markov network ?
?,?
can be broken up
into separated subgraphs by questions and the gradi-
ent of CLL(Eq. 1) can be computed locally on each
subgraph as
?
??
?
log?
?
(???)
=
?
?
(
?
?
?
(?
?
, ?
?
)??
?
[?
?
?
(?
?
, ?
?
)]
)
(2)
in which ?
?
and ?
?
are the evidence and query atoms
involving question ?. Eq. 2 can be computed fast
without approximation.
We initialize formula weights to the same posi-
tive value ?. Iteration started from uniform prior
can always converge to a better local maximum than
gaussian prior in our task.
5 Experiments
5.1 Data Preparation
To the best of our knowledge, there is not general
QA system(the system which can potentially answer
1125
all kinds of questions utilizing data from heteroge-
neous sources) released at present. Alteratively, we
test our methodology on cQA data based on obser-
vation that questions on cQA services are of var-
ious length, domain independent and wrote infor-
mally(even with grammar mistakes). General QA
systems will meet these challenges as well.
In our experiments, both training and test data
are from Chinese cQA services Baidu Zhidao and
Sina iAsk. To build training set, we randomly select
5800 frequent-asked questions from Baidu Zhidao.
A question is frequent-asked if it is lexically simi-
lar to at least five other questions. Then we ask 10
native-speakers to annotate these questions accord-
ing to question title and question description. If an
annotator cannot judge type from question title, he
can view the question description. If type can be
judged from the description, the question title will
be replaced by a sentence selected from it. If not,
this question will be labeled as Other.
Each question is annotated by two people. If a
question is labeled different types, another annotator
will judge it and make final decision. If this annota-
tor cannot judge the type, this question will also be
labeled as Other. As a result, disagreements show
up on eighteen percents of questions. After the third
annotator?s judgment, the distribution of each type
is shown in Fig. 1.
To examine the generalization capabilities, the
test data is composed of 700 questions randomly se-
lected from Baidu Zhidao and 700 questions from
Sina iAsk. The annotation process on test data is as
same as the one on training data.
5.2 Methods Compared and Results
We compare four methods listed as follows.
SVM with bi-grams. We extract bi-grams from
questions on training data as features. After filtering
the ones appearing only once, we collect 5700 bi-
grams. LIBSVM(Chang and Lin, 2001)is used as
the multi-class SVM classifier. All parameters are
adjusted to maximize the accuracy on test data. We
denote this method as ?SB?;
MLN with bi-grams. To compare MLN and
SVM, we treat bi-grams as strict patterns. If a ques-
tion contain a bi-gram, it matches the corresponding
pattern. We set ? = 0.01, ? = 0.3 and ? = 0.3.
As a result, 5700 bi-grams are represented by 10485
formulas. We denote this method as ?MB?;
MLNwith strict patterns and bi-grams. We ask
two native-speakers to write strict patterns for each
type. The pattern writers can view training data for
reference and write any Java-style regular expres-
sions. Then we carefully choose 50 most reliable
patterns. To overcome the low coverage, We also
use the method described in Sec. 4.2 to automati-
cally extract strict patterns from training set. We first
select top 3000 patterns by information gain, merge
these patterns with hand-crafted ones and combine
similar patterns. Then we represent these patterns
by formulas and learn the weight of each formula by
MLN. After removing the formula with low weights,
we finally retain 2462 patterns represented by 3879
formulas. To incorporate content information, we
extract bi-grams from questions with function words
removed and remove the ones with frequency lower
than two. With bi-grams added, we get 8173 formu-
las in total. All parameters here are the same as in
?MB?. We denote this method as ?MSB?;
MLN with strict patterns and soft patterns. To
incorporate content information, We cluster ques-
tions on training data with similarity threshold ?
1
=
0.4 and get 2588 clusters(soft patterns) which are
represented by 3491 formulas. We these soft pat-
terns with strict patterns extracted in ?MSB?, which
add up to 7370 formulas. We set ?
2
= 0.02 and the
other parameters as same as in ?MB?. We denote
this method as ?MSS?;
We separate test set into easy set and difficult set.
A question is classified into easy set iff it contains
function-words. As a result, the easy set contains
1253 questions. We measure the accuracy of these
four methods on easy data and the whole test data.
The results are shown in Tab 4. From the results we
can see that all methods perform better on easy ques-
tions and MLN outperforms SVM using same bi-
gram features. Although MSS is inferior to MSB on
F. num Easy data All data
SB NA 0.724 0.685
MB 10485 0.722 0.692
MSB 8173 0.754 0.714
MSS 7370 0.752 0.717
Table 4: Experimental results on Chinese cQA data
1126
F L S R D N
Prec. 0.63 0.65 0.83 0.76 0.69 0.55
Recall 0.55 0.74 0.86 0.76 0.44 0.58
F
1
0.59 0.69 0.84 0.76 0.54 0.56
Table 5: Precision, recall and F-score on each type
easy questions, it shows better overall performance
and uses less formulas.
We further investigate the performance on each
type. The precision, recall and F
1
-score of each type
by method MSS are shown in Tab. 5. From the re-
sults we can see that the performance on Solution
and Reason are significantly better than the others.
It is because the strict patterns for this two types are
simple and effective. A handful of patterns could
cover a wide range of questions with high precision.
It is difficult to distinguish Fact from List because
strict patterns for these two types are partly overlap
each other. Sometimes we need content information
to determine whether the answer is unique. Since
List appears more frequently than Fact on training
set, MLN tend to misclassify Fact toListwhich lead
to low recall of the former and low precision of the
latter. The recall of Definition is very low because
many definition questions on test set are short and
only consists of content words(e.g. a noun phrase).
This shortage could be remedied by building strict
patterns on POStagging sequence.
fraction lines, college entrance exam
???????????????...
Fact: 56.4% List: 33.3% Solu.: 5.5%
lose weight, summer, fast
???????????????...
Reas.: 53.8% Solu.: 42.3% List: 3.8%
TV series, interesting, recent
???????????????...
List: 84.0% Fact: 8.0% Navi.: 2.0%
converter, format, 3gp
??????3gp?mp4????...
Navi.: 75% List: 18.8% Solu.: 6.2%
Table 6: Selected soft patterns on training data
5.3 Case Study on Soft Patterns
To give an intuitive illustration of soft patterns, we
show some of them clustered on training data in Tab.
6. For each soft pattern, we list five most frequent
bi-grams and its distribution on each type(only top 3
frequent types are listed).
From the results we can see that soft patterns are
consistent with our ordinary intuitions. For exam-
ple, if user ask a questions about ?TV series?, he is
likely to ask for recommendation of recent TV series
and the question have a great chance to be List. If
user ask questions about ?lose weight?, he probably
ask something like ?How can I lose weight fast?? or
?Why my diet does not work?? . Thus the type is
likely to be Solution or Reason.
6 Conclusion and Future Work
We have proposed a new question taxonomy tai-
lored to general QA on heterogeneous sources.
This taxonomy provide indispensable information
for question distribution and answer summarization.
We build strict patterns and soft patterns to repre-
sent the information in function words and content
words. Also, fuzzy discriminative weight learning
is proposed for unifying strict and soft patterns into
Markov logic network.
Currently, we have not done anything fancy on the
structure of MLN. We just showed that under uni-
form prior and L1 regularization, the performance
of MLN is comparable to SVM. To give full play
to the advantages of MLN, future work will focus
on fast structure learning. Also, since questions on
online communities are classified into categories by
topic, we plan to perform joint question type infer-
ence on function-based taxonomy as well as topic-
based taxonomy by Markov logic. The model will
not only capture the relation between patterns and
types but also the relation between types in different
taxonomy.
Acknowledgment
This work was supported mainly by Canada?s IDRC
Research Chair in Information Technology program,
Project Number: 104519-006. It is also supported
by the Chinese Natural Science Foundation grant
No. 60973104.
1127
References
G. Andrew and J. Gao. 2008. Scalable training of L1-
regularized log-linear models. In Proc. of ICML 2007,
pp. 33-40.
A. Broder. 2002. A taxonomy of Web search. SIGIR
Forum, 36(2), 2002.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
W.B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval: Data Structures and Algorithms.
Prentice-Hall, 1992.
A.C. Graesser, N.K. Person and J.D. Huber. 1992. Mech-
anisms that generate questions. Questions and Infor-
mation Systems, pp. 167-187), Hillsdale, N.J.: Erl-
baum.
T.N. Huynh and R.J. Mooney. 2008. Discriminative
Structure and Parameter Learning for Markov Logic
Networks. In Proc. of ICML 2008, pp. 416-423.
B.J. Jansen and D. Booth. 2010. Classifying web queries
by topic and user intent. In Proc. of the 28th interna-
tional conference on human factors in computing sys-
tems, pp. 4285-4290.
J. Jeon, W.B. Croft and J.H. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proc. of ACM CIKM 2005,pp. 76-83.
S. Lee, V. Ganapathi and D. Koller. 2005. Effi-
cient structure learning of Markov networks using ?
1
-
regularization.. Advances in Neural Information Pro-
cessing Systems 18.
U. Lee, Z. Liu and J. Cho. 2005. Automatic identification
of user goals in Web search. In Proc. of WWW 2005.
W. Lehnert. 1977. Human and computational question
answering. Cognitive Science, vol. 1, 1977, pp. 47-63.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proc. of COLING 2002, pp. 556-562.
X. Li and D. Roth. 2004. Learning question classifiers:
the role of semantic information. Natural Language
Engineering.
Y. Liu, S. Li, Y. Cao, C.Y. Lin, D. Han and Y. Yu.
2008. Understanding and summarizing answers in
community-based question answering services. In
Proc. of COLING 2008.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Semantic
Kernels for Question/Answer Classification. In Proc.
of ACL 2007.
M.L. Nguyen, T.T. Nguyen and A. Shimazu. 2007. Sub-
tree Mining for Question Classification Problem. In
Proc. of IJCAI 2007.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In Proc. of EMNLP 2009, pp. 1-10
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. In Proc. of WWW 2004.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning 62:107-136.
P. Singla and P. Domingos. 2005. Discriminative Train-
ing of Markov Logic Networks. In Proc. of AAAI
2005.
S. Ye, T.S. Chua and J. Lu. 2009. Summarizing Defini-
tion from Wikipedia. In Proc. of ACL 2009.
D. Zhang and W.S. Lee. 2003. Question classification
using support vector machines. In Proc. of ACM SI-
GIR 2003, pp. 26-32.
J. Zhu , Z. Nie, X. Liu, B. Zhang and J.R. Wen. 2009.
StatSnowball: a Statistical Approach to Extracting En-
tity Relationships. In Proc. of WWW 2009, pp. 101-
110
1128
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 449?458,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
String Re-writing Kernel
Fan Bu1, Hang Li2 and Xiaoyan Zhu3
1,3State Key Laboratory of Intelligent Technology and Systems
1,3Tsinghua National Laboratory for Information Sci. and Tech.
1,3Department of Computer Sci. and Tech., Tsinghua University
2Microsoft Research Asia, No. 5 Danling Street, Beijing 100080,China
1bufan0000@gmail.com
2hangli@microsoft.com
3zxy-dcs@tsinghua.edu.cn
Abstract
Learning for sentence re-writing is a funda-
mental task in natural language processing and
information retrieval. In this paper, we pro-
pose a new class of kernel functions, referred
to as string re-writing kernel, to address the
problem. A string re-writing kernel measures
the similarity between two pairs of strings,
each pair representing re-writing of a string.
It can capture the lexical and structural sim-
ilarity between two pairs of sentences with-
out the need of constructing syntactic trees.
We further propose an instance of string re-
writing kernel which can be computed effi-
ciently. Experimental results on benchmark
datasets show that our method can achieve bet-
ter results than state-of-the-art methods on two
sentence re-writing learning tasks: paraphrase
identification and recognizing textual entail-
ment.
1 Introduction
Learning for sentence re-writing is a fundamental
task in natural language processing and information
retrieval, which includes paraphrasing, textual en-
tailment and transformation between query and doc-
ument title in search.
The key question here is how to represent the re-
writing of sentences. In previous research on sen-
tence re-writing learning such as paraphrase identifi-
cation and recognizing textual entailment, most rep-
resentations are based on the lexicons (Zhang and
Patrick, 2005; Lintean and Rus, 2011; de Marneffe
et al, 2006) or the syntactic trees (Das and Smith,
                  wrote     .                  Shakespeare  wrote  Hamlet.  
  *    was written by       .          Hamlet was written by Shakespeare.  
(B) ** 
* * 
(A) 
Figure 1: Example of re-writing. (A) is a re-writing rule
and (B) is a re-writing of sentence.
2009; Heilman and Smith, 2010) of the sentence
pairs.
In (Lin and Pantel, 2001; Barzilay and Lee, 2003),
re-writing rules serve as underlying representations
for paraphrase generation/discovery. Motivated by
the work, we represent re-writing of sentences by
all possible re-writing rules that can be applied into
it. For example, in Fig. 1, (A) is one re-writing rule
that can be applied into the sentence re-writing (B).
Specifically, we propose a new class of kernel func-
tions (Scho?lkopf and Smola, 2002), called string re-
writing kernel (SRK), which defines the similarity
between two re-writings (pairs) of strings as the in-
ner product between them in the feature space in-
duced by all the re-writing rules. SRK is different
from existing kernels in that it is for re-writing and
defined on two pairs of strings. SRK can capture the
lexical and structural similarity between re-writings
of sentences and does not need to parse the sentences
and create the syntactic trees of them.
One challenge for using SRK lies in the high com-
putational cost of straightforwardly computing the
kernel, because it involves two re-writings of strings
(i.e., four strings) and a large number of re-writing
rules. We are able to develop an instance of SRK,
referred to as kb-SRK, which directly computes the
number of common rewriting rules without explic-
449
itly calculating the inner product between feature
vectors, and thus drastically reduce the time com-
plexity.
Experimental results on benchmark datasets show
that SRK achieves better results than the state-of-
the-art methods in paraphrase identification and rec-
ognizing textual entailment. Note that SRK is very
flexible to the formulations of sentences. For ex-
ample, informally written sentences such as long
queries in search can also be effectively handled.
2 Related Work
The string kernel function, first proposed by Lodhi
et al (2002), measures the similarity between two
strings by their shared substrings. Leslie et al
(2002) proposed the k-spectrum kernel which repre-
sents strings by their contiguous substrings of length
k. Leslie et al (2004) further proposed a number of
string kernels including the wildcard kernel to fa-
cilitate inexact matching between the strings. The
string kernels defined on two pairs of objects (in-
cluding strings) were also developed, which decom-
pose the similarity into product of similarities be-
tween individual objects using tensor product (Basil-
ico and Hofmann, 2004; Ben-Hur and Noble, 2005)
or Cartesian product (Kashima et al, 2009).
The task of paraphrasing usually consists of para-
phrase pattern generation and paraphrase identifica-
tion. Paraphrase pattern generation is to automat-
ically extract semantically equivalent patterns (Lin
and Pantel, 2001; Bhagat and Ravichandran, 2008)
or sentences (Barzilay and Lee, 2003). Paraphrase
identification is to identify whether two given sen-
tences are a paraphrase of each other. The meth-
ods proposed so far formalized the problem as clas-
sification and used various types of features such
as bag-of-words feature, edit distance (Zhang and
Patrick, 2005), dissimilarity kernel (Lintean and
Rus, 2011) predicate-argument structure (Qiu et al,
2006), and tree edit model (which is based on a tree
kernel) (Heilman and Smith, 2010) in the classifica-
tion task. Among the most successful methods, Wan
et al (2006) enriched the feature set by the BLEU
metric and dependency relations. Das and Smith
(2009) used the quasi-synchronous grammar formal-
ism to incorporate features from WordNet, named
entity recognizer, POS tagger, and dependency la-
bels from aligned trees.
The task of recognizing textual entailment is to
decide whether the hypothesis sentence can be en-
tailed by the premise sentence (Giampiccolo et al,
2007). In recognizing textual entailment, de Marn-
effe et al (2006) classified sentences pairs on the
basis of word alignments. MacCartney and Man-
ning (2008) used an inference procedure based on
natural logic and combined it with the methods by
de Marneffe et al (2006). Harmeling (2007) and
Heilman and Smith (2010) classified sequence pairs
based on transformation on syntactic trees. Zanzotto
et al (2007) used a kernel method on syntactic tree
pairs (Moschitti and Zanzotto, 2007).
3 Kernel Approach to Sentence
Re-Writing Learning
We formalize sentence re-writing learning as a ker-
nel method. Following the literature of string kernel,
we use the terms ?string? and ?character? instead of
?sentence? and ?word?.
Suppose that we are given training data consisting
of re-writings of strings and their responses
((s1, t1),y1), ...,((sn, tn),yn) ? (?
????)?Y
where ? denotes the character set, ?? =
??
i=0?
i de-
notes the string set, which is the Kleene closure of
set ?, Y denotes the set of responses, and n is the
number of instances. (si, ti) is a re-writing consist-
ing of the source string si and the target string ti.
yi is the response which can be a category, ordinal
number, or real number. In this paper, for simplic-
ity we assume that Y = {?1} (e.g. paraphrase/non-
paraphrase). Given a new string re-writing (s, t) ?
?????, our goal is to predict its response y. That is,
the training data consists of binary classes of string
re-writings, and the prediction is made for the new
re-writing based on learning from the training data.
We take the kernel approach to address the learn-
ing task. The kernel on re-writings of strings is de-
fined as
K : (?????)? (?????)? R
satisfying for all (si, ti), (s j, t j) ? ?????,
K((si, ti),(s j, t j)) = ??(si, ti),?(s j, t j)?
where ? maps each re-writing (pair) of strings into
a high dimensional Hilbert space H , referred to as
450
feature space. By the representer theorem (Kimel-
dorf and Wahba, 1971; Scho?lkopf and Smola, 2002),
it can be shown that the response y of a new string
re-writing (s, t) can always be represented as
y = sign(
n
?
i=1
?iyiK((si, ti),(s, t)))
where ?i ? 0,(i = 1, ? ? ? ,n) are parameters. That is,
it is determined by a linear combination of the sim-
ilarities between the new instance and the instances
in training set. It is also known that by employing a
learning model such as SVM (Vapnik, 2000), such a
linear combination can be automatically learned by
solving a quadratic optimization problem. The ques-
tion then becomes how to design the kernel function
for the task.
4 String Re-writing Kernel
Let ? be the set of characters and ?? be the set of
strings. Let wildcard domain D ? ?? be the set of
strings which can be replaced by wildcards.
The string re-writing kernel measures the similar-
ity between two string re-writings through the re-
writing rules that can be applied into them. For-
mally, given re-writing rule set R and wildcard do-
main D, the string re-writing kernel (SRK) is defined
as
K((s1, t1),(s2, t2)) = ??(s1, t1),?(s2, t2)? (1)
where ?(s, t) = (?r(s, t))r?R and
?r(s, t) = n? i (2)
where n is the number of contiguous substring pairs
of (s, t) that re-writing rule r matches, i is the num-
ber of wildcards in r, and ? ? (0,1] is a factor pun-
ishing each occurrence of wildcard.
A re-writing rule is defined as a triple r =
(?s,?t ,?) where ?s,?t ? (? ? {?})? denote source
and target string patterns and ? ? ind?(?s)? ind?(?t)
denotes the alignments between the wildcards in the
two string patterns. Here ind?(? ) denotes the set of
indexes of wildcards in ? .
We say that a re-writing rule (?s,?t ,?) matches a
string pair (s, t), if and only if string patterns ?s and
?t can be changed into s and t respectively by sub-
stituting each wildcard in the string patterns with an
element in the strings, where the elements are de-
fined in the wildcard domain D and the wildcards
?s[i] and ?t [ j] are substituted by the same elements,
when there is an alignment (i, j) ? ? .
For example, the re-writing rule in Fig. 1 (A)
can be formally written as r = (? s,? t,?) where
? s = (?,wrote,?), ? t = (?,was,written,by,?) and
? = {(1,5),(3,1)}. It matches with the string pair in
Fig. 1 (B).
String re-writing kernel is a class of kernels which
depends on re-writing rule set R and wildcard do-
main D. Here we provide some examples. Obvi-
ously, the effectiveness and efficiency of SRK de-
pend on the choice of R and D.
Example 1. We define the pairwise k-spectrum ker-
nel (ps-SRK) K psk as the re-writing rule kernel un-
der R = {(?s,?t ,?)|?s,?t ? ?k,? = /0} and any
D. It can be shown that K psk ((s1, t1),(s2, t2)) =
Kspeck (s1,s2)K
spec
k (t1, t2) where K
spec
k (x,y) is equiv-
alent to the k-spectrum kernel proposed by Leslie et
al. (2002).
Example 2. The pairwise k-wildcard kernel (pw-
SRK) K pwk is defined as the re-writing rule kernel
under R= {(?s,?t ,?)|?s,?t ? (??{?})k,? = /0} and
D = ?. It can be shown that K pwk ((s1, t1),(s2, t2)) =
Kwc(k,k)(s1,s2)K
wc
(k,k)(t1, t2) where K
wc
(k,k)(x,y) is a spe-
cial case (m=k) of the (k,m)-wildcard kernel pro-
posed by Leslie et al (2004).
Both kernels shown above are represented as the
product of two kernels defined separately on strings
s1,s2 and t1, t2, and that is to say that they do not
consider the alignment relations between the strings.
5 K-gram Bijective String Re-writing
Kernel
Next we propose another instance of string re-
writing kernel, called the k-gram bijective string re-
writing kernel (kb-SRK). As will be seen, kb-SRK
can be computed efficiently, although it is defined
on two pairs of strings and is not decomposed (note
that ps-SRK and pw-SRK are decomposed).
5.1 Definition
The kb-SRK has the following properties: (1) A
wildcard can only substitute a single character, de-
noted as ???. (2) The two string patterns in a re-
writing rule are of length k. (3) The alignment
relation in a re-writing rule is bijective, i.e., there
is a one-to-one mapping between the wildcards in
451
the string patterns. Formally, the k-gram bijective
string re-writing kernel Kk is defined as a string
re-writing kernel under the re-writing rule set R =
{(?s,?t ,?)|?s,?t ? (??{?})k,? is bijective} and the
wildcard domain D = ?.
Since each re-writing rule contains two string pat-
terns of length k and each wildcard can only substi-
tute one character, a re-writing rule can only match
k-gram pairs in (s, t). We can rewrite Eq. (2) as
?r(s, t) = ?
?s?k-grams(s)
?
?t?k-grams(t)
??r(?s,?t) (3)
where ??r(?s,?t) = ? i if r (with i wildcards) matches
(?s,?t), otherwise ??r(?s,?t) = 0.
For ease of computation, we re-write kb-SRK as
Kk((s1, t1),(s2, t2))
= ?
?s1 ? k-grams(s1)
?t1 ? k-grams(t1)
?
?s2 ? k-grams(s2)
?t2 ? k-grams(t2)
K?k((?s1 ,?t1),(?s2 ,?t2))
(4)
where
K?k = ?
r?R
??r(?s1 ,?t1)??r(?s2 ,?t2) (5)
5.2 Algorithm for Computing Kernel
A straightforward computation of kb-SRK would
be intractable. The computation of Kk in Eq. (4)
needs computations of K?k conducted O((n? k +
1)4) times, where n denotes the maximum length
of strings. Furthermore, the computation of K?k in
Eq. (5) needs to perform matching of all the re-
writing rules with the two k-gram pairs (?s1 , ?t1),
(?s2 , ?t2), which has time complexity O(k!).
In this section, we will introduce an efficient algo-
rithm, which can compute K?k and Kk with the time
complexities of O(k) and O(kn2), respectively. The
latter is verified empirically.
5.2.1 Transformation of Problem
For ease of manipulation, our method transforms
the computation of kernel on k-grams into the com-
putation on a new data structure called lists of dou-
bles. We first explain how to make the transforma-
tion.
Suppose that ?1,?2 ? ?k are k-grams, we use
?1[i] and ?2[i] to represent the i-th characters of
them. We call a pair of characters a double. Thus
??? denotes the set of doubles and ?Ds ,?Dt ? (??
??1 = abbccbb ;               ??2 = abcccdd; 
??1 = cbcbbcb ;               ??2 = cbccdcd;  
Figure 2: Example of two k-gram pairs.
???= ?a?a???b?b?????????c?c???c?c?????????????
???= ?c?c???b?b???c?c???????????????c?c??????? 
Figure 3: Example of the pair of double lists combined
from the two k-gram pairs in Fig. 2. Non-identical dou-
bles are in bold.
?)k denote lists of doubles. The following operation
combines two k-grams into a list of doubles.
?1??2 = ((?1[1],?2[1]), ? ? ? ,(?1[k],?2[k])).
We denotes ?1 ? ?2[i] as the i-th element of the
list. Fig. 3 shows example lists of doubles combined
from k-grams.
We introduce the set of identical doubles I =
{(c,c)|c ? ?} and the set of non-identical doubles
N = {(c,c?)|c,c? ? ? and c 6= c?}. Obviously, I
?
N =
??? and I
?
N = /0.
We define the set of re-writing rules for double
lists RD = {rD = (?Ds ,?Dt ,?)|?Ds ,?Dt ? (I?{?})k,?
is a bijective alignment} where ?Ds and ?Dt are lists
of identical doubles including wildcards and with
length k. We say rule rD matches a pair of double
lists (?Ds ,?Dt ) iff. ?Ds ,?Dt can be changed into ?Ds
and ?Dt by substituting each wildcard pair to a dou-
ble in ??? , and the double substituting the wild-
card pair ?Ds [i] and ?Dt [ j] must be an identical dou-
ble when there is an alignment (i, j) ? ? . The rule
set defined here and the rule set in Sec. 4 only differ
on the elements where re-writing occurs. Fig. 4 (B)
shows an example of re-writing rule for double lists.
The pair of double lists in Fig. 3 can match with the
re-writing rule.
5.2.2 Computing K?k
We consider how to compute K?k by extending the
computation from k-grams to double lists.
The following lemma shows that computing the
weighted sum of re-writing rules matching k-gram
pairs (?s1 ,?t1) and (?s2 ,?t2) is equivalent to com-
puting the weighted sum of re-writing rules for dou-
ble lists matching (?s1??s2 ,?t1??t2).
452
                           a b * 1  c                    a b ?   c c ?   ?                         (a,a) (b,b)  ?   (c ,c)  (c ,c)  ?   ?                                          
                             
       c b c ?   ?   c ?                          (c, c ) (b,b)  (c ,c)  ?   ?   (c ,c)  ?                    
(A) (B) 
Figure 4: For re-writing rule (A) matching both k-gram
pairs shown in Fig. 2, there is a corresponding re-writing
rule for double lists (B) matching the pair of double lists
shown in Fig. 3.
?????????=??a?a?????b?b?????????????????????c?c???? 
?????????=??a?a?????b?b?????????????????????c?c???? 
Figure 5: Example of #???(?) for the two double lists
shown in Fig. 3. Doubles not appearing in both ?Ds and
?Dt are not shown.
Lemma 1. For any two k-gram pairs (?s1 ,?t1) and
(?s2 ,?t2), there exists a one-to-one mapping from
the set of re-writing rules matching them to the set of
re-writing rules matching the corresponding double
lists (?s1??s2 ,?t1??t2).
The re-writing rule in Fig. 4 (A) matches the k-
gram pairs in Fig. 2. Equivalently, the re-writing
rule for double lists in Fig. 4 (B) matches the pair
of double lists in Fig. 3. By lemma 1 and Eq. 5, we
have
K?k = ?
rD?RD
??rD(?s1??s2 ,?t1??t2) (6)
where ??rD(?Ds ,?Dt ) = ? 2i if the rewriting rule for
double lists rD with i wildcards matches (?Ds ,?Dt ),
otherwise ??rD(?Ds ,?Dt ) = 0. To get K?k, we just need
to compute the weighted sum of re-writing rules for
double lists matching (?s1 ??s2 ,?t1 ??t2). Thus,
we can work on the ?combined? pair of double lists
instead of two pairs of k-grams.
Instead of enumerating all possible re-writing
rules and checking whether they can match the given
pair of double lists, we only calculate the number of
possibilities of ?generating? from the pair of double
lists to the re-writing rules matching it, which can be
carried out efficiently. We say that a re-writing rule
of double lists can be generated from a pair of double
lists (?Ds , ?Dt ), if they match with each other. From
the definition of RD, in each generation, the identi-
cal doubles in ?Ds and ?Dt can be either or not sub-
stituted by an aligned wildcard pair in the re-writing
Algorithm 1: Computing K?k
Input: k-gram pair (?s1 ,?t1) and (?s2 ,?t2)
Output: K?k((?s1 ,?t1),(?s2 ,?t2))
1 Set (?Ds ,?Dt ) = (?s1??s2 ,?t1??t2) ;
2 Compute #???(?Ds ) and #???(?Dt );
3 result=1;
4 for each e ? ??? satisfies
#e(?Ds )+#e(?Dt ) 6= 0 do
5 ge = 0, ne = min{#e(?Ds ),#e(?Dt )} ;
6 for 0? i? ne do
7 ge = ge +a
(e)
i ? 2i;
8 result = result ?g;
9 return result;
rule, and all the non-identical doubles in ?Ds and ?Dt
must be substituted by aligned wildcard pairs. From
this observation and Eq. 6, K?k only depends on the
number of times each double occurs in the double
lists.
Let e be a double. We denote #e(?D) as the num-
ber of times e occurs in the list of doubles ?D. Also,
for a set of doubles S? ???, we denote #S(?D) as
a vector in which each element represents #e(?D) of
each double e ? S. We can find a function g such
that
K?k = g(#???(?s1??s2),#???(?t1??t2)) (7)
Alg. 1 shows how to compute K?k. #???(.) is com-
puted from the two pairs of k-grams in line 1-2. The
final score is made through the iterative calculation
on the two lists (lines 4-8).
The key of Alg. 1 is the calculation of ge based on
a(e)i (line 7). Here we use a
(e)
i to denote the number
of possibilities for which i pairs of aligned wildcards
can be generated from e in both ?Ds and ?Dt . a
(e)
i can
be computed as follows.
(1) If e ? N and #e(?Ds ) 6= #e(?Dt ), then a
(e)
i = 0
for any i.
(2) If e?N and #e(?Ds ) = #e(?Dt ) = j, then a
(e)
j =
j! and a(e)i = 0 for any i 6= j.
(3) If e ? I, then a(e)i =
(#e(?Ds )
i
)(#e(?Dt )
i
)
i!.
We next explain the rationale behind the above
computations. In (1), since #e(?Ds ) 6= #e(?Dt ), it is
impossible to generate a re-writing rule in which all
453
the occurrences of non-identical double e are substi-
tuted by pairs of aligned wildcards. In (2), j pairs of
aligned wildcards can be generated from all the oc-
currences of non-identical double e in both ?Ds and
?Dt . The number of combinations thus is j!. In (3),
a pair of aligned wildcards can either be generated
or not from a pair of identical doubles in ?Ds and
?Dt . We can select i occurrences of identical double
e from ?Ds , i occurrences from ?Dt , and generate all
possible aligned wildcards from them.
In the loop of lines 4-8, we only need to con-
sider a(e)i for 0? i?min{#e(?Ds ),#e(?Dt )}, because
a(e)i = 0 for the rest of i.
To sum up, Eq. 7 can be computed as below,
which is exactly the computation at lines 3-8.
g(#???(?Ds ),#???(?Dt )) = ?
e????
(
ne
?
i=0
a(e)i ?
2i) (8)
For the k-gram pairs in Fig. 2, we first create
lists of doubles in Fig. 3 and compute #???(?) for
them (lines 1-2 of Alg. 1), as shown in Fig. 5. We
next compute Kk from #???(?Ds ) and #???(?Dt ) in
Fig. 5 (lines 3-8 of Alg. 1) and obtain Kk = (1)(1+
? 2)(? 2)(2? 4)(1 + 6? 2 + 6? 4) = 12? 12 + 24? 10 +
14? 8 +2? 6.
5.2.3 Computing Kk
Algorithm 2 shows how to compute Kk. It pre-
pares two maps ms and mt and two vectors of coun-
ters cs and ct . In ms and mt , each key #N(.) maps a
set of values #???(.). Counters cs and ct count the
frequency of each #???(.). Recall that #N(?s1??s2)
denotes a vector whose element is #e(?s1 ??s2) for
e ? N. #???(?s1 ??s2) denotes a vector whose ele-
ment is #e(?s1??s2) where e is any possible double.
One can easily verify the output of the al-
gorithm is exactly the value of Kk. First,
K?k((?s1 ,?t1),(?s2 ,?t2)) = 0 if #N(?s1 ? ?s2) 6=
#N(?t1 ??t2). Therefore, we only need to consider
those ?s1 ??s2 and ?t1 ??t2 which have the same
key (lines 10-13). We group the k-gram pairs by
their key in lines 2-5 and lines 6-9.
Moreover, the following relation holds
K?k((?s1 ,?t1),(?s2 ,?t2)) = K?k((?
?
s1 ,?
?
t1),(?
?
s2 ,?
?
t2))
if #???(?s1??s2) = #???(?
?
s1??
?
s2) and #???(?t1?
?t2) = #???(?
?
t1 ??
?
t2), where ?
?
s1 , ?
?
s2 , ?
?
t1 , ?
?
t2 are
Algorithm 2: Computing Kk
Input: string pair (s1, t1) and (s2, t2), window
size k
Output: Kk((s1, t1),(s2, t2))
1 Initialize two maps ms and mt and two counters
cs and ct ;
2 for each k-gram ?s1 in s1 do
3 for each k-gram ?s2 in s2 do
4 Update ms with key-value pair
(#N(?s1??s2),#???(?s1??s2));
5 cs[#???(?s1??s2)]++ ;
6 for each k-gram ?t1 in t1 do
7 for each k-gram ?t2 in t2 do
8 Update mt with key-value pair
(#N(?t1??t2),#???(?t1??t2));
9 ct [#???(?t1??t2)]++ ;
10 for each key ?ms.keys?mt .keys do
11 for each vs ?ms[key] do
12 for each vt ?mt [key] do
13 result+= cs[vs]ct [vt ]g(vs,vt) ;
14 return result;
other k-grams. Therefore, we only need to take
#???(?s1??s2) and #???(?t1??t2) as the value un-
der each key and count its frequency. That is to say,
#??? provides sufficient statistics for computing K?k.
The quantity g(vs,vt) in line 13 is computed by
Alg. 1 (lines 3-8).
5.3 Time Complexity
The time complexities of Alg. 1 and Alg. 2 are
shown below.
For Alg. 1, lines 1-2 can be executed in
O(k). The time for executing line 7 is less
than #e(?Ds ) + #e(?Dt ) + 1 for each e satisfying
#e(?Ds ) 6= 0 or #e(?Dt ) 6= 0 . Since ?e???? #e(?Ds ) =
?e???? #e(?Dt ) = k, the time for executing lines 3-8
is less than 4k, which results in the O(k) time com-
plexity of Alg. 1.
For Alg. 2, we denote n = max{|s1|, |s2|, |t1|, |t2|}.
It is easy to see that if the maps and counters in the
algorithm are implemented by hash maps, the time
complexities of lines 2-5 and lines 6-9 are O(kn2).
However, analyzing the time complexity of lines 10-
454
                           a b * 1  c            
0
0.5
1
1.5
2
2.5
1 2 3 4 5 6 7 8
C/
n a
vg
2 
window size  K 
Worst
Avg.
Figure 6: Relation between ratio C/n2avg and window size
k when running Alg. 2 on MSR Paraphrases Corpus.
13 is quite difficult.
Lemma 2 and Theorem 1 provide an upper bound
of the number of times computing g(vs,vt) in line 13,
denoted as C.
Lemma 2. For ?s1 ?k-grams(s1) and ?s2 ,?
?
s2 ?k-
grams(s2), we have #???(?s1??s2) =
#???(?s1??
?
s2) if #N(?s1??s2) = #N(?s1??
?
s2).
Theorem 1. C is O(n3).
By Lemma 2, each ms[key] contains at most
n? k + 1 elements. Together with the fact that
?key ms[key] = (n? k + 1)
2, Theorem 1 is proved.
It can be also proved that C is O(n2) when k = 1.
Empirical study shows that O(n3) is a loose upper
bound for C. Let navg denote the average length of
s1, t1, s2 and t2. Our experiment on all pairs of sen-
tences on MSR Paraphrase (Fig. 6) shows that C is in
the same order of n2avg in the worst case and C/n
2
avg
decreases with increasing k in both average case and
worst case, which indicates that C is O(n2) and the
overall time complexity of Alg. 2 is O(kn2).
6 Experiments
We evaluated the performances of the three types
of string re-writing kernels on paraphrase identifica-
tion and recognizing textual entailment: pairwise k-
spectrum kernel (ps-SRK), pairwise k-wildcard ker-
nel (pw-SRK), and k-gram bijective string re-writing
kernel (kb-SRK). We set ? = 1 for all kernels. The
performances were measured by accuracy (e.g. per-
centage of correct classifications).
In both experiments, we used LIBSVM with de-
fault parameters (Chang et al, 2011) as the clas-
sifier. All the sentences in the training and test
sets were segmented into words by the tokenizer at
OpenNLP (Baldrige et al, ). We further conducted
stemming on the words with Iveonik English Stem-
mer (http://www.iveonik.com/ ).
We normalized each kernel by K?(x,y) =
K(x,y)?
K(x,x)K(y,y)
and then tried them under different
window sizes k. We also tried to combine the
kernels with two lexical features ?unigram precision
and recall? proposed in (Wan et al, 2006), referred
to as PR. For each kernel K, we tested the window
size settings of K1 + ...+Kkmax (kmax ? {1,2,3,4})
together with the combination with PR and we
report the best accuracies of them in Tab 1 and
Tab 2.
6.1 Paraphrase Identification
The task of paraphrase identification is to examine
whether two sentences have the same meaning. We
trained and tested all the methods on the MSR Para-
phrase Corpus (Dolan and Brockett, 2005; Quirk
et al, 2004) consisting of 4,076 sentence pairs for
training and 1,725 sentence pairs for testing.
The experimental results on different SRKs are
shown in Table 1. It can be seen that kb-SRK out-
performs ps-SRK and pw-SRK. The results by the
state-of-the-art methods reported in previous work
are also included in Table 1. kb-SRK outperforms
the existing lexical approach (Zhang and Patrick,
2005) and kernel approach (Lintean and Rus, 2011).
It also works better than the other approaches listed
in the table, which use syntactic trees or dependency
relations.
Fig. 7 gives detailed results of the kernels under
different maximum k-gram lengths kmax with and
without PR. The results of ps-SRK and pw-SRK
without combining PR under different k are all be-
low 71%, therefore they are not shown for clar-
Method Acc.
Zhang and Patrick (2005) 71.9
Lintean and Rus (2011) 73.6
Heilman and Smith (2010) 73.2
Qiu et al (2006) 72.0
Wan et al (2006) 75.6
Das and Smith (2009) 73.9
Das and Smith (2009)(PoE) 76.1
Our baseline (PR) 73.6
Our method (ps-SRK) 75.6
Our method (pw-SRK) 75.0
Our method (kb-SRK) 76.3
Table 1: Comparison with state-of-the-arts on MSRP.
455
                           a b * 1  c            
73.5
74
74.5
75
75.5
76
76.5
1 2 3 4
Accura
cy (%)
 
w i ndow size kmax 
kb_SR K+ PR
kb_SR K
ps_SRK +PR
pw_SRK +PR
P R
Figure 7: Performances of different kernels under differ-
ent maximum window size kmax on MSRP.
ity. By comparing the results of kb-SRK and pw-
SRK we can see that the bijective property in kb-
SRK is really helpful for improving the performance
(note that both methods use wildcards). Further-
more, the performances of kb-SRK with and without
combining PR increase dramatically with increasing
kmax and reach the peaks (better than state-of-the-art)
when kmax is four, which shows the power of the lex-
ical and structural similarity captured by kb-SRK.
6.2 Recognizing Textual Entailment
Recognizing textual entailment is to determine
whether a sentence (sometimes a short paragraph)
can entail the other sentence (Giampiccolo et al,
2007). RTE-3 is a widely used benchmark dataset.
Following the common practice, we combined the
development set of RTE-3 and the whole datasets of
RTE-1 and RTE-2 as training data and took the test
set of RTE-3 as test data. The train and test sets con-
tain 3,767 and 800 sentence pairs.
The results are shown in Table 2. Again, kb-SRK
outperforms ps-SRK and pw-SRK. As indicated
in (Heilman and Smith, 2010), the top-performing
RTE systems are often built with significant engi-
Method Acc.
Harmeling (2007) 59.5
de Marneffe et al (2006) 60.5
M&M, (2007) (NL) 59.4
M&M, (2007) (Hybrid) 64.3
Zanzotto et al (2007) 65.75
Heilman and Smith (2010) 62.8
Our baseline (PR) 62.0
Our method (ps-SRK) 64.6
Our method (pw-SRK) 63.8
Our method (kb-SRK) 65.1
Table 2: Comparison with state-of-the-arts on RTE-3.
                           a b * 1  c            
60.5
61.5
62.5
63.5
64.5
65.5
1 2 3 4
Accura
cy (%)
 
w i ndow size kmax 
kb_SR K+ PR
kb_SR K
ps_SRK +PR
pw_SRK +PR
PR
Figure 8: Performances of different kernels under differ-
ent maximum window size kmax on RTE-3.
neering efforts. Therefore, we only compare with
the six systems which involves less engineering. kb-
SRK still outperforms most of those state-of-the-art
methods even if it does not exploit any other lexical
semantic sources and syntactic analysis tools.
Fig. 8 shows the results of the kernels under dif-
ferent parameter settings. Again, the results of ps-
SRK and pw-SRK without combining PR are too
low to be shown (all below 55%). We can see that
PR is an effective method for this dataset and the
overall performances are substantially improved af-
ter combining it with the kernels. The performance
of kb-SRK reaches the peak when window size be-
comes two.
7 Conclusion
In this paper, we have proposed a novel class of ker-
nel functions for sentence re-writing, called string
re-writing kernel (SRK). SRK measures the lexical
and structural similarity between two pairs of sen-
tences without using syntactic trees. The approach
is theoretically sound and is flexible to formulations
of sentences. A specific instance of SRK, referred
to as kb-SRK, has been developed which can bal-
ance the effectiveness and efficiency for sentence
re-writing. Experimental results show that kb-SRK
achieve better results than state-of-the-art methods
on paraphrase identification and recognizing textual
entailment.
Acknowledgments
This work is supported by the National Basic Re-
search Program (973 Program) No. 2012CB316301.
References
Baldrige, J. , Morton, T. and Bierner G. OpenNLP.
http://opennlp.sourceforge.net/.
456
Barzilay, R. and Lee, L. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pp. 16?23.
Basilico, J. and Hofmann, T. 2004. Unifying collab-
orative and content-based filtering. Proceedings of
the twenty-first international conference on Machine
learning, pp. 9, 2004.
Ben-Hur, A. and Noble, W.S. 2005. Kernel methods for
predicting protein?protein interactions. Bioinformat-
ics, vol. 21, pp. i38?i46, Oxford Univ Press.
Bhagat, R. and Ravichandran, D. 2008. Large scale ac-
quisition of paraphrases for learning surface patterns.
Proceedings of ACL-08: HLT, pp. 674?682.
Chang, C. and Lin, C. 2011. LIBSVM: A library for sup-
port vector machines. ACM Transactions on Intelli-
gent Systems and Technology vol. 2, issue 3, pp. 27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm
Das, D. and Smith, N.A. 2009. Paraphrase identifi-
cation as probabilistic quasi-synchronous recognition.
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pp. 468?476.
de Marneffe, M., MacCartney, B., Grenager, T., Cer, D.,
Rafferty A. and Manning C.D. 2006. Learning to dis-
tinguish valid textual entailments. Proc. of the Second
PASCAL Challenges Workshop.
Dolan, W.B. and Brockett, C. 2005. Automatically con-
structing a corpus of sentential paraphrases. Proc. of
IWP.
Giampiccolo, D., Magnini B., Dagan I., and Dolan B.,
editors 2007. The third pascal recognizing textual en-
tailment challenge. Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pp. 1?9.
Harmeling, S. 2007. An extensible probabilistic
transformation-based approach to the third recogniz-
ing textual entailment challenge. Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pp. 137?142, 2007.
Heilman, M. and Smith, N.A. 2010. Tree edit models for
recognizing textual entailments, paraphrases, and an-
swers to questions. Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pp. 1011-1019.
Kashima, H. , Oyama, S. , Yamanishi, Y. and Tsuda, K.
2009. On pairwise kernels: An efficient alternative
and generalization analysis. Advances in Knowledge
Discovery and Data Mining, pp. 1030-1037, 2009,
Springer.
Kimeldorf, G. and Wahba, G. 1971. Some results on
Tchebycheffian spline functions. Journal of Mathemat-
ical Analysis and Applications, Vol.33, No.1, pp.82-
95, Elsevier.
Lin, D. and Pantel, P. 2001. DIRT-discovery of inference
rules from text. Proc. of ACM SIGKDD Conference
on Knowledge Discovery and Data Mining.
Lintean, M. and Rus, V. 2011. Dissimilarity Kernels
for Paraphrase Identification. Twenty-Fourth Interna-
tional FLAIRS Conference.
Leslie, C. , Eskin, E. and Noble, W.S. 2002. The spec-
trum kernel: a string kernel for SVM protein classifi-
cation. Pacific symposium on biocomputing vol. 575,
pp. 564-575, Hawaii, USA.
Leslie, C. and Kuang, R. 2004. Fast string kernels using
inexact matching for protein sequences. The Journal
of Machine Learning Research vol. 5, pp. 1435-1455.
Lodhi, H. , Saunders, C. , Shawe-Taylor, J. , Cristianini,
N. and Watkins, C. 2002. Text classification using
string kernels. The Journal of Machine Learning Re-
search vol. 2, pp. 419-444.
MacCartney, B. and Manning, C.D. 2008. Modeling se-
mantic containment and exclusion in natural language
inference. Proceedings of the 22nd International Con-
ference on Computational Linguistics, vol. 1, pp. 521-
528, 2008.
Moschitti, A. and Zanzotto, F.M. 2007. Fast and Effec-
tive Kernels for Relational Learning from Texts. Pro-
ceedings of the 24th Annual International Conference
on Machine Learning, Corvallis, OR, USA, 2007.
Qiu, L. and Kan, M.Y. and Chua, T.S. 2006. Para-
phrase recognition via dissimilarity significance clas-
sification. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pp. 18?26.
Quirk, C. , Brockett, C. and Dolan, W. 2004. Monolin-
gual machine translation for paraphrase generation.
Proceedings of EMNLP 2004, pp. 142-149, Barcelona,
Spain.
Scho?lkopf, B. and Smola, A.J. 2002. Learning with
kernels: Support vector machines, regularization, op-
timization, and beyond. The MIT Press, Cambridge,
MA.
Vapnik, V.N. 2000. The nature of statistical learning
theory. Springer Verlag.
Wan, S. , Dras, M. , Dale, R. and Paris, C. 2006. Using
dependency-based features to take the ?Para-farce?
out of paraphrase. Proc. of the Australasian Language
Technology Workshop, pp. 131?138.
Zanzotto, F.M. , Pennacchiotti, M. and Moschitti, A.
2007. Shallow semantics in fast textual entailment
457
rule learners. Proceedings of the ACL-PASCAL
workshop on textual entailment and paraphrasing, pp.
72?77.
Zhang, Y. and Patrick, J. 2005. Paraphrase identifica-
tion by text canonicalization. Proceedings of the Aus-
tralasian Language Technology Workshop, pp. 160?
166.
458
