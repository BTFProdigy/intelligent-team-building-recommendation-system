Speed and Accuracy in Shallow and Deep Stochastic Parsing
Ronald M. Kaplan , Stefan Riezler , Tracy Holloway King
John T. Maxwell III, Alexander Vasserman and Richard Crouch
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{kaplan|riezler|king|maxwell|avasserm|crouch}@parc.com
Abstract
This paper reports some experiments that com-
pare the accuracy and performance of two
stochastic parsing systems. The currently pop-
ular Collins parser is a shallow parser whose
output contains more detailed semantically-
relevant information than other such parsers.
The XLE parser is a deep-parsing system that
couples a Lexical Functional Grammar to a log-
linear disambiguation component and provides
much richer representations theory. We mea-
sured the accuracy of both systems against a
gold standard of the PARC 700 dependency
bank, and also measured their processing times.
We found the deep-parsing system to be more
accurate than the Collins parser with only a
slight reduction in parsing speed.1
1 Introduction
In applications that are sensitive to the meanings ex-
pressed by natural language sentences, it has become
common in recent years simply to incorporate publicly
available statistical parsers. A state-of-the-art statistical
parsing system that enjoys great popularity in research
systems is the parser described in Collins (1999) (hence-
forth ?the Collins parser?). This system not only is fre-
quently used for off-line data preprocessing, but also
is included as a black-box component for applications
such as document summarization (Daume and Marcu,
2002), information extraction (Miller et al, 2000), ma-
chine translation (Yamada and Knight, 2001), and ques-
tion answering (Harabagiu et al, 2001). This is be-
1This research has been funded in part by contract #
MDA904-03-C-0404 awarded from the Advanced Research and
Development Activity, Novel Intelligence from Massive Data
program. We would like to thank Chris Culy whose original ex-
periments inspired this research.
cause the Collins parser shares the property of robustness
with other statistical parsers, but more than other such
parsers, the categories of its parse-trees make grammati-
cal distinctions that presumably are useful for meaning-
sensitive applications. For example, the categories of
the Model 3 Collins parser distinguish between heads,
arguments, and adjuncts and they mark some long-
distance dependency paths; these distinctions can guide
application-specific postprocessors in extracting impor-
tant semantic relations.
In contrast, state-of-the-art parsing systems based on
deep grammars mark explicitly and in much more de-
tail a wider variety of syntactic and semantic dependen-
cies and should therefore provide even better support for
meaning-sensitive applications. But common wisdom has
it that parsing systems based on deep linguistic grammars
are too difficult to produce, lack coverage and robustness,
and also have poor run-time performance. The Collins
parser is thought to be accurate and fast and thus to repre-
sent a reasonable trade-off between ?good-enough? out-
put, speed, and robustness.
This paper reports on some experiments that put this
conventional wisdom to an empirical test. We investi-
gated the accuracy of recovering semantically-relevant
grammatical dependencies from the tree-structures pro-
duced by the Collins parser, comparing these dependen-
cies to gold-standard dependencies which are available
for a subset of 700 sentences randomly drawn from sec-
tion 23 of the Wall Street Journal (see King et al (2003)).
We compared the output of the XLE system, a
deep-grammar-based parsing system using the English
Lexical-Functional Grammar previously constructed as
part of the Pargram project (Butt et al, 2002), to the
same gold standard. This system incorporates sophisti-
cated ambiguity-management technology so that all pos-
sible syntactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and Ka-
plan, 1993). In accordance with LFG theory, the output
includes not only standard context-free phrase-structure
trees but also attribute-value matrices (LFG?s f(unctional)
structures) that explicitly encode predicate-argument re-
lations and other meaningful properties. XLE selects the
most probable analysis from the potentially large candi-
date set by means of a stochastic disambiguation com-
ponent based on a log-linear (a.k.a. maximum-entropy)
probability model (Riezler et al, 2002). The stochas-
tic component is also ?ambiguity-enabled? in the sense
that the computations for statistical estimation and selec-
tion of the most probable analyses are done efficiently
by dynamic programming, avoiding the need to unpack
the parse forests and enumerate individual analyses. The
underlying parsing system also has built-in robustness
mechanisms that allow it to parse strings that are outside
the scope of the grammar as a shortest sequence of well-
formed ?fragments?. Furthermore, performance parame-
ters that bound parsing and disambiguation work can be
tuned for efficient but accurate operation.
As part of our assessment, we also measured the pars-
ing speed of the two systems, taking into account all
stages of processing that each system requires to produce
its output. For example, since the Collins parser depends
on a prior part-of-speech tagger (Ratnaparkhi, 1996), we
included the time for POS tagging in our Collins mea-
surements. XLE incorporates a sophisticated finite-state
morphology and dictionary lookup component, and its
time is part of the measure of XLE performance.
Performance parameters of both the Collins parser and
the XLE system were adjusted on a heldout set consist-
ing of a random selection of 1/5 of the PARC 700 depen-
dency bank; experimental results were then based on the
other 560 sentences. For Model 3 of the Collins parser, a
beam size of 1000, and not the recommended beam size
of 10000, was found to optimize parsing speed at little
loss in accuracy. On the same heldout set, parameters of
the stochastic disambiguation system and parameters for
parsing performance were adjusted for a Core and a Com-
plete version of the XLE system, differing in the size of
the constraint-set of the underlying grammar.
For both XLE and the Collins parser we wrote con-
version programs to transform the normal (tree or f-
structure) output into the corresponding relations of
the dependency bank. This conversion was relatively
straightforward for LFG structures (King et al, 2003).
However, a certain amount of skill and intuition was
required to provide a fair conversion of the Collins
trees: we did not want to penalize configurations in the
Collins trees that encoded alternative but equally legit-
imate representations of the same linguistic properties
(e.g. whether auxiliaries are encoded as main verbs or
aspect features), but we also did not want to build into
the conversion program transformations that compensate
for information that Collins cannot provide without ap-
pealing to additional linguistic resources (such as identi-
fying the subjects of infinitival complements). We did not
include the time for dependency conversion in our mea-
sures of performance.
The experimental results show that stochastic parsing
with the Core LFG grammar achieves a better F-score
than the Collins parser at a roughly comparable parsing
speed. The XLE system achieves 12% reduction in error
rate over the Collins parser, that is 77.6% F-score for the
XLE system versus 74.6% for the Collins parser, at a cost
in parsing time of a factor of 1.49.
2 Stochastic Parsing with LFG
2.1 Parsing with Lexical-Functional Grammar
The grammar used for this experiment was developed in
the ParGram project (Butt et al, 2002). It uses LFG as a
formalism, producing c(onstituent)-structures (trees) and
f(unctional)-structures (attribute value matrices) as out-
put. The c-structures encode constituency and linear or-
der. F-structures encode predicate-argument relations and
other grammatical information, e.g., number, tense, state-
ment type. The XLE parser was used to produce packed
representations, specifying all possible grammar analyses
of the input.
In our system, tokenization and morphological analy-
sis are performed by finite-state transductions arranged in
a compositional cascade. Both the tokenizer and the mor-
phological analyzer can produce multiple outputs. For ex-
ample, the tokenizer will optionaly lowercase sentence
initial words, and the morphological analyzer will pro-
duce walk +Verb +Pres +3sg and walk +Noun +Pl for
the input form walks. The resulting tokenized and mor-
phologically analyzed strings are presented to the sym-
bolic LFG grammar.
The grammar can parse input that has XML de-
limited named entity markup: <company>Columbia
Savings</company> is a major holder of so-called junk
bonds. To allow the grammar to parse this markup,
the tokenizer includes an additional tokenization of the
strings whereby the material between the XML markup
is treated as a single token with a special morphologi-
cal tag (+NamedEntity). As a fall back, the tokenization
that the string would have received without that markup
is also produced. The named entities have a single mul-
tiword predicate. This helps in parsing both because it
means that no internal structure has to be built for the
predicate and because predicates that would otherwise be
unrecognized by the grammar can be parsed (e.g., Cie.
Financiere de Paribas). As described in section 5, it was
also important to use named entity markup in these ex-
periments to more fairly match the analyses in the PARC
700 dependency bank.
To increase robustness, the standard grammar is aug-
mented with a FRAGMENT grammar. This allows sen-
tences to be parsed as well-formed chunks specified by
the grammar, in particular as Ss, NPs, PPs, and VPs, with
unparsable tokens possibly interspersed. These chunks
have both c-structures and f-structures corresponding to
them. The grammar has a fewest-chunk method for de-
termining the correct parse.
The grammar incorporates a version of Optimality
Theory that allows certain (sub)rules in the grammar to be
prefered or disprefered based on OT marks triggered by
the (sub)rule (Frank et al, 1998). The Complete version
of the grammar uses all of the (sub)rules in a multi-pass
system that depends on the ranking of the OT marks in
the rules. For example, topicalization is disprefered, but
the topicalization rule will be triggered if no other parse
can be built. A one-line rewrite of the Complete grammar
creates a Core version of the grammar that moves the ma-
jority of the OT marks into the NOGOOD space. This ef-
fectively removes the (sub)rules that they mark from the
grammar. So, for example, in the Core grammar there is
no topicalization rule, and sentences with topics will re-
ceive a FRAGMENT parse. This single-pass Core grammar
is smaller than the Complete grammar and hence is faster.
The XLE parser also allows the user to adjust per-
formance parameters bounding the amount of work that
is done in parsing for efficient but accurate operation.
XLE?s ambiguity management technology takes advan-
tage of the fact that relatively few f-structure constraints
apply to constituents that are far apart in the c-structure,
so that sentences are typically parsed in polynomial time
even though LFG parsing is known to be an NP-complete
problem. But the worst-case exponential behavior does
begin to appear for some constructions in some sentences,
and the computational effort is limited by a SKIMMING
mode whose onset is controlled by a user-specified pa-
rameter. When skimming, XLE will stop processing the
subtree of a constituent whenever the amount of work ex-
ceeds that user-specified limit. The subtree is discarded,
and the parser will move on to another subtree. This guar-
antees that parsing will be finished within reasonable lim-
its of time and memory but at a cost of possibly lower
accuracy if it causes the best analysis of a constituent
to be discarded. As a separate parameter, XLE also lets
the user limit the length of medial constituents, i.e., con-
stituents that do not appear at the beginning or the end
of a sentence (ignoring punctuation). The rationale be-
hind this heuristic is to limit the weight of constituents in
the middle of the sentence but still to allow sentence-final
heavy constituents. This discards constituents in a some-
what more principled way as it tries to capture the psy-
cholinguistic tendency to avoid deep center-embedding.
When limiting the length of medial constituents, cubic-
time parsing is possible for sentences up to that length,
even with a deep, non-context-free grammar, and linear
parsing time is possible for sentences beyond that length.
The Complete grammar achieved 100% coverage of
section 23 as unseen unlabeled data: 79% as full parses,
21% FRAGMENT and/or SKIMMED parses.
2.2 Dynamic Programming for Estimation and
Stochastic Disambiguation
The stochastic disambiguation model we employ defines
an exponential (a.k.a. log-linear or maximum-entropy)
probability model over the parses of the LFG grammar.
The advantage of this family of probability distributions
is that it allows the user to encode arbitrary properties
of the parse trees as feature-functions of the probability
model, without the feature-functions needing to be inde-
pendent and non-overlapping. The general form of con-
ditional exponential models is as follows:
p?(x|y) = Z?(y)
?1e??f(x)
where Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant over the set X(y) of parses for sentence y, ? is
a vector of log-parameters, f is a vector of feature-
values, and ? ? f(x) is a vector dot product denoting the
(log-)weight of parse x.
Dynamic-programming algorithms that allow the ef-
ficient estimation and searching of log-linear mod-
els from a packed parse representation without enu-
merating an exponential number of parses have
been recently presented by Miyao and Tsujii (2002)
and Geman and Johnson (2002). These algorithms can
be readily applied to the packed and/or-forests of
Maxwell and Kaplan (1993), provided that each conjunc-
tive node is annotated with feature-values of the log-
linear model. In the notation of Miyao and Tsujii (2002),
such a feature forest ? is defined as a tuple ?C,D, r, ?, ??
where C is a set of conjunctive nodes, D is a set of dis-
junctive nodes, r ? C is the root node, ? : D ? 2C is
a conjunctive daughter function, and ? : C ? 2D is a
disjunctive daughter function.
A dynamic-programming solution to the problem of
finding most probable parses is to compute the weight
?d of each disjunctive node as the maximum weight of
its conjunctive daugher nodes, i.e.,
?d = max
c??(d)
?c (1)
and to recursively define the weight ?c of a conjunctive
node as the product of the weights of all its descendant
disjunctive nodes and of its own weight:
?c =
?
d??(c)
?d e
??f(c) (2)
Keeping a trace of the maximally weighted choices in a
computaton of the weight ?r of the root conjunctive node
r allows us to efficiently recover the most probable parse
of a sentence from the packed representation of its parses.
The same formulae can be employed for an effi-
cient calculation of probabilistic expectations of feature-
functions for the statistical estimation of the parameters
?. Replacing the maximization in equation 1 by a sum-
mation defines the inside weight of disjunctive node. Cor-
respondingly, equation 2 denotes the inside weight of a
conjunctive node. The outside weight ?c of a conjunctive
node is defined as the outside weight of its disjunctive
mother node(s):
?c =
?
{d|c??(d)}
?d (3)
The outside weight of a disjunctive node is the sum of
the product of the outside weight(s) of its conjunctive
mother(s), the weight(s) of its mother(s), and the inside
weight(s) of its disjunctive sister(s):
?d =
?
{c|d??(c)}
{?c e
??f(c)
?
{d?|d???(c),d? 6=d}
?d?} (4)
From these formulae, the conditional expectation of a
feature-function fi can be computed from a chart with
root node r for a sentence y in the following way:
?
x?X(y)
e??f(x)fi(x)
Z?(y)
=
?
c?C
?c?cfi(c)
?r
(5)
Formula 5 is used in our system to compute expectations
for discriminative Bayesian estimation from partially la-
beled data using a first-order conjugate-gradient routine.
For a more detailed description of the optimization prob-
lem and the feature-functions we use for stochastic LFG
parsing see Riezler et al (2002). We also employed a
combined `1 regularization and feature selection tech-
nique described in Riezler and Vasserman (2004) that
considerably speeds up estimation and guarantees small
feature sets for stochastic disambiguation. In the experi-
ments reported in this paper, however, dynamic program-
ming is crucial for efficient stochastic disambiguation,
i.e. to efficiently find the most probable parse from a
packed parse forest that is annotated with feature-values.
There are two operations involved in stochastic disam-
biguation, namely calculating feature-values from a parse
forest and calculating node weights from a feature forest.
Clearly, the first one is more expensive, especially for
the extraction of values for non-local feature-functions
over large charts. To control the cost of this compu-
tation, our stochastic disambiguation system includes
a user-specified parameter for bounding the amount of
work that is done in calculating feature-values. When the
user-specified threshold for feature-value calculation is
reached, this computation is discontinued, and the dy-
namic programming calculation for most-probable-parse
search is computed from the current feature-value anno-
tation of the parse forest. Since feature-value computa-
tion proceeds incrementally over the feature forest, i.e.
for each node that is visited all feature-functions that ap-
ply to it are evaluated, a complete feature annotation can
be guaranteed for the part of the and/or-forest that is vis-
ited until discontinuation. As discussed below, these pa-
rameters were set on a held-out portion of the PARC700
which was also used to set the Collins parameters.
In the experiments reported in this paper, we used a
threshold on feature-extraction that allowed us to cut off
feature-extraction in 3% of the cases at no loss in accu-
racy. Overall, feature extraction and weight calculation
accounted for 5% of the computation time in combined
parsing and stochastic selection.
3 The Gold-Standard Dependency Bank
We used the PARC 700 Dependency Bank (DEPBANK)
as the gold standard in our experiments. The DEPBANK
consists of dependency annotations for 700 sentences that
were randomly extracted from section 23 of the UPenn
Wall Street Journal (WSJ) treebank. As described by
(King et al, 2003), the annotations were boot-strapped
by parsing the sentences with a LFG grammar and trans-
forming the resulting f-structures to a collection of depen-
dency triples in the DEPBANK format. To prepare a true
gold standard of dependencies, the tentative set of depen-
dencies produced by the robust parser was then corrected
and extended by human validators2. In this format each
triple specifies that a particular relation holds between a
head and either another head or a feature value, for ex-
ample, that the SUBJ relation holds between the heads
run and dog in the sentence The dog ran. Average sen-
tence length of sentences in DEPBANK is 19.8 words, and
the average number of dependencies per sentence is 65.4.
The corpus is freely available for research and evaluation,
as are documentation and tools for displaying and prun-
ing structures.3
In our experiments we used a Reduced version of the
DEPBANK, including just the minimum set of dependen-
cies necessary for reading out the central semantic rela-
tions and properties of a sentence. We tested against this
Reduced gold standard to establish accuracy on a lower
bound of the information that a meaning-sensitive appli-
cation would require. The Reduced version contained all
the argument and adjunct dependencies shown in Fig.
1, and a few selected semantically-relevant features, as
shown in Fig. 2. The features in Fig. 2 were chosen be-
2The resulting test set is thus unseen to the grammar and
stochastic disambiguation system used in our experiments. This
is indicated by the fact that the upperbound of F-score for the
best matching parses for the experiment grammar is in the range
of 85%, not 100%.
3http://www2.parc.com/istl/groups/nltt/fsbank/
Function Meaning
adjunct adjuncts
aquant adjectival quantifiers (many, etc.)
comp complement clauses (that, whether)
conj conjuncts in coordinate structures
focus int fronted element in interrogatives
mod noun-noun modifiers
number numbers modifying nouns
obj objects
obj theta secondary objects
obl oblique
obl ag demoted subject of a passive
obl compar comparative than/as clauses
poss possessives (John?s book)
pron int interrogative pronouns
pron rel relative pronouns
quant quantifiers (all, etc.)
subj subjects
topic rel fronted element in relative clauses
xcomp non-finite complements
verbal and small clauses
Figure 1: Grammatical functions in DEPBANK.
cause it was felt that they were fundamental to the mean-
ing of the sentences, and in fact they are required by the
semantic interpreter we have used in a knowledge-based
application (Crouch et al, 2002).
Feature Meaning
adegree degree of adjectives and adverbs
(positive, comparative, superlative)
coord form form of a coordinating
conjunction (e.g., and, or)
det form form of a determiner (e.g., the, a)
num number of nouns (sg, pl)
number type cardinals vs. ordinals
passive passive verb (e.g., It was eaten.)
perf perfective verb (e.g., have eaten)
precoord form either, neither
prog progressive verb (e.g., were eating)
pron form form of a pronoun (he, she, etc.)
prt form particle in a particle verb
(e.g., They threw it out.)
stmt type statement type (declarative,
interrogative, etc.)
subord form subordinating conjunction (e.g. that)
tense tense of the verb (past, present, etc.)
Figure 2: Selected features for Reduced DEPBANK
.
As a concrete example, the dependency list in Fig. 3 is
the Reduced set corresponding to the following sentence:
He reiterated his opposition to such funding,
but expressed hope of a compromise.
An additional feature of the DEPBANK that is relevant
to our comparisons is that dependency heads are rep-
resented by their standard citation forms (e.g. the verb
swam in a sentence appears as swim in its dependencies).
We believe that most applications will require a conver-
sion to canonical citation forms so that semantic relations
can be mapped into application-specific databases or on-
tologies. The predicates of LFG f-structures are already
represented as citation forms; for a fair comparison we
ran the leaves of the Collins tree through the same stem-
mer modules as part of the tree-to-dependency transla-
tion. We also note that proper names appear in the DEP-
BANK as single multi-word expressions without any in-
ternal structure. That is, there are no dependencies hold-
ing among the parts of people names (A. Boyd Simpson),
company names (Goldman, Sachs & Co), and organiza-
tion names (Federal Reserve). This multiword analysis
was chosen because many applications do not require
the internal structure of names, and the identification of
named entities is now typically carried out by a separate
non-syntactic pre-processing module. This was captured
for the LFG parser by using named entity markup and for
the Collins parser by creating complex word forms with
a single POS tag (section 5).
conj(coord?0, express?3)
conj(coord?0, reiterate?1)
coord form(coord?0, but)
stmt type(coord?0, declarative)
obj(reiterate?1, opposition?6)
subj(reiterate?1, pro?7)
tense(reiterate?1, past)
obj(express?3, hope?15)
subj(express?3, pro?7)
tense(express?3, past)
adjunct(opposition?6, to?11)
num(opposition?6, sg)
poss(opposition?6, pro?19)
num(pro?7, sg)
pron form(pro?7, he)
obj(to?11, funding?13)
adjunct(funding?13, such?45)
num(funding?13, sg)
adjunct(hope?15, of?46)
num(hope?15, sg)
num(pro?19, sg)
pron form(pro?19, he)
adegree(such?45, positive)
obj(of?46, compromise?54)
det form(compromise?54, a)
num(compromise?54, sg)
Figure 3: Reduced dependency relations for He reiterated
his opposition to such funding, but expressed hope of a
compromise.
4 Conversion to Dependency Bank Format
A conversion routine was required for each system to
transform its output so that it could be compared to the
DEPBANK dependencies. While it is relatively straightfor-
ward to convert LFG f-structures to the dependency bank
format because the f-structure is effectively a dependency
format, it is more difficult to transform the output trees of
the Model 3 Collins parser in a way that fairly allocates
both credits and penalties.
LFG Conversion We discarded the LFG tree structures
and used a general rewriting system previously developed
for machine translation to rewrite the relevant f-structure
attributes as dependencies (see King et al (2003)). The
rewritings involved some deletions of irrelevant features,
some systematic manipulations of the analyses, and some
trivial respellings. The deletions involved features pro-
duced by the grammar but not included in the PARC 700
such as negative values of PASS, PERF, and PROG and
the feature MEASURE used to mark measure phrases. The
manipulations are more interesting and are necessary to
map systematic differences between the analyses in the
grammar and those in the dependency bank. For example,
coordination is treated as a set by the LFG grammar but as
a single COORD dependency with several CONJ relations
in the dependency bank. Finally, the trivial rewritings
were used to, for example, change STMT-TYPE decl in
the grammar to STMT-TYPE declarative in the de-
pendency bank. For the Reduced version of the PARC
700 substantially more features were deleted.
Collins Model 3 Conversion An abbreviated represen-
tation of the Collins tree for the example above is shown
in Fig. 4. In this display we have eliminated the head lex-
ical items that appear redundantly at all the nonterminals
in a head chain, instead indicating by a single number
which daughter is the head. Thus, S?2 indicates that the
head of the main clause is its second daughter, the VP,
and its head is its first VP daughter. Indirectly, then, the
lexical head of the S is the first verb reiterated.
(TOP?1
(S?2 (NP-A?1 (NPB?1 He/PRP))
(VP?1 (VP?1 reiterated/VBD
(NP-A?1 (NPB?2 his/PRP$
opposition/NN)
(PP?1 to/TO
(NPB?2 such/JJ
funding/NN))))
but/CC
(VP?1 expressed/VBD
(NP-A?1 (NPB?1 hope/NN)
(PP?1 of/IN
(NP-A?1 (NPB?2 a/DT
compromise/NN))))))))
Figure 4: Collins Model 3 tree for He reiterated his op-
position to such funding, but expressed hope of a compro-
mise.
The Model 3 output in this example includes standard
phrase structure categories, indications of the heads, and
the additional -A marker to distinguish arguments from
adjuncts. The terminal nodes of this tree are inflected
forms, and the first phase of our conversion replaces them
with their citation forms (the verbs reiterate and express,
and the decapitalized and standardized he for He and his).
We also adjust for systematic differences in the choice of
heads. The first conjunct tends to be marked as the head
of a coordination in Model 3 output, whereas the depen-
dency bank has a more symmetric representation: it in-
troduces a new COORD head and connects that up to the
conjunction, and it uses a separate CONJ relation for each
of the coordinated items. Similarly, Model 3 identifies
the syntactic markers to and that as the heads of com-
plements, whereas the dependency bank treats these as
selectional features and marks the main predicate of the
complements as the head. These adjustments are carried
out without penalty. We also compensate for the differ-
ences in the representation of auxiliaries: Model 3 treats
these as main verbs with embedded complements instead
of the PERF, PROG, and PASSIVE features of the DEP-
BANK, and our conversion flattens the trees so that the
features can be read off.
The dependencies are read off after these and a few
other adjustments are made. NPs under VPs are read off
either as objects or adjuncts, depending on whether or
not the NP is annotated with the argument indicator (-A)
as in this example; the -A presumably would be miss-
ing in a sentence like John arrived Friday, and Friday
would be treated as an ADJUNCT. Similarly, NP-As un-
der S are read off as subject. In this example, however,
this principle of conversion does not lead to a match with
the dependency bank: in the DEPBANK grammatical rela-
tions that are factored out of conjoined structures are dis-
tributed back into those structures, to establish the correct
semantic dependencies (in this case, that he is the subject
of both reiterate and express and not of the introduced
coord). We avoided the temptation of building coordinate
distribution into the conversion routine because, first, it is
not always obvious from the Model 3 output when dis-
tribution should take place, and second, that would be
a first step towards building into the conversion routine
the deep lexical and syntactic knowledge (essentially the
functional component of our LFG grammar) that the shal-
low approach explicitly discounts4.
For the same reasons our conversion routine does not
identify the subjects of infinitival complements with par-
ticular arguments of matrix verbs. The Model 3 trees pro-
vide no indication of how this is to be done, and in many
cases the proper assignment depends on lexical informa-
tion about specific predicates (to capture, for example, the
well-known contrast between promise and persuade).
Model 3 trees also provide information about certain
4However, we did explore a few of these additional transfor-
mations and found only marginal F-score increases.
long-distance dependencies, by marking with -g annota-
tions the path between a filler and a gap and marking the
gap by an explicit TRACE in the terminal string. The filler
itself is not clearly identified, but our conversion treats
all WH categories under SBAR as potential fillers and
attempts to propagate them down the gap-chain to link
them up to appropriate traces.
In sum, it is not a trivial matter to convert a Model 3
tree to an appropriate set of dependency relations, and the
process requires a certain amount of intuition and skill.
For our experiments we tried to define a conversion that
gives appropriate credit to the dependencies that can be
read from the trees without relying on an undue amount
of sophisticated linguistic knowledge5.
5 Experiments
We conducted our experiments by preparing versions of
the test sentences in the form appropriate to each sys-
tem. We used a configuration of the XLE parser that ex-
pects sentences conforming to ordinary text conventions
to appear in a file separated by double line-feeds. A cer-
tain amount of effort was required to remove the part-of-
speech tags and labeled brackets of the WSJ corpus in a
way that restored the sentences to a standard English for-
mat (for example, to remove the space between wo and n?t
that remains when the POS tags are removed). Since the
PARC 700 treats proper names as multiword expressions,
we then augmented the input strings with XML markup
of the named entities. These are parsed by the grammar
as described in section 2. We used manual named entity
markup for this experiment because our intent is to mea-
sure parsing technology independent of either the time
or errors of an automatic named-entity extractor. How-
ever, in other experiments with an automatic finite-state
extractor, we have found that the time for named-entity
recognition is negligible (on the order of seconds across
the entire corpus) and makes relatively few errors, so that
the results reported here are good approximations of what
might be expected in more realistic situations.
As input to the Collins parser, we used the part-of-
speech tagged version of section 23 that was provided
with the parser. From this we extracted the 700 sentences
in the PARC 700. We then modified them to produce
named entity input so that the parses would match the
PARC 700. This was done by putting underscores be-
tween the parts of the named entity and changing the final
part of speech tag to the appropriate one (usually NNP)
if necessary. (The number of words indicated at the be-
ginning of the input string was also reduced accordingly.)
An example is shown in (1).
5The results of this conversion are available at
http://www2.parc.com/istl/groups/nltt/fsbank/
(1) Sen. NNP Christopher NNP Dodd NNP ??
Sen. Christopher Dodd NNP
After parsing, the underscores were converted to spaces
to match the PARC 700 predicates.
Before the final evaluation, 1/5 of the PARC 700 de-
pendency bank was randomly extracted as a heldout set.
This set was used to adjust the performance parameters of
the XLE system and the Collins parser so as to optimize
parsing speed without losing accuracy. For example, the
limit on the length of medial phrases was set to 20 words
for the XLE system (see Sec. 2), and a regularizer penalty
of 10 was found optimal for the `1 prior used in stochas-
tic disambiguation. For the Collins parser, a beam size
of 1000 was found to improve speed considerably at lit-
tle cost in accuracy. Furthermore, the np-bracketing flag
(npbflag) was set to 0 to produce an extended set of NP
levels for improved argument/adjunct distinction6. The fi-
nal evaluation was done on the remaining 560 examples.
Timing results are reported in seconds of CPU time7. POS
tagging of the input to the Collins parser took 6 seconds
and this was added to the timing result of the Collins
parser. Time spent for finite-state morphology and dictio-
nary lookup for XLE is part of the measure of its timing
performance. We did not include the time for dependency
extraction or stemming the Collins output.
Table 1 shows timing and accuracy results for the Re-
duced dependency set. The parser settings compared are
Model 3 of the Collins parser adjusted to beam size 1000,
and the Core and Complete versions of the XLE sys-
tem, differing in the size of the grammar?s constraint-
set. Clearly, both versions of the XLE system achieve a
significant reduction in error rate over the Collins parser
(12% for the core XLE system and 20% for the complete
system) at an increase in parsing time of a factor of only
1.49 for the core XLE system. The complete version gives
an overall improvement in F-score of 5% over the Collins
parser at a cost of a factor of 5 in parsing time.
Table 1: Timing and accuracy results for Collins parser
and Complete and Core versions of XLE system on Re-
duced version of PARC 700 dependency bank.
time prec. rec. F-score
LFG core 298.88 79.1 76.2 77.6
LFG complete 985.3 79.4 79.8 79.6
Collins 1000 199.6 78.3 71.2 74.6
6A beam size of 10000 as used in Collins (1999) improved
the F-score on the heldout set only by .1% at an increase of pars-
ing time by a factor of 3. Beam sizes lower than 1000 decreased
the heldout F-score significantly.
7All experiments were run on one CPU of a dual proces-
sor AMD Opteron 244 with 1.8 GHz and 4GB main memory.
Loading times are included in CPU times.
6 Conclusion
We presented some experiments that compare the accu-
racy and performance of two stochastic parsing systems,
the shallow Collins parser and the deep-grammar-based
XLE system. We measured the accuracy of both systems
against a gold standard derived from the PARC 700 de-
pendency bank, and also measured their processing times.
Contrary to conventional wisdom, we found that the shal-
low system was not substantially faster than the deep
parser operating on a core grammar, while the deep sys-
tem was significantly more accurate. Furthermore, ex-
tending the grammar base of the deep system results in
much better accuracy at a cost of a factor of 5 in speed.
Our experiment is comparable to recent work on read-
ing off Propbank-style (Kingsbury and Palmer, 2002)
predicate-argument relations from gold-standard tree-
bank trees and automatic parses of the Collins parser.
Gildea and Palmer (2002) report F-score results in the
55% range for argument and boundary recognition based
on automatic parses. From this perspective, the nearly
75% F-score that is achieved for our deterministic rewrit-
ing of Collins? trees into dependencies is remarkable,
even if the results are not directly comparable. Our scores
and Gildea and Palmer?s are both substantially lower than
the 90% typically cited for evaluations based on labeled
or unlabeled bracketing, suggesting that extracting se-
mantically relevant dependencies is a more difficult, but
we think more valuable, task.
References
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proceedings of COL-
ING2002, Workshop on Grammar Engineering and
Evaluation, pages 1?7.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
D. Crouch, C. Condoravdi, R. Stolle, T.H. King,
V. de Paiva, J. Everett, and D. Bobrow. 2002. Scal-
ability of redundancy detection in focused document
collections. In Proceedings of Scalable Natural Lan-
guage Understanding, Heidelberg.
Hal Daume and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), Philadelphia, PA.
Anette Frank, Tracy H. King, Jonas Kuhn, and John
Maxwell. 1998. Optimality theory style constraint
ranking in large-scale LFG grammars. In Proceedings
of the Third LFG Conference.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochatic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?02), Philadelphia, PA.
Dan Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL?02), Philadelphia.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Annual Meeting and 10th Conference of the European
Chapter of the Asssociation for Computational Lin-
guistics (ACL?01), Toulouse, France.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the Work-
shop on ?Linguistically Interpreted Corpora? at the
10th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (LINC?03), Bu-
dapest, Hungary.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?02), Las Palmas, Spain.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st Conference of the North American Chapter of
the Association for Computational Linguistics (ANLP-
NAACL 2000), Seattle, WA.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Conference
(HLT?02), San Diego, CA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP-
1.
Stefan Riezler and Alexander Vasserman. 2004. Gradi-
ent feature testing and `1 regularization for maximum
entropy parsing. Submitted for publication.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting and 10th Conference of the Eu-
ropean Chapter of the Asssociation for Computational
Linguistics (ACL?01), Toulouse, France.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 248?255,
New York, June 2006. c?2006 Association for Computational Linguistics
Grammatical Machine Translation
Stefan Riezler and John T. Maxwell III
Palo Alto Research Center
3333 Coyote Hill Road, Palo Alto, CA 94304
Abstract
We present an approach to statistical
machine translation that combines ideas
from phrase-based SMT and traditional
grammar-based MT. Our system incor-
porates the concept of multi-word trans-
lation units into transfer of dependency
structure snippets, and models and trains
statistical components according to state-
of-the-art SMT systems. Compliant with
classical transfer-based MT, target depen-
dency structure snippets are input to a
grammar-based generator. An experimen-
tal evaluation shows that the incorpora-
tion of a grammar-based generator into an
SMT framework provides improved gram-
maticality while achieving state-of-the-art
quality on in-coverage examples, suggest-
ing a possible hybrid framework.
1 Introduction
Recent approaches to statistical machine translation
(SMT) piggyback on the central concepts of phrase-
based SMT (Och et al, 1999; Koehn et al, 2003)
and at the same time attempt to improve some of its
shortcomings by incorporating syntactic knowledge
in the translation process. Phrase-based translation
with multi-word units excels at modeling local or-
dering and short idiomatic expressions, however, it
lacks a mechanism to learn long-distance dependen-
cies and is unable to generalize to unseen phrases
that share non-overt linguistic information. Publicly
available statistical parsers can provide the syntactic
information that is necessary for linguistic general-
izations and for the resolution of non-local depen-
dencies. This information source is deployed in re-
cent work either for pre-ordering source sentences
before they are input to to a phrase-based system
(Xia and McCord, 2004; Collins et al, 2005), or
for re-ordering the output of translation models by
statistical ordering models that access linguistic in-
formation on dependencies and part-of-speech (Lin,
2004; Ding and Palmer, 2005; Quirk et al, 2005)1 .
While these approaches deploy dependency-style
grammars for parsing source and/or target text, a uti-
lization of grammar-based generation on the output
of translation models has not yet been attempted in
dependency-based SMT. Instead, simple target lan-
guage realization models that can easily be trained
to reflect the ordering of the reference translations in
the training corpus are preferred. The advantage of
such models over grammar-based generation seems
to be supported, for example, by Quirk et al?s (2005)
improvements over phrase-based SMT as well as
over an SMT system that deploys a grammar-based
generator (Menezes and Richardson, 2001) on n-
gram based automatic evaluation scores (Papineni et
al., 2001; Doddington, 2002). Another data point,
however, is given by Charniak et al (2003) who
show that parsing-based language modeling can im-
prove grammaticality of translations, even if these
improvements are not recorded under n-gram based
evaluation measures.
1A notable exception to this kind of approach is Chiang
(2005) who introduces syntactic information into phrase-based
SMT via hierarchical phrases rather than by external parsing.
248
In this paper we would like to step away from
n-gram based automatic evaluation scores for a
moment, and investigate the possible contributions
of incorporating a grammar-based generator into
a dependency-based SMT system. We present a
dependency-based SMT model that integrates the
idea of multi-word translation units from phrase-
based SMT into a transfer system for dependency
structure snippets. The statistical components of
our system are modeled on the phrase-based sys-
tem of Koehn et al (2003), and component weights
are adjusted by minimum error rate training (Och,
2003). In contrast to phrase-based SMT and to the
above cited dependency-based SMT approaches, our
system feeds dependency-structure snippets into a
grammar-based generator, and determines target lan-
guage ordering by applying n-gram and distortion
models after grammar-based generation. The goal of
this ordering model is thus not foremost to reflect the
ordering of the reference translations, but to improve
the grammaticality of translations.
Since our system uses standard SMT techniques
to learn about correct lexical choice and idiomatic
expressions, it allows us to investigate the contri-
bution of grammar-based generation to dependency-
based SMT2. In an experimental evaluation on the
test-set that was used in Koehn et al (2003) we
show that for examples that are in coverage of
the grammar-based system, we can achieve state-
of-the-art quality on n-gram based evaluation mea-
sures. To discern the factors of grammaticality
and translational adequacy, we conducted a man-
ual evaluation on 500 in-coverage and 500 out-of-
coverage examples. This showed that an incorpo-
ration of a grammar-based generator into an SMT
framework provides improved grammaticality over
phrase-based SMT on in-coverage examples. Since
in our system it is determinable whether an example
is in-coverage, this opens the possibility for a hy-
brid system that achieves improved grammaticality
at state-of-the-art translation quality.
2A comparison of the approaches of Quirk et al (2005) and
Menezes and Richardson (2001) with respect to ordering mod-
els is difficult because they differ from each other in their statis-
tical and dependency-tree alignment models.
2 Extracting F-Structure Snippets
Our method for extracting transfer rules for depen-
dency structure snippets operates on the paired sen-
tences of a sentence-aligned bilingual corpus. Sim-
ilar to phrase-based SMT, our approach starts with
an improved word-alignment that is created by in-
tersecting alignment matrices for both translation di-
rections, and refining the intersection alignment by
adding directly adjacent alignment points, and align-
ment points that align previously unaligned words
(see Och et al (1999)). Next, source and target sen-
tences are parsed using source and target LFG gram-
mars to produce a set of possible f(unctional) de-
pendency structures for each side (see Riezler et al
(2002) for the English grammar and parser; Butt et
al. (2002) for German). The two f-structures that
most preserve dependencies are selected for further
consideration. Selecting the most similar instead of
the most probable f-structures is advantageous for
rule induction since it provides for higher cover-
age with simpler rules. In the third step, the many-
to-many word alignment created in the first step is
used to define many-to-many correspondences be-
tween the substructures of the f-structures selected
in the second step. The parsing process maintains
an association between words in the string and par-
ticular predicate features in the f-structure, and thus
the predicates on the two sides are implicitly linked
by virtue of the original word alignment. The word
alignment is extended to f-structures by setting into
correspondence the f-structure units that immedi-
ately contain linked predicates. These f-structure
correspondences are the basis for hypothesizing can-
didate transfer rules.
To illustrate, suppose our corpus contains the fol-
lowing aligned sentences (this example is taken from
our experiments on German-to-English translation):
Dafu?r bin ich zutiefst dankbar.
I have a deep appreciation for that.
Suppose further that we have created the many-to-
many bi-directional word alignment
Dafu?r{6 7} bin{2} ich{1} zutiefst{3 4 5}
dankbar{5}
indicating for example that Dafu?r is aligned with
words 6 and 7 of the English sentence (for and that).
249
??
?
?
?
?
?
?
?
?
?
PRED sein
SUBJ
[
PRED ich
]
XCOMP
?
?
?
?
?
PRED dankbar
ADJ
?
?
?
?
?
[
PRED zutiefst
]
[
PRED dafu?r
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED have
SUBJ
[
PRED I
]
OBJ
?
?
?
?
?
?
?
?
?
?
?
PRED appreciation
SPEC
[
PRED a
]
ADJ
?
?
?
?
?
?
?
[
PRED deep
]
?
?
PRED for
OBJ
[
PRED that
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: F-structure alignment for induction of German-to-English transfer rules.
This results in the links between the predicates of the
source and target f-structures shown in Fig. 1.
From these source-target f-structure alignments
transfer rules are extracted in two steps. In the first
step, primitive transfer rules are extracted directly
from the alignment of f-structure units. These in-
clude simple rules for mapping lexical predicates
such as:
PRED(%X1, ich) ==> PRED(%X1, I)
and somewhat more complicated rules for mapping
local f-structure configurations. For example, the
rule shown below is derived from the alignment of
the outermost f-structures. It maps any f-structure
whose pred is sein to an f-structure with pred have,
and in addition interprets the subj-to-subj link as an
indication to map the subject of a source with this
predicate into the subject of the target and the xcomp
of the source into the object of the target. Features
denoting number, person, type, etc. are not shown;
variables %X denote f-structure values.
PRED(%X1,sein) PRED(%X1,have)
SUBJ(%X1,%X2) ==> SUBJ(%X1,%X2)
XCOMP(%X1,%X3) OBJ(%X1,%X3)
The following rule shows how a single source f-
structure can be mapped to a local configuration of
several units on the target side, in this case the sin-
gle f-structure headed by dafu?r into one that corre-
sponds to an English preposition+object f-structure.
PRED(%X1,for)
PRED(%X1, dafu?r) ==> OBJ(%X1,%X2)
PRED(%X2,that)
Transfer rules are required to only operate on con-
tiguous units of the f-structure that are consistent
with the word alignment. This transfer contiguity
constraint states that
1. source and target f-structures are each con-
nected.
2. f-structures in the transfer source can only be
aligned with f-structures in the transfer target,
and vice versa.
This constraint on f-structures is analogous to the
constraint on contiguous and alignment-consistent
phrases employed in phrase-based SMT. It prevents
the extraction of a transfer rule that would trans-
late dankbar directly into appreciation since appre-
ciation is aligned also to zutiefst and its f-structure
would also have to be included in the transfer. Thus,
the primitive transfer rule for these predicates must
be:
PRED(%X1,dankbar) PRED(%X1,appr.)
ADJ(%X1,%X2) ==> SPEC(%X1,%X2)
in set(%X3,%X2) PRED(%X2,a)
PRED(%X3,zutiefst) ADJ(%X1,%X3)
in set(%X4,%X3)
PRED(%X4,deep)
In the second step, rules for more complex map-
pings are created by combining primitive transfer
rules that are adjacent in the source and target f-
structures. For instance, we can combine the prim-
itive transfer rule that maps sein to have with the
primitive transfer rule that maps ich to I to produce
the complex transfer rule:
PRED(%X1,sein) PRED(%X1,have)
SUBJ(%X1,%X2) ==> SUBJ(%X1,%X2)
PRED(%X2,ich) PRED(%X2,I)
XCOMP(%X1,%X3) OBJ(%X1,%X3)
In the worst case, there can be an exponential
number of combinations of primitive transfer rules,
so we only allow at most three primitive transfer
rules to be combined. This produces O(n2) trans-
250
fer rules in the worst case, where n is the number of
f-structures in the source.
Other points where linguistic information comes
into play is in morphological stemming in f-
structures, and in the optional filtering of f-structure
phrases based on consistency of linguistic types. For
example, the extraction of a phrase-pair that trans-
lates zutiefst dankbar into a deep appreciation is
valid in the string-based world, but would be pre-
vented in the f-structure world because of the incom-
patibility of the types A and N for adjectival dankbar
and nominal appreciation. Similarly, a transfer rule
translating sein to have could be dispreferred be-
cause of a mismatch in the the verbal types V/A and
V/N. However, the transfer of sein zutiefst dankbar
to have a deep appreciation is licensed by compati-
ble head types V.
3 Parsing-Transfer-Generation
We use LFG grammars, producing c(onstituent)-
structures (trees) and f(unctional)-structures (at-
tribute value matrices) as output, for parsing source
and target text (Riezler et al, 2002; Butt et al, 2002).
To increase robustness, the standard grammar is aug-
mented with a FRAGMENT grammar. This allows
sentences that are outside the scope of the standard
grammar to be parsed as well-formed chunks speci-
fied by the grammar, with unparsable tokens possi-
bly interspersed. The correct parse is determined by
a fewest-chunk method.
Transfer converts source into a target f-structures
by non-deterministically applying all of the induced
transfer rules in parallel. Each fact in the German f-
structure must be transferred by exactly one trans-
fer rule. For robustness a default rule is included
that transfers any fact as itself. Similar to parsing,
transfer works on a chart. The chart has an edge for
each combination of facts that have been transferred.
When the chart is complete, the outputs of the trans-
fer rules are unified to make sure they are consistent
(for instance, that the transfer rules did not produce
two determiners for the same noun). Selection of
the most probable transfer output is done by beam-
decoding on the transfer chart.
LFG grammars can be used bidirectionally for
parsing and generation, thus the existing English
grammar used for parsing the training data can
also be used for generation of English translations.
For in-coverage examples, the grammar specifies c-
structures that differ in linear precedence of sub-
trees for a given f-structure, and realizes the termi-
nal yield according to morphological rules. In order
to guarantee non-empty output for the overall trans-
lation system, the generation component has to be
fault-tolerant in cases where the transfer system op-
erates on a fragmentary parse, or produces non-valid
f-structures from valid input f-structures. For gener-
ation from unknown predicates, a default morphol-
ogy is used to inflect the source stem correctly for
English. For generation from unknown structures, a
default grammar is used that allows any attribute to
be generated in any order as any category, with op-
timality marks set so as to prefer the standard gram-
mar over the default grammar.
4 Statistical Models and Training
The statistical components of our system are mod-
eled on the statistical components of the phrase-
based system Pharaoh, described in Koehn et al
(2003) and Koehn (2004). Pharaoh integrates the
following 8 statistical models: relative frequency of
phrase translations in source-to-target and target-
to-source direction, lexical weighting in source-to-
target and target-to-source direction, phrase count,
language model probability, word count, and distor-
tion probability.
Correspondingly, our system computes the fol-
lowing statistics for each translation:
1. log-probability of source-to-target transfer
rules, where the probability r(e|f) of a rule
that transfers source snippet f into target snip-
pet e is estimated by the relative frequency
r(e|f) = count(f ==> e)?
e? count(f ==> e?)
2. log-probability of target-to-source rules
3. log-probability of lexical translations from
source to target snippets, estimated from
Viterbi alignments a? between source word po-
sitions i = 1, . . . , n and target word positions
j = 1, . . . ,m for stems fi and ej in snippets
f and e with relative word translation frequen-
251
cies t(ej |fi):
l(e|f) =
?
j
1
|{i|(i, j) ? a?}|
?
(i,j)?a?
t(ej |fi)
4. log-probability of lexical translations from tar-
get to source snippets
5. number of transfer rules
6. number of transfer rules with frequency 1
7. number of default transfer rules (translating
source features into themselves)
8. log-probability of strings of predicates from
root to frontier of target f-structure, estimated
from predicate trigrams in English f-structures
9. number of predicates in target f-structure
10. number of constituent movements during gen-
eration based on the original order of the head
predicates of the constituents (for example,
AP[2] BP[3] CP[1] counts as two move-
ments since the head predicate of CP moved
from the first position to the third position)
11. number of generation repairs
12. log-probability of target string as computed by
trigram language model
13. number of words in target string
These statistics are combined into a log-linear model
whose parameters are adjusted by minimum error
rate training (Och, 2003).
5 Experimental Evaluation
The setup for our experimental comparison is
German-to-English translation on the Europarl par-
allel data set3. For quick experimental turnaround
we restricted our attention to sentences with 5 to
15 words, resulting in a training set of 163,141 sen-
tences and a development set of 1967 sentences. Fi-
nal results are reported on the test set of 1,755 sen-
tences of length 5-15 that was used in Koehn et al
(2003). To extract transfer rules, an improved bidi-
rectional word alignment was created for the train-
ing data from the word alignment of IBM model 4 as
3http://people.csail.mit.edu/koehn/publications/europarl/
implemented by GIZA++ (Och et al, 1999). Train-
ing sentences were parsed using German and En-
glish LFG grammars (Riezler et al, 2002; Butt et
al., 2002). The grammars obtain 100% coverage on
unseen data. 80% are parsed as full parses; 20% re-
ceive FRAGMENT parses. Around 700,000 transfer
rules were extracted from f-structures pairs chosen
according to a dependency similarity measure. For
language modeling, we used the trigram model of
Stolcke (2002).
When applied to translating unseen text, the sys-
tem operates on n-best lists of parses, transferred
f-structures, and generated strings. For minimum-
error-rate training on the development set, and for
translating the test set, we considered 1 German
parse for each source sentence, 10 transferred f-
structures for each source parse, and 1,000 gener-
ated strings for each transferred f-structure. Selec-
tion of most probable translations proceeds in two
steps: First, the most probable transferred f-structure
is computed by a beam search on the transfer chart
using the first 10 features described above. These
features include tests on source and target f-structure
snippets related via transfer rules (features 1-7) as
well as language model and distortion features on
the target c- and f-structures (features 8-10). In our
experiments, the beam size was set to 20 hypotheses.
The second step is based on features 11-13, which
are computed on the strings that were actually gen-
erated from the selected n-best f-structures.
We compared our system to IBM model 4 as pro-
duced by GIZA++ (Och et al, 1999) and a phrase-
based SMT model as provided by Pharaoh (2004).
The same improved word alignment matrix and the
same training data were used for phrase-extraction
for phrase-based SMT as well as for transfer-rule
extraction for LFG-based SMT. Minimum-error-rate
training was done using Koehn?s implementation of
Och?s (2003) minimum-error-rate model. To train
the weights for phrase-based SMT we used the first
500 sentences of the development set; the weights of
the LFG-based translator were adjusted on the 750
sentences that were in coverage of our grammars.
For automatic evaluation, we use the NIST metric
(Doddington, 2002) combined with the approximate
randomization test (Noreen, 1989), providing the de-
sired combination of a sensitive evaluation metric
and an accurate significance test (see Riezler and
252
Table 1: NIST scores on test set for IBM model 4 (M4),
phrase-based SMT (P), and the LFG-based SMT (LFG) on the
full test set and on in-coverage examples for LFG. Results in the
same row that are not statistically significant from each other are
marked with a ?.
M4 LFG P
in-coverage 5.13 *5.82 *5.99
full test set *5.57 *5.62 6.40
Table 2: Preference ratings of two human judges for transla-
tions of phrase-based SMT (P) or LFG-based SMT (LFG) under
criteria of fluency/grammaticality and translational/semantic
adequacy on 500 in-coverage examples. Ratings by judge 1 are
shown in rows, for judge 2 in columns. Agreed-on examples are
shown in boldface in the diagonals.
adequacy grammaticality
j1\j2 P LFG equal P LFG equal
P 48 8 7 36 2 9
LFG 10 105 18 6 113 17
equal 53 60 192 51 44 223
Maxwell (2005)). In order to avoid a random as-
sessment of statistical significance in our three-fold
pairwise comparison, we reduce the per-comparison
significance level to 0.01 so as to achieve a standard
experimentwise significance level of 0.05 (see Co-
hen (1995)). Table 1 shows results for IBM model
4, phrase-based SMT, and LFG-based SMT, where
examples that are in coverage of the LFG-based sys-
tems are evaluated separately. Out of the 1,755 sen-
tences of the test set, 44% were in coverage of the
LFG-grammars; for 51% the system had to resort to
the FRAGMENT technique for parsing and/or repair
techniques in generation; in 5% of the cases our sys-
tem timed out. Since our grammars are not set up
with punctuation in mind, punctuation is ignored in
all evaluations reported below.
For in-coverage examples, the difference between
NIST scores for the LFG system and the phrase-
based system is statistically not significant. On the
full set of test examples, the suboptimal quality on
out-of-coverage examples overwhelms the quality
achieved on in-coverage examples, resulting in a sta-
tistically not significant result difference in NIST
scores between the LFG system and IBM model 4.
In order to discern the factors of grammaticality
and translational adequacy, we conducted a manual
evaluation on randomly selected 500 examples that
were in coverage of the grammar-based generator.
Two independent human judges were presented with
the source sentence, and the output of the phrase-
based and LFG-based systems in a blind test. This
was achieved by displaying the system outputs in
random order. The judges were asked to indicate a
preference for one system translation over the other,
or whether they thought them to be of equal quality.
These questions had to be answered separately un-
der the criteria of grammaticality/fluency and trans-
lational/semantic adequacy. As shown in Table 2,
both judges express a preference for the LFG system
over the phrase-based system for both adequacy and
grammaticality. If we just look at sentences where
judges agree, we see a net improvement on trans-
lational adequacy of 57 sentences, which is an im-
provement of 11.4% over the 500 sentences. If this
were part of a hybrid system, this would amount to a
5% overall improvement in translational adequacy.
Similarly we see a net improvement on grammat-
icality of 77 sentences, which is an improvement
of 15.4% over the 500 sentences or 6.7% overall
in a hybrid system. Result differences on agreed-
on ratings are statistically significant, where sig-
nificance was assessed by approximate randomiza-
tion via stratified shuffling of the preferences be-
tween the systems (Noreen, 1989). Examples from
the manual evaluation are shown in Fig. 2.
Along the same lines, a further manual evaluation
was conducted on 500 randomly selected examples
that were out of coverage of the LFG-based gram-
mars. Across the combined set of 1,000 in-coverage
and out-of-coverage sentences, this resulted in an
agreed-on preference for the phrase-based system
in 204 cases and for the LFG-based system in 158
cases under the measure of translational adequacy.
Under the grammaticality measure the phrase-based
system was preferred by both judges in 157 cases
and the LFG-based system in 136 cases.
6 Discussion
The above presented evaluation of the LFG-based
translator shows promising results for examples that
are in coverage of the employed LFG grammars.
However, a back-off to robustness techniques in
parsing and/or generation results in a considerable
253
(1) src: in diesem fall werde ich meine verantwortung wahrnehmen
ref: then i will exercise my responsibility
LFG: in this case i accept my responsibility
P: in this case i shall my responsibilities
(2) src: die politische stabilita?t ha?ngt ab von der besserung der lebensbedingungen
ref: political stability depends upon the improvement of living conditions
LFG: the political stability hinges on the recovery the conditions
P: the political stability is rejects the recovery of the living conditions
(3) src: und schlie?lich mu? dieser agentur eine kritische haltung gegenu?ber der kommission selbst erlaubt sein
ref: moreover the agency must be able to criticise the commission itself
LFG: and even to the commission a critical stance must finally be allowed this agency
P: finally this is a critical attitude towards the commission itself to be agency
(4) src: nach der ratifizierung werden co2 emissionen ihren preis haben
ref: after ratification co2 emission will have a price tag
LFG: carbon dioxide emissions have its price following the ratification
P: after the ratification co2 emissions are a price
(5) src: die lebensmittel mu?ssen die sichere erna?hrung des menschen gewa?hrleisten
ref: man?s food must be safe to eat
LFG: food must guarantee the safe nutrition of the people
P: the people of the nutrition safe food must guarantee
(6) src: was wir morgen beschlie?en werden ist letztlich material fu?r das vermittlungsverfahren
ref: whatever we agree tomorrow will ultimately have to go into the conciliation procedure
LFG: one tomorrow we approved what is ultimately material for the conciliation procedure
P: what we decide tomorrow is ultimately material for the conciliation procedure
(7) src: die verwaltung mu? ku?nftig schneller reagieren ko?nnen
ref: in future the administration must be able to react more quickly
LFG: more in future the administration must be able to react
P: the administration must be able to react more quickly
(8) src: das ist jetzt u?ber 40 jahre her
ref: that was over 40 years ago
LFG: on 40 years ago it is now
P: that is now over 40 years ago
(9) src: das ist schon eine seltsame vorstellung von gleichheit
ref: a strange notion of equality
LFG: equality that is even a strange idea
P: this is already a strange idea of equality
(10) src: frau pra?sidentin ich beglu?ckwu?nsche herrn nicholson zu seinem ausgezeichneten bericht
ref: madam president i congratulate mr nicholson on his excellent report
LFG: madam president i congratulate mister nicholson on his report excellented
P: madam president i congratulate mr nicholson for his excellent report
Figure 2: Examples from manual evaluation: Preference for LFG-based system (LFG) over phrase-based system (P) under both
adequacy and grammaticality (ex 1-5), preference of phrased-based system over LFG (6-10) , together with source (src) sentences
and human reference (ref) translations. All ratings are agreed on by both judges.
loss in translation quality. The high percentage of
examples that fall out of coverage of the LFG-
based system can partially be explained by the ac-
cumulation of errors in parsing the training data
where source and target language parser each pro-
duce FRAGMENT parses in 20% of the cases. To-
gether with errors in rule extraction, this results in
a large number ill-formed transfer rules that force
the generator to back-off to robustness techniques.
In applying the parse-transfer-generation pipeline to
translating unseen text, parsing errors can cause er-
roneous transfer, which can result in generation er-
rors. Similar effects can be observed for errors in
translating in-coverage examples. Here disambigua-
tion errors in parsing and transfer propagate through
the system, producing suboptimal translations. An
error analysis on 100 suboptimal in-coverage exam-
ples from the development set showed that 69 sub-
optimal translations were due to transfer errors, 10
of which were due to errors in parsing.
The discrepancy between NIST scores and man-
ual preference rankings can be explained on the one
hand by the suboptimal integration of transfer and
generation in our system, making it infeasible to
work with large n-best lists in training and applica-
tion. Moreover, despite our use of minimum-error-
254
rate training and n-gram language models, our sys-
tem cannot be adjusted to maximize n-gram scores
on reference translation in the same way as phrase-
based systems since statistical ordering models are
employed in our framework after grammar-based
generation, thus giving preference to grammatical-
ity over similarity to reference translations.
7 Conclusion
We presented an SMT model that marries phrase-
based SMT with traditional grammar-based MT
by incorporating a grammar-based generator into a
dependency-based SMT system. Under the NIST
measure, we achieve results in the range of the
state-of-the-art phrase-based system of Koehn et
al. (2003) for in-coverage examples of the LFG-
based system. A manual evaluation of a large set
of such examples shows that on in-coverage ex-
amples our system achieves significant improve-
ments in grammaticality and also translational ad-
equacy over the phrase-based system. Fortunately,
it is determinable when our system is in-coverage,
which opens the possibility for a hybrid system that
achieves improved grammaticality at state-of-the-art
translation quality. Future work thus will concen-
trate on improvements of in-coverage translations
e.g., by stochastic generation. Furthermore, we in-
tend to apply our system to other language pairs and
larger data sets.
Acknowledgements
We would like to thank Sabine Blum for her invalu-
able help with the manual evaluation.
References
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Ma-
suichi, and Christian Rohrer. 2002. The parallel grammar
project. COLING?02, Workshop on Grammar Engineering
and Evaluation.
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003.
Syntax-based language models for statistical machine trans-
lation. MT Summit IX.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. ACL?05.
Paul R. Cohen. 1995. Empirical Methods for Artificial Intelli-
gence. The MIT Press.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005.
Clause restructuring for statistical machine translation.
ACL?05.
Yuan Ding and Martha Palmer. 2005. Machine translation
using probabilistic synchronous dependency insertion gram-
mars. ACL?05.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence statis-
tics. ARPA Workshop on Human Language Technology.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. HLT-NAACL?03.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. User
manual. Technical report, USC ISI.
Dekang Lin. 2004. A path-based transfer model for statistical
machine translation. COLING?04.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of transfer-
mappings from bilingual corpora. Workshop on Data-
Driven Machine Translation.
Eric W. Noreen. 1989. Computer Intensive Methods for Testing
Hypotheses. An Introduction. Wiley.
Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999.
Improved alignment models for statistical machine transla-
tion. EMNLP?99.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. HLT-NAACL?03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of ma-
chine translation. Technical Report IBM RC22176 (W0190-
022).
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. ACL?05.
Stefan Riezler and John Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing for mt. ACL-
05 Workshop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing
the Wall Street Journal using a Lexical-Functional Grammar
and discriminative estimation techniques. ACL?02.
Stefan Riezler, Tracy H. King, Richard Crouch, and Annie Za-
enen. 2003. Statistical sentence condensation using am-
biguity packing and stochastic disambiguation methods for
lexical-functional grammar. HLT-NAACL?03.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. International Conference on Spoken Language
Processing.
Fei Xia and Michael McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. COL-
ING?04.
255
Parsing the Wall Street Journal using a Lexical-Functional Grammar and
Discriminative Estimation Techniques
Stefan Riezler Tracy H. King Ronald M. Kaplan
Palo Alto Research Center Palo Alto Research Center Palo Alto Research Center
Palo Alto, CA 94304 Palo Alto, CA 94304 Palo Alto, CA 94304
riezler@parc.com thking@parc.com kaplan@parc.com
Richard Crouch John T. Maxwell III Mark Johnson
Palo Alto Research Center Palo Alto Research Center Brown University
Palo Alto, CA 94304 Palo Alto, CA 94304 Providence, RI 02912
crouch@parc.com maxwell@parc.com mj@cs.brown.edu
Abstract
We present a stochastic parsing system
consisting of a Lexical-Functional Gram-
mar (LFG), a constraint-based parser and
a stochastic disambiguation model. We re-
port on the results of applying this sys-
tem to parsing the UPenn Wall Street
Journal (WSJ) treebank. The model com-
bines full and partial parsing techniques
to reach full grammar coverage on unseen
data. The treebank annotations are used
to provide partially labeled data for dis-
criminative statistical estimation using ex-
ponential models. Disambiguation perfor-
mance is evaluated by measuring matches
of predicate-argument relations on two
distinct test sets. On a gold standard of
manually annotated f-structures for a sub-
set of the WSJ treebank, this evaluation
reaches 79% F-score. An evaluation on a
gold standard of dependency relations for
Brown corpus data achieves 76% F-score.
1 Introduction
Statistical parsing using combined systems of hand-
coded linguistically fine-grained grammars and
stochastic disambiguation components has seen con-
siderable progress in recent years. However, such at-
tempts have so far been confined to a relatively small
scale for various reasons. Firstly, the rudimentary
character of functional annotations in standard tree-
banks has hindered the direct use of such data for
statistical estimation of linguistically fine-grained
statistical parsing systems. Rather, parameter esti-
mation for such models had to resort to unsupervised
techniques (Bouma et al, 2000; Riezler et al, 2000),
or training corpora tailored to the specific grammars
had to be created by parsing and manual disam-
biguation, resulting in relatively small training sets
of around 1,000 sentences (Johnson et al, 1999).
Furthermore, the effort involved in coding broad-
coverage grammars by hand has often led to the spe-
cialization of grammars to relatively small domains,
thus sacrificing grammar coverage (i.e. the percent-
age of sentences for which at least one analysis is
found) on free text. The approach presented in this
paper is a first attempt to scale up stochastic parsing
systems based on linguistically fine-grained hand-
coded grammars to the UPenn Wall Street Journal
(henceforth WSJ) treebank (Marcus et al, 1994).
The problem of grammar coverage, i.e. the fact
that not all sentences receive an analysis, is tack-
led in our approach by an extension of a full-
fledged Lexical-Functional Grammar (LFG) and a
constraint-based parser with partial parsing tech-
niques. In the absence of a complete parse, a so-
called ?FRAGMENT grammar? allows the input to be
analyzed as a sequence of well-formed chunks. The
set of fragment parses is then chosen on the basis
of a fewest-chunk method. With this combination of
full and partial parsing techniques we achieve 100%
grammar coverage on unseen data.
Another goal of this work is the best possible ex-
ploitation of the WSJ treebank for discriminative es-
timation of an exponential model on LFG parses. We
define discriminative or conditional criteria with re-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 271-278.
                         Proceedings of the 40th Annual Meeting of the Association for
CS 1: FRAGMENTS
Sadj[fin]
S[fin]
NP
D 
the
NPadj
AP[attr]
A
golden
NPzero
N
share
VPall[fin]
VP[pass,fin]
AUX[pass,fin]
was
VPv[pass]
V[pass]
scheduled
VPinf
VPinf?pos
PARTinf
to
VPall[base]
VPv[base]
V[base]
expire
PPcl
PP
P
at
NP
D
the
NPadj
NPzero
N
beginning
FRAGMENTS
TOKEN
of
"The golden share was scheduled to expire at the beginning of"
?schedule<NULL, [132:expire]>[11:share]?PRED
?share?PRED 
?golden<[11:share]>?PRED  [11:share]SUBJADEGREE positive , ADJUNCT?TYPE nominal, ATYPE attributive23ADJUNCT
unspecifiedGRAINNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE nom , NUM  sg, PERS   311
SUBJ
?expire<[11:share]>?PRED  [11:share]SUBJ
?at<[170:beginning]>?PRED
?beginning ?PRED 
GERUND +, GRAIN unspecifiedNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE acc, NUM  sg, PCASE   at, PERS   3170
OBJ
ADV?TYPE	  vpadv
 , PSEM   locative, PTYPE   sem164
ADJUNCT	
INF?FORM to , PASSIVE   ?, VTYPE  main132
XCOMP
MOOD indicative, TENSE pastTNS?ASP
PASSIVE +, STMT?TYPE decl, VTYPE main67
FIRST
ofTOKEN229FIRST3218REST3188
Figure 1: FRAGMENT c-/f-structure for The golden share was scheduled to expire at the beginning of
spect to the set of grammar parses consistent with
the treebank annotations. Such data can be gathered
by applying labels and brackets taken from the tree-
bank annotation to the parser input. The rudimen-
tary treebank annotations are thus used to provide
partially labeled data for discriminative estimation
of a probability model on linguistically fine-grained
parses.
Concerning empirical evaluation of disambigua-
tion performance, we feel that an evaluation measur-
ing matches of predicate-argument relations is more
appropriate for assessing the quality of our LFG-
based system than the standard measure of match-
ing labeled bracketing on section 23 of the WSJ
treebank. The first evaluation we present measures
matches of predicate-argument relations in LFG f-
structures (henceforth the LFG annotation scheme)
to a gold standard of manually annotated f-structures
for a representative subset of the WSJ treebank. The
evaluation measure counts the number of predicate-
argument relations in the f-structure of the parse
selected by the stochastic model that match those
in the gold standard annotation. Our parser plus
stochastic disambiguator achieves 79% F-score un-
der this evaluation regime.
Furthermore, we employ another metric which
maps predicate-argument relations in LFG f-
structures to the dependency relations (henceforth
the DR annotation scheme) proposed by Carroll et
al. (1999). Evaluation with this metric measures the
matches of dependency relations to Carroll et al?s
gold standard corpus. For a direct comparison of our
results with Carroll et al?s system, we computed an
F-score that does not distinguish different types of
dependency relations. Under this measure we obtain
76% F-score.
This paper is organized as follows. Section 2
describes the Lexical-Functional Grammar, the
constraint-based parser, and the robustness tech-
niques employed in this work. In section 3 we
present the details of the exponential model on LFG
parses and the discriminative statistical estimation
technique. Experimental results are reported in sec-
tion 4. A discussion of results is in section 5.
2 Robust Parsing using LFG
2.1 A Broad-Coverage LFG
The grammar used for this project was developed in
the ParGram project (Butt et al, 1999). It uses LFG
as a formalism, producing c(onstituent)-structures
(trees) and f(unctional)-structures (attribute value
matrices) as output. The c-structures encode con-
stituency. F-structures encode predicate-argument
relations and other grammatical information, e.g.,
number, tense. The XLE parser (Maxwell and Ka-
plan, 1993) was used to produce packed represen-
tations, specifying all possible grammar analyses of
the input.
The grammar has 314 rules with regular expres-
sion right-hand sides which compile into a collec-
tion of finite-state machines with a total of 8,759
states and 19,695 arcs. The grammar uses several
lexicons and two guessers: one guesser for words
recognized by the morphological analyzer but not
in the lexicons and one for those not recognized.
As such, most nouns, adjectives, and adverbs have
no explicit lexical entry. The main verb lexicon con-
tains 9,652 verb stems and 23,525 subcategorization
frame-verb stem entries; there are also lexicons for
adjectives and nouns with subcategorization frames
and for closed class items.
For estimation purposes using the WSJ treebank,
the grammar was modified to parse part of speech
tags and labeled bracketing. A stripped down ver-
sion of the WSJ treebank was created that used
only those POS tags and labeled brackets relevant
for determining grammatical relations. The WSJ la-
beled brackets are given LFG lexical entries which
constrain both the c-structure and the f-structure of
the parse. For example, the WSJ?s ADJP-PRD la-
bel must correspond to an AP in the c-structure and
an XCOMP in the f-structure. In this version of the
corpus, all WSJ labels with -SBJ are retained and
are restricted to phrases corresponding to SUBJ in
the LFG grammar; in addition, it contains NP under
VP (OBJ and OBJth in the LFG grammar), all -LGS
tags (OBL-AG), all -PRD tags (XCOMP), VP under
VP (XCOMP), SBAR- (COMP), and verb POS tags
under VP (V in the c-structure). For example, our
labeled bracketing of wsj 1305.mrg is [NP-SBJ His
credibility] is/VBZ also [PP-PRD on the line] in the
investment community.
Some mismatches between the WSJ labeled
bracketing and the LFG grammar remain. These
often arise when a given constituent fills a gram-
matical role in more than one clause. For exam-
ple, in wsj 1303.mrg Japan?s Daiwa Securities Co.
named Masahiro Dozen president., the noun phrase
Masahiro Dozen is labeled as an NP-SBJ. However,
the LFG grammar treats it as the OBJ of the ma-
trix clause. As a result, the labeled bracketed version
of this sentence does not receive a full parse, even
though its unlabeled, string-only counterpart is well-
formed. Some other bracketing mismatches remain,
usually the result of adjunct attachment. Such mis-
matches occur in part because, besides minor mod-
ifications to match the bracketing for special con-
structions, e.g., negated infinitives, the grammar was
not altered to mirror the idiosyncrasies of the WSJ
bracketing.
2.2 Robustness Techniques
To increase robustness, the standard grammar has
been augmented with a FRAGMENT grammar. This
grammar parses the sentence as well-formed chunks
specified by the grammar, in particular as Ss, NPs,
PPs, and VPs. These chunks have both c-structures
and f-structures corresponding to them. Any token
that cannot be parsed as one of these chunks is
parsed as a TOKEN chunk. The TOKENs are also
recorded in the c- and f-structures. The grammar has
a fewest-chunk method for determining the correct
parse. For example, if a string can be parsed as two
NPs and a VP or as one NP and an S, the NP-S
option is chosen. A sample FRAGMENT c-structure
and f-structure are shown in Fig. 1 for wsj 0231.mrg
(The golden share was scheduled to expire at the
beginning of), an incomplete sentence; the parser
builds one S chunk and then one TOKEN for the
stranded preposition.
A final capability of XLE that increases cov-
erage of the standard-plus-fragment grammar is a
SKIMMING technique. Skimming is used to avoid
timeouts and memory problems. When the amount
of time or memory spent on a sentence exceeds
a threshhold, XLE goes into skimming mode for
the constituents whose processing has not been
completed. When XLE skims these remaining con-
stituents, it does a bounded amount of work per sub-
tree. This guarantees that XLE finishes processing
a sentence in a polynomial amount of time. In pars-
ing section 23, 7.2% of the sentences were skimmed;
26.1% of these resulted in full parses, while 73.9%
were FRAGMENT parses.
The grammar coverage achieved 100% of section
23 as unseen unlabeled data: 74.7% as full parses,
25.3% FRAGMENT and/or SKIMMED parses.
3 Discriminative Statistical Estimation
from Partially Labeled Data
3.1 Exponential Models on LFG Parses
We employed the well-known family of exponential
models for stochastic disambiguation. In this paper
we are concerned with conditional exponential mod-
els of the form:
p?(x|y) = Z?(y)
?1e??f(x)
where X(y) is the set of parses for sentence y,
Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant, ? = (?1, . . . , ?n) ? IRn is a vector of
log-parameters, f = (f1, . . . , fn) is a vector of
property-functions fi : X ? IR for i = 1, . . . , n
on the set of parses X , and ? ? f(x) is the vector dot
product
?n
i=1 ?ifi(x).
In our experiments, we used around 1000
complex property-functions comprising information
about c-structure, f-structure, and lexical elements
in parses, similar to the properties used in Johnson
et al (1999). For example, there are property func-
tions for c-structure nodes and c-structure subtrees,
indicating attachment preferences. High versus low
attachment is indicated by property functions count-
ing the number of recursively embedded phrases.
Other property functions are designed to refer to
f-structure attributes, which correspond to gram-
matical functions in LFG, or to atomic attribute-
value pairs in f-structures. More complex property
functions are designed to indicate, for example, the
branching behaviour of c-structures and the (non)-
parallelism of coordinations on both c-structure and
f-structure levels. Furthermore, properties refering
to lexical elements based on an auxiliary distribution
approach as presented in Riezler et al (2000) are
included in the model. Here tuples of head words,
argument words, and grammatical relations are ex-
tracted from the training sections of the WSJ, and
fed into a finite mixture model for clustering gram-
matical relations. The clustering model itself is then
used to yield smoothed probabilities as values for
property functions on head-argument-relation tuples
of LFG parses.
3.2 Discriminative Estimation
Discriminative estimation techniques have recently
received great attention in the statistical machine
learning community and have already been applied
to statistical parsing (Johnson et al, 1999; Collins,
2000; Collins and Duffy, 2001). In discriminative es-
timation, only the conditional relation of an analysis
given an example is considered relevant, whereas in
maximum likelihood estimation the joint probability
of the training data to best describe observations is
maximized. Since the discriminative task is kept in
mind during estimation, discriminative methods can
yield improved performance. In our case, discrimi-
native criteria cannot be defined directly with respect
to ?correct labels? or ?gold standard? parses since
the WSJ annotations are not sufficient to disam-
biguate the more complex LFG parses. However, in-
stead of retreating to unsupervised estimation tech-
niques or creating small LFG treebanks by hand, we
use the labeled bracketing of the WSJ training sec-
tions to guide discriminative estimation. That is, dis-
criminative criteria are defined with respect to the set
of parses consistent with the WSJ annotations.1
The objective function in our approach, denoted
by P (?), is the joint of the negative log-likelihood
?L(?) and a Gaussian regularization term ?G(?)
on the parameters ?. Let {(yj , zj)}mj=1 be a set of
training data, consisting of pairs of sentences y and
partial annotations z, let X(y, z) be the set of parses
for sentence y consistent with annotation z, and let
X(y) be the set of all parses produced by the gram-
mar for sentence y. Furthermore, let p[f ] denote the
expectation of function f under distribution p. Then
P (?) can be defined for a conditional exponential
model p?(z|y) as:
P (?) = ?L(?)?G(?)
= ? log
m?
j=1
p?(zj |yj) +
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
?
X(yj)
e??f(x)
+
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
+
m?
j=1
log
?
X(yj)
e??f(x) +
n?
i=1
?2i
2?2i
.
Intuitively, the goal of estimation is to find model pa-
1An earlier approach using partially labeled data for estimat-
ing stochastics parsers is Pereira and Schabes?s (1992) work on
training PCFG from partially bracketed data. Their approach
differs from the one we use here in that Pereira and Schabes
take an EM-based approach maximizing the joint likelihood of
the parses and strings of their training data, while we maximize
the conditional likelihood of the sets of parses given the corre-
sponding strings in a discriminative estimation setting.
rameters which make the two expectations in the last
equation equal, i.e. which adjust the model param-
eters to put all the weight on the parses consistent
with the annotations, modulo a penalty term from
the Gaussian prior for too large or too small weights.
Since a closed form solution for such parame-
ters is not available, numerical optimization meth-
ods have to be used. In our experiments, we applied
a conjugate gradient routine, yielding a fast converg-
ing optimization algorithm where at each iteration
the negative log-likelihood P (?) and the gradient
vector have to be evaluated.2 For our task the gra-
dient takes the form:
?P (?) =
?
?P (?)
??1
,
?P (?)
??2
, . . . ,
?P (?)
??n
?
, and
?P (?)
??i
= ?
m?
j=1
(
?
x?X(yj ,zj)
e??f(x)fi(x)
?
x?X(yj ,zj)
e??f(x)
?
?
x?X(yj)
e??f(x)fi(x)
?
x?X(yj)
e??f(x)
) +
?i
?2i
.
The derivatives in the gradient vector intuitively are
again just a difference of two expectations
?
m?
j=1
p?[fi|yj , zj ] +
m?
j=1
p?[fi|yj ] +
?i
?2i
.
Note also that this expression shares many common
terms with the likelihood function, suggesting an ef-
ficient implementation of the optimization routine.
4 Experimental Evaluation
4.1 Training
The basic training data for our experiments are sec-
tions 02-21 of the WSJ treebank. As a first step, all
sections were parsed, and the packed parse forests
unpacked and stored. For discriminative estimation,
this data set was restricted to sentences which re-
ceive a full parse (in contrast to a FRAGMENT or
SKIMMED parse) for both its partially labeled and
its unlabeled variant. Furthermore, only sentences
2An alternative numerical method would be a combination
of iterative scaling techniques with a conditional EM algorithm
(Jebara and Pentland, 1998). However, it has been shown exper-
imentally that conjugate gradient techniques can outperform it-
erative scaling techniques by far in running time (Minka, 2001).
which received at most 1,000 parses were used.
From this set, sentences of which a discriminative
learner cannot possibly take advantage, i.e. sen-
tences where the set of parses assigned to the par-
tially labeled string was not a proper subset of the
parses assigned the unlabeled string, were removed.
These successive selection steps resulted in a fi-
nal training set consisting of 10,000 sentences, each
with parses for partially labeled and unlabeled ver-
sions. Altogether there were 150,000 parses for par-
tially labeled input and 500,000 for unlabeled input.
For estimation, a simple property selection pro-
cedure was applied to the full set of around 1000
properties. This procedure is based on a frequency
cutoff on instantiations of properties for the parses
in the labeled training set. The result of this proce-
dure is a reduction of the property vector to about
half its size. Furthermore, a held-out data set was
created from section 24 of the WSJ treebank for ex-
perimental selection of the variance parameter of the
prior distribution. This set consists of 120 sentences
which received only full parses, out of which the
most plausible one was selected manually.
4.2 Testing
Two different sets of test data were used: (i) 700 sen-
tences randomly extracted from section 23 of the
WSJ treebank and given gold-standard f-structure
annotations according to our LFG scheme, and (ii)
500 sentences from the Brown corpus given gold
standard annotations by Carroll et al (1999) accord-
ing to their dependency relations (DR) scheme.3
Annotating the WSJ test set was bootstrapped
by parsing the test sentences using the LFG gram-
mar and also checking for consistency with the
Penn Treebank annotation. Starting from the (some-
times fragmentary) parser analyses and the Tree-
bank annotations, gold standard parses were created
by manual corrections and extensions of the LFG
parses. Manual corrections were necessary in about
half of the cases. The average sentence length of
the WSJ f-structure bank is 19.8 words; the average
number of predicate-argument relations in the gold-
standard f-structures is 31.2.
Performance on the LFG-annotated WSJ test set
3Both corpora are available online. The WSJ f-structure
bank at www.parc.com/istl/groups/nltt/fsbank/, and Carroll et
al.?s corpus at www.cogs.susx.ac.uk/lab/nlp/carroll/greval.html.
was measured using both the LFG and DR metrics,
thanks to an f-structure-to-DR annotation mapping.
Performance on the DR-annotated Brown test set
was only measured using the DR metric.
The LFG evaluation metric is based on the com-
parison of full f-structures, represented as triples
relation(predicate, argument). The predicate-
argument relations of the f-structure for one parse of
the sentence Meridian will pay a premium of $30.5
million to assume $2 billion in deposits. are shown
in Fig. 2.
number($:9, billion:17) number($:24, million:4)
detform(premium:3, a) mood(pay:0, indicative)
tense(pay:0, fut) adjunct(million:4, ?30.5?:28)
adjunct(premium:3, of:23) adjunct(billion:17, ?2?:19)
adjunct($:9, in:11) adjunct(pay:0, assume:7)
obj(pay:0, premium:3) stmttype(pay:0, decl)
subj(pay:0, ?Meridian?:5) obj(assume:7, $:9)
obj(of:23, $:24) subj(assume:7, pro:8)
obj(in:11, deposit:12) prontype(pro:8, null)
stmttype(assume:7, purpose)
Figure 2: LFG predicate-argument relation represen-
tation
The DR annotation for our example sentence, ob-
tained via a mapping from f-structures to Carroll et
al?s annotation scheme, is shown in Fig. 3.
(aux pay will) (subj pay Meridian )
(detmod premium a) (mod million 30.5)
(mod $ million) (mod of premium $)
(dobj pay premium ) (mod billion 2)
(mod $ billion) (mod in $ deposit)
(dobj assume $ ) (mod to pay assume)
Figure 3: Mapping to Carroll et al?s dependency-
relation representation
Superficially, the LFG and DR representations are
very similar. One difference between the annotation
schemes is that the LFG representation in general
specifies more relation tuples than the DR represen-
tation. Also, multiple occurences of the same lex-
ical item are indicated explicitly in the LFG rep-
resentation but not in the DR representation. The
main conceptual difference between the two an-
notation schemes is the fact that the DR scheme
crucially refers to phrase-structure properties and
word order as well as to grammatical relations in
the definition of dependency relations, whereas the
LFG scheme abstracts away from serialization and
phrase-structure. Facts like this can make a correct
mapping of LFG f-structures to DR relations prob-
lematic. Indeed, we believe that we still underesti-
mate by a few points because of DR mapping diffi-
culties. 4
4.3 Results
In our evaluation, we report F-scores for both types
of annotation, LFG and DR, and for three types
of parse selection, (i) lower bound: random choice
of a parse from the set of analyses (averaged over
10 runs), (ii) upper bound: selection of the parse
with the best F-score according to the annotation
scheme used, and (iii) stochastic: the parse selected
by the stochastic disambiguator. The error reduc-
tion row lists the reduction in error rate relative to
the upper and lower bounds obtained by the stochas-
tic disambiguation model. F-score is defined as 2 ?
precision? recall/(precision+ recall).
Table 1 gives results for 700 examples randomly
selected from section 23 of the WSJ treebank, using
both LFG and DR measures.
Table 1: Disambiguation results for 700 randomly
selected examples from section 23 of the WSJ tree-
bank using LFG and DR measures.
LFG DR
upper bound 84.1 80.7
stochastic 78.6 73.0
lower bound 75.5 68.8
error reduction 36 35
The effect of the quality of the parses on disam-
biguation performance can be illustrated by break-
ing down the F-scores according to whether the
parser yields full parses, FRAGMENT, SKIMMED, or
SKIMMED+FRAGMENT parses for the test sentences.
The percentages of test examples which belong to
the respective classes of quality are listed in the first
row of Table 2. F-scores broken down according to
classes of parse quality are recorded in the follow-
4See Carroll et al (1999) for more detail on the DR an-
notation scheme, and see Crouch et al (2002) for more de-
tail on the differences between the DR and the LFG annotation
schemes, as well as on the difficulties of the mapping from LFG
f-structures to DR annotations.
ing rows. The first column shows F-scores for all
parses in the test set, as in Table 1. The second col-
umn shows the best F-scores when restricting atten-
tion to examples which receive only full parses. The
third column reports F-scores for examples which
receive only non-full parses, i.e. FRAGMENT or
SKIMMED parses or SKIMMED+FRAGMENT parses.
Columns 4-6 break down non-full parses according
to examples which receive only FRAGMENT, only
SKIMMED, or only SKIMMED+FRAGMENT parses.
Results of the evaluation on Carroll et al?s Brown
test set are given in Table 3. Evaluation results for
the DR measure applied to the Brown corpus test set
broken down according to parse-quality are shown
in Table 2.
In Table 3 we show the DR measure along with an
evaluation measure which facilitates a direct com-
parison of our results to those of Carroll et al
(1999). Following Carroll et al (1999), we count
a dependency relation as correct if the gold stan-
dard has a relation with the same governor and de-
pendent but perhaps with a different relation-type.
This dependency-only (DO) measure thus does not
reflect mismatches between arguments and modi-
fiers in a small number of cases. Note that since
for the evaluation on the Brown corpus, no heldout
data were available to adjust the variance parame-
ter of a Bayesian model, we used a plain maximum-
likelihood model for disambiguation on this test set.
Table 3: Disambiguation results on 500 Brown cor-
pus examples using DO measure and DR measures.
DO DR
Carroll et al (1999) 75.1 -
upper bound 82.0 80.0
stochastic 76.1 74.0
lower bound 73.3 71.7
error reduction 32 33
5 Discussion
We have presented a first attempt at scaling up a
stochastic parsing system combining a hand-coded
linguistically fine-grained grammar and a stochas-
tic disambiguation model to the WSJ treebank.
Full grammar coverage is achieved by combining
specialized constraint-based parsing techniques for
LFG grammars with partial parsing techniques. Fur-
thermore, a maximal exploitation of treebank anno-
tations for estimating a distribution on fine-grained
LFG parses is achieved by letting grammar analyses
which are consistent with the WSJ labeled bracket-
ing define a gold standard set for discriminative es-
timation. The combined system trained on WSJ data
achieves full grammar coverage and disambiguation
performance of 79% F-score on WSJ data, and 76%
F-score on the Brown corpus test set.
While disambiguation performance of around
79% F-score on WSJ data seems promising, from
one perspective it only offers a 3% absolute im-
provement over a lower bound random baseline.
We think that the high lower bound measure high-
lights an important aspect of symbolic constraint-
based grammars (in contrast to treebank gram-
mars): the symbolic grammar already significantly
restricts/disambiguates the range of possible analy-
ses, giving the disambiguator a much narrower win-
dow in which to operate. As such, it is more appro-
priate to assess the disambiguator in terms of reduc-
tion in error rate (36% relative to the upper bound)
than in terms of absolute F-score. Both the DR and
LFG annotations broadly agree in their measure of
error reduction.
The lower reduction in error rate relative to the
upper bound for DR evaluation on the Brown corpus
can be attributed to a corpus effect that has also been
observed by Gildea (2001) for training and testing
PCFGs on the WSJ and Brown corpora.5
Breaking down results according to parse quality
shows that irrespective of evaluation measure and
corpus, around 4% overall performance is lost due
to non-full parses, i.e. FRAGMENT, or SKIMMED, or
SKIMMED+FRAGMENT parses.
Due to the lack of standard evaluation measures
and gold standards for predicate-argument match-
ing, a comparison of our results to other stochastic
parsing systems is difficult. To our knowledge, so
far the only direct point of comparison is the parser
of Carroll et al (1999) which is also evaluated on
Carroll et al?s test corpus. They report an F-score
5Gildea reports a decrease from 86.1%/86.6% re-
call/precision on labeled bracketing to 80.3%/81% when
going from training and testing on the WSJ to training on the
WSJ and testing on the Brown corpus.
Table 2: LFG F-scores for the 700 WSJ test examples and DR F-scores for the 500 Brown test examples
broken down according to parse quality.
WSJ-LFG all full non-full fragments skimmed skimmed+fragments
% of test set 100 74.7 25.3 20.4 1.4 3.4
upper bound 84.1 88.5 73.4 76.7 70.3 61.3
stochastic 78.6 82.5 69.0 72.4 66.6 56.2
lower bound 75.5 78.4 67.7 71.0 63.0 55.9
Brown-DR all full non-full fragments skimmed skimmed+fragments
% of test set 100 79.6 20.4 20.0 2.0 1.6
upper bound 80.0 84.5 65.4 65.4 56.0 53.5
stochastic 74.0 77.9 61.5 61.5 52.8 50.0
lower bound 71.1 74.8 59.2 59.1 51.2 48.9
of 75.1% for a DO evaluation that ignores predicate
labels, counting only dependencies. Under this mea-
sure, our system achieves 76.1% F-score.
References
Gosse Bouma, Gertjan von Noord, and Robert Malouf.
2000. Alpino: Wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands, Amsterdam, Netherlands.
Miriam Butt, Tracy King, Maria-Eugenia Nin?o, and
Fre?de?rique Segond. 1999. A Grammar Writer?s Cook-
book. Number 95 in CSLI Lecture Notes. CSLI Publi-
cations, Stanford, CA.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceed-
ings of the EACL workshop on Linguistically Inter-
preted Corpora (LINC), Bergen, Norway.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14(NIPS?01), Van-
couver.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML?00), Stanford, CA.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage stochastic parser. In Pro-
ceedings of the ?Beyond PARSEVAL? Workshop at the
3rd International Conference on Language Resources
and Evaluation (LREC?02), Las Palmas, Spain.
Dan Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Pittsburgh, PA.
Tony Jebara and Alex Pentland. 1998. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Advances in Neural Information
Processing Systems 11 (NIPS?98).
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Department of Statis-
tics, Carnegie Mellon University.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics (ACL?92),
Newark, Delaware.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized Stochastic Modeling of
Constraint-Based Grammars using Log-Linear Mea-
sures and EM Training. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hong Kong.
Adapting Existing Grammars: The XLE Experience
Ronald M. Kaplan and Tracy Holloway King and John T. Maxwell III
Palo Alto Research Center
Palo Alto, CA 94304 USA
kaplan, thking, maxwell @parc.com
Abstract
We report on the XLE parser and grammar develop-
ment platform (Maxwell and Kaplan, 1993) and de-
scribe how a basic Lexical Functional Grammar for
English has been adapted to two different corpora
(newspaper text and copier repair tips).
1 Introduction
Large-scale grammar development platforms should
be able to be used to develop grammars for a wide
variety of purposes. In this paper, we report on the
the XLE system (Maxwell and Kaplan, 1993), a
parser and grammar development platform for Lex-
ical Functional Grammars. We describe some of the
strategies and notational devices that enable the ba-
sic English grammar developed for the ParGram
project (Butt et al, 1999; Butt et al, 2002) to be
adapted to two corpora with different properties.
1.1 The Corpora
The STANDARD Pargram English grammar covers
the core phenomena of English (e.g., main and sub-
ordinate clauses, noun phrases, adjectives and ad-
verbs, prepositional phrases, coordination; see (Butt
et al, 1999)). We have built two different specialized
grammars on top of this: the EUREKA grammar and
the WSJ grammar.
The EUREKA grammar parses the Eureka cor-
pus of copier repair tips, a collection of documents
offering suggestions for how to diagnose and fix
particular copier malfunctions. These informal and
unedited documents were contributed by copier re-
pair technicians, and the corpus is characterized by
a significant amount of ungrammatical input (e.g.,
typos, incorrect punctuation, telegraphic sentences)
and much technical terminology (1). The goal of
parsing this corpus is to provide input to a semantics
and world-knowledge reasoning application (Ev-
erett et al, 2001).
(1) a. (SOLUTION 27032 70) If exhibiting 10-
132 faults replace the pre-fuser transport
sensor (Q10-130).
b. (SOLUTION 27240 80) 4. Enter into the
machine log, the changes that have been
made.
The WSJ grammar covers the UPenn Wall Street
Journal (WSJ) treebank sentences (Marcus et al,
1994). This corpus is characterized by long sen-
tences with many direct quotes and proper names,
(2a). In addition, for evaluation and training pur-
poses we also parsed a version of this corpus marked
up with labeled brackets and part-of-speech tags, as
in (2b). Riezler et al (2002) report on our WSJ pars-
ing experiments.
(2) a. But since 1981, Kirk Horse Insurance Inc.
of Lexington, Ky. has grabbed a 20% stake
of the market.
b. But since 1981, [NP-SBJ Kirk Horse In-
surance Inc. of Lexington, Ky.] has/VBZ
grabbed/VBN [NP a 20% stake of the mar-
ket].
2 Priority-based Grammar Specialization
The XLE system is designed so that the grammar
writer can build specialized grammars by both ex-
tending and restricting another grammar (in our case
the base grammar is the STANDARD Pargram En-
glish grammar). An LFG grammar is presented to
the XLE system in a priority-ordered sequence of
files containing phrase-structure rules, lexical en-
tries, abbreviatory macros and templates, feature
declarations, and finite-state transducers for tok-
enization and morphological analysis. XLE is ap-
plied to a single root file holding a CONFIGURA-
TION that identifies all the other files containing rel-
evant linguistic specifications, that indicates how
those components are to be assembled into a com-
plete grammar, and that specifies certain parameters
that control how that grammar is to be interpreted.
A key idea is that there can be only one definition
of an item of a given type with a particular name
(e.g., there can be only one NP rule although that sin-
gle rule can have many alternative expansions), and
items in a higher priority file override lower priority
items of the same type with the same name. This set
up is similar to the priority-override scheme of the
earlier LFG Grammar Writer?s Workbench (Kaplan
and Maxwell, 1996).
This arrangement makes it relatively easy to con-
struct a specialized grammar from a pre-existing
standard. The specialized grammar is defined by
a CONFIGURATION in its own root file that speci-
fies the relevant STANDARD grammar files as well
as the new files for the specialized grammar. The
files for the specialized grammar can also contain
items of different types (phrase-structure rules, lex-
ical entries, templates, etc.), and they are ordered
with higher priority than the STANDARD files.
Consider the configuration for the EUREKA gram-
mar. It specifies all of the STANDARD grammar files
as well as its own rule, template, lexicon, and mor-
phology files. A part of this configuration is shown
in (3) (the notationtemplates.lfg are shared by all the
languages? grammars, not just English).
(3) FILES ../standard/english-lexicons.lfg
../standard/english-rules.lfg
../standard/english-templates.lfg
../../common/notationtemplates.lfg
english-eureka-morphconfig
eureka-lexicons.lfg
eureka-rules.lfg
eureka-templates.lfg
This configuration specifies that the EUREKA rules,
templates, and lexical entries are given priority
over the STANDARD items by putting the spe-
cial EUREKA files at the end of the list. Thus, if
the ../standard/english-rules.lfg and eureka-rules.lfg
files both contain a rule expanding the NP category,
the one from the STANDARD file will be discarded in
favor of the EUREKA rule.
In the following subsections, we provide several
illustrations of how simple overriding has been used
for the EUREKA and WSJ grammar extensions.
2.1 Rules
The override convention makes it possible to: add
rules (e.g., for new or idiosyncratic constructions);
delete rules (e.g., to block constructions not found in
the new corpus); and modify rules to allow different
daughter sequences.
Rules may need to be added to allow for corpus-
specific constructions. This is illustrated in the EU-
REKA corpus by the identifier information that pre-
cedes each sentence, as in (1). In order to parse this
substring, a new category (FIELD) was defined with
an expansion that covers the identifier information
followed by the usual ROOT category of the STAN-
DARD grammar. The top-level category is one of
the parameters of a configuration, and the EUREKA
CONFIGURATION specifies that FIELD instead of the
STANDARD ROOT is the start-symbol of the gram-
mar. Thus the EUREKA grammar produces the tree
in (4) and functional-structure in (5) for (1a).
(4) FIELD
LP EURHEAD ID SUB-ID RP ROOT
( SOLUTION 27032 70 )
(5) PRED replace SUBJ, OBJ
SUBJ [ ]
OBJ [ ]
FIELD solution
TIP-ID 27032
SUB-TIP-ID 70
It is unusual in practice to need to delete a rule,
i.e., to eliminate completely the possibility of ex-
panding a given category of the STANDARD gram-
mar. This is generally only motivated when the spe-
cialized grammar applies to a domain where certain
constructions are rarely encountered, if at all. Al-
though there has been no need to delete rules for the
EUREKA and WSJ corpora, the override convention
also provides a natural way of achieving this effect.
For example, topicalization is extremely rare in the
the Eureka corpus and the STANDARD topicalization
rule sometimes introduces parsing inefficiency. This
can be avoided by having the high priority EUREKA
file replace the STANDARD rule with the one in (6).
(6) CPtop .
This vacuous rule expands the CPtop category to the
empty language, the language containing no strings;
so, this category is effectively removed from the
grammar.
Perhaps the most common change is to make
modifications to the behavior of existing rules. The
most direct way of doing this is simply to define a
new, higher priority expansion of the same left-hand
category. Since XLE only allows a single rule for a
given category, the old rule is discarded and the new
one comes into play. The new rule can be arbitrar-
ily different from the STANDARD one, but this is not
typically the case. It is much more common that the
specialized version incorporates most of the behav-
ior of the original, with minor extensions or restric-
tions. One way of producing the modified behavior
is to create a new rule that includes a copy of some
or all of the STANDARD rule?s right side along with
new material, and to give the new definition higher
priority than the old. For example, plurals in the Eu-
reka corpus can be formed by the addition of ?s in-
stead of the usual s, as in (7).
(7) (CAUSE 27416 10) A 7mfd inverter motor ca-
pacitor was installed on an unknown number of
UDH?s.
In order to allow for this, the N rule was rewritten to
allow a PL marker to optionally occur after any N,
as in (8).
(8) N copy of STANDARD N rule
(PL)
As a result of this rule modification, UDH?s in (7)
will have the tree and functional-structure in (9).
(9) a. N
PART PL
UDH ?s
b.
PRED UDH
NUM pl
Copying material from one version to another is
perhaps reasonable for relatively stable and simple
rules, like the N rule, but this can cause maintainabil-
ity problems with complicated rules in the STAN-
DARD grammar that are updated frequently. An al-
ternative strategy is to move the body of the STAN-
DARD N rule to a different rule, e.g., Nbody, which
in turn is called by the N rule in both the STANDARD
and EUREKA grammars. The Nbody category can be
supressed in the tree structure by invoking this rule
as a macro (notationally indicated as @Nbody).
(10) N @Nbody (PL).
Often the necessary modification can be made
simply by redefining a macro that existing rules al-
ready invoke. Consider the ROOT rule, in (11).
(11) ROOT @DECL-BODY @DECL-PUNCT
@INT-BODY @INT-PUNCT
@HEADER .
In the STANDARD grammar, the DECL-PUNCT
macro is defined as in (12a). However, this must
be modified in the EUREKA grammar because the
punctuation is much sloppier and often does not
occur at all; the EUREKA version is shown in (12b).
(12) a. DECL-PUNCT = PERIOD
EXCL-POINT .
b. DECL-PUNCT = ( PERIOD
EXCL-POINT
COLON
SEMI-COLON ).
The modular specifications that macros and tem-
plates provide allow rule behavior to be modified
without having to copy the parts of the rule that do
not change.
XLE also has a mechanism for systemati-
cally modifying the behavior of all rules: the
METARULEMACRO. For example, in order to
parse labeled bracketed input, as in (2b), the WSJ
grammar was altered so that constituents could
optionally be surrounded by the appropriately
labeled brackets. The METARULEMACRO is applied
to each rule in the grammar and produces as output
a modified version of that rule. This is used in
the STANDARD grammar for coordination and to
allow quote marks to surround any constituent. The
METARULEMACRO is redefined for the WSJ to add
the labeled bracketing possibilities for each rule, as
shown in (13).
(13) METARULEMACRO( CAT BASECAT RHS) =
LSB LABEL[ BASECAT] CAT RSB
copy of STANDARD coordination
copy of STANDARD surrounding quote .
The CAT, BASECAT, and RHS are arguments to
the METARULEMACRO that are instantiated to dif-
ferent values for each rule. RHS is instantiated to
the right-hand side of the rule, i.e., the rule expan-
sion. CAT and BASECAT are two ways of repre-
senting the left-hand side of the rule. For simple cat-
egories the CAT and BASECAT are the same (e.g.
NP for the NP rule). XLE also allows for complex
category symbols to specialize the expansion of par-
ticular categories in particular contexts. For exam-
ple, the VP rule is parameterized for the form of its
complement and its own form, so that VP[perf,fin]
is one of the complex VP categories. When the
METARULEMACRO applies to rules with complex
left-side categories, CAT refers to the category in-
cluding the parameters and the BASECAT refers to
the category without the parameters. For the VP ex-
ample, CAT is VP[perf,fin] and BASECAT is VP.
In the definition in (13), LSB and RSB parse the
brackets themselves, while the LABEL[ BASECAT]
parses the label in the bracketing and matches it to
the label in the tree (NP in (2b)); the consituent itself
is the CAT. Thus, a label-bracketed NP is assigned
the structure in (14).
(14) NP
LSB LABEL[NP] NP RSB
[ NP-SBJ Kirk Horse ]
These examples illustrate how the prioritized re-
definition of rules and macros has enabled us to in-
corporate the STANDARD rules in grammars that are
tuned to the special properties of the EUREKA and
WSJ corpora.
2.2 Lexical Entries
Just as for rules, XLE?s override conventions make
it possible to: add new lexical items or new part-of-
speech subentries for existing lexical items; delete
lexical items; and modify lexical items. In addition
to the basic priority overrides, XLE provides for
?edit lexical entries? (Kaplan and Newman, 1997)
that give finer control over the construction of the
lexicon. Edit entries were introduced as a way of rec-
onciling information from lexical databases of vary-
ing degrees of quality, but they are also helpful in
tailoring a STANDARD lexicon to a specialized cor-
pus. When working on specialized corpora, such as
the Eureka corpus, modifications to the lexicon are
extremely important for correctly handling techni-
cal terminology and eliminating word senses that are
not appropriate for the domain.
Higher-priority edit lexical entries provide for op-
erators that modify the definitions found in lower-
priority entries. The operators can: add a subentry
(+); delete a subentry ( ); replace a subentry (!);
or retain existing subentries (=). For example, the
STANDARD grammar might have an entry for button
as in (15).
(15) button !V @(V-SUBJ-OBJ %stem);
!N @(NOUN %stem);
ETC.
However, the EUREKA grammar might not need the
V entry but might require a special partname N en-
try. Assuming that the EUREKA lexicons are given
priority over the STANDARD lexicons, the entry in
(16) would accomplish this.
(16) button V ;
+N @(PARTNAME %stem);
ETC.
Note that the lexical entries in (15) and (16) end with
ETC. This is also part of the edit lexical entry sys-
tem. It indicates that other lower-priority definitions
of that lexical item will be retained in addition to
the new entries. For example, if in another EUREKA
lexicon there was an adjective entry for button with
ETC, the V, N, and A entries would all be used. The
alternative to ETC is ONLY which indicates that only
the new entry is to be used. In our button example, if
an adjective entry was added with ONLY, the V and
N entries would be removed, assuming that the ad-
jective entry occurred in the highest priority lexicon.
This machinery provides a powerful tool for build-
ing specialized lexicons without having to alter the
STANDARD lexicons.
The EUREKA corpus contains a large number of
names of copier parts. Due to their particular syn-
tax and to post-syntactic processing requirements, a
special lexical entry is added for each part name. In
addition, the regular noun parse of these entries is
deleted because whenever they occur in the corpus
they are part names. A sample lexical is shown in
(17); the ? is the escape character for the space.
(17) separator? finger
!PART-NAME @(PART-NAME %stem);
N;
ETC.
The first line in (17) states that separator finger can
be a PART NAME and when it is, it calls a template
PART-NAME that provides relevant information for
the functional-structure. The second line removes
the N entry, if any, as signalled by the before the
category name.
Because of the non-context free nature of Lexical
Functional Grammar, it sometimes happens that ex-
tensions in one part of the grammar require a cor-
responding adjustment in other rules or lexical en-
tries. Consider again the EUREKA ?s plurals. The
part-name UDH is singular when it appears with-
out the ?s and thus the morphological tag +Sg is ap-
pended to it. In the STANDARD grammar, the tag +Sg
has a lexical entry as in (18a) which states that +Sg is
of category NNUM and assigns sg to its NUM. How-
ever, if this is used in the EUREKA grammar, the sg
NUM specification will clash with the pl NUM spec-
ification when UDH appears with ?s, as seen in (7).
Thus, a new entry for +Sg is needed which has sg
as a default value, as in (18b). The first line of (18b)
states that NUM must exist but does not specify a
value, while the second line optionally supplies a sg
value to NUM; when the ?s is used, this option does
not apply since the form already has a pl NUM value.
(18) a. +Sg NNUM ( NUM)=sg
b. +Sg NNUM ( NUM)
(( NUM)=sg)
3 Tokenizing and Morphological Analysis
Tokenization and morphological analysis in XLE
are carried out by means of finite state transductions.
The STANDARD tokenizing transducer encodes the
punctuation conventions of normal English text,
which is adequate for many applications. However,
the Eureka and WSJ corpora include strings that must
be tokenized in non-standard ways. The Eureka part
identifiers have internal punctuation that would nor-
mally cause a string to be broken up (e.g. the hyphen
in PL1-B7), and the WSJ corpus is marked up with
labeled brackets and part-of-speech tags that must
also receive special treatment. An example of the
WSJ mark-up is seen in (19).
(19) [NP-SBJ Lloyd?s, once a pillar of the world
insurance market,] is/VBZ being/VBG
shaken/VBN to its very foundation.
Part-of-speech tags appear in a distinctive format,
beginning with a / and ending with a , with the in-
tervening material indicating the content of the tag
(VBZ for finite 3rd singular verb, VBG for a progres-
sive, VBN for a passive, etc.). The tokenizing trans-
ducer must recognize this pattern and split the tags
off as separate tokens. The tag-tokens must be avail-
able to filter the output of the morphological ana-
lyzer so that only verbal forms are compatible with
the tags in this example and the adjectival reading of
shaken is therefore blocked.
XLE tokenizing transducers are compiled from
specifications expressed in the sophisticated Xerox
finite state calculus (Beesley and Karttunen, 2002).
The Xerox calculus includes the composition, ig-
nore, and substitution operator discussed by Kaplan
and Kay (1994) and the priority-union operator of
Kaplan and Newman (1997). The specialized tok-
enizers are constructed by using these operators to
combine the STANDARD specification with expres-
sions that extend or restrict the standard behavior.
For example, the ignore operator is applied to allow
the part-of-speech information to be passed through
to the morphology without interrupting the standard
patterns of English punctuation.
XLE also allows separately compiled transduc-
ers to be combined at run-time by the operations
of priority-union, composition, and union. Priority-
union was used to supplement the standard morphol-
ogy with specialized ?guessing? transducers that ap-
ply only to tokens that would otherwise be unrec-
ognized. Thus, a finite-state guesser was added to
identify Eureka fault numbers (09-425), adjustment
numbers (12-23), part numbers (606K2100), part list
numbers (PL1-B7), repair numbers (2.4), tag num-
bers (P-102), and diagnostic code numbers (dC131).
Composition was used to apply the part-of-speech
filtering transducer to the output of the morpholog-
ical analyzer, and union provided an easy way of
adding new, corpus-specific terminology.
4 Optimality Marks
XLE supports a version of Optimality Theory (OT)
(Prince and Smolensky, 1993) which is used to rank
an analysis relative to other possible analyses (Frank
et al, 2001). In general, this is used within a specific
grammar to prefer or disprefer a construction. How-
ever, it can also be used in grammar extensions to
delete or include rules or parts of rules.
The XLE implementation of OT works as fol-
lows.1 OT marks are placed in the grammar and are
associated with particular rules, parts of rules, or
lexical entries. These marks are then ranked in the
grammar CONFIGURATION. In addition to a simple
ranking of constraints which states that a construc-
tion with a given OT mark is (dis)prefered to one
1The actual XLE OT implementation is more complicated
than this, allowing for UNGRAMMATICAL and STOPPOINT
marks as well. Only OT marks that are associated with NO-
GOOD are of interest here. For a full description, see (Frank et
al., 2001).
without it, XLE allows the marks to be specified as
NOGOOD. A rule or rule disjunct which has a NO-
GOOD OT mark associated with it will be ignored
by XLE. This can be used for grammar extensions
in that it allows a standard grammar to anticipate the
variations required by special corpora without using
them in normal circumstances.
Consider the example of the EUREKA ?s plurals
discussed in section 2.1. Instead of rewriting the N
rule in the EUREKA grammar, it would be possible
to modify it in the STANDARD grammar and include
an OT mark, as in (20).
(20) N original STANDARD N rules
(PL: @(OT-MARK EUR-PLURAL)).
The CONFIGURATION files of the STANDARD and
EUREKA grammars would differ in that the STAN-
DARD grammar would rank the EUR-PLURAL OT
mark as NOGOOD, as in (21a), while the EUREKA
grammar would simply not rank the mark, as in
(21b).
(21) a. STANDARD optimality order:
EUR-PLURAL NOGOOD
b. EUREKA optimality order:
NOGOOD
Given the OT marks, it would be possible to have
one large grammar that is specialized by different
OT rankings to produce the STANDARD, EUREKA,
and WSJ variants. However, from a grammar writ-
ing perspective this is not a desirable solution be-
cause it becomes difficult to keep track of which
constructions belong to standard English and are
shared among all the specializations and which are
corpus-specific. In addition, it does not distinguish a
core set of slowly changing linguistic specifications
for the basic patterns of the language, and thus does
not provide a stable foundation that the writers of
more specialized grammars can rely on.
5 Maintenance with Grammar Extensions
Maintenance is a serious issue for any large-scale
grammar development activity, and the maintenance
problems are compounded when multiple versions
are being created perhaps by several different gram-
mar writers. Our STANDARD grammar is now quite
mature and covers all the linguistically significant
constructions and most other constructions that we
have encountered in previous corpus analysis. How-
ever, every now and then, a new corpus, even a spe-
cialized one, will evidence a standard construction
that has not previously been accounted for. If spe-
cialized grammars were written by copying all the
STANDARD files and then modifying them, the im-
plementation of new standard constructions would
tend to appear only in the specialized grammar. Our
techniques for minimizing the amount of copying
encourages us to implement new constructions in the
STANDARD grammar and this makes them available
to all other specializations.
If a new version of a rule for a specialized gram-
mar is created by copying the corresponding STAN-
DARD rule, changes later made to the special rule
will not automatically be reflected in the STANDARD
grammar, and vice versa. This is the desired behav-
ior when adding unusual, corpus-specific construc-
tions. However, if the non-corpus specific parts of
the new rule are modified, these modifications will
not migrate to the STANDARD grammar. To avoid
this problem, the smallest rule possible should be
modified in the specialized grammar, e.g., modify-
ing the N head rule instead of the entire NP. For
this reason, having highly modularized rules and us-
ing macros and templates helps in grammar mainte-
nance both within a grammar and across specialized
grammar extensions.
As seen above, the XLE grammar development
platform provides a number of mechanisms to allow
for grammar extensions without altering the core
(STANDARD) grammar. However, there are still ar-
eas that could use improvement. For example, as
mentioned in section 2, the CONFIGURATION file
states which other files the grammar includes and
how they are prioritized. The CONFIGURATION con-
tains other information such as declarations of the
governable grammatical functions, the distributive
features, etc. As this information rarely changes
with grammar extensions, it would be helpful for
an extension configuration to incorporate by refer-
ence such additional parameters of the STANDARD
configuration. Currently these declarations must be
copied into each CONFIGURATION.
6 Discussion and Conclusion
As a result of the strategies and notational devices
outlined above, our specialized grammars share
substantial portions of the pre-existing STANDARD
grammar. The statistics in table (22) give an indica-
tion of the size of the STANDARD grammar and of
the additional material required for the EUREKA and
WSJ specializations. As can be seen from this table,
the specialized grammars require a relatively small
number of rules compared to the rules in the STAN-
DARD grammar. The number of lines that the rules
and lexical entries take up also provides a measure of
the relative size of the specifications. The WSJ lexi-
cons include many titles and proper nouns that may
ultimately be moved to the STANDARD files. The ta-
ble also shows the number of files called by the CON-
FIGURATION, as another indication of the size of the
specifications. This number is somewhat arbitrary as
separate files can be combined into a single multi-
sectioned file, although this is likely to reduce main-
tainability and readability.
(22)
STANDARD EUREKA WSJ
rules 310 32 14
lines:
rules 6,539 425 894
lexicons 44,879 5,565 15,135
files 14 5 8
The grammars compile into a collection of finite-
state machines with the number of states and arcs
listed in table (23). The WSJ grammar compiles into
the largest data structures, mainly because of its abil-
ity to parse labeled bracketed strings and part-of-
speech tags, (2b). This size increase is the result of
adding one disjunct in the METARULEMACRO and
hence reflects only a minor grammar change.
(23)
STANDARD EUREKA WSJ
states 4,935 5,132 8,759
arcs 13,268 13,639 19,695
In sum, the grammar specialization system used
in XLE has been quite sucessful in developing cor-
pus specific grammars using the STANDARD English
grammar as a basis. A significant benefit comes from
being able to distinguish truly unusual constructions
that exist only in the specialized grammar from those
that are (or should be) in the STANDARD grammar.
This allows idiosyncratic information to remain in a
specialized grammar while all the specialized gram-
mars benefit from and contribute to the continuing
development of the STANDARD grammar.
References
K. Beesley and L. Karttunen. 2002. Finite-State
Morphology: Xerox Tools and Techniques. Cam-
bridge University Press. To Appear.
M. Butt, T.H. King, M.-E. Nin?o, and F. Segond.
1999. A Grammar Writer?s Cookbook. CSLI
Publications, Stanford, CA.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project.
In Proceedings of COLING 2002. Workshop on
Grammar Engineering and Evaluation.
J. Everett, D. Bobrow, R. Stolle, R. Crouch,
V. de Paiva, C. Condoravdi, M. van den Berg,
and L. Polanyi. 2001. Making ontologies work
for resolving redundancies across documents.
Communications of the ACM, 45:55?60.
A. Frank, T. H. King, J. Kuhn, and J. T. Maxwell III.
2001. Optimality theory style constraint rank-
ing in large-scale LFG grammars. In Peter Sells,
editor, Formal and Empirical Issues in Optimal-
ity Theoretic Syntax. CSLI Publications, Stanford,
CA.
R. Kaplan and M. Kay. 1994. Regular models of
phonological rule systems. Computational Lin-
guistics, 20:331?378.
R. Kaplan and J. Maxwell. 1996. LFG Gram-
mar Writer?s Workbench. System documentation
manual; available on-line at PARC.
R. Kaplan and P. Newman. 1997. Lexical resource
conciliation in the Xerox Linguistic Environment.
In Proceedings of the ACL Workshop on Com-
putational Environments for Grammar Develop-
ment and Engineering.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. Mac-
Intyre, A. Bies, M. Ferguson, K. Katz, and
B. Schasberger. 1994. The Penn treebank: An-
notative predicate argument structure. In ARPA
Human Language Technology Workshop.
J. Maxwell and R. Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Compu-
tational Lingusitics, 19:571?589.
A. Prince and P. Smolensky. 1993. Optimality the-
ory: Constraint interaction in generative gram-
mar. RuCCS Technical Report #2, Rutgers Uni-
versity.
S. Riezler, T.H. King, R. Kaplan, D. Crouch, J. T.
Maxwell, III, and M. Johnson. 2002. Parsing
the Wall Street Journal using a lexical-functional
grammar and discriminative estimation tech-
niques. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics,
University of Pennsylvania.
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 57?64, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
On Some Pitfalls in Automatic Evaluation and Significance Testing for MT
Stefan Riezler and John T. Maxwell III
Palo Alto Research Center
3333 Coyote Hill Road, Palo Alto, CA 94304
Abstract
We investigate some pitfalls regarding the
discriminatory power of MT evaluation
metrics and the accuracy of statistical sig-
nificance tests. In a discriminative rerank-
ing experiment for phrase-based SMT we
show that the NIST metric is more sensi-
tive than BLEU or F-score despite their in-
corporation of aspects of fluency or mean-
ing adequacy into MT evaluation. In an
experimental comparison of two statistical
significance tests we show that p-values
are estimated more conservatively by ap-
proximate randomization than by boot-
strap tests, thus increasing the likelihood
of type-I error for the latter. We point
out a pitfall of randomly assessing signif-
icance in multiple pairwise comparisons,
and conclude with a recommendation to
combine NIST with approximate random-
ization, at more stringent rejection levels
than is currently standard.
1 Introduction
Rapid and accurate detection of result differences is
crucial in system development and system bench-
marking. In both situations a multitude of systems
or system variants has to be evaluated, so it is highly
desirable to employ automatic evaluation measures
for detection of result differences, and statistical hy-
pothesis tests to assess the significance of the de-
tected differences. When evaluating subtle differ-
ences between system variants in development, or
when benchmarking multiple systems, result differ-
ences may be very small in magnitude. This imposes
strong requirements on both automatic evaluation
measures and statistical significance tests: Evalua-
tion measures are needed that have high discrimi-
native power and yet are sensitive to the interesting
aspects of the evaluation task. Significance tests are
required to be powerful and yet accurate, i.e., if there
are significant differences they should be able to as-
sess them, but not if there are none.
In the area of statistical machine translation
(SMT), recently a combination of the BLEU evalua-
tion metric (Papineni et al, 2001) and the bootstrap
method for statistical significance testing (Efron and
Tibshirani, 1993) has become popular (Och, 2003;
Kumar and Byrne, 2004; Koehn, 2004b; Zhang et
al., 2004). Given the current practice of reporting
result differences as small as .3% in BLEU score,
assessed at confidence levels as low as 70%, ques-
tions arise concerning the sensitivity of the em-
ployed evaluation metrics and the accuracy of the
employed significance tests, especially when result
differences are small. We believe that is important to
accurately detect such small-magnitude differences
in order to understand how to improve systems and
technologies, even though such differences may not
matter in current applications.
In this paper we will investigate some pitfalls that
arise in automatic evaluation and statistical signifi-
cance testing in MT research. The first pitfall con-
cerns the discriminatory power of automatic eval-
uation measures. In the following, we compare the
sensitivity of three intrinsic evaluation measures that
differ with respect to their focus on different aspects
57
of translation. We consider the well-known BLEU
score (Papineni et al, 2001) which emphasizes flu-
ency by incorporating matches of high n-grams. Fur-
thermore, we consider an F-score measure that is
adapted from dependency-based parsing (Crouch et
al., 2002) and sentence-condensation (Riezler et al,
2003). This measure matches grammatical depen-
dency relations of parses for system output and ref-
erence translations, and thus emphasizes semantic
aspects of translational adequacy. As a third mea-
sure we consider NIST (Doddington, 2002), which
favors lexical choice over word order and does not
take structural information into account. On an ex-
perimental evaluation on a reranking experiment we
found that only NIST was sensitive enough to de-
tect small result differences, whereas BLEU and F-
score produced result differences that were statisti-
cally not significant. A second pitfall addressed in
this paper concerns the relation of power and ac-
curacy of significance tests. In situations where the
employed evaluation measure produces small result
differences, the most powerful significance test is
demanded to assess statistical significance of the re-
sults. However, accuracy of the assessments of sig-
nificance is seldom questioned. In the following,
we will take a closer look at the bootstrap test and
compare it with the related technique of approxi-
mate randomization (Noreen (1989)). In an exper-
imental evaluation on our reranking data we found
that approximate randomization estimated p-values
more conservatively than the bootstrap, thus increas-
ing the likelihood of type-I error for the latter test.
Lastly, we point out a common mistake of randomly
assessing significance in multiple pairwise compar-
isons (Cohen, 1995). This is especially relevant in
k-fold pairwise comparisons of systems or system
variants where k is high. Taking this multiplicity
problem into account, we conclude with a recom-
mendation of a combination of NIST for evaluation
and the approximate randomization test for signifi-
cance testing, at more stringent rejection levels than
is currently standard in the MT literature. This is es-
pecially important in situations where multiple pair-
wise comparisons are conducted, and small result
differences are expected.
2 The Experimental Setup: Discriminative
Reranking for Phrase-Based SMT
The experimental setup we employed to compare
evaluation measures and significance tests is a dis-
criminative reranking experiment on 1000-best lists
of a phrase-based SMT system. Our system is a
re-implementation of the phrase-based system de-
scribed in Koehn (2003), and uses publicly avail-
able components for word alignment (Och and Ney,
2003)1, decoding (Koehn, 2004a)2, language mod-
eling (Stolcke, 2002)3 and finite-state processing
(Knight and Al-Onaizan, 1999)4. Training and test
data are taken from the Europarl parallel corpus
(Koehn, 2002)5.
Phrase-extraction follows Och et al (1999) and
was implemented by the authors: First, the word
aligner is applied in both translation directions, and
the intersection of the alignment matrices is built.
Then, the alignment is extended by adding immedi-
ately adjacent alignment points and alignment points
that align previously unaligned words. From this
many-to-many alignment matrix, phrases are ex-
tracted according to a contiguity requirement that
states that words in the source phrase are aligned
only with words in the target phrase, and vice versa.
Discriminative reranking on a 1000-best list of
translations of the SMT system uses an `1 regu-
larized log-linear model that combines a standard
maximum-entropy estimator with an efficient, in-
cremental feature selection technique for `1 regu-
larization (Riezler and Vasserman, 2004). Training
data are defined as pairs {(sj , tj)}mj=1 of source sen-
tences sj and gold-standard translations tj that are
determined as the translations in the 1000-best list
that best match a given reference translation. The
objective function to be minimized is the conditional
log-likelihood L(?) subject to a regularization term
R(?), where T (s) is the set of 1000-best translations
for sentence s, ? is a vector or log-parameters, and
1http://www.fjoch.com/GIZA++.html
2http://www.isi.edu/licensed-sw/pharaoh/
3http://www.speech.sri.com/projects/srilm/
4http://www.isi.edu/licensed-sw/carmel/
5http://people.csail.mit.edu/people/
koehn/publications/europarl/
58
Table 1: NIST, BLEU, F-scores for reranker and baseline on development set
NIST BLEU F
baseline 6.43 .301 .385
reranking 6.58 .298 .383
approxrand p-value < .0001 .158 .424
bootstrap p-value < .0001 .1 -
f is a vector of feature functions:
L(?) + R(?) = ? log
m?
j=1
p?(tj |sj) + R(?)
= ?
m?
j=1
log
e??f(tj)
?
t?T (sj)
e??f(t)
+ R(?)
The features employed in our experiments con-
sist of 8 features corresponding to system compo-
nents (distortion model, language model, phrase-
translations, lexical weights, phrase penalty, word
penalty) as provided by PHARAOH, together with a
multitude of overlapping phrase features. For exam-
ple, for a phrase-table of phrases consisting of max-
imally 3 words, we allow all 3-word phrases and 2-
word phrases as features. Since bigram features can
overlap, information about trigrams can be gathered
by composing bigram features even if the actual tri-
gram is not seen in the training data.
Feature selection makes it possible to employ and
evaluate a large number of features, without con-
cerns about redundant or irrelevant features hamper-
ing generalization performance. The `1 regularizer is
defined by the weighted `1-norm of the parameters
R(?) = ?||?||1 = ?
n?
i=1
|?i|
where ? is a regularization coefficient, and n is num-
ber of parameters. This regularizer penalizes overly
large parameter values in their absolute values, and
tends to force a subset of the parameters to be ex-
actly zero at the optimum. This fact leads to a natural
integration of regularization into incremental feature
selection as follows: Assuming a tendency of the `1
regularizer to produce a large number of zero-valued
parameters at the function?s optimum, we start with
all-zero weights, and incrementally add features to
the model only if adjusting their parameters away
from zero sufficiently decreases the optimization cri-
terion. Since every non-zero weight added to the
model incurs a regularizer penalty of ?|?i|, it only
makes sense to add a feature to the model if this
penalty is outweighed by the reduction in negative
log-likelihood. Thus features considered for selec-
tion have to pass the following test:
?
?
?
?
?L(?)
??i
?
?
?
? > ?
This gradient test is applied to each feature and at
each step the features that pass the test with maxi-
mum magnitude are added to the model. This pro-
vides both efficient and accurate estimation with
large feature sets.
Work on discriminative reranking has been re-
ported before by Och and Ney (2002), Och et al
(2004), and Shen et al (2004). The main purpose of
our reranking experiments is to have a system that
can easily be adjusted to yield system variants that
differ at controllable amounts. For quick experimen-
tal turnaround we selected the training and test data
from sentences with 5 to 15 words, resulting in a
training set of 160,000 sentences, and a development
set of 2,000 sentences. The phrase-table employed
was restricted to phrases of maximally 3 words, re-
sulting in 200,000 phrases.
3 Detecting Small Result Differences by
Intrinsic Evaluations Metrics
The intrinsic evaluation measures used in our ex-
periments are the well-known BLEU (Papineni et
al., 2001) and NIST (Doddington, 2002) metrics,
and an F-score measure that adapts evaluation tech-
niques from dependency-based parsing (Crouch et
al., 2002) and sentence-condensation (Riezler et al,
2003) to machine translation. All of these measures
59
Set c = 0
Compute actual statistic of score differences |SX ? SY| on test data
For random shuffles r = 0, . . . , R
For sentences in test set
Shuffle variable tuples between system X and Y with probability 0.5
Compute pseudo-statistic |SXr ? SYr | on shuffled data
If |SXr ? SYr | ? |SX ? SY|
c + +
p = (c + 1)/(R + 1)
Reject null hypothesis if p is less than or equal to specified rejection level.
Figure 1: Approximate Randomization Test for Statistical Significance Testing
evaluate document similarity of SMT output against
manually created reference translations. The mea-
sures differ in their focus on different entities in
matching, corresponding to a focus on different as-
pects of translation quality.
BLEU and NIST both consider n-grams in source
and reference strings as matching entities. BLEU
weighs all n-grams equally whereas NIST puts more
weight on n-grams that are more informative, i.e.,
occur less frequently. This results in BLEU favor-
ing matches in larger n-grams, corresponding to giv-
ing more credit to correct word order. NIST weighs
lower n-grams more highly, thus it gives more credit
to correct lexical choice than to word order.
F-score is computed by parsing reference sen-
tences and SMT outputs, and matching grammatical
dependency relations. The reported value is the har-
monic mean of precision and recall, which is defined
as (2? precision ? recall )/( precision + recall ).
Precision is the ratio of matching dependency re-
lations to the total number of dependency relations
in the parse for the system translation, and recall is
the ratio of matches to the total number of depen-
dency relations in the parse for the reference trans-
lation. The goal of this measure is to focus on as-
pects of meaning in measuring similarity of system
translations to reference translations, and to allow
for meaning-preserving word order variation.
Evaluation results for a comparison of rerank-
ing against a baseline model that only includes fea-
tures corresponding to the 8 system components are
shown in Table 1. Since the task is a comparison
of system variants for development, all results are
reported on the development set of 2,000 exam-
ples of length 5-15. The reranking model achieves
an increase in NIST score of .15 units, whereas
BLEU and F-score decrease by .3% and .2% respec-
tively. However, as measured by the statistical sig-
nificance tests described below, the differences in
BLEU and F-scores are not statistically significant
with p-values exceeding the standard rejection level
of .05. In contrast, the differences in NIST score
are highly significant. These findings correspond to
results reported in Zhang et al (2004) showing a
higher sensitivity of NIST versus BLEU to small re-
sult differences. Taking also the results from F-score
matching in account, we can conclude that similar-
ity measures that are based on matching more com-
plex entities (such as BLEU?s higher n-grams or F?s
grammatical relations) are not as sensitive to small
result differences as scoring techniques that are able
to distinguish models by matching simpler entities
(such as NIST?s focus on lexical choice). Further-
more, we get an indication that differences of .3%
in BLEU score or .2% in F-score might not be large
enough to conclude statistical significance of result
differences. This leads to questions of power and ac-
curacy of the employed statistical significance tests
which will be addressed in the next section.
4 Assessing Statistical Significance of
Small Result Differences
The bootstrap method is an example for a computer-
intensive statistical hypothesis test (see, e.g., Noreen
(1989)). Such tests are designed to assess result
differences with respect to a test statistic in cases
where the sampling distribution of the test statistic
60
Set c = 0
Compute actual statistic of score differences |SX ? SY| on test data
Calculate sample mean ?B = 1B
?B
b=0 |SXb ? SYb | over bootstrap samples b = 0, . . . , B
For bootstrap samples b = 0, . . . , B
Sample with replacement from variable tuples for systems X and Y for test sentences
Compute pseudo-statistic |SXb ? SYb | on bootstrap data
If |SXb ? SYb | ? ?B (+?) ? |SX ? SY|
c + +
p = (c + 1)/(B + 1)
Reject null hypothesis if p is less than or equal to specified rejection level.
Figure 2: Bootstrap Test for Statistical Significance Testing
is unknown. Comparative evaluations of outputs of
SMT systems according to test statistics such as dif-
ferences in BLEU, NIST, or F-score are examples
of this situation. The attractiveness of computer-
intensive significance tests such as the bootstrap
or the approximate randomization method lies in
their power and simplicity. As noted in standard
textbooks such as Cohen (1995) or Noreen (1989)
such tests are as powerful as parametric tests when
parametric assumptions are met and they outper-
form them when parametric assumptions are vio-
lated. Because of their generality and simplicity they
are also attractive alternatives to conventional non-
parametric tests (see, e.g., Siegel (1988)). The power
of these tests lies in the fact that they answer only a
very simple question without making too many as-
sumptions that may not be met in the experimen-
tal situation. In case of the approximate random-
ization test, only the question whether two sam-
ples are related to each other is answered, with-
out assuming that the samples are representative of
the populations from which they were drawn. The
bootstrap method makes exactly this one assump-
tion. This makes it formally possible to draw in-
ferences about population parameters for the boot-
strap, but not for approximate randomization. How-
ever, if the goal is to assess statistical significance
of a result difference between two systems the ap-
proximate randomization test provides the desired
power and accuracy whereas the bootstrap?s advan-
tage to draw inferences about population parameters
comes at the price of reduced accuracy. Noreen sum-
marizes this shortcoming of the bootstrap technique
as follows: ?The principal disadvantage of [the boot-
strap] method is that the null hypothesis may be re-
jected because the shape of the sampling distribution
is not well-approximated by the shape of the boot-
strap sampling distribution rather than because the
expected value of the test statistic differs from the
value that is hypothesized.?(Noreen (1989), p. 89).
Below we describe these two test procedures in more
detail, and compare them in our experimental setup.
4.1 Approximate Randomization
An excellent introduction to the approximate ran-
domization test is Noreen (1989). Applications of
this test to natural language processing problems can
be found in Chinchor et al (1993).
In our case of assessing statistical significance of
result differences between SMT systems, the test
statistic of interest is the absolute value of the differ-
ence in BLEU, NIST, or F-scores produced by two
systems on the same test set. These test statistics are
computed by accumulating certain count variables
over the sentences in the test set. For example, in
case of BLEU and NIST, variables for the length of
reference translations and system translations, and
for n-gram matches and n-gram counts are accumu-
lated over the test corpus. In case of F-score, vari-
able tuples consisting of the number of dependency-
relations in the parse for the system translation, the
number of dependency-relations in the parse for the
reference translation, and the number of matching
dependency-relations between system and reference
parse, are accumulated over the test set.
Under the null hypothesis, the compared systems
are not different, thus any variable tuple produced by
one of the systems could have been produced just as
61
Table 2: NIST scores for equivalent systems under bootstrap and approximate randomization tests.
compared systems 1:2 1:3 1:4 1:5 1:6
NIST difference .031 .032 .029 .028 .036
approxrand p-value .03 .025 .05 .079 .028
bootstrap p-value .014 .013 .028 .039 .013
likely by the other system. So shuffling the variable
tuples between the two systems with equal probabil-
ity, and recomputing the test statistic, creates an ap-
proximate distribution of the test statistic under the
null hypothesis. For a test set of S sentences there
are 2S different ways to shuffle the variable tuples
between the two systems. Approximate randomiza-
tion produce shuffles by random assignments instead
of evaluating all 2S possible assignments. Signifi-
cance levels are computed as the percentage of trials
where the pseudo statistic, i.e., the test statistic com-
puted on the shuffled data, is greater than or equal to
the actual statistic, i.e., the test statistic computed on
the test data. A sketch of an algorithm for approxi-
mate randomization testing is given in Fig. 1.
4.2 The Bootstrap
An excellent introduction to the technique is the
textbook by Efron and Tibshirani (1993). In contrast
to approximate randomization, the bootstrap method
makes the assumption that the sample is a repre-
sentative ?proxy? for the population. The shape of
the sampling distribution is estimated by repeatedly
sampling (with replacement) from the sample itself.
A sketch of a procedure for bootstrap testing is
given in Fig. 2. First, the test statistic is computed on
the test data. Then, the sample mean of the pseudo
statistic is computed on the bootstrapped data, i.e.,
the test statistic is computed on bootstrap samples
of equal size and averaged over bootstrap samples.
In order to compute significance levels based on
the bootstrap sampling distribution, we employ the
?shift? method described in Noreen (1989). Here it
is assumed that the sampling distribution of the null
hypothesis and the bootstrap sampling distribution
have the same shape but a different location. The
location of the bootstrap sampling distribution is
shifted so that it is centered over the location where
the null hypothesis sampling distribution should be
centered. This is achieved by subtracting from each
value of the pseudo-statistic its expected value ?B
and then adding back the expected value ? of the
test statistic under the null hypothesis. ?B can be es-
timated by the sample mean of the bootstrap sam-
ples; ? is 0 under the null hypothesis. Then, similar
to the approximate randomization test, significance
levels are computed as the percentage of trials where
the (shifted) pseudo statistic is greater than or equal
to the actual statistic.
4.3 Power vs. Type I Errors
In order to evaluate accuracy of the bootstrap and the
approximate randomization test, we conduct an ex-
perimental evaluation of type-I errors of both boot-
strap and approximate randomization on real data.
Type-I errors indicate failures to reject the null hy-
pothesis when it is true. We construct SMT system
variants that are essentially equal but produce su-
perficially different results. This can be achieved by
constructing reranking variants that differ in the re-
dundant features that are included in the models, but
are similar in the number and kind of selected fea-
tures. The results of this experiment are shown in Ta-
ble 2. System 1 does not include irrelevant features,
whereas systems 2-6 were constructed by adding a
slightly different number of features in each step,
but resulted in the same number of selected features.
Thus competing features bearing the same informa-
tion are exchanged in different models, yet overall
the same information is conveyed by slightly dif-
ferent feature sets. The results of Table 2 show that
the bootstrap method yields p-values < .015 in 3
out of 5 pairwise comparisons whereas the approx-
imate randomization test yields p-values ? .025 in
all cases. Even if the true p-value is unknown, we
can say that the approximate randomization test es-
timates p-values more conservatively than the boot-
strap, thus increasing the likelihood of type-I error
for the bootstrap test. For a restrictive significance
level of 0.15, which is motivated below for multiple
62
comparisons, the bootstrap would assess statistical
significance in 3 out of 5 cases whereas statistical
significance would not be assessed under approxi-
mate randomization. Assuming equivalence of the
compared system variants, these assessments would
count as type-I errors.
4.4 The Multiplicity Problem
In the experiment on type-I error described above, a
more stringent rejection level than the usual .05 was
assumed. This was necessary to circumvent a com-
mon pitfall in significance testing for k-fold pairwise
comparisons. Following the argumentation given in
Cohen (1995), the probability of randomly assess-
ing statistical significance for result differences in
k-fold pairwise comparisons grows exponentially in
k. Recall that for a pairwise comparison of systems,
specifying that p < .05 means that the probability of
incorrectly rejecting the null hypothesis that the sys-
tems are not different be less than .05. Caution has
to be exercised in k-fold pairwise comparisons: For
a probability pc of incorrectly rejecting the null hy-
pothesis in a specific pairwise comparison, the prob-
ability pe of at least once incorrectly rejecting this
null hypothesis in an experiment involving k pair-
wise comparisons is
pe ? 1? (1? pc)
k
For large values of k, the probability of concluding
result differences incorrectly at least once is unde-
sirably high. For example, in benchmark testing of
15 systems, 15(15 ? 1)/2 = 105 pairwise compar-
isons will have to be conducted. At a per-comparison
rejection level pc = .05 this results in an experi-
mentwise error pe = .9954, i.e., the probability of
at least one spurious assessment of significance is
1? (1? .05)105 = .9954. One possibility to reduce
the likelihood that one ore more of differences as-
sessed in pairwise comparisons is spurious is to run
the comparisons at a more stringent per-comparison
rejection level. Reducing the per-comparison rejec-
tion level pc until an experimentwise error rate pe
of a standard value, e.g., .05, is achieved, will favor
pe over pc. In the example of 5 pairwise compar-
isons described above, a per-comparison error rate
pc = .015 was sufficient to achieve an experimen-
twise error rate pe ? .07. In many cases this tech-
nique would require to reduce pc to the point where
a result difference has to be unrealistically large to
be significant. Here conventional tests for post-hoc
comparisons such as the Scheffe? or Tukey test have
to be employed (see Cohen (1995), p. 185ff.).
5 Conclusion
Situations where a researcher has to deal with subtle
differences between systems are common in system
development and large benchmark tests. We have
shown that it is useful in such situations to trade in
expressivity of evaluation measures for sensitivity.
For MT evaluation this means that recording differ-
ences in lexical choice by the NIST measure is more
useful than failing to record differences by employ-
ing measures such as BLEU or F-score that incorpo-
rate aspects of fluency and meaning adequacy into
MT evaluation. Similarly, in significance testing, it
is useful to trade in the possibility to draw inferences
about the sampling distribution for accuracy and
power of the test method. We found experimental
evidence confirming textbook knowledge about re-
duced accuracy of the bootstrap test compared to the
approximate randomization test. Lastly, we pointed
out a well-known problem of randomly assessing
significance in multiple pairwise comparisons. Tak-
ing these findings together, we recommend for mul-
tiple comparisons of subtle differences to combine
the NIST score for evaluation with the approximate
randomization test for significance testing, at more
stringent rejection levels than is currently standard
in the MT literature.
References
Nancy Chinchor, Lynette Hirschman, and David D.
Lewis. 1993. Evaluating message understanding sys-
tems: An analysis of the third message understand-
ing conference (MUC-3). Computational Linguistics,
19(3):409?449.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage stochastic parser. In Pro-
ceedings of the ?Beyond PARSEVAL? Workshop at the
3rd International Conference on Language Resources
and Evaluation (LREC?02), Las Palmas, Spain.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
63
statistics. In Proceedings of the ARPA Workshop on
Human Language Technology.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York.
Kevin Knight and Yaser Al-Onaizan. 1999. A primer on
finite-state software for natural language processing.
Technical report, USC Information Sciences Institute,
Marina del Rey, CA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
and the 3rd Meeting of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?03), Edmonton, Cananda.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Technical re-
port, USC Information Sciences Institute, Marina del
Rey, CA.
Philipp Koehn. 2004a. PHARAOH. a beam search de-
coder for phrase-based statistical machine translation
models. user manual. Technical report, USC Informa-
tion Sciences Institute, Marina del Rey, CA.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP?04), Barcelona, Spain.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy conference / North American chapter of the Asso-
ciation for Computational Linguistics annual meeting
(HLT/NAACL?04), Boston, MA.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL?02), Philadelphia, PA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 1999 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?99).
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Ketherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy conference / North American chapter of the Asso-
ciation for Computational Linguistics annual meeting
(HLT/NAACL?04), Boston, MA.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceedings
of the Human Language Technology Conference and
the 3rd Meeting of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?03), Edmonton, Cananda.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and `1 regularization for re-
laxed maximum-entropy modeling. In Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP?04), Barcelona, Spain.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the Human Language Technology Confer-
ence and the 3rd Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL?03), Edmonton, Cananda.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proceedings of the Human Language Technology con-
ference / North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT/NAACL?04), Boston, MA.
Sidney Siegel. 1988. Nonparametric Statistics for the
Behavioral Sciences. Second Edition. MacGraw-Hill,
Boston, MA.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
CO.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC?04), Lisbon, Portu-
gal.
64
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 65?72,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Pruning the Search Space of a Hand-Crafted Parsing System with a
Probabilistic Parser
Aoife Cahill
Dublin City University
acahill@computing.dcu.ie
Tracy Holloway King
PARC
thking@parc.com
John T. Maxwell III
PARC
maxwell@parc.com
Abstract
The demand for deep linguistic analysis
for huge volumes of data means that it is
increasingly important that the time taken
to parse such data is minimized. In the
XLE parsing model which is a hand-crafted,
unification-based parsing system, most of
the time is spent on unification, searching
for valid f-structures (dependency attribute-
value matrices) within the space of the many
valid c-structures (phrase structure trees).
We carried out an experiment to determine
whether pruning the search space at an ear-
lier stage of the parsing process results in
an improvement in the overall time taken to
parse, while maintaining the quality of the
f-structures produced. We retrained a state-
of-the-art probabilistic parser and used it to
pre-bracket input to the XLE, constraining
the valid c-structure space for each sentence.
We evaluated against the PARC 700 Depen-
dency Bank and show that it is possible to
decrease the time taken to parse by ?18%
while maintaining accuracy.
1 Introduction
When deep linguistic analysis of massive data is re-
quired (e.g. processing Wikipedia), it is crucial that
the parsing time be minimized. The XLE English
parsing system is a large-scale, hand-crafted, deep,
unification-based system that processes raw text
and produces both constituent-structures (phrase
structure trees) and feature-structures (dependency
attribute-value matrices). A typical breakdown of
parsing time of XLE components is Morphology
(1.6%), Chart (5.8%) and Unifier (92.6%).
The unification process is the bottleneck in the
XLE parsing system. The grammar generates many
valid c-structure trees for a particular sentence: the
Unifier then processes all of these trees (as packed
structures), and a log-linear disambiguation module
can choose the most probable f-structure from the
resulting valid f-structures. For example, the sen-
tence ?Growth is slower.? has 84 valid c-structure
trees according to the current English grammar;1
however once the Unifier has processed all of these
trees (in a packed form), only one c-structure and
f-structure pair is valid (see Figure 1). In this in-
stance, the log-linear disambiguation does not need
to choose the most probable result.
The research question we pose is whether the
search space can be pruned earlier before unifi-
cation takes place. Bangalore and Joshi (1999),
Clark and Curran (2004) and Matsuzaki et al (2007)
show that by using a super tagger before (CCG and
HPSG) parsing, the space required for discrimini-
tive training is drastically reduced. Supertagging
is not widely used within the LFG framework, al-
though there has been some work on using hypertags
(Kinyon, 2000). Ninomiya et al (2006) propose a
method for faster HPSG parsing while maintaining
accuracy by only using the probabilities of lexical
entry selections (i.e. the supertags) in their discrim-
initive model. In the work presented here, we con-
1For example, is can be a copula, a progressive auxiliary or
a passive auxiliary, while slower can either be an adjective or an
adverb.
65
centrate on reducing the number of c-structure trees
that the Unifier has to process, ideally to one tree.
The hope was that this would speed up the parsing
process, but how would it affect the quality of the f-
structures? This is similar to the approach taken by
Cahill et al (2005) who do not use a hand-crafted
complete unification system (rather an automatically
acquired probabilistic approximation). They parse
raw text into LFG f-structures by first parsing with a
probabilistic CFG parser to choose the most proba-
ble c-structure. This is then passed to an automatic
f-structure annotation algorithm which deterministi-
cally generates one f-structure for that tree.
The most compact way of doing this would be to
integrate a statistical component to the parser that
could rank the c-structure trees and only pass the
most likely forward to the unification process. How-
ever, this would require a large rewrite of the sys-
tem. So, we first wanted to investigate a ?cheaper?
alternative to determine the viability of the pruning
strategy; this is the experiment reported in this pa-
per. This is implemented by stipulating constituent
boundaries in the input string, so that any c-structure
that is incompatible with these constraints is invalid
and will not be processed by the Unifier. This was
done to some extent in Riezler et al (2002) to au-
tomatically generate training data for the log-linear
disambiguation component of XLE. Previous work
obtained the constituent constraints (i.e. brackets)
from the gold-standard trees in the Penn-II Tree-
bank. However, to parse novel text, gold-standard
trees are unavailable.
We used a state-of-the-art probabilistic parser to
provide the bracketing constraints to XLE. These
parsers are accurate (achieving accuracy of over
90% on Section 23 WSJ text), fast, and robust.
The idea is that pre-parsing of the input text by a
fast and accurate parser can prune the c-structure
search space, reducing the amount of work done by
the Unifier, speed up parsing and maintain the high
quality of the f-structures produced.
The structure of this paper is as follows: Section
2 introduces the XLE parsing system. Section 3 de-
scribes a baseline experiment and based on the re-
sults suggests retraining the Bikel parser to improve
results (Section 4). Section 5 describes experiments
on the development set, from which we evaluate the
most successful system against the PARC 700 test
CS 1: ROOT
Sadj[fin]
S[fin]
NP
NPadj
NPzero
N
^ growth
VPall[fin]
VPcop[fin]
Vcop[fin]
is
AP[pred]
A
slower
PERIOD
.
"Growth is slower."
'be<[68:slow]>[23:growth]'PRED
'growth'PRED23SUBJ
'slow<[23:growth]>'PRED
[23:growth]SUBJ
'more'PRED-1ADJUNCT68
XCOMP
47
Figure 1: C- and F-Structure for ?Growth is slower.?
set (Section 6). Finally, Section 7 concludes.
2 Background
In this section we introduce Lexical Functional
Grammar, the grammar formalism underlying the
XLE, and briefly describe the XLE parsing system.
2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and
Bresnan, 1982) is a constraint-based theory of gram-
mar. It (minimally) posits two levels of repre-
sentation, c(onstituent)-structure and f(unctional)-
structure. C-structure is represented by context-
free phrase-structure trees, and captures surface
grammatical configurations such as word order.
The nodes in the trees are annotated with func-
tional equations (attribute-value structure con-
straints) which are resolved to produce an f-
structure. F-structures are recursive attribute-value
matrices, representing abstract syntactic functions.
F-structures approximate basic predicate-argument-
adjunct structures or dependency relations. Fig-
ure 1 shows the c- and f-structure for the sentence
?Growth is slower.?.
66
Parser Output: (S1 (S (NP (NN Growth)) (VP (AUX is) (ADJP (JJR slower))) (. .)))
Labeled: \[S1 \[S Growth \[VP is \[ADJP slower\] \].\] \]
Unlabeled:\[ \[ Growth \[ is \[ slower\] \].\] \]
Figure 2: Example of retained brackets from parser output to constrain the XLE parser
2.2 The XLE Parsing System
The XLE parsing system is a deep-grammar-based
parsing system. The experiments reported in this
paper use the English LFG grammar constructed
as part of the ParGram project (Butt et al, 2002).
This system incorporates sophisticated ambiguity-
management technology so that all possible syn-
tactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and
Kaplan, 1993). In accordance with LFG the-
ory, the output includes not only standard context-
free phrase-structure trees (c-structures) but also
attribute-value matrices (f-structures) that explic-
itly encode predicate-argument relations and other
meaningful properties. The f-structures can be de-
terministically mapped to dependency triples with-
out any loss of information, using the built-in or-
dered rewrite system (Crouch et al, 2002). XLE se-
lects the most probable analysis from the potentially
large candidate set by means of a stochastic disam-
biguation component based on a log-linear proba-
bility model (Riezler et al, 2002) that works on the
packed representations. The underlying parsing sys-
tem also has built-in robustness mechanisms that al-
low it to parse strings that are outside the scope of
the grammar as a list of fewest well-formed ?frag-
ments?. Furthermore, performance parameters that
bound parsing and disambiguation can be tuned for
efficient but accurate operation. These parameters
include at which point to timeout and return an error,
the amount of stack memory to allocate, the num-
ber of new edges to add to the chart and at which
point to start skimming (a process that guarantees
XLE will finish processing a sentence in polynomial
time by only carrying out a bounded amount of work
on each remaining constituent after a time threshold
has passed). For the experiments reported here, we
did not fine-tune these parameters due to time con-
straints; so default values were arbitrarily set and the
same values used for all parsing experiments.
3 Baseline experiments
We carried out a baseline experiment with two
state-of-the-art parsers to establish what effect pre-
bracketing the input to the XLE system has on the
quality and number of the solutions produced. We
used the Bikel () multi-threaded, head-driven chart-
parsing engine developed at the University of Penn-
sylvania. The second parser is that described in
Charniak and Johnson (2005). This parser uses a
discriminative reranker that selects the most proba-
ble parse from the 50-best parses returned by a gen-
erative parser based on Charniak (2000).
We evaluated against the PARC 700 Dependency
Bank (King et al, 2003) which provides gold-
standard analyses for 700 sentences chosen at ran-
dom from Section 23 of the Penn-II Treebank. The
Dependency Bank was bootstrapped by parsing the
700 sentences with the XLE English grammar, and
then manually correcting the output. The data is di-
vided into two sets, a 140-sentence development set
and a test set of 560 sentences (Kaplan et al, 2004).
We took the raw strings from the 140-sentence
development set and parsed them with each of the
state-of-the-art probabilistic parsers. As an upper
bound for the baseline experiment, we use the brack-
ets in the original Penn-II treebank trees for the 140
development set.
We then used the brackets from each parser out-
put (or original treebank trees) to constrain the XLE
parser. If the input to the XLE parser is bracketed,
the parser will only generate c-structures that respect
these brackets (i.e., only c-structures with brackets
that are compatible with the input brackets are con-
sidered during the unification stage). Figure 2 gives
an example of retained brackets from the parser out-
put. We do not retain brackets around PRN (paren-
thetical phrase) or NP nodes as their structure often
differed too much from XLE analyses of the same
phrases. We passed pre-bracketed strings to the XLE
and evaluated the output f-structures in terms of de-
pendency triples against the 140-sentence subset of
67
Non-Fragment Fragment
Penn-XLE Penn-XLE Penn-XLE Penn-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE parses (/140) 0 89 140 140
F-Score of subset 0 84.11 53.92 74.87
Overall F-Score 0 58.91 53.92 74.87
Table 1: Upper-bound results for original Penn-II trees
Non-Fragment Fragment
XLE Bikel-XLE Bikel-XLE XLE Bikel-XLE Bikel-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE Parses (/140) 119 0 84 135 140 140
F-Score of Subset 81.57 0 84.23 78.72 54.37 73.71
Overall F-Score 72.01 0 55.06 76.13 54.37 *73.71
XLE CJ-XLE CJ-XLE XLE CJ-XLE CJ-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE Parses (/140) 119 0 86 135 139 139
F-Score of Subset 81.57 0 86.57 78.72 53.96 75.64
Overall F-Score 72.01 0 58.04 76.13 53.48 *74.98
Table 2: Bikel (2002) and Charniak and Johnson (2005) out-of-the-box baseline results
the PARC 700 Dependency Bank.
The results of the baseline experiments are given
in Tables 1 and 2. Table 1 gives the upper bound
results if we use the gold standard Penn treebank
to bracket the input to XLE. Table 2 compares the
XLE (fragment and non-fragment) grammar to the
system where the input is pre-parsed by each parser.
XLE fragment grammars provide a back-off when
parsing fails: the grammar is relaxed and the parser
builds a fragment parse of the well-formed chunks.
We compare the parsers in terms of total number
of parses (out of 140) and the f-score of the sub-
set of sentences successfully parsed. We also com-
bine these scores to give an overall f-score, where
the system scores 0 for each sentence it could not
parse. When testing for statistical significance be-
tween systems, we compare the overall f-score val-
ues. Figures marked with an asterisk are not statisti-
cally significantly different at the 95% level.2
The results show that using unlabeled brackets
achieves reasonable f-scores with the non-fragment
grammar. Using the labeled bracketing from the out-
put of both parsers causes XLE to always fail when
parsing. This is because the labels in the output of
parsers trained on the Penn-II treebank differ con-
siderably from the labels on c-structure trees pro-
2We use the approximate randomization test (Noreen, 1989)
to test for significance.
duced by XLE. Interestingly, the f-scores for both
the CJ-XLE and Bikel-XLE systems are very sim-
ilar to the upper bounds. The gold standard upper
bound is not as high as expected because the Penn
trees used to produce the gold bracketed input are
not always compatible with the XLE-style trees. As
a simple example, the tree in Figure 1 differs from
the parse tree for the same sentence in the Penn
Treebank (Figure 3). The most obvious difference
is the labels on the nodes. However, even in this
small example, there are structural differences, e.g.
the position of the period. In general, the larger the
tree, the greater the difference in both labeling and
structure between the Penn trees and the XLE-style
trees. Therefore, the next step was to retrain a parser
to produce trees with structures the same as XLE-
style trees and with XLE English grammar labels on
the nodes. For this experiment we use the Bikel ()
parser, as it is more suited to being retrained on a
new treebank annotation scheme.
4 Retraining the Bikel parser
We retrained the Bikel parser so that it produces
trees like those outputted by the XLE parsing sys-
tem (e.g. Figure 1). To do this, we first created a
training corpus, and then modified the parser to deal
with this new data.
Since there is no manually-created treebank of
68
SNP
NN
Growth
VP
VBZ
is
ADJP-PRD
JJR
slower
?
?
Figure 3: Penn Treebank tree for ?Growth is slower.?
XLE-style trees, we created one automatically from
sections 02-21 of the Penn-II Treebank. We took the
raw strings from those sections and marked up NP
and SBAR constituents using the brackets from the
gold standard Penn treebank. The NP constituents
are labeled, and the SBAR unlabeled (i.e. the SBAR
constituents are forced to exist in the XLE parse, but
the label on them is not constrained to be SBAR).
We also tagged verbs, adjectives and nouns, based
on the gold standard POS tags.
We parsed the 39,832 marked-up sentences in the
standard training corpus and used the XLE disam-
biguation module to choose the most probable c-
and f-structure pair for each sentence. Ideally we
would have had an expert choose these. We au-
tomatically extracted the c-structure trees produced
by the XLE and performed some automatic post-
processing.3 This resulted in an automatically cre-
ated training corpus of 27,873 XLE-style trees. The
11,959 missing trees were mainly due to the XLE
parses not being compatible with the bracketed in-
put, but sometimes due to time and memory con-
straints.
Using the automatically-created training corpus
of XLE-style trees, we retrained the Bikel parser on
this data. This required adding a new language mod-
ule (?XLE-English?) to the Bikel parser, and regen-
erating head-finding rules for the XLE-style trees.
5 Experiments
Once we had a retrained version of the Bikel parser
that parses novel text into XLE-style trees, we car-
ried out a number of experiments on our develop-
ment set in order to establish the optimum settings
3The postprocessing included removing morphological in-
formation and the brackets from the original markup.
All Sentences
XLE Bikel-XLE
Non-fragment grammar
Labeled brackets
Total Parsing Time 964 336
Total XLE Parses (/140) 119 77
F-Score of Subset 81.57 86.11
Overall F-Score 72.01 52.84
Non-fragment grammar
Unlabeled brackets
Total Parsing Time 964 380
Total XLE Parses (/140) 119 89
F-Score of Subset 81.57 85.62
Overall F-Score 72.01 59.34
Fragment grammar
Labeled brackets
Total Parsing Time 1143 390
Total XLE Parses (/140) 135 140
F-Score of Subset 78.72 71.86
Overall F-Score 76.13 71.86
Fragment grammar
Unlabeled brackets
Total Parsing Time 1143 423
Total XLE Parses (/140) 135 140
F-Score of Subset 78.72 74.51
Overall F-Score 76.13 *74.51
Table 3: Bikel-XLE Initial Experiments
for the evaluation against the PARC 700 test set.
5.1 Pre-bracketing
We automatically pre-processed the raw strings from
the 140-sentence development set. This made sys-
tematic changes to the tokens so that the retrained
Bikel parser can parse them. The changes included
removing quotes, converting a and an to a, con-
verting n?t to not, etc. We parsed the pre-processed
strings with the new Bikel parser.
We carried out four initial experiments, experi-
menting with both labeled and unlabeled brackets
and XLE fragment and non-fragment grammars. Ta-
ble 3 gives the results for these experiments. We
compare the parsers in terms of time, total number
of parses (out of 140), the f-score of the subset of
sentences successfully parsed and the overall f-score
if the system achieves a score of 0 for all sentences
it does not parse. The time taken for the Bikel-XLE
system includes the time taken for the Bikel parser
to parse the sentences, as well as the time taken for
XLE to process the bracketed input.
Table 3 shows that using the non-fragment gram-
mar, the Bikel-XLE system performs better on the
69
subset of sentences parsed than XLE system alone,
though the results are not statistically significantly
better overall, since the coverage is much lower. The
number of bracketed sentences that can be parsed
by XLE increases if the brackets are unlabeled.
The table also shows that the XLE system performs
much better than Bikel-XLE when using the frag-
ment grammars. Although the Bikel-XLE system is
quite a bit faster, there is a drop in f-score; however
this is not statistically significant when the brackets
are unlabeled.
5.2 Pre-tagging
We performed some error analysis on the output of
the Bikel-XLE system and noticed that a consider-
able number of errors were due to mis-tagging. So,
we pre-tagged the input to the Bikel parser using the
MXPOST tagger (Ratnaparkhi, 1996). The results
for the non-fragment grammars are presented in Ta-
ble 4. Pre-tagging with MXPOST, however, does
not result in a statistically significantly higher re-
sult than parsing untagged input, although more sen-
tences can be parsed by both systems. Pre-tagging
also adds an extra time overhead cost.
No pretags MXPOST tags
XLE Bikel-XLE Bikel-XLE
Unlabeled
Total Parsing Time 964 380 493
# XLE Parses (/140) 119 89 92
F-Score of Subset 81.57 85.62 84.98
Overall F-Score 72.01 59.34 *61.11
Labeled
Total Parsing Time 964 336 407
# XLE Parses (/140) 119 77 80
F-Score of Subset 81.57 86.11 85.87
Overall F-Score 72.01 52.84 *54.91
Table 4: MXPOST pre-tagged, Non-fragment gram-
mar
5.3 Pruning
The Bikel parser can be customized to allow differ-
ent levels of pruning. The above experiments were
carried out using the default level. We carried out
experiments with three levels of pruning.4 The re-
4The default level of pruning starts at 3.5, has a maximum of
4 and relaxes constraints when parsing fails. Level 1 pruning is
the same as the default except the constraints are never relaxed.
Level 2 pruning has a start value of 3.5 and a maximum value
of 3.5. Level 3 pruning has a start and maximum value of 3.
sults are given in Table 5 for the experiment with
labeled brackets and the non-fragment XLE gram-
mar. More pruning generally results in fewer and
lower-quality parses. The biggest gain is with prun-
ing level 1, where the number and quality of brack-
eted sentences that can be parsed with XLE remains
the same as with the default level. This is because
Bikel with pruning level 1 does not relax the con-
straints when parsing fails and does not waste time
parsing sentences that cannot be parsed in bracketed
form by XLE.
Default L1 L2 L3
Total Parsing Time 336 137 137 106
# XLE Parses (/140) 77 77 76 75
F-Score of Subset 86.11 86.11 86.04 85.87
Overall F-Score 52.84 *52.84 *52.43 *52.36
Table 5: Pruning with Non-fragment grammar, La-
beled brackets, Levels default-3
5.4 Hybrid systems
Although pre-parsing with Bikel results in faster
XLE parsing time and high-quality f-structures
(when examining only the quality of the sentences
that can be parsed by the Bikel-XLE system), the
coverage of this system remains poor, therefore the
overall f-score remains poor. One solution is to build
a hybrid two-pass system. During the first pass all
sentences are pre-parsed by Bikel and the bracketed
output is parsed by the XLE non-fragment gram-
mar. In the second pass, the sentences that were
not parsed during the first pass are parsed with the
XLE fragment grammar. We carried out a number
of experiments with hybrid systems and the results
are given in Table 6.
The results show that again labeled brackets re-
sult in a statistically significant increase in f-score,
although the time taken is almost the same as the
XLE fragment grammar alone. Coverage increases
by 1 sentence. Using unlabeled brackets results in
3 additional sentences receiving parses, and parsing
time is improved by ?12%; however the increase in
f-score is not statistically significant.
Table 7 gives the results for hybrid systems with
pruning using labeled brackets. The more pruning
that the Bikel parser does, the faster the system,
but the quality of the f-structures begins to deteri-
70
XLE Bikel-XLE hybrid Bikel-XLE hybrid
(frag) (labeled) (unlabeled)
Total Parsing Time 1143 1121 1001
Total XLE Parses (/140) 135 136 138
F-Score of Subset 78.72 79.85 79.51
Overall F-Score 76.13 77.61 *78.28
Table 6: Hybrid systems compared to the XLE fragment grammar alone
XLE Bikel-XLE hybrid Bikel-XLE hybrid Bikel-XLE hybrid
(frag) (level 1) (level 2) (level 3)
Total Parsing Time 1143 918 920 885
Total XLE Parses (/140) 135 136 136 136
F-Score of Subset 78.72 79.85 79.79 79.76
Overall F-Score 76.13 77.61 77.55 77.53
Table 7: Hybrid systems with pruning compared to the XLE fragment grammar alone
orate. The best system is the Bikel-XLE hybrid sys-
tem with labeled brackets and pruning level 1. This
system achieves a statistically significant increase in
f-score over the XLE fragment grammar alone, de-
creases the time taken to parse by almost 20% and
increases coverage by 1 sentence. Therefore, we
chose this system to perform our final evaluation
against the PARC 700 Dependency Bank.
6 Evaluation against the PARC 700
We evaluated the system that performs best on the
development set against the 560-sentence test set of
the PARC 700 Dependency Bank. The results are
given in Table 8. The hybrid system achieves an
18% decrease in parsing time, a slight improvement
in coverage of 0.9%, and a 1.12% improvement in
overall f-structure quality.
XLE Bikel-XLE hybrid
(frag) (labeled, prune 1)
Total Parsing Time 4967 4077
Total XLE Parses (/560) 537 542
F-Score of Subset 80.13 80.63
Overall F-Score 77.04 78.16
Table 8: PARC 700 evaluation of the Hybrid system
compared to the XLE fragment grammar alone
7 Conclusions
We successfully used a state-of-the-art probabilistic
parser in combination with a hand-crafted system to
improve parsing time while maintaining the quality
of the output produced. Our hybrid system consists
of two phases. During phase one, pre-processed, to-
kenized text is parsed with a retrained Bikel parser.
We use the labeled brackets in the output to constrain
the c-structures generated by the XLE parsing sys-
tem. In the second phase, we use the XLE fragment
grammar to parse any remaining sentences that have
not received a parse in the first phase.
Given the slight increase in overall f-score per-
formance, the speed up in parsing time (?18%) can
justify more complicated processing architecture for
some applications.5 The main disadvantage of the
current system is that the input to the Bikel parser
needs to be tokenized, whereas XLE processes raw
text. One solution to this is to use a state-of-the-art
probabilistic parser that accepts untokenized input
(such as Charniak and Johnson, 2005) and retrain it
as described in Section 4.
Kaplan et al (2004) compared time and accuracy
of a version of the Collins parser tuned to maximize
speed and accuracy to an earlier version of the XLE
parser. Although the XLE parser was more accu-
rate, the parsing time was a factor of 1.49 slower
(time converting Collins trees to dependencies was
not counted in the parse time; time to produce f-
structures from c-structures was counted in the XLE
parse time). The hybrid system here narrows the
speed gap while maintaining greater accuracy.
The original hope behind using the brackets to
constrain the XLE c-structure generation was that
5For example, in massive data applications, if the parsing
task takes 30 days, reducing this by 18% saves more than 5
days.
71
the brackets would force the XLE to choose only
one tree. However, the brackets were sometimes
ambiguous, and sometimes more than one valid tree
was found. In the final evaluation against the PARC
700 test set, the average number of optimal solutions
was 4.05; so the log-linear disambiguation mod-
ule still had to chose the most probable f-structure.
However, this is considerably less to choose from
than the average of 341 optimal solutions produced
by the XLE fragment grammar for the same sen-
tences when unbracketed.
Based on the results of this experiment we have
integrated a statistical component into the XLE
parser itself. With this architecture the packed c-
structure trees are pruned before unification with-
out needing to preprocess the input text. The XLE
c-structure pruning results in a ?30% reduction in
parse time on the Wikipedia with little loss in preci-
sion. We hope to report on this in the near future.
Acknowledgments
The research in this paper was partly funded by Sci-
ence Foundation Ireland grant 04/BR/CS0370.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to alsmost parsing. Com-
putational Linguistics, 25(2):237?265.
Dan Bikel. Design of a Multi-lingual, Parallel-processing
Statistical Parsing Engine. In Proceedings of HLT,
YEAR = 2002, pages = 24?27, address = San Diego,
CA,.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The Par-
allel Grammar Project. In Proceedings of Workshop
on Grammar Engineering and Evaluation, pages 1?7,
Taiwan.
Aoife Cahill, Martin Forst, Michael Burke, Mairead Mc-
Carthy, Ruth O?Donovan, Christian Rohrer, Josef van
Genabith, and Andy Way. 2005. Treebank-based
acquisition of multilingual unification grammar re-
sources. Journal of Research on Language and Com-
putation, pages 247?279.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL, pages 173?180,
Ann Arbor, Michigan.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139,
Seattle, WA.
Stephen Clark and James R. Curran. 2004. The Impor-
tance of Supertagging for Wide-Coverage CCG Pars-
ing . In Proceedings of COLING, pages 282?288,
Geneva, Switzerland, Aug 23?Aug 27. COLING.
Richard Crouch, Ron Kaplan, Tracy Holloway King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad coverage parser. In Proceedings of
the LREC Workshop: Beyond PARSEVAL, pages 67?
74, Las Palmas, Canary Islands, Spain.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar, a Formal System for Grammatical Repre-
sentation. In Joan Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
Ron Kaplan, Stefan Riezler, Tracy Holloway King,
John T. Maxwell, Alexander Vasserman, and Richard
Crouch. 2004. Speed and Accuracy in Shallow and
Deep Stochastic Parsing. In Proceedings of HLT-
NAACL, pages 97?104, Boston, MA.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ron Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of LINC, pages
1?8, Budapest, Hungary.
Alexandra Kinyon. 2000. Hypertags. In Proceedings of
COLING, pages 446?452, Saarbru?cken.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG Parsing with Supertagging and
CFG-filtering. In Proceedings of IJCAI, pages 1671?
1676, India.
John T. Maxwell and Ronald M. Kaplan. 1993. The
interface between phrasal and functional constraints.
Computational Linguistics, 19(4):571?590.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely Lexicalized Models for Accurate and Fast
HPSG Parsing. In Proceedings of EMNLP, pages
155?163, Australia.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley, New
York.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. In Proceedings of EMNLP, pages
133?142, Philadelphia, PA.
Stefan Riezler, Tracy King, Ronald Kaplan, Richard
Crouch, John T. Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and Discriminative Estimation
Techniques. In Proceedings of ACL, pages 271?278,
Philadelphia, PA.
72
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 33?40
Manchester, August 2008
Speeding up LFG Parsing Using C-Structure Pruning
Aoife Cahill? John T. Maxwell III? Paul Meurer? Christian Rohrer? Victoria Rose?n?
?IMS, University of Stuttgart, Germany, {cahillae, rohrer}@ims.uni-stuttgart.de
?Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304, maxwell@parc.com
?Unifob Aksis, Bergen, Norway, paul.meurer@aksis.uib.no
?Unifob Aksis and University of Bergen, Norway, victoria@uib.no
Abstract
In this paper we present a method for
greatly reducing parse times in LFG pars-
ing, while at the same time maintaining
parse accuracy. We evaluate the method-
ology on data from English, German and
Norwegian and show that the same pat-
terns hold across languages. We achieve
a speedup of 67% on the English data and
49% on the German data. On a small
amount of data for Norwegian, we achieve
a speedup of 40%, although with more
training data we expect this figure to in-
crease.
1 Introduction
Efficient parsing of large amounts of natural lan-
guage is extremely important for any real-world
application. The XLE Parsing System is a large-
scale, hand-crafted, deep, unification-based sys-
tem that processes raw text and produces both
constituent structures (phrase structure trees) and
feature structures (dependency attribute-value ma-
trices). A typical breakdown of parsing time
of XLE components with the English grammar
is Morphology (1.6%), Chart (5.8%) and Unifier
(92.6%). It is clear that the major bottleneck in
processing is in unification. Cahill et al (2007)
carried out a preliminary experiment to test the
theory that if fewer c-structures were passed to
the unifier, overall parsing times would improve,
while the accuracy of parsing would remain sta-
ble. Their experiments used state-of-the-art prob-
abilistic treebank-based parsers to automatically
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mark certain constituents on the input sentences,
limiting the number of c-structures the XLE pars-
ing system would build. They achieved an 18%
speedup in parse times, while maintaining the ac-
curacy of the output f-structures. The experiments
presented in Cahill et al (2007) used the XLE sys-
tem as a black box and did not make any changes to
it. However, the results were encouraging enough
for a c-structure pruning mechanism to be fully in-
tegrated into the XLE system.
The paper is structured as follows: we present
the pruning model that has been integrated into the
XLE system (Section 2), and how it can be ap-
plied successfully to more than one language. We
present experiments for English (Section 3), Ger-
man (Section 4) and Norwegian (Section 5) show-
ing that for both German and English, a significant
improvement in speed is achieved, while the qual-
ity of the f-structures remains stable. For Norwe-
gian a speedup is also achieved, but more training
data is required to sustain the accuracy of the f-
structures. In Section 7 we present an error anal-
ysis on the German data. We then relate the work
presented in this paper to similar efficient parsing
strategies (Section 8) before concluding in Section
9.
2 XLE and the C-Structure Pruning
Mechanism
The XLE system is designed to deal with large
amounts of data in a robust manner. There are
several mechanisms which facilitate this, including
fragmenting and skimming. Fragmenting is called
when the grammar is unable to provide a complete
parse for the input sentence, and a fragment anal-
ysis of largest possible chunks is built. Skimming
is called when too much time or memory has been
used by XLE. Any constituents that have not been
33
fully processed are ?skimmed?, which means that
the amount of work carried out in processing the
constituent is limited. This guarantees that XLE
will finish processing the sentence in polynomial
time.
XLE uses a chart-based mechanism for build-
ing parses, and has been complemented with a c-
structure pruning mechanism to speed up parsing
time. During pruning, subtrees at a particular cell
in the chart are pruned if their probabilities are not
higher than a certain threshold. The chart pruner
uses a simple stochastic CFG model. The proba-
bility of a tree is the product of the probabilities
of each of the rules used to form the tree, includ-
ing the rules that lead to lexical items (such as N
? dog). The probability of a rule is basically the
number of times that that particular form of the
rule occurs in the training data divided by the num-
ber of times the rule?s category occurs in the train-
ing data, plus a smoothing term. This is similar
to the pruning described in Charniak and Johnson
(2005) where edges in a coarse-grained parse for-
est are pruned to allow full evaluation with fine-
grained categories.
The pruner prunes at the level of individual con-
stituents in the chart. It calculates the probabil-
ities of each of the subtrees of a constituent and
compares them. The probability of each subtree
is compared with the best subtree probability for
that constituent. If a subtree?s probability is lower
than the best probability by a given factor, then the
subtree is pruned. In practice, the threshold is the
natural logarithm of the factor used. So a value of
5 means that a subtree will be pruned if its prob-
ability is about a factor of 150 less than the best
probability.
If two different subtrees have different num-
bers of morphemes under them, then the proba-
bility model is biased towards the subtree that has
fewer morphemes (since there are fewer probabil-
ities multiplied together). XLE counteracts this by
normalizing the probabilities based on the differ-
ence in length.
To illustrate how this works, we give the follow-
ing example. The string Fruit flies like bananas has
two different analyses. Figures 1 and 2 give their
analyses along with hypothetical probabilities for
each rule.
These two analyses come together at the S con-
stituent that spans the whole sentence. The proba-
bility of the first analysis is 8.4375E-14. The prob-
S
NP
N
Fruit
N
flies
VP
V
like
NP
N
bananas
S ? NP VP 0.5000
NP ? N N 0.1500
N ? Fruit 0.0010
N ? flies 0.0015
VP ? V NP 0.2000
V ? like 0.0050
NP ? N 0.5000
N ? bananas 0.0015
8.4375E-14
Figure 1: Analysis (1) for the string Fruit flies like
bananas with hypothetical probabilities
S
NP
N
Fruit
VP
V
flies
PP
P
like
NP
N
bananas
S ? NP VP 0.5000
NP ? N 0.5000
N ? Fruit 0.0010
V ? flies 0.0025
VP ? V PP 0.1000
P ? like 0.0500
PP ? P NP 0.9000
NP ? bananas 0.0015
4.21875E-12
Figure 2: Analysis (2) for the string Fruit flies like
bananas with hypothetical probabilities
ability of the second analysis is 4.21875E-12. This
means that the probability of the second analysis
is 50 times higher than the probability of the first
analysis. If the threshold is less than the natural
logarithm of 50 (about 3.9), then the subtree of the
first analysis will be pruned from the S constituent.
3 Experiments on English
We carried out a number of parsing experiments to
test the effect of c-structure pruning, both in terms
of time and accuracy. We trained the c-structure
pruning algorithm on the standard sections of Penn
Treebank Wall Street Journal Text (Marcus et al,
1994). The training data consists of the original
WSJ strings, marked up with some of the Penn
34
Treebank constituent information. We marked up
NPs and SBARs as well as adjective and verbal
POS categories. This is meant to guide the train-
ing process, so that it does learn from parses that
are not compatible with the original treebank anal-
ysis. We evaluated against the PARC 700 Depen-
dency Bank (King et al, 2003), splitting it into 140
sentences as development data and the remaining
unseen 560 for final testing (as in Kaplan et al
(2004)). We experimented with different values
of the pruning cutoff on the development set; the
results are given in Table 1.
The results show that the lower the cutoff value,
the quicker the sentences can be parsed. Using
a cutoff of 4, the development sentences can be
parsed in 100 CPU seconds, while with a cutoff
of 10, the same experiment takes 182 seconds.
With no cutoff, the experiment takes 288 CPU sec-
onds. However, this increase in speed comes at a
price. The number of fragment parses increases,
i.e. there are more sentences that fail to be analyzed
with a complete spanning parse. With no pruning,
the number of fragment parses is 23, while with
the most aggressive pruning factor of 4, there are
39 fragment parses. There are also many more
skimmed sentences with no c-structure pruning,
which impacts negatively on the results. The ora-
cle f-score with no pruning is 83.07, but with prun-
ing (at all thresholds) the oracle f-score is higher.
This is due to less skimming when pruning is acti-
vated, since the more subtrees that are pruned, the
less likely the XLE system is to run over the time
or memory limits needed to trigger skimming.
Having established that a cutoff of 5 performs
best on the development data, we carried out the
final evaluation on the 560-sentence test set using
this cutoff. The results are given in Table 2. There
is a 67% speedup in parsing the 560 sentences, and
the most probable f-score increases significantly
from 79.93 to 82.83. The oracle f-score also in-
creases, while there is a decrease in the random f-
score. This shows that we are throwing away good
solutions during pruning, but that overall the re-
sults improve. Part of this again is due to the fact
that with no pruning, skimming is triggered much
more often. With a pruning factor of 5, there are
no skimmed sentences. There is also one sentence
that timed out with no pruning, which also lowers
the most probable and oracle f-scores.
Pruning Level None 5
Total Time 1204 392
Most Probable F-Score 79.93 82.83
Oracle F-Score 84.75 87.79
Random F-Score 75.47 74.31
# Fragment Parses 96 91
# Time Outs 1 0
# Skimmed Sents 33 0
Table 2: Results of c-structure pruning experi-
ments on English test data
4 Experiments on German
We carried out a similar set of experiments on
German data to test whether the methodology de-
scribed above ported to a language other than En-
glish. In the case of German, the typical time of
XLE components is: Morphology (22.5%), Chart
(3.5%) and Unifier (74%). As training data we
used the TIGER corpus (Brants et al, 2002). Set-
ting aside 2000 sentences for development and
testing, we used the remaining 48,474 sentences as
training data. In order to create the partially brack-
eted input required for training, we converted the
original TIGER graphs into Penn-style trees with
empty nodes and retained bracketed constituents of
the type NP, S, PN and AP. The training data was
parsed by the German ParGram LFG (Rohrer and
Forst, 2006). This resulted in 25,677 full parses,
21,279 fragmented parses and 1,518 parse fail-
ures.1 There are 52,959 features in the final prun-
ing model.
To establish the optimal pruning settings for
German, we split the 2,000 saved sentences into
371 development sentences and 1495 test sen-
tences for final evaluation. We evaluated against
the TiGer Dependency Bank (Forst et al, 2004)
(TiGerDB), a dependency-based gold standard for
German parsers that encodes grammatical rela-
tions similar to, though more fine-grained than,
the ones in the TIGER Treebank as well as mor-
phosyntactic features. We experimented with the
same pruning levels as in the English experiments.
The results are given in Table 3.
The results on the development set show a sim-
ilar trend to the English results. A cutoff of 4 re-
sults in the fastest system, however at the expense
1The reason there are more fragment parses than, for ex-
ample, the results reported in Rohrer and Forst (2006) is that
the bracketed input constrains the parser to only return parses
compatible with the bracketed input. If there is no solution
compatible with the brackets, then a fragment parse is re-
turned.
35
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 83.07 84.50 85.47 85.75 85.57 85.57 85.02 84.10
Time (CPU seconds) 288 100 109 123 132 151 156 182
# Time Outs 0 0 0 0 0 0 0
# Fragments 23 39 36 31 29 27 27 24
# Skimmed Sents 8 0 0 1 1 1 1 3
Table 1: Results of c-structure pruning experiments on English development data
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 83.69 83.45 84.02 82.86 82.82 82.95 83.03 82.81
Time (CPU seconds) 1313 331 465 871 962 1151 1168 1163
# Time Outs 6 0 0 5 5 5 5 6
# Fragments 65 104 93 81 74 73 73 68
Table 3: Results of c-structure pruning experiments on German development data
Pruning Level None 5
Total Time 3300 1655
Most Probable F-Score 82.63 82.73
Oracle F-Score 84.96 84.79
Random F-Score 73.58 73.72
# Fragment Parses 324 381
# Time Outs 2 2
Table 4: Results of c-structure pruning experi-
ments on German test data
of accuracy. A cutoff of 5 seems to provide the
best tradeoff between time and accuracy. Again,
most of the gain in oracle f-score is due to fewer
timeouts, rather than improved f-structures. In the
German development set, a cutoff of 5 leads to a
speedup of over 64% and a small increase in or-
acle f-score of 0.33 points. Therefore, for the fi-
nal evaluation on the unseen test-set, we choose a
cutoff of 5. The results are given in Table 4. We
achieve a speedup of 49% and a non-significant in-
crease in most probable f-score of 0.094. The time
spent by the system on morphology is much higher
for German than for English. If we only take the
unification stage of the process into account, the
German experiments show a speedup of 65.5%.
5 Experiments on Norwegian
As there is no treebank currently available for Nor-
wegian, we were unable to train the c-structure
pruning mechanism for Norwegian in the same
way as was done for English and German. There
is, however, some LFG-parsed data that has been
completely disambiguated using the techniques
described in Rose?n et al (2006). In total there
are 937 sentences from various text genres includ-
ing Norwegian hiking guides, Sophie?s World and
the Norwegian Wikipedia. We also use this dis-
ambiguated data as a gold standard for evaluation.
The typical time of XLE components with the Nor-
wegian grammar is: Morphology (1.6%), Chart
(11.2%) and Unifier (87.2%).
From the disambiguated text, we can automati-
cally extract partially bracketed sentences as input
to the c-structure pruning training method. We can
also extract sentences for training that are partially
disambiguated, but these cannot be used as part of
the test data. To do this, we extract the bracketed
string for each solution. If all the solutions pro-
duce the same bracketed string, then this is added
to the training data. This results in an average of
4556 features. As the data set is small, we do not
split it into development, training and test sections
as was done for English and German. Instead we
carry out a 10-fold cross validation over the entire
set. The results for each pruning level are given in
Table 5.
The results in Table 5 show that the pattern that
held for English and German does not quite hold
for Norwegian. While, as expected, the time taken
to parse the test set is greatly reduced when using
c-structure pruning, there is also a negative impact
on the quality of the f-structures. One reason for
this is that there are now sentences that could pre-
viously be parsed, and that now no longer can be
parsed, even with a fragment grammar.2 With c-
structure pruning, the number of fragment parses
increases for all thresholds, apart from 10. It is
also difficult to compare the Norwegian experi-
ment to the English and German, since the gold
standard is constrained to only consist of sentences
that can be parsed by the grammar. Theoretically
the oracle f-score for the experiment with no prun-
2With an extended fragment grammar, this would not hap-
pen.
36
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 98.76 94.45 95.60 96.40 96.90 97.52 98.00 98.33
Time (CPU seconds) 218.8 106.2 107.4 109.3 112 116.2 124 130.7
# Time Outs 0 0 0 0 0 0 0 0
# Parse Failures 0.2 5.7 3.9 2 3.2 4.2 4.6 4.2
# Fragments 1.3 7.7 6.5 4.7 2.8 1.8 1.5 1.2
Table 5: Results of c-structure pruning 10-fold cross validation experiments on Norwegian data
55
60
65
70
75
80
85
90
95
None 4 5 6 7 8 9 10
Figure 3: The lower-bound results for each of the
10 cross validation runs across the thresholds
ing should be 100. The slight drop is due to a
slightly different morphological analyzer used in
the final experiments that treats compound nouns
differently. A threshold of 10 gives the best results,
with a speedup of 40% and a drop in f-score of 0.43
points. It is difficult to choose the ?best? thresh-
old, as the amount of training data is probably not
enough to get an accurate picture of the data. For
example, Figure 3 shows the lower-bound results
for each of the 10 runs. It is difficult to see a clear
pattern for all the runs, indicating that the amount
of training data is probably not enough for a reli-
able experiment.
6 Size of Training Data Corpus
The size of the Norwegian training corpus is con-
siderably smaller than the training corpora for En-
glish or German, so the question remains how
much training data we need in order for the c-
structure pruning to deliver reliable results. In or-
der to establish a rough estimate for the size of
training corpus required, we carried out an experi-
ment on the German TIGER training corpus.
We randomly divided the TIGER training cor-
pus into sets of 500 sentences. We plot the learn-
ing curve of the c-structure pruning mechanism in
Figure 4, examining the effect of increasing the
size of the training corpus on the oracle f-score on
the development set of 371 sentences. The curve
shows that, for the German data, the highest oracle
f-score of 84.98 was achieved with a training cor-
pus of 32,000 sentences. Although the curve fluc-
tuates, the general trend is that the more training
data, the better the oracle f-score.3
7 Error Analysis
Given that we are removing some subtrees during
parsing, it can sometimes happen that the desired
analysis gets pruned. We will take German as an
example, and look at some of these cases.
7.1 Separable particles vs pronominal
adverbs
The word dagegen (?against it?) can be a separable
prefix (VPART) or a pronominal adverb (PADV).
The verb protestieren (?to protest?) does not take
dagegen as separable prefix. The verb stimmen
(?to agree?) however does. If we parse the sen-
tence in (1) with the verb protestieren and activate
pruning, we do not get a complete parse. If we
parse the same sentence with stimmen as in (2) we
do get a complete parse. If we replace dagegen
by dafu?r, which in the current version of the Ger-
man LFG can only be a pronominal adverb, the
sentence in (3) gets a parse. We also notice that
if we parse a sentence, as in (4), where dagegen
occurs in a position where our grammar does not
allow separable prefixes to occur, we get a com-
plete parse for the sentence. These examples show
that the pruning mechanism has learned to prune
the separable prefix reading of words that can be
both separable prefixes and pronominal adverbs.
(1) Sie
they
protestieren
protest
dagegen.
against-it
?They protest against it.?
(2) Sie
they
stimmen
vote
dagegen.
against-it
?They vote against it.?
3Unexpectedly, the curve begins to decline after 32,000
sentences. However, the differences in f-score are not statis-
tically significant (using the approximate randomization test).
Running the same experiment with a different random seed
results in a similarly shaped graph, but any decline in f-score
when training on more data was not statistically significant at
the 99% level.
37
32000, 84.97698
84
84.1
84.2
84.3
84.4
84.5
84.6
84.7
84.8
84.9
85
50
0
20
00
35
00
50
00
65
00
80
00
95
00
11
00
0
12
50
0
14
00
0
15
50
0
17
00
0
18
50
0
20
00
0
21
50
0
23
00
0
24
50
0
26
00
0
27
50
0
29
00
0
30
50
0
32
00
0
33
50
0
35
00
0
36
50
0
38
00
0
39
50
0
41
00
0
42
50
0
44
00
0
45
50
0
47
00
0
48
50
0
Number of Training Sentences
F-
Sc
o
re
Figure 4: The effect of increasing the size of the training data on the oracle f-score
(3) Er
he
protestiert
protests
dafu?r.
for-it
?He protests in favour of it.?
(4) Dagegen
against-it
protestiert
protests
er.
he
?Against it, he protests.?
7.2 Derived nominal vs non-derived nominal
The word Morden can be the dative plural of the
noun Mord (?murder?) or the nominalized form of
the verb morden (?to murder?). With c-structure
pruning activated (at level 5), the nominalized
reading, as in (6), gets pruned, whereas the dative
plural reading is not (5). At pruning level 6, both
readings are assigned a full parse. We see simi-
lar pruning of nominalized readings as in (7). If
we look in more detail at the raw counts for re-
lated subtrees gathered from the training data, we
see that the common noun reading for Morden oc-
curs 156 times, while the nominalized reading only
occurs three times. With more training data, the c-
structure pruning mechanism could possibly learn
when to prune correctly in such cases.
(5) Er
he
redet
speaks
von
of
Morden.
murders
?He speaks of murders.?
(6) Das
the
Morden
murdering
will
wants
nicht
not
enden.
end
?The murdering does not want to end.?
(7) Das
the
Arbeiten
working
endet.
ends
?The operation ends.?
7.3 Personal pronouns which also function as
determiners
There are a number of words in German that can
function both as personal pronouns and determin-
ers. If we take, for example, the word ihr, which
can mean ?her?, ?their?, ?to-her?, ?you-pl? etc.,
the reading as a determiner gets pruned as well as
some occurrences as a pronoun. In example (8),
we get a complete parse for the sentence with the
dative pronoun reading of ihr. However, in ex-
ample (9), the determiner reading is pruned and
we fail to get a complete parse. In example (10),
we also fail to get a complete parse, but in exam-
ple (11), we do get a complete parse. There is a
parameter we can set that sets a confidence value
in certain tags. So, for example, we set the con-
fidence value of INFL-F BASE[det] (the tag given
to the determiner reading of personal pronouns) to
be 0.5, which says that we are 50% confident that
the tag INFL-F BASE[det] is correct. This results in
38
examples 8, 9 and 11 receiving a complete parse,
with the pruning threshold set to 5.
(8) Er
he
gibt
gives
es
it
ihr.
her
?He gives it to her.?
(9) Ihr
her/their
Auto
car
fa?hrt.
drives
?Her/Their car drives.
(10) Ihr
you(pl)
kommt.
come
?You come.?
(11) Er
he
vertraut
trusts
ihr.
her
?He trusts her.?
7.4 Coordination of Proper Nouns
Training the German c-structure pruning mecha-
nism on the TIGER treebank resulted in a pecu-
liar phenomenon when parsing coordinated proper
nouns. If we parse four coordinated proper nouns
with c-structure pruning activated as in (12), we
get a complete parse. However, as soon as we add
a fifth proper noun as in (13), we get a fragment
parse. This is only the case with proper nouns,
since the sentence in (14) which coordinates com-
mon nouns gets a complete parse. Interestingly, if
we coordinate n proper nouns plus one common
noun, we also get a complete parse. The reason for
this is that proper noun coordination is less com-
mon than common noun coordination in our train-
ing set.
(12) Hans, Fritz, Emil und Maria singen.
?Hans, Fritz, Emil and Maria sing.?
(13) Hans, Fritz, Emil, Walter und Maria sin-
gen.
?Hans, Fritz, Emil, Walter and Maria sing.?
(14) Hunde, Katzen, Esel, Pferde und Affen
kommen.
?Dogs, cats, donkeys, horses and apes
come.?
(15) Hans, Fritz, Emil, Walter, Maria und
Kinder singen.
?Hans, Fritz, Emil, Walter, Maria and chil-
dren sing.?
We ran a further experiment to test what effect
adding targeted training data had on c-structure
pruning. We automatically extracted a specialized
corpus of 31,845 sentences from the Huge Ger-
man Corpus. This corpus is a collection of 200
million words of newspaper and other text. The
sentences we extracted all contained examples of
proper noun coordination and had been automati-
cally chunked. Training on this sub-corpus as well
as the original TIGER training data did have the
desired effect of now parsing example (13) with
c-structure pruning activated.
8 Related Work
Ninomiya et al (2005) investigate beam threshold-
ing based on the local width to improve the speed
of a probabilistic HPSG parser. In each cell of a
CYK chart, the method keeps only a portion of the
edges which have higher figure of merits compared
to the other edges in the same cell. In particular,
each cell keeps the edges whose figure of merit is
greater than ?
max
- ?, where ?
max
is the high-
est figure of merit among the edges in the chart.
The term ?beam thresholding? is a little confusing,
since a beam search is not necessary ? instead, the
CYK chart is pruned directly. For this reason, we
prefer the term ?chart pruning? instead.
Clark and Curran (2007) describe the use of
a supertagger with a CCG parser. A supertag-
ger is like a tagger but with subcategorization in-
formation included. Chart pruners and supertag-
gers are conceptually complementary, since chart
pruners prune edges with the same span and the
same category, whereas supertaggers prune (lexi-
cal) edges with the same span and different cate-
gories. Ninomiya et al (2005) showed that com-
bining a chunk parser with beam thresholding pro-
duced better results than either technique alone. So
adding a supertagger should improve the results
described in this paper.
Zhang et al (2007) describe a technique to
selectively unpack an HPSG parse forest to ap-
ply maximum entropy features and get the n-best
parses. XLE already does something similar when
it applies maximum entropy features to get the
n-best feature structures after having obtained a
packed representation of all of the valid feature
structures. The current paper shows that pruning
the c-structure chart before doing (packed) unifica-
tion speeds up the process of getting a packed rep-
resentation of all the valid feature structures (ex-
cept the ones that may have been pruned).
39
9 Conclusions
In this paper we have presented a c-structure prun-
ing mechanism which has been integrated into the
XLE LFG parsing system. By pruning the number
of c-structures built in the chart, the next stage of
processing, the unifier, has considerably less work
to do. This results in a speedup of 67% for En-
glish, 49% for German and 40% for Norwegian.
The amount of training data for Norwegian was
much less than that for English or German, there-
fore further work is required to fully investigate
the effect of c-structure pruning. However, the re-
sults, even from the small training data, were en-
couraging and show the same general patterns as
English and German. We showed that for the Ger-
man training data, 32,000 sentences was the opti-
mal number in order to achieve the highest oracle
f-score. There remains some work to be done in
tuning the parameters for the c-structure pruning,
as our error analysis shows. Of course, with sta-
tistical methods one can never be guaranteed that
the correct parse will be produced; however we can
adjust the parameters to account for known prob-
lems. We have shown that the c-structure pruning
mechanism described is an efficient way of reduc-
ing parse times, while maintaining the accuracy of
the overall system.
Acknowledgements
The work presented in this paper was supported
by the COINS project as part of the linguistic
Collaborative Research Centre (SFB 732) at the
University of Stuttgart and by the Norwegian Re-
search Council through the LOGON and TREPIL
projects.
References
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol, Bulgaria.
Cahill, Aoife, Tracy Holloway King, and John T.
Maxwell III. 2007. Pruning the Search Space of
a Hand-Crafted Parsing System with a Probabilistic
Parser. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 65?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 173?180, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Clark, Stephen and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Forst, Martin, Nu?ria Bertomeu, Berthold Crysmann,
Frederik Fouvry, Silvia Hansen-Schirra, and Valia
Kordoni. 2004. Towards a dependency-based gold
standard for German parsers ? The TiGer Depen-
dency Bank. In Proceedings of the COLING Work-
shop on Linguistically Interpreted Corpora (LINC
?04), Geneva, Switzerland.
Kaplan, Ronald M., John T. Maxwell, Tracy H. King,
and Richard Crouch. 2004. Integrating Finite-state
Technology with Deep LFG Grammars. In Pro-
ceedings of the ESSLLI 2004 Workshop on Combin-
ing Shallow and Deep Processing for NLP, Nancy,
France.
King, Tracy Holloway, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of the
EACL Workshop on Linguistically Interpreted Cor-
pora (LINC ?03), Budapest, Hungary.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ninomiya, Takashi, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of Beam
Thresholding, Unification Filtering and Hybrid Pars-
ing in Probabilistic HPSG Parsing. In Proceed-
ings of the Ninth International Workshop on Pars-
ing Technology, pages 103?114, Vancouver, British
Columbia, October. Association for Computational
Linguistics.
Rohrer, Christian and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC-2006),
Genoa, Italy.
Rose?n, Victoria, Paul Meurer, and Koenraad de Smedt.
2006. Towards a Toolkit Linking Treebanking and
Grammar Development. In Hajic, Jan and Joakim
Nivre, editors, Proceedings of the Fifth Workshop
on Treebanks and Linguistic Theories, pages 55?66,
December.
Zhang, Yi, Stephan Oepen, and John Carroll. 2007.
Efficiency in Unification-Based N-Best Parsing. In
Proceedings of the Tenth International Conference
on Parsing Technologies, pages 48?59, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
40
