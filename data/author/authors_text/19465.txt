Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1269?1278, Dublin, Ireland, August 23-29 2014.
Chinese Irony Corpus Construction and Ironic Structure Analysis 
 
 
Yi-jie Tang and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University, Taipei, Taiwan  
tangyj@nlg.csie.ntu.edu.tw,hhchen@ntu.edu.tw
 
  
 
 Abstract 
Non-literal expression recognition is a challenging task in natural language processing. An ironic expression 
implies the opposite of the literal meaning, causing problems in opinion mining and sentiment analysis. In this 
paper, ironic messages are collected from microblogs to form an irony corpus based on the use of emoticons, 
linguistic forms, and sentiment polarity. Five linguistic patterns are mined by using the proposed bootstrapping 
approach. We also analyze the linguistic structure and elements used to convey irony. Based on our observations, 
ironic words/phrases and contextual information are the necessary elements in irony, while the contextual infor-
mation can be hidden in linguistic forms. A rhetorical element, which is optional in irony, can also be used to 
help strengthen the effects and understandability of an ironic expression. The ironic elements in each instance of 
our irony corpus are labelled based on this structure. This corpus can be used to study the usage of ironic expres-
sions and the identification of ironic elements, and thus improve the performance of irony recognition. 
1 Introduction 
Dealing with non-literal meaning is a challenging task in natural language processing. Linguistic con-
text and background knowledge are required to interpret non-literal utterances properly. An ironic ex-
pression, where the meaning is the opposite of what is literally expressed, is one of the indirect and 
non-literal linguistic forms that cannot be easily processed and detected. One cannot capture the real 
meanings of opinions and sentiments expressed in a document or conversation if irony is not taken 
into account. 
The challenges of irony processing involve the following issues: (1) No comprehensive irony cor-
pus is available. (2) Irony analysis is related to semantics, pragmatics and discourse studies, which are 
the most challenging in natural language processing. (3) Contextual information and background 
knowledge are necessary, but they are hard to obtain and process. (4) Non-linguistic or non-verbal fac-
tors, e.g., intonations, gestures and talking speed in speech, and spaces, punctuations and typography 
in writing, have to be considered. 
This paper focuses on irony corpus construction, ironic pattern mining, and ironic structure analysis. 
Messages were collected from a microblogging platform based on emoticons, and ironic messages and 
patterns were extracted to build an irony corpus. The structure of ironic expressions and the clarifica-
tion of the uses of ironic elements were also analyzed. Labels representing the ironic elements are 
added to each message in the irony corpus. To the best of our knowledge, this is the first Chinese irony 
corpus available for research. 
This paper is organized as follows. Section 2 surveys the related work. Section 3 proposes a meth-
odology to construct an irony corpus. Section 4 presents the patterns mined from the corpus. Section 5 
discusses the results of ironic expressions collected from a different type of corpus. Section 6 makes 
the error analysis. Section 7 analyzes linguistic structure of Chinese irony. Section 8 concludes the 
remarks. 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
1269
2 Related Work 
Sarcasm and irony have been studied by linguistics and cognitive scientists (Giora and Fein, 1999; 
Gibbs and Colston, 2007) for years, but there has been no concrete claim on the linguistic structure of 
irony. Some studies have started focusing on the processing of sarcasm and irony recently, but it is 
still not clear whether sarcasm and irony differ significantly or represent the same concept.  
The research of non-literal expression identification has drawn attention in recent years. Katz and 
Giesbrecht (2006) use meaning vectors for literal and non-literal expression classification. Li and 
Sporleder (2010) focus on distinguishing literal and non-literal usages of idioms. 
Filatova (2012) uses crowdsourcing to generate an irony and sarcasm corpus. Veale and Hao (2010) 
construct a corpus of ironic similes using the wildcarded query ?as * as a *? on a search engine. Da-
vidov et al. (2010) collect messages from Twitter and product reviews from Amazon.com using the 
Mechanical Turk service. The #sarcasm hashtag is used as ground truth, and a k-nearest neighbor 
strategy is used for classification. Gonz?lez-Ib??ez et al. (2011) also make use of hashtags in Twitter 
as labels to build a sarcasm corpus. In their study, both human classification and automatic classifica-
tion achieve low accuracy in sarcasm detection. Reyes et al. (2012) analyze humor and irony based on 
the user-generated tags, such as ?#humor? and ?#irony?, in twitter. Lukin and Walker (2013) use a 
bootstrapping method to improve the performance of the classifiers for identifying sarcastic and nasty 
utterances in online dialogues. 
The hashtag-based approaches are not always suitable for irony corpus construction for all the lan-
guages. As of March 9, 2014, only 113 messages are found to contain the hashtag #?? (#irony) in 
Weibo, the largest Chinese language microblogging platform. This paper differs from the previous 
work in that we employ negative emoticons and positive words as clues to capture the irony. The lin-
guistic patterns mined from the irony corpus can be used to detect if a sentence is ironic.  
3 Irony Corpus Generation from Microblogs 
This section introduces a bootstrapping methodology to construct an irony corpus and mine irony pat-
terns. While Lukin and Walker (2013) also used a bootstrapping method to improve sarcasm and nas-
tiness classifiers, this paper, in contrast, focuses on irony pattern mining and corpus construction. 
3.1 An Emotion-Tagged Corpus 
The traditional definition of verbal irony is adopted, where the speaker says something that seems to 
be the opposite of what they mean (Gibbs and Colston, 2007). Under this definition, texts annotated 
with polarity information that expresses the actual meaning should be collected, and the literal mean-
ings of words in the texts should be identified. If any disagreement exists between the actual meaning 
and literal meaning, then we say the text contains irony. 
Nowadays, emoticons are used quite often in social media to express the feelings of the posters. The 
tagged emoticons specify their actual meanings in some sense. Based on this idea, messages were col-
lected from Plurk1, a microblogging platform similar to Twitter. It lets users post messages limited to 
140 characters, and allows them to use graphical emoticons in their messages.  
It was assumed that these emoticons can represent the poster?s sentiments, and, therefore, be re-
garded as sentiment labels of the messages. Among 35 emoticons, 23 are categorized into positive, and 
12 are categorized into negative. Collected messages are dated from Jun 21, 2008 to Nov 7, 2009, and 
all of them are in Traditional Chinese. 
On the other hand, the literal meanings of the posted messages need to be known. Many sentiment 
analysis algorithms (Liu, 2012) can be explored. A lexicon-based approach was adopted. The NTU 
Sentiment Dictionary, or NTUSD (Ku and Chen, 2007), was employed to determine the sentiment of a 
word. This dictionary provides 21,056 positive and 22,751 negative words. Most of these words are in 
Traditional Chinese.  
                                                 
1 http://www.plurk.com 
1270
3.2 Candidates Extraction 
Possible irony messages were extracted from the Plurk corpus by using NTUSD. Since the typical so-
cial function of irony is expressing negative meaning with positive words, as mentioned in Gibbs and 
Colston (2007), focus was directed on those messages with negative emoticons and positive words. A 
total of 3,178,372 messages was found containing at least one negative emoticon. Among them, 
304,754 messages with at least one positive word are found and form an irony candidate dataset. 
Discourse relation determines how two discourse units cohere to each other. Sentiment transition of 
two clausal arguments is identified based on their discourse relation (Zhou et al., 2011; Wang et al., 
2012; Huang et al., 2013). In the sentence ?he is nice but not attractive,? positive opinion in the begin-
ning is transformed to a negative one by the discourse connective ?but.? Both the positive word ?nice? 
and the negative phrase ?not attractive? are used literally. Thus, it was necessary to filter out messages 
containing such connectives.  
Messages are removed only when the positive word occurs earlier than the discourse connectives 
with a comparison function, due to Chinese grammatical structure. The Chinese discourse connectives 
used here include ???, ????, ????, ????, ???? (all the above are equivalent to the English 
word but), ???? (however), ??? (comparatively), ???? (unfortunately), ???? (contrarily), ??
?? (oppositely), and ???? (on the contrary). A total of 254,836 messages remains after this process.  
3.3 Pattern Mining 
Although irony can be used without any customary linguistic patterns, some ironic expressions do ex-
hibit specific forms of language use. Colston and O?Brien (2000) suggest that both irony and hyperbo-
le create contrasts between expected and ensuing events. It was assumed that exaggerated expressions 
could be used with irony to strengthen the effects of the speech act. In the expression??????
?? (I am really and extremely lucky!), the adverbs really and extremely are used to strengthen the 
ironic effect. Thus, combinations of degree adverb phrases and a positive adjective are used as patterns 
to find possible irony expressions automatically in the candidate dataset. 
Not all degree adverbs in Chinese are used because some of them are mostly used in formal texts 
and not frequently present in microblogs. The degree adverb phrases used here include the combina-
tions of the adverbs ??? (h?i), ??? (y?), ???? (w?im?an), ??? (k?) and ???? (truly) and the de-
gree adverbs ??? (really), ??? (extremely) and ???? (very).  
The following bootstrapping procedure was used to find more patterns. 
(1) Which patterns should be used is decided. At the very beginning of the bootstrapping procedure, 
the [degree adverb + positive adjective] pattern mentioned above is used. 
(2) Messages containing the patterns in step (1) are automatically retrieved from the candidates. 
NTUSD is used to determine sentiment polarity, and CKIP parser is used to get parts of speech2. 
(3) Messages retrieved in step (2) were reviewed by the annotator to decide which of them are 
actually ironic.  
(4) If the annotator finds new irony patterns in the reviewed messages, then the procedure starts 
again from step (1) and uses the patterns to repeat the process. 
This process was repeated for four times. After the fourth iteration, no more new patterns were 
found by the annotator. Finally, 2,825 messages are found to have any of the patterns, and 1,005 of 
them are confirmed to be ironic and make up the NTU Irony Corpus.3 Examples of these patterns and 
ironic messages are shown in Section 4. 
4 Irony Patterns 
All the patterns mined by the approach used in Section 3 are categorized into the following five groups.  
4.1 Degree Adverbs + Positive Adjective 
In this pattern, the following two components must exist: 
 
                                                 
2 http://ckipsvr.iis.sinica.edu.tw. 
3 The NTU Irony Corpus is available at http://nlg.csie.ntu.edu.tw/nlpresource/irony_corpus/. 
1271
(a) Degree adverb phrase + positive adjective phrase 
(b) Negative context 
 
The negative context can occur either before or after the component (a). For example, the following 
expression is used when someone has to wait for a long time to start ordering in a restaurant. In total, 
13.03% of all the messages in the corpus contain this pattern. 
 
(s1) ???????????????? 
I have to wait for half an hour to order. The service is definitely really good. 
 
The underlined expression is the contextual information described in (b), and the double-underlined 
expression is the linguistic form described in (a). In the second clause the adverbs ??? (h?i) and ??? 
(really) are combined to form a degree adverb phrase for intensification or hyperbole. Although the 
positive word good is used, the speaker means the opposite. The first clause indicates why they think 
the service is not good, and, therefore, provides the contextual information. 
4.2 The Use of Positive Adjective with High Intensity 
In this pattern, the following two components must exist: 
 
(a) Positive adjective with high intensity 
(b) Negative context 
 
Specific positive adjectives with high intensity are used to form ironic expressions with or without 
other rhetorical elements. Since the context is negative, the positive adjective is used to express non-
literal meanings. The adjectives we found in the corpus include ???? (great), ????? (remarkable) 
and ???? (genius). Only 2.09% of the messages in the corpus contain this pattern. For example, the 
word great is used in the following message: 
 
(s2) ?? plurk??????????...????????????? 
My Plurk account encountered an unknown error ?again?? This is indeed the greatest 
invention in the century. 
4.3 The Use of Positive Noun with High Intensity 
In this pattern, the following two components must exist: 
 
(a) Positive noun with high intensity 
(b) Negative context 
 
Specific nouns that represent highly positive meanings are also used to express irony. These nouns 
include ???? (superstar), ???? (big gift) and ???? (wonderful state). When they are used with a 
negative context, an ironic expression is formed. This is pattern is not found frequently in the corpus. 
Only 2.00% of the messages in the corpus contain this pattern.  An example is listed below: 
 
(s3) ?????????.......????? 
The big gift I received in the Mid Autumn Festival was?? a lot of fat in my body. 
4.4 The Use of ???? (very good) 
In this pattern, the following two components must exist: 
 
(a) Sentence boundary + ?? + punctuation 
(b) Negative context 
 
1272
A sentence boundary occurs before the word ???? (very good) because there is no subject. Multiple 
punctuations, and particularly exclamation marks and ellipses, can be used after ???? to increase the 
intensity. In the following example, exclamation marks are used: 
 
(s4) ??... ??!! ?????? 
I caught a cold? Very good!! My vacation is gone. 
 
Sometimes this pattern is followed by an exclamation word, such as ??? (a), ??? (ya), and ??? 
(ma). These exclamations, like punctuations, can help strengthen the level of the speaker?s feelings. In 
our irony corpus, this pattern is used in 50.84% of all ironic messages. Obviously, this is a common 
way when people want to express their negative feelings with an ironic expression. 
4.5 ???????? (It?s okay to be worse) 
In this pattern, the following expression must exist: 
 
??? + negative adjective + ?? 
(It is okay to be more + negative adjective) 
 
This pattern literally states that it is okay for something to become worse and is a commonly used pat-
tern to express irony in our corpus. It can be found in 33.53% of the messages in the corpus. In most 
cases, even when no proper contextual information is present, the listener can tell the literal meaning is 
not meant because it violates most people?s inclinations. Thus, the use of this pattern is usually non-
literal and ironic. An example is shown below. 
 
(s5) ?????...??????? 
It's -11?C?It is okay to be colder 
 
A message can contain more than one pattern, causing the sum of the percentages of the above five 
patterns to be greater than 100%. For example, both patterns 4.4 and 4.5 are used in the following 
message: 
 
(s6)??!!!!??????? ?????... 
Very Good!!!! It is okay for me to be more idiotic? 
 
The patterns in Sections 4.4 and 4.5 are mainly based on their linguistic forms and frequently used 
in ironic expressions. We argue that these patterns are more static than the others, and we call them the 
customary patterns. On the other hand, the patterns in Sections 4.1, 4.2 and 4.3 are called non-
customary patterns. 
5 Collecting Ironic Expressions from Blogs 
In order to understand how irony is conveyed in different types of media, we use the methodology and 
mined patterns described in Sections 3 and 4 to collect irony expressions from the Yahoo Kimo Blogs 
corpus. 
5.1 The Yahoo Blog Corpus 
The Yahoo Kimo Blog corpus, referred to the Yahoo corpus in the following sections, contains blog 
articles from November 1, 2005 to August 20, 2007 (Yang, Lin and Chen, 2009). Out of all the posts 
in the dataset, 2,764,202 posts have at least one emoticon. The articles posted in July 2006 are used 
here, and they are divided into 341,932 smaller units by the full stop symbol. All articles are in Tradi-
tional Chinese.  
Since the Plurk platform can be used as an instant messaging system, and readers of the message are 
usually on the author?s friend list, these messages are usually conversational. On the other hand, Ya-
1273
hoo blogs are not limited in length and a blog article itself is not part of the conversation. Thus, the 
blog articles are usually more formal compared to microblog messages. 
Although the articles are separated by a full stop into shorter units, these units are not necessarily 
identical to sentences due to the conventional usage of the Chinese period symbol. They can consist of 
multiple sentences and thus contain a discourse structure, which makes them suitable for this corpus 
study. 
5.2 Extract Ironic Expressions  
A similar approach to the steps described in Section 3.3, is used to collect ironic expressions from the 
Yahoo corpus, but four patterns of irony found in Plurk are used to perform step (1). These patterns, as 
listed below, are adopted because they are the most frequently used ones in our Plurk irony corpus. 
They can also reflect the uses of customary and non-customary irony patterns as the first two patterns 
are customary, and the last two are non-customary. Pattern 1 and Pattern 2 are the same patterns as 
mentioned in Section 4.4 and Section 4.5, respectively. Pattern 3 and Pattern 4 are two forms from the 
pattern described in Section 4.1. Only step (1) to step (3) are performed, and step (4) is bypassed; that 
is, the process is not repeated. 
 
 Pattern 1: 
(a) Sentence boundary + ?? + punctuation 
(b) Negative context 
 Pattern 2: 
  ??? + negative adjective + ?? 
 Pattern 3: 
(a) ?? + positive adjective 
(b) Negative context 
 Pattern 4: 
(a) ?? + positive expression 
(b) Negative context 
5.3 Results and Discussion 
A total of 36 ironic texts is obtained. All the four irony patterns seen in Plurk can be found in Yahoo. 
The final results are shown in Table 1. 
 
 Number of Ironic Expressions Percentage 
Pattern 1 14 38.89% 
Pattern 2 10 27.78% 
Pattern 3 5 13.89% 
Pattern 4 7 19.44% 
Table 1: Ironic texts found for the four Patterns in Yahoo. 
 
The proportions of the four patterns in Plurk and Yahoo are also compared. The percentages are 
calculated by dividing the occurrence of each pattern by the occurrence of all four patterns in the same 
datasets. As can be seen in Figure 1, the proportions of patterns (1) and (2) in Plurk are significantly 
higher than in Yahoo, and the proportions of patterns (3) and (4) in Plurk are significantly lower than 
in Yahoo (p<0.05 according to the t-test). This suggests that patterns (1) and (2) tend to be used in 
informal and conversational texts while patterns (3) and (4) tend to be used in formal articles to 
convey irony. Also, this may suggest that customary patterns are more likely to be used in 
conversations, and authors of formal articles prefer an indirect way to express irony, although more 
data are required for further studies in the future. 
 
 
 
 
 
1274
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
Pattern1 Pattern2 Pattern3 Pattern4
Plurk
Yahoo
 Figure 1: Comparison of the proportion of the four patterns in Plurk and Yahoo. 
6 Error Analysis 
In this section, we analyze why non-ironic messages were retrieved by the automatic processes. The 
1,820 wrong messages specified in Section 3.3 are classified into the following two categories. 
 
(1) Sentiment identification 
Using the patterns to find possible ironic messages involves the correct sentiment identification. 
NTUSD does not cover some new words used on Internet informal conversations. The sentiment of a 
word can also be changed depending on its context. For example, ???? (so strong) is listed as a posi-
tive term in NTUSD. However, it is used to indicate a negative condition in the example (s7). 
 
(s7) ?????????????????? 
 The side effect of the pain reliever was so strong, making me sleep through the whole night. 
 
(2) Opinion targets 
In a Plurk message, even though the message poster is talking about the same topic, more than 
one entity with associated opinions can be present. For example: 
 
(s8) ???????????? 
 The business of our company is running so well. I am so tired. 
 
The poster expresses negative sentiment by using the word ?tired.? Although the positive word ??
?? (very good) is also used, it modifies the word ?business? rather than the poster?s condition. That is, 
the opinion targets of the two words are different, and this causes problems when automatically re-
trieving ironic messages.  
7 Linguistic Structure of Irony 
In this section, the linguistic structure of irony is analyzed based on our observations on the corpus. 
7.1 Ironic Word 
As described, the literal meaning of an ironic word or phrase is opposite to the actual meaning. An 
ironic word/phrase is necessary to separate irony from regular utterances. If the ironic word of an 
utterance is reverted, the speaker?s actual sentiment or intention is reconstructed. 
However, it is not easy to identify the ironic word in an utterance. Sometimes more than one word 
can be an ironic word. In our corpus, 94.93% of the ironic words are adjectives, while others are used 
as adverbs, verbs or nouns. The recognition of ironic word/phrase is a challenging task, but other iron-
ic elements described in Sections 7.2 and 7.3 can be analyzed side by side to help improve the perfor-
mance. 
7.2 Contextual Information 
Contextual information is usually provided as part of ironic utterances to help convey irony. For 
example, the underlined sentence in the following utterance is crucial for irony interpretation: 
 
1275
(s9) ??????????? 
 I was injured. I was really lucky. 
 
Without the first sentence, it is hard to tell if lucky is actually meant. Although a speaker can still 
use ironic words/phrases without providing contextual information, this can be an ineffective way to 
communicate the actual meanings of irony. According to the cooperative principle proposed by Grice 
(1975), the speaker must give enough information in order to enable successful communication and 
implicatures. The four maxims of the cooperative principle include: 
(1) Maxim of Quantity: The speaker should make their contribution as informative as is required. 
Do not make the contribution more informative than is required. 
(2) Maxim of Quality: The speaker should not say what they believe to be false, and should not 
say that for which they lack adequate evidence. 
(3) Maxim of Relation: The speaker should be relevant. 
(4) Maxim of Manner: The speaker should avoid obscurity of expression, avoid ambiguity, be 
brief and be orderly. 
Based on Grice?s maxims, it is assumed enough, correct, relevant, and understandable contextual 
information should be provided with ironic expressions. However, the speaker sometimes assumes the 
listener already knows about the conditions where the irony takes place and has the required 
background knowledge; thus the contextual information is hidden in the ironic utterance. 
Four types of context can be used to interpret irony: 
(1) Linguistic context: The linguistic context refers to the words that are expressed before and/or 
after the irony words in a sentence or discourse. It is easier to obtain and analyze than the other 
three types of context. 
(2) Physical context: Physical context refers to what is actually present and/or happening in the 
environment or circumstance where the conversation is taking place. It is also related to the 
timing. In online conversations, participants are not usually in the same location, but they can 
be aware of the same ongoing events and situations. It is not necessary for the speaker to 
provide physical context information if they assume the objects or situations are noticeable to 
the listeners.  
(3) Epistemic context: The background knowledge shared by the participants in a conversion can 
also be used to interpret the irony. This type of context does not change over time. For example, 
people know rocks are hard, so they can understand the expression the bed is as soft as a rock is 
not literal. 
(4) Social context: Social relationship can be important for expressing and interpreting irony, 
especially in online messages.  
We argue that at least one type of contextual information must exist, but it can be hidden if the 
speaker thinks the listener is already aware of it. Physical, epistemic and social context can be hidden, 
while linguistic contextual information must be present. 
7.3 Rhetoric 
As shown in Section 4, degree adverbs, punctuations and exclamations can be used to convey irony. 
Some of them can even be repeated to intensify the effects. These elements increase contradiction and 
strengthen the degree of negative opinions. Unlike ironic words and context, rhetoric elements are not 
necessary to convey irony. 
Liebrecht et al. (2013) call the words used to strengthen evaluative utterances intensifiers. In their 
experiments, non-hyperbolic sarcastic messages often contain an explicit marker on Twitter. They ar-
gue that sarcasm is often signaled by hyperbolic words, including intensifiers and exclamations, and 
sarcastic utterances with hyperbolic words are easier to identify by listeners/readers than sarcastic ut-
terances without hyperbolic words. It can be seen that adverbs, adjectives, punctuations and exclama-
tions with high intensity observed in our irony patterns have very similar effects. 
Among the 113 messages containing the #?? (#irony) hashtag in Weibo, which was mentioned in 
Section 2, 83.19% do not exhibit hyperbole or uses of intensifiers. This observation is similar to the 
argument suggested in Liebrecht et al. (2013) and is one of the reasons why the hashtag is not suitable 
1276
for the irony pattern mining task in this study. In comparison, this methodology helps find more clues 
of irony that can be seen from their linguistic forms. 
7.4 Corpus Labeling 
To increase the usefulness of the corpus, ironic element tags are added to each message. An example 
is shown in Figure 2. 
Figure 2: An example message with ironic element tags. 
 
As can be seen in the example, ??? (good) is the word that is used in the opposite way, so it is 
marked with the ironic word/phrase label <ironic>. The preceding sentence states what actually hap-
pened, and is marked with the label <context>. The message poster also uses the degree adverb ??? 
(extremely) and used the exclamation ??? (ba, a sentence-final partical). These two words are marked 
with the <rhetoric> label. The sentiment polarity marks of the ironic word and contextual information, 
shown as either pos or neg, are also added.  
8 Conclusion 
In this paper, five types of irony patterns are mined, and an irony corpus is constructed based on 
linguistic forms and sentiment classification. Four verbal forms in Plurk and Yahoo were further 
examined. The former platform restricts short text conversation, and the latter platform allows for the 
long text description. The experimental results show that the customary forms tend to be used in 
informal and conversational texts while the non-customary forms tend to be used in formal articles to 
convey irony. The three basic elements that form a successful ironic speech act were also analyzed. 
These elements, including the words/phrases with reversed meanings, contextual information and 
rhetorical words, should be identified first in order to properly process ironic expressions and perform 
linguistic analysis. In the mined patterns, it was found that hyperbole was frequently present. In future 
work, we will explore other opinion mining and sentiment analysis algorithms, and focus on automatic 
recognition of hyperbole and the ironic elements. 
Acknowledgements 
This research was partially supported by National Taiwan University under grant 103R890858.  We 
are also very thankful to the anonymous reviewers for their helpful comments to revise this paper. 
References 
Herbert L. Colston and Jennifer O'Brien. 2000. Contrast of Kind Versus Contrast of Magnitude: the 
Pragmatic Accomplishments of Irony and Hyperbole. Discourse and Processes, 30(3):179-199. 
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-Supervised Recognition of Sarcastic Sen-
tences in Twitter and Amazon, In Proceedings of the Fourteenth Conference on Computational 
Natural Language Learning (CoNLL-2010), pages 107-116, Uppsala, Sweden. 
Elena Filatova. 2012. Irony and Sarcasm: Corpus Generation and Analysis Using Crowdsourcing. In 
Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC 
2012), pages 392-298, Istanbul, Turkey. 
Raymond W. Gibbs and Herbert L. Colston. 2007. Irony in Language and Thought. Lawrence Erl-
baum Associates, New York. 
<context sentiment="pos">????????????</context>???<rhetoric>??
</rhetoric><ironic sentiment="neg">?</ ironic>?<rhetoric>?</rhetoric>. 
 
English translation: 
<context sentiment="pos">The book I just bought has fallen apart.</context> The quality is <rheto-
ric>just extremely</rhetoric> <ironic sentiment="neg">good</ ironic>le<rhetoric>ba</rhetoric>. 
1277
Rachel Giora and Ofer Fein. 1999. Irony: Context and Salience. Metaphor and Symbol, 14:241-257. 
Roberto Gonz?lez-Ib??ez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying Sarcasm in 
Twitter: A Closer Look. In Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Short Papers, pages 581-586, Portland, Oregon, USA. 
H. P. Grice. 1975. Logic and Conversation. In P. Cole and J. J. Morgan, eds. Syntax and Semantics, 3: 
Speech Acts. New York: Academic Press. 
Hen-Hsen Huang, Chi-Hsin Yu, Tai-Wei Chang, Cong-Kai Lin and Hsin-Hsi Chen. 2013. Analyses of 
the Association between Discourse Relation and Sentiment Polarity with a Chinese Human-
Annotated Corpus. In Proceedings of ACL 2013 7th Linguistic Annotation Workshop & Interopera-
bility with Discourse, pages 70-78, Sofia, Bulgaria. 
Graham Katz and Eugenie Giesbrecht. 2006. Automatic Identification of Non-compositional Multi-
word Expressions Using Latent Semantic Analysis. In Proceedings of the ACL/COLING-06 Work-
shop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12-19, 
Sydney, Australia. 
Lun-Wei Ku and Hsin-Hsi Chen. 2007. Mining Opinions from the Web: Beyond Relevance Retrieval. 
Journal of American Society for Information Science and Technology, Special Issue on Mining Web 
Resources for Enhancing Information Retrieval, 58(12):1838-1850.  
Linlin Li and Caroline Sporleder. 2010. Linguistic Cues for Distinguishing Literal and Non-Literal 
Usages. In Proceedings of 23rd International Conference on Computational Linguistics (COLING 
2010), Poster Volume, pages 683-691, Beijing, China. 
Christine Liebrecht, Florian Kunneman, and Antal van den Bosch. 2013. The Perfect Solution for De-
tecting Sarcasm in Tweets #not. In Proceedings of the 4th Workshop on Computational Approaches 
to Subjectivity, Sentiment and Social Media Analysis, pages 29-37, Atlanta, Georgia. 
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language 
Technologies, Morgan & Claypool Publishers. 
Stephanie Lukin and Marilyn Walker. 2013. Really? Well. Apparently Bootstrapping Improves the 
Performance of Sarcasm and Nastiness Classifiers for Online Dialogue. In Proceedings of the 
Workshop on Language Analysis in Social Media, pages 30-40, Atlanta, Georgia. 
Antonio Reyes, Paolo Rosso, and Davide Buscaldi. 2012. From Humor Recognition to Irony Detec-
tion: The Figurative Language of Social Media. Data & Knowledge Engineering, 74:1-12. 
Tony Veale and Yanfen Hao. 2010. Detecting Ironic Intent in Creative Comparisons. In Proceedings 
of the 19th European Conference on Artificial Intelligence (ECAI 2010), pages 765-770, Lisbon, 
Portugal. 
Fei Wang, Yunfang Wu, and Likun Qiu. 2012. Exploiting Discourse Relations for Sentiment Analysis. 
In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012): 
Posters, pages 1311-1320, Mumbai, India. 
Changhua Yang, Kevin Lin, and Hsin-Hsi Chen 2009. Writer Meets Reader: Emotion Analysis of So-
cial Media from both the Writer?s and Reader?s Perspectives. In Proceedings of the 2009 
IEEE/WIC/ACM International Conference on Web Intelligence (WI 2009), pages 287-290, Milan, 
Italy. 
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei, and Kam-Fai Wong. 2011. Unsupervised Discov-
ery of Discourse Relations for Eliminating Intro-Sentence Polarity Ambiguities. In Proceedings of 
the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 
162-171, Edinburgh, Scotland, UK. 
 
1278
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 103?108,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
FAdR: A System for Recognizing False Online Advertisements 
  Yi-jie Tang and Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan tangyj@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.tw     Abstract 
More and more product information, in-cluding advertisements and user reviews, are presented to Internet users nowadays. Some of the information is false, mislead-ing or overstated, which can cause seri-ousness and needs to be identified. Au-thorities, advertisers, website owners and consumers all have the needs to detect such statements. In this paper, we propose a False Advertisements Recognition sys-tem called FAdR by using one-class and binary classification models. Illegal adver-tising lists made public by a government and product descriptions from a shopping website are obtained for training and test-ing. The results show that the binary SVM models can achieve the highest perfor-mance when unigrams with the weighting of log relative frequency ratios are used as features. Comparatively, the benefit of the one-class classification models is the ad-justable rejection rate parameter, which can be changed to suit different applica-tions. Verb phrases more likely to intro-duce overstated information are obtained by mining the datasets. These phrases help find problematic wordings in the advertis-ing texts. 1 Introduction As online commerce and advertising keep grow-ing, more and more consumers depend on infor-mation on the Internet to make purchasing deci-sions. This kind of information includes online advertisements posted by businesses, and discus-sions or reviews generated by users. However, false statements can also be presented to con-sumers. For example, some companies hire peo-ple to post fake product reviews in an attempt to 
promote their own products or reduce competi-tors? reputations (Ott et al., 2011). It is referred to as deceptive opinion spamming and explored in recent researches (Ott et al., 2011; Mukherjee et al., 2012; Mukherjee et al., 2013; Fei et al., 2013). False statements and exaggerated content can also be seen in online advertisements. These statements can also be regarded as opinion spams, while the authors, that is, the advertisers, can be more easily identified. Yeh (2014) report-ed the top two types of illegal advertisements on the web, TV and broadcast are food (62.61%) and cosmetic (24.26%). Of the dissemination media, the web is the major source of false ad-vertisements. Most inappropriate food-related advertisements contain overstated health claims. The medical effects and cure claims may also appear in cosmetic advertising. As a result, ad-vertising regulations are enforced in many coun-tries to protect consumers from fraudulent and misleading information. False, overstated or mis-leading information and mentions of curative effects can be prohibited by the authorities (FTC, 2000; DOH, 2009; CFIA, 2010). To regulate online advertising, the authorities need to review a large number of advertisements and determine their legality, which is cost- and time-consuming. Advertisers also need to know the legality of their advertisements to avoid vio-lating advertising laws. This becomes especially important when every Internet user can be an advertiser if s/he posts messages related to any product announcement, promotion, or sales. Website owners that accept advertisements have to present appropriate advertisement contents to users and avoid legal issues. Even Internet users should also identify false advertisements in order not to be misled. Thus, the recognition of false, misleading or overstated information is an emerging task.  This paper presents a False Advertisements Recognition system called FAdR, and take two 
103
major sources of illegal advertisements on the web, i.e., food and cosmetic advertising, as ex-amples. Section 2 surveys the related work. Sec-tion 3 introduces the datasets used in the experi-ments. Section 4 presents classification models and shows their performance. Section 5 mines the overstated phrases. Section 6 demonstrates the uses of FAdR system with screenshot. Both sentence and document levels are considered. 2 Related Work Gokhman et al. (2012) collected data from the Internet and explored methods to construct a gold standard corpus for ?deception? studies. Ott et al. (2011) studied methods to detect ?disrup-tive opinion spams.? Unlike conventional adver-tising spams, these fake opinions look authentic and are used to mislead users. Mukherjee et al. (2013) used reviewer?s behavioral footprints to detect spammer. As they pointed out, one of the largest problems to solve this issue is that there is no appropriate datasets for fake and non-fake reviews. Previous online advertising research mostly focuses on bidding, matching or recommendation of advertisements on websites. Ghosh et al. (2009) studied bidding strategies for advertise-ment allocations. Huang et al. (2008) proposed an advertisement recommendation method by classifying instant messages into the Yahoo cate-gories. Scaiano and Inkpen (2011) used Wikipe-dia for negative keyphrase generation to hide advertisements that users are not interested in. This paper, in contrast, focuses on identifying false statements in online advertisements with classification models. 3 Datasets We use the illegal advertising lists and state-ments made public by the Taipei City Govern-ment1 as the illegal advertising datasets. The con-tents of the government data are split into sen-tences by colon, period, question mark and ex-clamation mark. Two types of datasets are built for illegal food and cosmetic advertising, named FOOD_ILLEGAL and COS_ILLEGAL, respec-tively. Some illegal sentences in the illegal food advertising dataset are shown below: (1)  ?????????? Reduces waste produced by metabolism process. (2)  ????????                                                 1 http://www.health.gov.tw/Default.aspx?tabid=295 
Stops insomnia and pain. (3)  ?????? Cures hypertension. In the government website, the authority does not regularly announce legal advertising data. We adopt one-class classifiers with only illegal data for this scenario, as shown in Section 4.1. To experiment on binary classifiers, we collect product descriptions from a shopping website2 and verify their legality manually to construct the legal advertising datasets. The legal food and cosmetic adverting datasets are named FOOD_LEGAL and COS_LEGAL, respectively. The numbers of the sentences in FOOD_LEGAL, FOOD_ILLEGAL, COS_LEGAL, and COS_ILLEGAL are 5,059, 7,033, 10,520, and 11,381, respectively. 4 Classification Models One-class Na?ve Bayes and Bagging classifiers, and binary classifiers based on Na?ve Bayes and SVM models are implemented.  4.1 One-Class Classifiers  We adopt the OneClassClassifier module (Hempstalk et al., 2008) in the WEKA machine learning tool to train one-class classifiers with illegal statements only. The OneClassClassifier module provides a rejection rate parameter for adjusting the threshold between target and non-target instances.  The target class, which corre-sponds to the illegal class in this study, is the single class used to train the classifier.  Higher rejection rate means that more legal statements will be preferred, but illegal statements may be still incorrectly classified into legal ones. Na?ve Bayes and Bagging classifiers are chosen be-cause they achieve best performance among the algorithms we have explored in this experiment. Each instance in the dataset, i.e., a sentence, is represented by a word vector (w1, w2, ?, w1000), where wi is a binary value indicating whether a word occurs in the sentence or not. The vocabu-lary is selected from the illegal advertising da-tasets. To properly filter out common words, we count top 1,000 frequent words in the Sinica Balanced Corpus of Modern Chinese3 and re-move them from the vocabulary. The remaining top 1,000 words are used for vector representation. Total 532 illegal statements provided by the Department of Health form the training set. An                                                 2 http://www.7net.com.tw 3 http://app.sinica.edu.tw/kiwi/mkiwi/ 
104
illegal and a legal advertising dataset make up the test set. The former consists of 317 illegal sentences from Taipei City Government?s lists, and the latter contains 203 legal statement exam-ples from the Department of Health. Table 1 shows the accuracies of Na?ve Bayes and Bagging classifiers in the food dataset. The rejection rates from 0.7 to 0.8 are preferable for most applications, because they result in higher accuracy for legal statement classification while not significantly reducing the performance of illegal statement detection.  Using the 0.7 rejec-tion rate produces high performance for the ille-gal class while 0.8 rejection rate does better for the legal class.  The actual choice of rejection rate depends on the demands of users.  For an advertiser, it is important to avoid all possible problematic statements.  Thus, a lower rejection rate will be more suitable.  If the system is used by the authorities, a rejection rate higher than 0.7 may be preferable because they don?t misjudge too many legal advertisements.  Rejection rate 0.4 0.5 0.6 0.7 0.8 0.9 Na?ve Bayes Illegal 85.33% 82.39% 79.01% 74.49% 68.17% 59.14% Legal 31.07% 39.81% 53.40% 63.11% 72.82% 86.41% 
Bagging Illegal 92.78% 88.49% 84.65% 74.94% 69.07% 0.23% Legal 3.88% 17.48% 27.18% 65.72% 82.52% 99.77% Table 1: Accuracies of Classifiers in Different Rejec-tion Rates. 4.2 Binary Classifiers  We use FOOD_LEGAL and FOOD_ILLEGAL datasets, and COS_LEGAL and COS_ILLEGAL datasets to build binary classifiers for food and cosmetic advertising classification, respectively. Na?ve Bayes classifiers and SVM classifiers im-plemented with libSVM (Chang & Lin, 2011) are adopted. Ten-fold cross validation is used for the training and testing tasks. Total 1,000 highly fre-quent words are selected in the same way as in Section 4.1 to form a word-based unigram fea-ture set.  Two weighting schemes are considered. In the binary weighting, each sentence is represented by a word vector (w1, w2, ?, w1000), where wi is a binary value indicating whether a word occurs in the sentence or not.  In the weighting of log rela-tive frequency ratio, we follow the idea of collo-cation mining (Damerau, 1993). Relative fre-quency ratio between two datasets has been shown to be useful to discover collocations that are characteristic of a dataset when compared to the other dataset. It has been successfully applied to mine sentiment words from microblog and to 
model reader/writer emotion transition (Tang and Chen, 2011, 2012). The log relative frequency ratio (logRF) is defined formally as follows. Given two datasets A and B, the log relative frequency ratio for each wi?A?B is computed with the following formula. logRFAB (wi ) = log fA (wi )| A |fB (wi )| B |  logRFAB(wi) is a log ratio of relative frequen-cies of word wi in A and B, fA(wi) and fB(wi) are frequencies of wi in A and in B, respectively, and |A| and |B| are total words in A and in B, respec-tively. logRF values are used to estimate the dis-tribution of the words in datasets A and B. If wi has higher relative frequency in A than in B, then logRFAB(wi)>0, and vice versa. In our experi-ments, logRF is used to present each unigram?s distribution in the legal and illegal datasets, re-placing the binary value for a unigram feature. Tables 2 and 3 show the results of the classifi-cation models with different combinations of feature sets. When logRF is combined with Uni-gram, the accuracy is significantly improved in both the food and cosmetic datasets. We can also see that the performance of all FOOD models are higher than equivalent COS models. Possible reasons may be that the effects of cosmetics are related to body appearance, and inappropriate cure claims are also related to body improvement and appearance changes. There can be some overlaps between the words used in legal and illegal cosmetic advertising.   Classification Mod-els ? Na?ve Bayes SVM Illegal vs. Legal ? Features ? Illegal Legal Illegal Legal Unigram 92.59% 85.06% 89.46% 88.00% Unigram + logRF 94.32% 86.37% 94.70% 91.68% Table 2: Classification Accuracies for FOOD Datasets.  Classification Mod-els ? Na?ve Bayes SVM Illegal vs. Legal ? Features ? Illegal Legal Illegal Legal Unigram 86.48% 77.63% 82.47% 82.36% Unigram + logRF 88.20% 83.06% 88.46% 83.41% Table 3: Classification Accuracies for COS Datasets. 5 Overstated Phrase Mining Since the authority focuses on health claims in advertising, almost all illegal statements an-nounced by the government include an action related to health improvement and a name that refers to diseases or body conditions. Thus, we can observe that most of the illegal statements 
105
recognized and forbidden by the authority con-tain a health-related verb phrase consisting of a transitive verb and an object. These illegal adver-tising verb phrases can be mined from the da-tasets for the government?s and advertisers? ref-erence. We can also use these verb phrases to help the users of our system understand possible reasons why the sentences in advertisements are labeled as illegal. We propose a mining method based on log relative frequency ratio, which is described in Section 4.2. We compute logRFAB(wi) to obtain the words that are most likely to be used in ille-gal advertising. We identify transitive verbs and nouns in the word list based on POS tagging re-sults generated by the CKIP parser4, and then use them to examine if a verb phrase is presented in a sentence. Total 979 verb phrases are mined from the FOOD datasets, and 2,302 from the COS da-taset. Table 4 shows some examples.  Dataset Illegal advertising verb phrases Transitive verb Object noun 
FOOD 
?? (improve) ?? (physical condition) ?? (inactivate) ?? (bacteria) ?? (decompose) ??? (cholesterol) 
COS 
?? (purify) ?? (body) ?? (ease) ?? (pain) ?? (cure) ?? (acne vulgaris) Table 4: Example illegal verb phrases mined from the FOOD and COS datasets. 6 System Architecture The FAdR system is composed of pre-processing (Pre-Processor), recognition (Recog-nizer), and explanation (Explainer) modules. Figure 1 shows the overall system architecture. 6.1 Pre-processing Module Our classification models are sentence-based, so the main purpose of the Pre-processor in the sys-tem is detecting sentence boundaries. Four types of punctuations, including period, colon, excla-mation, and question mark, are used to segment a document into sentences. Line breaks are also regarded as a sentence boundary marker because                                                 4 http://ckipsvr.iis.sinica.edu.tw 
many advertisements in Chinese put sentences in separate lines and do not include any punctua-tion. Sentences with less than three characters or more than 80 characters are ignored. Word segmentation is performed by using the CKIP segmenter, which is an online service and can be accessed through the TCP socket. Seg-mented data will be represented by the corre-sponding feature sets based on classification model and converted to a format that the Recog-nizer can read as input.   
Recognizer Classification Models
Advertising Document
Sentence 
Segmenter
Word
Segmenter
Format 
Converter Feature Sets
Explainer
Advertising document
with sentence-based
legality labels and
explanations.
Pre-Processor
  Figure 1. System architecture of FAdR 6.2 Recognition Module All processed sentences are sent from the Pre-Processor to the Recognizer for legality identifi-cation.  Since our training tasks are done in WEKA, we can use the model files generated by WEKA for implementing the Recognizer. The Recogniz-er loads the pre-trained SVM models for food and cosmetic advertising classification, and then uses them for labeling the incoming sentences. For the One-Class models, the model files are pre-generated by training with different rejection rates from 0.4 to 0.9. When the user adjusts the threshold, the Recognizer chooses the corre-sponding model to perform illegal sentences identification. 
106
6.3 Explanation Module To give users more information on the possible reasons why the advertising contents are consid-ered illegal, the Explainer uses the illegal verb phrase list, which is discussed in Section 5, to extract the problematic words from the input sen-tences. If the verb and the object noun in a verb phrase from the list both occur in an illegal sen-tence, then the verb phrase will be shown besides the recognition results in the user interface. 6.4 User Interface Users can copy and paste the advertising con-tents to be recognized to the text field, or upload a document to the system. It usually takes less than 10 seconds on our server to process a doc-ument with 200 characters, so the system is suit-able to quickly process a large amount of data. If the users choose to use the one-class mod-els, they can adjust the threshold value to fit dif-ferent needs and receive useful results. Lowering the value can find as many problematic sentences as possible, but more legal sentences can also be misjudged.  Increasing the value can avoid wrongly labeling legal sentences as illegal, but more illegal sentences can be missed.  Figure 2 shows a system screenshot. The recognition results of a food advertisement with 11 sentences are demonstrated. Sentences la-belled as illegal are highlighted in red. Verb phrases possibly causing illegality are listed in grey colour for illegal sentences. The number of all sentences, the number of illegal sentences, and the final score are shown at the bottom. The correct score of an advertisement is defined as the number of correct sentences divided by total sentences in this advertisement. The sample ad-vertisement used in Figure 2 and its English translation are shown as follows.  <A food advertisement> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????    (The leading brand for Japanese tea. The first tea product combining three kinds of natural colour-ings in Taiwan. Can improve immunity. Can re-lieve stress. Can strengthen resistance to disease.  Can increase antibodies in your body. It is mild and not irritative. Good for daily use. Can pre-vent body cells from being harmed by free radi-
cals. Can strengthen immunity. It is healthy and tasty, and brings no body burden.)  
  Figure 2: Screenshot for Illegal Sentence Recognition 7 Conclusion Detecting false information on the Internet has become an important issue for users and organi-zations. In this paper, we present two types of classification methods to identify overstated sen-tences in online advertisements and build a false online advertisements recognition system FAdR. The recognition on both document and sentence levels is addressed in the demonstration. In the binary models, using combinations of unigrams and the log relative frequency ratio as features can achieve highest performance. On the other hand, the one-class models can be used to build a system that is adjustable by users for dif-ferent application domains. The authorities or website owners can use a rejection rate of 0.7 or 0.8 to highlight most seri-ous illegal advertisements. An advertisement 
107
with a score lower than 0.5 means it may critical-ly violate the regulations, and need to be regard-ed as illegal advertising. Since not all advertise-ment posters are professional advertisers, they may need detailed information on the legality of every sentence. The illegal verb phrases found in a sentence provide clues to the advertiser. The system is also useful for consumers, as they can check if the advertisement contents can be trust-ed before making a purchase decision. As future work, we will extend the methodol-ogy presented in this study to handle other types of advertisements and the materials in other lan-guages.  We will also investigate what linguistic patterns can be used to mine the overstated phrases in different languages. Acknowledgments This research was partially supported by Nation-al Taiwan University and Ministry of Science and Technology, Taiwan under 103R890858 and 102-2221-E-002-103-MY3. References  Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: a Library for Support Vector Machines. Available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. CFIA. 2010. Advertising Requirements. Canadian Food Inspection Agency. Available at http://www.inspection.gc.ca/english/fssa/labeti/advpube.shtml. Fred J. Damerau. 1993. Generating and Evaluating Domain-Oriented Multi-Word Terms from Text. Information Processing and Management, 29:433-477. DOH. 2009. Legal and Illegal Advertising Statements for Cosmetic Regulations. Department of Health of Taiwan. Available at http://www.doh.gov.tw/ufile/doc/0980305527.pdf. Geli Fei, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013. Exploiting Burstiness in Reviews for Review Spammer Detection. In Proceedings of the Interna-tional AAAI Conference on Weblogs and Social Media (ICWSM-2013), 175-184. FTC. 2000. Advertising and Marketing on the Inter-net: Rules of the Road, Bureau of Consumer Pro-tection. Federal Trade Commission, September 2000. Available at http://business.ftc.gov/sites/default/files/pdf/bus28-advertising-and-marketing-internet-rules-road.pdf. Stephanie Gokhman, Jeff Hancock, Poornima Prabhu, Myle Ott, and Claire Cardie. 2012. In Search of a 
Gold Standard in Studies of Deception. In Pro-ceedings of the EACL 2012 Workshop on Compu-tational Approaches to Deception Detection, 23?30. Arpita Ghosh, Preston McAfee, Kishore Papineni, and Sergei Vassilvitskii. 2009. Bidding for Representa-tive Allocations for Display Advertising. CoRR, abs/0910-0880, 2009. Hung-Chi Huang, Ming-Shun Lin and Hsin-Hsi Chen. 2008. Analysis of Intention in Dialogues Using Category Trees and Its Application to Advertise-ment Recommendation. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP 2008), 625-630. Kathryn Hempstalk, Eibe Frank, and Ian H. Witten. 2008. One-Class Classification by Combining Density and Class Probability Estimation. In Pro-ceedings of the 12th European Conference on Principles and Practice of Knowledge Discovery in Databases and 19th European Conference on Ma-chine Learning, 505-519. Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012. Spotting Fake Reviewer Groups in Consum-er Reviews. In Proceedings of the International World Wide Web Conference (WWW 2012), 191-200. Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui Wang, Meichun Hsu, Malu Castellanos, and Rid-dhiman Ghosh. 2013. Spotting Opinion Spammers using Behavioral Footprints. In Proceedings of SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2013), 632-640. Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Hancock. 2011. Finding Deceptive Opinion Spam by Any Stretch of the Imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, 309?319.  M. Scaiano and D. Inkpen. 2011. Finding Negative Key Phrases for Internet Advertising Campaigns Using Wikipedia. In Recent Advances in Natural Language Processing (RANLP 2011), 648?653. Yi-jie Tang and Hsin-Hsi Chen. 2011. Emotion Mod-eling from Writer/Reader Perspectives Using a Mi-croblog Dataset. In Proceedings of IJCNLP Work-shop on Sentiment Analysis where AI Meets Psy-chology, 11-19.  Yi-jie Tang and Hsin-Hsi Chen. 2012. Mining Senti-ment Words from Microblogs for Predicting Writ-er-Reader Emotion Transition. In Proceedings of the 8th International Conference on Language Re-sources and Evaluation (LREC 2012), 1226-1229. Ming-kung Yeh. 2014. Weekly Food and Drug Safety. No. 440, February, Food and Drug Administration, Taiwan. Available at http://www.fda.gov.tw/TC/PublishOther.aspx. 
108
