Proceedings of the 12th European Workshop on Natural Language Generation, pages 1?8,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Using NLG to Help Language-Impaired Users Tell Stories and  
Participate in Social Dialogues 
 
 
Ehud Reiter, Ross Turner 
University of Aberdeen 
Aberdeen, UK 
e.reiter@abdn.ac.uk 
csc272@abdn.ac.uk 
Norman Alm, Rolf Black, 
Martin Dempster, Annalu Waller 
University of Dundee 
Dundee, UK 
{nalm,rolfblack,martindempster, 
awaller}@computing.dundee.ac.uk 
 
 
Abstract 
Augmentative and Alternative Communication 
(AAC) systems are communication aids for 
people who cannot speak because of motor or 
cognitive impairments.  We are developing 
AAC systems where users select information 
they wish to communicate, and this is ex-
pressed using an NLG system.  We believe 
this model will work well in contexts where 
AAC users wish to go beyond simply making 
requests or answering questions, and have 
more complex communicative goals such as 
story-telling and social interaction. 
1 Introduction 
Many people have difficulty in communicating 
linguistically because of cognitive or motor im-
pairments.  Such people typically use communi-
cation aids to help them interact with other peo-
ple.  Such communication aids range from sim-
ple tools that do not involve computers, such as 
picture cards, to complex software systems that 
attempt to ?speak? for the impaired user. 
From a technological perspective, even the 
most complex communication aids have typi-
cally been based on fixed (canned) texts or sim-
ple fill-in-the-blank templates; essentially the 
user selects a text or template from a set of pos-
sible utterances, and the system utters it.  We 
believe that while this may be adequate if the 
user is simply making a request (e.g., please give 
me a drink) or answering a question (e.g., I live 
at home), it is not adequate if the user has a more 
complex communicative goal, such as engaging 
in social interaction, or telling a story. 
We are exploring the idea of supporting such 
interactions by building a system which uses ex-
ternal data and/or knowledge sources, plus do-
main and conversational models, to dynamically 
suggest possible messages (event, facts, or opin-
ions, represented as ontology instances) which 
are appropriate to the conversation. The user se-
lects the specific message which he wishes the 
system to speak, and possibly adds simple anno-
tations (e.g., I like this) or otherwise edits the 
message.  The system then creates an appropriate 
linguistic utterance from the selected message, 
taking into consideration contextual factors. 
In this paper we describe two projects on 
which we are working within this framework.  
The goal of the first project is to help non-
speaking children tell stories about their day at 
school to their parents; the goal of the second 
project is to help non-speaking adults engage in 
social conversation. 
2 Background 
2.1 Augmentative and alternative commu-
nication 
Augmentative and alternative communication 
(AAC) is a term that describes a variety of meth-
ods of communication for non-speaking people 
which can supplement or replace speech.  The 
term covers techniques which require no equip-
ment, such as sign language and cards with im-
ages; and also more technologically complex 
systems which use speech synthesis and a variety 
of strategies to create utterances.  
The most flexible AAC systems allow users to 
specify arbitrary words, but communication rates 
are extremely low, averaging 2-10 words per 
minute. This is because many AAC users interact 
slowly with computers because of their impair-
ments.  For example, some of the children we 
work with cannot use their hands, so they use 
scanning interfaces with head switches.  In other 
words, the computer displays a number of op-
1
tions to them, and then scans through these, 
briefly highlighting each option.  When the de-
sired option is highlighted, the child selects it by 
pressing a switch with her head.   This is ade-
quate for communicating basic needs (such as 
hunger or thirst); the computer can display a 
menu of possible needs, and the child can select 
one of the items.  But creating arbitrary messages 
with such an interface is extremely slow, even if 
word prediction is used; and in general such in-
terfaces do not well support complex social in-
teractions such as story telling (Waller, 2006).  
A number of research projects in AAC have 
developed prototype systems which attempt to 
facilitate this type of human-human interaction.  
At their most basic, these systems provide users 
with a library of fixed ?conversational moves? 
which can be selected and uttered.  These moves 
are based on models of the usual shape and con-
tent of conversational encounters (Todman & 
Alm, 2003), and for example include standard 
conversational openings and closings, such as 
Hello and How are you. They also include back-
channel communication such as Uh-huh, Great!, 
and Sorry, can you repeat that. 
It would be very useful to go beyond standard 
openings, closings, and backchannel messages, 
and allow the user to select utterances which 
were relevant to the particular communicative 
context and goals.  Dye et al(1998) developed a 
system based on scripts of common interactions 
(Schank & Abelson, 1977).  For example, a user 
could activate the MakeAnAppointment script, 
and then could select utterances relevant to this 
script, such as I would like to make an appoint-
ment to see the doctor.  As the interaction pro-
gressed, the system would update the selections 
offered to the user based on the current stage of 
the script; for example during time negotiation a 
possible utterance would be I would like to see 
him next week. This system proved effective in 
trials, but needed a large number of scripts to be 
generally effective.  Users could author their own 
texts, which were added to the scripts, but this 
was time-consuming and had to be done in ad-
vance of the conversation. 
Another goal of AAC is to help users narrate 
stories. Narrative and storytelling play a very 
important part in the communicative repertoire of 
all speakers (Schank, 1990). In particular, the 
ability to draw on episodes from one?s life his-
tory in current conversation is vital to maintain-
ing a full impression of one?s personality in deal-
ing with others (Polkinghorne, 1991). Story tell-
ing tools for AAC users have been developed, 
which include ways to introduce a story, tell it at 
the pace required (with diversions) and give 
feedback to comments from listeners (Waller, 
2006); but again these tools are based on a li-
brary of fixed texts and templates. 
2.2 NLG and AAC 
Natural language generation (NLG) systems 
generate texts in English and other human lan-
guages from non-linguistic input (Reiter and 
Dale, 2000).  In their review of NLP and AAC, 
Newell, Langer, and Hickey (1998) suggest that 
NLG could be used to generate complete utter-
ances from the limited input that AAC users are 
able to provide.  For example, the Compansion 
project (McCoy, Pennington, Badman 1998) 
used NLP and NLG techniques to expand tele-
graphic user input, such as Mary go store?, into 
complete utterances, such as Did Mary go to the 
store?  Netzer and Elhadad (2006) allowed users 
to author utterances in the symbolic language 
BLISS, and used NLG to translate this to English 
and Hebrew texts. 
In recent years there has been growing interest 
in data-to-text NLG systems (Reiter, 2007); 
these systems generate texts based on sensor and 
other numerical data, supplemented with ontolo-
gies that specify domain knowledge.  In princi-
ple, it seems that data-to-text techniques should 
allow NLG systems to provide more assistance 
than the syntactic help provided by Compansion.  
For example, if the user wanted to talk about a 
recent football (soccer) match, a data-to-text sys-
tem could get actual data about the match from 
the web, and generate potential utterances from 
this data, such as Arsenal beat Chelsea 2-1 and 
Van Persie scored two goals; the user could then 
select one of these to utter. 
In addition to helping users interact with other 
people, NLG techniques can also be used to edu-
cate and encourage children with disabilities.  
The STANDUP system (Manurung, Ritchie et 
al., 2008), for example, used NLG and computa-
tional humour techniques to allow children who 
use AAC devices to generate novel punning 
jokes.  This provided the children with successful 
experiences of controlling language, gave them 
an opportunity to play with language and explore 
new vocabulary (Waller et al, in press). In a 
small study with nine children with cerebral 
palsy, the children used their regular AAC tools 
more and also performed better on a test measur-
ing linguistic abilities after they used STANDUP 
for ten weeks. 
2
3 Our Architecture 
Our goal is help AAC users engage in com-
plex social interaction by using NLG and data-
to-text technology to create potential utterances 
and conversational contributions for the users. 
The general architecture is shown in Figure 1, 
and Sections 4 and 5 describe two systems based 
on this architecture. 
 
 
The system has the following components: 
Data analysis: read in data, from sensors, 
web information sources, databases, and so forth.  
This module analyses this data and identifies 
messages (in the sense of Reiter and Dale 
(2000)) that the user is likely to want to commu-
nicate; this analysis is partially based on domain, 
conversation, and user models, which may be 
represented as ontologies. 
Editing: allow the user to edit the messages.  
Editing ranges from adding simple annotations to 
specify opinions (e.g., add BAD to Arsenal beat 
Chelsea 2-1 if the user is a Chelsea fan), to using 
an on-screen keyboard to type free-text com-
ments.  Users can also delete messages, specify 
which messages they are most likely to want to 
utter, and create new messages.  Editing is done 
before the actual conversation, so the user does 
not have to do this under time pressure.  The 
amount of editing which can be done partially 
depends on the extent of the user?s disabilities. 
Narration: allows the user to select mes-
sages, and perhaps conversational moves (e.g., 
Hello), in an actual conversational context.  Edit-
ing is possible, but is limited by the need to keep 
the conversation flowing. 
NLG and Speech Synthesis: Generates actual 
utterances from the selected messages, taking 
into account linguistic context, especially a dia-
logue model. 
4 Narrative for Children: How was 
School Today 
The goal of the How was School Today project is 
to enable non-speaking children with major mo-
tor disabilities but reasonable cognitive skills to 
tell a story about what they did at school during 
the day.  The particular children we are working 
with have cerebral palsy, and use wheelchairs.  A 
few of them can use touch screens, but most of 
them use a head switch and scanning interface, 
as described above.  By ?story?, we mean some-
thing similar to Labov?s (1972) conversational 
narrative, i.e., a series of linked real-world events 
which are unusual or otherwise interesting, pos-
sibly annotated with information about the 
child?s feelings, which can be narrated orally. 
We are not expecting stories in the literary sense, 
with character development and complex plots. 
The motivation of the project is to provide the 
children with successful narrative experience. 
Typically developing children develop narrative 
skills from an early age with adults scaffolding 
conversations to elicit narrative, e.g. ?What did 
you do at school today?? (Bruner, 1975). As the 
child?s vocabulary and language competence 
develops, scaffolding is reduced. This progres-
sion is seldom seen in children with complex 
communication needs ? they respond to closed 
questions but seldom take control of conversa-
Sensor 
data 
Web info 
sources 
Other 
external data 
Data analysis: 
select possible 
messages to 
communicate 
Conversation 
model 
Domain model 
User model 
Editing: User adds 
annotations 
User 
 
NLG: 
Generate 
utterance 
Dialogue 
model 
Speech 
synthesis 
Conversation 
partner 
 
Narration: User 
selects what to say 
Prepare content 
Narrate content 
Figure 1:  General architecture 
3
tion (von Tetzchner and Grove, 2003).  Many 
children who use AAC have very limited narra-
tive skills (Soto et al 2006). Research has shown 
that providing children who use AAC with suc-
cessful narrative experiences by providing full 
narrative text can help the development of writ-
ten and spoken narrative skills  (Waller, 2008).  
The system follows the architecture described 
above.  Input data comes from RFID sensors that 
track where the child went during the day; an 
RFID reader is mounted on the child?s wheel-
chair, and RFID tags are placed around the 
school, especially in doorways so we can moni-
tor children entering and leaving rooms.  Teach-
ers have also been given RFID swipe cards 
which they can swipe against a reader, to record 
that they are interacting with the child; this is 
more robust than attempting to infer interaction 
automatically by tracking teachers? position. 
Teachers can also record interactions with ob-
jects (toys, musical instruments, etc), by using 
special swipe cards associated with these objects. 
Last but not least, teachers can record spoken 
messages about what happened during the day. 
An example of how the child?s wheelchair is set 
up is shown in Figure 2. 
   
 
 
Figure 2: System configuration 
 
The data analysis module combines sensor-
derived location and interaction data with a time-
table which records what the child was expected 
to do during the day, and a domain knowledge 
base which includes information about typical 
activities (e.g., if the child?s location is Swim-
mingPool, the child?s activity is probably 
Swimming).  From this it creates a series of 
events (each of which contain a number of mes-
sages) which describe the child?s lessons and 
activities, including divergences from what is 
expected in the timetable.  Several messages may 
be associated with an event.  The data analysis 
module also infers which events and messages it 
believes are most interesting to the child; this is 
partially based on heuristics about what children 
are interested in (e.g., swimming is more inter-
esting than lunch), and partially based on the 
general principle that unexpected things (diver-
gences from the timetable) are more interesting 
than expected things.  No more than five events 
are flagged as interesting, and only these events 
are shown in the editing interface. 
The editing interface allows children to re-
move events they do not want to talk about (per-
haps for privacy reasons) from the list of interest-
ing events.  It also allows children to add mes-
sages that express simple opinions about events; 
i.e., I liked it or I didn?t like it.  The interface is 
designed to be used with a scanning interface, 
and is based on symbols that represent events, 
annotations, etc. 
The narration interface, shown in Figure 3, is 
similar to the editing interface. It allows children 
to choose a specific event to communicate, 
which must be one of the ones they selected dur-
ing the editing phase.  Children are encouraged 
to tell events in temporal order (this is one of the 
narration skills we are trying to teach), but this is 
not mandated, and they can deviate from tempo-
ral order if they wish.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    Figure 3: Narration Interface 
 
The NLG system generates actual texts from 
the events selected by the children.  Most of this 
Tablet PC with NLG system and 
swipe-card RFID sensor  
long range 
RFID  
sensor for 
location 
tracking 
Events 
Opinion Annotations 
Messages  
for event 
4
is fairly simple, since the system deliberately 
uses simple ?child-like? language (Section 6).  
However, the system does need to make some 
decisions based on discourse context, including 
choosing appropriate referring expressions (es-
pecially pronouns), and temporal expressions 
(especially when children deviate from pure 
temporal order). 
4.1 Example 
For example, assume that the timetable speci-
fies the following information 
 
 
Assume that the sensors then recorded the fol-
lowing information 
 
Event 1 
      Location: CL_SEC2 
      Time: 13:23:00.0 - 14:07:00.0 
      Interactions: Mrs. Smith, Rolf, Ross 
 
Event 2 
      Location: HALL 
      Time: 14:10:00.0 ? 14:39:00.0 
      Interactions: none 
 
The data analysis module associates Event 1 with 
the Arts and Crafts timetable entry, since the lo-
cation is right, the timetabled teacher is present, 
and the times approximately match.  From this 
two messages are produced: one corresponding 
to I had Arts and Crafts this afternoon with Mrs. 
Smith (the core activity description), and the oth-
er corresponding to Rolf and Ross were there 
(additional information about people not time-
tabled to be there).  The child can add opinions 
using the editing interface; for example, if he 
added a positive annotation to the event, this 
would become an additional message corre-
sponding to It was great. 
For Event 2, the data analysis module notes 
that it does not match a timetabled event. The 
timetable indicates the child should be at Physio-
therapy after Art and Crafts; however, the sensor 
information indicates they were in the hall. The 
system generates a single message corresponding 
to Then I went to the Hall instead of Physiother-
apy to describe this event.  If the child added a 
negative annotation to this message, this would 
become an additional message expressed as I 
didn?t like it. 
4.2 Evaluation 
We conducted an initial evaluation of the How 
was School Today system in January, 2009.  
Two children used the system for four days: Ju-
lie, age 11, who had good cognitive skills but 
was non-verbal because of severe motor impair-
ments; and Jessica, age 13, who had less severe 
motor impairments but who had some cognitive 
and memory impairments (these are not the chil-
drens? real names).  Julie used the system as a 
communication and interaction aid, as described 
above; Jessica used the system partially as a 
memory aid.  The evaluation was primarily 
qualitative: we observed how Julie and Jessica 
used the system, and interviewed their teachers, 
speech therapists, care assistants, and Julie?s 
mother (Jessica?s parents were not available). 
The system worked very well for Julie; she 
learned it quickly, and was able to use it to have 
real conversations about her day with adults, al-
most for the first time in her life.  This validated 
our vision that our technology could help AAC 
users engage in real interaction, and go beyond 
simple question answering and communication 
of basic needs.  The system also worked rea-
sonably well as a memory aid for Jessica, but she 
had a harder time using it, perhaps because of her 
cognitive impairments. 
Staff and Julie?s mother were very supportive 
and pleased with the system.  They had sugges-
tions for improving the system, including a wider 
range of annotations; more phrases about the 
conversation itself, such as Guess what happened 
at school today; and allowing children to request 
teenager language (e.g., really cool). 
From a technical perspective, the system 
worked well overall.   School staff were happy to 
use the swipe cards, which worked well.  There 
were some problems with the location sensors, 
we need better techniques for distinguishing real 
readings from noise.  A surprising amount of 
effort was needed to enter up-to-date knowledge 
(e.g., daily lunch menus), this would need to be 
addressed if the system was used for a period of 
months as opposed to days. 
5 Social Conversation for Adults 
In our second project, we want to build a tool to 
help adults with cerebral palsy engage in social 
conversation about a football match, movie, 
weather, and so forth.  Many people with severe 
disabilities have great difficulty developing new 
interpersonal relationships, and indeed report that 
forming new relationships and taking part in new 
Time Activity Location Teacher 
?? ?? ?? ?? 
13.20 -14 Arts and 
Crafts 
CL_SEC2 Mrs Smith 
14 -14.40 Physiotherapy PHYSIO1 Mrs Jones 
?? ?? ?? ?? 
5
activities are major priorities in their lives (Datil-
lo et al, 2007).  Supporting these goals through 
the development of appropriate technologies is 
important as it could lead to improved social out-
comes. 
This project builds on the TALK system 
(Todman and Alm, 2003), which helped AAC 
users engage in active social conversation. 
TALK partially overcame the problem of low 
communication rate by requiring users to pre-
author their conversational material ahead of 
time, so that when it was needed it could simply 
be selected and output. TALK also used insights 
from Conversation Analysis (Sacks, 1995) to 
provide appropriate functionality in the system 
for social conversation. For example, it sup-
ported opening and closing statements, stepwise 
topic change, and the use of quick-fire utterances 
to provide fast, idiomatic responses to commonly 
encountered situations. This approach led to 
more dynamic AAC-facilitated interactions with 
higher communication rates, and had a positive 
impact on the perceived communicative compe-
tence of the user (Todman, Alm et al, 2007).   
TALK requires the user to spend a substantial 
amount of time pre-authoring material; this is 
perhaps its greatest weakness.  Our idea is to re-
duce the amount of pre-authoring needed, by us-
ing the architecture shown in Fig 1, where much 
of the material is automatically created from data 
sources, ontologies, etc, and the user?s role is 
largely to edit and annotate this material, not to 
create it from scratch. 
We developed an initial prototype system to 
demonstrate this concept in the domain of foot-
ball results (Dempster, 2008).  We are now 
working on another prototype, whose goal is to 
support social conversations about movies, mu-
sic, television shows, etc (which is a much 
broader domain than football).  We have created 
an ontology which can describe events such as 
watching a film, listening to a music track, or 
reading a book.  Each ?event? has both temporal 
and spatial properties which allow descriptions to 
be produced about where and when an event took 
place, and other particulars relating to that par-
ticular class of event.  For example, if the user 
listened to a radio show, we record the name of 
the show, the presenter and the station it was 
broadcast on.  Ultimately we plan to obtain in-
formation about movies, music tracks, etc from 
web-based databases such as IMDB (movies) 
and last.fm (music). 
Of course, databases such as IMDB do not 
contain information such as what the user 
thought of the movie, or who he saw it with.  
Hence we will allow users to add annotations 
with such information.  Some of these annota-
tions will be entered via a structured tool, such as 
a calendar interface that allows users to specify 
when they watched or listened to something. We 
would like to use NaturalOWL (Galanis and An-
droutsopoulos, 2007) as the NLG component of 
the system; it is well suited to describing objects, 
and is intended to be integrated with an ontology.  
As with the How Was School Today project, 
some of the main low-level NLG challenges are 
choosing appropriate referring expressions and 
temporal references, based on the current dis-
course context.  Speech output is done using Ce-
reproc (Aylett and Pidcock, 2007). 
An example of our current narration interface 
is shown in Figure 4.  In the editing interface, the 
user has specified that he went to a concert at 
8pm on Thursday, and that he rated it 8 out of 
10.  The narration interface gives the user a 
choice of a number of messages based on this 
information, together with some standard mes-
sages such as Thanks and Agree. 
 
 
 
Note that unlike the How Was School Today 
project, in this project we do not attempt to infer 
event information from sensors, but we allow 
(and expect) the user to enter much more infor-
mation at the editing stage.  We could in princi-
ple use sensors to pick up some information, 
such as the fact that the user was in the cinema 
from 12 to 2PM on Tuesday, but this is not the 
research focus of this project. 
We plan to evaluate the system using groups 
of both disabled and non-disabled users.  This 
has been shown in the past to be an effective ap-
proach for the evaluation of prototype AAC sys-
tems (Higginbotham, 1995). Initially pairs of 
non-disabled participants will be asked to pro-
duce short conversations with one person using 
the prototype and the other conversing normally.   
Quantitative measures of the communication rate 
6
will be taken as well as more qualitative observa-
tions relating to the usability of the system.  Af-
ter this evaluation we will improve the system 
based on our findings, and then conduct a final 
evaluation with a small group of AAC users. 
6 Discussion: Challenges for NLG 
From an NLG perspective, generating AAC texts 
of the sort we describe here presents different 
challenges from many other NLG applications. 
First of all, realization and even microplanning 
are probably not difficult, because in this context 
the AAC system should generate short simple 
sentences if possible.  This is because the system 
is speaking ?for? someone with limited or devel-
oping linguistic abilities, and it should try to pro-
duce something similar to what the user would 
say himself if he or she had the time to explicitly 
write a text using an on-screen keyboard. 
To take a concrete example, we had originally 
considered using past-perfect tense (a fairly 
complex linguistic construct) in the How was 
School project, when the narrative jumped to an 
earlier point in time.  For example I ate lunch at 
12.  I had gone swimming at 11.  But it was clear 
from corpora of child-written texts that these 
children never used perfect tenses, so instead we 
opted for I ate lunch at 12.  I went swimming at 
11.  This is less linguistically polished, but much 
more in line with what the children might actu-
ally produce. 
Given this desire for linguistic simplicity, re-
alisation is very simple, as is lexical choice (use 
simple words) and aggregation (keep sentences 
short).  The main microplanning challenges re-
late to discourse coherence, in particular refer-
ring expressions and temporal descriptions.   
On the other hand, there are major challenges 
in document planning.  In particular, in the How 
Was School project, we want the output to be a 
proper narrative, in the sense of Labov (1972).  
That is, not just a list of facts and events, but a 
structure with a beginning and end, and with ex-
planatory and other links between components 
(e.g., I had math in the afternoon because we 
went swimming in the morning, if the child nor-
mally has math in the morning).  We also wanted 
the narrative to be interesting and hold the inter-
est of the person the child is communicating 
with.  As pointed out by Reiter et al(2008), cur-
rent NLG systems do not do a good job of gener-
ating narratives.  
Similarly, in the Social Conversations project 
we want the system to generate a social dialogue, 
not just a list of facts about movies and songs.  
Little previous research has been done on gener-
ating social (as opposed to task-oriented) dia-
logues.  One exception is the NECA Socialite 
system (van Deemter et al 2008), but this fo-
cused on techniques for expressing affect, not on 
high-level conversational structure. 
For both stories and social conversations, it 
would be extremely useful to be able to monitor 
what the conversational partner is saying.  This is 
something we hope to investigate in the future.  
As most AAC users interact with a small number 
of conversational partners, it may be feasible to 
use a speech dictation system to detect at least 
some of what the conversational partner says. 
Last but not least, a major challenge implicit 
in our systems and indeed in the general architec-
ture is letting users control the NLG system.   
Our systems are intended to be speaking aids, 
ideally they should produce the same utterances 
as the user would if he was able to talk.  This 
means that users must be able to control the sys-
tems, so that it does what they want it to do, in 
terms of both content and expression.  To the 
best of our knowledge, little is known about how 
users can best control an NLG system. 
7 Conclusion 
Many people are in the unfortunate position of 
not being able to speak or type, due to cognitive 
and/or motor impairments.  Current AAC tools 
allow such people to engage in simple needs-
based communication, but they do not provide 
good support for richer use of language, such as 
story-telling and social conversation.  We are 
trying to develop more sophisticated AAC tools 
which support such interactions, by using exter-
nal data and knowledge sources to produce can-
didate messages, which can be expressed using 
NLG and speech synthesis technology.  Our 
work is still at an early stage, but we believe that 
it has the potential to help AAC users engage in 
richer interactions with other people.  
Acknowledgements 
We are very grateful to Julie, Jessica, and their 
teachers, therapists, carers, and parents for their 
help in building and evaluating the system de-
scribed in Section 4.  Many thanks to the anony-
mous referees and our colleagues at Aberdeen 
and Dundee for their very helpful comments.  
This research is supported by EPSRC grants 
EP/F067151/1 and EP/F066880/1, and by a 
Northern Research Partnership studentship. 
7
References 
Aylett, M. and C. Pidcock (2007). The CereVoice 
Characterful Speech Synthesiser SDK. Proceed-
ings of Proceedings of the 7th International Con-
ference on Intelligent Virtual Agents, pages 413-
414. 
Bruner, J. (1975). From communication to language: 
A psychological perspective. Cognition 3: 255-
289. 
Datillo, J., G. Estrella, L. Estrella, J. Light, D. 
McNaughton and M. Seabury (2007). "I have cho-
sen to live life abundantly": Perceptions of leisure 
by adults who use Augmentative and Alternative 
Communication. Augmentative & Alternative 
Communication 24(1): 16-28. 
van Deemter, K., B Krenn, P Piwek, M Klesen, M 
Schr?der and S Baumann. Fully generated scripted 
dialogue for embodied agents. Artificial Intelli-
gence 172: 1219?1244. 
Dempster, M. (2008). Using natural language genera-
tion to encourage effective communication in non-
speaking people. Proceedings of Young Research-
ers Consortium, ICCHP'08. 
Dye, R., N. Alm, J. Arnott, G. Harper, and A. Morri-
son (1998). A script-based AAC system for trans-
actional interaction.  Natural Language Engineer-
ing, 4(1), 57-71. 
Galanis, D. and I. Androutsopoulos (2007). Generat-
ing Multilingual Descriptions from Linguistically 
Annotated OWL Ontologies: the NaturalOWL Sys-
tem. Proceedings of ENLG 2007. 
Higginbotham, D. J. (1995). Use of nondisabled sub-
jects in AAC Research : Confessions of a research 
infidel. Augmentative and Alternative Communica-
tion 11(1): 2-5. 
Labov, W (1972).  Language in the Inner City. Uni-
versity of Pennsylvania Press. 
Manurung, R., G. Ritchie, H. Pain, A. Waller, D. 
O'Mara and R. Black (2008). The Construction of a 
Pun Generator for Language Skills Development. 
Applied Artificial Intelligence 22(9): 841 ? 869. 
McCoy, K., C. Pennington and A. Badman (1998). 
Compansion: From research prototype to practical 
integration. Natural Language Engineering 4:73-
95. 
Netzer, Y and Elhadad, M (2006). Using Semantic 
Authoring for Blissymbols Communication 
Boards. In Proc of HLT-2006. 
Newell, A., S. Langer and M. Hickey (1998). The role 
of natural language processing in alternative and 
augmentative communication. Natural Language 
Engineering 4:1-16. 
Polkinghorne, D. (1991). Narrative and self-concept. 
Journal of Narrative and Life History, 1(2/3), 135-
153 
Reiter, E (2007). An Architecture for Data-to-Text 
Systems. In Proceedings of ENLG-2007, pages 
147-155. 
Reiter, E. and R. Dale (2000).  Building Natural Lan-
guage Generation Systems.  Cambridge University 
Press. 
Reiter, E,  A. Gatt, F Portet, and M van der Meulen 
(2008). The Importance of Narrative and Other 
Lessons from an Evaluation of an NLG System 
that Summarises Clinical Data (2007). In Proceed-
ings of INLG-2008, pages 97-104. 
Sacks, H. (1995). Lectures on Conversation. G. Jef-
ferson. Cambridge, MA, Blackwell. 
Schank, R. C. (1990). Tell me a story: A new look at 
real and artificial intelligence. New York, Macmil-
lan Publishing Co. 
Schank, R., and R. Abelson (1977).  Scripts, plans, 
goals, and understanding. New Jersey: Lawrence 
Erlbaum. 
Soto, G., E. Hartmann, and D. Wilkins (2006). Ex-
ploring the Elements of Narrative that Emerge in 
the Interactions between an 8-Year-Old Child who 
uses an AAC Device and her Teacher. Augmenta-
tive and Alternative Communication 4:231 ? 241. 
Todman, J. and N. A. Alm (2003). Modelling conver-
sational pragmatics in communication aids. Jour-
nal of Pragmatics 35: 523-538. 
Todman, J., N. A. Alm, D. J. Higginbotham and P. 
File (2007). Whole Utterance Approaches in AAC. 
Augmentative and Alternative Communication 
24(3): 235-254. 
von Tetzchner, S. and N. Grove (2003). The devel-
opment of alternative language forms. In S. von 
Tetzchner and N. Grove (eds), Augmentative and 
Alternative Communication: Developmental Issues, 
pages 1-27. Wiley. 
Waller, A. (2006). Communication Access to Conver-
sational Narrative. Topics in Language Disorders 
26(3): 221-239. 
Waller, A. (2008). Narrative-based Augmentative and 
Alternative Communication: From transactional to 
interactional conversation. Proceedings of ISAAC 
2008, pages 149-160.  
Waller, A., R. Black, D. A. O'Mara, H. Pain, G. Rit-
chie and R. Manurung (In Press). Evaluating the 
STANDUP Pun Generating Software with Chil-
dren with Cerebral Palsy. ACM Transactions on 
Accessible Computing. 
8
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using NLG and Sensors to Support Personal Narrative for 
Children with Complex Communication Needs 
 
 
Rolf Black Joe Reddington, Ehud Reiter, Nava Tintarev Annalu Waller 
School of Computing Department of Computing Science School of Computing 
University of Dundee            University of Aberdeen University of Dundee            
rolfblack@ 
computing.dundee.ac.uk 
{j.reddington, e.reiter  n.tintarev}@abdn.ac.uk awaller@ 
computing.dundee.ac.uk 
 
 
Abstract 
We are building a tool that helps children with 
Complex Communication Needs1 (CCN) to 
create stories about their day at school. The 
tool uses Natural Language Generation (NLG) 
technology to create a draft story based on 
sensor data of the child?s activities, which the 
child can edit. This work is still in its early 
stages, but we believe it has great potential to 
support interactive personal narrative which is 
not well supported by current Augmentative 
and Alternative Communication (AAC) tools. 
1 Introduction 
Many tools have been developed to help children and 
adults who cannot speak (or who have limited speech) 
communicate better.  However, most of these tools have 
focused on supporting communication for practical 
goals, such as ?I am thirsty.? But human communica-
tion is also used for social goals; we develop friendships 
and other inter-personal relationships via social interac-
tion and communication. The bulk of conversation is 
characterized by free narrative (Cheepen 1988). One of 
the most important types of conversational narrative is 
personal narrative: someone telling a story about what 
happened to him or her. 
 People with limited or no functional speech do tell 
stories, but these tend to be in monologue form, or in a 
sequence of pre-stored utterances on voice output com-
munication aids (Waller 2006). Individuals who use 
                                                          
1
 The term Complex Communication Needs (CCN) describes 
individuals who, due to motor, language, cognitive, and/or 
sensory perceptual impairments (e.g., as a result of cerebral 
palsy), do not develop speech and language skills as expected. 
This heterogeneous group typically experiences restricted 
access to the environment, limited interactions with their 
communication partners, and few opportunities for communi-
cation (Light and Drager 2007). 
Augmentative and Alternative Communication (AAC) 
tools tend to be passive, responding to questions with 
single words or short sentences (e.g. Soto, Hartmann et 
al. 2006) and if able to initiate and maintain extended 
conversations tend to relate experience word for word 
each time they tell a story, even though much of conver-
sation is reused (Clarke and Clarke 1977). This is time 
consuming and physically exhausting ? typical rates 
range from 8 to 10 words per minute up to 12 to 15 per 
minute when techniques such as word prediction are 
used (Higginbotham, Shane et al 2007), with the result 
that people seldom engage in storytelling. Despite the 
importance of narrative, little work has been done on 
specific tools to help language-impaired individuals 
engage in personal storytelling. In this paper, we de-
scribe our work in progress on building a tool that uses 
Natural Language Generation (NLG) technology to help 
children tell stories about their day at school, describing 
both the work we have done to date, and the challenges 
that we face in further developing this concept. 
2 Background 
2.1 AAC 
Technology underpins much of Augmentative and Al-
ternative Communication (AAC), a field that attempts to 
augment natural speech and provides alternative ways to 
communicate for people with limited or no speech. At 
the simplest level, people with Complex Communica-
tion Needs (CCN) can cause a pre-stored message to be 
spoken by activating a single switch. At the most so-
phisticated level, literate users can generate novel text 
using input methods ranging from a single switch to a 
full keyboard.  
Despite advances in AAC, there are still many indi-
viduals for whom communication remains problematic. 
Although some individuals with CCN become effective 
communicators, most do not ? they tend to be passive 
communicators, responding mainly to questions or 
prompts at a one or two word level. Conversational 
1
skills such as initiation, elaboration and storytelling are 
seldom observed (Waller 2006). 
One reason for the reduced levels of communicative 
ability is the cognitive demands of AAC interfaces. Cur-
rent AAC technology provides the user with a purely 
physical link to speech output. The user is required to 
have sufficient cognitive abilities and physical stamina 
to translate what they want to say into the sequence of 
operations needed to produce the desired output. Mne-
monic codes and dynamic displays (Beukelman and 
Mirenda 2005) provide some help in the retrieval 
process, but users still have to master complex retrieval 
and production strategies.  
A second reason for the impoverished quality of 
conversation is the focus of AAC devices on transac-
tional communication; conversation which expresses 
needs wants and information transfer, for example, ?I 
am thirsty?, ?I use a straw for drinking?. Instead, inter-
active conversation is characterized by free narrative 
and phatic conversation, for example, ?Guess what 
happened this morning??, ?Hello?, and ?How are 
you?? Without easy access to extended interactive 
communication, it is difficult to develop the skills 
needed to initiate new topics and engage in storytelling.  
2.2 Importance of Narrative 
 Conversational narratives (oral stories told during 
interactive conversations) are crucial to social engage-
ment. Narratives provide a means for people to relate 
and share experiences, develop organizational skills, 
work through problems, develop self image, express 
personality, give form and meaning to life, and allow 
people to be interesting entertainers (Waller 2006).  
 Narrative skills develop experientially with children 
being able to engage in storytelling even before they are 
verbal (Bruner 1975). Early personal experience stories 
consist of a high point, for example, ?Mummy fall!? 
with adults scaffolding the full story, eliciting the ?who?, 
?what?, ?when? and ?where?. However, not all expe-
riences make good stories. An experience becomes a 
story if the storyteller has an emotional connection to 
the event (Labov, 1972), or if the event is unusual (Qua-
sthoff & Nikolaus, 1982). 
 Parents of typically developing children encourage 
development of narrative skills by eliciting stories from 
their children (Peterson and McCabe 1983), but the de-
velopment of narrative skills is problematic for people 
with CCN. We recall a study where disabled children 
were told different stories more often than typically 
developing peers who were read the same story night 
after night (Light, Binger et al 1994). In doing so, the 
disabled children did not have the chance to learn the 
sequence of stories, or the structure commonly used in 
narrative such as beginning, middle and end. As such, 
initially children should use the same story template 
consistently until they are ready to progress to another 
one. 
 It is difficult to provide access to event information 
which may become a story, and few AAC systems pro-
vide support for interactive story narration. However 
NLG gives us a possibility to change the underlying 
paradigm of AAC. Instead of placing the entire cogni-
tive load on the user, AAC devices can be designed to 
support the retrieval of story events and the scaffolding 
of story narration for individuals with CCN. 
2.3 NLG, Data-to-text 
NLG systems generate texts in English (or other human 
languages) from non-linguistic data (Reiter and Dale 
2000).  Our vision is to use an NLG system to generate 
a draft story, which the child can edit.  The non-
linguistic input to our story-generator is sensor data 
about the child?s activities, including location data 
(where the child was) and interaction data (what people 
and objects the child interacted with).  We also want to 
allow teachers and school staff to enter information 
about the child?s activities (such as voice messages). 
 A number of data-to-text systems (Reiter 2007) have 
been developed in recent years, which generate English 
summaries of sensor and other numerical data.  The 
most popular application area has been weather fore-
casting (generating textual weather forecasts from the 
results of a numerical atmosphere simulation model), 
and indeed several weather forecast generators have 
been fielded and used operationally (Goldberg, Driedger 
et al 1994; Reiter, Sripada et al 2005). A number of 
data-to-text systems have also been developed in the 
medical community, such as BabyTalk (Gatt, Portet et 
al. 2009), which generates summaries of clinical data 
from a neonatal intensive care unit, and the commercial 
Narrative Engine (Harris 2008) which summarizes data 
acquired during a doctor/patient encounter. 
 Most previous research in data-to-text has focused 
on summarizing technical data for expert users, with the 
goal of effectively communicating key information.  In 
our work, in contrast, the focus is on summarizing data 
about everyday events, with the goal of having some-
thing interesting to talk about.  There has been consider-
able work in the computational creativity community on 
generating interesting stories (P?r?z and Sharples 2004), 
but it has focused on fictional written stories, where the 
computer system can say whatever it wishes, without 
the constraint of describing real events. 
 Most previous work in NLG has focused on com-
puter systems which generate texts without human in-
put.  However, in our case we want children to be able 
to annotate (evaluate) and edit stories, as far as their 
abilities permit.  There has been some research on hu-
man post-editing of NLG texts (Sripada, Reiter et al 
2005), but this has focused on editing at the text level.  
2
Since editing at the text level is very laborious for AAC 
users, we need a higher-level interface that lets children 
edit content and structure without needing to type 
words.  We also want children to be able to control how 
a story is narrated, perhaps in response to a listener?s 
questions or body language.  For example children may 
wish to add comments such as ?It was awesome!?, or 
tell events out of sequence. 
    In short, we need to develop interfaces and interac-
tion techniques that allow our users to control the NLG 
system.  Unfortunately there has been very little pre-
vious work on this topic, indeed almost nothing is 
known about Human-Computer Interaction aspects of 
NLG systems.  Developing a better understanding of 
these aspects is one of the main research challenges we 
face from an NLG perspective. 
3 Current and ongoing work 
3.1 ?How was School today??? 
We developed an initial version of ?How was School 
today??? in 2009; see Reiter et al(2009) for more de-
tails about this system. 
 
 
 
Fig. 1: Participating pupil with support worker:  
The prototype system is mounted on the wheelchair, and 
the pupil has access to the system via head switch con-
trolled row/column scanning. 
 
 This system used Radio Frequency Identification 
(RFID), an emerging application in AAC to identify or 
give access to relevant vocabulary (Bart, Riny et al 
2008; DeRuyter and Fried-Oken 2010). Sensors were 
used to track both location (by putting tags on doors, 
which were automatically sensed by a long-range RFID 
reader) and interaction (by asking staff to manually 
swipe RFID cards in a short-range reader when the child 
interacted with a person or object). Staff could also 
record spoken messages about interesting events during 
the day (see Fig. 1). 
The software analyzed this data to remove sensor noise, 
and then compared it to a timetable which specified 
where children were supposed to be, what they were 
supposed to be doing and which teacher was supposed 
to be taking the class throughout the day.  This allowed 
the software to both fill in missing information, and to 
identify divergences from the schedule. The result of 
this process was a series of events (which corresponded 
to classes, for example, maths class), each of which had 
a set of associated messages (interactions during the 
event, divergence from schedule, etc.). 
 After the data analysis was completed, an NLG sys-
tem identified the events most interesting (to the child), 
using a heuristic that took into consideration both how 
inherent interesting an event was (for example, lunch 
was regarded as an inherently interesting event that 
children were likely to want to talk about) and also 
whether an event was unusual or not.  The latter is based 
on the observation that most personal narratives focus 
on unexpected or unusual events.  Unusual events were 
identified by the presence of recorded voice messages 
and by divergence from the timetable, e.g. a different 
teacher present or a different location. The system se-
lected the five most ?interesting? events and displayed 
them to the child in a simple visual editing interface.  In 
this interface the child could delete events he/she did 
not wish to talk about, and also annotate events with 
simple opinions (evaluations), such as I liked it, using 
the evaluation buttons on the interface, generating ap-
propriate utterances according to the last narrated event 
or message.(see Fig. 2).  
     When editing was finished, the NLG system generat-
ed texts describing the events and messages, which the 
child communicated using a simple narration interface 
(which was similar to the editing interface). Emphasis 
was placed on providing quick access to messages to 
minimize the length of pauses between utterances due to 
the physical accessing difficulties of the users. The 
narrative model is based on the Labov social narrative 
model (Labov 1972) which emphasizes the highpoint 
and evaluation. The dialogue model from beginning 
through to highpoint to the end with the user being able 
to add evaluations at any point of the narration. Stories 
are initially chronological order but interactively under 
the control of the user. This control of narration differs 
significantly from current AAC interventions where 
narrative tends to be output in a monologue format.  
 From an NLG perspective, the system was fairly 
straightforward. The most challenging microplanning 
tasks were choosing connectives, time phrases, lexical 
variety in embellishments, and pronouns based on dis-
course context. Connectives and time phrases were ne-
cessary since children could narrate events in different 
orders (for example, ?I went to maths.  Then I went to 
lunch? versus ?I went to lunch.  Before lunch, I went to 
maths?).  Document structuring  was simple because we 
Short range RFID reader 
and microphone for voice 
message recording 
Visual interface 
Access switch in head 
rest 
Long range RFID reader 
3
assumed that the children would choose their own order 
in which to narrate events. In fact some children are not 
able to do this; such children would need to be sup-
ported by a more sophisticated document planner that 
had a model of appropriate text structures in this do-
main. 
 We asked two children to use the system for one 
week for a qualitative formative evaluation.  Research-
ers supplied ongoing support during this, primarily trial 
observing how the children used the system, and dis-
cussing it with teachers, therapists and parents.  Gener-
ally it worked well for one child, Julie2, who had severe 
motor impairment (no independent means of mobility 
and interacted with a computer using a head switch with 
row/column scanning, see Fig. 1). Her expressive abili-
ties were limited but her comprehension skills were 
comparable to her non-disabled peers with some deve-
lopmental delay. The other child, Jessica, had more 
cognitive impairment, and found the interface too diffi-
cult. 
 
 
 
 
Fig. 2: Example screenshot from interface 
1: Navigation: Day and date of story, maximum of five 
story events, exit; 2: Event messages, numbers vary for 
each event. Here: 2 computer-generated messages, 3 
recorded messages, 1 user added evaluation; 
3: Sequential message navigation: previous, repeat, 
next; 4: Evaluation: delete event, negative evaluation, 
positive evaluation; 
 
   
 In a second evaluation, a third child, Eric, joined and 
all three children used the system over two weeks each. 
In this evaluation, we asked teachers and other staff to 
use the system without on-site support from the re-
searchers. This highlighted many practical usability 
issues, such as delays caused by starting the system in 
the morning, and problems caused by limited battery 
life. We eliminated the long-range RFID sensor because 
of its difficult setup; instead we asked staff to swipe 
                                                          
2
 The names of the children mentioned in this paper are 
changed to ensure anonymity. 
door cards when children entered rooms.  However, this 
strategy was not successful, as it was difficult for staff 
to remember to swipe both interactions and location 
changes. 
 The participants took the system home for use with 
their parents who gave positive feedback but also re-
ported issues with system usability (e.g. lack of access 
to stories from previous days) or suitability (too compli-
cated interface for Jessica).   
 Eric?s timetable was different from Julie and Jessi-
ca?s, because he visited college one morning a day, and 
we could not collect data during this period. Since some 
of the most exciting events in a school day happen out-
side the school building (sports and school trips as well 
as college), in the long term we do need to see if we can 
collect data outside as well as inside the school.  
3.2 HWST example 
     Julie used the system on her DynaVox? Vmax? 
Voice Output Communication Aid (VOCA) via head 
switch using row/column scanning. The above transcript 
shows an extract of a conversation Julie had with her 
Speech and Language Therapist (SLT) on day three 
about her experiences during day two. The researcher 
(RA) had been present all day for technical support. The 
conversation extract starts with Julie reporting about her 
morning break.  
     In this example Julie is able to quickly reply to con-
text related questions from her communication partner 
using the evaluations (?So what happened?? ? ?It was 
fun!?). Compared to conversations usually observed 
between aided and unaided partners Julie is able to con-
trol the conversation when starting a new topic after 
talking about the morning break, inviting her communi-
cation partner to prompt for more detailed information. 
Julie provides this with her next generated phrase. 
When she is asked about the event she replies with an 
evaluation the system has generated in relation to its 
previously generated message ?A visitor was there.?. 
We note that the system is able to refer to the correct 
gender of the visitor. 
 
1 Julie {next} [I had break.] 
2 Julie {next} [Lesley was there.] 
3 SLT Lesley was there?  
4 Julie ((Opens mouth in agreement, then turns back 
to screen)) 
5 SLT Ok mhmh. So what happened? 
6 Julie {positive evaluation} [It was fun.] 
7 SLT Oh good! ((laughs)) I?m glad to hear it! 
8 RA We like Lesley. 
9 SLT ((nods in direction of RA)) 
10 Julie ((smiles)) 
11 Julie:  {next} [Then I went to Junior Primary in-
stead of Reading Class.] 
1 
2 
3 
4 
4
12 SLT: Right, you went to Junior Primary? I wonder 
why that was?  
13 Julie: {next} [A visitor was there.] 
14 SLT:  Oh, a visitor, right. Wonder what the visitor 
was doing?  
15 Julie: {next} [?The dental hygienist came to give a 
talk.?] 
16 SLT: Oh, dental hygienist. 
17 Julie:  {previous} [A visitor was there.] 
18 SLT: That was the visitor, okay. That?s why you 
went to junior primary, uhm, what did you 
think of the talk? 
19 Julie:  {positive evaluation} [She was nice.] 
20 SLT: She was nice, that was good! ((laughs)) 
 
Notation: 
- Switch selected button by Julie: {curly brackets} 
- Natural speech: standard text.  
- Computer generated language accessed using one 
button: [standard text in square brackets].  
- Recorded messages accessed using one button: 
[?quoted standard text in square brackets?]. 
- Paralinguistic behaviors:  
((standard text in double brackets)).  
 
3.3  ?How was School Today? ? in the Wild 
We have now started a new project to further develop 
our work, called ?How was School today??? ? in the 
Wild (?in the wild? indicates that the focus is on how the 
technology works in a real school environment). The 
basic goal is to improve the system sufficiently so that it 
can be tried out over a period of several months, with 
children with varying levels and types of impairments; 
we will also work with several schools in the initial 
phase, although for practical reasons the evaluations 
may be at just one school. 
     During this project we will do some work on the 
issues described in Section 4; in particular we will try to 
make the system usable by children with different im-
pairments and ability levels (Section 4.1). This means 
having a very simple interface for children with consi-
derable cognitive impairments (such as Jessica); but 
also giving children with more cognitive abilities the 
opportunity to exert more control over the story (during 
both editing and narration), for example by supporting a 
richer range of annotations, and by making it easier to 
describe events and messages in any order. 
     Another intermediate goal is to improve the integra-
tion of voice messages entered by staff with the com-
puter-generated messages. This could be done by some 
combination of training staff to enter messages in a spe-
cific way (referring to the child in the first person); ask-
ing staff to annotate the messages so the computer 
knows something about their content; and/or using 
speech recognition to analyze the voice messages. In 
general there is a lot of interesting information that can 
only come from staff, and we need to think about the 
best way to help staff enter information in a way that is 
easy for them and useful for our system. 
 Now that a complete system is built, we are also able 
to thoroughly and formally evaluate the system. Mul-
tiple baseline single case study methodology will be 
used (Schlosser 2003) to evaluate the use and impact of 
our system. We intend to have up to four children (with 
varying ability levels) use the system for a period of 3 
months. This will give us a chance to observe the im-
pact of the system on the users and their environment 
such as the children?s interaction with the system and 
how staff at the school envisage using this new tool. 
The observations will be supported by semi-structured 
interviews with the children, their classroom teacher, 
their speech and language therapist and a parent. 
 We will look at the children?s conversations (with 
and without using our system) about interesting, staged 
events with different partners, analyzing them conversa-
tional characteristics such as narrative initiation, struc-
ture, length and evaluation. Analysis methods will 
include the Revised Edinburgh Functional Communica-
tion Profile (REFCP) (Wirz, Skinner et al 1990). 
     However, much of our focus will be on addressing 
the practical issues that make it difficult to use our cur-
rent research prototype over a period of months.  We 
have identified many such issues, both from our pre-
vious evaluations (Section 3.1) and also from a ques-
tionnaire that was distributed to school staff during an 
in-service day. 
      Location tracking ? There are problems with both 
of the techniques we have tried to date (automatically 
reading RFID tags on doors, and asking staff to swipe 
location information).  In this project we intend to try 
tracking the location of a child using Wi-Fi location 
tracking, which seems to be rapidly gaining popularity 
in the commercial world (Liu, Sen et al 2008). 
 Data entry, 2D bar codes ? We need to allow staff 
to easily enter and update information about the children 
(for example, their timetables) and sensor tags (e.g., if a 
new tag is given to a visitor).  For the latter, we want to 
investigate 2D bar codes, which could allow encoding 
of alphanumeric input data without reference to a cen-
tral database. 
 Portability, battery life ? The current system runs on 
a tablet PC (8?-12? touch screen, generic or VOCA 
hardware). During the evaluation, late powering up, run-
down batteries or simply forgetting a component caused 
significant data loss and usability issues. A future proto-
type should favor an ?always-on? system, such as a mo-
bile phone, allowing for easy portability and extended 
battery life. 
 Story generation ? The prototype system was only 
able to create a story towards the end of a day and gave 
5
only access to stories generated on that day. However, 
often the user desired to tell stories that had occurred on 
previous days, or to, say, tell a story at lunch that oc-
curred in the morning.  When data was insufficient for 
the system to create a story, the only output was an error 
message ?Can?t generate story right now.? This fru-
strated users, so future systems should be able to deliver 
a story with incomplete data.  
 Voice messages ? as mentioned above, we want to 
handle these in a more sophisticated way.  From a more 
practical perspective, we also want to make it easier for 
staff to listen to and change previously recorded mes-
sages.  We also want to allow parents to record messag-
es about events at home. 
4 Long-term vision and issues 
4.1 Supporting children with different levels 
and types of impairment 
A key issue in AAC is of course the diversity of AAC 
users. Children with CCN differ enormously in terms of 
cognitive ability, motor ability, and social ability. This 
was clear even in our initial evaluation where we 
worked just with two children, and discovered that our 
interface worked well for Julie but not Jessica. 
 Julie has little functional speech and severe physical 
impairments, and accesses her VOCA using a head 
switch through the slow process of scanning the inter-
face. Her VOCA interface consists of a grid of 15 to 30 
buttons per page, with more than 20 pages of vocabu-
lary. However, her cognitive skills were sufficient for 
her to master the interface on the second day. She used 
the system quite successfully, as shown in the example 
in Section 3.2.  
 Jessica also has severe physical impairments but 
does not use technology to support her communication 
(she has functional speech).  She has cognitive impair-
ments, which (amongst other things) affect her ability to 
remember and place events correctly in time. She had 
more difficulty mastering the interface than Julie. We 
simplified the interface for her (no editing, minimal 
control of narration), and then she displayed pragmatics 
known from typical language development in children, 
by telling her story with no room for interaction of her 
communication partner.  
 We also need to keep in mind that abilities are not 
static, but are likely to progress with age (see also Sec-
tion 4.2) and (hopefully) with the assistance of commu-
nication aids. For example, the WriteTalk project 
showed how pupils were both able to initiate and con-
trol communication more effectively with Talk:About? 
and how their formal writing skills improved over time 
(Waller et al, 1999). 
 In summary, some children may need a very simple 
interface because of cognitive impairments, but this 
should grow with them.  For example, the best narration 
tool for Jessica at her current stage of development is 
probably a single button that advances sequentially 
through the computer-generated story. The challenge is 
to provide an interface that Jessica can initially use via 
repeatedly pressing an ?Advance? button, but which 
gives her the possibility of exerting more control as her 
skills and abilities develop. 
 Other children (such as Julie) may have motor diffi-
culties that restrict the way in which they can interact 
with computer systems, and thus may require simple 
controls although they have reasonable cognitive skills. 
Restricted motor skills make certain tasks, such as en-
tering an arbitrary word, quite difficult and time con-
suming; hence the interface must avoid such tasks, and 
instead endeavor to give the child as much control as 
possible with a minimum amount of data entry.  Once 
these users master a basic story telling structure, it may 
help them develop their conversation skills if they use a 
wide variety of conversation patterns.  For this purpose, 
it may be worthwhile for the system to randomly vary 
the structure and language used in the narratives. 
 Still other children, for example on the autistic spec-
trum, may have good cognitive and motor abilities, but 
not have the experience of expressive communication 
necessary to develop interactive skills. These children 
are more likely to benefit from a system that supports 
the pragmatics of language in general and personal narr-
ative in particular.  For example, children on the high 
functioning end of autism may be comfortable with ra-
ther advanced software, which can help them adapt their 
storytelling according to the intended listener.  Indeed, 
giving these children more complex controls, if done 
correctly, can make the software fun and challenging in 
a positive way. 
     In the long term, as we broaden the range of children 
we work with, there may be overlaps between our work 
and research on tools to help typically developing child-
ren create stories, such as Robertson and Good (2005), 
and also between our work and research on tools to help 
adults with CCN tell personal narratives, such as Demp-
ster (2008).  Ideally it would be very nice to combine 
these efforts and create a story telling tool that could be 
used across the age and impairment spectrum. 
4.2 Narrative across the lifespan 
We would like our tool to be able to support children 
over time, as their abilities grow and as their expe-
riences accumulate. From the perspective of changing 
abilities, the challenge is to offer children an interface 
which is not only appropriate for their current stage of 
development (Sect 4.1), but also allows and indeed en-
6
courages them to exert more control over story content, 
language, and narration as their abilities grow. 
 We would also like our tool to become a repository 
of a child?s personal stories. The ability to relate rele-
vant stories can influence the quality of life, as well as 
social development and successful transitions. The life 
stories of people who use AAC are often held by parents 
and siblings (e.g., stories relating to health care 
(Hemsley, Balandin et al 2007)), and there is the inevit-
able concern that these stories and others are lost as 
parents age and siblings move away.  
 Technology has the potential both to support the 
acquisition of conversational skills for people who use 
AAC and to provide a repository for life stories. In the 
context of our work, it is essential that we provide ways 
of enabling children to develop their narrative skills so 
that they are more able to manage their own story repo-
sitory. In terms of development, young children will 
narrate recent stories regardless of conversational con-
text. By enabling the child to develop story structure by 
scaffolding interaction and enabling children to easily 
annotate stories, the child will begin to anticipate and 
control conversation. 
 Conversational narratives have traditionally not been 
supported by AAC tools partly due to the fact that they 
are so nebulous; they emerge during interactive conver-
sation (to date, events have to be manually input into a 
system and it is difficult to predict what events will be-
come a story); ?new? stories are repeated often (to date it 
is difficult to save conversation online); as stories age 
they are repeated in context (retrieval is often contextual 
e.g. topic based) and they grow longer having more em-
bellishments added to them. The technology we are de-
veloping provides an opportunity for children to access 
information about personal events over time, which they 
can communicate and narrate during a conversation. 
They can also evaluate (annotate) their stories, thereby 
embellishing and lengthening the stories.  However this 
will only be possible if the children can easily access 
previously experienced, generated and saved stories.  
 We can provide fast access to recent stories while 
anticipating the use of older stories such as for example 
those which closely match the current conversation top-
ic. In a research prototype called PROSE (Waller and 
Newell 1997), stories had to be physically tagged; there 
is now the potential to automate topic matching by re-
cognizing topic words spoken by a listener and parsing 
stored information for appropriate stories. Over a life-
time, some stories may fall into disuse, while others will 
be weighted more strongly depending on frequency of 
use and relevance. 
4.3 True dialogue in narration 
The ultimate goal of our research is to enable children to 
tell stories in the context of a social dialogue; for exam-
ple, we want children to be able to chat to their parents 
and other interested parties about what they did during 
the day.  
 Our current system incorporates a simple model of a 
conversation, where children are restricted at any point 
to choosing from a small number of options. The child 
chooses an event to talk about, and then goes through 
the sequence of messages associated with that event.  
The child has the freedom to switch to a different event, 
hence controlling the conversation, and to add annota-
tions/evaluations (for example ?it was fun!?). 
     This is adequate in many cases, but in the long term 
we would like to support more complex conversations; 
for example interrupting a discussion about today?s 
events to talk about what happened yesterday, or to dis-
cuss a particular teacher instead of an event.  We would 
also like children to easily be able to add conversational 
phrases, such as ?Guess what happened today at 
school?. 
 Because our children have motor and cognitive im-
pairments, we cannot present them with a large number 
of options for conversational moves. Ideally, the system 
would detect what the conversational partner wishes to 
talk about, and from this present the child with a small 
number of appropriate choices. For example, if the con-
versational partner asks the child what happened over 
the past week, our system would detect this and then 
give the child the option of talking about any individual 
weekday or the week in general. 
 One way of detecting what the conversational part-
ner intends is to use speech recognition and Natural 
Language Processing (NLP) technology to analyze what 
he or she says. Speech and NLP technology tend to 
work best when it is possible to train the system to the 
user?s voice, and also (in essence) train the user to un-
derstand what the speech/NLP system can and cannot 
do. This should be possible in our context, at least for 
people (such as parents) with whom the child regularly 
interacts. 
 Another possibility is to create a graphical user in-
terface for the conversational partner, perhaps on the 
same device that the child uses, which the partner could 
use to indicate what he/she wants to talk about.  This is 
probably technically easier, but does move away from 
the goal of having as natural a dialogue as possible. 
4.4 Pragmatics of interacting with others 
Currently, ?How was School today??? supports story-
telling between language-impaired children and adults 
who are the children?s parents, carers, teachers, and 
therapists.  But of course for normally developing child-
ren, many of their most important social interactions are 
with other children. 
 An interesting example here is the STANDUP sys-
tem, which was developed to help children who use 
7
AAC create and tell novel punning riddles. The study 
results suggested that children saved the jokes so that 
they could retell them to friends and family (Waller, 
Black et al 2009). Whilst the evidence is anecdotal, 
there did also appear to be a marked increase in joke 
telling by participants, both amongst their peers and 
with adults in the home environment. Hence STANDUP 
succeeded in supporting interaction with other children 
as well as with adults. 
 One of the key challenges in interacting with other 
children, and indeed with adults who are not formally 
involved in the care or teaching of the child, is to adapt 
the story to the interests of the recipients. In other 
words, a child?s parents and teachers will not insist on 
stories that are interesting to them, but other conversa-
tional partners will.  These conversational partners may 
also need additional information.  For example ?Jane 
came to take me to the OT room? makes more sense if 
the recipient knows that Jane is the occupational therap-
ist; parents and teachers already know this, but other 
people may need to be told this. Also if the conversa-
tional partner was present at an event, this should be 
acknowledged and indeed used in the story. For exam-
ple, "Did you really enjoy maths? I thought it was bor-
ing!? 
 In short, telling stories to peers and adults who do 
not know the child well requires adapting the story to 
the interests, knowledge, and involvement of the part-
ner; this is part of learning pragmatics. This is not some-
thing we are looking at currently, but it is something 
that we hope to look at in the future. 
4.5 Security and privacy issues 
We need to ensure that data about the children is private 
and secure.  Taken to its logical conclusion, our project 
would result in an intimate record of the child's life at 
school, home and beyond.  It is important that both the 
raw data and the generated content are under the control 
of the child and his/her guardians, with the child exer-
cising as much control as possible. This is especially 
important since children with learning difficulties are 
very vulnerable; there is potential for great harm if data 
about a child?s activities got into the hands of a mali-
cious outsider. 
 In a study on the software tool TalksBac, which 
supports personal narrative (Waller, Dennis et al 1997), 
privacy issues were coded along with stories. This al-
lowed the NLG process to decide the appropriateness of 
telling a story to a specific communication partner. 
Children in general do not care who they tell their sto-
ries to. Only when older children learn to distinguish 
which story is appropriate for a conversation partner. 
This process could be embedded into the prediction 
algorithm that presents stories for narration. Currently 
prediction on AAC devices only support character, word 
or phrase selection.   
 Another concern is information that is embarrassing 
or otherwise puts the child in a negative light; for exam-
ple, imagine a staff member entered the voice message 
"I refused to eat my lunch today".  We believe that the 
child should be free to delete such messages; she should 
never be forced to include material in a story that she 
does not want to include. 
 A related issue is whether we should allow stories 
generated for one child to use information acquired 
about another child.  In principle this is very valuable, 
for example it allows messages such as ?Jane didn?t eat 
her lunch today?. But is this acceptable from the pers-
pective of ensuring the privacy of data about Jane?s ac-
tivities? On the other hand, this is exactly the sort of 
thing that a normally developed child would say about a 
classmate. 
5 Conclusion 
In addition to having difficulty in communicating de-
sires and needs, language-impaired children also find it 
hard to participate in social linguistic interaction that 
would help create and build up friendships and other 
interpersonal relationships.  We believe that we can help 
these children participate in such interactions by giving 
them a tool that helps them tell a story about their day at 
school, by using an NLG system that has access to sen-
sor and other data about the child?s activities. We are 
still at an early stage in this work, but our initial proto-
type system has shown great potential to improve the 
quality of life of children with limited speech. Our cur-
rent work plans to explore this potential further while 
evaluating the efficacy of the system for four children 
with varying ability levels.  
Acknowledgements 
We would like to express our thanks to the children, 
their parents and staff and the special school where this 
project had its base. Without their valuable contribu-
tions and feedback this research would not have been 
possible. We would also like to thank DynaVox Sys-
tems Ltd for supplying the communication devices to 
run our system on. 
 This research was supported by the UK Engineering 
and Physical Sciences Research Council under grants 
EP/F067151/1, EP/F066880/1, EP/E011764/1, 
EP/H022376/1, and EP/H022570/1. 
8
References 
Agrawal, R. and Ramakrishnan, S. (2000) Privacy-
preserving data mining. ACM International 
Conference on Management of Data, pp. 439--450, 
Bart, H., V. Riny, et al (2008). LinguaBytes. 
Proceedings of the 7th international conference on 
Interaction design and children. Chicago, Illinois, 
ACM: 17-20. 
Beukelman, D. R. and P. Mirenda (2005). Augmentative 
and Alternative Communication: Management of 
Severe Communication Disorders in Children and 
Adults. Baltimore, Paul H. Brookes Publishing Co. 
Bruner, J. (1975). "From communication to language: A 
psychological perspective." Cognition 3: 255-289. 
Cheepen, C. (1988). The predictability of  informal 
conversation. Oxford, Printer Publishers Ltd. 
Clarke, H. H. and E. V. Clarke (1977). Psychology and 
Language. New York, Harcourt Brace Jovanovich. 
Dempster, M. (2008). Using natural language 
generation to encourage effective communication in 
nonspeaking people. Proceedings of Young 
Researchers Consortium, ICCHP'08. 
DeRuyter and Fried-Oken. (2010). "Context-sensitive 
messaging with RFID technology."   Retrieved 2010, 
April 10, from http://aac-rerc.psu.edu/index.php/projecttypes/list 
Gatt, A., F. Portet, et al (2009). "From Data to Text in 
the Neonatal Intensive Care Unit: Using NLG 
Technology for Decision Support and Information 
Management." AI Communications 22: 153-186. 
Goldberg, E., N. Driedger, et al (1994). "Using natural-
language processing to produce weather forecasts." 
IEEE Expert 9(2): 45-53. 
Harris, M. (2008). Building a Large-Scale Commer-cial 
NLG System for an EMR. Proc of INLG-2008. 
Hemsley, B., S. Balandin, et al (2007). "Family 
caregivers discuss roles and  needs in supporting 
adults with cerebral palsy and complex 
communication needs in the hospital setting." 
Journal of Developmental and Physical Disabilities 
19(2): 115-124. 
Higginbotham, D. J., H. Shane, et al (2007). "Access to 
AAC: Present, past, and future." Augmentative & 
Alternative Communication 23(3): 243-257. 
Labov, W. (1972). Language in the inner city: Studies in 
the Black English Vernacular. Philadelphia, 
University of Pennsylvania Press. 
Light, J., C. Binger, et al (1994). "Story Reading 
interactions between preschoolers who use AAC and 
their mothers." Augmentative and Alternative 
Communication 10: 255-268. 
Light, J. and K. Drager (2007). "AAC Technologies for 
Young Children with Complex Communication 
Needs: State of the Science and Future Research 
Directions." Augmentative and Alternative 
Communication 23(3): pp. 204 ? 216. 
Liu, X., A. Sen, et al (2008). A Software Client for Wi-
Fi Based Real-Time Location Tracking of Patients. 
Medical Imaging and Informatics. 
Berlin/Heidelberg, Springer. 4987/2008: 141-150. 
P?r?z, R. P. y. and M. Sharples (2004). "Three 
Computer-Based Models of StoryTelling: BRUTUS, 
MINSTREL, and MEXICA." Knowledge-Based 
Systems 17: 15-29. 
Peterson, C. and A. McCabe (1983). Developmental 
psycholinguistics: Three ways of looking at a child?s 
narrative. New York, Plenum. 
Reiter, E. (2007). An Architecture for Data-to-Text 
Systems. ENLG-2007. 
Reiter, E. and R. Dale (2000). Building Natural-
Language Generation Systems., Cambridge 
University Press. 
Reiter, E., S. Sripada, et al (2005). "Choosing Words in 
Computer-Generated Weather Forecasts." Artificial 
Intelligence 167: 137-169. 
Reiter, E., R. Turner, et al (2009). Using NLG to Help 
Language-Impaired Users Tell Stories and 
Participate in Social Dialogues. ENLG2009. Athens, 
Greece, Association for Computational Linguistics. 
Robertson, J. and J. Good (2005). "Story creation in 
virtual game worlds." Communications of the ACM 
48: 61-65. 
Schlosser, R. W. (2003). The Efficacy of Augmentative 
and Alternative Communication. San Diego, 
Elsevier Science. 
Soto, G., E. Hartmann, et al (2006). "Exploring the 
Elements of Narrative that Emerge in the 
Interactions between an 8-Year-Old Child who uses 
an AAC Device and her Teacher." Augmentative 
and Alternative Communication 22(4): pp. 231 - 
241. 
Sripada, S., E. Reiter, et al (2005). Evaluating an NLG 
System using Post-Edit Data: Lessons Learned. 
Proceedings of ENLG-2005, 10th European 
Workshop on Natural Language Generation, 
Aberdeen, Scotland. 
Waller, A. (2006). "Communication Access to 
Conversational Narrative." Topics in Language 
Disorders 26(3): 221-239. 
Waller, A., R. Black, et al (2009). "Evaluating the 
STANDUP Pun Generating Software with Children 
with Cerebral Palsy." ACM Trans. Access. Comput. 
1(3): 27. 
Waller, A., F. Dennis, et al (1997). "Evaluating the use 
of TalksBac, a predictive communication device for 
non-fluent aphasic adults." International Journal of 
Language and Communication Disorders 33: 45-70. 
Waller, A. and A. F. Newell (1997). "Towards a 
narrative based communication system." European 
Journal of Disorders of Communication 32: 289-
306. 
 
9
