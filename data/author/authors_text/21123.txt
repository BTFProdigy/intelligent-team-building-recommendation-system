Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032?1043,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Extraction of Morphological Lexicons
from Morphologically Annotated Corpora
Ramy Eskander, Nizar Habash, Owen Rambow
Center for Computational Learning Systems
Columbia University
{reskander,habash,rambow}@ccls.columbia.edu
Abstract
We present a method for automatically learn-
ing inflectional classes and associated lem-
mas from morphologically annotated corpora.
The method consists of a core language-
independent algorithm, which can be opti-
mized for specific languages. The method is
demonstrated on Egyptian Arabic and Ger-
man, two morphologically rich languages.
Our best method for Egyptian Arabic pro-
vides an error reduction of 55.6% over a sim-
ple baseline; our best method for German
achieves a 66.7% error reduction.
1 Introduction
Morphological lexicons specify all inflected forms
for each lexeme; in a language with rich morphol-
ogy, such a resource can be important for natural lan-
guage processing (NLP) tasks in order to limit data
sparseness. For example, a morphological lexicon is
an important component of a morphological tagger
or of a part-of-speech (POS) tagger for languages
with rich morphology.1 Traditionally, a morpholog-
ical lexicon has been created through painstaking
lexicographic and morphological analysis of the lan-
guage, drawing on unannotated corpora. Recently,
new approaches have emerged. Fully or largely un-
supervised approaches cannot link surface forms to
morphosyntactic features and are thus not suited for
building morphological lexicons. This problem is
overcome by approaches that use explicit linguistic
knowledge. In this paper, we investigate using exist-
ing morphologically annotated corpora. In a mor-
phologically annotated corpus, the words in natu-
rally occurring texts or transcribed speech are anno-
tated for the correct morphological analysis (includ-
1Note that a full-form morphological lexicon is functionally
equivalent to a morphological analyzer-generator, since one can
be used to create the other.
ing, of course, core POS) in context. While there
has been much work on computational morphology,
to our knowledge this is the first paper to study the
question of how to extract morphological lexicons
from morphologically annotated corpora, and how
to determine how much annotation is needed. In
this paper, we assume a corpus with each word an-
notated with morphosyntactic features and with a
lemma which tells us what lexeme the word form
is part of. The task is to predict the correspondence
between a word form and its lemma and morpholog-
ical features.
This paper makes two contributions. First, we
introduce an algorithm that learns unseen forms by
analogy. This algorithm is language-independent. It
incrementally merges complementary paradigm in-
formation about different lexemes into more abstract
and more informative inflectional classes. Second,
we explore how to model stems, and we propose
a generalization of the Semitic root-and-template
modeling. We use Egyptian Arabic (EGY), and Ger-
man (GER) as our test languages. We test on corpus
data, in order to simulate a standard real-world ap-
plication. The baseline just uses the word forms seen
in training and does not predict any unseen forms.
Our language-independent algorithm improves the
performance for both EGY and GER, with error re-
ductions over the baseline of 44.4% for EGY and of
66.7% for GER. By adding language-specific mod-
eling of the stem using templates, we obtain further
error reductions for EGY (up to 55.6%) but not for
GER.
Next, we review related work (Section 2) and
introduce the key linguistic concepts we use (Sec-
tion 3). We present our basic language-independent
method in Section 4, and our language-specific mod-
eling of stem variation in Section 5.
1032
2 Related Work
Approaches to Morphological Modeling Much
work has been done in the area of computational
morphology ranging from systems painstakingly de-
signed by hand (Koskenniemi, 1983; Buckwalter,
2004; Habash and Rambow, 2006; De?trez and
Ranta, 2012) to unsupervised methods that learn
morphology models from unannotated data (Creutz
and Lagus, 2007; Monson et al, 2008; Ham-
marstro?m and Borin, 2011; Dreyer and Eisner,
2011). There is a large continuum between these two
approaches. Closer to one end, we find work on min-
imally supervised methods for morphology learning
that make use of available resources such as paral-
lel data, dictionaries or some additional morpholog-
ical annotations (Yarowsky and Wicentowski, 2000;
Cucerzan and Yarowsky, 2002; Neuvel and Fulop,
2002; Snyder and Barzilay, 2008). Closer to the
other end, we find work that focuses on defining
morphological models with limited lexicons that are
then extended using raw text (Cle?ment et al, 2004;
Forsberg et al, 2006). The work presented in this
paper falls in the middle of this continuum: we are
interested in learning complete morphological mod-
els using rich morphological annotations and, op-
tionally, limited linguistic knowledge. We compare
the value of different amounts of annotation and how
they relate to additional linguistic knowledge.
Morphological Paradigms Many traditional and
modern theories of inflectional morphology orga-
nize natural language morphology by paradigms
(Stump, 2001; Walther, 2011; Camilleri, 2011).
Within the continuum we discussed above, we find
hierarchical representations of paradigm knowledge
that have been used in manually constructed mor-
phological models (Finkel and Stump, 2002; Habash
et al, 2005). Furthermore, De?trez and Ranta (2012)
introduce an implementation of Smart Paradigms
? heuristically organized paradigms minimizing the
number of forms needed to predict the full paradigm
of a particular lexeme.
Both Forsberg et al (2006) and Cle?ment et al
(2004) describe methods for automatically populat-
ing a lexicon from raw data given a set of morpho-
logical inflectional classes in a language. Our work
differs in that we use annotated data, but do not start
with a complete set of inflectional classes; thus, our
work is exactly complementary to this work.
The concept of a paradigm is also used in many
published efforts on unsupervised learning of mor-
phology, although not always in a way consistent
with its use in linguistics. For instance, Snover et
al. (2002) (and later on Can and Manandhar (2012))
define a paradigm as ?a set of suffixes and the stems
that attach to those suffixes and no others?. This
definition is quite limited since it is not modeling
the notion of lexeme. Chan (2006) defines a simpler
concept of paradigms in his probabilistic paradigm
model, which has many limitations, such as not han-
dling syncretism or irregular morphology, nor dis-
tinguishing inflection and derivation.
Dreyer and Eisner (2011) learn complete Ger-
man verb paradigms from a small set of complete
seed paradigms (50 or 100), which they choose ran-
domly from all verbs in the language. They model
stem changes using letter-based models, and use
a large unannotated corpus in addition to the seed
paradigms. Durrett and DeNero (2013) attack the
same problem as Dreyer and Eisner (2011). Instead
of using unannotated text, they model explicit rules
for affixes and stem changes. The major difference
between these two efforts and our work is that the
problem is defined differently: we assume that the
training and test data is defined by a corpus, not
by complete paradigms. Our methods therefore are
more sensitive to frequency effects of tokens. We
believe that our way of stating the problem is more
relevant to actual computational challenges for lan-
guages with limited morphological resources. We
empirically compare our approach to that of Durrett
and DeNero (2013) in Section 5.
None of the unsupervised approaches mentioned
model inflectional classes, i.e., meta-paradigmatic
representation that cluster the various paradigms of
different lexemes into a set of general classes of
paradigms. There is no explicit notion of mor-
phosyntactic features. In this paper we target the
learning and completion of inflectional classes from
morphologically annotated data. Our approach does
not sacrifice details of what paradigms should in-
clude: we handle syncretism and stem changes, and
allow for the prediction of new word forms from
morphosyntactic features and lemmas, unlike the
largely unsupervised work. Our work also differs
from most previous work in that we investigate how
to model stem change explicitly. Whereas other
approaches model stem syncretism through letter-
1033
based models (Yarowsky and Wicentowski, 2000;
Neuvel and Fulop, 2002; Dreyer and Eisner, 2011),
we explore the use of abstract stems.
In our previous work on the EGY morphological
analyzer CALIMA, we similarly used a lexicon of
annotated morphological forms and extended it au-
tomatically using a simpler approach to paradigm
completion (Habash et al, 2012).
3 Linguistic Terminology
In this section, we review key concepts from mor-
phology, and introduce the terminology we will use
in this paper.2 We then introduce our own formal-
ization of stems using vocalic templates.
Morphology is the study of word forms and their
decomposition into elementary morphemes, which
are the smallest meaning-bearing units of a lan-
guage. There are two types of morphological pro-
cesses: inflectional and derivational morphology. In
inflectional morphology, a core meaning is retained
and different word forms reflect different types of
morphosyntactic features such as person, number, or
tense. In derivational morphology, the core meaning
of a word is changed, and perhaps even its part-of-
speech (POS). In this paper, we restrict our interest
to inflectional morphology. Furthermore, we take
the written form of the word to be primary, and base
all morphological analyses on the written form.
We will refer to the set of all word forms that
are related through inflectional morphology alone as
lexeme. We can refer to a lexeme with a lemma,
which we take to be a language-specific and conven-
tionalized choice of one of the inflected forms. For
example, in English the verb is conventionally cited
in the infinitive (often with to, which can be omit-
ted), while in Arabic it is conventionally cited in per-
fective third person masculine singular. The lemma
is sometimes referred to as a ?citation form?. A
paradigm of a lexeme is a list of cells, where a cell
is a combination of a complete set of morphosyn-
tactic features (properties) and the corresponding in-
flected form of the lexeme. A paradigm is com-
plete if there are cells for all possible morphosyn-
tactic features (the list of possible morphosyntactic
features is of course language-dependent).
We can divide the word forms into affixes (i.e.,
prefixes and suffixes) and the stem. There is no sin-
2We (roughly) base our terminology and our conceptualiza-
tion on the inferential-realizational theory of Stump (2001).
gle correct way to do this for the words of a lan-
guage. Each lexeme has its own paradigm. We
can abstract from paradigms by grouping together
paradigms which share the same affixes in corre-
sponding cells, and where stems in corresponding
cells differ in some restricted manner. We can de-
fine the inflectional class (IC) more formally as a
set of abstract cells, where an abstract cell is a com-
bination of a complete set of morphosyntactic fea-
tures (properties) and an abstract representation of a
stem (an abstract stem) along with fully specified
affixes. The abstract stems (and thus the abstract
cells) must have the property that, given a single in-
stantiated word form of a lexeme along with its as-
sociated IC, we can derive the complete paradigm
of the lexeme deterministically. In the first results
we present, we simply assume that the stem is ei-
ther shared entirely with other abstract cells, or it is
entirely lexically instantiated. We explore language-
specific approaches to defining an abstract stem in
Section 5, where we also discuss relevant morpho-
logical facts of EGY and GER.
Prefixes, suffixes, and stems can be the same for
different cells of a single paradigm or IC. This is
called syncretism. We will refer to the cells which
share a stem as a stem syncretism zone or ?zone?
for short.
4 Language-Independent Inflectional
Class Construction Algorithm
In this section, we present our language-independent
IC construction algorithm (LICA). LICA consists of
a core algorithm for building ICs from seen data and
models of soft-stem syncretism and affix prediction.
4.1 Problem Definition
Starting with a corpus of words annotated as triples
of ?prefix+stem+suffix, lemma, features?, we want
to create a lexicon of complete ICs, with each
IC having an associated set of lemmas. The
following is an input example specifying the in-
flected form of the 3rd person plural imperfec-
tive inflection for the EGY lemma katab3 ?write?:
?y+iktib+uwA, katab, I3UP?.
3Arabic transliteration throughout the paper is presented in
the Habash-Soudi-Buckwalter scheme (Habash et al, 2007).
1034
IC1
IC3
IC2
IC4
CS = 3
CS = 2
CS = 1
IC1,3
IC2
IC4
CS = 2
IC1,2,3
IC4
IC1
L1 ? L2 ? L3
IC2
L4 ? L5
IC3
L6 ? L7
IC4
L8
IC1,3
L1 ? L2 ? L3
L6 ? l7
IC2
L4 ? L5
IC4
L8
IC1,2,3
L1 ? L2 ? L3
L4 ? L5
L6 ? l7
IC4
L8
2 31
Figure 1: This graph illustrates two merges that result in combining three ICs in two steps. The IC cells are represented
as solid (filled cells) or blank (empty cells) circles. CS is the compatibility score marking the number of matching
filled cells in an IC. The boxes to the right of the graph represent the lexicon which associates lemmas (L*) with ICs.
IC4 is not connected with any of the other ICs, because it has cell values that are incompatible with them.
4.2 Core Algorithm
Building the Initial Inflectional Classes An ini-
tial IC is constructed for each lemma found in the
input corpus using all the triplets involving said
lemma. Since the lemma itself is an inflected form
with an a priori fixed feature combination, we also
use the lemma to construct the initial IC, even if it
did not occur as a word form in the corpus. If all
inflected forms of a lemma appear in the training
data, the IC will be complete. However, typically
there are many unseen forms. If all seen forms re-
lated to one lexeme have the same stem, the abstract
stem chosen for the IC is a single stem variable;
the abstract cells of the new IC can of course dif-
fer in terms of affixes (which are always fully spec-
ified rather than represented as variables). In cases
when the seen forms of one lemma have more than
one stem, the IC created will simply be the same as
the paradigm, i.e., we have fully instantiated stems
as our abstract stems. We call such ICs suppletive
ICs. At this point, we have a repository of numerous
incomplete ICs and a lexicon consisting of a one-to-
one mapping between lemmas and these new ICs.
We then identify all ICs that are exactly the same (by
definition, these are not suppletive ICs), and merge
the associated sets of lemmas. We now have a new
lexicon in which some ICs are associated with more
than one lemma.
Constructing the Inflectional Class Graph We
construct an IC graph that connects all mergeable
ICs. Two ICs are mergeable if neither of the ICs
is suppletive, if they share at least one abstract cell
for some morphological feature, and if no morpho-
logical features are associated with different abstract
cells, i.e., there are no incompatibilities. Each point
in the graph represents a specific IC, while the edges
carry the following four scores that we use to deter-
mine mergeability order:
? The compatibility score is the number of non-
empty intersections between the two ICs.
? The originality score is the larger number of
previous merges of the two ICs. At the begin-
ning, this number is 0 for all edges.
? The completeness score is the size of the union
of non-empty rows from the two ICs
? The lexical size score is the sum of the number
of lemmas associated with the two ICs.
The edges of the graph are ranked according to the
compatibility score first (more is better). Any ties
are broken using the originality score (lower is bet-
ter), any remaining ties by the completeness score
(more is better) and then by the lexical size score
(more is better). We explored all possible orders of
tie breaking using EGY data, and determined the or-
der of the listing above to be best. We use it in all
experiments reported in this paper.
Merging the Inflectional Classes While the IC
graph is still connected, we repeat the following
merging procedure. Starting with the highest ranked
edge in the graph, we merge the two ICs connected
by the edge. In case of multiple edges that are
equally highly ranked (after tie breaking), we select
randomly among them. The merge creates a new IC
that has the union of the cells in the two ICs. The
lexicon is adjusted accordingly by associating with
the new IC the union of all the lemmas originally as-
sociated with the two ICs. The new IC inherits the
union of all the IC graph connections of its prede-
cessors. The IC graph edge scores between the new
1035
IC and other ICs are recalculated. Graph edges that
become incompatible after the merge are removed.
The two original ICs are removed from the graph
and lexicon. See Figure 1 for an illustration of the
merge process.
4.3 Completing Stems and Affixes
Soft Stem Syncretism Zones We extend the con-
cept of stem-syncretism zones into a statistical
model that computes the probability that the abstract
stems in two abstract cells are the same given their
feature combinations, i.e., that they belong to the
same syncretism zone. We refer to this approach
as ?soft? stem-syncretism zones or ?soft zones? for
short. The probabilities are computed for each
feature-combination pair as the ratio of the times the
abstract stem for the feature-combination pair are
equal divided by the times the abstract stem for the
feature-combination pair are not empty. The prob-
abilities can be computed after the initial IC graph
construction or after the merging process has con-
cluded; they can also be based on IC type counts
or weighted by the number of associated lemmas.
Applying soft zones to fully complete the ICs can
only be done after merging is completed. We try all
the possible alternatives for learning the SZs on the
EGY data, including experiments with small train-
ing data sizes. The best accuracy is obtained using
IC type weights learned after the merge. We use this
setting for all experiments.
When applying the soft zones to determine the
abstract stems of empty cells, we consider all filled
cells in the same IC and select the abstract stem from
the cell of the feature combination that has the high-
est soft-zone probability with the feature combina-
tion of the empty cell. Note that the copied abstract
stem can be either a stem variable, or, for a supple-
tive IC, a lexical form.
Predicting Affixes In building the ICs mentioned
above, some feature combinations and their corre-
sponding affixes (prefixes and suffixes) are missing
since they were not in the training data. We fill the
missing affixes in a particular IC i by copying them
from other ICs that we rank by overall seen affix
similarity to IC i. ICs with conflicting affixes for
any feature combinations are excluded completely.
For any remaining missing affix-feature combina-
tion, we use the most common affix for that feature
combination over all ICs.
4.4 Model Application
The complete ICs produced by the completion algo-
rithm, associated with their lemmas in the lexicon,
can now be used to predict the surface form of a
given lemma and morphological features. We use
the following procedure: If the given features are
the same as those used to define the lemma, then the
surface form is the same as the lemma form. If the
lemma was seen in training, we select its IC from the
model and generate the inflection associated with the
features. This may require instantiating a stem vari-
able (in case the IC is not suppletive), but this can be
done deterministically from the lemma.
If the lemma has not been seen, then we build
an IC on the fly using the lemma, i.e., an IC with
a single cell. We then pick an IC from the model
that does not conflict with that IC. The priority is
given to the IC with the largest lexicon size, which
is likely to be a result of several merges. If such an
IC does not exist, a backup mode returns the stem of
the lemma associated with the most frequent affixes
of the queried features.
4.5 Results on Egyptian Arabic
Data and Metrics We use a morphologically an-
notated EGY corpus based on the CALLHOME
EGY (CHE) corpus (Gadalla et al, 1997).4 We di-
vide the corpus into three parts: training, develop-
ment and test, of about 75K, 36K and 41K words,
respectively. We conduct our experiments on verbs
only since they have a large number of possible mor-
phosyntactic feature combinations. The verbs in
these experiments are uncliticized. Clitics are eas-
ily handled using a few orthographic rules (El Kholy
and Habash, 2010). On average, uncliticized verbs
are about 12% of all words in our corpus. The data
is represented in triplets as described in Section 4.1.
There is a total of 19 feature combinations of as-
pect/mood (perfective,imperfective and imperative),
person (first, second and third), gender (masculine,
feminine and neutral) and number (singular and plu-
ral). Some combinations are invalid such as the first
and third persons with the imperative form. The
lemma we use is the Arabic citation form for verbs,
which is the perfective third person masculine sin-
4The corpus was automatically annotated using information
from the CHE transcripts (Gadalla et al, 1997) and the Egyp-
tian Colloquial Arabic Lexicon (Kilany et al, 2002). For more
details, see Habash et al (2012) and Eskander et al (2013).
1036
gular (P3MS) inflection. We use this fact to fill the
P3MS cells when building the initial ICs.
We evaluate the accuracy of the automatically
generated bidirectional lexicon by generating sur-
face forms from lemmas and morphosyntactic fea-
tures. We use the model application method de-
scribed in Section 4.4.
Baseline Our baseline system consists of two
steps. First, we check the features. All cases of cita-
tion form features (in the case of EGY, P3MS) return
the lemma as the inflected form. Otherwise, we look
up the lemma and feature pair among all triplets in
the training data and return the inflected form if such
a triplet was found. If not, the baseline does not re-
turn an answer.
Results The results on tokens are summarized in
Table 2 under the column heading BL (baseline) and
NOTMP (LICA with no stem template). Our sys-
tem consistently improves over the baseline for all
training data sizes explored.
Error Analysis We performed an error analysis
using the best settings found on the development
set. We found that in 46% of the error types the
lemma is unseen. Additional 35% of the cases are
due to stem templates that are unseen in the com-
plete ICs although their corresponding lemmas are
seen. About one tenth of the cases are because of
the existence of multiple forms of the same lemma
and feature combinations, where our system assigns
a form that is different from the gold form. Finally,
gold errors contribute to about 9% of all errors.
4.6 Results on German
Data and Metrics For our experiments, we use
the TIGER corpus (Brants and Hansen, 2002). We
divide the corpus into three parts: training, develop-
ment and test, of about 709K, 143K and 37K words,
respectively. However, we use the first 75K words
in training and the first 36K words in development
to have the results comparable to EGY. We conduct
our experiments on verbs only. We disregard any
verbs with separable prefixes (as is common in work
in morphology learning). On average, verbs with-
out separable prefixes are about 9% of all words in
our corpus. The data is represented in triplets as de-
scribed in Section 4.1. There are 28 feature com-
binations of tense (present, past), person (first, sec-
ond and third), number (singular, plural), and mood
(indicative, subjunctive, imperative, past participle,
infinitive). Some combinations are invalid such as
any tense with the non-tensed participle or infinitive.
The infinitive is the lemma. We use the same metrics
as for EGY (Section 4.5).
Results The results are summarized in Table 5,
with the relevant results in the column NOTMP. We
see that our algorithm performs substantially better
than the baseline at all training set sizes, with greater
relative error reductions at smaller training sizes.
Error Analysis We inspected all error types in the
development set and found that nearly 8% of all er-
ror types are errors in the gold annotation of the de-
velopment set, and in 4% of cases, our predicted
form is correct because a lemma is shared by two
verbal paradigms (for example, werden has different
past participles depending on whether it is the pas-
sive auxiliary or the verb for ?become?).
5 Language-Specific Modeling of Stems
5.1 General Approach
We model the abstract stems using the notions of or-
thographic template and orthographic root. To
meet our definition of IC given above, we define
these two notions so that the root and template can
always be extracted deterministically. We define
them simply in terms of sets of letters: the set of
letters of the alphabet used to write the language
we are modeling is partitioned into the root let-
ters and the pattern letters. The orthographic tem-
plate is a string that specifies the template letters and
that has placeholders for the root letters, which we
write as ?2?. The orthographic root is a sequence
of strings that specifies the root letters that can fill
a vocalic template?s placeholders, in the order spec-
ified. Note that an IC along with a root is equiv-
alent to a paradigm, since the root can be inserted
deterministically into the abstract stem. This gives
us another way to specify a lexeme: since a com-
plete paradigm enumerates all inflected forms of a
lexeme, and since a complete IC along with a root
defines a complete paradigm, we can specify a lex-
eme to be a pair consisting of an IC along with a
root. This pair determines a complete mapping from
morphosyntactic features to surface word forms for
the lexeme.
The basic algorithm discussed earlier is modified
as follows: when we read in the training data, the
1037
EGY Inflectional Class (IC) Repository
IC-1 IC-2 IC-3
P1US 2a2a2+t 2a2?+ayt 2u2+t
P1UP 2a2a2+nA 2a2?+aynA 2u2+nA
P2MS 2a2a2+t 2a2?+ayt 2u2+t
P2FS 2a2a2+tiy 2a2?+aytiy 2u2+tiy
P2UP 2a2a2+tuwA 2a2?+aytuwA 2u2+tuwA
P3MS 2a2a2 2a2? 2A2
P3FS 2a2a2+it 2a2?+it 2A2+it
P3UP 2a2a2+uwA 2a2?+uwA 2A2+uwA
I1US Aa+22i2 Aa+2i2? Aa+2uw2
I1UP ni+22i2 ni+2i2? ni+2uw2
I2MS ti+22i2 ti+2i2? ti+2uw2
I2FS ti+22i2+iy ti+2i2?+iy ti+2uw2+iy
I2UP ti+22i2+uwA ti+2i2?+uwA ti+2uw2+uwA
I3MS yi+22i2 yi+2i2? yi+2uw2
I3FS ti+22i2 ti+2i2? ti+2uw2
I3UP yi+22i2+uwA yi+2i2?+uwA yi+2uw2+uwA
C2MS Ai22i2 2i2? 2uw2
C2FS Ai22i2+iy 2i2?+iy 2uw2+iy
C2UP Ai22i2+uwA 2i2?+uwA 2uw2+uwA
EGY Lexicon
IC-1 IC-2 IC-3
katab faraD mad? Hal? s?Af qAl
?k,t,b? ?f,r,D? ?m,d? ?H,l? ?s?,f? ?q,l?
?write? ?suppose? ?extend? ?solve? ?see? ?say?
Table 1: Example of three inflectional classes and associ-
ated lemmas with their roots in EGY. The various blocks
in the IC table specify different syncretism zones.
root of the lemma is used to decompose the stem of
the inflected form into a root and a template.
5.2 Choosing Root and Pattern Letters
Determining which letters count as root letters and
which count as pattern letters is language-specific.
In this paper, we use three approaches to choosing
root and pattern letters.
No Template (NOTMP) In this approach, we do
not model the stem at all, i.e., there are no pattern
letters. As a result, the ICs cannot express stem
changes; any verb with a stem change will be an
irregular verb and it will be modeled with a sup-
pletive IC. Note that in EGY, every verb manifests
a stem change between the perfective and imperfec-
tive forms, while in GER many verbs in fact do have
the same stem for all inflected forms. The results of
the NOTMP approach are the same basic approach
results already presented in Section 4.
Scholar (SCHLR) In this approach, we use lin-
guistic scholarship and intuition to choose a set of
letters to be the pattern letters. These are letters
which we know can change in stems within the
same IC. We discuss this approach for EGY in Sec-
tion 5.3.1 and for GER Section 5.4.1.
Empirical (EMPR) In this approach, we obtain
the set of pattern letters empirically. We align the
letters of all the stems in the development corpus
with the stem letters of their corresponding lemmas.
This allows us to identify which letters change be-
tween lemma stem and inflected stem. We order the
letters by the probability that a letter changes given
all occurrences of that letter, and by the probabil-
ity that a letter changes given all changes that oc-
cur. This gives us two rankings. For both rank-
ings and for each letter that changes, we construct
a set of letters by including all letters from the most
highly ranked letter down to the letter under con-
sideration. This gives us per ranking as many sets
as there are letters that change. We then choose the
best performing set among all sets generated by both
rankings. We present the results of this approach for
EGY in Section 5.3.1 and for GER in Section 5.4.1.
5.3 Egyptian Arabic
5.3.1 Choice of Pattern Letters
Scholar-based Pattern Letters For the EGY
SCHLR approach, we selected the following pattern
letters: 
 Z ???



??

@ @

@ @ AA?A?A?ww?yy?y??aui?. These
letters cover all the so-called weak root radicals,
Hamzated forms and diacritics that are often used
to discuss different verbal paradigms in Arabic
(Gadalla, 2000). Table 1 shows three EGY inflec-
tional classes. We follow the standard practice in
modeling Arabic morphology of assuming that each
placeholder in an orthographic template corresponds
to exactly one letter from the orthographic root (i.e.,
the root is a sequence of strings each of length one).
We would like to stress that our orthographic tem-
plate and root differ from the notions of pattern and
root in Semitic morphology (for an overview, see
(Habash, 2010)): while for katab ?write? the or-
thographic root as well as the ?real? root is ?k,t,b?,
the orthographic root for xal?ay? ?let? is ?x,l? (as
opposed to the real root ?x,l,y?). Henceforth, we
will omit the adjective ?orthographic?, since we will
not talk about the ?real? root. To inflect a lemma
for a particular set of features, we combine the root
with the 2 slots in the corresponding inflectional
class feature row, e.g., Hal? + P2FS ? ?H,l? +
1038
2a2?+aytiy? Hal?aytiy. Note that in this paper,
we do not use any rules; all regular phonological and
orthographic variation is ?compiled? into the ICs.
We also see examples of stem syncretism zones in
Table 1; they are marked with horizontal lines. The
stems associated with P3MS, P3FS and P3UP hap-
pen to be the same within each IC (although differ-
ent across ICs). Different ICs have different zones:
e.g., IC-1 has one zone for the P*** features, while
IC-2 and IC-3 have two identical zones for P3** and
P[12]**.
Empirically Determined Pattern Letters For
EGY, the EMPR approach, using the algorithm
described in Section 5.2, yields the following
optimal set of pattern letters: 
  HZ ???


?@

@ @
AA?A?wyy?y???aui. This set is very similar to the
SCHLR set except in that it omits the lexically
constant (per lexeme) Shadda diacrtic  ? (ortho-
graphic gemination marker), and some very infre-
quent Hamzated forms; and it also adds one letter
H ? as a result of orthographic inconsistency in the
training data.
Corpus Verb
BL NOTMP SCHLR EMPR
EMPR
Size Count IIC+HZ
0K 0 19.3 20.0 20.0 20.0 58.8
1K 95 51.8 64.2 68.3 68.1 83.3
5K 630 64.2 77.0 83.9 83.9 91.1
10K 1,201 70.4 84.3 89.4 89.1 92.4
25K 2,994 79.6 91.5 94.4 94.8 95.6
50K 5,966 84.5 94.2 96.9 96.7 97.2
75K 8,690 85.6 94.9 97.5 97.6 97.2
Table 2: Learning curve comparing system performance
for EGY on tokens (development set).
Results The template approaches SCHLR and
EMPR consistently beat NOTMP which does not
make use of any templatic stem modeling (see Ta-
ble 2). However, SCHLR and EMPR perform about
equally. This is not surprising since the two sets of
pattern letters are quite similar.
Error Analysis We conducted an error analysis of
EMPR using the best settings found on the develop-
ment set. About 86% of the error types in NOTMP
where the lemma is unseen are solved after intro-
ducing EMPR. Additionally, 71% of the cases in
NOTMP where the stem template is unseen and the
lemma is seen are solved. However, 7% of the error
types in EMPR are not present in NOTMP, and they
are all cases where the stem template is unseen while
the lemma is seen. Errors due to multiple stem forms
and gold errors remain the same in both NOTMP and
EMPR, contributing to 26% and 23%, respectively,
of all the error types in EMPR.
5.3.2 Other Enhancements with Linguistic
Knowledge
Other linguistic knowledge can also help enrich
the process of IC learning, especially under limited
annotation conditions. We use the following two
types of linguistic knowledge, which seamlessly in-
tegrate into the core merging algorithm described
above.
Iconic Inflectional Classes (IICs) are ICs that are
manually fully annotated, i.e., they have all the tem-
plate cells for all morphosyntactic features specified.
IICs are treated like any other ICs when constructing
the initial IC graph. They are different from other
ICs in that they initially have no lemmas associated
with them in the lexicon.
Hard Stem-Syncretism Zones (HZ) are stem-
syncretism zones determined manually by linguists
to hold for all ICs. As such they can be more fine-
grained than is needed to describe individual ICs. A
hard zone is not applied in case of any partial dis-
agreement within it. Unlike soft zones, they could
be applied before or after the merge process, and
they do not guarantee that the ICs will be completely
filled.
We conducted experiments where we added
external linguistic knowledge to our training data.
We added 112 IICs which are extracted from all
EGY verb inflections listed in a reference grammar
of EGY (Gadalla, 2000). Also we added the
following eight HZs for EGY (with reference to
features <tense, person, gender, number>):
(P1US-P1UP-P2MS-P2FS-P2UP), (I1UP-I2MS-
I3MS-I3FS), (I2FS-I2UP-I3UP), (P3FS-P3UP),
(C2FS-C2UP), (P3MS), (I1US), and (C2MS).
Applying the HZs on the completed ICs (before soft
zone application) gives higher results than applying
them on initial ICs. We only report below on the
setting of applying HZs after IC merge completion.
We present the accuracies of IC learning for the
baseline, and for the best setup with and without
IICs and HZs, for different training sizes in Table 2.
The baseline with no training data is at 19.3%
1039
because of all the cases with verbs appearing in
the citation form. As expected, IICs always help
improve accuracy, especially under limited (and
no) data conditions. However the benefits diminish
rapidly for larger training sets. When evaluating on
types only (results not presented in this paper), we
find that using IICs in our system with no data is
better than using the baseline with 75K words.
5.3.3 Blind Test Set
Table 3 shows the accuracies the different sys-
tems on our blind test set. The results for the test
set are lower than those of the development set, but
the trends are the same. We also compare our re-
sults to those obtained using the system of Durrett
and DeNero (2013) on the same test data. Note that
we apply their system to our problem ? predicting
unseen forms from annotated corpora (i.e., incom-
plete paradigms), not to the problem for which they
created their system ? predicting unseen forms from
complete paradigms. Our best system outperforms
theirs by 2.8% absolute in accuracy.
System Accuracy Error Reduction
Baseline 84.7
NOTMP 91.5 44.4
SCHLR 93.2 55.6
EMPR 93.2 55.6
Durrett & DeNero 90.4 37.3
Table 3: Results for EGY on tokens using a blind test set.
5.4 German
5.4.1 Choice of Pattern Letters
Scholar-based Pattern Letters We now discuss
our scholarship-based choice of pattern letters for
German. Like Arabic, German verb paradigms
can show stem changes which are typically vowel
changes. Furthermore, like Arabic, German has
prefixes, suffixes, and circumfixes. However, un-
like Arabic, German has many verbs (called ?weak
verbs?) which are regular in the sense that they
show no stem change at all. The irregular verbs,
or ?strong verbs?, show many different patterns of
stem changes. Another difference to Arabic is that
the affixes are not the same for all verb paradigms.
In particular, the weak verbs form several inflec-
tional classes (which, of course, differ only in af-
fixes). Finally, unlike Arabic, in the strong verbs
the orthographic root does not necessarily consist
GER Inflectional Class (IC) Repository
IC-1 IC-2 IC-3
PI1S 2o2 +e 2e2 +e 2e2 +e
PI2S 2o2 +st 2ie2 +st 2i2 +st
PI3S 2o2 +t 2ie2 +t 2i2 +t
PI1P 2o2 +en 2e2 +en 2e2 +en
PI2P 2o2 +t 2e2 +t 2e2 +t
PI3P 2o2 +en 2e2 +en 2e2 +en
PS1S 2o2 +e 2e2 +e 2e2 +e
PS2S 2o2 +est 2e2 +est 2e2 +est
XI1S 2o2 +te 2a2 + 2a2 +
XI2S 2o2 +est 2a2 +st 2a2 +st
XS1S 2o2 +te 2a?2 +e 2a?2 +e
XS2S 2o2 +test 2a?2 +est 2a?2 +est
PP ge+ 2o2 +t ge+ 2e2 +en 2e2 +en
INF 2o2 +en 2e2 +en 2e2 +en
GER Lexicon
IC-1 IC-2 IC-3
holen sohlen sehen lesen vergeben begeben
?h,l? ?s,hl? ?s,h? ?l,s? ?verg,b? ?beg,b?
?fetch? ?sole? ?see? ?read? ?forgive? ?occur?
Table 4: Example of three inflectional classes (some cells
omitted in the interest of space economy) and associated
lemmas with their roots in German (?X? stands for past
tense). The different blocks in the IC table specify differ-
ent stem syncretism zones.
of sequences of single letters: the strong verbs have
monosyllabic stems (plus perhaps derivational mor-
phology), with the vowel in this stem potentially un-
dergoing changes. However, the onset and coda of
the stem syllable can be any consonant cluster al-
lowed by German phonology. Thus, in German, we
model roots as pairs of strings of any length (which
represent the onset and coda of the stem syllable).
Table 4 shows a weak IC and two strong ICs.
Since German strong verbs can have any stem
vowel, we assume that all eight vowel letters of Ger-
man (aeioua?o?u?) are pattern letters, and all other con-
sonant letters are root letters. German weak verbs
show no stem changes at all; if we tailored our tem-
plates to them, we would define all letters to be root
letters, and there would be no pattern letters at all.
This is in fact the experiment we reported on in Sec-
tion 4.6, and whose results are shown as NOTMP
in Table 5. However, since we do not know dur-
ing training time whether a verb is weak or strong,
all verbs will be modeled with an orthographic tem-
plate, even though there is no stem change at all.
Empirically Determined Pattern Letters For
GER, the EMPR approach, using the algorithm de-
scribed in Section 5.2, yields oaa?? as the optimal
1040
set of pattern letters. The EMPR pattern letters dif-
fer from the SCHLR pattern letters by omitting five
vowels, but including the ? variant of the s.
Results In table 5 we see that using all eight vow-
els as pattern letters (SCHLR column) in fact de-
creases performance at every training size (and rel-
atively more at smaller training sizes). However,
if we use the empirically obtained pattern letter set
oaa??, we see that we perform better than SCHLR
at almost all training sizes, and slightly better than
NOTMP at larger training sizes.
Error Analysis A manual inspection of all devel-
opment error types again revealed 8% development
set annotation errors and 4% acceptable variations.
To investigate why NOTMP outperforms SCHLR
for German on the development set, we performed
an oracle experiment: we assumed we knew for each
seen verb in training whether it is a weak or a strong
verb. If it is weak, we model it using NOTMP, and if
it is strong, using SCHLR. We observe as expected
that the performance on weak verbs is very similar to
that obtained using NOTMP on all verbs. However,
for the strong verbs, it is only when we have more
than 75,000 words of training data that the oracle
outperforms NOTMP. We assume that the reason is
that the highly frequent verbs are strong, but occur
frequently enough so that their IC can be learned di-
rectly from the training data. Using SCHLR simply
adds noise for these very frequent verbs. It is only
for the less frequent strong verbs that SCHLR can
contribute, and then only when a large amount of
training data is available.
5.4.2 Blind Test Set
Table 6 shows the accuracies the different sys-
tems on our blind test set. The results for the test
set are lower than those of the development set, and
NOTMP, SCHLR,and EMPR produce very simi-
lar accuracy results. We also compare our results
to those obtained by running the system of Durrett
and DeNero (2013) on the same training and test
data. Our system outperforms Durrett and DeNero
(2013)?s system reducing the error of by 5%.5
5We also tested our system on Durrett and DeNero (2013)?s
problem definition and data, training on 200 GER paradigms,
and testing on 200 unseen paradigms. This is the case of testing
for unseen lemmas in our system. Our system gives an accu-
racy of 88.4% as opposed to 91.8% as reported by Durrett and
DeNero (2013). Our system was not designed for this task.
Corpus Verb
BL NOTMP SCHLR EMPR
Size Count
0K 0 13.7 25.2 25.2 25.2
1K 81 43.3 72.0 67.0 69.4
5K 461 64.8 83.5 83.0 83.1
10K 929 72.1 89.0 87.4 88.6
25K 2,362.0 79.5 93.6 93.0 92.4
50K 4,527 86.2 95.6 95.1 95.6
75K 6,728 88.9 96.8 96.2 97.0
Table 5: Learning curve comparing system performance
for GER on tokens on DEV corpus. BL=Baseline
System Accuracy Error Reduction
Baseline 89.5
NOTMP 96.5 66.7
SCHLR 96.5 66.7
EMPR 96.5 66.7
Durrett 96.3 64.8
Table 6: Results for GER on tokens using a blind test set.
6 Conclusion and Future Work
We presented a method for automatically learn-
ing inflectional classes and associated lemmas from
morphologically annotated corpora. In the future,
we plan to improve several aspects of our mod-
els, in particular, using more powerful language-
independent template transformations to automati-
cally optimize for stem and affix modeling. We
plan to take the insights from this paper and ap-
ply them to new dialects and languages with lim-
ited resources. We are interested in extending our
approach to languages with different morphological
systems, e.g., agglutinative or reduplicative. We will
explore ideas from unsupervised morphology learn-
ing to minimize the need for morphological annota-
tions.
Acknowledgment
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-0014.
Any opinions, findings and conclusions or recom-
mendations expressed in this paper are those of the
authors and do not necessarily reflect the views of
DARPA.
1041
References
Sabine Brants and Silvia Hansen. 2002. Developments
in the tiger annotation scheme and their realization in
the corpus. In In Proceedings of the Third Conference
on Language Resources and Evaluation LREC-02. Las
Palmas de Gran Canaria, pages 1643?1649.
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Maris Camilleri. 2011. Island morphology: Morphol-
ogy?s interactions in the study of stem patterns. Lin-
guistica, 51:65?84. Internal and External Boundaries
of Morphology.
Burcu Can and Suresh Manandhar. 2012. Probabilistic
hierarchical clustering of morphological paradigms.
EACL 2012, page 654.
Erwin Chan. 2006. Learning probabilistic paradigms for
morphology in a latent class model. In Proceedings of
the Eighth Meeting of the ACL Special Interest Group
on Computational Phonology at HLT-NAACL, pages
69?78.
Lionel Cle?ment, Beno??t Sagot, and Bernard Lang. 2004.
Morphology based automatic acquisition of large-
coverage lexica. In LREC 04, pages 1841?1844.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing (TSLP), 4(1).
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of the 6th conference on
Natural language learning-Volume 20, pages 1?7.
Gre?goire De?trez and Aarne Ranta. 2012. Smart
paradigms and the predictability and complexity of in-
flectional morphology. EACL 2012, page 645.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using a
dirichlet process mixture model. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 616?627.
Greg Durrett and John DeNero. 2013. Supervised Learn-
ing of Complete Morphological Paradigms. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT), Atlanta, GA.
Ahmed El Kholy and Nizar Habash. 2010. Techniques
for Arabic morphological detokenization and ortho-
graphic denormalization. In Proceedings of LREC-
2010, May.
Ramy Eskander, Nizar Habash, Ann Bies, Seth Kulick,
and Mohamed Maamouri. 2013. Automatic correc-
tion and extension of morphological annotations. In
Proceedings of the 7th Linguistic Annotation Work-
shop and Interoperability with Discourse, pages 1?
10, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Raphael Finkel and Gregory Stump. 2002. Generating
Hebrew Verb Morphology by Default Inheritance Hi-
erarchies. In Proceedings of the Workshop on Compu-
tational Approaches to Semitic Languages, pages 9?
18.
Markus Forsberg, Harald Hammarstro?m, and Aarne
Ranta. 2006. Morphological lexicon extraction from
raw text data. Advances in Natural Language Process-
ing, pages 488?499.
Hassan Gadalla, Hanaa Kilany, Howaida Arram, Ashraf
Yacoub, Alaa El-Habashi, Amr Shalaby, Krisjanis
Karins, Everett Rowson, Robert MacIntyre, Paul
Kingsbury, David Graff, and Cynthia McLemore.
1997. CALLHOME Egyptian Arabic Transcripts. In
Linguistic Data Consortium, Philadelphia.
Hassan Gadalla. 2000. Comparative Morphology of
Standard and Egyptian Arabic. LINCOM EUROPA.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 681?688, Sydney, Australia.
Nizar Habash, Owen Rambow, and George Kiraz. 2005.
Morphological Analysis and Generation for Arabic
Dialects. In Proceedings of the ACL Workshop
on Computational Approaches to Semitic Languages,
pages 17?24, Ann Arbor, Michigan.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012. A Morphological Analyzer for Egyptian Ara-
bic. In Proceedings of the Twelfth Meeting of the Spe-
cial Interest Group on Computational Morphology and
Phonology, pages 1?9, Montre?al, Canada.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Harald Hammarstro?m and Lars Borin. 2011. Unsuper-
vised learning of morphology. Computational Lin-
guistics, 37(2):309?350.
H. Kilany, H. Gadalla, H. Arram, A. Yacoub, A. El-
Habashi, and C. McLemore. 2002. Egyptian
Colloquial Arabic Lexicon. LDC catalog number
LDC99L22.
Kimmo Koskenniemi. 1983. Two-Level Model for Mor-
phological Analysis. In Proceedings of the 8th In-
1042
ternational Joint Conference on Artificial Intelligence,
pages 683?685.
Christian Monson, Jaime Carbonell, Alon Lavie, and Lori
Levin. 2008. Paramor: Finding paradigms across
morphology. Advances in Multilingual and Multi-
modal Information Retrieval, pages 900?907.
Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes. In
Proceedings of the ACL-02 workshop on Morphologi-
cal and phonological learning-Volume 6, pages 31?40.
Association for Computational Linguistics.
Matthew G Snover, Gaja E Jarosz, and Michael R Brent.
2002. Unsupervised learning of morphology using a
novel directed search algorithm: Taking the first step.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6, pages
11?20.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of ACL-08: HLT, pages 737?
745, Columbus, Ohio, June.
Gregory T. Stump. 2001. Inflectional Morphology. A
Theory of Paradigm Structure. Cambridge Studies in
Linguistics. Cambridge University Press.
Ge?raldine Walther. 2011. Measuring morphological
canonicity. Linguistica, 51:157?180. Internal and Ex-
ternal Boundaries of Morphology.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 207?216.
1043
Proceedings of NAACL-HLT 2013, pages 426?432,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Morphological Analysis and Disambiguation for Dialectal Arabic
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskander, and Nadi Tomeh
Center for Computational Learning Systems
Columbia University
{habash,ryanr,rambow,reskander,nadi}@ccls.columbia.edu
Abstract
The many differences between Dialectal Ara-
bic and Modern Standard Arabic (MSA) pose
a challenge to the majority of Arabic natural
language processing tools, which are designed
for MSA. In this paper, we retarget an exist-
ing state-of-the-art MSA morphological tag-
ger to Egyptian Arabic (ARZ). Our evalua-
tion demonstrates that our ARZ morphology
tagger outperforms its MSA variant on ARZ
input in terms of accuracy in part-of-speech
tagging, diacritization, lemmatization and to-
kenization; and in terms of utility for ARZ-to-
English statistical machine translation.
1 Introduction
Dialectal Arabic (DA) refers to the day-to-day na-
tive vernaculars spoken in the Arab World. DA
is used side by side with Modern Standard Arabic
(MSA), the official language of the media and edu-
cation (Holes, 2004). Although DAs are historically
related to MSA, there are many phonological, mor-
phological and lexical differences between them.
Unlike MSA, DAs have no standard orthographies
or language academies. Furthermore, different DAs,
such as Egyptian Arabic (henceforth, ARZ), Levan-
tine Arabic or Moroccan Arabic have important dif-
ferences among them, similar to those seen among
Romance languages (Holes, 2004; Abdel-Massih et
al., 1979). Most tools and resources developed for
natural language processing (NLP) of Arabic are de-
signed for MSA. Such resources are quite limited
when it comes to processing DA, e.g., a state-of-
the-art MSA morphological analyzer only has 60%
coverage of Levantine Arabic verb forms (Habash
and Rambow, 2006).
In this paper, we describe the process of retar-
geting an existing state-of-the-art tool for model-
ing MSA morphology disambiguation to ARZ, the
most commonly spoken DA. The MSA tool we
extend is MADA ? Morphological Analysis and
Disambiguation of Arabic (Habash and Rambow,
2005). The approach used in MADA, which was
inspired by earlier work by Hajic? (2000), disam-
biguates in context for every aspect of Arabic mor-
phology, thus solving all tasks in ?one fell swoop?.
The disadvantage of the MADA approach is its de-
pendence on two complex resources: a morpholog-
ical analyzer for the language and a large collection
of manually annotated words for all morphological
features in the same representation used by the an-
alyzer. For ARZ, such resources have recently be-
come available, with the development of the CAL-
IMA ARZ morphological analyzer (Habash et al,
2012b) and the release by the Linguistic Data Con-
sortium (LDC) of a large ARZ corpus annotated
morphologically in a manner compatible with CAL-
IMA (Maamouri et al, 2012a). In the work pre-
sented here, we utilize these new resources within
the paradigm of MADA, transforming MADA into
MADA-ARZ. The elegance of the MADA solution
makes this conceptually a simple extension.
Our evaluation demonstrates that our Egyptian
DA version of MADA, henceforth MADA-ARZ,
outperforms MADA for MSA on ARZ morpholog-
ical tagging and improves the quality of ARZ to En-
glish statistical machine translation (MT).
The rest of this paper is structured as follows:
Section 2 discusses related work. Section 3 presents
the challenges of processing Arabic dialects. Sec-
tion 4 outlines our approach. And Section 5 presents
and discusses our evaluation results.
426
2 Related Work
There has been a considerable amount of work on
MSA morphological analysis, disambiguation, part-
of-speech (POS) tagging, tokenization, lemmatiza-
tion and diacritization; for an overview, see (Habash,
2010). Most solutions target specific problems, such
as diacritization (Zitouni et al, 2006), tokenization
or POS tagging (Diab et al, 2007). In contrast,
MADA provides a solution to all of these problems
together (Habash and Rambow, 2005).
Previous work on DA morphological tagging fo-
cused on creating resources, using noisy or in-
complete annotations, and using unsupervised/semi-
supervised methods. Duh and Kirchhoff (2005)
adopt a minimally supervised approach that only re-
quires raw text data from several DAs, as well as a
MSA morphological analyzer. They report a POS
accuracy of 70.9% on a rather coarse-grained POS
tagset (17 tags).
Al-Sabbagh and Girju (2012) describe a super-
vised tagger for Egyptian Arabic social networking
corpora trained using transformation-based learning
(Brill, 1995). They report 94.5% F-measure on to-
kenization and 87.6% on POS tagging. Their tok-
enization and POS tagsets are comparable to the set
used by the Arabic Treebank (ATB). We do not com-
pare to them since their data sets are not public.
Stallard et al (2012) show that unsupervised
methods for learning DA tokenization can outper-
form MSA tokenizers on MT from Levantine Ara-
bic to English. We do not compare to them directly
since our work is on ARZ. However, we carry a sim-
ilar MT experiment in Section 5.
Mohamed et al (2012) annotated a small corpus
of Egyptian Arabic for morphological segmentation
and learned segmentation models using memory-
based learning (Daelemans and van den Bosch,
2005). Their best system achieves a 91.90% accu-
racy on the task of morpheme-segmentation. We
compare to their work and report on their test set
in Section 5.
There are some other morphological analyzers for
DA. Kilany et al (2002) worked on ARZ, but the
analyzer has very limited coverage. Their lexicon
was used as part of the development of CALIMA
(Habash et al, 2012b). Other efforts are not about
ARZ (Habash and Rambow, 2006; Salloum and
Habash, 2011).
Given the similarity between MSA and DA, there
has been some work on mapping DA to MSA to
exploit rich MSA resources (Chiang et al, 2006;
Abo Bakr et al, 2008; Salloum and Habash, 2011;
Salloum and Habash, 2013). Other researchers have
studied the value of simply combining DA and
MSA data, such as Zbib et al (2012) for DA to En-
glish MT. In our approach, we target DA directly,
and we evaluate the use of additional MSA anno-
tated resources to our training in Section 5.
3 Arabic Dialect Challenges
General Arabic Challenges Arabic, as MSA or
DA, poses many challenges for NLP. Arabic is a
morphologically complex language which includes
rich inflectional morphology and a number of cli-
tics. For example, the MSA word A? 	E?J.

J?J
?? wsyk-
tbwnhA (wa+sa+ya-ktub-uwna+hA)1 ?and they will
write it [lit. and+will+they-write-they+it]? has two
proclitics, one circumfix and one pronominal en-
clitic. Additionally, Arabic has a high degree of
ambiguity resulting from its diacritic-optional writ-
ing system and common deviation from spelling
standards (e.g., Alif and Ya variants) (Buckwalter,
2007). The Standard Arabic Morphological Ana-
lyzer for (SAMA) (Graff et al, 2009) produces 12
analyses per MSA word on average.
Differences between ARZ and MSA As men-
tioned above, most tools developed for MSA cannot
be expected to perform well on ARZ. This is due
to the numerous differences between the two vari-
ants. Lexically, the number of differences is quite
significant. For example, ARZ

?
	Q
K. Q? Trbyzh? ?table?
corresponds to MSA

???A? TAwlh?. Phonologically,
there are many important differences which relate
to orthography in DA, e.g., the MSA consonant H
/?/ is pronounced as /t/ in ARZ (or /s/ in more re-
cent borrowings from MSA); for a fuller discussion,
see (Habash, 2010; Habash et al, 2012a). Examples
of morphological differences include changes in the
1Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007): (in alphabetical or-
der) Abt?jHxd?rzs?SDTD???fqklmnhwy and the additional sym-
bols: ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', h?

?, ? ?.
427
morpheme form, e.g., the MSA future proclitic +?
sa+ appears in ARZ as +? ha+. There are some
morphemes in ARZ that do not exist in MSA such
as the negation circum-clitic ?+ . . . + A? mA+ . . . +?.
And there are MSA features that are absent from
ARZ, most notably case and mood.
Since there are no orthographic standards, ARZ
words may be written in a variety of ways reflect-
ing different writing rules, e.g., phonologically or
etymologically. A conventional orthography for Di-
alectal Arabic (CODA) has been proposed and used
for writing ARZ in the context of NLP applications
(Habash et al, 2012a; Al-Sabbagh and Girju, 2012;
Eskander et al, 2013). Finally, MSA and ARZ co-
exist and are often used interchangeably, especially
in more formal settings. The CALIMA morpholog-
ical analyzer we use addresses several of these issues
by modeling both ARZ and MSA together, includ-
ing a limited set of inter-dialect morphology phe-
nomena, and by mapping ARZ words into CODA
orthography internally while accepting a wide range
of spelling variants.
4 Approach
4.1 The MADA Approach
MADA is a method for Arabic morphological anal-
ysis and disambiguation (Habash and Rambow,
2005; Roth et al, 2008). MADA uses a morpholog-
ical analyzer to produce, for each input word, a list
of analyses specifying every possible morphological
interpretation of that word, covering all morphologi-
cal features of the word (diacritization, POS, lemma,
and 13 inflectional and clitic features). MADA then
applies a set of models (support vector machines
and N-gram language models) to produce a predic-
tion, per word in-context, for different morpholog-
ical features, such as POS, lemma, gender, number
or person. A ranking component scores the analy-
ses produced by the morphological analyzer using a
tuned weighted sum of matches with the predicted
features. The top-scoring analysis is chosen as the
predicted interpretation for that word in context.
4.2 Extending MADA into MADA-ARZ
Adjusting MADA to handle DA requires a number
of modifications. The most significant change is re-
placing the MSA analyzer SAMA with the ARZ
analyzer CALIMA to address the differences out-
lined in Section 3. In addition, new feature predic-
tion models are needed; these are trained using ARZ
data sets annotated by the LDC (Maamouri et al,
2006; Maamouri et al, 2012b). The data sets were
not usable as released due to numerous annotation
inconsistencies and differences from CALIMA, as
well due to gaps in CALIMA. We synchronized the
annotations with the latest version of CALIMA fol-
lowing a technique described by Habash and Ram-
bow (2005). The result of this synchronization step
is the data we use in this study (for training, de-
velopment and testing). Our synchronized annota-
tions fully match the LDC annotations in 90% of the
words (in full morphological tag). We performed a
manual analysis on randomly chosen 100 words that
did not fully match. The choice we made is cor-
rect or acceptable in 55% of the cases of mismatch
with the LDC annotation, which means that the our
choice is accurate in over 95% of all cases.
Some of the original MADA features (which
were needed for MSA) are not used in ARZ and
so are dropped in MADA-ARZ; these features are
case, mood, the question-marking proclitic, state
and voice. Additional ARZ feature values have been
added, e.g., to handle the progressive particle and
future marker, among others. These are provided
by CALIMA and are classified and selected by
MADA-ARZ. In our current implementation, ARZ
features that are not present in MSA, such as the
negation and indirect-object enclitics, are not classi-
fied by MADA-ARZ classifiers, but since they are
provided by CALIMA they can be selected by the
whole MADA-ARZ system.
5 Evaluation
We evaluate MADA-ARZ intrinsically ? in terms
of performance on morphological disambiguation
? and extrinsically in the context of MT.
5.1 POS Tagging, Diacritization,
Lemmatization and Segmentation
Experimental Settings We use two sets of anno-
tated data from the LDC: ATB-123, which includes
parts 1, 2 and 3 of the MSA Penn Arabic Treebank
428
Development Test
MADA MADA-ARZ MADA MADA-ARZ
Train Data MSA ARZ ALL MSA ARZ ALL
Morph Tag 35.8 84.0 77.3 35.7 84.5 75.5
Penn POS 77.5 89.6 90.2 79.0 90.0 90.1
MADA POS 80.7 90.8 91.3 82.1 91.1 91.4
Diacritic 31.3 82.6 72.9 32.2 83.2 72.2
Lemma 64.0 85.2 81.6 67.1 86.3 82.8
Full 26.2 74.3 65.4 27.0 75.4 64.7
ATB Segmentation 90.6 97.4 97.6 90.5 97.4 97.5
Table 1: Evaluation metrics on the ATB-ARZ development and test sets. The best results are bolded. We compare
MADA and MADA-ARZ with different training data conditions. Definitions of metrics are in Section 5.1. MSA
training data is ATB-123. ARZ training data is ATB-ARZ. ALL training data is ATB-123 plus ATB-ARZ.
(Maamouri et al, 2004); and ATB-ARZ, the Egyp-
tian Arabic Treebank (parts 1-5) (Maamouri et al,
2012a). For ATB-123 training, we use all of parts 1
and 2 plus the training portion of ATB-3 (as defined
by Zitouni et al (2006)); for development and test,
we split Zitouni et al (2006)?s devtest set into two.
We sub-divide ATB-ARZ into development, train-
ing, and test sets (roughly a 10/80/10 split). The
ATB-ARZ training data has 134K words, and the
ATB-123 training data has 711K words.
We evaluate two systems. We used the latest re-
lease of MADA for MSA (v3.2), trained on ATB-
123 (MSA), as our baseline. For MADA-ARZ,
we compare two training settings: using ATB-ARZ
(ARZ) and combining ATB-ARZ with ATB-123
(ALL). We present our results on the ATB-ARZ
development and blind test sets (21.1K words and
20.4K words). Tuning for MADA-ARZ was done
using a random 10% of the ATB-ARZ training data,
which was later integrated back into the training set.
Metrics We use several evaluation metrics to mea-
sure the effectiveness of MADA-ARZ. Morph Tag
refers to the accuracy of correctly predicting the full
CALIMA morphological tag (i.e., not the diacritics
or the lemma). Penn POS and MADA POS are also
tag accuracy metrics. Penn POS, also known as the
Reduced Tag Set, is a tag set reduction of the full
Arabic morphological tag set, which was proposed
for MSA (Kulick et al, 2006; Diab, 2007; Habash,
2010); since it retains no MSA-specific morpholog-
ical features, it also makes sense for ARZ. MADA
POS is the small POS tag set (36 tags) MADA uses
internally. Diacritic and Lemma are the accura-
cies of the choice of diacritized form and Lemma,
respectively. Full is the harshest metric, requiring
that every morphological feature of the chosen anal-
ysis be correct. Finally, ATB Segmentation is the
percentage of words with correct ATB segmentation
(splitting off all clitics except for the determiner +?@
Al+).
Results The results are shown in Table 1.
MADA-ARZ performs much better than the
MADA baselines in all evaluation metrics. Compar-
ing the two MADA-ARZ systems, it is evident that
adding MSA data (ATB123) results in slightly better
performance only for the Penn POS, MADA POS,
and ATB Segmentation metrics. Including the MSA
data results in accuracy reductions for the other met-
rics, but the resulting system still outperforms the
MADA MSA baseline in all cases. The results are
consistent for development and blind test.
The CMUQ-ECA Test Set Mohamed et al
(2012) reported on the task of ARZ raw orthogra-
phy morph segmentation (determining the morphs
in the raw word). The CMUQ-ECA test data
comprised 36 ARZ political comments and jokes
from the Egyptian web site www.masrawy.com.
The set contains 2,445 words including punctua-
tion. Their best system gets a 91.9% word-level ac-
curacy. Since MADA-ARZ modifies the spelling
429
Tokenization OOV BLEU METEOR TER
Punct 9.2 22.1 27.2 63.2
MADA ATB 5.8 24.4 29.6 60.5
MADA-ARZ ATB 4.9 25.2 29.9 59.4
Table 2: Machine translation results on the test set. ?Punct? refers to the baseline which only tokenizes at punctuation.
of the word when it maps into CODA, we needed
a manual analysis where no exact match with the
gold occurs (11.8% of the time). We determined
MADA-ARZ?s accuracy on their test set for morph-
segmentation to be 93.2%.
5.2 Egyptian Arabic to English MT
MT Experimental Settings We use the open-
source Moses toolkit (Koehn et al, 2007) to build
a phrase-based SMT system. We use MGIZA++
for word alignment (Gao and Vogel, 2008). Phrase
translations of up to 8 words are extracted in the
phrase table. We use SRILM (Stolcke, 2002) with
modified Kneser-Ney smoothing to build two 4-
gram language models. The first model is trained
on the English side of the bitext, while the other
is trained on the English Gigaword data. Feature
weights are tuned to maximize BLEU (Papineni et
al., 2002) on a development set using Minimum Er-
ror Rate Training (Och, 2003). We perform case-
insensitive evaluation in terms of BLEU , METEOR
(Banerjee and Lavie, 2005) and TER (Snover et al,
2006) metrics.
Data We trained on DA-English parallel data
(Egyptian and Levantine) obtained from several
LDC corpora. The training data amounts to 3.8M
untokenized words on the Arabic side. The dev set,
used for tuning the parameters of the MT system,
has 15,585 untokenized Arabic words. The test set
has 12,116 untokenized Arabic words. Both dev and
test data contain two sets of reference translations.
The English data is lower-cased and tokenized using
simple punctuation-based rules.
Systems We build three translation systems which
vary in tokenization of the Arabic text. The first
system applies only simple punctuation-based rules.
The second and third systems use MADA and
MADA-ARZ, respectively, to tokenize the Arabic
text in the ATB tokenization scheme (Habash and
Sadat, 2006). The Arabic text is also Alif/Ya nor-
malized.
Results The MT results are in Table 2, which also
shows the percentage of out-of-vocabulary (OOV)
words ? test words not in the training data. MADA-
ARZ delivers the best translation performance ac-
cording to all metrics. All MADA-ARZ improve-
ments over MADA are statistically significant at
the .01 level (except in the case of METEOR). All
improvements over Punct by MADA and MADA-
ARZ are also statistically significant. For BLEU
scores, we observe 3.1% absolute improvement to
Punct (14% relative), and 0.8% absolute improve-
ment to MADA (3.3% relative). In addition to bet-
ter morphological disambiguation, MADA-ARZ
reduces the OOV ratio (16% relative to MADA),
which we suspect contributes to the observed im-
provements in MT quality.
6 Conclusion and Future Work
We have presented MADA-ARZ, a system for
morphological tagging of ARZ. We have shown
that it outperforms an state-of-the-art MSA tagger
(MADA) on ARZ text, and that it helps ARZ-to-
English machine translation more than MADA.
In the future, we intend to perform further feature
engineering to improve the results of MADA-ARZ,
and extend the system to handle other DAs.
Acknowledgments
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-0014.
Any opinions, findings and conclusions or recom-
mendations expressed in this paper are those of the
authors and do not necessarily reflect the views of
DARPA.
430
References
Ernest T. Abdel-Massih, Zaki N. Abdel-Malek, and El-
Said M. Badawi. 1979. A Reference Grammar of
Egyptian Arabic. Georgetown University Press.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan.
2008. A Hybrid Approach for Converting Written
Egyptian Colloquial Dialect into Diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008. Cairo University.
Rania Al-Sabbagh and Roxana Girju. 2012. A super-
vised POS tagger for written Arabic social network-
ing corpora. In Jeremy Jancsary, editor, Proceedings
of KONVENS 2012, pages 39?52. ?GAI, September.
Main track: oral presentations.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Arbor,
Michigan.
Eric Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part-of-Speech Tagging. Computational Lin-
guistics, 21(4):543?565.
Tim Buckwalter. 2007. Issues in Arabic Morphologi-
cal Analysis. In A. van den Bosch and A. Soudi, edi-
tors, Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
Dialects. In Proceedings of the European Chapter of
ACL (EACL).
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2007.
Automated methods for processing arabic text: From
tokenization to base phrase chunking. In Antal
van den Bosch and Abdelhadi Soudi, editors, Arabic
Computational Morphology: Knowledge-based and
Empirical Methods. Kluwer/Springer.
Mona Diab. 2007. Improved Arabic Base Phrase Chunk-
ing with a New Enriched POS Tag Set. In Proceedings
of the 2007 Workshop on Computational Approaches
to Semitic Languages: Common Issues and Resources,
pages 89?96, Prague, Czech Republic, June.
Kevin Duh and Katrin Kirchhoff. 2005. POS tagging of
dialectal Arabic: a minimally supervised approach. In
Proceedings of the ACL Workshop on Computational
Approaches to Semitic Languages, Semitic ?05, pages
55?62, Ann Arbor, Michigan.
Ramy Eskander, Nizar Habash, Owen Rambow, and Nadi
Tomeh. 2013. Processing Spontaneous Orthogra-
phy. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL-HLT), Atlanta, GA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?57,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
David Graff, Mohamed Maamouri, Basma Bouziri,
Sondos Krouna, Seth Kulick, and Tim Buckwal-
ter. 2009. Standard Arabic Morphological Analyzer
(SAMA) Version 3.1. Linguistic Data Consortium
LDC2009E73.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A Morphological Analyzer and Generator for the Ara-
bic Dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 681?688, Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic Prepro-
cessing Schemes for Statistical Machine Translation.
In Proceedings of the 7th Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics/Human Language Technologies Conference
(HLT-NAACL06), pages 49?52, New York, NY.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In NAACL-HLT 2012 Workshop on Com-
putational Morphology and Phonology (SIGMOR-
PHON2012), pages 1?9.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?. 2000. Morphological tagging: Data vs. dic-
tionaries. In Proceedings of the 1st Meeting of the
431
North American Chapter of the Association for Com-
putational Linguistics (NAACL?00), Seattle, WA.
Clive Holes. 2004. Modern Arabic: Structures, Func-
tions, and Varieties. Georgetown Classics in Ara-
bic Language and Linguistics. Georgetown University
Press.
H. Kilany, H. Gadalla, H. Arram, A. Yacoub, A. El-
Habashi, and C. McLemore. 2002. Egyptian
Colloquial Arabic Lexicon. LDC catalog number
LDC99L22.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguis-
tic Theories Conference, pages 31?42, Prague, Czech
Republic.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank :
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, Mona
Diab, Nizar Habash, Owen Rambow, and Dalila
Tabessi. 2006. Developing and Using a Pilot Dialec-
tal Arabic Treebank. In The fifth international confer-
ence on Language Resources and Evaluation (LREC),
pages 443?448, Genoa, Italy.
Mohamed Maamouri, Ann Bies, Seth Kulick, Dalila
Tabessi, and Sondos Krouna. 2012a. Egyptian Ara-
bic Treebank Pilot.
Mohamed Maamouri, Sondos Krouna, Dalila Tabessi,
Nadia Hamrouni, and Nizar Habash. 2012b. Egyptian
Arabic Morphological Annotation Guidelines.
Emad Mohamed, Behrang Mohit, and Kemal Oflazer.
2012. Annotating and Learning Morphological Seg-
mentation of Egyptian Colloquial Arabic. In Proceed-
ings of the Language Resources and Evaluation Con-
ference (LREC), Istanbul.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of the 41st Annual Conference of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological tag-
ging, diacritization, and lemmatization using lexeme
models and feature ranking. In ACL 2008: The Con-
ference of the Association for Computational Linguis-
tics; Companion Volume, Short Papers, Columbus,
Ohio.
Wael Salloum and Nizar Habash. 2011. Dialectal
to Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, pages 10?21, Edinburgh, Scotland.
Wael Salloum and Nizar Habash. 2013. Dialectal Ara-
bic to English Machine Translation: Pivoting through
Modern Standard Arabic. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), Atlanta, GA.
Matt Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Error Rate with Targeted Human An-
notation. In Proceedings of the Association for Ma-
chine Transaltion in the Americas (AMTA 2006), Cam-
bridge, Massachusetts.
David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012.
Unsupervised Morphology Rivals Supervised Mor-
phology for Arabic MT. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 322?327, Jeju Island, Korea.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 49?
59, Montr?al, Canada.
Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.
2006. Maximum entropy based restoration of ara-
bic diacritics. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 577?584, Sydney, Australia.
432
Proceedings of NAACL-HLT 2013, pages 585?595,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Processing Spontaneous Orthography
Ramy Eskander, Nizar Habash, Owen Rambow, and Nadi Tomeh
Center for Computational Learning Systems
Columbia University
{reskander,habash,rambow,nadi}@ccls.columbia.edu
Abstract
In cases in which there is no standard or-
thography for a language or language vari-
ant, written texts will display a variety of or-
thographic choices. This is problematic for
natural language processing (NLP) because it
creates spurious data sparseness. We study
the transformation of spontaneously spelled
Egyptian Arabic into a conventionalized or-
thography which we have previously proposed
for NLP purposes. We show that a two-stage
process can reduce divergences from this stan-
dard by 69%, making subsequent processing
of Egyptian Arabic easier.
1 Introduction
In areas with diglossia, vernacular spoken variants
(?low?) of a language family co-exist with a largely
written variant (?high?), which is often not spoken
as a native language. Traditionally, the low variants
have not been written: written language is reserved
for formal occasions and in those formal occasions
only the high variant is used. Prototypical exam-
ples of diglossia are the German speaking parts of
Switzerland, and the Arab world. The advent of the
internet has changed linguistic behavior: it is now
common to find written informal conversations, in
the form of email exchanges, text messages, Twit-
ter exchanges, and interactions on blogs and in web
forums. These written conversations are typically
written in the low variants (or in a mixture of low and
high), since conversations in the high variant seem
unnatural to the discourse participants. For natural
language processing (NLP), this poses many chal-
lenges, one of which is the fact that the low vari-
ants have not been written much in the past and
do not have a standard orthography which is gen-
erally agreed on by the linguistic community (and
perhaps sanctioned by an authoritative institution).
Instead, each discourse participant devises a spon-
taneous orthography, in which she chooses among
conventions from the high variant to render the spo-
ken language. We are thus faced with a large number
of ways to spell the same word, none of which can
be assumed as ?standard? since there is no standard.
As a result, the increased data sparseness adds to the
challenges of NLP tasks such as machine transla-
tion, compared to languages for which orthography
is standardized.
In this paper, we work on Egyptian Arabic
(EGY). We follow the conventions which we have
previously proposed for the normalized orthogra-
phy for EGY (Habash et al, 2012), called CODA
(Conventional Orthography for Dialectal Arabic). In
this paper, we investigate how easy it is to con-
vert spontaneous orthography of EGY written in
Arabic script into CODA orthography automatically.
We will refer to this process as ?normalization? or
?codafication?. We present a freely available system
called CODAFY, which we propose as a preproces-
sor for NLP modules for EGY. We show that a ?do
nothing? baseline achieves a normalization perfor-
mance of 75.5%, and CODAFY achieves a normal-
ization performance of 92.4%, an error reduction of
69.2% over this baseline on an unseen test set.
The paper is structured as follows. We first re-
view relevant linguistic facts in Section 2 and then
present the conventionalized orthography we use in
this paper. After reviewing related work in Sec-
tion 4, we present our data (Section 5), our approach
(Section 6), and our results (Section 7). We conclude
585
with a discussion of future work.
2 Linguistic Facts
2.1 Writing without a Standard Orthography
An orthography is a specification of how the words
of a language are mapped to and from a particular
script (in our case, the Arabic script). In cases when
a standard orthography is absent, writers make deci-
sions about spontaneous orthography based on vari-
ous criteria. Most prominent among them is phonol-
ogy: how can my pronunciation of the word be
rendered in the chosen writing system, given some
(language-specific) assumptions about grapheme-to-
phoneme mapping? Often, these assumptions come
from the ?high? variant of the language, or some re-
lated language. Another criterion for choosing or-
thography is a cognate in a related language or lan-
guage variant (Modern Standard Arabic or MSA,
the high variant for EGY), where a cognate pair is
a pair of words (or morphemes) in two languages
or language variants which are related by etymol-
ogy (in some unspecified manner) and which have
roughly the same meaning. Finally, the chosen
spontaneous orthography can be altered to reflect
speech effects, notably the lengthening of syllables
to represent emphasis or other effects (such as Q
J
J



J?
ktyyyyr1 ?very?).
It is important to distinguish typos from sponta-
neous orthography. We define spontaneous orthog-
raphy to be an intentional choice of graphemes to
render the words in a language or language variant.
We define a typographical error (typo) to be an un-
intended sequence of graphemes. For example, @Y?
kdA and ?Y? kdh can be intended spellings for EGY
/kida/ ?like this?, while @Q? krA is not a plausible in-
tentional spelling since it neither relates /kida/ to an
MSA cognate, nor does the sequence of graphemes
represent the phonology of EGY using standard as-
sumptions. Instead, we can explain the spelling by
assuming that the writer accidentally substituted the
grapheme P for the grapheme X, which look some-
what alike and are near each other on some Arabic
keyboard layouts. Of course, when we encounter
1Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007): (in alphabetical or-
der) Abt?jHxd?rzs?SDTD???fqklmnhwy and the additional sym-
bols: ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', h?

?, ? ?.
a specific spelling in a corpus, it can be, in certain
cases, difficult to determine whether it is a conscious
choice or a typo.
2.2 Relevant Differences between EGY and
MSA
Lexical Variations Lexically, the number of differ-
ences is quite significant. For example, EGY H@Q?
mrAt ?wife [of]? corresponds to MSA

?k. ?
	P zwjh?. In
such cases of lexical difference, no cognate spelling
is available.
Phonological Variations There is an extensive
literature on the phonology of Arabic dialects (Wat-
son, 2002; Holes, 2004; Habash, 2010). Several
phonological differences exist between EGY and
MSA which relate to orthography. Here, we dis-
cuss one representative difference. The MSA con-
sonant H /?/ is pronounced as /t/ in EGY (or /s/
in more recent borrowings from MSA). For exam-
ple, MSA Q

?K
 yk?r ?increase (imperfective)? is pro-
nounced /yak?ur/ in MSA versus /yiktar/ in EGY,
giving rise to the EGY phonological spelling Q

?K

yktr.
Morphological Variations There are a lot of
morphological differences between MSA and EGY.
For orthography, two differences are most relevant.
The MSA future proclitic /sa/+ (spelled +? s+) ap-
pears in EGY as /ha/+ or /Ha/+. The two forms ap-
pear in free variation, and we have not been able
to find a variable that predicts which form is used
when. This variation is not a general phonological
variation between /h/ and /H/, we find it only in this
morpheme. Predictably, this leads to two spellings
in EGY: +h H+ and +? h+. Negation in EGY is
realized as the circum-clitic /ma?/+ . . . +/?/. The prin-
cipal orthographic question is whether the prefix is
a separate word or is part of the main word; both
variants are found.
3 CODA
CODA is a conventionalized orthography for Arabic
dialects (Habash et al, 2012). In this section, we
summarize CODA so that the reader can understand
the goals of this paper. CODA has five key proper-
ties.
1. CODA is an internally consistent and coherent
convention for writing DA: every word has a
586
single orthographic rendering.
2. CODA is created for computational purposes.
3. CODA uses the Arabic script as used for MSA,
with no extra symbols from, for example, Per-
sian or Urdu.
4. CODA is intended as a unified framework for
writing all dialects. In this paper, we only dis-
cuss the instantiation of CODA for EGY.
5. CODA aims to maintain a level of dialectal
uniqueness while using conventions based on
similarities between MSA and the dialects.
We list some features of CODA relevant to this
paper.
Phonological Spelling CODA generally uses
phonological spelling for EGY (as MSA spelling
does for MSA).
Etymologically Spelled Consonants A limited
number of consonants may be spelled differently
from their phonology if the following two conditions
are met: (1) the consonant must be an EGY root
radical and (2) the EGY root must have a cognate
MSA root. If the conditions are met, then we spell
the consonant using the corresponding radical from
the cognate MSA root of the dialectal word?s root.
One such example is the spelling of the EGY verb
pronounced /kitir/ as Q

? k?r ? it increased?.
Morphologically Faithful CODA preserves di-
alectal morphology and spells the dialectal mor-
phemes (clitics and inflections) phonologically. For
example, for the attachable future marker clitic, the
variant +h is chosen, not the MSA +?, so that
EGY /Hatiktar/ (and its variant /hatiktar/) are both
spelled Q

?

Jk Htk?r. The negation prefix and indi-
rect object pronoun (l+pronoun) suffixes are sepa-
rated, e.g., ?A?? I?

? A? mA qlt lhA? /ma?ultilha??/ ?I
did not tell her?.
Alif-Maqsura The letter ? ? is often used in
Egypt to write word-final ?


y and vice versa (even
when writing MSA). In CODA, all rules for us-
ing Alif-Maqsura are the same as MSA. For exam-
ple, EGY /maSr??/ ?Egyptian? can be seen in sponta-
neous orthography as ?Q??? mSr?, but in CODA it
is ?


Q??? mSry.
Ta-Marbuta As in MSA, the Ta-Marbuta ( ? h?) is
used morphemically in CODA. The Ta-Marbuta is
always written as

? h? in CODA, e.g., /?arba?a/ ?four?
is

??K. P

@ ?rb?h? in CODA, though it can be found as
??K. P

@ ?rb?h (or ??K. P@ Arb?h) in spontaneous orthog-
raphy.
Lexical Exceptions EGY CODA guidelines in-
clude a word list specifying ad hoc spellings of EGY
words that may be inconsistent with the default map-
ping outlined above. An example is /kida/ ?like this?,
which we find as both @Y? kdA and ?Y? kdh in spon-
taneous orthography; the CODA spelling is ?Y? kdh.
4 Related Work
To our knowledge, this paper is the first to discuss
the task of automatically providing a conventional-
ized spelling for a written Arabic dialect text. While
there is no direct precedent, we discuss here some
related research.
Our proposed work has some similarity to auto-
matic spelling correction (ASC) and related tasks
such as post editing for optical character recogni-
tion (OCR). Our task is different from ASC since
ASC work assumes a standard orthography that the
writer is also assumed to aim for. Both supervised
and unsupervised approaches to this task have been
explored. Unsupervised approaches rely on improv-
ing the fluency of the text and reducing the percent-
age of out-of-vocabulary words using NLP tools, re-
sources, and heuristics, e.g., morphological analyz-
ers, language models, and edit-distance measure, re-
spectively (Kukich, 1992; Oflazer, 1996; Ben Oth-
mane Zribi and Ben Ahmed, 2003; Shaalan et al,
2003; Haddad and Yaseen, 2007; Hassan et al,
2008; Shaalan et al, 2010; Alkanhal et al, 2012).
Supervised approaches learn models of correction
by training on paired examples of errors and their
corrections. This data is hard to come by nat-
urally, though for applications such as OCR cor-
pora can be created from the application itself (Ko-
lak and Resnik, 2002; Magdy and Darwish, 2006;
Abuhakema et al, 2008; Habash and Roth, 2011).
There has been some work on conversion of di-
alectal Arabic to MSA. Al-Gaphari and Al-Yadoumi
(2010) introduced a rule-based method to convert
Sanaani dialect to MSA, and Shaalan et al (2007)
used a rule-based lexical transfer approach to trans-
form from EGY to MSA. Similarly, both Sawaf
587
(2010) and Salloum and Habash (2011) showed
that translating dialectal Arabic to MSA can im-
prove dialectal Arabic machine translation into En-
glish by pivoting on MSA. A common feature across
these conversion efforts is the use of morphological
analysis and morphosyntactic transformation rules
(for example, Al-Gaphari and Al-Yadoumi (2010)).
While all this work is similar to ours in that dialec-
tal input is processed, our output is still dialectal,
while the work on conversion aims for a transforma-
tion into MSA.
The work most closely related to ours is that of
Dasigi and Diab (2011). They identify the spelling
variants in a given document and normalize them.
However, they do not present a system that con-
verts spontaneous spelling to a pre-existing conven-
tion such as CODA, and thus their results cannot
be directly related to ours. Furthermore, their tech-
nique is different. First, similarity metrics based on
string difference are used to identify if two strings
are similar. Also, a contextual string similarity is
used based on the fact that if two words are ortho-
graphic variants of each other, then they are bound
to appear in similar contexts. After identifying the
similar strings, the strings of interest are modeled in
a vector space and clustered according to the simi-
larity of their vectors.
5 Data
In this work, we use a manually annotated
EGY Arabic corpus, developed by the Linguis-
tic Data Consortium (LDC), and labeled as ?ARZ?
(Maamouri et al, 2012), parts 1, 2, 3, 4 and 5.
The corpus consists of about 160K words (excluding
numbers and punctuations), and follows the part-of-
speech (POS) guidelines used by the LDC for Egyp-
tian Arabic. The corpus contains a full analysis of
Egyptian Arabic text in spontaneous orthography.
The analysis includes the correct CODA orthogra-
phy of the raw text, in addition to the full morpho-
logical/POS annotations.
Data Preparation We divide the ARZ corpus into
three parts: training, development and test, which
are of about 122K, 19K and 19K words, respec-
tively. We only consider the orthographic informa-
tion in the ARZ corpus: for every word in the cor-
pus, we retain the spontaneous orthographic form
and its CODA-compliant form.
We manually checked the CODA-compliant an-
notations for about 500 words in the development
corpus. We found that the accuracy of the gold an-
notations in this subset is about 93%. We performed
next an error analysis for the erroneous gold annota-
tions. About one half of the gold errors are CODA
phonological and orthographical errors. Examples
of the CODA phonological errors include wrong ad-
ditions and deletions of @ A and ?


y, in addition to
the H/ H t/? transformations, and the transforma-
tions that correspond to the different phonological
forms of pronouncing the letter ? h. The CODA or-
thographical errors are those errors where a word or-
thography looks the same as its pronunciation, while
it should not be, such as the ?/

? h/h? transformations.
One fifth of the gold errors are annotation typos,
such as writing H@P??

? qTwrAt instead of H@PA?

?
qTArAt ?trains?. Moreover, 9% of the gold errors are
wrong merges for the negation particle A? mA and
the indirect object pronouns (l+pronouns). Since we
use the gold in our study, and given the error analy-
sis for the gold, we expect a qualitatively better gold
standard to yield better results.
Transformation Statistics We observe two types
of transformations when converting from sponta-
neous orthography to CODA: character substitu-
tions that do not affect the word length, and charac-
ter additions/deletions that change the word length.
Tables 1 and 2 show the most common character
substitution and addition/deletion transformations,
respectively, as they appear in the training corpus,
associated with their frequencies relative to the oc-
currence of transformations. The character sub-
stitutions are dominant, and constitute about 84%
of all the transformations in the training corpus.
While the classification of the character substitu-
tions is automatically generated, the classification of
the character additions/deletions is done manually
using a random sample of 400 additions/deletions
in the training corpus. This is because many addi-
tions/deletions are ambiguous.
6 Approach
We describe next the various approaches for spon-
taneous orthography codafication. Our codafication
techniques fall into two main categories: contex-
588
Transformation Frequency %
@/

@/ @/

@ A/?/A?/A? ? @/

@/ @/

@ A/?/A?/A? 38.5
?


y ? ? ? 29.7
? h ?

? h? 16.9
? h ? h H 2.5
H ?? H/? t/s 1.0
@ A ? ? h 0.7

? q ? @/

@/ @/

@/Z/ ?'/Z?' A/?/A?/A?/?/w?/y? 0.4
? w ? ? h 0.3
	
X ? ? X/ 	P d/z 0.3

? h?? H t 0.3
@ A ?

? h? 0.2
	
? D ? X/ 	P/ 	? d/z/D? 0.2
Table 1: Spontaneous to CODA character substitu-
tion transformations
Transformation Frequency %
Errors in closed class words 22.0
Missing space after A?/B/ AK
 mA/lA/yA 19.0
@ A additions & deletions 16.8
Gold errors 10.3
Speech effects 8.5
Missing space before ? l (+pron) 8.5
?/ ?/ @? w/h/wA ? ?/ ?/ @? w/h/wA 8.3
?


y additions & deletions 3.0
Table 2: Spontaneous to CODA character addi-
tion/deletion transformations
tual and non-contextual, where the non-contextual
approaches are a lot faster than the contextual ones.
6.1 Speech Effect Handling
Before applying any codafication techniques, we
perform a special preprocessing step for speech ef-
fects, which represent redundant repetitions of some
letter in sequence. Sometimes people intend these
repetitions to show affirmation or intensification.
This is simply handled by removing the repetitions
when a letter is repeated more than twice in a row,
except for some letters whose repetitions for more
than once indicates a speech effect; these letters
are @ A,

@ A?, Z ?, Z?' y?, ? ? and

? h?. Handling
speech effects on its own corrects about 2% of the
non-CODA spontaneous orthography to its CODA-
compliant form, without introducing any new errors.
In all experiments we report in this paper, we have
initially processed speech effects.
6.2 Character Edit Classification (CEC)
In this approach, a set of transformations is applied
on a character level, where a character may receive
a change or not. As a result, a word changes if one
or more of its characters is changed. The output of
these transformations is what constitutes the CODA
orthography. This is a surface modeling technique
that does not depend on the word context (though it
does depend on character context inside the word).
First, we train classifiers for the most frequent
transformations from EGY spontaneous orthogra-
phy to the corresponding CODA, listed in Tables 1
and 2 in Section 5. Second, we apply the trained
classifiers to generate the CODA output.
Training the classifiers For each transformation
listed in the data section, we train a separate classi-
fier. The classifiers are trained on our training cor-
pus using the k-nearest neighbor algorithm (k-NN)
(Wang et al, 2000), which is a method for classi-
fying objects based on the closest training examples
in the feature space. We did experiments using the
other classification methods included in the WEKA
machine learning tool (Hall et al, 2009), including
SVMs, Na?ve Bayes, and decision trees. However,
k-NN gives the best results for our problem.
In the training process, a set of nine static features
is applied, which are the character that is queried for
the transformation with its preceding and following
two characters (a special token indicates a character
position that does not exist because it is beyond the
word boundary), along with the first two and the last
two characters in the underlying word.
In this model, each data point is a character, where
a classifier determines whether a character should
receive a substitution, deletion, or addition of an-
other character. The effect of each classifier is ex-
amined separately on the development set. We then
determine for each classification whether it helps or
weakens the process by comparing its effect to the
baseline which is doing nothing, i.e., no change to
a word occurs. Those classifiers that on their own
perform worse than the baseline are eliminated. For
eliminating the classifiers, we examine them in a de-
scending order according to the frequencies of their
corresponding transformations. We now discuss the
seven classifiers we retain in our system:
589
1. The different @ A form ( @/

@/ @/

@ A/? /A?/A?) classifier.
The classifier can change any @ A form into any
other @ A form. The arbitrary selection of the
different @ A forms represents the most frequent
divergence from CODA in Arabic spontaneous
orthography.
2. The ?


/? y/? classifier. The classifier handles
transformations between ?


y and ? ? in both
directions, as their selection is mostly arbitrary
in EGY spontaneous orthography.
3. The ?/

?/? h/h?/w classifier. The classifier han-
dles transformations between ? h,

? h? and ? w
in both directions. These transformations are
likely to happen at word endings, since they
represent common misspellings in writing ? h,

? h? and ? w, where ? h is often substituted for
the graphically similar

? h?, and ? h and ? w can
both be used to represent the 3rd person mascu-
line singular accusative or genitive clitic. (Note
that in Table 1, we list transformations between
? h and

? h? as well as between ? h and ? w; the
remaining transformations are not frequent.)
4. The ?/h h/H classifier. The transformation
from ? h to h H is likely to happen at word
beginnings, since it represents a common devi-
ation in writing the h H future particle.
5. The @ A deletion classifier. The classifier han-
dles the deletion of extra @ A at some positions,
which is a common deviation in EGY sponta-
neous orthography, where the CODA orthog-
raphy requires only short vowels instead.
6. The @ A addition classifier. The classifier han-
dles the addition of @ A in some positions, where
it is mostly omitted in EGY spontaneous or-
thography, such as adding @ A after the h H fu-
ture particle and the H. b progressive particle
(when used with the 1st person singular imper-
fective), as well as the ? w plural pronoun at
word endings. When training this classifier, we
target the letter after which the @ A should be
added.
7. The space addition classifier. The classifier
handles the addition of spaces in the middle
of words, i.e., splitting a word into two words.
This is required to add spaces after A?/B/ AK

mA/lA/yA for negation and vocation, and be-
fore the indirect object l+pronoun, so that the
text becomes CODA-compliant.
Generating CODA Orthography Next, we ap-
ply the trained classifiers on the spontaneous-
orthography text. Each classifier determines a set
of character corrections, where the characters may
receive transformations corresponding to those on
which the classifier is trained. The classifiers are
independent of one another, so their order of appli-
cation is irrelevant.
By way of example, we apply the classifiers on the
word ??K. P@ Arb?h, ?four?. The first classifier, corre-
sponding to the different @ A forms, determines the
transformation of @ A to

@ ?, while the ?/

? h/h? clas-
sifier determines the correction of ? h to

? h?. The
other classifiers are either not involved since they do
not work on any of the word characters, or they de-
termine that no character transformation should hap-
pen for this word. Thus applying the CEC tech-
nique in this case changes the word ??K. P@ Arb?h to

??K. P

@ ?rb?h?, which is the correct CODA form.
6.3 Maximum Likelihood Estimate (MLE)
Another surface modeling approach for spontaneous
orthography codafication is to use a maximum like-
lihood model that operates on the word level. In this
approach, we build a unigram model that replaces
every word in the spontaneous orthography with its
most likely CODA form as seen in the training data.
This assumes that the underlying word exists in the
training corpus. For unseen words, the technique
keeps them with no change.
The MLE approach chooses the correct CODA
form for most of the words seen in training, making
this approach highly dependent on the training data.
It is efficient at correcting common misspellings in
frequent words, especially those that are from closed
classes.
6.4 Morphological Tagger
In addition to the approaches discussed above, we
use a morphological tagger, MADAARZ (Mor-
phological Analysis and Disambiguation for Egyp-
590
tian Arabic) (Habash et al, 2013). Although
MADAARZ is originally developed to work as a
morphological tagger, it still can help the codafica-
tion process, since the choice of a full morpholog-
ical analysis for a word in context determines its
CODA spelling. Therefore, MADAARZ is able to
correct many word misspellings that are common in
spontaneous orthography. These corrections include
( @/

@/ @/

@ A/?/A?/A?), ?


/? y/? and ?/

? h/h? transforma-
tions. However, MADAARZ , as a codafication tech-
nique, uses the context of the word, which makes
it a contextual modeling approach unlike CEC and
MLE. It is much slower than they are.
6.5 Combined Techniques
The CEC and MLE techniques can be applied
alone, or they can be applied together in a pipeline in
either order. This gives a total of four possible com-
binations. Next, we conducted experiments with
MADAARZ , running alone and as a pre- or postpro-
cessor for a combination of CEC and/or MLE. In
all cases, when we first apply one module and then
another on the output of the first, we train the sec-
ond module on the training corpus which has been
passed through the first module. The results of run-
ning the different codafication approaches are dis-
cussed next.
7 Evaluation
7.1 Accuracy Evaluation
The different codafication approaches, discussed in
the previous section, are tested against the develop-
ment set, which was not used as part of our train-
ing. The evaluation metric we use is a word accuracy
metric, i.e., we evaluate how well we can correctly
predict the CODA form of the input spontaneous or-
thography.
Table 3 lists the effects of using the different
codafication approaches. For each approach, two
numbers are reported; exact and normalized. In
the exact evaluation, the output of the codafica-
tion approach is exactly matched against the correct
CODA orthography, while in the normalized eval-
uation, the match is relaxed for the ( @/

@/ @/

@ A/?/A?/A?)
and ?


/? y/? alternations, i.e., these differences do
not count as errors. In many NLP applications (such
as machine translation), the input is normalized for
these two phenomena, so that the normalized evalu-
ation gives a sense of the relevance of codafication
to downstream processes which normalize.
In this evaluation we compare our different
codafication techniques, CEC, MLE, CEC+MLE
and MLE+CEC, against the baseline. We also show
the effect of using MADAARZ as a codafication sys-
tem. We see that MLE on its own outperforms CEC.
Running CEC first and then MLE gives us our best
result using surface techniques, namely 91.5%, for
an error reduction of 63.4% against the baseline.
This configuration also gives the highest normalized
accuracy of 95.2%, for an error reduction of 49.5%
against the baseline.
We now turn to deep modeling techniques.
The performance of MADAARZ on its own as a
codafication system is close to the performance of
CEC+MLE, by which it is outperformed in the
exact-match accuracy by 0.4%.
The best deep modeling (and the best overall)
performance is achieved when running MADAARZ
on top of MLE. This gives the highest accuracy
of 92.6% (exact) and 95.8% (normalized), for er-
ror reductions of 68.1% (exact) and 55.8% (nor-
malized) against the baseline, respectively. Note
that the non-contextual modeling techniques CEC
(5,584 words/sec) and MLE (6,698 words/sec)
are a lot faster than the deep modeling tech-
nique MADAARZ (53 words/sec), while their com-
bination CEC+MLE+MADAARZ is the slowest
among all the approaches, operating at a rate of 52
words/sec. Thus, a small drop in accuracy results in
a large increase in speed.
We also evaluated using MADAMSA (v 3.2)
(Morphological Analysis and Disambiguation for
MSA) (Habash and Rambow, 2005; Habash et al,
2010). MADAMSA is able to do some codafication,
but it performs far worse than our codafication ap-
proaches.
Table 4 lists the results of the best perform-
ing codafication surface approach, CEC+MLE, and
deep approach, MLE+MADAARZ , when applied
on the test set, which was not used as part of our
training or development, i.e., a completely blind
test. We see that on the test set, the addition
of MADAARZ improves results relatively more as
compared to the development set.
591
Approach
Exact Match Norm Match
w/s
Acc% ER% Acc% ER%
Baseline 76.8 90.5
MADAMSA 83.6 29.3 91.7 12.6 70
CEC 90.0 56.9 93.9 35.8 5,584
MLE 90.5 59.1 94.6 43.2 6,698
CEC+MLE 91.5 63.4 95.2 49.5 4,284
MLE+CEC 90.7 59.9 94.7 44.2 4,284
MADAARZ 91.1 61.6 95.2 49.5 53
MADAARZ+CEC 91.5 63.4 95.4 51.6 53
MADAARZ+MLE 91.9 65.1 95.8 55.8 53
CEC+MADAARZ 92.2 66.4 95.6 53.7 53
MLE+MADAARZ 92.6 68.1 95.8 55.8 53
MADAARZ+CEC+MLE 91.8 64.7 95.6 53.7 52
CEC+MLE+MADAARZ 92.0 65.5 95.8 55.8 52
Table 3: Comparison of the performance of the different codafication approaches on the development corpus.
Acc stands for Accuracy; ER is error reduction against the Baseline. w/s is speed (words/sec).
Approach
Exact Match Norm Match
Acc% ER% Acc% ER%
Baseline 75.5 89.7
CEC+MLE 91.3 64.5 94.8 49.5
MLE+MADAARZ 92.9 71.0 95.5 56.3
Table 4: Comparison of the performance of the
different codafication approaches on the test cor-
pus. Acc stands for Accuracy; ER is error reduction
against the Baseline.
7.2 Extrinsic Evaluation
Morphological Analysis We tested the effect of
codafication on morphological tagging, specifically
full POS and lemma determination in context by
the morphological tagger MADAARZ . Here, we
are evaluating MADAARZ not on its conversion
to CODA (as above), but on its core functional-
ity, namely morphological tagging. We compare
the performance of MADAARZ against running
CEC+MLE+MADAARZ . When tested on the de-
velopment set, the initial CEC+MLE codafication
step helps MADAARZ improve the identification
of the complete Arabic (Buckwalter) POS tag from
84% to 85.3%, for an error reduction of 8.1%, while
the correct lemma choice increases from 85.2% to
85.7%, for an error reduction of 3.4%. When tested
on the test set, we get improvements on the choice
of the complete Buckwalter POS tag and lemma
from 84.5% to 85.4% (5.8% error reduction) and
from 86.3% to 86.7% (2.9% error reduction), re-
spectively.
Arabic to English MT The goal of this exper-
iment is to test the effect of codafication on ma-
chine translation from dialectal Arabic to English.
We use the open-source Moses toolkit (Koehn et
al., 2007) to build a phrase-based SMT system. We
use MGIZA++ for word alignment (Gao and Vogel,
2008). Phrase translations of up to 8 words are ex-
tracted in the phrase table. We use SRILM (Stol-
cke, 2002) with modified Kneser-Ney smoothing to
build two 4-gram language models. The first model
is trained on the English side of the bitext, while
the other is trained on the English Gigaword data.
Feature weights are tuned to maximize BLEU (Pap-
ineni et al, 2002) on a development set using MERT
(Och, 2003). We perform case-insensitive evalua-
tion in terms of the BLEU metric.
We train the system on dialectal Arabic-English
parallel data, obtained from several LDC corpora,
which amounts to ?500k sentences with 3.8M unto-
kenized words on the Arabic side. The development
set, used for tuning the parameters of the MT sys-
tem, has 1,547 sentences with 15,585 untokenized
Arabic words. The test set has 1,065 sentences with
592
12,116 untokenized Arabic words. Both develop-
ment and test sets have two reference translations
each. The English data is lower-cased and tokenized
using simple punctuation-based rules.
We build two systems which vary in preprocess-
ing of the Arabic text. The baseline system ap-
plies only simple punctuation-based rules. The sec-
ond system applies our codafication in addition to
punctuation separation. The Arabic text is Alif/Ya
normalized and is kept untokenized in both set-
tings. The baseline system achieves a BLEU score
of 22.1%. The system using codafication obtains a
BLEU score of 22.6%, and outperforms the baseline
by 0.5% absolute BLEU points. This result shows
that improvements observed in intrinsic evaluation
of codafication carry on to the extrinsic task of ma-
chine translation.
7.3 Error Analysis
We conducted an error analysis for the best perform-
ing codafication approach on the development set.
The most frequent error types are listed in Table 5.
About two thirds of the errors are CODA phonolog-
ical and orthographical errors, denoted by CODA-
Phon and CODA-Orth, respectively. The wrong ad-
ditions and deletions of @ A and ?


y and the H/ H t/?
transformations are examples of CODA phonologi-
cal errors. The CODA orthographic errors include
cases such as the ?


/? y/? transformations. 21% of
the errors are not real errors in the codafication out-
put, but result from gold errors. Finally, about 13%
of the errors are wrong merges and splits for the the
negation particle A? mA, the vocative particle AK
 yA
and the indirect-object l+pronouns.
8 Conclusion and Future Work
We have presented the problem of transforming
spontaneous orthography of the Egyptian Arabic di-
alect into a conventionalized form, CODA. Our best
technique involves a combination of character trans-
formations, whole-word transformations, and the
use of a full morphological tagger. The tagger can
be omitted for a small decrease in performance and
a large increase in speed.2 In future work, we plan
to extend our approach to other Arabic dialects. We
2Our system will be freely available. Please contact the au-
thors for more information.
Error Type Description Percentage
Gold Error Annotation Error 21.0
CODA-Orth ? h ? ? h? 13.7
CODA-Phon @ A ?  8.7
Merge A? mA/NEG_PART 7.3
CODA-Phon ? ?


y 6.8
CODA-Phon ? @ A 5.9
CODA-Orth ?


y ? ? ? 4.1
Merge AK
 yA/VOC_PART 3.7
CODA-Phon ? h ? h H 3.2
CODA-Phon ?


y ? ? ? 3.2
Table 5: System Error Analysis: the most frequent
error types.
will also investigate incorporating the unsupervised
work of Dasigi and Diab (2011) into our algorithm,
as well as other unsupervised techniques.
Acknowledgment
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-0014.
Any opinions, findings and conclusions or recom-
mendations expressed in this paper are those of the
authors and do not necessarily reflect the views of
DARPA. We thank three anonymous reviewers for
helpful comments, and Ryan Roth for help with run-
ning MADA.
593
References
G. Abuhakema, R. Faraj, A. Feldman, and E. Fitzpatrick.
2008. Annotating an Arabic Learner Corpus for Er-
ror. Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08).
G Al-Gaphari and M Al-Yadoumi. 2010. A method
to convert Sana?ani accent to Modern Standard Ara-
bic. International Journal of Information Science and
Management, pages 39?49.
Mohamed I. Alkanhal, Mohammed A. Al-Badrashiny,
Mansour M. Alghamdi, and Abdulaziz O. Al-
Qabbany. 2012. Automatic Stochastic Arabic
Spelling Correction With Emphasis on Space Inser-
tions and Deletions. IEEE Transactions on Audio,
Speech & Language Processing, 20:2111?2122.
Chiraz Ben Othmane Zribi and Mohammed Ben Ahmed.
2003. Efficient Automatic Correction of Misspelled
Arabic Words Based on Contextual Information. In
Proceedings of the Knowledge-Based Intelligent Infor-
mation and Engineering Systems Conference, Oxford,
UK.
Pradeep Dasigi and Mona Diab. 2011. CODACT: To-
wardsIdentifying Orthographic Variants in Dialectal
Arabic. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
318?326, Chaing Mai, Thailand.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?57,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Ryan Roth. 2011. Using deep mor-
phology to improve automatic error detection in ara-
bic handwriting recognition. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 875?884, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2010.
MADA+TOKAN Manual. Technical Report CCLS-
10-01, Center for Computational Learning Systems
(CCLS), Columbia University.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC), Istanbul.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskan-
der, and Nadi Tomeh. 2013. Morphological Analysis
and Disambiguation for Dialectal Arabic. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT),
Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-Words in Arabic: A Hybrid
Approach. International Journal of Computer Pro-
cessing Of Languages (IJCPOL).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Ahmed Hassan, Sara Noeman, and Hany Hassan. 2008.
Language Independent Text Correction using Finite
State Automata. In Proceedings of the International
Joint Conference on Natural Language Processing
(IJCNLP 2008).
Clive Holes. 2004. Modern Arabic: Structures, Func-
tions, and Varieties. Georgetown Classics in Ara-
bic Language and Linguistics. Georgetown University
Press.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Okan Kolak and Philip Resnik. 2002. OCR error cor-
rection using a noisy channel model. In Proceedings
of the second international conference on Human Lan-
guage Technology Research.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Surveys,
24(4).
Mohamed Maamouri, Ann Bies, Seth Kulick, Dalila
Tabessi, and Sondos Krouna. 2012. Egyptian Arabic
Treebank Pilot.
Walid Magdy and Kareem Darwish. 2006. Arabic OCR
Error Correction Using Character Segment Correction,
594
Language Modeling, and Shallow Morphology. In
Proceedings of 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2006),
pages 408?414, Sydney, Austrailia.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of the 41st Annual Conference of the Association for
Computational Linguistics, pages 160?167, Sapporo,
Japan.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73?90.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Wael Salloum and Nizar Habash. 2011. Dialectal
to Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, pages 10?21, Edinburgh, Scotland.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in the
Americas (AMTA), Denver, Colorado.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Conference on Language Engineering, ELSE,
Cairo, Egypt.
K. Shaalan, Abo Bakr, and I. H. Ziedan. 2007. Transfer-
ring Egyptian Colloquial into Modern Standard Ara-
bic. In International Conference on Recent Advances
in Natural Language Processing (RANLP), Borovets,
Bulgaria.
K. Shaalan, R. Aref, and A. Fahmy. 2010. An approach
for analyzing and correcting spelling errors for non-
native Arabic learners. Proceedings of Informatics
and Systems (INFOS).
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
Jun Wang, Zucker, and Jean-Daniel. 2000. Solving
multiple-instance problem: A lazy learning approach.
In Pat Langley, editor, 17th International Conference
on Machine Learning, pages 1119?1125.
Janet C. E. Watson. 2002. The Phonology and Morphol-
ogy of Arabic. Oxford University Press.
595
Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 1?9,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A Morphological Analyzer for Egyptian Arabic
Nizar Habash and Ramy Eskander and Abdelati Hawwari
Center for Computational Learning Systems
Columbia University
New York, NY, USA
{habash,reskander,ahawwari}@ccls.columbia.edu
Abstract
Most tools and resources developed for nat-
ural language processing of Arabic are de-
signed for Modern Standard Arabic (MSA)
and perform terribly on Arabic dialects, such
as Egyptian Arabic. Egyptian Arabic differs
from MSA phonologically, morphologically
and lexically and has no standardized orthog-
raphy. We present a linguistically accurate,
large-scale morphological analyzer for Egyp-
tian Arabic. The analyzer extends an existing
resource, the Egyptian Colloquial Arabic Lex-
icon, and follows the part-of-speech guide-
lines used by the Linguistic Data Consortium
for Egyptian Arabic. It accepts multiple or-
thographic variants and normalizes them to a
conventional orthography.
1 Introduction
Dialectal Arabic (DA) refers to the day-to-day na-
tive vernaculars spoken in the Arab World. DA
is used side by side with Modern Standard Ara-
bic (MSA), the official language of the media and
education (Holes, 2004). Although DAs are his-
torically related to MSA, there are many phono-
logical, morphological and lexical differences be-
tween them. Unlike MSA, DAs have no stan-
dard orthographies or language academies. Fur-
thermore, different DAs, such as Egyptian Arabic
(henceforth, EGY), Levantine Arabic or Moroccan
Arabic have important differences among them sim-
ilar to those seen among Romance languages (Er-
win, 1963; Cowell, 1964; Abdel-Massih et al, 1979;
Holes, 2004). Most tools and resources developed
for natural language processing (NLP) of Arabic are
designed for MSA. Such resources are quite limited
when it comes to processing DA, e.g., a state-of-the-
art MSA morphological analyzer has been reported
to only have 60% coverage of Levantine Arabic verb
forms (Habash and Rambow, 2006). Most efforts to
address this gap have been lacking. Some have taken
a quick-and-dirty approach to model shallow mor-
phology in DA by extending MSA tools, resulting
in linguistically inaccurate models (Abo Bakr et al,
2008; Salloum and Habash, 2011). Others have at-
tempted to build linguistically accurate models that
are lacking in coverage (at the lexical or inflectional
levels) or focusing on representations that are not
readily usable for NLP text processing, e.g., phono-
logical lexicons (Kilany et al, 2002).
In this paper we present the Columbia Ara-
bic Language and dIalect Morphological Analyzer
(CALIMA) for EGY.1 We built this tool by ex-
tending an existing resource for EGY, the Egyptian
Colloquial Arabic Lexicon (ECAL) (Kilany et al,
2002). CALIMA is a linguistically accurate, large-
scale morphological analyzer. It follows the part-of-
speech (POS) guidelines used by the Linguistic Data
Consortium for EGY (Maamouri et al, 2012b). It
accepts multiple orthographic variants and normal-
izes them to CODA, a conventional orthography for
DA (Habash et al, 2012).
The rest of the paper is structured as follows: Sec-
tion 2 presents relevant motivating linguistic facts.
Section 3 discusses related work. Section 4 details
the steps taken to create CALIMA starting with
ECAL. Section 5 presents a preliminary evaluation
and statistics about the coverage of CALIMA. Fi-
nally, Section 6 outlines future plans and directions.
1Although we focus on Egyptian Arabic in this paper, the
CALIMA name will be used in the future to cover a variety of
dialects.
1
2 Motivating Linguistic Facts
We present some general Arabic (MSA/DA) NLP
challenges. Then we discuss differences between
MSA and DA ? specifically EGY.
2.1 General Arabic Linguistic Challenges
Arabic, as MSA or DA, poses many challenges
for NLP. Arabic is a morphologically complex lan-
guage which includes rich inflectional morphology,
expressed both templatically and affixationally, and
several classes of attachable clitics. For example, the
MSA word A? 	E?J.

J?J
?? wa+sa+ya-ktub-uwna+hA
2
?and they will write it? has two proclitics (+? wa+
?and? and +? sa+ ?will?), one prefix -?


ya- ?3rd
person?, one suffix 	??- -uwna ?masculine plural?
and one pronominal enclitic A?+ +hA ?it/her?. The
stem ktub can be further analyzed into the root ktb
and pattern 12u3.
Additionally, Arabic is written with optional dia-
critics that primarily specify short vowels and con-
sonantal doubling, e.g., the example above will most
certainly be written as wsyktbwnhA. The absence of
these diacritics together with the language?s com-
plex morphology lead to a high degree of ambiguity,
e.g., the Standard Arabic Morphological Analyzer
(SAMA) (Graff et al, 2009) produces an average of
12 analyses per MSA word.
Moreover, some letters in Arabic are often spelled
inconsistently which leads to an increase in both
sparsity (multiple forms of the same word) and
ambiguity (same form corresponding to multiple
words), e.g., variants of the Hamzated Alif,

@ ? or
@ A?, are often written without their Hamza (Z ?): @ A.
and the Alif-Maqsura (or dot-less Ya) ? ? and the
regular dotted Ya ?


y are often used interchangeably
in the word-final position (Buckwalter, 2007).
Arabic complex morphology and ambiguity are
handled using tools for disambiguation and tok-
enization (Habash and Rambow, 2005; Diab et al,
2007).
2Arabic orthographic transliteration is presented in the HSB
scheme (Habash et al, 2007): (in alphabetical order)
@ H.
H H h. h p X
	
XP 	P ? ? ?
	
? ?
	
? ?
	
?
	
?

? ? ? ?
	
? ? ? ?


A b t ? j H x d ? r z s ? S D T D? ? ? f q k l m n h w y
and the additional letters: ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', ~ ?, ? ?.
We distinguish between morphological analysis,
whose target is to produce all possible morphologi-
cal/POS readings of a word out of context, and mor-
phological disambiguation, which attempts to tag
the word in context (Habash and Rambow, 2005).
The work presented in this paper is only about mor-
phological analysis.
2.2 Differences between MSA and DA
Contemporary Arabic is in fact a collection of vari-
eties: MSA, which has a standard orthography and
is used in formal settings, and DAs, which are com-
monly used informally and with increasing presence
on the web, but which do not have standard or-
thographies. DAs mostly differ from MSA phono-
logically, morphologically, and lexically (Gadalla,
2000; Holes, 2004). These difference are not mod-
eled as part of MSA NLP tools, leaving a gap in
coverage when using them to process DAs. All ex-
amples below are in Egyptian Arabic (EGY).
Phonologically, the profile of EGY is quite simi-
lar to MSA, except for some important differences.
For example, the MSA consonants

?/
	
X/ H q/?/?
are generally pronounced in EGY (Cairene) as ?/z/s
(Holes, 2004). Some of these consonants shift in dif-
ferent ways in different words: e.g., MSA I.
	
K
	
X ?anb
?fault? and H.
	
Y? ki?b ?lying? are pronounced zanb
and kidb. EGY has five long vowels compared with
MSA?s three long vowels. Unlike MSA, long vow-
els in EGY predictably shorten under certain condi-
tions, often as a result of cliticization. For example,
compare the following forms of the same verb:
	
?A ?
?Af /?a?f/ ?he saw? and A? 	?A ? ?Af+hA /?afha/ ?he saw
her? (Habash et al, 2012).
Morphologically, the most important difference
is in the use of clitics and affixes that do not ex-
ist in MSA. For instance, the EGY equivalent of
the MSA example above is A??J.

J?J
k? wi+Ha+yi-
ktib-uw+hA ?and they will write it?. The optionality
of vocalic diacritics helps hide some of the differ-
ences resulting from vowel changes; compare the
undiacritized forms: EGY wHyktbwhA and MSA
wsyktbwnhA. In this example, the forms of the cli-
tics and affixes are different in EGY although they
have the same meaning; however, EGY has clitics
that are not part of MSA morphology, e.g., the in-
direct pronominal object clitic (+l+uh ?for him?)
2
??A??J.

J?J
k? wi+Ha+yi-ktib-uw+hA+l+uh ?and they
will write it for him?. Another important example is
the circumfix negation ?+ + A? mA+ +? which sur-
rounds some verb forms: ?.

J? A? mA+katab+? ?he
did not write? (the MSA equivalent is two words:
I.

J?K
 ?? lam yaktub). Another important morpho-
logical difference from MSA is that DAs in general
and not just EGY drop the case and mood features
almost completely.
Lexically, the number of differences is very
large. Examples include ?. bas ?only?,

?
	Q
K. Q?
tarabayza~ ?table?, H@Q? mirAt ?wife [of]? and ??X
dawl ?these?, which correspond to MSA ?

?
	
? faqaT,

???A? TAwila~, ?k. ? 	P zawja~ and ZB

?? haw?lA?, re-
spectively.
An important challenge for NLP work on DAs
in general is the lack of an orthographic standard.
EGY writers are often inconsistent even in their
own writing. The differences in phonology between
MSA and EGY are often responsible: words can
be spelled as pronounced or etymologically in their
related MSA form, e.g., H. Y? kidb or H.
	
Y? ki?b.
Some clitics have multiple common forms, e.g., the
future particle h Ha appears as a separate word or as
a proclitic +h/+? Ha+/ha+, reflecting different pro-
nunciations. The different spellings may add some
confusion, e.g., ?J.

J? ktbw may be @?J.

J? katabuwA
?they wrote? or ?J.

J? katabuh ?he wrote it?. Finally,
shortened long vowels can be spelled long or short,
e.g., A?
	
?A ?/ A?
	
? ? ?Af+hA/?f+hA ?he saw her?.
3 Related Work
3.1 Approaches to Arabic Morphology
There has been a considerable amount of work on
Arabic morphological analysis (Al-Sughaiyer and
Al-Kharashi, 2004; Habash, 2010). Altantawy et al
(2011) characterize the various approaches explored
for Arabic and Semitic computational morphology
as being on a continuum with two poles: on one end,
very abstract and linguistically rich representations
and morphological rules are used to derive surface
forms; while on the other end, simple and shallow
techniques focus on efficient search in a space of
precompiled (tabulated) solutions. The first type is
typically implemented using finite-state technology
and can be at many different degrees of sophistica-
tion and detail (Beesley et al, 1989; Kiraz, 2000;
Habash and Rambow, 2006). The second type is typ-
ically implemented using hash-tables with a simple
search algorithm. Examples include the Buckwalter
Arabic Morphological Analyzer (BAMA) (Buck-
walter, 2004), its Standard Arabic Morphological
Analyzer (SAMA) (Graff et al, 2009) incarnation,
and their generation-oriented extension, ALMOR
(Habash, 2007). These systems do not represent the
morphemic, phonological and orthographic rules di-
rectly, and instead compile their effect into the lexi-
con itself, which consists of three tables for prefixes,
stems and suffixes and their compatibilities. A pre-
fix or suffix in this approach is a string consisting of
all the word?s prefixes and suffixes, respectively, as
a single unit (including null affix sequences). Dur-
ing analysis, all possible splits of a word into com-
patible prefix-stem-suffix combination are explored.
More details are discussed in Section 4.5. Numer-
ous intermediate points exist between these two ex-
tremes (e.g., ElixirFM (Smr?, 2007)). Altantawy et
al. (2011) describe a method for converting a lin-
guistically complex and abstract implementation of
Arabic verbs in finite-state machinery into a simple
precompiled tabular representation.
The approach we follow in this paper is closer
to the second type. We start with a lexicon of in-
flected forms and derive from it a tabular represen-
tation compatible with the SAMA system for MSA.
However, as we do this, we design the tables and ex-
tend them in ways that capture generalizations and
extend orthographic coverage.
3.2 Arabic Dialect Morphology
The majority of the work discussed above has fo-
cused on MSA, while only a few efforts have tar-
geted DA morphology (Kilany et al, 2002; Riesa
and Yarowsky, 2006; Habash and Rambow, 2006;
Abo Bakr et al, 2008; Salloum and Habash, 2011;
Mohamed et al, 2012). These efforts generally fall
in two camps. First are solutions that focus on ex-
tending MSA tools to cover DA phenomena. For
example, both Abo Bakr et al (2008) and Salloum
and Habash (2011) extended the BAMA/SAMA
databases (Buckwalter, 2004; Graff et al, 2009) to
accept DA prefixes and suffixes. Both of these ef-
forts were interested in mapping DA text to some
MSA-like form; as such they did not model DA lin-
3
guistic phenomena, e.g., the ADAM system (Sal-
loum and Habash, 2011) outputs only MSA diacrit-
ics that are discarded in later processing.
The second camp is interested in modeling DA di-
rectly. However, the attempts at doing so are lacking
in coverage in one dimension or another. The earli-
est effort on EGY that we know of is the Egyptian
Colloquial Arabic Lexicon (ECAL) (Kilany et al,
2002). It was developed as part of the CALLHOME
Egyptian Arabic (CHE) corpus (Gadalla et al, 1997)
which contains 140 telephone conversations and
their transcripts. The lexicon lists all of the words
appearing in the CHE corpus and provides phono-
logical, orthographic and morphological informa-
tion for them. This is an important resource; how-
ever, it is lacking in many ways: the orthographic
forms are undiacritized, no morpheme segmenta-
tions are provided, and the lexicon has only some
66K fully inflected forms and as such lacks general
morphological coverage. Another effort is the work
by Habash and Rambow (2006) which focuses on
modeling DAs together with MSA using a common
multi-tier finite-state-machine framework. Although
this approach has a lot of potential, in practice, it
is closer to the first camp in its results since they
used MSA lexicons as a base. Finally, two previ-
ous efforts focused on modeling shallow dialectal
segmentation using supervised methods (Riesa and
Yarowsky, 2006; Mohamed et al, 2012). Riesa and
Yarowsky (2006) presented a supervised algorithm
for online morpheme segmentation for Iraqi Arabic
that cut the out-of-vocabulary rates by half in the
context of machine translation into English. Mo-
hamed et al (2012) annotated a collection of EGY
for morpheme boundaries and used this data to de-
velop an EGY tokenizer. Although these efforts
model DA directly, they remain at a shallow level
of representation (undiacritized surface morph seg-
mentation).
We use the ECAL lexicon as a base for CAL-
IMA and extend it further. Some of the expansion
techniques we used are inspired by previous solu-
tions (Abo Bakr et al, 2008; Salloum and Habash,
2011). For the morphological representation, we
follow the Linguistic Data Consortium guidelines
which extend the MSA POS guidelines to multi-
ple dialects (Maamouri et al, 2006; Maamouri et
al., 2012b). To address the problem of orthographic
variations, we follow the proposal by Habash et al
(2012) who designed a conventional orthography for
DA (or CODA) for NLP applications in the CAL-
IMA databases. However, to handle input in a vari-
ety of spellings, we extend our analyzer to accept
non-CODA-compliant word forms but map them
only to CODA-compliant forms as part of the anal-
ysis.
4 Approach
We describe next the various steps for creating
CALIMA starting with ECAL. The details of the
approach are to some degree dependent on this
unique resource; however, some aspects of the ap-
proach may be generalizable to other resources, and
languages or dialects.
4.1 The Egyptian Colloquial Arabic Lexicon
ECAL has about 66K entries: 27K verbs, 36K
nouns and adjectives, 1.5K proper nouns and 1K
closed classes. For each entry, the lexicon pro-
vides a phonological form, an undiacritized Ara-
bic script orthography, a lemma (in phonological
form), and morphological features, among other
information. There are 36K unique lemmas and
1,464 unique morphological feature combinations.
The following is an example ECAL entry for the
word ?????J
J.? mbyklmw? ?he did not talk to him?.
3
We only show Arabic orthography, phonology, and
lemma+features:
mbyklmw?
mabiykallimU$4
kallim:verb+pres-3rd-masc-sg+DO-3rd-masc-sg+neg
Our goal for CALIMA is to have a much larger
coverage, a CODA-compliant diacritized orthogra-
phy, and a morpheme-based morphological analysis.
The next steps allow us to accomplish these goals.
4.2 Diacritic Insertion
First, we built a component to diacritize the ECAL
undiacritized Arabic script entries in a way that is
consistent with ECAL phonological form. This was
implemented using a finite-state transducer (FST)
that maps the phonological form to multiple possible
3The same orthographic form has another reading ?they did
not talk? which of course has different morphological features.
4The phonological form as used in ECAL. For transcrip-
tion details, see (Kilany et al, 2002).
4
diacritized Arabic script forms. The form that is the
same as the undiacritized ECAL orthography (ex-
cept for diacritics) is used as the diacritized orthog-
raphy for the rest of the process. The FST consists of
about 160 transformations that we created manually.
All except for 100 cases are generic mappings, e.g.,
two repeated b consonants are turned into H. b?,
5
or a short vowel u can be orthographically a short
vowel (just the diacritic u) or a long vowel uw which
shortened. The exceptional 100 cases were specified
by hand in the FST as complete string mappings.
These were mostly odd spellings of foreign words
or spelling errors. We did not attempt to correct or
change the ECAL letter spelling; we only added di-
acritics.
After diacritization, we modify the Arabic orthog-
raphy in the example above to: mabiykal~imuw?.
4.3 Morphological Tag Mapping
Next, we wrote rules to convert from ECAL
diacritized Arabic and morphology to CODA-
compliant diacritized Arabic and LDC EGY POS
compliant tags. The rules fall into three categories:
ignore rules specify which ECAL entries to ex-
clude due to errors; correction rules correct for some
ECAL entry errors; and prefix/suffix/stem rules are
used to identify specific pairs of prefix/suffix/stem
substrings and morphological features to map to
appropriate prefix/suffix/stem morphemes, respec-
tively. For stems, the majority of the rules also
identify roots and patterns. Since multiple root-
pattern combinations may be possible for a partic-
ular word, the appropriate root-pattern is chosen by
enforcing consistency across all the inflected forms
of the lemma of the word and minimizing the over-
all number of roots in the system. We do not use
or report on root-patterns in CALIMA in this paper
since this information is not required by the LDC
tags; however, we plan on using them in future ef-
forts exploiting templatic morphology.
At the time of writing this paper, the system in-
cluded 4,632 rules covering all POS. These include
1,248 ignore rules, 1,451 correction rules, 83 pre-
fix rules, and 441 suffixes rules. About 1,409 stem
rules are used to map core POS tags and iden-
tify templatic roots and patterns. Some rules were
5The ? diacritic or Shadda indicates the presence of conso-
nantal doubling.
semi-automatically created, but all were manually
checked. The rules are specified in a simple format
that is interpreted and applied by a separate rule pro-
cessing script. Developing the script and writing the
rules took about 3 person-months of effort.
As an example, the following three rules are used
to handle the circumfix ma++? ?not? and the pro-
gressing particle bi+.
PRE: ma,+neg => ? ,+neg >> mA/NEG_PART#
PRE: bi,+pres => ? ,+subj >> bi/PROG_PART+
SUF: ?,+neg => ?,? >> +?/NEG_PART
The input to the rule processor is a pair of surface
form and morphological features. Each rule matches
on a surface substring and a combination of mor-
phological features (first two comma-separated to-
kens in the rule) and rewrites the parts it matched
on (second two comma-separated tokens in the rule
after =>). The type of the rule, i.e. prefix or suf-
fix rule, determines how the matching is applied. In
addition, the rule generates a substring of the tar-
get tag (last token in the rule). The first and third
rules above handle a circumfix; the +neg feature is
not deleted in the first rule (which handles the pre-
fix) to allow the third rule (which handles the suffix)
to fire. The second rule rewrites the feature +pres
(present tense) as +subj (subjunctive) which is con-
sistent with the form of the verb after removing the
progressive particle bi+. After applying these rules
in addition to a few others, the above example is
turned into CODA and EGY POS compliant forms
(# means word boundary):6
mA#bi+yi+kal~im+huw+?
NEG_PART#PROG_PART+IV3MS+IV+IVSUFF_DO:3MS+NEG_PART
The stem rules, whose results are not shown here,
determine that the root is klm and the pattern is
1a22i3.
We extended the set of mapped ECAL entries
systematically. We copied entries and modified them
to include additional clitics that are not present with
all entries, e.g., the conjunction +
	
? fa+ ?then?, and
the definite article +?@ Al+.
4.4 Orthographic Lemma Identification
The ECAL lemmas are specified in a phonological
form, e.g., in the example above, it is kallim. To de-
termine the diacritized Arabic orthography spelling
6CODA guidelines state that the negative particle A? mA is
not to be cliticized except in a very small number of words
(Habash et al, 2012).
5
of the lemma, we relied on the existence of the
lemma itself as an entry and other ad hoc rules to
identify the appropriate form. Using this technique,
we successfully identified the orthographic lemma
form for 97% of the cases. The remainder were
manually corrected. We followed the guidelines for
lemma specification in SAMA, e.g., verbs are cited
using the third person masculine singular perfective
form. For our example, the CALIMA lemma is
kal?im.
4.5 Table Construction
We converted the mapped ECAL entries to a
SAMA-like representation (Graff et al, 2009). In
SAMA, morphological information is stored in six
tables. Three tables specify complex prefixes, com-
plex suffixes and stems. A complex prefix/suffix is
a set of prefix/suffix morphemes that are treated as a
single database entry, e.g., wi+Ha+yi is a complex
prefix made of three prefix morphemes. Each com-
plex prefix, complex suffix and stem has a class cat-
egory which abstract away from all similarly behav-
ing complex affixes and stems. The other three ta-
bles specify compatibility across the class categories
(prefix-stem, prefix-suffix and stem-suffix). We ex-
tracted triples of prefix-stem-suffix and used them to
build the six SAMA-like tables. The generated ta-
bles are usable by the sama-analyze engine provided
as part of SAMA3.1 (Graff et al, 2009). We also
added back off mode support for NOUN_PROP.
Prefix/stem/suffix class categories are generated
automatically. We identified specific features of the
word?s stem and affixes to generate specific affix
classes that allow for correct coverage expansion.
For example, in a complex suffix, the first morpheme
is the only one interacting with the stem. As such,
there is no need to give each complex suffix its own
class category, but rather assign the class category
based on the first morpheme. This allows us to auto-
matically extend the coverage of the analyzer com-
pared to that of the ECAL lexicon.
We also go further in terms of generalizations. For
instance, some of the pronoun clitics in EGY have
two forms that depend on whether the stem ends
with vowel-consonant or two consonants, e.g., A?E. A

J?
kitAb+hA ?her book? as opposed to A? 	DK. @ Aibn+ahA
?her son?. This information is used to give the suf-
fixes +hA and +ahA different class categories that
are generalizable to other similarly behaving clitics.
At this stage of our system, which we refer to as
CALIMA-core in Section 5.2, there are 252 unique
complex prefixes and 550 unique complex suffixes,
constructed from 43 and 86 unique simple prefixes
and suffixes, respectively. The total number of pre-
fix/suffix class categories is only 41 and 78, respec-
tively.
4.6 Various Table Extensions
We extended the CALIMA-core tables in a simi-
lar approach to the extension of SAMA tables done
by Salloum and Habash (2011). We distinguish two
types of extensions.
Additional Clitics and POS Tags We added a
number of clitics and POS tags that are not part of
ECAL, e.g., the prepositional clitic +? Ea+ ?on?
and multiple POS tags for the proclitic +
	
? fa+ (as
CONJ, SUB_CONJ and CONNEC_PART). Here we copied a
related entry and modified it but kept its category
class. For example, in the case of +? Ea+ ?on?, we
copied a prepositional clitic with similar distribution
and behavior: +H. bi+ ?with?.
Non-CODA Orthography Support We extended
the generated tables to include common non-CODA
orthographic variants. The following are some ex-
amples of the expansions. First, we added the vari-
ant ?+ +w for two suffixes: ?+ +uh ?his/him? and
@?+ +uwA ?they/you [plural]?. Second, we added
the form ha+ for the future particle Ha+. Third,
we introduced non-CODA-compliant Hamza forms
as variants for some stems. Finally, some of the
extensions target specific stems of frequently used
words, such as the adverb ? 	?QK. brDh ?also? which
can be written as ?XQK. brdh and ?
	
?QK. brDw among
other forms. The non-CODA forms are only used to
match on the input word, with the returned analysis
being a corrected analysis. For example, the word
?J.

J?J
? hyktbw returns the analysis @?J.

J?J
k Hyk-
tbwA Ha/FUT_PART+yi/IV3P+ktib/IV+uwA/3P ?they
will write? among other analyses. The orthographic
variations supported include 16 prefix cases, 41 stem
cases, and eight suffix cases.
After all the clitic, POS tag and orthographic ex-
tensions, the total number of complex prefix entries
6
substantially increases from 352 to 2,421, and the
number of complex suffix entries increases from 826
to 1,179. The number of stem entries increases from
around 60K to 100K. The total number of recogniz-
able word forms increases from 4M to 48M. We will
refer to the system with all the extensions as CAL-
IMA in Section 5.
5 Current Status
In this section, we present some statistics on the cur-
rent status of the CALIMA analyzer. As with all
work on morphological analyzers, there are always
ways to improve the quality and coverage.
5.1 System Statistics
CALIMA has 100K stems corresponding to 36K
lemmas. There are 2,421 complex prefixes and
1,179 complex suffixes (unique diacritized form and
POS tag combinations). The total number of ana-
lyzable words by CALIMA is 48M words (com-
pared to the 66K entries in ECAL). This is still lim-
ited compared to the SAMA3.1 analyzer (Graff et
al., 2009) whose coverage of MSA reaches 246M
words. See Table 1.
5.2 Coverage Evaluation
We tested CALIMA against a manually annotated
EGY corpus of 3,300 words (Maamouri et al,
2012a) which was not used as part of its develop-
ment, i.e., a completely blind test.7 This evaluation
is a POS recall evaluation. It is not about selecting
the correct POS answer in context. We do not con-
sider whether the diacritization or the lemma choice
are correct or not. We compare CALIMA coverage
with that of ECAL and a state-of-the-art MSA an-
alyzer, SAMA3.1 (Graff et al, 2009). For the pur-
pose of completeness, we also compare CALIMA-
core and an extended version of SAMA3.1. The
SAMA3.1 extensions include two EGY verbal pro-
clitics (Ha/FUT_PART and bi/PROG_PART), some alter-
native suffixes that have no case or mood, and all the
orthographic variations used inside CALIMA. We
7We ignore some specific choices made by the annotators,
most importantly the use of ".VN" to mark verbal nominals,
which is not even supported in SAMA3.1. We also ignore
some annotation choices that are not consistent with the latest
LDC guidelines (Maamouri et al, 2012b), such as using gender-
marked plurals in some contexts, e.g., 3MP instead of 3P.
also compare the performance of different merged
versions of SAMA3.1 and CALIMA. The results
are presented in Table 1.
The second column in Table 1, Correct Answer
indicates the percentage of the test words whose cor-
rect analysis in context appears among the analyses
returned by the analyzer. The third column, No Cor-
rect Answer, presents the percentage of time one or
more analyses are returned, but none matching the
correct answer. The fourth column, No Analysis, in-
dicates the percentage of words returning no anal-
yses. The last column presents the total number of
recognizable words in the system.
CALIMA provides among its results a correct an-
swer for POS tags over 84% of the time. This is al-
most 27% absolute over the original list of words
from ECAL and almost 21% absolute over the
SAMA3.1 system. The various extensions in CAL-
IMA give it about 10% absolute over CALIMA-
core (and increase its size 10-fold). The limited
extensions to SAMA3.1 reduce the difference be-
tween it and CALIMA-core by 50% relative. The
overall performance of CALIMA-core merged with
SAMA3.1 is comparable to CALIMA, although
CALIMA has three times the number of no-analysis
cases. Merging CALIMA and extended SAMA3.1
increases the performance to 92%, an 8% absolute
increase over CALIMA alone. The final rate of no-
analysis cases is only 1%.
5.3 Error Analysis
We analyzed a sample of 100 cases where no an-
swer was found (No Correct Answer + No Analy-
sis) for CALIMA+extended SAMA3.1. About a
third of the cases (30%) are due to gold tag errors.
Irrecoverable typographical errors occur 5% of the
time, e.g., 	?

	
? fyn instead of ?


	
? fy ?in?. Only 2%
of the cases involve a speech effect, e.g., ?J

J

J
?g.
jmyyyyyl ?beautiful!!!?. A fifth of the cases (22%) in-
volve a non-CODA orthographic choice which was
not extended, e.g., the shortened long vowel in HAm.k
HjAt instead of the CODA-compliant HAg. Ag HAjAt
?things?. Another fifth of the cases (20%) are due to
incomplete paradigms, i.e., the lemma exists but not
the specific inflected stem. Finally, 21% of the cases
receive a SAMA3.1 analysis that is almost correct,
except for the presence of some mood/case mark-
7
Correct Answer No Correct Answer No Analysis Words
ECAL 57.4% 14.7% 27.9% 66K
SAMA3.1 63.7% 27.1% 9.3% 246M
extended SAMA3.1 68.8% 24.9% 6.3% 511M
CALIMA-core 73.9% 10.8% 15.3% 4M
CALIMA 84.1% 8.0% 7.9% 48M
CALIMA-core + SAMA3.1 84.4% 12.8% 2.8% 287M
CALIMA + extended SAMA3.1 92.1% 7.0% 1.0% 543M
Table 1: Comparison of seven morphological analysis systems on a manually annotated test set. The second column
indicates the percentage of the test words whose correct analysis in context appears among the analyses returned by the
analyzer. The third column presents the percentage of time one or more analyses are returned, but none matching the
correct answer. The fourth column indicates the percentage of words returning no analyses. The last column presents
the total number of recognizable words in the system.
ers that are absent in EGY, and which we did not
handle. Overall, these are positive results that sug-
gest the next steps should involve additional ortho-
graphic and morphological extensions and paradigm
completion.
6 Outlook
We plan to continue improving the coverage of
CALIMA using a variety of methods. First, we are
investigating techniques to automatically fill in the
paradigm gaps using information from multiple en-
tries in ECAL belonging to different lemmas that
share similar characteristics, e.g., hollow verbs in
Form I. Another direction is to update our tables
with less common orthographic variations, perhaps
using information from the phonological forms in
ECAL. Manual addition of specific entries will also
be considered to fill in lexicon gaps. Furthermore,
we plan to add additional features which we did not
discuss such as the English and MSA glosses for all
the entries in CALIMA. We also plan to make this
tool public so it can be used by other people work-
ing on EGY NLP tasks, from annotating corpora to
building morphological disambiguation tools.
Acknowledgments
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-0014.
Any opinions, findings and conclusions or recom-
mendations expressed in this paper are those of
the authors and do not necessarily reflect the views
of DARPA. We thank Mohamed Maamouri, Owen
Rambow, Seth Kulick, Mona Diab and Mike Ciul,
for helpful discussions and feedback.
References
Ernest T. Abdel-Massih, Zaki N. Abdel-Malek, and El-
Said M. Badawi. 1979. A Reference Grammar of
Egyptian Arabic. Georgetown University Press.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan.
2008. A Hybrid Approach for Converting Written
Egyptian Colloquial Dialect into Diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008. Cairo University.
Imad Al-Sughaiyer and Ibrahim Al-Kharashi. 2004.
Arabic Morphological Analysis Techniques: A Com-
prehensive Survey. Journal of the American Society
for Information Science and Technology, 55(3):189?
213.
Mohamed Altantawy, Nizar Habash, and Owen Ram-
bow. 2011. Fast Yet Rich Morphological Analysis.
In Proceedings of the 9th International Workshop on
Finite-State Methods and Natural Language Process-
ing (FSMNLP 2011), Blois, France.
Kenneth Beesley, Tim Buckwalter, and Stuart Newton.
1989. Two-Level Finite-State Analysis of Arabic Mor-
phology. In Proceedings of the Seminar on Bilingual
Computing in Arabic and English, page n.p.
Tim Buckwalter. 2004. Buckwalter arabic morpho-
logical analyzer version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Tim Buckwalter. 2007. Issues in Arabic Morphologi-
cal Analysis. In A. van den Bosch and A. Soudi, edi-
tors, Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
Mark W. Cowell. 1964. A Reference Grammar of Syrian
Arabic. Georgetown University Press.
8
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky, 2007.
Arabic Computational Morphology: Knowledge-
based and Empirical Methods, chapter Automated
Methods for Processing Arabic Text: From Tokeniza-
tion to Base Phrase Chunking. Springer.
Wallace Erwin. 1963. A Short Reference Grammar of
Iraqi Arabic. Georgetown University Press.
Hassan Gadalla, Hanaa Kilany, Howaida Arram, Ashraf
Yacoub, Alaa El-Habashi, Amr Shalaby, Krisjanis
Karins, Everett Rowson, Robert MacIntyre, Paul
Kingsbury, David Graff, and Cynthia McLemore.
1997. CALLHOME Egyptian Arabic Transcripts. In
Linguistic Data Consortium, Philadelphia.
Hassan Gadalla. 2000. Comparative Morphology of
Standard and Egyptian Arabic. LINCOM EUROPA.
David Graff, Mohamed Maamouri, Basma Bouziri,
Sondos Krouna, Seth Kulick, and Tim Buckwal-
ter. 2009. Standard Arabic Morphological Analyzer
(SAMA) Version 3.1. Linguistic Data Consortium
LDC2009E73.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A Morphological Analyzer and Generator for the Ara-
bic Dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 681?688, Sydney, Australia.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC), Istanbul.
Nizar Habash. 2007. Arabic Morphological Representa-
tions for Machine Translation. In Antal van den Bosch
and Abdelhadi Soudi, editors, Arabic Computational
Morphology: Knowledge-based and Empirical Meth-
ods. Kluwer/Springer.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Clive Holes. 2004. Modern Arabic: Structures, Func-
tions, and Varieties. Georgetown Classics in Ara-
bic Language and Linguistics. Georgetown University
Press.
H. Kilany, H. Gadalla, H. Arram, A. Yacoub, A. El-
Habashi, and C. McLemore. 2002. Egyptian
Colloquial Arabic Lexicon. LDC catalog number
LDC99L22.
George Anton Kiraz. 2000. Multi-Tiered Nonlinear
Morphology Using Multi-Tape Finite Automata: A
Case Study on Syriac and Arabic. Computational Lin-
guistics, 26(1):77?105.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, Mona
Diab, Nizar Habash, Owen Rambow, and Dalila
Tabessi. 2006. Developing and Using a Pilot Dialectal
Arabic Treebank. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Genoa,
Italy.
Mohamed Maamouri, Ann Bies, Seth Kulick, Dalila
Tabessi, and Sondos Krouna. 2012a. Egyptian Ara-
bic Treebank Pilot.
Mohamed Maamouri, Sondos Krouna, Dalila Tabessi,
Nadia Hamrouni, and Nizar Habash. 2012b. Egyptian
Arabic Morphological Annotation Guidelines.
Emad Mohamed, Behrang Mohit, and Kemal Oflazer.
2012. Annotating and Learning Morphological Seg-
mentation of Egyptian Colloquial Arabic. In Proceed-
ings of the Language Resources and Evaluation Con-
ference (LREC), Istanbul.
Jason Riesa and David Yarowsky. 2006. Minimally Su-
pervised Morphological Segmentation with Applica-
tions to Machine Translation. In Proceedings of the
7th Conference of the Association for Machine Trans-
lation in the Americas (AMTA06), pages 185?192,
Cambridge,MA.
Wael Salloum and Nizar Habash. 2011. Dialectal
to Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, pages 10?21, Edinburgh, Scotland.
Otakar Smr?. 2007. Functional Arabic Morphology. For-
mal System and Implementation. Ph.D. thesis, Charles
University in Prague, Prague, Czech Republic.
9
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 1?10,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Automatic Correction and Extension of Morphological Annotations
Ramy Eskander, Nizar Habash
Center for Computational Learning Systems, Columbia University
{reskander,habash}@ccls.columbia.edu
Ann Bies, Seth Kulick, Mohamed Maamouri
Linguistic Data Consortium, University of Pennsylvania
{bies,skulick,maamouri}@ldc.upenn.edu
Abstract
For languages with complex morpholo-
gies, limited resources and tools, and/or
lack of standard grammars, developing an-
notated resources can be a challenging
task. Annotated resources developed un-
der time/money constraints for such lan-
guages tend to tradeoff depth of represen-
tation with degree of noise. We present
two methods for automatic correction and
extension of morphological annotations,
and demonstrate their success on three di-
vergent Egyptian Arabic corpora.
1 Introduction
Annotated corpora are essential for most research
in natural language processing (NLP). For exam-
ple, the development of treebanks, such as the
Penn Treebank and the Penn Arabic Treebank,
has been essential in pushing research on part-
of-speech (POS) tagging and parsing of English
and Arabic (Marcus et al, 1993; Maamouri et al,
2004). The creation of such resources tends to be
quite expensive and time consuming: guidelines
need to be developed, annotators hired, trained,
and regularly evaluated for quality control. For
languages with complex morphologies, limited re-
sources and tools, and/or lack of standard gram-
mars, such as any of the Dialectal Arabic (DA)
varieties, developing annotated resources can be a
challenging task. As a result, annotated resources
developed under time/money constraints for such
languages tend to tradeoff depth of representation
with degree of noise. In the extremes, we find rich
morphological representations that may be noisy
and inconsistent or simple by highly consistent
and reliable annotations that have limited usabil-
ity. Furthermore, such resources are often devel-
oped by different research groups leading to many
inconstancies that make pooling these resources
not a very easy task.
In this paper, we describe two general tech-
niques to address the limitations of the two types
of annotations: corrections of rich noisy annota-
tions and extensions of clean but shallow ones.
We present our work on Egyptian Arabic, an im-
portant Arabic dialect with limited resources, and
rich and ambiguous morphology. Resulting from
this effort is the largest Egyptian Arabic corpus
annotated in one common representation by pool-
ing resources from three very different sources:
a non-final, pre-release version of the ARZ1 cor-
pora from the Linguistic Data Consortium (LDC)
(Maamouri et al, 2012g), the LDC?s CallHome
Egypt transcripts (Gadalla et al, 1997) and CMU?s
Egyptian Arabic corpus (CMUEAC) (Mohamed et
al., 2012).
Although the paper focuses on Arabic, the ba-
sic problem is relevant to other languages, espe-
cially spontaneously written colloquial language
forms such as those used in social media. The
general solutions we propose are language inde-
pendent given availability of specific language re-
sources.
Next we discuss some related work and rel-
evant linguistic facts (Sections 2 and 3, respec-
tively). Section 4 presents our annotation cor-
rection technique; and Section 5 presents out an-
notation extension technique. Finally, Section 6
presents some statistics on the Egyptian Arabic
corpus annotated in one unified representation re-
sulting from our correction and extension work.
2 Related Work
Much work has been done on automatic spelling
correction. Both supervised and unsupervised ap-
proaches have been used employing a variety of
1ARZ is the language code for Egyptian Ara-
bic, http://www-01.sil.org/iso639-3/
documentation.asp?id=arz
1
tools, resources, and heuristics, e.g., morpholog-
ical analyzers, language models, annotated data
and edit-distance measures, respectively (Kukich,
1992; Oflazer, 1996; Shaalan et al, 2003; Hassan
et al, 2008; Kolak and Resnik, 2002; Magdy and
Darwish, 2006). Our work is different from these
approaches in that it extends beyond spelling of
word forms to deeper annotations. However, we
use some of these techniques to correct not just
the words, but also malformed POS tags.
A number of efforts exist on treebank en-
richment for many languages including Arabic
(Palmer et al, 2008; Hovy et al, 2006; Alkuh-
lani and Habash, 2011; Alkuhlani et al, 2013).
Our morphological extension effort is similar to
Alkuhlani et al (2013)?s work except that they
start with tokenizations, reduced POS tags and de-
pendency trees and extend them to full morpho-
logical information.
There has been a lot of work on Arabic POS tag-
ging and morphological disambiguation (Habash
and Rambow, 2005; Smith et al, 2005; Hajic? et
al., 2005; Habash, 2010; Habash et al, 2013).
The work by Habash et al (2013) uses one of the
resources we improve on in this paper. In their
work, they simply attempt to ?synchronize? un-
known/malformed annotations with the morpho-
logical analyzer they use, thus forcing a reading on
the word to make the unknown/malformed annota-
tion usable. In our work, we address the cleaning
issue directly. We intend to make these automatic
corrections and extensions available in the future
so that they can be used in future disambiguation
tools.
Maamouri et al (2009) described a set of man-
ual and automatic techniques used to improve on
the quality of the Penn Arabic Treebank. Their
work is most similar to ours except in the follow-
ing aspects: we work only on morphology and for
dialectal Arabic, whereas their work is primarily
on syntax and standard Arabic. Furthermore, the
challenge of malformed tags is not a major prob-
lem for them, while it is a core problem for us.
Furthermore, we work with data that has partial
annotations that we extend, while their work was
for very rich syntax/morphology annotations.
3 Linguistic Facts
The Arabic language is a collection of variants,
most prominent amongst which is Modern Stan-
dard Arabic (MSA), the official language of the
media and education. The other variants, the Ara-
bic dialects, are the day-to-day native vernaculars
spoken in the Arab World. While MSA is the of-
ficial language, it is not the native language of any
modern day Arabic speakers. Their differences
from MSA are comparable to the differences be-
tween Romance languages and Latin.2
Egyptian Arabic poses many challenges for
NLP. Arabic in general is a morphologically com-
plex language which includes rich inflectional
morphology, expressed both templatically and af-
fixationally, and several classes of attachable cl-
itics. For example, the Egyptian Arabic word
A??J.

J?J
?? wi+ha+yi-ktib-uw+hA
3 ?and they will
write it? has two proclitics (+? wi+ ?and? and + ?
ha+ ?will?), one prefix -?


yi- ?3rd person?, one
suffix ?- -uw ?masculine plural? and one pronom-
inal enclitic A?+ +hA ?it/her?. The word is consid-
ered an inflected form of the lemma katab ?write
[lit. he wrote]?. An important challenge for NLP
work on dialectal Arabic in general is the lack of
an orthographic standard. Egyptian Arabic writ-
ers are often inconsistent even in their own writ-
ing (Habash et al, 2012a), e.g., the future particle
h Ha appears as a separate word or as a proclitic
+h/+? Ha+/ha+, reflecting different pronuncia-
tions. Arabic orthography in general drops dia-
critical marks that mark short vowels and gemi-
nation. However in analyses, we want these dia-
critics to be indicated. Moreover, some letters in
Arabic (in general) are often spelled inconsistently
which leads to an increase in both sparsity (multi-
ple forms of the same word) and ambiguity (same
form corresponding to multiple words), e.g., vari-
ants of Hamzated Alif,

@ ? or @ A?, are often writ-
ten without their Hamza (Z ?): @ A; and the Alif-
Maqsura (or dotless Ya) ? ? and the regular dotted
Ya ?


y are often used interchangeably in word fi-
nal position (El Kholy and Habash, 2010). For the
purposes of normalizing the representations used
in computational models, we follow the work of
Habash et al (2012a) who devised a conventional
orthography for dialectal Arabic (CODA) for use
in computational processing of Arabic dialects..
An analysis of an Egyptian word for our work
consists of a surface form that may not be in
2Habash and Rambow (2006) reported that a state-of-the-
art MSA morphological analyzer has only 60% coverage of
Levantine Arabic verb forms.
3Arabic orthographic transliteration is presented in the
Habash-Soudi-Buckwalter scheme (Habash et al, 2007):
@ H.
H H h. h p X
	
XP 	P ? ? ?
	
? ?
	
? ?
	
?
	
?

? ? ? ?
	
? ? ? ?


A b t ? j H x d?r z s ? S D T D? ? ? f q k l m n hw y
in addition to ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', ~ ?, ? ?.
2
CODA (henceforth, RAW), a fully diacritized
CODA form (henceforth, DIAC), a morpheme
split form (henceforth, MORPH), which may
slightly differ from the allomorphic DIAC surface
forms, a POS tag for each morpheme and stem,
and a lemma (henceforth LEM). For instance,
the Egyptian Arabic example used above has the
following analysis:
RAW whyktbuwhA
DIAC wiHayiktibuwhA
MORPH wi+Ha+yi+ktib+uwA+hA
POS CONJ+FUT_PART+IV3P+IV
+IVSUFF_SUBJ:3P+IVSUFF_DO:3FS
LEM katab
The morphological analyzers we use in the pa-
per, CALIMA (Habash et al, 2012b) and SAMA
(Graff et al, 2009), both generate the different lev-
els of representation discussed above.
4 Automatic Morphological Correction
In this section, we present the effort on auto-
matic morphological correction of rich noisy an-
notations. We next describe the data set we work
with and the problems it has. This is followed by
a discussion of our approach and results including
an error analysis.
4.1 Data
We use a non-final, pre-release version of six man-
ually annotated Egyptian Arabic corpora devel-
oped by the LDC, and labeled as ?ARZ?, parts one
through six. The published versions of these cor-
pora (Maamouri et al, 2012a-f) do not include the
annotation errors discussed in this paper. Rather,
in the official releases of the data from the LDC,
such problematic cases with an unknown POS tag
sequence (as in the example at the end of Sec-
tion 4.2) were caught and given a NO_FUNC POS
tag instead, in order to allow syntactic annotation
of the data to proceed, and in order to meet data
publication deadlines. The combined corpus con-
sists of about 274K words. The annotations are
very detailed contextually selected morphological
analyses that include for each RAW word its LEM,
POS, MORPH and DIAC as described earlier. The
LDC used the CALIMA4 Egyptian Arabic mor-
phological analyzer (Habash et al, 2012b) to pro-
vide the annotators with sets of analyses to se-
lect from.5 CALIMA?s non-lexical morphologi-
4Columbia Arabic Language and dIalect Morphological
Analyzer
5SAMA, the Standard Arabic Morphological Analyzer
(Graff et al, 2009), was used to provide the annotators with
cal coverage (i.e. model of affixes and stem POS
combinations) is almost complete; and its lexical
entries are of high precision. However, CALIMA
lacks some lexical items, i.e., its lexical recall is
not perfect ? Habash et al (2012b) report coverage
of 84% for basic CALIMA and 92% for CALIMA
extended with SAMA (Graff et al, 2009) (hence-
forth, CALIMA+SAMA or simply the analyzer).6
Many missing entries are a result of spelling vari-
ants that are not modeled in CALIMA. In cases
when CALIMA fails to provide analyses or the
annotators disagree with all the provided analy-
ses, the annotators enter the information manually
or copy and modify CALIMA provided analyses,
which sometimes introduces errors.
For the purpose of this work, we consider
all analyses in the corpus that are in the CAL-
IMA+SAMA morphological analyzer to be cor-
rect. We will not attempt to modify them. Al-
most 30% of the corpus analyses are not in the
analyzer, i.e. analyzer out-of-vocabulary (OOV).
We discuss next the general patterns of these anal-
yses. We refer to the original corpus analyses as
the ?Baseline? analyses.
4.2 Patterns of OOV Analyses in Baseline
About 3.3% of all OOV analyses (and 1% of all
corpus words) are tagged as TYPOs.7 We do not
address these cases in this paper.
Over half of the POS OOVs (56%) in the
pre-release data involve a different category of
a nominal (NOUN/NOUN_PROP/ADJ). This is
a well known issue even in MSA. The rest
of the cases involve incorrect feature combina-
tions such as giving the unaccusative verb
	
Y
	
?
	
J

K @
Aitnaf?i? ?be performed? the POS PV_PASS
(passive perfective).8 Another example is assign-
ing the feminine singular pronoun ?


X diy the
POS DEM_PRON instead of DEM_PRON_FS.
Or the imperative verb @? 	?? @ AilguwA ?cancel [you
plural]? the POS CV+CVSUFF_SUBJ:2MS (for
?you masculine singular?) instead of the correct
CV+CVSUFF_SUBJ:2MP. A tiny percentage of
all POS tags in the corpus (0.02%) include case-
related variation (e.g. CONJ vs Conj); these add
to type sparsity, but are trivial to handle.
analyses for the MSA tokens.
6In our work, we distinguish between morphological anal-
ysis, which refers to producing the various readings of a word
out of context, and morphological tagging (or disambigua-
tion), which identifies the appropriate analysis in context.
7The rate of TYPO words in the ARZ data is almost 18
times the rate in the MSA PATB data sets.
8The inflected verb Aitnaf?i? is the passive voice of the
verb with the lemma naf?a? or the active voice of the verb
with the lemma Aitnaf?i?.
3
Among LEMs and DIACs, there is consider-
able variation in the Arabic spelling, particularly
involving the spelling of Alif/Hamza forms, the
Egyptian long vowels /e:/ and /o:/ and often re-
quiring adjustment to conform to CODA guide-
lines.9 The following are some examples. Specific
CODA cases include spelling ?Y? kidah ?as such?
as @Y? kdA or spelling ?


?

? qawiy [pronounced
/awi/] ?very? as ?


?@ Awy. The preposition ?J

	
? fiyh
?in it? is incorrectly spelled as fiyuh (allomorphic
form is incorrect). The word I
K. bayt ?house? is
spelled biyt (long vowel spelling error). And fi-
nally the interjection

B l? ?no!? is spelled as (the
implausible form) Z? la?.
Among LEMs, over 63% of the errors is due
to inconsistency in assigning lemmas of punctu-
ation and digit, a trivial challenge. 29% of the
cases are spelling errors such as those discussed
above. The remaining 10% are due to not follow-
ing the specific format guidelines of lemmas (e.g.,
must be singular, uncliticized, and with a sense id
number). Among DIACs, almost all of the mis-
matches are non-CODA-compliant spelling varia-
tions. One third is Alif/Hamza forms, and another
quarter is long vowel spelling. One eighth involves
diacritic choice.
Combinations of these error types occur,
of course. One extreme case is the pro-
gressive particle prefix bi, which should be
tagged as bi/PROG_PART, but appears addi-
tionally as b/PROG_PART, ba/PROG_PART,
bi/PART_PROG, bi/PRO_PART, and
bi/FUT_PART.
Example For the rest of this section, we con-
sider the example word @??g.

AJ
k Hy?jlwA ?and
they will postpone?. Figure 1 contrasts an erro-
neous analysis in the pre-release data with a cor-
rected version of it. There are multiple problems
in this example. First, the POS tag is both in-
ternally inconsistent and is inconsistent with the
MORPH choice. The POS has a singular subject
prefix (IV3MS) and a plural subject suffix (IV-
SUFF_SUBJ:P); and the plural subject suffix is
written using the morpheme (+uh), which corre-
sponds to a direct object enclitic. The two mor-
phemes, +uh and +uwA, are homophonous, which
is the most likely cause for this error. Second, the
future marker (Ha+) is written in a non-CODA-
9LDC annotators were not asked to comply with CODA
guidelines during the annotation task. Therefore, multiple
spelling variants for OOV Egyptian Arabic words were to be
expected.
compliant way (ha+) in the analysis. And finally,
the lemma is malformed, containing multiple ex-
tra sense id digits. It is important to point out
that there are multiple ways to correct the anal-
ysis. For example, it can be Ha+yi+?aj?il+uh
FUT_PART+IV3MS+IV+IVSUFF_DO:3MS ?he
will postpone it?.10
4.3 Approach
Our target is to provide correct morphological
analyses for the OOV annotations in the pre-
release version of the ARZ corpus. Since not
all of the OOV annotations are wrong in prin-
ciple, we do not force map them all to CAL-
IMA+SAMA in-vocabulary variants, especially
for open class categories, where we know CAL-
IMA+SAMA may be deficient. As such, our gen-
eral solution focuses on correcting closed classes
(some stems and all of the affixes) by mapping
them to in-vocabulary variants. We also use a set
of language-specific preprocessing corrections for
common orthographic variations (for all open and
closed classes). An important tool we use through-
out to rank choices and break ties is modified Lev-
enshtein edit distance.11
Next, we present the four steps of our correction
process: annotation preprocessing, morpheme-
POS correction, lemma correction and surface
DIAC generation.
Annotation Preprocessing When first reading
the pre-release annotations, we perform a prepro-
cessing step that includes a set of deterministic
corrections for common non-CODA-compliant or-
thographic variations and errors, and POS tagging
typos. The corrections apply to the POS tags, lem-
mas, morphemes and surface forms. Examples of
these corrections include the following: reorder-
ing diacritics, e.g., saji?l? saj?il; removing du-
plicate diacritics, e.g., saj?iil? saj?il; adjusting
Alif-Hamza forms to match the diacritics that fol-
10Since our approach currently considers words out of con-
text, such a correction is not preferred because it requires
more character edits (see Figure 2). We acknowledge this
to be a limitation and plan to address it in the future.
11The Levenshtein edit distance is defined as the minimum
number of single-character edits (insertion, deletion and sub-
stitution) required to change one string into the other. For
Arabic words and morphemes, we modify the cost of sub-
stitutions involving two phonologically or orthographically
similar letters to count as half edits. We acquire the list of
such letter substitutions from Eskander et al (2013), who re-
port them as the most frequent source of errors in Egyptian
Arabic orthography. We map all diacritic-only morphemes
to empty morphemes in both ways at a cost of half edit also.
For POS tag edit distance, we use the standard definition of
Levenshtein edit distance. Edit cost is an area where a lot of
tuning could be done and we plan to explore it in the future.
4
RAW ??g. AJ
? hyAjlw
Analysis Incorrect Annotation Correct Annotation
DIAC hayi?aj?iluh Hayi?aj?iluwA
MORPH ha+yi+?aj?il+uh Ha+yi+?aj?il+uwA
POS FUT_PART+IV3MS+IV+IVSUFF_SUBJ:P FUT_PART+IV3P+IV+IVSUFF_SUBJ:P
LEM ?aj?ill1 ?aj?il_1
Figure 1: An incorrect annotation example with a possible correction.
low them, e.g., A?aSl? ?aSl; and POS tag capital-
ization, e.g., Fut_Part? FUT_PART.
Morpheme-POS Correction For morpheme
correction purposes, we define an abstract rep-
resentation that combines all the closed-class
morphemes and POS tags. For open-class
stems, we simply use the POS tag. For exam-
ple, the abstract morpheme representation for
the correct version of the word in Figure 1 is
Ha/FUT_PART+yi/IV3P+IV+uwA/IVSUFF_SUBJ:P.
We will refer to this representation as the inflec-
tional morph-tag (IMT).
We build two models for this task. First, we
build an IMT language model from the CAL-
IMA+SAMA databases. This models all possible
inflections in the analyzer without the open class
stems. This model includes 304K sequences. Sec-
ond, we construct a map from all the seen IMTs
in the ARZ corpus to all the in-vocabulary IMTs
in the IMT language model. The mapping in-
cludes a cost that is based on the edit distance dis-
cussed earlier. Figure 2 shows the top mappings
for the IMTs in our example. Both models are im-
plemented as finite state machines using the ATT
FSM toolkit (Mohri et al, 1998).
The input, possibly incorrect, IMT is con-
verted into an FSM that is then composed
with the mapping transducer and the language
model automaton to generate a cost-ranked list
of mappings. The output for our example is
listed in Figure 3. We then replace the input
POS and MORPH with the top ranked correction:
Ha/FUT_PART+yi/IV3MS+IV+uh/IVSUFF_SUBJ:P
at a cost of 4.0. The open class stem is not modi-
fied.
Lemma Correction We generate a map that in-
cludes all the possible lemmas for every possi-
ble stem morpheme in CALIMA+SAMA. For a
given ARZ word analysis, if the stem morpheme
is in CALIMA+SAMA, then we pick the lemma
from its corresponding lemma set. When there is
more than one possible lemma, we pick the lemma
that is closest to the provided pre-release ARZ
Base IMT
Morpheme
Mapped IMT
Morphemes Cost
ha/FUT_PART Ha/FUT_PART 0.5
sa/FUT_PART 1.0
yi/IV3MS
yi/IV3MS 0.0
ya/IV3MS 1.0
y/IV3MS 1.0
yu/IV3MS 1.0
yi/IV3P 2.0
IV
IV 0.0
PV 1.0
CV 1.0
uh/IVSUFF_SUBJ:P uwA/IVSUFF_SUBJ:P 1.5
na/IVSUFF_SUBJ:FP 3.0
Figure 2: Top mappings for the IMT morphemes
ha/FUT_PART, yi/IV3P, IV and uh/IVSUFF_SUBJ:P
Input: ha/FUT_PART+yi/IV3P+IV+uh/IVSUFF_SUBJ:P
FSM Output Cost
Ha/FUT_PART+yi/IV3P+IV+uwA/IVSUFF_SUBJ:P 4.0
Ha/FUT_PART+y/IV3P+IV+uwA/IVSUFF_SUBJ:P 5.0
Ha/FUT_PART+ti/IV2P+IV+uwA/IVSUFF_SUBJ:P 6.0
Ha/FUT_PART+yi/IV3MS+IV+uh/IVSUFF_DO:3MS 6.5
Ha/FUT_PART+yi/IV3MS+IV+kuw/IVSUFF_DO:2P 7.0
Ha/FUT_PART+yi/IV3MS+IV+nA/IVSUFF_DO:1P 7.0
Ha/FUT_PART+tu/IV2P+IV+uwA/IVSUFF_SUBJ:P 7.0
sa/FUT_PART+ya/IV3FP+IV+na/IVSUFF_SUBJ:FP 7.0
sa/FUT_PART+yu/IV3FP+IV+na/IVSUFF_SUBJ:FP 7.0
Ha/FUT_PART+yi/IV3MS+IV+kum/IVSUFF_DO:2P 7.5
Figure 3: Top corrections for the input
ha/FUT_PART+yi/IV3P+IV+uh/IVSUFF_SUBJ:P
lemma, based on their string edit distance as de-
fined earlier. If the stem morpheme is not in CAL-
IMA+SAMA (e.g., open class), then we keep the
ARZ lemma as it is.
In our example, the stem morpheme ?aj?il/IV
is paired in CALIMA+SAMA with the lemma
?aj?il_1. Accordingly, ?aj?il_1 replaces the in-
put pre-release ARZ lemma.
Surface DIAC Generation After correcting the
morphemes and POS tags in the input word,
we use them to generate a new surface DIAC
form. For all the closed-class morphemes and
in-vocabulary open-class stems, we use CAL-
IMA+SAMA to identify all the MORPH+POS to
DIAC mappings. For open-class stems that are
5
OOVs, we use their corresponding DIAC form in
the input word.12 This may lead to many possible
sequences. We rank them by their edit distance
(defined above) to the surface DIAC of the input
word.
In our example, this process is rather trivial:
every morpheme is paired with only one surface
DIAC in the morphological analyzer. The surface
DIACs corresponding to Ha/FUT_PART, yi/IV3P,
?aj?il/IV and uwA/IVSUFF_SUBJ:P are Ha, yi,
?aj?il and uwA, respectively. The final combined
surface is Hayi?aj?iluwA.
A more interesting example is the word A 	JJ
??
?alay+nA ?upon us? which has the analysis
?ala?/PREP+nA/PRON_1P. The MORPH stem
?ala? has two DIAC forms: ?ala? and ?alay. The
second form is only used when an enclitic is
present. It is selected in this example because it
has a smaller edit distance to the full word input
DIAC form than the surface stem ?ala?. In the
future, we plan to use more sophisticated genera-
tion and detokenization techniques (El Kholy and
Habash, 2010).
4.4 Results and Error Analysis
Results We conducted a manual evaluation for
1,000 words from the internal, pre-release ARZ
after applying the automatic correction process.
This set is a blind test set, i.e., not used as part
of the development. The results are listed in Ta-
ble 1 for the lemmas, POS tags, diacritized mor-
phemes and diacritized surface forms, in addition
to the complete morphological analyses (token-
based), where the correction output is compared
to the pre-release ARZ annotations (the baseline).
The results are listed for different subsets of
the data. The first row lists the results consider-
ing the complete 1,000 words, where all the in-
vocabulary words are considered correct. This is
only intended to give an overall estimate of the
correctness of the set. The second row lists the re-
sults for CALIMA+SAMA OOV words only. The
third row is the same as the second, but exclud-
ing punctuations, digits and typos. Focusing on
the last row, we see that we achieve between 58%
and 24% error reduction on different features, and
reach almost 40% error reduction on all features
combined.
Error Analysis For POS, 99.7% of all the cor-
rect cases in the Baseline were not changed. Only
12Since the surface DIAC splits are not provided, we deter-
mine the exact boundary of the surface DIAC stem by mini-
mizing the edit distance between the prefixing/suffixing mor-
phemes and the full input surface DIAC form.
one case was changed and it was caused by an er-
ror in the input MORPH splits. Of the erroneous
cases in the Baseline, 40% were not changed.
Among the attempted changes, 71% successfully
fixed the baseline problem. Almost all of the failed
changes are due to implausible null pronouns in
the Baseline that were not handled in the cur-
rent implementation, which only considered cor-
rect null pronouns. We plan to address these in
the future. Among the errors that were not ad-
dressed, the most common case involves nominal
form (41%) followed by hard features to resolve
and open class passive-voice inconsistency (each
27%).
Regarding lemmas, 93.9% of all correct base-
line lemmas remained correct. In the rest, over-
correction attempts resulting from matching the
OOV lemma to the wrong in-vocabulary lemma
backfired. Around 8.7% of the erroneous baseline
lemmas were not modified and 1.6% were mod-
ified incorrectly. The rest, 92.8%, were success-
fully fixed. Almost all of the system errors result-
ing from changes involve over correction by map-
ping to incorrect INV lemma forms.
Finally, as for diacritized forms, 96.9% of the
correct baseline DIACs remained correct; the rest
fell victim to over-correction. Among incorrect
baseline cases, 43% remained unchanged; and
45% were fixed; 4% were over-corrected and 8%
only partially corrected. Remaining DIAC errors
are mostly in open classes where the analyzer re-
call problems cannot help.
5 Automatic Morphological Extension
In this section, we present the general technique
we use to extend shallow annotations. We discuss
the data sets, the approach and evaluation results
next.
5.1 Data
We conduct our experiments on two differ-
ent Egyptian Arabic corpora: the CALLHOME
Egypt (CHE) corpus (Gadalla et al, 1997) and
Carnegie Mellon University Egyptian Arabic cor-
pus (CMUEAC) (Mohamed et al, 2012).
CHE The CHE corpus contains 140 telephone
conversation transcripts of about 179K words.
Each word is represented by its phonological form
and undiacritized Arabic script orthography. The
orthography used is quite similar to the CODA
standard we use. Being a transcript corpus, it is
quite clean and free of spelling variations. We use
a technique described in more detail in Habash et
6
POS
LEM POS MORPH DIAC +MORPH All
All words
Baseline 79.8% 93.2% 92.2% 91.1% 87.3% 72.7%
System 95.7% 95.5% 93.8% 93.6% 91.5% 90.0%
Analyzer OOV
Baseline 47.1% 82.4% 79.7% 76.8% 66.8% 28.4%
System 88.9% 88.42% 83.9% 83.4% 77.9% 73.9%
Analyzer OOV, no Baseline 71.3% 82.5% 74.1% 69.7% 59.0% 43.0%
Punc/Digit/Typos System 88.0% 87.3% 80.5% 79.7% 71.3% 65.3%
Table 1: Accuracy of the automatic morphological correction of internal, pre-release ARZ data.
al. (2012b) to combine the phonological form and
undiacritized Arabic script into diacritized Arabic
script, i.e. DIAC. For example, the undiacritized
word ? 	JJ
? ?ynh ?his eye? is combined with its pro-
nunciation /?e:nu/ producing the diacritized form
?aynuh.
CMUEAC The CMUEAC corpus includes
about 23K words that are only annotated for
morph splits. The corpus text includes sponta-
neously written Egyptian Arabic text collected off
the web. To use the same example as above, the
word ? 	JJ
? ?ynh ?his eye? is segmented as ?yn+h in-
dicating that there is a base word plus an enclitic.
5.2 Approach
Our approach to morphological extension is to au-
tomatically annotate the corpus using a very rich
morphological tagger, and then use the limited
manual annotations to adjust the morphological
choice. We use a morphological tagger, MADA-
ARZ (Morphological Analysis and Disambigua-
tion for Egyptian Arabic) (Habash et al, 2013).
MADA-ARZ produces, for each input word, a
contextually ranked list of analyses specifying all
the morphological interpretations of that word as
provided by the CALIMA+SAMA morphological
analyzer.
CHE In the case of CHE, we select the first
choice from the ranked list of analyses whose
DIAC matches the diacritized word in CHE. For
example, for the word ? 	JJ
? ?ynh MADA-ARZ
generates 45 different morphological analyses
with different lemmas, POS, orthographies and
diacritics: ?ayn+uh ?his eye?, ?ay?in+a~ ?sam-
ple? and ?ay?in+uh ?he appointed him?. The
diacritized word ?ayn+uh allows us to select the
following full analysis:
Metric CHE CMUEAC
LEM 97.2 82.0
POS 95.2 79.6
MORPH 96.8 77.6
DIAC 97.2 78.4
POS+MORPH 92.8 74.0
All 92.8 72.0
Table 2: Accuracy of automatic morphological ex-
tension of CHE and CMUEAC.
RAW Eynh
DIAC Eaynuh
MORPH Eayn+uh
POS NOUN+POSS_PRON_3MS
LEM Eayn_1
Although this example may not require the full
power of a tagger, but just the out-of-context an-
alyzer, other cases involving POS ambiguity un-
realized through diacritization necessitate the use
of a tagger, e.g., the word I.

KA? kAtib can be
an ADJ meaning ?writing? or a NOUN meaning
?writer/author?.
CMUEAC In the case of CMUEAC, we se-
lect the first choice from the ranked list of anal-
yses whose undiacritized MORPH splits match the
word tokenization. In the case of the word ? 	JJ
?
?yn+h, the tokenization cannot distinguish be-
tween the noun reading ?ayn+uh ?his eye? and
the verbal reading ?ay?in+uh ?he appointed him?.
MADA-ARZ effectively selects in such cases.
We expect the performance on CMUEAC to
be worse than CHE given the difference in the
amount of information between the two corpora.
5.3 Results and Error Analysis
We evaluate the accuracy of the morphological
extension process on both CHE and CMUEAC
using two 300 word samples that were manu-
ally enriched. Table 2 presents the accuracies of
the assigned LEMs, POS tags, DIAC forms and
7
MORPHs, in addition to the complete morpholog-
ical analysis. All results are token-based.
CHE CHE analyses have high accuracies rang-
ing between 95.2% and 97.2% for the different
analysis features, with the complete analysis hav-
ing an accuracy of 92.8%. One third of the er-
rors is due to gold diacritization errors in the
CHE corpus. 28% of the errors are due to wrong
verbal features (person, number and gender) for
forms that are not distinguishable in DIAC, e.g.,
I.

J? katabt ?I/you wrote? and I.

J?

K tiktib ?you
write/she writes?. The rest of the errors are be-
cause of failure in assigning the correct POS tags
for nouns, particles and verbs with percentages of
22%, 11% and 6%, respectively.
CMUEAC CMUEAC analyses have much
lower accuracies compared to CHE, ranging be-
tween 77.6% and 82.0% for different features,
with the complete analysis accuracy at 72.0%. The
CMUEAC is much harder to extend for two rea-
sons: the text, being naturally occurring, con-
tains a lot of orthographic noise; and tokeniza-
tion information is not sufficient to disambiguate
many analyses. For CMUEAC, a quarter of the
errors is due to gold tokenization errors in the
original CMUEAC corpus. Another quarter of
the errors results from MADA-ARZ assigning an
MSA analysis instead of an Egyptian Arabic anal-
ysis.13 Failure to assign the correct POS tags for
particles, verbs and nouns represents 14%, 10%
and 7% of the errors, respectively. Other errors
are because of wrong verbal features (13%) and
wrong diacritization (6%).
As expected, relatively richer annotations (i.e.,
diacritics) are easier to extend to full morpholog-
ical information that relatively poorer annotations
(i.e., tokenization). Of course, the tradeoff is still
there as tokenizations are much easier and cheaper
to annotate. We plan to explore the question of
what would be an optimal set of poor annotations
that can help us extend to the full morphology at
high accuracy in the future.
6 Egyptian Corpus
After applying morphological corrections to pre-
release ARZ and morphological extensions to
CHE and CMUEAC, we have now three big cor-
pora that are automatically adjusted to include
the same rich morphological information, that is:
13MADA-ARZ is trained on a combination of MSA and
Egyptian Arabic text and as such may select an MSA analysis
in cases that are ambiguous.
lemma, POS tag, diacritized morphemes, and dia-
critized surface. We combine the three resources
together in one morphologically rich corpus that
contains about 46K sentences and 447K words,
representing 61K unique lemmas. We intend to
make these automatic corrections and extensions
available in the future to provide extensive sup-
port for Egyptian Arabic processing for different
purposes.
7 Conclusion and Future Work
We presented two methods for automatic correc-
tion and extension of morphological annotations
and demonstrated their success on three different
Egyptian Arabic corpora, which now have annota-
tions that are automatically adjusted to include the
same rich morphological information although at
different degrees of quality that correspond to the
amount of initial information.
We presented two methods for automatic cor-
rection and extension of morphological annota-
tions and demonstrated their success on three dif-
ferent Egyptian Arabic corpora, which now have
annotations that are automatically adjusted to in-
clude the same rich morphological information al-
though at different degrees of quality that corre-
spond to the amount of initial information.
In the future, we plan to study how to optimize
the amount of basic information to annotate man-
ually in order to maximize the benefit of auto-
matic extensions. We also plan to provide feed-
back to the annotation process to reduce the per-
centage of errors generated by the annotators, per-
haps through a tighter integration of the correc-
tion/extension techniques with the annotation pro-
cess. We also plan on using the cleaned up corpus
to extend the existing analyzer for Egyptian Ara-
bic.
Acknowledgment
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under contracts No. HR0011-12-C-
0014 and HR0011-11-C-0145. Any opinions,
findings and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of DARPA. We
also would like to thank Emad Mohamed and Ke-
mal Oflazer for providing us with the CMUEAC
corpus. We thank Ryan Roth for help with
MADA-ARZ. Finally, we thank Owen Rambow,
Mona Diab and Warren Churchill for helpful dis-
cussions.
8
References
Sarah Alkuhlani and Nizar Habash. 2011. A Corpus
for Modeling Morpho-Syntactic Agreement in Ara-
bic: Gender, Number and Rationality. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL?11), Portland,
Oregon, USA.
Sarah Alkuhlani, Nizar Habash, and Ryan Roth. 2013.
Automatic morphological enrichment of a morpho-
logically underspecified treebank. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 460?470, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Ahmed El Kholy and Nizar Habash. 2010. Techniques
for Arabic Morphological Detokenization and Or-
thographic Denormalization. In Proceedings of the
seventh International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing spontaneous orthog-
raphy. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 585?595, Atlanta, Georgia, June.
Association for Computational Linguistics.
Hassan Gadalla, Hanaa Kilany, Howaida Arram,
Ashraf Yacoub, Alaa El-Habashi, Amr Shalaby,
Krisjanis Karins, Everett Rowson, Robert MacIn-
tyre, Paul Kingsbury, David Graff, and Cynthia
McLemore. 1997. CALLHOME Egyptian Ara-
bic Transcripts. In Linguistic Data Consortium,
Philadelphia.
David Graff, Mohamed Maamouri, Basma Bouziri,
Sondos Krouna, Seth Kulick, and Tim Buckwal-
ter. 2009. Standard Arabic Morphological Analyzer
(SAMA) Version 3.1. Linguistic Data Consortium
LDC2009E73.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A Morphological Analyzer and Generator for the
Arabic Dialects. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 681?688, Sydney, Aus-
tralia.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalu-
ation Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In NAACL-HLT 2012 Workshop on Com-
putational Morphology and Phonology (SIGMOR-
PHON2012), pages 1?9, Montr?al, Canada.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?, Otakar Smr?, Tim Buckwalter, and Hubert
Jin. 2005. Feature-based tagger of approximations
of functional Arabic morphology. In Proceedings of
the Workshop on Treebanks and Linguistic Theories
(TLT), Barcelona, Spain.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction us-
ing Finite State Automata. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2008).
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In NAACL ?06: Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Short
Papers on XX, pages 57?60, Morristown, NJ, USA.
Okan Kolak and Philip Resnik. 2002. OCR error cor-
rection using a noisy channel model. In Proceed-
ings of the second international conference on Hu-
man Language Technology Research.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Sur-
veys, 24(4).
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, and Seth Kulick.
2009. Creating a methodology for large-scale cor-
rection of treebank annotation: The case of the ara-
bic treebank. In MEDAR Second International Con-
ference on Arabic Language Resources and Tools,
Egypt. Citeseer.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012a.
Egyptian Arabic Treebank DF Part 1 V2.0. LDC
catalog number LDC2012E93.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012b.
Egyptian Arabic Treebank DF Part 2 V2.0. LDC
catalog number LDC2012E98.
9
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012c.
Egyptian Arabic Treebank DF Part 3 V2.0. LDC
catalog number LDC2012E89.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012d.
Egyptian Arabic Treebank DF Part 4 V2.0. LDC
catalog number LDC2012E99.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012e.
Egyptian Arabic Treebank DF Part 5 V2.0. LDC
catalog number LDC2012E107.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012f.
Egyptian Arabic Treebank DF Part 6 V2.0. LDC
catalog number LDC2012E125.
Mohamed Maamouri, Sondos Krouna, Dalila Tabessi,
Nadia Hamrouni, and Nizar Habash. 2012g. Egyp-
tian Arabic Morphological Annotation Guidelines.
Walid Magdy and Kareem Darwish. 2006. Ara-
bic OCR Error Correction Using Character Segment
Correction, Language Modeling, and Shallow Mor-
phology. In Proceedings of 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2006), pages 408?414, Sydney, Austrailia.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330, June.
Emad Mohamed, Behrang Mohit, and Kemal Oflazer.
2012. Annotating and Learning Morphological Seg-
mentation of Egyptian Colloquial Arabic. In Pro-
ceedings of the Language Resources and Evaluation
Conference (LREC), Istanbul.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 1998. A rational design for a weighted finite-
state transducer library. In D. Wood and S. Yu, ed-
itors, Automata Implementation, Lecture Notes in
Computer Science 1436, pages 144?58. Springer.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73?90.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohamed Maamouri, Aous Mansouri, and
Wajdi Zaghouani. 2008. A Pilot Arabic Prop-
bank. In Proceedings of LREC, Marrakech, Mo-
rocco, May.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Conference on Language Engineering,
ELSE, Cairo, Egypt.
Noah Smith, David Smith, and Roy Tromble. 2005.
Context-Based Morphological Disambiguation with
Random Fields. In Proceedings of the 2005 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP05), pages 475?482, Vancou-
ver, Canada.
10
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 30?38,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Automatic Transliteration of Romanized Dialectal Arabic
Mohamed Al-Badrashiny
?
, Ramy Eskander, Nizar Habash and Owen Rambow
?
Department of Computer Science, The George Washington University, Washington, DC
?
badrashiny@gwu.edu
Center for Computational Learning Systems, Columbia University, NYC, NY
{reskander,habash,rambow}@ccls.columbia.edu
Abstract
In this paper, we address the problem
of converting Dialectal Arabic (DA) text
that is written in the Latin script (called
Arabizi) into Arabic script following the
CODA convention for DA orthography.
The presented system uses a finite state
transducer trained at the character level
to generate all possible transliterations for
the input Arabizi words. We then filter
the generated list using a DA morpholog-
ical analyzer. After that we pick the best
choice for each input word using a lan-
guage model. We achieve an accuracy of
69.4% on an unseen test set compared to
63.1% using a system which represents a
previously proposed approach.
1 Introduction
The Arabic language is a collection of varieties:
Modern Standard Arabic (MSA), which is used
in formal settings and has a standard orthogra-
phy, and different forms of Dialectal Arabic (DA),
which are commonly used informally and with in-
creasing presence on the web, but which do not
have standard orthographies. While both MSA
and DA are commonly written in the Arabic script,
DA (and less so MSA) is sometimes written in
the Latin script. This happens when using an Ara-
bic keyboard is dispreferred or impossible, for ex-
ample when communicating from a mobile phone
that has no Arabic script support. Arabic written
in the Latin script is often referred to as ?Arabizi?.
Arabizi is not a letter-based transliteration from
the Arabic script as is, for example, the Buck-
walter transliteration (Buckwalter, 2004). Instead,
roughly speaking, writers use sound-to-letter rules
inspired by those of English
1
as well as informally
1
In different parts of the Arab World, the basis for the
Latin script rendering of DA may come from different lan-
established conventions to render the sounds of the
DA sentence. Because the sound-to-letter rules
of English are very different from those of Ara-
bic, we obtain complex mappings between the two
writing systems. This issue is compounded by the
underlying problem that DA itself does not have
any standard orthography in the Arabic script. Ta-
ble 1 shows different plausible ways of writing an
Egyptian Arabic (EGY) sentence in Arabizi and
in Arabic script.
Arabizi poses a problem for natural language
processing (NLP). While some tools have recently
become available for processing EGY input, e.g.,
(Habash et al., 2012b; Habash et al., 2013; Pasha
et al., 2014), they expect Arabic script input (or a
Buckwalter transliteration). They cannot process
Arabizi. We therefore need a tool that converts
from Arabizi to Arabic script. However, the lack
of standard orthography in EGY compounds the
problem: what should we convert Arabizi into?
Our answer to this question is to use CODA, a
conventional orthography created for the purpose
of supporting NLP tools (Habash et al., 2012a).
The goal of CODA is to reduce the data sparseness
that comes from the same word form appearing in
many spontaneous orthographies in data (be it an-
notated or unannotated). CODA has been defined
for EGY as well as Tunisian Arabic (Zribi et al.,
2014), and it has been used as part of different ap-
proaches for modeling DA morphology (Habash
et al., 2012b), tagging (Habash et al., 2013; Pasha
et al., 2014) and spelling correction (Eskander et
al., 2013; Farra et al., 2014).
This paper makes two main contributions. First,
we clearly define the computational problem of
transforming Arabizi to CODA. This improves
over previous work by unambiguously fixing the
guages that natively uses the Latin script, such as English
or French. In this paper, we concentrate on Egyptian Arabic,
which uses English as its main source of sound-to-letter rules.
30
target representation for the transformation. Sec-
ond, we perform experiments using different com-
ponents in a transformation pipeline, and show
that a combination of character-based transduc-
tion, filtering using a morphological analyzer, and
using a language model outperforms other archi-
tectures, including the state-of-the-art system de-
scribed in Darwish (2013). Darwish (2013) pre-
sented a conversion tool, but did not discuss con-
version into a conventionalized orthography, and
did not investigate different architectures. We
show in this paper that our proposed architecture,
which includes an EGY morphological analyzer,
improves over Darwish?s architecture.
This paper is structured as follows. We start out
by presenting relevant linguistic facts (Section 2)
and then we discuss related work. We present our
approach in Section 4 and our experiments and re-
sults in Section 5.
2 Linguistic Facts
2.1 EGY Spontaneous Orthography
An orthography is a specification of how to use
a particular writing system (script) to write the
words of a particular language. In cases where
there is no standard orthography, people use a
spontaneous orthography that is based on dif-
ferent criteria. The main criterion is phonol-
ogy: how to render a word pronunciation in
the given writing system. This mainly de-
pends on language-specific assumptions about the
grapheme-to-phoneme mapping. Another crite-
rion is to use cognates in a related language (sim-
ilar language or a language variant), where two
words represent a cognate if they are related et-
ymologically and have the same meaning. Ad-
ditionally, a spontaneous orthography may be af-
fected by speech effects, which are the lengthen-
ing of specific syllables to show emphasis or other
effects (such as
Q



J


J






J? ktyyyyr
2
?veeeery?).
EGY has no standard orthography. Instead,
it has a spontaneous orthography that is related
to the standard orthography of Modern Standard
Arabic. Table 1 shows an example of writing a
sentence in EGY spontaneous orthography in dif-
ferent variants.
2
Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al., 2007): (in alphabetical
order) Abt?jHxd?rzs?SDT
?
D??fqklmnhwy and the additional
symbols: ? Z, ?

@,
?
A @

,
?
A

@, ?w

?', ?y Z?', h?

?, ? ?.
2.2 Arabizi
Arabizi is a spontaneous orthography used to write
DA using the Latin script, the so-called Arabic
numerals, and other symbols commonly found on
various input devices such as punctuation. Arabizi
is commonly used by Arabic speakers to write in
social media and SMS and chat applications.
The orthography decisions made for writing
in Arabizi mainly depend on a phoneme-to-
grapheme mapping between the Arabic pronunci-
ation and the Latin script. This is largely based
on the phoneme-to-grapheme mapping used in En-
glish. Crucially, Arabizi is not a simple transliter-
ation of Arabic, under which each Arabic letter in
some orthography is replaced by a Latin letter (as
is the case in the Buckwalter transliteration used
widely in natural language processing but nowhere
else). As a result, it is not straightforward to con-
vert Arabizi to Arabic. We discuss some specific
aspects of Arabizi.
Vowels While EGY orthography omits vocalic
diacritics representing short vowels, Arabizi uses
the Latin script symbols for vowels (a, e, i, o, u, y)
to represent EGY?s short and long vowels, making
them ambiguous. In some cases, Arabizi words
omit short vowels altogether as is done in Arabic
orthography.
Consonants Another source of ambiguity is the
use of a single Latin letter to refer to multiple Ara-
bic phonemes. For example, the Latin letter "d" is
used to represent the sounds of the Arabic letters
X d and
	
? D. Additionally, some pairs of Arabizi
letters can ambiguously map to a single Arabic let-
ter or pairs of letters: "sh" can be use to represent

? ? or ?? sh. Arabizi also uses digits to repre-
sent some Arabic letters. For example, the dig-
its 2, 3, 5, 6, 7 and 9 are used to represent the
Hamza (glottal stop), and the sounds of the letters
? ? , p x, ? T, h H and ? S, respectively. How-
ever, when followed by "?", the digits 3, 6, 7 and
9 change their interpretations to the dotted version
of the Arabic letter:
	
? ?,
	
?
?
D, p x and
	
? D, re-
spectively. Moreover, "?" (as well as "q") may also
refer to the glottal stop.
Foreign Words Arabizi contains a large num-
ber of foreign words, that are either borrowings
such as mobile or instances of code switching such
as I love you.
Abbreviations Arabizi may also include some
abbreviations such as isa which means ?<? @ Z A

?
	
?@

?
An ?A? Allh ?God willing?.
31
Orthography Example
CODA
hPAJ
.
?@
	
?? ?


G
.
Am

?

?


	
?

? A?
mA ?ft? SHAby mn AmbArH
Non-CODA
hPAJ
.
?@
	
?? ?G
.
Ag??

?


	
??

?A?
Arabic Script mA?wft? SwHAb? mn AmbArH
hPAJ
.
	
K @

	
?? ?G
.
Am

?

?


	
?

??
m?ft? SHAb? mn
?
AnbArH
hPAJ
.
?@

	
?? ?


G
.
Am

?

?



J
	
?

? A?
mA ?fty? SHAby mn
?
AmbArH
Arabizi
mashoftesh sohaby men embare7
ma shftesh swhabi mn imbareh
mshwftish swhaby min ambare7
Table 1: The different spelling variants in EGY and Arabizi for writing the sentence "I have not seen my
friends since yesterday" versus its corresponding CODA form.
2.3 CODA
CODA is a conventionalized orthography for Di-
alectal Arabic (Habash et al., 2012a). In CODA,
every word has a single orthographic representa-
tion. CODA has five key properties (Eskander
et al., 2013). First, CODA is an internally con-
sistent and coherent convention for writing DA.
Second, CODA is primarily created for computa-
tional purposes, but is easy to learn and recognize
by educated Arabic speakers. Third, CODA uses
the Arabic script as used for MSA, with no ex-
tra symbols from, for example, Persian or Urdu.
Fourth, CODA is intended as a unified framework
for writing all dialects. CODA has been defined
for EGY (Habash et al., 2012a) as well as Tunisian
Arabic (Zribi et al., 2014). Finally, CODA aims
to maintain a level of dialectal uniqueness while
using conventions based on similarities between
MSA and the dialects. For a full presentation of
CODA and a justification and explanation of its
choices, see (Habash et al., 2012a).
CODA has been used as part of different ap-
proaches for modeling DA morphology (Habash
et al., 2012b), tagging (Habash et al., 2013; Pasha
et al., 2014) and spelling correction (Eskander et
al., 2013; Farra et al., 2014). Converting Dialec-
tal Arabic (written using a spontaneous Arabic or-
thography or Arabizi) to CODA is beneficial to
NLP applications that better perform on standard-
ized data with less sparsity (Eskander et al., 2013).
Table 1 shows the CODA form corresponding
to spontaneously written Arabic.
3 Related Work
Our proposed work has some similarities to Dar-
wish (2013). His work is divided into two sec-
tions: language identification and transliteration.
He used word and sequence-level features to iden-
tify Arabizi that is mixed with English. For Arabic
words, he modeled transliteration from Arabizi to
Arabic script, and then applied language model-
ing on the transliterated text. This is similar to our
proposed work in terms of transliteration and lan-
guage modeling. However, Darwish (2013) does
not target a conventionalized orthography, while
our system targets CODA. Additionally, Darwish
(2013) transliterates Arabic words only after filter-
ing out non-Arabic words, while we transliterate
the whole input Arabizi. Finally, he does not use
any morphological information, while we intro-
duce the use of a morphological analyzer to sup-
port the transliteration pipeline.
Chalabi and Gerges (2012) presented a hybrid
approach for Arabizi transliteration. Their work
relies on the use of character transformation rules
that are either handcrafted by a linguist or au-
tomatically generated from training data. They
also employ word-based and character-based lan-
guage models for the final transliteration choice.
Like Darwish (2013), the work done by Chalabi
and Gerges (2012) is similar to ours except that
it does not target a conventionalized orthography,
and does not use deep morphological information,
while our system does.
There are three commercial products that con-
32
vert Arabizi to Arabic, namely: Microsoft Maren,
3
Google Ta3reeb
4
and Yamli.
5
However, since
these products are for commercial purposes, there
is not enough information about their approaches.
But given their output, it is clear that they do
not follow a well-defined standardized orthogra-
phy like we do. Furthermore, these tools are pri-
marily intended as input method support, not full
text transliteration. As a result, their users? goal
is to produce Arabic script text not Arabizi text.
We expect, for instance, that users of these input
method support systems will use less or no code
switching to English, and they may employ char-
acter sequences that help them arrive at the target
Arabic script form, which otherwise they would
not write if they are targeting Arabizi.
Eskander et al. (2013) introduced a system
to convert spontaneous EGY to CODA, called
CODAFY. The difference between CODAFY and
our proposed system is that CODAFY works on
spontaneous text written in Arabic script, while
our system works on Arabizi, which involves a
higher degree of ambiguity. However, we use
CODAFY as a black-box module in our prepro-
cessing.
Additionally, there is some work on convert-
ing from dialectal Arabic to MSA, which is sim-
ilar to our work in terms of processing a dialec-
tal input. However, our final output is in EGY
and not MSA. Shaalan et al. (2007) introduced a
rule-based approach to convert EGY to MSA. Al-
Gaphari and Al-Yadoumi (2010) also used a rule-
based method to transform from Sanaani dialect to
MSA. Sawaf (2010), Salloum and Habash (2011)
and Salloum and Habash (2013) used morpholog-
ical analysis and morphosyntactic transformation
rules for processing EGY and Levantine Arabic.
There has been some work on machine translit-
eration by Knight and Graehl (1997). Al-Onaizan
and Knight (2002) introduced an approach for ma-
chine transliteration of Arabic names. Freeman
et al. (2006) also introduced a system for name
matching between English and Arabic, which
Habash (2008) employed as part of generating
English transliterations from Arabic words in the
context of machine translation. This work is sim-
ilar to ours in terms of text transliteration. How-
ever, our work is not restricted to names.
3
http://www.getmaren.com
4
http://www.google.com/ta3reeb
5
http://www.yamli.com/
4 Approach
4.1 Defining the Task
Our task is as follows: for each Arabizi word in
the input, we choose the Arabic script word which
is the correct CODA spelling of the input word
and which carries the intended meaning (as deter-
mined in the context of the entire available text).
We do not merge two or more input words into
a single Arabic script word. If CODA requires
two consecutive input Arabizi words to be merged,
we indicate this by attaching a plus to the end of
the first word. On the other hand, if CODA re-
quires an input Arabizi word to be broken into two
or more Arabic script words, we indicate this by
inserting a dash between the words. We do this
to maintain the bijection between input and out-
put words, i.e., to allow easy tracing of the Arabic
script back to the Arabizi input.
4.2 Transliteration Pipeline
The proposed system in this paper is called 3AR-
RIB.
6
Using the context of an input Arabizi word,
3ARRIB produces the word?s best Arabic script
CODA transliteration. Figure 1 illustrates the dif-
ferent components of 3ARRIB in both the train-
ing and processing phases. We summarize the full
transliteration process as follows. Each Arabizi
sentence input to 3ARRIB goes through a pre-
processing step of lowercasing (de-capitalization),
speech effects handling, and punctuation split-
ting. 3ARRIB then generates a list of all possi-
ble transliterations for each word in the input sen-
tence using a finite-state transducer that is trained
on character-level alignment from Arabizi to Ara-
bic script. We then experiment with different com-
binations of the following two components:
Morphological Analyzer We use CALIMA
(Habash et al., 2012b), a morphological analyzer
for EGY. For each input word, CALIMA provides
all possible morphological analyses, including the
CODA spelling for each analysis. All generated
candidates are passed through CALIMA. If CAL-
IMA has no analysis for a candidate, then that
candidate gets filtered out; otherwise, the CODA
spellings of the analyses from CALIMA become
the new candidates in the rest of the transliteration
pipeline. For some words, CALIMA may sug-
gest multiple CODA spellings that reflect different
analyses of the word.
6
3ARRIB (pronounced /ar-rib/) means ?Arabize!?.
33
FSM 
Candidates
FSM
CALIMA
(+tokenization)
Best 
Selections
LMFST model
SRILM
Arabizi ? Arabic 
Parallel Data
Giza++
Training phase
 Input Arabizi 
Script
FST
Egyptian Corpus
CALIMA 
Output
A* Search
Preprocessing
MADAMIRA
 Output Arabic 
Script
Figure 1: An illustration of the different components of the 3ARRIB system in both the training and
processing phases. FST: finite-state Transducer; LM: Language Model; CALIMA: Morphological Ana-
lyzer for Dialectal Arabic; MADAMIRA: Morphological Tagger for Arabic.
Language Model We disambiguate among the
possibilities for all input words (which consti-
tute a ?sausage? lattice) using an n-gram language
model.
4.3 Preprocessing
We apply the following preprocessing steps to the
input Arabizi text:
? We separate all attached emoticons such as
(:D, :p, etc.) and punctuation from the words.
We only keep the apostrophe because it is
used in Arabizi to distinguish between dif-
ferent sounds. 3ARRIB keeps track of any
word offset change, so that it can reconstruct
the same number of tokens at the end of the
pipeline.
? We tag emoticons and punctuation to protect
them from any change through the pipeline.
? We lowercase all letters.
? We handle speech effects by replacing any
sequence of the same letter whose length is
greater than two by a sequence of exactly
length two; for example, iiiii becomes ii.
4.4 Character-Based Transduction
We use a parallel corpus of Arabizi-Arabic words
to learn a character-based transduction model.
The parallel data consists of two sources. First,
we use 2,200 Arabizi-to-Arabic script pairs from
the training data used by (Darwish, 2013). We
manually revised the Arabic side to be CODA-
compliant. Second, we use about 6,300 pairs
of proper names in Arabic and English from
the Buckwalter Arabic Morphological Analyzer
(Buckwalter, 2004). Since proper names are typ-
ically transliterated, we expect them to be a rich
source for learning transliteration mappings.
The words in the parallel data are turned into
space-separated character tokens, which we align
using Giza++ (Och and Ney, 2003). We then use
the phrase extraction utility in the Moses statistical
machine translation system (Koehn et al., 2007) to
extract a phrase table which operates over char-
acters. The phrase table is then used to build a
finite-state transducer (FST) that maps sequences
of Arabizi characters into sequences of Arabic
script characters. We use the negative logarithmic
conditional probabilities of the Arabizi-to-Arabic
pairs in the phrase tables as costs inside the FST.
We use the FST to transduce an input Arabizi word
to one or more words in Arabic script, where ev-
ery resulting word in Arabic script is given a prob-
abilistic score.
As part of the preprocessing of the parallel data,
we associate all Arabizi letters with their word
location information (beginning, middle and end-
ing letters). This is necessary since some Arabizi
34
mapping phenomena happen only at specific loca-
tions. For example, the Arabizi letter "o" is likely
to be transliterated into

@ ? in Arabic if it appears
at the beginning of the word, but almost never so
if it appears in the middle of the word.
For some special Arabizi cases, we directly
transliterate input words to their correct Arabic
form using a table, without going through the FST.
For example, isa is mapped to ?

<? @ Z A

?
	
?@

?
An ?A?
Allh ?God willing?. There are currently 32 entries
in this table.
4.5 Morphological Analyzer
For every word in the Arabizi input, all the candi-
dates generated by the character-based transduc-
tion are passed through the CALIMA morpholog-
ical analyzer. For every candidate, CALIMA pro-
duces a list of all the possible morphological anal-
yses. The CODA for these analyses need not be
the same. For example, if the output from the char-
acter based transducer is Aly, then CALIMA pro-
duces the following CODA-compliant spellings:
??@

?
Al? ?to?, ?


?@

?
Al? ?to me? and ?


?

@
?
Aly ?automatic?
or ?my family?. All of these CODA spellings are
the output of CALIMA for that particular input
word. The output from CALIMA then becomes
the set of final candidates of the input Arabizi in
the rest of the transliteration pipeline. If a word
is not recognized by CALIMA, it gets filtered out
from the transliteration pipeline. However, if all
the candidates of some word are not recognized
by CALIMA, then we retain them all since there
should be an output for every input word.
We additionally run a tokenization step that
makes use of the generated CALIMA morphologi-
cal analysis. The tokenization scheme we target is
D3, which separates all clitics associated with the
word (Habash, 2010). For every word, we keep
a list of the possible tokenized and untokenized
CODA-compliant pairs. We use the tokenized or
untokenized forms as inputs to either a tokenized
or untokenized language model, respectively, as
described in the next subsection. The untokenized
form is necessary to retain the surface form at the
end of the transliteration process.
Standalone clitics are sometimes found in Ara-
bizi such as lel ragel (which corresponds to
?g
.
@P +?? ll+ rAjl ?for the man?). Since CALIMA
does not handle most standalone clitics, we keep
a lookup table that associates them with their tok-
enization information.
4.6 Language Model
We then use an EGY language model that is
trained on CODA-compliant text. We investi-
gate two options: a language model that has stan-
dard CODA white-space word tokenization con-
ventions (?untokenized?), and a language model
that has a D3 tokenized form of CODA in which
all clitics are separated (?tokenized?). The output
of the morphological analyzer (which is the input
to the LM component) is processed to match the
tokenization used in the LM.
The language models are built from a large
corpus of 392M EGY words.
7
The corpus is
first processed using CODAFY (Eskander et al.,
2013), a system for spontaneous text convention-
alization into CODA. This is necessary so that
our system remains CODA-compliant across the
whole transliteration pipeline. Eskander et al.
(2013) states that the best conventionalization re-
sults are obtained by running the MLE component
of CODAFY followed by an EGY morphological
tagger, MADA-ARZ (Habash et al., 2013). In the
work reported here, we use the newer version of
MADA-ARZ, named MADAMIRA (Pasha et al.,
2014). For the tokenized language model, we run
a D3 tokenization step on top of the processed text
by MADAMIRA. The processed data is used to
build a language model with Kneser-Ney smooth-
ing using the SRILM toolkit (Stolcke, 2002).
We use A* search to pick the best transliteration
for each word given its context. The probability of
any path in the A* search space combines the FST
probability of the words with the probability from
the language model. Thus, for any certain path of
selected Arabic possibilities A
0,i
= {a
0
, a
1
, ...a
i
}
given the corresponding input Arabizi sequence
W
0,i
= {w
0
, w
1
, ...w
i
}, the transliteration prob-
ability can be defined by equation (1).
P (A
0,i
|W
0,i
) =
i?
j=0
(P (a
j
|w
j
) ? P (a
j
|a
j?N+1,j?1
)) (1)
Where, N is the maximum affordable n-
gram length in the LM, P (a
j
|w
j
) is the
FST probability of transliterating the Ara-
bizi word w
j
into the Arabic word a
j
, and
P (a
j
|a
j?N+1,j?1
) is the LM probability of the se-
quence {a
j?N+1
, a
j?N+2
, ...a
j
}.
7
All of the resources we use are available from the Lin-
guistic Data Consortium: www.ldc.upenn.edu.
35
5 Experiments and Results
5.1 Data
We use two in-house data sets for development
(Dev; 502 words) and blind testing (Test; 1004
words). The data contains EGY Arabizi SMS
conversations that are mapped to Arabic script in
CODA by a CODA-trained EGY native speaker.
5.2 Experiments
We conducted a suite of experiments to evaluate
the performance of our approach and identify op-
timal settings on the Dev set. The optimal result
and the baseline are then applied to the blind Test
set. During development, the following settings
were explored:
? INV-Selection: The training data of the finite
state transducer is used to generate the list of
possibilities for each input Arabizi word. If
the input word cannot be found in the FST
training data, the word is kept in Arabizi.
? FST-ONLY: Pick the top choice from the list
generated by the finite state transducer.
? FST-CALIMA: Pick the top choice from the
list after the CALIMA filtering.
? FST-CALIMA-Tokenized-LM-5: Run the
full pipeline of 3ARRIB with a 5-gram to-
kenized LM.
8
? FST-CALIMA-Tokenized-LM-5-MLE:
The same as FST-CALIMA-Tokenized-
LM-5, but for an Arabizi word that appears
in training, force its most frequently seen
mapping directly instead of running the
transliteration pipeline for that word.
? FST-CALIMA-Untokenized-LM-5: Run
the full pipeline of 3ARRIB with a 5-gram
untokenized LM.
? FST-Untokenized-LM-5: Run the full
pipeline of 3ARRIB minus the CALIMA fil-
tering with a 5-gram untokenized LM. This
setup is analogous to the transliteration ap-
proach proposed by (Darwish, 2013). Thus
we use it as our baseline.
Each of the above experiments is evaluated
with exact match, and with Alif/Ya normalization
(El Kholy and Habash, 2010; Habash, 2010).
8
3, 5, and 7-gram LMs have been tested. The 3 and 5-
gram LMs give the same performance while the 7-gram LM
is the worst.
5.3 Results
Table 2 summarizes the results on the Dev set.
Our best performing setup is FST-CALIMA-
Tokenized-LM-5 which has 77.5% accuracy and
79.1% accuracy with normalization. The baseline
system, FST-Untokenized-LM-5, gives 74.1% ac-
curacy and 74.9 % accuracy with normalization.
This highlights the value of morphological filter-
ing as well as sparsity-reducing tokenization.
Table 3 shows how we do (best system and best
baseline) on a blind Test set. Although the accu-
racy drops overall, the gap between the best sys-
tem and the baseline increases.
5.4 Error Analysis
We conducted two error analyses for the best per-
forming transliteration setting on the Dev set. We
first analyze in which component the Dev set er-
rors occur. About 29% of the errors are cases
where the FST does not generate the correct an-
swer. An additional 15% of the errors happen be-
cause the correct answer is not covered by CAL-
IMA. The language model does not include the
correct answer in an additional 8% of the errors.
The rest of the errors (48%) are cases where the
correct answer is available in all components but
does not get selected.
Motivated by the value of Arabizi transliteration
for machine translation into English, we distin-
guish between two types of words: words that re-
main the same when translated into English, such
as English words, proper nouns, laughs, emoti-
cons, punctuations and digits (EN-SET) versus
EGY-only words (EGY-SET). Examples of words
in EN-SET are: love you very much (code switch-
ing), Peter (proper noun), haha (laugh), :D (emoti-
con), ! (punctuation) and 123 (digits).
While the overall performance of our best set-
tings is 77.5%, the accuracy of the EGY-SET by
itself is 84.6% as opposed to 46.2% for EN-SET.
This large difference reflects the fact that we do
not target English word transliteration into Arabic
script explicitly.
We now perform a second error analysis only on
the errors in the EGY-SET, in which we categorize
the errors by their linguistic type. About 25% of
the errors are non-CODA-compliant system out-
put, where the answer is a plausible non-CODA
form, i.e., a form that may be written or read eas-
ily by a native speaker who is not aware of CODA.
For example, the system generates the non-CODA
36
System Exact-Matching A/Y-normalization
INV-Selection 37.1 40.6
FST-ONLY (pick top choice) 63.1 65.1
FST-CALIMA (pick top choice) 66.1 68.9
FST-CALIMA-Tokenized-LM-5 77.5 79.1
FST-CALIMA-Tokenized-LM-5-MLE 68.7 73.5
FST-CALIMA-Untokenized-LM-5 77.3 78.9
FST-Untokenized-LM-5 74.1 74.9
Table 2: Results on the Dev set in terms of accuracy (%).
System Exact-Matching A/Y-normalization
FST-CALIMA-Tokenized-LM-5 69.4 73.9
FST-Untokenized-LM-5 63.1 65.4
Table 3: Results on the blind Test set in terms of accuracy (%).
form

??
	
?
	
JJ


? mynf?? instead of the correct CODA
form

??
	
?
	
JK


A? mA ynf?? ?it doesn?t work?. Ignor-
ing the CODA-related errors increases the overall
accuracy by about 3.0% to become 80.5%. The ac-
curacy of the EGY-SET rises to 88.3% as opposed
to 84.6% when considering CODA compliance.
Ambiguous Arabizi input contributes to an ad-
ditional 27% of the errors, where the system as-
signs a plausible answer that is incorrect in con-
text. For example, the word matar in the input
Arabizi fel matar ?at the airport? has two plausi-
ble out-of-context solutions: PA?? mTAr ?airport?
(contextually correct) and Q?? mTr ?rain? (contex-
tually incorrect).
In about 2% of the errors, the Arabizi input con-
tains a typo making it impossible to produce the
gold reference. For example, the input Arabizi
ba7bet contains a typo where the final t should turn
into k, so that it means ?J
.
kAK
.
bAHbk ?I love you
[2fs]?.
In the rest of the errors (about 46%), the sys-
tem fails to come up with the correct answer. In-
stead, it assigns a completely different word or
even an impossible word. For example, the cor-
rect answer for the input Arabizi sora ?picture? is

?P?? Swrh?, while the system produces the word
P?? swr ?wall?. Another example is the input Ara-
bizi talabt ?I asked for?, where the output from the
system is

?J
.
? A? TAlbh? ?student?, while the correct
answer is

IJ
.
?? tlbt ?I asked for, ordered? instead.
6 Conclusion and Future Work
We presented a method for converting dialectal
Arabic (specifically, EGY) written in Arabizi to
Arabic script following the CODA convention for
DA orthography. We achieve a 17% error reduc-
tion over our implementation of a previously pub-
lished work (Darwish, 2013) on a blind test set.
In the future, we plan to improve several aspects
of our models, particularly FST character map-
ping, the morphological analyzer coverage, and
language models. We also plan to work on the
problem of automatic identification of non-Arabic
words. We will extend the system to work on other
Arabic dialects. We also plan to make the 3AR-
RIB system publicly available.
Acknowledgement
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-
0014. Any opinions, findings and conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reflect the
views of DARPA.
References
G. Al-Gaphari and M. Al-Yadoumi. 2010. A method
to convert Sana?ani accent to Modern Standard Ara-
bic. International Journal of Information Science
and Management, pages 39?49.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 Workshop on Computational Ap-
proaches to Semitic Languages.
37
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Achraf Chalabi and Hany Gerges. 2012. Romanized
Arabic Transliteration. In Proceedings of the Sec-
ond Workshop on Advances in Text Input Methods
(WTIM 2012).
Kareem Darwish. 2013. Arabizi Detection and Con-
version to Arabic. CoRR.
Ahmed El Kholy and Nizar Habash. 2010. Techniques
for Arabic Morphological Detokenization and Or-
thographic Denormalization. In Proceedings of the
seventh International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing Spontaneous Or-
thography. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), Atlanta, GA.
Noura Farra, Nadi Tomeh, Alla Rozovskaya, and
Nizar Habash. 2014. Generalized Character-Level
Spelling Error Correction. In Proceedings of the
Conference of the Association for Computational
Linguistics (ACL), Baltimore, Maryland, USA.
Andrew Freeman, Sherri Condon, and Christopher
Ackerman. 2006. Cross linguistic name matching
in English and Arabic. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 471?478, New York City,
USA.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalu-
ation Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In Proceedings of the Twelfth Meeting of the
Special Interest Group on Computational Morphol-
ogy and Phonology, pages 1?9, Montr?al, Canada.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-08: HLT, Short Papers, pages 57?
60, Columbus, Ohio.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of the European
chapter of the Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Association for Computational
Linguistics, Prague, Czech Republic.
Franz Joseph Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan M. Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
Wael Salloum and Nizar Habash. 2011. Dialectal to
Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Pro-
ceedings of the First Workshop on Algorithms and
Resources for Modelling of Dialects and Language
Varieties, pages 10?21, Edinburgh, Scotland.
Wael Salloum and Nizar Habash. 2013. Dialectal
Arabic to English Machine Translation: Pivoting
through Modern Standard Arabic. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT), At-
lanta, GA.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in
the Americas (AMTA), Denver, Colorado.
Khaled Shaalan, Hitham Abo Bakr, and Ibrahim
Ziedan. 2007. Transferring Egyptian Colloquial
into Modern Standard Arabic. In International Con-
ference on Recent Advances in Natural Language
Processing (RANLP), Borovets, Bulgaria.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Ines Zribi, Rahma Boujelbane, Abir Masmoudi,
Mariem Ellouze, Lamia Belguith, and Nizar Habash.
2014. A Conventional Orthography for Tunisian
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
38
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 93?103,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Transliteration of Arabizi into Arabic Orthography: Developing a 
Parallel Annotated Arabizi-Arabic Script SMS/Chat Corpus 
 
Ann Bies, Zhiyi Song, Mohamed Maamouri, Stephen Grimes, Haejoong Lee,  
Jonathan Wright, Stephanie Strassel, Nizar Habash?, Ramy Eskander?, Owen Rambow? 
Linguistic Data Consortium, University of Pennsylvania 
{bies,zhiyi,maamouri,sgrimes,haejoong, 
jdwright,strassel}@ldc.upenn.edu 
?Computer Science Department, New York University Abu Dhabi 
?
nizar.habash@nyu.edu 
?Center for Computational Learning Systems, Columbia University 
?
{reskander,rambow}@ccls.columbia.edu 
 
  
 
Abstract 
This paper describes the process of creating a 
novel resource, a parallel Arabizi-Arabic 
script corpus of SMS/Chat data.  The lan-
guage used in social media expresses many 
differences from other written genres: its vo-
cabulary is informal with intentional devia-
tions from standard orthography such as re-
peated letters for emphasis; typos and non-
standard abbreviations are common; and non-
linguistic content is written out, such as 
laughter, sound representations, and emoti-
cons.  This situation is exacerbated in the 
case of Arabic social media for two reasons.  
First, Arabic dialects, commonly used in so-
cial media, are quite different from Modern 
Standard Arabic phonologically, morphologi-
cally and lexically, and most importantly, 
they lack standard orthographies. Second, 
Arabic speakers in social media as well as 
discussion forums, SMS messaging and 
online chat often use a non-standard romani-
zation called Arabizi.  In the context of natu-
ral language processing of social media Ara-
bic, transliterating from Arabizi of various 
dialects to Arabic script is a necessary step, 
since many of the existing state-of-the-art re-
sources for Arabic dialect processing expect 
Arabic script input.  The corpus described in 
this paper is expected to support Arabic NLP 
by providing this resource. 
1 Introduction 
The language used in social media expresses 
many differences from other written genres: its 
vocabulary is informal with intentional devia-
tions from standard orthography such as repeated 
letters for emphasis; typos and non-standard ab-
breviations are common; and non-linguistic con-
tent is written out, such as laughter, sound repre-
sentations, and emoticons. 
This situation is exacerbated in the case of Ar-
abic social media for two reasons.  First, Arabic 
dialects, commonly used in social media, are 
quite different from Modern Standard Arabic 
(MSA) phonologically, morphologically and lex-
ically, and most importantly, they lack standard 
orthographies (Maamouri et.al. 2014). Second, 
Arabic speakers in social media as well as dis-
cussion forums, Short Messaging System (SMS) 
text messaging and online chat often use a non-
standard romanization called ?Arabizi? (Dar-
wish, 2013).  Social media communication in 
Arabic takes place using a variety of orthogra-
phies and writing systems, including Arabic 
script, Arabizi, and a mixture of the two.  Alt-
hough not all social media communication uses 
Arabizi, the use of Arabizi is prevalent enough to 
pose a challenge for Arabic NLP research. 
In the context of natural language processing 
of social media Arabic, transliterating from 
Arabizi of various dialects to Arabic script is a 
necessary step, since many of the existing state-
of-the-art resources for Arabic dialect processing 
and annotation expect Arabic script input (e.g., 
Salloum and Habash, 2011; Habash et al. 2012c; 
Pasha et al., 2014). 
To our knowledge, there are no naturally oc-
curring parallel texts of Arabizi and Arabic 
script.  In this paper, we describe the process of 
creating such a novel resource at the Linguistic 
Data Consortium (LDC).  We believe this corpus 
will be essential for developing robust tools for 
converting Arabizi into Arabic script. 
93
The rest of this paper describes the collection 
of Egyptian SMS and Chat data and the creation 
of a parallel text corpus of Arabizi and Arabic 
script for the DARPA BOLT program.1  After 
reviewing the history and features in Arabizi 
(Section 2) and related work on Arabizi (Section 
3), in Section 4, we describe our approach to col-
lecting the Egyptian SMS and Chat data and the 
annotation and transliteration methodology of the 
Arabizi SMS and Chat into Arabic script, while 
in Section 5, we discuss the annotation results, 
along with issues and challenges we encountered 
in annotation. 
2 Arabizi and Egyptian Arabic Dialect 
2.1 What is Arabizi? 
Arabizi is a non-standard romanization of Arabic 
script that is widely adopted for communication 
over the Internet (World Wide Web, email) or 
for sending messages (instant messaging and 
mobile phone text messaging) when the actual 
Arabic script alphabet is either unavailable for 
technical reasons or otherwise more difficult to 
use.  The use of Arabizi is attributed to different 
reasons, from lack of good input methods on 
some mobile devices to writers? unfamiliarity 
with Arabic keyboard.  In some cases, writing in 
Arabizi makes it easier to code switch to English 
or French, which is something educated Arabic 
speakers often do.  Arabizi is used by speakers of 
a variety of Arabic dialects. 
Because of the informal nature of this system, 
there is no single ?correct? encoding, so some 
character usage overlaps.  Most of the encoding 
in the system makes use of the Latin character 
(as used in English and French) that best approx-
imates phonetically the Arabic letter that one 
wants to express (for example, either b or p cor-
responds to ?).  This may sometimes vary due to 
regional variations in the pronunciation of the 
Arabic letter (e.g., j is used to represent ? in the 
Levantine dialect, while in Egyptian dialect g is 
used) or due to differences in the most common 
non-Arabic second language (e.g., sh corre-
sponds to ? in the previously English dominated 
Middle East Arab countries, while ch shows a 
predominantly French influence as found in 
North Africa and Lebanon).  Those letters that do 
not have a close phonetic approximate in the Lat-
in script are often expressed using numerals or 
other characters, so that the numeral graphically 
                                                 
1 http://www.darpa.mil/Our_Work/I2O/Programs/Broad_Op 
erational_Language_Translation_%28BOLT%29.aspx 
approximates the Arabic letter that one wants to 
express (e.g., the numeral 3 represents ? because 
it looks like a mirror reflection of the letter). 
Due to the use of Latin characters and also 
frequent code switching in social media Arabizi, 
it can be difficult to distinguish between Arabic 
words written in Arabizi and entirely unrelated 
foreign language words (Darwish 2013).  For 
example, mesh can be the English word, or 
Arabizi for ?? ?not?.  However, in context these 
cases can be clearly labeled as either Arabic or a 
foreign word.  An additional complication is that 
many words of foreign origin have become Ara-
bic words (?borrowings?).  Examples include 
banadoora ?????? ?tomato? and mobile ?????? 
?mobile phone?.  It is a well-known practical and 
theoretical problem to distinguish borrowings 
(foreign words that have become part of a lan-
guage and are incorporated fully into the mor-
phological and syntactic system of the host lan-
guage) from actual code switching (a bilingual 
writer switches entirely to a different language, 
even if for only a single word).  Code switching 
is easy to identify if we find an extended passage 
in the foreign language which respects that lan-
guage?s syntax and morphology, such as Bas eh 
ra2yak I have the mask.  The problem arises 
when single foreign words appear without Arabic 
morphological marking: it is unclear if the writer 
switched to the foreign language for one word or 
whether he or she simply is using an Arabic 
word of foreign origin.  In the case of banadoora 
?????? ?tomato?, there is little doubt that this has 
become a fully Arabic word and the writer is not 
code switching into Italian; this is also signaled 
by the fact that a likely Arabizi spelling (such as 
banadoora) is not in fact the Italian orthography 
(pomodoro).  However, the case is less clear cut 
with mobile ?????? ?mobile phone?: even if it is a 
borrowing (clearly much more recent than bana-
doora ?????? ?tomato?), a writer will likely spell 
the word with the English orthography as mobile 
rather than write, say, mubail.  More research is 
needed on this issue.  However, because of the 
difficulty of establishing the difference between 
code switching and borrowing, we do not attempt 
to make this distinction in this annotation 
scheme. 
2.2 Egyptian Arabic Dialect 
Arabizi is used to write in multiple dialects of 
Arabic, and differences between the dialects 
themselves have an effect on the spellings cho-
sen by individual writers using Arabizi.  Because 
Egyptian Arabic is the dialect of the corpus cre-
94
ated for this project, we will briefly discuss some 
of the most relevant features of Egyptian Arabic 
with respect to Arabizi transliteration.  For a 
more extended discussion of the differences be-
tween MSA and Egyptian Arabic, see Habash et 
al. (2012a) and Maamouri et al. (2014). 
Phonologically, Egyptian Arabic is character-
ized by the following features, compared with 
MSA: 
(a) The loss of the interdentals /?/ and /?/ 
which are replaced by /d/ or /z/ and /t/ or /s/ 
respectively, thus giving those two original 
consonants a heavier load. Examples in-
clude  ??? /zakar/ ?to mention?, ???  /daba?/ 
?to slaughter?,  ???  /talg/ ?ice?,  ???  /taman/ 
?price?, and  ???  /sibit/ ?to stay in place, 
become immobile?. 
(b) The exclusion of /q/ and /?/ from the conso-
nantal system, being replaced by the /?/ and 
/g/, e.g., ???  /?u?n/ ?cotton?, and  ???  
/gamal/ ?camel?. 
At the level of morphology and syntax, the 
structures of Egyptian Arabic closely resemble 
the overall structures of MSA with relatively mi-
nor differences to speak of.  Finally, the Egyptian 
Arabic lexicon shows some significant elements 
of semantic differentiation. 
The most important morphological difference 
between Egyptian Arabic and MSA is in the use 
of some Egyptian clitics and affixes that do not 
exist in MSA.  For instance, Egyptian Arabic has 
the future proclitics h+ and ?+ as opposed to the 
standard equivalent s+. 
Lexically, there are lexical differences be-
tween Egyptian Arabic and MSA where no ety-
mological connection or no cognate spelling is 
available.  For example, the Egyptian Arabic ??  
/bu??/ ?look? is ???? /?unZur/ in MSA. 
3 Related Work 
Arabizi-Arabic Script Transliteration  Previ-
ous efforts on automatic transliterations from 
Arabizi to Arabic script include work by Chalabi 
and Gerges (2012), Darwish (2013) and Al-
Badrashiny et al. (2014).  All of these approaches 
rely on a model for character-to-character map-
ping that is used to generate a lattice of multiple 
alternative words which are then selected among 
using a language model.  The training data used 
by Darwish (2013) is publicly available but it is 
quite limited (2,200 word pairs).  The work we 
are describing here can help substantially im-
prove the quality of such system.  We use the 
system of Al-Badrashiny et al. (2014) in this pa-
per as part of the automatic transliteration step 
because they target the same conventional or-
thography of dialectal Arabic (CODA) (Habash 
et al., 2012a, 2012b), which we also target.  
There are several commercial products that con-
vert Arabizi to Arabic script, namely: Microsoft 
Maren, 2  Google Ta3reeb, 3  Basis Arabic chat 
translator4 and Yamli.5  Since these products are 
for commercial purposes, there is little infor-
mation available about their approaches, and 
whatever resources they use are not publicly 
available for research purposes.  Furthermore, as 
Al-Badrashiny et al. (2014) point out, Maren, 
Ta3reeb and Yamli are primarily intended as in-
put method support, not full text transliteration.  
As a result, their users? goal is to produce Arabic 
script text not Arabizi text, which affects the 
form of the romanization they utilize as an in-
termediate step.  The differences between such 
?functional romanization? and real Arabizi in-
clude that the users of these systems will use less 
or no code switching to English, and may em-
ploy character sequences that help them arrive at 
the target Arabic script form faster, which other-
wise they would not write if they were targeting 
Arabizi (Al-Badrashiny et al., 2014). 
Name Transliteration  There has been some 
work on machine transliteration by Knight and 
Graehl (1997).  Al-Onaizan and Knight (2002) 
introduced an approach for machine translitera-
tion of Arabic names. Freeman et al. (2006) also 
introduced a system for name matching between 
English and Arabic.  Although the general goal 
of transliterating from one script to another is 
shared between these efforts and ours, we are 
considering a more general form of the problem 
in that we do not restrict ourselves to names. 
Code Switching  There is some work on code 
switching between Modern Standard Arabic 
(MSA) and dialectal Arabic (DA).  Zaidan and 
Callison-Burch (2011) were interested in this 
problem at the inter-sentence level.  They 
crawled a large dataset of MSA-DA news com-
mentaries, and used Amazon Mechanical Turk to 
annotate the dataset at the sentence level.  
Elfardy et al. (2013) presented a system, AIDA, 
that tags each word in a sentence as either DA or 
MSA based on the context.  Lui et al. (2014) 
proposed a system for language identification in 
                                                 
2 http://www.getmaren.com 
3 http://www.google.com/ta3reeb 
4 http://www.basistech.com/arabic-chat-translator-
transforms-social-media-analysis/ 
5 http://www.yamli.com/ 
95
multilingual documents using a generative mix-
ture model that is based on supervised topic 
modeling algorithms.  Darwish (2013) and Voss 
et al. (2014) deal with exactly the problem of 
classifying tokens in Arabizi as Arabic or not.  
More specifically, Voss et al. (2014) deal with 
Moroccan Arabic, and with both French and 
English, meaning they do a three-way classifica-
tion.  Darwish (2013)'s data is more focused on 
Egyptian and Levantine Arabic and code switch-
ing with English. 
Processing Social Media Text  Finally, while 
English NLP for social media has attracted con-
siderable attention recently (Clark and Araki, 
2011; Gimpel et al., 2011; Gouws et al., 2011; 
Ritter et al., 2011; Derczynski et al., 2013), there 
has not been much work on Arabic yet.  Darwish 
et al. (2012) discuss NLP problems in retrieving 
Arabic microblogs (tweets).  They discuss many 
of the same issues we do, notably the problems 
arising from the use of dialectal Arabic such as 
the lack of a standard orthography.  Eskander et 
al. (2013) described a method for normalizing 
spontaneous orthography into CODA. 
4 Corpus Creation 
This work was prepared as part of the DARPA 
Broad Operational Language Translation 
(BOLT) program which aims at developing tech-
nology that enables English speakers to retrieve 
and understand information from informal for-
eign language sources including chat, text mes-
saging and spoken conversations. LDC collects 
and annotates informal linguistic data of English, 
Chinese and Arabic, with Egyptian Arabic being 
the representative of the Arabic language family.  
 
 
Egyptian Arabic has the advantage over all other 
dialects of Arabic of being the language of the 
largest linguistic community in the Arab region, 
and also of having a rich level of internet com-
munication.  
4.1 SMS and Chat Collection 
In BOLT Phase 2, LDC collected large volumes 
of naturally occurring informal text (SMS) and 
chat messages from individual users in English, 
Chinese and Egyptian Arabic (Song et al., 2014).  
Altogether we recruited 46 Egyptian Arabic par-
ticipants, and of those 26 contributed data.  To 
protect privacy, participation was completely 
anonymous, and demographic information was 
not collected.  Participants completed a brief lan-
guage test to verify that they were native Egyp-
tian Arabic speakers.  On average, each partici-
pant contributed 48K words.  The Egyptian Ara-
bic SMS and Chat collection consisted of 2,140 
conversations in a total of 475K words after 
manual auditing by native speakers of Egyptian 
Arabic to exclude inappropriate messages and 
messages that were not Egyptian Arabic.  96% of 
the collection came from the personal SMS or 
Chat archives of participants, while 4% was col-
lected through LDC?s platform, which paired 
participants and captured their live text messag-
ing (Song et al., 2014).  A subset of the collec-
tion was then partitioned into training and eval 
datasets.   
Table 1 shows the distribution of Arabic script 
vs. Arabizi in the training dataset.  The conversa-
tions that contain Arabizi were then further anno-
tated and transliterated to create the Arabizi-
Arabic script parallel corpus, which consists of 
 
 
 Total Arabic 
script only 
Arabizi 
only 
Mix of Arabizi and Arabic script 
Arabizi Arabic script 
Conversations 1,503 233 987 283 
Messages 101,292 18,757 74,820 3,237 4,478 
Sentence units 94,010 17,448 69,639 3,017 3,906 
Words 408,485 80,785 293,900 10,244 23,556 
Table 1. Arabic SMS and Chat Training Dataset 
 
1270 conversations. 6   All conversations in the 
training dataset were also translated into English 
to provide Arabic-English parallel training data. 
                                                 
6 In order to form single, coherent units (Sentence units) of 
an appropriate size for downstream annotation tasks using 
this data, messages that were split mid-sentence (often mid-
Not surprisingly, most Egyptian conversations 
in our collection contain at least some Arabizi; 
                                                                          
word) due to SMS messaging character limits were rejoined, 
and very long messages (especially common in chat) were 
split into two or more units, usually no longer than 3-4 sen-
tences. 
96
only 15% of conversations are entirely written in 
Arabic script, while 66% are entirely Arabizi.  
The remaining 19% contain a mixture of the two 
at the conversation level.  Most of the mixed 
conversations were mixed in the sense that one 
side of the conversation was in Arabizi and the 
other side was in Arabic script, or in the sense 
that at least one of the sides switched between 
the two forms in mid-conversation.  Only rarely 
are individual messages in mixed scripts.  The 
annotation for this project was performed on the 
Arabizi tokens only.  Arabic script tokens were 
not touched and were kept in their original 
forms.  
The use of Arabizi is predominant in the SMS 
and Chat Egyptian collection, in addition to the 
presence of other typical cross-linguistic text ef-
fects in social media data.  For example, the use 
of emoticons and emoji is frequent.  We also ob-
served the frequent use of written out representa-
tions of speech effects, including representations 
of laughter (e.g., hahaha), filled pauses (e.g., 
um), and other sounds (e.g., hmmm).  When these 
representations are written in Arabizi, many of 
them are indistinguishable from the same repre-
sentations in English SMS data.  Neologisms are 
also frequently part of SMS/Chat in Egyptian  
 
Arabic, as they are in other languages.  English 
words use Arabic morphology or determiners, as 
in el anniversary ?the anniversary?.  Sometimes 
English words are spelled in a way that is closer 
phonetically to the way an Egyptian speaker 
would pronounce them, for example lozar for 
?loser?, or beace for ?peace?. 
The adoption of Arabizi for SMS and online 
chat may also go some way to explaining the 
high frequency of code mixing in the Egyptian 
Arabic collection.  While the auditing process 
eliminated messages that were entirely in a non-
target language, many of the acceptable messag-
es contain a mixture of Egyptian Arabic and 
English. 
4.2 Annotation Methodology 
All of the Arabizi conversations, including the 
conversations containing mixtures of Arabizi and 
Arabic script were then annotated and translit-
erated: 
1. Annotation on the Arabizi source text to 
flag certain features 
2. Correction and normalization of the trans-
literation according to CODA conventions 
 
 
 
Figure 1. Arabizi Annotation and Transliteration Tool 
 
The annotators were presented with the source 
conversations in their original Arabizi form as 
well as the transliteration output from an auto-
matic Arabization system, and used a web-based 
tool developed by LDC (see Figure 1) to perform 
the two annotation tasks, which allowed annota-
tors perform both annotation and transliteration 
token by token, sentence by sentence and review 
the corrected transliteration in full context.  The 
GUI shows the full conversation in both the orig-
inal Arabizi and the resulting Arabic script trans-
literation for each sentence.  Annotators must 
97
annotate each sentence in order, and the annota-
tion is displayed in three columns.  The first col-
umn shows the annotation of flag features on the 
source tokens, the second column is the working 
panel where annotators correct the automatic 
transliteration and retokenize, and the third col-
umn displays the final corrected and retokenized 
result. 
Annotation was performed according to anno-
tation guidelines developed at the Linguistic Da-
ta Consortium specifically for this task (LDC, 
2014). 
4.3 Automatic Transliteration 
To speed up the annotation process, we utilized 
an automatic Arabizi-to-Arabic script translitera-
tion system (Al-Badrashiny et al., 2014) which 
was developed using a small vocabulary of 2,200 
words from Darwish (2013) and an additional 
6,300 Arabic-English proper name pairs (Buck-
walter, 2004).  The system has an accuracy of 
69.4%.  We estimate that using this still allowed 
us to cut down the amount of time needed to type 
in the Arabic script version of the Arabizi by 
two-thirds.  This system did not identify Foreign 
words or Names and transliterated all of the 
words.  In one quarter of the errors, the provided 
answer was plausible but not CODA-compliant 
(Al-Badrashiny et al., 2014). 
4.4 Annotation on Arabizi Source Text to 
Flag Features 
This annotation was performed only on sentences 
containing Arabizi words, with the goal of tag-
ging any words in the source Arabizi sentences 
that would be kept the same in the output of an 
English translation with the following flags: 
 
? Punctuation (not including emoticons) 
o Eh ?!//Punct  
o Ma32ula ?!//Punct 
o Ebsty ?//Punct  
 
? Sound effects, such as laughs (?haha? or 
variations), filled pauses, and other sounds 
(?mmmm? or ?shh? or ?um? etc.) 
o hahhhahhah//Sound akeed 3arfa :p da 
enty t3rafy ablia :pp 
o Hahahahaahha//Sound Tb ana ta7t fel 
ahwaa 
o Wala Ana haha//Sound 
o Mmmm//Sound okay 
 
? Foreign language words and numbers.  All 
cases of code switching and all cases of bor-
rowings which are rendered in Arabizi us-
ing standard English orthography are 
marked as ?Foreign?. 
o ana kont mt25er fe t2demm l pro-
jects//Foreign 
o oltilik okay//Foreign ya Babyy//Foreign 
balashhabal!!!! 
o zakrty ll sat//Foreign 
o Bat3at el whatsapp//Foreign 
o La la la merci//Foreign gedan bs la2 
o We 9//Foreign galaeeb dandash lel ban-
at 
 
? Names, mainly person names 
o Youmna//Name 7atigi?? 
 
4.5 Correction and Normalization of the 
Transliteration According to CODA 
Conventions 
The goal of this task was to correct all spelling in 
the Arabic script transliteration to CODA stand-
ards (Habash et al., 2012a, 2012b).  This meant 
that annotators were required to confirm both (1) 
that the word was transliterated into Arabic script 
correctly and also (2) that the transliterated word 
conformed to CODA standards.  The automatic 
transliteration was provided to the annotators, 
and manually corrected by annotators as needed. 
Correcting spelling to a single standard (CO-
DA), however, necessarily included some degree 
of normalization of the orthography, as the anno-
tators had to correct from a variety of dialect 
spellings to a single CODA-compliant spelling 
for each word.  Because the goal was to reach a 
consistent representation of each word, ortho-
graphic normalization was almost the inevitable 
effect of correcting the automatic transliteration.  
This consistent representation will allow down-
stream annotation tasks to take better advantage 
of the SMS/Chat data.  For example, more con-
sistent spelling of Egyptian Arabic words will 
lead to better coverage from the CALIMA mor-
phological analyzer and therefore improve the 
manual annotation task for morphological anno-
tation, as in Maamouri et al. (2014). 
 
Modern Standard Arabic (MSA) cognates and 
Egyptian Arabic sound changes 
Annotators were instructed to use MSA or-
thography if the word was a cognate of an MSA 
98
root, including for those consonants that have 
undergone sound changes in Egyptian Arabic.7 
? use mqfwl ?????  and not ma>fwl ?????  for 
?locked? 
? use HAfZ ???? and not HAfz ???? for the 
name (a proper noun)  
 
Long vowels 
Annotators were instructed to reinstate miss-
ing long vowels, even when they were written as 
short vowels in the Arabizi source, and to correct 
long vowels if they were included incorrectly. 
? use sAEap ???? and not saEap  ???  for 
?hour? 
? use qAlt   ????  and not qlt ??? for ?(she) 
said? 
 
Consonantal ambiguities 
Many consonants are ambiguous when written 
in Arabizi, and many of the same consonants are 
also difficult for the automatic transliteration 
script.  Annotators were instructed to correct any 
errors of this type.   
? S vs. s/ ?  vs. ? 
o use SAyg ????  and not  sAyg  ????  for 
?jeweler? 
? D vs. Z/ ?  vs. ? 
o use DAbT ????  and not  ZAbT ???? for 
?officer? 
o use Zlmp  ????  and not Dlmp  ????  for 
?darkness? 
? Dotted ya vs. Alif Maqsura/ ? vs. ?.  Alt-
hough the dotted ya/ ? and Alif Maqsura/ ? 
are often used interchangeably in Egyptian 
Arabic writing conventions, it was neces-
sary to make the distinction between the 
two for this task. 
o use Ely ???  and not ElY  ???  for ?Ali? 
(the proper name)  
? Taa marbouta.  In Arabizi and so also in the 
Arabic script transliteration, the taa mar-
bouta/ ? may be written for both nominal fi-
nal -h/ ? and verbal final -t/ ?, but for dif-
ferent reasons. 
o mdrsp Ely  ???  ?????  ?Ali?s school? 
o mdrsth  ??????  ?his school? 
 
Morphological ambiguities 
Spelling variation and informal usage can 
combine to create morphological ambiguities as 
well.  For example, the third person masculine 
                                                 
7 Both Arabic script and the Buckwalter transliteration 
(http://www.qamus.org/transliteration.htm) are shown for 
the transliterated examples in this paper. 
singular pronoun and the third person plural ver-
bal suffix can be ambiguous in informal texts.  
For example: 
? use byHbwA bED  ???  ?????? and not byHbh 
bED  ???  ?????  for ?(They) loved each oth-
er? 
? use byEmlwA  ???????  and not byEmlh  ??????  
for ?(They) did? or ?(They) worked? 
In addition, because final -h is sometimes re-
placed in speech by final /-uw/, it was occasion-
ally necessary to correct cases of overuse of the 
third person plural verbal suffix (-wA) to the 
pronoun -h as well. 
 
Merging and splitting tokens written with in-
correct word boundaries 
Annotators were instructed to correct any 
word that was incorrectly segmented.  The anno-
tation tool allowed both the merging and splitting 
of tokens. 
Clitics were corrected to be attached when 
necessary according to (MSA) standard writing 
conventions.  These include single letter proclit-
ics (both verbal and nominal) and the negation 
suffix -$, as well as pronominal clitics such as 
possessive pronouns and direct object pronouns.  
For example, 
? use fAlbyt  ??????    and not  
fAl  byt  ???  ??? or  flbyt  ?????   for ?in the 
house? 
? use EAlsTH ?????? and not  
EAl sTH ??? ??? or ElsTH ????? for ?on the 
roof? 
The conjunction w- / -? is always attached to 
its following word. 
? use wkAn  ????  and not w kAn  ??? ?    for 
?and was? 
? use wrAHt  ????? and not w  rAHt ????  ?  
for ?and (she) left? 
Words that were incorrectly segmented in the 
Arabizi source were also merged.  For example, 
? use msHwrp ?????? and not  
ms Hwrp ???? ??  for ?bewitched 
(fem.sing.)? 
? use $ErhA ????? and not $Er hA  ?? ???   for 
?her hair? 
Particles that are not attached in standard 
MSA written forms were corrected as necessary 
by the splitting function of the tool.  For exam-
ple,  
? use yA Emry  ???? ?? and not yAEmry  
??????  for ?Hey, dear!? 
? use lA trwH  ????  ? and not lAtrwH  ?????  
for ?Do not go? 
99
 
Abbreviations in Arabizi 
Three abbreviations in Arabizi received spe-
cial treatment: msa, isa, 7ma.  These three abbre-
viations only were expanded out to their full 
form using Arabic words in the corrected Arabic 
script transliteration. 
? msa: use mA $A' All~h  ? ???  ?? for ?As 
God wills? 
? isa: use <n $A' All~h  ? ???  ?? for ?God 
willing? 
? 7ma: use AlHmd ll~h for     ??? ??  ?Thank 
God, Praised be the Lord? 
All other Arabic abbreviations were not ex-
panded, and were transliterated simply letter for 
letter.  When the abbreviation was in English or 
another foreign language, it was kept as is in the 
transliteration, using both consonants and semi-
vowels to represent it. 
? use Awkyh  ????  for ?OK? (note that this is 
an abbreviation in English, but not in Egyp-
tian Arabic) 
 
Correcting Arabic typos 
Annotators were instructed to correct typos in 
the transliterated Arabic words, including typos 
in proper names.  However, typos and non-
standard spellings in the transliteration of a for-
eign words were kept as is and not corrected. 
? Ramafan  ?????  should be corrected to 
rmDAn  ?????  for ?Ramadan? 
? babyy  ????  since it is the English word ?ba-
by? it should not be corrected 
 
Flagged tokens in the correction task 
Tokens flagged during task 1 as Sound and 
Foreign were transliterated into Arabic script but 
were not corrected during task 2.  Note that even 
when a whole phrase or sentence appeared in 
English, the transliteration was not corrected. 
? ks  ??  for ?kiss? 
? Dd yA hAf fAn  ??? ???  ??  ??  for ?did you 
have fun? 
The transliteration of proper names was cor-
rected in the same way as all other words. 
Emoticons and emoji were replaced in the 
transliteration with #.  Emoticons refer to a set of 
numbers or letters or punctuation marks used to 
express feelings or mood.  Emoji refers to a spe-
cial set of images used in messages.  Both Emot-
icons and Emoji are frequent in SMS/Chat data. 
5 Discussion 
Annotation and transliteration were performed 
on all sentence units that contain Arabizi.  Sen-
tence units that contain only Arabic script were 
ignored and untouched during annotation.  In 
total, we reviewed 1270 conversations, among 
which over 42.6K sentence units (more than 
300K words) were deemed to be containing 
Arabizi and hence annotated and transliterated. 
The corpus files are in xml format.  All con-
versations have six layers: source, annotation on 
the source Arabizi tokens, automatic translitera-
tion via 3ARRIB, manual correction of the au-
tomatic transliteration, re-tokenized corrected 
transliteration, and human translation.  See Ap-
pendix A for examples of the file format. 
Each conversation was annotated by one anno-
tator, with 10 percent of the data being reviewed 
by a second annotator as a QC procedure.  Twen-
ty six conversations (roughly 3400 words) were 
also annotated dually by blind assignment to 
gauge inter-annotator agreement. 
As we noted earlier, code switching is fre-
quent in the SMS and Chat Arabizi data.  There 
were about 23K words flagged as foreign words.  
Written out speech effects in this type of data are 
also prevalent, and 6610 tokens were flagged as 
Sounds (laughter, filled pause, etc.).  Annotators 
most often agreed with each other in the detec-
tion and flagging of tokens as Foreign, Name, 
Sound or Punctuation, with over 98% agreement 
for all flags. 
The transliteration annotation was more diffi-
cult than the flagging annotation, because apply-
ing CODA requires linguistic knowledge of Ara-
bic.  Annotators went through several rounds of 
training and practice and only those who passed 
a test were allowed to work on the task.  In an 
analysis of inter-annotator agreement in the dual-
ly annotated files, the overall agreement between 
the two annotators was 86.4%.  We analyzed all 
the disagreements and classified them in four 
high level categories: 
? CODA  60% of the disagreements were related 
to CODA decisions that did not carefully follow 
the guidelines.  Two-fifths of these cases were 
related to Alif/Ya spelling (mostly Alif Hamza-
tion, rules of hamza support) and about one-fifth 
involved the spelling of common dialectal words.  
An additional one-third were due to non-CODA 
root, pattern or affix spelling.  Only one-tenth of 
the cases were because of split or merge deci-
sions.  These issues suggest that additional train-
ing may be needed.  Additionally, since some of 
100
the CODA errors may be easy to detect and cor-
rect using available tools for morphological 
analysis of Egyptian Arabic (such as the CALI-
MA-ARZ analyzer), we will consider integrating 
such support in the annotation interface in the 
future.  
? Task  In 23% of the overall disagreements, the 
annotators did not follow the task guidelines for 
handling punctuation, sounds, emoticons, names 
or foreign words.  Examples include disagree-
ment on whether a question mark should be split 
or kept attached, or whether a non-Arabic word 
should be corrected or not.  Many of these cases 
can also be caught as part of the interface; we 
will consider the necessary extensions in the fu-
ture. 
? Ambiguity  In 12% of the cases, the annota-
tors? disagreement reflected a different reading 
of the Arabizi resulting in a different lemma or 
inflectional feature.  These differences are una-
voidable and reflect the natural ambiguity in the 
task. 
? Typos  Finally, in less than 5% of the cases, 
the disagreement was a result of a typographical 
error unrelated to any of the above issues.  
Among the cases that were easy to adjudicate, 
one of the two annotators was correct 60% more 
than the other.  This is consistent with the obser-
vation that more training may be needed to fill in 
some of the knowledge gaps or increase the an-
notator?s attention to detail. 
6 Conclusion 
This is the first Arabizi-Arabic script parallel 
corpus that supports research on transliteration 
from Arabizi to Arabic script.  We expect to 
make this corpus available through the Linguistic 
Data Consortium in the near future. 
This work focuses on the novel challenges of 
developing a corpus like this, and points out the 
close interaction between the orthographic form 
of written informal genres of Arabic and the spe-
cific features of individual Arabic dialects.  The 
use of Arabizi and the use of Egyptian Arabic in 
this corpus come together to present a host of 
spelling ambiguities and multiplied forms that 
were resolved in this corpus by the use of CODA 
for Egyptian Arabic.  Developing a similar cor-
pus and transliteration for other Arabic dialects 
would be a rich area for future work. 
We believe this corpus will be essential for 
NLP work on Arabic dialects and informal gen-
res.  In fact, this corpus has recently been used in 
development by Eskander et al. (2014). 
Acknowledgements 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
(DARPA) under Contract No. HR0011-11-C-
0145. The content does not necessarily reflect the 
position or the policy of the Government, and no 
official endorsement should be inferred. 
Nizar Habash performed most of his contribu-
tion to this paper while he was at the Center for 
Computational Learning Systems at Columbia 
University. 
References 
Mohamed Al-Badrashiny, Ramy Eskander, Nizar Ha-
bash, and Owen Rambow. 2014. Automatic Trans-
literation of Romanized Dialectal Arabic. In Pro-
ceedings of the Conference on Computational Nat-
ural Language Learning (CONLL), Baltimore, 
Maryland, 2014. 
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number 
LDC2004L02, ISBN 1-58563-324-0.  
Achraf Chalabi and Hany Gerges. 2012. Romanized 
Arabic Transliteration. In Proceedings of the Sec- 
ond Workshop on Advances in Text Input Methods 
(WTIM 2012).  
Eleanor Clark and Kenji Araki. 2011. Text normaliza-
tion in social media: Progress, problems and ap- 
plications for a pre-processing system of casual 
English. Procedia - Social and Behavioral Scienc-
es, 27(0):2 ? 11. 
Kareem Darwish, Walid Magdy, and Ahmed Mourad. 
2012. Language processing for arabic microblog 
re- trieval. In Proceedings of the 21st ACM Inter-
national Conference on Information and 
Knowledge Management, CIKM ?12, pages 2427?
2430, New York, NY, USA. ACM.  
Kareem Darwish. 2013. Arabizi Detection and Con- 
version to Arabic. CoRR, arXiv:1306.6755 [cs.CL]. 
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina 
Bontcheva. 2013. Twitter part-of-speech tagging 
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference Recent 
Advances in Natural Language Processing RANLP 
2013, pages 198?206, Hissar, Bulgaria, September. 
INCOMA Ltd. Shoumen, Bulgaria.  
Heba Elfardy, Mohamed Al-Badrashiny, and Mona 
Diab. 2013. Code Switch Point Detection in Ara-
bic. In Proceedings of the 18th International Con-
ference on Application of Natural Language to In-
formation Systems (NLDB2013), MediaCity, UK, 
June.  
Ramy Eskander, Mohamed Al-Badrashiny, Nizar Ha-
bash and Owen Rambow. 2014. Foreign Words 
101
and the Automatic Processing of Arabic Social 
Media Text Written in Roman Script. In Arabic 
Natural Language Processing Workshop, EMNLP, 
Doha, Qatar. 
Ramy Eskander, Nizar Habash, Owen Rambow, and 
Nadi Tomeh. 2013. Processing Spontaneous Or- 
thography. In Proceedings of the 2013 Conference 
of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies (NAACL-HLT), Atlanta, GA.  
Andrew T. Freeman, Sherri L. Condon and Christo-
pher M. Ackerman. 2006. Cross Linguistic Name 
Matching in English and Arabic: A ?One to Many 
Mapping? Extension of the Levenshtein Edit Dis-
tance Algorithm. In Proceedings of HLT-NAACL, 
New York, NY. 
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, 
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Mi-
chael Heilman, Dani Yogatama, Jeffrey Flanigan, 
and Noah A. Smith. 2011. Part-of-speech tagging 
for twitter: Annotation, features, and experiments. 
In Proceedings of ACL-HLT ?11.  
Stephan Gouws, Donald Metzler, Congxing Cai, and 
Eduard Hovy. 2011. Contextual bearing on linguis-
tic variation in social media. In Proceedings of the 
Workshop on Languages in Social Media, LSM 
?11, pages 20?29, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics. 
Nizar Habash, Mona Diab, and Owen Rambow 
(2012a).Conventional Orthography for Dialectal 
Arabic: Principles and Guidelines ? Egyptian Ara-
bic. Technical Report CCLS-12-02, Columbia 
University Center for Computational Learning Sys-
tems.  
Nizar Habash, Mona Diab, and Owen Rabmow. 
2012b. Conventional Orthography for Dialectal 
Arabic. In Proceedings of the Language Resources 
and Evaluation Conference (LREC), Istanbul.  
Nizar Habash, Ramy Eskander, and Abdelati Haw-
wari. 2012c. A Morphological Analyzer for Egyp-
tian Arabic. In Proceedings of the Twelfth Meeting 
of the Special Interest Group on Computational 
Morphology and Phonology, pages 1?9, Montr?al, 
Canada.  
Kevin Knight and Jonathan Graehl. 1997. Machine 
Transliteration. In Proceedings of the Conference 
of the Association for Computational Linguistics 
(ACL). 
Linguistic Data Consortium. 2014. BOLT Program: 
Romanized Arabic (Arabizi) to Arabic Translitera-
tion and Normalization Guidelines, Version 3.1. 
Linguistic Data Consortium, April 21, 2014.  
Marco Lui, Jey Han Lau, and Timothy Baldwin. 
2014. Automatic detection and language identifica-
tion of multilingual documents. In Proceedings of 
the Language Resources and Evaluation Confer-
ence (LREC), Reykjavik, Iceland.  
Mohamed Maamouri, Ann Bies, Seth Kulick, Michael 
Ciul, Nizar Habash and Ramy Eskander. 2014. De-
veloping a dialectal Egyptian Arabic Treebank: 
Impact of Morphology and Syntax on Annotation 
and Tool Development. In Proceedings of the Lan-
guage Resources and Evaluation Conference 
(LREC), Reykjavik, Iceland. 
Yaser Al-Onaizan and Kevin Knight. 2002. Machine 
Transliteration of Names in Arabic Text. In Pro-
ceedings of ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, 
Ahmed El Kholy, Ramy Eskander, Nizar Habash, 
Manoj Pooleery, Owen Rambow, and Ryan M. 
Roth. 2014. MADAMIRA: A Fast, Comprehensive 
Tool for Morphological Analysis and Disambigua-
tion of Arabic. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Rey-
kjavik, Iceland.  
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11. 
Wael Salloum and Nizar Habash. 2011. Dialectal to 
Standard Arabic Paraphrasing to Improve Arabic- 
English Statistical Machine Translation. In Pro- 
ceedings of the First Workshop on Algorithms and 
Resources for Modelling of Dialects and Language 
Varieties, pages 10?21, Edinburgh, Scotland.  
Zhiyi Song, Stephanie Strassel, Haejoong Lee, Kevin 
Walker, Jonathan Wright, Jennifer Garland, Dana 
Fore, Brian Gainor, Preston Cabe, Thomas Thom-
as, Brendan Callahan, Ann Sawyer. Collecting 
Natural SMS and Chat Conversations in Multiple 
Languages: The BOLT Phase 2 Corpus. In Pro-
ceedings of the Language Resources and Evalua-
tion Conference (LREC) 2014, Reykjavik, Iceland. 
Clare Voss, Stephen Tratz, Jamal Laoudi, and Dou- 
glas Briesch. 2014. Finding romanized Arabic dia-
lect in code-mixed tweets. In Proceedings of the 
Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, 
Iceland. 
Omar F Zaidan and Chris Callison-Burch. 2011. The 
arabic online commentary dataset: an annotated da-
taset of informal arabic with high dialectal content. 
In Proceedings of ACL, pages 37?41. 
102
Appendix A: File Format Examples 
 
 
 
Example 1: 
 
<su id="s1582"> 
  <source>marwan ? ana walahi knt gaya today :/</source> 
   <annotated_arabizi> 
        <token id="t0" tag="name">marwan</token> 
       <token id="t1" tag="punctuation">?</token> 
       <token id="t2">ana</token> 
      <token id="t3">walahi</token> 
   <token id="t4">knt</token> 
        <token id="t5">gaya</token> 
       <token id="t6" tag="foreign">today</token> 
        <token id="t7">:/</token> 
     </annotated_arabizi> 
    <auto_transliteration> :/ ???? ???? ??? ?? ???  ?????? </auto_transliteration> 
<corrected_transliteration> # ????  ???? ??? ?? ???  ?????? </corrected_transliteration> 
<retokenized_transliteration> # ???? ???? ??? ?? ???  ?????? </retokenized_transliteration> 
     <translation lang="eng">Marwan? I swear I was coming today :/</translation> 
     <messages> 
<message id="m2377" time="2013-10-01 22:03:34 UTC" participant="139360">marwan ? ana 
walahi knt gaya today :/</message> 
     </messages> 
  </su> 
 
Example 2: 
 
<su id="s3"> 
<source>W sha3rak ma2sersh:D haha</source> 
<annotated_arabizi> 
<token id="t0">W</token> 
<token id="t1">sha3rak</token> 
<token id="t2">ma2sersh:D</token> 
<token id="t3" tag="sound">haha</token> 
</annotated_arabizi> 
<auto_transliteration> ?? # [-]????? ???? [+]? </auto_transliteration> 
<corrected_transliteration> ?? #[-]????[-]?? ???? [+]? </corrected_transliteration> 
<retokenized_transliteration> ?? # ???? ?? ????? </retokenized_transliteration> 
<translation lang="eng">And your hair did not become short? :D Haha</translation> 
<messages> 
<message id="m0004" medium="IM" time="2012-12-22 15:36:31 UTC" participant="138112">W 
sha3rak ma2sersh:D haha</message> 
</messages> 
</su> 
 
 
 
103
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 114?120,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Pipeline Approach to Supervised Error Correction
for the QALB-2014 Shared Task
Nadi Tomeh
?
Nizar Habash
?
Ramy Eskander
?
Joseph Le Roux
?
{nadi.tomeh,leroux}@lipn.univ-paris13.fr
?
nizar.habash@nyu.edu
?
, ramy@ccls.columbia.edu
?
?
Universit?e Paris 13, Sorbonne Paris Cit?e, LIPN, Villetaneuse, France
?
Computer Science Department, New York University Abu Dhabi
?
Center for Computational Learning Systems, Columbia University
Abstract
This paper describes our submission to
the ANLP-2014 shared task on auto-
matic Arabic error correction. We present
a pipeline approach integrating an er-
ror detection model, a combination of
character- and word-level translation mod-
els, a reranking model and a punctuation
insertion model. We achieve an F
1
score
of 62.8% on the development set of the
QALB corpus, and 58.6% on the official
test set.
1 Introduction
Devising algorithms for automatic error correction
generated considerable interest in the community
since the early 1960s (Kukich, 1992) for at least
two reasons. First, typical NLP tools lack in ro-
bustness against errors in their input. This sen-
sitivity jeopardizes their usefulness especially for
unedited text, which is prevalent on the web. Sec-
ond, automated spell and grammar checkers facil-
itate text editing and can be of great help to non-
native speakers of a language. Several resources
and shared tasks appeared recently, including the
HOO task (Dale and Kilgarriff, 2010) and the
CoNLL task on grammatical error correction (Ng
et al., 2013b). In this paper we describe our partic-
ipation to the first shared task on automatic error
correction for Arabic (Mohit et al., 2014).
While non-word errors are relatively easy to
handle, the task is more challenging for gram-
matical and semantic errors. Detecting and cor-
recting such errors require context-sensitive ap-
proaches in order to capture the dependencies be-
tween the words of a text at various lexical and se-
mantic levels. All the more so for Arabic which
brings dependence down to the morphological
level (Habash, 2010).
A particularity interesting approach to error cor-
rection relies on statistical machine translation
(SMT) (Brockett et al., 2006), due to its context-
sensitivity and data-driven aspect. Therefore, the
pipeline system which we describe in Section 2
has as its core a phrase-based SMT component
(PBSMT) (Section 2.3). Nevertheless, several fac-
tors may hinder the success of this approach, such
as data sparsity, discrepancies between transla-
tion and error correction tasks, and the difficulty
of incorporating context-sensitive features into the
SMT decoder.
We address all these issues in our system which
achieves a better correction quality than a simple
word-level PBSMT baseline on the QALB corpus
(Zaghouani et al., 2014) as we show in our exper-
iments in Section 3.
2 Pipeline Approach to Error Correction
The PBSMT system accounts for context by learn-
ing, from a parallel corpus of annotated errors,
mappings from erroneous multi-word segments of
text to their corrections, and using a language
model to help select the suitable corrections in
context when multiple alternatives are present.
Furthermore, since the SMT approach is data-
driven, it is possible to address multiple types of
errors at once, as long as examples of them appear
in the training corpus. These errors may include
non-word errors, wrong lexical choices and gram-
matical errors, and can also handle normalization
issues (Yvon, 2010).
One major issue is data sparsity, since large
amount of labeled training data is necessary to
provide reliable statistics of all error types. We ad-
114
dress this issue by backing-off the word-level PB-
SMT model with a character-level correction com-
ponent, for which richer statistics can be obtained.
Another issue may stem from the inherent dif-
ference in nature between error correction and
translation. Unlike translation, the input and out-
put vocabularies in the correction task overlap sig-
nificantly, and the majority of input words are typi-
cally correct and are copied unmodified to the out-
put. The SMT system should handle correct words
by selecting their identities from all possible op-
tions, which may fail resulting in over-correction.
To help the SMT decoder decide, we augment our
pipeline with a problem zone detection compo-
nent, which supplies prior information on which
input words need to be corrected.
The final issue concerns the difficulty of incor-
porating features that require context across phrase
boundaries into the SMT decoder. A straightfor-
ward alternative is to use such features to rerank
the hypotheses in the SMT n-best hypotheses lists.
Since punctuation is particularity noisy in Ara-
bic data, we add a specialized punctuation inser-
tion component to our pipeline, depicted in Figure
1.
2.1 Error Detection
We formalize the error detection problem as a
sequence labeling problem (Habash and Roth,
2011). Errors are classified into substitution, in-
sertion and deletion errors. Substitutions involve
an incorrect word form that should be replaced by
another correct form. Insertions are words that
are incorrectly added into the text and should be
deleted. Deletions are simply missing words that
should be added.
We group all error classes into a simple binary
problem tag: a word from the input text is tagged
as ?PROB? if it is the result of an insertion or
a substitution of a word. Deleted words, which
cannot be tagged themselves, cause their adjacent
words to be marked as PROB instead. In this way,
the subsequent components in the pipeline can be
alerted to the possibility of a missing word via its
surroundings. Any words not marked as PROB are
given an ?OK? tag.
Gold tags, necessary for training, can be gener-
ated by comparing the text to its correction using
some sequence alignment technique, for which we
use SCLITE (Fiscus, 1998).
For this task, we use Yamcha (Kudo and Mat-
sumoto, 2003) to train an SVM classifier using
morphological and lexical features. We employ
a quadratic polynomial kernel. The static feature
window context size is set to +/- 2 words; the pre-
vious two (dynamic) predicted tags are also used
as features.
The feature set includes the surface forms and
their normalization after ?Alef?, ?Ya? and digit
normalization, the POS tags and the lemmas of the
words. These morphological features are obtained
using MADA 3.0 (Habash et al., 2009).
1
We also
use a set of word, POS and lemma 3-gram lan-
guage models scores as features. These LMs are
built using SRILM (Stolcke, 2002).
The error detection component is integrated into
the pipeline by concatenating the predicted tags
with the words of the input text. The SMT model
uses this additional information to learn distinct
mappings conditional on the predicted correctness
of words.
2.2 Character-level Back-off Correction
Each word that is labeled as error (PROB) in the
output of the error detection component is mapped
to multiple possible corrections using a weighted
finite-state transducer similar to the transducers
used in speech recognition (Mohri et al., 2002).
The WFST, for which we used OpenFST (Al-
lauzen et al., 2007), operates on the character
level, and the character mapping is many-to-many
(similar to the phrase-based SMT framework).
The score of each proposed correction is a com-
bination of the scores of character mappings used
to build it. The list is filtered using WFST scores
and an additional character-level LM score. The
result is a list of error-tagged words and their cor-
rection suggestions, which constitutes a small on-
the-fly phrase table used to back-off primary PB-
SMT table.
During training, the mapping dictionary is
learned from the training after aligning it at the
character level using SCLITE. Mapping weights
are computed as their normalized frequencies in
the aligned training corpus.
2.3 Word-level PBSMT Correction
We formalize the correction process as a phrase-
based statistical machine translation problem
(Koehn et al., 2003), at the word-level, and solve
1
We did not use MADAMIRA (the newest version of
MADA) since it was not available when this component was
built.
115
Character-level 
Correction 
Error 
Detection 
Word-level 
PBSMT Correction 
N-best 
Reranking 
Punctuation 
Insertion 
,	 ? .	 ?
Input Error-tagged text 
N-best hypotheses 
Reranked best hypothesis Output 
Back-off Phrase 
tables 
Primary 
Figure 1: Input text is run through the error detection component which labels the problematic words.
The labeled text is then fed to the character-level correction components which constructs a back-off
phrase table. The PBSMT component then uses two phrase tables to generate n-best correction hy-
potheses. The reranking component selects the best hypothesis, and pass it to the punctuation insertion
component in order to produce the final output.
it using Moses, a well-known PBSMT tool (Koehn
et al., 2007). The decoder constructs a correction
hypothesis by first segmenting the input text into
phrases, and mapping each phrase into its best cor-
rection using a combination of scores including a
context-sensitive LM score.
Unlike translation, error correction is mainly
monotonic, therefore we set disallow reordering
by setting the distortion limit in Moses to 0.
2
When no mapping can be found for a given
phrase in the primary phrase table, the decoder
looks it up in the back-off model. The decoder
searches the space of all possible correction hy-
potheses, resulting from alternative segmentations
and mappings, and returns the list of n-best scor-
ing hypotheses.
2.4 N-best List Reranking
In this step, we combine LM information with lin-
guistically and semantically motivated features us-
ing learning to rank methods (Tomeh et al., 2013).
Discriminative reranking (Liu, 2009) allows each
hypothesis to be represented as an arbitrary set of
features without the need to explicitly model their
interactions. Therefore, the system benefits from
global and potentially complex features which are
not available to the baseline decoder.
Each hypothesis in an n-best list is represented
by a d-dimensional feature vector. Word error rate
(WER) is computed for each hypotheses by com-
paring it to the reference correction. The resulting
2
Only 0.14% of edits in the QALB corpus are actually
reordering.
scored n-best list is used for supervised training
of a reranking model. We employ a pairwise ap-
proach to ranking which takes pairs of hypotheses
as instances in learning, and formalizes the rank-
ing problem as pairwise classification.
For this task we use RankSVM (Joachims,
2002) which is a method based on Support Vec-
tor Machines (SVMs). We use only linear kernels
to keep complexity low. We use a rich set of fea-
tures including LM scores on surface forms, POS
tags and lemmas. We also use a feature based on a
global model of the semantic coherence of the hy-
potheses (Tomeh et al., 2013). The new top ranked
hypothesis is the output of this step which is then
fed to the next component.
2.5 Punctuation Insertion
We developed a model that predicts the occurrence
of periods and commas in a given Arabic text.
The core model is a decision tree classifier trained
on the QALB parallel training data using WEKA
(Hall et al., 2009). For each space between two
words, the classifier decides whether or not to in-
sert a punctuation mark, using a window size of
three words surrounding the underlying space.
The model uses the following features:
? A class punctuation feature, that is whether to
insert a period, a comma or none at the cur-
rent space location;
? The part-of-speech of the previous word;
? The existence of a conjunctive or connective
proclitic in the following word; that is a ?wa?
116
Precision?Recall Curve
Recall
Prec
ision
0.0 0.2 0.4 0.6 0.8 1.0
0.2
0.4
0.6
0.8
1.0
?
8.33
?
5.02
?
1.7
1.61
4.93
AUC= 0.715PRBE= 0.483, Cutoff= ?0.349Prec@rec(0.800)= 0.345, Cutoff= ?1.045
Figure 2: Evaluation of the error detection com-
ponent. AUC: Area Under the Curve, PRBE:
precision-recall break-even point. Classifier
thresholds are displayed on the right vertical axis.
or ?fa? proclitic that is either a conjunction, a
sub-conjunction or a connective particle.
We obtain POS and proclitic information using
MADAMIRA (Pasha et al., 2014). The output of
this component is the final output of the system.
3 Experiments
All the models we use in our pipeline are trained
in a supervised way using the training part of the
QALB corpus (Zaghouani et al., 2014), while we
reserve the development part of the corpus for test-
ing.
3.1 Error detection
We evaluate the error detection binary classifier in
terms of standard classification measures as shown
in Figure 2. Each point on the curve is computed
by selecting a threshold on the classifier score.
The threshold we use correspond to recall equal
to 80%, at which the precision is very low which
leaves much room for improvement in the perfor-
mance of the error detection component.
3.2 Character-level correction
We evaluate the character-level correction model
by measuring the percentage of erroneous phrases
that have been mapped to their in-context refer-
ence corrections. We found this percentage to be
41% on QALB dev data. We limit the size of
such phrases to one in order to focus on out-of-
vocabulary words.
3.3 Punctuation insertion
To evaluate the punctuation insertion indepen-
dently from the pipeline, we first remove the pe-
riods and commas from input text. Considering
only the locations where periods and commas ex-
ist, our model gives a recall of 49% and a precision
of 53%, giving an F
1
-score of 51%.
When we apply our punctuation model in the
correction pipeline, we find that it is always better
to keep the already existing periods and commas
in the input text instead of overwriting them by
the model prediction.
While developing the model, we ran experi-
ments where we train the complete list of fea-
tures produced by MADAMIRA; that is part-of-
speech, gender, number, person, aspect, voice,
case, mood, state, proclitics and enclitics. This
was done for two preceding words and two follow-
ing words. However, the results were significantly
outperformed by our final set-up.
3.4 The pipeline
The performance of the pipeline is evaluated in
terms of precision, recall and F
1
as computed by
the M
2
Scorer (Dahlmeier and Ng, 2012b). The
results presented in Table 1 show that a simple
PBSMT baseline achieves relatively good perfor-
mance compared to more sophisticated models.
The character-level back-off model helps by im-
proving recall at the expense of decreased preci-
sion. The error detection component hurts the per-
formance which could be explained by its intrin-
sic bad performance. Since more investigation is
needed to clarify on this point, we drop this com-
ponent from our submission. Both reranking and
punctuation insertion improve the performance.
Our system submission to the shared task (back-
off+PBSMT+Rank+PI) resulted in an F
1
score of
58.6% on the official test set, with a precision of
76.9% and a recall of 47.3%.
4 Related Work
Both rule-based and data-driven approaches to
error correction can be found in the literature
(Sidorov et al., 2013; Berend et al., 2013; Yi et
al., 2013) as well as hybridization of them (Putra
and Szabo, 2013). Unlike our approach, most of
117
System PR RC F
1
PBSMT 75.5 49.5 59.8
backoff+PBSMT 74.1 51.8 60.9
ED+backoff+PBSMT 61.3 45.4 52.2
backoff+PBSMT+Rank 75.7 52.1 61.7
backoff+PBSMT+Rank+PI 74.9 54.2 62.8
Table 1: Pipeline precision, recall and F
1
scores.
ED: error detection, PI: punctuation insertion.
the proposed systems build distinct models to ad-
dress individual types of errors (see the CoNLL-
2013, 2014 proceedings (Ng et al., 2013a; Ng
et al., 2014), and combine them afterwords us-
ing Integer Linear Programming for instance (Ro-
zovskaya et al., 2013). This approach is relatively
time-consuming when the number of error types
increases.
Interest in models that target all errors at once
has increased, using either multi-class classifiers
(Farra et al., 2014; Jia et al., 2013), of-the-shelf
SMT techniques (Brockett et al., 2006; Mizu-
moto et al., 2011; Yuan and Felice, 2013; Buys
and van der Merwe, 2013; Buys and van der
Merwe, 2013), or building specialized decoders
(Dahlmeier and Ng, 2012a).
Our system addresses the weaknesses of the
SMT approach using additional components in a
pipeline architecture. Similar work on word-level
and character-level model combination has been
done in the context of translation between closely
related languages (Nakov and Tiedemann, 2012).
A character-level correction model has also been
considered to reduce the out-of-vocabulary rate in
translation systems (Habash, 2008).
5 Conclusion and Future Work
We described a pipeline approach based on
phrase-based SMT with n-best list reranking. We
showed that backing-off word-level model with a
character-level model improves the performance
by ameliorating the recall of the system.
The main focus of our future work will be on
better integration of the error detection model, and
on exploring alternative methods for combining
the character and the word models.
Acknowledgments
This material is partially based on research funded
by grant NPRP-4-1058-1-168 from the Qatar Na-
tional Research Fund (a member of the Qatar
Foundation). The statements made herein are
solely the responsibility of the authors.
Nizar Habash performed most of his contri-
bution to this paper while he was at the Center
for Computational Learning Systems at Columbia
University.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In CIAA, pages 11?23.
Gabor Berend, Veronika Vincze, Sina Zarrie?, and
Rich?ard Farkas. 2013. Lfg-based features for noun
number and article grammatical errors. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning: Shared Task,
pages 62?67, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting esl errors using phrasal
smt techniques. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th Annual Meeting of the Association
for Computational Linguistics, ACL-44, pages 249?
256, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Buys and Brink van der Merwe. 2013. A tree
transducer model for grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 43?51, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
EMNLP-CoNLL, pages 568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In HLT-
NAACL, pages 568?572.
Robert Dale and Adam Kilgarriff. 2010. Helping our
own: Text massaging for computational linguistics
as a new shared task. In INLG.
Noura Farra, Nadi Tomeh, Alla Rozovskaya, and Nizar
Habash. 2014. Generalized character-level spelling
error correction. In ACL (2), pages 161?167.
Jon Fiscus. 1998. Speech Recognition Scor-
ing Toolkit (SCTK). National Institute of Standard
Technology (NIST). http://www.itl.nist.
gov/iad/mig/tools/.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
arabic handwriting recognition. In Proceedings of
118
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-08: HLT, Short Papers, pages 57?
60, Columbus, Ohio.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013. Gram-
matical error correction as multiclass classification
with single model. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 74?81, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?02,
pages 133?142.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference (HLT/NAACL), pages 127?133,
Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Comput. Surv.,
24(4):377?439, December.
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval. Now Publishers Inc., Hanover, MA,
USA.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revision
log of language learning sns for automated japanese
error correction of second language learners. In IJC-
NLP, pages 147?155.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech & Language,
16(1):69?88.
Preslav Nakov and J?org Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In ACL (2), pages 301?305.
Hwee Tou Ng, Joel Tetreault, Siew Mei Wu, Yuanbin
Wu, and Christian Hadiwinoto, editors. 2013a. Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task.
Association for Computational Linguistics, Sofia,
Bulgaria, August.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013b. The conll-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant, editors. 2014. Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task. Association for
Computational Linguistics, Baltimore, Maryland,
June.
Arfath Pasha, Mohamed Al-Badrashiny, Mona T. Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. Madamira: A fast, comprehensive tool for
morphological analysis and disambiguation of ara-
bic. In LREC, pages 1094?1101.
Desmond Darma Putra and Lili Szabo. 2013. Uds
at conll 2013 shared task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 88?95,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
119
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The university of illinois sys-
tem in the conll-2013 shared task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Grigori Sidorov, Anubhav Gupta, Martin Tozer, Do-
lors Catala, Angels Catena, and Sandrine Fuentes.
2013. Rule-based system for automatic grammar
correction using syntactic n-grams for english lan-
guage learning (l2). In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 96?101, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Nadi Tomeh, Nizar Habash, Ryan Roth, Noura Farra,
Pradeep Dasigi, and Mona Diab. 2013. Reranking
with linguistic and semantic features for arabic op-
tical character recognition. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
549?555, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim.
2013. Kunlp grammatical error correction system
for conll-2013 shared task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 123?127,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52?61, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Franc?ois Yvon. 2010. Rewriting the orthography
of sms messages. Natural Language Engineering,
16:133?159, 3.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale arabic error annotation: Guidelines and
framework. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC?14), Reykjavik, Iceland, May. Euro-
pean Language Resources Association (ELRA).
120
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 160?164,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The Columbia System in the QALB-2014 Shared Task
on Arabic Error Correction
Alla Rozovskaya Nizar Habash
?
Ramy Eskander Noura Farra Wael Salloum
Center for Computational Learning Systems, Columbia University
?
New York University Abu Dhabi
{alla,ramy,noura,wael}@ccls.columbia.edu
?
nizar.habash@nyu.edu
Abstract
The QALB-2014 shared task focuses on
correcting errors in texts written in Mod-
ern Standard Arabic. In this paper, we
describe the Columbia University entry in
the shared task. Our system consists of
several components that rely on machine-
learning techniques and linguistic knowl-
edge. We submitted three versions of the
system: these share several core elements
but each version also includes additional
components. We describe our underlying
approach and the special aspects of the dif-
ferent versions of our submission. Our
system ranked first out of nine participat-
ing teams.
1 Introduction
The topic of text correction has seen a lot of in-
terest in the past several years, with a focus on
correcting grammatical errors made by learners of
English as a Second Language (ESL). The two
most recent CoNLL shared tasks were devoted to
grammatical error correction for non-native writ-
ers (Ng et al., 2013; Ng et al., 2014).
The QALB-2014 shared task (Mohit et al.,
2014) is the first competition that addresses the
problem of text correction in Modern Standard
Arabic (MSA) texts. The competition makes
use of the recently developed QALB corpus (Za-
ghouani et al., 2014). The shared task covers all
types of mistakes that occur in the data.
Our system consists of statistical models, lin-
guistic resources, and rule-based modules that ad-
dress different types of errors.
We briefly discuss the task in Section 2. Sec-
tion 3 gives an overview of the Columbia system
and describes the system components. In Sec-
tion 4, we evaluate the complete system on the de-
velopment data and show the results obtained on
test. Section 5 concludes.
2 Task Description
The QALB-2014 shared task addresses the prob-
lem of correcting errors in texts written in Modern
Standard Arabic (MSA). The task organizers re-
leased training, development, and test data. All
of the data comes from online commentaries writ-
ten to Aljazeera articles.
1
The training data con-
tains 1.2 million words; the development and the
test data contain about 50,000 words each. The
data was annotated and corrected by native Arabic
speakers. For more detail on the QALB corpus, we
refer the reader to Zaghouani et al. (2014). The re-
sults in the subsequent sections are reported on the
development set.
It should be noted that in the annotation process,
the annotators did not assign error categories but
only specified an appropriate correction. In spite
of this, it is possible, to isolate certain error types
automatically, by using the corrections in coordi-
nation with the input words. The first type con-
cerns punctuation errors. Errors involving punc-
tuation account for about 39% of all errors in the
data. In addition to punctuation mistakes, another
very common source of errors refers to subopti-
mal spelling for two groups of letters ? Alif (and
its Hamzated versions) and Ya (and its undotted or
Alif Maqsura versions). For more detail on this
and other Arabic phenomena, we refer the reader
to Habash (2010; Buckwalter (2007; El Kholy and
Habash (2012). Mistakes associated with Alif and
1
http://www.aljazeera.net/
160
Component System
CLMB-1 CLMB-2 CLMB-3
MADAMIRA
MLE
Na??ve Bayes
GSEC
MLE-unigram
Punctuation
Dialectal
Patterns
Table 1: The three versions of the Columbia sys-
tem and their components.
Ya spelling constitute almost 30% of all errors.
3 System Overview
The Columbia University system consists of sev-
eral components designed to address different
types of errors. We submitted three versions of the
system. We refer to these as CLMB-1, CLMB-2,
and CLMB-3. Table 1 lists all of the components
and indicates which components are included in
each version. The components are applied in the
order shown in the table. Below we describe each
component in more detail.
3.1 MADAMIRA Corrector
MADAMIRA (Pasha et al., 2014) is a tool
designed for morphological analysis and dis-
ambiguation of Modern Standard Arabic.
MADAMIRA performs morphological analysis
in context. This is a knowledge-rich resource
that requires a morphological analyzer and a
large corpus where every word is marked with
its morphological features. The task organizers
provided the shared task data pre-processed
with MADAMIRA, including all of the features
generated by the tool for every word. In addition
to the morphological analysis and contextual
morphological disambiguation, MADAMIRA
also performs Alif and Ya spelling correction
for the phenomena associated with these letters
discussed in Section 2. The corrected form was
included among the features and can be used
for correcting the input. We use the corrections
proposed by MADAMIRA and apply them to the
data. As we show in Section 4, while the form
proposed by MADAMIRA may not necessarily
be correct, MADAMIRA performs at a very high
precision. MADAMIRA corrector is used in the
CLMB-1 and CLMB-2 systems.
3.2 Maximum Likelihood Model
The Maximum Likelihood Estimator (MLE) is a
supervised component that is trained on the train-
ing data of the shared task. Given the annotated
training data, a map is defined that specifies for ev-
ery word n-gram in the source text the most likely
n-gram corresponding to it in the target text. The
MLE model considers source n-grams of lengths
between 1 to 3; the MLE-unigram model that is
part of the CLMB-3 version only considers n-
grams of length 1.
The MLE approach performs well on errors that
have been observed in the training data and can
be unambiguously corrected without using the sur-
rounding context, i.e. do not have many alternative
corrections. Consequently, MLE fails on words
that have many possible corrections, as well as
words not seen in training.
3.3 Na??ve Bayes for Unseen Words
The Na??ve Bayes component addresses errors for
words that were not seen in training. The system
uses the approach proposed in Rozovskaya and
Roth (2011) that proved to be successful for cor-
recting errors made by English as a Second Lan-
guage learners. The model operates at the word
level and targets word replacement errors that in-
volve single tokens. Candidate corrections are
generated using a character confusion table that is
based on the training data. The model is a Na??ve
Bayes classifier trained on the Arabic Gigaword
corpus (Parker et al., 2011) with word n-gram fea-
tures in the 4-word window around the word to be
corrected. The Na??ve Bayes component is used in
the CLMB-1 system.
3.4 The GSEC Model
The CLMB-3 system implements a Generalized
Character-Level Error Correction model (GSEC)
proposed in Farra et al. (2014). GSEC is a super-
vised model that operates at the character level.
Because of this, the source and the target side of
the training data need to be aligned at the charac-
ter level. We use the alignment tool Sclite (Fiscus,
1998). The alignment maps each source charac-
ter to itself, a different character, a pair of char-
acters, or an empty string. For the shared task,
punctuation corrections are ignored since punctu-
ation errors are handled by the punctuation correc-
tor described in the following section. It should
161
also be noted that the model was not trained to
insert missing characters. The model is a multi-
class SVM classifier (Kudo, 2005) that makes use
of character-level features using a window of four
characters that may occur within the word bound-
aries as well as in the surrounding context. Due
to a long training time, GSEC was trained on a
quarter of the training data. The system is post-
processed with a unigram word-level maximum-
likelihood model described in Section 3.2. For
more detail on the GSEC approach, we refer the
reader to Farra et al. (2014).
3.5 Punctuation Corrector
The shared task data contains a large number of
punctuation mistakes. Punctuation errors, such as
missing periods and commas, account for about
30% of all errors in the data. Most of these errors
involve incorrectly omitting a punctuation symbol.
Our punctuation corrector is a statistical model
that inserts periods and commas. The system is
a decision tree model trained on the shared task
training data using WEKA (Hall et al., 2009). For
punctuation insertion, every space that is not fol-
lowed or preceded by a punctuation mark is con-
sidered.
To generate features, we use a window of size
three around the target space. The features are de-
fined as follows:
? The part-of-speech of the previous word
? The existence of a conjunctive or connective
proclitic in the following word; that is a ?w?
or ?f? proclitic that is either a conjunction, a
sub-conjunction or a connective particle
The part-of-speech and proclitic information is
obtained by running MADAMIRA on the text.
We also ran experiments where the model is
trained with a complete list of features produced
by MADAMIRA; that is part-of-speech, gender,
number, person, aspect, voice, case, mood, state,
proclitics and enclitics. This was done for two pre-
ceding words and two following words. However,
this model did not perform as well as the one de-
scribed above, which we used in the final system.
Note that the punctuation model predicts pres-
ence or absence of a punctuation mark in a spe-
cific location and is applied to the source data
from which all punctuation marks have been re-
moved. However, when we apply our punctuation
model in the correction pipeline, we find that it
is always better to keep the already existing peri-
ods and commas in the input text instead of over-
writing them with the model prediction. In other
words, we only attempt to add missing punctua-
tion.
3.6 Dialectal Usage Corrector
Even though the shared task data is written in
MSA, MSA is not a native language for Arabic
speakers. Typically, an Arabic speaker has a native
proficiency in one of the many Arabic dialects and
learns to write and read MSA in a formal setting.
For this reason, even in MSA texts produced by
native Arabic speakers, one typically finds words
and linguistic features specific to the writer?s na-
tive dialect that are not found in the standard lan-
guage.
To address such errors, we use Elissa (Salloum
and Habash, 2012), which is Dialectal to Standard
Arabic Machine Translation System. Elissa uses
a rule-based approach that relies on the existence
of a dialectal morphological analyzer (Salloum
and Habash, 2011), a list of hand-written trans-
fer rules, and dialectal-to-standard Arabic lexi-
cons. Elissa uses different dialect identification
techniques to select dialectal words and phrases
(dialectal multi-word expressions) that need to be
handled. Then equivalent MSA paraphrases of the
selected words/phrases are generated and an MSA
lattice for each input sentence is constructed. The
paraphrases within the lattice are then ranked us-
ing language models and the n-best sentences are
extracted from lattice. We use 5-gram language
models trained using SRILM (Stolcke, 2002) on
about 200 million untokenized, Alif /Ya normal-
ized words extracted from Arabic GigaWord. This
component is employed in the CLMB-2 system.
3.7 Pattern-Based Corrector
We created a set of rules that account for very
common phenomena involving incorrectly split or
merged tokens. The MADAMIRA corrector de-
scribed above does not handle splits and merges;
however, some of the cases are handled in the
MLE method. Note that the MLE method is re-
strictive since it does not correct words not seen
in training, while the pattern-based corrector is
more general. The rules were created through
analysis of samples of the QALB Shared Task
162
training data. Some of the rules use regular ex-
pressions, while others make use of the rule-
based Standard Arabic Morphological Analyzer
(SAMA) (Maamouri et al., 2010), the same out-
of-context analyzer used inside of MADAMIRA.
Rules for splitting words
? All digits are separated from words.
? A space is added after all word medial Ta-
Marbuta characters.
? A space is added after the very common
?ElY? ?at/about/on? preposition if it is at-
tached to the following word.
? If a word has a morphological analysis that
includes ?lmA? (as negation particle, relative
pronoun or pseudo verb), ?hA? (a demonstra-
tive pronoun), or ?Ebd? and ?>bw? in proper
nouns, a space is inserted after those parts of
the analysis.
? If a word has no morphological analysis, but
starts with a set of commonly mis-attached
words, and the rest of the word has an anal-
ysis, the word is split after the mis-attached
word sequence.
Rules for merging words
? All lone occurrences of the conjunction w
?and? are attached to the following word.
? All sequences of the punctuation marks (., ?,
!) that occur between two and six times are
merged: e.g ! ! ! ? !!!.
4 Experimental Results
In Section 3, we described the individual sys-
tem components that address different types of
errors. In this section, we show how the sys-
tem improves when each component is added into
the system. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012), the official
scorer of the shared task.
Table 2 reports performance results of each ver-
sion of the Columbia system on the development
data. Table 3 shows the performance results for the
best-performing system, CLMB-1, as each system
component is added.
System P R F1
CLMB-1 72.22 62.79 67.18
CLMB-2 69.49 61.72 65.38
CLMB-3 69.71 59.42 64.15
Table 2: Performance of the Columbia systems
on the development data.
System P R F1
MADAMIRA 83.33 32.94 47.21
+ MLE 86.52 42.52 57.02
+ NB 85.80 43.27 57.53
+ Punc. 73.66 59.51 65.83
+ Patterns 72.22 62.79 67.18
Table 3: Performance of the CLMB-1 system on
the development data and the contribution of
its components.
System P R F1
CLMB-1 73.34 63.23 67.91
CLMB-2 70.86 62.21 66.25
CLMB-3 71.45 60.00 65.22
Table 4: Performance of the Columbia systems
on the test data.
Finally, Table 4 reports results obtained on the
test data. These results are comparable to the per-
formance observed on the development data. In
particular, CLMB-1 achieves the highest score.
5 Conclusion
We have described the Columbia University sys-
tem that participated in the first shared task
on grammatical error correction for Arabic and
ranked first out of nine participating teams. We
have presented three versions of the system; all of
these incorporate several components that target
different types of mistakes, which we presented
and evaluated in this paper.
Acknowledgments
This material is based on research funded by grant
NPRP-4-1058-1-168 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
The statements made herein are solely the respon-
sibility of the authors. Nizar Habash performed
most of his contribution to this paper while he was
at the Center for Computational Learning Systems
at Columbia University.
163
References
T. Buckwalter. 2007. Issues in Arabic Morphological
Analysis. In A. van den Bosch and A. Soudi, editors,
Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
D. Dahlmeier and H. T. Ng. 2012. Better evaluation
for grammatical error correction. In Proceedings of
NAACL.
A. El Kholy and N. Habash. 2012. Orthographic and
morphological processing for English?Arabic sta-
tistical machine translation. Machine Translation,
26(1-2).
N. Farra, N. Tomeh, A. Rozovskaya, and N. Habash.
2014. Generalized character-level spelling error cor-
rection. In Proceedings of ACL.
J. Fiscus. 1998. Sclite scoring package ver-
sion 1.5. US National Institute of Standard
Technology (NIST), URL http://www. itl. nist.
gov/iaui/894.01/tools.
N. Y. Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies 3.1.
M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA data
mining software: an update. SIGKDD Explorations,
11(1):10?18.
T. Kudo. 2005. YamCha: Yet another multipurpose
chunk annotator. http://chasen.org/ taku/software/.
M. Maamouri, D. Graff, B. Bouziri, S. Krouna, A. Bies,
and S. Kulick. 2010. LDC Standard Arabic Mor-
phological Analyzer (SAMA) Version 3.1. Linguistic
Data Consortium.
B. Mohit, A. Rozovskaya, N. Habash, W. Zaghouani,
and O. Obeid. 2014. The first QALB shared task on
automatic text correction for Arabic. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proceedings of
CoNLL: Shared Task.
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of CoNLL: Shared Task.
R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda.
2011. Arabic Gigaword Fifth Edition. Linguistic
Data Consortium.
A. Pasha, M. Al-Badrashiny, A. E. Kholy, R. Eskan-
der, M. Diab, N. Habash, M. Pooleery, O. Rambow,
and R. Roth. 2014. MADAMIRA: A fast, compre-
hensive tool for morphological analysis and disam-
biguation of arabic. In Proceedings of LREC.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks.
In Proceedings of ACL.
W. Salloum and N. Habash. 2011. Dialectal to stan-
dard arabic paraphrasing to improve arabic-english
statistical machine translation. In Proceedings of
the First Workshop on Algorithms and Resources for
Modelling of Dialects and Language Varieties.
W. Salloum and N. Habash. 2012. Elissa: A dialectal
to standard arabic machine translation system. In
Proceedings of COLING (Demos).
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Proceedings of International Con-
ference on Spoken Language Processing.
W. Zaghouani, B. Mohit, N. Habash, O. Obeid,
N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani,
and K. Oflazer. 2014. Large scale arabic error anno-
tation: Guidelines and framework. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC?14).
164
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 1?12,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Foreign Words and the Automatic Processing
of Arabic Social Media Text Written in Roman Script
Ramy Eskander, Mohamed Al-Badrashiny
?
, Nizar Habash
?
and Owen Rambow
Center for Computational Learning Systems, Columbia University
{reskander,rambow}@ccls.columbia.edu
?
Department of Computer Science, The George Washington University
?
badrashiny@gwu.edu
?
Computer Science Department, New York University Abu Dhabi
?
nizar.habash@nyu.edu
Abstract
Arabic on social media has all the prop-
erties of any language on social media
that make it tough for natural language
processing, plus some specific problems.
These include diglossia, the use of an
alternative alphabet (Roman), and code
switching with foreign languages. In this
paper, we present a system which can
process Arabic written in Roman alpha-
bet (?Arabizi?). It identifies whether each
word is a foreign word or one of an-
other four categories (Arabic, name, punc-
tuation, sound), and transliterates Arabic
words and names into the Arabic alphabet.
We obtain an overall system performance
of 83.8% on an unseen test set.
1 Introduction
Written language used in social media shows dif-
ferences from that in other written genres: the
vocabulary is informal (and sometimes the syn-
tax is as well); there are intentional deviations
from standard orthography (such as repeated let-
ters for emphasis); there are typos; writers use
non-standard abbreviations; non-linguistic sounds
are written (haha); punctuation is used creatively;
non-linguistic signs such as emoticons often com-
pensate for the absence of a broader communica-
tion channel in written communication (which ex-
cludes, for example, prosody or visual feedback);
and, most importantly for this paper, there fre-
quently is code switching. These facts pose a well-
known problem for natural language processing of
social media texts, which has become an area of
interest as applications such as sentiment analy-
sis, information extraction, and machine transla-
tion turn to this genre.
This situation is exacerbated in the case of Ara-
bic social media. There are three principal rea-
sons. First, the Arabic language is a collection of
varieties: Modern Standard Arabic (MSA), which
is used in formal settings, and different forms of
Dialectal Arabic (DA), which are commonly used
informally. This situation is referred to as ?diglos-
sia?. MSA has a standard orthography, while
the dialects do not. What is used in Arabic so-
cial media is typically DA. This means that there
is no standard orthography to begin with, result-
ing in an even broader variation in orthographic
forms found. Diglossia is seen in other linguistic
communities as well, including German-speaking
Switzerland, in the Czech Republic, or to a some-
what lesser extent among French speakers. Sec-
ond, while both MSA and DA are commonly writ-
ten in the Arabic script, DA is sometimes writ-
ten in the Roman script. Arabic written in Roman
is often called ?Arabizi?. It is common in other
linguistic communities as well to write informal
communication in the Roman alphabet rather than
in the native writing system, for example, among
South Asians. And third, educated speakers of
Arabic are often bilingual or near-bilingual speak-
ers of another language as well (such as English
or French), and will code switch between DA and
the foreign language in the same utterance (and
sometimes MSA as well). As is well known, code
switching is common in many linguistic commu-
nities, for example among South Asians.
In this paper, we investigate the issue of pro-
cessing Arabizi input with code switching. There
are two tasks: identification of tokens that are
not DA or MSA (and should not be transliterated
into Arabic script for downstream processing), and
then the transliteration into Arabic script of the
parts identified as DA or MSA. In this paper, we
1
use as a black box an existing component that we
developed to transliterate from Arabizi to Arabic
script (Al-Badrashiny et al., 2014). This paper
concentrates on the task of identifying which to-
kens should be transliterated. A recent release
of annotated data by the Linguistic Data Consor-
tium (LDC, 2014c; Bies et al., 2014) has enabled
novel research on this topic. The corpus pro-
vides each token with a tag, as well as a translit-
eration if appropriate. The tags identify foreign
words, as well as Arabic words, names, punctua-
tion, and sounds. Only Arabic words and names
are transliterated. (Note that code switching is not
distinguished from borrowing.) Emoticons, which
may be isolated or part of an input token, are also
identified, and converted into a conventional sym-
bol (#). This paper presents taggers for the tags,
and an end-to-end system which takes Arabizi in-
put and produces a complex output which consists
of a tag for each input token and a transliteration
of Arabic words and names into the Arabic script.
To our knowledge, this is the first system that han-
dles the complete task as defined by the LDC data.
This paper focuses on the task of identifying for-
eign words (as well as the other tags), on creating
a single system, and on evaluating the system as a
whole.
This paper makes three main contributions.
First, we clearly define the computational prob-
lem of dealing with social media Arabizi, and pro-
pose a new formulation of the evaluation metric
for the LDC corpus. Second, we present novel
modules for the detection of foreign words as well
as of emoticons, sounds, punctuation marks, and
names in Arabizi. Third, we compose a single sys-
tem from the various components, and evaluate the
complete system.
This paper is structured as follows. We start by
presenting related work (Section 2), and then we
present relevant linguistic facts and explain how
the data is annotated (Section 3). After summariz-
ing our system architecture (Section 4) and exper-
imental setup (Section 5), we present our systems
for tagging in Sections 6, 7 and 8. The evaluation
results are presented in Section 9.
2 Related Work
While natural language processing for English in
social media has attracted considerable attention
recently (Clark and Araki, 2011; Gimpel et al.,
2011; Gouws et al., 2011; Ritter et al., 2011; Der-
czynski et al., 2013), there has not been much
work on Arabic yet. We give a brief summary of
relevant work on Arabic.
Darwish et al. (2012) discuss NLP problems in
retrieving Arabic microblogs (tweets). They dis-
cuss many of the same issues we do, notably the
problems arising from the use of DA such as the
lack of a standard orthography. However, they do
not deal with DA written in the Roman alphabet
(though they do discuss non-Arabic characters).
There is some work on code switching be-
tween Modern Standard Arabic (MSA) and di-
alectal Arabic (DA). Zaidan and Callison-Burch
(2011) are interested in this problem at the inter-
sentence level. They crawl a large dataset of
MSA-DA news commentaries. They use Ama-
zon Mechanical Turk to annotate the dataset at the
sentence level. Then they use a language model-
ing approach to predict the class (MSA or DA)
for an unseen sentence. There is other work on
dialect identification, such as AIDA (Elfardy et
al., 2013; Elfardy et al., 2014). In AIDA, some
statistical and morphological analyses are applied
to capture code switching between MSA and DA
within the same sentence. Each word in the sen-
tence is tagged to be either DA or MSA based
on the context. The tagging process mainly de-
pends on the language modeling (LM) approach,
but if a word is unknown in the LM, then its tag is
assigned through MADAMIRA, a morphological
disambiguator Pasha et al. (2014).
Lui et al. (2014) proposed a system that does
language identification in multilingual documents,
using a generative mixture model that is based
on supervised topic modeling algorithms. This is
similar to our work in terms of identifying code
switching. However, our system deals with Ara-
bizi, a non-standard orthography with high vari-
ability, making the identification task much harder.
Concerning specifically NLP for Arabizi, Dar-
wish (2013) (published in an updated version as
(Darwish, 2014)) is similar to our work in that
he identifies English in Arabizi text and he also
transliterates Arabic text from Arabizi to Arabic
script. We compare our transliteration method to
his in Al-Badrashiny et al. (2014). For identifi-
cation of non-Arabic words in Arabizi, Darwish
(2013) uses word and sequence-level features with
CRF modeling; while we use SVMs and decision
trees. Darwish (2013) identifies three tags: Ara-
bic, foreign and others (such as email addresses
and URLs). In contrast, we identify a bigger
set: Arabic, foreign, names, sounds, punctuation
2
and emoticons. Furthermore, Darwish (2013) uses
around 5K words for training his taggers and 3.5K
words for testing; this is considerably smaller than
our training and test sets of 113K and 32K words,
respectively.
Chalabi and Gerges (2012) presented a hybrid
approach for Arabizi transliteration. Their work
does not address the detection of English words,
punctuation, emoticons, and so on. They also do
not handle English when mixed with Arabizi.
Voss et al. (2014) deal with exactly the prob-
lem of classifying tokens in Arabizi as Arabic or
not. More specifically, they deal with Moroccan
Arabic, and with both French and English, mean-
ing they do a three-way classification. There are
many differences between our work and theirs:
they have noisy training data, and they have a
much more balanced test set. They also only deal
with foreignness, and do not address the other tags
we deal with, nor do they actually discuss translit-
eration itself.
3 Linguistic Facts and Data Annotation
3.1 Arabizi
Arabizi refers to Arabic written using the Roman
script (Darwish, 2013; Voss et al., 2014). Ara-
bizi orthography is spontaneous and has no stan-
dard references, although there are numerous com-
monly used conventions making specific usage of
the so-called Arabic numerals and punctuation in
addition to Roman script letters. Arabizi is com-
monly used by Arabic speakers to write mostly in
dialectal Arabic in social media, SMS and chat ap-
plications.
Arabizi orthography decisions mainly depend
on a phoneme-to-grapheme mapping between the
Arabic pronunciation and the Roman script. This
is largely based on the phoneme-to-grapheme
mapping used in English (in Middle Eastern Arab
countries) or French (in Western North African
Arab countries). Since there is no standard or-
thography for Arabizi, it is not a simple translit-
eration of Arabic. For example, in Arabizi, words
omit vowels far less frequently than is done when
writers follow standard Arabic orthography. Fur-
thermore, there are several cases of many-to-many
mappings between Arabic phonemes and Roman
script letters: for example, the letter ?t? is used to
represent the sound of the Arabic letters

H t
1
and
1
Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al., 2007): (in alphabetical
? T (which itself can be also be represented using
the digit ?6?).
Text written in Arabizi also tends to have a large
number of foreign words, that are either borrow-
ings such as telephone, or code switching, such
as love you!. Note that Arabizi often uses the
source language orthography for borrowings (es-
pecially recent borrowings), even if the Arabic
pronunciation is somewhat modified. As a re-
sult, distinguishing borrowings from code switch-
ing is, as is usually the case, hard. And, as in any
language used in social media and chat, Arabizi
may also include abbreviations, such as isa which
means ?<? @ Z A

?
	
?@

?
An ?A? Allh ?God willing? and
lol ?laugh out loud?.
The rows marked with Arabizi in Figure 1
demonstrate some of the salient features of Ara-
bizi. The constructed example in the figure is of
an SMS conversation in Egyptian Arabic.
3.2 Data Annotation
The data set we use in this paper was created by
the Linguistic Data Consortium (Bies et al., 2014;
LDC, 2014a; LDC, 2014b; LDC, 2014c). We
summarize below the annotation decisions. The
system we present in this paper aims at predicting
exactly this annotation automatically. The input
text is initially segregated into Arabic script and
Arabizi. Arabic script text is not modified in any
way. Arabizi text undergoes two sets of annotation
decisions: Arabizi word tagging and Arabizi-to-
Arabic transliteration. All of the Arabizi annota-
tions are initially done using an automatic process
(Al-Badrashiny et al., 2014) and then followed by
manual correction and validation.
Arabizi Word Tagging Each Arabizi word re-
ceives one of the following five tags:
? Foreign All words from languages other than
Arabic are tagged as Foreign if they would
be kept in the same orthographic form when
translated into their source language (which
in our corpus is almost always English).
Thus, non-Arabic words that include Arabic
affixes are not tagged as Foreign. The defini-
tion of ?foreign? thus means that uninflected
borrowings spelled as in the source language
orthography are tagged as ?foreign?, while
borrowings that are spelled differently, as
well as borrowing that have been inflected
order) Abt?jHxd?rzs?SDT
?
D??fqklmnhwy and the additional
symbols: ? Z, ?

@,
?
A @

,
?
A

@, ?w

?', ?y Z?', h?

?, ? ?.
3
(1) Arabizi Youmna i need to know anti gaya wala la2 ?
Tag Name Foreign Foreign Foreign Foreign Arabic Arabic Arabic Arabic Punct
Arabic ?
	
??
?

?


@ YJ


	
K ?

K ?
	
K ?



?
	
K @

?K


Ag
.
B?

B ?
ymn? Ay nyd tw nw Anty jAyh? wlA l? ?
English Youmna I need to know you coming or not ?
(2) Arabizi Mmmm ok ana 7aseb el sho3?l now w ageelk isa :-)
Tag Sound Foreign Arabic Arabic Arabic Arabic Foreign Arabic Arabic Arabic Arabic
Arabic ??
?
?J


??@ A
	
K @ I
.



?Ag [+]?@ ?
	
?

? ?A
	
K [+]? ??[-]?


k
.
@ ?<? @[-]ZA

?[-]
	
?@ #
mmm Awkyh AnA HAsyb Al[+] ?gl nAw w[+] Ajy[-]lk An[-]?A?[-]Allh #
English mmm OK I will-leave the work now and I-come-to-you God-willing :-)
(3) Arabizi qishta!:D
Tag Arabic
Arabic #[-]!

??

?

?
q?Th?![-]#
English cream!:D (slang for cool!)
Figure 1: A short constructed SMS conversation written in Arabizi together with annotation of word
tags and transliteration into Arabic script. A Romanized transliteration of the Arabic script and English
glosses are provided for clarity. The cells with gray background are the parts of the output that we
evaluate.
following Arabic morphology, are not tagged
as ?foreign? (even if the stem is spelled as in
the source language, such as Almobile). The
Arabic transliterations of these words are not
manually corrected.
? Punct Punctuation marks are a set of conven-
tional signs that are used to aid interpretation
by indicating division of text into sentences
and clauses, etc. Examples of punctuation
marks are the semicolon ;, the exclamation
mark ! and the right brace }. Emoticons are
not considered punctuation and are handled
as part of the transliteration task discussed
below.
? Sound Sounds are a list of interjections that
have no grammatical meaning, but mimic
non-linguistic sounds that humans make, and
that often signify emotions. Examples of
sounds are hahaha (laughing), hmm (wonder-
ing) and eww (being disgusted). It is common
to stretch sounds out to make them stronger,
i.e., to express more intense emotions. For
example, hmm could be stretched out into
hmmmmm to express a stronger feeling of
wondering. The Arabic transliterations of
these words are not manually corrected.
? Name Proper names are tagged as such and
later manually corrected.
? Arabic All other words are tagged as Arabic
and are later manually corrected.
See the rows marked with Tag in Figure 1 for
examples of these different tags. It is impor-
tant to point out that the annotation of this data
was intended to serve a project focusing on ma-
chine translation from dialectal Arabic into En-
glish. This goal influenced some of the annotation
decisions and was part of the reason for this selec-
tion of word tags.
Arabizi-to-Arabic Transliteration The second
annotation task is about converting Arabizi to an
Arabic-script-based orthography. Since, dialectal
Arabic including Egyptian Arabic has no standard
orthography in Arabic script, the annotation uses a
conventionalized orthography for Dialectal Arabic
called CODA (Habash et al., 2012a; Eskander et
al., 2013; Zribi et al., 2014). Every word has a
single orthographic representation in CODA.
In the corpus we use, only words tagged as
Arabic or Name are manually checked and cor-
rected. The transliteration respects the white-
space boundaries of the original Arabizi words. In
cases where an Arabizi word represents a prefix
or suffix that should be joined in CODA to the
next or previous word, a [+] symbol is added to
mark this decision. Similarly, for Arabizi words
that should be split into multiple CODA words,
the CODA words are written with added [-] sym-
bol delimiting the word boundaries.
The Arabic transliteration task also includes
handling emoticons. Emoticons are digital icons
or sequences of keyboard symbols serving to rep-
resent facial expressions or to convey the writer?s
emotions. Examples of emoticons are :d, :-(, O.O
and ? used to represent laughing, sadness, being
surprised and positive emotion, respectively. All
emoticons, whether free-standing or attached to a
4
word, are replaced by a single hash symbol (#).
Free-standing emoticons are tagged as Arabic. At-
tached emoticons are not tagged separately; the
word they are attached to is tagged according to
the usual rules. See Figure 1 for examples of these
different decisions.
Since words tagged as Foreign, Punct, or Sound
are not manually transliterated in the corpus, in
our performance evaluation we combine the de-
cisions of tags and transliteration. For foreign
words, punctuation and sounds, we only consider
the tags for accuracy computations; in contrast, for
names and Arabic words, we consider both the tag
and transliteration.
4 System Architecture
Figure 2 represent the overall architecture of our
system. We distinguish below between existing
components that we use and novel extensions that
we contribute in this paper.
4.1 Existing Arabization System
For the core component of Arabizi-to-Arabic
transliteration, we use a previously published sys-
tem (Al-Badrashiny et al., 2014), which converts
Arabizi into Arabic text following CODA conven-
tions (see Section 3). The existing system uses a
finite state transducer trained on 8,500 Arabizi-to-
Arabic transliteration pairs at the character level to
obtain a large number of possible transliterations
for the input Arabizi words. The generated list is
then filtered using a dialectal Arabic morphologi-
cal analyzer. Finally, the best choice for each input
word is selected using a language model. We use
this component as a black box except that we re-
train it using additional training data. In Figure 2,
this component is represented using a central black
box.
4.2 Novel Extension
In this paper, we add Word Type Tagging as a
new set of modules. We tag the Arabizi words into
five categories as discussed above: Arabic, For-
eign, Names, Sounds, and Punctuation. Figure 2
illustrates the full proposed system. First, we pro-
cess the Arabizi input to do punctuation and sound
tagging, along with emoticon detection. Then we
run the transliteration system to produce the cor-
responding Arabic transliteration. The Arabizi in-
put and Arabic output are then used together to
do name tagging and foreign word tagging. The
Arabic tag is assigned to all untagged words, i.e.,
words not tagged as Foreign, Names, Sounds, or
Punctuation. The outputs from all steps are then
combined to produce the final Arabic translitera-
tion along with the tag.
5 Experimental Setup
5.1 Data Sets
We define the following sets of data:
? Train-S: A small size dataset that is used to
train all taggers in all experiments to deter-
mine the best performing setup (feature engi-
neering).
? Train-L: A larger size dataset that is used to
train the best performing setup.
? Dev: The development set that is used to
measure the system performance in all exper-
iments
? Test: A blind set that is used to test the best
system (LDC, 2014a).
The training and development sets are extracted
from (LDC, 2014b). Table 1 represents the tags
distribution in each dataset. Almost one of every
five words is not Arabic text and around one of
every 10 words is foreign.
5.2 Arabizi-to-Arabic Transliteration
Accuracy
For the Arabizi-to-Arabic transliteration system,
we report on using the two training data sets
with two modifications. First, we include the
8,500 word pairs from Al-Badrashiny et al. (2014),
namely 2,200 Arabizi-to-Arabic script pairs from
the training data used by Darwish (2013) (man-
ually revised to be CODA-compliant) and about
6,300 pairs of proper names in Arabic and En-
glish from the Buckwalter Arabic Morphologi-
cal Analyzer (Buckwalter, 2004). (Since these
pairs are not tagged, we do not use them to train
the taggers.) Second, we exclude all the foreign
tagged words from training the transliteration sys-
tem since they were not manually corrected.
Table 2 shows the overall transliteration accu-
racy of Arabic words and names only, using dif-
ferent training data sets and evaluating on Dev
(as determined by the gold standard). The ac-
curacy when using the original Arabizi-to-Arabic
transliteration system from Al-Badrashiny et al.
(2014) gives an accuracy of 68.6%. Retraining it
on Train-S improves the accuracy to 76.9%. The
accuracy goes up further to 79.5% when using the
5
Arabizi-to-Arabic 
Transliteration Arabizi 
Punctuation, 
Sound & 
Emoticon 
Detection 
Name Tagger 
Foreign Word 
Tagger 
Combiner 
Arabic  
& 
Tagged 
Arabizi 
Figure 2: The architecture of our complete Arabizi processing system. The "Punctuation, Sound and
Emoticon Detection" component does labeling that is read by the "Name" and "Foreign Word" taggers,
While the actual Aribizi-to-Arabic transliteration system is used as a black box.
Data # Words Arabic Foreign Name Sound Punct Emoticon
Train-S 21,950 80.5% 12.1% 2.8% 1.7% 1.3% 1.6%
Train-L 113,490 82.3% 9.8% 2.4% 1.8% 1.1% 2.6%
Dev 5,061 76.3% 16.2% 2.9% 1.8% 1.2% 1.5%
Test 31,717 86.1% 6.0% 2.7% 1.6% 0.9% 2.8%
Table 1: Dataset Statistics
Data Translit. Acc.
Al-Badrashiny et al. (2014) 68.6%
Train-S 76.9%
Train-L 79.5%
Table 2: Transliteration accuracy of Arabic words
and names when using different training sets and
evaluating on Dev
bigger training set Train-L. The overall transliter-
ation accuracy of Arabic words and names on Test
using the bigger training set Train-L is 83.6%.
6 Tagging Punctuation, Emoticons and
Sounds
6.1 Approach
We start the tagging process by detecting three
types of closed classes: punctuation, sounds and
emoticons. Simple regular expressions perform
very well at detecting their occurrence in text. The
regular expressions are applied to the Arabizi in-
put, word by word, after lower-casing, since both
emoticons and sounds could contain either small
or capital letters.
Since emoticons can be composed of just con-
catenated punctuation marks, their detection is re-
quired before punctuation is tagged. Once de-
tected, emoticons are replaced by #. Then punctu-
ation marks are detected. If a non-emoticon word
is only composed of punctuation marks, then it
gets tagged as Punct. Sounds are targeted next.
A word gets tagged as Sound if it matches the
sound detection expression, after stripping out any
attached punctuation marks and/or emoticons.
6.2 Results
Table 6 in Section 9 shows the accuracy, recall,
precision and F-score for the classification of the
Punct and Sound tags and detection of emoticons.
Since emoticons can be part of another word, and
in that case do not receive a specific tag (as spec-
ified in the annotation guidelines by the LDC),
emoticon evaluation is concerned with the num-
ber of detected emoticons within an Arabizi word,
as opposed to a binary tagging decision. In other
words, emoticon identification is counted as cor-
rect (?positive?) if the number of detected emoti-
cons in a word is correct in the test token. The
Punct and Sound tags represent standard binary
classification tasks and are evaluated in the usual
way.
7 Tagging Names
7.1 Approach
We consider the following set of binary features
for learning a model of name tagging. The fea-
tures are used either separately or combined using
a modeling classifier implemented with decision
trees.
? Capitalization A word is considered a name
if the first letter in Arabizi is capitalized.
6
? MADAMIRA MADAMIRA is a system for
morphological analysis and disambiguation
of Arabic (Pasha et al., 2014). We run
MADAMIRA on the Arabic output after run-
ning the Arabizi-to-Arabic transliteration. If
the selected part-of-speech (POS) of a word
is proper noun (NOUN_PROP), then the
word is tagged as Name.
? CALIMA CALIMA is a morphological an-
alyzer for Egyptian Arabic (Habash et al.,
2012b). If the Arabic transliteration of a
given Arabizi word has a possible proper
noun analysis in CALIMA, then the word is
tagged as Name.
? Maximum Likelihood Estimate (MLE) An
Arabizi word gets assigned the Name tag if
Name is the most associated tag for that word
in the training set.
? Tharwa Tharwa is a large scale Egyptian
Arabic-MSA-English lexicon that includes
POS tag information (Diab et al., 2014). If
an Arabizi word appears in Tharwa as an En-
glish gloss with a proper noun POS, then it is
tagged as Name.
? Name Language Model We use a list of
280K unique lower-cased English words as-
sociated with their probability of appearing
capitalized (Habash, 2009). When using this
feature, any probability that is not equal to
one is rounded to zero.
All the features above are modeled after case-
lowering the Arabizi input, and removing speech
effects. Any attached punctuation marks and/or
emoticons are stripped out. One exception is the
capitalization feature, where the case of the first
letter of the Arabizi word is preserved. The tech-
niques above are then combined together using de-
cision trees. In this approach, the words tagged as
Name are given a weight that balances their infre-
quent occurrence in the data.
7.2 Results
Table 3 shows the performance of the Name tag-
ging on Dev using Train-S. The best results are
obtained when looking up the MLE value in the
training data, with an accuracy and F-score of
97.8% and 56.0%, respectively. When using
Train-L, the accuracy and F-score given by MLE
go up to 98.1% and 63.9%, respectively. See Ta-
ble 6. The performance of the combined approach
Feature Accuracy Recall Precision F-Score
Capitalization 85.6 28.3 6.4 10.4
MADAMIRA 95.9 24.8 28.3 26.5
CALIMA 86.3 50.3 10.9 17.9
MLE 97.8 46.9 69.4 56.0
THARWA 96.3 22.8 33.0 26.9
NAME-LM 84.5 30.3 6.3 10.4
All Combined 97.7 49.7 63.2 55.6
(Decision Trees)
Table 3: Name tagging results on Dev with Train-S
does not outperform the most effective single clas-
sifier, MLE. This is because adding other features
decreases the precision by an amount that exceeds
the increase in the recall.
8 Tagging Foreign Words
As shown earlier, around 10% of all words in Ara-
bizi text are foreign, mostly English in our data set.
Tagging foreign words is challenging since there
are many words that can be either Arabic (in Ara-
bizi) or a word in a foreign languages. For exam-
ple the Arabizi word mesh can refer to the English
reading or the Arabic word

?? m? ?not?. There-
fore, simple dictionary lookup is not sufficient to
determine whether a word is Arabic or Foreign.
Our target in this section is to identify the foreign
words in the input Arabizi text .
8.1 Baseline Experiments
We define a foreignness index formula that gives
each word a score given its unigram probabili-
ties against Arabic and English language models
(LMs).
? (w) = ?P
E
(w) + (1? ?) (1? P
A
(w
t
)) (1)
?(w) is the foreignness score of the Arabizi word
w. P
E
(w) is the unigram probability of w in the
English LM, and P
A
(w
t
) is the unigram proba-
bility in the Arabic LM of the transliteration into
Arabic (w
t
) proposed by our system for the Ara-
bizi word w. ? is a tuning parameter varying from
zero to one. From equation 1 we define the mini-
mum and maximum ? values as follows:
?
min
= ?P
E
min
+ (1? ?) (1? P
A
max
)
?
max
= ?P
E
max
+ (1? ?) (1? P
A
min
)
(2)
Where P
E
min
and P
E
max
are the minimum and
maximum uni-gram probabilities in the English
LM. And P
A
min
and P
A
max
are the minimum
7
and maximum uni-gram probabilities in the Ara-
bic LM. The foreignness index Foreignness(w)
is the normalized foreignness score derived using
equations 1 and 2 as follow:
Foreignness (w) =
? (w)? ?
min
?
max
? ?
min
(3)
If the foreignness index of a word is higher than
a certain threshold ?, we consider the word For-
eign. We define three baseline experiments as fol-
lows:
? FW-index-manual: Use brute force search
to find the best ? and ? that maximize the
foreign words tagging on Dev.
? FW-index-SVM: Use the best ? from above
and train an SVM model using the foreign-
ness index as sole feature. Then use this
model to classify each word in Dev.
? LM-lookup: The word is said to be Foreign
if it exists in the English LM and does not
exist in the Arabic LM.
8.2 Machine Learning Experiments
We conducted a suite of experiments by train-
ing different machine learning techniques using
WEKA (Hall et al., 2009) on the following groups
of features. We performed a two-stage feature ex-
ploration, where we did an exhaustive search over
all features in each group in the first phase, and
then exhaustively searched over all retained fea-
ture groups. In addition, we also performed an ex-
haustive search over all features in the first three
groups.
? Word n-gram features: Run the input Ara-
bizi word through an English LM and the cor-
responding Arabic transliteration through an
Arabic LM to get the set of features that are
defined in "Group1" in Table 4. Then find the
best combination of features that maximizes
the F-score on Dev.
? FW-char-n-gram features: Run the input
Arabizi word through a character-level n-
gram LM of the Arabizi words that are tagged
as foreign in the training data. We get the set
of features that are defined in "Group2" in Ta-
ble 4. Then find the best feature combination
from this group that maximizes the F-score
on Dev.
? AR-char-n-gram features: Run the input
Arabizi word through a character-level n-
gram LM of the Arabizi words that are tagged
Group Description
Group1 Uni and bi-grams probabilities from English and
Arabic LMs
Group2 1,2,3,4, and 5 characters level n-grams of foreign
words
Group3 1,2,3,4, and 5 characters level n-grams of Arabic
words
Use the Arabizi word itself as a feature
Group4 Was the input Arabizi word tagged as foreign in
the gold training data?
Was the input Arabizi word tagged as Arabic in the
gold training data?
Does the input word has speech effects?
Group5 Word length
Is the Arabizi word capitalized?
Table 4: List of the different features that are used
in the foreign word tagging
as non-foreign in the training data. We get the
set of features that are defined in "Group3" in
Table 4. Then find the best feature that maxi-
mizes the F-score on Dev.
? Word identity: Use the input Arabizi word
to get all features that are defined in "Group4"
in Table 4. Then find the best combination of
features that maximizes the F-score on Dev.
? Word properties: Use the input Arabizi
word to get all features that are defined in
"Group5" in Table 4. Then find the best com-
bination of features that maximizes the F-
score on Dev.
? Best-of-all-groups: Use the best selected set
of features from each of the above experi-
ments. Then find the best combination of
these features that maximizes the F-score on
Dev.
? All-features: Use all features from all
groups.
? Probabilistic-features-only: Find the best
combination of features from "Group1",
"Group2", and "Group3" in Table 4 that max-
imizes the F-score on Dev.
8.3 Results
Table 5 shows the results on Dev using Train-S.
It can be seen that the decision tree classifier is
doing better than the SVM except in the "Word
properties" and "All-features" experiments. The
best performing setup is "Probabilistic-features-
only" with decision trees which has 87.3% F-
score. The best selected features are EN-Unigram,
AR-char-2-grams, FW-char-1-grams, FW-char-2-
grams, FW-char-5-grams.
8
Experiment Recall Precision F-Score Classifier Selected Features
LM-lookup 7.6 95.4 14.1
FW-index-manual 75.0 51.0 60.7 ? =0.8 , ? = 0.23
FW-index-SVM 4.0 89.0 7.7 SVM
Word n-gram features 76.7 73.2 74.9 AR-unigram, EN-unigram
AR-char-n-gram features 55.4 34.8 42.8 AR-char-4-grams
FW-char-n-gram features 42.4 52.2 46.8 FW-char-3-grams
Word properties 2.4 28.6 4.5 Has-speech-effect, Word-length, Is-capitalized
Word identity 70.3 63.0 66.4 SVM FW-tagged-list
Best-of-all-groups 82.1 76.1 79.0 AR-unigram, EN-unigram, Word-length
All-features 69.4 87.7 77.5 All features from all groups
Probabilistic-features-only 84.5 80.6 82.5 AR-unigram, EN-unigram, AR-char-3-grams, FW-
char-3-grams
Word n-gram features 82.8 80.5 81.6 AR-unigram, EN-unigram
AR-char-n-gram features 80.6 63.2 70.8 AR-char-5-grams
FW-char-n-gram features 73.8 76.3 75.0 FW-char-3-grams
Word properties 1.9 25.4 3.6 Has-speech-effect, Word-length
Word identity 73.2 60.9 66.5 Decision-Tree FW-tagged-list
Best-of-all-groups 87.0 81.5 84.1 AR-unigram, EN-unigram, AR-char-5-grams, FW-
char-3-grams
All-features 92.0 53.4 67.6 All features from all groups
Probabilistic-features-
only
89.9 84.9 87.3 EN-Unigram, AR-char-2-grams, FW-char-1-
grams, FW-char-2-grams, FW-char-5-grams
Table 5: Foreign words tagging results on Dev in terms of F-score (%).
9 System Evaluation
9.1 Development and Blind Test Results
We report the results on Dev using Train-L and
with the best settings determined in the previous
three sections. Table 6 summarizes the recall, pre-
cision and F-score results for the classification of
the Punct, Sound, Foreign, Name and Arabic tags,
in addition to emoticon detection.
We report our results on Test, our blind test
set, using Train-L and with the best settings de-
termined in the previous three sections in Table 7.
The punctuation, sounds and emoticons have
high F-scores but lower than expected. This is
likely due to the limitations of the regular expres-
sions used. The performance on these tags drops
further on the test set. A similar drop is seen for
the Foreign tag. Name is the hardest tag overall.
But it performs slightly better in test compared to
the development set, and so does the Arabic tag.
Tag Accuracy Recall Precision F-Score
Punct 99.8 100.0 88.7 94.0
Sound 99.4 93.5 78.9 85.6
Foreign 95.8 91.6 84.0 87.6
Name 98.1 57.5 71.8 63.9
Arabic 94.5 95.6 97.3 96.4
Emoticon 100.0 97.5 98.7 98.1
Detection
Table 6: Tagging results on Dev using Train-L
Tag Accuracy Recall Precision F-Score
Punct 99.8 98.2 80.1 88.3
Sound 99.3 87.4 74.2 80.3
Foreign 96.5 92.3 64.3 75.8
Name 98.6 53.7 90.2 67.3
Arabic 95.4 96.3 98.5 97.4
Emoticon 99.2 85.3 93.6 89.3
Detection
Table 7: Tagging results on Test using Train-L
9.2 Overall System Evaluation
In this subsection we report on evaluating the over-
all system accuracy. This includes the correct tag-
ging and Arabizi to Arabic transliteration. How-
ever, since there is no manually annotated gold
transliteration for foreign words, punctuation, or
sounds into Arabic, we cannot compare the system
transliteration of foreign words to the gold translit-
eration. Thus, we define the following metric to
judge the overall system accuracy.
Overall System Accuracy Metric A word is
said to be correctly transliterated according to the
following rules:
1. If the gold tag is anything other than Arabic
and Name, the produced tag must match the
gold tag.
2. If the gold tag is either Arabic or Name, the
produced tag and the produced transliteration
must both match the gold.
9
Data Baseline Accuracy System Accuracy
Dev 65.7% 82.5%
Test 76.8% 83.8%
Table 8: Baseline vs. System Accuracy
Tag
Gold Errors System Errors
Typos
Not Over Not Over
Tagged generated Tagged generated
Punct 100.0 0.0 0.0 0.0 0.0
Sound 79.3 10.3 10.3 0.0 0.0
Foreign 47.2 1.9 12.3 20.3 18.4
Name 26.3 13.7 45.3 8.4 6.3
Table 9: Error Analysis of tag classification errors
As a baseline, we use the most frequent tag,
which is Arabic in our case, along with the translit-
eration of the word using our black box system.
Then we apply the above evaluation metric on both
Dev and Test. The results are shown in table 8. The
baseline accuracies on Dev and Test are 65.7% and
76.8% respectively. By considering the actual out-
put of our system, the accuracy on the Dev and Test
data increases to 82.5% and 83.8% respectively.
9.3 Error Analysis
We conducted an error analysis for tag classifica-
tion on the development set. The analysis is done
for the tags that we built models for, which are
Punct, Sound, Foreign and Name.
2
Table 9 shows
the different error types for classifying the tags.
Tagging errors could be either gold errors or sys-
tem errors. These errors could be either due to
tag over-generation or because the correct tag is
not detected. Additionally, there are typos in the
input Arabizi that sometimes prevent the system
from assigning the correct tags. Gold errors con-
tribute to a large portion of the tagging errors, rep-
resenting 100.0%, 89.6%, 49.1% and 40.0% for
the Punct, Sound, Foreign and Name tags, respec-
tively.
10 Conclusion and Future Work
We presented a system for automatic processing of
Arabic social media text written in Roman script,
or Arabizi. Our system not only transliterates the
Arabizi text in the Egyptian Arabic dialect but also
classifies input Arabizi tokens as sounds, punc-
tuation marks, names, foreign words, or Arabic
words, and detects emoticons. We define a new
2
As mentioned in Section 4, the Arabic tag is assigned to
any remaining untagged words after running the classification
models.
task-specific metric for evaluating the complete
system. Our best setting achieves an overall per-
formance accuracy of 83.8% on a blind test set.
In the future, we plan to extend our work to
other Arabic dialects and other language contexts
such as Judeo-Arabic (Arabic written in Hebrew
script with code switching between Arabic and
Hebrew). We plan to explore the use of this com-
ponent in the context of specific applications such
as machine translation from Arabizi Arabic to En-
glish, and sentiment analysis in social media. We
also plan to make the system public so it can be
used by other people working on Arabic NLP tasks
related to Arabizi.
Acknowledgement
This paper is based upon work supported by
DARPA Contract No. HR0011-12-C-0014. Any
opinions, findings and conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of
DARPA. Nizar Habash performed most of his con-
tribution to this paper while he was at the Center
for Computational Learning Systems at Columbia
University.
References
Mohamed Al-Badrashiny, Ramy Eskander, Nizar
Habash, and Owen Rambow. 2014. Automatic
Transliteration of Romanized Dialectal Arabic. In
Proceedings of the Eighteenth Conference on Com-
putational Natural Language Learning, pages 30?
38, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Ann Bies, Zhiyi Song, Mohamed Maamouri, Stephen
Grimes, Haejoong Lee, Jonathan Wright, Stephanie
Strassel, Nizar Habash, Ramy Eskander, and Owen
Rabmow. 2014. Transliteration of Arabizi into Ara-
bic Orthography: Developing a Parallel Annotated
Arabizi-Arabic Script SMS/Chat Corpus. In Arabic
Natural Language Processing Workshop, EMNLP,
Doha, Qatar.
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Achraf Chalabi and Hany Gerges. 2012. Romanized
Arabic Transliteration. In Proceedings of the Sec-
ond Workshop on Advances in Text Input Methods
(WTIM 2012).
Eleanor Clark and Kenji Araki. 2011. Text normal-
ization in social media: Progress, problems and ap-
plications for a pre-processing system of casual en-
glish. Procedia - Social and Behavioral Sciences,
27(0):2 ? 11. Computational Linguistics and Re-
lated Fields.
10
Kareem Darwish, Walid Magdy, and Ahmed Mourad.
2012. Language Processing for Arabic Microblog
Retrieval. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management, CIKM ?12, pages 2427?2430, New
York, NY, USA. ACM.
Kareem Darwish. 2013. Arabizi Detection and Con-
version to Arabic. CoRR.
Kareem Darwish. 2014. Arabizi Detection and Con-
version to Arabic . In Arabic Natural Language Pro-
cessing Workshop, EMNLP, Doha, Qatar.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In
Proceedings of the International Conference Recent
Advances in Natural Language Processing RANLP
2013, pages 198?206, Hissar, Bulgaria, September.
INCOMA Ltd. Shoumen, BULGARIA.
Mona Diab, Mohamed Al-Badrashiny, Maryam
Aminian, Mohammed Attia, Pradeep Dasigi, Heba
Elfardy, Ramy Eskander, Nizar Habash, Abdelati
Hawwari, and Wael Salloum. 2014. Tharwa: A
Large Scale Dialectal Arabic - Standard Arabic - En-
glish Lexicon. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Reyk-
javik, Iceland.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code Switch Point Detection in Arabic.
In Proceedings of the 18th International Conference
on Application of Natural Language to Information
Systems (NLDB2013), MediaCity, UK, June.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2014. AIDA: Identifying Code Switching
in Informal Arabic Text. In Workshop on Compu-
tational Approaches to Linguistic Code Switching,
EMNLP, Doha, Qatar, October.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing Spontaneous Or-
thography. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), Atlanta, GA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2,
HLT ?11, pages 42?47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Stephan Gouws, Donald Metzler, Congxing Cai, and
Eduard Hovy. 2011. Contextual bearing on lin-
guistic variation in social media. In Proceedings of
the Workshop on Languages in Social Media, LSM
?11, pages 20?29, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalu-
ation Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In Proceedings of the Twelfth Meeting of the
Special Interest Group on Computational Morphol-
ogy and Phonology, pages 1?9, Montr?al, Canada.
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine trans-
lation. In Khalid Choukri and Bente Maegaard, ed-
itors, Proceedings of the Second International Con-
ference on Arabic Language Resources and Tools.
The MEDAR Consortium, April.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
LDC. 2014a. BOLT Phase 2 SMS and Chat Ara-
bic DevTest Data ? Source Annotation, Translit-
eration and Translation. LDC catalog number
LDC2014E28.
LDC. 2014b. BOLT Phase 2 SMS and Chat Ara-
bic Training Data ? Source Annotation, Translit-
eration and Translation R1. LDC catalog number
LDC2014E48.
LDC. 2014c. BOLT Program: Romanized Arabic
(Arabizi) to Arabic Transliteration and Normaliza-
tion Guidelines. Version 3. Linguistic Data Consor-
tium.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. In Proceedings of LREC.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan M. Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Clare Voss, Stephen Tratz, Jamal Laoudi, and Dou-
glas Briesch. 2014. Finding Romanized Arabic
Dialect in Code-Mixed Tweets. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Hrafn Loftsson, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios
11
Piperidis, editors, Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC?14), Reykjavik, Iceland, may. Euro-
pean Language Resources Association (ELRA).
Omar F Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of ACL, pages 37?41.
Ines Zribi, Rahma Boujelbane, Abir Masmoudi,
Mariem Ellouze, Lamia Belguith, and Nizar Habash.
2014. A Conventional Orthography for Tunisian
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.
12
