A Web-based Instructional Platform for Constraint-Based Grammar
Formalisms and Parsing
W. Detmar Meurers
Dept. of Linguistics
Ohio State University
dm@ling.osu.edu
Gerald Penn
Dept. of Computer Science
University of Toronto
gpenn@cs.toronto.edu
Frank Richter
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
fr@sfs.uni-tuebingen.de
Abstract
We propose the creation of a web-based
training framework comprising a set of
topics that revolve around the use of fea-
ture structures as the core data structure
in linguistic theory, its formal foundations,
and its use in syntactic processing.
1 Introduction
Feature structures have been used prolifically at ev-
ery level of linguistic theory, and they form the
mathematical foundation of our most comprehen-
sive and rigorous schools of syntactic theory, includ-
ing Lexical-Functional Grammar and Head-driven
Phrase Structure Grammar. This data structure is
popular because it shares many properties with the
first-order terms of classical logic, and in addi-
tion provides named access to substructures through
paths of features. Often it also includes a type sys-
tem reminiscent of the taxonomical classification
systems that are widely used in knowledge represen-
tation, psychology and the natural sciences.
For teaching a subject like computational linguis-
tics, which draws on a broad curriculum from many
traditional disciplines to audiences with mixed back-
grounds themselves, feature-structure-based theo-
retical and computational linguistics have three im-
portant properties. First, they are a mature disci-
pline, in which a great deal of accomplishments have
been made over the last 20 years, spanning from em-
pirical and conceptual advances in linguistic theory
to its mathematical and computational foundations,
to grammar development and efficient processing.
Second, they are pervasive as an already existing
representation standard for many levels of linguistic
study. Third, they are transparent, reducing com-
plex theories of grammar to a basic collection of
mathematical concepts and algorithms for answer-
ing formal questions about those theories. One can
address the distinction between descriptions of ob-
jects and the objects themselves, the difference be-
tween consistency and truth, and what it means for a
syntactic theory to be not only elegant but correct in
a precise and provable sense.
The purpose of this paper is to discuss how these
three properties can be cast into an instructional set-
ting to arrive at a framework for teaching computa-
tional linguistics that highlights the integrated nature
and precision with which work in this very hetero-
geneous discipline can be presented. In principle,
the framework we are proposing is open-ended, in
the sense that additional modules should be added
by students and other researchers, subject to the de-
sign principles given in Section 3. We are currently
designing three of the core modules for this frame-
work: formal foundations, constraint-based gram-
mar implementation, and parsing.
2 Problems of seminar-style courses
The contents of our core modules are based on a
series of previous seminar-style courses, in partic-
ular on constraint-based grammar implementation,
which also started integrating interactive compo-
nents and web-based materials into traditional face-
to-face teaching. These are described in detail in
Section 5. The traditional seminar-style teaching
method underlying the courses mentioned therein
                     July 2002, pp. 19-26.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
has a number of inherent problems, however. These
problems become particularly pressing when topics
as diverse as linguistic theory, grammar implemen-
tation, parsing, mathematical foundations of linguis-
tic theory and feature logics are combined in a single
course that is addressed to a mixed audience with
varying backgrounds in computer science, knowl-
edge representation, artificial intelligence and lin-
guistics, in any combination of these subjects.
First, the seminar-style teaching format as used in
those grammar implementation courses presupposes
a fairly coherent audience of linguists with a shared
background of linguistic knowledge. Second, since
computers are only used as a medium to implement
grammars and since the implementation platform is
not optimized for web-based training, it is neces-
sary that there be a relatively low number of stu-
dents per teacher. Third, the theoretical material is
in the form of overheads and research papers, which
are in electronic form but not easily accessible with-
out the accompanying lecture as part of a seminar-
style course. Fourth, the background lectures of the
courses lack the support of the kind of graphical,
interactive visualization that teaching software can
in principle offer. Finally, the courses follow a sin-
gle path through the materials as determined by the
teacher, which the student cannot change according
to their specific interests and their prior knowledge.
We believe that these shortcomings can be over-
come by shifting from a seminar-style to a web-
based training format in a way that preserves the
positive aspects of successful hands-on courses. On
the other hand, to successfully shift from seminar-
style to web-based training we believe it is essential
to do this based on a scientific understanding of the
nature and possibilities of web-based learning. In
the next section we therefore embed our work in the
context of education and collaborate learning tech-
nology research.
3 Education and collaborative learning
technology research
Our perspective on web-based training draws its in-
spiration primarily from work in building ?learn-
ing communities? in education research (Lin et al,
1995; Nonaka, 1994), in which:
1. a precise context is established to introduce
tacit knowledge and experience, in this case
on subjects in computational linguistics and the
traditional disciplines it draws from,
2. conflicting perspectives are shared, concepts
are objectified and submitted to a process of
justification and arbitration, and
3. the concepts are then integrated into the knowl-
edge base as modules upon which further in-
structional material or grammar implementa-
tions can be constructed.
We thus intend to provide an environment that
teaches students by actively encouraging them to
participate in research that extends our collective
knowledge in this area. In principle, there are no
boundaries to the material that could be included in
the evolving framework. We intend to make it avail-
able as an open-source standard for grammar de-
velopment and instruction in the hope that this will
encourage researchers and educators to contribute
modules to it, and to use a feature-structure based
approach for their own research and courses.
Scardamalia and Bereiter (1993) identify seven
global characteristics that technologies must have to
support this kind of participation:
Balance: a distinction between public and private
and between individual and group knowledge pro-
cesses. That includes free access to others? work, in-
cluding implementations of concepts as algorithms
or grammars, and opportunities to borrow ideas into
their own work that would be prohibitively time-
consuming or otherwise advanced to formulate on
their own. Such technologies must also encour-
age time for personal ?reflection and refinement?
and anonymous public or private contribution to the
knowledge space. The present framework achieves
this by providing an open-source setting combined
with a web-based instructional tool for self-paced
learning and individual design of both the contents
and order of the curriculum.
Contribution and notification: to prevent ideas
from being presented in an insulated structure that
discourages questioning, debate, or revision. As dis-
cussed in Section 4.2, this is achieved by providing
extensive linking and annotation of resources using
web-compatible metalanguages for integrating mod-
ules at the implementational, formal and instruc-
tional levels.
Source referencing: a means of preserving the
boundaries of a contributor?s idea and its credit as
well as a history of prior accounts and antecedents
to the idea. In the present framework, this is pro-
vided by means of a requirements analysis compo-
nent that requires contributed modules to identify
the contribution by new concepts or resources pro-
vided, existing concepts or resources imported for it
to work, and an account of existing alternatives with
a description of its distinction from them.
Storage and retrieval: which places contribu-
tions in a ?communal context? of related contribu-
tions by others to encourage joint work between con-
tributors working on problems with significant over-
lap. The present framework must organize the pre-
sentation of existing modules along several thematic
dimensions to accomplish this.
Multiple points of entry: for stu-
dents/contributors with different backgrounds
and levels of experience. Material is made acces-
sible in more basic or fundamental modules by
projecting the formal content of the subject into a
graphically based common-sense domain at which
it can be grasped more intuitively (see Section 4.3).
Accessibility in more advanced modules is provided
by links specified in the requirements analysis
component to more basic modules that the former
rely upon.
Coherence-producing mechanisms: feedback
to contributors and framework moderators of mod-
ules that are ?fading? for lack of attention or further
development. These can either be reinstated or refor-
mulated, moved to a private space of more periph-
eral modules, or deleted outright. This is a way of
encouraging activity that is productive, and restrict-
ing the chance of confusion or information overload.
Such a coherence mechanism must exist within this
framework.
Links to external resources: to situate the justifi-
cation and discussion of contributions in a wide con-
text. We make use of the web-based training plat-
form ILIAS1 which is available as open source soft-
ware and offers a high degree of flexibility in terms
of the integration of internal and external resources.
1http://www.ilias.uni-koeln.de/ios/index-e.html
4 Integration of the framework
The goal of our current work is to transform previ-
ous, seminar-style courses and new input into teach-
ing materials that are fit for web-based training in the
general framework outlined in the previous section.
This clearly involves much more than simply refor-
matting old teaching materials into web-compatible
formats. Instead, it requires an analysis of the con-
tents of the courses, the interleaving and hyperlink-
ing of the textual materials, and the development
of graphical, interactive solutions for presenting and
interacting with the content of the material. Since
the nature of the textual material as such is familiar
(instructional notes, reference guides to major sec-
tions with indices, system documentation, annotated
system source code, and annotated grammar source
code), we use the limited space in this paper to high-
light the integrated nature of the approach as well as
the web-based training specific issues of hyperlink-
ing and visualization.
4.1 Integration of linguistic and computational
aspects
Our approach is distinguished by its integration of
grammars, the parsers that use them and the on-
line instructional materials. Compared to the LKB
system2, which as mentioned in Section 5.2 has
also been used successfully in teaching grammar
development, the greater range of formal expres-
sive devices available to our parsing system, called
TRALE, allows for more readable and compact
grammars, which we believe to be of central impor-
tance in a teaching context. To illustrate this, we
are currently porting the LinGO3 English Resource
Grammar (ERG) from the LKB (on which the ERG
was designed) to the TRALE system.
Given the scope of our web-based training frame-
work as including an integrated module on parsing,
it is also relevant that the TRALE system itself can
be relatively compact and transparent at the source-
code level since it exploits its close affinity to the
underlying Prolog on which it is implemented. This
contrasts with the perspective of Copestake et al
(2001), who concede that the LKB is unsuitable for
teaching parsing.
2http://www-csli.stanford.edu/?aac/lkb.html
3http://lingo.stanford.edu/csli/
4.2 The use of hyperlinks
Several different varieties of links are distinguished
within the course material, giving a first-class repre-
sentation to the transfer of knowledge between the
linguistic, computational and mathematical sources
that inform this interdisciplinary area. We intend to
distinguish the following kinds of links:
Conceptual/taxonomical: connecting instances
of key concepts and terms used throughout the
course material with their definitions and prove-
nience;
Empirical context: connecting instances of de-
sign decisions, algorithms and formal definitions to
encyclopedic discussions of their linguistic motiva-
tion and empirical significance;
Denotational: connecting instances of construc-
tional terms and issues within linguistics as well as
correctness conditions of algorithms to the mathe-
matical definitions that formalize them within the
foundations of constraint-based linguistics;
Operational: connecting mathematical defini-
tions and instances of related linguistic discussions
to computational instructional material describing
the algorithms used to construct, refute or transform
the formal objects representing them in a practical
system;
Implementational: connecting discussions of al-
gorithms to the actual annotated system source code
in the TRALE system used to implement them, and
mathematical definitions and discussions of linguis-
tic constructions to the actual annotated grammar
source code used to represent them in a typical im-
plementation.
The idea behind this classification is that when
more course material is added to the web-based
training framework we are proposing, the new mate-
rial will take into account these distinctions to obtain
a conceptually coherent use of hyperlinks through-
out the framework.
4.3 Visualization
Our three core modules make use of a number of
graphical user interfaces: a tool for interleaved vi-
sualization and interaction with trees and attribute
value matrices, one for the presentation of lexical
rules and their interaction, an Emacs-based source-
level debugger, and a program for the graphical ex-
ploration of the formal foundations of typed feature
logic. The first two are extensions of tools we al-
ready used for our previous courses, and the third is
an extension of the ALE source-level debugger, so
we here focus on the last, new development.
The main goal of the MorphMoulder (MoMo) is
to project the formality of its subject, the formal
foundations of constraint languages over typed fea-
ture structures, onto a graphical level at which it can
be grasped more intuitively.4 The transparency of
this level is essential for providing multiple points
of entry (Section 3) to this fundamentally impor-
tant module. The MoMo tool allows the user to
explore the relationship between the two levels of
the formal architecture: the descriptions and the el-
ements described. To this end, the user works with
a graphical interface on a whiteboard. Labeled di-
rected graphs representing feature structures can be
constructed on the whiteboard from their basic com-
ponents, nodes and arcs. The nodes are depicted
as colored balls, which are assigned types, and the
arcs are depicted as arrows that may be labeled by
feature names. Once a feature structure has been
constructed, the user may examine its logical prop-
erties. The three main functions of the MoMo tool
allow one to check (1) whether a feature structure
complies with a given signature, (2) whether a well-
formed feature structure satisfies a description or a
set of descriptions, and (3) whether a well-formed
feature structure is a model of a description or a set
of descriptions. In the context of the course, the
functions of MoMo thus lead the user from under-
standing the well-formedness of feature structures
with respect to a signature to an understanding of
feature structures in their role as a logical model of
a theory. If a student has chosen course modules that
include a focus on formal foundations of feature log-
ics or feature logics based linguistic theory, the first
introduction to the subject by MoMo can easily be
followed up by a course module with rigorous math-
ematical definitions.
In constraint-based frameworks, the user declares
the primitives of the empirical domain in terms of
a type hierarchy with appropriate attributes and at-
tribute values. Consider a signature that licenses
lists of various birds, which may then be classified
according to certain properties. First of all, the sig-
4MoMo is written by Ekaterina Ovchinnikova, U. Tu?bingen.
nature needs to comprise a type hierarchy and fea-
ture appropriateness conditions for lists. Let type list
be an immediate supertype of the types non-empty-
list and empty-list in the type hierarchy (henceforth
abbreviated as nelist and elist). Let the appropri-
ateness conditions declare the attributes HEAD and
TAIL appropriate for (objects of) type nelist, the val-
ues of TAIL at nelist be of type list, and the values
of HEAD at type nelist be of type bird (for lists of
birds). Finally no attributes are appropriate for the
type elist. A typical choice for the interpretation of
that kind of signature in constraint-based formalisms
is the collection of totally well-typed and sort re-
solved feature structures. All nodes of totally well-
typed and sort resolved feature structures are of a
maximally specific type (types with no subtypes);
and they have outgoing arcs for all and only those
features that are appropriate to their type, with the
feature values again obeying appropriateness. Our
signature for lists thus declares an ontology of fea-
ture structures with nodes of type nelist or elist (but
never of type list), where the former must bear the
outgoing arcs HEAD and TAIL, and the latter have no
outgoing arcs. They signal the end of the list. The
HEAD values of non-empty lists must be in the de-
notation of the type bird.
Figure 1 illustrates how the MoMo tool can be
used to study the relationship between signatures
and the feature structures they license by letting
the user construct feature structures and interac-
tively explore whether particular feature structures
are well-formed according to the signature. To the
left of the whiteboard there are two clickable graph-
ics consoles of possible nodes and arcs from which
the user may choose to draw feature structures. The
consoles offer nodes of all maximally specific types
and arcs of all attributes that are declared in the
signature. In the present example, parrot, wood-
pecker, and canary are the maximally specific sub-
types of bird.
Each color of edge represents a different attribute,
and each color of node represents a different type.
The grayed outlines on edges and nodes indicate that
all of the respective edges and nodes in this partic-
ular example are licensed by the signature that was
provided. The HEAD arc originating at the node of
type elist, however, violates the appropriateness con-
ditions of the signature. The feature structure de-
Figure 1: Graphically evaluating well-typedness of
feature structures.
picted here, therefore, is not well-formed. The sig-
nature check thus fails on the given feature structure,
as indicated by the red light in the upper function
console to the right of the whiteboard.
Similarly, MoMo can graphically depict satisfia-
bility and modellability of a single description or set
of descriptions. To this end, the user may be asked to
construct a description that a given feature structure
satisfies or models; or she may be asked to construct
feature structures that satisfy or model a given de-
scription (or set of descriptions). The system will
give systematic feedback on the correct or incorrect
usage of the syntax of the description language as
well as on to which extent a feature structure satis-
fies or models descriptions, systematically guiding
the user to correct solutions.
Figure 2 shows a successful satisfiability check of
a well-formed feature structure. The feature struc-
ture is derived from the one in Figure 1 by re-
moving the incorrect HEAD arc and its substructure
from the elist node. The query, asked in a sepa-
rate window, is whether the feature structure satis-
fies the constraint (nelist, head:(parrot,
color:green), tail:nelist). Since this
is the case, the green light on the function console to
the right is signaling succeed. If we were to perform
model checking of the same feature structure against
the same constraint, checking would fail, and MoMo
would indicate the nodes of the feature structure that
do not satisfy the given constraint.
Figure 2: Graphically evaluating constraint satisfac-
tion of feature structures.
MoMo?s descriptions are a syntactic parallel to
TRALE?s descriptions, thus introducing the student
not only to the syntax and semantics of constraint
languages but also to the language that will be used
for the implementation of grammars later in the
course. The close relationship of description lan-
guages also facilitates a comparison of their model-
theoretic semantics and the truth conditions of gram-
mars with the structure and semantics of algorithms
that use descriptions for constraint resolution and in
parsing. Finally, their common structure allows for a
tight network of hyperlinks across the boundaries of
different course modules and course topics, linking
them to a common source of mathematical, imple-
mentational and linguistic indices, which explain the
usage of common mathematical concepts across the
different areas of application of typed feature struc-
tures.
5 From seminar-style courses to
web-based training
Having discussed the ideas driving the web-based
teaching platform and exemplified one of the tools,
we now return to the courses which have informed
our work on the three core modules currently being
developed in terms of their content and the use of a
web- and implementation environment they make.
5.1 Grammar implementation in ALE
ALE5 (Carpenter and Penn, 1996) is a conserva-
tive extension of Prolog based on typed feature
structures, with a built-in parser and semantic-head-
driven generator. The demand for such a utility
was so great when it was beta-released in 1992
that it immediately became the subject of early
work in graphical front-end development for large
constraint-based grammars: first with the Pleuk sys-
tem (Calder, 1993), then as one of several systems
supported by Gertjan van Noord?s HDrug6, followed
by an ALE-mode Emacs user interface (Laurens,
1995). It also provided the computational support
for one of the very first web-based computational
linguistics courses, Colin Matheson?s widely used
HPSG Development in ALE7. A follow-up course on
computational morphology8, also by Colin Mathe-
son, was based on ALE-RA9, a morphological ex-
tension of ALE by Tomaz Erjavec.
Our current web-based training module is sup-
ported by an extension of ALE, called TRALE,
that uses a slightly different interpretation of typing
found in many linguistic theories and an enhanced
constraint language that supports constraints with
complex antecedents (Penn, 2000).
5http://www.cs.toronto.edu/?gpenn/ale.html
6http://grid.let.rug.nl/?vannoord/hdrug/
7http://www.ltg.hcrc.ed.ac.uk/projects/ledtools/ale-hpsg/
8http://www.ltg.ed.ac.uk/projects/ledtools/ale-ra/
9http://nl.ijs.si/et/Thesis/ALE-RA/
5.2 Constraint-based grammar
implementation
Over the past five years, we have held another course
on Constraint-Based Grammar Implementation in
a variety of settings, from summer schools to reg-
ular curriculum courses.10 It offers hands-on ex-
perience to linguists interested in the formalization
of linguistic knowledge in a constraint-based gram-
mar formalism. The course is taught in an interac-
tive fashion in a computer laboratory and combines
background lectures with practical exercises on how
to specify grammars in ConTroll11 (Go?tz and Meur-
ers, 1997), a processing system for constraint-based
grammars intended to process with HPSG theories
directly from the form in which they are constructed
by linguists.
The background lectures of the Constraint-based
grammar implementation courses introduce the rel-
evant mathematical and computational knowledge
and focus on the main ingredients of constraint-
based grammars: highly structured lexical represen-
tations, constituent structures, and the encoding of
well-formedness constraints on grammatical repre-
sentations. In the lab, students work on exercises
exploring the theoretical concepts covered in the lec-
tures. In a later part of the course, they are given
the opportunity to undertake individualized gram-
mar projects for modeling theoretically and empir-
ically significant syntactic constructions of their na-
tive language.
This course was the first hands-on computational
syntax course at the European Summer School
in Language, Logic, and Information (ESSLLI,
1997: Aix-en-Provence), and was also offered at the
LSA Linguistic Institute (1999: University of Illi-
nois, Urbana-Champaign)12 and the Computational
Linguistics and Represented Knowledge (CLaRK)
Summer School (1999: Eberhard-Karls Universita?t,
Tu?bingen)13. Generally regarded as a highly suc-
cessful course and teaching method, every subse-
quent ESSLLI summer school has offered at least
one similar course: Practical HPSG Grammar Engi-
neering (1998: Ann Copestake, Dan Flickinger, and
10The courses were taught by E. Hinrichs and D. Meurers.
11http://www.sfs.uni-tuebingen.de/controll/
12http://ling.osu.edu/?dm/lehre/lsa99/
13http://ling.osu.edu/?dm/lehre/clark99/
Stephan Oepen)14, Development of large scale LFG
grammars: Linguistics, Engineering and Resources
(1999: Miriam Butt, Annette Frank, and Jonas
Kuhn)15, Grammatical Resources: Logic, Struc-
ture, Control (1999: Michael Moortgat and Richard
T. Oehrle)16, An Introduction to Grammar Engi-
neering using HPSG (2000: Ann Copestake, Rob
Malouf)17, Advanced Grammar Engineering using
HPSG (2000: Dan Flickinger, Stephan Oepen)18,
and An Introduction to Stochastic Attribute-Value
Grammars (2001: Rob Malouf, Miles Osborne)19.
5.3 Introduction to theory-driven CL
A further source of material for the core modules
of our web-based training framework is the graduate
level Introduction to Theory-driven Computational
Linguistics at the Ohio State University.20 It covers
the basic issues of the following topics: finite state
automata and transducers, formal language theory,
computability and complexity, recognizers/parsers
for context free grammars, memoization, and pars-
ing with complex categories.
The theoretical material is combined with prac-
tical exercises in Prolog implementing different as-
pects of parsers. At the end of the course, students
complete a project consisting of building and testing
a grammar fragment for a short English text of their
choice. The traditional one-quarter course includes
weekly exercises, extensive web-based course mate-
rial for students, and a course workbook21 as a guide
through the theoretical material.
5.4 Model-theoretic introduction to Syntax
Our approach to teaching the fundamentals of math-
ematical theories through graphical metaphors in
the context of syntax derives from our experience
with this method in teaching Syntax I (HPSG) at
the Eberhard-Karls Universita?t Tu?bingen in 1998,
14http://www.coli.uni-sb.de/esslli/Seiten/Oepen.html
15http://www.let.uu.nl/esslli/Courses/butt.html
16http://www.let.uu.nl/esslli/Courses/moortgat-oehrle.html
17http://www.cs.bham.ac.uk/?esslli/notes/copestake.html
18http://www.cs.bham.ac.uk/?esslli/notes/oepen.html
19http://odur.let.rug.nl/?malouf/esslli01/
20The course was taught by D. Meurers; see http://ling.osu.
edu/?dm/2001/winter/684.01/
21This workbook is based, with kind permission from the
authors, on the module workbook for ?Techniques in Natural
Language Processing 1? by Chris Mellish, Pete Whitelock and
Graeme Ritchie, 1994, Dept. of AI, University of Edinburgh.
1999 and 2001.22 In these seminars, which did not
presuppose any prior knowledge of model-theoretic
methods in logic, the mathematical foundations of
feature logic were introduced by intuitive means but
with as much precision as possible without strict for-
malization. An introduction to a standardized ver-
sion of the logical description language of HPSG
was accompanied with problem sets that required
the students to construct three-dimensional feature
structure models (made of styrofoam and wires) of
descriptions and sets of descriptions. The informal
but very concrete understanding of the relationship
between a theory cast in a constraint language and its
feature structure models had a very positive result on
students? ability to grasp and build working analyses
of unseen constructions compared to the results of
the more traditional method of teaching constraint-
based syntax used in previous years. At the same
time, the teaching method successfully used an ap-
peal to prior world knowledge rather than unfamiliar
mathematical notation in order to make the students
familiar with the basic concepts of constraint satis-
faction and truth in feature logics.
6 Summary and Outlook
The interdisciplinary nature of computational lin-
guistics and the diverse backgrounds of the student
audience makes it particularly attractive to teach a
subject like constraint-based grammar formalisms
and parsing using a web-based instructional plat-
form which integrates formal and computational
foundations, linguistic theory, and grammar im-
plementation. We discussed several seminar-style
courses which have informed our proposal in terms
of content, highlighted the problems of the tradi-
tional face-to-face teaching, and described our en-
vironment of web-based teaching materials plus im-
plementational support. We argued that a web-based
training framework for the topic can be organized
around feature structures as a central data structure
in formal foundations, linguistics and implementa-
tion. We outlined the educational and collaborative
learning background in which an informed proposal
on web-based training must be embedded and used
the newly developed tool MoMo as an illustration
22The courses were taught by F. Richter and M. Sailer; see
http://www.sfs.uni-tuebingen.de/?fr/teaching/
of how we envisage projecting the formal content of
the subject into a graphically based common-sense
domain in which it can be grasped more intuitively.
The three core modules on formal founda-
tions, constraint-based grammar implementation,
and parsing will be completed and made publicly
available at the end of 2003. The joint project
is funded by the German Federal Ministry for Re-
search Technology (BMBF) as part of the consor-
tium Media-intensive teaching modules in the com-
putational linguistics curriculum (MiLCA).23
References
J. Calder. 1993. Graphical interaction with constraint-
based grammars. In Proceedings of PACLING ?93,
pages 160?168, Vancouver, British Columbia.
B. Carpenter and G. Penn. 1996. Compiling typed
attribute-value logic grammars. In H. Bunt and
M. Tomita, editors, Recent Advances in Parsing Tech-
nologies, pages 145?168. Kluwer, Dordrecht.
A. Copestake, J. Carroll, D. Flickinger, R. Malouf, and
S. Oepen. 2001. Using an open-source unification-
based system for CL/NLP teaching. In Proceedings
of the EACL/ACL Workshop on Sharing Tools and Re-
sources for Research and Education, pages 35?38.
T. Go?tz and W. D. Meurers. 1997. The ConTroll system
as large grammar development platform. In Proceed-
ings of the EACL/ACL Workshop on Computational
Environments for Grammar Development and Linguis-
tic Engineering, pages 38?45. http://ling.osu.edu/?dm/
papers/envgram.html.
O. Laurens. 1995. An Emacs user interface for ALE.
Technical Report CSS-IS TR 95-07, School of Com-
puting Science, Simon Fraser University.
X. Lin, J.D. Bransford, and C.E. Hmelo. 1995. Instruc-
tional design and development of learning communi-
ties: an invitation to dialogue. Educational Technol-
ogy, 35(5):53?63.
I. Nonaka. 1994. A dynamic theory of organizational
knowledge creation. Organizational Science, 5(1).
G. Penn. 2000. Applying Constraint Handling Rules
to HPSG. In Proceedings of the Workshop on Rule-
Based Constraint Reasoning and Programming, CL
2000.
M. Scardamalia and C. Bereiter. 1993. Technologies
for knowledge-building discourse. Communications
of the ACM, 36(5):37?41.
23http://milca.sfs.uni-tuebingen.de/A4/HomePage/top.html
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 109?114,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Constraint-based Computational Semantics:
A Comparison between LTAG and LRS
Laura Kallmeyer
University of T?bingen
Collaborative Research Center 441
lk@sfs.uni-tuebingen.de
Frank Richter
University of T?bingen
Collaborative Research Center 441
fr@sfs.uni-tuebingen.de
Abstract
This paper compares two approaches to
computational semantics, namely seman-
tic unification in Lexicalized Tree Ad-
joining Grammars (LTAG) and Lexical
Resource Semantics (LRS) in HPSG.
There are striking similarities between the
frameworks that make them comparable in
many respects. We will exemplify the dif-
ferences and similarities by looking at sev-
eral phenomena. We will show, first of all,
that many intuitions about the mechanisms
of semantic computations can be imple-
mented in similar ways in both frame-
works. Secondly, we will identify some
aspects in which the frameworks intrin-
sically differ due to more general differ-
ences between the approaches to formal
grammar adopted by LTAG and HPSG.
1 Introduction
This paper contrasts two frameworks for compu-
tational semantics, the proposal for semantics in
LTAG described in (Kallmeyer and Romero, 2005)
and LRS (Richter and Sailer, 2004), a computa-
tional semantics framework formulated in Head-
Driven Phrase Structure Grammar (HPSG).
There are significant differences between LTAG
and HPSG. LTAG is a mildly context-sensitive
lexicalized formalism characterized by an ex-
tended domain of locality. HPSG is based on the
idea of a separation of the lexicon and syntactic
structure and on the strict locality of general gram-
mar principles that are formulated in an expres-
sive and very flexible logical description language.
These fundamental differences are reflected in the
respective architectures for semantics: LTAG as-
sumes a separate level of underspecified semantic
representations; LRS uses the description logic of
syntax for semantic specifications.
However, despite the different mathematical
structures, we find striking similarities between
LTAG semantics with unification and LRS. They
both show similar intuitions underlying specific
analyses, use the same higher order type-theoretic
language (Ty2, (Gallin, 1975)) as a means for
specifying the truth conditions of sentences, and
employ a feature logic in the combinatorial seman-
tics instead of the lambda calculus. Because of
these similarities, analyses using both approaches
are closely related and can benefit from each other.
The paper is structured as follows: Sections 2
and 3 will introduce the two frameworks. The
next three sections (4?6) will sketch analyses of
some phenomena in both frameworks that will re-
veal relevant relations between them. Section 7
presents a summary and conclusion.
2 LTAG semantics
In (Kallmeyer and Romero, 2005), each elemen-
tary tree is linked to a semantic representation (a
set of Ty2 formulas and scope constraints). Ty2
formulas (Gallin, 1975) are typed ?-terms with in-
dividuals and situations as basic types. The scope
constraints of the form x ? y specify subordina-
tion relations between Ty2 terms. In other words,
x ? y indicates that y is a component of x.
A semantic representation is equipped with a
semantic feature structure description. Semantic
computation is done on the derivation tree and
consists of certain feature value equations between
mother and daughter nodes in the derivation tree.
(1) John always laughs.
As an example, see Fig. 1 showing the deriva-
tion tree for (1) with semantic representations and
109
l1 : laugh( 1 )
?
?
?
NP
[
GLOBAL
[
I 1
]
]
VP
[
B
[
P l1
]
]
?
?
?
np vp
john(x) l2 : always( 3 ),
3 ? 4
[
GLOBAL
[
I x
]
] ?
?
?
VPr
[
B
[
P l2
]
]
VPf
[
B
[
P 4
]
]
?
?
?
Figure 1: LTAG semantics of (1)
semantic feature structure descriptions as node
labels. The additional feature equations in this
example are depicted using dotted lines. They
arise from top-bottom feature identifications par-
allel to the unifications performed in FTAG (Vijay-
Shanker and Joshi, 1988) and from identifications
of global features. They yield 1 = x and 4 = l1.
Applying these identities to the semantic represen-
tations after having built their union leads to (2).
The constraint 3 ? l1 states that l1 : laugh(x) is
a component of 3 .
(2) john(x), l2 : always( 3 ), l1 : laugh(x),
3 ? l1
Note that the feature structure descriptions do
not encode the semantic expressions one is inter-
ested in. They only encode their contributions to
functional applications by restricting the argument
slots of certain predicates in the semantic repre-
sentations: They state which elements are con-
tributed as possible arguments for other seman-
tic expressions and which arguments need to be
filled. They thereby simulate lambda abstraction
and functional application while assembling the
semantic representations. To achieve this, a re-
stricted first order logic is sufficient.
Semantic computation is local on the derivation
tree: The new feature equations that are added de-
pend only on single edges in the derivation tree.
Because of this, even with the extension to seman-
tics, the formalism is still mildly context-sensitive.
3 LRS
In LRS the feature logic specifies the entire gram-
mar, including well-formed Ty2 terms as seman-
tic representations, and their mode of composi-
tion. Instead of the lambda calculus of tradi-
tional Montague Grammar, LRS crucially uses a
novel distinction between three aspects of the log-
ical representations of signs (external content, in-
ternal content, and parts). LRS constraints es-
tablish sub-term relationships between pieces of
semantic representations within and across signs,
thereby specifying the combinatorial properties of
the semantics. The subterm or component-of con-
ditions (symbolized as /) are imposed by gram-
mar principles. Since these principles are descrip-
tions of object-language expressions, they permit
the application of various underspecification tech-
niques of computational semantics, although an
LRS grammar does not employ underspecified se-
mantic representations, in contrast to LTAG se-
mantics.
Fig. 2 shows an HPSG description of the syn-
tactic tree and the LRS specifications of (1). The
syntactic trees in HPSG correspond to the derived
trees of LTAG. Since HPSG does not have deriva-
tion trees, the LRS principles refer to derived trees.
NP
?
?
exc 1
inc 1
p ? 1 john?
?
?
John
A
?
?
exc 5
inc 5 always( 3 )
p ? 5 , 5a always?
?
?
always
V
?
?
exc 4
inc 2 laugh( 1 )
p ? 2 , 2a laugh?
?
?
laughs
adj head
VP
?
?
exc 4
inc 2
p ? 2 , 2a , 5 , 5a ?
?
?
& 2 / 3 & 5 / 4
comp head
S
?
?
exc 4 always(laugh(john))
inc 2
p ? 2 , 2a , 5 , 5a , 1 ?
?
?
Figure 2: LRS analysis of (1)
Each word lexically specifies its contribution to
the overall meaning of the sentence (P(ARTS)), the
part of its semantics which is outscoped by all
signs the word combines with (INC(ONT)), and
the overall semantic contribution of its maximal
projection (EXC(ONT)). Feature percolation prin-
ciples identify INC and EXC, respectively, along
head projections and collect the elements of the
PARTS lists of the daughters at each phrase. The
combination of the adjunct with a verbal pro-
jection introduces two component-of constraints:
The EXC of always must be within the EXC of
laughs, and the INC of laughs must be in the
scope of always. The semantic argument of
110
laughs (john) is identified by subcategorization
(not shown in Fig. 2). A closure condition requires
that the semantic representation of an utterance
use up all and only the PARTS contributions of all
signs, which yields 4 = always(laugh(john)).
4 Quantifier scope
4.1 Specifying a scope window
(3) Exactly one student admires every professor:
? > ?,? > ?
(4) John seems to have visited everybody:
seem > ?,? > seem
Quantificational NPs in English can in princi-
ple scope freely (see (3) and (4)). An analysis of
quantifier scope must guarantee only two things:
1. the proposition to which a quantifier attaches
must be in its nuclear scope, and 2. a quantifier
cannot scope higher than the next finite clause.
One way to model this is to define a scope win-
dow delimited by a maximal scope and a minimal
scope for a quantifier. Both LTAG and LRS, spec-
ify such scope windows for quantifiers. We will
now outline the two analyses.
(5) Everybody laughs.
(Kallmeyer and Romero, 2005) use global fea-
tures MAXS and MINS for the limits of the scope
window. Fig. 3 shows the LTAG analysis of (5).
The feature identifications (indicated by dotted
lines) lead to the constraints 2 ? 5 , 5 ? l1.
These constraints specify an upper and a lower
boundary for the nuclear scope 5 . With the as-
signments following from the feature identifica-
tions we obtain the semantic representation (6):
(6)
l1 : laugh(x),
l2 : every(x, 4 , 5 ), l3 : person(x)
2 ? l1,
4 ? l3, 2 ? 5 , 5 ? l1
There is one possible disambiguation consis-
tent with the scope constraints, namely 2 ? l2,
4 ? l3, 5 ? l1. This leads to the semantics
every(x, person(x), laugh(x)).
In LRS, the EXCONT value of the utterance is
the upper boundary while the INCONT value of the
syntactic head a quantifier depends on is the lower
boundary for scope, as illustrated in Fig. 4. The
upper boundary is obtained through the interaction
of 1) a PROJECTION PRINCIPLE stating that the
l1 : laugh( 1 ),
2 ? 3
np
l2 : every(x, 4 , 5 ),
l3 : person(x),
4 ? l3,
6 ? 5 , 5 ? 7
?
?
?
?
GLOBAL
[
MINS l1
MAXS 2
]
NP
[
GLOBAL
[
I 1
]
]
?
?
?
?
?
?
?
?
GLOBAL
[
I x
]
NP
[
GLOBAL
[
MINS 7
MAXS 6
]
]
?
?
?
?
Figure 3: LTAG analysis of (5) Everybody laughs
PARTS list of a phrase contains all elements on the
PARTS lists of its daughters, and 2) the EXCONT
PRINCIPLE which states that a) the PARTS list of
each non-head contains its own EXCONT, and b)
in an utterance, everything on the PARTS list is a
component of the EXCONT. This leads to the con-
straint 4  6 in Fig. 4, among others. The lower
boundary is obtained from the SEMANTICS PRIN-
CIPLE which states that if the non-head of a headed
phrase is a quantifier, then the INCONT of the head
is a component of its nuclear scope. This yields
1  ? in Fig. 4.
S
?
?
EXC 6 ?x
(
person
(
x
)
? laugh
(
x
))
INC 1
P ?x, 1 , 1a , 2 , 2a , 4 , 4a ?
?
?
NP VP
?
?
?
EXC 4 ?x (? ? ?)
INC 2 person
(
x
)
P ?x, 2 , 2a person,
4 , 4a ? ? ??
?
?
?
?
?
EXC 6
INC 1 laugh
(
x
)
P ? 1 , 1a laugh?
?
?
everybody laughs
Relevant subterm constraints: 2  ? (from the lexical entry
of everybody), 1  ?, 4  6
Figure 4: LRS analysis of (5) Everybody laughs
The striking similarity between the two anal-
yses shows that, despite the fundamental differ-
ences between the frameworks, central insights
can be modelled in parallel.
4.2 Nested quantifiers
The use of the upper limit of the scope windows is,
however, slightly different: EXCONT contains the
quantifier itself as a component while MAXS limits
only the nuclear scope, not the quantifier. Conse-
quently, in LTAG the quantifier can scope higher
111
than the MAXS limiting its nuclear scope but in
this case it takes immediate scope over the MAXS.
(7) Two policemen spy on someone from every
city: ? > ? > 2 (among others)
The LTAG analysis is motivated by nested quan-
tifiers. In sentences such as (7), the embedded
quantifier can take scope over the embedding one
but if so, this must be immediate scope. In other
words, other quantifiers cannot intervene. In (7),
the scope order ? > 2 > ? is therefore not pos-
sible.1 The LTAG analysis is such that the max-
imal nuclear scope of the embedded quantifier is
the propositional label of the embedding quanti-
fier.2
In LRS, the way the scope window is speci-
fied, a corresponding constraint using the EXCONT
of the embedded quantifier cannot be obtained.
The LRS principle governing the distribution of
embedded quantifiers in complex NPs states di-
rectly that in this syntactic environment, the em-
bedded quantifier may only take direct scope over
the quantifier of the matrix NP. This principle
does not refer to the notion of external content at
all. At this point it is an open question whether
LRS could learn from LTAG here and adapt the
scope window so that an analogous treatment of
nested quantifiers would be possible.
5 LTAG?s extended domain of locality
Whereas the treatment of quantification sketched
in the preceding section highlights the similarities
between LTAG semantics and LRS, this and the
following section will illustrate some fundamental
differences between the frameworks.
In spite of the parallels mentioned above, even
INCONT and MINS differ sometimes, namely in
sentences containing bridge verbs. This is related
to the fact that LTAG has an extended domain of
locality whereas HPSG does not. Let us illustrate
the difference with the example (8).
(8) Mary thinks John will come.
1(Joshi et al, 2003) propose an extra mechanism that
groups quantifiers into sets in order to derive these con-
straints. (Kallmeyer and Romero, 2005) however show that
these constraints can be derived even if the upper limit MAXS
for nuclear scope is used as sketched above.
2Note that this approach requires constraints of the form
l ? n with l being a label, n a variable. This goes
beyond the polynomially solvable normal dominance con-
straints (Althaus et al, 2003). This extension, though, is
probably still polynomially solvable (Alexander Koller, per-
sonal communication).
In LTAG, the two elementary verb trees (for
thinks and will come) have different global MINS
features. The one for thinks is the label of the think
proposition while the one for will come is the label
of the embedded proposition. As a consequence, a
quantifier which attaches to the matrix verb cannot
scope into the embedded clause. This distinction
of different MINS values for different verb trees is
natural in LTAG because of the extended domain
of locality.
In LRS, all verbal nodes in the constituent struc-
ture of (8) carry the same INCONT value, namely
the proposition of the embedded verb. Conse-
quently, the minimal scope of quantifiers attaching
either to the embedding or to the embedded verb
is always the proposition of the embedded verb.
However, due to the requirement that variables be
bound, a quantifier binding an argument of the em-
bedding verb cannot have narrow scope over the
embedded proposition.
How to implement the LTAG idea of different
INCONT values for the embedding and the embed-
ded verb in LRS is not obvious. One might intro-
duce a new principle changing the INCONT value
at a bridge verb, whereby the new INCONT would
get passed up, and the embedded INCONT would
no longer be available. This would be problem-
atic: Take a raising verb as in (9) (adjoining to the
VP node in LTAG) instead of a bridge verb:
(9) Most people seem to everybody to like the
film.
Here the minimal scope of most people should
be the like proposition while the minimal scope
of everybody is the seem proposition. In LTAG
this does not pose a problem since, due to the ex-
tended domain of locality, most people attaches to
the elementary tree of like even though the seem
tree is adjoined in between. If the INCONT treat-
ment of LRS were modified as outlined above and
seem had an INCONT value that differed from the
INCONT value of the embedded like proposition,
then the new INCONT value would be passed up
and incorrectly provide the minimal scope of most
people. LRS must identify the two INCONTs.
The difference between the two analyses illus-
trates the relevance of LTAG?s extended domain of
locality not only for syntax but also for semantics.
6 Negative Concord
The analysis of negative concord in Polish de-
scribed in this section highlights the differences
112
in the respective implementation of underspeci-
fication techniques in LTAG and LRS. Recall
that both LTAG and LRS use component-of con-
straints. But in LTAG, these constraints link ac-
tual Ty2-terms (i.e., objects) to each other, while
in LRS, these constraints are part of a description
of Ty2-terms.
(10) Janek nie pomaga ojcu.
Janek NM helps father
?Janek doesn?t help his father.?
(11) a. Janek nie pomaga nikomu.
Janek NM helps nobody
?Janek doesn?t help anybody.?
b. ?Janek pomaga nikomu.
(12) Nikt nie przyszed?.
nobody NM came
?Nobody came.?
The basic facts of sentential negation and nega-
tive concord in Polish are illustrated in (10)?(12):
The verbal prefix nie is obligatory for sentential
negation, and it can co-occur with any number
of n-words (such as nikt, ?anybody?) without ever
leading to a double negation reading. As a conse-
quence, (12) expresses only one logical sentential
negation, although the negation prefix nie on the
verb and the n-word nikt can carry logical nega-
tion alone in other contexts. LRS takes advantage
of the fact that its specifications of semantic repre-
sentations are descriptions of logical expressions
which can, in principle, mention the same parts
of the expressions several times. Fig. 5 shows
that both nikt and the verb nie przyszed? introduce
descriptions of negations ( 4 and 2 , respectively).
The constraints of negative concord in Polish will
then conspire to force the negations contributed by
the two words to be the same in the overall logical
representation 6 of the sentence.
Such an analysis is not possible in LTAG. Each
negation in the interpretation corresponds to ex-
actly one negated term introduced in the seman-
tic representations. Therefore, the negative parti-
cle nie necessarily introduces the negation while
the n-word nikt requires a negation in the proposi-
tion it attaches to. An analysis along these lines is
sketched in Fig. 6 (?GL? stands for ?GLOBAL?).
The requirement of a negation is checked with
a feature NEG indicating the presence of a nega-
tion. The scope of the negation (feature N-SCOPE)
?
?
EXC 6 ??e?x
(
person
(
x
)
? come
(
e, x
))
INC 1
P ?e, x, 0 , 1 , 1a , 1b , 2 , 3 , 3a , 4 , 5 , 5a ?
?
?
nikt nie przyszed?
?
?
?
EXC 5 ?x (? ? ?)
INC 3 person
(
x
)
P ?x, 3 , 3a person,
4??, 5 , 5a ? ? ??
?
?
?
?
?
?
?
?
EXC 6
INC 1 come
(
e, x
)
P ?e, 1 , 1a come e,
1b come, 2??,
0 ?e??
?
?
?
?
?
1  ?, 2  6 , 5  ?, 3  ?, 1  ?, 1  ?, 1  ?
Figure 5: LRS analysis of (12) Nikt nie przyszed?
marks the maximal scope of the existential quan-
tifier of the n-word nikt (constraint 7 ? 6 ).3
S
NP VP
V
NP nie V
nikt przyszed?
l1 : ? 1 ,
l2 : come( 2 , 3 )
1 ? l2, 4 ? l1
np
l3 : some(x, 5 , 6 ),
l4 : person(x)
5 ? l4,
7 ? 6 , 6 ? 8
?
?
?
?
?
?
?
GL
?
?
?
MAXS 4
N-SCOPE 1
MINS l2
NEG yes
?
?
?
NP
[
GL
[
I 2
]
]
?
?
?
?
?
?
?
?
?
?
?
?
GL
[
I x
]
NP
?
?GL
[
N-SCOPE 7
MINS 8
NEG yes
]
?
?
?
?
?
?
?
Figure 6: LTAG analysis of (12) Nikt nie przyszed?
This example illustrates that the two frame-
works differ substantially in their treatment of un-
derspecification: 1. LRS employs partial descrip-
tions of fully specified models, whereas LTAG
generates underspecified representations in the
style of (Bos, 1995) that require the definition of
a disambiguation (a ?plugging? in the terminol-
ogy of Bos). 2. LRS constraints contain not Ty2
terms but descriptions of Ty2 terms. Therefore, in
contrast to LTAG, two descriptions can denote the
same formula. Here, LTAG is more limited com-
pared to LRS. On the other hand, the way seman-
tic representations are defined in LTAG guarantees
3See (Lichte and Kallmeyer, 2006) for a discussion of
NEG and N-SCOPE in the context of NPI-licensing.
113
that they almost correspond to normal dominance
constraints, which are known to be polynomially
parsable. The difference in the use of underspecifi-
cation techniques reflects the more general differ-
ence between a generative rewriting system such
as LTAG, in which the elements of the grammar
are objects, and a purely description-based for-
malism such as HPSG, in which token identities
between different components of linguistic struc-
tures are natural and frequently employed.
7 Summary and Conclusion
LTAG and LRS have several common characteris-
tics: They both 1. use a Ty2 language for seman-
tics; 2. allow underspecification (LTAG scope con-
straints ? versus LRS component-of constraints
); 3. use logical descriptions for semantic com-
putation; 4. are designed for computational appli-
cations. Due to these similarities, some analyses
can be modelled in almost identical ways (e.g., the
quantifier scope analyses, and the identification of
arguments using attribute values rather than func-
tional application in the lambda calculus). We take
the existence of this clear correspondence as in-
dicative of deeper underlying insight into the func-
tioning of semantic composition in natural lan-
guages.
Additionally, the differences between the
frameworks that can be observed on the level of
syntax carry over to semantics: 1. LTAG?s ex-
tended domain of locality allows the localization
within elementary trees of syntactic and seman-
tic relations between elements far apart from each
other on the level of constituent structure. 2. LTAG
(both syntax and semantics) is a formalism with
restricted expressive power that guarantees good
formal properties. The restrictions, however, can
be problematic. Some phenomena can be more
easily described in a system such as HPSG and
LRS while their description is less straightfor-
ward, perhaps more difficult or even impossible
within LTAG. The concord phenomena described
in section 7 are an example of this.
A further noticable difference is that within the
(Kallmeyer and Romero, 2005) framework, the
derivation tree uniquely determines both syntac-
tic and semantic composition in a context-free
way. Therefore LTAG semantics is mildly context-
sensitive and can be said to be compositional.
As far as LRS is concerned, it is not yet known
whether it is compositional or not; compositional-
ity (if it holds at all) is at least less straightforward
to show than in LTAG.
In conclusion, we would like to say that the sim-
ilarities between these two frameworks permit a
detailed and direct comparison. Our comparative
study has shed some light on the impact of the dif-
ferent characteristic properties of our frameworks
on concrete semantic analyses.
Acknowledgments
For many long and fruitful discussions of various
aspects of LTAG semantics and LRS, we would
like to thank Timm Lichte, Wolfgang Maier, Mari-
bel Romero, Manfred Sailer and Jan-Philipp S?hn.
Furthermore, we are grateful to three anonymous
reviewers for helpful comments.
References
Ernst Althaus, Denys Duchier, Alexander Koller, Kurt
Mehlhorn, Joachim Niehren, and Sven Thiel. 2003.
An efficient graph algorithm for dominance con-
straints. Journal of Algorithms, 48(1):194?219.
Johan Bos. 1995. Predicate logic unplugged. In Paul
Dekker and Martin Stokhof, editors, Proceedings of
the 10th Amsterdam Colloquium, pages 133?142.
Daniel Gallin. 1975. Intensional and Higher-Order
Modal Logic with Applications to Montague Seman-
tics. North Holland mathematics studies 19. North-
Holland Publ. Co., Amsterdam.
Aravind K. Joshi, Laura Kallmeyer, and Maribel
Romero. 2003. Flexible Composition in LTAG:
Quantifier Scope and Inverse Linking. In Harry
Bunt, Ielka van der Sluis, and Roser Morante, ed-
itors, Proceedings of the Fifth International Work-
shop on Computational Semantics IWCS-5, pages
179?194, Tilburg.
Laura Kallmeyer and Maribel Romero. 2005. Scope
and Situation Binding in LTAG using Semantic Uni-
fication. Submitted to Research on Language and
Computation. 57 pages., December.
Timm Lichte and Laura Kallmeyer. 2006. Licensing
German Negative Polarity Items in LTAG. In Pro-
ceedings of The Eighth International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+8), Sydney, Australia, July.
Frank Richter and Manfred Sailer. 2004. Basic con-
cepts of lexical resource semantics. In Arnold Beck-
mann and Norbert Preining, editors, ESSLLI 2003 ?
Course Material I, (= Collegium Logicum, 5), pages
87?143. Kurt G?del Society, Wien.
K. Vijay-Shanker and Aravind K. Joshi. 1988. Feature
structures based tree adjoining grammar. In Pro-
ceedings of COLING, pages 714?719, Budapest.
114
