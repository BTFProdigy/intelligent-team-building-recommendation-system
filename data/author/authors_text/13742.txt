Portability Issues for Speech Recognition Technologies 
Lori Lamel, Fabrice Lefevre, Jean-Luc Gauvain and Gilles Adda
Spoken Language Processing Group,
CNRS-LIMSI, 91403 Orsay, France
flamel,lefevre,gauvain,gaddag@limsi.fr
ABSTRACT
Although there has been regular improvement in speech recog-
nition technology over the past decade, speech recognition is far
from being a solved problem. Most recognition systems are tuned
to a particular task and porting the system to a new task (or lan-
guage) still requires substantial investment of time and money, as
well as expertise. Todays state-of-the-art systems rely on the avail-
ability of large amounts of manually transcribed data for acous-
tic model training and large normalized text corpora for language
model training. Obtaining such data is both time-consuming and
expensive, requiring trained human annotators with substantial a-
mounts of supervision.
In this paper we address issues in speech recognizer portabil-
ity and activities aimed at developing generic core speech recogni-
tion technology, in order to reduce the manual effort required for
system development. Three main axes are pursued: assessing the
genericity of wide domain models by evaluating performance under
several tasks; investigating techniques for lightly supervised acous-
tic model training; and exploring transparent methods for adapting
generic models to a specific task so as to achieve a higher degree of
genericity.
1. INTRODUCTION
The last decade has seen impressive advances in the capability
and performance of speech recognizers. Todays state-of-the-art
systems are able to transcribe unrestricted continuous speech from
broadcast data with acceptable performance. The advances arise
from the increased accuracy and complexity of the models, which
are closely related to the availability of large spoken and text cor-
pora for training, and the wide availability of faster and cheaper
computational means which have enabled the development and im-
plementation of better training and decoding algorithms. Despite
the extent of progress over the recent years, recognition accuracy is
still extremely sensitive to the environmental conditions and speak-
ing style: channel quality, speaker characteristics, and background
This work was partially financed by the European Commission
under the IST-1999 Human Language Technologies project 11876
Coretex.
.
noise have an important impact on the acoustic component of the
speech recognizer, whereas the speaking style and the discourse
domain have a large impact on the linguistic component.
In the context of the EC IST-1999 11876 project CORETEX we
are investigating methods for fast system development, as well as
development of systems with high genericity and adaptability. By
fast system development we refer to: language support, i.e., the
capability of porting technology to different languages at a reason-
able cost; and task portability, i.e. the capability to easily adapt a
technology to a new task by exploiting limited amounts of domain-
specific knowledge. Genericity and adaptability refer to the capac-
ity of the technology to work properly on a wide range of tasks and
to dynamically keep models up to date using contemporary data.
The more robust the initial generic system is, the less there is a
need for adaptation. Concerning the acoustic modeling component,
genericity implies that it is robust to the type and bandwidth of the
channel, the acoustic environment, the speaker type and the speak-
ing style. Unsupervised normalization and adaptation techniques
evidently should be used to enhance performance further when the
system is exposed to data of a particular type.
With today?s technology, the adaptation of a recognition system
to a new task or new language requires the availability of suffi-
cient amount of transcribed training data. When changing to new
domains, usually no exact transcriptions of acoustic data are avail-
able, and the generation of such transcribed data is an expensive
process in terms of manpower and time. On the other hand, there
often exist incomplete information such as approximate transcrip-
tions, summaries or at least key words, which can be used to pro-
vide supervision in what can be referred to as ?informed speech
recognition?. Depending on the level of completeness, this infor-
mation can be used to develop confidence measures with adapted or
trigger language models or by approximate alignments to automatic
transcriptions. Another approach is to use existing recognizer com-
ponents (developed for other tasks or languages) to automatically
transcribe task-specific training data. Although in the beginning the
error rate on new data is likely to be rather high, this speech data
can be used to re-train a recognition system. If carried out in an
iterative manner, the speech data base for the new domain can be
cumulatively extended over time without direct manual transcrip-
tion.
The overall objective of the work presented here is to reduce
the speech recognition development cost. One aspect is to develop
?generic? core speech recognition technology, where by ?generic?
we mean a transcription engine that will work reasonably well on a
wide range of speech transcription tasks, ranging from digit recog-
nition to large vocabulary conversational telephony speech, with-
out the need for costly task-specific training data. To start with we
assess the genericity of wide domain models under cross-task con-
Table 1: Brief descriptions and best reported error rates for the corpora used in this work.
Corpus Test Year Task Train (#spkr) Test (#spkr) Textual Resources Best WER
BN 98 TV & Radio News 200h 3h Closed-captions, commercial transcripts,
manual transcripts of audio data
13.5
TI-digits 93 Small Vocabulary 3.5h (112) 4h (113) - 0.2
ATIS 93 H-M Dialog 40h (137) 5h (24) Transcriptions 2.5
WSJ 95 News Dictation 100h (355) 45mn (20) Newspaper, newswire 6.6
S9 WSJ 93 Spontaneous Dictation 43mn (10) Newspaper, newswire 19.1
ditions, i.e., by recognizing task-specific data with a recognizer de-
veloped for a different task. We chose to evaluate the performance
of broadcast news acoustic and language models, on three com-
monly used tasks: small vocabulary recognition (TI-digits), read
and spontaneous text dictation (WSJ), and goal-oriented spoken di-
alog (ATIS). The broadcast news task is quite general, covering a
wide variety of linguistic and acoustic events in the language, en-
suring reasonable coverage of the target task. In addition, there are
sufficient acoustic and linguistic training data available for this task
that accurate models covering a wide range of speaker and language
characteristics can be estimated.
Another research area is the investigation of lightly supervised
techniques for acoustic model training. The strategy taken is to
use a speech recognizer to transcribe unannotated data, which are
then used to estimate more accurate acoustic models. The light
supervision is applied to the broadcast news task, where unlim-
ited amounts of acoustic training data are potentially available. Fi-
nally we apply the lightly supervised training idea as a transpar-
ent method for adapting the generic models to a specific task, thus
achieving a higher degree of genericity. In this work we focus on
reducing training costs and task portability, and do not address lan-
guage transfer.
We selected the LIMSI broadcast news (BN) transcription sys-
tem as the generic reference system. The BN task covers a large
number of different acoustic and linguistic situations: planned to
spontaneous speech; native and non-native speakers with different
accents; close-talking microphones and telephone channels; quiet
studio, on-site reports in noisy places to musical background; and
a variety of topics. In addition, a lot of training resources are avail-
able including a large corpus of annotated audio data and a huge
amount of raw audio data for the acoustic modeling; and large
collections of closed-captions, commercial transcripts, newspapers
and newswires texts for linguistic modeling. The next section pro-
vides an overview of the LIMSI broadcast news transcription sys-
tem used as our generic system.
2. SYSTEM DESCRIPTION
The LIMSI broadcast news transcription system has two main
components, the audio partitioner and the word recognizer. Data
partitioning [6] serves to divide the continuous audio stream into
homogeneous segments, associating appropriate labels for cluster,
gender and bandwidth with the segments. The speech recognizer
uses continuous density HMMs with Gaussian mixture for acous-
tic modeling and n-gram statistics estimated on large text corpora
for language modeling. Each context-dependent phone model is a
tied-state left-to-right CD-HMM with Gaussian mixture observa-
tion densities where the tied states are obtained by means of a de-
cision tree. Word recognition is performed in three steps: 1) initial
hypothesis generation, 2) word graph generation, 3) final hypoth-
esis generation. The initial hypotheses are used for cluster-based
acoustic model adaptation using the MLLR technique [13] prior to
word graph generation. A 3-gram LM is used in the first two de-
coding steps. The final hypotheses are generated with a 4-gram LM
and acoustic models adapted with the hypotheses of step 2.
In the baseline system used in DARPA evaluation tests, the acous-
tic models were trained on about 150 hours of audio data from the
DARPA Hub4 Broadcast News corpus (the LDC 1996 and 1997
Broadcast News Speech collections) [9]. Gender-dependent acous-
tic models were built using MAP adaptation of SI seed models for
wide-band and telephone band speech [7]. The models contain
28000 position-dependent, cross-word triphone models with 11700
tied states and approximately 360k Gaussians [8].
The baseline language models are obtained by interpolation of
models trained on 3 different data sets (excluding the test epochs):
about 790M words of newspaper and newswire texts; 240M word
of commercial broadcast news transcripts; and the transcriptions of
the Hub4 acoustic data. The recognition vocabulary contains 65120
words and has a lexical coverage of over 99% on all evaluation test
sets from the years 1996-1999. A pronunciation graph is associated
with each word so as to allow for alternate pronunciations. The
pronunciations make use of a set of 48 phones set, where 3 phone
units represent silence, filler words, and breath noises. The lexicon
contains compound words for about 300 frequent word sequences,
as well as word entries for common acronyms, providing an easy
way to allow for reduced pronunciations [6].
The LIMSI 10x system obtained a word error of 17.1% on the
1999 DARPA/NIST evaluation set and can transcribe unrestricted
broadcast data with a word error of about 20% [8].
3. TASK INDEPENDENCE
Our first step in developing a ?generic? speech transcription en-
gine is to assess the most generic system we have under cross-
task conditions, i.e., by recognizing task-specific data with a rec-
ognizer developed for a different task. Three representative tasks
have been retained as target tasks: small vocabulary recognition
(TI-digits), goal-oriented human-machine spoken dialog (ATIS),
and dictation of texts (WSJ). The broadcast news transcription task
(Hub4E) serves as the baseline. The main criteria for the task se-
lection were that they are realistic enough and task-specific data
should be available. The characteristics of these four tasks and the
available corpora are summarized in Table 1.
For the small vocabulary recognition task, experiments are car-
ried out on the adult speaker portion of the TI-digits corpus [14],
containing over 17k utterances from a total of 225 speakers. The
vocabulary contains 11 words, the digits ?1? to ?9?, plus ?zero? and
?oh?. Each speaker uttered two versions of each digit in isolation
and 55 digit strings. The database is divided into training and test
sets (roughly 3.5 hours each, corresponding to 9k strings). The
speech is of high quality, having been collected in a quiet environ-
ment. The best reported WERs on this task are around 0.2-0.3%.
The digit phonemic coverage being very low, only 108 context-
dependent models are used in our recognition system. The task-
Table 2: Word error rates (%) for BN98, TI-digits, ATIS94,
WSJ95 and S9 WSJ93 test sets after recognition with three dif-
ferent configurations: (left) BN acoustic and language models;
(center) BN acoustic models combined with task-specific lex-
ica and LMs and (right) task-dependent acoustic and language
models.
Test Set BN models Task LMs Task models
BN98 13.6 13.6 13.6
TI-digits 17.5 1.7 0.4
ATIS94 22.7 4.7 4.4
WSJ95 11.6 9.0 7.6
S9 WSJ93 12.1 13.6 15.3
specific LM for the TI-digits is a simple grammar allowing any se-
quence of up to 7 digits. Our task-dependent system performance
is 0.4% WER.
The DARPA Air Travel Information System (ATIS) task is cho-
sen as being representative of a goal-oriented human-machine di-
alog task, and the ARPA 1994 Spontaneous Speech Recognition
(SPREC) ATIS-3 data (ATIS94) [4] is used for testing purposes.
The test data amounts for nearly 5 hours of speech from 24 speakers
recorded with a close-talking microphone. Around 40h of speech
data are available for training. The word error rates for this task in
the 1994 evaluation were mainly in the range of 2.5% to 5%, which
we take as state-of-the-art for this task. The acoustic models used
in our task-specific system include 1641 context-dependent phones
with 4k independent HMM states. A back-off trigram language
model has been estimated on the transcriptions of the training ut-
terances. The lexicon contains 1300 words, with compounds words
for multi-word entities in the air-travel database (city and airport
names, services etc.). The WER obtained with our task-dependent
system is 4.4%.
For the dictation task, the Wall Street Journal continuous speech
recognition corpus [17] is used, abiding by the ARPA 1995 Hub3
test (WSJ95) conditions. The acoustic training data consist of 100
hours of speech from a total of 355 speakers taken from the WSJ0
and WSJ1 corpora. The Hub3 baseline test data consist of stu-
dio quality read speech from 20 speakers with a total duration of
45 minutes. The best result reported at the time of the evaluation
was 6.6%. A contrastive experiment is carried out with the WSJ93
Spoke 9 data comprised of 200 spontaneous sentences spoken by
journalists [11]. The best performance reported in the 1993 evalua-
tion on the spontaneous data was 19.1% [18], however lower word
error rates have since been reported on comparable test sets (14.1%
on the WSJ94 Spoke 9 test data). 21000 context and position-
dependent models have been trained for the WSJ system, with 9k
independent HMM states. A 65k-word vocabulary was selected
and a back-off trigram model obtained by interpolating models trained
on different data sets (training utterance transcriptions and newspa-
pers data). The task-dependent WSJ system has a WER of 7.6% on
the read speech test data and 15.3% on the spontaneous data.
For the BN transcription task, we follow the conditions of the
1998 ARPA Hub4E evaluation (BN98) [15]. The acoustic training
data is comprised of 150 hours of North-American TV and radio
shows. The best overall result on the 1998 baseline test was 13.5%.
Three sets of experiments are reported. The first are cross-task
recognition experiments carried out using the BN acoustic and lan-
guage models to decode the test data for the other tasks. The second
set of experiments made use of mixed models, that is the BN acous-
tic models and task-specific LMs. Due to the different evaluation
paradigms, some minor modifications were made in the transcrip-
tion procedure. First of all, in contrast with the BN data, the data
for the 3 tasks is already segmented into individual utterances so
the partitioning step was eliminated. With this exception, the de-
coding process for the WSJ task is exactly the same as described in
the previous section. For the TI-digits and ATIS tasks, word decod-
ing is carried out in a single trigram pass, and no speaker adaptation
was performed.
The WERs obtained for the three recognition experiments are
reported in Table 2. A comparison with Table 1 shows that the
performances of the task-dependent models are close to the best re-
ported results even though we did not devote too much effort in op-
timizing these models. We can also observe by comparing the task-
dependent (Table 2, right) and mixed (Table 2, middle) conditions,
that the BN acoustic models are relatively generic. These mod-
els seem to be a good start towards truly task-independent acoustic
models. By using task-specific language models For the TI-digits
and ATIS we can see that the gap in performance is mainly due
a linguistic mismatch. For WSJ the language models are more
closely matched to BN and only a small 1.6% WER reduction is
obtained. On the spontaneous journalist dictation (WSJ S9 spoke)
test data there is even an increase in WER using the WSJ LMs,
which can be attributed to a better modelization of spontaneous
speech effects (such as breath and filler words) in the BN models.
Prior to introducing our approach for lightly supervised acoustic
model training, we describe our standard training procedure in the
next section.
4. ACOUSTIC MODEL TRAINING
HMM training requires an alignment between the audio signal
and the phone models, which usually relies on a perfect ortho-
graphic transcription of the speech data and a good phonetic lex-
icon. In general it is easier to deal with relatively short speech seg-
ments so that transcription errors will not propagate and jeopardize
the alignment. The orthographic transcription is usually considered
as ground truth and training is done in a closely supervised man-
ner. For each speech segment the training algorithm is provided
with the exact orthographic transcription of what was spoken, i.e.,
the word sequence that the speech recognizer should hypothesize
when confronted with the same speech segment.
Training acoustic models for a new corpus (which could also re-
flect a change of task and/or language), usually entails the follow-
ing sequence of operations once the audio data and transcription
files have been loaded:
1. Normalize the transcriptions to a common format (some ad-
justment is always needed as different text sources make use
of different conventions).
2. Produce a word list from the transcriptions and correct blatant
errors (these include typographical errors and inconsistencies).
3. Produce a phonemic transcription for all words not in our mas-
ter lexicon (these are manually verified).
4. Align the orthographic transcriptions with the signal using ex-
isting models and the pronunciation lexicon (or bootstrap mod-
els from another task or language). This procedure often re-
jects a substantial portion of the data, particularly for long seg-
ments.
5. Eventually correct transcription errors and realign (or just ig-
nore these if enough audio data is available)
6. Run the standard EM training procedure.
This sequence of operations is usually iterated several times to
refine the acoustic models. In general each iteration recovers a por-
tion of the rejected data.
5. LIGHTLY SUPERVISED ACOUSTIC
MODEL TRAINING
One can imagine training acoustic models in a less supervised
manner, by using an iterative procedure where instead of using
manual transcriptions for alignment, at each iteration the most likely
word transcription given the current models and all the information
available about the audio sample is used. This approach still fits
within the EM training framework, which is well-suited for miss-
ing data training problems. A completely unsupervised training
procedure is to use the current best models to produce an ortho-
graphic transcription of the training data, keeping only words that
have a high confidence measure. Such an approach, while very en-
ticing, is limited since the only supervision is provided by the con-
fidence measure estimator. This estimator must in turn be trained
on development data, which needs to be small to keep the approach
interesting.
Between using carefully annotated data such as the detailed tran-
scriptions provided by the LDC and no transcription at all, there is
a wide spectrum of possibilities. What is really important is the
cost of producing the associated annotations. Detailed annotation
requires on the order of 20-40 times real-time of manual effort, and
even after manual verification the final transcriptions are not ex-
empt from errors [2]. Orthographic transcriptions such as closed-
captions can be done in a few times real-time, and therefore are
quite a bit less costly. These transcriptions have the advantage that
they are already available for some television channels, and there-
fore do not have to be produced specifically for training speech
recognizers. However, closed-captions are a close, but not exact
transcription of what is being spoken, and are only coarsely time-
aligned with the audio signal. Hesitations and repetitions are not
marked and there may be word insertions, deletions and changes
in the word order. They also are missing some of the additional
information provided in the detailed speech transcriptions such as
the indication of acoustic conditions, speaker turns, speaker identi-
ties and gender and the annotation of non-speech segments such as
music. NIST found the disagreement between the closed-captions
and manual transcripts on a 10 hour subset of the TDT-2 data used
for the SDR evaluation to be on the order of 12% [5].
Another approach is to make use of other possible sources of
contemporaneous texts from newspapers, newswires, summaries
and the Internet. However, since these sources have only an indirect
correspondence with the audio data, they provide less supervision.
The basic idea is of light supervision is to use a speech recog-
nizer to automatically transcribe unannotated data, thus generat-
ing ?approximate? labeled training data. By iteratively increasing
the amount of training data, more accurate acoustic models are ob-
tained, which can then be used to transcribe another set of unanno-
tated data. The modified training procedure used in this work is:
1. Train a language model on all texts and closed captions after
normalization
2. Partition each show into homogeneous segments and label the
acoustic attributes (speaker, gender, bandwidth) [6]
3. Train acoustic models on a very small amount of manually
annotated data (1h)
4. Automatically transcribe a large amount of training data
5. (Optional) Align the closed-captions and the automatic tran-
scriptions (using a standard dynamic programming algorithm)
6. Run the standard acoustic model training procedure on the
speech segments (in the case of alignment with the closed
captions only keep segments where the two transcripts are in
agreement)
7. Reiterate from step 4.
It is easy to see that the manual work is considerably reduced, not
only in generating the annotated corpus but also during the training
procedure, since we no longer need to extend the pronunciation lex-
icon to cover all words and word fragments occurring in the training
data and we do not need to correct transcription errors. This ba-
sic idea was used to train acoustic models using the automatically
generated word transcriptions of the 500 hours of audio broadcasts
used in the spoken document retrieval task (part of the DARPA
TDT-2 corpus used in the SDR?99 and SDR?00 evaluations) [3].
This corpus is comprised of 902 shows from 6 sources broadcast
between January and June 1998: CNN Headline News (550 30-
minute shows), ABC World News Tonight (139 30-minute shows),
Public Radio International The World (122 1-hour shows), Voice of
America VOA Today and World Report (111 1-hour shows). These
shows contain about 22k stories with time-codes identifying the
beginning and end of each story.
First, the recognition performance as a function of the available
acoustic and language model training data was assessed. Then we
investigated the accuracy of the acoustic models obtained after rec-
ognizing the audio data using different levels of supervision via
the language model. With the exception of the baseline Hub4 lan-
guage models, none of the language models include a component
estimated on the transcriptions of the Hub4 acoustic training data.
The language model training texts come from contemporaneous
sources such as newspapers and newswires, and commercial sum-
maries and transcripts, and closed-captions. The former sources
have only an indirect correspondence with the audio data and pro-
vide less supervision than the closed captions. For each set of LM
training texts, a new word list was selected based on the word fre-
quencies in the training data. All language models are formed by
interpolating individual LMs built on each text source. The interpo-
lation coefficients were chosen in order to minimize the perplexity
on a development set composed of the second set of the Nov98
evaluation data (3h) and a 2h portion of the TDT2 data from Jun98
(not included in the LM training data). The following combinations
were investigated:
 LMa (baseline Hub4 LM): newspaper+newswire (NEWS), com-
mercial transcripts (COM) predating Jun98, acoustic transcripts
 LMn t c: NEWS, COM, closed-captions through May98
 LMn t: NEWS, COM through May98
 LMn c: NEWS, closed-captions through May98
 LMn: NEWS through May98
 LMn to: NEWS through May98, COM through Dec97
 LMno: NEWS through Dec97
Table 3: Word error rate for various conditions using acous-
tic models trained on the HUB4 training data with detailed
manual transcriptions. All runs were done in less than 10xRT,
except the last row. ?1S? designates one set of gender-
independent acoustic models, whereas ?4S? designates four sets
of gender and bandwidth dependent acoustic models.
Training Conditions bn99 1 bn99 2 Average
1h 1S, LMn t c 35.2 31.9 33.3
69h 1S, LMn t c 20.2 18.0 18.9
123h 1S, LMn t c 19.3 17.1 18.0
123h 4S, LMn t c 18.5 16.1 17.1
123h 4S, LMa 18.3 16.3 17.1
123h 4S, LMa, 50x 17.1 14.5 15.6
Table 4: Word error rate for different language models and increasing quantities of automatically labeled training data on the 1999
evaluation test sets using gender and bandwidth independent acoustic models. LMn t c: NEWS, COM, closed-captions through
May98 LMn t: NEWS, COM through May98 LMn c: NEWS, closed-captions through May98 LMn: NEWS through May98
LMn to: NEWS through May98, COM through Dec97 LMno: NEWS through Dec97.
Amount of training data %WER
raw unfiltered LMn t c LMn t LMn c LMn LMn to LMno
150h 123h 18.0 18.6 19.1 20.6 18.7 20.9
1h 1h 33.3 33.7 34.4 35.9 33.9 36.1
14h 8h 26.4 27.6 27.4 29.0 27.6 30.6
28h 17h 25.2 25.7 25.6 28.1 25.7 28.9
58h 28h 24.3 25.2 25.7 27.4 25.1 27.9
It should be noted that all of the conditions include newspaper
and newswire texts from the same epoch as the audio data. These
provide an important source of knowledge particularly with re-
spect to the vocabulary items. Conditions which include the closed
captions in the LM training data provide additional supervision in
the decoding process when transcribing audio data from the same
epoch.
For testing purposes we use the 1999 Hub4 evaluation data, which
is comprised of two 90 minute data sets selected by NIST. The first
set was extracted from 10 hours of data broadcast in June 1998,
and the second set from a set of broadcasts recorded in August-
September 1998 [16]. All recognition runs were carried out in un-
der 10xRT unless stated otherwise. The LIMSI 10x system ob-
tained a word error of 17.1% on the evaluation set (the combined
scores in the penultimate row in Table 3 4S, LMa) [8]. The word
error can be reduced to 15.6% for a system running at 50xRT (last
entry in Table 3).
As can be seen in Table 3, the word error rates with our orig-
inal Hub4 language model (LMa) and the one without the tran-
scriptions of the acoustic data (LMn t c) give comparable results
using the 1999 acoustic models trained on 123 hours of manually
annotated data (123h, 4S). The quality of the different language
models listed above are compared in the first row of Table 3 us-
ing speaker-independent (1S) acoustic models trained on the same
Hub4 data (123h). As can be observed, removing any text source
leads to a degradation in recognition performance. It appears it is
more important to include commercial transcripts (LMn t), even
if they are old (LMn to) than the closed captions (LMn c). This
suggests that the commercial transcripts more accurately represent
spoken language than closed-captioning. Even if only newspaper
and newswire texts are available (LMn), the word error increases
by only 14% over the best configuration (LMn t c), and even using
older newspaper and newswire texts (LMno) does not substantially
increase the word error rate. The second row of Table 3 gives the
word error rates with acoustic models trained on only 1 hour of
manually transcribed data. These are the models used to initialize
the process of automatically transcribing large quantities of data.
These word error rates range from 33% to 36% across the language
models.
We compared a straightforward approach of training on all the
automatically annotated data with one in which the closed-captions
are used to filter the hypothesized transcriptions, removing words
that are ?incorrect?. In the filtered case, the hypothesized transcrip-
tions are aligned with the closed captions story by story, and only
regions where the automatic transcripts agreed with the closed cap-
tions were kept for training purposes. To our surprise, somewhat
comparable recognition results were obtained both with and with-
out filtering, suggesting that inclusion of the closed-captions in the
language model training material provided sufficient supervision
(see Table 5).1 It should be noted that in both cases the closed-
caption story boundaries are used to delimit the audio segments
after automatic transcription.
To investigate this further we are assessing the effects of reduc-
ing the amount of supervision provided by the language model
training texts on the acoustic model accuracy (see Table 4). With
14 hours (raw) of approximately labeled training data, the word er-
ror is reduced by about 20% for all LMs compared with training on
1h of data which has carefully manual transcriptions. Using larger
amounts of data transcribed with the same initial acoustic models
gives smaller improvements, as seen by the entries for 28h and 58h.
The commercial transcripts (LMn+t and LMn+to), even if predat-
ing the data epoch, are seen to be more important than the closed-
captions (LMn+c), supporting the earlier observation that they are
closer to spoken language. Even if only news texts from the same
period (LMn) are available, these provide adequate supervision for
lightly supervised acoustic model training.
Table 5: Word error rates for increasing quantities of auto-
matically label training data on the 1999 evaluation test sets
using gender and bandwidth independent acoustic models with
the language model LMn t c (trained on NEWS, COM, closed-
captions through May98).
Amount of training data %WER
raw unfiltered filtered unfiltered filtered
14h 8h 6h 26.4 25.7
28h 17h 13h 25.2 23.7
58h 28h 21h 24.3 22.5
140h 76h 57h 22.4 21.1
287h 140h 108h 21.0 19.9
503h 238h 188h 20.2 19.4
6. TASK ADAPTATION
The experiments reported in the section 3 show that while direct
recognition with the reference BN acoustic models gives relatively
1The difference in the amounts of data transcribed and actually
used for training is due to three factors. The first is that the total du-
ration includes non-speech segments which are eliminated prior to
recognition during partitioning. Secondly, the story boundaries in
the closed captions are used to eliminate irrelevant portions, such
as commercials. Thirdly, since there are many remaining silence
frames, only a portion of these are retained for training.
Table 6: Word error rates (%) for TI-digits, ATIS94, WSJ95 and S9 WSJ93 test sets after recognition with three different configura-
tions, all including task-specific lexica and LMs: (left) BN acoustic models, (middle left) unsupervised adaptation of the BN acoustic
models, (middle right) supervised adaptation of the BN acoustic models and (right) task-dependent acoustic models.
Test Set BN models Unsupervised Adaptation Supervised Adaptation Task-dep. models
BN models BN models
TI-digits 1.7 0.8 0.5 0.4
ATIS94 4.7 4.7 3.2 4.4
WSJ95 9.0 6.9 6.7 7.6
S9 WSJ93 13.6 12.6 11.4 15.3
competitive results, the WER on the targeted tasks can still be im-
proved. Since we want to minimize the cost and effort involved in
tuning to a target task, we are investigating methods to transpar-
ently adapt the reference acoustic models. By transparent we mean
that the procedure is automatic and can be carried out without any
human expertise. We therefore apply the approach presented in the
previous section, that is the reference BN system is used to tran-
scribe the training data of the destination task. This supposes of
course that audio data have been collected. However, this can be
carried out with an operational system and the cost of collecting
task-specific training data is greatly reduced since no manual tran-
scriptions are needed. The performance of the BN models under
cross task conditions is well within the range for which the approx-
imate transcriptions can be used for acoustic model adaptation.
The reference acoustic models are then adapted by means of a
conventional adaptation technique such as MLLR and MAP. Thus
there is no need to design a new set of models based on the training
data characteristics. Adaptation is also preferred to the training
of new models as it is likely that the new training data will have
a lower phonemic contextual coverage than the original reference
models.
The cross-task unsupervisedadaptation is evaluated for the tasks:
TI-digits, ATIS and WSJ. The 100 hours of the WSJ data were tran-
scribed using the BN acoustic and language models. For ATIS, only
26 of the 40 hours of training data from 276 speakers were tran-
scribed, due to time constraints. For TI-digits, the training data was
transcribed using a mixed configuration, combining the BN acous-
tic models with the simple digit loop grammar.2 For completeness
we also used the task-specific audio data and the associated tran-
scriptions to carry out supervised adaptation of the BN models.
Gender-dependent acoustic models were estimated using the cor-
responding gender-dependent BN models as seeds and the gender-
specific training utterances as adaptation data. For WSJ and ATIS,
the speaker ids were directly used for gender identification since
in previous experiments with this test set there were no gender
classification errors. Only the acoustic models used in the sec-
ond and third word decoding passes have been adapted. For the
TI-digits, the gender of each training utterance was automatically
classified by decoding each utterance twice, once with each set of
gender-dependent models. Then, the utterance gender was deter-
mined based on the best global score between the male and female
models (99.0% correct classification).
Both the MLLR and MAP adaptation techniques were applied.
The recognition tests were carried out under mixed conditions (i.e.,
with the adapted acoustic models and the task-dependent LM). The
2In order to assess the quality of the automatic transcription, we
compared the system hypotheses to the manually provided training
transcriptions. For resulting word error rates on the training data
are 11.8% for WSJ, 29.1% for ATIS and 1.2% for TI-digits.
BN models are first adapted using MLLR with a global transforma-
tion, followed by MAP adaptation.
The word error rates obtained with the task-adapted BN mod-
els are given in Table 6 for the four test sets. Using unsupervised
adaptation the performance is improved for TIdigits (53% relative),
WSJ (19% relative) and S9 (7% relative).
The manual transcriptions for the targeted tasks were used to
carry out supervised model adaptation. The results (see the 4th col-
umn of Table 6) show a clear improvement over unsupervisedadap-
tation for both the TI-digits (60% relative) and ATIS (47% relative)
tasks. A smaller gain of about 10% relative is obtained for the spon-
taneous dictation task, and only 3% relative for read WSJ data. The
gain appears to be correlated with the WER of the transcribed data:
the difference between BN and task specific models is smaller for
WSJ than ATIS and TI-digits. The TI-digit task is the only task for
which the best performance is obtained using task-dependent mod-
els rather than BN models adapted with supervised. For the other
tasks, the lowest WER is obtained when the supervised adapted BN
acoustic models are used: 3.2% for ATIS, 6.7% for WSJ and 11.4%
for S9. This result confirms our hypothesis that better performance
can be achieved by adapting generic models with task-specific data
than by directly training task-specific models.
7. CONCLUSIONS
This paper has explored methods to reduce the cost of developing
models for speech recognizers. Two main axes have been explored:
developing generic acoustic models and the use of low cost data for
acoustic model training.
We have explored the genericity of state-of-the-art speech recog-
nition systems, by testing a relatively wide-domain system on data
from three tasks ranging in complexity. The generic models were
taken from the broadcast news task which covers a wide range of
acoustic and linguistic conditions. These acoustic models are rel-
atively task-independent as there is only a small increase in word
error relative to the word error obtained with task-dependent acous-
tic models, when a task-dependent language model is used. There
remains a large difference in performance on the digit recogni-
tion task which can be attributed to the limited phonetic coverage
of this task. On a spontaneous WSJ dictation task, the broadcast
news acoustic and language are more robust to deviations in speak-
ing style than the read-speech WSJ models. We also have shown
that unsupervised acoustic model adaptation can reduce the perfor-
mance gap between task-independent and task-dependent acoustic
models, and that supervised adaptation of generic models can lead
to better performance than that achieved with task-specific models.
Both supervised and unsupervised adaptation are less effective for
the digits task indicating that these may be a special case.
We have investigated the use of low cost data to train acoustic
models for broadcast news transcription, with supervision provided
the language models. Recognition results obtained with acoustic
models trained on large quantities of automatically annotated data
are comparable (under a 10% relative increase in word error) to
results obtained with acoustic models trained on large quantities
of manually annotated data. Given the significantly higher cost of
detailed manual transcription (substantially more time consuming
than producing commercial transcripts, and more expensive since
closed captions and commercial transcripts are produced for other
purposes), such approaches are very promising as they require sub-
stantial computation time, but little manual effort. Another advan-
tage offered by this approach is that there is no need to extend the
pronunciation lexicon to cover all words and word fragments oc-
curring in the training data. By eliminating the need for manual
transcription, automated training can be applied to essentially un-
limited quantities of task-specific training data. While the focus of
our work has been on reducing training costs and task portability,
we have been exploring these in a multi-lingual context.
REFERENCES
[1] G. Adda, M. Jardino, J.L. Gauvain, ?Language Modeling for Broad-
cast News Transcription,? ESCA Eurospeech?99, Budapest, 4, pp.
1759-1760, Sept. 1999.
[2] C. Barras, E. Geoffrois et al,?Transcriber: development and use of a
tool for assisting speech corpora production,? SpeechCommunication,
33(1-2), pp. 5-22, Jan. 2001.
[3] C. Cieri, D. Graff, M. Liberman, ?The TDT-2 Text and Speech
Corpus,? DARPA Broadcast News Workshop, Herndon. (see also
http://morph.ldc.upenn.edu/TDT).
[4] D. Dahl, M. Bates et al, ?Expanding the Scope of the ATIS Task : The
ATIS-3 Corpus,? Proc. ARPA Spoken Language Systems Technology
Workshop, Plainsboro, NJ, pp. 3-8, 1994.
[5] J. Garofolo, C. Auzanne, E. Voorhees, W. Fisher, ?1999 TREC-8 Spo-
ken Document Retrieval Track Overview and Results,? 8th Text Re-
trieval Conference TREC-8, Nov. 1999.
[6] J.L. Gauvain, G. Adda, et al, ?Transcribing Broadcast News: The
LIMSI Nov96 Hub4 System,? Proc. ARPA Speech Recognition Work-
shop, pp. 56-63, Chantilly, Feb. 1997.
[7] J.L. Gauvain, C.H. Lee, ?Maximum a Posteriori Estimation for Mul-
tivariate Gaussian Mixture Observation of Markov Chains,? IEEE
Trans. on SAP, 2(2), pp. 291-298, April 1994.
[8] J.L. Gauvain, L. Lamel, ?Fast Decoding for Indexation of Broadcast
Data,? ICSLP?2000, 3, pp. 794-798, Beijing, Oct. 2000.
[9] D. Graff, ?The 1996 Broadcast News Speech and Language-Model
Corpus,? Proc. DARPA Speech Recognition Workshop, Chantilly, VA,
pp. 11-14, Feb. 1999.
[10] T. Kemp, A. Waibel, ?UnsupervisedTraining of a Speech Recognizer:
Recent Experiments,? Eurospeech?99, 6, Budapest, pp. 2725-2728,
Sept. 1999.
[11] F. Kubala, J. Cohen et al, ?The Hub and Spoke Paradigm for CSR
Evaluation,? Proc. ARPA SpokenLanguageSystems TechnologyWork-
shop, Plainsboro, NJ, pp. 9-14, 1994.
[12] L. Lamel, J.L. Gauvain, G. Adda, ?Lightly Supervised Acoustic
Model Training,? Proc. ISCA ITRW ASR2000, pp. 150-154, Paris,
Sept. 2000.
[13] C.J. Leggetter, P.C. Woodland, ?Maximum likelihood linear regres-
sion for speaker adaptation of continuous density hidden Markov
models,? Computer Speech & Language, 9(2), pp. 171-185, 1995.
[14] R.G. Leonard, ?A Database for speaker-independent digit recogni-
tion,? Proc. ICASSP, 1984.
[15] D.S. Pallett, J.G. Fiscus, et al ?1998 Broadcast News Benchmark Test
Results,? Proc. DARPA Broadcast News Workshop, pp. 5-12, Hern-
don, VA, Feb. 1999.
[16] D. Pallett, J. Fiscus, M. Przybocki, ?Broadcast News 1999 Test Re-
sults,? NIST/NSA Speech Transcription Workshop, College Park, May
2000.
[17] D.B. Paul, J.M. Baker, ?The Design for the Wall Street Journal-based
CSR Corpus,? Proc. ICSLP, Kobe, Nov. 1992.
[18] G. Zavaliagkos, T. Anastsakos et al, ?ImprovedSearch, Acoustic, and
Language Modeling in the BBN BYBLOS Large Vocabulary CSR
Systems,? Proc. ARPA Spoken Language Systems Technology Work-
shop, Plainsboro, NJ, pp. 81-88, 1994.
[19] G. Zavaliagkos, T. Colthurst, ?Utilizing Untranscribed Training Data
to Improve Performance,? DARPA Broadcast News Transcription and
Understanding Workshop, Landsdowne, pp. 301-305, Feb. 1998.
Proceedings of NAACL HLT 2009: Short Papers, pages 61?64,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Bayesian Networks for Semantic Frame Composition
in a Spoken Dialog System
Marie-Jean Meurs, Fabrice Lefe`vre and Renato de Mori
Universite? d?Avignon et des Pays de Vaucluse
Laboratoire Informatique d?Avignon (EA 931), F-84911 Avignon, France.
{marie-jean.meurs,fabrice.lefevre,renato.demori}@univ-avignon.fr
Abstract
A stochastic approach based on Dynamic
Bayesian Networks (DBNs) is introduced for
spoken language understanding. DBN-based
models allow to infer and then to compose
semantic frame-based tree structures from
speech transcriptions. Experimental results on
the French MEDIA dialog corpus show the
appropriateness of the technique which both
lead to good tree identification results and can
provide the dialog system with n-best lists of
scored hypotheses.
1 Introduction
Recent developments in Spoken Dialog Systems
(SDSs) have renewed the interest for the extrac-
tion of rich and high-level semantics from users?
utterances. Shifting every SDS component from
hand-crafted to stochastic is foreseen as a good op-
tion to improve their overall performance by an in-
creased robustness to speech variabilities. For in-
stance stochastic methods are now efficient alter-
natives to rule-based techniques for Spoken Lan-
guage Understanding (SLU) (He and Young, 2005;
Lefe`vre, 2007).
The SLU module links up the automatic speech
recognition (ASR) module and the dialog manager.
From the user?s utterance analysis, it derives a repre-
sentation of its semantic content upon which the di-
alog manager can decide the next best action to per-
form, taking into account the current dialog context.
In this work, the overall objective is to increase the
relevancy of the semantic information used by the
system. Generally the internal meaning representa-
tion is based on flat concept sets obtained by either
keyword spotting or conceptual decoding. In some
cases a dialog act can be added on top of the concept
set. Here we intend to consider an additional se-
mantic composition step which will capture the ab-
stract semantic structures conveyed by the basic con-
cept representation. A frame formalism is applied to
specify these nested structures. As such structures
do not rely on sequential constraints, pure left-right
branching semantic parser (such as (He and Young,
2005)) will not apply in this case.
To derive automatically such frame meaning rep-
resentations we propose a system based on a two
decoding step process using dynamic Bayesian net-
works (DBNs) (Bilmes and Zweig, 2002): first ba-
sic concepts are derived from the user?s utterance
transcriptions, then inferences are made on sequen-
tial semantic frame structures, considering all the
available previous annotation levels (words and con-
cepts). The inference process extracts all possible
sub-trees (branches) according to lower level infor-
mation (generation) and composes the hypothesized
branches into a single utterance-span tree (composi-
tion). A hand-craft rule-based approach is used to
derive the seed annotated training data. So both ap-
proaches are not competing and the stochastic ap-
proach is justified as only the DBN system is able
to provide n-best lists of tree hypotheses with confi-
dence scores to a stochastic dialog manager (such as
the very promising POMDP-based approaches).
The paper is organized as follows. The next sec-
tion presents the semantic frame annotation on the
MEDIA corpus. Then Section 3 introduces the DBN-
based models for semantic composition and finally
Section 4 reports on the experiments.
61
HOTEL LOCATION
location_event
LODGING
lodging_hotel lodging_location
frame
frame frame
FE FE
FE
Figure 1: Frames, FEs and relations associated to the se-
quence ?staying in a hotel near the Festival de Cannes?
2 Semantic Frames on the MEDIA corpus
MEDIA is a French corpus of negotiation di-
alogs among users and a tourist information phone
server (Bonneau-Maynard et al, 2005). The corpus
contains 1,257 dialogs recorded using a Wizard of
Oz system. The semantic corpus is annotated with
concept-value pairs corresponding to word segments
with the addition of specifier tags representing some
relations between concepts. The annotation utilizes
83 basic concepts and 19 specifiers.
Amongst the available semantic representations,
the semantic frames (Lowe et al, 1997) are probably
the most suited to the task, mostly because of their
ability to represent negotiation dialogs. Semantic
frames are computational models describing com-
mon or abstract situations involving roles, the frame
elements (FEs). The FrameNet project (Fillmore et
al., 2003) provides a large frame database for En-
glish. As no such resource exists for French, we
elaborated a frame ontology to describe the semantic
knowledge of the MEDIA domain. The MEDIA on-
tology is composed of 21 frames and 86 FEs. All are
described by a set of manually defined patterns made
of lexical units and conceptual units (frame and FE
evoking words and concepts). Figure 1 gives the an-
notation of word sequence ?staying in a hotel near
the Festival de Cannes?. The training data are auto-
matically annotated by a rule-based process. Pattern
matching triggers the instantiation of frames and
FEs which are composed using a set of logical rules.
Composition may involve creation, modification or
deletion of frame and FE instances. About 70 rules
are currently used. This process is task-oriented and
is progressively enriched with new rules to improve
its accuracy. A reference frame annotation for the
training corpus is established in this way and used
for learning the parameters of the stochastic models
introduced in the next section.
concept concept
conecone
concepttrans concepttrans
FrameFEFrameFE
Frame-FE transFrame-FE trans
Frame trans Frame trans
concept concept
pntrapntra
conecone
concepttrans concepttrans
psps
FE transFE trans
Figure 2: Frames, FEs as one or 2 unobserved variables
concept concept
conecone
conceptrtasnF concepttasnF
masEemasEe
masEertasnFmasEertasnF
concept concept
pntrapntra
conecone
concepttasnF
concepttasnF
masEertasnF masEertasnF
psps
m-rtasnFm-rtasnF
Figure 3: 2-level decoding of frames and FEs
3 DBN-based Frame Models
The generative DBN models used in the system are
depicted on two time slices (two words) in figures 2
and 3. In practice, a regular pattern is repeated suffi-
ciently to fit the entire word sequence. Shaded nodes
are observed variables whereas empty nodes are hid-
den. Plain lines represent conditional dependencies
between variables and dashed lines indicate switch-
ing parents (variables modifying the conditional re-
lationship between others). An example of a switch-
ing parent is given by the trans nodes which in-
fluence the frame and FE nodes: when trans node
is null the frame or FE stays the same from slice to
slice, when trans is 1 a new frame or FE value is
predicted based on the values of its parent nodes in
the word sequence using frame (or FE) n-grams.
In the left DBN model of Figure 2 frames and FEs
are merged in a single compound variable. They
are factorized in the right model using two variables
jointly decoded. Figure 3 shows the 2-level model
where frames are first decoded then used as observed
values in the FE decoding step. Merging frames and
FEs into a variable reduces the decoding complex-
ity but leads to deterministic links between frames
62
and FEs. With their factorization, on the contrary, it
is possible to deal with the ambiguities in the frame
and FE links. During the decoding step, every com-
bination is tested, even not encountered in the train-
ing data, by means of a back-off technique. Due
to the increase in model complexity, a sub-optimal
beam search is applied for decoding. In this way,
the 2-level approach reduces the complexity of the
factored approach while preserving model general-
ization.
Because all variables are observed at training
time, the edge?s conditional probability tables are
directly derived from observation counts. To im-
prove their estimates, factored language models
(FLMs) are used along with generalized parallel
backoff (Bilmes and Kirchhoff, 2003). Several FLM
implementations of the joint distributions are used
in the DBN models, corresponding to the arrows in
Figures 2 and 3. In the FLMs given below, n is the
history length (n = 1 for bigrams), the uppercase
and lowercase letters FFE, F , FE, C and W re-
spectively stand for frame/FE (one variable), frame,
FE, concept and word variables:
? Frame/FE compound variable:
P (FFE) '?nk=0 P (ffek|ffek?1);
P (C|FFE) '?nk=0 P (ck|ck?1, ffek);
P (W |C,FFE) '?nk=0 P (wk|wk?1, ck, ffek).
? Frame and FE variables, joint decoding:
P (F ) '?nk=0 P (fk|fk?1);
P (FE|F ) '?nk=0 P (fek|fek?1, fk);
P (C|FE,F ) '?nk=0 P (ck|ck?1, fek, fk);
P (W |C,FE, F ) '?nk=0 P (wk|wk?1, ck, fek, fk).
? Frame and FE variables, 2-level decoding:
? First stage: same as frame/FE compound variables
but only decoding frames
? Second stage: same as joint decodind but frames are
observed
P (F? ) '?nk=0 P (f?k|f?k?1);
P (FE|F? ) '?nk=0 P (fek|fek?1, f?k);
P (C|F? , FE) '?nk=0 P (ck|ck?1, f?k, fek);
P (W |C, F? , FE) '?nk=0 P (wk|wk?1, ck, f?k, fek).
Variables with hat have observed values.
Due to the frame hierarchical representation,
some overlapping situations can occurred when de-
termining the frame and FE associated to a concept.
To address this difficulty, a tree-projection algorithm
is performed on the utterance tree-structured frame
annotation and allows to derive sub-branches associ-
ated to a concept (possibly more than one). Starting
from a leaf of the tree, a compound frame/FE class
is obtained by aggregating the father vertices (either
frames or FEs) as long as they are associated to the
same concept (or none). The edges are defined both
by the frame?FE attachments and the FE?frame
sub-frame relations.
Thereafter, either the branches are considered di-
rectly as compound classes or the frame and FE in-
terleaved components are separated to produce two
class sets. These compound classes are considered
in the decoding process then projected back after-
wards to recover the two types of frame?FE con-
nections. However, some links are lost because de-
coding is sequential. A set of manually defined rules
is used to retrieve the missing connections from the
set of hypothesized branches. Theses rules are sim-
ilar to those used in the semi-automatic annotation
of the training data but differ mostly because the
available information is different. For instance, the
frames cannot anymore be associated to a particular
word inside a concept but rather to the whole seg-
ment. The training corpus provides the set of frame
and FE class sequences on which the DBN parame-
ters are estimated.
4 Experiments and Results
The DBN-based composition systems were evalu-
ated on a test set of 225 speakers? turns manually
annotated in terms of frames and FEs. The rule-
based system was used to perform a frame annota-
tion of the MEDIA data. On the test set, an aver-
age F-measure of 0.95 for frame identification con-
firms the good reliability of the process. The DBN
model parameters were trained on the training data
using jointly the manual transcriptions, the manual
concept annotations and the rule-based frame anno-
tations.
Experiments were carried out on the test set under
three conditions varying the input noise level:
? REF (reference): speaker turns manually tran-
scribed and annotated;
? SLU: concepts decoded from manual transcrip-
tions using a DBN-based SLU model comparable
to (Lefe`vre, 2007) (10.6% concept error rate);
? ASR+SLU: 1-best hypotheses of transcriptions
63
Inputs REF SLU ASR + SLU
DBN models Frames FE Links Frames FE Links Frames FE Links
frame/FEs p?/r? 0.91/0.93 0.91/0.86 0.93/0.98 0.87/0.82 0.91/0.83 0.93/0.98 0.86/0.80 0.90/0.86 0.92/0.98
(compound) F?-m 0.89 0.86 0.92 0.81 0.82 0.92 0.78 0.84 0.92
frames and FEs p?/r? 0.92/0.92 0.92/0.85 0.94/0.98 0.88/0.81 0.92/0.83 0.93/0.97 0.87/0.79 0.90/0.86 0.94/0.97
(2 variables) F?-m 0.90 0.86 0.94 0.80 0.83 0.91 0.78 0.84 0.93
frames then FEs p?/r? 0.92/0.94 0.91/0.82 0.92/0.98 0.88/0.86 0.91/0.80 0.92/0.97 0.87/0.81 0.89/0.82 0.93/0.98
(2-level) F?-m 0.91 0.83 0.93 0.83 0.80 0.90 0.79 0.80 0.92
Table 1: Precision (p?), Recall (r?) and F-measure (F?-m) on the MEDIA test set for the DBN-based frame composition
systems.
generated by an ASR system and concepts decoded
using them (14.8% word error rate, 24.3% concept
error rate).
All the experiments reported in the paper were per-
formed using GMTK (Bilmes and Zweig, 2002),
a general purpose graphical model toolkit and
SRILM (Stolcke, 2002), a language modeling
toolkit.
Table 1 is populated with the results on the test
set for the DBN-based frame composition systems
in terms of precision, recall and F-measure. For the
FE figures, only the reference FEs corresponding to
correctly identified frames are considered. Only the
frame and FE names are considered, neither their
constituents nor their order matter. Finally, results
are given for the sub-frame links between frames
and FEs. Table 1 shows that the performances of the
3 DBN-based systems are quite comparable. Any-
how the 2-level system can be considered the best
as besides its good F-measure results, it is also the
most efficient model in terms of decoding complex-
ity. The good results obtained for the sub-frame
links confirm that the DBN models combined with a
small rule set can be used to generate consistent hi-
erarchical structures. Moreover, as they can provide
hypotheses with confidence scores they can be used
in a multiple input/output context (lattices and n-best
lists) or in a validation process (evaluating and rank-
ing hypotheses from other systems).
5 Conclusion
This work investigates a stochastic process for gen-
erating and composing semantic frames using dy-
namic Bayesian networks. The proposed approach
offers a convenient way to automatically derive se-
mantic annotations of speech utterances based on
a complete frame and frame element hierarchical
structure. Experimental results, obtained on the ME-
DIA dialog corpus, show that the performance of the
DBN-based models are definitely good enough to be
used in a dialog system in order to supply the dialog
manager with a rich and thorough representation of
the user?s request semantics. Though this can also
be obtained using a rule-based approach, the DBN
models alone are able to derive n-best lists of se-
mantic tree hypotheses with confidence scores. The
incidence of such outputs on the dialog manager de-
cision accuracy needs to be asserted.
Acknowledgment
This work is supported by the 6th Framework Re-
search Program of the European Union (EU), LUNA
Project, IST contract no 33549,www.ist-luna.eu
References
J. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In NAACL
HLT.
J. Bilmes and G. Zweig. 2002. The graphical models
toolkit: An open source software system for speech
and time-series processing. In IEEE ICASSP.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
D. Mostefa, and the Media consortium. 2005. Seman-
tic annotation of the MEDIA corpus for spoken dialog.
In ISCA Eurospeech.
C.J. Fillmore, C.R. Johnson, and M.R.L. Petruck. 2003.
Background to framenet. International Journal of
Lexicography, 16.3:235?250.
Y. He and S. Young. 2005. Spoken language understand-
ing using the hidden vector state model. Speech Com-
munication, 48(3-4):262?275.
F. Lefe`vre. 2007. Dynamic bayesian networks and dis-
criminative classifiers for multi-stage semantic inter-
pretation. In IEEE ICASSP.
J.B. Lowe, C.F. Baker, and C.J. Fillmore. 1997. A frame-
semantic approach to semantic annotation. In SIGLEX
Workshop: Why, What, and How?
A. Stolcke. 2002. Srilm an extensible language model-
ing toolkit. In IEEE ICASSP.
64
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 272?275,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
k-Nearest Neighbor Monte-Carlo Control Algorithm
for POMDP-based Dialogue Systems
F. Lefe`vre?, M. Gas?ic?, F. Jurc???c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
Spoken Dialogue Systems Group
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{frfl2, mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
In real-world applications, modelling di-
alogue as a POMDP requires the use of
a summary space for the dialogue state
representation to ensure tractability. Sub-
optimal estimation of the value func-
tion governing the selection of system re-
sponses can then be obtained using a grid-
based approach on the belief space. In
this work, the Monte-Carlo control tech-
nique is extended so as to reduce training
over-fitting and to improve robustness to
semantic noise in the user input. This tech-
nique uses a database of belief vector pro-
totypes to choose the optimal system ac-
tion. A locally weighted k-nearest neigh-
bor scheme is introduced to smooth the de-
cision process by interpolating the value
function, resulting in higher user simula-
tion performance.
1 Introduction
In the last decade dialogue modelling as a Partially
Observable Markov Decision Process (POMDP)
has been proposed as a convenient way to improve
spoken dialogue systems (SDS) trainability, nat-
uralness and robustness to input errors (Young et
al., 2009). The POMDP framework models dia-
logue flow as a sequence of unobserved dialogue
states following stochastic moves, and provides a
principled way to model uncertainty.
However, to deal with uncertainty, POMDPs
maintain distributions over all possible states. But
then training an optimal policy is an NP hard
problem and thus not tractable for any non-trivial
application. In recent works this issue is ad-
dressed by mapping the dialog state representation
?Fabrice Lefe`vre is currently on leave from the Univer-
sity of Avignon, France.
space (the master space) into a smaller summary
space (Williams and Young, 2007). Even though
optimal policies remain out of reach, sub-optimal
solutions can be found by means of grid-based al-
gorithms.
Within the Hidden Information State (HIS)
framework (Young et al, 2009), policies are rep-
resented by a set of grid points in the summary be-
lief space. Beliefs in master space are first mapped
into summary space and then mapped into a sum-
mary action via the dialogue policy. The resulting
summary action is then mapped back into master
space and output to the user.
Methods which support interpolation between
points are generally required to scale well to large
state spaces (Pineau et al, 2003). In the current
version of the HIS framework, the policy chooses
the system action by associating each new belief
point with the single, closest, grid point. In the
present work, a k-nearest neighbour extension is
evaluated in which the policy decision is based on
a locally weighted regression over a subset of rep-
resentative grid points. This method thus lies be-
tween a strictly grid-based and a point-based value
iteration approach as it interpolates the value func-
tion around the queried belief point. It thus re-
duces the policy?s dependency on the belief grid
point selection and increases robustness to input
noise.
The next section gives an overview of the
CUED HIS POMDP dialogue system which we
extended for our experiments. In Section 3, the
grid-based approach to policy optimisation is in-
troduced followed by a presentation of the k-
nn Monte-Carlo policy optimization in Section 4,
along with an evaluation on a simulated user.
272
2 The CUED Spoken Dialogue System
2.1 System Architecture
The CUED HIS-based dialogue system pipelines
five modules: the ATK speech recogniser, an
SVM-based semantic tuple classifier, a POMDP
dialogue manager, a natural language generator,
and an HMM-based speech synthesiser. During
an interaction with the system, the user?s speech
is first decoded by the recogniser and an N-best
list of hypotheses is sent to the semantic classifier.
In turn the semantic classifier outputs an N-best
list of user dialogue acts. A dialogue act is a se-
mantic representation of the user action headed by
the user intention (such as inform, request,
etc) followed by a list of items (slot-value pairs
such as type=hotel, area=east etc). The
N-best list of dialogue acts is used by the dialogue
manager to update the dialogue state. Based on
the state hypotheses and the policy, a machine ac-
tion is determined, again in the form of a dialogue
act. The natural language generator translates the
machine action into a sentence, finally converted
into speech by the HMM synthesiser. The dia-
logue system is currently developed for a tourist
information domain (Towninfo). It is worth not-
ing that the dialogue manager does not contain any
domain-specific knowledge.
2.2 HIS Dialogue Manager
The unobserved dialogue state of the HIS dialogue
manager consists of the user goal, the dialogue his-
tory and the user action. The user goal is repre-
sented by a partition which is a tree structure built
according to the domain ontology. The nodes in
the partition consist mainly of slots and values.
When querying the venue database using the par-
tition, a set of matching entities can be produced.
The dialogue history consists of the grounding
states of the nodes in the partition, generated us-
ing a finite state automaton and the previous user
and system action. A hypothesis in the HIS ap-
proach is then a triple combining a partition, a user
action and the respective set of grounding states.
The distribution over all hypotheses is maintained
throughout the dialogue (belief state monitoring).
Considering the ontology size for any real-world
problem, the so-defined state space is too large for
any POMDP learning algorithm. Hence to obtain a
tractable policy, the state/action space needs to be
reduced to a smaller scale summary space. The set
of possible machine dialogue acts is also reduced
in summary space. This is mainly achieved by re-
Master Space
Masters Sppp c  uamyppp
eus Sers Sppp us SamypppMMM
Mast er S pr
cr S pr uSr pSm Mayt
Summary Space
Master Spcu rmymty
c
um
a
ycr
y rcsUsing MMIL for the High Level Semantic Annotation of the
French MEDIA Dialogue Corpus.?
Lina Maria Rojas-Barahona
LORIA/INRIA, France
lina.rojas@loria.fr
Thierry Bazillon
Univ. Avignon, France
thierry.bazillon@univ-avignon.fr
Matthieu Quignard
LORIA/INRIA, France
matthieu.quignard@loria.fr
Fabrice Lefevre
Univ. Avignon, France
fabrice.lefevre@univ-avignon.fr
Abstract
The MultiModal Interface Language formalism (MMIL) has been selected as the High Level
Semantic (HLS) formalism for annotating the French MEDIA dialogue corpus. This corpus is com-
posed of human-machine dialogues in the domain of hotel reservation and tourist information. Utter-
ances in dialogues have been previously annotated with a concept-value flat semantics for studying
and evaluating spoken language understanding modules in dialogue systems. We are now interested
in investigating the use of more complex representations to improve the understanding capability.
The MMIL intermediate language is a high level semantic formalism that bears relevant linguistic
information, from syntax up to discourse. This representation should increase the expressivity of
the current annotation though at the expense of the annotation process complexity. In this paper we
present our first attempt in defining the annotation guidelines for the HLS annotation of the MEDIA
corpus and its effect on the annotation process itself, revealed by annotators? disagreements due to
the different levels of hierarchy and the granularity of the features defined in MMIL.
1 Introduction
MMIL is an ontology-oriented representation language that has been used in several natural language
processing (NLP) applications, Denis et al (2010). It permits the integration of divergent resources in
distributed systems as well as the representation of various levels of linguistic analysis. In this work we
are particularly interested in exploring the representation of these linguistic levels for analyzing utter-
ances in the context of human-machine interactions. To be able to evaluate the representation on a large
set of data the French MEDIA dialogue corpus is used, Bonneau-Maynard et al (2005). The MEDIA
corpus collects about 70 hours of spontaneous speech in the task of hotel room reservation and tourist
information. It has been created using a Wizard-of-Oz technique, as a consequence, the utterances are
made of many disfluencies, hesitations, false starts, truncations or fillers words (e.g., euh or ben). Thus,
the syntactic analysis is relevant for keeping valuable information for further processing (e.g., reference
resolution). The semantics describe fine grained predicates, arguments and features based on the domain
knowledge. Similarly, the possibility of link references for pragmatic analysis and the representation of
the illocutionary force of utterances are relevant to improve the understanding in NLP applications. We
selected MMIL for the semantic annotation because it supports the representation of all these features.
Although these features enrich the semantic annotation of utterances in the corpus, they also increase
the complexity of the annotation and compromise the agreement between annotators. The possibility
of representing different instantiations in MMIL has been the main cause of disagreement between an-
notators. On the one hand, linguists tend to annotate the surface form of the utterance. On the other
?This work is supported by the French Agence Nationale de la Recherche (ANR) and is part of the Project PORT-MEDIA
(www.port-media.org).
375
hand, application designers are more biased towards its canonical representation by keeping relevant
task oriented actions and features. The trade-off between these two lines of representation is significant
for building appropriately the annotation guidelines for the semantic annotation. The annotation would
keep the most valuable information in a multilevel representation for enhancing the understanding ca-
pability of NLP applications. In this paper we introduce briefly MMIL and we describe the annotation
methodology and the inter-annotation agreement.
2 The High Level Representation
MMIL permits the representation of communicative actions that are represented as components. A com-
ponent is a structure that gathers the communicative event and its propositional content. Components
are made up of two main types of entities: events, which are entities anchored in the time dimension,
and participants, which are entities not bounded by time. Entities are linked together by relations and
are described by sets of features (i.e. pairs of attribute-value), Denis et al (2010). Every component
has a unique communicative event with the illocutionary force represented by means of the dialogueAct
feature. The propositional content is represented as a main event with its arguments, which can be either
events or participants, linked to the communicative event by a relation propContent. In this represen-
tation, predicates are usually represented as events and predicate arguments are usually represented as
participants. Relations between participants and events usually describe the thematic roles.
French: "/1euh vous venez de dire que pre?ce?demment qu? il n? a y avait plus de chambres disponibles a` ces dates et maintenant
vous en avez/2 donc je voulais juste m? assurer qu? au Novotel vous avez bien une chambre double euh pour un couple avec un
enfant avec une baignoire dans la chambre euh il me il me faut un Parc ?a? proximite? et euh cent dix euros maximum la nuit
est-ce-que vous pouvez ve?rifier"
English:"/1um you just said earlier that there are not more rooms available on these dates and now there are/2 so I just
wanted to be sure that you have at the Novotel a double room for uh a couple with one child with a bath in the room uh I
need a park nearby and uh hundred and ten euros up at night is that you can check"
Figure 1: Example of a complex utterance of the MEDIA Corpus.
Speak
Inform
Comprendre
(Understand)
negative
Coordination
adversative
State
State
negative
Pe?riodeDe
Temps
(Time)
demonstrat.
Chambre
(Room)
disponible
propContent
patient
member
memberpatient
aPe?riodeRe?servation
Speak
RequestAck
State
Chambre
(Room)
indefinite
Hotel
Couple
location.
Relative
proche
(near)
parc
(park)
Enfant
(Child)
Prix
(Price)
inferieur
(lower)
110
euros
propContent
patient
aBe?ne?ficiaires
attribute
aLocalisation
aPrix
Figure 2: HLS as an abstraction of the meaning of the French utterance shown in Figure 1. Left: this component expresses the inform
of a misunderstanding of the first segment (?/1" in Figure 1). Right: this component is a request acknowledgment, representing the second
segment(?/2" in Figure 1). Note that events are exemplify by square boxes while participants are exemplify by ellipses.
376
Let us focus on the MMIL representation for a typical utterance of the MEDIA corpus, given in
Figure 1. In this utterance the user first announces an inconsistency, then asks for clarification. Thus,
two MMIL components with different communicative actions, inform and request acknowledgment, have
been used, as shown in Figure 2. The component on the left has a main event that describes the misun-
derstanding expressed in the first segment1 of the utterance. It is represented by the ontological concept
?Understand" and by the syntactic feature polarity with the negative value. It also contains a coordinated
entity mirroring an adversative coordination between two events, state. The event state represents the
status of something, therefore the negated state event can be understood as ?there are not more rooms
available on these dates" while the positive state represents ?now there are". The participants symbolize
the arguments ?rooms" and ?dates" respectively. The component on the right expresses the clarification
request of the second segment. It verifies the status of the hotel with the specific constraints.
3 The Annotation Methodology
In the process of defining the annotation guidelines, we elaborated a specification document that de-
scribes the representation of dialogue acts, events and exemplifies the high-level semantics. Moreover, it
delves into the methodology that might be applied for the automatic and manual annotation. Afterwards,
a linguist expert and a project designer were in charge of defining the annotation guidelines. For this
purpose, they annotated manually a subset of utterances which were supposed to be representative of
the most complex aspects of the HLS annotation, in terms of their semantic constituents. 330 utterances
were selected. They are all directly related to the reservation task (first two rows in Figure 4) and mostly
occurred in the first 3 turns of the dialogues when the user is describing his goal, defined as an overall
objective along with a set of constraints. Hereafter, we present the preliminary evaluation of the experts?
agreement on these utterances.
The annotation process has been supported by an annotation tool: ATool. It accesses two knowledge-
bases, one for the MMIL formalism and the other for the MEDIA domain. The latter is adapted from
the MEDIA evaluation campaign, Bonneau-Maynard et al (2006). ATool permits annotators to navi-
gate through utterances, while displaying the MMIL representation. Annotators can design the MMIL
components graphs, define the MMIL entities by associating features, values and segment. ATool will
suggest the possible features and values for the MMIL formalism and for the domain according to the
knowledge-bases ensuring the integrity of the constructed MMIL components in the annotation.
The MEDIA corpus is rich in expressions that evoke several communicative actions. Figure 4 shows
a few examples. For the purpose of the task, we are interested in the underlying meaning of sentences,
thus politeness and indirectness are discarded from the HLS representation. For this reason, in requests
the speaker is the patient, while the hearer is the agent (see Figure 4). Because when translating the
utterance into its deep instantiation, the speaker will benefit from the execution of the action, while the
hearer has the obligation to perform the action. All the expressions in the corpus that bear the seman-
tics of ?command for a reservation" (e.g., je veux re?server, je souhaite re?server, je voudrais faire une
re?servation, j?aimerais faire une re?servation, all equivalent to I would like to reserve), have been normal-
ized with the deep component shown in Figure 3, exemplifying unequivocally the user?s desire to request
for a reservation. The possible arguments and roles have been detailed in the domain knowledge-base.
As a consequence the knowledge-base defines relations between hotels, rooms, customers, prices, equip-
ments, services, locations and dates. Besides, the grammatical relations and features, such as coordina-
tion, have been defined in the MMIL knowledge-base. Coordination is indicated with the ?coordtype"
feature and it is used in cases of conjunction (je veux une chambre simple et deux chambres double, I
want a single room and two double bedrooms), disjunction (Paris ou en proche banlieue, in Paris or
suburbs) or adversation (en ville mais pas trop loin de la mer, in the city but not too far from the sea).
For annotating events we can find the main verb in the utterance and represent it as the main event
in MMIL by following a domain-specific classification of verbs, from which Figure 4 shows some
equivalences among dialogue acts and verbs. For each participant or event, several features can be
1Segments are sequence of words that are depicted as ?/i", where i is the number of the segment.
377
Speak
Request
Reserver
je arg0 argi
propContent
patient[0]
patient[1] patient[n]
Figure 3: Canonical representation of a booking request in
MEDIA.
D. Act EvType Examples Semantic Roles
Request Reserver re?server [la chambre] aObjetRe?serve?
re?server [pour le
troisie`me
week-end de novembre
une nuit]
aPe?riodeRe?servation
[a? Clermont-Ferrand] aLocalisation
[pour quatre chambres
doubles]
aObjetRe?serve?
Inform Inform [j?] ai des informations
supple?mentaires
agent
Request Inform [j?] aurais aim?l? avoir
exactement [les dates]
patient[0],
patient[1]
Request State [Il] est [?a? combien] patient, aPrix
Request Repeter pouvez-[vous] re?pe?ter agent
Inform Repeter [je] vais me re?pe?ter agent
Accept oui
Reject non
Figure 4: Some of the observed dialogue acts and main
events with their arguments in the corpus.
added. The most important of them are ?object type" (for participants) or ?event type" (for events),
which specify their ontological concepts. They may be re?server (reserve), h?tel (hotel), chambre (room),
pe?riodedetemps (time), ville (city), person, adulte (adult), enfant (child), localisationnomme?e (places),
among others. There are more specific features, for instance, the journey dates, hotel features (e.g.,
name, standing, services, etc). Some of these features have predefined values, such as the gender of an
object (either masculine or feminine). On the other side, features such as cardinality, have not predefined
values, in that case, the annotator has to manually indicate the correct value.
Obviously, the annotation task difficulty increases with the utterance?s complexity. The representa-
tion is rather tedious to define in elliptical utterances, such as multiple reservations, in which implicit
and explicit information must be taken under consideration. Furthermore, the MMIL formalism does
not support the association of discontinuous segments to entities, generating some imprecisions in the
HLS annotation. For instance, in je voudrais une chambre pour deux personnes euh simple (I would
like a room for two people uh simple),?une chambre" (a room) and ?simple" should be linked to an
unique participant, having as object type (?Room") and as type of room ?simple". However, given that
the speaker has not mentioned ?simple"right after ?chambre", there is a new element imbricated between
them: ?pour deux personnes". As a result, the annotator must integrate the subsegment ?pour deux per-
sonnes" in the ?Room" participant. Even though this subsegment is also associated to the ?Personne"
participant.
4 Results
When analyzing the sample of 330 utterances that were annotated, we found a perfect agreement be-
tween annotators in the detection of dialogue-acts, main events, as well as main arguments. In constrast,
when measuring fine-grained features inside components we found eight types of disagreement, namely
conjunctions, disjunctions, creation of participant for simple features, groups of features inside entities,
features of entities, values of features, relation names and relation among entities. The most frequent
cases concern the first two, which refer to coordination: conjunctions (20%) and disjunctions (5%). The
inter-annotator agreement for the coordinate entities was computed, obtaining the kappa measure, Car-
letta (1996), of 0.25 for conjunctions and 0.15 for disjunctions, meaning a fair and slight agreement
respectively. Although the other cases were less frequent, the inter-annotator agreement was even lower,
indicating no agreement.
In spite of the disagreement, when measuring the global similarity between the MMIL components
created by both annotators we found a high score of 98%. This metric measures the graph similarity
378
by computing the similarity between entities and relations, including the fine-grained features inside
entities. The speech-act, main-event and main arguments are in compliance with the specifications in
both annotations.
Case Annotator 1 Annotator 2
Conjunctions 68 56
Disjunctions 18 10
Part. for simple feats. 11 0
Grouping feats. 0 2
Case Discrepancy
Features 4
Features? values 5
Name of relations. 5
Relation among entities. 2
Figure 5: Left: the Table displays the number of utterances by annotator for the listed cases. Annotator 1, is the liguist expert, Annotator 2
is the project designer. Right: fhe Table shows the number of utterances with a completely discrepant annotation: different features for same
entities, different values for same features, different relation between same entities and entities related differently in a component.
These issues show that the disagreement cases were less frequent. So far, annotators have not being
so rigourous when segmenting the text inside features. Therefore, segmentation needs to be checked in
both annotations. After this experiment, we are defining the final certified annotation and deriving the
annotation guidelines formally.
5 Discussion
Defining the annotation guidelines for high level semantic representation is controversial. The multiple
features that can be represented in the selected MMIL formalism, as well as the multiple instantiations
offer different possibilities for representing the same utterance. In general representing spoken utterances
is cumbersome, because of the linguist phenomena present in spontaneous speech. As a consequence,
annotators have to deal not only with the explicit, but also with the implicit information, and in some
cases the representations might be subjective. For these reasons, we defined the standard for the annota-
tion, and based on it, we carried out an annotation experiment on a sample of 330 complex utterances,
directly related to the reservation task; involving two annotator profiles i.e., a linguist and a project
designer. Afterwards, we measured the similarity between the annotated MMIL components and the
inter-annotation agreement obtaining a 98% of similarity and only eight major cases of disagreement,
coordination discrepancy being the most frequent. Right now, we are refining the final annotation guide-
lines based on these results. This first experiment analyzes the most complex and numerous utterances
in the corpus covering reservation requests and affirmations. Subsequently, misunderstanding, questions
and clarifications will be analyzed following the same methodology. As a result, we will be able to
reduce the disagreement between annotators in order to produce the annotation of the whole MEDIA
corpus, which will be made freely available to the research community.
References
Bonneau-Maynard, H., C. Ayache, F. Bechet, A. Denis, A. Kuhn, F. Lefe`vre, D. Mostefa, M. Quignard, S. Ros-
set, C. Servan, , and J. Villaneau (2006). Results of the french evalda-media evaluation campaign for literal
understanding. In 5th International Conference on Language Resources and Evaluation (LREC2006).
Bonneau-Maynard, H., S. Rosset, C. Ayache, A. Kuhn, and D. Mostefa (2005). Semantic annotation of the french
media dialog corpus. In INTERSPEECH-2005, 3457-3460.
Carletta, J. (1996). Assessing agreement on classification tasks: the kappa statistic. Comput. Linguist. 22(2),
249?254.
Denis, A., L. M. Rojas-Barahona, and M. Quignard. (2010). Extending MMIL semantic representation: Ex-
periments in dialogue systems and semantic annotation of corpora. In proceedings of the Fifth Joint ISO-
ACL/SIGSEM Workshop on Interoperable Semantic Annotation (ISA-5), Hong Kong, January 2010.
379
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440?446,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The LIGA (LIG/LIA) Machine Translation System for WMT 2011
Marion Potet1, Raphae?l Rubino2, Benjamin Lecouteux1, Ste?phane Huet2,
Herve? Blanchon1, Laurent Besacier1 and Fabrice Lefe`vre2
1UJF-Grenoble1, UPMF-Grenoble2
LIG UMR 5217
Grenoble, F-38041, France
FirstName.LastName@imag.fr
2Universite? d?Avignon
LIA-CERI
Avignon, F-84911, France
FirstName.LastName@univ-avignon.fr
Abstract
We describe our system for the news com-
mentary translation task of WMT 2011. The
submitted run for the French-English direction
is a combination of two MOSES-based sys-
tems developed at LIG and LIA laboratories.
We report experiments to improve over the
standard phrase-based model using statistical
post-edition, information retrieval methods to
subsample out-of-domain parallel corpora and
ROVER to combine n-best list of hypotheses
output by different systems.
1 Introduction
This year, LIG and LIA have combined their efforts
to produce a joint submission to WMT 2011 for the
French-English translation task. Each group started
by developing its own solution whilst sharing re-
sources (corpora as provided by the organizers but
also aligned data etc) and acquired knowledge (cur-
rent parameters, effect of the size of n-grams, etc.)
with the other. Both LIG and LIA systems are stan-
dard phrase-based translation systems based on the
MOSES toolkit with appropriate carefully-tuned se-
tups. The final LIGA submission is a combination
of the two systems.
We summarize in Section 2 the resources used
and the main characteristics of the systems. Sec-
tions 3 and 4 describe the specificities and report
experiments of resp. the LIG and the LIA system.
Section 5 presents the combination of n-best lists
hypotheses generated by both systems. Finally, we
conclude in Section 6.
2 System overview
2.1 Used data
Globally, our system1 was built using all the French
and English data supplied for the workshop?s shared
translation task, apart from the Gigaword monolin-
gual corpora released by the LDC. Table 1 sums up
the used data and introduces designations that we
follow in the remainder of this paper to refer to cor-
pora. Four corpora were used to build translation
models: news-c, euro, UN and giga, while three
others are employed to train monolingual language
models (LMs). Three bilingual corpora were de-
voted to model tuning: test09 was used for the de-
velopment of the two seed systems (LIG and LIA),
whereas test08 and testcomb08 were used to tune the
weights for system combination. test10 was finally
put aside to compare internally our methods.
2.2 LIG and LIA system characteristics
Both LIG and LIA systems are phrase-based trans-
lation models. All the data were first tokenized with
the tokenizer provided for the workshop. Kneser-
Ney discounted LMs were built from monolingual
corpora using the SRILM toolkit (Stolcke, 2002),
while bilingual corpora were aligned at the word-
level using GIZA++ (Och and Ney, 2003) or its
multi-threaded version MGIZA++ (Gao and Vogel,
2008) for the large corpora UN and giga. Phrase
table and lexicalized reordering models were built
with MOSES (Koehn et al, 2007). Finally, 14 fea-
tures were used in the phrase-based models:
1When not specified otherwise ?our? system refers to the
LIGA system.
440
CORPORA DESIGNATION SIZE (SENTENCES)
English-French Bilingual training
News Commentary v6 news-c 116 k
Europarl v6 euro 1.8 M
United Nation corpus UN 12 M
109 corpus giga 23 M
English Monolingual training
News Commentary v6 mono-news-c 181 k
Shuffled News Crawl corpus (from 2007 to 2011) news-s 25 M
Europarl v6 mono-euro 1.8 M
Development
newstest2008 test08 2,051
newssyscomb2009 testcomb09 502
newstest2009 test09 2,525
Test
newstest2010 test10 2,489
Table 1: Used corpora
? 5 translation model scores,
? 1 distance-based reordering score,
? 6 lexicalized reordering score,
? 1 LM score and
? 1 word penalty score.
The score weights were optimized on the test09 cor-
pus according to the BLEU score with the MERT
method (Och, 2003). The experiments led specifi-
cally with either LIG or LIA system are respectively
described in Sections 3 and 4. Unless otherwise
indicated, all the evaluations were performed using
case-insensitive BLEU and were computed with the
mteval-v13a.pl script provided by NIST. Ta-
ble 2 summarizes the differences between the final
configuration of the systems.
3 The LIG machine translation system
LIG participated for the second time to the WMT
shared news translation task for the French-English
language pair.
3.1 Pre-processing
Training data were first lowercased with the PERL
script provided for the campaign. They were also
processed in order to normalize a special French
form (named euphonious ?t?) as described in (Potet
et al, 2010).
The baseline system was built using a 4-gram LM
trained on the monolingual corpora provided last
year and translation models trained on news-c and
euro (Table 3, System 1). A significant improve-
ment in terms of BLEU is obtained when taking into
account a third corpus, UN, to build translation mod-
els (System 2). The next section describes the LMs
that were trained using the monolingual data pro-
vided this year.
3.2 Language model training
Target LMs are standard 4-gram models trained
on the provided monolingual corpus (mono-news-c,
mono-euro and news-s). We decided to test two dif-
ferent n-gram cut-off settings. The fist set has low
cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram,
3-gram and 4-gram counts), whereas the second one
(LM2) is more aggressive: 1-5-7-7. Experiment re-
sults (Table 3, Systems 3 and 4) show that resorting
to LM2 leads to an improvement of BLEU with re-
spect to LM1. LM2 was therefore used in the sub-
sequent experiments.
441
FEATURES LIG SYSTEM LIA SYSTEM
Pre-processing Text lowercased Text truecasedNormalization of French euphonious
?t?
Reaccentuation of French words start-
ing with a capital letter
LM Training on mono-news-c, news-s and
mono-euro
Training on mono-news-c and news-s
4-gram models 5-gram models
Translation model
Training on news-c, euro and UN Training on 10 M sentence pairs se-
lected in news-c, euro, UN and giga
Phrase table filtering
Use of -monotone-at-punctuation op-
tion
Table 2: Distinct features between final configurations retained for the LIG and LIA systems
3.3 Translation model training
Translation models were trained from the parallel
corpora news-c, euro and UN. Data were aligned
at the word-level and then used to build standard
phrase-based translation models. We filtered the ob-
tained phrase table using the method described in
(Johnson et al, 2007). Since this technique drasti-
cally reduces the size of the phrase table, while not
degrading (and even slightly improving) the results
on the development and test corpora (System 6), we
decided to employ filtered phrase tables in the final
configuration of the LIG system.
3.4 Tuning
For decoding, the system uses a log-linear com-
bination of translation model scores with the LM
log-probability. We prevent phrase reordering over
punctuation using the MOSES option -monotone-at-
punctuation. As the system can be beforehand tuned
by adjusting the log-linear combination weights on
a development corpus, we used the MERT method
(System 5). Optimizing weights according to BLEU
leads to an improvement with respect to the sys-
tem with MOSES default value weights (System 5
vs System 4).
3.5 Post-processing
We also investigated the interest of a statistical
post-editor (SPE) to improve translation hypotheses.
About 9,000 sentences extracted from the news do-
main test corpora of the 2007?2009 WMT transla-
tion tasks were automatically translated by a sys-
tem very similar to that described in (Potet et al,
2010), then manually post-edited. Manual correc-
tions of translations were performed by means of the
crowd-sourcing platform AMAZON MECHANICAL
TURK2 ($0.15/sent.). These collected data make
a parallel corpus whose source part is MT output
and target part is the human post-edited version of
MT output. This are used to train a phrase-based
SMT (with Moses without the tuning step) that au-
tomatically post-edit the MT output. That aims at
learning how to correct translation hypotheses. Sys-
tem 7 obtained when post-processing MT 1-best out-
put shows a slight improvement. However, SPE was
not used in the final LIG system since we lacked
time to apply SPE on the N-best hypotheses for the
development and test corpora (the N-best being nec-
essary for combination of LIG and LIA systems).
Ths LIGA submission is thus a constrained one.
3.6 Recasing
We trained a phrase-based recaser model on the
news-s corpus using the provided MOSES scripts
and applied it to uppercase translation outputs. A
common and expected loss of around 1.5 case-
sensitive BLEU points was observed on the test cor-
pus (news10) after applying this recaser (System 7)
with respect to the score case-insensitive BLEU pre-
viously measured.
2http://www.mturk.com/mturk/welcome
442
? SYSTEM DESCRIPTION BLEU SCORE
test09 test10
1 Training: euro+news-c 24.89 26.01
2 Training: euro+news-c+UN 25.44 26.43
3 2 + LM1 24.81 27.19
4 2 + LM2 25.37 27.25
5 4 + MERT on test09 26.83 27.53
6 5 + phrase-table filtering 27.09 27.64
7 6 + SPE 27.53 27.74
8 6 + recaser 24.95 26.07
Table 3: Incremental improvement of the LIG system in
terms of case-insensitive BLEU (%), except for line 8
where case-sensitive BLEU (%) are reported
4 The LIA machine translation system
This section describes the particularities of the MT
system which was built at the LIA for its first partic-
ipation to WMT.
4.1 System description
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated French
words starting with a capital letter. We significantly
cleaned up the crawled parallel giga corpus, keeping
19.3 M of the original 22.5 M sentence pairs. For ex-
ample, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with cap-
ital letters were removed. The whole training ma-
terial is truecased, meaning that the words occur-
ing after a strong punctuation mark were lowercased
when they belonged to a dictionary of common all-
lowercased forms; the others were left unchanged.
The training of a 5-gram English LM was re-
strained to the news corpora mono-news-c and news-
s that we consider large enough to ignore other data.
In order to reduce the size of the LM, we first limited
the vocabulary of our model to a 1 M word vocabu-
lary taking the most frequent words in the news cor-
pora. We also resorted to cut-offs to discard infre-
quent n-grams (2-2-3-5 thresholds on 2- to 5-gram
counts) and uses the SRILM option prune, which
allowed us to train the LM on large data with 32 Gb
RAM.
Our translation models are phrase-based models
(PBMs) built with MOSES with the following non-
default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized re-
ordering model scores were optimized on the devel-
opment corpus thanks to the MERT algorithm.
Besides the size of used data, we experimented
with two advanced features made available for
MOSES. Firstly, we filtered phrase tables using the
default setting -l a+e -n 30. This dramatically
reduced phrase tables by dividing their size by a
factor of 5 but did not improve our best configu-
ration from the BLEU score perspective (Table 4,
line 1); the method was therefore not kept in the
LIA system. Secondly, we introduced reordering
constraints in order to consider quoted material as
a block. This method is particularly useful when ci-
tations included in sentences have to be translated.
Two configurations were tested: zone markups in-
clusion around quotes and wall markups inclusion
within zone markups. However, the measured gains
were finally too marginal to include the method in
the final system.
4.2 Parallel corpus subsampling
As the only news parallel corpus provided for the
workshop contains 116 k sentence pairs, we must
resort to parallel out-of-domain corpora in order to
build reliable translation models. Information re-
trieval (IR) methods have been used in the past to
subsample parallel corpora. For example, Hilde-
brand et al (2005) used sentences belonging to the
development and test corpora as queries to select the
k most similar source sentences in an indexed paral-
lel corpus. The retrieved sentence pairs constituted
a training corpus for the translation models.
The RALI submission for WMT10 proposed a
similar approach that builds queries from the mono-
lingual news corpus in order to select sentence pairs
stylistically close to the news domain (Huet et al,
2010). This method has the major interest that it
does not require to build a new training parallel
corpus for each news data set to translate. Fol-
lowing the best configuration tested in (Huet et al,
443
2010), we index the three out-of-domain corpora us-
ing LEMUR3, and build queries from English news-s
sentences where stop words are removed. The 10 top
sentence pairs retrieved per query are selected and
added to the new training corpus if they are not re-
dundant with a sentence pair already collected. The
process is repeated until the training parallel cor-
pus reaches a threshold over the number of retrieved
pairs.
Table 4 reports BLEU scores obtained with the
LIA system using the in-domain corpus news-c and
various amounts of out-of-domain data. MERT was
re-run for each set of training data. The first four
lines display results obtained with the same num-
ber of sentence pairs, which corresponds to the
size of news-c appended to euro. The experiments
show that using euro instead of the first sentences of
UN and giga significantly improves BLEU scores,
which indicates the better adequacy of euro with re-
spect to the test10 corpus. The use of the IR method
to select sentences from euro, UN and giga leads to
a similar BLEU score to the one obtained with euro.
The increase of the collected pairs up to 3 M pairs
generates a significant improvement of 0.9 BLEU
point. A further rise of the amount of collected
pairs does not introduce a major gain since retriev-
ing 10 M sentence pairs only augments BLEU from
29.1 to 29.3. This last configuration which leads to
the best BLEU was used to build the final LIA sys-
tem. Let us note that 2 M, 3 M and 15 M queries
were required to respectively obtain 3 M, 5 M and
10 M sentence pairs because of the removal of re-
dundant sentences in the increased corpus.
For a matter of comparison, a system was also
built taking into account all the training material,
i.e. 37 M sentence pairs4. This last system is out-
performed by our best system built with IR and has
finally close performance to the one obtained with
news-c+euro relatively to the quantity of used data.
5 The system combination
System combination is based on the 500-best out-
puts generated by the LIA and the LIG systems.
3www.lemurproject.org
4For this experiment, the data were split into three parts
to build independent alignment models: news-c+euro, UN and
giga, and they were joined afterwards to build translation mod-
els.
USED PARALLEL CORPORA FILTERING
without with
news-c + euro (1.77 M) 28.1 28.0
news-c + 1.77 M of UN 27.2 -
news-c + 1.77 M of giga 27.1 -
news-c + 1.77 M with IR 28.2 -
news-c + 3 M with IR 29.1 29.0
news-c + 5 M with IR 28.8 -
news-c + 10 M with IR 29.3 29.2
All data 28.9 29.0
Table 4: BLEU (%) on test10 measured with the LIA
system using different training parallel corpora
They both used the MOSES option distinct, en-
suring that the hypotheses produced for a given sen-
tence are different inside an N-best list. Each N-best
list is associated with a set of 14 scores and com-
bined in several steps.
The first step takes as input lowercased 500-best
lists, since preliminary experiments have shown a
better behavior using only lowercased output (with
cased output, combination presents some degrada-
tions). The score combination weights are opti-
mized on the development corpus, in order to max-
imize the BLEU score at the sentence level when
N-best lists are reordered according to the 14 avail-
able scores. To this end, we resorted to the SRILM
nbest-optimize tool to do a simplex-based
Amoeba search (Press et al, 1988) on the error func-
tion with multiple restarts to avoid local minima.
Once the optimized feature weights are com-
puted independently for each system, N-best lists
are turned into confusion networks (Mangu et al,
2000). The 14 features are used to compute poste-
riors relatively to all the hypotheses in the N-best
list. Confusion networks are computed for each sen-
tence and for each system. In Table 5 we present
the ROVER (Fiscus, 1997) results for the LIA and
LIG confusion networks (LIA CNC and LIG CNC).
Then, both confusion networks computed for each
sentence are merged into a single one. A ROVER
is applied on the combined confusion network and
generates a lowercased 1-best.
The final step aims at producing cased hypothe-
ses. The LIA system built from truecased corpora
achieved significantly higher performance than the
444
LIG LIA LIG CNC LIA CNC LIG+LIA
case-insensitive test10 27.6 29.3 28.1 29.4 29.7
BLEU test11 28.5 29.4 28.5 29.3 29.9
case-sensitive test10 26.1 28.4 27.0 28.4 28.7
BLEU test11 26.9 28.4 27.5 28.4 28.8
Table 5: Performance measured before and after combining systems
LIG system trained on lowercased corpora (Table 5,
two last lines). In order to get an improvement when
combining the outputs, we had to adopt the follow-
ing strategy. The 500-best truecased outputs of the
LIA system are first merged in a word graph (and
not a mesh lattice). Then, the lowercased 1-best
previously obtained with ROVER is aligned with the
graph in order to find the closest existing path, which
is equivalent to matching an oracle with the graph.
This method allows for several benefits. The new
hypothesis is based on a ?true? decoding pass gener-
ated by a truecased system and discarded marginal
hypotheses. Moreover, the selected path offers a
better BLEU score than the initial hypothesis with
and without case. This method is better than the one
which consists of applying the LIG recaser (section
3.6) on the combined (un-cased) hypothesis.
The new recased one-best hypothesis is then used
as the final submission for WMT. Our combination
approach improves on test11 the best single sys-
tem by 0.5 case-insensitive BLEU point and by 0.4
case-sensitive BLEU (Table 5). However, it also in-
troduces some mistakes by duplicating in particular
some segments. We plan to apply rules at the seg-
ment level in order to reduce these artifacts.
6 Conclusion
This paper presented two statistical machine trans-
lation systems developed at different sites using
MOSES and the combination of these systems. The
LIGA submission presented this year was ranked
among the best MT system for the French-English
direction. This campaign was the first shot for LIA
and the second for LIG. Beside following the tradi-
tional pipeline for building a phrase-based transla-
tion system, each individual system led to specific
works: LIG worked on using SPE as post-treatment,
LIA focused on extracting useful data from large-
sized corpora. And their combination implied to ad-
dress the interesting issue of matching results from
systems with different casing approaches.
WMT is a great opportunity to chase after perfor-
mance and joining our efforts has allowed to save
considerable amount of time for data preparation
and tuning choices (even when final decisions were
different among systems), yet obtaining very com-
petitive results. This year, our goal was to develop
state-of-the-art systems so as to investigate new ap-
proaches for related topics such as translation with
human-in-the-loop or multilingual interaction sys-
tems (e.g. vocal telephone information-query di-
alogue systems in multiple languages or language
portability of such systems).
References
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates:recognizer output vot-
ing error reduction (ROVER). In Proceedings of the
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?354, Santa Barbara, CA,
USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of the
ACL Workshop: Software Engineering, Testing, and
Quality Assurance for Natural Language Processing,
pages 49?57, Columbus, OH, USA.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th confer-
ence of the European Association for Machine Trans-
lation (EAMT), Budapest, Hungary.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry, and
Philippe Langlais. 2010. The RALI machine trans-
lation system for WMT 2010. In Proceedings of the
ACL Joint 5th Workshop on Statistical Machine Trans-
lation and Metrics (WMT), Uppsala, Sweden.
Howard Johnson, Joel Martin, George Foster, and Roland
445
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, jun.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), Companion Vol-
ume, pages 177?180, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The LIG machine translation for WMT 2010.
In Proceedings of the ACL Joint 5th Workshop on Sta-
tistical Machine Translation and Metrics (WMT), Up-
psala, Sweden.
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1988. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Denver, CO, USA.
446
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 72?81,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Concept Annotation using Latent Dirichlet Allocation and
Segmental Methods
Nathalie Camelin, Boris Detienne, Ste?phane Huet, Dominique Quadri and Fabrice Lefe`vre
LIA - University of Avignon, BP 91228
84911 Avignon Cedex 09, France
{nathalie.camelin,boris.detienne,stephane.huet,dominique.quadri,fabrice.lefevre}@univ-avignon.fr
Abstract
Training efficient statistical approaches for
natural language understanding generally re-
quires data with segmental semantic annota-
tions. Unfortunately, building such resources
is costly. In this paper, we propose an ap-
proach that produces annotations in an unsu-
pervised way. The first step is an implementa-
tion of latent Dirichlet alocation that produces
a set of topics with probabilities for each topic
to be associated with a word in a sentence.
This knowledge is then used as a bootstrap to
infer a segmentation of a word sentence into
topics using either integer linear optimisation
or stochastic word alignment models (IBM
models) to produce the final semantic anno-
tation. The relation between automatically-
derived topics and task-dependent concepts is
evaluated on a spoken dialogue task with an
available reference annotation.
1 Introduction
Spoken dialogue systems in the field of information
query are basically used to interface a database with
users using speech. When probabilistic models are
used in such systems, good performance can only be
reached at the price of collecting a lot of field data,
which must be transcribed and annotated at the se-
mantic level. It becomes then possible to train effi-
cient models in a supervised manner. However, the
annotation process is costly and as a consequence
represents a real difficulty hindering the widespread
development of these systems. Therefore any means
to avoid it would be profitable as portability to new
tasks, domains or languages would be greatly facili-
tated.
To give a full description of the architecture of a
dialogue system is out of the scope of this paper. In-
stead we limit ourselves to briefly recall that once
a speech recognizer has transcribed the signal it is
common (though avoidable for very simple tasks) to
use a module dedicated to extract the meaning of
the user?s queries. This meaning representation is
then conveyed to an interaction manager that decides
upon the next best action to perform considering the
current user?s input and the dialogue history. One
of the very first steps to build the spoken language
understanding (SLU) module is the identification of
literal concepts in the word sequence hypothesised
by the speech recogniser. An example of a semantic
representation in terms of literal concept is given in
Figure 1. Once the concepts are identified they can
be further composed to form the overall meaning of
the sentence, for instance by means of a tree repre-
sentation based on hierarchical semantic frames.
To address the issue of concept tagging several
techniques are available. Some of these techniques
now classical rely on probabilistic models, that can
be either discriminative or generative. Among these,
the most efficiently studied this last decade are: hid-
den Markov models, finite state transducers, max-
imum entropy Markov models, support vector ma-
chines, dynamic fields (CRF). In (Hahn et al, 2010)
it is shown that CRFs obtain the best performance on
a tourist information retrieval task in French (ME-
DIA (Bonneau-Maynard et al, 2005)), but also in
two other comparable corpora in Italian and Polish.
To be able to apply any such technique, basic con-
72
words concept normalized value
donnez-moi null
le refLink-coRef singular
tarif object payment-amount-room
puisque connectProp imply
je voudrais null
une chambre number-room 1
qui cou?te object payment-amount-room
pas plus de comparative-payment less than
cinquante payment-amount-integer-room 50
euros payment-unit euro
Figure 1: Semantic concept representation for the query ?give me the rate since I?d like a room charged not more than
fifty euros?.
cept units have to be defined by an expert. In the best
case, most of these concepts can be derived straight-
forwardly from the pieces of information lurking in
the database tables (mainly table fields but not ex-
clusively). Some others are general (dialogic units
but also generic entities such as number, dates, etc).
However, to provide an efficient and usable informa-
tion to the reasoning modules (the dialogue manager
in our case) concepts have to be fine-grained enough
and application-dependent (even general concepts
might have to be tailored to peculiar uses). To that
extent it seems out of reach to derive the concept
definitions using a fully automatic procedure. Any-
how the process can be bootstrapped, for instance
by induction of semantic classes such as in (Siu and
Meng, 1999) or (Iosif et al, 2006). Our assumption
here is that the most time-consuming parts of con-
cept inventory and data tagging could be obtained in
an unsupervised way even though a final (but hope-
fully minimal) manual procedure is still required to
tag the classes so as to manually correct automatic
annotation.
Unlike the previous attempts cited above which
developed ad hoc approaches, we investigate here
the use of broad-spectrum knowledge extraction
methods. The notion most related to that of concept
in SLU is the topic, as used in information retrieval
systems. Anyhow for a long time, the topic detec-
tion task was limited to associate a single topic to
a document and thus was not fitted to our require-
ments. The recently proposed LDA technique al-
lows to have a probabilistic representation of a doc-
ument as a mixture of topics. Then multiple topics
can co-occur inside a document and the same topic
can be repeated. From these characteristics it is pos-
sible to consider the application of LDA to unsu-
pervised concept inventory and concept tagging for
SLU. A shortcoming is that LDA does not modelize
at all the sequentiality of the data. To address this is-
sue we propose to conclude the procedure with a fi-
nal step to introduce specific constraints for a correct
segmentation of the data: the assignments of topics
proposed by LDA are modified to be more segmen-
tally coherent.
The paper is organised as follows. Principles
of automatic induction of semantic classes are pre-
sented in Section 2, followed by the presentation of
an induction system based on LDA. The additional
step of segmentation is presented in Section 3 with
two variants: stochastic word alignment (GIZA) and
integer linear programming (ILP). Then evaluations
and results are reported in Section 4 on the French
MEDIA dialogue task.
2 Automatic induction of semantic classes
2.1 Context modeling
The idea of automatic induction of semantic classes
is based on the assumption that concepts often share
the same context (syntactic or lexical). Imple-
mented systems are based on the observation of co-
occurring words according to two different ways.
The observation of consecutive words (bigrams or
trigrams) enables the generation of lexical com-
pounds supposed to follow syntactic rules. The com-
parison of right and left contexts considering pairs
of words enables to cluster words (and word com-
pounds) into semantic classes.
73
In (Siu and Meng, 1999) and (Pargellis et al,
2001), iterative systems are presented. Their im-
plementations differ in the metrics chosen to eval-
uate the similarity during the generation of syntactic
rules and semantic classes, but also in the number
of words taken into account in a word context and
the order of successive steps (which ones to gener-
ate first: syntactic rules or semantic classes?). An
iterative procedure is executed to obtain a sufficient
set of rules in order to automatically extract knowl-
edge from the data.
While there may be still room for improvement in
these techniques we decided instead to investigate
general knowledge extraction approaches in order to
evaluate their potential. For that purpose a global
strategy based on an unsupervised machine learning
technique is adopted in our work to produce seman-
tic classes.
2.2 Implementation of an automatic induction
system based on LDA
Several approaches are available for topic detection
in the context of knowledge extraction and informa-
tion retrieval. They all more or less rely on the pro-
jection of the documents of interest in a semantic
space to extract meaningful information. However,
as the considered spaces (initial document words
and latent semantics) are discrete the performance
of the proposed approaches for the topic extraction
tasks are pretty unstable, and also greatly depend on
the quantity of data available. In this work we were
motivated by the recent development of a very at-
tractive technique with major distinct features such
as the detection of multiple topics in a single docu-
ment. LDA (Blei et al, 2003) is the first principled
description of a Dirichlet-based model of mixtures
of latent variables. LDA will be used in our work
to annotate the dialogue data in terms of topics in
an unsupervised manner. Then the relation between
automatic topics and expected concepts will be ad-
dressed manually.
Basically LDA is a generative probabilistic model
for text documents. LDA follows the assumption
that a set of observations can be explained by latent
variables. More specifically documents are repre-
sented by a mixture of topics (latent variables) and
topics are characterized by distributions over words.
The LDA parameters are {?, ?}. ? represents the
Dirichlet parameters of K latent topic mixtures as
? = [?1, ?2, . . . , ?K ]. ? is a matrix representing a
multinomial distribution in the form of a conditional
probability table ?k,w = P (w|k). Based on this rep-
resentation, LDA can estimate the probability of a
new document d of N words d = [w1, w2, . . . , wN ]
using the following procedure.
A topic mixture vector ? is drawn from the Dirich-
let distribution (with parameter ?). The correspond-
ing topic sequence ? = [k1, k2, . . . , kN ] is generated
for the whole document accordingly to a multino-
mial distribution (with parameter ?). Finally each
word is generated by the word-topic multinomial
distribution (with parameter ?, that is p(wi|ki, ?)).
After this procedure, the joint probability of ?, ? and
d is then:
p(?, ?, d|?, ?) = p(?|?)
N?
i=1
p(ki|?)p(wi|ki, ?)
(1)
To obtain the marginal probability of d, a final in-
tegration over ? and a summation over all possible
topics considering a word is necessary:
p(d|?, ?) =
?
p(?|?)
?
?
N?
i=1
?
ki
p(ki|?)p(wi|ki, ?)
?
?
(2)
The framework is comparable to that of probabilis-
tic latent semantic analysis, but the topic multino-
mial distribution in LDA is assumed to be sampled
from a Dirichlet prior and is not linked to training
documents. This approach is illustrated in Figure 2.
Training of the ? and ? parameters is possible us-
ing a corpus of documents, with a fixed number of
topics to predict. A variational inference procedure
is described in (Blei et al, 2003) which alleviates
the intractability due to the coupling between ? and
? in the summation over the latent topics. Once the
parameters for the Dirichlet and multinomial distri-
butions are available, topic scores can be derived for
any given document or word sequence.
In recent years, several studies have been carried
out in language processing based on LDA. For in-
stance, (Tam and Schultz, 2006) worked on unsuper-
vised language model adaptation; (Celikyilmaz et
al., 2010) ranked candidate passages in a question-
answering system; (Phan et al, 2008) implemented
LDA to classify short and sparse web texts.
74
LATENT DIRICHLET ALLOCATION
?                                    k w?
?
M
N
Figure 1: Graphical model representation of LDA. The boxes are ?plates? representing replicates.
The outer plate represents documents, while the inner plate represents the repeated choice
of topics and words within a document.
where p(zn | ?) is simply ? i for the unique i such that zin = 1. Integrating over ? and summing over
z, we obtain the marginal distribution of a document:
p(w | ?, ?) =
?
p(? | ?)
(
N?
n=1
?
zn
p(zn | ?) p(wn |zn, ?)
)
d ?. (3)
Finally, taking the product of the marginal probabilities of single documents, we obtain the proba-
bility of a corpus:
p(D | ?, ?) = M?
d=1
?
p(? d | ?)
(
Nd?
n=1
?
zdn
p(zdn | ? d)p(wdn |zdn, ?)
)
d ? d .
The LDA model is represented as a probabilistic graphical model in Figure 1. As the figure
makes clear, there are three levels to the LDA representation. The parameters ? and ? are corpus-
level parameters, assumed to be sampled once in the process of generating a corpus. The variables
? d are document-level variables, sampled once per document. Finally, the variables zdn and wdn are
word-level variables and are sampled once for each word in each document.
It is important to distinguish LDA from a simple Dirichlet-multinomial clustering model. A
classical clustering model would involve a two-level model in which a Dirichlet is sampled once
for a corpus, a multinomial clustering variable is selected once for each document in the corpus,
and a set of words are selected for the document conditional on the cluster variable. As with many
clustering models, such a model restricts a document to being associated with a single topic. LDA,
on the other hand, involves three levels, and notably the topic node is sampled repeatedly within the
document. Under this model, documents can be associated with multiple topics.
Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling,
where they are referred to as hierarchical models (Gelman et al, 1995), or more precisely as con-
ditionally independent hierarchical models (Kass and Steffey, 1989). Such models are also often
referred to as parametric empirical Bayes models, a term that refers not only to a particular model
structure, but also to the methods used for estimating parameters in the model (Morris, 1983). In-
deed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters
such as ? and ? in simple implementations of LDA, but we also consider fuller Bayesian approaches
as well.
997
Figure 2: Graphical representation for LDA variables
(from (Blei et al, 2003)). The grey circle is the only ob-
servable variable.
In our work LDA is employed to annotate each
user?s utterance of a dialogue corpus with topic. Ut-
terances longer than one word are included in the
training set as its sequence of words. Once the
model has been trained, inference on data corpus as-
signs the topic with the highest probability to each
word in a document. This probability is computed
from the probability of the topic to appear in the doc-
ument and the probability of the word to be gener-
ated by th opic. As a con equence we obtain a full
topic annotati n of the utterance.
Notice that LDA considers a user utterance as a
bag of words. This implies that each topic is as-
signed to a word without any consideration for its
i m diate co text. An additional segmental process
is required if we want to introduce some context in-
formation in the topic assignment.
3 Segmental annotation
3.1 Benefits of a segmental annotation
The segmental annotation of the data is not a strict
requirement for language understanding. Up to quite
recently, most approaches for literal interpretation
were limited to lexical-concept relations; for in-
stance this is the case of the Phoenix system (Ward,
1991) based on the detection of keywords. However
in an NLP perspective, the segmental approach al-
lows to connect the various levels of sentence analy-
sis (lexical, syntactic and semantic). Even though, in
order to simplify its application, segments are gen-
erally designed specifically for the semantic anno-
tation and do not have any constraint on their rela-
tion with the actual syntactic units (chunks, phrasal
groups, etc). To get relieved of such constraints not
only simplifies the annotation process itself but as
ultimately the interpretation module is to be used in-
side a spoken dialogue system, data will be noisy
and generally bound the performance of the syn-
tactic analysers (due to highly spontaneous and un-
grammatical utterances from the users, combined
with errors from the speech recognizer).
Another interesting property of segmental ap-
proach is to offer a convenient way to dissociate the
detection of a conceptual unit from the extraction of
its associated value. The value corresponds to the
nor alisation of the surface form (see last column
in 1); for instance if the segment ?not more than?
is associated to the concept comparative-payment,
its value is ?less than?. The same value would
be associated to ?not exceeding? or ?inferior to?.
Value extraction requires a link between concepts
and words based on which the normalisation prob-
lem can be addressed by means of regular expres-
sions or concept-dependent language models (even
allowing integrated approaches such as described
in (Lefe`vre, 2007)). In the case of global approaches
(not segmental), value extraction must be dealt with
directly at the level of the conceptual unit tagging,
as in (Mairesse et al, 2009). This additional level is
very complex (as some values may not be enumer-
able, such as numbers and dates) and is only afford-
able when the number of authorised values (for the
enumerable cases) is low.
To refine the LDA output, the topic-to-word align-
ment is discarded and an automatic procedure is
used to derive the best alignment between topics and
words. While the underlying probabilistic models
are pretty comparable, the major interest of this ap-
pro ch is to eparate the tasks of detecting topics and
aligning topics with words. It is then possible to in-
troduce additional constraints (such as locality, num-
ber of segments, limits on repetitions etc) in the lat-
ter task which would otherwise hinder topic detec-
tion. Conversely the alignment is self-coherent and
able to question the associations proposed during
topic detection with respect to its own constraints
only. Two approaches were designed to this pur-
pose: one based on IBM alignment models and an-
other one based on integer linear optimisation.
75
3.2 Alignment with IBM models (GIZA)
Once topic assignments for the documents in the
corpus have been proposed by LDA, a filtering pro-
cess is done to keep only the most relevant topics
of each document. The ?max most probable top-
ics are kept according to the probability p(k|wi, d)
that topic k generated the word wi of the document
d. ?max is a value fixed empirically according to
the expected set of topics in a sentence. Then, the
obtained topic sequences are disconnected from the
words. At this point, the topic and word sequences
can be considered as a translation pair to produce
a word-topic parallel corpus. These data can be
used with classical approaches in machine transla-
tion to align source and target sentences at the word
level. Since these alignment models can align sev-
eral words with a single topic, only the first occur-
rence is kept for consecutive repetitions of the same
topic. These models are expected to correct some er-
rors made by LDA, and to assign in particular words
previously associated with discarded topics to more
likely ones.
In our experiments the statistical word alignment
toolkit GIZA++ (Och and Ney, 2003) is used to
train the so-called IBM models 1-4 as well as the
HMM model. To be able to train the most informa-
tive IBM model 4, the following training pipeline
was considered: 5 iterations of IBM1, 5 iterations
of HMM, 3 iterations of IBM3 and 3 iterations of
IBM4. The IBM4 model obtained at the last iter-
ation is finally used to align words and topics. In
order to improve alignment, IBM models are usu-
ally trained in both directions (words towards con-
cepts and vice versa) and symmetrised by combin-
ing them. For this purpose, we resorted to the default
symmetrization heuristics used by MOSES, a widely
used machine translation system toolkit (Koehn et
al., 2007).
3.3 Alignment with Integer Linear
Programming (ILP)
Another approach to the re-alignment of LDA out-
puts is based on a general optimisation technique.
ILP is a widely used tool for modelling and solv-
ing combinatorial optimisation problems. It broadly
aims at modelling a decision process as a set of equa-
tions or inequations (called constraints) which are
linear with regards to so-called decision variables.
An ILP is also composed of a linear objective func-
tion. Solving an ILP consists in assigning values to
decision variables, such that all constraints are sat-
isfied and the objective function is optimised. We
refer to (Chen et al, 2010) for an overview of appli-
cations and methods of ILP.
We provide two ILP formulations for solving the
topic assignment problem related to a given docu-
ment. They both take as input data an ordered set d
of words wi, i = 1...N , a set of K available topics
and, for each word wi ? d and topic k = 1...K,
the natural logarithm of the probability p(k|wi, d)
that k is assigned to wi in the considered document
d. Model [ILP ] simply finds the highest-probability
assignment of one topic to each word in the doc-
ument, such that at most ?max different topics are
assigned.
[ILP ] : max
N?
i=1
K?
k=1
log(p(k|wi, d)) xik (3)
?K
k=1 xik = 1 i (4)
yk ? xik ? 0 i, k (5)?K
k=1 yk ? ?max (6)
xik ? {0, 1} i, k
yk ? {0, 1} k
In this model, decision variable xik is equal to 1 if
topic k is assigned to word wi, and equal to 0 other-
wise. Constraints (4) ensure that exactly one topic is
assigned to each word. Decision variable yk is equal
to 1 if topic k is used. Constraints (5) force vari-
able yk to take a value of 1 if at least one variable
xik is not null. Moreover, Constraints (6) limit the
total number of topics used. The objective function
(3) merely states that we want to maximize the total
probability of the assignment. Through this model,
our assignment problem is identified as a p-centre
problem (see (ReVelle and Eiselt, 2005) for a survey
on such location problems).
Numerical experiments show that [ILP ] tends to
give sparse assignments: most of the time, adja-
cent words are assigned to different topics even if
the total number of topics is correct. To prevent
this unnatural behaviour, we modified [ILP ] to con-
sider groups of consecutive words instead of isolated
76
words. Model [ILP seg] partitions the document
into segments of consecutive words, and assigns one
topic to each segment, such that at most ?max seg-
ments are created. For the sake of convenience, we
denote by p?(k|wij , d) =
?j
l=i log(p(k|wl, d)) the
logarithm of the probability that topic k is assigned
to all words from i to j in the current document.
[ILP seg] : max
N?
i=1
N?
j=i
K?
k=1
p?(k|wij , d) xijk (7)
i?
j=1
N?
l=i
K?
k=1
xjlk = 1 i (8)
N?
i=1
N?
j=i
K?
k=1
xijk ? ?max (9)
xijk ? {0, 1} i, j, k
In this model, decision variable xijk is equal to 1
if topic k is assigned to all words from i to j, and
0 otherwise. Constraints (8) ensure that each word
belongs to a segment that is assigned a topic. Con-
straints (9) limit the number of segments. Due to
the small size of the instances considered in this pa-
per, both [ILP ] and [ILP seg] are well solved by a
direct application of an ILP solver.
4 Evaluation and results
4.1 MEDIA corpus
The MEDIA corpus is used to evaluate the pro-
posed approach and to compare the various con-
figurations. MEDIA is a French corpus related to
the domain of tourism information and hotel book-
ing (Bonneau-Maynard et al, 2005). 1,257 dia-
logues were recorded from 250 speakers with a wiz-
ard of Oz technique (a human agent mimics an auto-
matic system). This dataset contains 17k user utter-
ances and 123,538 words, for a total of 2,470 distinct
words.
The MEDIA data have been manually transcribed
and semantically annotated. The semantic annota-
tion uses 75 concepts (e.g. location, hotel-state,
time-month. . . ). Each concept is supported by a se-
quence of words, the concept support. The null con-
cept is used to annotate every words segment that
does not support any of the 74 other concepts (and
does not bear any information wrt the task). On aver-
age, a concept support contains 2.1 words, 3.4 con-
cepts are included in a utterance and 32% of the ut-
terances are restrained to a single word (generally
?yes? or ?no?). Table 1 gives the proportions of ut-
terances according to the number of concepts in the
utterance.
# concepts 1 2 3 [4,72]
% utterances 49.4 14.1 7.9 28.6
Table 1: Proportion of user utterances as a function of the
number of concepts in the utterance.
Notice that each utterance contains at least one
concept (the null label being considered as a con-
cept). As shown in Table 2, some concepts are sup-
ported by few segments. For example, 33 concepts
are represented by less than 100 concept supports.
Considering that, we can foresee that finding these
poorly represented concepts will be hard for LDA.
[1,100[ [100,500[ [500,1k[ [1k,9k[ [9k,15k]
33 21 6 14 1 (null)
Table 2: Number of concepts according to their occur-
rence range.
4.2 Evaluation protocol
Unlike previous studies, we chose a fully automatic
way to evaluate the systems. In (Siu and Meng,
1999), a manual process is introduced to reject in-
duced classes or rules that are not relevant to the
task and also to name the semantic classes with the
appropriate label. Thus, they were able to evaluate
their semi-supervised annotation on the ATIS cor-
pus. In (Pargellis et al, 2001), the relevance of the
generated semantic classes was manually evaluated
giving a mark to each induced semantic rule.
To evaluate the unsupervised procedure it is nec-
essary to associate each induced topic with a MEDIA
concept. To that purpose, the reference annotation
is used to align topics with MEDIA concepts at the
word level. A co-occurrence matrix is computed and
each topic is associated with its most co-occurring
concept.
As MEDIA reference concepts are very fine-
grained, we also define a high-level concept hier-
77
archy containing 18 clusters of concepts. For ex-
ample, a high-level concept payment is created from
the 4 concepts payment-meansOfPayment, payment-
currency, payment-total-amount, payment-approx-
amount; a high-level concept location corresponds
to 12 concepts (location-country, location-district,
location-street, . . . ). Thus, two levels of concepts
are considered for the evaluation: high-level and
fine-level.
The evaluation is presented in terms of the classi-
cal F-measure, defined as a combination of precision
and recall measures. Two levels are also considered
to measure topic assignment quality:
? alignment corresponds to a full evaluation
where each word is considered and associated
with one topic;
? generation corresponds to the set of topics gen-
erated for a turn (no order, no word-alignment).
4.3 System descriptions
Four systems are evaluated in our experiments.
[LDA] is the result of the unsupervised learning
of LDA models using GIBBSLDA++ tool1. It as-
signs the most probable topic to each word occur-
rence in a document as described in Section 2.2.
This approach requires prior estimation of the num-
ber of clusters that are expected to be found in the
data. To find an optimal number of clusters, we ad-
justed the number K of topics around the 75 ref-
erence concepts. 2k training iterations were made
using default values for ? and ?.
[GIZA] is the system based on the GIZA++
toolkit2 which re-aligns for each sentence the topic
sequence assigned by [LDA] to word sequence as
described in Section 3.2.
[ILP ] and [ILP seg] systems are the results of
the ILP solver IBM ILOG CPLEX3 applied to the
models described in Section 3.3.
For the three last systems, the value ?max has to
be fixed according to the desired concept annota-
tion. As on average a concept support contains 2.1
words, ?max is defined empirically according to the
number of words: with i = [[2, 4]]: ?max = i with
1http://gibbslda.sourceforge.net/
2http://code.google.com/p/giza-pp/
3http://www-01.ibm.com/software/integration/optimization/cplex-
optimizer/
 56 57 58 59 60 61 62 63 64 65 66 67
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 3: F-measure of the high-level concept generation
as a function of the number of topics.
 44 46 48 50 52 54 56
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 4: F-measure of the high-level concept alignment
as a function of the number of topics.
i = [[5, 10]] words: ?max = i? 2 and for utterances
containing more than 10 words: ?max = i/2.
For the sake of simplicity, single-word utterances
are processed separately with prior knowledge. City
names, months, days or answers (e.g. ?yes?, ?no?,
?yeah?) and numbers are identified in these one-
word utterances.
4.4 Results
Examples of topics generated by [LDA], with K =
100 topics, are shown in Table 3.
Plots comparing the different systems imple-
mented w.r.t. the different evaluation levels in terms
of F-measure are reported in Figures 3, 4, 5 and 6
(high-level vs fine-level, alignment vs generation).
The [LDA] system generates topics which are
78
Topic 0 Topic 13 Topic 18 Topic 35 Topic 33 Topic 43
information time-date sightseeing politeness location answer-yes
words prob. words prob. words prob. words prob. words prob. words prob.
d? 0.28 du 0.16 de 0.30 au 0.31 de 0.30 oui 0.62
plus 0.17 au 0.11 la 0.24 revoir 0.27 Paris 0.12 et 0.02
informations 0.16 quinze 0.08 tour 0.02 madame 0.09 la 0.06 absolument 0.008
autres 0.10 dix-huit 0.07 vue 0.02 merci 0.08 pre`s 0.06 autre 0.008
de?tails 0.03 de?cembre 0.06 Eiffel 0.02 bonne 0.01 proche 0.05 donc 0.007
obtenir 0.03 mars 0.06 sur 0.02 journe?e 0.01 Lyon 0.03 jour 0.005
alors 0.01 dix-sept 0.04 mer 0.01 villes 0.004 aux 0.02 Notre-Dame 0.004
souhaite 0.003 nuits 0.04 sauna 0.01 biento?t 0.003 gare 0.02 d?accord 0.004
Table 3: Examples of topics discovered by LDA (K = 100).
 47 48 49 50 51 52 53 54 55 56 57 58
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 5: F-measure of the fine-level concept generation
as a function of the number of topics.
correctly correlated with the high-level concepts. It
can be observed that the bag of 75 topics reaches
an F-measure of 61.5% (Fig. 3). When not enough
topics are required from [LDA], induced topics are
too wide to fit the fine-grained concept annotation of
MEDIA. On the other hand if too many topics are re-
quired, the performance of bag of high-level topics
stays the same while a substantial decrease of the
F-measure is observed in the alignment evaluation
(Fig. 4). This effect can be explained by the auto-
matic alignment method chosen to transpose topics
into reference concepts. Indeed, the increase of the
number of topics makes them co-occur with many
concepts, which often leads to assign them to the
most frequent concept null in the studied corpus.
From the high-level to fine-level concept evalua-
tions, results globally decrease by 10%. An addi-
tional global loss of 10% is also observed for both
the generation and alignment scorings. In the fine-
 34 36 38 40 42 44 46 48
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 6: F-measure of the fine-level concept alignment
as a function of the number of topics.
level evaluation, a maximum F-measure of 52.2%
is observed for the generation of 75 topics (Fig. 5)
whereas the F-measure decreases to 41.5% in the
alignment evaluation (Fig. 6).
To conclude on the [LDA] system, we can see that
it generates topics having a good correlation with the
high-level concepts, seemingly the best representa-
tion level between topics and concepts. From these
results it seems obvious that an additional step is
needed to obtain a more accurate segmental annota-
tion, which is expected with the following systems.
The [GIZA] system improves the results. It is
very likely that the filtering process helps to dis-
card the irrelevant topics. Therefore, the automatic
alignment between words and the filtered topics in-
duced by [LDA] with IBM models seems more ro-
bust when more topics (a higher value for K) is re-
quired from [LDA], specifically in high-level con-
cept alignment (Fig. 4).
79
Systems based on the ILP technique perform bet-
ter than other systems whatever the evaluation. Con-
sidering [LDA] as the baseline, we can expect sig-
nificant gains of performance. For example, an F-
measure of 66% is observed for the ILP systems
considering the high-level concept generation for 75
topics (Figure 4), where the maximum for [LDA]
was 61.5%, and an F-measure of 55% is observed
(instead of 50.5% for [LDA]) considering the high-
level concept alignment.
No significant difference was finally measured be-
tween both ILP models for the concept generation
evaluations. Even though [ILP seg] seems to ob-
tain slightly better results in the alignment evalua-
tion. This could be expected since [ILP seg] intrin-
sically yields alignments with grouped topics, closer
to the reference alignment used for the evaluation.
It is worth noticing that unlike [LDA] system be-
haviour, the results of [ILP ] are not affected when
more topics are generated by [LDA]. A large num-
ber of topics enables [ILP ] to pick up the best topic
for a given segment among in a longer selection list.
As for [LDA], the same losses are observed be-
tween high-level and fine-level concepts and gener-
ation and alignment paradigms. Nevertheless, an F-
measure of 54.8% is observed at the high-level con-
cept in alignement evaluation (Figure 4) that corre-
sponds to a precision of 56.2% and a recall of 53.5%,
which is not so low considering a fully-automatic
high-level annotation system.
5 Conclusions and perspectives
In this paper an unsupervised approach for con-
cept extraction and segmental annotation has been
proposed and evaluated. Based on two steps
(topic inventory and assignment with LDA, then re-
segmentation with either IBM alignment models or
ILP) the technique has been shown to offer perfor-
mance above 50% for the retrieval of reference con-
cepts. It confirms the applicability of the technique
to practical tasks with an expected gain in data pro-
duction.
Future work will investigate the use of n-grams
to increase LDA accuracy to provide better hypothe-
ses for the following segmentation method. Besides,
other levels of data representation will be examined
(use of lemmas, a priori semantic classes like city
names. . . ) in order to better generalise on the data.
ACKNOWLEDGEMENTS
This work is supported by the ANR funded project
PORT-MEDIA (www.port-media.org) and the LIA
OptimNLP project (www.lia.univ-avignon.fr).
References
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of the 9th
European Conference on Speech Communication and
Technology.
A. Celikyilmaz, D. Hakkani-Tur, and G. Tur. 2010. Lda
based similarity modeling for question answering. In
Proceedings of the NAACL HLT 2010 Workshop on Se-
mantic Search, pages 1?9. Association for Computa-
tional Linguistics.
Der-San Chen, Robert G. Batson, and Yu Dang. 2010.
Applied Integer Programming: Modeling and Solu-
tion. Wiley, January.
Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-
rice Lefvre, Patrick Lehnen, Renato De Mori, Alessan-
dro Moschitti, Hermann Ney, and Giuseppe Riccardi.
2010. Comparing stochastic approaches to spoken
language understanding in multiple languages. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, PP(99):1.
E. Iosif, A. Tegos, A. Pangos, E. Fosler-Lussier, and
A. Potamianos. 2006. Unsupervised combination of
metrics for semantic class induction. In Proceedings
of the IEEE Spoken Language Technology Workshop,
pages 86?89.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL, Companion Volume, pages 177?180,
Prague, Czech Republic.
F. Lefe`vre. 2007. Dynamic bayesian networks and
discriminative classifiers for multi-stage semantic in-
terpretation. In Proceedings of ICASSP, Honolulu,
Hawai.
F. Mairesse, M. Gas?ic?, F. Jurc???c?ek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Spoken language
80
understanding from unaligned data using discrimina-
tive classification models. In Proceedings of ICASSP,
Taipei, Taiwan.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
A. Pargellis, E. Fosler-Lussier, A. Potamianos, and C.H.
Lee. 2001. Metrics for measuring domain indepen-
dence of semantic classes. In Proceedings of the 7th
European Conference on Speech Communication and
Technology.
X.H. Phan, L.M. Nguyen, and S. Horiguchi. 2008.
Learning to classify short and sparse text & web with
hidden topics from large-scale data collections. In
Proceeding of the 17th international conference on
World Wide Web, pages 91?100. ACM.
C. S. ReVelle and H. A. Eiselt. 2005. Location analysis:
A synthesis and survey. European Journal of Opera-
tional Research, 165(1):1?19, August.
K.C. Siu and H.M. Meng. 1999. Semi-automatic acqui-
sition of domain-specific semantic structures. In Pro-
ceedings of the 6th European Conference on Speech
Communication and Technology.
Y.C. Tam and T. Schultz. 2006. Unsupervised language
model adaptation using latent semantic marginals. In
Proceedings of INTERSPEECH, pages 2206?2209.
W Ward. 1991. Understanding Spontaneous Speech.
In Proceedings of ICASSP, pages 365?368, Toronto,
Canada.
81
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 97?104,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Alignment for Segmental-based Language Understanding
St?phane Huet and Fabrice Lef?vre
Universit? d?Avignon, LIA-CERI, France
{stephane.huet,fabrice.lefevre}@univ-avignon.fr
Abstract
Recent years? most efficient approaches for
language understanding are statistical. These
approaches benefit from a segmental semantic
annotation of corpora. To reduce the produc-
tion cost of such corpora, this paper proposes
a method that is able to match first identified
concepts with word sequences in an unsuper-
vised way. This method based on automatic
alignment is used by an understanding sys-
tem based on conditional random fields and
is evaluated on a spoken dialogue task using
either manual or automatic transcripts.
1 Introduction
One of the very first step to build a spoken language
understanding (SLU) module for dialogue systems
is the extraction of literal concepts from word se-
quences hypothesised by a speech recogniser. To
address this issue of concept tagging, several tech-
niques are available. These techniques rely on mod-
els, now classic, that can be either discriminant
or generative. Among these, we can cite: hidden
Markov models, finite state transducers, maximal
entropy Markov models, support vector machines,
dynamic Bayesian networks (DBNs) or conditional
Markov random fields (CRFs) (Lafferty et al, 2001).
In (Hahn et al, 2011), it is shown that CRFs obtain
the best performance on a reference task (MEDIA) in
French (Bonneau-Maynard et al, 2005), but also on
two other comparable corpora in Italian and Polish.
Besides, the comparison of the understanding results
of manually vs automatically transcribed utterances
has shown the robustness of CRFs.
Among the approaches evaluated in (Hahn et al,
2011) was a method using log-linear models compa-
rable to those used in stochastic machine translation,
which turned out to have lower performance than
CRF. In this paper, we further exploit the idea of ap-
plying automatic translation techniques to language
understanding but limiting ourselves to the objective
of obtaining a segmental annotation of training data.
In many former approaches literal interpretation
was limited to list lexical-concept relations; for in-
stance this is the case of the PHOENIX system (Ward,
1991) based on the detection of keywords. The
segmental approach allows a finer-grained analysis
considering sentences as segment sequences during
interpretation. This characteristic enables the ap-
proach to correctly connect the various levels of
sentence analysis (lexical, syntactic and semantic).
However, in order to simplify its practical appli-
cation, segments have been designed specifically
for semantic annotation and do not integrate any
constraint in their relation with the syntactic units
(chunks, phrasal groups, etc.). Not only it simpli-
fies the annotation process itself but as the overall
objective is to use the interpretation module inside
a spoken dialogue system, transcribed speech data
are noisy and generally bound the performance of
syntactic analysers (due to highly spontaneous and
ungrammatical utterances from the users, combined
with errors from the speech recognizer).
Among other interesting proprieties, segmental
approaches offer a convenient way to dissociate the
detection of a conceptual unit from the estimation of
its associated value. The value corresponds to the
normalisation of the surface form. For instance, if
97
the segment ?no later than eleven? is associated with
the concept departure-time, its value is ?morn-
ing?; the same value is associated with the segments
?between 8 and noon? or ?in the morning?. The
value estimation requires a link between concepts
and sentence words. Then it becomes possible to
treat the normalisation problem by means of regular
expressions or concept-dependent language models
(allowing an integrated approach such as described
in (Lef?vre, 2007)). In the case of global approaches
(not segmental), value detection must be directly
incorporated in the conceptual units to identify, as
in (Mairesse et al, 2009). The additional level is a
real burden and is only affordable when the number
of authorised values is low.
Obviously a major drawback of the approach is its
cost: associating concept tags with a dialogue tran-
scription is already a tedious task and its complexity
is largely increased by the requirement for a precise
delimitation of the support (lexical segment) corre-
sponding to each tag. The SLU evaluation campaign
MEDIA has been the first opportunity to collect and
distribute a reasonably-sized corpus endowed with
segmental annotations.
Anyhow the difficulty remains unchanged each
time a corpus has to be collected for a new task.
We propose in this study a new method that reduces
the effort required to build training data for segmen-
tal annotation models. Making the assumption that
the concepts evoked in a sentence are automatically
detected beforehand or provided by an expert, we
study how to associate them with their lexical sup-
ports without prior knowledge. A conceptual seg-
mental annotation is obtained using alignment tech-
niques designed to align multilingual parallel cor-
pora in the machine translation domain. This anno-
tation can be considered as unsupervised since it is
done without a training corpus with links between
word sequences and concepts.
We present in the paper the necessary adaptations
for the application of the alignment techniques in
this new context. They have been kept to their mini-
mal so as to maintain the highest level of generality,
which in return benefits from the availability of ex-
isting software tools. Using a reference annotation,
we evaluate the alignment quality from the unsuper-
vised approach in two interesting situations depend-
ing on whether the correct order of the concepts is
known or not. Finally, the end-to-end evaluation of
the approach is made by measuring the impact of the
alignments on the CRF-based understanding system.
After a brief recall of the conceptual decoding
principles in Section 2, the principles of automatic
alignment of parallel corpora are described in Sec-
tion 3 along with the specificities due to the align-
ment of semantic concepts. Section 4 presents the
experiments and comments on the results, while
Section 5 concludes the paper.
2 Segmental conceptual decoding
If literal interpretation can be seen as the transla-
tion of natural language to the set of semantic tag
sequences, then the methods and models of machine
translation can be used. Since the number of con-
cepts is generally much lower than the vocabulary
size, this particular type of translation can also be
considered as a mere classification problem in which
the conceptual constituents represent the class to
identify. Interpretation can thus be performed by
methods and models of classification.
Discriminant approaches model the conditional
probability distribution of the semantic constituent
sequence (or concepts) c1 . . . cn considering a word
sequence w1 . . . wT : P (cn1 |w
T
1 ). In generative ap-
proaches, the joint probability P (cn1 , w
T
1 ) is mod-
elized instead and can be used to compute inferences
either for prediction/decoding or parameter training.
Generative models (such as hidden Markov mod-
els) have been first introduced to address the under-
standing problem with stochastic approaches (Levin
and Pieraccini, 1995). Recent variants offer
more degrees of freedom in modeling (see for in-
stance (He and Young, 2005) or (Lef?vre, 2007)).
Since then log-linear models have clearly shown
their superiority for tasks of sequence tagging (Hahn
et al, 2011).
Several variants of log-linear models differ in
their conditional variable independence assumptions
and use different normalisation steps. CRFs (Laf-
ferty et al, 2001) represent linear chains of random
independent variables, all conditioned over the en-
tire sequence and the normalisation is global over
the sequence.
Some generative approaches such as DBNs make
inferences in multi-level models (Lef?vre, 2007)
98
Figure 1: Example of an alignment of words with their conceptual units.
and intrinsically take into account segmentation.
For models unable to handle multi-level repre-
sentations (as CRF), it is convenient to represent
segments directly at the tag level. For this purpose
the BIO formalism can be used: B is added to tags
starting a segment, I to tags inside a segment and
O to out-of-domain tags (if these are not already
handled through a specific NULL tag). In the case
displayed in Figure 1, the concept sequence be-
comes: B-cmd-task I-cmd-task I-cmd-task
B-null I-null B-loc-town I-loc-town
I-loc-town I-loc-town I-loc-town
B-time-date I-time-date B-time-date
I-time-date I-time-date.
3 Semantic concept alignment
Automatic alignment is a major issue in machine
translation. For example, word-based alignments
are used to generate phrase tables that are core com-
ponents for many current statistical machine trans-
lation systems (Koehn et al, 2007). The alignment
task aims at finding the mapping between words of
two sentences in relation of translation. It faces sev-
eral difficulties:
? some source words are not associated with a
translated word;
? others are translated by several words;
? matched words may occur at different positions
in both sentences according to the syntactic
rules of the considered languages.
Several statistical models have been proposed to
align two sentences (Brown et al, 1993). One of
their main interests is their ability to be built in an
unsupervised way from a parallel corpus aligned at
the sentence level, but not at the word level. For-
mally, from a sentence S = s1 . . . sm expressed in a
source language and its translation T = t1 . . . tn ex-
pressed in a target language, an IBM-style alignment
A = a1 . . . am connects each source word to a tar-
get word (aj ? {1, ..., n}) or to the so-called NULL
token which accounts for untranslated target words.
IBM statistical models evaluate the translation of S
into T from the computation of P (S,A|T ); the best
alignment A? can be deduced from this criterion us-
ing the Viterbi algorithm:
A? = argmaxAP (S,A|T ) . (1)
IBM models differ according to their complexity
level. IBM1 model makes the strong assumption
that alignments are independent and can be evalu-
ated only through the transfer probabilities P (si|tj).
The HMM model, which is an improvement over
IBM2, adds a new parameter P (aj |aj?1, n) that as-
sumes a first-order dependency between alignment
variables. The next models (IBM3 to IBM5) are
mainly based on two types of parameters:
? distortion, which measures how words of T are
reordered with respect to the index of the words
from S they are aligned with,
? fertility, which measures the usual number of
words that are aligned with a target word tj .
In order to improve alignments, IBM models are
usually applied in both translation directions. These
two alignments are then symmetrized by combining
them. This last step is done via heuristic methods;
a common approach is to start with the intersection
and then iteratively add links from the union (Och et
al., 1999).
If we have at our disposal a method that can find
concepts contained in an utterance, segmental anno-
tation can be obtained by aligning words S = wT1
with the found concepts T = cn1 (Fig. 1). Con-
cepts are ideally generated in the correct order with
respect to the word segments of the analysed utter-
ance. In a more pragmatic way, concepts are likely
to be produced as bag-of-concepts rather than or-
dered sequences.
99
Statistical alignment methods used in machine
translation are relevant in our context if we consider
that the target language is the concept language.
There are nevertheless differences with genuine lan-
guage translation. First, each word is aligned to at
most one concept, while a concept is aligned with
one word or more. Consequently, it is expected that
word fertilities are one for the alignment of words
toward concepts and concept fertilities are one or
more in the reverse direction. Another consequence
is that NULL words are useless in our context. These
specificities of the alignment process raise some dif-
ficulties with regard to IBM models. Indeed, ac-
cording to the way probabilities are computed, the
alignment of concepts toward words only allows one
word to be chosen per concept, which prevents this
direction from having a sufficient number of links
between words and concepts.
Another significant difference with translation is
related to the translated token order. While word
order is not random in a natural language and fol-
lows syntactic rules, it is not the case anymore when
a word sequence have to be aligned with a bag-of-
concepts. HMM and IBM2 to IBM5 models have
parameters that assume that the index of a matched
source word or the indices of the translations of the
adjacent target words bear on the index of target
words. Therefore, the randomness of the concept
indices can disrupt performance obtained with these
models, contrary to IBM1. As shown in the next
section, it is appropriate to find ways to explicitly
re-order concept sequences than to let the distortion
parameters handle the problem alone.
4 Experiments and results
4.1 Experimental setup
The evaluation of the introduced methods was car-
ried out on the MEDIA corpus (Bonneau Maynard et
al., 2008). This corpus consists of human-machine
dialogues collected with a wizard of Oz procedure
in the domain of negotiation of tourist services. Pro-
duced for a realistic task, it is annotated with 145 se-
mantic concepts and their values (more than 2k in to-
tal for the enumerable cases). The audio data are dis-
tributed with their manual transcripts and automatic
speech recognition (ASR) hypotheses. The corpus
is divided into three parts: a training set (approxi-
matively 12k utterances), a development set (1.2k)
and a test set (3k).
The experiments led on the alignment methods
were evaluated on the development corpus using
MGIZA++ (Gao and Vogel, 2008), a multi-thread
version of GIZA++ (Och and Ney, 2003) which also
allows previously trained IBM alignments models
to be applied on the development and test corpora.1
The conceptual tagging process was evaluated on the
test corpus, using WAPITI (Lavergne et al, 2010)
to train the CRF models. Several setups have been
tested:
? manual vs ASR transcriptions,
? inclusion (or not) of values during the error
computation.
Several concept orderings (before automatic align-
ment) have also been considered:
? a first ideal one, which takes reference concept
sequences as they are, aka sequential order;
? two more realistic variants that sort concepts ei-
ther alphabetically or randomly, in order to
simulate bag-of-concepts. Alphabetical order
is introduced solely to show that a particular
order (which is not related to the natural order)
might misled the alignment process by intro-
ducing undue regularities.
To give a rough idea, these experiments required
a few minutes of computing time to train alignment
models of 12k utterances, a few hours to train CRF
models (using 8 CPUs on our cluster of Xeon CPUs)
and a few seconds to apply alignment and CRF mod-
els in order to decode the test corpus.
4.2 Experimental results for alignment
Alignment quality is estimated using the alignment
error rate (AER), a metric often employed in ma-
chine translation (Och and Ney, 2000). If H stands
for hypothesis alignments andR for reference align-
ments, AER is computed by the following relation:2
AER = 1?
2? |H ?R|
|H|+ |R|
. (2)
1With previousa, previoust, previousn, etc pa-
rameters.
2This equation is a simplification of the usually provided one
because all alignments are considered as sure in our case.
100
In our context, this metrics is evaluated by repre-
senting a link between source and target identities by
(wi, cj), instead of the usual indices (i, j). Indeed,
alignments are then used to tag words. Besides, con-
cepts to align have positions that differ from the ones
in the reference when they are reordered to simulate
bags-of-concepts.
As mentioned in the introduction, we resort to
widely used tools for alignment in order to be as gen-
eral as possible in our approach. We do not modify
the algorithms and rely on their generality to deal
with specificities of the studied domain. To train
iteratively the alignment models, we use the same
pipeline as in MOSES, a widely used machine trans-
lation system (Koehn et al, 2007):
1. 5 iterations of IBM1,
2. 5 iterations of HMM,
3. 3 iterations of IBM3 then
4. 3 iterations of IBM4.
To measure the quality of the built models, the
model obtained at the last iteration of this chain is
applied on the development corpus.
All the words of an utterance should normally
be associated with one concept, which makes the
IBM models? NULL word useless. However, in the
MEDIA corpus, a null semantic concept is associ-
ated with words that do not correspond to a concept
relevant for the tourist domain and may be omit-
ted by counting on the probability with the NULL
word included in the IBM models. Two versions
were specifically created to test this hypothesis: one
with all the reference concept sequences and another
without the null tags. The results measured when
taking into account these tags (AER of 14.2 %) are
far better than the ones obtained when they are dis-
carded (AER of 27.4 %), in the word ? concept
alignment direction.3 We decided therefore to keep
the null in all the experiments.
Table 1 presents the alignment results measured
on the development corpus according to the way
concepts are reordered with respect to the reference
and according to the considered alignment direction.
3For a fair comparison between both setups, the null con-
cept was ignored in H and R for this series of experiments.
The three first lines exhibit the results obtained with
the last IBM4 iteration. As expected, the AER mea-
sured with this model in the concept? word direc-
tion (second line), which can only associate at most
one word per concept, is clearly higher than the one
obtained in the opposite direction (first line). Quite
surprisingly, an improvement in terms of AER (third
line) over the best direction (first line) is observed
using the default MOSES heuristics (called grow-
diag-final) that symmetrizes alignments obtained in
both directions.
IBM1 models, contrary to other models, do not
take into account word index inside source and tar-
get sentences, which makes them relevant to deal
with bag-of-concepts. Therefore, we measured how
AER varies when using models previously built in
the training chain. The results obtained by applying
IBM1 and by symmetrizing alignments (last line),
show finally that these simple models lead to lower
performance than the one measured with IBM4 or
even HMM (last line), the concepts being ordered
alphabetically or randomly (two last columns).
The previous experiments have shown that align-
ment is clearly of lower quality when algorithms are
faced with bags-of-concepts instead of well-ordered
sequences. In order to reduce this phenomenon, se-
quences are reordered after a first alignmentA1 gen-
erated by the symmetrized IBM4 model. Two strate-
gies have been considered to fix the new position of
each concept ci. The first one averages the indices
of the words wi that are aligned with ci according to
A1:
pos1(cj) =
?
is.t.(i,j)?A1 i
Card({(i, j) ? A1})
. (3)
The second one weights each word index with their
transfer probabilities determined by IBM4:
pos2(cj) =
?
is.t.(i,j)?A1 i? f(wi, cj)?
is.t.(i,j)?A1 f(wi, cj)
(4)
where
f(wi, cj) = ?P (cj |wi) + (1? ?)P (wi|cj) (5)
and ? is a coefficient fixed on the development cor-
pus.
Training alignment models on the corpus re-
ordered according to pos1 (Tab. 2, second column)
101
Sequential order Alphabetic order Random order
word? concept IBM4 14.4 29.2 28.6
concept? word IBM4 40.9 51.6 49.0
symmetrized IBM4 12.8 27.3 25.7
symmetrized IBM1 33.2 33.2 33.1
symmetrized HMM 14.8 29.9 28.7
Table 1: AER (%) measured on the MEDIA development corpus with respect to the alignment model used and its
direction.
Initial 1st reordering iteration Last reordering iteration
pos1 pos2 pos2
Alphabetic order 27.3 22.2 21.0 19.4
Random order 25.7 21.9 20.2 18.5
Table 2: AER (%) measured on the MEDIA development corpus according to the strategy used to reorder concepts.
or pos2 (third column) leads to a significant im-
provement of the AER. This reordering step can be
repeated as long as performance goes on improving.
By proceeding like this until step 3 for the alphabetic
order and until step 7 for the random order, values of
AER below 20 % (last column) are finally obtained.
It is noteworthy that random reordering has better
results than alphabetic reordering. Indeed, HMM,
IBM3 and IBM4 models have probabilities that are
more biased in this latter case, where the same se-
quences occur more often although many are not in
the reference.
4.3 Experimental results for spoken language
understanding
In order to measure how spoken language un-
derstanding is disturbed by erroneous alignments,
CRFs parameters are trained under two conditions:
one where concept tagging is performed by an ex-
pert and one where corpora are obtained using au-
tomatic alignment. The performance criterion used
to evaluate the understanding task is the concept er-
ror rate (CER). CER is computed in a similar way
as word error rate (WER) used in speech recogni-
tion; it is obtained from the Levenshtein alignment
between both hypothesized and reference sequences
as the ratio of the sum of the concepts in the hy-
pothesis substituted, inserted or omitted on the total
number of concepts in the manual reference anno-
tation. The null concept is not considered during
the score computation. The CER can also take into
account the normalized values in addition to the con-
cept tags.
Starting from a state-of-the-art system (Manual
column), degradations due to various alignment con-
ditions are reported in Table 3. It can be noted that
the absolute increase in CER is at most 8.0 % (from
17.6 to 25.6 with values) when models are trained on
the corpus aligned with IBM models; the ordering
information brings it back to 3.7 % (17.6 to 21.3),
and finally with automatic transcription the impact
of the automatic alignments is smaller (resp. 5.8 %
and 2.0 %). As expected random order is preferable
to alphabetic order (slight gain of 1 %).
In Table 4, the random order alignments are used
but this time the n-best lists of alignments are con-
sidered and not only the 1-best hypotheses. Instead
of training CRFs with only one version of the align-
ment for a concept-word sequence pair, we filter
out from the n-best lists the alignments having a
probability above a given threshold. It can be ob-
served that varying this confidence threshold allows
an improvement of the SLU performance (CER can
be reduced by 0.8 % for manual transcription and
0.4 % for automatic transcription). However, this
improvement is not propagated to scores with val-
ues (CER was reduced at best by 0.1 for manual
transcription and was increased for automatic tran-
102
Automatic alignments
Manual Sequential Alphabetic order Random order
Manual transcription 13.9 (17.6) 17.7 (21.3) 22.6 (26.4) 22.0 (25.6)
ASR transcription (wer 31 %) 24.7 (29.8) 27.1 (31.8) 31.5 (36.4) 30.6 (35.6)
Table 3: CER (%) measured for concept decoding on the MEDIA test corpus with several alignment methods of the
training data. Inside parenthesis, CER for concepts and values.
scription). After closer inspection of the scoring
alignments, an explanation for this setback is that
the manually-designed rules used for value extrac-
tion are perturbed by loose segmentation. This is
particularly the case for the concept used to anno-
tate co-references, which has confusions between
the values singular and plural (e.g. ?this? is sin-
gular and ?those? plural). This issue can be solved
by an ad hoc adaptation of the rules. However, it
would infringe our objective of relying upon unsu-
pervised approaches and minimizing human exper-
tise. Therefore, a better answer would be to resort to
a probabilistic scheme also for value extraction (as
proposed in (Lef?vre, 2007)).
The optimal configuration (confidence threshold
of 0.3, 4th row of Table 4) is close to the baseline
1-best system in terms of the number of training
utterances. We also tried a slightly different setup
which adds the filtered alignments to the former cor-
pus before CRF parameter training (i.e. the 1-best
is not filtered in the n-best list). In that case perfor-
mance remains pretty stable with respect to the filter-
ing process (CER is around 21.4 % for concepts and
25.2 % for concept+value for thresholds between 0.1
and 0.7).
5 Conclusion
In this study an unsupervised approach is proposed
to the problem of conceptual unit alignment for spo-
ken language understanding. We show that unsuper-
vised statistical word alignment from the machine
translation domain can be used in this context to as-
sociate semantic concepts with word sequences. The
quality of the derived alignment, already good in the
general case (< 20 % of errors on the word-concept
associations), is improved by knowledge of the cor-
rect unit order (< 15 %). The impact of automatic
alignments on the understanding performance is an
absolute increase of +8 % in terms of CER, but is re-
duced to less than +4 % in the ordered case. When
automatic transcripts are used, these gaps decrease
to +6 % and below +3 % respectively. From these
results we do believe that the cost vs performance
ratio is in favour of the proposed method.
Acknowledgements
This work is partially supported by the ANR funded
project PORT-MEDIA.4
References
H?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Seman-
tic annotation of the MEDIA corpus for spoken dialog.
In Proceedings of Eurospeech, pages 3457?3460, Lis-
boa, Portugal.
H?l?ne Bonneau Maynard, Alexandre Denis, Fr?d?ric
B?chet, Laurence Devillers, F. Lef?vre, Matthieu
Quignard, Sophie Rosset, and Jeanne Villaneau. 2008.
MEDIA : ?valuation de la compr?hension dans les
syst?mes de dialogue. In L??valuation des technolo-
gies de traitement de la langue, les campagnes Tech-
nolangue, pages 209?232. Herm?s, Lavoisier.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engineer-
ing, Testing, and Quality Assurance for Natural Lan-
guage Processing, pages 49?57, Columbus, OH, USA.
Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-
rice Lef?vre, Patrick Lehen, Renato De Mori, Alessan-
dro Moschitti, Hermann Ney, and Giuseppe Riccardi.
2011. Comparing stochastic approaches to spoken
language understanding in multiple languages. IEEE
Transactions on Audio, Speech and Language Pro-
cessing (TASLP), 19(6):1569?1583.
4www.port-media.org
103
# train utterances Manual transcription ASR transcription
(WER = 31 %)
1-best 12795 22.0 (25.6) 30.6 (35.6)
filtered 10-best (conf thres = 0.1) 18955 21.7 (25.8) 31.2 (36.9)
filtered 10-best (conf thres = 0.2) 15322 21.3 (25.5) 30.7 (36.3)
filtered 10-best (conf thres = 0.3) 13374 21.2 (25.7) 30.2 (36.0)
filtered 10-best (conf thres = 0.5) 10963 21.4 (25.7) 30.6 (36.2)
filtered 10-best (conf thres = 0.7) 9647 25.4 (29.1) 32.9 (38.2)
Table 4: CER (%) measured for concept decoding on the MEDIA test corpus with filtered n-best lists of random order
alignments of the training data. Inside parenthesis, CER for concepts and values.
Yulan He and Steve Young. 2005. Spoken language
understanding using the hidden vector state model.
Speech Communication, 48(3?4):262?275.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL, Companion Volume, pages 177?180,
Prague, Czech Republic.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of ICML, pages 282?289, Williamstown,
MA, USA.
Thomas Lavergne, Olivier Capp?, and Fran?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL, pages 504?513, Uppsala, Sweden.
Fabrice Lef?vre. 2007. Dynamic bayesian networks and
discriminative classifiers for multi-stage semantic in-
terpretation. In Proceedings of ICASSP, Honolulu,
Hawai.
Esther Levin and Roberto Pieraccini. 1995. Concept-
based spontaneous speech understanding system. In
Proceedings of Eurospeech, pages 555?558, Madrid,
Spain.
Fran?ois Mairesse, Milica Ga?ic?, Filip Jurc??c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2009. Spoken language understanding from unaligned
data using discriminative classification models. In
Proceedings of ICASSP, Taipei, Taiwan.
Franz Joseph Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine trans-
lation. In Proceedings of Coling, volume 2, pages
1086?1090, Saarbr?cken, Germany.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 20?
28, College Park, MD, USA.
Wayne Ward. 1991. Understanding spontaneous speech:
the Phoenix system. In Proceedings of ICASSP, pages
365?368, Toronto, Canada.
104
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 154?157,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Factored Machine Translation Systems for Russian-English
Ste?phane Huet, Elena Manishina and Fabrice Lefe`vre
Universite? d?Avignon, LIA/CERI, France
FirstName.LastName@univ-avignon.fr
Abstract
We describe the LIA machine transla-
tion systems for the Russian-English and
English-Russian translation tasks. Various
factored translation systems were built us-
ing MOSES to take into account the mor-
phological complexity of Russian and we
experimented with the romanization of un-
translated Russian words.
1 Introduction
This paper presents the factored phrase-based
Machine Translation (MT) systems (Koehn and
Hoang, 2007) developed at LIA, for the Russian-
English and English-Russian translation tasks at
WMT?13. These systems use only data provided
for the evaluation campaign along with the LDC
English Gigaword corpus.
We summarize in Section 2 the resources used
and the main characteristics of the systems based
on the MOSES toolkit (Koehn et al, 2007). Sec-
tion 3 reports experiments on the use of fac-
tored translation models. Section 4 describes the
transliteration process used to improve the Russian
to English task. Finally, we conclude in Section 5.
2 System Architecture
2.1 Pre-processing
The corpora available for the workshop were pre-
processed using an in-house script that normal-
izes quotes, dashes, spaces and ligatures. Long
sentences or sentences with many numeric or
non-alphanumeric characters were also discarded.
Since the Yandex corpus is provided as lower-
cased, we decided to lowercase all the other cor-
pora. The same pipeline was applied to the LDC
Gigaword; also only the documents classified as
?story? were retained. Table 1 summarizes the
used data and introduces designations that we fol-
low in the remainder of this paper to refer to these
corpora.
Russian is a morphologically rich language with
nouns, adjectives and verbs inflected for case,
number and gender. This property requires in-
troducing morphological information inside the
MT system to handle the lack of many inflec-
tional forms inside training corpora. For this
purpose, each corpus was previously tagged with
Part-of-Speech (PoS) tags. The tagger TREE-
TAGGER (Schmid, 1995) was selected for its
good performance on several comparable tasks.
The Russian tagger associates each word (e.g.
????? (boxes)) with a complex PoS including
morphological information (e.g. ?Ncmpnn? for
?Noun Type=common Gender=masculine Num-
ber=plural Case=nominative Animate=no?) and
its lemma (e.g. ???? (box)). A description of
the Russian tagset can be found in (Sharoff et al,
2008). The English tagger provides also a lemma-
tization and outputs PoS from the Penn Treebank
tagset (Marcus et al, 1993) (e.g. ?NNS? for
?Noun plural?).
In order to simplify the comparison of differ-
ent setups, we used the tokenizer included in the
TREETAGGER tool to process all the corpora.
2.2 Language Models
Kneser-Ney discounted LMs were built
from monolingual corpora using the SRILM
toolkit (Stolcke, 2002). 5-gram LMs were trained
for words, 7-gram LMs for lemmas and PoS. A
LM was built separately on each monolingual cor-
pus: mono-news-c and news-s. Since ldc was too
large to be processed as one file, it was split into
three parts according to the original publication
year of the document. These LMs were combined
through linear interpolation. Weights were fixed
by optimizing the perplexity on a corpus made of
the WMT test sets from 2008 to 2011 for English
and on the WMT 2012 test set for Russian (the
154
CORPORA DESIGNATION SIZE (SENTENCES)
English-Russian Bilingual training
News Commentary v8 news-c 146 k
Common Crawl crawl 755 k
Yandex yandex 978 k
English Monolingual training
News Commentary v8 mono-news-c 247 k
Shuffled News Crawl corpus (from 2007 to 2012) news-s 68 M
LDC Gigaword ldc 190 M
Russian Monolingual training
News Commentary v8 mono-news-c 182 k
Shuffled News Crawl corpus (from 2008 to 2012) news-s 20 M
Development
newstest2012 test12 3,003
Table 1: Used bilingual and monolingual corpora
only available at that time).
2.3 Alignment and Translation Models
All parallel corpora were aligned using
MGIZA++ (Gao and Vogel, 2008). Our transla-
tion models are phrase-based models (PBMs) built
with MOSES using default settings. Weights of
LM, phrase table and lexicalized reordering model
scores were optimized on test12, thanks to the
MERT algorithm (Gao and Vogel, 2008). Since
only one development corpus was made available
for Russian, we used a 3-fold cross-validation
so that MERT is repeated three times for each
translation model on a 2,000-sentence subsample
of test12.
To recase the corpora, translation models were
trained using a word-to-word translation model
trained on the parallel corpora aligning lowercased
and cased sentences of the monolingual corpora
mono-news-c and news-s.
3 Experiments with Factored
Translation Models
The evaluation was performed using case-
insensitive BLEU and was computed with the
mteval-v13a.pl script provided by NIST.
The BLEU scores shown in the tables below are
all averaged on the test parts obtained from the 3-
fold cross validation process.
In the remainder of the paper, we employ the
notation proposed by Bojar et al (2012) to refer
to factored translation models. For example, tW-
W:tL-L+tP-P+gLaP-W, where ?t? and ?g? stand
for ?translation? and ?generation?, denotes a trans-
lation system with two decoding paths:
? a first one directly translates words to words
(tW-W),
? a second one is divided into three steps:
1. translation from lemmas to lemmas (tL-
L),
2. translation from PoS to PoS (tP-P) and
3. generation of target words from target
lemmas and PoS (gLaP-W).
3.1 Baseline Phrase-Based Systems
Table 2 is populated with the results of PBMs
which use words as their sole factor. When LMs
are built on mono-news-c and news-s, an improve-
ment of BLEU is observed each time a training
parallel corpus is used, both for both translation di-
rections (columns 1 and 3). We can also notice an
absolute increase of 0.4 BLEU score when the En-
glish LM is additionally trained on ldc (column 2).
3.2 Decomposition of factors
Koehn and Hoang (2007) suggested from their ex-
periments for English-Czech systems that ?it is
beneficial to carefully consider which morpholog-
ical information to be used.? We therefore tested
various decompositions of the complex Russian
PoS tagset (P) output by TREETAGGER. We con-
sidered the grammatical category alone (C), mor-
phological information restrained to case, number
155
EN? RU RU? EN
+LDC
news-c 26.52 26.82 19.89
+crawl 29.49 29.82 21.06
+yandex 31.08 31.49 22.16
Table 2: BLEU scores measured with standard
PBMs.
Tagset #tags Examples
C 17 Af, Vm, P, C
M1 95 fsg, -s-, fsa, ?
M2 380 fsg, -s-, fsa, ??? (that)
M3 580 fsg, -s-1ife, fsa3, ??? (that)
P 604 Afpfsg, Vmif1s-a-e, P-3fsa, C
Table 3: Statistics on Russian tagsets.
and gender (M1), the fields included in M1 along
with additional information (lemmas) for conjunc-
tions, particles and adpositions (M2), and finally
the information included in M2 enriched with per-
son for pronouns and person, tense and aspect for
verbs (M3). Table 3 provides the number of tags
and shows examples for each used tagset.
To speed up the training of translation models,
we experimented with various setups for factor de-
composition from news-c. The results displayed
on Table 4 show that factors with morphologi-
cal information lead to better results than a PBM
trained on word forms (line 1) but that finally the
best system is achieved when the complex PoS tag
output by TREETAGGER is used without any de-
composition (last line).
tW-W 19.89
tW-WaC 19.81
tW-WaM1 20.04
tW-WaCaM1 19.95
tW-WaM2 19.92
tW-WaCaM2 19.91
tW-WaM3 19.98
tW-WaCaM3 19.89
tW-WaP 20.30
Table 4: BLEU scores for EN?RU using news-c
as training parallel corpus.
tL-W 29.23
tW-W 31.49
tWaP-WaP 31.62
tW-W:tL-W 31.69
tW-WaP 31.80
tW-WaP:tL-WaP 31.89
Table 5: BLEU scores for RU?EN using the three
available parallel corpora.
3.3 Experimental Results for Factored
Models
The many inflections for Russian induce a hight
out-of-vocabulary rate for the PBMs, which gener-
ates many untranslated Russian words for Russian
to English. We experimented with the training of
a PMB on lemmatized Russian corpora (Table 5,
line 1) but observed a decrease in BLEU score
w.r.t. a PBM trained on words (line 2). With two
decoding paths ? one from words, one from lem-
mas (line 4) ? using the MOSES ability to manage
multiple decoding paths for factored translation
models, an absolute improvement of 0.2 BLEU
score was observed.
Another interest of factored models is disam-
biguating translated words according to their PoS.
Translating a (word, PoS) pair results in an ab-
solute increase of 0.3 BLEU (line 5), and of 0.4
BLEU when considering two decoding paths (last
line). Disambiguating source words with PoS did
not seem to help the translation process (line 3).
The Russian inflections are far more problem-
atic in the other translation direction since mor-
phological information, including case, gender
and number, has to be induced from the English
words and PoS, which are restrained for that lan-
guage to the grammatical category and knowledge
about number (singular/plural for nouns, 3rd per-
son singular or not for verbs). Disambiguating
translated Russian words with their PoS resulted
in a dramatic increase of BLEU by 1.6 points (Ta-
ble 6, last line vs line 3). The model that trans-
lates independently PoS and lemmas, before gen-
erating words, albeit appealing for its potential to
deal with data sparsity, turned out to be very dis-
appointing (first line). We additionally led ex-
periments training generation models gLaP-W on
monolingual corpora instead of the less volumi-
nous parallel corpora, but we did not observed a
gain in terms of BLEU.
156
tL-L+tP-P+gLaP-W 17.06
tW-W 22.16
tWaP-WaP 23.34
tWaP-LaP+gLaP-W 23.48
tW-LaP+gLaP-W 23.58
tW-WaP 23.72
Table 6: BLEU scores for EN?RU using the three
available parallel corpora.
BEFORE AFTER
tW-WaP 31.80 32.15
tW-WaP:tL-WaP 31.89 32.21
Table 7: BLEU scores for RU ? EN before and
after transliteration.
4 Transliteration
Words written in Cyrillic inside the English trans-
lation output were transliterated into Latin letters.
We decided to restrain the use of transliteration for
the English to Russian direction since we found
that many words, especially proper names, are in-
tentionally used in Latin letters in the Russian ref-
erence.
Transliteration was performed in two steps.
Firstly, untranslated words in Cyrillic are looked
up in the guessed-names.ru-en file provided for the
workshop and built from Wikipedia. Secondly, the
remaining words are romanized with rules of the
BGN/PCGN romanization method for Russian (on
Geographic Names, 1994). Transliterating words
in Cyrillic resulted in an absolute improvement of
0.3 BLEU for our two best factor-based system
(Table 7, last column).
The factored model with the tW-WaP:tL-
WaP translation path and a transliteration post-
processing step is the final submission for the
Russian-English workshop translation task, while
the tW-WaP is the final submission for the other
translation direction.
5 Conclusion
This paper presented experiments carried out with
factored phrase-based translation models for the
two-way Russian-English translation tasks. A mi-
nor gain was observed after romanizing Russian
words (+0.3 BLEU points for RU ? EN) and
higher improvements using word forms, PoS inte-
grating morphological information and lemma as
factors (+0.4 BLEU points for RU? EN and +1.6
for EN ? RU w.r.t. to a phrase-based restrained
to word forms). However, these improvements
were observed with setups which disambiguate
words according to their grammatical category or
morphology, while results integrating a generation
step and dealing with data sparsity were disap-
pointing. It seems that further work should be
done to fully exploit the potential of this option
inside MOSES.
References
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran. 2012.
Probes in a taxonomy of factored phrase-based mod-
els. In 7th NAACL Workshop on Statistical Machine
Translation (WMT), pages 253?260.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of
the ACL Workshop: Software Engineering, Testing,
and Quality Assurance for Natural Language Pro-
cessing, pages 49?57.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 868?-876.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics (ACL), Companion Vol-
ume, pages 177?180.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 2:313?330.
U.S. Board on Geographic Names. 1994. Romaniza-
tion systems and roman-script spelling conventions.
Technical report, Defense Mapping Agency.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
ACL SIGDAT Workshop, pages 47?50.
Serge Sharoff, Mikhail Kopotev, Tomaz? Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing
and evaluating a russian tagset. In 6th International
Conference on Language Resources and Evaluation
(LREC), pages 279?285.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing (ICSLP).
157
