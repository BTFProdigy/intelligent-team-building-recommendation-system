Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 130?133,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating language errors in texts: investigating argumentation and
decision schemas
Camille ALBERT, Laurie BUSCAIL
Marie GARNIER, Arnaud RYKNER
LLA, Universite? Toulouse le Mirail
31000 TOULOUSE France
mhl.garnier@gmail.com
Patrick SAINT-DIZIER
IRIT-CNRS, 118, route de Narbonne,
31062 TOULOUSE France
stdizier@irit.fr
Abstract
In this short paper, we present annotations
for tagging grammatical and stylistic er-
rors, together with attributes about the na-
ture of the correction which are then in-
terpreted as arguments. A decision model
is introduced in order for the author to be
able to decide on the best correction to
make. This introduces an operational se-
mantics for tags and related attributes.
1 Aims and Situation
Non-native English speaking authors producing
documents in English often encounter lexical,
grammatical and stylistic difficulties that make
their texts difficult for native speakers to under-
stand. As a result, the professionalism and the
credibility of these texts is often affected. Our
main aim is to develop procedures for the correc-
tion of those errors which cannot (and will not in
the near future) be treated by the most advanced
text processing systems such as those proposed in
the Office Suite, OpenOffice and the like. In the
type of errors taken into consideration, several lev-
els are often intertwinned: morphology, lexicon,
grammar, style, textual structure, domain usages,
context of production, target audience, etc..
While we attempt to correct errors, it turns out
that, in a large number of cases, (1) there may
be ambiguities in the analysis of the nature of er-
rors, (2) errors can receive various types and lev-
els of corrections depending on the type of docu-
ment, reader, etc., and (3) some corrections can-
not be successfully done without an interaction
with the author. To achieve these aims we need
to produce a model of the cognitive strategies de-
ployed by human experts (e.g. translators cor-
recting texts, teachers) when they detect and cor-
rect errors. Our observations show that it is not
a simple and straightforward strategy, but that er-
ror diagnosis and corrections are often based on a
complex analytical and decisional process. Since
we want our system to have a didactic capacity,
in order to help writers understand their errors,
we propose an analysis of error diagnosis based
on argumentation theory, outlining arguments for
or against a certain correction and their relative
strength paired with a decision theory.
The modelling of correction strategies is based
on the annotation of a large variety of types of doc-
uments in English produced by a large diversity of
French speakers. Annotations allow us to iden-
tify and categorize errors as well as the parame-
ters at stake (e.g. category change, length of new
corrected segment) at stake when making correc-
tions. This is carried out by bilingual correctors
in collaboration with didacticians. Those parame-
ters are a priori neutral in the annotation schemas.
We then define a preference model that assigns po-
larity (positive, negative) and a weight to each of
these parameters, together with additional param-
eters among which the target reader, the type of
document, etc. An argumentation model that con-
siders these parameters as weighted arguments, for
or against a certain correction, can thus be intro-
duced. Paired with a decision model, optimal cor-
rections can be proposed to the author, together
with explanations. This approach confers a formal
interpretation to our annotation schema.
Works on the correction of grammatical errors
made by human authors (Brockett, 2006), (Han et
al. 2005), (Lee et al 2006), (Tetreau et al2008),
(Writer?s v. 8.2) recently started to appear. The ap-
proach presented here, which is still preliminary,
is an attempt to include some didactic aspects into
the correction by explaining to the user the nature
of her/his errors, whether grammatical or stylis-
tic, while weighing the pros and cons of a cor-
rection, via argumentation and decision theories
(Boutiler et al. 1999), (Amgoud et al. 2008).
Persuasion aspects also matter within the didacti-
cal perspective (e.g. Persuation Technology sym-
130
posiums), (Prakken 2006).
In this document, we present the premisses of
an approach to correcting complex grammar and
style errors, which allow us to evaluate difficulties,
challenges, deadlocks, etc. Annotations are used
here for the development of an application.
2 The annotated corpus
The documents analyzed range from spontaneous
short productions, with little control and proof-
reading, such as personal emails or posts on fo-
rums, to highly controlled documents such as pub-
lications or professional reports. We also consider
personal web pages and wiki texts. Within each
of these types, we also observed variation in the
control of the quality of the writing. For exam-
ple, emails sent to friends are less controlled than
those produced in a professional environment, and
even in this latter framework, messages sent to the
hierarchy or to foreign colleagues receive more at-
tention than those sent to close colleagues. Be-
sides the level of control, other parameters, such
as style, are taken into consideration (e.g. oral
vs. academic). Therefore, the different corpora we
have collected form a certain continuum over sev-
eral parameters (control, orality, etc.); they allow
us to observe a large variety of language produc-
tions.
More details on the elaboration of corpora, def-
inition of attributes and their stability, and annota-
tion scenarios can be found in (Albert et al, 2009).
3 The Annotation System
Let us now briefly introduce the annotation
schema we have developed. It is an ongoing ef-
fort which is gradually evaluated by real users.
This schema is an attempt to reflect, in a fac-
tual and declarative way, the different parameters
taken into consideration by didacticians and hu-
man translators when detecting and correcting er-
rors. It contains several groups of tags which are
given below. The values for each attribute are
based on a granularity level evaluated by the di-
dacticians of our group. They are still preliminary
and require evaluation and revisions. Their struc-
ture has been designed so that they can be used in
an argumentation framework.
(a) Error delimitation and characterization:
<error-zone> tags the group of words involved in
the error. The zone is meant to be as minimal as
possible. This tag has several attributes:
comprehension: from 0 to 4 (0 being worse): indi-
cates if the segment is understandable, in spite of
the error,
agrammaticality: from 0 to 2: indicates how un-
grammtical the error is.
categ: main category of the error: lexical, syntac-
tic, stylistic, semantic, textual,
source: calque (direct copy), overcorrection, etc.
(b) Delimitation of the correction:
<correction-zone> tags the text fragment in-
volved in the correction. It is equal or larger than
the error zone.
(c) Characterization of a given correction:
Each correction is characterized by a tag
<correction> and associated attributes, positively
oriented ones are underlined:
surface: size of the text segment affected by the
correction: minimal, average, maximal,
grammar: indicates, whenever appropriate, if the
correction proposed is the standard one as sug-
gested by grammar rules; values are: by-default,
alternative, unlikely,
meaning: indicates if the meaning has been al-
tered: yes, somewhat, no,
var-size: is an integer that indicates the
increase/decrease in number of words of the cor-
rection w.r.t. the original fragment,
change: indicates if the changes in the correction
are syntactic, lexical, stylistic, semantic or textual,
comp: indicates if the proposed correction is a text
fragment which is easy to understand or not; val-
ues are: yes, average, no,
fix: indicates, when mentioned, that the error is
very specific to that string of words and that the
correction is idiosyncratic and cannot be extended
to any other such structure.
qualif: indicates the certainty level of the annota-
tor and didacticians, it qualifies the certainty of the
error detection and of the proposed correction se-
paretely,
correct: gives the correction.
An example is the N N construction (for the
sake of readability, we do not consider longer N
chains), with erroneous segments like: the mean-
ing utterance or goal failure:
It is difficult to characterize <correction-zone>
<error-zone comprehension=?2?
agrammaticality=?1?
categ=?syntax? source=?calque?>
the meaning utterance
<correction qualif=?high? grammar=?by-default?
131
surface= ?minimal? meaning= ?not altered? Var-size=?+2?
change=?synt? comp=?yes?
correct= ?the meaning of the utterance?>
</correction>
<correction qualif=?high? grammar=?unlikely?
surface= ?minimal? meaning= ?somewhat? Var-size=?0?
change=?lexical+synt? comp=?average?
correct= ?the meaningful utterance?>
</correction>
</error-zone> </correction-zone> without a context.
These tags are relatively simple and intuitive.
After some basic training, 2 independent annota-
tors covered about 25 pages (emails and reports)
so that we can measure the stability of the annota-
tions and the annotators comprehension and agree-
ment/disagreement. Results are not easy to ana-
lyze in a simple way since annotators disagree on
some error existence and nature. In about 20% of
the cases we observed such forms of disagreement.
Beside this observation, annotations turn out to be
quite convenient, although, for each error, a con-
siderable analysis effort is required for its analysis.
Annotating texts is very much time consuming, in
particular when there are several possibilities of
corrections.
4 From annotations to correction rules
Our corpus (texts, emails) has been annotated fol-
lowing the above schema. Several steps are re-
quired in order to reach the correction rule stage of
drafting rules of corrections. The approach is still
exploratory, and needs further elaborations and
evaluations. This is achieved through a gradual
and manually controlled machine learning strat-
egy. As a result, we get 23 main categories of
errors based on the elements involved in the gram-
matical and stylistic aspects, e.g.: incorrect argu-
ment structure, incorrect adverb position, incor-
rect embedded clause construction, incorrect co-
ordination, incorrect paragraph start.
To define a correction rule, the segment of
words in the error zone first gets a morphosyn-
tactic tagging, so that it can be easily identified
as an erroneous pattern in any circumstance. All
the errors that have the same erroneous pattern are
grouped to form a single correction procedure. In
that same category (named ?incorrect N N con-
structions?), another pattern is [N(+plural) N] (e.g.
horses carriage), and it results in a different cor-
rection rule.
Concerning the pattern ?Det N N?, when all the
corresponding errors are grouped, another type of
correction is found that corresponds to the inver-
sion (the predicate meaning ? the meaning of the
predicate). Informally, a correction rule is defined
as the union of all the corrections found for that
particular pattern:
(1) merge all corrections which are similar, i.e.
where the position of each word in the erroneous
segment is identical to the one it has in the cor-
rection; the values of the different attributes of the
<correction> tag are averaged,
(2) append all corrections which have a different
correction following the word to word criterion
above, and also all corrections for which the at-
tribute ?fix? is true.
(3) tag the corrections with all the appropriate
morphosyntactic details,
(4) remove the text segments or keep them as ex-
amples.
For the above example, we get the following
rule:
<correction-rule>
<error-zone comprehension=?2? agrammaticality=?1?
categ=?syntax? source=?calque?
pattern=?[Det N(1) N(2)?]>
<correction qualif=?high? grammar=?by-default?
surface= ?minimal? meaning= ?not altered? Var-size=?+2?
change=?synt? comp=?yes?
web-correct= ?[Det N(1) of the N(2)]? >
</correction>
<correction qualif=?high? grammar=?unlikely?
surface= ?minimal?
meaning= ?somewhat? Var-size=?0?
change=?lexical+synt? comp=?average?
correct=?[Det Adj(deriv(N(1)) N(2)]?
exemple=?the meaningful utterance?>
</correction>
<correction qualif=?high? grammar=?by-default?
surface= ?minimal?
meaning= ?not altered? Var-size=?+2?
change=?synt? comp=?yes?
web-correct= ?[Det N(2) of the N(1)]? >
</correction> </error-zone> </correction-rule>
We observe here several competing solutions:
when we have a segment like the meaning pred-
icate we have no information as to the noun or-
der and the type of preposition to insert (however,
?of? is the most frequent one). In this example,
the best solution is to use the web as a corpus.
The attribute web-correct is a shortcut for a func-
tion that triggers a web search: the instanciated
132
pattern is submitted to a search engine to evaluate
its occurence frequency. The most frequent one is
adopted. Other rules contain e.g. interactions with
the user to get a missing argument or to correct a
pronoun.
The form: pattern ? correct (or) web-correct
is a rewriting rule that operates the correction un-
der constraints given in the ?correct? attribute and
under didactic constraints given in the associated
attributes. Several corrections from the same rule
or from different rules may be competing. This
is a very frequent situation, e.g.: the position of
the adverb which may equally be either before the
main verb, or at the beginning, or at the end of the
sentence. A correction rule is active for a given
correction iff all the constraints it contains in the
?correct? attribute are met.
5 Using argumentation to structure the
correction space
Our goal, within an ?active didactics? perspective,
consists in identifying the best corrections and
proposing them to the writer together with expla-
nations, so that he can make the most relevant de-
cisions. Classical decision theory must be paired
with argumentation to produce explanations. In
our framework, argumentation is based on the at-
tributes associated with the tags of the correction
rules. This view confers a kind of operational se-
mantics to the tags and attributes we have defined.
Formally, a decision based on practical argu-
ments is represented by a vector (D, K, G, R) de-
fined as follows:
(1) D is a vector composed of decision variables
associated with explanations: the list of the differ-
ent decisions which can be taken into considera-
tion, including no correction. The final decision is
then made by the writer,
(2) K is a structure of stratified knowledge, pos-
sibly inconsistent. Stratifications encode priori-
ties (e.g. Bratman, 1987, Amgoud et al 2008).
K includes, for example, knowledge about read-
ers (e.g. in emails they like short messages, close
to oral communication), grammatical and stylistic
conventions or by-default behaviors, global con-
straints on texts or sentences. Each strata is asso-
ciated with a weight wK ? [0, 1]
(3) G is a set of goals, possibly inconsistent, that
correspond to positive attributes Ai to promote in
a correction. These goals depend on the type of
document being written. For example, for emails,
we may have the following goals: (meaning: no,
comp: yes, grammar: by-default). These goals
may have different weights. The form of a goal
is:
(attribute? name, value,weight)
where weight is: wAi ? [0, 1].
(4) R is a set of rejections: i.e. criteria that are not
desired, e.g., for emails: (surface: not(minimal),
change: style, semantic, textual). Format is the
same as for G. R and G have an empty intersec-
tion. These rejections may also have weights.
Some attributes may remain neutral (e.g. var-size)
for a given type of document or profile.
The global scenario for correcting an error
is as follows: while checking a text, when an
error pattern (or more if patterns are ambigu-
ous) is activated, the corrections proposed in the
<correction> tag are activated and a number of
them become active because the corresponding
?correct? attribute is active. Then, the attributes in
each of the correction, which form arguments, are
integrated in the decision process. Their weight in
G or R is integrated in a decision formula; these
weights may be reinforced or weakened via the
knowledge and preferences given in K. For each
correction decision, a meta-argument that contains
all the weighted pros and cons is produced. This
meta-argument is the motivation and explanation
for realizing the correction as suggested. It has no
polarity.
References
Amgoud, L., Dimopoulos, Y., Moraitis, P., Making de-
cisions through preference-based argumentation. In
Proceedings of the International Conference on Prin-
ciples of Knowledge Representation and Reasoning
(KR08), AAAI Press, 2008.
Bratman, M., Intentions, plans, and practical reason.
Harvard University Press, Massachusetts, 1987.
Brockett et al (2006) Correcting ESL Errors Using
Phrasal SMT Techniques, Proc of COLING/ACL
Han et al, Detecting Errors in English Article Usage
by Non-native Speakers, NLE, 2005
Lee, H., Seneff, A., Automatic Grammar Correction
for Second-language Learners, Proc of InterSpeech,
2006
Prakken, H., Formal systems for persuasion dialogue,
Knowledge Engineering Review, 21:163188, 2006.
Tetreault, M., Chodorow, C. Native Judgments of Non-
native Usage, Proc of COLING Workshop on Hu-
man Judgements in Comp. Ling, 2008.
133
Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing, pages 35?38,
Avignon, France, April 23, 2012. c?2012 Association for Computational Linguistics
LELIE: A Tool Dedicated to Procedure and Requirement Authoring
Flore Barcellini, Corinne Grosse
CNAM, 41 Rue Gay Lussac,
Paris, France,
Flore.Barcellini@cnam.fr
Camille Albert, Patrick Saint-Dizier
IRIT-CNRS, 118 route de Narbonne,
31062 Toulouse cedex France
stdizier@irit.fr
Abstract
This short paper relates the main features of
LELIE, phase 1, which detects errors made
by technical writers when producing pro-
cedures or requirements. This results from
ergonomic observations of technical writers
in various companies.
1 Objectives
The main goal of the LELIE project is to produce
an analysis and a piece of software based on lan-
guage processing and artificial intelligence that
detects and analyses potential risks of different
kinds (first health and ecological, but also social
and economical) in technical documents. We con-
centrate on procedural documents and on require-
ments (Hull et al 2011) which are, by large, the
main types of technical documents used in compa-
nies.
Given a set of procedures (e.g., production
launch, maintenance) over a certain domain pro-
duced by a company, and possibly given some
domain knowledge (ontology, terminology, lexi-
cal), the goal is to process these procedures and to
annotate them wherever potential risks are identi-
fied. Procedure authors are then invited to revise
these documents. Similarly, requirements, in par-
ticular those related to safety, often exhibit com-
plex structures (e.g., public regulations, to cite the
worse case): several embedded conditions, nega-
tion, pronouns, etc., which make their use difficult,
especially in emergency situations. Indeed, proce-
dures as well as safety requirements are dedicated
to action: little space should be left to personal
interpretations.
Risk analysis and prevention in LELIE is based
on three levels of analysis, each of them potentially
leading to errors made by operators in action:
1. Detection of inappropriate ways of writing:
complex expressions, implicit elements, com-
plex references, scoping difficulties (connec-
tors, conditionals), inappropriate granularity
level, involving lexical, semantic and prag-
matic levels, inappropriate domain style,
2. Detection of domain incoherencies in proce-
dures: detection of unusual ways of realizing
an action (e.g., unusual instrument, equip-
ment, product, unusual value such as temper-
ature, length of treatment, etc.) with respect
to similar actions in other procedures or to
data extracted from technical documents,
3. Confrontation of domain safety requirements
with procedures to check if the required safety
constraints are met.
Most industrial areas have now defined author-
ing recommendations on the way to elaborate,
structure and write procedures of various kinds.
However, our experience with technical writers
shows that those recommendations are not very
strictly followed in most situations. Our objective
is to develop a tool that checks ill-formed struc-
tures with respect to these recommendations and
general style considerations in procedures and re-
quirements when they are written.
In addition, authoring guidelines do not specify
all the aspects of document authoring: our investi-
gations on author practices have indeed identified
a number of recurrent errors which are linguistic
or conceptual which are usually not specified in
authoring guidelines. These errors are basically
identified from the comprehension difficulties en-
countered by technicians in operation using these
documents to realize a task or from technical writ-
ers themselves which are aware of the errors they
should avoid.
35
2 The Situation and our contribution
Risk management and prevention is now a major
issue. It is developed at several levels, in particu-
lar via probabilistic analysis of risks in complex
situations (e.g., oil storage in natural caves). De-
tecting potential risks by analyzing business errors
on written documents is a relatively new approach.
It requires the taking into account of most of the
levels of language: lexical, grammatical and style
and discourse.
Authoring tools for simplified language are not
a new concept; one of the first checkers was de-
veloped at Boeing1, initially for their own simpli-
fyed English and later adapted for the ASD Sim-
plified Technical English Specification2. A more
recent language checking system is Acrolinx IQ by
Acrolinx3. Some technical writing environments
also include language checking functionality, e.g.,
MadPak4. Ament (2002) and Weiss (2000) devel-
oped a number of useful methodological elements
for authoring technical documents and error iden-
tification and correction.
The originality of our approach is as follows.
Authoring recommendations are made flexible and
context-dependent, for example if negation is not
allowed in instructions in general, there are, how-
ever, cases where it cannot be avoided because
the positive counterpart cannot so easily be formu-
lated, e.g., do not dispose of the acid in the sewer.
Similarly, references may be allowed if the refer-
ent is close and non-ambiguous. However, this
requires some knowledge.
Following observations in cognitive ergonomics
in the project, a specific effort is realized concern-
ing the well-formedness (following grammatical
and cognitive standards) of discourse structures
and their regularity over entire documents (e.g.,
instruction or enumerations all written in the same
way).
The production of procedures includes some
controls on contents, in particular action verb argu-
ments, as indicated in the second objective above,
via the Arias domain knowledge base, e.g., avoid-
ing typos or confusions among syntactically and
semantically well-identified entities such as instru-
ments, products, equipments, values, etc.
1http://www.boeing.com/phantom/sechecker/
2ASD-STE100, http://www.asd-ste100.org/
3http://www.acrolinx.com/
4http://www.madcapsoftware.com/products/
madpak/
There exists no real requirement analysis sys-
tem based on language that can check the qual-
ity and the consistency of large sets of authoring
recommendations. The main products are IBM
Doors and Doors Trek5, Objecteering6, and Re-
qtify7, which are essentially textual databases with
advanced visual and design interfaces, query facil-
ities for retrieving specific requirements, and some
traceability functions carried out via predefined
attributes. These three products also include a for-
mal language (essentially based on attribute-value
pairs) that is used to check some simple forms of
coherence among large sets of requirements.
The authoring tool includes facilities for French-
speaking authors who need to write in English,
supporting typical errors they make via ?language
transfer? (Garnier, 2011). We will not address this
point here.
This project, LELIE, is based on the TextCoop
system (Saint-Dizier, 2012), a system dedicated
to language analysis, in particular discourse (in-
cluding the taking into account of long-distance
dependencies). This project also includes the Arias
action knowledge base that stores prototypical ac-
tions in context, and can update them. It also in-
cludes an ASP (Answer Set Programming) solver
8 to check for various forms of incoherence and in-
completeness. The kernel of the system is written
in SWI Prolog, with interfaces in Java. The project
is currently realized for French, an English version
is under development.
The system is based on the following principles.
First, the system is parameterized: the technical
writer may choose the error types he wants to be
checked, and the severity level for each error type
when there are several such levels (e.g., there are
several levels of severity associated with fuzzy
terms which indeed show several levels of fuzzi-
ness). Second, the system simply tags elements
identified as errors, the correction is left to the
author. However, some help or guidelines are of-
fered. For example, guidelines for reformulating
a negative sentence into a positive one are pro-
posed. Third, the way errors are displayed can be
customized to the writer?s habits.
We present below a kernel system that deals
5http://www.ibm.com/software/awdtools/
doors/
6http://www.objecteering.com/
7http://www.geensoft.com/
8For an overview of ASP see Brewka et al (2011).
36
with the most frequent and common errors made
by technical writers independently of the technical
domain. This kernel needs an in-depth customiza-
tion to the domain at stake. For example, the verbs
used or the terminological preferences must be im-
plemented for each industrial context. Our system
offers the control operations, but these need to be
associated with domain data.
Finally, to avoid the variability of document for-
mats, the system input is an abstract document
with a minimal number of XML tags as required
by the error detection rules. Managing and trans-
forming the original text formats into this abstract
format is not dealt with here.
3 Categorizing language and conceptual
errors found in technical documents
In spite of several levels of human proofreading
and validation, it turns out that texts still contain
a large number of situations where recommenda-
tions are not followed. Reasons are analyzed in e.g.
e.g., (B?guin, 2003), (Mollo et al, 2004, 2008).
Via ergonomics analysis of the activity of techni-
cal writers, we have identified several layers of re-
current error types, which are not in general treated
by standard text editors such as Word or Visio, the
favorite editors for procedures.
Here is a list of categories of errors we have
identified. Some errors are relevant for a whole
document, whereas others must only be detected in
precise constructions (e.g., in instructions, which
are the most constrained constructions):
? General layout of the document: size of sen-
tences, paragraphs, and of the various forms
of enumerations, homogeneity of typography,
structure of titles, presence of expected struc-
tures such as summary, but also text global or-
ganization following style recommendations
(expressed in TextCoop via a grammar), etc.
? Morphology: in general passive constructions
and future tenses must be avoided in instruc-
tions.
? Lexical aspects: fuzzy terms, inappropriate
terms such as deverbals, light verb construc-
tions or modals in instructions, detection of
terms which cannot be associated, in partic-
ular via conjunctions. This requires typing
lexical data.
? Grammatical complexity: the system checks
for various forms of negation, referential
forms, sequences of conditional expressions,
long sequences of coordination, complex
noun complements, and relative clause em-
beddings. All these constructions often make
documents difficult to understand.
? Uniformity of style over a set of instructions,
over titles and various lists of equipments,
uniformity of expression of safety warnings
and advice.
? Correct position in the document of specific
fields: safety precautions, prerequisites, etc.
? Structure completeness, in particular com-
pleteness of case enumerations with respect
to to known data, completeness of equipment
enumerations, via the Arias action base.
? Regular form of requirements: context of
application properly written (e.g., via con-
ditions) followed by a set of instructions.
? Incorrect domain value, as detected by Arias.
When a text is analyzed, the system annotates
the original document (which is in our current
implementation a plain text, a Word or an XML
document): revisions are only made by technical
writers.
Besides tags which must be as explicit as possi-
ble, colors indicate the severity level for the error
considered (the same error, e.g., use of fuzzy term,
can have several severity levels). The most severe
errors must be corrected first. At the moment, we
propose four levels of severity:
ERROR Must be corrected.
AVOID Preferably avoid this usage, think about
an alternative,
CHECK this is not really bad, but it is recom-
mended to make sure this is clear; this is also
used to make sure that argument values are
correct, when a non-standard one is found.
ADVICE Possibly not the best language realiza-
tion, but this is probably a minor problem. It
is not clear whether there are alternatives.
The model, the implementation and the results
are presented in detail in (Barcellini et al, 2012).
37
4 Perspectives
We have developed the first phase of the LELIE
project: detecting authoring errors in technical
documents that may lead to risks. We identified a
number of errors: lexical, business, grammatical,
and stylistic. Errors have been identified from er-
gonomics investigations. The system is now fully
implemented on the TextCoop platform and has
been evaluated on a number of documents. It is
now of much interest to evaluate user?s reactions.
We have implemented the system kernel. The
main challenge ahead of us is the customization to
a given industrial context. This includes:
? Accurately testing the system on the com-
pany?s documents so as to filter out a few
remaining odd error detections,
? Introducing the domain knowledge via the
domain ontology and terminology, and en-
hancing the rules we have developed to take
every aspect into account,
? Analyzing and incorporating into the system
the authoring guidelines proper to the com-
pany that may have an impact on understand-
ing and therefore on the emergence of risks,
? Implementing the interfaces between the orig-
inal user documents and our system, with the
abstract intermediate representation we have
defined,
? Customizing the tags expressing errors to the
users profiles and expectations, and enhanc-
ing correction schemas.
When sufficiently operational, the kernel of the
system will be made available on line, and proba-
bly the code will be available in open-source mode
or via a free or low cost license.
Acknowledgements
This project is funded by the French National Re-
search Agency ANR. We also thanks reviewers
and the companies that showed a strong interest in
our project, let us access to their technical docu-
ments and allowed us to observed their technical
writers.
References
Kurt Ament. 2002. Single Sourcing. Building modular
documentation, W. Andrew Pub.
Flore Barcellini, Camille Albert, Corinne Grosse,
Patrick Saint-Dizier. 2012. Risk Analysis and Pre-
vention: LELIE, a Tool dedicated to Procedure and
Requirement Authoring, LREC 2012, Istanbul.
Patrice B?guin. 2003. Design as a mutual learning pro-
cess between users and designers, Interacting with
computers, 15 (6).
Sarah Bourse, Patrick Saint-Dizier. 2012. A Repository
of Rules and Lexical Resources for Discourse Struc-
ture Analysis: the Case of Explanation Structures,
LREC 2012, Istanbul.
Gerhard Brewka, Thomas Eiter, Miros?aw
Truszczyn?ski. 2011. Answer set programming at
a glance. Communications of the ACM 54 (12),
92?103.
Marie Garnier. 2012. Automatic correction of adverb
placement errors: an innovative grammar checker
system for French users of English, Eurocall?10 pro-
ceedings, Elsevier.
Walther Kintsch. 1988. The Role of Knowledge in Dis-
course Comprehension: A Construction-Integration
Model, Psychological Review, vol 95-2.
Elizabeth C. Hull, Kenneth Jackson, Jeremy Dick. 2011.
Requirements Engineering, Springer.
William C. Mann, Sandra A. Thompson. 1988. Rhetor-
ical Structure Theory: Towards a Functional Theory
of Text Organisation, TEXT 8 (3), 243?281. Sandra
A. Thompson. (ed.), 1992. Discourse Description:
diverse linguistic analyses of a fund raising text,
John Benjamins.
Dan Marcu. 1997. The Rhetorical Parsing of Natural
Language Texts, ACL?97.
Dan Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization, MIT Press.
Vanina Mollo, Pierre Falzon. 2004. Auto and allo-
confrontation as tools for reflective activities. Ap-
plied Ergonomics, 35 (6), 531?540.
Vanina Mollo, Pierre Falzon. 2008. The development of
collective reliability: a study of therapeutic decision-
making, Theoretical Issues in Ergonomics Science,
9(3), 223?254.
Dietmar R?sner, Manfred Stede. 1992. Customizing
RST for the Automatic Production of Technical
Manuals, In Robert Dale et al (eds.) Aspects of
Automated Natural Language Generation. Berlin:
Springer, 199?214.
Dietmar R?sner, Manfred Stede. 1994. Generating
multilingual technical documents from a knowledge
base: The TECHDOC project, In: Proc. of the Inter-
national Conference on Computational Linguistics,
COLING-94, Kyoto.
Patrick Saint-Dizier. 2012. Processing Natural Lan-
guage Arguments with the TextCoop Platform, Jour-
nal of Argumentation and Computation.
Edmond H. Weiss. 2000. Writing remedies. Practical
exercises for technical writing, Oryx Press.
38
