Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
  	
 ffSecond Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 483?489, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBM: Combining lexicon-based ML and heuristics
for Social Media Polarities
Carlos Rodr??guez-Penagos, Jordi Atserias, Joan Codina-Filba`,
David Garc??a-Narbona, Jens Grivolla, Patrik Lambert, Roser Saur??
Barcelona Media
Av. Diagonal 177, Barcelona 08018
Corresponding author: carlos.rodriguez@barcelonamedia.org
Abstract
This paper describes the system implemented
by Fundacio? Barcelona Media (FBM) for clas-
sifying the polarity of opinion expressions in
tweets and SMSs, and which is supported by
a UIMA pipeline for rich linguistic and sen-
timent annotations. FBM participated in the
SEMEVAL 2013 Task 2 on polarity classifi-
cation. It ranked 5th in Task A (constrained
track) using an ensemble system combining
ML algorithms with dictionary-based heuris-
tics, and 7th (Task B, constrained) using an
SVM classifier with features derived from the
linguistic annotations and some heuristics.
1 Introduction
We introduce the FBM system for classifying the
polarity of short user-generated text (tweets and
SMSs), which participated in the two subtasks of
SEMEVAL 2013 Task 2 on Sentiment Analysis in
Twitter. These are: Task A. Contextual Polarity Dis-
ambiguation, and Task B. Message Polarity Classifi-
cation. The former aimed at classifying the polarity
of already identified opinion expressions (or cues),
whereas the latter consisted in classifying the polar-
ity of the whole text (Wilson et al, 2013).
The literature agrees on two main approaches for
classifying opinion expressions: using supervised
learning methods and applying dictionary/rule-
based knowledge (see (Liu, 2012) for an overview).
Each of them on its own has been used in work-
able systems, and a principled combination of both
of them can yield good results on noisy data, since
generally one (dictionaries/rules) offers good preci-
sion while the other (ML) is able to discover unseen
examples and thus enhances recall.
FBM combined both approaches in order to bene-
fit from their respective strengths and compensating
as much as possible their weaknesses. For Task A
we used linguistic (lexical and syntactic) annotations
to implement both types of approaches. On the one
hand, we built machine learning classifiers based on
Support Vector Machines (SVMs) and Conditional
Random Fields (CRFs). On the other, we imple-
mented a basic classification system mainly based
on polarity dictionaries and negation information, as
well as simple decision tree-like heuristics extracted
from the training data. For task B we trained an
SVM classifier using some of the annotations from
Task A.
The paper first presents the process of data com-
pilation and preprocessing (section 2), and then de-
scribes the systems for Tasks A (section 3) and B
(section 4). Results and conclusions are discussed
in the last section.
2 Data Compilation and Processing
2.1 Making data available
The corpus of SMSs was provided to the partici-
pants by the organizers of the task. As for the corpus
of tweets, legal restrictions on twitter data distribu-
tion required the participants to download the tex-
tual contents of the corpus from a list of tweet ids.
We retrieved the tweet text using the official twit-
ter API instead of script provided by the organizers,
but not all the tweets were available for download
483
due to restrictions of different types (e.g. geograph-
ical), or because the twitter account was temporarily
suspended. In total, we managed to retrieve 10,764
tweets out of 11,777 ids provided by the organizers
(91.4%). It is worth pointing out that the restric-
tions on tweets distribution can become an issue for
future users of the dataset, as the amount of avail-
able tweets will diminish over time. By contrast, the
twitter test corpus was distributed with the full text
to avoid those problems.
2.2 Leveraging the data with rich linguistic
information
We applied the same linguistic processing to both
corpora (SMSs and tweets), even though the SMS
test data presents very different characteristics from
the twitter data, not only because of what can be ap-
preciated as genre differences, but also due to the
fact that is apparently written in Singaporean En-
glish, which differs significantly from American or
British English. No efforts were made to adapt
our linguistic processing modules and dictionaries
to this data.
Tweets and SMSs were processed with a UIMA1-
based pipeline consisting of a set of linguistic and
opinion-oriented modules, which includes:
Basic linguistic processing: Sentence segmen-
tation, tokenization, POS-tagging, lemmatiza-
tion.
Syntax: Dependency parsing.
Lexicon-based annotations:
? Basic polarity, distinguishing among: positive,
negative, and neutral, as encoded in Wilson et
al. (2010).
? Polarity strength, using the score for pos-
itive and negative polarity in SentiWordnet
3.0 (Baccianella et al, 2010). Each Sen-
tiWordNet synset has an associated triplet of
numerical scores (positive, negative,
and objective) expressing the intensity of
positive, negative and objective polarity of the
terms it contains. They range from 0.0 to 1.0,
and their sum is 1.0 for each synset (Esuli and
Sebastiani, 2007). We selected only the synset
1http://uima.apache.org/uima-specification.html
with positive or negative scores higher than 0.5,
containing a total of 16,791 words.
? Subjectiviy clues, from Wilson et al (2010),
which are classified as weak or strong depend-
ing on their degree of subjectivity.
? Sentiment expressions, from the Linguistic In-
quiry and Word Count (LIWC) 2001 Dictio-
nary (Pennebaker et al, 2001).
? In-house compiled lexicons of negation mark-
ers (such as ?no?, ?never?, ?none?) and quanti-
fiers (?all?, ?many?, etc.), the latter further clas-
sified into low, medium and high according to
their quantification degree.
The different classifiers employed by FBM con-
structed their vectors from this output to learn global
and contextual polarities.
3 Task A: Ensemble System
Our system combined Machine Learning and rule-
based approaches. The aim was to combine the
strengths of each individual component while avoid-
ing as much as possible their weaknesses. In what
follows we describe each system component as well
as the way the ensemble system worked out the col-
lective decisions.
3.1 Conditional Random Fields
One of the classifiers uses the Conditional Random
Fields implementation of a biomedical Named En-
tity Recognition system (JNET from JulieLab) 2, ex-
ploiting the classification capabilities of the system
(rather than its span detection) by strongly associat-
ing already defined ?marked instances? with a polar-
ity, and exploring a 5-word window. It uses depen-
dency labels, POS tags, polar words, sentiwordnet
and LWIC sentiment annotations, as well as indica-
tions for quantifiers and negation markers.
3.2 Support Vector Machines
This classifier was implemented using an SVM algo-
rithm with a linear kernel and the C parameter set to
0.2 (determined using a 5 fold cross-validation). The
features set includes those that we used in RepLab
2http://www.julielab.de
484
2012 (Chenlo et al, 2012) (including number of:
characters, words, links, hashtags, positive and neg-
ative emoticons, question-exclamation marks, ad-
jectives, nouns, verbs, adverbs, uppercased words,
words with duplicated vowels), plus a set of new
features at tweet level obtained from the linguistic
annotations: number of high/medium/low polarity
quantifiers, number of positive and negative polar
words, sentiwordnet applied to both the cue and the
whole tweet.
Moreover, the RepLab polarity calculation based
on different dictionaries was modified to take into
account negation (in a 3-word window) potentially
inverting the polarity (negPol). This polarity mea-
sure was applied to the cue and to the whole tweet,
thus generating two additional features.
3.3 Heuristic Approach
In task A, in parallel to the supervised learning sys-
tem, we developed a method (named Heur) based
on polarity dictionary lookup and simple heuristics
(see Figure 1) taking into account opinion words
as well as negation markers and quantifiers. These
heuristics were implemented so as to maximize the
number of correct positive and negative labels in the
training data. To this end, we calculated the aggre-
gate polarity of a cue segment as the sum of word
polarities found in the polarity lexicon. The aggre-
gate values in the training set ranged from -3 to +3,
taking respectively 1, 0 and -1 as the polarity of pos-
itive, neutral and negative words. The label distri-
bution of cue segments with an aggregate polarity
value of -1 is shown in Table 1.
Aggregate polarity -1
Negation no yes
negative 1,032 30
neutral 37 4
positive 178 71
Table 1: Cue segment polarity statistics in training data
for an aggregate polarity value of -1.
In this case, if no negation is present in the cue
segment, a majority (1,032) of examples had the
negative label. In case there was at least a negation, a
majority (71) of examples had a positive label. This
behaviour was observed with all negative aggregate
1: if has polar word(CUE) then
2: polarity= lex(P)-0.5*lex(QP)
3: -lex(N)+0.5*lex(QN)
4: if polarity>0 then
5: if has negation(CUE) then negative
6: else positive
7: end if
8: else if polarity<0 then
9: if has negation(CUE) then positive
10: else negative
11: end if
12: else
13: if has negation(CUE) then positive
14: else negative
15: end if
16: end if
17: else if has negation(CUE) then negative
18: else
19: polarity= tlex(P)-0.5*tlex(QP)
20: -tlex(N)+0.5*tlex(QN)
21: if polarity<0 then negative
22: else if tlex(NEU)>0 then neutral
23: else if polarity>0 then positive
24: else if has negemo(CUE) then negative
25: else if has posemo(CUE) then positive
26: else unknwn
27: end if
28: end if
Figure 1: Heuristics used by the lexicon-based system to
classify the polarity of a segment marked up as opinion
cue (Task A).
polarity values in training data, yielding the rule in
lines 8 to 11 of Figure 1. Similar rules were ex-
tracted for the other aggregate polarity values (lines
4 to 16 of Figure 1).
Figure 1 details the complete classification algo-
rithm. Note (lines 1 to 17) that we first rely on the
basic polarity lexicon annotations (described in sec-
tion 2). The final aggregate polarity formula (lines
2-3) was refined to distinguish sentiment words
which act as quantifiers, such as pretty in pretty mad.
The word pretty is both a positive polar word and a
quantifier. We want its polarity to be positive in case
it occurs in isolation, but less than one so that the
sum with a following negative polar word (such as
mad) be negative. We thus give this kind of words
a polarity of 0.5 by substracting 0.5 for each polar
word which is also a quantifier. In the polarity for-
mula of lines 2-3, lex(X) refers to the number of
words annotated as X, P and N refer respectively
to positive and negative polar words, and QP and
485
QN refer to positive and negative polar words which
are also quantifiers. Quantifiers which are not polar
words are not taken into account because they are
not likely to change the opinion polarity.
In case that no annotations from the basic polar-
ity, quantifiers, and negative markers lexicons are
found (lines 18 to 28), we look up in dictionaries
built from the training data (tlex in lines 19-20).
To build these dictionaries, we counted how many
times each word was labeled positive, negative and
neutral. We considered that a word has a given po-
larity if the number of times it was assigned to this
class is greater than the number of times it was as-
signed to any other class by a given threshold. We
calculated the polarity in the same way as before,
but now with the counts from the lexicon automati-
cally compiled from the training data. To improve
the recall of the dictionary lookup, we performed
some text normalization: lowercasing, deletion of
repeated characters (such as gooood) and deletion of
the hashtag ?#? character. Finally, if no polar word
is found in the automatically compiled lexicon, we
look at the sentiment annotations (extracted from the
LIWC dictionary).
3.4 Ensemble Voting Algorithm
As already mentioned, we combined the results from
the described polarity methods to build a collective
decision. Table 2 shows the performance (in terms
of F1 measure) of the different single methods over
the tweet test data.
SVM Heur Heur+ CRF
Test 80.74 83.47 84.62 62.85
Table 2: Twitter Task A results for different methods
Although the heuristic method outperforms the
ML methods, they are not only different in nature
(ML vs. heuristic) but also use different information
(see Table 5). This suggests that the ensemble solu-
tion will be complementary and capable of obtaining
better results than any of the individual methods by
itself.
The development set was used to calculate the en-
semble response given the individual votes of the
different systems in a way similar to the behavior
knowledge space method (Huang and Suen, 1993).
Table 3 shows an example of how the assemble
voting is built. For each method vote combina-
tion (SVM-Heuristics-CRF) the number of positives
/ negatives / neutral is calculated in the development
data. The ensemble (EV) selects the vote that max-
imizes the number of correct votes in the develop-
ment data (in bold).
SVM Heur CRF EV
# Instances
pos neg neu
? + ? ? 0 6 0
? ? + ? 1 23 2
? ? ? ? 3 125 2
? u + + 1 0 0
+ u n ? 0 1 0
+ ? + + 17 13 2
+ + + + 314 18 17
+ ? n + 3 1 0
Table 3: Oracle building example (EV: Ensemble Vote,
+:positive, ?:negative, n:neutral, u:unknown)
The test data contains some combination of votes
that were not seen in the development data. Thus,
in order to deal with these unseen combinations of
votes in the test set we use the following backup
heuristics based on the preformance figures of the
individual methods: Use the vote of the heuristic
method. If this method does not vote (u), then se-
lect the SVM vote.
Table 4 shows the results of the proposed ensem-
ble method, the well-known majority voting and the
upper bound of this ensemble method (calculated
with the same strategy over the test data), over the
development and test tweet data
Ensemble Majority Upper
Voting Voting Bound
Dev 85.48 81.31 85.48
Test 85.50 82.70 89.37
Table 4: Results for different ensemble strategies
In the development corpus, the upper bound and
ensemble results are the same, given that they ap-
ply the same knowledge. The difference is in the
test dataset, where the ensemble voting is calculated
based on the knowledge obtained from the develop-
ment corpus, while the upper bound uses the knowl-
edge that can be derived from the test corpus.
486
Table 5 illustrates the features used by each com-
ponent.
SVM SVM CRF Heur
(task A) (task B)
word ? ? ?
lemma
pos ? ?
deps ?
pol ? ? ? ?
polW ?
sent ? ? ?
sentiwn ? ? ?
quant ? ? ? ?
neg ? ? ? ?
links ?
hashTags ?
Table 5: Information used (pos: part-of-speech; deps: de-
pendencies; pol: basic polarity classification; polW: basic
polarity word; sent: LIWC sentiments; sentwn: Senti-
Wordnet; quant/neg: quantifiers and negation markers.)
4 Task B: A Support Vector
Machine-based System
The system presented for task B is based on ML us-
ing a SVM model. The feature vector used as input
for the SVM component is composed of the annota-
tions provided by the linguistic annotation pipeline,
extended with a feature obtained by applying nega-
tion to the next polar words (window of size 3).
The features used do not include the words (or
their lemmas) because the number of tweets avail-
able for training is small (104) compared to the num-
ber of different words (4 ? 104). A model based on
bag-of-words would suffer from overfitting and thus
be very domain and time-dependent. If the train and
test sets were randomly selected from a bigger set,
the use of words could increase the model?s accu-
racy, but the model would also be too narrowly ap-
plied to this specific dataset.
From the annotation pipeline we extracted as fea-
tures: the polar words (PolW) and their basic po-
larity (Pol); the sentiment annotations from LIWC
(Sent); the negation markers (Neg) and quantifiers
(Quant). The model was trained using Weka (Hall
et al, 2009).
The model used is SVM with the C parameter set
to 1.0 and applying a 10 fold cross-validation. The
option of doing first a model to discriminate polar
and neutral tweets was discarded because Weka al-
ready does that when training classifiers for more
than two training classes, and the combination of the
two classifiers (a first one between polar and opin-
ionated and a second one between positive and neg-
ative) would produce the same results.
5 Results and Discussion
The results of our system in each subcorpus and task
are presented in Table 5 (average of the F1-measure
over the classes positive and negative, constrained
track), with the ranking achieved in the competition
in parentheses.
Tweet Corpus SMS Corpus
Task A 0.86 (5th) 0.73 (11th)
Task B 0.61 (7th) 0.47 (28th)
Table 6: FBM system performance (F1 average over pos-
itive and negative classes, constrained track) and rankings
Given the differences in style and vocabularies be-
tween the SMS and tweet corpora, and the fact that
we made not effort whatsoever to adapt our system
or models to them, the drop in performance from
one to the other is considerable, but to be expected
since domain customization is an important aspect
of opinion mining.
Task A: The confusion matrix in Table 7 shows
an acceptable performance for the most frequent
classes in the corpus (with an error of 7.75% and
19.5% for postive and negative cues, respectively)
and a very poor job for neutral cues (98.1% of er-
ror), clearly a minority class in the training corpus
(5% of the data).
GOLD: Pos Neg Neu
SYSTEM: Pos 2,522 296 126
Neg 206 1,240 31
Neu 6 5 3
Table 7: Task A confusion matrix
Given the skewed distribution of polarity cate-
gories in the test corpus, however, neutral mistakes
amount to only 23% of our system error, and so we
487
focus our analysis on the problems in positive and
negative cues, respectively amounting to 31.7% and
44.8% of the total error. There are 2 main sources of
error:
? Limitations of the dictionaries employed,
which were short in covering somewhat fre-
quent slang words (e.g., wacky, baddest, shit-
loads), expressions (e.g., ouch, yukk, C?MON),
or phrases (e.g., over the top), some of which
express a particular polarity but contain a word
expressing just the opposite (have a blast, to
want something bad/ly).
? Problems in UGC processing, mainly related to
normalization (e.g., fooooool) and tokenization
(Perfect...not sure), which put at risk the cor-
rect identification of lexical elements that are
crucial for polarity classification.
Task B: The average F-score of positive and neg-
ative classes was 0.62 in the development set (that
was included in the training set) and the averaged F-
score for the test set was 0.61 (so they are very simi-
lar). If focusing on precision and recall, the positive
and negative classes have higher precision but lower
recall in the test set. We think that this low degrada-
tion of perfomance indicates the model?s potential
for generalization.
6 Conclusions
From our results, we can conclude that the use of
ensemble combination of orthogonal methods pro-
vides good performance for Task A. Similar results
could be expected for Task B (judging from mix-
ing dictionaries and ML in similar tasks at RepLab
2012 (Chenlo et al, 2012)). The ML methods that
we applied for Task B are essentially additive, and
hence have difficulties in applying features such as
polarity shifters. To overcome this, one of the fea-
tures includes negation of polar words when a polar-
ity shifter is near.
Overall, the SemEval Tasks have make evident the
usual challenges when mining opinions from Social
Media channels: noisy text, irregular grammar and
orthography, highly specific lingo, etc. Moreover,
temporal dependencies can affect the performance if
the training and test data have been gathered at dif-
ferent times, as is the case with text of such a volatile
nature as tweets and SMSs.
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
35.00%
40.00%
45.00%
50.00%
train
dev
test
Figure 2: Distribution of tweets over time
The histogram in Figure 2 shows that this also ap-
plies to the Semeval tweets dataset. It illustrates the
distribution of tweets over time (extrapolated from
the sequential ids) in the 3 subcorpora (train, devel-
opment and test), showing some divergence between
the test corpus on the one hand, and the develop-
ment and training corpora on the other. Neverthe-
less, our system shows little performance degrada-
tion between development and testing results, as at-
tested in Table 4 (ensemble voting column).
Our work here and at other competitions already
cited validate a system that combines stochastic and
symbolic methodologies in a principled, data-driven
approach. Time and domain dependencies of Social
Media data make system and model generalization
highly desirable, and our system hybrid nature also
contribute to this objective.
Acknowledgments
This work has been partially funded by the Spanish
Government project Holopedia, TIN2010-21128-
C02-02, the CENIT program project Social Media,
CEN-20101037, and the Marie Curie Reintegration
Grant PIRG04-GA-2008-239414.
488
References
Baccianella, Stefano, Andrea Esuli and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation, Valletta, Malta.
Chenlo, Jose M., Jordi Atserias, Carlos Rodr??guez-
Penagos and Roi Blanco. 2012. FBM-Yahoo!
at RepLab 2012. In: P. Forner, J. Karlgren,
C. Womser-Hacker (eds.) CLEF 2012 Evalua-
tion Labs and Workshop, Online Working Notes.
http://clef2012.org/index.php?page=Pages/procee-
dings.php.
Esuli, Andrea and Fabrizio Sebastiani. 2007. SEN-
TIWORDNET: a high-coverage lexical resource for
opinion mining. Technical Report ISTI-PP-002/2007,
Institute of Information Science and Technologies
(ISTI) of the Italian National Research Council
(CNR).
Hall, Mark, Frank Eibe, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann and Ian H. Witten. 2009.
The WEKA data mining software: an update. In:
ACM SIGKDD Explorations Newsletter, 1: 10?18.
Huang, Y. S. and C. Y. Suen. 1993. Behavior-knowledge
space method for combination of multiple classifiers.
In Proceedings of IEEE Computer Vision and Pattern
Recognition, 347?352.
Liu, Bing. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
(5-1), 1?167.
Pennebaker, James W., Martha E. Francis and Roger
J. Booth. 2001. Linguistic inquiry and word count:
LIWC 2001. Mahway: Lawrence Erlbaum Asso-
ciates.
Wilson, Theresa, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov and Alan. Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13.
Wilson, Theresa, Janyce Wiebe and Paul Hoffmann.
2010. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3), 399?433.
489
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 368?375,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
VERTa participation in the WMT14 Metrics Task 
 
 
Elisabet Comelles 
Universitat de Barcelona 
Barcelona, Spain 
elicomelles@ub.edu 
Jordi Atserias 
Yahoo! Labs 
Barcelona, Spain 
jordi@yahoo-inc.com 
 
 
Abstract 
In this paper we present VERTa, a lin-
guistically-motivated metric that com-
bines linguistic features at different lev-
els. We provide the linguistic motivation 
on which the metric is based, as well as 
describe the different modules in VERTa 
and how they are combined. Finally, we 
describe the two versions of VERTa, 
VERTa-EQ and VERTa-W, sent to 
WMT14 and report results obtained in 
the experiments conducted with the 
WMT12 and WMT13 data into English. 
1 Introduction 
In the Machine Translation (MT) process, the 
evaluation of MT systems plays a key role both 
in their development and improvement. From the 
MT metrics that have been developed during the 
last decades, BLEU (Papineni et al., 2002) is one 
of the most well-known and widely used, since it 
is fast and easy to use. Nonetheless, researchers 
such as (Callison-Burch et al., 2006) and (Lavie 
and Dekowski, 2009) have claimed its weak-
nesses regarding translation quality and its ten-
dency to favour statistically-based MT systems. 
As a consequence, other more complex metrics 
that use linguistic information have been devel-
oped. Some use linguistic information at lexical 
level, such as METEOR (Denkowski and Lavie, 
2011); others rely on syntactic information, ei-
ther using constituent (Liu and Hildea, 2005) or 
dependency analysis (Owczarzack et al., 2007a 
and 2007b; He et al., 2010); others use more 
complex information such as semantic roles 
(Gim?nez and M?rquez, 2007 and 2008a; Lo et 
al., 2012). All these metrics focus on partial as-
pects of language; however, other researchers 
have tried to combine information at different 
linguistic levels in order to follow a more holistic 
approach. Some of these metrics follow a ma-
chine-learning approach (Leusch and Ney, 2009; 
Albrecht and Hwa, 2007a and 2007b), others 
combine a wide variety of metrics in a simple 
and straightforward way (Gim?nez, 2008b; 
Gim?nez and M?rquez, 2010; Specia and Gim?-
nez, 2010). However, very little research has 
been performed on the impact of the linguistic 
features used and how to combine this informa-
tion from a linguistic point of view. Hence, our 
proposal is a linguistically-based metric, VERTa 
(Comelles et al., 2012), which uses a wide vari-
ety of linguistic features at different levels, and 
aims at combining them in order to provide a 
wider and more accurate coverage than those 
metrics working at a specific linguistic level. In 
this paper we provide a description of the lin-
guistic information used in VERTa, the different 
modules that form VERTa and how they are 
combined according to the language evaluated 
and the type of evaluation performed. Moreover, 
the two versions of VERTa participating in 
WMT14, VERTa-EQ and VERTa-W are de-
scribed. Finally, for the sake of comparison, we 
use the data available in WMT12 and WMT13 to 
compare both versions to the metrics participat-
ing in those shared tasks. 
2 Linguistic Motivation 
Before developing VERTa, we analysed those 
linguistic phenomena that an MT metric should 
cover. From this analysis, we decided to organise 
the information into the following groups: 
 Lexical information. The use of lexical 
semantics plays a key role when compar-
ing a hypothesis and reference segment, 
since it allows for identifying relations of 
synonymy, hypernymy and hyponymy. 
 Morphological information. This type of 
information is crucial when dealing with 
languages with a rich inflectional mor-
phology, such as Spanish, French or Cata-
368
lan because it helps in covering phenom-
ena related to tense, mood, gender, num-
ber, aspect or case. In addition, morphol-
ogy in combination with syntax (morpho-
syntax) is also important to identify 
agreement (i.e. subject-verb agreement). 
This type of information should be taken 
into account when evaluating the fluency 
of a segment. 
 Syntactic information. This type of in-
formation covers syntactic structure, syn-
tactic relations and word order.  
 Semantic information. Named Entities 
(NEs), sentence polarity and time expres-
sions are included here. 
All this information described above should be 
taken into account when developing a metric that 
aims at covering linguistic phenomena at differ-
ent levels and evaluate both adequacy and flu-
ency. 
3 Metric Description 
In order to cover the above linguistic features, 
VERTa is organised into different modules: 
Lexical similarity module, Morphological simi-
larity module, Dependency similarity module and 
Semantic similarity module. Likewise, an Ngram 
similarity module has also been added in order to 
account for similarity between chunks in the hy-
pothesis and reference segments. Each metric 
works first individually and the final score is the 
Fmean of the weighted combination of the Preci-
sion and Recall of each metric in order to get the 
results which best correlate with human assess-
ment. This way, the different modules can be 
weighted depending on their importance regard-
ing the type of evaluation (fluency or adequacy) 
and language evaluated. In addition, the modular 
design of this metric makes it suitable for all lan-
guages. Even those languages that do not have a 
wide range of NLP tools available could be 
evaluated, since each module can be used iso-
lated or in combination. 
All metrics use a weighted precision and recall 
over the number of matches of the particular 
element of each level (words, dependency triples, 
ngrams, etc) as shown below. 
 
)(
))((
h
hnmatchWP D
 
R
W nmatch
D
( (r))
(r) 
Where r is the reference, h is the hypothesis 
and ? is a function that given a segment will 
return the elements of each level (e.g. words at 
lexical level and triples at dependency level). D 
is the set of different functions to project the 
level element into the features associated to each 
level, such as word-form, lemma or partial-
lemma at lexical level. nmatch () is a function 
that returns the number of matches according to 
the feature ? (i.e. the number of lexical matches 
at the lexical level or the number of dependency 
triples that match at the dependency level). Fi-
nally, W is the set of weights ]0 1] associated to 
each of the different features in a particular level 
in order to combine the different kinds of 
matches considered in that level.  
All modules forming VERTa and the linguis-
tic features used are described in detail in the 
following subsections. 
3.1 Lexical module 
Inspired by METEOR, the lexical module 
matches lexical items in the hypothesis segment 
to those in the reference segment taking into ac-
count several linguistic features. However, while 
METEOR uses word-form, synonymy, stemming 
and paraphrasing, VERTa relies on word-form, 
synonymy1, lemma, partial lemma2, hypernyms 
and hyponyms. In addition, a set of weights is 
assigned to each type of match depending on 
their importance as regards semantics (see Table 
1). 
 W
  
Match Examples 
HYP REF 
1 1 Word-form east east 
2 1 Synonym believed considered 
3 1 Hypernym barrel keg 
4 1 Hyponym keg barrel 
5 .8 Lemma is_BE are_BE 
6 .6 Part-lemma danger dangerous 
Table 1. Lexical matches and examples. 
3.2 Morphological similarity module 
The morphological similarity module is based on 
the matches established in the lexical module 
(except for the partial-lemma match) in combina-
tion with Part-of-Speech (PoS) tags from the an-
notated corpus3. The aim of this module is to 
                                                 
1 Information on synonyms, lemmas, hypernyms and 
hyponyms is obtained from WordNet 3.0. 
2 Lemmas that share the first four letters. 
3 The corpus has been PoS tagged using the Stanford 
Parser (de Marneffe et al. 2006). 
369
compensate the broader coverage of the lexical 
module, preventing matches such as invites and 
invite, which although similar in terms of mean-
ing, do not coincide as for their morphological 
information. Therefore, this module turns more 
appropriate to assess the fluency of a segment 
rather than its adequacy. In addition, this module 
will be particularly useful when evaluating lan-
guages with a richer inflectional morphology (i.e. 
Romance languages). 
In line with the lexical similarity metric, the 
morphological similarity metric establishes 
matches between items in the hypothesis and the 
reference sentence and a set of weights (W) is 
applied. However, instead of comparing single 
lexical items as in the previous module, in this 
module we compare pairs of features in the order 
established in Table 2. 
 
W Match Examples 
HYP REF 
1 (Word-
form, PoS) 
(he, PRP) (he, PRP) 
1 (Synonym, 
PoS) 
(VIEW, 
NNS) 
(OPINON, 
NNS) 
1 (Hypern., 
PoS) 
(PUBLICA-
TION, NN) 
(MAGA-
ZINE, NN) 
1 (Hypon., 
PoS) 
(MAGA-
ZINE, NN) 
(PUBLI-
CATION, 
NN) 
.8 (LEMMA, 
PoS) 
can_(CAN, 
MD) 
Could_(C
AN, MD) 
Table 2. Morphological module matches. 
3.3 Dependency similarity module 
The dependency similarity metric helps in cap-
turing similarities between semantically compa-
rable expressions that show a different syntactic 
structure (see Example 1), as well as changes in 
word order (see Example 2). 
Example 1: 
HYP: ...the interior minister... 
REF: ...the minister of interior... 
In example 1 both hypothesis and reference 
chunks convey the same meaning but their syn-
tactic constructions are different. 
Example 2: 
HYP: After a meeting Monday night with the 
head of Egyptian intelligence chief Omar 
Suleiman Haniya said.... 
REF: Haniya said, after a meeting on Monday 
evening with the head of Egyptian Intelligence 
General Omar Suleiman... 
In example 2, the adjunct realised by the PP 
After a meeting Monday night with the head of 
Egyptian intelligence chief Omar Suleiman oc-
cupies different positions in the hypothesis and 
reference strings. In the hypothesis it is located at 
the beginning of the sentence, preceding the sub-
ject Haniya, whereas in the reference, it is placed 
after the verb. By means of dependencies, we can 
state that although located differently inside the 
sentence, both subject and adjunct depend on the 
verb. 
This module works at sentence level and fol-
lows the approach used by (Owczarzack et al., 
2007a and 2007b) and (He et al., 2010) with 
some linguistic additions in order to adapt it to 
our metric combination. Similar to the morpho-
logical module, the dependency similarity metric 
also relies first on those matches established at 
lexical level ? word-form, synonymy, hy-
pernymy, hyponymy and lemma ? in order to 
capture lexical variation across dependencies and 
avoid relying only on surface word-form. Then, 
by means of flat triples with the form La-
bel(Head, Mod) obtained from the parser4, four 
different types of dependency matches have been 
designed (see Table 3) and weights have been 
assigned to each type of match. 
 
W Match Type Match Descr. 
1 Complete Label1=Label2 
Head1=Head2 
Mod1=Mod2 
1 Partial_no_label Label1?Label2 
Head1=Head2 
Mod1=Mod2 
.9 Partial_no_mod Label1=Label2 
Head1=Head2 
Mod1?Mod2 
.7 Partial_no_head Label1=Label2 
Head1?Head2 
Mod1=Mod2 
Table 3. Dependency matches. 
 
In addition, dependency categories also re-
ceive a different weight depending on how in-
formative they are: dep, det and _5 which receive 
0.5, whereas the rest of categories are assigned 
the maximum weight (1). 
Finally, a set of language-dependent rules has 
been added with two goals: 1) capturing similari-
ties between different syntactic structures con-
                                                 
4 Both hypothesis and reference strings are annotated 
with dependency relations by means of the Stanford 
parser (de Marneffe et al. 2006). 
5 _ stands for no_dep_label 
370
veying the same meaning; and 2) restricting cer-
tain dependency relations (i.e. subject word order 
when translating from Arabic to English).  
3.4 Ngram similarity module 
The ngram similarity metric matches chunks in 
the hypothesis and reference segments and relies 
on the matches set by the lexical similarity met-
ric, which allows us to work not only with word-
forms but also with synonyms, lemmas, partial-
lemmas, hypernyms and hyponyms as shown in 
Example 3, where the chunks [the situation in 
the area] and [the situation in the region] do 
match, even though area and region do not share 
the same word-form but a relation of synonymy. 
Example 3: 
HYP: ? the situation in the area? 
REF: ? the situation in the region? 
3.5 Semantics similarity module 
As confirmed by the lexical module, semantics 
plays an important role in the evaluation of ade-
quacy. This has also been claimed by (Lo and 
Wu, 2010) who report that their metric based on 
semantic roles outperforms other well-known 
metrics when adequacy is assessed. With this 
aim in mind the semantic similarity module uses 
other semantic features at sentence level: NEs, 
time expressions and polarity. 
Regarding NEs, we use Named-Entity recog-
nition (NER) and Named-Entity linking (NEL). 
Following previous NE-based metrics (Reeder et 
al., 2011 and Gim?nez, 2008) the NER metric6 
aims at capturing similarities between NEs in the 
hypothesis and reference segments. On the other 
hand NEL7 focuses only on those NEs that ap-
pear on Wikipedia, which allows for linking NEs 
regardless of their external form. Thus, EU and 
European Union will be captured as the same 
NE, since both of them are considered as the 
same organisation in Wikipedia. 
As regards time expressions, the TIMEX met-
ric matches temporal expressions in the hypothe-
sis and reference segments regardless of their 
form. The tool used is the Stanford Temporal 
Tagger (Chang and Manning, 2012) which rec-
ognizes not only points in time but also duration. 
By means of this metric, different syntactic struc-
tures conveying the same time expression can be 
                                                 
6 In order to identify NEs we use the Supersense Tag-
ger (Ciaramita and Altun, 2006). 
7 The NEL module uses a graph-based NEL tool 
(Hachey, Radford and Curran, 2010) which links NEs 
in a text with those in Wikipedia pages. 
matched, such as on February 3rd and on the 
third of February. 
Finally, it has been reported that negation 
might pose a problem to SMT systems (Wetzel 
and Bond, 2012). In order to answer such need, a 
module that checks the polarity of the sentence 
has been added using the dictionary strategy de-
scribed (Atserias et al., 2012):  
 Adding 0.5 for each weak positive word. 
 Adding 1.0 for each strong positive word. 
 Subtracting 0.5 for each weak negative 
word. 
 Subtracting 1.0 for each strong negative 
word. 
For each query term score, the value is propa-
gated to the query term positions by reducing its 
strength in a factor of 1/n, where n is the distance 
between the query term and the polar term. 
According to the experiments performed, this 
module shows a low correlation with human 
judgements on adequacy, since only partial as-
pects of translation are considered, whereas hu-
man judges assess whole segments. However, 
regardless of how well/bad the module correlates 
with human judgements, it proves useful to 
check partial aspects of the segments translated, 
such as the correct translation of NEs or the cor-
rect translation of negation. 
3.6 Metrics combination 
The modular design of VERTa allows for pro-
viding different weights to each module depend-
ing on the type of evaluation and the language 
evaluated. Thus following linguistic criteria 
when evaluating adequacy, those modules which 
must play a key role are the lexical and depend-
ency module, since they are more related to se-
mantics; whereas, when evaluating fluency those 
related to morphology, morphosyntax and con-
stituent word order will be the most important. 
Moreover, metrics can also be combined depend-
ing on the type of language evaluated. If a lan-
guage with a rich inflectional morphology is as-
sessed, the morphology module should be given 
a higher weight; whereas if the language evalu-
ated does not show such a rich inflectional mor-
phology, the weight of the morphology module 
should be lower. 
4 Experiments and results 
Experiments were carried out on WMT data, 
specifically on WMT12 and WMT13 data, all 
languages into English. Languages ?all? include 
French, German, Spanish and Czech for WMT12 
371
and French, German, Spanish, Czech and Rus-
sian for WMT13. Both segment and system level 
evaluations were performed. Evaluation sets pro-
vided by WMT organizers were used to calculate 
both segment and system level correlations. 
Since VERTa has been mainly designed to as-
sess either adequacy or fluency separately, our 
goal for WMT14 was to find the best combina-
tion in order to evaluate whole translation qual-
ity. Firstly we decided to explore the influence of 
each module separately. To this aim, all modules 
described above, except for the semantics one 
were used and tested separately. Secondly, all 
modules were assigned the same weight and 
tested in combination (VERTa-EQ). The reason 
why the semantics module was disregarded is 
that it does not usually correlate well with human 
judgements, as stated above. Each module was 
set as follows: 
 Lexical module. As described above, ex-
cept for the use of hypernyms/hyponyms 
matches that were disregarded. 
 Morphological module. As described 
above, except for the lemma-PoS match 
and the hypernyms/hyponyms-PoS match. 
 Dependency module. As described above. 
 Ngram module. As described above, using 
a 2-gram length. 
 
Finally, we used the module combination 
aimed at evaluating adequacy, which is mainly 
based on the dependency and lexical modules, 
but with a stronger influence of the ngram mod-
ule in order to control word order (VERTa-W). 
Weights were manually assigned, based on re-
sults obtained in previous experiments conducted 
for adequacy and fluency (Comelles et al., 2012), 
as follows: 
 Lexical module:  0.41 
 Morphological module: 0 
 Dependency module: 0.40 
 Ngram module: 0.19 
 
Experiments aimed at evaluating the influence 
of each module (see Table 4 and Table 5) show 
that the dependency module, in the case of 
WMT12 data, and the lexical module in the case 
of WMT13 data, are the most effective ones. 
However, the influence of the ngram module and 
the morphological module varies depending on 
the source language. The fact that the depend-
ency module correlates better with human 
judgements than others might be due to its flexi-
bility to capture different syntactic constructions 
that convey the same meaning. In addition, the 
good performance of the lexical module is due to 
the use of lexical semantic relations. On the other 
hand, in general the morphological module 
shows a better performance than the ngram one, 
which might be due to the type of source lan-
guages and the possible translation mistakes. All 
source languages are highly-inflected languages 
and this might cause problems when translating 
into English, since its inflectional morphology is 
not as rich as theirs. As for the low performance 
of the ngram module in the cs-en (especially, in 
WMT12 data), it might be due to the fact that 
Czech word order is unrestricted, whereas Eng-
lish shows a stricter word order and this might 
cause translation issues. A longer ngram distance 
might have been more appropriate to control 
word order in this case. 
 
Module fr-en de-en es-en cs-en 
Lexical .16 .20 .18 .14 
Morph. .17 .19 .18 .12 
Depend. .18 .24 .20 .17 
Ngram .16 .17 .15 .08 
Table 4. Segment-level Kendall?s tau correla-
tion per module with WMT12 data. 
 
Module fr-
en 
de-
en 
es-
en 
cs-
en 
ru-
en 
Lexical .239 .254 .294 .227 .220 
Morph. .236 .243 .295 .214 .191 
Depend. .232 .247 .275 .220 .199 
Ngram .237 .245 .283 .213 .189 
Table 5. Segment-level Kendall?s tau correla-
tion per module with WMT13 data. 
 
Finally, two versions of VERTa were com-
pared: the unweighted combination (VERTa-EQ) 
and the weighted one (VERTa-W). These two 
versions were also compared to some of the best 
performing metrics in WMT12 (see Table 6 and 
Table 7) and WMT13 (see Table 8 and Table 9): 
Spede07-pP, METEOR, SEMPOR and AMBER 
(Callison-Burch et al., 2012); SIMPBLEU-
RECALL, METEOR and DEPREF-ALIGN 8 ).  
As regards WMT12 data at segment level, the 
unweighted version achieves similar results to 
those obtained by the best performing metrics. 
On the other hand, VERTa-W?s results are 
slightly worse, especially for fr-en and es-en 
pairs, which is due to the fact that the morpho-
logical module has been disregarded in this ver-
                                                 
8 http://www.statmt.org/wmt13/papers.html 
372
sion. Regarding system level correlation, neither 
VERTa-EQ nor VERTa-W achieves a high cor-
relation with human judgements. 
 
Metric fr-en de-en es-en cs-en 
Spede07-pP .26 .28 .26 .21 
METEOR .25 .27 .24 .21 
VERTa-EQ .26 .28 .26 .20 
VERTa-W .24 .28 .25 .20 
Table 6. Segment-level Kendall?s tau correla-
tion WMT12. 
 
Metric fr-en de-en es-en cs-en 
SEMPOR .80 .92 .94 .94 
AMBER .85 .79 .97 .83 
VERTa-EQ .83 .71 .89 .66 
VERTa-W .79 .73 .91 .66 
Table 7. System-level Spearman?s rho correla-
tion WMT12. 
 
As for segment level WMT13 results (see Ta-
ble 8), although both VERTa-EQ and VERTa-
W?s performance is worse than that of the two 
best-performing metrics, both versions achieve a 
third and fourth position for all language pairs, 
except for fr-en. As regards system level correla-
tions (see Table 9), both versions of VERTa 
show the best performance for de-en and ru-en 
pairs, as well as for the average score. 
5 Conclusions and Future Work 
In this paper we have presented VERTa, a lin-
guistically-based MT metric. VERTa allows for 
modular combination depending on the language 
and type of evaluation conducted. Although 
VERTa has been designed to evaluate adequacy 
and fluency separately, in order to evaluate 
whole MT quality, a couple of versions have 
been used: VERTa-EQ, an unweighted version 
that uses all modules, and VERTa-W a weighted 
version that uses the lexical, dependency and 
ngram modules. 
Experiments have shown that the modules that 
best correlate with human judgements are the 
dependency and lexical modules. In addition, 
both VERTa-EQ and VERTa-W have been com-
pared to the best performing metrics in WMT12 
and WMT13 shared tasks. VERTa-EQ has 
proved to be in line with results obtained by 
Spede07-pP and METEOR in WMT12 at seg-
ment level, while in WMT13, both VERTa and 
VERTa-W occupy the third and fourth position 
after METEOR and DEPREF-ALIGN as regards 
segment level and the first position at system 
level.  
In the future, we plan to continue working on 
the improvement of VERTa and use automatic 
tuning of module?s weight in order to achieve the 
final version that best correlates with human 
judgements on ranking. Likewise, we would like 
to explore the use of VERTa to evaluate other 
languages but English and how NLP tool errors 
may influence the performance of the metric. 
6 Acknowledgements 
We would like to acknowledge Victoria Arranz 
and Irene Castell?n for their valuable comments 
and sharing their knowledge. 
This work has been partially funded by the Span-
ish Government (projects SKATeR, TIN2012-
38584-C06-06 and Holopedia, TIN2010-21128-
C02-02).
 
Metric fr-en de-en es-en cs-en ru-en Average 
SIMPBLEU-RECALL .303 .318 .388 .260 .234 .301 
METEOR .264 .293 .324 .265 .239 .277 
VERTa-EQ .252 .280 .318 .239 .215 .261 
VERTa-W .253 .278 .314 .238 .222 .261 
DEPREF-ALIGN .257 .267 .312 .228 .200 .253 
Table 8. Segment-level Kendall?s tau correlation WMT13. 
 
Metric fr-en de-en es-en cs-en ru-en Average 
METEOR .984 .961 .979 .964 .789 .935 
DEPREF-ALIGN .995 .966 .965 .964 .768 .931 
VERTa-EQ .989 .970 .972 .936 .814 .936 
VERTa-W .989 .980 .972 .945 .868 .951 
Table 9. System-level Spearman?s rho correlation WMT13.
373
Reference 
J. S. Albrecth and R. Hwa. 2007. A Re-examination 
of Machine Learning Approaches for Sentence-
Level MT Evaluation. In The Proceedings of the 
45th Annual Meeting of the ACL, Prague, Czech 
Republic.  
J. S. Albrecth and R. Hwa. 2007. Regression for Sen-
tence-Level MT Evaluation with Pseudo Refer-
ences. In The Proceedings of the 45th Annual 
Meeting of the ACL, Prague, Czech Republic.  
J. Atserias, R. Blanco, J. M. Chenlo and C. 
Rodriquez. 2012. FBM-Yahoo at RepLab 2012, 
CLEF (Online Working Notes/Labs/Workshop) 
2012, September 20, 2012. 
C. Callison-Burch, M. Osborne and P. Koehn. 2006. 
Re-evaluating the role of BLEU in machine trans-
lation research. In Proceedings of the EACL 2006. 
C. Callison-Burch, P. Kohen, Ch. Monz, M. Post, R. 
Soricut and L. Specia. 2012. Findings of the 2012 
Workshop on Statistical Machine Translation. In 
Proceedings of the 7th Workshop on Statistical 
Machine Translation. Montr?al. Canada. 
A. X. Chang and Ch. D. Manning. 2012. SUTIME: A 
Library for Recognizing and Normalizing Time 
Expressions. 8th International Conference on Lan-
guage Resources and Evaluation (LREC 2012). 
M. Ciaramita and Y. Altun. 2006. Broad-coverage 
sense disambiguation and information extraction 
with a supersense sequence tagger. Empirical 
Methods in Natural Language Processing 
(EMNLP). 
E. Comelles, J. Atserias, V. Arranz and I. Castell?n. 
2012. VERTa: Linguistic features in MT evalua-
tion. Proceedings of the Eighth International Con-
ference on Language Resources and Evaluation 
(LREC'12), Istanbul, Turkey. 
M.C. de Marneffe, B. MacCartney and Ch. D. Man-
ning. 2006. Generating Typed Dependency Parses 
from Phrase Structure Parses in Proceedings of the 
5th Edition of the International Conference on 
Language Resources and Evaluation (LREC-
2006). Genoa, Italy. 
M. J. Denkowski and A. Lavie. 2011. METEOR 1.3: 
Automatic Metric for Reliable Optimization and 
Evaluation of Machine Translation Systems in 
Proceedings of the 6th Workshop on Statistical 
Machine Translation (ACL-2011). Edinburgh, 
Scotland, UK. 
J. Gim?nez and Ll. M?rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogeneous 
MT systems in Proceedings of the 2nd Workshop 
on Statistical Machine Translation (ACL), Prague, 
Czech Repubilc. 
 
J. Gim?nez and Ll. M?rquez. 2008. A smorgasbord of 
features for automatic MT evaluation in Proceed-
ings of the 3rd Workshop on Statistical Machine 
Translation (ACL). Columbus. OH. 
J. Gimenez. 2008. Empirical Machine Translation 
and its Evaluation. Doctoral Dissertation. UPC. 
J. Gim?nez and Ll. M?rquez. 2010. Linguistic Meas-
ures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4),77?86. 
Springer. 
B. Hachey, W. Radford and J. R. Curran. 2011. 
Graph-based named entity linking with Wikipedia 
in Proceedings of the 12th International confer-
ence on Web information system engineering, 
pages 213-226, Springer-Verlag, Berlin, Heidel-
berg. 
Y. He, J. Du, A. Way and J. van Genabith. 2010. The 
DCU Dependency-based Metric in WMT-Metrics 
MATR 2010. In Proceedings of the Joint Fifth 
Workshop on Statistical Machine Translation and 
Metrics MATR (WMT 2010),  Uppsala, Sweden. 
A. Lavie and M. J. Denkowski. 2009. The METEOR 
Metric for Automatic Evaluation of Machine 
Translation. Machine Translation, 23. 
G. Leusch and H. Ney. 2008. BLEUSP, INVWER, 
CDER: Three improved MT evaluation measures.  
In  NIST Metrics for Machine Translation 2008 
Evaluation (MericsMATR08), Waikiki, Honolulu, 
Hawaii,  October 2008. 
D. Liu and D. Hildea. 2005. Syntactic Features for 
Evaluation of Machine Translation in Proceedings 
of the ACL Workshop on Intrinsic and Extrinsic 
Evaluation Measures for Machine Translation 
and/or Summarization, Ann Arbor 
Ch.Lo and D. Wu. 2010. Semantic vs. Syntactic vs. 
Ngram Structure for Machine Translation Evalua-
tion.  In Proceedings of the 4th Workshop on Syntax 
Semantics and Structure in Statistical Translation. 
Beijing. China. 
Ch. Lo, A. K. Tumurulu and D. Wu. 2012. Fully 
Automatic Semantic MT Evaluation. Proceedings 
of the 7th Wrokshop on Statistical Machine Trans-
lation,  Montr?al, Canada, June 7-8. 
K. Owczarzak,  J. van Genabith  and A. Way. 2007. 
Dependency-Based Automatic Evaluation for Ma-
chine Translation in Proceedings of SSST, NAACL-
HLT/AMTA Workshop on Syntax and Structure I 
Statistical Translation, Rochester, New York. 
K. Owczarzak,  J. van Genabith  and A. Way. 2007. 
Labelled Dependencies in Machine Translation 
Evaluation in Proceedings of the ACL Workshop 
on Statistical Machine Translation, Prague, Czech 
Republic. 
374
K. Papineni, S. Roukos, T. Ward and W. Zhu. 2002. 
BLEU: A Method for Automatic Evaluation of 
Machine Translation. In Proceedings of the 40th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL02). Philadelphia. PA. 
F. Reeder, K. Miller, J. Doyon and J. White. 2001. 
The Naming of Things and the Confusion of 
Tongues: an MT Metric. Proceedings of the Work-
shop on MT Evaluation ?Who did what to whom?? 
at Machine Translation Summit VIII. 
L. Specia and J. Gim?nez. 2010. Combining Confi-
dence Estimation and Reference-based Metrics for 
Segment-level MT Evaluation. The Ninth Confer-
ence of the Association for Machine Translation in 
the Americas (AMTA 2010), Denver, Colorado. 
D. Wetzel and F. Bond. 2012. Enriching Parallel Cor-
pora for Statistical Machine Translation with Se-
mantic Negation Rephrasing. Proceedings of SSST-
6, Sixth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, Jeju, Republic of 
Korea. 
 
375
