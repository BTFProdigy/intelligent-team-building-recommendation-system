Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 130?133,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating language errors in texts: investigating argumentation and
decision schemas
Camille ALBERT, Laurie BUSCAIL
Marie GARNIER, Arnaud RYKNER
LLA, Universite? Toulouse le Mirail
31000 TOULOUSE France
mhl.garnier@gmail.com
Patrick SAINT-DIZIER
IRIT-CNRS, 118, route de Narbonne,
31062 TOULOUSE France
stdizier@irit.fr
Abstract
In this short paper, we present annotations
for tagging grammatical and stylistic er-
rors, together with attributes about the na-
ture of the correction which are then in-
terpreted as arguments. A decision model
is introduced in order for the author to be
able to decide on the best correction to
make. This introduces an operational se-
mantics for tags and related attributes.
1 Aims and Situation
Non-native English speaking authors producing
documents in English often encounter lexical,
grammatical and stylistic difficulties that make
their texts difficult for native speakers to under-
stand. As a result, the professionalism and the
credibility of these texts is often affected. Our
main aim is to develop procedures for the correc-
tion of those errors which cannot (and will not in
the near future) be treated by the most advanced
text processing systems such as those proposed in
the Office Suite, OpenOffice and the like. In the
type of errors taken into consideration, several lev-
els are often intertwinned: morphology, lexicon,
grammar, style, textual structure, domain usages,
context of production, target audience, etc..
While we attempt to correct errors, it turns out
that, in a large number of cases, (1) there may
be ambiguities in the analysis of the nature of er-
rors, (2) errors can receive various types and lev-
els of corrections depending on the type of docu-
ment, reader, etc., and (3) some corrections can-
not be successfully done without an interaction
with the author. To achieve these aims we need
to produce a model of the cognitive strategies de-
ployed by human experts (e.g. translators cor-
recting texts, teachers) when they detect and cor-
rect errors. Our observations show that it is not
a simple and straightforward strategy, but that er-
ror diagnosis and corrections are often based on a
complex analytical and decisional process. Since
we want our system to have a didactic capacity,
in order to help writers understand their errors,
we propose an analysis of error diagnosis based
on argumentation theory, outlining arguments for
or against a certain correction and their relative
strength paired with a decision theory.
The modelling of correction strategies is based
on the annotation of a large variety of types of doc-
uments in English produced by a large diversity of
French speakers. Annotations allow us to iden-
tify and categorize errors as well as the parame-
ters at stake (e.g. category change, length of new
corrected segment) at stake when making correc-
tions. This is carried out by bilingual correctors
in collaboration with didacticians. Those parame-
ters are a priori neutral in the annotation schemas.
We then define a preference model that assigns po-
larity (positive, negative) and a weight to each of
these parameters, together with additional param-
eters among which the target reader, the type of
document, etc. An argumentation model that con-
siders these parameters as weighted arguments, for
or against a certain correction, can thus be intro-
duced. Paired with a decision model, optimal cor-
rections can be proposed to the author, together
with explanations. This approach confers a formal
interpretation to our annotation schema.
Works on the correction of grammatical errors
made by human authors (Brockett, 2006), (Han et
al. 2005), (Lee et al 2006), (Tetreau et al2008),
(Writer?s v. 8.2) recently started to appear. The ap-
proach presented here, which is still preliminary,
is an attempt to include some didactic aspects into
the correction by explaining to the user the nature
of her/his errors, whether grammatical or stylis-
tic, while weighing the pros and cons of a cor-
rection, via argumentation and decision theories
(Boutiler et al. 1999), (Amgoud et al. 2008).
Persuasion aspects also matter within the didacti-
cal perspective (e.g. Persuation Technology sym-
130
posiums), (Prakken 2006).
In this document, we present the premisses of
an approach to correcting complex grammar and
style errors, which allow us to evaluate difficulties,
challenges, deadlocks, etc. Annotations are used
here for the development of an application.
2 The annotated corpus
The documents analyzed range from spontaneous
short productions, with little control and proof-
reading, such as personal emails or posts on fo-
rums, to highly controlled documents such as pub-
lications or professional reports. We also consider
personal web pages and wiki texts. Within each
of these types, we also observed variation in the
control of the quality of the writing. For exam-
ple, emails sent to friends are less controlled than
those produced in a professional environment, and
even in this latter framework, messages sent to the
hierarchy or to foreign colleagues receive more at-
tention than those sent to close colleagues. Be-
sides the level of control, other parameters, such
as style, are taken into consideration (e.g. oral
vs. academic). Therefore, the different corpora we
have collected form a certain continuum over sev-
eral parameters (control, orality, etc.); they allow
us to observe a large variety of language produc-
tions.
More details on the elaboration of corpora, def-
inition of attributes and their stability, and annota-
tion scenarios can be found in (Albert et al, 2009).
3 The Annotation System
Let us now briefly introduce the annotation
schema we have developed. It is an ongoing ef-
fort which is gradually evaluated by real users.
This schema is an attempt to reflect, in a fac-
tual and declarative way, the different parameters
taken into consideration by didacticians and hu-
man translators when detecting and correcting er-
rors. It contains several groups of tags which are
given below. The values for each attribute are
based on a granularity level evaluated by the di-
dacticians of our group. They are still preliminary
and require evaluation and revisions. Their struc-
ture has been designed so that they can be used in
an argumentation framework.
(a) Error delimitation and characterization:
<error-zone> tags the group of words involved in
the error. The zone is meant to be as minimal as
possible. This tag has several attributes:
comprehension: from 0 to 4 (0 being worse): indi-
cates if the segment is understandable, in spite of
the error,
agrammaticality: from 0 to 2: indicates how un-
grammtical the error is.
categ: main category of the error: lexical, syntac-
tic, stylistic, semantic, textual,
source: calque (direct copy), overcorrection, etc.
(b) Delimitation of the correction:
<correction-zone> tags the text fragment in-
volved in the correction. It is equal or larger than
the error zone.
(c) Characterization of a given correction:
Each correction is characterized by a tag
<correction> and associated attributes, positively
oriented ones are underlined:
surface: size of the text segment affected by the
correction: minimal, average, maximal,
grammar: indicates, whenever appropriate, if the
correction proposed is the standard one as sug-
gested by grammar rules; values are: by-default,
alternative, unlikely,
meaning: indicates if the meaning has been al-
tered: yes, somewhat, no,
var-size: is an integer that indicates the
increase/decrease in number of words of the cor-
rection w.r.t. the original fragment,
change: indicates if the changes in the correction
are syntactic, lexical, stylistic, semantic or textual,
comp: indicates if the proposed correction is a text
fragment which is easy to understand or not; val-
ues are: yes, average, no,
fix: indicates, when mentioned, that the error is
very specific to that string of words and that the
correction is idiosyncratic and cannot be extended
to any other such structure.
qualif: indicates the certainty level of the annota-
tor and didacticians, it qualifies the certainty of the
error detection and of the proposed correction se-
paretely,
correct: gives the correction.
An example is the N N construction (for the
sake of readability, we do not consider longer N
chains), with erroneous segments like: the mean-
ing utterance or goal failure:
It is difficult to characterize <correction-zone>
<error-zone comprehension=?2?
agrammaticality=?1?
categ=?syntax? source=?calque?>
the meaning utterance
<correction qualif=?high? grammar=?by-default?
131
surface= ?minimal? meaning= ?not altered? Var-size=?+2?
change=?synt? comp=?yes?
correct= ?the meaning of the utterance?>
</correction>
<correction qualif=?high? grammar=?unlikely?
surface= ?minimal? meaning= ?somewhat? Var-size=?0?
change=?lexical+synt? comp=?average?
correct= ?the meaningful utterance?>
</correction>
</error-zone> </correction-zone> without a context.
These tags are relatively simple and intuitive.
After some basic training, 2 independent annota-
tors covered about 25 pages (emails and reports)
so that we can measure the stability of the annota-
tions and the annotators comprehension and agree-
ment/disagreement. Results are not easy to ana-
lyze in a simple way since annotators disagree on
some error existence and nature. In about 20% of
the cases we observed such forms of disagreement.
Beside this observation, annotations turn out to be
quite convenient, although, for each error, a con-
siderable analysis effort is required for its analysis.
Annotating texts is very much time consuming, in
particular when there are several possibilities of
corrections.
4 From annotations to correction rules
Our corpus (texts, emails) has been annotated fol-
lowing the above schema. Several steps are re-
quired in order to reach the correction rule stage of
drafting rules of corrections. The approach is still
exploratory, and needs further elaborations and
evaluations. This is achieved through a gradual
and manually controlled machine learning strat-
egy. As a result, we get 23 main categories of
errors based on the elements involved in the gram-
matical and stylistic aspects, e.g.: incorrect argu-
ment structure, incorrect adverb position, incor-
rect embedded clause construction, incorrect co-
ordination, incorrect paragraph start.
To define a correction rule, the segment of
words in the error zone first gets a morphosyn-
tactic tagging, so that it can be easily identified
as an erroneous pattern in any circumstance. All
the errors that have the same erroneous pattern are
grouped to form a single correction procedure. In
that same category (named ?incorrect N N con-
structions?), another pattern is [N(+plural) N] (e.g.
horses carriage), and it results in a different cor-
rection rule.
Concerning the pattern ?Det N N?, when all the
corresponding errors are grouped, another type of
correction is found that corresponds to the inver-
sion (the predicate meaning ? the meaning of the
predicate). Informally, a correction rule is defined
as the union of all the corrections found for that
particular pattern:
(1) merge all corrections which are similar, i.e.
where the position of each word in the erroneous
segment is identical to the one it has in the cor-
rection; the values of the different attributes of the
<correction> tag are averaged,
(2) append all corrections which have a different
correction following the word to word criterion
above, and also all corrections for which the at-
tribute ?fix? is true.
(3) tag the corrections with all the appropriate
morphosyntactic details,
(4) remove the text segments or keep them as ex-
amples.
For the above example, we get the following
rule:
<correction-rule>
<error-zone comprehension=?2? agrammaticality=?1?
categ=?syntax? source=?calque?
pattern=?[Det N(1) N(2)?]>
<correction qualif=?high? grammar=?by-default?
surface= ?minimal? meaning= ?not altered? Var-size=?+2?
change=?synt? comp=?yes?
web-correct= ?[Det N(1) of the N(2)]? >
</correction>
<correction qualif=?high? grammar=?unlikely?
surface= ?minimal?
meaning= ?somewhat? Var-size=?0?
change=?lexical+synt? comp=?average?
correct=?[Det Adj(deriv(N(1)) N(2)]?
exemple=?the meaningful utterance?>
</correction>
<correction qualif=?high? grammar=?by-default?
surface= ?minimal?
meaning= ?not altered? Var-size=?+2?
change=?synt? comp=?yes?
web-correct= ?[Det N(2) of the N(1)]? >
</correction> </error-zone> </correction-rule>
We observe here several competing solutions:
when we have a segment like the meaning pred-
icate we have no information as to the noun or-
der and the type of preposition to insert (however,
?of? is the most frequent one). In this example,
the best solution is to use the web as a corpus.
The attribute web-correct is a shortcut for a func-
tion that triggers a web search: the instanciated
132
pattern is submitted to a search engine to evaluate
its occurence frequency. The most frequent one is
adopted. Other rules contain e.g. interactions with
the user to get a missing argument or to correct a
pronoun.
The form: pattern ? correct (or) web-correct
is a rewriting rule that operates the correction un-
der constraints given in the ?correct? attribute and
under didactic constraints given in the associated
attributes. Several corrections from the same rule
or from different rules may be competing. This
is a very frequent situation, e.g.: the position of
the adverb which may equally be either before the
main verb, or at the beginning, or at the end of the
sentence. A correction rule is active for a given
correction iff all the constraints it contains in the
?correct? attribute are met.
5 Using argumentation to structure the
correction space
Our goal, within an ?active didactics? perspective,
consists in identifying the best corrections and
proposing them to the writer together with expla-
nations, so that he can make the most relevant de-
cisions. Classical decision theory must be paired
with argumentation to produce explanations. In
our framework, argumentation is based on the at-
tributes associated with the tags of the correction
rules. This view confers a kind of operational se-
mantics to the tags and attributes we have defined.
Formally, a decision based on practical argu-
ments is represented by a vector (D, K, G, R) de-
fined as follows:
(1) D is a vector composed of decision variables
associated with explanations: the list of the differ-
ent decisions which can be taken into considera-
tion, including no correction. The final decision is
then made by the writer,
(2) K is a structure of stratified knowledge, pos-
sibly inconsistent. Stratifications encode priori-
ties (e.g. Bratman, 1987, Amgoud et al 2008).
K includes, for example, knowledge about read-
ers (e.g. in emails they like short messages, close
to oral communication), grammatical and stylistic
conventions or by-default behaviors, global con-
straints on texts or sentences. Each strata is asso-
ciated with a weight wK ? [0, 1]
(3) G is a set of goals, possibly inconsistent, that
correspond to positive attributes Ai to promote in
a correction. These goals depend on the type of
document being written. For example, for emails,
we may have the following goals: (meaning: no,
comp: yes, grammar: by-default). These goals
may have different weights. The form of a goal
is:
(attribute? name, value,weight)
where weight is: wAi ? [0, 1].
(4) R is a set of rejections: i.e. criteria that are not
desired, e.g., for emails: (surface: not(minimal),
change: style, semantic, textual). Format is the
same as for G. R and G have an empty intersec-
tion. These rejections may also have weights.
Some attributes may remain neutral (e.g. var-size)
for a given type of document or profile.
The global scenario for correcting an error
is as follows: while checking a text, when an
error pattern (or more if patterns are ambigu-
ous) is activated, the corrections proposed in the
<correction> tag are activated and a number of
them become active because the corresponding
?correct? attribute is active. Then, the attributes in
each of the correction, which form arguments, are
integrated in the decision process. Their weight in
G or R is integrated in a decision formula; these
weights may be reinforced or weakened via the
knowledge and preferences given in K. For each
correction decision, a meta-argument that contains
all the weighted pros and cons is produced. This
meta-argument is the motivation and explanation
for realizing the correction as suggested. It has no
polarity.
References
Amgoud, L., Dimopoulos, Y., Moraitis, P., Making de-
cisions through preference-based argumentation. In
Proceedings of the International Conference on Prin-
ciples of Knowledge Representation and Reasoning
(KR08), AAAI Press, 2008.
Bratman, M., Intentions, plans, and practical reason.
Harvard University Press, Massachusetts, 1987.
Brockett et al (2006) Correcting ESL Errors Using
Phrasal SMT Techniques, Proc of COLING/ACL
Han et al, Detecting Errors in English Article Usage
by Non-native Speakers, NLE, 2005
Lee, H., Seneff, A., Automatic Grammar Correction
for Second-language Learners, Proc of InterSpeech,
2006
Prakken, H., Formal systems for persuasion dialogue,
Knowledge Engineering Review, 21:163188, 2006.
Tetreault, M., Chodorow, C. Native Judgments of Non-
native Usage, Proc of COLING Workshop on Hu-
man Judgements in Comp. Ling, 2008.
133
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 19?22,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
An Analysis of the Calque Phenomena Based on Comparable Corpora
Marie Garnier
CAS, Universite? Toulouse le Mirail
31000 Toulouse France
mhl.garnier@gmail.com
Patrick Saint-Dizier
IRIT-CNRS, 118, route de Narbonne,
31062 Toulouse France
stdizier@irit.fr
Abstract
In this short paper we show how Compara-
ble corpora can be constructed in order to
analyze the notion of ?calque?. We then in-
vestigate the way comparable corpora con-
tribute to a better linguistic analysis of the
calque effect and how it can help improve
error correction for non-native language
productions.
1 Aims and Situation
Non-native speakers of a language (called the tar-
get language) producing documents in that lan-
guage (e.g. French authors like us writing in En-
glish) often encounter lexical, grammatical and
stylistic difficulties that make their texts difficult
to understand. As a result, the professionalism and
the credibility of these texts is often affected. Our
main aim is to develop procedures for the correc-
tion of those errors which cannot (and will not in
the near future) be treated by the most advanced
text processing systems such as those proposed in
the Office Suite, OpenOffice and the like, or ad-
vanced writing assistance tools like Antidote. In
contrast with tutoring systems, we want to leave
decisions as to the proper corrections up to the
writer, providing him/her with arguments for and
against a given correction in case several correc-
tions are possible.
To achieve these aims we need to produce a
model of the cognitive strategies deployed by hu-
man experts (e.g. translators correcting texts,
teachers) when they detect and correct errors. Our
observations show that it is not a simple and
straightforward strategy, but that error diagnosis
and corrections are often based on a complex ana-
lytical and decisional process.
Most errors result from a lack of knowledge
of the target language. A very frequent strategy
for authors is to imitate the constructions of their
native language so that the production resembles
standard terms and constructions of the target lan-
guage. This approach based on analogy is called a
calque when surface forms are taken into consider-
ation (Hammadou, 2000), (Vinay et al 1963). The
errors produced in this context may be quite com-
plex to characterize, and they are often difficult to
understand. When attempting to correct these er-
rors, we find it interesting to have access to some
of the characteristics of the native language of the
author so that a kind of ?retro-analysis? of the error
can be carried out. This would allow a much better
rate of successful corrections, even on apparently
complex errors involving long segments of words
in a sentence.
Works on the correction of grammatical errors
made by human authors (e.g. Writer?s v. 8.2) have
recently started to appear. These systems do not
propose any explicit analysis of the errors nor do
they help the user to understand them. The ap-
proach presented here, which is still preliminary,
is an attempt to include some didactic aspects into
the correction by explaining to the user the nature
of her/his errors, whether grammatical or stylis-
tic, while weighing the pros and cons of a cor-
rection, via argumentation and decision theories
(Boutiler et al. 1999), (Amgoud et al. 2008).
Persuasion aspects are also important within the
didactical perspective (e.g. Persuation Technology
symposiums), (Prakken 2006). Finally, the calque
(direct copy) effect has been studied in the didac-
tics of language learning, but has never received
much attention in the framework of error correc-
tion, where a precise analysis of its facets needs to
be conducted.
In this short document we present the premises
of an approach to correcting complex grammati-
cal and lexical errors based on an analysis of the
calque effect. Calque effects cannot easily be re-
duced to the violation of a few grammar rules of
the target language: they need an analysis of their
19
own. For that purpose, we introduce several ways
of constructing and annotating the forms calque
effects can take in source and target language in
bilingual corpora. These corpora are both rela-
tively parallel, but also relatively comparable in
the sense that they convey the same information
even though the syntax is incorrect. From these
annotations, different strategies can then be de-
ployed to develop correction rules. The languages
considered here are French, Spanish and English,
which have quite rigid and comparable structures.
We are investigating two other languages: Ben-
gali and Thai, which have a very different structure
(the former has a strong case structure and some
free phrase order, the latter has a lot of optional
forms and functions with a strong influence from
context). Besides correcting errors, the goal is to
make an analysis of the importance of the calque
effect and its facets over various language pairs.
2 Constructing comparable corpora
2.1 General parameters of the corpora
The documents used to construct the corpora range
from spontaneous short productions, with little
control and proofreading, such as emails or posts
on forums, wiki texts, personal web pages, to
highly controlled documents such as publications
or professional reports. Within each of these types,
we also observed variation in the control of the
quality of the writing. For example, emails sent to
friends are less controlled than those produced in
a professional environment, and even in this latter
framework, messages sent to hierarchy or to for-
eign colleagues receive more attention than those
sent to close colleagues. Besides the level of con-
trol, other parameters, such as target audience, are
taken into consideration. Therefore, the different
corpora we have collected form a continuum over
several parameters (control, orality, audience, lan-
guage level of the writer, etc.); they allow us to
observe a large variety of language productions.
The analysis of errors has been carried out by a
number of linguists which are either bilingual or
with a good expertise of the target language. For
each document, either a bilingual expert or two
linguists which are respectively native speakers of
the source language and target language were in-
volved in the analysis, in order to guarantee a cor-
rect apprehension of the calque effect, together
with a correct analysis of the idiosyncrasies and
the difficulties of each language in the pair.
Calque effects cover a large range of phenom-
ena. Here are three major situations, for the pur-
pose of illustration:
(1) Lexical calque: occurs when a form which
is specific to the source language is used; this is
particularly frequent for prepositions introducing
verb objects: Our team participated to this project
where in should be used instead of to.
(2) Position calque: occurs when a word or a con-
struction is misplaced. For example, in French
the adverb is often positioned after the main verb
whereas in English it must not appear between the
verb and its object: I dine regularly at the restau-
rant should be I regularly dine ....
(3) Temporal calque: occurs for temporal se-
quences concerning the grammatical tenses of
verbs in related clauses or sentences: When I will
get a job, I will buy a house the future in French is
translated into English by the present tense: When
I get a job.
2.2 Scenarios for developing corpora
In (Albert et al 2009), we present the different
categories of errors encountered in the different
types of documents we have studied, and the way
they are annotated. These categories differ sub-
stantially according to text type. The approach
presented below is based on this analysis.
In our effort to construct a corpus, we cannot
use documents with several translations, such as
notices or manuals written in several languages
since we do not know how and by whom (human
or machine) the translations have been done. In
what follows, we present the two scenarios that
seem to be the most relevant ones for our analy-
sis.
A first scenario in constructing comparable cor-
pora is simply to consider texts written by for-
eign authors, to manually detect errors (those com-
plex errors not handled by text editors) and to pro-
pose a correction. Beside the correction, a trans-
lation of the alleged source text (what the au-
thor would have produced in his own language) is
given. This study was carried out for the follow-
ing pairs: French to English, French to Spanish
and Spanish to English. So far, about 200 pages of
textual document have been analyzed and tagged.
The result is a corpus where the erroneous text seg-
ments are associated with a triple:
(1) the original erroneous segment, with the error
category,
20
(2) the correction in the target language (since
there may exist several corrections, the by-default
correction is given first, followed by other, less
prototypical corrections),
(3) the most direct translation of this segment into
the author?s native language, possibly a few alter-
natives if they are frequent. This translation is pro-
duced by a native speaker of a source language.
We have 22 texts representing papers or reports,
about 20 web pages and about 80 emails or blog
posts. These are produced by 55 different French
authors, over a few domains: computer science,
linguistics, health, leisure and tourism. Balance
over domains and authors has been enforced as
much as possible.
Here is an example based on our annotation
schemas, mentioning some relevant attributes:
.... <error-zone error-type=?future?>
When I will get </error-zone>
<correction errror-rev=?present?>
When I get </correction>
<transl calque=?future?> Quand j?aurais </transl>.....
A second scenario we are developing is to take
existing texts in the source language, with a po-
tentially high risk of calque effects, which are rep-
resentative of the types of productions advocated
above and of increasing difficulty, and to ask quite
a large and representative population of users to
translate these texts. Emails need to be translated
in a short period of time while more formal texts
do not bear any time constraints, so that authors
can revise them at will. We then have a corpus
which can be used to study how the calque effect
functions and how it can optimally be used in au-
tomatic error correction.
In this latter scenario, important features are as
follows:
Corpus: we built a set of short corpora (8 cor-
pora), so that the task for each translator is not too
long. Each corpus is about 5 pages long. It con-
tains 2 pages of emails, some really informal and
others more formal, 1 page in the style of a web
page and 2 pages of more formal document (re-
port, procedure, letter, etc.). Those texts are either
real texts or texts we have slightly adapted in order
to increase the potential number of calque effects.
Translators: we use a large population of transla-
tors (about 70), where the language competence is
the major parameter. Age and profession are also
noted, but seem to be less important. Each corpus
is translated by 8 to 10 translators with different
competences, so that we have a better understand-
ing of the forms calques may take. Comptence
is measured retroactively via the quality of their
translations. For emails, translators are instructed
to follow the provided text, possibly via some per-
sonal variation if they do not feel comfortable with
the text. The goal is to improve naturalness (prob-
ably also in a later stage to study the forms of vari-
ations).
Protocol: in terms of timing, translators are asked
to translate emails in a very short time span, which
varies depending on the ability of the translator;
conversely, they have as much time as needed for
the other documents, which can be proofread over
several days, as in real situations. No dictionary or
online grammar is allowed.
3 Analysing the facets of the calque effect
Let us now briefly present how these corpora allow
us to have a better linguistic analysis of the calque
effect and how this analysis can help us improve
error correction.
The first level of analysis is the evaluation of
the importance of a calque error per category and
subcategory. For the pair French to English, we
are studying:
? lexical calques, among which: incorrect
preposition, incorrect verb structure (transi-
tive vs. intransitive uses), argument diver-
gences (as for the verb to miss),
? lexical choice calques which account for
forms used in English, which are close to
French forms, but with different meanings I
passed an exam this morning should be: I
took an exam this morning, .
? structural calques, which account for syntac-
tic structures constructed by analogy from
French. In this category fall constructions
such as the incorrect adverb position or the
position of quite: a quite difficult exercise
which must be quite a difficult exercise
? A few basic style calques, with in particular
the problem of temporal sequence.
In terms of frequency, here are some examples
of results related to calque effects, obtained from
a partial analysis realized so far on 1200 lines of
emails produced by about 35 different authors, for
the pair French to English. Note that, in average,
21
emails have one error per line.
Lexical calques: incorrect lexical choice of
preposition: 62, determiner: 30, adverbs: 12,
modals: 26, incorrect idiomatic expression: 70.
Grammatical calques: incorrect position of ad-
verbs: 38, adjectives: 7; argument omissions: 52,
incorrect passive forms: 8.
Style: incorrect temporal sequences: 26, aspect:
20, punctuation: 76.
Alongside an evaluation of the distribution and
frequency of the different categories of calque, in
conjunction with the parameters considered in the
corpus constitution (in particular foreign language
level and type of document), we can analyze the
evolution of the calque effect: when (i.e. at what
language competence stage) and how they emerge,
expand, and disappear. Another question is the
analysis of the level of genericity of calques: some
may be individual, related to the way a certain in-
dividual has experienced learning a foreign lan-
guage, whereas some may be widespread among
a certain linguistic population. Examining differ-
ent document types is also interesting. It shows
the performance of a subject when he must write
hastily, with little control, in contrast with highly
controlled productions. This allows us to analyze
what remanence level of calques appear when the
subject does not have the time to proofread his
text, as opposed to those which are still present
when he has time to proofread it. This also betrays
a possible error hierarchy in the subject?s mind,
since the subject will be tempted to first correct
the errors he thinks are the most important.
It is also interesting to take into consideration
corpora over several language pairs, and in partic-
ular to contrast the French to English and Spanish
to English pairs. Although French and Spanish are
in the same language family, the calque effects ob-
served are quite different. This is not surprising for
a number of lexical calques, but more interesting
for grammatical calques. For example, the gram-
mar of pronouns and reflexives is quite different in
Spanish, leading to forms such as David is me, a
calque of David soy yo.
Finally, if we consider the two scenarios above,
where the first one is probably a direct production
in English, whereras the latter is a production via
an explicit translation, it becomes clear that they
require a different kind of effort. It is thus inter-
esting to compare the frequency of the different
calque categories encountered and their distribu-
tion over subjects. The translation from an explicit
source is probably more constraining in terms of
form and contents than text produced directly (or
almost) in English. This is under investigation.
4 Perspectives
The work presented here is essentially the
premises of a detailed analysis of the calque ef-
fect and, working on a language pair basis, on how
this analysis can be used to substantially improve
the performances of the correction for non trivial
lexical and grammatical errors that current text ed-
itors cannot detect and correct. We have shown
how corpora have been built. So far, they are quite
small, but sufficient to make a preliminary and in-
dicative analysis of the problems, and to suggest
directions for research. These corpora are also too
small to be used in any kind of statistical machine
learning procedure to automatically correct errors.
Our goal is thus to propose some elements of
a strategy for didacticians teaching foreign lan-
guages so that students can improve their perfor-
mance, based on the knowledge of these effects.
References
Albert, C., Garnier, M., Rykner, A., Saint-Dizier,
P., Analyzing a corpus of documents produced by
French writers in English: annotating lexical, gram-
matical and stylistic errors and their distribution,
Corpus Linguistics conference, Liverpool, 2009.
Amgoud, L., Dimopoulos, Y., Moraitis, P., Making de-
cisions through preference-based argumentation. In
Proceedings of the International Conference on Prin-
ciples of Knowledge Representation and Reasoning
(KR08), AAAI Press, 2008.
Boutilier, C., Dean, T., Hanks, S., Decision-theoretic
planning: Structural assumptions and computational
leverage. Journal of Artificial Intelligence Research,
11:194, 1999.
Chuquet, H., Paillard, M., Approche Linguistique des
Problmes de Traduction, Paris, Ophrys, 1989.
Hammadou, J., The Impact of Analogy and Con-
tent Knowledge on Reading Comprehension: What
Helps, What Hurts, ERIC, 2000.
Prakken, H., Formal systems for persuasion dialogue,
Knowledge Engineering Review, 21:163188, 2006.
Vinay, Jean-Paul and Darbelnay, Jean: Stylistique
Compare?e du Francais et de l?Anglais, Paris, Didier,
1963.
22
