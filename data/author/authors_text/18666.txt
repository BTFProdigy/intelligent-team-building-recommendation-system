Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 29?34,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Choosing an Evaluation Metric for Parser Design
Woodley Packard
sweaglesw@sweaglesw.org
Abstract
This paper seeks to quantitatively evaluate the
degree to which a number of popular met-
rics provide overlapping information to parser
designers. Two routine tasks are considered:
optimizing a machine learning regularization
parameter and selecting an optimal machine
learning feature set. The main result is that the
choice of evaluation metric used to optimize
these problems (with one exception among
popular metrics) has little effect on the solu-
tion to the optimization.
1 Introduction
The question of how best to evaluate the perfor-
mance of a parser has received considerable atten-
tion. Numerous metrics have been proposed, and
their relative merits have been debated. In this pa-
per, we seek to quantitatively evaluate the degree to
which a number of popular metrics provide overlap-
ping information for two concrete subtasks of the
parser design problem.
The motivation for this study was to confirm our
suspicion that parsing models that performed well
under one metric were likely to perform well un-
der other metrics, thereby validating the widespread
practice of using just a single metric when conduct-
ing research on improving parser performance. Our
results are cautiously optimistic on this front.1
We use the problem of selecting the best per-
former from a large space of varied but related parse
1Note that we are not suggesting that these metrics provide
redundant information for other uses, e.g. predicting utility for
any particular downstream task.
disambiguation models (?parsers? henceforth) as the
setting for our study. The parsers are all conditional
log-linear disambiguators with quadratic regulariza-
tion, coupled to the English Resource Grammar
(ERG) (Flickinger, 2000), a broad-coverage HPSG-
based hand-built grammar of English. Analyses
from the ERG consist of a syntax tree together with
an underspecified logical formula called an MRS
(Copestake et al, 2005).
The parsers differ from each other along two di-
mensions: the feature templates employed, and the
degree of regularization used. There are 57 differ-
ent sets of traditional and novel feature templates
collecting a variety of syntactic and semantic data
about candidate ERG analyses. For each set of fea-
ture templates, parsers were trained with 41 different
values for the quadratic regularization parameter, for
a total of 2337 different parsers.
TheWeScience Treebank of about 9100 sentences
(Ytrest?l et al, 2009) was used both for training and
testing the parsers, with 10-fold cross validation.
We break down the problem of selecting the best
parser into two tasks. The first task is to identify
the optimal value for the regularization parameter
for each set of feature templates. The second task
is to compare the different sets of feature templates
to each other, considering only the optimal value of
the regularization parameter for each, and select the
overall best. We attack each task with each of 14
metrics, and discuss the results.
2 Prior Work
Comparisons of parser metrics have been under-
taken in the past. Carroll et al(1998) describe a
29
broad range of parser evaluation metrics, and com-
ment on their advantages and disadvantages, but do
not offer a quantitative comparison. A number of pa-
pers such as Clark and Curran (2007) have explored
the difficulty of parser comparison across different
underlying formalisms.
Crouch et al(2002) compare two variant
dependency-based metrics in some detail on a single
LFG-based parsing model, concluding that despite
some differences in the metrics? strategies, they of-
fer similar views on the performance of their parser.
The literature specifically seeking to quantita-
tively compare a broad range of metrics across a
large array of parsers is small. Emms (2008) de-
scribes the tree-distance metric and compares the
rankings induced by several variants of that met-
ric and PARSEVAL on a collection of six statisti-
cal parsers, finding broad compatibility, but observ-
ing frequent disagreement about the relative ranks
of two parsers whose scores were only marginally
different.
3 Metrics
In our setup, the overall score a metric assigns to
a parser is the average of the scores awarded for
the parser?s analyses of each sentence in the tree-
bank (termed macro-averaging, in contrast to micro-
averaging which is also common). For sentences
where the parser selects several candidate analyses
as tied best analyses, the actual metric score used is
the average value of the metric applied to the differ-
ent tied best analyses. Fourteen metrics are consid-
ered:
? Exact Tree Match (ETM) (Toutanova et al,
2005) - 100% if the returned tree is identical
to the gold tree, and 0% otherwise.
? Exact MRS Match (EMM) - 100% if the re-
turned MRS is equivalent to the gold MRS, and
0% otherwise.
? Average Crossing Brackets (AXB) - the num-
ber of brackets (constituents) in the returned
tree that overlap incompatibly with some
bracket in the gold tree. Sign-inverted for com-
parability to the other metrics.
? Zero Crossing Brackets (ZXB) - 100% if the
AXB score is 0, and 0% otherwise.
? Labeled PARSEVAL (LP) (Abney et al, 1991)
- the harmonic mean (F1) of the precision and
recall for comparing the set of labeled brack-
ets in the returned tree with the set of labeled
brackets in the gold tree. Labels are rule names.
? Unlabeled PARSEVAL (UP) - identical to LP,
except ignoring the labels on the brackets.
? Labeled Syntactic Dependencies (LSD) (Buch-
holz and Marsi, 2006) - the F1 for comparing
the sets of directed bilexical syntactic depen-
dencies extracted from the returned and gold
trees, labeled by the rule name that joins the
dependent to the dependee.
? Unlabeled Syntactic Dependencies (USD) -
identical to LSD, except ignoring the labels.
? Labeled Elementary Dependencies (LED) - the
F1 for comparing the sets of elementary depen-
dency triples (Oepen and L?nning, 2006) ex-
tracted from the returned and gold MRS. These
annotations are similar in spirit to those used in
the PARC 700 Dependency Bank (King et al,
2003) and other semantic dependency evalua-
tion schemes.
? Unlabeled Elementary Dependencies (UED) -
identical to LED, except ignoring all labeling
information other than the input positions in-
volved.
? Leaf Ancestor (LA) (Sampson and Babarczy,
2003) - the average of the edit distances be-
tween the paths through the returned and gold
trees from root to each leaf.
? Lexeme Name Match (LNM) - the percentage
of input words parsed with the gold lexeme2.
? Part-of-Speech Match (POS) - the percentage
of input words parsed with the gold part of
speech.
? Node Count Match (NCM) - 100% if the gold
and returned trees have exactly the same num-
ber of nodes, and 0% otherwise.
2In the ERG, lexemes are detailed descriptions of the syn-
tactic and semantic properties of individual words. There can
be multiple candidate lexemes for each word with the same part
of speech.
30
 24
 26
 28
 30
 32
 34
 36
 38
 40
 42
 0.001  0.01  0.1  1  10  100  1000
Ex
act
 M
atc
h A
ccu
rac
y (
%)
Regularization Variance Parameter
Regularized Performance of pcfg baseline
pcfg baseline
Figure 1: ETM for ?pcfg baseline?
Note that the last three metrics are not commonly
used in parser evaluation, and we have no reason
to expect them to be particularly informative. They
were included for variety ? in a sense serving as con-
trols, to see how informative a very unsophisticated
metric can be.
4 Optimizing the Regularization
Parameter
The first half of our problem is: given a set of fea-
ture templates T , determine the optimal value for the
regularization parameter ?. We interpret the word
?optimal? relative to each of our 14 metrics. This is
quite straightforward: to optimize relative to metric
?, we simply evaluate ?(M(T, ?)) for each value of
?, where M(T, ?) is a parser trained using feature
templates T and regularization parameter ?, and de-
clare the value of ? yielding the greatest value of
? the winner. Figure 1 shows values of the ETM
as a function of the regularization parameter ? for
T = ?pcfg baseline?3; as can easily be seen, the op-
timal value is approximately ??? = 2.
We are interested in how ??? varies with different
choices of ?. Figure 2 shows all 14 metrics as func-
tions of ? for the same T = ?pcfg baseline.? The
actual scores from the metrics vary broadly, so the
vertical axes of the superimposed plots have been
rescaled to allow for easier comparison.
A priori we might expect the optimal ??? to be
3Note that we are not actually considering a PCFG here; in-
stead we are looking at a conditional log-linear model whose
features are shaped like PCFG configurations.
-1.5
-1
-0.5
 0
 0.5
 1
 0.001  0.01  0.1  1  10  100  1000
Z-S
cor
es
Regularization
Z-Score Comparison of Metrics
Figure 2: Z-scores for all metrics for ?pcfg baseline?
quite different for different ?, but this does not turn
out to be the case. The curves for all of the met-
rics peak in roughly the same place, with one no-
ticeable outlier (AXB). The actual peak4 regulariza-
tion parameters for the 14 metrics were all in the
range [1.8, 3.9] except for the outlier AXB, which
was 14.8.
Relative to the range under consideration, the op-
timal regularization parameters can be seen by in-
spection to depend very little on the metric. Near the
optima, the graphs are all quite flat, and we calcu-
lated that by choosing the optimal regularization pa-
rameter according to any of the metrics (with the ex-
ception of the outlier AXB), the maximum increase
in error rate visible through the other metrics was
1.6%. If we ignore LNM, POS and NCM (the non-
standard metrics we included for variety) in addition
to AXB, the maximum increase in error rate result-
ing from using an alternate metric to optimize the
regularization parameter drops to 0.41%.
?pcfg baseline? is just one of 57 sets of feature
templates. However, the situation is essentially the
same with each of the remaining 56. The average
maximum error rate increase observed across all of
the sets of feature templates when optimizing on any
metric (including AXB, LNM, POS and NCM) was
2.54%; on the worst single set of feature templates it
was 6.7%. Excluding AXB, the average maximum
error rate increase was 1.7%. Additionally exclud-
4Due to noisiness near the tops of the graphs, the reported
optimum regularization parameters are actually the averages of
the best 3 values. We attribute the noise to the limited size of
our corpus.
31
ing LNM, POS and NCM it was 0.81%.
Given the size of the evaluation corpus we are
using, the significance of an error rate increase of
0.81% is very marginal. We conclude that, at least
in circumstances similar to ours, the choice of met-
ric used to optimize regularization parameters is not
important, provided we avoid AXB and the variety
metrics LNM, POS and NCM.
5 Choosing a Set of Feature Templates
The second half of our problem is: given a col-
lection T of different sets of feature templates, se-
lect the optimal performer. Again, we interpret
the word ?optimal? relative to each of our 14 met-
rics, and the selection is straightforward: given
a metric ?, we first form a set of parsers P =
{M(T, argmax? ?(M(T, ?))) : T ? T } and then
select argmaxp?P ?(p). That is, we train parsers
using the ?-optimal regularization parameter for
each T ? T , and then select the ?-optimal parser
from that set.
In our experiments, all 14 of the metrics ranked
the same set of feature templates as best.
It is also interesting to inspect the order that each
metric imposes on P . There was some disagree-
ment between the metrics about this order. We com-
puted pairwise Spearman rank correlations coeffi-
cients5 for the different metrics. As with the task
of choosing a regularization parameter, the metrics
AXB, LNM, POS and NCM were outliers. The av-
erage pairwise Spearman rank correlation exclud-
ing these metrics was 0.859 and the minimum was
0.761.
An alternate method of quantifying the degree of
agreement is described below.
5.1 Epsila
Consider two metrics ? : P 7? R and ? : P 7? R.
Assume for simplicity that for both ? and ?, larger
values are better and 100 is perfect. If x, y ? P
then the error rate reduction from y to x under ?
is ??(x, y) = ?(x)??(y)100??(y) . Let ?,? be the smallest
number such that ?x, y ? P : ??(x, y) > ?,? ?
5The Spearman rank correlation coefficient of two metrics
is defined as the Pearson correlation coefficient of the ranks the
metrics assign to the elements of P . It takes values between?1
and 1, with larger values indicating higher ranking agreement.
??(x, y) > 0. Informally, this says for all pairs of
parsers x and y, if x is at least ?,? better than y when
evaluated under ?, then we are guaranteed that x is
at least a tiny bit better than y when evaluated under
?. For an unrestricted domain of parsers, we are not
guaranteed that such epsila exist or are small enough
to be interesting. However, since our P is finite, we
can find an  that will provide the required property
at least within P .
?,? serves as a measure of how similar ? and ?
are: if ?,? is small, then small improvements seen
under ? will be visible as improvements under ?,
whereas if ?,? is large, then small improvements
seen under ? may in fact be regressions when evalu-
ating with ?.
We computed pairwise epsila for our 14 metrics.
A large portion of pairwise epsila were around 5%,
with some being considerably smaller or larger.
5.2 Clustering
In order to make sense of the idea that these ep-
sila provide a similarity measure, we applied Quality
Threshold clustering (Heyer et al, 1999) to discover
maximal clusters of metrics within which all pair-
wise epsila are smaller than a given threshold. Small
thresholds produce many small clusters, while larger
thresholds produce fewer, larger clusters.
At a 1% threshold, almost all of the metrics form
singleton clusters; that is, a 1% error rate reduction
on any given metric is generally not enough to guar-
antee that any other metrics will see any error reduc-
tion at all. The exceptions were that {ETM, EMM}
formed a cluster, and {UED, LED} formed a cluster.
Increasing the threshold to 3%, a new cluster
{USD, LSD} forms (indicating that a 3% error rate
reduction in USD always is visible as some level
of error rate reduction in LSD, and vice versa), and
ZXB joins the {ETM, EMM} cluster.
By the time we reach a 5% threshold, the major-
ity (7 out of 11) of the ?standard? parser evaluation
metrics have merged into a single cluster, consisting
of {ETM, EMM, ZXB, LA, LSD, UED, LED}. The
PARSEVALmetrics form a cluster of their own {UP,
LP}.
Increasing the threshold even more to 10% causes
10 out of 11 ?standard? evaluation metrics to cluster
together; the only holdout is AXB (average number
of crossing brackets), which does not join the cluster
32
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
 2  4  6  8  10  12  14
Z-S
cor
es
Metric
Z-Score Comparison of Feature Sets
Figure 3: Z-scores for all feature sets on the Y axis (one
line per feature set); different metrics on the X axis. The
?control? metrics and the outlier AXB are on the far right
end.
even at a 20% threshold.
5.3 Visualization
To qualitatively illustrate the degree of variation in
scores attributable to differences in metric as op-
posed to differences in feature sets, and the extent of
the metrics? agreements in ranking the feature sets,
we plotted linearly rescaled scores from the metrics
(at their optimum regularization parameter value) in
two ways.
In Figure 3, the scores of each feature set are plot-
ted as a function of which metric is being used. To
the extent that the lines are horizontal, the metrics
provide identical information. To the extent that the
lines do not cross, the metrics agree about the rela-
tive ordering of the feature sets. Note that the three
control metrics and the outlier metric AXB are plot-
ted on the far right of the figure, and show signifi-
cantly more line crossings.
In Figure 4, the score from each metric is plot-
ted as a function of which feature set is being evalu-
ated, sorted in increasing order of the LP metric. As
can be seen, the increasing trend of the LP metric
is clearly mirrored in all the other metrics graphed,
although there is a degree of variability.
6 Conclusions
From both subtasks, we saw that the Average Cross-
ing Brackets metric (AXB) is a serious outlier. We
cannot say whether it provides complementary in-
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
 0  10  20  30  40  50  60
Z-S
cor
es
Feature Set
Z-Score Comparison of Metrics
Figure 4: Z-scores for all metrics except AXB, LNM,
POS and NCM on the Y axis (one line per metric); dif-
ferent feature sets on the X axis.
formation or actually misleading information; in-
deed, that might depend on the nature of the down-
stream application.
We can say with confidence that for the subtask of
optimizing a regularization parameter, there is very
little difference between the popular metrics {ETM,
EMM, ZXB, LA, LP, UP, LSD, USD, LED, UED}.
For the subtask of choosing the optimal set of fea-
ture templates, there was even greater agreement: all
14 metrics arrived at the same result. Although they
did not impose the exact same rankings, the rankings
were similar. It is interesting (and entertaining) that
even the three ?control? metrics (LNM, POS and
NCM) selected the same optimal feature set. It is
particularly surprising that even the absurdly simple
NCM metric, which does nothing but check whether
two trees have the same number of nodes, irrespec-
tive of their structure or labels, when averaged over
thousands of items, can identify the best feature set.
Our findings agree with (Crouch et al, 2002)?s
suggestion that different metrics can offer similar
views on error rate reduction.
Clustering based on epsila at the 5% and 10%
thresholds showed interesting insights as well. We
demonstrated that a 5% error rate reduction as seen
on any of {ETM, EMM, ZXB, LA, LSD, UED,
LED} is also visible from the others (although the
popular PARSEVAL metrics were outliers at this
threshold). This has the encouraging implication
that a decision made on the basis of strong evidence
from just one metric is not likely to be contradicted
33
by evaluations by other metrics. However, we must
point out that the precise values of these thresholds
are dependent on our setup. They would likely be
larger if a significantly larger number of parsers or a
significantly more varied group of parsers were con-
sidered, and conversely would perhaps be smaller if
a larger evaluation corpus were used (reducing the
noise).
Our data only directly apply to the tasks of se-
lecting the value of the regularization parameter and
selecting feature templates for a conditional log-
likelihood model for parsing with the ERG. How-
ever, we expect the results to generalize at least to
similar tasks with other precision grammars, and
probably treebank-derived parsers as well. Explo-
ration of how well these results hold for other tasks
and for other types of parsers is an excellent subject
for future research.
References
S. Abney, D. Flickinger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, et al 1991. Procedure for quan-
titatively comparing the syntactic coverage of English
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306?311. Association
for Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
J. Carroll, T. Briscoe, and A. Sanfilippo. 1998. Parser
evaluation: a survey and a new proposal. In Proceed-
ings of the 1st International Conference on Language
Resources and Evaluation, pages 447?454.
S. Clark and J. Curran. 2007. Formalism-independent
parser evaluation with CCG and DepBank. In An-
nual Meeting-Association for Computational Linguis-
tics, volume 45, page 248.
A. Copestake, D. Flickinger, C. Pollard, and I.A. Sag.
2005. Minimal recursion semantics: An introduction.
Research on Language & Computation, 3(4):281?332.
R. Crouch, R.M. Kaplan, T.H. King, and S. Riezler.
2002. A comparison of evaluation metrics for a broad-
coverage stochastic parser. In Beyond PARSEVAL
workshop at 3rd Int. Conference on Language Re-
sources an Evaluation (LREC 2002).
Martin Emms. 2008. Tree distance and some other
variants of evalb. In Bente Maegaard Joseph Mari-
ani Jan Odjik Stelios Piperidis Daniel Tapias Nicoletta
Calzolari (Conference Chair), Khalid Choukri, edi-
tor, Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6(01):15?28.
L.J. Heyer, S. Kruglyak, and S. Yooseph. 1999. Explor-
ing expression data: identification and analysis of co-
expressed genes. Genome research, 9(11):1106.
T.H. King, R. Crouch, S. Riezler, M. Dalrymple, and
R. Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th Interna-
tional Workshop on Linguistically Interpreted Corpora
(LINC-03), pages 1?8.
S. Oepen and J.T. L?nning. 2006. Discriminant-based
MRS banking. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006).
G. Sampson and A. Babarczy. 2003. A test of the leaf-
ancestor metric for parse accuracy. Natural Language
Engineering, 9(04):365?380.
K. Toutanova, C.D. Manning, D. Flickinger, and
S. Oepen. 2005. Stochastic HPSG parse disambigua-
tion using the Redwoods corpus. Research on Lan-
guage & Computation, 3(1):83?105.
Gisle Ytrest?l, Dan Flickinger, and Stephan Oepen.
2009. Extracting and Annotating Wikipedia Sub-
Domains. Towards a New eScience Community Re-
source. In Proceedings of the Seventh Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries, Groningen, The Netherlands.
34
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 69?78,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Simple Negation Scope Resolution through Deep Parsing:
A Semantic Solution to a Semantic Problem
Woodley Packard
?
, Emily M. Bender
?
, Jonathon Read
?
, Stephan Oepen
??
, and Rebecca Dridan
?
?
University of Washington, Department of Linguistics
?
Teesside University, School of Computing
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
ebender@uw.edu, sweaglesw@sweaglesw.org, j.read@tees.ac.uk, {oe |rdridan}@ifi.uio.no
Abstract
In this work, we revisit Shared Task 1
from the 2012
*
SEM Conference: the au-
tomated analysis of negation. Unlike the
vast majority of participating systems in
2012, our approach works over explicit
and formal representations of proposi-
tional semantics, i.e. derives the notion of
negation scope assumed in this task from
the structure of logical-form meaning rep-
resentations. We relate the task-specific
interpretation of (negation) scope to the
concept of (quantifier and operator) scope
in mainstream underspecified semantics.
With reference to an explicit encoding
of semantic predicate-argument structure,
we can operationalize the annotation deci-
sions made for the 2012
*
SEM task, and
demonstrate how a comparatively simple
system for negation scope resolution can
be built from an off-the-shelf deep parsing
system. In a system combination setting,
our approach improves over the best pub-
lished results on this task to date.
1 Introduction
Recently, there has been increased community in-
terest in the theoretical and practical analysis of
what Morante and Sporleder (2012) call modality
and negation, i.e. linguistic expressions that mod-
ulate the certainty or factuality of propositions.
Automated analysis of such aspects of meaning
is important for natural language processing tasks
which need to consider the truth value of state-
ments, such as for example text mining (Vincze
et al, 2008) or sentiment analysis (Lapponi et al,
2012). Owing to its immediate utility in the cura-
tion of scholarly results, the analysis of negation
and so-called hedges in bio-medical research liter-
ature has been the focus of several workshops, as
well as the Shared Task at the 2011 Conference on
Computational Language Learning (CoNLL).
Task 1 at the First Joint Conference on Lex-
ical and Computational Semantics (
*
SEM 2012;
Morante and Blanco, 2012) provided a fresh, prin-
cipled annotation of negation and called for sys-
tems to analyze negation?detecting cues (affixes,
words, or phrases that express negation), resolv-
ing their scopes (which parts of a sentence are ac-
tually negated), and identifying the negated event
or property. The task organizers designed and
documented an annotation scheme (Morante and
Daelemans, 2012) and applied it to a little more
than 100,000 tokens of running text by the nov-
elist Sir Arthur Conan Doyle. While the task and
annotations were framed from a semantic perspec-
tive, only one participating system actually em-
ployed explicit compositional semantics (Basile et
al., 2012), with results ranking in the middle of
the 12 participating systems. Conversely, the best-
performing systems approached the task through
machine learning or heuristic processing over syn-
tactic and linguistically relatively coarse-grained
representations; see ? 2 below.
Example (1), where ?? marks the cue and {}
the in-scope elements, illustrates the annotations,
including how negation inside a noun phrase can
scope over discontinuous parts of the sentence.
1
(1) {The German} was sent for but professed to
{know} ?nothing? {of the matter}.
In this work, we return to the 2012
*
SEM
task from a deliberately semantics-centered point
of view, focusing on the hardest of the three
sub-problems: scope resolution.
2
Where Morante
and Daelemans (2012) characterize negation as an
?extra-propositional aspect of meaning? (p. 1563),
1
Our running example is a truncated variant of an item
from the Shared Task training data. The remainder of the
original sentence does not form part of the scope of this cue.
2
Resolving negation scope is a more difficult sub-problem
at least in part because (unlike cue and event identification) it
is concerned with much larger, non-local and often discontin-
uous parts of each utterance. This intuition is confirmed by
Read et al (2012), who report results for each sub-problem
using gold-standard inputs; in this setup, scope resolution
showed by far the lowest performance levels.
69
we in fact see it as a core piece of composi-
tionally constructed logical-form representations.
Though the task-specific concept of scope of
negation is not the same as the notion of quan-
tifier and operator scope in mainstream under-
specified semantics, we nonetheless find that re-
viewing the 2012
*
SEM Shared Task annotations
with reference to an explicit encoding of seman-
tic predicate-argument structure suggests a sim-
ple and straightforward operationalization of their
concept of negation scope. Our system imple-
ments these findings through a notion of functor-
argument ?crawling?, using as our starting point
the underspecified logical-form meaning represen-
tations provided by a general-purpose deep parser.
Our contributions are three-fold: Theoretically,
we correlate the structures at play in the Morante
and Daelemans (2012) view on negation with
formal semantic analyses; methodologically, we
demonstrate how to approach the task in terms of
underspecified, logical-form semantics; and prac-
tically, our combined system retroactively ?wins?
the 2012
*
SEM Shared Task. In the following
sections, we review related work (? 2), detail our
own setup (? 3), and present and discuss our ex-
perimental results (? 4 and ? 5, respectively).
2 Related Work
Read et al (2012) describe the best-performing
submission to Task 1 of the 2012
*
SEM Confer-
ence. They investigated two approaches for scope
resolution, both of which were based on syntac-
tic constituents. Firstly, they created a set of 11
heuristics that describe the path from the preter-
minal of a cue to the constituent whose projec-
tion is predicted to match the scope. Secondly
they trained an SVM ranker over candidate con-
stituents, generated by following the path from a
cue to the root of the tree and describing each
candidate in terms of syntactic properties along
the path and various surface features. Both ap-
proaches attempted to handle discontinuous in-
stances by applying two heuristics to the predicted
scope: (a) removing preceding conjuncts from the
scope when the cue is in a conjoined phrase and
(b) removing sentential adverbs from the scope.
The ranking approach showed a modest advan-
tage over the heuristics (with F
1
equal to 77.9
and 76.7, respectively, when resolving the scope
of gold-standard cues in evaluation data). Read et
al. (2012) noted however that the annotated scopes
did not align with the Shared Task?provided con-
stituents for 14% of the instances in the training
data, giving an F
1
upper-bound of around 86.0 for
systems that depend on those constituents.
Basile et al (2012) present the only submission
to Task 1 of the 2012
*
SEM Conference which
employed compositional semantics. Their scope
resolution pipeline consisted primarily of the C&C
parser and Boxer (Curran et al, 2007), which pro-
duce Discourse Representation Structures (DRSs).
The DRSs represent negation explicitly, including
representing other predications as being within the
scope of negation. Basile et al (2012) describe
some amount of tailoring of the Boxer lexicon to
include more of the Shared Task scope cues among
those that produce the negation operator in the
DRSs, but otherwise the system appears to directly
take the notion of scope of negation from the DRS
and project it out to the string, with one caveat: As
with the logical-forms representations we use, the
DRS logical forms do not include function words
as predicates in the semantics. Since the Shared
Task gold standard annotations included such ar-
guably semantically vacuous (see Bender, 2013,
p. 107) words in the scope, further heuristics are
needed to repair the string-based annotations com-
ing from the DRS-based system. Basile et al re-
sort to counting any words between in-scope to-
kens which are not themselves cues as in-scope.
This simple heuristic raises their F
1
for full scopes
from 20.1 to 53.3 on system-predicted cues.
3 System Description
The new system described here is what we call
the MRS Crawler. This system operates over
the normalized semantic representations provided
by the LinGO English Resource Grammar (ERG;
Flickinger, 2000).
3
The ERG maps surface strings
to meaning representations in the format of Mini-
mal Recursion Semantics (MRS; Copestake et al,
2005). MRS makes explicit predicate-argument
relations, as well as partial information about
scope (see below). We used the grammar together
with one of its pre-packaged conditional Maxi-
mum Entropy models for parse ranking, trained
on a combination of encyclopedia articles and
tourism brochures. Thus, the deep parsing front-
end system to our MRS Crawler has not been
3
In our experiments, we use the 1212 release of the ERG,
in combination with the ACE parser (http://sweaglesw
.org/linguistics/ace/). The ERG and ACE are DELPH-
IN resources; see http://www.delph-in.net.
70
? h
1
,
h
4
:_the_q?0:3?(ARG0 x
6
, RSTR h
7
, BODY h
5
), h
8
:_german_n_1?4:10?(ARG0 x
6
),
h
9
:_send_v_for?15:19?(ARG0 e
10
, ARG1 , ARG2 x
6
), h
2
:_but_c?24:27?(ARG0 e
3
, L-HNDL h
9
, R-HNDL h
14
),
h
14
:_profess_v_to?28:37?(ARG0 e
13
, ARG1 x
6
, ARG2 h
15
), h
16
:_know_v_1?41:45?(ARG0 e
17
, ARG1 x
6
, ARG2 x
18
),
h
20
:_no_q?46:53?(ARG0 x
18
, RSTR h
21
, BODY h
22
), h
19
:thing?46:53?(ARG0 x
18
),
h
19
:_of_p?54:56?(ARG0 e
23
, ARG1 x
18
, ARG2 x
24
),
h
25
:_the_q?57:60?(ARG0 x
24
, RSTR h
27
, BODY h
26
), h
28
:_matter_n_of?61:68?(ARG0 x
24
, ARG1 )
{ h
27
=
q
h
28
, h
21
=
q
h
19
, h
15
=
q
h
16
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: MRS analysis of our running example (1).
adapted to the task or its text type; it is applied
in an ?off the shelf? setting. We combine our
system with the outputs from the best-performing
2012 submission, the system of Read et al (2012),
firstly by relying on the latter for system negation
cue detection,
4
and secondly as a fall-back in sys-
tem combination as described in ? 3.4 below.
Scopal information in MRS analyses delivered
by the ERG fixes the scope of operators?such as
negation, modals, scopal adverbs (including sub-
ordinating conjunctions like while), and clause-
embedding verbs (e.g. believe)?based on their
position in the constituent structure, while leaving
the scope of quantifiers (e.g. a or every, but also
other determiners) free. From these underspec-
ified representations of possible scopal configu-
rations, a scope resolution component can spell
out the full range of fully-connected logical forms
(Koller and Thater, 2005), but it turns out that such
enumeration is not relevant here: the notion of
scope encoded in the Shared Task annotations is
not concerned with the relative scope of quantifiers
and negation, such as the two possible readings of
(2) represented informally below:
5
(2) Everyone didn?t leave.
a. ?(x)?leave(x) ? Everyone stayed.
b. ??(x)leave(x) ? At least some stayed.
However, as shown below, the information about
fixed scopal elements in an underspecified MRS is
sufficient to model the Shared Task annotations.
3.1 MRS Crawling
Fig. 1 shows the ERG semantic analysis for our
running example. The heart of the MRS is a mul-
tiset of elementary predications (EPs). Each ele-
4
Read et al (2012) predicted cues using a closed vocabu-
lary assumption with a supervised classifier to disambiguate
instances of cues.
5
In other words, a possible semantic interpretation of the
(string-based) Shared Task annotation guidelines and data is
in terms of a quantifier-free approach to meaning representa-
tion, or in terms of one where quantifier scope need not be
made explicit (as once suggested by, among others, Alshawi,
1992). From this interpretation, it follows that the notion of
scope assumed in the Shared Task does not encompass inter-
actions of negation operators and quantifiers.
mentary prediction includes a predicate symbol,
a label (or ?handle?, prefixed to predicates with
a colon in Fig. 1), and one or more argument
positions, whose values are semantic variables.
Eventualities (e
i
) in MRS denote states or activ-
ities, while instance variables (x
j
) typically corre-
spond to (referential or abstract) entities. All EPs
have the argument position ARG0, called the dis-
tinguished variable (Oepen and L?nning, 2006),
and no variable is the ARG0 of more than one non-
quantifier EP.
The arguments of one EP are linked to the argu-
ments of others either directly (sharing the same
variable as their value), or indirectly (through so-
called ?handle constraints?, where =
q
in Fig. 1 de-
notes equality modulo quantifier insertion). Thus
a well-formed MRS forms a connected graph. In
addition, the grammar links the EPs to the ele-
ments of the surface string that give rise to them,
via character offsets recorded in each EP (shown
in angle brackets in Fig. 1). For the purposes of
the present task, we take a negation cue as our en-
try point into the MRS graph (as our initial active
EP), and then move through the graph according
to the following simple operations to add EPs to
the active set:
Argument Crawling Add to the scope all EPs
whose distinguished variable or label is an argu-
ment of the active EP; for arguments of type h
k
,
treat any =
q
constraints as label equality.
Label Crawling Add all EPs whose label is iden-
tical to that of the active EP.
Functor Crawling Add all EPs that take the dis-
tinguished variable or label of the active EP as an
argument (directly or via =
q
constraints).
Our MRS crawling algorithm is sketched in
Fig. 2. To illustrate how the rules work, we will
trace their operation in the analysis of example (1),
i.e. traverse the EP graph in Fig. 1.
The negation cue is nothing, from character po-
sition 46 to 53. This leads us to _no_q as our en-
try point into the graph. Our algorithm states that
for this type of cue (a quantifier) the first step is
71
1: Activate the cue EP
2: if the cue EP is a quantifier then
3: Activate EPs reached by functor crawling from the distinguished variable (ARG0) of the cue EP
4: end if
5: repeat
6: for each active EP X do
7: Activate EPs reached by argument crawling or label crawling unless they are co-modifiers of the negation cue.
a
8: Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctions
reached by ARG1: whether, when, because, to, with, although, unless, until, or as.
9: end for
10: until a fixpoint is reached (no additional EPs were activated)
11: Deactivate zero-pronoun EPs (from imperative constructions)
12: Apply semantically empty word handling rules (iterate until a fixpoint is reached)
13: Apply punctuation heuristics
Figure 2: Algorithm for scope detection by MRS crawling
a
Formally: If an EP shares its label with the negation cue, or is a quantifier whose restriction (RSTR) is =
q
equated with the
label of the negation cue, it cannot be in-scope unless its ARG0 is an argument of the negation cue, or the ARG0 of the negation
cue is one of its own arguments. See ? 3.3 for elaboration.
functor crawling (see ? 3.3 below), which brings
_know_v_1 into the scope. We proceed with ar-
gument crawling and label crawling, which pick
up _the_q?0:3? and _german_n_1 as the ARG1.
Further, as the ARG2 of _know_v_1, we reach
thing and through recursive invocation we acti-
vate _of_p and, in yet another level of recursion,
_the_q?57:60? and _matter_n_of. At this point,
crawling has no more links to follow. Thus, the
MRS crawling operations ?paint? a subset of the
MRS graph as in-scope for a given negation cue.
3.2 Semantically Empty Word Handling
Our crawling rules operate on semantic represen-
tations, but the annotations are with reference to
the surface string. Accordingly, we need projec-
tion rules to map from the ?painted? MRS to the
string. We can use the character offsets recorded
in each EP to project the scope to the string. How-
ever, the string-based annotations also include
words which the ERG treats as semantically vacu-
ous. Thus in order to match the gold annotations,
we define a set of heuristics for when to count vac-
uous words as in scope. In (1), there are no se-
mantically empty words in-scope, so we illustrate
these heuristics with another example:
(3) ?I trust that {there is} ?nothing? {of consequence
which I have overlooked}??
The MRS crawling operations discussed above
paint the EPs corresponding to is, thing, of, conse-
quence, I, and overlooked as in-scope (underlined
in (3)). Conversely, the ERG treats the words that,
there, which, and have as semantically empty. Of
these, we need to add all except that to the scope.
Our vacuous word handling rules use the syntac-
tic structure provided by the ERG as scaffolding to
help link the scope information gleaned from con-
tentful words to vacuous words. Each node in the
syntax tree is initially colored either in-scope or
out-of-scope in agreement with the decision made
by the crawler about the lexical head of the corre-
sponding subtree. A semantically empty word is
determined to be in-scope if there is an in-scope
syntax tree node in the right position relative to it,
as governed by a short list of templates organized
by the type of the semantically empty word (par-
ticles, complementizers, non-referential pronouns,
relative pronouns, and auxiliary verbs).
As an example, the rule for auxiliary verbs like
have in our example (3) is that they are in scope
when their verb phrase complement is in scope.
Since overlooked is marked as in-scope by the
crawler, the semantically empty have becomes in-
scope as well. Sometimes the rules need to be
iterated. For example, the main rule for relative
pronouns is that they are in-scope when they fill
a gap in an in-scope constituent; which fills a gap
in the constituent have overlooked, but since have
is the (syntactic) lexical head of that constituent,
the verb phrase is not considered in-scope the first
time the rules are tried.
Similar rules deal with that (complementizers
are in-scope when the complement phrase is an ar-
gument of an in-scope verb, which is not the case
here) and there (non-referential pronouns are in-
scope when they are the subject of an in-scope VP,
which is true here).
72
3.3 Re-Reading the Annotation Guidelines
Our MRS crawling algorithm was defined by look-
ing at the annotated data rather than the annota-
tion guidelines for the Shared Task (Morante et al,
2011). Nonetheless, our algorithm can be seen as
a first pass formalization of the guidelines. In this
section, we briefly sketch how our algorithm cor-
responds to different aspects of the guidelines.
For negated verbs, the guidelines state that ?If
the negated verb is the main verb in the sen-
tence, the entire sentence is in scope.? (Morante
et al, 2011, 17). In terms of our operations de-
fined over semantic representations, this is ren-
dered as follows: all arguments of the negated
verb are selected by argument crawling, all in-
tersective modifiers by label crawling, and func-
tor crawling (Fig. 2, line 8) captures modal auxil-
iaries and non-intersective modifiers. The guide-
lines treat predicative adjectives under a separate
heading from verbs, but describe the same desired
annotations (scope over the whole clause; ibid.,
p. 20). Since these structures are analogous in the
semantic representations, the same operations that
handle negated verbs also handle negated predica-
tive adjectives correctly.
For negated subjects and objects, the guidelines
state that the negation scopes over ?all the clause?
and ?the clause headed by the verb? (Morante et
al., 2011, 19), respectively. The examples given in
the annotation guidelines suggest that these are in
fact meant to refer to the same thing. The negation
cue for a negated nominal argument will appear
as a quantifier EP in the MRS, triggering line 3 of
our algorithm. This functor crawling step will get
to the verb?s EP, and from there, the process is the
same as the last two cases.
In contrast to subjects and objects, negation of
a clausal argument is not treated as negation of the
verb (ibid., p. 18). Since in this case, the negation
cue will not be a quantifier in the MRS, there will
be no functor crawling to the verb?s EP.
For negated modifiers, the situation is somewhat
more complex, and this is a case where our crawl-
ing algorithm, developed on the basis of the anno-
tated data, does not align directly with the guide-
lines as given. The guidelines state that negated at-
tributive adjectives have scope over the entire NP
(including the determiner) (ibid., p. 20) and anal-
ogously negated adverbs have scope over the en-
tire clause (ibid., p. 21). However, the annotations
are not consistent, especially with respect to the
treatment of negated adjectives: while the head
noun and determiner (if present) are typically an-
notated as in scope, other co-modifiers, especially
long, post-nominal modifiers (including relative
clauses) are not necessarily included:
(4) ?A dabbler in science, Mr. Holmes, a picker up
of shells on the shores of {the} great ?un?{known
ocean}.
(5) Our client looked down with a rueful face at {his}
own ?un?{conventional appearance}.
(6) Here was {this} ?ir?{reproachable Englishman}
ready to swear in any court of law that the accused
was in the house all the time.
(7) {There is}, on the face of it, {something}
?un?{natural about this strange and sudden friend-
ship between the young Spaniard and Scott Eccles}.
Furthermore, the guidelines treat relative clauses
as subordinate clauses and thus negation inside a
relative clause is treated as bound to that clause
only, and includes neither the head noun of the
relative clause nor any of its other dependents in
its scope. However, from the perspective of MRS,
a negated relative clause is indistinguishable from
any other negated modifier of a noun. This treat-
ment of relative clauses (as well as the inconsis-
tencies in other forms of co-modification) is the
reason for the exception noted at line 7 of Fig. 2.
By disallowing the addition of EPs to the scope if
they share the label of the negation cue but are not
one of its arguments, we block the head noun?s EP
(and any EPs only reachable from it) in cases of
relative clauses where the head verb inside the rel-
ative clause is negated. It also blocks co-modifiers
like great, own, and the phrases headed by ready
and about in (4)?(7). As illustrated in these exam-
ples, this is correct some but not all of the time.
Having been unable to find a generalization cap-
turing when comodifiers are annotated as in scope,
we stuck with this approximation.
For negation within clausal modifiers of verbs,
the annotation guidelines have further informa-
tion, but again, our existing algorithm has the cor-
rect behavior: The guidelines state that a negation
cue inside of the complement of a subordinating
conjunction (e.g. if ) has scope only over the sub-
ordinate clause (ibid., p. 18 and p. 26). The ERG
treats all subordinating conjunctions as two-place
predicates taking two scopal arguments. Thus,
as with clausal complements of clause-embedding
verbs, the embedding subordinating conjunction
and any other arguments it might have are inac-
cessible, since functor crawling is restricted to a
handful of specific configurations.
73
As is usually the case with exercises in for-
malization, our crawling algorithm generalizes be-
yond what is given explicitly in the annotation
guidelines. For example, all arguments that are
treated as semantically nominal (including PP ar-
guments where the preposition is semantically
null) are treated in the same way as subjects and
objects; similarly, all arguments which are seman-
tically clausal (including certain PP arguments)
are handled the same way as clausal complements.
This is possible because we take advantage of the
high degree of normalization that the ERG accom-
plishes in mapping to the MRS representation.
There are also cases where we are more spe-
cific. The guidelines do not handle coordination in
detail, except to state that in coordinated clauses
negation is restricted to the clause it appears in
(ibid., p. 17?18) and to include a few examples of
coordination under the heading ?ellipsis?. In the
case of VP coordination, our existing algorithm
does not need any further elaboration to pick up
the subject of the coordinated VP but not the non-
negated conjunct, as shown in discussion of (1) in
? 3.1 above. In the case of coordination of negated
NPs, recall that to reach the main portion of the
negated scope we must first apply functor crawl-
ing. The functor crawling procedure has a general
mechanism to transparently continue crawling up
through coordinated structures while blocking fu-
ture crawling from traversing them again.
6
On the other hand, there are some cases in the
annotation guidelines which our algorithm does
not yet handle. We have not yet provided any anal-
ysis of the special cases for save and expect dis-
cussed in Morante et al, 2011, pp. 22?23, and also
do not have a means of picking out the overt verb
in gapping constructions (p. 24).
Finally, we note that even carefully worked out
annotation guidelines such as these are never fol-
lowed perfectly consistently by the human annota-
tors who apply them. Because our crawling algo-
rithm so closely models the guidelines, this puts
our system in an interesting position to provide
feedback to the Shared Task organizers.
3.4 Fall-Back Configurations
The close match between our crawling algorithm
and the annotation guidelines supported by the
mapping to MRS provides for very high precision
6
This allows ate to be reached in We ate bread but no fish.,
while preventing but and bread from being reached, which
they otherwise would via argument crawling from ate.
and recall when the analysis engine produces the
desired MRS.
7
However, the analysis engine does
not always provide the desired analysis, largely
because of idiosyncrasies of the genre (e.g. voca-
tives appearing mid-sentence) that are either not
handled by the grammar or not well modeled in the
parse selection component. In addition, as noted
above, there are a handful of negation cues we do
not yet handle. Thus, we also tested fall-back con-
figurations which use scope predictions based on
MRS in some cases, and scope predictions from
the system of Read et al (2012) in others.
Our first fall-back configuration (Crawler
N
in
Table 1) uses MRS-based predictions whenever
there is a parse available and the cue is one that
our system handles. Sometimes, the analysis
picked by the ERG?s statistical model is not the
correct analysis for the given context. To com-
bat such suboptimal parse selection performance,
we investigated using the probability of the top
ranked analysis (as determined by the parse selec-
tion model and conditioned on the sentence) as a
confidence metric. Our second fall-back configu-
ration (Crawler
P
in Table 1) uses MRS-based pre-
dictions when there is a parse available whose con-
ditional probability is at least 0.5.
8
4 Experiments
We evaluated the performance of our system using
the Shared Task development and evaluation data
(respectively CDD and CDE in Table 1). Since we
do not attempt to perform cue detection, we report
performance using gold cues and also using the
system cues predicted by Read et al (2012). We
used the official Shared Task evaluation script to
compute all scores.
4.1 Data Sets
The Shared Task data consists of chapters from
the Adventures of Sherlock Holmes mystery nov-
els and short stories. As such, the text is carefully
edited turn-of-the-20th-century British English,
9
7
And in fact, the task is somewhat noise-tolerant: some
parse selection decisions are independent of each other, and
a mistake in a part of the analysis far enough away from the
negation cue does not harm performance.
8
This threshold was determined empirically on the devel-
opment data. We also experimented with other confidence
metrics?the probability ratio of the top-ranked and second
parse or the entropy over the probability distribution of the
top 10 parses?but found no substantive differences.
9
In contrast, the ERG was engineered for the analysis of
contemporary American English, and an anecdotal analysis
of parse failures and imperfect top-ranked parses suggests
74
Gold Cues System Cues
Scopes Tokens Scopes Tokens
Set Method Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
C
D
D
Ranker 100.0 68.5 81.3 84.8 86.8 85.8 91.7 66.1 76.8 79.5 84.9 82.1
Crawler 100.0 53.0 69.3 89.3 67.0 76.6 90.8 53.0 66.9 84.7 65.9 74.1
Crawler
N
100.0 64.9 78.7 89.0 83.5 86.1 90.8 64.3 75.3 82.6 82.1 82.3
Crawler
P
100.0 70.2 82.5 86.4 86.8 86.6 91.2 67.9 77.8 80.0 84.9 82.4
Oracle 100.0 76.8 86.9 91.5 89.1 90.3
C
D
E
Ranker 98.8 64.3 77.9 85.3 90.7 87.9 87.4 61.5 72.2 82.0 88.8 85.3
Crawler 100.0 44.2 61.3 85.8 68.4 76.1 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
N
98.6 56.6 71.9 83.8 88.4 86.1 86.0 54.2 66.5 78.4 85.7 81.9
Crawler
P
98.8 65.5 78.7 86.1 90.4 88.2 87.6 62.7 73.1 82.6 88.5 85.4
Oracle 100.0 70.3 82.6 89.5 93.1 91.3
Table 1: Scope resolution performance of various configurations over each subset of the Shared Task
data. Ranker refers to the system of Read et al (2012); Crawler refers to our current system in isolation,
or falling back to the Ranker prediction either when the sentence is not covered by the parser (Crawler
N
),
or when the parse probability is predicted to be less than 0.5 (Crawler
P
); finally, Oracle simulates best
possible selection among the Ranker and Crawler predictions (and would be ill-defined on system cues).
annotated with token-level information about the
cues and scopes in every negated sentence. The
training set contains 848 negated sentences, the
development set 144, and the evaluation set 235.
As there can be multiple usages of negation in one
sentence, this corresponds to 984, 173, and 264
instances, respectively.
Being rule-based, our system does not require
any training data per se. However, the majority of
our rule development and error analysis were per-
formed against the designated training data. We
used the designated development data for a single
final round of error analysis and corrections. The
system was declared frozen before running with
the formal evaluation data. All numbers reported
here reflect this frozen system.
10
4.2 Results
Table 1 presents the results of our various config-
urations in terms of both (a) whole scopes (i.e. a
true positive is only generated when the predicted
scope matches the gold scope exactly) and (b) in-
scope tokens (i.e. a true positive for every token
the system correctly predicts to be in scope). The
table also details the performance upper-bound for
system combination, in which an oracle selects the
system prediction which scores the greater token-
wise F
1
for each gold cue.
The low recall levels for Crawler can be mostly
that the archaic style in the 2012
*
SEM Shared Task texts
has a strong adverse effect on the parser.
10
The code and data are available from http://www
.delph-in.net/crawler/, for replicability (Fokkens et al,
2013).
attributed to imperfect parser coverage. Crawler
N
,
which falls back just for parse failure brings the
recall back up, and results in F
1
levels closer to
the system of Read et al (2012), albeit still not
quite advancing the state of the art (except over
the development set). Our best results are from
Crawler
P
, which outperforms all other configura-
tions on the development and evaluation sets.
The Oracle results are interesting because they
show that there is much more to be gained in com-
bining our semantics-based system with the Read
et al (2012) syntactically-focused system. Further
analysis of these results to draw out the patterns of
complementary errors and strengths is a promising
avenue for future work.
4.3 Error Analysis
To shed more light on specific strengths and weak-
nesses of our approach, we performed a manual er-
ror analysis of scope predictions by Crawler, start-
ing from gold cues so as to focus in-depth analy-
sis on properties specific to scope resolution over
MRSs. This analysis was performed on CDD, in
order to not bar future work on this task. Of the
173 negation cue instances in CDD, Crawler by it-
self makes 94 scope predictions that exactly match
the gold standard. In comparison, the system of
Read et al (2012) accomplishes 119 exact scope
matches, of which 80 are shared with Crawler; in
other words, there are 14 cue instances (or 8%
of all cues) in which our approach can improve
over the best-performing syntax-based submission
to the original Shared Task.
75
We reviewed the 79 negation instances where
Crawler made a wrong prediction in terms of ex-
act scope match, categorizing the source of failure
into five broad error types:
(1) Annotation Error In 11% of all instances, we
consider the annotations erroneous or inconsistent.
These judgments were made by two of the authors,
who both were familiar with the annotation guide-
lines and conventions observable in the data. For
example, Morante et al (2011) unambiguously
state that subordinating conjunctions shall not be
in-scope (8), whereas relative pronouns should be
(9), and a negated predicative argument to the cop-
ula must scope over the full clause (10):
(8) It was after nine this morning {when we} reached
his house and {found} ?neither? {you} ?nor?
{anyone else inside it}.
(9) ?We can imagine that in the confusion of flight
something precious, something which {he could}
?not? {bear to part with}, had been left behind.
(10) He said little about the case, but from that little we
gathered that he also was not ?dis?{satisfied} at the
course of events.
(2) Parser Failure Close to 30% of Crawler fail-
ures reflect lacking coverage in the ERG parser,
i.e. inputs for which the parser does not make
available an analysis (within certain bounds on
time and memory usage).
11
In this work, we have
treated the ERG as an off-the-shelf system, but
coverage could certainly be straightforwardly im-
proved by adding analyses for phenomena partic-
ular to turn-of-the-20th-century British English.
(3) MRS Inadequacy Another 33% of our false
scope predictions are Crawler-external, viz. owing
to erroneous input MRSs due to imperfect disam-
biguation by the parser or other inadequacies in
the parser output. Again, these judgments (assign-
ing blame outside our own work) were double-
checked by two authors, and we only counted
MRS imperfections that actually involve the cue
or in-scope elements. Here, we could anticipate
improvements by training the parse ranker on in-
domain data or otherwise adapting it to this task.
(4) Cue Selection In close to 9% of all cases,
there is a valid MRS, but Crawler fails to pick out
an initial EP that corresponds to the negation cue.
This first type of genuine crawling failure often re-
lates to cues expressed as affixation (11), as well
11
Overall parsing coverage on this data is about 86%, but
of course all parser failures on sentences containing negation
surface in our error analysis of Crawler in isolation.
Scopes Tokens
Method Prec Rec F
1
Prec Rec F
1
C
D
E
Boxer 76.1 41.0 53.3 69.2 82.3 75.2
Crawler 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
P
87.6 62.7 73.1 82.6 88.5 85.4
Table 2: Comparison to Basile et al (2012).
as to rare usages of cue expressions that predomi-
nantly occur with different categories, e.g. neither
as a generalized quantifier (12):
(11) Please arrange your thoughts and let me know, in
their due sequence, exactly what those events are
{which have sent you out} ?un?{brushed} and un-
kempt, with dress boots and waistcoat buttoned
awry, in search of advice and assistance.
(12) You saw yourself {how} ?neither? {of the inspec-
tors dreamed of questioning his statement}, extraor-
dinary as it was.
(5) Crawler Deficiency Finally, a little more
than 16% of incorrect predictions we attribute to
our crawling rules proper, where we see many
instances of under-coverage of MRS elements
(13, 14) and a few cases of extending the scope too
wide (15). In the examples below, erroneous scope
predictions by Crawler are indicated through un-
derlining. Hardly any of the errors in this category,
however, involve semantically vacuous tokens.
(13) He in turn had friends among the indoor
servants who unite in {their} fear and
?dis?{like of their master}.
(14) He said little about the case, but from that
little we gathered that {he also was} ?not?
{dissatisfied at the course of events}.
(15) I tell you, sir, {I could}n?t move a finger, ?nor?
{get my breath}, till it whisked away and was gone.
5 Discussion and Comparison
The example in (1) nicely illustrates the strengths
of the MRS Crawler and of the abstraction pro-
vided by the deep linguistic analysis made pos-
sible by the ERG. The negated verb in that sen-
tence is know, and its first semantic argument is
The German. This semantic dependency is di-
rectly and explicitly represented in the MRS, but
the phrase expressing the dependent is not adja-
cent to the head in the string. Furthermore, even
a system using syntactic structure to model scope
would be faced with a more complicated task than
our crawling rules: At the level of syntax the de-
pendency is mediated by both verb phrase coordi-
nation and the control verb profess, as well as by
the semantically empty infinitival marker to.
76
The system we propose is very similar in spirit
to that of Basile et al (2012). Both systems map
from logical forms with explicit representations of
scope of negation out to string-based annotations
in the format provided by the Shared Task gold
standard. The main points of difference are in the
robustness of the system and in the degree of tai-
loring of both the rules for determining scope on
the logical form level and the rules for handling se-
mantically vacuous elements. The system descrip-
tion in Basile et al (2012) suggests relatively little
tailoring at either level: aside from adjustments to
the Boxer lexicon to make more negation cues take
the form of the negation operator in the DRS, the
notion of scope is directly that given in the DRS.
Similarly, their heuristic for picking up semanti-
cally vacuous words is string-based and straight-
forward. Our system, on the other hand, models
the annotation guidelines more closely in the def-
inition of the MRS crawling rules, and has more
elaborated rules for handling semantically empty
words. The Crawler alone is less robust than the
Boxer-based system, returning no output for 29%
of the cues in CDE. These factors all point to
higher precision and lower recall for the Crawler
compared to the Boxer-based system. At the to-
ken level, that is what we see. Since full-scope re-
call depends on token-level precision, the Crawler
does better across the board at the full-scope level.
A comparison of the results is shown in Table 2.
A final key difference between our results and
those of Basile et al (2012) is the cascading with
a fall-back system. Presumably a similar system
combination strategy could be pursued with the
Boxer-based system in place of the Crawler.
6 Conclusion and Outlook
Our motivation in this work was to take the design
of the 2012
*
SEM Shared Task on negation analy-
sis at face value?as an overtly semantic problem
that takes a central role in our long-term pursuit of
language understanding. Through both theoreti-
cal and practical reflection on the nature of repre-
sentations at play in this task, we believe we have
demonstrated that explicit semantic structure will
be a key driver of further progress in the analy-
sis of negation. We were able to closely align
two independently developed semantic analyses?
the negation-specific annotations of Morante et al
(2011), on the one hand, and the broad-coverage,
MRS meaning representations of the ERG, on the
other hand. In our view, the conceptual correla-
tion between these two semantic views on nega-
tion analysis reinforces their credibility.
Unlike the rather complex top-performing sys-
tems from the original 2012 competition, our MRS
Crawler is defined by a small set of general rules
that operate over general-purpose, explicit mean-
ing representations. Thus, our approach scores
high on transparency, adaptability, and replicabil-
ity. In isolation, the Crawler provides premium
precision but comparatively low recall. Its limi-
tations, we conjecture, reflect primarily on ERG
parsing challenges and inconsistencies in the tar-
get data. In a sense, our approach pushes a
larger proportion of the task into the parser, mean-
ing (a) there should be good opportunities for
parser adaptation to this somewhat idiosyncratic
text type; (b) our results can serve to offer feed-
back on ERG semantic analyses and parse rank-
ing; and (c) there is a much smaller proportion
of very task-specific engineering. When embed-
ded in a confidence-thresholded cascading archi-
tecture, our system advances the state of the art
on this task, and oracle combination scores sug-
gest there is much remaining room to better ex-
ploit the complementarity of approaches in our
study. In future work, we will seek to better un-
derstand the division of labor between the systems
involved through contrastive error analysis and
possibly another oracle experiment, constructing
gold-standard MRSs for part of the data. It would
also be interesting to try a task-specific adaptation
of the ERG parse ranking model, for example re-
training on the pre-existing treebanks but giving
preference to analyses that lead to correct Crawler
results downstream.
Acknowledgments
We are grateful to Dan Flickinger, the main devel-
oper of the ERG, for many enlightening discus-
sions and continuous assistance in working with
the analyses available from the grammar. This
work grew out of a discussion with colleagues of
the Language Technology Group at the University
of Oslo, notably Elisabeth Lien and Jan Tore L?n-
ning, to whom we are indebted for stimulating co-
operation. Furthermore, we have benefited from
comments by participants of the 2013 DELPH-
IN Summit, in particular Joshua Crowgey, Guy
Emerson, Glenn Slayden, Sanghoun Song, and
Rui Wang.
77
References
Alshawi, H. (Ed.). 1992. The Core Language Engine.
Cambridge, MA, USA: MIT Press.
Basile, V., Bos, J., Evang, K., and Venhuizen, N.
2012. UGroningen. Negation detection with Dis-
course Representation Structures. In Proceedings of
the 1st Joint Conference on Lexical and Computa-
tional Semantics (p. 301 ? 309). Montr?al, Canada.
Bender, E. M. 2013. Linguistic fundamentals for nat-
ural language processing: 100 essentials from mor-
phology and syntax. San Rafael, CA, USA: Morgan
& Claypool Publishers.
Copestake, A., Flickinger, D., Pollard, C., and Sag,
I. A. 2005. Minimal Recursion Semantics. An intro-
duction. Research on Language and Computation,
3(4), 281 ? 332.
Curran, J., Clark, S., and Bos, J. 2007. Linguistically
motivated large-scale NLP with C&C and Boxer.
In Proceedings of the 45th Meeting of the Associa-
tion for Computational Linguistics Demo and Poster
Sessions (p. 33 ? 36). Prague, Czech Republic.
Flickinger, D. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engi-
neering, 6 (1), 15 ? 28.
Fokkens, A., van Erp, M., Postma, M., Pedersen,
T., Vossen, P., and Freire, N. 2013. Offspring
from reproduction problems. What replication fail-
ure teaches us. In Proceedings of the 51th Meet-
ing of the Association for Computational Linguistics
(p. 1691 ? 1701). Sofia, Bulgaria.
Koller, A., and Thater, S. 2005. Efficient solving and
exploration of scope ambiguities. In Proceedings of
the 43rd Meeting of the Association for Computa-
tional Linguistics: Interactive Poster and Demon-
stration Sessions (p. 9 ? 12). Ann Arbor, MI, USA.
Lapponi, E., Read, J., and ?vrelid, L. 2012. Repre-
senting and resolving negation for sentiment analy-
sis. In Proceedings of the 2012 ICDM workshop on
sentiment elicitation from natural text for informa-
tion retrieval and extraction. Brussels, Belgium.
Morante, R., and Blanco, E. 2012. *SEM 2012 Shared
Task. Resolving the scope and focus of negation. In
Proceedings of the 1st Joint Conference on Lexical
and Computational Semantics (p. 265 ? 274). Mon-
tr?al, Canada.
Morante, R., and Daelemans, W. 2012. ConanDoyle-
neg. Annotation of negation in Conan Doyle stories.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation. Istanbul,
Turkey.
Morante, R., Schrauwen, S., and Daelemans, W. 2011.
Annotation of negation cues and their scope guide-
lines v1.0 (Tech. Rep. # CTRS-003). Antwerp, Bel-
gium: Computational Linguistics & Psycholinguis-
tics Research Center, Universiteit Antwerpen.
Morante, R., and Sporleder, C. 2012. Modality and
negation. An introduction to the special issue. Com-
putational Linguistics, 38(2), 223 ? 260.
Oepen, S., and L?nning, J. T. 2006. Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Read, J., Velldal, E., ?vrelid, L., and Oepen, S. 2012.
UiO1. Constituent-based discriminative ranking for
negation resolution. In Proceedings of the 1st Joint
Conference on Lexical and Computational Seman-
tics (p. 310 ? 318). Montr?al, Canada.
Vincze, V., Szarvas, G., Farkas, R., M?ra, G., and
Csirik, J. 2008. The BioScope corpus. Biomedical
texts annotated for uncertainty, negation and their
scopes. BMC Bioinformatics, 9(Suppl 11).
78
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812?816,
Dublin, Ireland, August 23-24, 2014.
UW-MRS: Leveraging a Deep Grammar for Robotic Spatial Commands
Woodley Packard
University of Washington
sweaglesw@sweaglesw.org
Abstract
This paper describes a deep-parsing ap-
proach to SemEval-2014 Task 6, a novel
context-informed supervised parsing and
semantic analysis problem in a controlled
domain. The system comprises a hand-
built rule-based solution based on a pre-
existing broad coverage deep grammar of
English, backed up by a off-the-shelf data-
driven PCFG parser, and achieves the best
score reported among the task participants.
1 Introduction
SemEval-2014 Task 6 involves automatic transla-
tion of natural language commands for a robotic
arm into structured ?robot control language?
(RCL) instructions (Dukes, 2013a). Statements of
RCL are trees, with a fixed vocabulary of content
words like prism at the leaves, and markup like
action: or destination: at the nonterminals.
The yield of the tree largely aligns with the words
in the command, but there are frequently substitu-
tions, insertions, and deletions.
A unique and interesting property of this task
is the availability of highly relevant machine-
readable descriptions of the spatial context of each
command. Given a candidate RCL fragment de-
scribing an object to be manipulated, a spatial
planner provided by the task organizers can auto-
matically enumerate the set of task-world objects
that match the description. This information can
be used to resolve some of the ambiguity inherent
in natural language.
The commands come from the Robot Com-
mands Treebank (Dukes, 2013a), a crowdsourced
corpus built using a game with a purpose (von
Ahn, 2006). Style varies considerably, with miss-
ing determiners, missing or unexpected punc-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organizers. Licence details:
http://creativecommons.org/licenses/by/4.0/
tuation, and missing capitalization all common
(Dukes, 2013b). Examples (1) and (2) show typi-
cal commands from the dataset.
(1) drop the blue cube
(2) Pick yellow cube and drop it on top of blue cube
Although the natural language commands vary
in their degree of conformance to what might be
called standard English, the hand-built gold stan-
dard RCL annotations provided with them (e.g.
Figure 1) are commendable in their uniformity and
accuracy, in part because they have been automat-
ically verified against the formal before and after
scene descriptions using the spatial planner.
(event: (action: drop)
(entity: (color: blue)
(type: cube))
Figure 1: RCL corresponding to Example (1).
2 Related Work
Automatic interpretation of natural language is
a difficult and long-standing research problem.
Some approaches have taken a relatively shal-
low view; for instance, ELIZA (Weizenbaum,
1966) used pattern matching to somewhat con-
vincingly participate in an English conversation.
Approaches taking a deeper view tend to parse
utterances into structured representations. These
are usually abstract and general-purpose in na-
ture, e.g. the syntax trees produced by main-
stream PCFG parsers and the DRS produced by
the Boxer system (Bos, 2008). As a notable ex-
ception, Dukes (2014) presents a novel method to
produce RCL output directly.
The English Resource Grammar (ERG;
Flickinger, 2000) employed as a component in
the present work is a broad-coverage precision
hand-written unification grammar of English,
following the Head-driven Phrase Structure
Grammar theory of syntax (Pollard & Sag, 1994).
The ERG produces Minimal Recursion Semantics
812
(MRS; Copestake et al., 2005) analyses, which
are flat structures that explicitly encode predicate
argument relations (and other data). A simplified
MRS structure is shown in Figure 2. With minor
modifications to allow determinerless NPs and
some unexpected measure noun lexemes (as in
?two squares to the left?, etc), the ERG yields
analyses for 99% of the commands in the training
portion of the Robot Command Treebank.
(
INDEX = e,
{
pron(x), cube n(y),
drop v cause(e, x, y), blue a( , y)
})
Figure 2: Highly simplified view of the MRS pro-
duced by the ERG for Example (1).
3 ERG-based RCL Synthesis
This section outlines the method my sys-
tem employs to synthesize RCL outputs from
the MRS analyses produced by the ERG.
The ERG provides a ranked list of candidate
MRS analyses for each input. As a first step,
grossly inappropriate analyses are ruled out, e.g.
those proposing non-imperative main verbs or
domain-inappropriate parts of speech (?block? as
a verb). An attempt is made to convert each re-
maining analysis into a candidate RCL statement.
If conversion is successful, the result is tested for
coherence with respect to the known world state,
using the supplied spatial planner. An RCL state-
ment is incoherent if it involves picking up or mov-
ing an entity which does not exist, or if its com-
mand type (take, move, drop) is incompatible
with the current state of the robot arm, e.g. drop
is incoherent when the robot arm is not holding
anything. Processing stops as soon as a coherent
result is found.1
3.1 From MRS to RCL
Given an individual (imperative) MRS structure,
the first step in conversion to RCL is to iden-
tify the sequence of top-level verbal predications.
The INDEX property of the MRS provides an en-
try point. In a simple command like Example (1),
the INDEX will point to a single verbal predica-
tion, whereas in a compound command such as
1Practically speaking, conversion from MRS to RCL is
accomplished by a relatively short C program embodying
these rules and steps (about 1500 lines in the final version):
http://sweaglesw.org/svn/semeval-2014-task6/tags/dublin
Example (2), the INDEX will point to a coordina-
tion predication, which itself will have left and
right arguments which must be visited recursively.
Each verbal predication visited in this manner gen-
erates an event: RCL statement whose action:
property is determined by a looking up the ver-
bal predicate in a short hand-written table (e.g.
drop v cause maps to action: drop). If the
predicate is not found in the table, the most com-
mon action move is guessed.
Every RCL event: element must have an
entity: subelement, representing the object to
be moved by the action. Although in princi-
ple MRS makes no guarantees about the gener-
alizability of the semantic interpretation of argu-
ment roles across different predicates, in prac-
tice the third argument of every verbal predicate
relevant to this domain represents the object to
be moved; hence, synthesis of an event: pro-
ceeds by inspecting the third argument of the
MRS predicate which gave rise to it. Some types
of event: also involve a destination: subele-
ment, which encodes the location where the en-
tity should come to rest. When present, a verbal
predicate?s fourth argument almost always iden-
tifies a prepositional predication holding this in-
formation, although there are exceptions (e.g. for
move v from-to rel it is the fifth). When no such
resultative role is present, the first prepositional
modifier (if any) of the verbal event variable is
used for the destination: subelement.
Synthesis of an entity: element from a
referential index like y in Figure 2 or a
spatial-relation: element from a preposi-
tional predication proceeds in much the same way:
the RCL type: or relation: is determined by
a simple table lookup, and subelements are built
based on connections indicated in the MRS. One
salient difference is the treatment of predicates
that are not found in their respective lookup ta-
bles. Whereas unknown command predicates de-
fault to the most common action move, unknown
modifying spatial relations are simply dropped,2
and unknown entity types cause conversion to fail,
on the theory that an incorrect parse is likely. Pru-
dent rejection of suspect parses only rarely elim-
inates all available analyses, and generally helps
to find the most appropriate one. On development
data, the first analysis produced by the ERG was
2If the spatial relation is part of a mandatory
destination: element, this can then cause conversion to fail.
813
convertible for 87% of commands, and the first
RCL hypothesis was spatially coherent for 96% of
commands. These numbers indicate that the parse
ranking component of the ERG works quite well.
3.2 Polishing the Rules
I split the 2500 task-supplied annotated commands
into a randomly-divided training set (2000 com-
mands) and development set (500 commands).
Throughout this work, the development set was
only used for estimating performance on unseen
data and tuning system combination settings; the
contents of the development set were never in-
spected for rule writing or error analysis pur-
poses. Although the conversion architecture out-
lined above constitutes an effective framework,
there were quite a few details to be worked
through, such as the construction of the lookup ta-
bles, identification of cases requiring special han-
dling, elimination of undesirable parses, modest
extension of the ERG, etc. An error-analysis
tool which performed a fine-grained comparison
of the synthesized RCL statements with the gold-
standard ones and agglomerated common error
types proved invaluable when writing rules. 3 Pol-
ishing the system in this manner took about two
weeks of part-time effort; I maintained a log giv-
ing a short summary of each tweak (e.g. ?map
center n of rel to type: region?). These
tweaks required varying amounts of time to imple-
ment, from a few seconds up to perhaps an hour;
system accuracy as a function of the number of
such tweaks is shown in Figure 3.
3.3 Anaphora and Ellipsis
Some commands use anaphora to evoke the iden-
tity or type of previously mentioned entities. Typ-
ically, the pronoun ?it? refers to a specific entity
while the pronoun ?one? refers to the type of an
entity (e.g. ?Put the red cube on the blue one.?).
Empirically, the antecedent is nearly always the
first entity: element in the RCL statement, and
this heuristic works well in the system. A small
fraction of commands (< 0.5% of the training
data) elide the pronoun, in commands like ?Take
the blue tetrahedron and place in front left corner.?
In principle these could be detected and accommo-
dated through the addition of a simple mal-rule to
3The error-analysis tool walks the system and gold
RCL trees in tandem, recording differences and printing the
most common mismatches. It consists of about 100 lines of
Python and shell script, and took perhaps an hour to build.
0
20
40
60
80
100
10 20 30 40 50 60 70 80
A
cc
u
ra
cy
Approx. Number of Tweaks
Training Set
Development Set
Figure 3: Tuning the MRS-to-RCL conversion
system by tweaking/adding rules. Development-
set accuracy was only checked occasionally during
rule-writing to avoid over-fitting.
the ERG (Bender et al., 2004), but for simplicity
my system ignores this problem, leading to errors.
4 Robustness Strategies
If none of the analyses produced by the ERG result
in coherent RCL statements, the system produces
no output. On the one hand this results in quite a
high precision: on the training data, 96.75% of the
RCL statements produced are exactly correct. On
the other hand, in some scenarios a lower precision
result may be preferable to no result. The ERG-
based system fails to produce any output for 3.1%
of the training data inputs, a number that should be
expected to increase for unseen data (since conver-
sion can sometimes fail when the MRS contains
unrecognized predicates).
In order to produce a best-guess answer for
these remaining items, I employed the Berkeley
parser (Petrov et al., 2006), a state-of-the-art data-
driven system that induces a PCFG from a user-
supplied corpus of strings annotated with parse
trees. The RCL treebank is not directly suitable as
training material for the Berkeley parser, since the
yield of an RCL tree is not identical to (or even
in 1-to-1 correspondence with) the words of the
input utterance. In the interest of keeping things
simple, I produced a phrase structure translation
of the RCL treebank by simply discarding the el-
ements of the RCL trees that did not correspond
to any input, and inserting (X word) nodes for in-
put words that were not aligned to any RCL frag-
ment. The question of where in the tree to insert
these X nodes is presumably of considerable im-
portance, but again in the interest of simplicity I
simply clustered them together with the first RCL-
814
Sevent:
?
?
?
?
?
?
action:
drop
drop
entity:
?
?
?
?
color:
? ?
X
the
blue
blue
type:
cube
cube
Figure 4: Automatic phrase structure tree transla-
tion of the RCL statement shown in Figure 1.
aligned word appearing after them. Unaligned in-
put tokens at the end of the sentence were added
as siblings of the root node. Figure 4 shows the
phrase structure tree resulting from the translation
of the RCL statement shown in Figure 1.
Using this phrase structure treebank, the Berke-
ley parser tools make it possible to automatically
derive a similar phrase structure tree for any input
string, and indeed when the input string is a com-
mand such as the ones of interest in this work, the
resulting tree is quite close to an RCL statement.
Deletion of the X nodes yields a robust system
that frequently produces the exact correct RCL,
at least for those items where only input-aligned
RCL leaves are required. The most common type
of non-input-aligned RCL fragment is the id: el-
ement, identifying the antecedent of an anaphor.
As with the ERG-based system, a heuristic select-
ing the first entity as the antecedent whenever an
anaphor is present works quite well.
Improving the output of the statistical system
via tweaks of the type used in the ERG-based sys-
tem was much more challenging, due to the rel-
ative impoverishedness of the information made
available by the parser. Accurately detecting situ-
ations to improve without causing collateral dam-
age proved difficult. However, the base accu-
racy of the statistical system was quite good, and
when used as a back-off it improved overall sys-
tem scores considerably, as shown in Table 5.
5 Results and Discussion
The combined system performs best on both por-
tions of the data. Over the development data, the
MRS-based system performs considerably better
than the statistical system, in part due to the use
of spatial planning in the MRS-based system (time
did not permit adding spatial planning to the statis-
Dev Eval
System P R P R
MRS-only (?SP) 90.7 88.0 92.1 80.3
MRS-only (+SP) 95.4 92.2 96.1 82.4
Robust-only (?SP) 88.2 88.2 81.5 81.5
Combined (?SP) 90.8 90.8 90.5 90.5
Combined (+SP) 95.0 95.0 92.5 92.5
ERG coverage 98.6 91.0
Figure 5: Evaluation results. ?SP indicates
whether or not spatial planning was used. The ro-
bust and combined systems always returned a re-
sult, so P = R.
tical system). The statistical system has a slightly
higher recall than the MRS-only system without
spatial planning, but the MRS-only system has a
higher precision ? markedly so on the evalua-
tion data. This is consistent with previous find-
ings combining precision grammars with statisti-
cal systems (Packard et al., 2014).
ERG coverage dropped precipitously from
roughly 99% on the development data to 91%
on the evaluation data. This is likely the major
cause of the 10% absolute drop in the recall of the
MRS-only system. The fact that the robust sta-
tistical system encounters a comparable drop on
the evaluation data suggests that the text is qual-
itatively different from the (also held-out) devel-
opment data. One possible explanation is that
whereas the development data was randomly se-
lected from the 2500 task-provided training com-
mands, the evaluation data was taken as the se-
quentially following segment of the treebank, re-
sulting in the same distribution of game-with-a-
purpose participants (and hence writing styles) be-
tween the training and development sets but a dif-
ferent distribution for the evaluation data. 4
Dukes (2014) reports an accuracy of 96.53%,
which appears to be superior to the present system;
however, that system appears to have used more
training data than was available for the shared task,
and averaged scores over the entire treebank, mak-
ing direct comparison difficult.
Acknowledgements
I am grateful to Dan Flickinger, Emily Bender and
Stephan Oepen for their many helpful suggestions.
4Reviewers suggested dropping sentence initial punctua-
tion and reading ?cell? as ?tile.? This trick boosts the MRS-
only recall to 91.1% and the combined system to 94.5%,
demonstrating both the frailty of NLP systems to unexpected
inputs and the presence of surprises in the evaluation data.
ERG coverage rose from 91.0% to 98.6%.
815
References
Bender, E. M., Flickinger, D., Oepen, S., Walsh, A., &
Baldwin, T. (2004). Arboretum: Using a preci-
sion grammar for grammar checking in CALL.
In Instil/icall symposium 2004.
Bos, J. (2008). Wide-coverage semantic analysis with
boxer. In J. Bos & R. Delmonte (Eds.), Seman-
tics in text processing. step 2008 conference pro-
ceedings (pp. 277?286). College Publications.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I.
(2005). Minimal recursion semantics: An intro-
duction. Research on Language & Computation,
3(2), 281?332.
Dukes, K. (2013a). Semantic annotation of robotic
spatial commands. In Language and Technology
Conference (LTC). Poznan, Poland.
Dukes, K. (2013b). Train robots: A dataset for
natural language human-robot spatial interac-
tion through verbal commands. In Interna-
tional Conference on Social Robotics (ICSR).
Embodied Communication of Goals and Inten-
tions Workshop.
Dukes, K. (2014). Contextual Semantic Parsing using
Crowdsourced Spatial Descriptions. Computa-
tion and Language. arXiv:1405.0145 [cs.CL].
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(01), 15-28.
Packard, W., Bender, E. M., Read, J., Oepen, S., & Dri-
dan, R. (2014). Simple negation scope reso-
lution through deep parsing: A semantic solu-
tion to a semantic problem. In Proceedings of
the 52nd annual meeting of the Association for
Computational Linguistics. Baltimore, USA.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st Inter-
national Conference on Computational Linguis-
tics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics (pp. 433?
440).
Pollard, C., & Sag, I. A. (1994). Head-Driven Phrase
Structure Grammar. Chicago, USA: The Uni-
versity of Chicago Press.
von Ahn, L. (2006). Games with a purpose. Computer,
39(6), 92?94.
Weizenbaum, J. (1966). ELIZA ? a computer pro-
gram for the study of natural language commu-
nication between man and machine. Communi-
cations of the ACM, 9(1), 36?45.
816
