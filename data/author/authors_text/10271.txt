Automatic Extraction of Briefing Templates
Dipanjan Das Mohit Kumar
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{dipanjan, mohitkum, air}@cs.cmu.edu
Alexander I. Rudnicky
Abstract
An approach to solving the problem of au-
tomatic briefing generation from non-textual
events can be segmenting the task into two
major steps, namely, extraction of briefing
templates and learning aggregators that col-
late information from events and automati-
cally fill up the templates. In this paper, we
describe two novel unsupervised approaches
for extracting briefing templates from hu-
man written reports. Since the problem is
non-standard, we define our own criteria for
evaluating the approaches and demonstrate
that both approaches are effective in extract-
ing domain relevant templates with promis-
ing accuracies.
1 Introduction
Automated briefing generation from non-textual
events is an unsolved problem that currently lacks a
standard approach in the NLP community. Broadly,
it intersects the problem of language generation
from structured data and summarization. The prob-
lem is relevant in several domains where the user
has to repeatedly write reports based on events in
the domain, for example, weather reports (Reiter
et al, 2005), medical reports (Elhadad et al, 2005),
weekly class project reports (Kumar et al, 2007) and
so forth. On observing the data from these domains,
we notice a templatized nature of report items. Ex-
amples (1)-(3) demonstrate equivalents in a particu-
lar domain (Reiter et al, 2005).
(1) [A warm front] from [Iceland] to
[northern Scotland] will move [SE]
across [the northern North Sea] [today
and tomorrow]
(2) [A warm front] from [Iceland] to [the
Faeroes] will move [ENE] across [the
Norwegian Sea] [this evening]
(3) [A ridge] from [the British Isles] to
[Iceland] will move [NE] across [the
North Sea] [today]
In each sentence, the phrases in square brackets at
the same relative positions form the slots that take
up different values at different occasions. The cor-
responding template is shown in (4) with slots con-
taining their respective domain entity types. Instan-
tiations of (4) may produce (1)-(3) and similar sen-
tences. This kind of sentence structure motivates an
approach of segmenting the problem of closed do-
main summarization into two major steps of auto-
matic template extraction and learning aggregators,
which are pattern detectors that assimilate informa-
tion from the events, to populate these templates.
(4) [PRESSURE ENTITY] from [LOCATION] to
[LOCATION] will move [DIRECTION] across
[LOCATION] [TIME]
In the current work we address the first problem of
automatically extracting domain templates from hu-
man written reports. We take a two-step approach to
the problem; first, we cluster report sentences based
on similarity and second, we extract template(s) cor-
responding to each cluster by aligning the instances
in the cluster. We experimented with two indepen-
dent, arguably complementary techniques for clus-
tering and aligning ? a predicate argument based ap-
proach that extracts more general templates contain-
ing one predicate and a ROUGE (Lin, 2004) based
265
approach that can extract templates containing mul-
tiple verbs. As we will see below, both approaches
show promise.
2 Related Work
There has been instances of template based sum-
marization in popular Information Extraction (IE)
evaluations like MUC (Marsh & Perzanowski, 1998;
Onyshkevych, 1994) and ACE (ACE, 2007) where
hand engineered slots were to be filled for events in
text; but the focus lay on template filling rather than
their creation. (Riloff, 1996) describes an interesting
work on the generation of extraction patterns from
untagged text, but the analysis is syntactic and the
patterns do not resemble the templates that we aim
to extract. (Yangarber et al, 2000) describe another
system called ExDisco, that extracts event patterns
from un-annotated text starting from seed patterns.
Once again, the text analysis is not deep and the pat-
terns extracted are not sentence surface forms.
(Collier, 1998) proposed automatic domain tem-
plate extraction for IE purposes where MUC type
templates for particular types of events were con-
structed. The method relies on the idea from (Luhn,
1958) where statistically significant words of a cor-
pus were extracted. Based on these words, sen-
tences containing them were chosen and aligned
using subject-object-verb patterns. However, this
method did not look at arbitrary syntactic patterns.
(Filatova et al, 2006) improved the paradigm by
looking at the most frequent verbs occurring in a
corpus and aligning subtrees containing the verb,
by using the syntactic parses as a similarity metric.
However, long distance dependencies of verbs with
constituents were not looked at and deep semantic
analysis was not performed on the sentences to find
out similar verb subcategorization frames. In con-
trast, in our predicate argument based approach we
look into deeper semantic structures, and align sen-
tences not only based on similar syntactic parses,
but also based on the constituents? roles with re-
spect to the main predicate. Also, they relied on
typical Named Entities (NEs) like location, organi-
zation, person etc. and included another entity that
they termed as NUMBER. However, for specific
domains like weather forecasts, medical reports or
student reports, more varied domain entities form
slots in templates, as we observe in our data; hence,
existence of a module handling domain specific en-
tities become essential for such a task. (Surdeanu
et al, 2003) identify arguments for predicates in a
sentence and emphasize how semantic role infor-
mation may assist in IE related tasks, but their pri-
mary focus remained on the extraction of PropBank
(Kingsbury et al, 2002) type semantic roles.
To our knowledge, the ROUGE metric has not
been used for automatic extraction of templates.
3 The Data
3.1 Data Description
Since our focus is on creating summary items from
events or structured data rather than from text, we
used a corpus from the domain of weather fore-
casts (Reiter et al, 2005). This is a freely avail-
able parallel corpus1 consisting of weather data
and human written forecasts describing them. The
dataset showed regularity in sentence structure and
belonged to a closed domain, making the variations
in surface forms more constrained than completely
free text. After sentence segmentation we arrived at
a set of 3262 sentences. From this set, we selected
3000 for template extraction and kept aside 262 sen-
tences for testing.
3.2 Preprocessing
For semantic analysis, we used the ASSERT toolkit
(Pradhan et al, 2004) that produces shallow seman-
tic parses using the PropBank conventions. As a
by product, it also produces syntactic parses of sen-
tences, using the Charniak parser (Charniak, 2001).
For each sentence, we maintained a part-of-speech
tagged (leaves of the parse tree), parsed, baseNP2
tagged and semantic role tagged version. The
baseNPs were retrieved by pruning the parse trees
and not by using a separate NP chunker. The rea-
son for having a baseNP tagged corpus will become
clear as we go into the detail of our template ex-
traction techniques. Figure 1 shows a typical out-
put from the Charniak parser and Figure 2 shows the
same tree with nodes under the baseNPs pruned.
We identified the need to have a domain entity
tagger for matching constituents in the sentences.
1http://www.csd.abdn.ac.uk/research/sumtime/
2A baseNP is a noun-phrase with no internal noun-phrase
266
ADVP
IN
A low theover Norwegian Sea will move North and weaken
DT NN DT JJ NN MD VB RB CC VB
NP
NP
PP
NP
S
VP
VP
VP
VP
Figure 1: Parse tree for a sentence in the data.
ADVP
IN
A low theover Norwegian Sea will move North and weaken
MD VB RB CC VB
NP
NP
PP
NP
S
VP
VP
VP
VP
Figure 2: Pruned parse tree for a sentence in the cor-
pus
Any tagger for named entities was not suitable for
weather forecasts since unique constituent types as-
sumed significance unlike newswire data. Since the
development of such a tagger was beyond the scope
of the present work, we developed a module that
took baseNP tagged sentences as input and produced
tags across words and baseNPs that were domain en-
tities. The development of such a module by hand
was easy because of a limited vocabulary (< 1000
words) of the data and the closed set nature of most
entity types (e.g the direction entity could take up a
finite set of values). From inspection, thirteen dis-
tinct entity types were recognized in the domain.
Figure 3 shows an example output from the entity
recognizer with the sentence from Figure 2 as input.
[ A low ]
DIRECTION and weaken
A low over the Norwegian Sea will move North and weaken 
ENTITY RECOGNIZER
LOCATIONover [ the Norwegian Sea ]PRESSURE ENTITY
will move [ North ]
Figure 3: Example output of the entity recognizer
We now provide a detailed description of our clus-
tering and template extraction algorithms.
4 Approach and Experiments
We adopted two parallel approaches. First, we
investigated a predicate-argument based approach
where we consider the set of all propositions in our
dataset, and cluster them based on their verb sub-
categorization frame. Second, we used ROUGE,
a summarization evaluation metric that is generally
used to compare machine generated and human writ-
ten summaries. We uniquely used this metric for
clustering similar summary items, after abstracting
the surface forms to a representation that facilitates
comparison of a pair of sentences. The following
subsections detail both the techniques.
4.1 A Predicate-Argument Based Approach
Analysis of predicate-argument structures seemed
appropriate for template extraction for a few rea-
sons: Firstly, complicated sentences with multiple
verbs are broken down into propositions by a seman-
tic role labeler. The propositions3 are better gen-
eralizable units than whole sentences across a cor-
pus. Secondly, long distance dependencies of con-
stituents with a particular verb, are captured well by
a semantic role labeler. Finally, if verbs are con-
sidered to be the center of events, then groups of
sentences with the same semantic role sequences
seemed to form clusters conveying similar meaning.
We explain the complete algorithm for template ex-
traction in the following subsections.
(5) [ARG0 A low over the Norwegian Sea]
[AGM-MOD will] [TARGET move ]
[ARGM-DIR North ] and weaken
(6) [ARG0 A high pressure area ] [AGM-MOD
will ] [TARGET move] [ARGM-DIR
southwestwards] and build on Sunday.
4.1.1 Verb based clustering
We performed a verb based clustering as the first
step. Instead of considering a unique set of verbs,
we considered related verbs as a single verb type.
The relatedness of verbs was derived from Word-
net (Fellbaum, 1998), by merging verbs that appear
in the same synset. This kind of clustering is not
3sentence fragments with one verb
267
ideal in a corpus containing a huge variation in event
streams, like newswire. However, the results were
good for the weather domain where the number of
verbs used is limited. The grouping procedure re-
sulted in a set of 82 clusters with 6632 propositions.
4.1.2 Matching Role Sequences
Each verb cluster was considered next. Instead
of finding structural similarities of the propositions
in one go, we first considered the semantic role
sequences for each proposition. We searched for
propositions that had exactly similar role sequences
and grouped them together. To give an exam-
ple, both sentences 5 and 6 have the matching role
sequence ARG0?ARGM-MOD?TARGET?ARGM-
DIR. The intuition behind such clustering is straight-
forward. Propositions with a matching verb type
with the same set of roles arranged in a similar fash-
ion would convey similar meaning. We observed
that this was indeed true for sentences tagged with
correct semantic role labels.
Instead of considering matching role sequences
for a set of propositions, we could as well have
considered matching bag of roles. However, for
the present corpus, we decided to use strict role se-
quence instead because of the sentences? rigid struc-
ture and absence of any passive sentences. This
subclustering step resulted in smaller clusters, and
many of them contained a single proposition. We
threw out these clusters on the assumption that the
human summarizers did not necessarily have a tem-
plate in mind while writing those summary items.
As a result, many verb types were eliminated and
only 33 verb-type clusters containing several sub-
clusters each were produced.
4.1.3 Looking inside Roles
Groups of propositions with the same verb-type
and semantic role sequences were considered in this
step. For each group, we looked at individual se-
mantic roles to find out similarity between them. We
decided at first to look at syntactic parse tree similar-
ities between constituents. However, there is a need
to decide at what level of abstraction should one con-
sider matching the parse trees. After considerable
speculation, we decided on pruning the constituents?
parse trees till the level of baseNPs and then match
the resulting tag sequences.
Scotland
IN
A low theover Sea
NP
NP
PP
NP
NP
NP PP
NP
Norwegian A frontal trough
IN
across
Figure 4: Matching ARG0s for two propositions
LOCATIONIN
A low theover SeaNorwegian A frontal trough
IN
across Scotland
PRESSURE ENTITY
LOCATION
PRESSURE ENTITY
Figure 5: Abstracted tag sequences for two con-
stituents
The parses with pruned trees from the preprocess-
ing steps provide the necessary information for con-
stituent matching. Figure 4 shows matching syntac-
tic trees for two ARG0s from two propositions of a
cluster. It is at this step that we use the domain entity
tags to abstract away the constituents? syntactic tags.
Figure 5 shows the constituents of Figure 4 with the
tree structure reduced to tag sequences and domain
entity types replacing the tags whenever necessary.
This abstraction step produces a number of unique
domain entity augmented tag sequences for a partic-
ular semantic role. As a final step of template gen-
eration, we concatenate these abstracted constituent
types for all the semantic roles in the given group.
To focus on template-like structures we only con-
sider tag sequences that occur twice or more in the
group.
The templates produced at the end of this step are
essentially tag sequences interspersed with domain
entities. In our definition of templates, the slots are
the entity types and the fixed parts are constituted
by word(s) used by the human experts for a partic-
ular tag sequence. Figure 6 shows some example
templates. The upper case words in the figure corre-
spond to the domain entities identified by the entity
tagger and they form the slots in the templates. A
total of 209 templates were produced.
268
PRESSURE_ENTITY to DIRECTION of LOCATION will drift slowly
WAVE will run_0.5/move_0.5 DIRECTION then DIRECTION
Associated PRESSURE_ENTITY will move DIRECTION across LOCATION TIME
PRESSURE_ENTITY expected over LOCATION by_0.5/on_0.5 DAY
Figure 6: Example Templates. Upper case tokens
correspond to slots. For fixed parts, when there is a
choice between words, the probability of the occur-
rence of words in that particular syntactic structure
are tagged alongside.
4.2 A ROUGE Based Approach
ROUGE (Lin, 2004) is the standard automatic eval-
uation metric in the Summarization community. It is
derived from the BLEU (Papineni et al, 2001) score
which is the evaluation metric used in the Machine
Translation community. The underlying idea in the
metric is comparing the candidate and the refer-
ence sentences (or summaries) based on their token
co-occurrence statistics. For example, a unigram
based measure would compare the vocabulary over-
lap between the candidate and reference sentences.
Thus, intuitively, we may use the ROUGE score as
a measure for clustering the sentences. Amongst
the various ROUGE statistics, the most appealing is
Weighted Longest Common Subsequence(WLCS).
WLCS favors contiguous LCS which corresponds
to the intuition of finding the common template.
We experimented with other ROUGE statistics but
we got better and easily interpretable results using
WLCS and so we chose it as the final metric. In
all the approaches the data was first preprocessed
(baseNP and NE tagged) as described in the previ-
ous subsection. In the following subsections, we de-
scribe the various clustering techniques that we tried
using the ROUGE score followed by the alignment
technique.
4.2.1 Clustering
Unsupervised Clustering: As the ROUGE score
defines a distance metric, we can use this score for
doing unsupervised clustering. We tried hierarchical
clustering approaches but did not obtain good clus-
ters, evaluated empirically. In empirical evaluation,
we manually looked at the output clusters and made
a judgement call whether the candidate clusters are
reasonably coherent and potentially correspond to
templates. The reason for the poor performance of
the approach was the classical parameter estimation
problem of determining a priori the number of clus-
ters. We could not find an elegant solution for the
problem without losing the motivation of an auto-
mated approach.
Figure 7: Deterministic clustering based on Graph
connectivity. In the figure the squares with the same
pattern belong to the same cluster.
Non-parametric Unsupervised Clustering:
Since the unsupervised technique did not give
good results, we experimented with a non-
parametric clustering approach, namely, Cross-
Association(Chakrabarti et al, 2004). It is a
non-parametric unsupervised clustering algorithm
for similarity (boolean) matrices. We obtain the
similarity matrix in our domain by thresholding the
ROUGE similarity score matrix. This technique
also did not give us good clusters, evaluated empiri-
cally. The plausible reason for the poor performance
seems to be that the technique is based on MDL
(Minimum Description Length) principle. Since in
our domain we expect a large number of clusters
with small membership along many singletons,
MDL principle is not likely to perform well.
Deterministic Clustering:
As the unsupervised techniques did not perform
well, we tried deterministic clustering based on
graph connectivity. The underlying intuition is that
all the sentences X1...n that are ?similar? to any
other sentence Yi should be in the same cluster even
though Xj and Xk may not be ?similar? to each
other. Thus we find the connected components in the
similarity matrix and label them as individual clus-
ters.4
4This approach is similar to agglomerative single linkage
clustering.
269
We created a similarity matrix by thresholding the
ROUGE score. In the event, the clusters obtained by
this approach were also not good, evaluated empir-
ically. This led us to revisit the similarity function
and tune it. We factored the ROUGE-WLCS score,
which is an F-measure score, into its component Pre-
cision and Recall scores and experimented with var-
ious combinations of using the Precision and Recall
scores. We finally chose a combined Precision and
Recall measure (not f-measure) in which both the
scores were independently thresholded. The moti-
vation for the measure is that in our domain we de-
sire to have high precision matches. Additionally
we need to control the length of the sentences in the
cluster for which we require a Recall threshold. F-
measure (which is the harmonic mean of Precision
and Recall) does not give us the required individual
control. We set up our experiments such that while
comparing two sentences the longer sentence is al-
ways treated as the reference and the shorter one as
the candidate. This helps us in interpreting the Pre-
cision/Recall measures better and thresholding them
accordingly. The approach gave us 149 clusters,
which looked good on empirical evaluation. We can
argue that using this modified similarity function for
previous unsupervised approaches could have given
better results, but we did not reevaluate those ap-
proaches as our aim of getting a reasonable cluster-
ing approach is fulfilled with this simple scheme and
tuning the unsupervised approaches can be interest-
ing future work.
4.3 Alignment
After obtaining the clusters using the Deterministic
approach we needed to find out the template corre-
sponding to each of the cluster. Fairly intuitively we
computed the Longest Common Subsequence(LCS)
between the sentences in each cluster which we then
claim to be the template corresponding to the clus-
ter. This resulted in a set of 149 templates, similar to
the Predicate Argument based approach, as shown
in figure 6.
5 Results
5.1 Evaluation Scheme
Since there is no standard way to evaluate template
extraction for summary creation, we adopted a mix
of subjective and automatic measures for evaluating
the templates extracted. We define precision for this
particular problem as:
precision = number of domain relevant templatestotal number of extracted templates
This is a subjective measure and we undertook a
study involving three subjects who were accustomed
to the language used in the corpus. We asked the
human subjects to mark each template as relevant
or non-relevant to the weather forecast domain. We
also asked them to mark the template as grammatical
or ungrammatical if it is non-relevant.
Our other metric for evaluation is automatic re-
call. It is based on using the ROUGE-WLCS met-
ric to determine a match between the preprocessed
(baseNP and NE tagged) test corpora with the pro-
posed set of correct templates, a set determined
by taking an intersection of only the relevant tem-
plates marked by each judge. For the ROUGE based
method, the test corpus consists of 262 sentences,
while for the predicate-argument based method it
consists of a set of 263 propositions extracted from
the 262 sentences using ASSERT followed by a fil-
tering of invalid propositions (e.g. ones starting
with a verb). Amongst different ROUGE scores
(precision/recall/f-measure), we consider precision
as the criterion for deciding a match and experi-
mented with different thresholding values.
Main Verb Precision Main Verb Precision
deepen 0.67 weaken 0.83
expect 0.76 lie 0.57
drift 0.93 continue 0.97
build 0.95 fill 0.80
cross 0.78 move 0.86
Table 1: Precision for top 10 most frequently occur-
ring verbs
5.2 Results: Predicate-Argument Based
Approach
Table 1 shows the precision values for top 10 most
frequently occurring verbs. (Since a major propor-
tion (> 90%) of the templates are covered by these
verbs, we don?t show all the precision values; it also
helps to contain space.) The overall precision value
achieved was 84.21%, the inter-rater Fleiss? kappa
measure (Fleiss, 1971) between the judges being
270
? = 0.69, demonstrating substantial agreement. The
precision values are encouraging, and in most cases
the reason for low precision is because of erroneous
performance of the semantic role labeler system,
which is corroborated by the percentage (47.47%) of
ungrammatical templates among the irrelevant ones.
Results for the automated recall values are shown
in Figure 8, where precision values are varied to
observe the recall. For 0.9 precision in ROUGE-
WLCS, the recall is 0.3 which shows that there is
a 30% near exact coverage over propositions, while
for 0.6 precision in ROUGE-WLCS, the recall is an
encouraging 81%.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0.4 0.5 0.6 0.7 0.8 0.9
Rec
all
Precision Threshold forMatching Test Sentences
ROUGESRL
Figure 8: Automated Recall based on ROUGE-
WLCS measure comparing the test corpora with
the set of templates extracted by the Predicate-
Argument (SRL) and the ROUGE based method.
5.3 Results: ROUGE based approach
Various precision and recall thresholds for ROUGE
were considered for clustering. We empirically set-
tled on a recall threshold of 0.8 since this produces
the set of clusters with optimum number of sen-
tences. The number of clusters and number of sen-
tences in clusters at this recall values are shown in
Figure 9 for various precision thresholds.
Precision was measured in the same way as the
predicate argument approach and the value obtained
was 76.3%, with Fleiss? kappa measure of ? = 0.79.
The percentage of ungrammatical templates among
the irrelevant ones was 96.7%, strongly indicating
that post processing the templates using a parser can,
in future, give substantial improvement. During er-
ror analysis, we observed simple grammatical er-
rors in templates; first or last word being preposi-
 130
 140
 150
 160
 170
 180
 190
 0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0
 200
 400
 600
 800
 1000
 1200
 1400
No. o
f Clus
ters
No. o
f Se
nten
ces
 in 
Cluste
rs
Precision Threshold
No. of ClustersNo. of Sentences in Clusters
Figure 9: Number of clusters and total number of
sentences in clusters for various Precision Thresh-
olds at Recall Threshold=0.8
tions. So a fairly simple error recovery module that
strips the leading and trailing prepositions was in-
troduced. 20 templates out of the 149 were mod-
ified by the error recovery module and they were
evaluated again by the three judges. The precision
obtained for the modified templates was 35%, with
Fleiss? kappa ? = 1, boosting the overall precision
to 80.98%. The overall high precision is motivat-
ing as this is a fairly general approach that does not
require any NLP resources. Figure 8 shows the auto-
mated recall values for the templates and abstracted
sentences from the held-out dataset. For high preci-
sion points, the recall is low because there is not an
exact match for most cases.
6 Conclusion and Future Work
In this paper, we described two new approaches
for template extraction for briefing generation. For
both approaches, high precision values indicate that
meaningful templates are being extracted. However,
the recall values were moderate and they hint at
possible improvements. An interesting direction of
future research is merging the two approaches and
have one technique benefit from the other. The ap-
proaches seem complementary as the ROUGE based
technique does not use the structure of the sentence
at all whereas the predicate-argument approach is
heavily dependent on it. Moreover, the predicate
argument based approach gives general templates
with one predicate while ROUGE based approach
271
can extract templates containing multiple verbs. It
would also be desirable to establish the generality
of the techniques, by using other domains such as
newswire, medical reports and others.
Acknowledgements We would like to express our
gratitude to William Cohen and Noah Smith for their
valuable suggestions and inputs during the course of
this work. We also thank the three anonymous re-
viewers for helpful suggestions. This work was sup-
ported by DARPA grant NBCHD030010. The con-
tent of the information in this publication does not
necessarily reflect the position or the policy of the
US Government, and no official endorsement should
be inferred.
References
ACE (2007). Automatic content extraction program.
http://www.nist.gov/speech/tests/ace/.
Chakrabarti, D., Papadimitriou, S., Modha, D. S.,
& Faloutsos, C. (2004). Fully automatic cross-
associations. Proceedings of KDD ?04 (pp. 79?
88). New York, NY, USA: ACM Press.
Charniak, E. (2001). Immediate-head parsing for
language models. Proceedings of ACL ?01 (pp.
116?123).
Collier, R. (1998). Automatic template creation
for information extraction. Doctoral dissertation,
University of Sheffield.
Elhadad, N., Kan, M.-Y., Klavans, J. L., & McKe-
own, K. (2005). Customization in a unified frame-
work for summarizing medical literature. Artifi-
cial Intelligence in Medicine, 33, 179?198.
Fellbaum, C. (1998). WordNet ? An Electronic Lex-
ical Database. MIT Press.
Filatova, E., Hatzivassiloglou, V., & McKeown,
K. (2006). Automatic creation of domain tem-
plates. Proceedings of COLING/ACL 2006 (pp.
207?214).
Fleiss, J. (1971). Measuring nominal scale agree-
ment among many raters. Psychological Bulletin
(pp. 378?382).
Kingsbury, P., Palmer, M., & Marcus, M. (2002).
Adding semantic annotation to the penn treebank.
Proceedings of the HLT?02.
Kumar, M., Garera, N., & Rudnicky, A. I. (2007).
Learning from the report-writing behavior of in-
dividuals. IJCAI (pp. 1641?1646).
Lin, C.-Y. (2004). ROUGE: A package for auto-
matic evaluation of summaries. Proceedings of
Workshop on Text Summarization.
Luhn, H. P. (1958). The automatic creation of litera-
ture abstracts. IBM Journal of Research Develop-
ment, 2, 159?165.
Marsh, E., & Perzanowski, D. (1998). MUC-7 Eval-
uation of IE Technology: Overview of Results.
Proceedings of MUC-7. Fairfax, Virginia.
Onyshkevych, B. (1994). Issues and methodology
for template design for information extraction.
Proceedings of HLT ?94 (pp. 171?176). Morris-
town, NJ, USA.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.
(2001). Bleu: a method for automatic evaluation
of machine translation.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., &
Jurafsky, D. (2004). Shallow semantic parsing
using support vector machines. Proceedings of
HLT/NAACL ?04. Boston, MA.
Reiter, E., Sripada, S., Hunter, J., Yu, J., & Davy,
I. (2005). Choosing words in computer-generated
weather forecasts. Artif. Intell., 167, 137?169.
Riloff, E. (1996). Automatically generating extrac-
tion patterns from untagged text. AAAI/IAAI, Vol.
2 (pp. 1044?1049).
Surdeanu, M., Harabagiu, S., Williams, J., &
Aarseth, P. (2003). Using predicate-argument
structures for information extraction. Proceedings
of ACL 2003.
Yangarber, R., Grishman, R., Tapanainen, P., & Hut-
tunen, S. (2000). Automatic acquisition of domain
knowledge for information extraction. Proceed-
ings of the 18th conference on Computational lin-
guistics (pp. 940?946). Morristown, NJ, USA.
272
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67?71,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Non-textual Event Summarization by Applying Machine Learning to
Template-based Language Generation
Mohit Kumar and Dipanjan Das and Sachin Agarwal and Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University, Pittsburgh, USA
mohitkum,dipanjan,sachina,air@cs.cmu.edu
Abstract
We describe a learning-based system that
creates draft reports based on observation
of people preparing such reports in a tar-
get domain (conference replanning). The
reports (or briefings) are based on a mix
of text and event data. The latter consist
of task creation and completion actions,
collected from a wide variety of sources
within the target environment. The report
drafting system is part of a larger learning-
based cognitive assistant system that im-
proves the quality of its assistance based
on an opportunity to learn from observa-
tion. The system can learn to accurately
predict the briefing assembly behavior and
shows significant performance improve-
ments relative to a non-learning system,
demonstrating that it?s possible to create
meaningful verbal descriptions of activity
from event streams.
1 Introduction
We describe a system for recommending items for
a briefing created after a session with a crisis man-
agement system in a conference replanning do-
main. The briefing system is learning-based, in
that it initially observes how one set of users cre-
ates such briefings then generates draft reports for
another set of users. This system, the Briefing
Assistant(BA), is part of a set of learning-based
cognitive assistants each of which observes users
and learns to assist users in performing their tasks
faster and more accurately.
The difference between this work from
most previous efforts, primarily based on text-
extraction approaches is the emphasis on learning
to summarize event patterns. This work also
differs in its emphasis on learning from user
behavior in the context of a task.
Report generation from non-textual sources has
been previously explored in the Natural Language
Generation (NLG) community in a variety of do-
mains, based on, for example, a database of events.
However, a purely generative approach is not suit-
able in our circumstances, as we want to summa-
rize a variety of tasks that the user is performing
and present a summary tailored to a target audi-
ence, a desirable characteristic of good briefings
(Radev and McKeown, 1998). Thus we approach
the problem by applying learning techniques com-
bined with a template-based generation system to
instantiate the briefing-worthy report items. The
task of instantiating the briefing-worthy items is
similar to the task of Content Selection (Duboue,
2004) in the Generation pipeline however our ap-
proach minimizes linguistic involvement. Our
choice of a template-based generative system was
motivated by recent discussions in the NLG com-
munity (van Deemter et al, 2005) about the prac-
ticality and effectiveness of this approach.
The plan of the paper is as follows. We describe
relevant work from existing literature in the next
section. Then, we provide brief system description
followed by experiments and results. We conclude
with a summary of the work.
2 Related Work
Event based summarization has been studied in the
summarization community. (Daniel et al, 2003)
described identification of sub-events in multiple
documents. (Filatova and Hatzivassiloglou, 2004)
mentioned the use of event-based features in ex-
tractive summarization and (Wu, 2006; Li et al,
2006) describe similar work based on events oc-
curring in text. However, unlike the case at hand,
all the work on event-based summarization used
text as source material.
Non-textual summarization has also been ex-
plored in the Natural Language Generation (NLG)
community within the broad task of generating
67
reports based on database of events in specific
domains such as medical (Portet et al, 2009),
weather (Belz, 2007), sports (Oh and Shrobe,
2008) etc. However, in our case we want to sum-
marize a variety of tasks that the user is perform-
ing and present a summary to an intended audi-
ence (as defined by a report request).
Recent advances in NLG research use statis-
tical approaches at various stages of processing
in the generation pipeline like content selection
(Duboue and McKeown, 2003; Barzilay and Lee,
2004), probabilistic generation rules (Belz, 2007).
Our proposed approach differs from these in that
we apply machine learning after generation of all
the templates, as a post-processing step, to rank
them for inclusion in the final briefing. We could
have used a general purpose template-based gen-
eration framework like TG/2 (Busemann, 2005),
but since the number of templates and their corre-
sponding aggregators is limited, we chose an ap-
proach based on string manipulation.
We found in our work that an approach based
on modeling individual users and then combining
the outputs of such models using a voting scheme
gives the best results, although our approach is
distinguishable from collaborative filtering tech-
niques used for driving recommendation systems
(Hofmann, 2004). We believe this is due to the
fact that the individual sessions from which rank-
ing models are learned, although they range over
the same collection of component tasks, can lead
to very different (human-generated) reports. That
is, the particular history of a session will affect
what is considered to be briefing-worthy.
3 System Overview
Figure 1: Briefing Assistant Data Flow.
The Briefing Assistant Model: We treat the
task of briefing generation in the current domain1
as non-textual event-based summarization. The
1More details about the domain and the interaction of BA
with the larger system are mentioned in a longer version of
the paper (Kumar et al, 2009)
Figure 2: The category tree showing the informa-
tion types that we expect in a briefing.
events are the task creation and task completion
actions logged by various cognitive assistants in
the system (so-called specialists). As part of the
design phase for the template-based generation
component, we identified a set of templates, based
on the actual briefings written by users in a sepa-
rate experiment. Ideally, we would like to adopt
a corpus-based approach to automatically extract
the templates in the domain, like (Kumar et al,
2008), but since the sample briefings available to
us were very few, the application of such corpus-
based techniques was not necessary. Based on
this set of templates we identified the patterns that
needed to be extracted from the event logs in order
to populate the templates. A ranking model was
also designed for ordering instantiations of this set
of templates and to recommend the top 4 most rel-
evant ones for a given session.
The overall data flow for BA during a session
(runtime) is shown in Figure 1. The various spe-
cialist modules generate task related events that
are logged in a database. The aggregators operate
over this database and emails to extract relevant
patterns. These patterns in turn are used to popu-
late templates which constitute candidate briefing
items. The candidate briefing items are then or-
dered by the ranking module and presented to the
user.
Template Design and Aggregators: The set
of templates used in the current instantiation of
the BA was derived from a corpus of human-
generated briefings collected in a previous exper-
iment using the same crisis management system.
The set of templates was designed to cover the
range of items that users in that experiment chose
to include in their reports corresponding to nine
categories shown in Figure 2. We found that in-
formation can be conveyed at different levels of
granularity (for example, qualitatively or quantita-
tively). The appropriate choice of granularity for
68
a particular session is a factor that the system can
learn2.
Ranking Model, Classifiers and Features: The
ranking module orders candidate templates so
that the four most relevant ones appear in the
briefing draft. The ranking system consists of
a consensus-based classifier, based on individual
classifier models for each user in the training set.
The prediction from each classifier are combined
(averaged) to produce a final rank of each tem-
plate.
We used the Minorthird package (Cohen, 2004)
for modeling. Specifically we allowed the sys-
tem to experiment with eleven different learning
schemes and select the best one based on cross-
validation within the training corpus. The schemes
were Naive Bayes, Voted Perceptron, Support
Vector Machines, Ranking Perceptron, K Nearest
Neighbor, Decision Tree, AdaBoost, Passive Ag-
gressive learner, Maximum Entropy learner, Bal-
anced Winnow and Boosted Ranking learner.
The features3 used in the system are static or
dynamic. Static features reflect the properties of
the templates irrespective of the user?s activity
whereas the dynamic features are based on the
actual events that took place. We used the In-
formation Gain (IG) metric for feature selection,
experimenting with seven different cut-off values
All, 20, 15, 10, 7, 5, 4 for the total number of se-
lected features.
4 Experiments and Results
Experimental Setup: Two experimental condi-
tions were used to differentiate performance based
on knowledge engineering, designated MinusL
and performance based on learning, designated
PlusL.4
Email Trigger: In the simulated conference
replanning crisis, the briefing was triggered by
an email containing explicit information requests,
not known beforehand. To customize the brief-
ing according to the request, a natural language
processing module identified the categories of in-
formation requested. The details of the module
are beyond the scope of the current paper as it
2The details of template design process including sample
templates, categories of templates and details of aggregators
are presented in (Kumar et al, 2009)
3Detailed description of the features are mentioned in
(Kumar et al, 2009)
4The details of the experimental setup as part of the larger
cognitive assistant system are presented in (Kumar et al,
2009).
is external to our system; it took into account
the template categories we earlier identified. Fig-
ure 4 shows a sample briefing email stimulus.
The mapping from the sample email in the figure
to the categories is as follows: ?expected atten-
dance? - Property-Session; ?how many sessions
have been rescheduled?, ?how many still need to
be rescheduled?, ?any problems you see as you
try to reschedule? - Session-Reschedule; ?status
of food service (I am worried about the keynote
lunch)? - Catering Vendors.
Training: Eleven expert users5 were asked to
provide training by using the system then generat-
ing the end of session briefing using the BA GUI.
For this training phase, no item ranking was per-
formed by the system, i.e. all the templates were
populated by the aggregators and recommenda-
tions were random. The expert user was asked
to select the best possible four items and was fur-
ther asked to judge the usefulness of the remaining
items. The resulting training data consists of the
activity log, extracted features and the user-labeled
items. The trigger message for the training users
did not contain any specific information request.
Test: Subjects were recruited to use the crisis
management system in MinusL and PlusL condi-
tion, although they were not aware of the condition
of the system and they were not involved with the
project. There were 54 test runs in the MinusL
condition and 47 in the PlusL condition. Out of
these runs, 29 subjects in MinusL and 43 subjects
in PlusL wrote a briefing using the BA. We report
the evaluation scores for this latter set.
Evaluation: The base performance metric is
Recall, defined in terms of the briefing templates
recommended by the system compared to the tem-
plates ultimately selected by the user. We justify
this by noting that Recall can be directly linked to
the expected time savings for the users. We cal-
culate two variants of Recall: Category-based?
calculated by matching the categories of the BA
recommended templates and user selected ones
ignoring the granularity and Template-based?
calculated by matching the exact templates. The
first metric indicates whether the right category of
information was selected and the latter indicates
whether the information was presented at the ap-
propriate level of detail.
We also performed subjective human evaluation
5Members of the project from other groups who were
aware of the scenario and various system functionalities but
not the ML methods
69
using a panel of three judges. The judges assigned
scores (0-4) to each of the bullets based on the
coverage of the crisis, clarity and conciseness, ac-
curacy and the correct level of granularity. They
were advised about certain briefing-specific char-
acteristics (e.g. negative bullet items are useful
and hence should be rated favorably). They were
also asked to provide a global assessment of report
quality, and evaluate the coverage of the requests
in the briefing stimulus email message. This pro-
cedure was very similar to the one used as the basis
for template selection.
Experiment: The automatic evaluation met-
ric used for the trained system configuration is
the Template-based recall measure. To obtain
the final system configuration, we automatically
evaluate the system under the various combina-
tions of parameter settings with eleven different
learning schemes and seven different feature se-
lection threshold (as mentioned in previous sec-
tions). Thus a total of 77 different configurations
are tested. For each configuration, we do a eleven-
fold cross-validation between the 11 training users
i.e. we leave one user as the test user and consider
the remaining ten users as training users. We av-
erage the performance across the 11 test cases and
obtain the final score for the configuration. We
choose the configuration with the highest score as
the final trained system configuration. The learned
system configuration in the current test includes
Balanced Winnow (Littlestone, 1988) and top 7
features.
Results: We noticed that four users in PlusL
condition took more than 8 minutes to complete
the briefing when the median time taken by the
users in PlusL condition was 55 seconds, so we
did not include these users in our analysis in order
to maintain the homogeneity of the dataset. These
four data points were identified as extreme outliers
using a procedure suggested by (NIST, 2008)6.
There were no extreme outliers in MinusL condi-
tion.
Figure 3a shows the Recall values for the Mi-
nusL and PlusL conditions. The learning delta
i.e. the difference between the recall values of
PlusL and MinusL is 33% for Template-based re-
call and 21% for Category-based recall. These
differences are significant at the p < 0.001 level.
6Extreme outliers are defined as data points that are out-
side the range [Q1?3?IQ,Q3+3?IQ] in a box plot. Q1 is
lower quartile, Q3 is upper quartile and IQ is the difference
(Q3?Q1) is the interquartile range.
The statistical significance for the Template-based
metric, which was the metric used for select-
ing system parameters during the training phase,
shows that learning is effective in this case. Since
the email stimulus processing module extracts the
briefing categories from the email the Category-
based and Template-based recall is expected to be
high for the baseline MinusL case. In our test, the
email stimuli had 3 category requests and so the
Category-based recall of 0.77 and Template-based
recall of 0.67 in MinusL is not unexpected.
Figure 3b shows the Judges? panel scores for
the briefings in MinusL and PlusL condition. The
learning delta in this case is 3.6% which is also
statistically significant, at p < 0.05. The statistical
significance of the learning delta validates that the
briefings generated during PlusL conditions are
better than MinusL condition. The absolute differ-
ence in the qualitative briefing scores between the
two conditions is small because MinusL users can
select from all candidates, while the recommenda-
tions they receive are random. Consequently they
need to spend more time in finding the right items.
The average time taken for a briefing in MinusL
condition is about 83 seconds and 62 seconds in
PlusL (see Figure 3c). While the time difference
is high (34%) it is not statistically significant due
to high variance.
Four of the top 10 most frequently selected fea-
tures across users for this system are dynamic fea-
tures. This indicates that the learning model is
capturing the user?s world state and the recom-
mendations are related to the underlying events.
We believe this validates the process we used to
generate briefing reports from non-textual events.
5 Summary
The Briefing Assistant is not designed to learn
the generic attributes of good reports; rather it?s
meant to rapidly learn the attributes of good re-
ports within a particular domain and to accom-
modate specific information needs on a report-by-
report basis. We found that learned customiza-
tion produces reports that are judged to be of bet-
ter quality. We also found that a consensus-based
modeling approach, which incorporates informa-
tion from multiple users, yields the best perfor-
mance. We believe that our approach can be used
to create flexible summarization systems for a va-
riety of applications.
70
(a) (b) (c)
Figure 3: (a) Recall values for MinusL and PlusL conditions (b) Briefing scores from the judges? panel
for MinusL and PlusL conditions (c) Briefing time taken for MinusL and PlusL conditions.
Figure 4: Template categories corresponding to
the Briefing request email.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of HLT-NAACL.
Stephan Busemann. 2005. Ten years after: An update
on TG/2 (and friends). In Proceedings of European
Natural Language Generation Workshop.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net, 10th Jun 2009.
Naomi Daniel, Dragomir Radev, and Timothy Allison.
2003. Sub-event based multi-document summariza-
tion. In Proceedings of HLT-NAACL.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of
EMNLP.
Pablo A. Duboue. 2004. Indirect supervised learning
of content selection logic. In Proceedings of INLG.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Text Sum-
marization Branches Out: Proceedings of the ACL-
04 Workshop.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22(1):89?115.
Mohit Kumar, Dipanjan Das, and Alexander I. Rud-
nicky. 2008. Automatic extraction of briefing tem-
plates. In Proceedings of IJCNLP.
Mohit Kumar, Dipanjan Das, Sachin Agarwal, and
Alexander I. Rudnicky. 2009. Non-textual event
summarization by applying machine learning to
template-based language generation. Technical Re-
port CMU-LTI-09-012, Language Technologies In-
stitute, Carnegie Mellon University.
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and Chunfa
Yuan. 2006. Extractive summarization using inter-
and intra- event relevance. In Proceedings of ACL.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. Machine Learning, 2(4):285?318.
NIST. 2008. NIST/SEMATECH e-
handbook of statistical methods.
http://www.itl.nist.gov/div898/handbook/, 10th
Jun 2009.
Alice Oh and Howard Shrobe. 2008. Generating base-
ball summaries from multiple perspectives by re-
ordering content. In Proceedings of INLG.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(7-8):789?816.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470?500.
Kees van Deemter, Emiel Krahmer, and Mariet The-
une. 2005. Real versus template-based natural lan-
guage generation: A false opposition? Computa-
tional Linguistics, 31(1):15?24.
Mingli Wu. 2006. Investigations on event-based sum-
marization. In Proceedings of ACL.
71
