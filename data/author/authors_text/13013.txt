Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 186?189,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SZTERGAK : Feature Engineering for Keyphrase Extraction
G
?
abor Berend
Department of Informatics
University of Szeged
2.
?
Arp?ad t?er Szeged, H-6720, Hungary
berendg@inf.u-szeged.hu
Rich
?
ard Farkas
Hungarian Academy of Sciences
103. Tisza Lajos k?or?ut
Szeged, H-6720, Hungary
rfarkas@inf.u-szeged.hu
Abstract
Automatically assigning keyphrases to
documents has a great variety of applica-
tions. Here we focus on the keyphrase
extraction of scientific publications and
present a novel set of features for the su-
pervised learning of keyphraseness. Al-
though these features are intended for ex-
tracting keyphrases from scientific papers,
because of their generality and robust-
ness, they should have uses in other do-
mains as well. With the help of these fea-
tures SZTERGAK achieved top results on
the SemEval-2 shared task on Automatic
Keyphrase Extraction from Scientific Arti-
cles and exceeded its baseline by 10%.
1 Introduction
Keyphrases summarize the content of documents
with the most important phrases. They can be
valuable in many application areas, ranging from
information retrieval to topic detection. However,
since manually assigned keyphrases are rarely pro-
vided and creating them by hand would be costly
and time-consuming, their automatic generation
is of great interest nowadays. Recent state-of-
the-art systems treat this kind of task as a super-
vised learning task, in which phrases of a docu-
ment should be classified with respect to their key
phrase characteristics based on manually labeled
corpora and various feature values.
This paper focuses on the task of keyphrase ex-
traction from scientific papers and we shall intro-
duce new features that can significantly improve
the overall performance. Although the experimen-
tal results presented here are solely based on sci-
entific articles, due to the robustness and univer-
sality of the features, our approach is expected to
achieve good results when applied on other do-
mains as well.
2 Related work
In keyphrase extraction tasks, phrases are ex-
tracted from one document that are the most char-
acteristic of its content (Liu et al, 2009; Wit-
ten et al, 1999). In these approaches keyphrase
extraction is treated as a classification task, in
which certain n-grams of a specific document act
as keyphrase candidates, and the task is to classify
them as proper keyphrases or not.
While Frank et al (1999) exploited domain spe-
cific knowledge to improve the quality of auto-
matic tagging, others like Liu et al (2009) analyze
term co-occurence graphs. It was Nguyen and Kan
(2007) who dealt with the special characteristics of
scientific papers and introduced the state-of-the-
art feature set to keyphrase extraction tasks. Here
we will follow a similar approach and make sig-
nificant improvements by the introduction of novel
features.
3 The SZTERGAK system
The SZTERGAK framework treats the reproduc-
tion of reader-assigned keyphrases as a supervised
learning task. In our setting a restricted set of to-
ken sequences extracted from the documents was
used as classification instances. These instances
were ranked regarding to their posteriori proba-
bilities of the keyphrase class, estimated by a
Na??ve Bayes classifier. Finally, we chose the top-
15 candidates as keyphrases.
Our features can be grouped into four main cat-
egories: those that were calculated solely from
the surface characteristics of phrases, those that
took into account the document that contained a
keyphrase, those that were obtained from the given
document set and those that were based on exter-
nal sources of information.
186
3.1 Preprocessing
Since there are parts of a document (e.g. tables
or author affiliations) that can not really contribute
to the keyphrase extractor, several preprocessing
steps were carried out. Preprocessing included the
elimination of author affiliations and messy lines.
The determination of the full title of an article
would be useful, however, it is not straightforward
because of multi-line titles. To solve this prob-
lem, a web query was sent with the first line of
a document and its most likely title was chosen
by simply selecting the most frequently occurring
one among the top 10 responses provided by the
Google API. This title was added to the document,
and all the lines before the first occurrence of the
line Abstract were omitted.
Lines unlikely to contain valuable information
were also excluded from the documents. These
lines were identified according to statistical data
of their surface forms (e.g. the average and
the deviation of line lengths) and regular expres-
sions. Lastly, section and sentence boundaries
were found in a rule-based way, and the POS and
syntactic tagging (using the Stanford parser (Klein
and Manning, 2003)) of each sentence were car-
ried out.
When syntactically parsed sentences were ob-
tained, keyphrase aspirants were extracted. The 1
to 4-long token sequences that did not start or end
with a stopword and consisted only of POS-codes
of an adjective, a noun or a verb were de-
fined to be possible keyphrases (resulting in classi-
fication instances). Tokens of key phrase aspirants
were stemmed to store them in a uniform way, but
they were also appended by the POS-code of the
derived form, so that the same root forms were dis-
tinguished if they came from tokens having differ-
ent POS-codes, like there shown in Table 1.
Textual Appearance Canonical form
regulations regul nns
Regulation regul nn
regulates regul vbz
regulated regul vbn
Table 1: Standardization of document terms.
3.2 The extended feature set
The features characterizing the extracted
keyphrase aspirants can be grouped into four
main types, namely phrase-, document-, corpus-
level and external knowledge-based features.
Below we will describe the different types of
features as well as those of KEA (Witten et al,
1999) which are cited as default features by most
of the literature dealing with keyphrase extraction.
3.2.1 Standard features
Features belonging to this set contain those of
KEA, namely Tf-idf and the first occurrence.
The Tf-idf feature assigns the tf-idf metric to
each keyphrase aspirant.
The first occurrence feature contains the rela-
tive first position for each keyphrase aspirant. The
feature value was obtained by dividing the abso-
lute first token position of a phrase by the number
of tokens of the document in question.
3.2.2 Phrase-level features
Features belonging to this group were calcu-
lated solely based on the keyphrase aspirants
themselves. Such features are able to get the
general characteristics of phrases functioning as
keyphrases.
Phrase length feature contains the number of
tokens a keyphrase aspirant consists of.
POS feature is a nominal one that stores
the POS-code sequence of each keyphrase aspi-
rant. (For example, for the phrase full JJ
space NN its value was JJ NN.)
Suffix feature is a binary feature that stores
information about whether the original form of
a keyphrase aspirant finished with some specific
ending according to a subset of the Michigan Suf-
ficiency Exams? Suffix List.
1
3.2.3 Document-level features
Since keyphrases should summarize the particular
document they represent, and phrase-level features
introduced above were independent of their con-
text, document-level features were also invented.
Acronymity feature functions as a binary fea-
ture that is assigned a true value iff a phrase is
likely to be an extended form of an acronym in the
same document. A phrase is treated as an extended
form of an acronym if it starts with the same letter
as the acronym present in its document and it also
contains all the letters of the acronym in the very
same order as they occur in the acronym.
PMI feature provides a measure of the mul-
tiword expression nature of multi-token phrases,
1
http://www.michigan-proficiency-exams.com/suffix-
list.html
187
and it is defined in Eq. (1), where p(t
i
) is the
document-level probability of the occurrence of
ith token in the phrase. This feature value is a gen-
eralized form of pointwise mutual information for
phrases with an arbitrary number of tokens.
pmi(t
1
, t
2
, ..., t
n
) =
log(
p(t
1
,t
2
,...,t
n
)
p(t
1
)?p(t
2
)?...?p(t
n
)
)
log(p(t
1
, t
2
, ..., t
n
))
n?1
(1)
Syntactic feature values refer to the average
minimal normalized depth of the NP-rooted parse
subtrees that contain a given keyphrase aspirant at
the leaf nodes in a given document.
3.2.4 Corpus-level features
Corpus-level features are used to determine the
relative importance of keyphrase aspirants based
on a comparison of corpus-level and document-
level frequencies.
The sf-isf feature was created to deal with logi-
cal positions of keyphrases and the formula shown
in Eq. (2) resembles that of tf-idf scores (hence
its name, i.e. Section Frequency-Inverted Section
Frequency). This feature value favors keyphrase
aspirants k that are included in several sections of
document d (sf ), but are present in a relatively
small number of sections in the overall corpus
(isf ). Phrases with higher sf-isf scores for a given
document are those that are more relevant with re-
spect to that document.
sfisf(k, d) = sf(k, d) ? isf(k) (2)
Keyphraseness feature is a binary one which
has a true value iff a phrase is one of the 785 dif-
ferent author-assigned keyphrases provided in the
training and test corpora.
3.2.5 External knowledge-based features
Apart from relying on the given corpus, further en-
hancements in performance can be obtained by re-
lying on external knowledge sources.
Wikipedia-feature is assigned a true value
for keyphrase aspirants for which there exists a
Wikipedia article with the same title. Preliminary
experiments showed that this feature is noisy, thus
we also investigated a relaxed version of it, where
occurrences of Wikipedia article titles were looked
for only in the title and abstract of a paper.
Besides using Wikipedia for feature calculation,
it was also utilized to retrieve semantic orienta-
tions of phrases. Making use of redirect links of
Wikipedia, the semantic relation of synonymity
Feature combinations F-score
Standard features (SF) 14.57
SF + phrase length feature 20.93
SF + POS feature 19.60
SF + suffix feature 16.35
SF + acronymity feature 16.87
SF + PMI feature 15.68
SF + syntactic feature 14.20
SF + sf-isf feature 14.79
SF + keyphraseness feature 15.17
SF + Wikipedia feature - full paper 14.37
SF + Wikipedia feature - abstract 16.50
SF + Wikipedia redirect 14.50
Shared Task best baseline 12.87
All features 23.82
All features - keyphraseness excluded 22.11
Table 2: Results obtained with different features.
can be exploited. For example, as there exists a
redirection between Wikipedia articles XML and
Extensible Markup Language, it may be
assumed that these phrases mean the same. For
this reason during the training phase we treated
a phrase equivalent to its redirected version, i.e.
if there is a keyphrase aspirant that is not as-
signed in the gold-standard reader annotation but
the Wikipedia article with the same title has a redi-
rection to such a phrase that is present among pos-
itive keyphrase instances of a particular document,
the original phrase can be treated as a positive in-
stance as well. In this way the ratio of positive ex-
amples could be increased from 0.99% to 1.14%.
4 Results and discussion
The training and test sets of the shared task (Kim
et al, 2010) consisted of 144 and 100 scien-
tific publications from the ACL repository, respec-
tively. Since the primary evaluation of the shared
task was based on the top-15 ranked automatic
keyphrases compared to the keyphrases assigned
by the readers of the articles, these results are re-
ported here. The evaluation results can be seen in
Table 2 where the individual effect of each feature
is given in combination with the standard features.
It is interesting to note the improvement ob-
tained by extending standard features with the
simple feature of phrase length. This indicates
that though the basic features were quite good,
they did not take into account the point that reader
188
keyphrases are likely to consist of several words.
Morphological features, such as POS or suffix
features were also among the top-performing ones,
which seems to show that most of the keyphrases
tend to have some common structure. In contrast,
the syntactic feature made some decrease in the
performance when it was combined just with the
standard ones. This can be due to the fact that the
input data were quite noisy, i.e. some inconsisten-
cies arose in the data during the pdf to text con-
version of articles, which made it difficult to parse
some sentences correctly.
It was also interesting to see that Wikipedia fea-
ture did not improve the result when it was applied
to the whole document. However, our previous ex-
periences on keyphrase extraction from scientific
abstracts showed that this feature can be very use-
ful. Hence, we relaxed the feature to handle occur-
rences just from the abstract. This modification of
the feature yielded a 14.8% improvement in the F-
measure. A possible explanation for this is that
Wikipedia has articles of very common phrases
(such as Calculation or Result) and the dis-
tribution of such non-keyphrase terms is higher in
the body of the articles than in abstracts.
The last row of Table 2 contains the result
achieved by the complete feature set excluding
keyphraseness. As keyphraseness exploits author-
assigned keyphrases and ? to the best of our
knowledge ? other participants of the shared task
did not utilize author-assigned keyphrases, this re-
sult is present in the final ranking of the shared
task systems. However, we believe that if the task
is to extract keyphrases from an article to gain se-
mantic meta-data for an NLP application (e.g. for
information retrieval or summarization), author-
assigned keyphrases are often present and can be
very useful. This latter statement was proved by
one of our experiments where we used the au-
thor keyphrases assigned to the document itself as
a binary feature (instead of using the pool of all
keyphrases). This feature set could achieve an F-
score of 27.44 on the evaluation set and we believe
that this should be the complete feature set in a
real-world semantic indexing application.
5 Conclusions
In this paper we introduced a wide set of new fea-
tures that are able to enhance the overall perfor-
mance of supervised keyphrase extraction applica-
tions. Our features include those calculated simply
on surface forms of keyphrase aspirants, those that
make use of the document- and corpus-level envi-
ronment of phrases and those that rely on exter-
nal knowledge. Although features were designed
to the specific task of extracting keyphrases from
scientific papers, due to their generality it is highly
assumable that they can be successfully utilized on
different domains as well.
The features we selected in SZTERGAK per-
formed well enough to actually achieve the
third place on the shared task by excluding the
keyphraseness feature and would be the first by
using any author-assigned keyphrase-based fea-
ture. It is also worth emphasizing that we think
that there are many possibilities to further extend
the feature set (e.g. with features that take the
semantic relatedness among keyphrase aspirants
into account) and significant improvement could
be achievable.
Acknowledgement
The authors would like to thank the annotators of
the shared task for the datasets used in the shared
task. This work was supported in part by the
NKTH grant (project codename TEXTREND).
References
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ing of 16th IJCAI, pages 668?673.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 task 5 : Au-
tomatic keyphrase extraction from scientific articles.
In Proc. of the 5th SIGLEX Workshop on Semantic
Evaluation.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 423?430.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on EMNLP.
Thuy Dung Nguyen and Minyen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proc. of International Conference on Asian Digital
Libraries (ICADL 07), pages 317?326.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction. In ACM
DL, pages 254?255.
189
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 549?553, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SZTE-NLP: Sentiment Detection on Twitter Messages
Viktor Hangya, Ga?bor Berend, Richa?rd Farkas
University of Szeged
Department of Informatics
hangyav@gmail.com, {berendg,rfarkas}@inf.u-szeged.hu
Abstract
In this paper we introduce our contribution
to the SemEval-2013 Task 2 on ?Sentiment
Analysis in Twitter?. We participated in ?task
B?, where the objective was to build mod-
els which classify tweets into three classes
(positive, negative or neutral) by their con-
tents. To solve this problem we basically fol-
lowed the supervised learning approach and
proposed several domain (i.e. microblog) spe-
cific improvements including text preprocess-
ing and feature engineering. Beyond the su-
pervised setting we also introduce some early
results employing a huge, automatically anno-
tated tweet dataset.
1 Introduction
In the past few years, the popularity of social me-
dia has increased. Many studies have been made in
the area (Jansen et al, 2009; O?Connor et al, 2010;
Bifet and Frank, 2010; Sang and Bos, 2012). Peo-
ple post messages on a variety of topics, for example
products, political issues, etc. Thus a big amount of
user generated data is created day-by-day. The man-
ual processing of this data is impossible, therefore
automatic procedures are needed.
In this paper we introduce an approach which is
able to assign sentiment labels to Twitter messages.
More precisely, it classifies tweets into positive, neg-
ative or neutral polarity classes. The system partici-
pated in the SemEval-2013 Task 2: Sentiment Anal-
ysis in Twitter, Task?B Message Polarity Classifica-
tion (Wilson et al, 2013). In our approach we used
a unigram based supervised model because it has
been shown that it works well on short messages like
tweets (Jiang et al, 2011; Barbosa and Feng, 2010;
Agarwal et al, 2011; Liu, 2010). We reduced the
size of the dictionary by normalizing the messages
and by stop word filtering. We also explored novel
features which gave us information on the polarity of
a tweet, for example we made use of the acronyms
in messages.
In the ?constrained? track of Task?B we used the
given training and development data only. For the
?unconstrained? track we downloaded tweets using
the Twitter Streaming API1 and automatically anno-
tated them. We present some preliminary results on
exploiting this huge dataset for training our classi-
fier.
2 Approach
At the beginning of our experiments we used a
unigram-based supervised model. Later on, we re-
alized that the size of our dictionary is huge, so
in the normalization phase we tried to reduce the
number of words in it. We investigated novel fea-
tures which contain information on the polarity of
the messages. Using these features we were able to
improve the precision of our classifier. For imple-
mentation we used the MALLET toolkit, which is a
Java-based package for natural language processing
(McCallum, 2002).
2.1 Normalization
One reason for the unusually big dictionary size is
that it contains one word in many forms, for exam-
1https://dev.twitter.com/docs/
streaming-apis/streams/public
549
ple in upper and lower case, in a misspelled form,
with character repetition, etc. On the other hand, it
contained numerous special annotations which are
typical for blogging, such as Twitter-specific anno-
tations, URL?s, smileys, etc. Keeping these in mind
we made the following normalization steps:
? First, in order to get rid of the multiple forms
of a single word we converted them into lower
case form then we stemmed them. For this pur-
pose we used the Porter Stemming Algorithm.
? We replaced the @ and # Twitter-specific tags
with the [USER] and [TAG] notations, respec-
tively. Besides we converted every URL in the
messages to the [URL] notation.
? Smileys in messages play an important role
in polarity classification. For this reason we
grouped them into positive and negative smi-
ley classes. We considered :), :-),: ), :D, =), ;),
; ), (: and :(, :-(, : (, ):, ) : smileys as positive
and negative, respectively.
? Since numbers do not contain information re-
garding a message polarity, we converted them
as well to the [NUMBER] form. In ad-
dition, we replaced the question and excla-
mation marks with the [QUESTION MARK]
and [EXCLAMATION MARK] notations. Af-
ter this we removed the unnecessary char-
acters ?"#$%&()*+,./:;<=>\?{}?, with
the exception that we removed the ? character
only if a word started or ended with it.
? In the case of words which contained character
repetitions ? more precisely those which con-
tained the same character at least three times
in a row ?, we reduced the length of this se-
quence to three. For instance, in the case
of the word yeeeeahhhhhhh we got the form
yeeeahhh. This way we unified these charac-
ter repetitions, but we did not loose this extra
information.
? Finally we made a stop word filtering in order
to get rid of the undesired words. To identify
these words we did not use a stop word dictio-
nary, rather we filtered out those words which
appeared too frequently in the training corpus.
We have chosen this method because we would
like to automatically detect those words which
are not relevant in the classification.
Before the normalization step, the dictionary con-
tained approximately 41, 000 words. After the above
introduced steps we managed to reduce the size of
the dictionary to 15, 000 words.
2.2 Features
After normalizing Twitter messages, we searched
for special features which characterize the polarity
of the tweets. One such feature is the polarity of
each word in a message. To determine the polarity
of a word, we used the SentiWordNet sentiment lex-
icon (Baccianella et al, 2010). In this lexicon, a pos-
itive, negative and an objective real value belong to
each word, which describes the polarity of the given
word. We consider a word as positive if the related
positive value is greater than 0.3, we consider it as
negative if the related negative value is greater than
0.2 and we consider it as objective if the related ob-
jective value is greater than 0.8. The threshold of the
objective value is high because most words are ob-
jective in this lexicon. After calculating the polarity
of each word we created three new features for each
tweet which are the number of positive, negative and
objective words, respectively. We also checked if a
negation word precedes a positive or negative word
and if so we inverted its polarity.
We also tried to group acronyms by their polarity.
For this purpose we used an acronym lexicon which
can be found on the www.internetslang.com
website. For each acronym we used the polarity of
each word in the acronym?s description and we de-
termined the polarity of the acronym by calculat-
ing the rate of positive and negative words in the
description. This way we created two new fea-
tures which are the number of positive and negative
acronyms in a given message.
Our intuition was that people like to use character
repetitions in their words for expressing their happi-
ness or sadness. Besides normalizing these tokens
(see Section 2.1), we created a new feature as well,
which represents the number of this kind of words
in a tweet.
Beyond character repetitions people like to write
words or a part of the text in upper case in order to
550
call the reader?s attention. Because of this we cre-
ated another feature which is the number of upper
case words in the given text.
3 Collected Data
In order to achieve an appropriate precision with su-
pervised methods we need a big amount of training
data. Creating this database manually is a hard and
time-consuming task. In many cases it is hard even
for humans to determine the polarity of a message,
for instance:
After a whole 5 hours away from work, I
get to go back again, I?m so lucky!
In the above tweet we cannot decide precisely the
polarity because the writer can be serious or just sar-
castic.
In order to increase the size of the training data
we acquired additional tweets, which we used in
the unconstrained run for Task?B. We created an ap-
plication which downloads tweets using the Twitter
Streaming API. The API supports language filter-
ing, which was used to get rid of non-English mes-
sages. Our manual investigations of the downloaded
tweets revealed, however, that this filter allows a big
amount of non-English tweets, which is probably
due to the fact that some Twitter users did not set
their language. We used Twitter4J2 API (which is
a Java library for the Twitter API) for downloading
these tweets. We automatically annotated the down-
loaded tweets using the Twitter Sentiment3 web ap-
plication, similar to Barbosa and Feng (2010) but
we used only one annotator. This web application
also supports language detection, but after this extra
filtration, our dataset still contained a considerable
amount of non-English messages. After 16 hours
of data collection we got 350, 000 annotated tweets,
where the distribution of neutral, positive and neg-
ative classes was approximately 60%, 20%, 20%,
respectively. For further testing purposes we have
chosen 10, 000 tweets from each class.
4 Results
We report results on the two official test sets of the
shared task. The ?twitter? test set consists of 3, 813
2http://twitter4j.org
3http://www.sentiment140.com
tweets while the ?sms? set consists of 2, 094 sms
messages. We evaluated both test databases in two
ways, in the so-called constrained run we only used
the official training database, while in the uncon-
strained run we also used a part of the additional
data, which was mentioned in the 3 section. The
official training database contained 4, 028 positive,
1, 655 negative and 3, 821 neutral tweets while for
the unconstrained run we used an additional 10, 000
tweets from each class. This way in each phase we
got four kinds of runs, which were evaluated with
the Na??ve Bayes and Maximum Entropy classifiers.
In Table 1 the evaluation of the unigram-based
model with the Na??ve Bayes learner can be seen.
The table contains the F-scores for the positive, neg-
ative and neutral labels for each of the four runs.
The avg column contains the average F-score for the
positive and negative labels, which was the official
evaluation metric for SemEval-2013 Task 2. We got
the best scores for the neutral label whilst the worst
scores are obtained for the negative label, which is
due to the fact that there were much less negative
instances in the training database. It can be seen
that the F-scores for the unconstrained run are better
both for the tweet and sms test databases. For the
unigram-based model the F-scores are higher when
we used the Maximum Entropy model (see Table 2).
pos neg neut avg
twitter-constrained 0.59 0.09 0.65 0.34
twitter-unconstrained 0.60 0.17 0.65 0.38
sms-constrained 0.46 0.16 0.63 0.31
sms-unconstrained 0.47 0.38 0.53 0.42
Table 1: Unigram-based model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.60 0.33 0.67 0.46
twitter-unconstrained 0.60 0.40 0.66 0.50
sms-constrained 0.47 0.31 0.69 0.39
sms-unconstrained 0.52 0.47 0.66 0.49
Table 2: Unigram-based model, Maximum Entropy
learner
In Tables 3 and 4 the evaluation results can be
seen for the normalized model. The normalization
551
step increased the precision for both learning al-
gorithms and the Maximum Entropy learner is still
better than Na??ve Bayes. Besides this we noticed
that for both learners in the case of the tweet test
database, the unconstrained run had lower scores
than the constrained whilst in the case of the sms
test database this phenomenon did not appear.
pos neg neut avg
twitter-constrained 0.65 0.32 0.67 0.48
twitter-unconstrained 0.62 0.21 0.63 0.41
sms-constrained 0.56 0.27 0.72 0.41
sms-unconstrained 0.52 0.35 0.66 0.43
Table 3: Normalized model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.66 0.40 0.68 0.53
twitter-unconstrained 0.61 0.42 0.64 0.51
sms-constrained 0.61 0.38 0.77 0.49
sms-unconstrained 0.57 0.47 0.72 0.52
Table 4: Normalized model, Maximum Entropy
learner
The evaluation results of the feature-based model
can be seen in Tables 5 and 6. In the case of the
Na??ve Bayes learner, the features did not increase the
F-scores, only for the sms-unconstrained run. For
the other runs the achieved scores decreased. In the
case of the Maximum Entropy learner the features
increased the F-scores, slightly for the constrained
runs and a bit more for the unconstrained runs.
From this analysis we can conclude that the nor-
malization of the messages yielded a considerable
increase in the F-scores. We discussed above that
this step also significantly reduced the size of the
dictionary. The features increased the precision too,
especially for the unconstrained run. This means
that these features represent properties which are
useful for those training data which are not from the
same corpus as the test messages. We compared two
machine learning algorithms and from the results we
concluded that the Maximum Entropy learner per-
forms better than the Na??ve Bayes on this task. Our
experiments also showed that the external, automat-
ically labeled training database helped only in the
classification of sms messages. This is due to the
fact that the smses and our external database are
from a different distribution than the official tweet
database.
pos neg neut avg
twitter-constrained 0.65 0.32 0.67 0.48
twitter-unconstrained 0.62 0.17 0.79 0.39
sms-constrained 0.56 0.38 0.74 0.47
sms-unconstrained 0.54 0.29 0.70 0.41
Table 5: Feature-based model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.66 0.41 0.69 0.54
twitter-unconstrained 0.63 0.43 0.65 0.53
sms-constrained 0.62 0.39 0.79 0.50
sms-unconstrained 0.61 0.49 0.75 0.55
Table 6: Feature-based model, Maximum Entropy
learner
5 Conclusions and Future Work
Recently, sentiment analysis on Twitter messages
has gained a lot of attention due to the huge amount
of Twitter users and their tweets. In this paper we ex-
amined different methods for classifying Twitter and
sms messages. We proposed special features which
characterize the polarity of the messages and we
concluded that due to the informality (slang, spelling
mistakes, etc.) of the messages it is crucial to nor-
malize them properly.
In the future, we plan to investigate the utility of
relations between Twitter users and between their
tweets and we are interested in topic-dependent sen-
timent analysis.
Acknowledgments
This work was supported in part by the Euro-
pean Union and the European Social Fund through
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment Analysis
552
of Twitter Data. In Proceedings of the Workshop on
Language in Social Media (LSM 2011), pages 30?38,
June.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Poster volume, Coling 2010, pages 36?44,
August.
Albert Bifet and Eibe Frank. 2010. Sentiment Knowl-
edge Discovery in Twitter Streaming Data.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter Power: Tweets as Electronic
Word of Mouth. In Journal of the American society
for information science and technology, pages 2169?
2188.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter Sentiment
Classification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 151?160, June.
Bing Liu. 2010. Sentiment Analysis and Subjectivity. In
N. Indurkhya and F. J. Damerau, editors, Handbook of
Natural Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
International AAAI Conference on Weblogs and Social
Media, May.
Erik Tjong Kim Sang and Johan Bos. 2012. Predicting
the 2011 Dutch Senate Election Results with Twitter.
In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 53?60, April.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 Task 2: Sentiment Analysis in Twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
553
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 116?121,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Detecting noun compounds and light verb constructions: a contrastive study
Veronika Vincze1, Istva?n Nagy T.2 and Ga?bor Berend2
1Hungarian Academy of Sciences, Research Group on Artificial Intelligence
vinczev@inf.u-szeged.hu
2Department of Informatics, University of Szeged
{nistvan,berendg}@inf.u-szeged.hu
Abstract
In this paper, we describe our methods to
detect noun compounds and light verb con-
structions in running texts. For noun com-
pounds, dictionary-based methods and POS-
tagging seem to contribute most to the per-
formance of the system whereas for light
verb constructions, the combination of POS-
tagging, syntactic information and restrictions
on the nominal and verbal component yield
the best result. However, focusing on deverbal
nouns proves to be beneficial for both types
of MWEs. The effect of syntax is negligible
on noun compound detection whereas it is un-
ambiguously helpful for identifying light verb
constructions.
1 Introduction
Multiword expressions are lexical items that can be
decomposed into single words and display idiosyn-
cratic features (Sag et al, 2002; Calzolari et al,
2002; Kim, 2008). They are frequent in language
use and they usually exhibit unique and idiosyn-
cratic behavior, thus, they often pose a problem to
NLP systems. A compound is a lexical unit that
consists of two or more elements that exist on their
own. Light verb constructions are verb and noun
combinations in which the verb has lost its meaning
to some degree and the noun is used in one of its
original senses (e.g. have a walk or give advice).
In this work, we aim at identifying nominal com-
pounds and light verb constructions by using rule-
based methods. Noun compounds belong to the
most frequent MWE-classes (in the Wikipedia cor-
pus we developed for evaluation (see 3.2), about
75% of the annotated multiword expressions were
noun compounds) and they are productive, i.e. new
nominal compounds are being formed in language
use all the time, which yields that they cannot be
listed exhaustively in a dictionary (as opposed to
e.g. prepositional compounds). Their inner syntactic
structure varies: they can contain nouns, adjectives
and prepositions as well.
Light verb constructions are semi-productive, that
is, new light verb constructions might enter the lan-
guage following some patterns (e.g. give a Skype
call on the basis of give a call). On the other hand,
they are less frequent in language use (only 9.5% of
multiword expressions were light verb constructions
in the Wikipedia database) and they are syntactically
flexible, that is, they can manifest in various forms:
the verb can be inflected, the noun can occur in its
plural form and the noun can be modified. The nom-
inal and the verbal component may not even be ad-
jacent in e.g. passive sentences.
Our goal being to compare how different ap-
proaches perform in the case of the different types
of multiword expressions, we have chosen these two
types of MWEs that are dissimilar in several aspects.
2 Related work
There are several applications developed for identi-
fying MWEs, which can be classified according to
the methods they make use of (Piao et al, 2003).
First, statistical models rely on word frequencies,
co-occurrence data and contextual information in
deciding whether a bigram or trigram (or even an
n-gram) of words can be labeled as a multiword ex-
pression or not. Such systems are used for several
116
languages and several types of multiword expres-
sions, see e.g. Bouma (2010). The advantage of
statistical systems is that they can be easily adapted
to other languages and other types of multiword ex-
pressions, however, they are not able to identify rare
multiword expressions (as Piao et al (2003) empha-
size, 68% of multiword expressions occur at most
twice in their corpus).
Some hybrid systems make use of both statisti-
cal and linguistic information as well, that is, rules
based on syntactic or semantic regularities are also
incorporated into the system (Evert and Kermes,
2003; Bannard, 2007; Cook et al, 2007; Al-Haj and
Wintner, 2010). This results in better coverage of
multiword expressions. On the other hand, these
methods are highly language-dependent because of
the amount of linguistic rules encoded, thus, it re-
quires much effort to adapt them to different lan-
guages or even to different types of multiword ex-
pressions. However, the combination of different
methods may improve the performance of MWE-
extracting systems (Pecina, 2010).
Several features are used in identifying multi-
word expressions, which are applicable to differ-
ent types of multiword expressions to various de-
grees. Co-occurrence statistics and POS-tags seem
to be useful for all types of multiword expressions,
for instance the tool mwetoolkit (Ramisch et al,
2010a) makes use of such features, which is illus-
trated through the example of identifying English
compound nouns (Ramisch et al, 2010b).
Caseli et al (2010) developed an alignment-based
method for extracting multiword expressions from
parallel corpora. This method is also applied to
the pediatrics domain (Caseli et al, 2009). Zarrie?
and Kuhn (2009) argue that multiword expressions
can be reliably detected in parallel corpora by using
dependency-parsed, word-aligned sentences. Sinha
(2009) detects Hindi complex predicates (i.e. a com-
bination of a light verb and a noun, a verb or an ad-
jective) in a Hindi?English parallel corpus by iden-
tifying a mismatch of the Hindi light verb meaning
in the aligned English sentence. Van de Cruys and
Moiro?n (2007) describe a semantic-based method
for identifying verb-preposition-noun combinations
in Dutch, which relies on selectional preferences for
both the noun and the verb. Cook et al (2007) dif-
ferentiate between literal and idiomatic usages of
verb and noun constructions in English. They make
use of syntactic fixedness of idioms when develop-
ing their unsupervised method. Bannard (2007) also
seeks to identify verb and noun constructions in En-
glish on the basis of syntactic fixedness. Samardz?ic?
and Merlo (2010) analyze English and German light
verb constructions in parallel corpora. They found
that linguistic features (i.e. the degree of composi-
tionality) and the frequency of the construction both
have an effect on aligning the constructions.
3 Experiments
In order to identify multiword expressions, simple
methods are worth examining, which can serve as a
basis for implementing more complex systems and
can be used as features in machine learning set-
tings. Our aim being to compare the effect of dif-
ferent methods on the identification of noun com-
pounds and light verb constructions, we considered
it important to develop methods for both MWE types
that make use of their characteristics and to adapt
those methods to the other type of MWE ? in this
way, the efficacy and the MWE-(in)dependence of
the methods can be empirically evaluated, which can
later have impact on developing statistical MWE-
detectors.
Earlier studies on the detection of light verb con-
structions generally take syntactic information as a
starting point (Cook et al, 2007; Bannard, 2007;
Tan et al, 2006), that is, their goal is to classify verb
+ object constructions selected on the basis of syn-
tactic pattern as literal or idiomatic. However, we
do not aim at classifying LVC candidates filtered by
syntactic patterns but at identifying them in running
text without assuming that syntactic information is
necessarily available. In our investigations, we will
pay distinctive attention to the added value of syn-
tactic features on the system?s performance.
3.1 Methods for MWE identification
For identifying noun compounds, we made use of a
list constructed from the English Wikipedia. Lower-
case n-grams which occurred as links were collected
from Wikipedia articles and the list was automati-
cally filtered in order to delete non-English terms,
named entities and non-nominal compounds etc. In
the case of the method ?Match?, a noun compound
117
candidate was marked if it occurred in the list. The
second method we applied for noun compounds in-
volved the merge of two possible noun compounds:
if A B and B C both occurred in the list, A B C was
also accepted as a noun compound (?Merge?). Since
the methodology of dictionary building was not ap-
plicable for collecting light verb constructions (i.e.
they do not function as links in Wikipedia), we could
not apply these two methods to them.
In the case of ?POS-rules?, a noun compound
candidate was marked if it occurred in the list and
its POS-tag sequence matched one of the previ-
ously defined patterns (e.g. JJ (NN|NNS)). For
light verb constructions, the POS-rule method meant
that each n-gram for which the pre-defined patterns
(e.g. VB.? (NN|NNS)) could be applied was ac-
cepted as light verb constructions. For POS-tagging,
we used the Stanford POS Tagger (Toutanova and
Manning, 2000). Since the methods to follow rely
on morphological information (i.e. it is required
to know which element is a noun), matching the
POS-rules is a prerequisite to apply those methods
to identify MWEs.
The ?Suffix? method exploited the fact that many
nominal components in light verb constructions are
derived from verbs. Thus, in this case only construc-
tions that contained nouns ending in certain deriva-
tional suffixes were allowed and for nominal com-
pounds the last noun had to have this ending.
The ?Most frequent? (MF) method relied on the
fact that the most common verbs function typically
as light verbs (e.g. do, make, take, have etc.) Thus,
the 15 most frequent verbs typical of light verb con-
structions were collected and constructions where
the stem of the verbal component was among those
of the most frequent ones were accepted. As for
noun compounds, the 15 most frequent nouns in En-
glish were similarly collected1 and the lemma of the
last member of the possible compound had to be
among them.
The ?Stem? method pays attention to the stem of
the noun. In the case of light verb constructions, the
nominal component is typically one that is derived
from a verbal stem (make a decision) or coincides
with a verb (have a walk). In this case, we accepted
1as listed at http://en.wikipedia.org/wiki/
Most\_common\_words\_in\_English
only candidates that had the nominal component /
the last noun whose stem was of verbal nature, i.e.
coincided with a stem of a verb.
Syntactic information can also be exploited in
identifying MWEs. Typically, the syntactic relation
between the verb and the nominal component in a
light verb construction is dobj or prep ? using
Stanford parser (Klein and Manning, 2003)). The re-
lation between the members of a typical noun com-
pound is nn or amod in attributive constructions.
The ?Syntax? method accepts candidates among
whose members these syntactic relations hold.
We also combined the above methods to identify
noun compounds and light verb constructions in our
databases (the union of candidates yielded by the
methods is denoted by ? while the intersection is
denoted by ? in the respective tables).
3.2 Results
For the evaluation of our models, we developed a
corpus of 50 Wikipedia articles, in which several
types of multiword expressions (including nomi-
nal compounds and light verb constructions) and
Named Entities were marked. The database contains
2929 occurrences of nominal compounds and 368
occurrences of light verb constructions and can be
downloaded under the Creative Commons licence at
http://www.inf.u-szeged.hu/rgai/mwe.
Table 1 shows the results of our experiments.
Methods were evaluated on the token level, i.e. each
occurrence of a light verb construction had to be
identified in text. It can be seen that the best result
for noun compound identification can be obtained
if the three dictionary-based methods are combined.
We also evaluated the method of POS-rules without
taking into account dictionary matches (POS-rules
w/o dic), which result serves as the baseline for com-
paring the effect of LVC-specific methods on noun
compound detection.
As can be seen, by adding any of the LVC-specific
features, the performance of the system declines, i.e.
none of them can beat the baseline. While the fea-
ture ?Stem? (and its combinations) improve preci-
sion, recall severely falls back: especially ?Most fre-
quent noun? (MFN) has an extremely poor effect on
it. This was expected since the lexical constraint
on the last part of the compound heavily restricts
the scope of the noun compounds available. On the
118
other hand, the 15 most frequent nouns in English
are not derived from verbs hence they do not end in
any of the pre-defined suffixes, thus, the intersection
of the features ?MFN? and ?Suffix? does not yield
any noun compound (the intersection of all the three
methods also behaves similarly). It must be men-
tioned, however, that the union of all features yields
the best recall as expected and the best F-measure
can be achieved by the union of ?Suffix? and ?Stem?.
The effect of adding syntactic rules to the system
is not unequivocal. In many cases, the improvement
is marginal (it does not exceed 1% except for the
POS-rules w/o dic method) or the performance even
degrades. The latter is most obvious in the case of
the combination of dictionary-based rules, which is
mainly caused by the decline in recall, however, pre-
cision improves. The overall decline in F-score may
thus be related to possible parsing errors.
In the case of light verb constructions, the recall
of the baseline (POS-rules) is high, however, its pre-
cision is low (i.e. not all of the candidates defined
by the POS patterns are light verb constructions).
The ?Most frequent verb? (MFV) feature proves to
be the most useful: the verbal component of the light
verb construction is lexically much more restricted
than the noun, which is exploited by this feature.
The other two features put some constraints on the
nominal component, which is typically of verbal na-
ture in light verb constructions: ?Suffix? simply re-
quires the noun to end in a given n-gram (without ex-
ploiting further grammatical information) whereas
?Stem? allows nouns derived from verbs. When
combining a verbal and a nominal feature, union re-
sults in high recall (the combinations typical verb +
non-deverbal noun or atypical verb + deverbal noun
are also found) while intersection yields high preci-
sion (typical verb + deverbal noun combinations are
found only).
We also evaluated the performance of the ?Syn-
tax? method without directly exploiting POS-rules.
Results are shown in Table 2. It is revealed that
the feature dobj is much more effective in identify-
ing light verb constructions than the feature prep,
on the other hand, dobj itself outperforms POS-
rules. If we combine the dobj feature with the
best LVC-specific feature (namely, MFV), we can
achieve an F-measure of 26.46%. The feature dobj
can achieve a recall of 59.51%, which suggests
Method P R F
Dobj 10.39 59.51 17.69
Prep 0.46 7.34 0.86
Dobj ? Prep 2.09 38.36 3.97
Dobj ? MFV 31.46 22.83 26.46
Prep ? MFV 3.24 5.12 4.06
Dobj ? Prep ? MFV 8.78 19.02 12.02
Table 2: Results of syntactic methods for light verb con-
structions in terms of precision (P), recall (R) and F-
measure (F). Dobj: verb + object pairs, Prep: verb +
prepositional complement pairs, MFV: the verb is among
the 15 most frequent light verbs.
that about 40% of the nominal components in our
database are not objects of the light verb. Thus, ap-
proaches that focus on only verb-object pairs (Cook
et al, 2007; Bannard, 2007; Tan et al, 2006) fail to
identify a considerable part of light verb construc-
tions found in texts.
The added value of syntax was also investigated
for LVC detection as well. As the results show, syn-
tax clearly helps in identifying LVCs ? its overall
effect is to add up to 4% to the F-score. The best
result, again, is yielded by the MFV method, which
is about 30% above the baseline.
4 Discussion
When contrasting results achieved for light verb
constructions and noun compounds, it is revealed
that the dictionary-based method applying POS-
rules yields the best result for noun compounds and
the MFV feature combined with syntactic informa-
tion is the most useful for LVC identification. If
no dictionary matches were taken into consideration,
the combination of the features ?Suffix? and ?Stem?
achieved the best result, however, ?Stem? alone can
also perform similarly. Since ?Stem? identifies de-
verbal nouns, that is, nouns having an argument
structure, it is not surprising that this feature is valu-
able in noun compound detection because the first
part in the compound is most probably an argument
of the deverbal noun (as in noun compound detection
the object of detection is noun compound, in other
words, we detect noun compounds). Thus, it will be
worth examining how the integration of the ?Stem?
feature can improve dictionary-based models.
Making use of only POS-rules does not seem to
119
Method
Noun compounds NC + syntax LVC LVC + syntax
P R F P R F P R F P R F
Match 37.7 54.73 44.65 49.64 48.31 48.97 - - - - - -
Merge 40.06 57.63 47.26 51.69 47.86 49.70 - - - - - -
POS-rules 55.56 49.98 52.62 59.18 46.39 52.02 - - - - - -
Combined 59.46 52.48 55.75 62.07 45.81 52.72 - - - - - -
POS-rules w/o dic 28.33 66.23 39.69 29.97 64.18 40.87 9.35 72.55 12.86 7.02 76.63 16.56
Suffix 27.02 8.91 13.4 28.58 8.84 13.5 9.62 16.3 12.1 11.52 15.22 13.11
MF 12.26 1.33 2.4 12.41 1.29 2.34 33.83 55.16 41.94 40.21 51.9 45.31
Stem 29.87 37.62 33.3 31.69 36.63 33.99 8.56 50.54 14.64 11.07 47.55 17.96
Suffix?MF 0 0 0 - - - 44.05 10.05 16.37 11.42 54.35 18.88
Suffix?MF 23.36 10.24 14.24 24.50 10.13 14.34 19.82 61.41 29.97 23.99 57.88 33.92
Suffix?Stem 28.4 6.49 10.56 30.03 6.42 10.58 10.35 11.14 11.1 12.28 11.14 11.68
Suffix?Stem 29.35 40.05 33.87 31.12 39.06 34.64 8.87 57.61 15.37 11.46 54.35 18.93
MF?Stem 9.16 0.41 0.78 9.6 0.41 0.79 39.53 36.96 38.2 46.55 34.78 39.81
MF?Stem 29.13 38.55 33.18 31.85 36.04 33.81 10.42 68.75 18.09 13.36 64.67 22.15
Suffix?MF?Stem 0 0 0 - - - 47.37 7.34 12.7 50.0 6.79 11.96
Suffix?MF?Stem 28.68 40.97 33.74 30.33 39.95 34.48 10.16 72.28 17.82 13.04 68.2 21.89
Table 1: Experimental results in terms of precision (P), recall (R) and F-measure (F). Match: dictionary match, Merge:
merge of two overlapping noun compounds, POS-rules: matching of POS-patterns, Combined: the union of Match,
Merge and POS-rules, POS-rules w/o dic: matching POS-patterns without dictionary lookup, Suffix: the (head) noun
ends in a given suffix, MF: the head noun/verb is among the 15 most frequent ones, Stem: the (head) noun is deverbal.
be satisfactory for LVC detection. However, the
most useful feature for identifying LVCs, namely,
MFV/MFN proves to perform poorly for noun com-
pounds, which can be explained by the fact that the
verbal component of LVCs usually comes from a
well-defined set of frequent verbs, thus, it is lexically
more restricted than the parts of noun compounds.
The feature ?Stem? helps improve recall and this fea-
ture can be further enhanced since in some cases,
the Porter stemmer did not render the same stem to
derivational pairs such as assumption ? assume. For
instance, derivational information encoded in word-
net relations might contribute to performance.
Concerning syntactic information, it has clearly
positive effects on LVC identification, however, this
influence is ambiguous in the case of noun com-
pounds. Since light verb constructions form a syn-
tactic phrase and noun compounds behave syntac-
tically as one unit (having an internal syntactic hi-
erarchy though), this result suggests that for noun
compound detection, POS-tagging provides enough
information while for light verb constructions, syn-
tactic information is expected to improve the system.
5 Conclusions
In this paper, we aimed at identifying noun com-
pounds and light verb constructions in running texts
with rule-based methods and compared the effect
of several features on detecting those two types
of multiword expressions. For noun compounds,
dictionary-based methods and POS-tagging seem
to contribute most to the performance of the sys-
tem whereas for light verb constructions, the com-
bination of POS-tagging, syntactic information and
restrictions on the nominal and verbal component
yield the best result. Although the effect of syntax
is negligible on noun compound detection, it is un-
ambiguously helpful for identifying light verb con-
structions. Our methods can be improved by extend-
ing the set and scope of features and refining POS-
and syntactic rules and they can be also adapted to
other languages by creating language-specific POS-
rules, lists of suffixes and light verb candidates.
For higher-level of applications, it is necessary to
know which tokens form one (syntactic or semantic)
unit, thus, we believe that our results in detecting
noun compounds and light verb constructions can be
fruitfully applied in e.g. information extraction or
machine translation as well.
Acknowledgments
This work was supported in part by the National In-
novation Office of the Hungarian government within
the framework of the project MASZEKER.
120
References
Hassan Al-Haj and Shuly Wintner. 2010. Identifying
multi-word expressions by leveraging morphological
and syntactic idiosyncrasy. In Proceedings of Coling
2010, Beijing, China, August.
Colin Bannard. 2007. A measure of syntactic flexi-
bility for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions, MWE
?07, pages 1?8, Morristown, NJ, USA. ACL.
Gerlof Bouma. 2010. Collocation extraction beyond the
independence assumption. In Proceedings of the ACL
2010 Conference Short Papers, pages 109?114, Upp-
sala, Sweden, July. ACL.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine MacLeod,
and Antonio Zampolli. 2002. Towards best practice
for multiword expressions in computational lexicons.
In Proceedings of LREC-2002, pages 1934?1940, Las
Palmas.
Helena de Medeiros Caseli, Aline Villavicencio, Andre?
Machado, and Maria Jose? Finatto. 2009. Statistically-
driven alignment-based multiword expression identi-
fication for technical domains. In Proceedings of
the Workshop on Multiword Expressions: Identifica-
tion, Interpretation, Disambiguation and Applications,
pages 1?8, Singapore, August. ACL.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation, 44(1-2):59?77.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions, pages
41?48, Morristown, NJ, USA. ACL.
Stefan Evert and Hannah Kermes. 2003. Experiments on
candidate data for collocation extraction. In Proceed-
ings of EACL 2003, pages 83?86.
Su Nam Kim. 2008. Statistical Modeling of Multiword
Expressions. Ph.D. thesis, University of Melbourne,
Melbourne.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?03, pages 423?430, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44(1-2):137?158.
Scott S. L. Piao, Paul Rayson, Dawn Archer, Andrew
Wilson, and Tony McEnery. 2003. Extracting multi-
word expressions with a semantic tagger. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions: analysis, acquisition and treatment, pages 49?
56, Morristown, NJ, USA. ACL.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010a. Multiword Expressions in the wild?
The mwetoolkit comes in handy. In Coling 2010:
Demonstrations, Beijing, China, August.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010b. Web-based and combined language
models: a case study on noun compound identifica-
tion. In Coling 2010: Posters, Beijing, China, August.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proceedings
of CICLing-2002, pages 1?15, Mexico City, Mexico.
Tanja Samardz?ic? and Paola Merlo. 2010. Cross-lingual
variation of light verb constructions: Using paral-
lel corpora and automatic alignment for linguistic re-
search. In Proceedings of the 2010 Workshop on NLP
and Linguistics: Finding the Common Ground, pages
52?60, Uppsala, Sweden, July. ACL.
R. Mahesh K. Sinha. 2009. Mining Complex Predicates
In Hindi Using A Parallel Hindi-English Corpus. In
Proceedings of the Workshop on Multiword Expres-
sions: Identification, Interpretation, Disambiguation
and Applications, pages 40?46, Singapore, August.
ACL.
Yee Fan Tan, Min-Yen Kan, and Hang Cui. 2006. Ex-
tending corpus-based identification of light verb con-
structions using a supervised learning framework. In
Proceedings of the EACL Workshop on Multi-Word
Expressions in a Multilingual Contexts, pages 49?56,
Trento, Italy, April. ACL.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings of
EMNLP 2000, pages 63?70, Stroudsburg, PA, USA.
ACL.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction. In
Proceedings of the Workshop on a Broader Perspective
on Multiword Expressions, MWE ?07, pages 25?32,
Morristown, NJ, USA. ACL.
Sina Zarrie? and Jonas Kuhn. 2009. Exploiting Transla-
tional Correspondences for Pattern-Independent MWE
Identification. In Proceedings of the Workshop on
Multiword Expressions: Identification, Interpretation,
Disambiguation and Applications, pages 23?30, Sin-
gapore, August. ACL.
121
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 99?103,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
How to Evaluate Opinionated Keyphrase Extraction?
Ga?bor Berend
University of Szeged
Department of Informatics
A?rpa?d te?r 2., Szeged, Hungary
berendg@inf.u-szeged.hu
Veronika Vincze
Hungarian Academy of Sciences
Research Group on Artificial Intelligence
Tisza Lajos krt. 103., Szeged, Hungary
vinczev@inf.u-szeged.hu
Abstract
Evaluation often denotes a key issue in
semantics- or subjectivity-related tasks. Here
we discuss the difficulties of evaluating opin-
ionated keyphrase extraction. We present our
method to reduce the subjectivity of the task
and to alleviate the evaluation process and
we also compare the results of human and
machine-based evaluation.
1 Introduction
Evaluation is a key issue in natural language pro-
cessing (NLP) tasks. Although for more basic tasks
such as tokenization or morphological parsing, the
level of ambiguity and subjectivity is essentially
lower than for higher-level tasks such as question
answering or machine translation, it is still an open
question to find a satisfactory solution for the (auto-
matic) evaluation of certain tasks. Here we present
the difficulties of finding an appropriate way of eval-
uating a highly semantics- and subjectivity-related
task, namely opinionated keyphrase extraction.
There has been a growing interest in the NLP
treatment of subjectivity and sentiment analysis ?
see e.g. Balahur et al (2011) ? on the one hand and
on keyphrase extraction (Kim et al, 2010) on the
other hand. The tasks themselves are demanding for
automatic systems due to the variety of the linguis-
tic ways people can express the same linguistic con-
tent. Here we focus on the evaluation of subjective
information mining through the example of assign-
ing opinionated keyphrases to product reviews and
compare the results of human- and machine-based
evaluation on finding opinionated keyphrases.
2 Related Work
As the task we aim at involves extracting keyphrases
that are responsible for the author?s opinion toward
the product, aspects of both keyphrase extraction
and opinion mining determine our methodology and
evaluation procedure. There are several sentiment
analysis approaches that make use of manually an-
notated review datasets (Zhuang et al, 2006; Li
et al, 2010; Jang and Shin, 2010) and Wei and
Gulla (2010) constructed a sentiment ontology tree
in which attributes of the product and sentiments
were paired.
For evaluating scientific keyphrase extraction,
several methods have traditionally been applied. In
the case of exact match, the gold standard key-
words must be in perfect overlap with the ex-
tracted keywords (Witten et al, 1999; Frank et al,
1999) ? also followed in the SemEval-2010 task
on keyphrase extraction (Kim et al, 2010), while
in other cases, approximate matches or semanti-
cally similar keyphrases are also accepted (Zesch
and Gurevych, 2009; Medelyan et al, 2009). In this
work we applied the former approach for the evalu-
ation of opinion phrases and made a thorough com-
parison with the human judgement.
Here, we use the framework introduced in Berend
(2011) and conducted further experiments based on
it to point out the characteristics of the evaluation
of opinionated keyphrase extraction. Here we pin-
point the severe differences in performance mea-
sures when the output is evaluated by humans com-
pared to strict exact match principles and also exam-
ine the benefit of hand-annotated corpus as opposed
99
to an automatically crawled one. In addition, the
extent to which original author keyphrases resemble
those of independent readers? is also investigated in
this paper.
3 Methodology
In our experiments, we used the methodology de-
scribed in Berend (2011) to extract opinionated
keyphrase candidates from the reviews. The sys-
tem treats it as a supervised classification task us-
ing Maximum Entropy classifier, in which certain
n-grams of the product reviews are treated as classi-
fication instances and the task is to classify them as
proper or improper ones. It incorporates a rich fea-
ture set, relying on the usage of SentiWordNet (Esuli
et al, 2010) and further orthological, morphological
and syntactic features. Next, we present the diffi-
culties of opinionated keyphrase extraction and offer
our solutions to the emerging problems.
3.1 Author keyphrases
In order to find relevant keyphrases in the texts,
first the reviews have to be segmented into ana-
lyzable parts. We made use of the dataset de-
scribed in Berend (2011), which contains 2000 prod-
uct reviews each from two quite different domains,
i.e. mobile phone and video film reviews from the re-
view portal epinions.com. In the free-text parts
of the reviews, the author describes his subjective
feelings and views towards the product, and in the
sections Pros and cons and Bottomline he summa-
rizes the advantages and disadvantages of the prod-
uct, usually by providing some keyphrases or short
sentences. However, these pros and cons are noisy
since some authors entered full sentences while oth-
ers just wrote phrases or keywords. Furthermore,
the segmentation also differs from review to review
or even within the same review (comma, semicolon,
ampersand etc.). There are also non-informative
comments such as none among cons. For the above
reasons, the identification of the appropriate gold
standard phrases is not unequivocal.
We had to refine the pros and cons of the re-
views so that we could have access to a less noisy
database. Refinement included segmenting pros
and cons into keyphrase-like units and also bring-
ing complex phrases into their semantically equiva-
Auth. Ann1 Ann2 Ann3
Auth. ? 0.415 0.324 0.396
Ann1 0.601 ? 0.679 0.708
Ann2 0.454 0.702 ? 0.713
Ann3 0.525 0.690 0.688 ?
Table 1: Inter-annotator agreement among the author?s
and annotators? sets of opinion phrases. Elements above
and under the main diagonal refer to the agreement rates
in Dice coefficient for pro and con phrases, respectively.
lent, yet much simpler forms, e.g. instead of ?even I
found the phones menus to be confusing?, we would
like to have ?confusing phones menus?. Refinement
was carried out both automatically by using hand-
crafted transformation rules (based on POS patterns
and parse trees) and manual inspection. The an-
notation guidelines for the human refinement and
various statistics on the dataset can be accessed at
http://rgai.inf.u-szeged.hu/proCon.
3.2 Annotator keyphrases
The second problem with regard to opinionated
keyphrase extraction is the subjectivity of the task.
Different people may have different opinions on the
very same product, which is often reflected in their
reviews. On the other hand, people can gather dif-
ferent information from the very same review due
to differences in interpretation, which again compli-
cates the way of proper evaluation.
In order to evaluate the difficulty of identifying
opinion-related keyphrases, we decided to apply the
following methodology. We selected 25 reviews re-
lated to the mobile phone Nokia 6610, which were
also collected from the website epinions.com.
The task for three linguists was to write positive
and negative aspects of the product in the form of
keyphrases, similar to the original pros and cons. In
order not to be influenced by the keyphrases given
by the author of the review, the annotators were only
given the free-text part of the review, i.e. the origi-
nal Pros and cons and Bottomline sections were re-
moved. In this way, three different pro and con an-
notations were produced for each review, besides,
those of the original author were also at hand. The
inter-annotator agreement rate is in Table 1.
Concerning the subjectivity of the task, pro and
con phrases provided by the three annotators and
100
Eval Ref Top-5 Top-10 Top-15
3Ann? man 32.14 44.66 53.92
3Ann? auto 27.68 38.17 45.78
Merged? man 28.52 41.09 52.18
Merged? auto 27.39 37.67 46.34
3Ann? man 34.89 43.31 44.92
3Ann? auto 29.96 34.34 35.54
Merged? man 24.75 26.12 22.22
Merged? auto 21.39 20.94 21.89
Author man 27.14 33.5 35.24
Author auto 20.61 22.34 25.03
Table 2: F-scores of the human evaluation of the automat-
ically extracted opinion phrases. Columns Eval and Ref
show the way gold standard phrases were obtained and if
they were refined manually or automatically.
the original author showed a great degree of variety
although they had access to the very same review.
Sometimes it happened that one annotator did not
give any pro or con phrases for a review whereas the
others listed a bunch of them, which reflects that the
very same feature can be judged as still tolerable,
neutral or absolutely negative for different people.
Thus, as even human annotations may differ from
each other to a great extent, it is not unequivocal to
decide which human annotation should be regarded
as the gold standard upon evaluation.
3.3 Evaluation methodology
Since the comparison of annotations highlighted
the subjectivity of the task, we voted for smooth-
ing the divergences of annotations. We wanted to
take into account all the available annotations which
were manually prepared and regarded as acceptable.
Thus, an annotator formed the union and the inter-
section of the pro and con features given by each an-
notator either including or excluding those defined
by the original author. With this, we aimed at elim-
inating subjectivity since in the case of union, every
keyphrase mentioned by at least one annotator was
taken into consideration while in the case of inter-
section, it is possible to detect keyphrases that seem
to be the most salient for the annotators as regards
the given document. Thus, four sets of pros and cons
were finally yielded for each review depending on
whether the unions or intersections were determined
purely on the phrases of the annotators excluding the
original phrases of the author or including them. The
following example illustrates the way new sets were
created based on the input sets (in italics):
Pro1 : radio, organizer, phone book
Pro2 : radio, organizer, loudspeaker
Pro3 : radio, organizer, calendar
Union: radio, organizer, calendar, loud-
speaker, phone book
Intersection: radio, organizer
Proauthor : clear, fun
Merged Union: radio, organizer, calen-
dar, loudspeaker, phone book, clear, fun
Merged Intersection: ?
The reason behind this methodology was that it
made it possible to evaluate our automatic meth-
ods in two different ways. Comparing the automatic
keyphrases to the union of human annotations means
that a bigger number of keyphrases is to be identi-
fied, however, with a bigger number of gold standard
keywords it is more probable that the automatic key-
words occur among them. At the same time having a
larger set of gold standard tags might affect the recall
negatively since there are more keyphrases to return.
On the other hand, in the case of intersection it can
be measured whether the most important features
(i.e. those that every annotator felt relevant) can be
extracted from the text. Note that our strategy is sim-
ilar to the one applied in the case of BLEU/ROUGE
score (Papineni et al, 2002; Lin, 2004) with respect
to the fact that multiple good solutions are taken
into account whereas the application of union and
intersection is determined by the nature of the task:
different annotators may attach several outputs (in
other words, different numbers of keyphrases) to the
same document in the case of keyphrase extraction,
which is not realistic in the case of machine trans-
lation or summarization (only one output is offered
for each sentence / text).
3.4 Results
In our experiments, we used the opinion phrase ex-
traction system based on the paper of Berend (2011).
Results vary whether the manually or the automat-
ically refined set of the original sets of pros and
cons were regarded as positive training examples
and also whether the evaluation was carried out
101
Mobiles Movies
A/A 9.95 9.55 8.61 7.58 7.1 6.24
A/M 13.51 12.73 11.2 9.95 9.05 7.72
M/A 10.15 9.7 8.69 7.52 6.92 5.97
M/M 15.27 14.11 12.17 12.22 10.63 8.67
Table 3: F-scores achieved with different keyphrase re-
finement strategies. A and M as the first (second) charac-
ter indicate the fact that the training (testing) was based
on the automatically and manually defined sets of gold
standard expressions, respectively.
against purely the original set of author-assigned
keyphrases or the intersection/union of the man-
ual annotations including and excluding the author-
assigned keyphrases on the 25 mobile phone re-
views. Results of the various combinations in the
experiments for the top 5, 10 and 15 keyphrases
are reported in Table 2 containing both cases when
human and automatic refinement of the gold stan-
dard opinion phrases were carried out. Automatic
keyphrases were manually compared to the above
mentioned sets of keyphrases, i.e. human annotators
judged them as acceptable or not. Human evaluation
had the advantage over automated ones, that they
could accept the extracted term ?MP3? when there
was only its mistyped version ?MP+? in the set of
gold standard phrases (as found in the dataset).
Table 3 presents the results of our experiments on
keyphrase refinement on the mobiles and movies do-
mains. In these settings strict matches were required
instead of human evaluation. Results differ with re-
spect to the fact whether the automatically or manu-
ally refined sets of the original author phrases were
utilized for training and during the strict evaluation.
Having conducted these experiments, we could ex-
amine the possibility of a fully automatic system that
needs no manually inspected training data, but it can
create it automatically as well.
4 Discussion and conclusions
Both human and automatic evaluation reveal that
the results yielded when the system was trained on
manually refined keyphrases are better. The usage
of manually refined keyphrases as the training set
leads to better results (the difference being 5.9 F-
score on average), which argues for human annota-
tion as opposed to automatic normalization of the
gold standard opinion phrases. Note, however, that
even though results obtained with the automatic re-
finement of training instances tend to stay below the
results that are obtained with the manual refinement
of gold standard phrases, they are still comparable,
which implies that with more sophisticated rules,
training data could be automatically generated.
If the inter-annotator agreement rates are com-
pared, it can be seen that the agreement rates be-
tween the annotators are considerably higher than
those between a linguist and the author of the prod-
uct review. This may be due to the fact that the
linguists were to conform to the annotation guide-
lines whereas the keyphrases given by the authors
of the reviews were not limited in any way. Still,
it can be observed that among the author-annotator
agreement rates, the con phrases could reach higher
agreement than the pro phrases. This can be due to
psychological reasons: people usually expect things
to be good hence they do not list all the features that
are good (since they should be good by nature), in
contrast, they list negative features because this is
what deviates from the normal expectations.
In this paper, we discussed the difficulties of eval-
uating opinionated keyphrase extraction and also
conducted experiments to investigate the extent of
overlap between the keyphrases determined by the
original author of a review and those assigned by
independent readers. To reduce the subjectivity of
the task and to alleviate the evaluation process, we
presented our method that employs several indepen-
dent annotators and we also compared the results of
human and machine-based evaluation. Our results
reveal that for now, human evaluation leads to bet-
ter results, however, we believe that the proper treat-
ment of polar expressions and ambiguous adjectives
might improve automatic evaluation among others.
Besides describing the difficulties of the auto-
matic evaluation of opinionated keyphrase extrac-
tion, the impact of training on automatically crawled
gold standard opinionated phrases was investigated.
Although not surprisingly they lag behind the ones
obtained based on manually refined training data,
the automatic creation of gold standard keyphrases
can be a much cheaper, yet feasible option to manu-
ally refined opinion phrases. In the future, we plan to
reduce the gap between manual and automatic eval-
uation of opinionated keyphrase extraction.
102
Acknowledgments
This work was supported in part by the NIH grant
(project codename MASZEKER) of the Hungarian
government.
References
Alexandra Balahur, Ester Boldrini, Andres Montoyo, and
Patricio Martinez-Barco, editors. 2011. Proceedings
of the 2nd Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA 2.011).
ACL, Portland, Oregon, June.
Ga?bor Berend. 2011. Opinion expression mining by ex-
ploiting keyphrase extraction. In Proceedings of 5th
International Joint Conference on Natural Language
Processing, pages 1162?1170, Chiang Mai, Thailand,
November. Asian Federation of Natural Language Pro-
cessing.
Andrea Esuli, Stefano Baccianella, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ing of 16th International Joint Conference on Artifi-
cial Intelligence, pages 668?673. Morgan Kaufmann
Publishers.
Hayeon Jang and Hyopil Shin. 2010. Language-specific
sentiment analysis in morphologically rich languages.
In Coling 2010: Posters, pages 498?506, Beijing,
China, August. Coling 2010 Organizing Committee.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Tim-
othy Baldwin. 2010. Semeval-2010 task 5: Auto-
matic keyphrase extraction from scientific articles. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, SemEval ?10, pages 21?26, Mor-
ristown, NJ, USA. ACL.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 653?
661, Beijing, China, August. Coling 2010 Organizing
Committee.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. ACL.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318?1327, Singapore, Au-
gust. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the ACL, pages 311?318, Philadel-
phia, Pennsylvania, USA, July. ACL.
Wei Wei and Jon Atle Gulla. 2010. Sentiment learn-
ing on product reviews via sentiment ontology tree. In
Proceedings of the 48th Annual Meeting of the ACL,
pages 404?413, Uppsala, Sweden, July. ACL.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction. In ACM DL,
pages 254?255.
Torsten Zesch and Iryna Gurevych. 2009. Approxi-
mate Matching for Evaluating Keyphrase Extraction.
In Proceedings of the 7th International Conference
on Recent Advances in Natural Language Processing,
pages 484?489, September.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, CIKM ?06, pages 43?50,
New York, NY, USA. ACM.
103
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 62?67,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
LFG-based Features for Noun Number and Article Grammatical Errors
Ga?bor Berend1, Veronika Vincze2, Sina Zarriess3, Richa?rd Farkas1
1University of Szeged
Department of Informatics
{berendg,rfarkas}@inf.u-szeged.hu
2Research Group on Artificial Intelligence
Hungarian Academy of Sciences
vinczev@inf.u-szeged.hu
3University of Stuttgart
Institute for Natural Language Processing
zarriesa@ims.uni-stuttgart.de
Abstract
We introduce here a participating system
of the CoNLL-2013 Shared Task ?Gram-
matical Error Correction?. We focused on
the noun number and article error cate-
gories and constructed a supervised learn-
ing system for solving these tasks. We car-
ried out feature engineering and we found
that (among others) the f-structure of an
LFG parser can provide very informative
features for the machine learning system.
1 Introduction
The CoNLL-2013 Shared Task aimed at identify-
ing and correcting grammatical errors in the NU-
CLE learner corpus of English (Dahlmeier et al,
2013). This task has become popular in the natural
language processing (NLP) community in the last
few years (Dale and Kilgariff, 2010), which mani-
fested in the organization of shared tasks. In 2011,
the task Helping Our Own (HOO 2011) was held
(Dale and Kilgariff, 2011), which targeted the pro-
motion of NLP tools and techniques in improving
the textual quality of papers written by non-native
speakers of English within the field of NLP. The
next year, HOO 2012 (Dale et al, 2012) specifi-
cally focused on the correction of determiner and
preposition errors in a collection of essays writ-
ten by candidates sitting for the Cambridge ESOL
First Certificate in English (FCE) examination. In
2013, the CoNLL-2013 Shared Task has continued
this direction of research.
The CoNLL-2013 Shared Task is based on the
NUCLE corpus, which consists of about 1,400
student essays from undergraduate university stu-
dents at The National University of Singapore
(Dahlmeier et al, 2013). The corpus contains over
one million words and it is completely annotated
with grammatical errors and corrections. Among
the 28 error categories, this year?s shared task fo-
cused on the automatic detection and correction of
five specific error categories.
In this paper, we introduce our contribution of
the CoNLL-2013 Shared Task. We propose a su-
pervised learning-based approach. The main con-
tribution of this work is the exploration of several
feature templates for grammatical error categories.
We focused on the two ?nominal? error categories:
1.1 Article and Determiner Errors
This error type involved all kinds of errors
which were related to determiners and articles
(ArtOrDet). It required multiple correction
strategies. On the one hand, superfluous articles
or determiners should be deleted from the text.
On the other hand, missing articles or determin-
ers should be inserted and at the same time it was
sometimes also necessary to replace a certain type
of article or determiner to an other type. Here is
an example:
For nations like Iran and North Ko-
rea, the development of nuclear power
is mainly determined by the political
forces. ? For nations like Iran and
North Korea, the development of nu-
clear power is mainly determined by po-
litical forces.
62
1.2 Wrong Number of the Noun
The wrong number of nouns (Nn) meant that either
a singular noun should occur in the plural form or
a plural noun should occur in the singular form.
A special case of such errors was that sometimes
uncountable nouns were used in the plural, which
is ungrammatical. The correction involved here
the change of the number. Below we provide an
example:
All these measures are implemented to
meet the safety expectation of the op-
eration of nuclear power plant. ? All
these measures are implemented to meet
the safety expectation of the operation
of nuclear power plants.
2 System Description
Our approach for grammatical error detection was
to construct supervised classifiers for each candi-
date of grammatical error locations. In general,
our candidate extraction and features are based
on the output of the preprocessing step provided
by the organizers which contained both the POS-
tag sequences and the constituency phrase struc-
ture outputs for every sentence in the training and
test sets determined by Stanford libraries. We em-
ployed the Maximum Entropy based supervised
classification model using the MALLET API (Mc-
Callum, 2002), which was responsible for suggest-
ing the various corrections.
The most closely related approach to ours is
probably the work of De Felice and Pulman
(2008). We also employ a Maximum Entropy clas-
sifier and a syntax-motivated feature set. However,
we investigate deeper linguistic features (based on
the f-structure of an LFG parser).
In the following subsections we introduce our
correction candidate recognition procedure and
the features used for training and prediction of
the machine learning classifier. We employed the
same feature set for each classification task.
2.1 Candidate Locations
We used the following heuristics for the recogni-
tion of the possible locations of grammatical er-
rors. We also describe the task of various classi-
fiers at these candidate locations.
Article and Determiner Error category We
handled the beginning of each noun phrase
(NP) as a possible location for errors related
to articles or determiners. The NP was
checked if it started with any definite or
indefinite article. If it did, we asked our
three-class classifier whether to leave it
unmodified, change its type (i.e. an indefinite
to a definite one or vice versa) or simply
delete it. However, when there was no article
at all at the beginning of a noun phrase,
the decision made by a different three-class
classifier was whether to leave that position
empty or to put a definite or indefinite article
in that place.
Wrong Number of the Noun Error category
Every token tagged as a noun (either in plural
or singular) was taken into consideration at
this subtask. We constructed two ? i.e. one
for the word forms originally written in plu-
ral and singular ? binary classifiers whether
the number (i.e. plural or singular) of the
noun should be changed or left unchanged.
2.2 LFG parse-based features
We looked for the minimal governing NP for each
candidate location. We reparsed this NP with-
out context by a Lexical Functional Grammar
(LFG) parser and we acquired features from its
f-structure. In the following paragraph, LFG is
introduced briefly while Table 1 summarizes the
features extracted from the LFG parse.
Lexical Functional Grammar (LFG) (Bresnan,
2000) is a constraint-based theory of grammar. It
posits two levels of representation, c(onstituent)-
structure and f(unctional)-structure.
C-structure is represented by context free
phrase-structure trees, and captures surface gram-
matical configurations. F-structures approximate
basic predicate-argument and adjunct structures.
The experiments reported in this paper use the
English LFG grammar constructed as part of the
ParGram project (Butt et al, 2002). The gram-
mar is implemented in XLE, a grammar develop-
ment environment, which includes a very efficient
LFG parser. Within the spectrum of approaches to
natural language parsing, XLE can be considered
a hybrid system combining a hand-crafted gram-
mar with a number of automatic ambiguity man-
agement techniques:
(i) c-structure pruning where, based on informa-
tion from statistically obtained parses, some trees
are ruled out before f-structure unification (Cahill
et al, 2007)
63
COORD NP/PP is coordinated +/-
COORD-LEVEL syntactic category of coordi-
nated phrase
DEG-DIM dimension for comparitive NPs,
(?equative?/?pos?/?neg?)
DEGREE semantic type of adjec-
tival modifier (?posi-
tive?/?comparative?/?superlative?)
DET-TYPE type of determiner
(?def?/?indef?/?demon?)
LOCATION-TYPE marks locative NPs
NAME-TYPE ?first name?/?last name?
NSYN syntactic noun type (?com-
mon?/?proper?/?pronoun?)
PRON-TYPE syntactic pronoun type (e.g.
?pers?, ?refl?, ?poss?)
PROPER-TYPE type of proper noun (e.g. ?com-
pany?, ?location?, ?name?)
Table 1: Short characterization of the LFG fea-
tures incorporated in our models designed to cor-
rect noun phrase-related grammatical errors
(ii) an Optimality Theory-style constraint mecha-
nism for filtering and ranking competing analyses
(Frank et al, 2001),
and (iii) a stochastic disambiguation component
which is based on a log-linear probability model
(Riezler et al, 2002) and works on the packed rep-
resentations.
Although we use a deep, hand-crafted LFG
grammar for processing the data, our approach is
substantially different from other grammar-based
approaches to CALL. For instance, Fortmann and
Forst (2004) supplement a German LFG devel-
oped for newspaper text with so-called malrules
that accept marked or ungrammatical input of
some predefined types. In our work, we apply an
LFG parser developed for standard texts to get a
rich feature representation that can be exploited
by a classifier. While malrules would certainly be
useful for finding other error types, such as agree-
ment errors, the NP- and PP-errors are often ana-
lyzed as grammatical by the parser (e.g. ?the po-
litical forces? vs. ?political forces?). Thus, the
grammaticality of a phrase predicted by the gram-
mar is not necessarily a good indicator for correc-
tion in our case.
2.3 Phrase-based contextual features
Besides the LFG features describing the internal
structure of the minimal NP that dominates a can-
didate location, we defined features describing its
context as well. Phrase-based contextual features
searched for those minimal prepositional and noun
phrases that governed a token at a certain can-
Final results Corrected output
P 0.0552 0.1260
R 0.0316 0.0292
F 0.0402 0.0474
Table 2: Overall results aggregated over the five
error types
didate location of the sentence where a decision
was about to be taken. Then features encoding the
types of the phrases that preceded and succeeded
a given minimal governing noun or prepositional
phrase were extracted.
The length of those minimal governing noun
and prepositional phrases as well as those of the
preceding and succeeding ones were taken into
account as numeric features. The motivation be-
hind using the span size of the minimal governing
and neighboring noun and prepositional phrases
is that it was assumed that grammatical errors in
the sentence result in unusual constituency subtree
patterns that could manifest in minimal governing
phrases having too long spans for instance. The
relative position of the candidate position inside
the smallest dominating noun and prepositional
phrases was also incorporated as a feature since
this information might carry some information for
noun errors.
2.4 Token-based contextual features
A third group of features described the context of
the candidate location at the token level. Here, two
sets of binary features were introduced to mark the
fact if the token was present in the four token-sized
window to its left or right. Dedicated nominal fea-
tures were introduced to store the word form of
the token immediately preceding a decision point
within a sentence and the POS-tags at the preced-
ing and actual token positions.
Two lists were manually created which con-
sisted of entirely uncountable nouns (e.g. blood)
and nouns that are uncountable most of the times
(e.g. aid or dessert). When generating fea-
tures for those classifiers that can modify the plu-
rality of a noun, we marked the fact in a binary
feature if they were present in any of these lists.
Another binary feature indicated if the actual noun
to be classified could be found at an earlier point
of the document.
64
Only erroneous All sentences
P 0.1260 0.1061
R 0.0292 0.0085
F 0.0474 0.0158
Table 3: Overall results aggregated over the five
error types
Only erroneous All sentences
P 0.2500 0.0167
R 0.0006 0.0006
F 0.0012 0.0012
Table 4: Overall results aggregated over the five
error types, not using the LFG parser based fea-
tures
3 Results
It is important to note that our officially submit-
ted architecture included an unintended step which
meant that whenever our system predicted that at
a certain point an indefinite article should be in-
serted or (re-)written, the indefinite article an was
put at that place erroneously when the succeeding
token started with a consonant (e.g. outputting an
serious instead of a serious).
Since the output that contained this kind of error
served as the basis of the official ranking we in-
clude in Table 2 the results achieved with the out-
put affected by this unintended behavior, however,
in the following we present our results in such a
manner where this kind of error is eliminated from
the output of our system.
Upon training our systems we followed two
strategies. For the first approach we used all the
sentences regardless if they had any error in them
at all. However, in an alternative approach we uti-
lized only those sentences from the training corpus
that had at least one error in them from the five er-
ror categories to be dealt with in the shared task.
The different results achieved on the test set ac-
cording to the two approaches are detailed in Ta-
ble 3. Turning off the LFG features ended up in
the results detailed in Table 4.
Since our framework in its present state only
aims at the correction of errors explicitly re-
lated to noun phrases, no error categories besides
ArtOrDet and Nn (for more details see Sections
1.1 and 1.2, respectively) could be possibly cor-
rected by our system. Note that these two error
categories covered 66.1% of the corrections on the
test set, so with our approach this was the highest
possibly achievable score in recall.
In order to get a clearer picture on the effective-
ness of our proposed methodology on the two error
types that we aimed at, we present results focusing
on those two error classes.
Nn ArtOrDet
P 0.4783 (44/92) 0.0151 (4/263)
R 0.1111 (44/396) 0.0058 (4/690)
F 0.1803 0.0084
Table 5: The scores achieved and the number of
true positive, suggestions, real errors for the Noun
Number (Nn) and Article and Determiner Errors
(ArtOrDet) categories.
4 Error Analysis
In order to analyze the performance of our system
in more detail, we carried out an error analysis.
As our system was optimized for errors related to
nouns (i.e. Nn and ArtOrDet errors), we focus
on these error categories in our discussion and ne-
glect verbal and prepositional errors.
Some errors in our system?s output were due
to pronouns, which are conventionally tagged as
nouns (e.g. something), but were incorrectly put
in the plural, resulting in the erroneous correc-
tion somethings. These errors would have been
avoided by including a list of pronouns which
could not be used in the plural (even if they are
tagged as nouns).
Another common source of errors was that
countable and uncountable uses of nouns which
can have both features in different senses or
metonymic usage (e.g. coffee as a substance is un-
countable but coffee meaning ?a cup of coffee? is
countable) were hard to separate. Performance on
this class of nouns could be ameliorated by apply-
ing word sense disambiguation/discrimination or
a metonymy detector would also prove useful for
e.g. mass nouns.
A great number of nominal errors involved
cases where a singular noun occurred in the text
without any article or determiner. In English, this
is only grammatical in the case of uncountable
nouns which occur in generic sentences, for in-
stance:
Radio-frequency identification is a
technology which uses a wireless non-
contact system to scan and transfer the
data [...]
65
The above sentence offers a definition of radio-
frequency identification, hence it is a generic state-
ment and should be left as it is. In other cases,
two possible strategies are available for correc-
tion. First, the noun gets an article or a determiner.
The actual choice among the articles or determin-
ers depends on the context: if the noun has been
mentioned previously and thus is already known
(definite) in the context, it usually gets a definite
article (or a possessive determiner). If it is men-
tioned for the first time, it gets an indefinite arti-
cle (unless it is a unique thing such as the sun).
The difficulty of the problem lies in the fact that
in order to adequately assign an article or deter-
miner to the noun, it is not sufficient to rely only
on the sentence. Thus, is also necessary to go be-
yond the sentence and move on the level of text
or discourse, which requires natural language pro-
cessing techniques that we currently lack but are
highly needed. With the application of such tech-
niques, we would have probably achieved better
results but this remains now for future work.
Second, the noun could be put in the plural.
This strategy is usually applied when either there
are more than one of the thing mentioned or it is a
generic sentence (i.e. things are discussed in gen-
eral and no specific instances of things are spo-
ken of). In this case, the detection of generic sen-
tences/events would be helpful, which again re-
quires deep semantic processing of the discourse
and is also a possible direction for future work.
To conclude, the successful identification of
noun number and article errors would require a
much deeper semantic (and even pragmatic) anal-
ysis and representation of the texts in question.
5 Discussion and further work
Comparing the columns of Table 3 we can con-
clude that restricting the training sentences to only
those which had some kind of grammatical error
in them had a useful effect on the overall effec-
tiveness of our system.
In a similar way, it can be stated based on the
results in Table 4 that composing features from the
output of an LFG parser is essentially beneficial
for the determination of Nn-type errors. Table 5
reveals, however, that those features which work
relatively well on the correction of Nn type errors
are less useful on ArtOrDet-type errors without
any modification.
As our only target at this point was to suggest
error corrections related to noun phrases, our ob-
vious future plans include the extension of our sys-
tem to deal with error categories of different types.
Simultaneously, we are planning to utilize large
scale corpus statistics, such as the Google N-gram
Corpus to build a more effective system.
Acknowledgements
This work was supported in part by the European
Union and the European Social Fund through the
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
References
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The Parallel Grammar Project. In Proceedings of
COLING-2002 Workshop on Grammar Engineering
and Evaluation, Taipei, Taiwan.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2007. Speeding
up LFG Parsing using C-Structure Pruning. In Col-
ing 2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 33 ? 40.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations (BEA 2013), Atlanta, Georgia, USA. Asso-
ciation for Computational Linguistics.
Robert Dale and Adam Kilgariff. 2010. Helping Our
Own: Text massaging for computational linguistics
as a new shared task. In Proceedings of the 6th Inter-
national Natural Language Generation Conference,
pages 261?265, Dublin, Ireland.
Robert Dale and Adam Kilgariff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation, Nancy, France.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceed-
ings of the 22nd International Conference on Com-
66
putational Linguistics (Coling 2008), pages 169?
176.
Christian Fortmann and Martin Forst. 2004. An LFG
Grammar Checker for CALL. In Proceedings of
ICALL 2004.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars.
In Peter Sells, editor, Formal and Empirical Issues in
Optimality Theoretic Syntax, pages 367?397. CSLI
Publications.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Stefan Riezler, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a Lexical-Functional Grammar and Discrimina-
tive Estimation Techniques. In Proceedings of ACL
2002.
67
