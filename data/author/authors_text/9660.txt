Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 121?128
Manchester, August 2008
 
 
Other-Anaphora Resolution in Biomedical Texts with Automatically 
Mined Patterns 
 
Chen Bin#, Yang Xiaofeng$, Su Jian^ and Tan Chew Lim* 
#*School of Computing, National University of Singapore  
$^Institute for Infocomm Research, A-STAR, Singapore 
{#chenbin, *tancl}@comp.nus.edu.sg 
{$xiaofengy, ^sujian}@i2r.a-star.edu.sg 
? Abstract 
This paper proposes an other-anaphora 
resolution approach in bio-medical texts. 
It utilizes automatically mined patterns to 
discover the semantic relation between an 
anaphor and a candidate antecedent. The 
knowledge from lexical patterns is incor-
porated in a machine learning framework 
to perform anaphora resolution. The ex-
periments show that machine learning 
approach combined with the auto-mined 
knowledge is effective for other-
anaphora resolution in the biomedical 
domain. Our system with auto-mined pat-
terns gives an accuracy of 56.5%., yield-
ing 16.2% improvement against the base-
line system without pattern features, and 
9% improvement against the system us-
ing manually designed patterns.  
1 Introduction 
The last decade has seen an explosive growth in 
the amount of textual information in biomedi-
cine. There is a need for an effective and effi-
cient text-mining system to gather and utilize the 
knowledge encoded in the biomedical literature. 
For a correct discourse analysis, a text-mining 
system should have the capability of understand-
ing the reference relations among different ex-
pressions in texts. Hence, anaphor resolution, the 
task of resolving a given text expression to its 
referred expression in prior texts, is important for 
an intelligent text processing system. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
In linguistics, an expression that points back 
to a previously mentioned expression is called an 
anaphor, and the expression being referred to by 
the anaphor is called its antecedent. Most pre-
vious work on anaphora resolution aims at identi-
ty-anaphora in which both an anaphor and its 
antecedent are mentions of the same entity. 
In this paper, we focus on a special type of 
anaphora resolution, namely, other-anaphora 
resolution, in which an anaphor to be resolved 
has a prefix modifier ?other? or ?another?. The 
antecedent of an other-anaphor is a complement 
expression to the anaphor in a super set. In other 
words, an other-anaphor is a set of elements ex-
cluding the element(s) specified by the antece-
dent. If the modifier ?other? or ?another? is re-
moved, an anaphor becomes the super set includ-
ing the antecedent. Thus, other-anaphora in fact 
represents a ?part-whole? relation. Consider the 
following text  
 ?IL-10 inhibits nuclear stimulation of nuclear 
factor kappa B (NF kappa B).  
Several other transcription factors including NF- 
IL-6, AP-1, AP-2, GR, CREB, Oct-1, and Sp-1 
are not affected by IL-10.?  
Here, the expression ?other transcription fac-
tors? is an other-anaphor, while the ?NF kappa 
B? is its antecedent. The anaphor refers to any 
transcription factors except the antecedent.  By 
removing the lexical modifier ?other?, we can 
get a supper set ?transcription factors? that in-
cludes the antecedent. The anaphor and antece-
dent thus have a ?part-whole? relation1.  
Other-anaphora resolution is an important 
sub-task in information extraction for biomedical 
                                                 
1 Other-anaphora could be also held between ex-
pressions that have subset-set or member-collection 
relations. In this paper, we treat them in a uniform 
way by using the patterned-based method. 
121
  
domain. It also contributes to biomedical ontolo-
gy building as it targeted at a ?part-whole? rela-
tion which is in the same hierarchical orders as in 
ontology. Furthermore, other-anaphora resolu-
tion is a first-step exploration in the resolution of 
bridging anaphora. Furthermore, other-anaphora 
resolution is a first-step exploration in the resolu-
tion of bridging, a special anaphora phenomenon 
in which the semantic relation between an ana-
phor and its antecedent is more complex (e.g. 
part-whole) than co-reference. 
Previous work on other-anaphora resolution 
relies on knowledge resources, for example, on-
tology like WordNet to determine the ?part-
whole? relation. However, in the biomedical do-
main, a document is full of technical terms which 
are usually missing in a general-purpose ontolo-
gy. To deal with this problem, pattern-based ap-
proaches have been widely employed, in which a 
pattern that represents the ?part-whole? relation 
is designed. Two expressions are connected with 
the specific pattern and form a query. The query 
is searched in a large corpus for the occurrence 
frequency which would indicate how likely the 
two given expressions have the part-whole rela-
tion. The solution can avoid the efforts of con-
structing the ontology knowledge for the "part-
whole" relation. However, the pattern is designed 
in an ad-hoc method, usually from linguistic in-
tuition and its effectiveness for other-anaphora 
resolution is not guaranteed. 
In this paper, we propose a method to auto-
matically mine effective patterns for other-
anaphora resolution in biomedical texts. Our me-
thod runs on a small collection of seed word 
pairs. It searches a large corpus (e.g., PubMed 
abstracts as in our system) for the texts where the 
seed pairs co-occur, and collects the surrounding 
words as the surface patterns. The automatically 
found patterns will be used in a machine learning 
framework for other-anaphora resolution. To our 
knowledge, our work is the first effort of apply-
ing the pattern-base technique to other-anaphora 
resolution in biomedical texts. 
The rest of this paper is organized as follows. 
Section 2 introduces previous related work. Sec-
tion 3 describes the machine learning framework 
for other-anaphora resolution. Section 4 presents 
in detail our method for automatically pattern 
mining. Section 5 gives experiment results and 
has some discussions. Finally, Section 6 con-
cludes the paper and shows some future work. 
2 Related Work 
Previous work on other-anaphora resolution 
commonly depends on human engineered know-
ledge and/or deep semantic knowledge for the 
?part-whole? relation, and mostly works only in 
the news domain. 
Markert et al, (2003) presented a pattern-
based algorithm for other-anaphor resolution. 
They used a manually designed pattern ?ANTE-
CEDENT and/or other ANAPHOR ?. Given two 
expression to be resolved, a query is formed by 
instantiating the pattern with the two given ex-
pressions. The query is searched in the Web. The 
higher the hit number returned, the more likely 
that the anaphor and the antecedent candidate 
have the ?part-whole? relation. The anaphor is 
resolved to the candidate with the highest hit 
number. Their work was tested on 120 other-
anaphora cases extracted from Wall Street Jour-
nal. The final accuracy was 52.5%. 
Modjeska et al, (2003) also presented a simi-
lar pattern-based method for other-anaphora res-
olution, using the same pattern ?ANTECEDENT 
and/or other ANAPHOR?. The hit number re-
turned from the Web is used as a feature for a 
Na?ve Bayesian Classifier to resolve other-
anaphors. Other features include surface words, 
substring matching, distance, gender/number 
agreement, and semantic tag of the NP. They 
evaluated their method with 500 other-anaphor 
cases extracted from Wall Street Journal, and 
reported a result of 60.8% precision and 53.4% 
recall. 
Markert and Nissim (2005) compared three 
systems for other-anaphora resolution, using the 
same data set as in (Modjeska et al, 2003). 
The first system consults WordNet for the 
part-whole relation. The WordNet provides in-
formation on meronym/holonym (part-of rela-
tion) and hypernym/ hyponym (type-of relation). 
Their system achieves a performance of 56.8% 
for precision and 37.0% for recall. 
The second and third systems employ the pat-
tern based approach, employing the same manual 
pattern ?ANTECEDENT and/or other ANA-
PHOR?. The second system did search in British 
Nation Corpus, giving 62.6% precision and 
26.2% recall. The third system did search in the 
Web as in (Markert et al, 2003), giving 53.8% 
precision and 51.7% recall. 
122
  
3 Anaphora Resolution System 
3.1 Corpus 
In our study, we used the GENIA corpus2 for our 
other-anaphora resolution in biomedical texts. 
The corpus consists of 2000 MEDLINE abstracts 
(around 440,000 words). From the GENIA cor-
pus, we extracted 598 other-anaphora cases. The 
598 cases do not contain compound prepositions 
or idiomatic uses of ?other?, like ?on the other 
hand? and ?other than?. And all these anaphors 
have their antecedents found in the current and 
previous two sentences of the other-anaphor. On 
average, there are 15.33 candidate antecedents 
for each anaphor to be resolved. 
To conduct other-anaphora resolution, an in-
put document is preprocessed through a pipeline 
of NLP components, including tokenization, sen-
tence boundary detection, part-of-speech (POS) 
tagging, noun phrase (NP) chunking, and named-
entity recognition (NER). These preprocessing 
modules are aimed to determine the boundaries 
of each NP in a text, and to provide necessary 
information of an NP for subsequent processing. 
In our system, we employed the tool-kits built by 
our group for these components. The POS tagger 
was trained and tested on the GENIA corpus 
(version 2.1) and achieved an accuracy of 97.4%. 
The NP-chunking module, evaluated on UPEN 
WSJ TreeBank, produced 94% F-measure. The 
NER module, trained on GENIA corpus (version 
3.0), achieved 71.2% F-measure covering 22 ent-
ity types (e.g., Virus, Protein, Cell, DNA, etc). 
3.2 Learning Framework 
Our other-anaphora resolution system adopts the 
common learning-based model for identity-
anaphora resolution, as employed by (Soon et al, 
2001) and (Ng and Cardie, 2002). 
In the learning framework, a training or test-
ing instance has the form of ?? ?????? ,???  
where ??????  is the ?
th candidates of the antece-
dent of anaphor ???. An instance is labelled as 
positive if ??????  is the antecedent of  ??? , or 
negative if ??????  is not the antecedent of  ???. 
An instance is associated with a feature vector 
which records different properties and relations 
between ???  and ?????? . The features used in 
our system will be discussed later in the paper. 
During training, for each other-anaphor, we 
consider as the candidate antecedents the preced-
ing NPs in its current and previous two sentences. 
                                                 
2 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ 
A positive instance is formed by pairing the ana-
phor and the correct antecedent. And a set of 
negative instances is formed by pairing the ana-
phor and each of the other candidates.  
Based on these generated training instances, 
we can train a binary classifier using any dis-
criminative learning algorithm. In our work, we 
employed support vector machine (SVM) due to 
its good performance in high dimensional feature 
vector spaces. 
During the resolution process, for each other-
anaphor encountered, all of the preceding NPs in 
a three-sentence window are considered. A test 
instance is created for each of the candidate ante-
cedents. The feature vector is presented to the 
trained classifier to determine the other-
anaphoric relation. The candidate with highest 
SVM outcome value is selected as the antecedent.  
3.3 Baseline Features 
Knowledge is usually represented as features for 
machine learning. In our system, we used the 
following groups of features for other-anaphora 
resolution 
 
? Word Distance Indicator 
This feature measures the word distance between 
an anaphor and a candidate antecedent, with the 
assumption that the candidate closer to the ana-
phor has a higher preference to be the antecedent. 
? Same Sentence Indicator 
This feature is either 0 or 1 indicating whether an 
anaphor and a candidate antecedent are in the 
same sentence. Here, the assumption is that the 
candidate in the same sentence as the anaphor is 
preferred for the antecedent. 
? Semantic Group Indicators 
A named-entity can be classified to a semantic 
category such as ?DNA?, ?RNA?, ?Protein? and 
so on3. Thus we use a set of features to record the 
category pair of an anaphor and a candidate ante-
cedent. For example, ?DNA-DNA? is generated 
for the case when both anaphor and candidate are 
DNAs. And ?DNA-Protein? is generated if an 
anaphor is a DNA and a candidate is a protein. 
These features indicate whether a semantic group 
can refer to another.  
Note that an anaphor and its antecedent may 
possibly belong to different semantic categories. 
For example, in the GENIA corpus we found that 
                                                 
3 In our study, we followed the semantic categories defined 
in the annotation scheme of the GENIA corpus.  
123
  
in some cases an expression of a protein name 
actually denotes the gene that encodes the pro-
tein. Thus for a given anaphor and a candidate 
under consideration, it is necessary to record the 
pair-wise semantic groups, instead of using a 
single feature indicating whether two expressions 
are of the same group. 
The semantic group for a named entity is giv-
en by our preprocessing NER. For the common 
NPs produced from the NP chunker, we classify 
the semantic group by looking for the words in-
side NPs. For example, an NP ending with 
?cells? is classified to ?Cell? group while an NP 
ending with ?gene? or ?allele? is classified to 
?DNA? group. 
? Lexical Pattern Indicators 
In some cases, the surrounding words of an ana-
phor and a candidate antecedent strongly indicate 
the ?part-whole? relation. For example, in 
?...asthma and other hypereosinophilic diseas-
es?, the reference between ?other hypereosino-
philic diseases? and ?asthma? is clear if the in-
between words ?and other? are taken into con-
sideration. Another example of such a hint pat-
tern is ?one? the other ?? The feature is 1 if the 
specific patterns are present for the current ana-
phor and candidate pair. A candidate with such a 
feature is preferred to be the antecedent. 
? Hierarchical Name Indicator  
This feature indicates whether an antecedent 
candidate is a substring of an anaphor or vice 
versa. This feature is used to capture cases like 
?Jun? and ?JunB? (?Jun? is a family of protein 
while ?JunB? is a member of this family). In 
many cases, an expression that is a super set 
comes with certain postfix words, for example, 
?family members? in  
?Fludarabine caused a specific depletion of 
STAT1 protein (and mRNA) but not of other 
STAT family members.?  
This kind of phenomenon is more common in 
bio-medical texts than in news articles. 
3.4 SVM Training and Classification 
In our system, we utilized the open-source soft-
ware SVM-Light4 for the classifier training and 
testing.  SVM is a robust statistical model which 
has been applied to many NLP tasks. SVM tries 
to learn a separating line to separate the positive 
instances from negative instances. Kernel trans-
formations are applied for non-linear separable 
                                                 
4 http://svmlight.joachims.org/ 
cases (Vapnik, 1995). In our study, we just used 
the default learning parameters provided by 
SVM-Light with the linear kernel. A more so-
phisticated kernel may further improve the per-
formance. 
4 Using Auto-mined Pattern Features 
The baseline features listed in Section 3.3 only 
rely on shallow lexical, position and semantic 
information about an anaphor and a candidate 
antecedent. It could not, nevertheless, disclose 
the ?part-whole? relation between two given ex-
pressions. In section 2, we have shown some ex-
isting pattern-based solutions that mine the ?part-
whole? relation in a large corpus with some pat-
terns that can represent the relation. However, 
these manually designed patterns are usually se-
lected by heuristics, which may not necessarily 
lead to a high coverage with a good accuracy in 
different domains. To overcome this shortcom-
ing, we would like to use an automatic method to 
mine effective patterns from a large data set. 
First, we create a set of seed pairs of the ?part-
whole? relation. And then, we use the seed pairs 
to discover the patterns that encode the ?part-
whole? relation from a large data set (PubMed as 
in our system). Such a solution is supposed to 
improve the coverage of lexical patterns, while 
still retain the desired ?part-whole? relation for 
other-anaphora resolution. 
The overview of our system with the automat-
ic mined patterns is illustrated in figure 1. 
Seed Pairs 
Generation
Pattern Mining
SVM
GENIA 
Corpus
Seed 
Pairs
Lexical 
Patterns
GENIA
T st 
Cas s
PubMED 
Corpus
 
Figure 1: System Overview 
There are three major parts in our system, 
namely, seed-pairs generation, pattern mining 
and SVM learning and classification. In the sub-
sequent subsections, we will discuss each of the 
three parts in details. 
124
  
4.1 Seed Pairs Preparation 
A seed pair is a pair of phrases/words following 
?part-whole? order, for example,  
?integrin alpha? - ?adhesion molecules? 
where ?integrin alpha? is a kind of ?adhesion 
molecules?.  
We extracted the seed pairs automatically 
from the GENIA corpus. The auto-extracting 
procedure makes uses of some lexical clues like 
?A, such as B, C and D?, ?A (e.g. B and C)?, ?A 
including B? and etc. The capital letter A, B, C 
and D refer to a noun phrase such as ?integrin 
alpha? and ?adhesion molecules?. For each oc-
currence of ?A such as B, C and D?, the program 
will generate seed pairs ?B-A?, ?C-A? and ?D-
A?. 
Consider the following example, 
?Mouse thymoma line EL-4 cells produce cyto-
kines such as interleukin (IL) -2, IL-3, IL-4, IL-
10, and granulocyte-macrophage colony-
stimulating factor in response to phorbol 12-
myristate 13-acetate (PMA).? 
We can extract the following seed pairs, 
?interleukin (IL) -2? ? ?cytokines? 
?IL -3? ? ?cytokines? 
?IL -4? ? ?cytokines? 
?IL -10? ? ?cytokines? 
?granulocyte-macrophage colony-stimulating 
factor? ? ?cytokines?  
A similar action is taken for other lexical 
clues. Totally, we got 909 distinct seed pairs ex-
tracted from the GENIA corpus. 
After the seed pairs have been extracted, an 
automatic verification of the seed pairs is per-
formed. The first purpose of the verification is to 
correct chunking errors. For example, ?HLA 
Class II Gene? may likely be wrongly split into 
?HLA Class? and ?II Gene?. This kind of errors 
is repaired by several simple syntactic rules. The 
second purpose of the verification is to remove 
the inappropriate seed pairs. In our system, we 
abandoned the seed pairs containing pronouns 
like ?those?, ?they?, or nouns like ?element?, 
?member? and ?agent?. Such seed pairs may ei-
ther find no patterns, or lead to meaningless pat-
terns because ?those? or ?elements? have no spe-
cific semantics and could refer to anything. 
4.2 Pattern Mining 
Having obtained the set of seed pairs, we will use 
them to mine patterns for the ?part-whole? rela-
tion. For each seed pair ?antecedent - anaphor? 
(anaphor represents the NP for the ?whole?, 
while antecedent represents the NP for the 
?part?), our system will search in a large data set 
for two queries: ?antecedent * anaphor? and 
?anaphor * antecedent? where the ?*? denotes 
any sequence of words or symbols. For a re-
turned search results, the text in between ?ante-
cedent? and ?anaphora? is extracted as a pattern. 
In our study, we used PubMed 2007 data set 
for the pattern mining. The data set contains 
about 52,000 abstracts with around 9,400,000 
words, and is an ideal large-scale resource for 
pattern mining. 
Consider, as an example, a seed pair ?NK 
kappa B ? ? ?transcription factor?. Suppose that 
a returned sentence for the query ?NK kappa B * 
transcription factor? is  
?...NK kappa B family transcription factors...? 
And a returned sentence for the query ?transcrip-
tion factor * NK kappa B? is 
?...transcription factors, including NF kappa 
B...? 
We can extract a pattern, 
?ANTECEDENT family ANAPHOR? from the 
first sentence and a pattern 
?ANAPHOR, including ANTECEDENT? from 
the second sentence.  
We restrict the patterns so that no pattern span 
across two or more sentences. In other words, the 
pattern shall not contain the symbol ?.?. The vi-
olated patterns will be removed. 
The count that a pattern occurs in the PubMed 
for a seed pair is recorded. As a pattern could be 
reduced by different seed pairs, we define the 
occurrence frequency of a pattern as the sum of 
the counts of the pattern for all the seed pairs, 
using following formula: 
???? ? =  ???(???? , ?? )
????
                           ??(1)   
where ???? ?  is the frequency of pattern ???? ; ??  is 
a seed pair; ?  is the set of all seed pairs. 
???(???? , ?? ) is the count of the pattern ????  for 
?? . 
All the mined patterns are sorted according to 
its frequency as defined in ??(1). 
4.3 Pattern Application 
For classifier training and testing, the patterns 
with high frequency are used as features. In our 
system, we used the top 40 patterns, while we 
also examined the influence the number of the 
patterns on the performance. (See Section 5.2) 
Given an instance ??(?????? , ???) and a pat-
tern feature ????  , a query is constructed by in-
125
  
stantiating with ??????  and ??? . For example, 
for an instance ??("?? ????? ?", "???????-  
?????? ???????")  and a pattern feature ?ANA-
PHOR, including ANTECEDENT?, we can get 
a query ?transcription factors, including NF 
kappa B?. The query is searched in the PubMed 
data set. The count of the query is recorded. The 
value of the pattern feature of a candidate is cal-
culated by normalizing the occurrence frequency 
among all the candidates of the anaphor. 
For demonstration, suppose we have an ana-
phor ?other transcription factors? with two ante-
cedent candidates ?IL-10? and ?NF kappa B?. 
Given a pattern feature ?ANAPHOR, including 
ANTECEDENT?, the count of the query ?tran-
scription factors, including IL-10? is 100 while 
that for ?transcription factors, including NF-
Kappa B? is 300. Then the values of the pattern 
feature for ?IL-10? and ?NF kappa B? are 0.25 
(
100
100+300
) and 0.75 (
300
100+300
), respectively. 
The value of a pattern feature can be inter-
preted as a degree of belief that an anaphor and a 
candidate antecedent have the ?part-whole? rela-
tion, with regard to the specific pattern. Since the 
value of a pattern feature is normalized among 
all the candidates, it could indicate the preference 
of a candidate against other competing candi-
dates. 
5 Experiment Results 
5.1 Experiments Setup 
In our experiments, we conducted a 3-fold cross 
validation to evaluate the performances. The total 
598 other-anaphora cases were divided into 3 
sets of size 200, 199 and 199 respectively. For 
each experiment, two sets were used for training 
while the other set was used for testing.  
For evaluation, we used the accuracy as the 
performance metric, which is defined as the cor-
rectly resolved other-anaphors divided by all the 
testing other-anaphors, that is, 
 
???????? =
# of correctly resolved anaphors
 # of total anaphors
 
5.2 Experiments Results 
Table 1 shows the performance of different 
other-anaphora resolution systems. The first line 
is for the baseline system with only the normal 
features as described in Section 3.3. From the 
table, we can find that the baseline system only 
achieves around 40% accuracy. A performance is 
lower than a similar system in news domain by 
Modjeska et al, (2003) where they reported  
51.6 % precision with 40.6% recall. This differ-
ence is probably because they utilized more se-
mantic knowledge such as hypernymy and mero-
nymy acquired from WordNet. Such knowledge, 
nevertheless, is not easily available in the bio-
medical domain. 
 
Sys Fold-1 Fold-2 Fold-3 Overall 
Baseline 
No Pattern 
42.0 % 
84/200 
38.2 % 
76/199 
40.7 % 
81/199 
40.3 % 
241/598 
Manual 
Pattern 
49.0 % 
98/200 
45.7 % 
91/199 
47.7 % 
95/199 
47.5 % 
284/598 
Auto-
mined 
Pattern 
59.0 % 
118/200 
53.8 % 
107/199 
56.8 % 
113/199 
56.5 % 
338/598 
Table 1: Performance Comparisons 
In our experiments, we tested the system with 
manually designed pattern features. We tried 10 
patterns that can represent the ?part-whole? rela-
tion. Table 2 summaries the patterns used in the 
system. Among them, the pattern ?Anaphor such 
as Antecedent? and ?Antecedent and other Ana-
phor? are commonly used in previous pattern 
based approaches (Markert et al, 2003; Mod-
jeska et al, 2003). 
 
Pattern 
ANTECEDENT is a kind of ANAPHOR 
ANTECEDENT is a type of ANAPHOR 
ANTECEDENT is a member of ANAPHOR 
ANTECEDENT is a part of ANAPHOR 
ANAPHOR such as ANTECEDENT 
ANTECEDENT and other ANAPHOR 
ANTECEDENT within ANAPHOR 
ANTECEDENT is a component of ANAPHOR 
ANTECEDENT is a sort of ANAPHOR 
ANTECEDENT belongs to ANAPHOR 
Table 2: Manually Selected Patterns 
 
The second line of Table 1 shows the results 
of the system with the manual pattern features. 
We can find that adding these pattern features 
produces an overall accuracy of 47%, yielding an 
increase of 7% accuracy against the baseline sys-
tem without the pattern features.  
The improvement in accuracy is consistent 
with previous work using the pattern-based ap-
proaches in the news domain (Modjeska et al, 
2003). However, we found the performance in 
the biomedical domain is worse than that in the 
news domain. For example, Modjeska et al 
(2003) reported a precision around 53%. This 
difference of performance suggests that the ma-
126
  
nually designed patterns may not necessarily 
work equally well in different domains.  
The last system we examined in the experi-
ment is the one with the automatically mined 
pattern features. Table 3 summarizes the top 
mined patterns ranked based on their occurrence 
frequency. Some of the patterns are intuitively 
good representation of the ?part-whole? relation. 
For example, ?ANAPHOR, including ANTE-
CEDENT?. ?ANAPHOR, such as ANTECE-
DENT? and ?ANAPHOR and other ANTECE-
DENT? which are in the manually designed pat-
tern list, are generated.  
The last line of Table 1 lists the result of the 
system with automatically mined pattern fea-
tures. It outperforms the baseline system (up to 
16% accuracy), and the system with manually 
selected patterns (9% accuracy). These results 
prove that our pattern features are effective for 
the other-anaphora resolution.  
 
Pattern Freq 
ANAPHOR, including ANTECEDENT 1213 
ANAPHOR including ANTECEDENT 726 
ANTECEDENT family ANAPHOR 583 
ANAPHOR such as ANTECEDENT 542 
ANTECEDENT transcription ANAPHOR 439 
ANAPHOR, such as ANTECEDENT 295 
ANTECEDENT and other ANAPHOR 270 
ANAPHOR and ANTECEDENT 250 
ANTECEDENT, dendritic ANAPHOR 246 
ANTECEDENT and ANAPHOR 238 
ANTECEDENT human ANAPHOR 223 
ANAPHOR (e.g., ANTECEDENT  213 
ANTECEDENT/rel ANAPHOR 188 
ANTECEDENT-like ANAPHOR 188 
ANAPHOR against ANTECEDENT  163 
Table 3: Auto-Mined Patterns 
To further compare the manually designed 
patterns and the automatically discovered pat-
terns. We examined the coverage rate of the two 
pattern sets. The coverage rate measures the ca-
pability that a set of patterns could lead to posi-
tive anaphor-antecedent pairs. An other-anaphor 
is said to be covered by a pattern set, if the ana-
phor and its antecedent could be hit (i.e., the cor-
responding query has a non-zero hit number) by 
at least one pattern in the list. Thus the coverage 
rate could be defined as 
????????(?)  
=   
#anaphors covered by the pattern set P
# total anaphors
 
The coverage rates of the two pattern sets are 
tabulated in table 4. It is apparent that the auto-
mined patterns have a significantly higher cover-
age (more than twice) than the manually de-
signed patterns. 
 
Patterns Coverage Rate 
Manually Designed 36.0 % 
Auto-Mined 92.1 % 
Table 4: Coverage Comparison 
In our experiments we were also concerned 
about the usefulness of each individual pattern. 
For this purpose, we examined the loss of the 
accuracy when withdrawing a pattern feature 
from the feature list. The top 10 patterns with the 
largest accuracy loss are summarized in table 5. 
 
Pattern 
Acc 
Loss 
ANAPHOR, including ANTECEDENT 4.18% 
ANAPHOR including ANTECEDENT 3.18% 
ANAPHOR such as ANTECEDENT 2.84% 
ANTECEDENT transcription ANAPHOR 2.17% 
ANTECEDENT and other ANAPHOR 2.01% 
ANAPHOR, such as ANTECEDENT 1.84% 
ANTECEDENT family ANAPHOR 1.84% 
ANAPHOR (e.g., ANTECEDENT 1.51% 
ANTECEDENT-like ANAPHOR 1.17% 
ANTECEDENT/rel ANAPHOR 1.17% 
Table 5: Usefulness of Each Pattern 
The process of automatic pattern mining 
would generate numerous surface patterns. It is 
not reasonable to use all the patterns as features. 
As mentioned in section 4.3, we rank the pattern 
based on their occurrence frequency and select 
the top ones as the features. It would be interest-
ing to see how the number of patterns influences 
the performance of anaphora resolution. In figure 
2, we plot the accuracy under different number 
top pattern features. We can find by using more 
patterns, the coverage keeps increasing. The ac-
curacy also increases, but it reaches the peak 
with around 40 patterns. With more patterns, the 
accuracy remains at the same level. This is be-
cause the low frequency patterns usually are not 
that indicative of the ?part-whole? relation. In-
cluding these pattern features would bring noises 
but not help the performance. The flat curve after 
the peak point suggests that the machine learning 
algorithm can effectively identify the importance 
of the pattern features for the resolution decision, 
and therefore including non-indicative patterns 
would not damage the performance. 
In our experiment, we also interested to com-
pare the utility of PubMed with other general 
data sets. Thus, we tested pattern mining by us-
127
  
ing the Google-5-grams corpus5 which lists the 
hit number of all the queries of five words or less 
in the Web. Unfortunately, we found that the per-
formance is worse than using PubMed. The pat-
terns mined from the Web corpus only gives an 
accuracy of around 41%, almost the same as the 
baseline system without using any pattern fea-
tures. The bad performance is due to the fact that 
most of bio-medical names are quite long (2~4 
words) and occur infrequently in the non-
technique data set. Consequently, a query formed 
by a biomedical seed pair usually cannot be 
found in the Web corpus (We found the coverage 
of the auto-mined patterns mined from the corpus 
is only about 20%). 
 
Figure 2: Performance of Various No. of Patterns 
6 Conclusion & Future Works 
In this paper, we have presented how to automat-
ically mined pattern features for learning-based 
other-anaphora resolution in bio-medical texts. 
The patterns that represent the ?part-whole? rela-
tions are automatically mined from a large data 
set. They are used as features for a SVM-based 
classifier learning and testing. The results of our 
experiments show a reasonably good perfor-
mance with 56.5% accuracy). It outperforms 
(16% in accuracy) the baseline system without 
the pattern features, and also beats (9%) the sys-
tem with manually designed pattern features. 
There are several directions for future work. 
We would like to employ a pattern pruning 
process to remove those less indicative patterns 
such as ?ANAPHOR, ANTECEDENT?. And we 
also plan to perform pattern normalization which 
integrates two similar or literally identical pat-
                                                 
5 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?  
   catalogId=LDC2006T13 
terns into a single one. By doing so, the useful 
patterns may come to the top of the pattern list. 
Also we would like to explore ontology re-
sources like MESH and Genes Ontology, which 
can provide enriched hierarchies of bio-medical 
terms and thus would benefit other-anaphora res-
olution. 
Acknowledgements  
This study on co-reference resolution is partially supported 
by a Specific Targeted Research Project (STREP) of the 
European Union's 6th Framework Programme within IST 
call 4, Bootstrapping of Ontologies and Terminologies 
STrategic REsearch Project (BOOTStrep). 
References 
Castano J, Zhang J and Pustejovsky J. Anaphora Resolution 
in Biomedical Literature. Submitted to International Sym-
posium on Reference Resolution 2002, Alicante, Spain 
Clark H. Bridging. In Thinking. Readings in Cognitive 
Science. Johnson-Laird and Wason edition. Cambridge. 
Cambridge University Press; 1977.411?420 
Gasperin C and Vieira R. Using Word Similarity Lists for 
Resolving Indirect Anaphora. In Proceedings of ACL 
Workshop on Reference Resolution and Its Application. 
30 June 2004; Barcelona. 2004.40-46 
Girju R, Badulescu A and Moldovan D. Automatic Discov-
ery of Part-Whole Relations. Computational Linguistics, 
2006, 32(2):83-135 
Bernauer J.. Analysis of Part-Whole Relation and Subsump-
tion in Medical Domain. Data Knowledge Enginnering 
1996, 20:405-415 
Markert K. and Nissim M. Comparing Knowledge Sources 
for Nominal Anaphora Resolution. Computational Lin-
guistics, 2005, 31(3):367-402 
Markert K, Modjeska N and Nissim M. Using the Web for 
Nominal Anaphora Resolution. In Proceedings of EACL 
Workshop on the Computational Treatment of Anaphora. 
14 April 2003; Budapest. 2003.39-46 
Mitokov R. Anaphor Resolution. The State of The Art. 
Working Paper, University of Wolverhampton, UK, 1999 
Modjeska N, Markert K and Nissim M. Using the Web in 
Machine Learning for Other-anaphor Resolution. In Pro-
ceedings of the 2003 Conference on Empirical Methods in 
Natural Language Processing. July2003,Sapporo.176-183 
Soon WM, Ng HT and Lim CY. A Machine Learning Ap-
proach to Coreference Resolution of Noun Phrases. Com-
putational Linguistics, 2001, 27(4).521-544 
Vapnik, V. Chapter 5 Methods of Pattern Recognition. In 
The Nature of Statistical Learning Theory. New York. 
Springer-Verlag, 1995.123-167 
Varzi C.  Parts, Wholes, and Part-whole Relation. The Pros-
pects of the Mereotopology. Data & Knowledge Engi-
neering, 1996, 20.259-286 
Vieira R, Bick E, Coelho J, Muller V, Collovini S, Souza J 
and Rino L. Semantic Tagging for Resolution of Indirect 
Anaphora. In Proceedings of 7th SIGdial Workshop on 
Discourse and Dialogue. July 2006; Sydney.76-79 
Burges C. A Tutorial on Supporting Vector Machines for 
Pattern Recognition. Data Mining and Knowledge Dis-
covery 1998, 2:121-167 
Ng V. and Cardie C. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of An-
nual Conference for Association of Computational Lin-
guistics 2002, Philadelphia.104-111 
128
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 188?196,
Beijing, August 2010
A Twin-Candidate Based Approach for Event Pronoun Resolution us-
ing Composite Kernel  
Chen Bin1 Su Jian2 Tan Chew Lim1 
1National University of Singapore 2Institute for Inforcomm Research, A-STAR 
{chenbin,tancl}@comp.nus.edu.sg sujian@i2r.a-star.edu.sg 
 
Abstract 
Event Anaphora Resolution is an important 
task for cascaded event template extraction 
and other NLP study. In this paper, we provide 
a first systematic study of resolving pronouns 
to their event verb antecedents for general 
purpose. First, we explore various positional, 
lexical and syntactic features useful for the 
event pronoun resolution. We further explore 
tree kernel to model structural information 
embedded in syntactic parses. A composite 
kernel is then used to combine the above di-
verse information. In addition, we employed a 
twin-candidate based preferences learning 
model to capture the pair wise candidates? pre-
ference knowledge. Besides we also look into 
the incorporation of the negative training in-
stances with anaphoric pronouns whose ante-
cedents are not verbs. Although these negative 
training instances are not used in previous 
study on anaphora resolution, our study shows 
that they are very useful for the final resolu-
tion through random sampling strategy. Our 
experiments demonstrate that it?s meaningful 
to keep certain training data as development 
data to help SVM select a more accurate hyper 
plane which provides significant improvement 
over the default setting with all training data. 
1 Introduction 
Anaphora resolution, the task of resolving a giv-
en text expression to its referred expression in 
prior texts, is important for intelligent text 
processing systems. Most previous works on 
anaphora resolution mainly aims at object ana-
phora in which both the anaphor and its antece-
dent are mentions of the same real world objects 
In contrast, an event anaphora as first defined 
in (Asher, 1993) is an anaphoric reference to an 
event, fact, and proposition which is representa-
tive of eventuality and abstract entity. Consider 
the following example: 
This was an all-white, all-Christian community 
that all the sudden was taken over -- not taken 
over, that's a very bad choice of words, but [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one of the town's two meat-packing plants 13 
years ago. 
The anaphor [It]2 in the above example refers 
back to an event, ?all-white and all-Christian city 
of Postville is diluted by different ethnic groups.? 
Here, we take the main verb of the event, [in-
vaded]1 as the representation of this event and 
the antecedent for pronoun [It]2.  
According to (Asher, 1993), antecedents of 
event pronoun include both gerunds (e.g. de-
struction) and inflectional verbs (e.g. destroying). 
In our study, we focus on the inflectional verb 
representation, as the gerund representation is 
studied in the conventional anaphora resolution. 
For the rest of this paper, ?event pronouns? are 
pronouns whose antecedents are event verbs 
while ?non-event anaphoric pronouns? are those 
with antecedents other than event verbs. 
 Entity anaphora resolution provides critical 
links for cascaded event template extraction. It 
also provides useful information for further infe-
rence needed in other natural language 
processing tasks such as discourse relation and 
entailment. Event anaphora (both pronouns and 
noun phrases) contributes a significant propor-
tion in anaphora corpora, such as OntoNotes. 
19.97% of its total number of entity chains con-
tains event verb mentions. 
In (Asher, 1993) chapter 6, a method to re-
solve references to abstract entities using dis-
course representation theory is discussed. How-
ever, no computation system was proposed for 
entity anaphora resolution. (Byron, 2002) pro-
posed semantic filtering as a complement to sa-
lience calculations to resolve event pronoun tar-
geted by us. This knowledge deep approach only 
188
works for much focused domain like trains spo-
ken dialogue with handcraft knowledge of rele-
vant events for only limited number of verbs in-
volved.  Clearly, this approach is not suitable for 
general event pronoun resolution say in news 
articles. Besides, there?s also no specific perfor-
mance report on event pronoun resolution, thus 
it?s not clear how effective their approach is. 
(M?ller, 2007) proposed pronoun resolution sys-
tem using a set of hand-crafted constraints such 
as ?argumenthood? and ?right-frontier condition? 
together with logistic regression model based on 
corpus counts. The event pronouns are resolved 
together with object pronouns. This explorative 
work produced an 11.94% F-score for event pro-
noun resolution which demonstrated the difficul-
ty of event anaphora resolution. In (Pradhan, 
et.al, 2007), a general anaphora resolution sys-
tem is applied to OntoNotes corpus. However, 
their set of features is designed for object ana-
phora resolution. There is no specific perfor-
mance reported on event anaphora. We suspect 
the event pronouns are not correctly resolved in 
general as most of these features are irrelevant to 
event pronoun resolution.  
In this paper, we provide the first systematic 
study on pronominal references to event antece-
dents. First, we explore various positional, lexi-
cal and syntactic features useful for event pro-
noun resolution, which turns out quite different 
from conventional pronoun resolution except 
sentence distance information. These have been 
used together with syntactic structural informa-
tion using a composite kernel. Furthermore, we 
also consider candidates? preferences informa-
tion using twin-candidate model. 
Besides we further look into the incorporation 
of negative instances from non-event anaphoric 
pronoun, although these instances are not used in 
previous study on co-reference or anaphora reso-
lution as they make training instances extremely 
unbalanced. Our study shows that they can be 
very useful for the final resolution after random 
sampling strategy.  
We further demonstrate that it?s meaningful to 
keep certain training data as development data to 
help SVM select a more accurate hyper-plane 
which provide significant improvement over the 
default setting with all training data.  
The rest of this paper is organized as follows.  
Section 2 introduces the framework for event 
pronoun resolution, the considerations on train-
ing instance, the various features useful for event 
pronoun resolution and SVM classifier with ad-
justment of hyper-plane. Twin-candidate model 
is further introduced to capture the preferences 
among candidates. Section 3 presents in details 
the structural syntactic feature and the kernel 
functions to incorporate such a feature in the res-
olution. Section 4 presents the experiment results 
and some discussion. Section 5 concludes the 
paper. 
2 The Resolution Framework 
Our event-anaphora resolution system adopts the 
common learning-based model for object ana-
phora resolution, as employed by (Soon et al, 
2001) and (Ng and Cardie, 2002a). 
2.1 Training and Testing instance 
In the learning framework, training or testing 
instance of the resolution system has a form of 
               where        is the i
th candi-
date of the antecedent of anaphor    . An in-
stance is labeled as positive if        is the ante-
cedent of      , or negative if        is not the 
antecedent of     . An instance is associated 
with a feature vector which records different 
properties and relations between     and       . 
The features used in our system will be discussed 
later in this paper.  
During training, for each event pronoun, we 
consider the preceding verbs in its current and 
previous two sentences as its antecedent candi-
dates. A positive instance is formed by pairing an 
anaphor with its correct antecedent. And a set of 
negative instances is formed by pairing an ana-
phor with its candidates other than the correct 
antecedent. In addition, more negative instances 
are generated from non-event anaphoric pro-
nouns. Such an instance is created by pairing up 
a non-event anaphoric pronoun with each of the 
verbs within the pronoun?s sentence and previous 
two sentences. This set of instances from non-
event anaphoric pronouns is employed to provide 
extra power on ruling out non-event anaphoric 
pronouns during resolution. This is inspired by 
the fact that event pronouns are only 14.7% of all 
the pronouns in the OntoNotes corpus. Based on 
these generated training instances, we can train a 
binary classifier using any discriminative learn-
ing algorithm. 
189
The natural distribution of textual data is of-
ten imbalanced. Classes with fewer examples are 
under-represented and classifiers often perform 
far below satisfactory. In our study, this becomes 
a significant issue as positive class (event ana-
phoric) is the minority class in pronoun resolu-
tion task. Thus we utilize a random down sam-
pling method to reduce majority class samples to 
an equivalent level with the minority class sam-
ples which is described in (Kubat and Matwin, 
1997) and (Estabrooks et al 2004). In (Ng and 
Cardie, 2002b), they proposed a negative sample 
selection scheme which included only negative 
instances found in between an anaphor and its 
antecedent. However, in our event pronoun reso-
lution, we are distinguishing the event-anaphoric 
from non-event anaphoric which is different 
from (Ng and Cardie, 2002b). 
2.2 Feature Space 
In a conventional pronoun resolution, a set of 
syntactic and semantic knowledge has been re-
ported as in (Strube and M?ller, 2003; Yang et al 
2004;2005a;2006). These features include num-
ber agreement, gender agreement and many oth-
ers. However, most of these features are not use-
ful for our task, as our antecedents are inflection-
al verbs instead of noun phrases. Thus we have 
conducted a study on effectiveness of potential 
positional, lexical and syntactic features. The 
lexical knowledge is mainly collected from cor-
pus statistics. The syntactic features are mainly 
from intuitions. These features are purposely en-
gineered to be highly correlated with positive 
instances. Therefore such kind of features will 
contribute to a high precision classifier.  
? Sentence Distance 
This feature measures the sentence distance be-
tween an anaphor and its antecedent candidate 
under the assumptions that a candidate in the 
closer sentence to the anaphor is preferred to be 
the antecedent. 
? Word Distance  
This feature measures the word distance between 
an anaphor and its antecedent candidate. It is 
mainly to distinguish verbs from the same sen-
tence. 
? Surrounding Words and POS Tags 
The intuition behind this set of features is to find 
potential surface words that occur most frequent-
ly with the positive instances. Since most of 
verbs occurred in front of pronoun, we have built 
a frequency table from the preceding 5 words of 
the verb to succeeding 5 surface words of the 
pronoun. After the frequency table is built, we 
select those words with confidence1  > 70% as 
features. Similar to Surrounding Words, we have 
built a frequency table to select indicative sur-
rounding POS tags which occurs most frequently 
with positive instances. 
? Co-occurrences of Surrounding Words 
The intuition behind this set of features is to cap-
ture potential surface patterns such as ?It 
caused?? and ?It leads to?. These patterns are 
associated with strong indication that pronoun 
?it? is an event pronoun. The range for the co-
occurrences is from preceding 5 words to suc-
ceeding 5 words. All possible combinations of 
word positions are used for a co-occurrence 
words pattern. For example ?it leads to? will 
generate a pattern as ?S1_S2_lead_to? where S1 
and S2 mean succeeding position 1 and 2. Simi-
lar to previous surrounding words, we will con-
duct corpus statistics analysis and select co-
occurrence patterns with a confidence greater 
than 70%. Following the same process, we have 
examined co-occurrence patterns for surrounding 
POS tags.  
? Subject/Object Features 
This set of features aims to capture the relative 
position of the pronoun in a sentence. It denotes 
the preference of pronoun?s position at the clause 
level. There are 4 features in this category as 
listed below. 
Subject of Main Clause 
This feature indicates whether a pronoun is at the 
subject position of a main clause. 
Subject of Sub-clause 
This feature indicates whether a pronoun is at the 
subject position of a sub-clause. 
Object of Main Clause 
This feature indicates whether a pronoun is at the 
object position of a main clause. 
Object of Sub-clause 
This feature indicates whether a pronoun is at the 
object position of a sub-clause. 
? Verb of Main/Sub Clause 
Similar to the Subject/Object features of pro-
noun, the following two features capture the rela-
                                                 
1               
                                        
                    
 
190
tive position of a verb in a sentence. It encodes 
the preference of verb position between main 
verbs in main/sub clauses. 
Main Verb in Main Clause 
This feature indicates whether a verb is a main 
verb in a main clause. 
Main Verb in Sub-clause 
This feature indicates whether a verb is a main 
verb in a sub-clause. 
2.3 Support Vector Machine 
In theory, any discriminative learning algorithm 
is applicable to learn a classifier for pronoun res-
olution. In our study, we use Support Vector Ma-
chine (Vapnik, 1995) to allow the use of kernels 
to incorporate the structure feature. One advan-
tage of SVM is that we can use tree kernel ap-
proach to capture syntactic parse tree information 
in a particular high-dimension space. 
Suppose a training set   consists of labeled 
vectors          , where    is the feature vector 
of a training instance and    is its class label. The 
classifier learned by SVM is: 
                     
   
  
where    is the learned parameter for a support 
vector   . An instance   is classified as positive 
if       . Otherwise,   is negative. 
? Adjust Hyper-plane with Development Data 
Previous works on pronoun resolution such as 
(Yang et al 2006) used the default setting for 
hyper-plane which sets       . And an in-
stance is positive if        and negative oth-
erwise. In our study, we look into a method of 
adjusting the hyper-plane?s position using devel-
opment data to improve the classifier?s perfor-
mance.  
Considering a default model setting for SVM 
as shown in Figure 2(for illustration purpose, we 
use a 2-D example). 
 
Figure 2: 2-D SVM Illustration 
The objective of SVM learning process is to find 
a set of weight vector   which maximizes the 
margin (defined as  
   
) with constraints defined 
by support vectors. The separating hyper-plane is 
given by         as bold line in the center. 
The margin is the region between the two dotted 
lines (bounded by         and     
    ). The margin is a space without any in-
formation from training instances. The actual 
hyper-plane may fall in any place within the 
margin. It does not necessarily occur in the. 
However, the hyper-plane is used to separate 
positive and negative instances during classifica-
tion process without consideration of the margin. 
Thus if an instance falls in the margin, SVM can 
only decide class label from hyper-plane which 
may cause misclassification in the margin. 
 Based on the previous discussion, we propose 
an adjustment of the hyper-plane using develop-
ment data. For simplicity, we adjust the hyper-
plane function value instead of modeling the 
function itself. The hyper-plane function value 
will be further referred as a threshold  . The fol-
lowing is a modified version of a learned SVM 
classifier. 
        
                          
   
   
                         
   
   
  
where   is the threshold,    is the learned para-
meter for a feature    and    is its class label. A 
set of development data is used to adjust the hy-
per-plane function threshold   in order to max-
imize the accuracy of the learned SVM classifier 
on development data. The adjustment of hyper-
plane is defined as: 
                            
   
  
where        is an indicator function which out-
put 1 if       is same sign as   and 0 otherwise. 
Thereafter, the learned threshold    is applied to 
the testing set. 
3 Incorporating Structural Syntactic In-
formation 
A parse tree that covers a pronoun and its ante-
cedent candidate could provide us much syntac-
tic information related to the pair which is expli-
citly or implicitly represented in the tree. There-
fore, by comparing the common sub-structures 
between two trees we can find out to what degree 
two trees contain similar syntactic information, 
which can be done using a convolution tree ker-
nel. The value returned from tree kernel reflects 
similarity between two instances in syntax. Such 
191
syntactic similarity can be further combined with 
other knowledge to compute overall similarity 
between two instances, through a composite ker-
nel. Normally, parsing is done at sentence level. 
However, in many cases a pronoun and its ante-
cedent candidate do not occur in the same sen-
tence. To present their syntactic properties and 
relations in a single tree structure, we construct a 
syntax tree for an entire text, by attaching the 
parse trees of all its sentences to an upper node. 
Having obtained the parse tree of a text, we shall 
consider how to select the appropriate portion of 
the tree as the structured feature for a given in-
stance. As each instance is related to a pronoun 
and a candidate, the structured feature at least 
should be able to cover both of these two expres-
sions. 
3.1 Structural Syntactic Feature 
Generally, the more substructure of the tree is 
included, the more syntactic information would 
be provided, but at the same time the more noisy 
information that comes from parsing errors 
would likely be introduced. In our study, we ex-
amine three possible structured features that con-
tain different substructures of the parse tree: 
 
? Minimum Expansion Tree 
This feature records the minimal structure cover-
ing both pronoun and its candidate in parse tree. 
It only includes the nodes occurring in the short-
est path connecting the pronoun and its candidate, 
via the nearest commonly commanding node.  
When the pronoun and candidate are from differ-
ent sentences, we will find a path through pseudo 
?TOP? node which links all the parse trees. Con-
sidering the example given in section 1,  
This was an all-white, all-Christian community 
that all the sudden was taken over -- not taken 
over, that's a very bad choice of words, but [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one of the town's two meat-packing plants 13 
years ago. 
The minimum expansion structural feature of the 
instance {invaded, it} is annotated with bold 
lines and shaded nodes in figure 1.  
? Simple Expansion Tree 
Minimum-Expansion could, to some degree, de-
scribe the syntactic relationships between the 
candidate and pronoun. However, it is incapable 
of capturing the syntactic properties of the can-
didate or the pronoun, because the tree structure 
surrounding the expression is not taken into con-
sideration. To incorporate such information, fea-
ture Simple-Expansion not only contains all the 
nodes in Minimum-Expansion, but also includes 
the first-level children of these nodes2 except the 
punctuations. The simple-expansion structural 
feature of instance {invaded, it} is annotated in 
figure 2. In the left sentence?s tree, the node ?NP? 
for ?perhaps different groups? is terminated to 
provide a clue that we have a noun phrase at the 
object position of the candidate verb. 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 1: Minimum-Expansion Tree 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 2: Simple Expansion Tree 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 3: Full-Expansion Tree 
? Full Expansion Tree 
This feature focuses on the whole tree structure 
between the candidate and pronoun. It not only 
includes all the nodes in Simple-Expansion, but 
also the nodes (beneath the nearest commanding 
parent) that cover the words between the candi-
                                                 
2 If the pronoun and the candidate are not in the same sen-
tence, we will not include the nodes denoting the sentences 
before the candidate or after the pronoun. 
192
date and the pronoun3. Such a feature keeps the 
most information related to the pronoun and can-
didate pair. Figure 3 shows the structure for fea-
ture full-expansion for instance {invaded, it}. As 
illustrated, the ?NP? node for ?perhaps different 
groups? is further expanded to the POS level. All 
its child nodes are included in the full-expansion 
tree except the surface words. 
3.2 Convolution Parse Tree Kernel and Com-
posite Kernel 
To calculate the similarity between two struc-
tured features, we use the convolution tree kernel 
that is defined by Collins and Duffy (2002) and 
Moschitti (2004). Given two trees, the kernel 
will enumerate all their sub-trees and use the 
number of common sub-trees as the measure of 
similarity between two trees. The above tree ker-
nel only aims for the structured feature. We also 
need a composite kernel to combine the struc-
tured feature and the flat features from section 
2.2. In our study we define the composite kernel 
as follows: 
             
            
              
 
            
              
 
where       is the convolution tree kernel de-
fined for the structured feature, and       is the 
kernel applied on the flat features. Both kernels 
are divided by their respective length4 for norma-
lization. The new composite kernel      , de-
fined as the sum of normalized       and      , 
will return a value close to 1 only if both the 
structured features and the flat features have high 
similarity under their respective kernels. 
3.3 Twin-Candidate Framework using Rank-
ing SVM Model 
In a ranking SVM kernel as described in (Mo-
schitti et al 2006) for Semantic Role Labeling, 
two argument annotations (as argument trees) are 
presented to the ranking SVM model to decide 
which one is better.  In our case, we present two 
syntactic trees from two candidates to the rank-
ing SVM model. The idea is inspired by (Yang, 
et.al, 2005b;2008). The intuition behind the 
twin-candidate model is to capture the informa-
tion of how much one candidate is more pre-
                                                 
3 We will not expand the nodes denoting the sentences other 
than where the pronoun and the candidate occur. 
4  The length of a kernel   is defined as            
                   
ferred than another. The candidate wins most of 
the pair wise comparisons is selected as antece-
dent. 
The feature vector for each training instance 
has a form of                    . An in-
stance is positive if       is a better antecedent 
choice than       . Otherwise, it is a negative 
instance. For each feature vector, both tree struc-
tural features and flat features are used.  Thus 
each feature vector has a form of    
              where    and    are trees of candi-
date i and j respectively,    and    are flat feature 
vectors of candidate i and j respectively.  
In the training instances generation, we only 
generate those instances with one candidate is 
the correct antecedent. This follows the same 
strategy used in (Yang et al 2008) for object 
anaphora resolution. 
In the resolution process, a list of m candi-
dates is extracted from a three sentences window. 
A total of  
 
 
  instances are generated by pairing-
up the m candidates pair-wisely. We used a 
Round-Robin scoring scheme for antecedent se-
lection. Suppose a SVM output for an instance 
                   is 1, we will give a score 
1 for        and -1 for        and vice versa. At 
last, the candidate with the highest score is se-
lected as antecedent. In order to handle a non-
event anaphoric pronoun, we have set a threshold 
to distinguish event anaphoric from non-event 
anaphoric. A pronoun is considered as event 
anaphoric if its score is above the threshold. In 
our experiments, we kept a set of development 
data to find out the threshold in an empirical way. 
4 Experiments and Discussions 
4.1 Experimental Setup 
OntoNotes Release 2.0 English corpus as in 
(Hovy et al 2006) is used in our study, which 
contains 300k words of English newswire data 
(from the Wall Street Journal) and 200k words of 
English broadcast news data (from ABC, CNN, 
NBC, Public Radio International and Voice of 
America).  Table 1 shows the distribution of var-
ious entities. We focused on the resolution of 
502 event pronouns encountered in the corpus. 
The resolution system has to handle both the 
event pronoun identification and antecedent se-
lection tasks. To illustrate the difficulty of event 
pronoun resolution, 14.7% of all pronoun men-
tions are event anaphoric and only 31.5% of 
193
event pronoun can be resolved using ?most re-
cent verb? heuristics. Therefore a most-recent-
verb baseline will yield an f-score 4.63%. 
To conduct event pronoun resolution, an input 
raw text was preprocessed automatically by a 
pipeline of NLP components. The noun phrase 
identification and the predicate-argument extrac-
tion were done based on Stanford Parser (Klein 
and Manning, 2003a;b) with F-score of 86.32% 
on Penn Treebank corpus.  
Non-Event Anaphora:        4952   80.03% 
Event  
Anaphora: 
1235  
19.97% 
Event NP:        733   59.35% 
Event  
Pronoun: 
502   40.65% 
It:       29.0% 
This:   16.9% 
That:  54.1% 
Table 1: The distribution of various types of 6187 
anaphora in OntoNotes 2.0 
For each pronoun encountered during resolu-
tion, all the inflectional verbs within the current 
and previous two sentences are taken as candi-
dates. For the current sentence, we take only 
those verbs in front of the pronoun. On average, 
each event pronoun has 6.93 candidates. Non-
event anaphoric pronouns will generate 7.3 nega-
tive instances on average.  
4.2 Experiment Results and Discussion 
In this section, we will present our experimental 
results with discussions. The performance meas-
ures we used are precision, recall and F-score. 
All the experiments are done with a 10-folds 
cross validation. In each fold of experiments, the 
whole corpus is divided into 10 equal sized por-
tions. One of them is selected as testing corpus 
while the remaining 9 are used for training. In 
experiments with development data, 1 of the 9 
training portions is kept for development purpose. 
In case of statistical significance test for differ-
ences is needed, a two-tailed, paired-sample Stu-
dent?s t-Test is performed at 0.05 level of signi-
ficance. 
In the first set of experiments, we are aiming 
to investigate the effectiveness of each single 
knowledge source. Table 2 reports the perfor-
mance of each individual experiment. The flat 
feature set yields a baseline system with 40.6% f-
score. By using each tree structure along, we can 
only achieve a performance of 44.4% f-score 
using the minimum-expansion tree. Therefore, 
we will further investigate the different ways of 
combining flat and syntactic structure knowledge 
to improve resolution performances. 
 Precision Recall F-score 
Flat 0.406 0.406 0.406 
Min-Exp 0.355 0.596 0.444 
Simple-Exp 0.347 0.512 0.414 
Full-Exp 0.323 0.476 0.385 
Table 2: Contribution from Single Knowledge Source 
The second set of experiments is conducted to 
verify the performances of various tree structures 
combined with flat features. The performances 
are reported in table 3. Each experiment is re-
ported with two performances. The upper one is 
done with default hyper-plane setting. The lower 
one is done using the hyper-plane adjustment as 
we discussed in section 2.3. 
 Precision Recall F-score 
Min-Exp + 
Flat 
0.433 0.512 0.469 
(0.727) (0.446) (0.553) 
Simple-Exp 
+Flat 
0.423 0.534 0.472 
(0.652) (0.492) (0.561) 
Full-Exp + 
Flat 
0.416 0.526 0.465 
(0.638) (0.496) (0.558) 
Table 3: Comparison of Different Tree Structure +Flat 
As table 3 shows, minimum-expansion gives 
highest precision in both experiment settings. 
Minimum-expansion emphasizes syntactic struc-
tures linking the anaphor and antecedent. Al-
though using only the syntactic path may lose the 
contextual information, but it also prune out the 
potential noise within the contextual structures. 
In contrast, the full-expansion gives the highest 
recall. This is probably due to the widest know-
ledge coverage provides by the full-expansion 
syntactic tree. As a trade-off, the precision of 
full-expansion is the lowest in the experiments. 
One reason for this may be due to OntoNotes 
corpus is from broadcasting news domain. Its 
texts are less-formally structured. Another type 
of noise is that a narrator of news may read an 
abnormally long sentence. It should appear as 
several separate sentences in a news article. 
However, in broadcasting news, these sentences 
maybe simply joined by conjunction word ?and?. 
Thus a very nasty and noisy structure is created 
from it. Comparing the three knowledge source, 
simple-expansion achieves moderate precision 
and recall which results in the highest f-score. 
From this, we can draw a conclusion that simple-
expansion achieves a balance between the indica-
tive structural information and introduced noises. 
In the next set of experiments, we will com-
pare different setting for training instances gen-
eration. A typical setting contains no negative 
194
instances generated from non-event anaphoric 
pronoun. This is not an issue for object pronoun 
resolution as majority of pronouns in an article is 
anaphoric. However in our case, the event pro-
noun consists of only 14.7% of the total pro-
nouns in OntoNotes. Thus we incorporate the 
instances from non-event pronouns to improve 
the precision of the classifier. However, if we 
include all the negative instances from non-event 
anaphoric pronouns, the positive instances will 
be overwhelmed by the negative instances. A 
down sampling is applied to the training in-
stances to create a more balanced class distribu-
tion. Table 4 reports various training settings 
using simple-expansion tree structure.  
Simple-Exp Tree Precision Recall F-score 
Without Non-
event Negative 
0.423 0.534 0.472 
Incl. All Negative 0.733 0.410 0.526 
Balanced Negative 0.599 0.506 0.549 
Development Data 0.652 0.492 0.561 
Table 4: Comparison of Training Setup, Simple-Exp 
In table 4, the first line is experiment without 
any negative instances from non-event pronouns. 
The second line is the performance with all nega-
tive instances from non-event pronouns. Third 
line is performance using a balanced training set 
using down sampling. The last line is experiment 
using hyper-plane adjustment. The first line 
gives the highest recall measure because it has no 
discriminative knowledge on non-event anaphor-
ic pronoun. The second line yields the highest 
precision which complies with our claim that 
including negative instances from non-event 
pronouns will improve precision of the classifier 
because more discriminative power is given by 
non-event pronoun instances. The balanced train-
ing set achieves a better f-score comparing to 
models with no/all negative instances. This is 
because balanced training set provides a better 
weighted positive/negative instances which im-
plies a balanced positive/negative knowledge 
representation. As a result of that, we achieve a 
better balanced f-score. In (Ng and Cardie, 
2002b), they concluded that only the negative 
instances in between the anaphor and antecedent 
are useful in the resolution. It is same as our 
strategy without negative instances from non-
event anaphoric pronouns. However, our study 
showed an improvement by adding in negative 
instances from non-event anaphoric pronouns as 
showed in table 4. This is probably due to our 
random sampling strategy over the negative in-
stances near to the event anaphoric instances. It 
empowers the system with more discriminative 
power. The best performance is given by the hy-
per-plane adaptation model. Although the num-
ber of training instances is further reduced for 
development data, we can have an adjustment of 
the hyper-plane which is more fit to dataset.  
In the last set of experiments, we will present 
the performance from the twin-candidates based 
approach in table 5. The first line is the best per-
formance from single candidate system with hy-
per-plane adaptation. The second line is perfor-
mance using the twin-candidates approach. 
Simple-Exp Tree Precision Recall F-score 
Single Candidate 0.652 0.492 0.561 
Twin-Candidates 0.626 0.540 0.579 
Table 5: Single vs. Twin Candidates, Simple-Exp 
Comparing to the single candidate model, the 
recall is significantly improved with a small 
trade-off in precision. The difference in results is 
statistically significant using t-test at 5% level of 
significance. It reinforced our intuition that pre-
ferences between two candidates are contributive 
information sources in co-reference resolution.  
5 Conclusion and Future Work 
The purpose of this paper is to conduct a syste-
matic study of the event pronoun resolution. We 
propose a resolution system utilizing a set of flat 
positional, lexical and syntactic feature and 
structural syntactic feature. The state-of-arts 
convolution tree kernel is used to extract indica-
tive structural syntactic knowledge. A twin-
candidates preference learning based approach is 
incorporated to reinforce the resolution system 
with candidates? preferences knowledge. Last but 
not least, we also proposed a study of the various 
incorporations of negative training instances, 
specially using random sampling to handle the 
imbalanced data. Development data is also used 
to select more accurate hyper-plane in SVM for 
better determination. 
To further our research work, we plan to em-
ploy more semantic information into the system 
such as semantic role labels and verb frames.  
Acknowledgment 
We would like to thank Professor Massimo Poesio 
from University of Trento for the initial discussion of 
this work. 
195
References  
N. Asher. 1993. Reference to Abstract Objects in Dis-
course. Kluwer Academic Publisher. 1993. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer.1995. 
M. Kubat and S. Matwin, 1997. Addressing the curse 
of imbalanced data set: One sided sampling. In 
Proceedings of the Fourteenth International Con-
ference on Machine Learning,1997. pg179?186. 
T. Joachims. 1999. Making large-scale svm learning 
practical. In Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.1999. 
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun 
phrases. In Computational Linguistics, Vol:27(4), 
pg521? 544. 
D. Byron. 2002. Resolving Pronominal Reference to 
Abstract Entities, in Proceedings of the 40th An-
nual Meeting of the Association for Computational 
Linguistics (ACL?02). July 2002. , USA  
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?02). July 
2002. , USA 
V. Ng and C. Cardie. 2002a. Improving machine 
learning approaches to coreference resolution. In 
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?02). 
July 2002. , USA. pg104?111. 
V. Ng, and C. Cardie. 2002b. Identifying anaphoric 
and non-anaphoric noun phrases to improve core-
ference resolution. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics (COLING02). (2002) 
M. Strube and C. M?ller. 2003. A Machine Learning 
Approach to Pronoun Resolution in Spoken Dialo-
gue. . In Proceedings of the 41st Annual Meeting of 
the Association for Computational Linguistics 
(ACL?03), 2003 
D. Klein and C. Manning. 2003a. Fast Exact Infe-
rence with a Factored Model for Natural Language 
Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press, pp. 3-10. 
D. Klein and C.Manning. 2003b. Accurate Unlexica-
lized Parsing. In Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics (ACL?03), 2003.  pg423-430. 
X. Yang, G. Zhou, J. Su, and C.Tan. 2003. Corefe-
rence Resolution Using Competition Learning Ap-
proach. In Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics 
(ACL?03), 2003. pg176?183. 
A. Moschitti. 2004. A study on convolution kernels 
for shallow semantic parsing. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL?04), pg335?342. 
A. Estabrooks, T. Jo, and N. Japkowicz. 2004. A mul-
tiple resampling method for learning from imba-
lanced data sets. In Computational Intelligence  
Vol:20(1). pg18?36. 
X. Yang, J. Su, G. Zhou, and C. Tan. 2004. Improving 
pronoun resolution by incorporating coreferential 
information of candidates. In Proceedings of 42th 
Annual Meeting of the Association for Computa-
tional Linguistics, 2004. pg127?134. 
X. Yang, J. Su and C.Tan. 2005a. Improving Pronoun 
Resolution Using Statistics-Based Semantic Com-
patibility Information. In Proceedings of Proceed-
ings of the 43rd Annual Meeting of the Association 
for Computational Linguistics (ACL?05). June 
2005.  
X. Yang, J. Su and C.Tan. 2005b. A Twin-Candidates 
Model for Coreference Resolution with Non-
Anaphoric Identification Capability. In Proceed-
ings of IJCNLP-2005. Pp. 719-730, 2005 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. 
Weischedel. 2006. OntoNotes: The 90\% Solution. 
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, 2006 
X. Yang, J. Su and C.Tan. 2006. Kernel-Based Pro-
noun Resolution with Structured Syntactic Know-
ledge. In Proceedings of the 44th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?06). July 2006. Australia. 
A. Moschitti, Making tree kernels practical for natural 
language learning. In Proceedings EACL 2006, 
Trento, Italy, 2006. 
C. M?ller. 2007. Resolving it, this, and that in unre-
stricted multi-party dialog. In Proceedings of the 
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL?07). 2007.  Czech Re-
public. pg816?823. 
X. Yang, J. Su and C.Tan. 2008. A Twin-Candidates 
Model for Learning-Based Coreference Resolution. 
In Computational Linguistics, Vol:34(3). pg327-
356. 
S. Pradhan, L. Ramshaw, R. Weischedel, J. Mac-
Bride, and L. Micciulla. 2007. Unrestricted Corefe-
rence: Identifying Entities and Events in Onto-
Notes. In Proceedings of the IEEE International 
Conference on Semantic Computing (ICSC), Sep. 
2007. 
196
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 872?881,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
	
				
		
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 12?23,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Discourse Analysis for Article-Wide Temporal Classification
Jun-Ping Ng1, Min-Yen Kan1,2, Ziheng Lin3, Wei Feng4, Bin Chen5, Jian Su5, Chew-Lim Tan1
1School of Computing, National University of Singapore, Singapore
2Interactive and Digital Media Institute, National University of Singapore, Singapore
3Research & Innovation, SAP Asia Pte Ltd, Singapore
4Department of Computer Science, University of Toronto, Canada
5Institute for Infocomm Research, Singapore
junping@comp.nus.edu.sg
Abstract
In this paper we classify the temporal relations
between pairs of events on an article-wide ba-
sis. This is in contrast to much of the exist-
ing literature which focuses on just event pairs
which are found within the same or adjacent
sentences. To achieve this, we leverage on dis-
course analysis as we believe that it provides
more useful semantic information than typical
lexico-syntactic features. We propose the use
of several discourse analysis frameworks, in-
cluding 1) Rhetorical Structure Theory (RST),
2) PDTB-styled discourse relations, and 3)
topical text segmentation. We explain how
features derived from these frameworks can be
effectively used with support vector machines
(SVM) paired with convolution kernels. Ex-
periments show that our proposal is effective
in improving on the state-of-the-art signifi-
cantly by as much as 16% in terms of F1, even
if we only adopt less-than-perfect automatic
discourse analyzers and parsers. Making use
of more accurate discourse analysis can fur-
ther boost gains to 35%.
1 Introduction
A good amount of research had been invested in un-
derstanding temporal relationships within text. Par-
ticular areas of interest include determining the re-
lationship between an event mention and a time ex-
pression (timex), as well as determining the relation-
ship between two event mentions. The latter, which
we refer to as event-event (E-E) temporal classifica-
tion is the focus of this work.
For a given event pair which consists of two
events e1 and e2 found anywhere within an article,
we want to be able to determine if e1 happens be-
fore e2 (BEFORE), after e2 (AFTER), or within the
same time span as e2 (OVERLAP).
Consider this sentence1:
At least 19 people were killed and 114 people were
wounded in Tuesday?s southern Philippines airport blast,
officials said, but reports said the death toll could climb
to 30.
(1)
Three event mentions found within the sentence are
bolded. We say that there is an OVERLAP rela-
tionship between the ?killed ? wounded? event pair
as these two events happened together after the air-
port blast. Similarly there is a BEFORE relationship
between both the ?killed ? said?, and ?wounded ?
said? event pairs, as the death and injuries happened
before reports from the officials.
Being able to infer these temporal relationships
allows us to build up a better understanding of the
text in question, and can aid several natural lan-
guage understanding tasks such as information ex-
traction and text summarization. For example, we
can build up a temporal characterization of an article
by constructing a temporal graph denoting the rela-
tionships between all events within an article (Ver-
hagen et al, 2009). This can then be used to help
construct an event timeline which layouts sequen-
tially event mentions in the order they take place (Do
et al, 2012). The temporal graph can also be used
in text summarization, where temporal order can be
used to improve sentence ordering and thereby the
eventual generated summary (Barzilay et al, 2002).
Given the importance and value of temporal re-
lations, the community has organized shared tasks
1From article AFP ENG 20030304.0250 of the ACE 2005
corpus (ACE, 2005).
12
to spur research efforts in this area, including the
TempEval-1, -2 and -3 evaluation workshops (Ver-
hagen et al, 2009; Verhagen et al, 2010; Uzzaman
et al, 2012). Most related work in this area have
focused primarily on the task defintitions of these
evaluation workshops. In the task definitions, E-
E temporal classification involves determining the
relationship between events found within the same
sentence, or in adjacent sentences. For brevity we
will refer to this loosely as intra-sentence E-E tem-
poral classification in the rest of this paper.
This definition however is limiting and insuffi-
cient. It was adopted as a trade-off between com-
pleteness, and the need to simplify the evaluation
process (Verhagen et al, 2009). In particular, one
deficiency is that it does not allow us to construct the
complete temporal graph we seek. As illustrated in
Figure 1, being able to perform only intra-sentence
E-E temporal classification may result in a forest of
disconnected temporal graphs. A sentence s3 sepa-
rates events C and D, as such an intra-sentence E-E
classification system will not be able to determine
the temporal relationship between them. While we
can determine the relationship between A and C in
the figure with the use of temporal transitivity rules
(Setzer et al, 2003; Verhagen, 2005), we cannot re-
liably determine the relationship between say A and
D.
A
B C
D E
s1
s2
s3
s4
Figure 1: A disconnected temporal graph of events within
an article. Horizontal lines depict sentences s1 to s4, and
the circles identify events of interest.
In this work, we seek to overcome this limitation,
and study what can enable effective article-wide E-E
temporal classification. That is, we want to be able
to determine the temporal relationship between two
events located anywhere within an article.
The main contribution of our work is going
beyond the surface lexical and syntactic features
commonly adopted by existing state-of-the-art ap-
proaches. We suggest making use of semantically
motivated features derived from discourse analysis
instead, and show that these discourse features are
superior.
While we are just focusing on E-E temporal
classification, our work can complement other ap-
proaches such as the joint inference approach pro-
posed by Do et al (2012) and Yoshikawa et al
(2009) which builds on top of event-timex (E-T) and
E-E temporal classification systems. We believe that
improvements to the underlying E-T and E-E classi-
fication systems will help with global inference.
2 Related Work
Many researchers have worked on the E-E temporal
classification problem, especially as part of the Tem-
pEval series of evaluation workshops. Bethard and
Martin (2007) presented one of the earliest super-
vised machine learning systems, making use of sup-
port vector machines (SVM) with a variety of lexical
and syntactic features. Kolya et al (2010) described
a conditional random field (CRF) based learner mak-
ing use of similar features. Other researchers includ-
ing Uzzaman and Allen (2010) and Ha et al (2010)
made use of Markov Logic Networks (MLN). By
leveraging on the transitivity properties of temporal
relationships (Setzer et al, 2003), they found that
MLNs are useful in inferring new temporal relation-
ships from known ones.
Recognizing that the temporal relationships be-
tween event pairs and time expressions are related,
Yoshikawa et al (2009) proposed the use of a joint
inference model and showed that improvements in
performance are obtained. However this gain is at-
tributed to the joint inference model they had devel-
oped, making use of similar surface features.
To the best of our knowledge, the only piece
of work to have gone beyond sentence boundaries
and tackle the problem of article-wide E-E temporal
classification is by Do et al (2012). Making use of
integer linear programming (ILP), they built a joint
inference model which is capable of classifying tem-
poral relationships between any event pair within
a given document. They also showed that event
co-reference information can be useful in determin-
ing these temporal relationships. However they did
not make use of features directed specifically at de-
termining the temporal relationships of event pairs
13
across different sentences. Other than event co-
reference information, they adopted the same mix
of lexico-syntactic features.
Underlying these disparate data-driven methods
for similar temporal processing tasks, the reviewed
works all adopted a similar set of surface fea-
tures including vocabulary features, part-of-speech
tags, constituent grammar parses, governing gram-
mar nodes and verb tenses, among others. We ar-
gue that these features are not sufficiently discrimi-
native of temporal relationships because they do not
explain how sentences are combined together, and
thus are unable to properly differentiate between the
different temporal classifications. Supporting our
argument is the work of Smith (2010), where she
argued that syntax cannot fully account for the un-
derlying semantics beneath surface text. D?Souza
and Ng (2013) found out as much, and showed that
adopting richer linguistic features such as lexical re-
lations from curated dictionaries (e.g. Webster and
WordNet) as well as discourse relations help tempo-
ral classification. They had shown that the Penn Dis-
course TreeBank (PDTB) style (Prasad et al, 2008)
discourse relations are useful. We expand on their
study to assess the utility of adopting additional dis-
course frameworks as alternative and complemen-
tary views.
3 Making Use of Discourse
To highlight the deficiencies of surface features, we
quote here an example from Lascarides and Asher
(1993):
[A] Max opened the door. The room was pitch dark.
[B] Max switched off the light. The room was pitch dark.
(2)
The two lines of text A and B in Example 2 have
similar syntactic structure. Given only syntactic fea-
tures, we may be drawn to conclude that they share
similar temporal relationships. However in the first
line of text, the events temporally OVERLAP, while
in the second line they do not. Clearly, syntax alone
is not going to be useful to help us arrive at the cor-
rect temporal relations.
If existing surface features are insufficient, what is
sufficient? Given a E-E pair which crosses sentence
boundaries, how can we determine the temporal re-
lationship between them? We take our cue from the
work of Lascarides and Asher (1993). They sug-
gested instead that discourse relations hold the key
to interpreting such temporal relationships.
Building on their observations, we believe that
discourse analysis is integral to any solution for the
problem of article-wide E-E temporal classification.
We thus seek to exploit a series of different discourse
analysis studies, including 1) the Rhetorical Struc-
ture Theory (RST) discourse framework, 2) Penn
Discourse Treebank (PDTB)-styled discourse rela-
tions based on the lexicalized Tree Adjoining Gram-
mar for Discourse (D-LTAG), and 3) topical text seg-
mentation, and validate their effectiveness for tem-
poral classification.
RST Discourse Framework. RST (Mann and
Thompson, 1988) is a well-studied discourse anal-
ysis framework. In RST, a piece of text is split into a
sequence of non-overlapping text fragments known
as elementary discourse units (EDUs). Neighboring
EDUs are related to each other by a typed relation.
Most RST relations are hypotactic, where one of the
two EDUs participating in the relationship is demar-
cated as a nucleus, and the other a satellite. The nu-
cleus holds more importance, from the point of view
of the writer, while the satellite?s purpose is to pro-
vide more information to help with the understand-
ing of the nucleus. Some RST relations are however
paratactic, where the two participating EDUs are
both marked as nuclei. A discourse tree can be com-
posed by viewing each EDU as a leaf node. Nodes
in the discourse tree are linked to one another via the
discourse relations that hold between the EDUs.
RST discourse relations capture the semantic re-
lation between two EDUs, and these often offer a
clue to the temporal relationship between events in
the two EDUs too. As an example, let us refer once
again to Example 2. Recall that in the second line of
text ?switched off? happens BEFORE ?dark?. The
RST discourse structure for the second line of text
is shown on the left of Figure 2. We see that the
two sentences are related via a ?Result? discourse
relation. This fits our intuition that when there is
causation, there should be a BEFORE/AFTER rela-
tionship. The RST discourse relation in this case is
very useful in helping us determine the relationship
between the two events.
PDTB-styled Discourse Relations. Another widely
adopted discourse relation annotation is the PDTB
framework (Prasad et al, 2008). Unlike the RST
14
Max switched off the light. The room was pitch dark.
RESULT
The room was pitch dark.
CONTINGENCY :: CAUSE
arg1 arg2
Max switched off the light.
Figure 2: RST and PDTB discourse structures for the second line of text in Example 2. The structure on the left is the
RST discourse structure, while the structure on the right is for PDTB.
framework, the discourse relations in PDTB build on
the work on D-LTAG by Webber (2004), a lexicon-
grounded approach to discourse analysis. Practi-
cally, this means that instead of starting from a pre-
identified set of discourse relations, PDTB-styled
annotations are more focused on detecting possible
connectives (can be either explicit or implicit) within
the text, before identifying the text fragments which
they connect and how they are related to one another.
Applied again to the second line of text we have in
Example 2, we get a structure as shown on the right
side of Figure 2. From the figure we can see that
the two sentences are related via a ?Cause? relation-
ship. Similar to what we have explained earlier for
the case of RST, the presence of a causal effect here
strongly hints to us that events in the two sentences
share a BEFORE/AFTER relationship.
At this point we want to note the differences be-
tween the use of the RST framework and PDTB-
styled discourse relations in the context of our work.
The theoretical underpinnings behind these two dis-
course analysis are very different, and we believe
that they can be complementary to each other. First,
the RST framework breaks up text within an article
linearly into non-overlapping EDUs. Relations can
only be defined between neighboring EDUs. How-
ever this constraint is not found in PDTB-styled re-
lations, where a text fragment can participate in one
discourse relation, and a subsequence of it partic-
ipate in another. PDTB relations are also not re-
stricted only to adjacent text fragments. In this as-
pect, the flexibility of the PDTB relations can com-
plement the seemingly more rigid RST framework.
Second, with PDTB-styled relations not every
sentence needs to be in a relation with another as
the PDTB framework does not aim to build a global
discourse tree that covers all sentence pairs. This is
a problem when we need to do an article-wide anal-
ysis. The RST framework does not suffer from this
limitation however as we can build up a discourse
tree connecting all the text within a given article.
Topical Text Segmentation. A third complemen-
tary type of inter-sentential analysis is topical text
segmentation. This form of segmentation separates
a piece of text into non-overlapping segments, each
of which can span several sentences. Each segment
represents passages or topics, and provides a coarse-
grained study of the linear structure of the text (Sko-
rochod?Ko, 1972; Hearst, 1994). The transition be-
tween segments can represent possible topic shifts
which can provide useful information about tempo-
ral relationships.
Referring to Example 32, we have delimited the
different lines of text into segments with parenthe-
ses along with a subscript. Segment (1) talks about
the casualty numbers seen at a medical centre, while
Segment (2) provides background information that
informs us a bomb explosion had taken place. The
segment boundary signals to us a possible temporal
shift and can help us to infer that the bombing event
took place BEFORE the deaths and injuries had oc-
curred.
(The Davao Medical Center, a regional government hos-
pital, recorded 19 deaths with 50 wounded. Medical
evacuation workers however said the injured list was
around 114, spread out at various hospitals.)1
(A powerful bomb tore through a waiting shed at the
Davao City international airport at about 5.15 pm (0915
GMT) while another explosion hit a bus terminal at the
city.)2
(3)
4 Methodology
Having motivated the use of discourse analysis for
our problem, we now proceed to explain how we can
make use of them for temporal classification. The
different facets of discourse analysis that we are ex-
ploring in this work are structural in nature. RST
2From article AFP ENG 20030304.0250 of the ACE 2005
corpus.
15
EDU2 EDU3
r2
r1
EDU1
A
B
Figure 3: A possible RST discourse tree. The two circles
denote two events A and B which we are interested in.
t1 t2
t3
t4
r1 r2
r3
B
A
Figure 4: A possible PDTB-styled discourse annotation
where the circles represent events we are interested in.
and PDTB discourse relations are commonly repre-
sented as graphs, and we can also view the output
of text segmentation as a graph with individual text
segments forming vertices, and the transitions be-
tween them forming edges.
Considering this, we propose the use of support
vector machines (SVM), adopting a convolution ker-
nel (Collins and Duffy, 2001) for its kernel function
(Vapnik, 1999; Moschitti, 2006). The use of convo-
lution kernels allows us to do away with the exten-
sive feature engineering typically required to gener-
ate flat vectorized representations of features. This
process is time consuming and demands specialized
knowledge to achieve representations that are dis-
criminative, yet are sufficiently generalized. Con-
volution kernels had also previously been shown to
work well for the related problem of E-T temporal
classification (Ng and Kan, 2012), where the fea-
tures adopted are similarly structural in nature.
We now describe our use of the discourse analysis
frameworks to generate appropriate representations
for input to the convolution kernel.
RST Discourse Framework. Recall that the RST
framework provides us with a discourse tree for an
entire input article. In recent years several automatic
RST discourse parsers have been made available. In
our work, we first make use of the parser by Feng
and Hirst (2012) to obtain a discourse tree represen-
tation of our input. To represent the meaningful por-
tion of the resultant tree, we encode path information
between the two sentences of interest.
We illustrate this procedure using the example
discourse tree illustrated in Figure 3. EDUs includ-
ing EDU1 to EDU3 form the vertices while dis-
course relations r1 and r2 between the EDUs form
the edges. For a E-E pair, {A,B}, we can obtain a
feature structure by first locating the EDUs within
which A and B are found. A is found inside EDU1
and B is found within EDU3. We trace the short-
est path between EDU1 and EDU3, and use this
path as the feature structure for the E-E pair, i.e.
{r1 ? r2}.
PDTB-styled Discourse Relations. We make use of
the automatic PDTB discourse parser from Lin et al
(2013) to obtain the discourse relations over an input
article. Similar to how we work with the RST dis-
course framework, for a given E-E pair, we retrieve
the relevant text fragments and use the shortest path
linking the two events as a feature structure for our
convolution kernel classifier.
An example of a possible PDTB-styled discourse
annotation is shown in Figure 4. The horizontal
lines represent different sentences in an article. The
parentheses delimit text fragments, t1 to t4, which
have been identified as arguments participating in
discourse relations, r1 to r3. For a given E-E pair
{A,B}, we use the trace of the shortest path be-
tween them i.e. {r1 ? r2} as a feature structure.
We take special care to regularize the input (as,
unlike EDUs in RST, arguments to different PDTB
relations may overlap, as in r2 and r3). We model
each PDTB discourse annotation as a graph and em-
ploy Dijkstra?s shortest path algorithm. The graph
resulting from the annotation in Figure 4 is given in
Figure 5. Each text fragment ti maps to a vertex
ni in the graph. PDTB relations between text frag-
ments form edges between corresponding vertices.
As r2 relates t2 to both t3 and t4, two edges link
up n2 to the corresponding vertices n3 and n4 re-
spectively. By doing this, Dijkstra?s algorithm will
always allow us to find the desired shortest path.
n1 n2 n3 n4
r1
r2 r3
r2
Figure 5: Graph derived from discourse annotation in
Figure 4.
16
Topical Text Segmentation. Taking as input a com-
plete text article, we make use of the state-of-the-art
text segmentation system from Kazantseva and Sz-
pakowicz (2011). The output of the system is a se-
ries of non-overlapping, linear text segments, which
we can number sequentially.
In Figure 6 the horizontal lines represent sen-
tences. Parentheses with subscripts mark out the
segment boundaries. We can see two segments s1
and s2 here. Given a target E-E pair {A,B} (repre-
sented as circles inside the figure), we identify the
segment number of the corresponding segment in
which each of A and B is found. We build a fea-
ture structure with the identified segment numbers,
i.e. {s1 ? s2} to capture the segmentation.
A
B
s1
s2
Figure 6: A possible segmentation of three sentences into
two segments.
5 Results
We conduct a series of experiments to validate the
utility of our proposed features.
Data Set. We make use of the same data set built
by Do et al (2012). The data set consists of 20
newswire articles which originate from the ACE
2005 corpus (ACE, 2005). Initially, the data set
consist of 324 event mentions, and a total of 375
annotated E-E pairs. We perform the same temporal
saturation step as described in Do et al (2012), and
obtained a total of 7,994 E-E pairs3.
A breakdown of the number of instances by each
temporal classes is shown in Table 1. Unlike earlier
data sets such as that for TempEval-2 where more
than half (about 55%) of test instances belong to the
3Though we have obtained the data set from the original au-
thors, there was a discrepancy in the number of E-E pairs. The
original paper reported a total of 376 annotated E-E pairs. Be-
sides this, we also repeated the saturation steps iteratively until
no new relationship pairs are generated. We believe this to be
an enhancement as it ensures that all inferred temporal relation-
ships are generated.
OVERLAP class, OVERLAP instances make up just
10% of the data set.
This difference is due mainly to the fact that our
data set consists not only of intra-sentence E-E pairs,
but also of article-wide E-E pairs. Figure 7 shows
the number of instances for each temporal class bro-
ken down by the number of sentences (i.e. sentence
gap) that separate the events within each E-E pair.
We see that as the sentence gap increases, the pro-
portion of OVERLAP instances decreases. The in-
tuitive explanation for this is that when event men-
tions are very far apart in an article, it becomes more
unlikely that they happen within the same time span.
Class AFTER BEFORE OVERLAP
# E-E pairs 3,588 (45%) 3,589 (45%) 815 (10%)
Table 1: Number of E-E pairs in data set attributable to
each temporal class. Percentages shown in parentheses.
Figure 7: Breakdown of number of E-E pairs for each
temporal class based on sentence gap.
Experiments. The work done in Do et al (2012) is
highly related to our experiments, and so we have
reported the relevant results for local E-E classifi-
cation in Row 1 of Table 2 as a reference. While
largely comparable, note that a direct comparison is
not possible because 1) the number of E-E instances
we have is slightly different from what was reported,
and 2) we do not have access to the exact partitions
they have created for 5-fold cross-validation.
As such, we have implemented a baseline adopt-
ing similar surface lexico-syntactic features used in
previous work (Mani et al, 2006; Bethard and Mar-
tin, 2007; Ng and Kan, 2012; Do et al, 2012), in-
cluding 1) part-of-speech tags, 2) tenses, 3) depen-
dency parses, 4) relative position of events in article,
17
System Precision Recall F1
(1) DO2012 43.86 52.65 47.46
(2) BASE 59.55 38.14 46.50
(3) BASE + RST + PDTB + TOPICSEG 71.89 41.99 53.01
(4) BASE + RST + PDTB + TOPICSEG + COREF 75.23 43.58 55.19
(5) BASE + O-RST + PDTB + O-TOPICSEG + O-COREF 78.35 54.24 64.10
Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive
row is statistically significant, but a comparison is not possible between rows (1) and (2).
5) the number of sentences between the target events
and 6) VerbOcean (Chklovski and Pantel, 2004) re-
lations between events. This baseline system, and
the subsequent systems we will describe, comprises
of three separate one-vs-all classifiers for each of the
temporal classes. The result obtained by our base-
line is shown in Row 2 (i.e. BASE) in Table 2. We
note that our baseline is competitive and performs
similarly to the results obtained by Do et al (2012).
However as we do not have the raw judgements from
Do?s system, we cannot test for statistical signifi-
cance.
We also implemented our proposed features and
show the results obtained in the remaining rows of
Table 2. In Row 3, RST denotes the RST discourse
feature, PDTB denotes the PDTB-styled discourse
features, and TOPICSEG denotes the text segmen-
tation feature. Compared to our own baseline, there
is a relative increase of 14% in F1, which is statis-
tically significant when verified with the one-tailed
Student?s paired t-test (p < 0.01).
In addition, Do et al (2012) have shown the value
of event co-reference. Therefore we have also in-
cluded this feature by making use of an automatic
event co-reference system by Chen et al (2011).
The result obtained after adding this feature (de-
noted by COREF) is shown in Row 4. The relative in-
crease in F1 of about 4% from Row 3 is statistically
significant (p < 0.01) and affirms that event co-
reference is a useful feature to have, together with
our proposed features. We note that our complete
system in Row 4 gives a 16% improvement in F1,
relative to the reference system DO2012 in Row 1.
To get a better idea of the performance we can ob-
tain if oracular versions of our features are available,
we also show the results obtained if hand-annotated
RST discourse structures, text segments, as well as
event co-reference information were used. Annota-
tions for the RST discourse structures and text seg-
ments were performed by the first author (RST an-
notations were made following the annotation guide-
lines given by Carlson and Marcu (2001)). Oracular
event co-reference information was included in the
dataset that we have used.
In Row 5 the prefix O denotes oracular versions
of the features we had proposed. From the results
we see that there is a marked increase of over 15%
in F1 relative to Row 4. Compared to Do?s state-of-
the-art system, there is also a relative gain of at least
35%. These oracular results further confirm the im-
portance of non-local discourse analysis for tempo-
ral processing.
6 Discussion
Ablation tests. We performed ablation tests to as-
sess the efficacy of the discourse features used in
our earlier experiments. Starting from the full sys-
tem, we dropped each discourse feature in turn to see
the effect this has on overall system performance.
Our test is performed over the same data set, again
with 5-fold cross-validation. The results in Table 3
show a statistically significant (based on the one-
tailed Student?s paired t-test) drop in F1 in each case,
which proves that each of our proposed features is
useful and required.
From the ablation tests, we also observe that the
RST discourse feature contributes the most to over-
all system performance while the PDTB discourse
feature contributes the least. However we should not
conclude prematurely that the former is more use-
ful than the latter; as the results are obtained using
parses from automatic systems, and are not reflec-
tive of the full utility of ground truth discourse an-
notations.
Useful Relations. The ablation test results showed
us that discourse relations (in particular RST dis-
18
Figure 8: Proportion of occurence in temporal classes for every RST and PDTB relation.
Ablated Feature Change in F1 Sig
?RST -9.03 **
?TOPICSEG -2.98 **
?COREF -2.18 **
?PDTB -1.42 *
Table 3: Ablation test results. ?**? and ?*? denote statis-
tically significance against the full system with p < 0.01
and p < 0.05, respectively.
course relations) are the most important in our sys-
tem. We have also motivated our work earlier with
the intuition that certain relations such as the RST
?Result? and the PDTB ?Cause? relations provide
very useful temporal cues. We now offer an intro-
spection into the use of these discourse relations.
Figure 8 illustrates the relative proportion of tem-
poral classes in which each RST and PDTB re-
lation appear. If the relations are randomly dis-
tributed, we should expect their distribution to fol-
low that of the temporal classes as shown in Table 1.
However we see that many of the relations do not
follow this distribution. For example, we observe
that several relations such as the RST ?Condition?
and PDTB ?Cause? relations are almost exclusively
found within AFTER and BEFORE event pairs only,
while the RST ?Manner-means? and PDTB ?Syn-
chrony? relations occur in a disproportionately large
number of OVERLAP event pairs. These relations
are likely useful in disambiguating between the dif-
ferent temporal classes.
To verify this, we examine the convolution tree
fragments that lie on the support vector of our SVM
classifier. The work of Pighin and Moschitti (2010)
in linearizing kernel functions allows us to take a
look at these tree fragments. Applying the lineariza-
tion process leads to a different classifier from the
one we have used. The identified tree fragments are
therefore just an approximation to those actually em-
ployed by our classifier. However, this analysis still
offers an introspection as to what relations are most
influential for classification.
BEFORE OVERLAP
B1 (Temporal ... O1 (Manner-means ...
B2 (Temporal (Elaboration ...
B3 (Condition (Explanation ...
B4 (Condition (Attribution ...
B5 (Elaboration (Bckgrnd ...
Table 4: Subset of top RST discourse fragments on sup-
port vectors identified by linearizing kernel function.
Table 4 shows a subset of the top RST discourse
fragments identified for the BEFORE and OVER-
LAP one-vs-all classifiers. The list is in line with
what we expect from Figure 8. The former consists
of fragments containing relations such as ?Tempo-
ral? and ?Condition?, while the latter has a sole frag-
ment containing ?Manner-Means?.
To illustrate what these fragments may mean, we
show several example sentences from our data set
in Example 4. Sentence A consists of the tree frag-
ment B1, i.e. ?(Temporal...?. Its corresponding dis-
course structure is illustrated in the top half of Fig-
ure 9. This fragment indicates to us (correctly) that
the event ?wielded? happened BEFORE Milosevic
was ?swept out? of power. Sentence B is made
up of tree fragment O1, i.e. ?(Manner-means...?,
19
and its discourse structure is shown in the bottom
half of Figure 9. As with the previous example, the
fragment suggests (correctly) that there should be a
OVERLAP relationship for the ?requested ? said?
event pair.
[A] Milosevic and his wife wielded enormous power in
Yugoslavia for more than a decade before he was swept
out of power after a popular revolt in October 2000.
[B] The court order was requested by Jack Welch?s at-
torney, Daniel K. Webb, who said Welch would likely be
asked about his business dealings, his health and entries
in his personal diary.
(4)
Milosevic ? wielded? 
a decade 
before.. swept out.. 
power
after a?  October
2000.
temporal
temporal
The court? requested
by Jack .. Webb,
elaboration
who said Welch would ?
diary.
attribution
manner-means
Figure 9: RST discourse structures for sentences A (top
half) and B (bottom half) in Example 4.
Segment Numbers. From the ablation test results,
text segmentation is the next most important feature
after the RST discourse feature. This is interesting
given that the defined feature structure for topical
text segmentation is not the most intuitive. By us-
ing actual segment numbers, the structure may not
generalize well for articles of different lengths for
example, as each article may have vastly different
number of segments. The transition across segments
may also not carry the same semantic significance
for different articles.
Our experiments have however shown that this
feature design is useful in improving performance.
This is likely because:
1. The default settings of the text segmentation
system we had used are such that precision is
favoured over recall (Kazantseva and Szpakow-
icz, 2011, p. 292). As such there is just an aver-
age of between two to three identified segments
per article. This makes the feature more gener-
alizable despite making use of actual segment
numbers.
2. The style of writing in newswire articles which
we are experimenting on generally follows
common journalistic guidelines. The semantics
behind the transitions across the coarse-grained
segments that were identified are thus likely to
be of a similar nature across many different ar-
ticles.
We leave for future work an investigation into
whether more fine-grained topic segments can lead
to further performance gains. In particular, it will be
interesting to study if work on argumentative zoning
(Teufel and Kan, 2011) can be applied to newswire
articles, and whether the subsequent learnt docu-
ment structures can be used to delineate topic seg-
ments more accurately.
Error Analysis. Besides examining the features we
had used, we also want to get a better idea of the er-
rors made by our classifier. Recall that we are using
separate one-vs-all classifiers for each of the tempo-
ral classes, so each of the three classifiers generates
a column in the aggregate confusion matrix shown
in Table 5. In cases where none of the SVM clas-
sifiers return a positive confidence value, we do not
assign a temporal class (captured as column N). The
high number of event pairs which are not assigned to
any temporal class explains the lower recall scores
obtained by our system, as observed in Table 2.
Predicted
O B A N
O 119 (14.7%) 114 (14.1%) 104 (12.8%) 474 (58.5%)
B 19 (0.5%) 2067 (57.9%) 554 (15.5%) 928 (26.0%)
A 16 (0.5%) 559 (15.7%) 2046 (57.3%) 947 (26.5%)
Table 5: Confusion matrix obtained for the full system,
classifying into (O)VERLAP, (B)EFORE, (A)FTER, and
(N)o result.
Additionally, an interesting observation is the low
percentage of OVERLAP instances that our classi-
fier managed to predict correctly. About 57% of
BEFORE and AFTER instances are classified cor-
20
rectly, however only about 15% of OVERLAP in-
stances are correct.
Figure 10 offers more evidence to suggest that
our classifier works better for the BEFORE and AF-
TER classes than the OVERLAP class. We see that
as sentence gap increases, we achieve a fairly con-
sistent performance for both BEFORE and AFTER
instances. OVERLAP instances are concentrated
where the sentence gap is less than 7, with the best
accuracy figure coming in below 30%.
Although not definitive, this may be because our
data set consists of much fewer OVERLAP in-
stances than the other two classes. This bias may
have led to insufficient training data for accurate
OVERLAP classification. It will be useful to inves-
tigate if using a more balanced data set for training
can help overcome this problem.
Figure 10: Accuracy of the classifer for each temporal
class, plotted against the sentence gap of each E-E pair.
7 Conclusion
We believe that discourse features play an important
role in the temporal ordering of events in text. We
have proposed the use of different discourse anal-
ysis frameworks and shown that they are effective
for classifying the temporal relationships of article-
wide E-E pairs. Our proposed discourse-based fea-
tures are robust and work well even though auto-
matic discourse analysis is noisy. Experiments fur-
ther show that improvements to these underlying
discourse analysis systems will benefit system per-
formance.
In future work, we will like to explore how to
better exploit the various discourse analysis frame-
works for temporal classification. For instance, RST
relations are either hypotactic or paratactic. Marcu
(1997) made use of this to generate automatic sum-
maries by considering EDUs which are nuclei to be
more salient. We believe it is interesting to examine
how such information can help. We are also inter-
ested to apply discourse features in the context of a
global inferencing system (Yoshikawa et al, 2009;
Do et al, 2012), as we think such analyses will also
benefit these systems as well.
Acknowledgments
We like to express our gratitude to Quang Xuan Do,
Wei Lu, and Dan Roth for generously making avail-
able the data set they have used for their work in
EMNLP 2012. We would also like to thank the
anonymous reviewers who reviewed this paper for
their valuable feedback.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
ACE. 2005. The ACE 2005 (ACE05) Evaluation Plan.
October.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring Strategies for Sentence Order-
ing in Multidocument News Summarization. Journal
of Artificial Intelligence Research (JAIR), 17:35?55.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal Relation Classification Using Syntactic and
Semantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval),
pages 129?132, June.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, Informa-
tion Sciences Institute, University of Southern Califor-
nia, July.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A Unified Event Coreference Resolution by In-
tegrating Multiple Resolvers. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 102?110, Novem-
ber.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bOcean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33?40, July.
21
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
Inference for Event Timeline Construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP), pages
677?689, July.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
Temporal Relations with Rich Linguistic Knowledge.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT), pages 918?927, June.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistics Features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL), pages 60?68, July.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and
James C. Lester. 2010. NCSU: Modeling Temporal
Relations with Markov Logic and Lexical Ontology.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval), pages 341?344, July.
Marti A. Hearst. 1994. Multi-Paragraph Segmentation
of Expository Text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 9?16, June.
Anna Kazantseva and Stan Szpakowicz. 2011. Lin-
ear Text Segmentation Using Affinity Propagation.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 284?293, July.
Anup Kumar Kolya, Asif Ekbal, and Sivaji Bandyopad-
hyay. 2010. JU CSE TEMP: A First Step Towards
Evaluating Events, Time Expressions and Temporal
Relations. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
345?350, July.
Alex Lascarides and Nicholas Asher. 1993. Temporal
Interpretation, Discourse Relations and Commonsense
Entailment. Linguistics and Philosophy, 16(5):437?
493.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013. A
PDTB-styled End-to-End Discourse Parser. Natural
Language Engineering, FirstView:1?34, February.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 753?760, July.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From Discourse Structures to Text
Summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, volume 97,
pages 82?88, July.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), September.
Jun-Ping Ng and Min-Yen Kan. 2012. Improved Tem-
poral Relation Classification using Dependency Parses
and Selective Crowdsourced Annotations. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 2109?2124, December.
Daniele Pighin and Alessandro Moschitti. 2010. On Re-
verse Feature Engineering of Syntactic Tree Kernels.
In Proceedings of the 14th Conference on Natural Lan-
guage Learning (CoNLL), August.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC), May.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple.
2003. Using Semantic Inferences for Temporal An-
notation Comparison. In Proceedings of the 4th In-
ternational Workshop on Inference in Computational
Semantics (ICoS), September.
Eduard F. Skorochod?Ko. 1972. Adaptive Method of
Automatic Abstracting and Indexing. In Proceedings
of the IFIP Congress, pages 1179?1182.
Carlota S. Smith. 2010. Temporal Structures in Dis-
course. Text, Time, and Context, 87:285?302.
Simone Teufel and Min-Yen Kan. 2011. Robust Argu-
mentative Zoning for Sensemaking in Scholarly Doc-
uments. In Advanced Language Technologies for Dig-
ital Libraries, pages 154?170. Springer.
Naushad Uzzaman and James F. Allen. 2010. TRIPS and
TRIOS System for TempEval-2: Extracting Temporal
Information. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
276?283, July.
Naushad Uzzaman, Hector Llorens, James F. Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2012. TempEval-3: Evaluating Events, Time Expres-
sions, and Temporal Relations. Computing Research
Repository (CoRR), abs/1206.5333.
Vladimir N. Vapnik, 1999. The Nature of Statistical
Learning Theory, chapter 5. Springer.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The TempEval Challenge: Identifying
22
Temporal Relations in Text. Language Resources and
Evaluation, 43(2):161?179.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
57?62, July.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
Bonnie Webber. 2004. D-LTAG: Extending Lexicalized
TAG to Discourse. Cognitive Science, 28(5):751?779.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly Identifying
Temporal Relations with Markov Logic. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL) and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language Pro-
cessing (AFNLP), pages 405?413, August.
23
