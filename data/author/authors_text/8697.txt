  
Trajectory Based Word Sense Disambiguation 
 
Xiaojie Wang ??      Yuji Matsumoto ? 
?Graduate School of Information Science, Nara Institute of Science and Technology 
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan 
?School of Information Engineering, Beijing University of Posts and Technology 
Beijing, 100876, China 
{xiaoji-w, matsu}@is.naist.jp 
  
Abstract 
 
Classifier combination is a promising way 
to improve performance of word sense 
disambiguation. We propose a new 
combinational method in this paper. We first 
construct a series of Na?ve Bayesian 
classifiers along a sequence of orderly 
varying sized windows of context, and 
perform sense selection for both training 
samples and test samples using these 
classifiers. We thus get a sense selection 
trajectory along the sequence of context 
windows for each sample. Then we make use 
of these trajectories to make final 
k-nearest-neighbors-based sense selection for 
test samples. This method aims to lower the 
uncertainty brought by classifiers using 
different context windows and make more 
robust utilization of context while perform 
well. Experiments show that our approach 
outperforms some other algorithms on both 
robustness and performance. 
 
1  Introduction 
 
Word sense disambiguation (WSD) has long 
been a central issue in Natural Language 
Processing (NLP). In many NLP tasks, such as 
Machine Translation, Information Retrieval etc., 
WSD plays a very important role in improving 
the quality of systems. Many different algorithms 
have been used for this task, including some 
machine learning (ML) algorithms, such as 
Na?ve Bayesian model, decision trees, and 
example based learners. Since different 
algorithms have different strengths and perform 
well on different feature space, classifier 
combination is a reasonable candidate to achieve 
better performance by taking advantages of 
different approaches. In the field of ML, 
ensembles of classifiers have been shown to be 
successful in last decade (Dietterich 1997). For the 
specific task of WSD, classifier combination has 
been received more and more attention in recent 
years. 
Kilgarriff and Rosenzweig (2000) presented 
the first empirical study. They combined the 
output of the participating SENSEVAL1 systems 
via simple voting. Pedersen (2000) built an 
ensemble of Na?ve Bayesian classifiers, each of 
which is based on lexical features that represent 
co-occurring words in varying sized windows of 
context. The sense that receives majority of the 
votes was assigned as the final selection. 
Stevenson and Wilks (2001) presented a 
classifier combination framework where three 
different disambiguation modules were 
combined using a memory-based approach. 
Hoste et al (2002) used word experts consisted 
of four memory-based learners trained on 
different context. Output of the word experts is 
based on majority voting or weighted voting. 
Florian et al(2002) and Florian and Yarowsky 
(2002) used six different classifiers as 
components of their combination. They 
compared several different strategies of 
combination, which include combining the 
posterior distribution, combination based on 
order statistics and several different voting.   
Klein et al (2002) combined a number of 
different first-order classifiers using majority 
voting, weighted voting and maximum entropy. 
In Park (2003), a committee of classifiers was 
used to learn from the unlabeled examples. The 
label of an unlabeled example is predicted by 
weighted majority voting. Frank at al. (2003) 
presented a locally weighted Na?ve Bayesian 
model. For a given test instance, they first chose 
k-nearest-neighbors from training samples for it, 
then constructed a Na?ve Bayesian classifier by 
using these k-nearest-neighbors in stead of all 
training samples.  
This paper presents a new combinational 
approach. We firstly construct a series of Na?ve 
Bayesian classifiers along a sequence of orderly 
varying sized windows of context, and make 
sense selection for both training samples and test 
samples using these classifiers. We thus get a 
trajectory of sense selection for each sample, and 
then use the sense trajectory based 
k-nearest-neighbors to make final decision for 
  
test samples.  
This method is motivated by an observation 
that there is an unavoidable uncertainty when a 
classifier is used to make sense selection. Our 
approach aims to alleviate this uncertainty and 
thus make more robust utilization of context 
while perform well. Experiments show our 
approach outperform some other algorithms on 
both robustness and performance. 
The remainder of this paper is organized as 
follows: Section 2 gives the motivation of our 
approach, describes the uncertainty in sense 
selection brought by classifiers themselves. In 
section 3, we present the decision trajectory 
based approach. We then implement some 
experiments in section 4, and give some 
evaluations and discussions in section 5. Finally, 
we draw some conclusions. 
 
2  The Trajectory of Sense Selection 
 
Our method is originally motivated by an 
observation on relation between sense selection 
by a classifier and the context it uses to make this 
selection. 
As well known, context is the only means to 
identify the sense of a polysemous word. Ide 
(1998) identified three types of context: 
micro-context, topical context and domain. In 
practice, a context window ( l , r ), which 
includes l  words to the left and r  words to 
the right of the target word, is predetermined by 
human or chosen automatically by a performance 
criterion. Only information in the context 
window is then used for classifiers and 
disambiguating. What is the best window size for 
WSD has been long for a problem. Weaver (1955) 
hoped we could find a minimum value of the 
window size which can lead to the correct choice 
of sense for the target ambiguous word. 
Yarowsky (1994) argued the optimal value is 
sensitive to the type of ambiguity. Semantic or 
topic-based ambiguities warrant a larger window 
(from 20 to 50), while more local syntactic 
ambiguities warrant a smaller window (3 or 4). 
Leacock at el (1998) showed the local context is 
superior to topical context as an indicator of 
word sense. Yarowsky (2002) suggested that 
different algorithms prefer different window 
sizes.  
Followed by these works, it is clear that 
different window sizes might cause different 
sense selection for an occurrence of the target 
word even when a same algorithm is used. 
Yarowsky (2002) gave a investigation on how 
the performance changes with different window 
sizes for several different algorithms and several 
different types of word. In fact, even for human, 
different window sizes might cause different 
sense selections for a same occurrence of an 
ambiguous word. For example, considering word 
???(It has two different senses: ?read? and 
?think?) in senesce S1. 
 
S1: ?/  ?/ ?/?/  ?/   ??/   ?/  ?/. 
(I) (think) (this) (book) (worthy)  (a) (read) 
 
When we use a context window (1,1), it is not 
clear which sense should be more possible in this 
sentence. When we use (3,3), because the 
collocation with ?  give a very strong 
indication for ??s sense, it is natural that we 
select the sense of ?read? for ?. When we use 
window (6,6), we select the sense of ?think? for 
it. 
Here, the occurrence of the ambiguous word is 
the same; it is the difference of context windows 
that make the sense selection different. Since the 
context window is a built-in parameter of a 
classifier, as long as we use a classifier to 
distinguish an ambiguous word, we had to 
choose a window size. Supposing a classifier is 
an observer, choosing a window size is necessary 
for the observer to implement an observation. 
Different choices of the window size might cause 
different observational results for the same 
occurrence. That means there is an uncertainty 
brought by observer itself. It reminds us that the 
relation between the window size and the sense 
selection is to some extent similar with the 
relation between a particle?s position and its 
momentum in Heisenberg Uncertainty Principle.  
By the Uncertainty Principle, when we 
measure the position and the momentum of a 
particle, we cannot measure them with 
zero-variance simultaneously. In Quantum 
Theory, the wave-function is used to describe the 
state of a particle. The method to deal with this 
problem in Quantum Theory suggests us an idea 
to deal with the similar problem in WSD.  
Firstly, since the existence of the uncertainty 
of sense selection at different window sizes, 
sense selection for the target word at only one 
context window cannot give a complete 
description of its sense. To grasp a complete 
description of its sense, it is necessary to get 
sense selections along a series of observation, i.e. 
using a sequence of context window to get a 
trajectory of sense selection.  
Secondly, unlike that in Uncertainty Principle, 
the intuition is that, in most of time, when we 
have enough observations, we can be doubtlessly 
  
sure the sense of the target word. So, we make 
final unique sense selection based on the 
trajectory of sense selection. Since the final 
selection is based on a sense trajectory along 
different window sizes, we thus think it may 
helpful to alleviate the uncertainty brought by 
difference of context windows.  
In this way, our approach aims to improve 
robustness of WSD. Here the robustness means 
that sense selection is not sensitive to the 
window size. This kind of robustness is 
especially important to WSD system in noise or 
oral corpus, where there are many occasional 
inserted words near the target word. Besides 
robustness, to achieve better performance is also 
necessary, if robustness is at a low level of 
performance, it is useless.  
 
3  Decision Trajectory Based WSD 
 
In our approach, we firstly use Na?ve Bayesian 
(NB) algorithm to construct a sense selection 
trajectory along a sequence of orderly varying 
sized windows of context for each sample, 
including both training samples and test samples. 
Then we use k-nearest-neighbors(KNN) 
algorithm to make final decision for each test 
sample based on these trajectories.  
Let w  be an ambiguous word, it has n  
different senses, 1s ? is ? ns . Supposing we 
have q  training samples 1S ? jS ? qS , where 
iq  samples are tagged with sense is , 
qqi =? . We present our approach in two 
stages: training stage and test stage. Figure 1 
gives a skeleton of the algorithm.  
In the training stage, we first choose a 
sequence of context windows.   
 mT : ( 1p ,? kp ... mp ) 
Where kp =( kl , kr ) is a context window 
which includes kl  words to the left of word w  
and kr  words to the right. We call mT  a 
trajectory of context windows, kp  is a window 
point in this trajectory. For example, a trajectory 
((1,1), (3,3), (5,5), (7,7), (9,9)) includes 5 points.  
For each window point kp  in mT , we 
construct a classier by using NB algorithm based 
on context word in kp . Let )( kpC  denote the 
classifier, it can be thought as an operator that make 
sense selection upon samples. With the change of 
the window point, we can get a operate vector:  
))(),...,(),...,(( 1 mk pCpCpCC =   
Training stage: 
1. To construct a operator vector C  along a 
sequence of context windows Tm :  
   ))(),...,(),...,(( 1 mk pCpCpCC = . 
)( kpC  is a NB classifier learned by all the 
tagged data using kp  as the context 
window.   
2.  For each training sample jS , to operate C  
upon it to construct a sense trajectory, j?  
( qj ,...,1= ). 
 
Test stage:  
1.  For a new sample S , to construct its 
decision trajectory ?  by operating C  
upon it.  
2.  For qj ,...,1= , to calculate ),( jd ??  
3.  to make KNN-based sense choice for S . 
Figure 1. The algorithm of trajectory-based WSD 
 
 
For a sample jS  for sense is , we use 
)( kpC ( jS ) to denote using )( kpC  to classify 
jS , we can get a sense selection denoted by 
)( kj p? , i.e., )( kpC ( jS )= )( kj p? . We call 
)( kj p?  a point decision. If )( kj p? = is , we 
borrow a term to call jS  an eigen-sample of the 
operator )( kpC , is  is its eigenvaluve. 
With the change of the window point, we get a 
sequence of point decisions for sample jS  
along the window trajectory mT , we denoted it 
by 
  j? =( )( 1pj? ,?, )( mj p? ) 
We call it decision trajectory of sample jS  
along the context windows trajectory mT . If all 
elements of j?  is is , i.e. j? =( is ,?, is ), we 
call jS  an eigen-sample of operator C , j?  is 
a eigen-trajectory of C .  
In this way, we transfer training samples into 
training decision trajectories, which will be used 
as instance for final KNN-based sense selection. 
An eigen-trajectory is a good indication for a 
sample, but when all the training samples are 
eigen-samples, it is not a good thing for 
disambiguating new samples. We will discuss 
this case in section 5.2.  
After finishing training stage of our approach, 
we have a context windows trajectory mT , a 
sequence of classifiers )( kpC  along mT ? and 
  
a decision trajectory for each training sample. All 
these compose of our classifier for a new sample 
in test. When a new sample is given, we first 
calculate a decision trajectory ?  for it by using 
C  operating upon it. Let  
 
 ? =( )( 1p? ,?, )( mp? )  
We then calculate the similarity between ?  and 
j? , kj ,...2,1=  by using (3.1). 
m
pp
Sim
m
i
iji
j
?
== 1
))(),((
),(
???
??      (3.1) 
Where 1),( =yx?  at yx = , and 0),( =yx?  
at yx ? . We then choose h  training decision 
trajectory samples as ? ?s h  nearest neighbors, 
supposing that ih  samples are tagged with 
sense is  among these nearest neighbors, 
hhi =? , ii qh ? , then by solving (3.2), We 
choose ?is  as the final sense selection for the 
new sample. 
?
????
? =
ihj
j
ni
Simi
11
),(maxarg ??            (3.2) 
If all training samples are eign-samples, the 
similarity between ?  and j?  for the same 
sense are the same, (3.2) is changed to:  
},...,1|,})({{|maxarg
1
mkipi k
ni
===
??
? ?  (3.3) 
Then the final KNN-based decision is simplified 
to majority voting along the decision trajectory 
of the new sample. 
 
4  Experiments 
 
4.1 Experimental Data  
 
All experimental data were extracted from 
Chinese People Daily from January 1995 to 
December 1996. Eight Chinese ambiguous words 
were used in the experiments as listed in the first 
column of Table1. In the Second column, we 
give some information about samples used in 
experiments. The number before each bracket is 
the number of senses. Numbers in each bracket 
are amounts of samples used for each sense. 
They were annotated by a Chinese native speaker, 
and checked by another native speaker. Some 
samples without inter-agreement between two 
native speakers had been excluded. 
Only word co-occurrences in given windows 
are used as features through all experiments in 
this paper. 
4.2  Experimental  Method 
 
In order to do a comparative study, we have 
implemented not only our algorithm, but also 
four other related algorithms in our experiments. 
They fall into two classes. NB (Manning and 
Schutze 1999) and KNN (Ng and Lee 1996) are 
two components of our approach. Locally 
weighted NB(LWNB, Frank et al 2003) and 
Ensemble NB(ENB Pedersen 2000) are two 
combinational approaches. Since our aim is to 
compare not only the performance but also the 
robustness of these algorithms, we implemented 
each algorithm in following way. 
We note our approach TB_KNN when (3.2) is 
used for final decision, and TB_VOTE when (3.3) 
is used for final decision.  
We firstly constructed a sequence of context 
windows kp =( kl , kr ) 40,...,1=k  in 
following way:  
1. Initiate: 1,0 11 == rl  
2. Generate next window:   
??
?
<=+=
=+==
++
++
kkkkkk
kkkkkk
rlifrrll
rlifrrll
11
11
,1
1,
 
39,...,1=k  
 
We then constructed a sequence of window 
trajectories. 
),...,( 1 ii ppT =    40,...,1=i  
We implemented TB_KNN and TB_VOTE on 
each trajectory from 1T  to 40T .  
Obviously, our iT -based sense selection and 
ip -based selection in fact make use of same 
context surrounding the target word. ip  is the 
biggest window along iT . We implemented NB 
classifiers (noted by P) from 1p  to 40p . 
  KNN was implemented along the same 
sequence of context window, from 1p  to 40p .  
For the implementation of algorithm LWNB, 
we used the measure in (Ng and Lee 1996) to find k 
nearest neighbors for each sample, and then 
constructed a NB classifier according to 
(Frank2003). This algorithm was also implemented 
for each context window along the sequence from 
1p  to 40p . 
ENB was implemented according to (Petersen 
2000). Different left and right window sizes we  
used is (1,2,3,4,5,6,10,15,20). Since one 
implementation of this algorithm make use of all 
these different window sizes. It cannot be 
implemented along above windows sequence, so 
there is only one implementation for this 
  
algorithm. 
For each ambiguous word, we implemented 
above experiments respectively, each experiment 
was a 10-fold cross-validation, at each time 90% 
of the data were used as training data, 5% were 
used as development data, and other 5% were 
used as test data. 
 
4.3 Experimental Results 
 
We give the results curves for word ??? in 
Figure 2 and for word ??? in Figure 3. In both 
figures, x-axis is the context window, from (0,1) 
to (20,20), y-axis is F-measure, and different 
marker style is for different algorithms. Results 
curves for other six target words have similar 
shapes.  
We list a summary of results for all 8 words in 
Table 1. In TB_KNN column, there are three values: 
mean, maximum and standard variance of 
F-measure of 40 different trajectories from 1T  to 
40T . Results are summarized in the same way in 
column TB_VOTE. For column P, KNN and 
LWNB, three values are mean, maximum and 
standard variance of F-measure of 40 different 
points from 1p  to 40p . In column ENB, there is 
only one F-measure. 
 
5  Evaluation 
 
5.1 Comparison with other algorithms 
 
As we have mentioned, we compare results of 
each algorithm on both performance and 
robustness. Performance can be compared 
directly from F-measure point-wise along a 
sequence of context windows(or trajectories). We 
also use mean and maximum (max) along the 
sequence to give an overall comparison. 
Robustness of an algorithm means that sense 
decision varies gracefully with the change of 
context windows (or trajectories) it uses. 
Intuitionally, it can be reflected by a 
context-performance curve, a flat curve is more 
robust than a sharp one. We also use standard 
variance (S.V.) along a sequence of sense 
selection to give an overall comparison. A 
sequence with small standard variance is more 
robust than that with a big one. 
From Figures 2 and 3, we can get an 
intuitional impression that TB_KNN not only 
achieve the best performance at most of points, 
but also has the flattest curve shape. This means 
TB_KNN outperforms other algorithm on both 
performance and robustness. This can be detailed 
in Table 1 by comparing mean/max/S.V. of 
TB_KNN with their correspondences in other 
algorithms. 
Comparing values in the TB_KNN column 
with their correspondences in column P, we can 
find all values of TB_KNN are consistently 
better than those in P. For ?mean? and ?max?, a 
bigger one is better, while for S.V., a little one is 
better. Comparing values in the TB_KNN 
column with their correspondences in column 
KNN, we can find nearly all values of TB_KNN 
are better than those in KNN. (Except that 
KNN?s max and S.V. for word ??? are better 
then those in TB_KNN). All differences are 
significant. This means our decision trajectory 
based classifier is better than a NB classifier or a 
KNN classifier. The combination takes 
advantages of both NB and KNN methods. It 
seems that KNN directly based on word 
co-occurrence features suffers deeply from data 
sparseness. While KNN based on decision 
trajectory can alleviate the influence of data 
sparseness. In our final KNN decision, sense 
selection is also not sensitive to the number of 
nearest neighbors. 
Comparing values in TB_KNN column with 
their correspondences in column LWNB, we can 
find most of values in TB_KNN are better than 
their correspondences in LWNB. But the 
differences are not so bigger than those described 
points in the trajectory
F-
M
ea
su
re
1.0
.9
.8
.7
?
TB_K
TB_V
P
KNN
LWNB
Figure 2. context-performance curves for ??? 
points in the trajectory
F-
M
ea
su
re
1.0
.9
.8
.7
?
TB_K
TB_V
P
KNN
LWNB
  Figure 3. context-performance curves for ??? 
  
Word Num-Sen TB_KNN TB_VOTE  P KNN LWNB ENB 
?  3(68,73,35) 0.88/0.91/0.03 0.86/0.89/0.03 0.83/0.88/0.06 0.86/0.91/0.04 0.87/0.92/0.04 0.89 
? 3(31,62,64) 0.95/0.98/0.04 0.94/0.96/0.05 0.90/0.95/0.05 0.83/0.91/0.05 0.90/0.96/0.04 0.96 
? 3(25,28,18) 0.89/0.94/0.03 0.88/0.94/0.04 0.80/0.90/0.10 0.69/0.82/0.07 0.76/0.87/0.07 0.82 
? 4(42,36,31,28) 0.80/0.84/0.04 0.79/0.83/0.04 0.74/0.83/0.06 0.75/0.81/0.05 0.74/0.80/ 0.04 0.79 
? 2(24,33) 0.93/ 0.97/0.03 0.92/0.97/0.04 0.88/0.95/0.05 0.85/0.92/0.05 0.91/ 0.97/0.04 0.89 
? 2(40,36) 0.91/0.96/0.06 0.89/0.94/0.07 0.85/0.96/0.18 0.89/0.97/0.06 0.87/0.97/0.06 0.84 
? 2(43,52) 0.86/0.89/0.03 0.84/0.87/0.04 0.83/0.88/0.04 0.73/0.80/0.03 0.82/0.88/0.04 0.89 
? 2(15,15) 0.83/ 0.92/0.05 0.82/ 0.89/0.05 0.77/0.87/0.07 0.49/0.82/0.13 0.79/0.89/0.06 0.77 
Table 1 result summary 
 
in above paragraph, especially when the number 
of training samples is relatively big. In Frank et 
al.(2003), the number of training samples is 
large.(Most of them are more than several 
hundreds.) They used 50 local training samples 
to construct a NB classifier. It is always 
impossible in our experiments and in most WSD 
tasks. 
  Although not all of the values of mean in 
TB_KNN column are bigger than their 
correspondences in ENB, all maximums are 
bigger (or equal) than those in ENB. Comparing 
with ENB, We think the trajectory based approach 
may make use of NB decisions in a more 
systematical way than selecting some classifiers 
for voting in ENB, and also, our approach receives 
benefits from the final KNN decision, which can 
make some exceptions under consideration. 
Let us give a discussion on how our 
trajectory-based approach makes use of 
information in context. 
Firstly, although each NB classifier use 
bag-of-words as its features, because window 
size for NB classifiers is extended sequentially, 
the decision trajectory thus reflects influences 
brought by context words in different positions. 
That is to say, changing the position of a 
co-occurrence word in a sentence might cause 
different final decision in trajectory-based 
approach. While in point-based approach, as 
long as the co-occurrence word is in the context 
window, a classifier based on bag-of-words 
features always makes the same selection no 
matter how to change the position of that word. 
From this view, the trajectory-based approach in 
fact makes use of position information of words 
in context. 
Secondly, because of its implicit utilization of 
position information of context words, it may 
make use of information from some decisions 
locally correct but globally wrong. For example, 
we consider sentence S1 in section 2 again.  
 
S1:?/ ?/ ?/?/ ?/    ??/  ?/  ?/. 
(I) (think) (this) (book) (worthy) (a)  (read) 
 
On the one hand, as we have said, when we 
use context window (3,3), we select the sense of 
?read? for ?. Although it is a wrong sense 
selection for this word in this sentence (when 
context window is (6,6)), it is a correct selection 
for the local collocation (when ? collocates 
with ?, its sense is ?read?). By saving this 
information, we cannot only make use of 
information of sense selection for the sentence, 
but also information for this collocation. In other 
words, the sentence S1 gives us two samples for 
different senses of the target word.  
On the other hand, that a polysemous word 
changes their probability for different sense with 
the change of context window is one type of 
pattern for sense ambiguity, the trajectory based 
approach seems an efficient way to grasp this 
pattern of ambiguity. 
 
5.2 Trajectory 
 
In TB_KNN, we need to calculate a sense 
decision trajectory for each training sample, not 
all of these trajectories are eigen-trajecories. In 
TB_VOTE, we don?t calculate sense decision 
trajectories for training samples, all training 
decision trajectories are regarded as eigen- 
trajectory, final decision for a new sample 
reduces to majority voting along the trajectory. 
Comparing TB_KNN and TB_VOTE, we can 
find that both performance and robustness of 
TB_VOTE fall. This means existence of 
non-eigen-trajectory is in fact helpful, which can 
make some exceptions under consideration by 
using KNN.  
  In above experiments, we generated a 
trajectory by adding one context word each time. 
We further explored if a looser trajectory can get 
the same performance. We first excluded even 
points in original trajectories in above 
  
experiments to get some new trajectories. For 
example, by excluding even points of the 
trajectory },...,{ 40140 ppT = , we got:  
 20,...,1},,..,,...,{ 3912120
' == ? kpppT k   
Note this 20'T  is different from 20T  in above 
experiments, where 20T  is:  
20,...,1},,..,,...,{ 20120 == kpppT k   
In this way, we got 20 different trajectories 
TG2: 20'1' ,...,TT , jT '  includes half number 
of points comparing with its correspondence 
jT2  in above experiments. The longest 
trajectory includes 20 points. We repeated above 
TB_KNN experiment along these new 
trajectories. Results are listed in column 
TB_KNN TG2 in Table2. We excluded even 
points to generate TG3 and TG4 which include 
at most 10 and 5 points respectively in their 
trajectories. We also repeated same TB_KNN 
experiment on TG3 and TG4.  
 
 TB_KNN TG2 TB_KNN TG3 TB_KNN TG4
? 0.87/0.90/0.03 0.87/0.91/0.03 0.86/0.89/0.03 
? 0.95/0.97/0.01 0.95/0.96/0.01 0.94/0.95/0.02 
? 0.90/0.93/0.02 0.90/0.91/0.02 0.90/0.93/0.03 
? 0.80/0.84/0.02 0.78/0.82/0.03 0.77/0.81/0.02 
? 0.93/0.97/0.03 0.93/0.95/0.02 0.92/0.96/0.03 
? 0.91/0.94/0.06 0.91/0.93/0.06 0.90/0.94/0.09 
? 0.86/0.90/0.03 0.86/0.90/0.04 0.82/0.85/0.03 
? 0.83/0.92/0.05 0.85/0.92/0.05 0.82/0.92/0.07 
Table 2: shorter length in the trajectory  
From Table 2, we can find that performance 
of classifiers using trajectories with small 
number of points do not decrease significantly. 
That is to say, a shorter trajectory can also 
achieve good performance.  
  
6  conclusions 
 
This paper presents a new type of classifier 
combination method. We firstly construct a 
sequence of NB classifiers along orderly varying 
sized windows of context, and get a trajectory of 
sense selection for each sample, then use the 
sense trajectory based KNN to make final 
decision for test samples. Experiments show that 
our approach outperforms some other algorithms 
on both robustness and performance. 
We will do further investigations on the 
trajectory to see if there exists some skeletal 
points like quantum numbers in the 
wavefunction in Quantum Theory. 
References 
 
Thomas G. Dietterich. 1997. Machine Learning 
Research: Four Current Directions. AI Magazine. 
Vol. 18, No. 4 pp.97-136. 
Radu Florian, Silviu Cucerzan, C Schafer and D. 
Yarowsky. 2002. Combining Classifiers for 
Word Sense Disambiguation. Journal of Natural 
Language Engineering. Vol. 8 No.4.  
Radu Florian and D. Yarowsky. 2002. Modeling 
Consensus: Classifier Combination for Word 
Sense Disambiguation. In Proceedings of 
EMNLP'02, pp25-32.  
Eibe Frank, M. Hall and Bernhard Pfahringer. 
2003. Locally Weighted Na?ve Bayes. 
Proceedings of the Conference on Uncertainty 
in Artificial Intelligence.  
V?ronique Hoste, I. Hendrickx, W. Daelemans, 
and A. van den Bosch.2002. Parameter 
optimization for machine-learning of word 
sense disambiguation. Natural Language 
Engineering,8(3).  
Nancy Ide, J Veronis.1998. Introduction to the 
Special Issue on Word Sense Disambiguation: 
The State of the Art. Computational Linguistics, 
24(1):1-40.  
Dan Klein, K. Toutanova, H. Tolga Ilhan, S. D. 
Kamvar, and C. D. Manning. 2002. Combining 
Heterogeneous Classifiers for Word-Sense 
Disambiguation. In Workshop on Word Sense 
Disambiguation at ACL 40, pages 74-80.  
Adam Kilgarriff and J. Rosenzweig (2000). 
Framework and results for English Senseval. 
Computers and the Humanities. 34(1):15-48.  
Chris D. Manning and H. Schutze. 1999. 
Foundations of Statistical Natural Language 
Processing. MIT Press.  
Rada Mihalcea. 2002. Word Sense Disambiguation 
Using Pattern Learning and Automatic Feature 
Selection, Journal of Natural Language and 
Engineering, 8(4):343-358.  
Hwee Tou Ng, Hian Beng Lee. 1996. Integrating 
Multiple Knowledge Sources to Disambiguate 
Word Sense: An Exemplar-Based Approach. In 
Proceedings of the Thirty-Fourth ACL.  
Ted Pedersen 2000. A Simple Approach to 
Building Ensembles of Naive Bayesian 
Classifiers for Word Sense Disambiguation. In 
the Proceedings of the NAACL-00.  
David Yarowsky 1994. Decision Lists for 
Lexical Ambiguity Resolution: Application to 
Accent Restoration in Spanish and French.'' In 
Proceedings of the 32nd ACL. pp. 88-95.  
David Yarowsky and R. Florian.2002. 
Evaluating Sense Disambiguation Performance 
Across Diverse Parameter Spaces. Journal of 
Natural Language Engineering, Vol.8, No 4. 
BUPT Systems in the SIGHAN Bakeoff 2007 
Ying Qin   Caixia Yuan   Jiashen Sun   Xiaojie Wang 
Center of Intelligent Science and Technology Research 
Beijing University of Posts and Telecommunications 
Beijing, 100876, China 
qinyingmail@163.com, yuancx@gmail.com, 
b.bigart911@gmail.com, xjwang@bupt.edu.cn 
 
 
Abstract 
Chinese Word Segmentation(WS), Name 
Entity Recognition(NER) and Part-Of-
Speech(POS) are three important Chinese 
Corpus annotation tasks. With the great 
improvement in these annotations on some 
corpus, now, the robustness, a capability of 
keeping good performances for a system by 
automatically fitting the different corpus 
and standards, become a focal problem. 
This paper introduces the work on 
robustness of WS and POS annotation 
systems from Beijing University of Posts 
and Telecommunications(BUPT), and two 
NER systems. The WS system combines a 
basic WS tagger with an adaptor used to fit 
a specific standard given. POS taggers are 
built for different standards under a two 
step frame, both steps use ME but with 
incremental features. A multiple 
knowledge source system and a less 
knowledge Conditional Random Field 
(CRF) based systems are used for NER. 
Experiments show that our WS and POS 
systems are robust. 
1 Introduction 
In the last SIGHAN bakeoff, there is no single 
system consistently outperforms the others on 
different test standards of Chinese WS and NER 
standards(Sproat and Emerson, 2003). 
Performances of some systems varied significantly 
on different corpus and different standards, this 
kind of systems can not satisfy demands in 
practical applications. The robustness, a capability 
of keeping good performances for a system by 
automatically fitting the different corpus and 
standard, thus become a focal problem in WS and 
NER, it is the same for Chinese Part-of-
Speech(POS) task which is new in the SIGHAN 
bakeoff 2007.  
It is worthy to distinguish two kinds of different 
robustness, one is for different corpus (from 
different sources or different domain and so on) 
under a same standard, we call it corpus robustness, 
and another is for different standards (for different 
application goals or demands and so on) for a same 
corpus. We call it standard robustness. The 
SIGHAN bakeoff series seems to focus more on 
later. We think corpus robustness should be 
received more attentions in the near future. 
We participant all simplified Chinese track on 
WS, NER and POS task in the SIGHAN bakeoff 
2007. There are more than two tracks for WS and 
POS. This gives us a chance to test the robustness 
of our systems. This paper reports our WS, NER 
and POS systems in the SIGHAN Bakeoff 2007, 
especially on the work of achieving robustness of 
WS and POS systems.  
This paper is arranged as follows, we introduce 
our WS, NER and POS system separately in 
section 2, section 3 and section 4, experiments and 
results are listed in section 5, finally we draw some 
conclusions. 
2 Word Segmentation 
WS system includes three sequent steps, which are 
basic segmentation, disambiguation and out-of 
vocabulary (OOV) recognition. In each step, we 
construct a basic work unit first, and then have an 
adaptor to tune the basic unit to fit different 
standards. 
94
Sixth SIGHAN Workshop on Chinese Language Processing
2.1 Basic Segmentation 
For constructing a basic work unit for WS, a 
common wordlist containing words ratified by four 
different segmentation standards (from SXU, NCC, 
PKU and CTB separately) are built. We finally get 
64,000 words including about 1500 known entity 
words as the common wordlist. A forward-
backward maximum matching algorithm with the 
common wordlist is employed as the common unit 
of our basic segmentor.  
To cater for different characteristics in different 
segmentation standards, we construct another 
wordlist containing words for each specification.  
A wordlist based adaptor is built to implement the 
tuning task after basic segmentation.  
2.2 Disambiguation 
Disambiguation of overlapping Ambiguity (OA) is 
a major task in this step.  
Strings with OA are also detected during basic 
forward-backward maximum matching in basic 
WS step. These strings are common OA strings for 
different standards. Class-based bigram model is 
applied to resolve the ambiguities. In class-based 
bigram, all named entities, all punctuation and 
factoids is one class respectively and each word is 
one class. We train the bigram transition 
probability based on the corpus of Chinese 
People?s Daily 2000 newswire.  
For corpus from different standards, overlapping 
ambiguity strings with less than 3 overlapping 
chain are extracted from each train corpus. We do 
not work on all of them but on some strings with a 
frequency that is bigger than a given value. A 
disambiguation adaptor using the highest 
probability segmentations is built for OA strings 
from each different standard.  
2.3 OOV Recognition 
In OOV recognition, we have a similar model 
which consists of a common part based on 
common characteristics and an individual part 
automatically constructed for each standard. 
We divide OOV into factoid which contains 
non-Chinese characters like date, time, ordinal 
number, cardinal number, phone number, email 
address and non-factoid.  
Factoid is recognized by an automaton. To 
compatible to different standards, we also built 
core automata and several adaptors. 
Non-factoid is tackled by a unified character-
based segmentation model based on CRF. We first 
transform the WS training dataset into character-
based two columns format as the training dataset in 
NER task. The right column is a boundary tag of 
each character. The boundary tags are B I and S, 
which B is the tag of the first character of a word 
which contains more than two characters, I is the 
other non-initial characters in a word, S is for the 
single character word. Then the transformed 
training data is used to train the CRF model. 
Features in the model are current character and 
other three characters within the context and 
bigrams.  
The trigger of non-factoid recognition is 
continual single character string excluding all the 
punctuations in a line after basic word matching, 
disambiguation and factoid incorporation. The 
model will tell whether these consecutive 
characters can form multi-character words in a 
given context. 
At last, several rules are used to recognize some 
proper names separated by coordinate characters 
like ???, ???, ??? and symbol ??? in foreign 
person names.  
3 Named Entity Recognition 
We built two NER systems separately. One is a 
unified named entity model based on CRF. It used 
only a little knowledge include a small scale of 
entity dictionary, a few linguistic rules to process 
some special cases such as coordinate relation in 
corpus and some special symbols like dot among a 
transliteration foreign person name.  
Another one is an individual model for each 
kind of entity based on Maximum Entropy where 
more rules found from corpus are used on entity 
boundary detection. Some details on this model 
can be found in Suxiang Zhang et al2006. 
4 POS Tagging 
In POS, we construct POS taggers for different 
standards under a two steps frame, both steps use 
ME but with incremental features. First, we use 
normal features based Maximum Entropy (ME) to 
train a basic model, and then join some 
probabilistic features acquired from error analysis 
to training a finer model.  
95
Sixth SIGHAN Workshop on Chinese Language Processing
4.1 Normal Features for ME 
In the first step of feature selection for ME 
tagger, we select contextual syntactic features for 
all words basing on a series of incremental 
experiments. 
For shrinking the search space, the model only 
assigns each word a label occurred in the training 
data. That is, the model builds a subset of all POS 
tags for each token and restricts all possible labels 
of a word within a small candidate set, which 
greatly saves computing cost. 
We enlarged PKU training corpus by using one 
month of Peking University's China Daily corpus 
(June in 2003) and CTB training corpus by using 
CTB 2.0 data which includes 325 passages. 
To adapt with the given training corpus, the 
samples whose labels are not included in the 
standard training data were omitted firstly. After 
preprocessing, we get two sets of training samples 
for PKU and CTB with 1178 thousands tokens and 
206 thousands tokens respectively. But the NCC 
test remains its original data due to we have no 
corpus with this standard. 
4.2 Probabilistic feature for ME 
By detecting the label errors when training and 
testing using syntactic features such as words 
around the current tokens and tags of previous 
tokens, words with multiple possible tags are 
obviously error-prone. We thus define some 
probabilistic features especially for multi-tag 
words.  
We find labels of these tokens are most closely 
related to POS tag of word immediately previous 
to them. For instance, in corpus of Peking 
University, word ?Report? has three different tags 
of ?n(noun), v(verb), vn(noun verb)?. But when we 
taken into account its immediately previous words, 
we can find that when previous word's label is 
?q(quantifier)?, ?Report? is labeled as ?n? with a 
frequency of 91.67%, ?v? with a frequency of 
8.33% and ?vn? with a frequency of 0.0%. We can 
assume that ?Report? is labeled as ?n? with the 
91.67% probability when previous word's label is 
?q?, and so on. 
 Such probability is calculated from the whole 
training data and is viewed as discriminating 
probabilistic feature when choosing among the 
multiple tags for each word.  But for words with 
only one possible tag, no matter what the label of 
previous word is, the label for them is always the 
tag occurred in the training data.  
5 Experiments 
We participant all simplified Chinese tracks on WS, 
NER and POS task in the SIGHAN bakeoff 2007. 
Our systems only deal with Chinese in GBK code. 
There are some mistakes in some results submitted 
to bakeoff organizer due to coding transform from 
GBK to UTF-16. We then use WS evaluation 
program in the SIGHAN bakeoff 2006 to re-
evaluate WS system using same corpus, as for POS, 
since there is no POS evaluation in the SIGHAN 
bakeoff 2006, we implement a evaluation using 
ourselves? program using same corpus.  
Table 1 shows evaluation results of WS using 
evaluation programs from both the SIGHAN 
bakeoff 2007 and the SIGHAN bakeoff 2006. 
Table 2 lists evaluation results of NER using 
evaluation program from the SIGHAN bakeoff 
2007. Table 3 gives evaluation results of POS 
using evaluation programs from both the SIGHAN 
bakeoff 2007 and ourselves(BUPT).  
 
 
Track UTF-16 
(SIGHAN4) 
GBK 
(SIGHAN 3) 
CTB 0.9256 0.950 
SXU 0.8741 0.969 
NCC 0.9592 0.972 
Table 1. WS results (F-measure) 
 
 
SIGHAN 4 R P F 
System-1 0.8452 0.872 0.8584 
System-2 0.8675 0.9163 0.8912 
Table 2. NER results (F-measure) 
 
 
Track UTF-16 
(SIGHAN 4) 
GBK 
(BUPT) 
CTB 0.9689 0.9689 
NCC 0.9096 0.9096 
PKU 0.6649 0.9462 
Table 3. POS Results (F-measure) 
 
From the table 1 and Table 3, we can find our 
system is robust enough. WS system keeps at a 
relatively steady performance. Difference in POS 
96
Sixth SIGHAN Workshop on Chinese Language Processing
between NCC and other two tracks is mainly due 
to the difference of the training corpus.  
6 Conclusion 
Recently, the robustness, a capability of keeping 
good performances for a system by automatically 
fitting the different corpus and standards, become a 
focal problem. This paper introduces our WS, NER 
and POS systems, especially on how they can get a 
robust performance. 
The SIGHAN bakeoff series seems to focus 
more on standard robustness. We think corpus 
robustness should be received more attentions in 
the near future. 
 
Acknowledgement 
Thanks to Zhang Yan, Zhang Bichuan, Zhang 
Taozheng, Liu Haipeng and Jiang Huixing for all 
the work they done to make the WS, NER and 
POS systems go on wheels in a very short time.  
References 
Berger, A., Della Pietra, S. and Della Pietra, V.: A 
Maximum Entropy Approach to Natural 
Language Processing. Computational 
Linguistics. 22(1): pp 39-71, 1996. 
Thomas Emerson. 2005. The Second International 
Chinese Word Segmentation Bakeoff. In 
Proceedings of the Fourth SIGHAN Workshop 
on Chinese Language Processing, Jeju Island, 
Republic of Korea. 
NanYuan Liang. 1987 A Written Chinese 
Segmentation system? CDWS.  Journal of 
Chinese Information Processing, Vol.2: 44-52 
YaJuan Lv, Tie-jun Zhao, et al 2001. Leveled 
unknown Chinese Words resolution by dynamic 
programming. Journal Information Processing, 
15(1): 28-33.  
Yintang Yan, XiaoQiang Zhou. 2000. Study of 
Segmentation Strategy on Ambiguous Phrases 
of Overlapping Type  Journal of The China 
Society For Scientific and Technical Information  
Vol. 19 , ?6  
Richard Sproat and Thomas Emerson. 2003. The 
First International Chinese Word Segmentation 
Bakeoff. In Proceedings of the Second SIGHAN 
Workshop on Chinese Language Processing, 
Sapporo, Japan. 
Caixia Yuan, Xiaojie Wang, Yixin Zhong. Some 
Improvements on Maximum Entropy Based 
Chinese POS Tagging. The Journal of China 
Universities of Posts and Telecommunications, 
Vol. 13, pp 99-103, 2006. 
Suxiang Zhang, Xiaojie Wang, Juan Wen, Ying 
Qin, Yixin Zhong. A Probabilistic Feature 
Based Maximum Entropy Model for Chinese 
Named Entity Recognition, in proceedings of 
21st International Conference on the Computer 
Processing of Oriental Languages,December 
17-19, 2006, Singapore.  
 
 
97
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 54?62,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Accurate Learning for Chinese Function Tags from Minimal Features
Caixia Yuan
1,2
, Fuji Ren
1,2
and Xiaojie Wang
2
1
The University of Tokushima, Tokushima, Japan
2
Beijing University of Posts and Telecommunications, Beijing, China
{yuancai,ren}@is.tokushima-u.ac.jp
xjwang@bupt.edu.cn
Abstract
Data-driven function tag assignment has
been studied for English using Penn Tree-
bank data. In this paper, we address
the question of whether such method can
be applied to other languages and Tree-
bank resources. In addition to simply
extend previous method from English to
Chinese, we also proposed an effective
way to recognize function tags directly
from lexical information, which is eas-
ily scalable for languages that lack suf-
ficient parsing resources or have inher-
ent linguistic challenges for parsing. We
investigated a supervised sequence learn-
ing method to automatically recognize
function tags, which achieves an F-score
of 0.938 on gold-standard POS (Part-of-
Speech) tagged Chinese text ? a statisti-
cally significant improvement over exist-
ing Chinese function label assignment sys-
tems. Results show that a small number
of linguistically motivated lexical features
are sufficient to achieve comparable per-
formance to systems using sophisticated
parse trees.
1 Introduction
Function tags, such as subject, object, time, loca-
tion, etc. are conceptually appealing by encoding
an event in the format of ?who did what to whom,
where, when?, which provides useful semantic in-
formation of the sentences. Lexical semantic re-
sources such as Penn Treebank (Marcus et al,
1994) have been annotated with phrase tree struc-
tures and function tags. Figure 1 shows the parse
tree with function tags for a sample sentence form
the Penn Chinese Treebank 5.0
1
(Xue et al, 2000)
(file 0043.fid).
1
released by Linguistic Data Consortium (LDC) catalog
NO. LDC2005T01
Figure 1: Simplified parse tree with function tags
(in black bold) for example sentence.
When dealing with the task of function tag
assignment (or function labeling thereafter), one
basic question that must be addressed is what
features can be extracted in practice for distin-
guishing different function tag types. In answer-
ing this question, several pieces of work (Blaheta
and Charniak, 2000; Blaheta, 2004; Merlo and
Musillo, 2005; Gildea and Palmer, 2002) have
already been proposed. (Blaheta and Charniak,
2000; Blaheta, 2004) described a statistical sys-
tem trained on the data of Penn Treebank to au-
tomatically assign function tags for English text.
The system first passed sentences through an au-
tomatic parser, then extracted features from the
parse trees and predicted the most plausible func-
tion label of constituent from these features. Not-
ing that parsing errors are difficult or even impos-
sible to recover at function tag recognition stage,
the alternative approaches are obtained by assign-
ing function tags at the same time as producing
parse trees (Merlo and Musillo, 2005), through
learning deeper syntactic properties such as finer-
grained labels, features from the nodes to the left
of the current node.
Through all that research, however, success-
fully addressing function labeling requires accu-
rate parsing model and training data, and the re-
54
sults of them show that the performance ceil-
ing of function labeling is limited by the parsers
they used. Given the imperfection of existing
automatic parsers, which are far from producing
gold-standard results, function tags output by such
models cannot be satisfactory for practical use.
The limitation is even more pertinent for the lan-
guages that do not have sophisticated parsing re-
sources, or languages that have inherent linguistic
challenges for parsing (like Chinese). It is there-
fore worthwhile to investigate alternatives to func-
tion labeling for languages under the parsing bot-
tleneck, both in terms of features used and effec-
tive learning algorithms.
In current study, we focused on the use of
parser-independent features for function labeling.
Specifically, our proposal is to classify function
types directly from lexical features like words and
their POS tags and the surface sentence informa-
tion like the word position. The hypothesis that
underlies our proposal is that lexical features are
informative for different function types, and cap-
ture fundamental properties of the semantics that
sometimes can not be concluded from the glance
of parse structure. Such cases come when distin-
guishing phrases of the same structure that differ
by just one word ? for instance, telling ?3??
(in Shanghai)?, which is locative, from ?3?
(in May)?, which is temporal.
At a high level, we can say that class-based dif-
ferences in function labels are reflected in statistics
over the lexical features in large-scale annotated
corpus, and that such knowledge can be encoded
by learning algorithms. By exploiting lexical in-
formation collected from Penn Chinese Treebank
(CTB) (Xue et al, 2000), we investigate a super-
vised sequence learning model to test our core hy-
pothesis ? that function tags could be guessed pre-
cisely through informative lexical features and ef-
fective learning methods. At the end of this pa-
per, we extend previous function labeling meth-
ods from English to Chinese. The result proves, at
least for Chinese language, our proposed method
outperforms previous ones that utilize sophisti-
cated parse trees.
In section 2 we will introduce the CTB re-
sources and function tags used in our study. In
section 3, we will describe the sequence learn-
ing algorithm in the framework of maximum mar-
gin learning, showing how to approximate func-
tion tagging by simple lexical statistics. Section 4
Table 1: Complete set of function labels in Chi-
nese Treebank and function labels used in our sys-
tem (selected labels).
type labels in CTB selected labels
clause types IMP imperative
Q question
(function/form)
ADV adverbial
?
discrepancies
grammatical roles EXT extent
?
FOC focus
?
IO indirect object
?
OBJ direct object
?
PRD predicate
?
SBJ subject
?
TPC topic
?
adverbials BNF beneficiary
?
CND condition
?
DIR direction
?
IJ interjective
?
LGS logic subject
?
LOC locative
?
MNR manner
?
PRP purpose/reason
?
TMP temporal
?
VOC vocative
?
miscellaneous APP appositive
HLN headline
PN proper names
SHORT short form
TTL title
WH wh-phrase
gives a detailed discussion of our experiment and
comparison with pieces of related work. Some fi-
nal remarks will be given in Section 5.
2 Chinese Function Tags
The label such as subject, object, time, location,
etc. are named as function tags
2
in Penn Chi-
nese Treebank (Xue et al, 2000), a complete list
of which is shown in Table 1. Among the 5 cat-
egories, grammatical roles such as SBJ, OBJ are
useful in recovering predicate-argument structure,
while adverbials are actually semantically oriented
labels (though not true for all cases, see (Merlo
and Palmer, 2006)) that carry semantic role infor-
mation.
As for the task of function parsing, it is reason-
able to ignore the IMP and Q in Table 1 since they
do not form natural syntactic or semantic classes.
In addition, we regard the miscellaneous labels as
an ?O? label (out of any function chunks) like la-
beling constituents that do not bear any function
2
The annotation guidelines of Penn Chinese Treebank talk
of function tags. We will use the term function labels and
function tags identically, and hence make no distinction be-
tween function labeling and function tagging throughout this
paper. Also, the term function chunk signifies a sequence of
words that are decorated with the same function label.
55
tags. Punctuation marks like comma, semi-colon
and period that separate sentences are also denoted
as ?O?. But the punctuation that appear within one
sentence like double quotes are denoted with the
same function labels with the content they quote.
In the annotation guidelines of CTB (Xue et al,
2000), the function tag ?PRD? is assigned to non-
verbal predicate. Since VP (verb phrase) is always
predicate, ?PRD? is assumed and no function tag
is attached to it. We make a slight modification to
such standard by calling this kind of VP ?verbal
predicates?, and assigning them with function la-
bel ?TAR (target verb)?, which is grouped into the
same grammar roles type with ?PRD?.
To a large extent, PP (preposition phrase) al-
ways plays a functional role in sentence, like ?PP-
MNR? in Figure 1. But there are many such PPs
bare of any function type in CTB resources. Like
in the sentence ?'c??O 25% (increase
by 25% over the same period of last year)?, ?'
c?? (over the same period of last year)? is la-
beled as ?PP? in CTB without any function labels
attached, thus losing to describe the relationship
with the predicate ?O (increases)?. In order to
capture various relationships related to the predi-
cate, we assign function label ?ADT (adjunct)? for
this scenario, and merge it with other adverbials
to form adverbials category. There are 1,415 such
cases in CTB resources, which account for a large
proportion of adverbials types.
After the modifications discussed above, in our
final system we use 20 function labels
3
(18 origi-
nal CTB labels shown in Table 2 and two newly
added labels) that are grouped into two types:
grammatical roles and adverbials.
We calculate the frequency (the number of times
each tag occurs) and average length (the average
number of words each tag covers) of each func-
tion category in our selected sentences, which are
listed in Table 2. As can be seen, the frequency of
adverbials is much smaller than that of grammati-
cal roles. Furthermore, the average length of most
adverbials are somewhat larger than 4. Such data
distribution is likely to be one cause of the lower
identification accuracy of adverbials as we will see
in the experiments.
From the layer of function labeling, sentences
3
ADV includes ADV and ADVP in CTB recourses,
grouped into adverbials. In function labeling level, EXT that
signifies degree, amount of the predicates should be grouped
into adverbials like in the work of (Blaheta and Charniak,
2000) and (Merlo and Musillo, 2005).
Table 2: Categories of function tags with their rel-
ative frequencies and average length.
Function Labels Frequency Average Length
grammatical roles 99507 2.62
FOC 133 1.89
IO 126 1.26
OBJ 25834 4.15
PRD 4428 5.20
SBJ 23809 3.02
TPC 676 3.51
TAR 44501 1.25
adverbials 33287 2.11
ADT 1415 4.51
ADV 21891 1.32
BNF 465 4.66
CND 68 3.15
DIR 1558 4.68
EXT 1048 1.99
IJ 1 1.00
LGS 204 5.42
LOC 2051 4.27
MNR 1053 4.48
PRP 224 4.91
TMP 3309 2.25
in CTB are described with the structure of ?SV?
which indicates a sentence is basically composed
of ?subject + verb?. But in order to identify objects
and complements of predicates, we express sen-
tence by ?SVO? framework in our system, which
regards sentence as a structure of ?subject + verb +
object?. The structure transformation is obtained
through a preprocessing procedure, by upgrading
OBJs and complements (EXT, DIR, etc.) which
are under VP in layered brackets.
3 Learning Function Labels
Function labeling deals with the problem of pre-
dicting a sequence of function tags y = y
1
, ..., y
T
,
from a given sequence of input words x =
x
1
, ..., x
T
, where y
i
? ?. Therefore the function
labeling task can be formulated as a stream of se-
quence learning problem. The general approach
is to learn a w-parameterized mapping function
F : X?Y ? < based on training sample of input-
output pairs and to maximize F (x, y;w) over the
response variable to make a prediction.
There has been several algorithms for label-
ing sequence data including hidden Markov model
(Rabiner, 1989), maximum entropy Markov model
(Mccallum et al, 2000), conditional random fields
(Lafferty et al, 2001) and hidden Markov support
vector machine (HM-SVM) (Altun et al, 2003;
Tsochantaridis et al, 2004), among which HM-
SVM shows notable advantages by its learning
56
non-linear discriminant functions via kernel func-
tion, the properties inherited from support vec-
tor machines (SVMs). Furthermore, HM-SVM
retains some of the key advantages of Markov
model, namely the Markov chain dependency
structure between labels and an efficient dynamic
programming formulation.
In this paper we investigate the application of
the HM-SVM model to Chinese function labeling
task. In order to keep the completeness of paper,
we here address briefly the HM-SVM algorithm,
more details of which could be founded in (Altun
et al, 2003; Tsochantaridis et al, 2004), then we
will concentrate on the techniques of applying it to
our specific task.
3.1 Learning Model
The framework from which HM-SVM are derived
is a maximum margin formulation for joint fea-
ture functions in kernel learning setting. Given n
labeled examples (x
1
, y
1
), ..., (x
n
, y
n
), the notion
of a separation margin proposed in standard SVMs
is generalized by defining the margin of a train-
ing example with respect to a discriminant func-
tion F (x, y;w), as:
?
i
= F (x
i
, y
i
;w)?max
y/?y
i
F (x
i
, y;w). (1)
Then the maximum margin problem can be de-
fined as finding a weight vector w that maxi-
mizes min
i
?
i
. By fixing the functional margin
(max
i
?
i
? 1) like in the standard setting of SVMs
with binary labels, we get the following hard-
margin optimization problem with a quadratic ob-
jective:
min
w
1
2
||w||
2
, (2)
with constraints,
F (x
i
, y
i
;w)? F (x
i
, y;w) ? 1,?
n
i=1
,?
y 6=y
i
.
In the particular setting of SVM, F is as-
sumed to be linear in some combined feature
representation of inputs and outputs ?(x, y), i.e.
F (x, y;w) = ?w,?(x, y)?. ?(x, y) can be
specified by extracting features from an obser-
vation/label sequence pair (x, y). Inspired by
HMMs, we propose to define two types of fea-
tures, interactions between neighboring labels
along the chain as well as interactions between at-
tributes of the observation vectors and a specific
label. For instance, in our function labeling task,
we might think of a label-label feature of the form
?(y
t?1
, y
t
) = [[y
t?1
= SBJ ? y
t
= TAR]], (3)
that equals 1 if a SBJ is followed by a TAR. Anal-
ogously, a label-observation feature may be
?(x
t
, y
t
) = [[y
t
= SBJ ? x
t
is a noun]], (4)
which equals 1 if x at position t is a noun and la-
beled as SBJ. The described feature map exhibits
a first-order Markov property and as a result, de-
coding can be performed by a Viterbi algorithm in
O(T |?|
2
).
All the features extracted at location t are sim-
ply stacked together to form ?(x, y; t). Finally,
this feature map is extended to sequences (x, y) of
length T in an additive manner as
?(x, y) =
T
?
t=1
?(x, y; t). (5)
3.2 Features
It deserves to note that features in HM-SVM
model can be easily changeable regardless of de-
pendency among them. In this prospect, features
are very far from independent can be cooperated
in the model.
By observing the particular property of function
structure in Chinese sentences, we design several
sets of label-observation features which are inde-
pendent of parse trees, namely:
Words and POS tags: The lexical context is ex-
tremely important in function labeling, as indi-
cated by their importance in related task of phrase
chunking. Due to long-distance dependency of
function structure, intuitively, more wider con-
text window will bring more accurate prediction.
However, the wider context window is more likely
to bring sparseness problem of features and in-
crease computation cost. So there should be a
proper compromise among them. In our experi-
ment, we start from a context of [-2, +2] and then
expand it to [-4, 4], that is, four words (and POS
tags) around the word in question, which is closest
to the average length of most function types shown
in Table 2.
Bi-gram of POS tags: Apart from POS tags them-
selves, we also try on the bi-gram of POS tags. We
regard POS tag sequence as an analog to function
57
chains, which reveals somewhat the dependent re-
lations among words.
Verbs: Function labels like subject and object
specify the relations between verb and its argu-
ments. As observed in English verbs (Levin,
1993), each class of verb is associated with a set
of syntactic frames. Similar criteria can also be
found in Chinese. In this sense, we can rely on
the surface verb for distinguishing argument roles
syntactically. Besides the verbs themselves, we
also take into account the special words sharing
common property with verbs in Chinese language,
which are active voice ?r(BA)? and passive voice
?Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 158?161,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Word Segmentation and Named Entity Recognition for SIGHAN 
Bakeoff3 
 
Zhang Suxiang 
CISTR,  
Beijing University of 
Posts and 
Telecommunications 
zsuxiang@163.com 
Qin Ying  
CISTR,  
Beijing University of 
Posts and 
Telecommunications 
qinyingmail@163.com 
Wen Juan 
CISTR,  
Beijing University of 
Posts and 
Telecommunications 
mystery999@163.com 
Wang Xiaojie 
CISTR,  
Beijing University of 
Posts and 
Telecommunications 
xjwang@bupt.edu.cn
 
 
Abstract 
We have participated in three open tracks 
of Chinese word segmentation and 
named entity recognition tasks of 
SIGHAN Bakeoff3. We take a 
probabilistic feature based Maximum 
Entropy (ME) model as our basic frame 
to combine multiple sources of 
knowledge. Our named entity recognizer 
achieved the highest F measure for 
MSRA, and word segmenter achieved the 
medium F measure for MSRA. We find 
effective combining of the external 
multi-knowledge is crucial to improve 
performance of word segmentation and 
named entity recognition. 
1 Introduction 
Word Segmentation (WS) and Named Entity 
Recognition (NER) are two basic tasks for 
Chinese Processing. The main difficulty is 
ambiguities widely exist in these two tasks. Our 
system is thus pay special attentions on various 
ambiguities resolution. After preprocessing we 
take Maximum Entropy (ME) model as the 
unified frame for WS and NER. ME is a 
effective model which often used to combine 
multiple sources of knowledge into various 
features. For finer-grain utilization of features, 
we use probabilistic features instead of binary 
features normally used. By exploring some often 
used features and some new features, our system 
performs well in this SIGHAN contest.  
In the rest sections of this paper, we give a 
brief introduction to our system sequentially. 
Section 2 describes the preprocessing in the 
system, including rough segmentation and 
factoid identification. Section 3 is on ambiguity 
resolution of WS. NER is introduced in Section 4. 
We give some experimental results in Section 5. 
Finally we draw some conclusions. 
2 Preprocessing 
The first step in preprocessing is to do a rough 
segmentation. By using both Forward Maximum 
Matching (FMM) and Backward Maximum 
Matching (BMM) approaches, we get an initial 
segmentation simultaneously detecting some of 
segmentation ambiguities in text. We use two 
different wordlists in this step. One is a basic 
wordlists with about 60 thousands words. We 
think this wordlist is relatively steady in Chinese. 
Another includes some words from special 
training corpus.  
We then cope with factoid recognition by 
using automata. Four automata are built to 
identify time, date, number and other (like 
telephone number and model of product) 
respectively. For covering some exceptional 
structures, we use some templates to post-
process some outputs from automata. 
Overlapping and combination ambiguities 
detected in preprocessing will be treated in next 
round of our system. It is the topic of next 
section. 
3 Disambiguation 
3.1 Overlapping ambiguity 
We only detect overlapping ambiguity with 
length of chain no more than 3 because these 
kinds of overlapping account for over 98% of all 
occurrences according to (Yan, 2000). The class-
based bigram model trained on tagged corpus of 
People?s Daily 2000 (about 12 million Chinese 
characters) is applied to resolve the ambiguities. 
In class-based bigram, all named entities, all 
punctuation and factoids is one class separately 
and each word is one class. For MSRA test we 
158
evaluate the performance of our overlapping 
disambiguation with precision of 84.1%. 
3.2 Combination ambiguity 
We use some templates to describe the POS 
properties of combination ambiguity and their 
segmentation words. In our system there are 155 
most frequent combination words. Due to the 
fact that instances of combination ambiguity is 
deficient in given training corpus, to enlarge 
training examples we convert the People Daily 
2000 to meet the standard of different guidelines 
then extract examples for training besides the 
given training corpora. For example, ??  is a 
combination ambiguity according to the 
guideline of MSRA whereas it is always one unit 
??in People Daily 2000. Noticing that when  
takes the sense of result, it is always tagged as a 
noun and a verb when it takes the meaning of 
fructification, we can easily enlarge the training 
examples of  ??.  
We then use ME model to combination 
ambiguity resolution. There are six features used 
in the model as below. 
(1) Contextual words; 
(2) Contextual characters; 
(3) Bigram collocations; 
(4) If the transfer probability of adjacent 
words to the target word exists; 
(5) If keywords indicate segmentation exists; 
(6) The most frequent segmentation from prior 
distribution 
 
4 Named entity recognition 
4.1 Personal name recognition 
We propose a probabilistic feature based 
maximum entropy approach to NER. Where, 
probabilistic feature functions are used instead of 
binary feature functions, it is one of the several 
differences between this model and the most of 
the previous ME based model. We also explore 
several new features in our model, which 
includes confidence functions, position of 
features etc. Like those in some previous works, 
we use sub-models to model Chinese Person 
Names, Foreign Names respectively, but we 
bring some new techniques in these sub-models. 
In standard ME, feature function is a binary 
function, for example, if we use CPN denotes the 
Chinese person Name, SN denotes Surname, a 
typical feature is: 
)1(),(
??
? ??=
otherwise
SNxandCPNy
yxfi ?
?????
 
But in Chinese, firstly, most of words used as 
surname are also used as normal words. The 
probabilities are different for them to be used as 
surname. Furthermore, a surname is not always 
followed by a given name, both cases are not 
binary. To model these phenomena, we give 
probability values to features, instead of binary 
values.  
For example, a feature function can be set 
value as follows: 
)2(
0
andCPNyif0.985 
),(
??
? ??=
otherwise
x
yxf
?
 
Or 
)3(
0
xCPNyif0.01805 
),(
??
? ??=
otherwise
and
yxf
?
 
Chinese characters used for translating foreign 
personal name are different from those in 
Chinese personal name. We built the foreign 
name model by collecting suffixes, prefixes, 
frequently-used characters, estimate their 
probabilities used in foreign personal name. 
These probabilities also used in model as 
probability features. 
We also design a confidence function for a 
character sequence nCCCW ...21=  to help model to 
estimate the probability of W as a person name. 
iC  may be a character or a word. Let Ff1 is 
probability of the C1, iMf is the probability of the 
iC , nEf  is the probability of the Cn. So the 
confidence function is 
)4(),(
12
1 nE
ni
iMF fffPERSONwK ++= ?
?<=<=
 
This function is included in ME frame as a 
feature. 
Candidate person name collection is the first 
step of NER. Since the ambiguity of Chinese 
word segmentation always exists. We propose 
some patterns for model different kind of 
segmentation ambiguity. Some labels are used to 
express specific roles of Chinese characters in 
person names. 
We have seven patterns as follows; first two 
patterns are non-ambiguity, while the others 
model some possible ambiguity in Chinese 
person name brought by word segmenter. 
(1) BCD: the Chinese personal name is 
composed of three Hanzi ((Chinese character). 
B: Surname of a Chinese personal name. 
159
C: Head character of 2-Hanzi given names. 
D: Tail character of 2-Hanzi of given names. 
(2) BD: the Chinese personal name is 
composed of two Hanzi (Chinese character). 
(3) BCH:  
H: the last given name and its next context are 
composed of a word. 
(4) UCD: 
U: the surname and its previous context are 
composed of a word.  
(5) BE: 
E: the first given name and the last given name 
are composed of a word.  
(6) UD: 
U: the surname and the first given name are 
composed of a word. 
(7) CD?The Chinese personal name is only 
composed of two given names. 
Based on the People?s Daily corpus and 
maximum entropy, we achieve models of 
Chinese personal name and transliterated 
personal name respectively.  
Here, How can we know whether a person 
name is composed of two or three Hanzi, we 
used another technology to limit boundary. We 
think out the co-appearing about the last given 
name and its next context, now, we have made a 
statistics about personal name and its next 
context to decide the length of the Chinese 
personal name. For example: 
????????????,  
In this sentence, we collect a candidate 
Chinese person name ???? ?, but the last 
given name ??? is a specific character, it has 
different meaning, now, we make a decision 
whether ??? is belong to personal name or not. 
????? 3)()( NRnumberNRnumber <  
So, ??? is not included in the personal name, 
???? is a correct choice. 
Another problem we have met is to recognize 
transliterated personal name, because many 
transliterated personal characters has included 
the Chinese surname, however, the condition that 
we can recognize the Chinese personal name is 
Chinese surname, therefore, a section of the 
transliterated personal name will often be 
recognized a Chinese personal name. 
In our system, we design a dynamitic priority 
method to check ambiguous character, when we 
examine a ambiguous character like ??? or ???, 
we will search different characters which maybe 
belong to Chinese personal name or transliterated 
personal name with forward and backward 
direction. According to the collection result, we 
will decide to use Chinese personal model or 
transliterated personal model to recognize 
personal name. 
For example: 
??/?/???/?/??/??/?/??/?/?/?/
?/?/?/?/?/?/?/?/??/??/??/??/?/
?/??/?/?/?/?/??/???/?? 
The correct candidate personal name is ???
?????? and not ?????. 
4.2 Location recognition 
We collect 196 keywords such like ??,?,?,?,
?,??, when the system search these keywords 
in a string, it will collect some characters or 
words which maybe belong to a location with 
backward direction, and the candidate location 
can be inputted into location model to recognize. 
The approach is similar to the personal name 
recognition, the difference is its contextual, the 
contextual used for location is 
2112 ++?? iiiii wwwww , which always can be 
used as feature during location entity recognition. 
We trained model based on the People?s Daily. 
We design some rules to help rectify wrong 
result, when a transliterated location name is lack 
of keyword like ???, it maybe recognized as a 
transliterated personal name. We collect some 
specific words list such as ???,?,??,??? 
to correct the wrong personal name. If the 
current word is in the list, the following words 
are accepted as candidate location entity. 
4.3 Organization recognition 
Organization name recognition is very different 
from other kinds of entities. An organization is 
often composed of several characters or words, 
and its length is dynamitic. According to 
statistical result about People?s Daily and MSR 
corpus, we decided the maximum length of an 
organization is 7 in a sentence.  
We computed the probability of every word or 
character of an organization, and defined the 
probability threshold. 
According to the different keyword, we 
designed sixteen classifiers; every classifier has 
its knowledge base, the different classifier can 
achieve organization recognition goal. 
We computed the probability threshold (>0.02) 
of a candidate organization.  
Combined the BIO-tagged method and the 
probability threshold, the organization can be 
recognized. 
160
4.4 Combination of Knowledge from 
Various Sources 
Human knowledge is very useful for NER. 
Knowledge from various sources can be 
incorporated into ME model, which are shown as 
follows. 
1. Chinese family name list (including 925 
items) and given names list (including 2453 
items):  
2. Transliterated character list (including 1398 
items). 
3. Location keyword list (including 607 items): 
If the word belongs to the list, 2~6 words before 
the salient word are accepted as candidate 
Location. 
4. Abbreviated location like ??/Beijing?, ??
/Tianjin? name list. Moreover, on Microsoft 
corpus, the word ??? of ?????? is also 
labeled as location ???/China?. 
5. Organization keyword list (including 875 
items): If the current word is in organization 
keyword list, 2~6 words before keywords are 
accepted as the candidate Organization. 
6. A location name dictionary. Some 
frequently used locations are included in the 
dictionary, like ???/United States? and ???
?/Singapore?. 
7. An organization name dictionary. Some 
frequently used organization names are included 
in the dictionary, like ????/State Council? 
and ????/United Nations?. 
8. Person name list: we collect some person 
names which come from the MSR train corpus. 
Moreover, the famous person name are included 
in the list such as ????,????. 
5 Evaluation result 
We evaluated our word segmenter and named 
entity recognizer on the SIGHAN Microsoft 
Research Asia (MSRA) corpus in open track. 
The Table 1 is the official result of word 
segmentation by our system. 
 
Corpus OOV- 
Rate 
OOV- 
Recall 
IV 
Recall-
rate 
F 
measure 
MSR 0.034 0.804 0.976 0.97 
UPUC 0.087 0.593 0.957 0.911 
 
Table 1 Official SIGHAN evaluation result for word 
segmentation in the open track 
 
Table 2 shows the official result of entity 
recognition. 
 
Type R P F 
Person 95.39% 96.71% 96.04% 
Location 87.77% 93.06% 90.34% 
Organization 87.68% 84.20% 85.90% 
 
Table2 Official SIGHAN evaluation result for entity 
recognition in the open track 
 
6 Conclusions 
A probabilistic feature based ME model is used 
to Chinese word segmentation and named entity 
recognition tasks. Our word segmenter achieved 
the medium result in the open word segmentation 
track of MSRA corpus, while entity recognition 
achieved the top one performance. 
Acknowledgement 
The research work is supported by China 
Ministry Of Education funded project (MZ115-
022): ?Tools for Chinese and Minority Language 
Processing? 
References 
A L Berger. 1996.  A Maximum Entropy Approach to 
Natural Language Processing.  Computational Linguistic, 
22 (1): 39- 71. 
Yan Yintang, Zhou XiaoQiang. 2000.12 Study of 
Segmentation Strategy on Ambiguous Phrases of 
Overlapping Type  Journal of The China Society For 
Scientific and Technical Information  Vol. 19 , ?6  
Liang NanYuan. 1987 A Written Chinese Segmentation 
system? CDWS.  Journal of Chinese Information Processing, 
Vol.2: 44-52 
ZHANG Hua-ping and Liu Qun. 2004 Automatic 
Recognition of Chinese Personal Name Based on Role 
Tagging. CHINESE JOURNAL OF COMPUTERS Vol (27) 
pp 85-91. 
Lv YaJuan, ZhaoTie-jun et al 2001. Leveled unknown 
Chinese Words resolution by dynamic programming. 
Journal Information Processing, 15(1): 28-33.  
Borthwick .A 1999.  Maximum Entropy Approach to Named 
Entity Recognition.  hD Dissertation.  
161
Combining Segmenter and Chunker for Chinese Word Segmentation
Masayuki Asahara, Chooi Ling Goh, Xiaojie Wang, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology, Japan
{masayu-a,ling-g,xiaoji-w,matsu}@is.aist-nara.ac.jp
Abstract
Our proposed method is to use a Hidden
Markov Model-based word segmenter and a
Support Vector Machine-based chunker for
Chinese word segmentation. Firstly, input sen-
tences are analyzed by the Hidden Markov
Model-based word segmenter. The word seg-
menter produces n-best word candidates to-
gether with some class information and confi-
dence measures. Secondly, the extracted words
are broken into character units and each char-
acter is annotated with the possible word class
and the position in the word, which are then
used as the features for the chunker. Finally, the
Support Vector Machine-based chunker brings
character units together into words so as to de-
termine the word boundaries.
1 Methods
We participate in the closed test for all four sets of data
in Chinese Word Segmentation Bakeoff. Our method is
based on the following two steps:
1. The input sentence is segmented into a word se-
quence by Hidden Markov Model-based word seg-
menter. The segmenter assigns a word class with
a confidence measure for each word at the hidden
states. The model is trained by Baum-Welch algo-
rithm.
2. Each character in the sentence is annotated with the
word class tag and the position in the word. The
n-best word candidates derived from the word seg-
menter are also extracted as the features. A sup-
port vector machine-based chunker corrects the er-
rors made by the segmenter using the extracted fea-
tures.
We will describe each of these steps in more details.
1.1 Hidden Markov Model-based Word Segmenter
Our word segmenter is based on Hidden Markov Model
(HMM). We first decide the number of hidden states
(classes) and assume that the each word can belong to
all the classes with some probability. The problem is de-
fined as a search for the sequence of word classes C =
c1, . . . , cn given a word sequence W = w1, . . . , wn. The
target is to find W and C for a given input S that maxi-
mizes the following probability:
argmax
W,C
P (W |C)P (C)
We assume that the word probability P (W |C) is con-
strained only by its word class, and that the class prob-
ability P (C) is constrained only by the class of the pre-
ceding word. These probabilities are estimated by the
Baum-Welch algorithm using the training material (See
(Manning and Schu?tze., 1999)). The learning process is
based on the Baum-Welch algorithm and is the same as
the well-known use of HMM for part-of-speech tagging
problem, except that the number of states are arbitrarily
determined and the initial probabilities are randomly as-
signed in our model.
1.2 Correction by Support Vector Machine-based
Chunker
While the HMM-based word segmenter achieves good
accuracy for known words, it cannot identify compound
words and out-of-vocabulary words. Therefore, we in-
troduce a Support Vector Machine(below SVM)-based
chunker (Kudo and Matsumoto, 2001) to cover the er-
rors made by the segmenter. The SVM-based chunker
re-assigns new word boundaries to the output of the seg-
menter.
An SVM (Vapnik, 1998) is a binary classifier. Sup-
pose we have a set of training data for a binary class
problem: (x1, y1), . . . , (xN , yN ), where xi ? Rn is a
feature vector of the i th sample in the training data and
yi ? {+1,?1} is the label of the sample. The goal is to
find a decision function which accurately predicts y for
an unseen x. An SVM classifier gives a decision function
f(x) for an input vector x where
f(x) = sign(
?
zi?SV
?iyiK(x, zi) + b).
f(x) = +1 means that x is a positive member, and
f(x) = ?1 means that x is a negative member. The vec-
tors zi are called support vectors, which receive a non-
zero weight ?i. Support vectors and the parameters are
determined by solving a quadratic programming prob-
lem. K(x, z) is a kernel function which maps vectors
into a higher dimensional space. We use a polynomial
kernel of degree 2 given by K(x, z) = (1 + x ? z)2.
The SVM classifier determines the position tag for
each character. We introduce the word class tag as the
feature, which is generated by the word segmenter. Since
we perform chunking by character units, the feature used
by the classifier will be the information for the character
unit.
The training data for our SVM-based chunker is con-
structed from the output of the HMM-based word seg-
menter defined in the previous section. In the current
setting, the HMM produces all the possible tags (class
labels) for each of the word within a predefined probabil-
ity bound. All the words in the output are then segmented
into characters, and each of the characters is tagged with
pairs of a word class and a position tag. For example,
in the paired tag ?0-B?, ?0? is a class label of the word
which the character belongs to and ?B? indicates the char-
acter?s position in the word. The number of classes is
determined in advance of the HMM learning. The po-
sition tag consists of the following four tags (S/B/E/I):
S means a single-character word; B is the first charac-
ter in a multi-character word; E is the last character in a
multi-character word; I is the intermediate character in a
multi-character word longer than 2 characters. As shown
in Figure 1, we set the HMM-based word segmenter to
produce the classes of n-best word candidates to take into
account multiple possibility of word boundaries.
The correct word boundary can be defined by assigning
either of two kinds of tags to each of the characters. Look
at the rightmost column of Figure 1 named as ?Chunker
Outputs.? The label ?B? in this column shows that the
character is the first character of a correct word, and ?I?
shows that the character is the other part of a word. This
means that the preceding positions of ?B? tags are the
word boundaries.
Those two tags correspond to the two classes of the
SVM chunker: In the training (and test) phrase, we use
window size of two characters to the left and right direc-
tion to learn (and estimate) the class for a character. For
example, the shadowed parts in Figure 1 are used as the
Figure 1: The Extracted Features for the Chunker
features to learn (or estimate) the word boundary tag ?I?
for the character ? ?.
2 Model Validation
To find out the best setting of learning, we would like to
determine ?the number of word classes? and ?the depth of
n-best word candidates? by using some sort of confidence
measure. We perform validation experiments for these
two types of parameters by using the training material
provided.
2.1 Validation Tests for HMM-based Word
Segmenter
The first validation experiment is to determine ?the num-
ber of word classes? of the HMM. 80% of the material is
used for the HMM training, and the other 20% is used as
the validation set. We test two settings for the number of
classes ? 5 & 10. The results are shown in Table 1.
Table 1: Validation Results for HMM
Data # of classes Rec. Prec. F
AS 5 0.845 0.768 0.804
AS 10 0.900 0.857 0.878
CTB 5 0.909 0.844 0.875
CTB 10 0.912 0.848 0.879
HK 5 0.867 0.742 0.799
HK 10 0.867 0.741 0.799
PK 5 0.942 0.902 0.921
PK 10 0.944 0.905 0.924
In most cases, models perform slightly better with the
increasing of the number of classes. When the corpus
size is large like the Academia Sinica data, this tendency
becomes more significant.
Whereas it is known that the Baum Welch algorithm is
very sensitive to the initialization of the classes, we ran-
domly assigned the initial classes without making much
effort. There are two reasons: (1) Since the word seg-
menter outputs are used as the clues to the chunker in our
method, we only need some consistent class annotations.
(2) The initial classes did not affect on the word segmen-
tation accuracy in our pilot experiments.
2.2 Validation Tests for SVM-based Chunker
The second validation test is for the chunking model to
determine both ?the number of word classes? and ?the
depth of the n-best candidates?. 80% of the material used
for the HMM training, another 10% is used for the chunk-
ing model training and the last 10% is used for the val-
idation test. The results are shown in Table 2, 3 and 4.
Since the training of this model is time- and resource-
consuming, the Academia Sinica data being very large
could not get enough time to finish the validation test.
Table 2: Validation Results (CTB) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.957 0.930 0.943
5 2 0.957 0.931 0.944
5 3 0.957 0.930 0.943
5 4 0.957 0.930 0.943
10 1 0.956 0.929 0.943
10 2 0.957 0.928 0.942
10 3 0.956 0.929 0.942
10 4 0.955 0.928 0.941
Table 3: Validation Results (HK) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.853 0.793 0.822
5 2 0.859 0.799 0.828
5 3 0.859 0.799 0.828
5 4 0.859 0.800 0.828
10 1 0.856 0.793 0.823
10 2 0.858 0.797 0.826
10 3 0.857 0.796 0.826
10 4 0.858 0.797 0.826
The results show that the chunker actually improves
the word segmentation accuracy compared with the out-
put of the HMM word segmenter for these three data sets.
The segmentation errors made by the word segmenter for
compound words and unknown words are corrected. The
Table 4: Validation Results (PK) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.960 0.934 0.947
5 2 0.961 0.935 0.948
5 3 0.962 0.936 0.949
5 4 0.962 0.935 0.948
10 1 0.961 0.932 0.946
10 2 0.962 0.935 0.948
10 3 0.961 0.934 0.947
10 4 0.961 0.934 0.947
improvement in Chinese Treebank (CTB) data set is sig-
nificant, because the data set contains many compound
words.
There is no significant difference in the results between
the different depths of n-best answers. Still, we choose
the best model for the test materials among them. If we
need to have a faster analyzer, we should employ only the
best answer of the word segmentation.
For the HMM, the larger number of classes tends to
get better accuracy than smaller ones. However, for the
chunking model, the result is the other way round. The
model with the smaller number of classes gets slightly
better accuracy. So, there should be trade-off between
smaller and larger number of classes.
3 Final Models for Test Material
For the final models, 80% of the training material is used
for HMM training and 100% of the material is used for
the chunking model training. The parameters, namely
?the number of word classes? and ?the depth of n-best
word candidates?, are determined by the validation tests
described in Section 2. While there is no significant dif-
ference between the depths of n-best answers, we choose
the best model among them for the testing. The parame-
ters are shown in Table 7.
We cannot create the model using all the original
Academia Sinica data because of its large size. Therefore,
we use 80% of the data for HMM training (5 classes) and
only 10% for chunking model training (with only the best
candidates).
Table 7: The Models for the Test Material
? with respect to F-Measure in Our Validation Test
Data # of classes n-best F
AS 5 1 N/A
CTB 5 2 0.943
HK 5 4 0.828
PK 5 3 0.948
Table 5: Throughput Speeds (characters per second)
Data Word Seg. (# of words) Fea. Ext. (n-best) Chunker (# of SV) Total Speed
AS 57000 (462750) 7640 (Only Best) 279 (96452) 241
CTB 54400 (77324) 4040 (to 2nd Best) 894 (16736) 671
HK 38900 (93231) 3870 (to 4th Best) 649 (14904) 524
PK 57400 (215865) 6209 (to 3rd Best) 254 (49736) 200
Table 6: Results for the Test Materials
Data T. Rec. T. Prec. F OOV Rec. IV Rec. Ranking
AS 0.944 0.945 0.945 0.574 0.952 3rd/6
CTB 0.852 0.807 0.829 0.412 0.949 8th/10
HK 0.940 0.908 0.924 0.415 0.980 5th/6
PK 0.933 0.916 0.924 0.357 0.975 2nd/4
4 Throughput Speeds
As described, our system is based on three modules:
HMM-based word segmenter, Feature extractor and
SVM-based chunker. The word segmenter is composed
by ChaSen (written in C/C++) (Matsumoto et. al., 2003)
which is adopted for GB/Big5 encoding. The feature
extractor is written in Perl. The SVM-based chunker is
composed by YamCha (written in C++) (Kudo and Mat-
sumoto, 2001).
Table 5 shows the speeds 1 of the three modules indi-
vidually and of the total system. ?# of words? means the
size of the word segmenter lexicon. Note that, if a word
belongs to more than one class, we regard them as differ-
ent words in our definition. ?# of SV? means the number
of support vectors in the chunker. The total system speed
depends highly on that of the chunker. It is known that
the speed of SVM classifiers depends on the number of
support vectors and the number of features.
5 Conclusion
We presented our method for Chinese Word Segmenta-
tion Bakeoff in 2nd SIGHAN Workshop. The results for
the test materials are shown in Table 6. The proposed
method is purely corpus-based statistical/machine learn-
ing method. Although we did not incorporate any heuris-
tic rules (e.g. part-of-speeches, functional words and
concatenation for numbers) into the model, the method
achieved considerable accuracy for the word segmenta-
tion task.
Acknowledgments
We thank Mr. Taku Kudo of NAIST for his development
of the SVM-based chunker YamCha.
1The throughput speeds are measured on a machine: In-
tel(R) Xeon(TM) CPU 2.80GHz ? 2, Memory 4GB, RedHat
Linux 9.
References
T. Kudo and Y. Matsumoto. 2001. Chunking with Sup-
port Vector Machines. In Proc. of NAACL 2001, pages
192?199.
C. D. Manning and H. Schu?tze. 1999. Foundation of
Statistical Natural Language Processing. Chapter 9.
Markov Models, pages 317?340.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano, K.
Takaoka and M. Asahara 2003. Morphological Ana-
lyzer ChaSen-2.3.0 Users Manual Tech. Report. Nara
Institute of Science and Technology, Japan.
L. A. Ramshaw and M. P. Marcus. 1995 Text chunking
using transformation-bases learning In Proc. of the 3rd
Workshop on Very Large Corpora, pages 83?94.
V. N. Vapnik. 1998. Statistical Learning Theory. A
Wiley-Interscience Publication.
