Proceedings of the 12th Conference of the European Chapter of the ACL, pages 505?513,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
User Simulations for context-sensitive speech recognition in Spoken
Dialogue Systems
Oliver Lemon
Edinburgh University
olemon@inf.ed.ac.uk
Ioannis Konstas
University of Glasgow
konstas@dcs.gla.ac.uk
Abstract
We use a machine learner trained on a
combination of acoustic and contextual
features to predict the accuracy of incom-
ing n-best automatic speech recognition
(ASR) hypotheses to a spoken dialogue
system (SDS). Our novel approach is to
use a simple statistical User Simulation
(US) for this task, which measures the
likelihood that the user would say each
hypothesis in the current context. Such
US models are now common in machine
learning approaches to SDS, are trained on
real dialogue data, and are related to the-
ories of ?alignment? in psycholinguistics.
We use a US to predict the user?s next dia-
logue move and thereby re-rank n-best hy-
potheses of a speech recognizer for a cor-
pus of 2564 user utterances. The method
achieved a significant relative reduction of
Word Error Rate (WER) of 5% (this is
44% of the possible WER improvement
on this data), and 62% of the possible se-
mantic improvement (Dialogue Move Ac-
curacy), compared to the baseline policy
of selecting the topmost ASR hypothesis.
The majority of the improvement is at-
tributable to the User Simulation feature,
as shown by Information Gain analysis.
1 Introduction
A crucial problem in the design of spoken dia-
logue systems (SDS) is to decide for incoming
recognition hypotheses whether a system should
accept (consider correctly recognized), reject (as-
sume misrecognition), or ignore (classify as noise
or speech not directed to the system) them.
Obviously, incorrect decisions at this point can
have serious negative effects on system usability
and user satisfaction. On the one hand, accept-
ing misrecognized hypotheses leads to misunder-
standings and unintended system behaviors which
are usually difficult to recover from. On the other
hand, users might get frustrated with a system that
behaves too cautiously and rejects or ignores too
many utterances. Thus an important feature in di-
alogue system engineering is the tradeoff between
avoiding task failure (due to misrecognitions) and
promoting overall dialogue efficiency, flow, and
naturalness.
In this paper, we investigate the use of machine
learning trained on a combination of acoustic fea-
tures and features computed from dialogue context
to predict the quality of incoming n-best recogni-
tion hypotheses to a SDS. These predictions are
then used to select a ?best? hypothesis and to de-
cide on appropriate system reactions. We evalu-
ate this approach in comparison with a baseline
system that works in the standard way: always
choosing the topmost hypothesis in the n-best list.
In such systems, complex repair strategies are re-
quired when the top hypothesis is incorrect.
The main novelty of this work is that we ex-
plore the use of predictions from simple statisti-
cal User Simulations to re-rank n-best lists of ASR
hypotheses. These User Simulations are now com-
monly used in statistical learning approaches to di-
alogue management (Williams and Young, 2003;
Schatzmann et al, 2006; Young, 2006; Young et
al., 2007; Schatzmann et al, 2007), but they have
not been used for context-sensitive ASR before.
In our model, the system?s ?belief? b(h) in a
recognition hypothesis h is factored in two parts:
the observation probability P (o|h) (approximated
by the ASR confidence score) and the User Simu-
lation probability P (h|us,C) of the hypothesis:
b(h) = P (o|h).P (h|us,C) (1)
where us is the state of the User Simulation in
context C. The context is simply a window of di-
505
alogue acts in the dialogue history, that the US is
sensitive to (see section 3).
The paper is organized as follows. After a short
relation to previous work, we describe the data
(Section 5) and derive baseline results (Section
6). Section 3 describes the User Simulations that
we use for re-ranking hypotheses. Section 7 de-
scribes our learning experiments for classifying
and selecting from n-best recognition hypotheses
and Section 9 reports our results.
2 Relation to Previous Work
In psycholinguistics, the idea that human dialogue
participants simulate each other to some extent is
gaining currency. (Pickering and Garrod, 2007)
write:
?if B overtly imitates A, then A?s com-
prehension of B?s utterance is facilitated
by A?s memory for A?s previous utter-
ance.?
We explore aspects of this idea in a computa-
tional manner. Similar work in the area of spoken
dialogue systems is described below.
(Litman et al, 2000) use acoustic-prosodic in-
formation extracted from speech waveforms, to-
gether with information derived from their speech
recognizer, to automatically predict misrecog-
nized turns in a corpus of train-timetable informa-
tion dialogues. In our experiments, we also use
recognizer confidence scores and a limited num-
ber of acoustic-prosodic features (e.g. amplitude
in the speech signal) for hypothesis classification,
but we also use User Simulation predictions.
(Walker et al, 2000) use a combination of fea-
tures from the speech recognizer, natural language
understanding, and dialogue manager/discourse
history to classify hypotheses as correct, partially
correct, or misrecognized. Our work is related to
these experiments in that we also combine con-
fidence scores and higher-level features for clas-
sification. However, both (Litman et al, 2000)
and (Walker et al, 2000) consider only single-best
recognition results and thus use their classifiers as
?filters? to decide whether the best recognition hy-
pothesis for a user utterance is correct or not. We
go a step further in that we classify n-best hypothe-
ses and then select among the alternatives. We also
explore the use of more dialogue and task-oriented
features (e.g. the dialogue move type of a recogni-
tion hypothesis) for classification.
(Gabsdil and Lemon, 2004) similarly perform
reordering of n-best lists by combining acoustic
and pragmatic features. Their study shows that di-
alogue features such as the previous system ques-
tion and whether a hypothesis is the correct answer
to a particular question contributed more to classi-
fication accuracy than the other attributes.
(Jonson, 2006) classifies recognition hypothe-
ses with labels denoting acceptance, clarification,
confirmation and rejection. These labels were
learned in a similar way to (Gabsdil and Lemon,
2004) and correspond to varying levels of con-
fidence, being essentially potential directives to
the dialogue manager. Apart from standard fea-
tures Jonson includes attributes that account for
the whole n-best list, i.e. standard deviation of
confidence scores.
As well as the use of a User Simulation, the
main difference between our approach and work
on hypothesis reordering (e.g. (Chotimongkol and
Rudnicky, 2001)) is that we make a decision re-
garding whether a dialogue system should accept,
clarify, reject, or ignore a user utterance. Like
(Gabsdil and Lemon, 2004; Jonson, 2006), our
approach is more generally applicable than pre-
ceding research, since we frame our methodology
in the Information State Update (ISU) approach
to dialogue management (Traum et al, 1999) and
therefore expect it to be applicable to a range of
related multimodal dialogue systems.
3 User Simulations
What makes this study different from the previous
work in the area of post-processing of the ASR hy-
potheses is the incorporation of a User Simulation
output as an additional feature. The history of a di-
alogue between a user and a dialogue system plays
an important role as to what the user might be ex-
pected to say next. As a result, most of the stud-
ies mentioned in the previous section make vari-
ous efforts to capture history by including relevant
features directly in their classifiers.
Various statistical User Simulations have been
trained on corpora of dialogue data in order to
simulate real user behaviour (Schatzmann et al,
2006; Young, 2006; Georgila et al, 2006; Young
et al, 2007; Schatzmann et al, 2007). We devel-
oped a simple n-gram User Simulation, using n-
grams of dialogue moves. It treats a dialogue as
a sequence of lists of consecutive user and system
turns in a high level semantic representation, i.e.
506
< SpeechAct >,< Task > pairs, for example
< provide info >,< music genre(punk) >.
It takes as input the n ? 1 most recent lists of
< SpeechAct >,< Task > pairs in the dialogue
history, and uses the statistics in the training set
to compute a distribution over the possible next
user actions. If no n-grams match the current his-
tory, the model can back-off to n-grams of lower
order. We use this model to assess the likelihood
of each candidate ASR hypothesis. Intuitively, this
is the likelihood that the user really would say the
hypothesis in the current dialogue situation. The
benefit of using n-gram models is that they are fast
and simple to train even on large corpora.
The main hypothesis that we investigate is that
by using the User Simulation model to predict the
next user utterance, we can effectively increase the
performance of the speech recogniser module.
4 Evaluation metrics
To evaluate performance we use Dialogue Move
Accuracy (DMA), a strict variant of Concept Er-
ror Rate (CER) as defined by (Boros et al, 1996),
which takes into account the semantic aspects of
the difference between the classified utterance and
the true transcription. CER is similar to WER,
since it takes into account deletions, insertions
and substitutions on the semantic (rather than the
word) level of the utterance. DMA is stricter than
CER in the sense that it does not allow for par-
tial matches in the semantic representation. In
other words, if the classified utterance corresponds
to the same semantic representation as the tran-
scribed then we have 100% DMA, otherwise 0%.
Sentence Accuracy (SA) is the alignment of a
single hypothesis in the n-best list with the true
transcription. Similarly to DMA, it accounts for
perfect alignment between the hypothesis and the
transcription, i.e. if they match perfectly we have
100% SA, otherwise 0%.
5 Data Collection
For our experiments, we use data collected in a
user study with the Town-Info spoken dialogue
system, using the HTK speech recognizer (Young,
2007). In this study 18 subjects had to solve 10
search/browsing tasks with the system, resulting in
180 complete dialogues and 2564 utterances (av-
erage 14.24 user utterances per dialogue).
For each utterance we have a series of files of
60-best lists produced by the speech recogniser,
namely the transcription hypotheses on a sentence
level along with the acoustic model score and the
equivalent transcriptions on a word level, with in-
formation such as the duration of each recognised
frame and the confidence score of the acoustic and
language model of each word.
5.1 Labeling
We transcribed all user utterances and parsed the
transcriptions offline using a natural language un-
derstanding component (a robust Keyword Parser)
in order to get a gold-standard labeling of the data.
We devised four labels with decreasing order of
confidence: ?opt? (optimal), ?pos? (positive), ?neg?
(negative), ?ign? (ignore). These are automatically
generated using two different modules: a key-
word parser that computes the < SpeechAct ><
Task > pair as described in the previous sec-
tion and a Levenshtein Distance calculator, for the
computation of the DMA and WER of each hy-
pothesis respectively. The reason for opting for a
more abstract level, namely the semantics of the
hypotheses rather than individual word recogni-
tion, is that in SDS it is usually sufficient to rely
on the meaning of message that is being conveyed
by the user rather than the precise words that they
used.
Similar to (Gabsdil and Lemon, 2004; Jonson,
2006) we ascribe to each utterance either of the
?opt?, ?pos?, ?neg?, ?ign? labels according to the
following schema:
? opt: The hypothesis is perfectly aligned and
semantically identical to the transcription
? pos: The hypothesis is not entirely aligned
(WER < 50) but is semantically identical to
the transcription
? neg: The hypothesis is semantically identical
to the transcription but does not align well
(WER > 50) or is semantically different to
the transcription
? ign: The hypothesis was not addressed to
the system (crosstalk), or the user laughed,
coughed, etc.
The 50% value for the WER as a threshold for
the distinction between the ?pos? and ?neg? cate-
gory is adopted from (Gabsdil, 2003), based on
the fact that WER is affected by concept accuracy
(Boros et al, 1996). In other words, if a hypothe-
sis is erroneous as far as its transcript is concerned
507
Transcript: I?d like to find a bar please
I WOULD LIKE TO FIND A BAR PLEASE pos
I LIKE TO FIND A FOUR PLEASE neg
I?D LIKE TO FIND A BAR PLEASE opt
WOULD LIKE TO FIND THE OR PLEASE ign
Table 1: Example hypothesis labelling
then it is highly likely that it does not convey the
correct message from a semantic point of view.
We always label conceptually equivalent hypothe-
ses to a particular transcription as potential candi-
date dialogue strategy moves, and total misrecog-
nitions as rejections. In table 5.1 we show exam-
ples of the four labels. Note that in the case of
silence, we give an ?opt? to the empty hypothesis.
6 The Baseline and Oracle Systems
The baseline for our experiments is the behavior
of the Town-Info spoken dialogue system that was
used to collect the experimental data. We evaluate
the performance of the baseline system by analyz-
ing the dialogue logs from the user study.
As an oracle for the system we defined the
choice of either the first ?opt? in the n-best list,
or if this does not exist the first ?pos? in the list.
In this way it is guaranteed that we always get as
output a perfect match to the true transcript as far
as its Dialogue Move is concerned, provided there
exists a perfect match somewhere in the list.
6.1 Baseline and Oracle Results
Table 2 summarizes the evaluation of the baseline
and oracle systems. We note that the Baseline sys-
tem already performs quite well on this data, when
we consider that in about 20% of n-best lists there
is no semantically correct hypothesis.
Baseline Oracle
WER 47.72% 42.16%
DMA 75.05% 80.20%
SA 40.48% 45.27%
Table 2: Baseline and Oracle results (statistically
significant at p < 0.001)
7 Classifying and Selecting N-best
Recognition Hypotheses
We use a threshold (50%) on a hypothesis? WER
as an indicator for whether hypotheses should be
clarified or rejected. This is adopted from (Gabs-
dil, 2003), based on the fact that WER correlates
with concept accuracy (CA, (Boros et al, 1996)).
7.1 Classification: Feature Groups
We represent recognition hypotheses as 13-
dimensional feature vectors for automatic classi-
fication. The feature vectors combine recognizer
confidence scores, low-level acoustic information,
and information from the User Simulation.
All the features used by the system are extracted
by the dialogue logs, the n-best lists per utterance
and per word and the audio files. The majority
of the features chosen are based on their success
in previous systems as described in the literature
(see section 2). The novel feature here is the User
Simulation score which may make redundant most
of the dialogue features used in other studies.
In order to measure the usefulness of each can-
didate feature and thus choose the most important
we use the metrics of Information Gain and Gain
Ratio (see table 3 in section 8.1) on the whole
training set, i.e. 93240 hypotheses.
In total 13 attributes were extracted, that can be
grouped into 4 main categories; those that concern
the current hypothesis to be classified, those that
concern low-level statistics of the audio files, those
that concern the whole n-best list, and finally the
User Simulation feature.
? Current Hypothesis Features (CHF) (6):
acoustic score, overall model confidence
score, minimum word confidence score,
grammar parsability, hypothesis length and
hypothesis duration.
? Acoustic Features (AF) (3): minimum, max-
imum and RMS amplitude
? List Features (LF) (3): n-best rank, deviation
of confidence scores in the list, match with
most frequent Dialogue Move
? User Simulation (US) (1): User Simulation
confidence score
The Current Hypothesis features (CHF) were
extracted from the n-best list files that contained
the hypotheses? transcription along with overall
acoustic score per utterance and from the equiv-
alent files that contained the transcription of each
word along with the start of frame, end of frame
and confidence score:
508
Acoustic score is the negative log likelihood as-
cribed by the speech recogniser to the whole hy-
pothesis, being the sum of the individual word
acoustic scores. Intuitively this is considered to
be helpful since it depicts the confidence of the
statistical model only for each word and is also
adopted in previous studies. Incorrect alignments
shall tend to adapt less well to the model and thus
have low log likelihood.
Overall model confidence score is the average
of the individual word confidence scores.
Minimum word confidence score is also com-
puted by the individual word transcriptions and ac-
counts for the confidence score of the word which
the speech recogniser is least certain of. It is ex-
pected to help our classifier distinguish between
poor overall hypothesis recognitions since a high
overall confidence score can sometimes be mis-
leading.
Grammar Parsability is the negative log
likelihood of the transcript for the current hy-
pothesis as produced by the Stanford Parser, a
wide-coverage Probabilistic Context-Free Gram-
mar (PCFG) (Klein and Manning, 2003) 1. This
feature seems helpful since we expect that a highly
ungrammatical hypothesis is likely not to match
with the true transcription semantically.
Hypothesis duration is the length of the hy-
pothesis in milliseconds as extracted from the n-
best list files with transcriptions per word that in-
clude the start and the end time of the recognised
frame. The reason for the inclusion of this fea-
ture is that it can help distinguish between short
utterances such as yes/no answers, medium-sized
utterances of normal answers and long utterances
caused by crosstalk.
Hypothesis length is the number of words in a
hypothesis and is considered to help in a similar
way as the above feature.
The Acoustic Features (AF) were extracted di-
rectly from the wave files using SoX: Minimum,
maximum and RMS amplitude are straightforward
features common in the previous studies men-
tioned in section 2.
The List Features (LF) were calculated based
on the n-best list files with transcriptions per utter-
ance and per word and take into account the whole
list:
N-best rank is the position of the hypothesis in
the list and could be useful in the sense that ?opt?
1http://nlp.stanford.edu/software/lex-parser.shtml
and ?pos? may be found in the upper part of the list
rather than the bottom.
Deviation of confidence scores in the list is
the deviation of the overall model confidence score
of the hypothesis from the mean confidence score
in the list. This feature is extracted in the hope
that it will indicate potential clusters of confidence
scores in particular positions in the list, i.e. group
hypotheses that deviate in a specific fashion from
the mean and thus indicating them being classified
with the same label.
Match with most frequent Dialogue Move is
the only boolean feature and indicates whether the
Dialogue Move of the current hypothesis, i.e. the
pair of < SpeechAct >< Task > coincides with
the most frequent one. The trend in n-best lists
is to have a majority of utterances that belong to
one or two labels and only one hypothesis belong-
ing to the ?opt? category and/or a few to the ?pos?
category. As a result, the idea behind this feature
is to extract such potential outliers which are the
desired goal for the re-ranker.
Finally, the User Simulation score is given as
an output from the User Simulation model and
adapted for the purposes of this study (see section
3 for more details). The model is operating with 5-
grams. Its input is given by two different sources:
the history of the dialogue, namely the 4 previous
Dialogue Moves, is taken from the dialogue log
and the current hypothesis? semantic parse which
is generated on the fly by the same keyword parser
used in the automatic labelling.
User Simulation score is the probability that
the current hypothesis? Dialogue Move has really
been said by the user given the 4 previous Dia-
logue Moves. The potential advantages of this fea-
ture have been discussed in section 3.
7.2 Learner and Selection Procedure
We use the memory based learner TiMBL (Daele-
mans et al, 2002) to predict the class of each of
the 60-best recognition hypotheses for a given ut-
terance.
TiMBL was trained using different parameter
combinations mainly choosing between number of
k-nearest neighbours (1 to 5) and distance metrics
(Weighted Overlap andModified Value Difference
Metric). In a second step, we decide which (if any)
of the classified hypotheses we actually want to
pick as the best result and how the user utterance
should be classified as a whole.
509
1. Scan the list of classified n-best recognition
hypotheses top-down. Return the first result
that is classified as ?opt?.
2. If 1. fails, scan the list of classified n-best
recognition hypotheses top-down. Return the
first result that is classified as ?pos?.
3. If 2. fails, count the number of negs and igns
in the classified recognition hypotheses. If
the number of negs is larger or equal than the
number of igns then return the first ?neg?.
4. Else return the first ?ign? utterance.
8 Experiments
Experiments were conducted in two layers: the
first layer concerns only the classifier, i.e. the abil-
ity of the system to correctly classify each hypoth-
esis to either of the four labels ?opt?, ?pos?, ?neg?,
?ign? and the second layer the re-ranker, i.e. the
ability of the system to boost the speech recog-
niser?s accuracy.
All results are drawn from the TiMBL classi-
fier trained with the Weighted Overlap metric and
k = 1 nearest neighbours settings. Both layers
are trained on 75% of the same Town-Info Corpus
of 126 dialogues containing 60-best lists for 1554
user utterances or a total of 93240 hypotheses. The
first layer was tested against a separate Town-Info
Corpus of 58 dialogues containing 510 user utter-
ances or a total of 30600 hypotheses, while the
second was tested on the whole training set with
10-fold cross-validation.
Using this corpus, a series of experiments was
carried out using different sets of features in order
to both determine and illustrate the increasing per-
formance of the classifier. These sets were deter-
mined not only by the literature but also by the In-
formation Gain measures that were calculated on
the training set using WEKA, as shown in table 3.
8.1 Information Gain
Quite surprisingly, we note that the rank given by
the Information Gain measure coincides perfectly
with the logical grouping of the attributes that was
initially performed (see table 3).
As a result, we chose to use this grouping for
the final 4 feature sets on which the classifier
experiments were performed, in the following
order:
Experiment 1: List Features (LF)
InfoGain Attribute
1.0324 userSimulationScore
0.9038 rmsAmp
0.8280 minAmp
0.8087 maxAmp
0.4861 parsability
0.3975 acousScore
0.3773 hypothesisDuration
0.2545 hypothesisLength
0.1627 avgConfScore
0.1085 minWordConfidence
0.0511 nBestRank
0.0447 standardDeviation
0.0408 matchesFrequentDM
Table 3: Information Gain
Experiment 2: List Features + Current Hypothe-
sis Features (LF+CHF)
Experiment 3: List Features + Current Hypothe-
sis Features + Acoustic Features (LF+CHF+AF)
Experiment 4: List Features + Current Hy-
pothesis Features + Acoustic Features + User
Simulation (LF+CHF+AF+US)
Note that the User Simulation score is a very
strong feature, scoring first in the Information
Gain rank, validating our central hypothesis.
The testing of the classifier using each of the
above feature sets was performed on the remain-
ing 25% of the Town-Info corpus comprising of 58
dialogues, consisting of 510 utterances and taking
the 60-best lists resulting in a total of 30600 vec-
tors. In each experiment we measured Precision,
Recall, F-measure per class and total Accuracy of
the classifier .
For the second layer, we used a trained instance
of the TiMBL classifier on the 4th feature set (List
Features + Current Hypothesis Features + Acous-
tic Features + User Simulation) and performed re-
ranking using the algorithm presented in section
7.2 on the same training set used in the first layer
using 10-fold cross validation.
9 Results and Evaluation
We performed two series of experiments in two
layers: the first corresponds to the training of the
classifier alone and the second to the system as a
whole measuring the re-ranker?s output.
510
Feature set (opt) Precision Recall F1
LF 42.5% 58.4% 49.2%
LF+CHF 62.4% 65.7% 64.0%
LF+CHF+AF 55.6% 61.6% 58.4%
LF+CHF+AF+US 70.5% 73.7% 72.1%
Table 4: Results for the ?opt? category
Feature set (pos) Precision Recall F1
LF 25.2% 1.7% 3.2%
LF+CHF 51.2% 57.4% 54.1%
LF+CHF+AF 51.5% 54.6% 53.0%
LF+CHF+AF+US 64.8% 61.8% 63.3%
Table 5: Results for the ?pos? category
9.1 First Layer: Classifier Experiments
In these series of experiments we measure preci-
sion, recall and F1-measure for each of the four
labels and overall F1-measure and accuracy of the
classifier. In order to have a better view of the
classifier?s performance we have also included the
confusion matrix for the final experiment with all
13 attributes. Tables 4 -7 show per class and per
attribute set measures, while Table 8 shows a col-
lective view of the results for the four sets of at-
tributes and the baseline being the majority class
label ?neg?. Table 9 shows the confusion matrix
for the final experiment.
In tables 4 - 8 we generally notice an increase
in precision, recall and F1-measure as we pro-
gressively add more attributes to the system with
the exception of the addition of the Acoustic Fea-
tures which seem to impair the classifier?s perfor-
mance. We also make note of the fact that in the
case of the 4th attribute set the classifier can dis-
tinguish very well the ?neg? and ?ign? categories
with 86.3% and 99.9% F1-measure respectively.
Most importantly, we observe a remarkable boost
in F1-measure and accuracy with the addition of
the User Simulation score. We find a 37.36% rel-
ative increase in F1-measure and 34.02% increase
Feature set (neg) Precision Recall F1
LF 54.2% 96.4% 69.4%
LF+CHF 70.7% 75.0% 72.8%
LF+CHF+AF 69.5% 73.4% 71.4%
LF+CHF+AF+US 85.6% 87.0% 86.3%
Table 6: Results for the ?neg? category
Feature set (ign) Precision Recall F1
LF 19.6% 1.3% 2.5%
LF+CHF 63.5% 48.7% 55.2%
LF+CHF+AF 59.3% 48.9% 53.6%
LF+CHF+AF+US 99.9% 99.9% 99.9%
Table 7: Results for the ?ign? category
Feature set F1 Accuracy
Baseline - 51.1%
LF 37.3% 53.1%
LF+CHF 64.1% 64.8%
LF+CHF+AF 62.6% 63.4%
LF+CHF+AF+US 86.0% 84.9%
Table 8: F1-Measure and Accuracy for the four
attribute sets
in the accuracy compared to the 3rd experiment,
which contains all but the User Simulation score
attribute and a 66.20% relative increase of the ac-
curacy compared to the Baseline. In table 7 we
make note of a rather low recall measure for the
?ign? category in the case of the LF experiment,
suggesting that the list features do not add extra
value to the classifier, partially validating the In-
formation Gain measure (Table 3).
Taking a closer look at the 4th experiment with
all 13 features we notice in table 9 that most er-
rors occur between the ?pos? and ?neg? category.
In fact, for the ?neg? category the False Positive
Rate (FPR) is 18.17% and for the ?pos? 8.9%, all
in all a lot larger than for the other categories.
9.2 Second Layer: Re-ranker Experiments
In these experiments we measure WER, DMA
and SA for the system as a whole. In order to
make sure that the improvement noted was re-
ally attributed to the classifier we computed the
p-values for each of these measures using the
Wilcoxon signed rank test for WER andMcNemar
chi-square test for the DMA and SA measures.
In table 10 we note that the classifier scores
opt pos neg ign
opt 232 37 46 0
pos 47 4405 2682 8
neg 45 2045 13498 0
ign 5 0 0 7550
Table 9: Confusion Matrix for LF+CHF+AF+US
511
Baseline Classifier Oracle
WER 47.72% 45.27% ** 42.16%***
DMA 75.05% 78.22% * 80.20% ***
SA 40.48% 42.26% 45.27%***
Table 10: Baseline, Classifier, and Oracle results
(*** = p < 0.001, ** = p < 0.01, * = p < 0.05)
Label Precision Recall F1
opt 74.0% 64.1% 68.7%
pos 76.3% 46.2% 57.6%
neg 81.9% 94.4% 87.7%
ign 99.9% 99.9% 99.9%
Table 11: Precision, Recall and F1: high-level fea-
tures
45.27% WER making a notable relative reduction
of 5.13% compared to the baseline and 78.22%
DMA incurring a relative improvement of 4.22%.
The classifier scored 42.26% on SA but it was
not considered significant compared to the base-
line (0.05 < p < 0.10). Comparing the classifier?s
performance with the Oracle it achieves a 44.06%
of the possible WER improvement on this data,
61.55% for the DMA measure and 37.16% for the
SA measure.
Finally, we also notice that the Oracle has a
80.20% for the DMA, which means that 19.80%
of the n-best lists did not include at all a hypothe-
sis that matched semantically to the true transcript.
10 Experiment with high-level features
We trained a Memory Based Classifier based only
on the higher level features of merely the User
Simulation score and the Grammar Parsability
(US + GP). The idea behind this choice is to try
and find a combination of features that ignores low
level characteristics of the user?s utterances as well
as features that heavily rely on the speech recog-
niser and thus by default are not considered to be
very trustworthy.
Quite surprisingly, the results taken from an ex-
periment with just the User Simulation score and
the Grammar Parsability are very promising and
comparable with those acquired from the 4th ex-
periment with all 13 attributes. Table 11 shows
the precision, recall and F1-measure per label and
table 12 illustrates the classifier?s performance in
comparison with the 4th experiment.
Table 12 shows that there is a somewhat consid-
Feature set F1 Accuracy Ties
LF+CHF+AF+US 86.0% 84.9% 4993
US+GP 85.7% 85.6% 115
Table 12: F1, Accuracy and number of ties cor-
rectly resolved for LF+CHF+AF+US and US+GP
feature sets
erable decrease in the recall and a corresponding
increase in the precision of the ?pos? and ?opt? cat-
egories compared to the LF + CHF + AF + US at-
tribute set, which account for lower F1-measures.
However, all in all the US + GP set manages to
classify correctly 207 more vectors and quite in-
terestingly commits far fewer ties and manages to
resolve more compared to the full 13 attribute set.
11 Conclusion
We used a combination of acoustic features and
features computed from dialogue context to pre-
dict the quality of incoming recognition hypothe-
ses to an SDS. In particular we use a score com-
puted from a simple statistical User Simulation,
which measures the likelihood that the user re-
ally said each hypothesis. The approach is novel
in combining User Simulations, machine learning,
and n-best processing for spoken dialogue sys-
tems. We employed a User Simulation model,
trained on real dialogue data, to predict the user?s
next dialogue move. This prediction was used to
re-rank n-best hypotheses of a speech recognizer
for a corpus of 2564 user utterances. The results,
obtained using TiMBL and an n-gram User Sim-
ulation, show a significant relative reduction of
Word Error Rate of 5% (this is 44% of the pos-
sible WER improvement on this data), and 62%
of the possible Dialogue Move Accuracy improve-
ment, compared to the baseline policy of selecting
the topmost ASR hypothesis. The majority of the
improvement is attributable to the User Simulation
feature. Clearly, this improvement would result in
better dialogue system performance overall.
Acknowledgments
We thank Helen Hastie and Kallirroi Georgila.
The research leading to these results has re-
ceived funding from the EPSRC (project no.
EP/E019501/1) and from the European Commu-
nity?s Seventh Framework Programme (FP7/2007-
2013) under grant agreement no. 216594 (CLAS-
SiC project www.classic-project.org)
512
References
M. Boros, W. Eckert, F. Gallwitz, G. Go?rz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In Proceedings ICSLP ?96, volume 2,
pages 1009?1012, Philadelphia, PA.
Ananlada Chotimongkol and Alexander I. Rudnicky.
2001. N-best Speech Hypotheses Reordering Using
Linear Regression. In Proceedings of EuroSpeech
2001, pages 1829?1832.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2002. TIMBL: Tilburg Mem-
ory Based Learner, version 4.2, Reference Guide. In
ILK Technical Report 02-01.
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recogni-
tion performance in spoken dialogue systems. In
Proceedings of ACL-04, pages 344?351.
Malte Gabsdil. 2003. Classifying Recognition Results
for Spoken Dialogue Systems. In Proceedings of the
Student Research Workshop at ACL-03.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2006. User simulation for spoken dialogue
systems: Learning and evaluation. In Proceedings
of Interspeech/ICSLP, pages 1065?1068.
R. Jonson. 2006. Dialogue Context-Based Re-ranking
of ASR Hypotheses. In Proceedings IEEE 2006
Workshop on Spoken Language Technology.
D. Klein and C. Manning. 2003. Fast exact inference
with a factored model for natural language parsing.
Journal of Advances in Neural Information Process-
ing Systems, 15(2).
Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2000. Predicting Automatic Speech Recognition
Performance Using Prosodic Cues. In Proceedings
of NAACL.
M. Pickering and S. Garrod. 2007. Do people use lan-
guage production to make predictions during com-
prehension? Journal of Trends in Cognitive Sci-
ences, 11(3).
J Schatzmann, K Weilhammer, M N Stuttle, and S J
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. Knowledge En-
gineering Review, 21:97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proceedings of HLT/NAACL.
David Traum, Johan Bos, Robin Cooper, Staffan Lars-
son, Ian Lewin, Colin Matheson, and Massimo Poe-
sio. 1999. A Model of Dialogue Moves and In-
formation State Revision. Technical Report D2.1,
Trindi Project.
Marilyn Walker, Jerry Wright, and Irene Langkilde.
2000. Using Natural Language Processing and Dis-
course Features to Identify Understanding Errors
in a Spoken Dialogue System. In Proceedings of
ICML-2000.
Jason Williams and Steve Young. 2003. Using wizard-
of-oz simulations to bootstrap reinforcement-
learning-based dialog management systems. In
Proc. 4th SIGdial workshop.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
SJ Young. 2006. Using POMDPs for Dialog Manage-
ment. In IEEE/ACL Workshop on Spoken Language
Technology (SLT 2006), Aruba.
Steve Young. 2007. ATK: An Application Toolkit
for HTK, Version 1.6. Technical report, Cambridge
University Engineering Department.
513
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 989?999,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatically Detecting and Attributing Indirect Quotations
Silvia Pareti? Tim O?Keefe?? Ioannis Konstas James R. Curran? Irena Koprinska?
ILCC, School of Informatics ? e-lab, School of IT
University of Edinburgh University of Sydney
United Kingdom NSW 2006, Australia
{s.pareti,i.konstas}@sms.ed.ac.uk {tokeefe,james,irena}@it.usyd.edu.au
Abstract
Direct quotations are used for opinion min-
ing and information extraction as they have an
easy to extract span and they can be attributed
to a speaker with high accuracy. However,
simply focusing on direct quotations ignores
around half of all reported speech, which is
in the form of indirect or mixed speech. This
work presents the first large-scale experiments
in indirect and mixed quotation extraction and
attribution. We propose two methods of ex-
tracting all quote types from news articles and
evaluate them on two large annotated corpora,
one of which is a contribution of this work.
We further show that direct quotation attribu-
tion methods can be successfully applied to in-
direct and mixed quotation attribution.
1 Introduction
Quotations are crucial carriers of information, par-
ticularly in news texts, with up to 90% of sentences
in some articles being reported speech (Bergler
et al, 2004). Reported speech is a carrier of evi-
dence and factuality (Bergler, 1992; Saur?? and Puste-
jovsky, 2009), and as such, text mining applications
use quotations to summarise, organise and validate
information. Extraction of quotations is also rele-
vant to researchers interested in media monitoring.
Most quotation attribution studies (Pouliquen
et al, 2007; Glass and Bangay, 2007; Elson and
McKeown, 2010) thus far have limited their scope
to direct quotations (Ex.1a), as they are delimited
?*These authors contributed equally to this work.
by quotation marks, which makes them easy to ex-
tract. However, annotated resources suggest that di-
rect quotations represent only a limited portion of all
quotations, i.e., around 30% in the Penn Attribution
Relation Corpus (PARC), which covers Wall Street
Journal articles, and 52% in the Sydney Morning
Herald Corpus (SMHC), with the remainder being in-
direct (Ex.1c) or mixed (Ex.1b) quotations. Retriev-
ing only direct quotations can miss key content that
can change the interpretation of the quotation (Ex.
1b) and will entirely miss indirect quotations.
(1) a. ?For 10 million, you can move $100 mil-
lion of stocks,? a specialist on the Big Board
gripes. ?That gives futures traders a lot
more power.?
b. Police would only apply for the restrictions
when ?we have a lot of evidence that late-
night noise. . . is disturbing the residents of
that neighbourhood?, Superintendent Tony
Cooke said.
c. Mr Walsh said Rio was continuing to hold
discussions with its customers to arrive at a
mutually agreed price.
Previous work on extracting indirect and mixed
quotations has suffered from a lack of large-scale
data, and has instead used hand-crafted lexica of re-
porting verbs with rule-based approaches. The lack
of data has also made comparing the relative merit
of these approaches difficult, as existing evaluations
are small-scale and do not compare multiple meth-
ods on the same data.
In this work we address this lack of clear, com-
parable results by evaluating two baseline meth-
989
Method Language Test Size Results
(quotations) P R
Krestel et al (2008) hand-built grammar English 133 74% 99%
Sarmento and Nunes (2009) patterns over text Portuguese 570 88% 5%1
Fernandes et al (2011) ML and regex Portuguese 205 64%2 67%2
de La Clergerie et al (2011) patterns over parse French 40 87% 70%
Schneider et al (2010) hand-built grammar English N/D 56%2 52%2
Table 1: Related work on direct, indirect and mixed quotation extraction. Note that they are not directly comparable
as they apply to different languages and greatly differ in evaluation style and size of test set. 1 Figure estimated by the
authors for extracting 570 quotations from 26k articles. 2 Results are for quotation extraction and attribution jointly.
ods against both a token-based approach that uses a
Conditional Random Field (CRF) to predict IOB la-
bels, and a maximum entropy classifier that predicts
whether parse nodes are quotations or not. We eval-
uate these approaches on two large-scale corpora
from the news domain that together include over
18,000 quotations. One of these corpora (SMHC) is a
a contribution of this work, while our results are the
first presented on the other corpus (PARC). Instead
of relying on a lexicon of reporting verbs, we de-
velop a classifier to detect verbs introducing a quo-
tation. To inform future research we present results
for direct, indirect, and mixed quotations, as well as
overall results.
Finally, we use the direct quotation attribution
methods described in O?Keefe et al (2012) and
show that they can be successfully applied to indi-
rect and mixed quotations, albeit with lower accu-
racy. This leads us to conclude that attributing indi-
rect and mixed quotations to speakers is harder than
attributing direct quotations.
With this work, we set a new state of the art in
quotation extraction. We expect that the main con-
tribution of this work will be that future methods can
be evaluated in a comparable way, so that the relative
merit of various approaches can be determined.
2 Background
Pareti (2012) defines an attribution as having a
source span, a cue span, and a content span:
Source is the span of text that indicates who the
content is attributed to, e.g. ?president Obama?,
?analysts?, ?China?, ?she?.
Cue is the lexical anchor of the attribution relation,
usually a verb, e.g. ?say?, ?add?, ?quip?.
Content is the span of text that is attributed.
Based on the type of attitude the source expresses
towards a proposition or eventuality, attributions are
subcategorised (Prasad et al, 2006) into assertions
(Ex.2a) and beliefs (Ex.2b), which imply different
degrees of commitment, facts (Ex.2c), expressing
evaluation or knowledge, and eventualities (Ex.2d),
expressing intention or attitude.
(2) a. Mr Abbott said that he will win the election.
b. Mr Abbott thinks he will win the election.
c. Mr Abbott knew that Gillard was in Sydney.
d. Mr Abbott agreed to the public sector cuts.
Only assertion attributions necessarily imply a
speech act. Their content corresponds to a quotation
span and their source is generally referred to in the
literature as the speaker. Direct, indirect and mixed
quotations differ in the degree of factuality they en-
tail, since the former are by convention interpreted
as a verbatim transcription of an utterance whereas
indirect and the non-quoted portion of mixed quota-
tions can be paraphrased forms of the original word-
ing, and are thus filtered by the writer?s perspective.
The first speaker attribution systems (Zhang et al,
2003; Mamede and Chaleira, 2004; Glass and Ban-
gay, 2007) originate from the narrative domain and
were concerned with the identification of different
characters for speech synthesis applications. Direct
quotation attribution, with direct quotations being
given or extracted heuristically, has been the focus
of further studies in both the narrative (Elson and
McKeown, 2010) and news (Pouliquen et al, 2007;
Liang et al, 2010) domains. The few studies that
990
have addressed the extraction and attribution of in-
direct and mixed quotations are discussed below.
Krestel et al (2008) developed a quotation ex-
traction and attribution system that combines a lexi-
con of 53 common reporting verbs and a hand-built
grammar to detect constructions that match 6 gen-
eral lexical patterns. They evaluate their work on 7
articles from the Wall Street Journal, which contain
133 quotations, achieving macro-averaged Precision
(P ) of 99% and Recall (R) of 74% for quotation
span detection. PICTOR (Schneider et al, 2010) re-
lies instead on a context-free grammar for the extrac-
tion and attribution of quotations. PICTOR yielded
75% P and 86% R in terms of words correctly as-
cribed to a quotation or speaker, while it achieved
56% P and 52% R when measured in terms of com-
pletely correct quotation-speaker pairs.
SAPIENS (de La Clergerie et al, 2011) extracts
quotations from French news, by using a lexicon
of reporting verbs and syntactic patterns to extract
the complement of a reporting verb as the quota-
tion span and its subject as the source. They eval-
uated 40 randomly sampled quotations and found
that their system made 32 predictions and correctly
identified the span in 28 of the 40 cases. Verba-
tim (Sarmento and Nunes, 2009) extracts quotations
from Portuguese news feeds by first finding one of
35 speech verbs and then matching the sentence to
one of 19 patterns. Their manual evaluation shows
that 11.9% of the quotations Verbatim finds are er-
rors and that the system identifies approximately one
distinct quotation for every 46 news articles.
The system presented by Fernandes et al (2011)
also works over Portuguese news. Their work is the
closest to ours as they partially apply supervised ma-
chine learning to quotation extraction. Their work
introduces GloboQuotes, a corpus of 685 news items
containing 1,007 quotations of which 802 were used
to train an Entropy Guided Transformation Learn-
ing (ETL) algorithm (dos Santos and Milidiu?, 2009).
They treat quotation extraction as an IOB labelling
task, where they use ETL with POS and NE features
to identify the beginning of a quotation, while the
inside and outside labels are found using regular ex-
pressions. Finally they use ETL to attribute quota-
tions to their source. The overall system achieves
64% P and 67% R.
We have summarised these approaches in Table 1,
SMHC PARC
Corpus Doc Corpus Doc
Docs 965 - 2,280 -
Tokens 601k 623.3 1,139k 499.9
Quotations 7,991 8.3 10,526 4.6
Direct 4,204 4.4 3,262 1.4
Indirect 2,930 3.0 5,715 2.5
Mixed 857 0.9 1,549 0.6
Table 2: Comparison of the SMHC and PARC corpora, re-
porting their document and token size and per-type occur-
rence of quotations overall and per document (average).
which shows that the majority of evaluations thus far
have been small-scale. Furthermore, the published
results do not include any comparisons with previ-
ous work, which prevents a quantitative comparison
of the approaches, and they do not include results
broken down by whether the quotation is direct, in-
direct, or mixed. It is these issues that motivate our
work.
3 Corpora
We perform our experiments over two large corpora
from the news domain.
3.1 Penn Attribution Relations Corpus (PARC)
Our first corpus (Pareti, 2012), which we will re-
fer to as PARC, is a semi-automatically built ex-
tension to the attribution annotations included in
the PDTB (Prasad et al, 2008). The corpus covers
2,280 Wall Street Journal articles and contains an-
notations of assertions, beliefs, facts, and eventual-
ities, which are altogether referred to as attribution
relations (ARs). For this work we use only the asser-
tions, as they correspond to quotations (direct, indi-
rect and mixed). The drawback of this corpus is that
it is not yet fully annotated, i.e., it comprises positive
and unlabelled data.
The corpus includes a test set of 14 articles that
are fully annotated, which enables us to properly
evaluate our work and estimate that a proportion of
30-50% of ARs are unlabelled in the rest of the cor-
pus. The test set was manually annotated by two ex-
pert annotators. The annotators identified 491 ARs,
of which 22% were nested within another AR, with
991
an agreement score of 87%1. The agreement for the
selection of the content and source spans of com-
monly annotated ARs was 95% and 94% respec-
tively. In this work we address only non-embedded
assertions, so the final test-set includes 267 quotes,
totalling 321 non-discontinuous gold spans.
3.2 Sydney Morning Herald Corpus (SMHC)
We based our second corpus on the existing anno-
tations of direct quotations within Sydney Morning
Herald articles presented in O?Keefe et al (2012).
In that work we defined direct quotations as any
text between quotation marks, which included the
directly-quoted portion of mixed quotations, as well
as scare quotes. Under that definition direct quo-
tations could be automatically extracted with very
high accuracy, so annotations in that work were
over the automatically extracted direct quotations.
As part of this work one annotator removed scare
quotes, updated mixed quotations to include both
the directly and indirectly quoted portions, and
added whole new indirect quotations. The anno-
tation scheme was developed to be comparable to
the scheme used in the PARC corpus (Pareti, 2012),
although the SMHC corpus only includes assertions
and does not annotate the lexical cue.
The resulting corpus contains 7,991 quotations
taken from 965 articles from the 2009 Sydney Morn-
ing Herald (we refer to this corpus as SMHC). The
annotations in this corpus also include the speakers
of the quotations, as well as gold standard Named
Entities (NEs). We use 60% of this corpus as train-
ing data (4,872 quotations), 10% as development
data (759 quotations), and 30% as test data (2,360
quotations). Early experiments were conducted over
the development data, while the final results were
trained on both the training and development sets
and were tested on the unseen test data.
3.3 Comparison
Table 2 shows a comparison of the two corpora and
the quotations annotated within them. SMHC has a
higher density of quotations per document, 8.3 vs.
4.6 in PARC, since articles are fully annotated and
1The agreement was calculated using the agr metric de-
scribed in Wiebe and Riloff (2005) as the proportion of com-
monly annotated ARs with respect to the ARs identified overall
by Annotator A and Annotator B respectively
P R F
Bsay 94.4 43.5 59.5
Blist 75.4 71.1 73.2
k-NN 88.9 72.6 79.9
Table 3: Results for the k-NN verb-cue classifier. Bsay
classifies as verb-cue all instances of say while Blist
marks as verb-cues all verbs from a pre-compiled list in
Krestel et al (2008).
were selected to contain at least one quotation. PARC
is instead only partially annotated and comprises ar-
ticles with no quotations. Excluding null-quotation
articles from PARC, the average incidence of anno-
tated quotations per article raises to 7.1. The corpora
also differ in quotation type distribution, with di-
rect quotations being largely predominant in SMHC
while indirect are more common in PARC.
4 Experimental Setup
4.1 Quotation Extraction
Quotation extraction is the task of extracting the
content span of all of the direct, indirect, and mixed
quotations within a given document. More pre-
cisely, we consider quotations to be acts of com-
munication, which correspond to assertions in Pareti
(2012). Some quotations have content spans that are
split into separate, non-adjacent spans, as in exam-
ple (1a). Ideally the latter span should be marked as
a continuation of a quotation, however we consider
this to be out of scope for this work, so we treat each
span as a separate quotation.
4.2 Preprocessing
As a pre-processing step, both corpora were to-
kenised and POS tagged, and the potential speak-
ers anonymised to prevent over-fitting. We used the
Stanford factored parser (Klein and Manning, 2002)
to retrieve both the Stanford dependencies and the
phrase structure parse. Quotation marks were nor-
malised to a single character, as the quotation di-
rection is often incorrect for multi-paragraph quo-
tations.
4.3 Verb-cue Classifier
Verbs are by far the most common introducer of a
quotation. In PARC verbs account for 96% of all
992
cues, the prepositional phrase according to for 3%,
with the remaining 1% being nouns, adverbials and
prepositional groups. Attributional verbs are not a
closed set, they can vary across styles and genres,
and their attributional use is highly dependent on the
context in which they occur. It is therefore not possi-
ble to simply rely on a pre-compiled list of common
speech verbs. Quotations in PARC are introduced by
232 verb types, 87 of which are unique occurrences.
Not all of the verbs are speech verbs, for example
add, which is the second most frequent after say, or
the manner verb gripe (Ex.1a).
We used the attributional cues in the PARC cor-
pus to develop a separate component of our system
to identify attribution verb-cues. The classifier pre-
dicts whether the head of each verb group is a verb-
cue using the k-nearest neighbour (k-NN) algorithm,
with k equal to 3. The classifier uses 20 feature
types, including:
? Lexical (e.g. token, lemma, adjacent tokens)
? VerbNet classes membership
? Syntactic (e.g. node-depth in the sentence, par-
ent and sibling nodes)
? Sentence features (e.g. distance from sentence
start/end, within quotation markers).
We compared the system to one baseline, Bsay,
that marks every instance of say as a verb-cue, and
another, Blist, that marks every instance of a verb
that is on the list of 53 verbs presented in Krestel
et al (2008). We tested the system on the test set for
PARC, which contains 1809 potential verb-cues, of
which 354 are positive and 1455 are negative.
The results in Table 3 show that the verb-cue
classifier can outperform expert-derived knowledge.
The classifier was able to identify verb-cues with P
of 88.9% and R of 72.6%. While frequently oc-
curring verbs are highly predictive, the inclusion of
VerbNet classes (Schuler, 2005) and contextual fea-
tures allows for a more accurate classification of pol-
ysemous and unseen verbs.
Since PARC contains labelled and unlabelled attri-
butions, which is detrimental for training, we used
the verb-cue classifier to identify in the corpus sen-
tences that we suspected contained an unlabelled at-
tribution. Sentences containing a verb classified as a
cue that do not contain a quotation were removed
from the training set for the quotation extraction
model.
4.4 Evaluation
We use two metrics, listed below, for evaluating the
quotation spans predicted by our model against the
gold spans from the annotation.
Strict The first is a strict metric where a predicted
span is only considered to be correct if it exactly
matches a span from the gold standard. The stan-
dard precision, recall, and F -score can be calculated
using this definition of correctness. The drawback of
this strict score is that if a prediction is incorrect by
as little as one token it will be considered completely
incorrect.
Partial We also consider an overlap metric
(Hollingsworth and Teufel, 2005), which allows
partially correct predictions to be proportionally
counted. Precision (P ), recall (R), and F -score for
this method are:
P =
?
g?gold
?
p?pred overlap(g, p)
|pred|
(1)
R =
?
g?gold
?
p?pred overlap(p, g)
|gold|
(2)
F =
2PR
(P + R)
(3)
Where overlap(x, y) returns the proportion of to-
kens of y that are overlapped by x. For each of these
metrics we report the micro-average, as the number
of quotations in each document varies significantly.
When reporting P for the typewise results we re-
strict the set of predicted quotations to only those
with the requisite type, while still considering the
full set of gold quotations. Similarly, when calculat-
ing R we restrict the set of gold quotations to only
those with the required type.
4.5 Baselines
We have developed two baselines inspired by the
current lexical/syntactic pattern-based approaches
in the literature, which combine speech verbs and
hand-crafted rules.
993
Blex Lexical: cue verb + the longest of the spans be-
fore or after it until the sentence boundary.
Bsyn Syntactic: cue verb + verb syntactic object.
Bsyn is close to the model in de La Clergerie
et al (2011).
Instead of relying on a lexicon of verbs, our base-
lines use those identified by the verb-cue classifier.
As direct quotations are not always explicitly intro-
duced by a cue-verb, we defined a separate baseline
with a rule-based approach (Brule) that returns text
between quotation marks that has at least 3 tokens,
and where the non-stopword and non-proper noun
tokens are not all title cased. In our full results we
apply each method along with Brule and greedily
take the longest predicted spans that do not overlap.
5 Supervised Approaches
We present two supervised approaches to quotation
extraction, which operate over the tokens and the
phrase-structure parse nodes respectively. Despite
the difference in the item being classified, these ap-
proaches have some common features:
Lexical: unigram and bigram versions of the token,
lemma, and POS tags within a window of 5 to-
kens either side of the target, all indexed by po-
sition.
Sentence: features indicating whether the sentence
contains a quotation mark, a NE, a verb-cue, a
pronoun, or any combination of these. There is
also a sentence length feature.
Dependency: relation with parent, relations with
any dependants, as well as versions of these
that include the head and dependent tokens.
External knowledge: position-indexed features for
whether any of the tokens in the sentence match
a known role, organisation, or title. The titles
come from a small hand-built list, while the
role and organisation lists were built by recur-
sively following the WordNet (Fellbaum, 1998)
hyponyms of person and organization respec-
tively.
Other: features for whether the target is within quo-
tation marks, and whether there is a verb-cue
near the end of the sentence.
Strict Partial
P R F P R F
PARC Brule 75 94 83 96 94 95
Token 97 91 94 98 97 97
SMHC Brule 87 93 90 98 94 96
Token 94 90 92 99 97 98
Table 4: PARC and SMHC results on direct quotations.
The token based approach is trained and tested on all quo-
tations.
5.1 Token-based Approach
The token-based approach treats quotation extrac-
tion as analogous to NE tagging, where there are a
sequence of tokens that need to be individually la-
belled. Each token is given either an I, an O, or a B
label, where B denotes the first token in a quotation,
I denotes the token is inside a quotation, and O indi-
cates that the token is not part of a quotation. For NE
tagging it is common to use a sentence as a single
sequence, as NEs do not cross sentence boundaries.
This does not work for quotations, as they can cross
sentence and even paragraph boundaries. As such,
we treat the entire document as a single sequence,
which allows the predicted quotations to span both
sentence and paragraph bounds.
We use a linear chain Conditional Random Field
(CRF)2 as the learning algorithm, with the common
features listed above, as well as the following fea-
tures:
Verb: features indicating whether the current token
is a (possibly indirect) dependent of a verb-cue,
and another for whether the token is at the start
of a constituent that is a dependent of a verb-
cue.
Ancestor: the labels of all constituents that contain
the current token in their span, indexed by their
depth in the parse tree.
Syntactic: the label, depth, and token span size of
the highest constituent where the current token
is the left-most token in the constituent, as well
as its parent, and whether either of those con-
tains a verb-cue.
2http://www.chokkan.org/software/crfsuite/
994
Indirect Mixed All1
Strict P R F P R F P R F
Blex 34 32 33 17 26 20 46 44 45
Bsyn 78 46 58 61 40 49 80 63 70
Token 66 54 59 55 58 56 76 70 73
Constituent 61 50 55 50 38 43 70 64 67
ConstituentG 66 42 51 68 49 57 76 62 68
Partial P R F P R F P R F
Blex 56 66 61 78 79 78 73 79 76
Bsyn 89 58 70 88 75 81 92 74 82
Token 79 74 76 85 90 87 87 86 87
Constituent 78 67 72 84 82 83 86 80 83
ConstituentG 80 54 65 90 80 85 90 74 81
Table 5: Results on PARC. 1All reports the results over all quotations (direct, indirect and mixed). For the baselines,
this is a combination of the strategy in Blex or Bsyn with the rules for direct quotations. ConstituentG shows the
results for the constituent model using the gold parse.
5.2 Constituent-based Approach
The constituent approach classifies whole phrase
structure nodes as either quotation or not a quota-
tion. Ideally each quotation would match exactly
one constituent, however this is not always the case
in our data. In cases without an exact match we la-
bel every constituent that is a subspan of the quo-
tation as a quotation as long as it has a parent that
is not a subspan of the quotation. In these cases
multiple nodes will be labelled quotation, so a post-
processing step is introduced that rebuilds quota-
tions by merging predicted spans that are adjacent or
overlapping within a sentence. Restricting the merg-
ing process this way loses the ability to predict quo-
tations that cover more than a sentence, but without
this restriction too many predicted quotations are er-
roneously merged.
This approach uses a maximum entropy classi-
fier3 with L1 regularisation. In early experiments
we found that the constituent-based approach per-
formed poorly when trained on all quotations, so for
these experiments the constituent classifier is trained
only on indirect and mixed quotations. The classifier
uses the common features listed above as well as the
following features:
Span: length of the span, features for whether there
is a verb or a NE.
3http://scikit-learn.org/
Node: the label, number of descendants, number of
ancestors, and number of children of the target.
Context: dependency, node, and span features for
the parent and siblings of the target.
In addition the lexical features described earlier
are applied to both the start and end tokens of the
node?s span, as well as the highest token in the de-
pendency parse that is within the span.
6 Results
6.1 Direct Quotations
Table 4 shows the results for predicting direct quota-
tions on PARC and SMHC. In both corpora and with
both metrics the token-based approach outperforms
Brule. Although direct quotations should be trivial
to extract, and a simple system that returns the con-
tent between quotation marks should be hard to beat,
there are two main factors that confound the rule-
based system.
The first is the presence of mixed quotations,
which is most clearly demonstrated in the difference
between the strict precision scores and the partial
precision scores for Brule. Brule will find all of
the directly-quoted portions of mixed quotes, which
do not exactly match a quotation, and so will re-
ceive a low precision score with the strict metric.
However the partial overlap score will reward these
995
Indirect Mixed All1
Strict P R F P R F P R F
Blex 37 42 40 15 36 21 50 50 50
Bsyn 63 49 55 67 36 47 82 72 76
Token 69 53 60 80 91 85 82 75 78
Constituent 54 49 51 64 42 51 77 72 75
Partial P R F P R F P R F
Blex 52 68 59 87 77 82 77 84 81
Bsyn 75 59 66 89 66 76 91 80 85
Token 82 67 74 88 84 86 92 86 89
Constituent 77 63 69 91 75 82 91 82 86
Table 6: Results on SMHC. 1All reports the results over all quotations (direct, indirect and mixed). For the baselines,
this is a combination of the strategy in Blex or Bsyn with the rules for direct quotations.
predictions, as they do partially match a quote, so
there is a large difference in those scores. Note that
the reduced strict score does not occur for the token
method, which correctly identifies mixed quotations.
The other main issue is the presence of quotation
marks around items such as book titles and scare
quotes (i.e. text that is in quotation marks to distance
the author from a particular wording or claim). In
Section 4.5 we described the methods that we use to
avoid scare quotes and titles, which are rule-based
and imperfect. While these methods increase the
overall F -score of Brule, they do have a negative
impact on recall, which is why the recall is lower
than might be expected. These results demonstrate
that although direct quotations can be accurately ex-
tracted with rules, the accuracy will be lower than
might be anticipated and the returned spans will in-
clude a number of mixed quotations, which will be
missing some content.
6.2 Indirect and Mixed Quotations
The token approach was also the most effective
method for extracting indirect and mixed quotations
as Tables 5 and 6 show. Indirect quotations were
extracted with strict F -scores of 59% and 60% and
partial F -scores of 76% and 74% in PARC and SMHC
respectively, while mixed quotes were found with
strict F -scores of 56% and 85% and partial F -scores
of 87% and 86%.
Although there is a strong interconnection be-
tween syntax and attribution, results for Bsyn show
that merely considering attribution as a syntactic re-
lation (Skadhauge and Hardt, 2005) has a large im-
pact on recall: only a subset of inter-sentential quo-
tations can be effectively matched by verb comple-
ment boundaries.
The constituent model yielded lower results than
the token one, and in particular it greatly lowered
the recall of mixed quotations in both corpora. Since
the model heavily relies on syntax, it is particularly
affected by errors made by the parser. The conjunc-
tion and in Example 3 is incorrectly attached by the
parser to the cue said, leading the classifier to iden-
tify two separate spans. In order to verify the impact
of incorrect parsing on the model, we ran the con-
stituent model using gold standard parses for PARC.
This resulted in an increase in strict P and increased
the F -score for mixed quotations to 57%, similarly
to the score achieved by the token model. However,
it surprisingly negatively affected R for indirect quo-
tations.
(3) Graeme Hugo, said strong links between Aus-
tralia?s 700,000 ethnic Chinese and China
could benefit both countries and were unlikely
to pose a threat.
The tables also report results for the extraction of
all quotations, irrespective of their type. For this
score, the baseline models for indirect and mixed
quotations are combined with Brule for direct quo-
tations.
6.3 Model Comparison
We designed the features for the token and con-
stituent models to be largely similar. This al-
996
lows us to conclude that the difference in perfor-
mance between the token and constituent models
is largely driven by the class labelling and learn-
ing method. Overall, the token-based approach out-
performed both the baselines and the constituent
method. Qualitatively we found that the token-based
approach was making reasonable predictions most
of the time, but would often fail when a quotation
was attributed to a speaker through a parenthetical
clause, as in Example 4.
(4) Finding lunar ice, said Tidbinbilla?s
spokesman, Glen Nagle, would give a
major boost to NASA?s hopes of returning
humans to the moon by 2020.
The token-based approach has a reasonable bal-
ance of the various label types, and benefits from a
decoding step that allows it to make trade-offs be-
tween good local decisions and a good overall so-
lution. By comparison, the constituent-based ap-
proach has a large class imbalance, as there are many
more negative (i.e. not quotation) parse nodes than
there are positive, which makes finding a good deci-
sion boundary difficult. We experimented with re-
ducing the number of negative nodes to consider,
but found that the overall F -score was equivalent or
worse, largely driven by a drop in recall. We also
found that in many cases the constituent-approach
predicted quotes that were too short, or that were
only the second half of a conjunction, without the
first half being labelled. We expect that these issues
would be corrected with the addition of a decoding
step, that forces the classifier to make a good global
decision.
7 Speaker Attribution
While the focus of this paper is on extracting quota-
tions, we also present results on finding the speaker
of each quotation. As discussed in Section 2, quo-
tation attribution has been addressed in the litera-
ture before, including some work that includes large-
scale data (Elson and McKeown, 2010). However,
the large-scale evaluations that exist cover only di-
rect quotations, whereas we present results for di-
rect, indirect, and mixed quotations.
For this evaluation we use four of the methods that
were introduced in O?Keefe et al (2012). The first
is a simple rule-based approach (Rule) that returns
the entity closest to the speech verb nearest the quo-
tation, or if there is no such speech verb then the
entity nearest the end of the quotation. The second
method uses a CRF which is able to choose between
up to 15 entities that are in the paragraph containing
the quotation or any preceding it. The third method
(No seq.) is a binary MaxEnt classifier that predicts
whether each entity is the speaker or not the speaker,
with the entity achieving the highest speaker proba-
bility predicted. In O?Keefe et al (2012) this model
achieved the best results on the direct quotations in
SMHC, despite not using the sequence features or de-
coding methods that were available to other models.
The final method that we evaluate (Gold) is the ap-
proach that uses sequence features that use the gold-
standard labels from previous decisions. As noted
by O?Keefe et al, this method is not realisable in
practise, however we include these results so that
we can reassess the claims of O?Keefe et al when
direct, indirect, and mixed quotations are included.
For our results to be comparable we use the list of
speech verbs that was presented in Elson and McK-
eown (2010) and used in O?Keefe et al (2012).
Table 7 shows the accuracy of the two meth-
ods on both PARC and SMHC, broken down by the
type of the quotation. The first observation that
we make about these results in comparison to the
O?Keefe et al results, is that the accuracy is gener-
ally lower, even for direct quotations. This discrep-
ancy is caused by differences in our data compared
to theirs, notably that the sequence of quotations is
altered in ours by the introduction of indirect quota-
tions, and that some of the direct quotations that they
evaluated would be considered mixed quotations in
our corpora. The rule based method performs par-
ticularly poorly on PARC, which is likely caused by
the relative scarcity of direct quotations and the fact
that it was designed for direct quotations only. Di-
rect quotations are much more frequent in SMHC, so
the rules that rely on the sequence of speakers would
likely perform relatively better than on PARC.
While the approach using gold-standard sequence
features unsurprisingly performed the best, the most
straightforward learned model (No seq.), trained
without any sequence information, equalled or out-
performed the two other non-gold approaches for all
quotation types on both corpora. This indicates that
the CRF model evaluated here was not able to effec-
997
Corpus Method Dir. Ind. Mix. All
PARC Rule 70 60 47 62
CRF 82 68 65 73
No seq. 85 74 65 77
Gold 88 79 74 82
SMHC Rule 89 76 78 84
CRF 83 72 71 78
No seq. 91 79 81 87
Gold 93 81 83 89
Table 7: Speaker attribution accuracy results for both cor-
pora over gold standard quotations.
tively use the sequence information that is present.
8 Conclusion
In this work we have presented the first large-scale
experiments on the entire quotation extraction and
attribution task: evaluating the extraction and at-
tribution of direct, indirect and mixed quotations
over two large news corpora. One of these corpora
(SMHC) is a novel contribution of this work, while
our results are the first presented for the other cor-
pus (PARC). This work has shown that while rule-
based approaches that return the object of a speech
verb are indeed effective, they are outperformed by
supervised systems that can take advantage of addi-
tional evidence. We also show that state-of-the-art
quotation attribution methods are less accurate on
indirect and mixed quotations than they are on di-
rect quotations.
Future work will include extending these methods
to extract all attributions, i.e. beliefs, eventualities,
and facts, as well as the source spans. We will also
evaluate the effect of adding a decoding step to the
constituent approach. This work provides an accu-
rate and complete quotation extraction and attribu-
tion system that can be used for a wide range of tasks
in information extraction and opinion mining.
Acknowledgements
We would like to thank Bonnie Webber for her feed-
back and assistance. Pareti has been supported by a
Scottish Informatics & Computer Science Alliance
(SICSA) studentship; O?Keefe has been supported
by a University of Sydney Merit scholarship and
a Capital Markets CRC top-up scholarship. This
work has been supported by ARC Discovery grant
DP1097291 and the Capital Markets CRC Com-
putable News project.
References
Sabine Bergler. 1992. Evidential analysis of re-
ported speech. Ph.D. thesis, Brandeis University.
Sabine Bergler, Monia Doandes, Christine Gerard,
and Rene? Witte. 2004. Attributions. In Exploring
Attitude and Affect in Text: Theories and Applica-
tions, Technical Report SS-04-07, pages 16?19.
Papers from the 2004 AAAI Spring Symposium.
Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-
cal Denis, Gaelle Recource, and Victor Mignot.
2011. Extracting and visualizing quotations from
news wires. Human Language Technology. Chal-
lenges for Computer Science and Linguistics,
pages 522?532.
C??cero Nogueira dos Santos and Ruy Luiz Milidiu?.
2009. Entropy guided transformation learning. In
Foundations of Computational, Intelligence Vol-
ume 1, Studies in Computational Intelligence,
pages 159?184. Springer.
David K. Elson and Kathleen R. McKeown. 2010.
Automatic attribution of quoted speech in literary
narrative. In Proceedings of the Twenty-Fourth
Conference of the Association for the Advance-
ment of Artificial Intelligence, pages 1013?1019.
Christine Fellbaum. 1998. WordNet: An electronic
lexical database. MIT press Cambridge, MA.
William Paulo Ducca Fernandes, Eduardo Motta,
and Ruy Luiz Milidiu?. 2011. Quotation extraction
for portuguese. In Proceedings of the 8th Brazil-
ian Symposium in Information and Human Lan-
guage Technology (STIL 2011), pages 204?208.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification
in fiction books. In Proceedings of the 18th An-
nual Symposium of the Pattern Recognition Asso-
ciation of South Africa (PRASA07), pages 1?6.
Bill Hollingsworth and Simone Teufel. 2005. Hu-
man annotation of lexical chains: Coverage and
agreement measures. In ELECTRA Workshop on
Methodologies and Evaluation of Lexical Cohe-
sion Techniques in Real-world Applications (Be-
yond Bag of Words), page 26.
998
Dan Klein and Christopher D Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in neural informa-
tion processing systems, pages 3?10.
Ralf Krestel, Sabine Bergler, and Rene? Witte. 2008.
Minding the source: Automatic tagging of re-
ported speech in newspaper articles. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08).
Jisheng Liang, Navdeep Dhillon, and Krzysztof
Koperski. 2010. A large-scale system for an-
notating and querying quotations in news feeds.
In Proceedings of the 3rd International Semantic
Search Workshop, pages 1?5.
Nuno Mamede and Pedro Chaleira. 2004. Charac-
ter identification in children stories. Advances in
Natural Language Processing, pages 82?90.
Tim O?Keefe, Silvia Pareti, James R. Curran, Irena
Koprinska, and Matthew Honnibal. 2012. A se-
quence labelling approach to quote attribution. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
pages 790?799.
Silvia Pareti. 2012. A database of attribution rela-
tions. In Proceedings of the Eight International
Conference on Language Resources and Evalua-
tion, pages 3213?3217.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In Proceedings of Recent Advances
in Natural Language Processing, pages 487?492.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind
Joshi, and Bonnie Webber. 2006. Annotating at-
tribution in the Penn Discourse TreeBank. In Pro-
ceedings of the Workshop on Sentiment and Sub-
jectivity in Text, pages 31?38.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,
Alan Lee, Aravind Joshi, Livio Robaldo, and
Bonnie Webber. 2008. The Penn Discourse Tree-
Bank 2.0 annotation manual. In Technical report,
University of Pennsylvania: Institute for Research
in Cognitive Science.
Luis Sarmento and Sergio Nunes. 2009. Automatic
extraction of quotes and topics from news feeds.
In 4th Doctoral Symposium on Informatics Engi-
neering.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. In Lan-
guage Resources and Evaluation, pages 227?268.
Nathan Schneider, Rebecca Hwa, Philip Gianfor-
toni, Dipanjan Das, Michael Heilman, Alan W.
Black, Frederik L. Crabbe, and Noah A. Smith.
2010. Visualizing topical quotations over time
to understand news discourse. Technical report,
Carnegie Mellon University.
Karin K. Schuler. 2005. Verbnet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D.
thesis, Faculties of Computer and Information
Science of the University of Pennsylvania.
Peter R. Skadhauge and Daniel Hardt. 2005. Syn-
tactic identification of attribution in the RST tree-
bank. In Proceedings of the Sixth International
Workshop on Linguistically Interpreted Corpora.
Janyce Wiebe and Ellen Riloff. 2005. Creating
subjective and objective sentence classifiers from
unannotated texts. In Computational Linguistics
and Intelligent Text Processing, pages 486?497.
Springer.
Jason Zhang, Alan Black, and Richard Sproat.
2003. Identifying speakers in children?s stories
for speech synthesis. In Proceedings of EU-
ROSPEECH, pages 2041?2044.
999
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503?1514,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Inducing Document Plans for Concept-to-text Generation
Ioannis Konstas and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
ikonstas@inf.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In a language generation system, a content
planner selects which elements must be in-
cluded in the output text and the ordering be-
tween them. Recent empirical approaches per-
form content selection without any ordering
and have thus no means to ensure that the out-
put is coherent. In this paper we focus on
the problem of generating text from a database
and present a trainable end-to-end generation
system that includes both content selection
and ordering. Content plans are represented
intuitively by a set of grammar rules that op-
erate on the document level and are acquired
automatically from training data. We de-
velop two approaches: the first one is inspired
from Rhetorical Structure Theory and repre-
sents the document as a tree of discourse re-
lations between database records; the second
one requires little linguistic sophistication and
uses tree structures to represent global patterns
of database record sequences within a doc-
ument. Experimental evaluation on two do-
mains yields considerable improvements over
the state of the art for both approaches.
1 Introduction
Concept-to-text generation broadly refers to the task
of automatically producing textual output from non-
linguistic input (Reiter and Dale, 2000). Depend-
ing on the application and the domain at hand, the
input may assume various representations including
databases, expert system knowledge bases, simula-
tions of physical systems, or formal meaning rep-
resentations. Generation systems typically follow
a pipeline architecture consisting of three compo-
nents: content planning (selecting and ordering the
parts of the input to be mentioned in the output text),
sentence planning (determining the structure and
lexical content of individual sentences), and surface
realization (verbalizing the chosen content in natu-
ral language). Traditionally, these components are
hand-engineered in order to ensure output of high
quality.
More recently there has been growing interest
in the application of learning methods because of
their promise to make generation more robust and
adaptable. Examples include learning which con-
tent should be present in a document (Duboue and
McKeown, 2002; Barzilay and Lapata, 2005), how it
should be aligned to utterances (Liang et al, 2009),
and how to select a sentence plan among many al-
ternatives (Stent et al, 2004). Beyond isolated com-
ponents, a few approaches have emerged that tackle
concept-to-text generation end-to-end. Due to the
complexity of the task, most models simplify the
generation process, e.g., by treating sentence plan-
ning and surface realization as one component (An-
geli et al, 2010), by implementing content selection
without any document planning (Konstas and Lap-
ata, 2012; Angeli et al, 2010; Kim and Mooney,
2010), or by eliminating content planning entirely
(Belz, 2008; Wong and Mooney, 2007).
In this paper we present a trainable end-to-end
generation system that captures all components of
the traditional pipeline, including document plan-
ning. Rather than breaking up the generation pro-
cess into a sequence of local decisions, each learned
separately (Reiter et al, 2005; Belz, 2008; Chen and
Mooney, 2008; Kim and Mooney, 2010), our model
performs content planning (i.e., document planning
and content selection), sentence planning (i.e., lex-
1503
Database Records
temp(time:6-21, min:9, mean:15, max:21)
wind-spd(time:6-21, min:15, mean:20, max:30)
sky-cover(time:6-9, percent:25-50)
sky-cover(time:9-12, percent:50-75)
wind-dir(time:6-21, mode:SSE)
gust(time:6-21, min:20, mean:30, max:40)
Output Text
Cloudy, with a high around 20. South southeast wind
between 15 and 30 mph. Gusts as high as 40 mph.
(a) WEATHERGOV
Database Records
desktop(cmd:lclick, name:start, type:button)
start(cmd:lclick, name:settings, type:button)
start-target(cmd:lclick, name:control panel, type:button)
win-target(cmd:dblclick, name:users and passwords, type:item)
contMenu(cmd:lclick, name:advanced, type:tab)
action-contMenu(cmd:lclick, name:advanced, type:button)
Output Text
Click start, point to settings, and then click control panel. Double-
click users and passwords. On the advanced tab, click advanced.
(b) WINHELP
Figure 1: Database records and corresponding text for (a) weather forecasting and (b) Windows trou-
bleshooting. Each record has a type (e.g., win-target), and a set of fields. Each field has a value, which
can be categorical (in typewriter), an integer (in bold), or a literal string (in italics).
icalization of input entries), and surface realization
jointly. We focus on the problem of generating text
from a database. The input to our model is a set of
database records and collocated descriptions, exam-
ples of which are shown in Figure 1.
Given this input, we define a probabilistic
context-free grammar (PCFG) that captures the
structure of the database and how it can be verbal-
ized. Specifically, we extend the model of Kon-
stas and Lapata (2012) which also uses a PCFG to
perform content selection and surface realization,
but does not capture any aspect of document plan-
ning. We represent content plans with grammar
rules which operate on the document level and are
embedded on top of the original PCFG. We essen-
tially learn a discourse grammar following two ap-
proaches. The first one is linguistically naive but
applicable to multiple languages and domains; it ex-
tracts rules representing global patterns of record
sequences within a sentence and among sentences
from a training corpus. The second approach learns
document plans based on Rhetorical Structure The-
ory (RST; Mann and Thomson, 1988); it therefore
has a solid linguistic foundation, but is resource in-
tensive as it assumes access to a text-level discourse
parser.
We learn document plans automatically using
both representations and develop a tractable decod-
ing algorithm for finding the best output, i.e., deriva-
tion in our grammar. To the best of our knowledge,
this is the first data-driven model to incorporate doc-
ument planning in a joint end-to-end system. Exper-
imental evaluation on the WEATHERGOV (Liang et
al., 2009) and WINHELP (Branavan et al, 2009) do-
mains shows that our approach improves over Kon-
stas and Lapata (2012) by a wide margin.
2 Related Work
Content planning is a fundamental component in a
natural generation system. Not only does it deter-
mine which information-bearing units to talk about,
but also arranges them into a structure that cre-
ates coherent output. It is therefore not surpris-
ing that many content planners have been based
on theories of discourse coherence (Hovy, 1993;
Scott and de Souza, 1990). Other work has re-
lied on generic planners (Dale, 1988) or schemas
(Duboue and McKeown, 2002). In all cases, con-
tent plans are created manually, sometimes through
corpus analysis. A few researchers recognize that
this top-down approach to planning is too inflexible
and adopt a generate-and-rank architecture instead
(Mellish et al, 1998; Karamanis, 2003; Kibble and
Power, 2004). The idea is to produce a large set
of candidate plans and select the best one according
to a ranking function. The latter is typically devel-
oped manually taking into account constraints relat-
ing to discourse coherence and the semantics of the
domain.
Duboue and McKeown (2001) present perhaps
the first empirical approach to content planning.
They use techniques from computational biology
to learn the basic patterns contained within a plan
and the ordering among them. Duboue and McK-
eown (2002) learn a tree-like planner from an
aligned corpus of semantic inputs and correspond-
ing human-authored outputs using evolutionary al-
1504
gorithms. More recent data-driven work focuses on
end-to-end systems rather than individual compo-
nents, however without taking document planning
into account. For example, Kim and Mooney (2010)
first define a generative model similar to Liang et
al. (2009) that selects which database records to
talk about and then use an existing surface real-
izer (Wong and Mooney, 2007) to render the cho-
sen records in natural language. Their content plan-
ner has no notion of coherence. Angeli et al (2010)
adopt a more unified approach that builds on top of
the alignment model of Liang et al (2009). They
break record selection into a series of locally coher-
ent decisions, by first deciding on what records to
talk about. Each choice is based on a history of
previous decisions, which is encoded in the form
of discriminative features in a log-linear model.
Analogously, they choose fields for each record,
and finally verbalize the input using automatically
extracted domain-specific templates from training
data.
Konstas and Lapata (2012) propose a joint model,
which recasts content selection and surface realiza-
tion into a parsing problem. Their model optimizes
the choice of records, fields and words simultane-
ously, however they still select and order records lo-
cally. We replace their content selection mechanism
(which is based on a simple markovized chaining of
records) with global document representations. A
plan in our model is identified either as a sequence
of sentences, each containing a sequence of records,
or as a tree where the internal nodes denote dis-
course information and the leaf nodes correspond to
records.
3 Problem Formulation
The generator takes as input a set of database
records d and outputs a text g that verbalizes some
of these records. Each record token ri ? d, with
1 ? i ? |d|, has a type ri.t and a set of fields f as-
sociated with it. Fields have different values f .v and
types f .t (i.e., integer, categorical, or literal strings).
For example, in Figure 1b, win-target is a record
type with three fields: cmd (denotes the action the
user must perform on an object on their screen,
e.g., left-click), name (denotes the name of the ob-
ject), and type (denotes the type of the object). The
values of these fields are dblclick, users and pass-
words, and item; name is a literal string, the rest are
Grammar Rules
1. S? R(start)
2. R(ri.t)? FS(r j,start) R(r j.t) | FS(r j,start)
3. FS(r,r. fi)? F(r,r. f j) FS(r,r. f j) | F(r,r. f j)
4. F(r,r. f )?W(r,r. f ) F(r,r. f ) |W(r,r. f )
5. W(r,r. f )? ? | g( f .v)
Figure 2: Grammar G of the original model. Paren-
theses denote features, and impose constraints on the
grammar.
categorical.
During training, our algorithm is given a corpus
consisting of several scenarios, i.e., database records
paired with texts w (see Figure 1). For each sce-
nario, the model first decides on a global document
plan, i.e., it selects which types of records belong to
each sentence (or phrase) and how these sentences
(or phrases) should be ordered. Then it selects ap-
propriate record tokens for each type and progres-
sively chooses the most relevant fields; then, based
on the values of the fields, it generates the final text,
word by word.
4 Original Model
Our work builds on the model developed by Kon-
stas and Lapata (2012). The latter is essentially
a PCFG which captures both the structure of the
input database and the way it renders into natural
language. This grammar-based approach lends it-
self well to the incorporation of document planning
which has traditionally assumed tree-like represen-
tations. We first briefly describe the original model
and then present our extensions in Section 5.
Grammar Grammar G in Figure 2 defines a set
of non-recursive CFG rewrite rules that capture the
structure of the database, i.e., the relationship be-
tween records, records and fields, fields and words.
These rules are domain-independent and could be
applied to any database provided it follows the same
structure. Non-terminal symbols are in capitals, the
terminal symbol ? corresponds to the vocabulary of
the training set and g( f .v) is a function which gener-
ates integers given the field value f .v. Note that all
non-terminals have features (in parentheses) which
1505
act as constraints and impose non-recursion (e.g., in
rule (2) i 6= j, so that a record cannot emit itself).
Rule (1) defines the expansion from the start sym-
bol S to the first record R of type start. The rules
in (2) implement content selection, by choosing ap-
propriate records from the database and generating
a sequence. R(ri.t) is the source record, R(r j.t) is
the target record and FS(r j.start) is a place-holder
symbol for the set of fields of record token r j. This
method is locally optimal, since it only keeps track
of the previous type of record for each re-write. The
rules in (3) conclude content selection on the field
level, i.e., after we have chosen a record, we select
and order the corresponding fields. Finally, the rules
in (4) and (5) correspond to surface realization. The
former rule binarizes the sequence of words emitted
by a particular field r. f in an attempt to capture local
dependencies between words, such as multi-word
expressions (e.g., right click, radio button). The lat-
ter rule defines the emission of words and integer
numbers1, given a field type and its value. Note that
the original model lexicalizes field values of cate-
gorical and integer type only.
Training The rules of grammar G are associated
with weights that are learned using the EM algo-
rithm (Dempster et al, 1977). During training, the
records, fields and values of database d and the
words w from the associated text are observed, and
the model learns the mapping between them. Notice
that we use w to denote the gold-standard text and
g to refer to the words generated by the model. The
mapping between the database and the observed text
is unknown and thus the weights of the rules define
a hidden correspondence h between records, fields
and their values.
Decoding Given a trained grammar G and an in-
put scenario from a database d, the model generates
text by finding the most likely derivation, i.e., se-
quence of rewrite rules for the input. Although re-
sembling parsing, the generation task is subtly dif-
ferent. In parsing, we observe a string of words and
our goal is to find the most probable syntactic struc-
ture, i.e., hidden correspondence h?. In generation,
1The function g( f .v) : Z? Z, generates an integer in the
following six ways (Liang et al, 2009): identical, rounding
up/down to a multiple of 5, rounding off a multiple of 5 and
adding or subtracting some noise modelled by a geometric dis-
tribution.
however, the string is not observed; instead, we must
find the best text g?, by maximizing both over h and g,
where g = g1 . . .gN is a sequence of words licensed
by G. More formally:
g? = f
(
argmax
g,h
P
(
(g,h)
))
(1)
where f is a function that takes as input a derivation
tree (g,h) and returns g?. Konstas and Lapata (2012)
use a modified version of the CYK parser (Kasami,
1965; Younger, 1967) to find g?. Specifically, they
intersect grammar G with a n-gram language model
and calculate the most probable generation g? as:
g? = f
(
argmax
g,h
p(g) ? p(g,h |d)
)
(2)
where p(g,h |d) is the decoding likelihood for a se-
quence of words g = g1 . . .gN of length N and the
hidden correspondence h that emits it, i.e., the likeli-
hood of the grammar for a given database input sce-
nario d. p(g) is a measure of the quality of each out-
put and is provided by the n-gram language model.
5 Extensions
In this section we extend the model of Konstas and
Lapata (2012) by developing two more sophisticated
content selection approaches which are informed by
a global plan of the document to be generated.
5.1 Planning with Record Sequences
Grammar Our key idea is to replace the content
selection mechanism of the original model with a
document plan which essentially defines a gram-
mar on record types. We split a document into
sentences, each terminated by a full-stop. Then a
sentence is further split into a sequence of record
types. Contrary to the original model, we observe a
complete sequence2 of record types, split into sen-
tences. This way we learn domain-specific pat-
terns of frequently occurring record type sequences
among the sentences of a document, as well as more
local structures within a sentence. We thus substitute
rules (1)?(2) in Figure 2 with sub-grammar GRSE
based on record type sequences:
Definition 1 (GRSE grammar)
GRSE = {?R, NRSE , PRSE , D}
2Note that a sequence is different from a permutation, as we
may allow repetitions or omissions of certain record types.
1506
where ?R is a set of terminal symbols R(r.t), and
NRSE is a set of non-terminal symbols:
NRSE = {D, SENT}
where D represents the start symbol and SENT a
sequence of records. PRSE is a set of production rules
of the form:
(a) D? SENT (ti, . . . , t j) . . . SENT (tl, . . . , tm)
(b) SENT (ti, . . . , t j)? R(ra.ti) . . . R(rk.t j) ?
where t is a record type, ti, t j, tl and tm may overlap
and ra, rk are record tokens of type ti and t j respec-
tively. The corresponding weights for the production
rules PRSE are:
Definition 2 (GRSE weights)
(a) p(ti, . . . , t j, . . . tl, . . . , tm | D)
(b) p(ti) ? ... ? p(t j) = 1|s(ti)| ? . . . ?
1
|s(t j)|
where s(t) is a function that returns the set of records
with type t (Liang et al, 2009).
Rule (a) defines the expansion from the start sym-
bol D to a sequence of sentences, each represented
by the non-terminal SENT . Similarly to the original
grammar G, we employ the use of features (in paren-
theses) to denote a sequence of record types. The
same record types may recur in different sentences,
but not in the same one. The weight of rule (a) is
simply the joint probability of all the record types
present, ordered and segmented appropriately into
sentences in the document, given the start symbol.
Once record types have been selected (on a per
sentence basis) we move on to rule (b) which de-
scribes how each non-terminal SENT expands to
an ordered sequence of records R, as they are ob-
served within a sentence (see the terminal sym-
bol ?.? at the end of the rule). Notice that a record
type ti may correspond to several record tokens ra.
Rules (3)?(5) in grammar G make decisions on these
tokens based on the overall content of the database
and the field/value selection. The weight of this
rule is the product of the weights of each record
type. This is set to the uniform distribution over
{1, ..., |s(t)|} for record type t, where |s(t)| is the
number of records with that type.
Figure 3d shows an example tree for the database
input in Figure 1b, using GRSE and assuming that the
alignments between records and text are given. The
top level of the tree refers to the sequence of record
types as they are observed in the text. The first sen-
tence contains three records with types ?desktop?,
?start? and ?start-target?, each corresponding to the
textual segments click start, point to settings, and
then click control panel. The next level on the tree,
denotes the choice of record tokens for each sen-
tence, provided that we have decided on the choice
and order of their types (see Figure 3b). In Fig-
ure 3d, the bottom-left sub-tree corresponds to the
choice of the first three records of Figure 1b.
Training A straightforward way to train the ex-
tended model would be to embed the parameters of
GRSE in the original model and then run the EM al-
gorithm using inside-outside at the E-step. Unfortu-
nately, this method will induce a prohibitively large
search space. Rule (a) enumerates all possible com-
binations of record type sequences and the number
grows exponentially even for a few record types and
a small sequence size. To tackle this problem, we ex-
tracted rules for GRSE from the training data, based
on the assumption that there will be far fewer unique
sequences of record types per dataset than exhaus-
tively enumerating all possibilities.
For each scenario, we obtain a word-by-word
alignment between the database records and the cor-
responding text. In our experiments we used Liang
et al?s (2009) unsupervised model, however any
other semi- or fully supervised method could be
used. As we show in Section 7, the quality of the
alignment inevitably correlates with the quality of
the extracted grammar and the decoder?s output. We
then map the aligned record tokens to their corre-
sponding types, merge adjacent words with the same
type and segment on punctuation (see Figure 3b).
Next, we create the corresponding tree according to
GRSE (Figure 3d) and binarize it. We experimented
both with left and right binarization and adhered to
the latter, as it obtained a more compact set of rules.
Finally, we collectively count the rule weights on the
resulting treebank and extract a rule set, discarding
rules with frequency less than three.
Using the extracted (weighted) GRSE rules, we run
the EM algorithm via inside-outside and learn the
weights for the remaining rules in G. Decoding re-
mains the same as in Konstas and Lapata (2012);
the only requirement is that the extracted grammar
remains binarized in order to guarantee the cubic
1507
desktop1
Click start,
start1
point to settings,
start-target1
and then click control panel.
win-target1
Double-click users and passwords.
contMenu1
On the advanced tab ,p
action-contMenu1
click advanced.
(a) Record token alignments
[
desktop start start-target?win-target?contMenu action-contMenu?
]
(b) Record type segmentation
[Click start,]desktop1.t [point to settings, ]start1.t [and then
click control panel.]start?target1.t [Double-click users and
passwords.]win?target1.t [On the advanced tab,]contMenu1.t [click
advanced.]action?contMenu1.t
(c) Segmentation of text into EDUs
D
SENT(c, a-c)
R(a-c1.t)R(c1.t)
SENT(w-t)
R(w-t1.t)
SENT(d, s, s-t)
R(s-t1.t)R(s1.t)R(d1.t)
(d) Document plan using the GRSE grammar
D
Elaboration[N][S]
Elaboration[N][S]
R(a-c1.t)R(c1.t)
Elaboration[N][S]
R(w-t1.t)Elaboration[N][S]
Joint[N][N]
R(s-t1.t)R(s1.t)
R(d1.t)
(e) Document plan using the GRST grammar
Figure 3: Grammar extraction example from the WINHELP domain using GRSE and GRST . For GRSE , we take
the alignments of records on words and map them to their corresponding types (a); we then segment record
types into sentences (b); and finally, create a tree using grammar GRSE (c). For GRST , we segment the text
into EDUs based on the records they align to (d) and output the discourse tree (omitted here for brevity?s
sake); we build the document plan once we substitute the EDUs with their corresponding record types (e).
bound of the Viterbi search algorithm. Note that the
original grammar is limited to the generation of cat-
egorical and integer values. We extend it to support
the generation of strings. The following rule adds a
simple verbatim lexicalization for string values:
W(r,r. f )? gen str( f .v, i)
gen str( f .v, i) : V ?V, f .v ?V
where V is the set of words for the fields of type
string, and gen str is a function that takes the value
of a string-typed field f .v, and the position i in
the string, and generates the corresponding word at
that position. For example, gen str(users and pass-
words, 3) = passwords. The weight of this rule is set
to 1.
5.2 Planning with Rhetorical Structure Theory
Grammar RST (Mann and Thompson, 1988) is a
theory of text organization which provides a frame-
work for analyzing text. A basic tenet of the the-
ory is that a text consists of hierarchically organized
text spans or elementary discourse units (EDUs) that
stand in various relations to one another (e.g., Elab-
oration, Attribution). These ?rhetorical relations?
hold between two adjacent parts of the text, where
typically, one part is ?nuclear? and one a ?satellite?.
An analysis of a text consists in identifying the re-
lations holding between successively larger parts of
the text, yielding a natural hierarchical description
of the rhetorical organization of the text. From its
very inception, RST was conceived as a way to char-
acterize text and textual relations for the purpose of
text generation.
In order to create a RST-inspired document plan
for our input (i.e., database records paired with
texts), we make the following assumption: each
record corresponds to a unique non-overlapping
span in the collocated text, and can be therefore
mapped to an EDU. Assuming the text has been seg-
mented and aligned to a sequence of records, we
can create a discourse tree with record types (in
place of their corresponding EDUs) as leaf nodes.
Again, we define a sub-grammar GRST which re-
places rules (1)?(2) from Figure 2:
1508
Definition 3 (GRST grammar)
GRST = {?R, NRST , PRST , D}
where ?R is the alphabet of leaf nodes as de-
fined in Section 5.1, NRST is a set of non-terminals
corresponding to rhetorical relations augmented
with nucleus-satellite information (e.g., Elabora-
tion[N][S] stands for the elaboration relation be-
tween the nucleus EDU left-adjoining with the satel-
lite EDU), PRST is the set of production rules of the
form PRST ? NRST ?{NRST ??R}?{NRST ??R} as-
sociated with a weight for each rule, and D ? NRST
is the root symbol. Figure 3e gives the discourse tree
for the database input of Figure 1b, using GRST .
Training In order to obtain the weighted produc-
tions of GRST , we use an existing state-of-the-art dis-
course parser3 (Feng and Hirst, 2012) trained on the
RST-DT corpus (Carlson et al, 2001). The latter
contains a selection of 385 Wall Street Journal arti-
cles which have been annotated using the framework
of RST and an inventory of 78 rhetorical relations,
classified into 18 coarse-grained categories (Carl-
son and Marcu, 2001). Figure 4 gives a comparison
of the distribution of relations extracted for the two
datasets we used, against the gold-standard annota-
tion of RST-DT. The statistics for the RST-DT cor-
pus are taken from Williams and Power (2008). The
relative frequencies of relations on both datasets fol-
low closely the distribution of those in RST-DT, thus
empirically supporting the application of the RST
framework to our data.
We segment each document in our training set
into EDUs based on the record-to-text alignments
given by the model of Liang et al (2009) (see Fig-
ure 3c). We then run the discourse parser on the
resulting EDUs, and retrieve the corresponding dis-
course tree; the internal nodes are labelled with
one of the RST relations. Finally, we replace the
leaf EDUs with their respective terminal symbols
R(r.t) ? ?R (Figure 3e) and collect the resulting
grammar productions; their weights are calculated
via maximum likelihood estimation based on their
collective counts in the parse trees licensed by GRST .
Training and decoding of the extended generation
model (after we embed GRST in the original gram-
mar G) is performed identically to Section 5.1.
3Publicly available from http://www.cs.toronto.edu/
?weifeng/software.html.
6 Experimental Design
Data Since our aim was to evaluate the planning
component of our model, we used datasets whose
documents are at least a few sentences long. Specif-
ically, we generated weather forecasts and trou-
bleshooting guides for an operating system. For
the first domain (henceforth WEATHERGOV) we used
the dataset of Liang et al (2009), which consists
of 29,528 weather scenarios for 3,753 major US
cities (collected over four days). The database has
12 record types, each scenario contains on average
36 records, 5.8 out of which are mentioned in the
text. A document has 29.3 words and is four sen-
tences long. The vocabulary is 345 words. We used
25,000 scenarios from WEATHERGOV for training,
1,000 scenarios for development and 3,528 scenar-
ios for testing.
For the second domain (henceforth WINHELP) we
used the dataset of Branavan et al (2009), which
consists of 128 scenarios. These are articles from
Microsoft?s Help and Support website4 and contain
step-by-step instructions on how to perform tasks on
the Windows 2000 operating system. In its original
format, the database provides a semantic representa-
tion of the textual guide, i.e., it represents the user?s
actions on the operating system?s UI. We semi-
automatically converted this representation into a
schema of records, fields and values, following the
conventions adopted in Branavan et al (2009).5 The
final database has 13 record types. Each scenario has
9.2 records and each document 51.92 words with 4.3
sentences. The vocabulary is 629 words. We per-
formed 10-fold cross-validation on the entire dataset
for training and testing. Compared to WEATHER-
GOV, WINHELP documents are longer with a larger
vocabulary. More importantly, due to the nature of
the domain, i.e., giving instructions, content selec-
tion is critical not only in terms of what to say but
also in what order.
Grammar Extraction and Parameter Setting
We obtained alignments between database records
and textual segments for both domains and gram-
mars (GRSE and GRST ) using the unsupervised model
of Liang et al (2009). On WEATHERGOV, we ex-
tracted a GRSE grammar with 663 rules (after bi-
4support.microsoft.com
5The dataset can be downloaded from http://homepages.
inf.ed.ac.uk/ikonstas/index.php?page=resources
1509
E
la
bo
ra
ti
on
A
tt
ri
bu
ti
on
Jo
in
t
C
on
tr
as
t
E
xp
la
na
ti
on
B
ac
kg
ro
un
d
E
na
bl
em
en
t
C
au
se
E
va
lu
at
io
n
C
om
pa
ri
so
n
C
on
di
ti
on
To
pi
c-
C
om
m
en
t
Te
m
po
ra
l
E
xp
la
na
ti
on
S
um
m
ar
y
To
pi
c
C
ha
ng
e
0
20
40
60 RST-DT
WEATHERGOV
WINHELP
Figure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power,
2008).
narization). The WINHELP dataset is considerably
smaller, and as a result the procedure described in
Section 5.1 yields a very sparse grammar. To al-
leviate this, we horizontally markovized the right-
hand side of each rule (Collins, 1999; Klein and
Manning, 2003).6 After markovization, we obtained
a GRSE grammar with 516 rules. On WEATHERGOV,
we extracted 434 rules for GRST . On WINHELP we
could not follow the horizontal markovization pro-
cedure, since the discourse trees are already bina-
rized. Instead, we performed vertical markovization,
i.e., annotated each non-terminal with their parent
node (Johnson, 1998) and obtained a GRST grammar
with 419 rules. The model of Konstas and Lapata
(2012) has two parameters, namely the number of
k-best lists to keep in each derivation, and the or-
der of the language model. We tuned k experimen-
tally on the development set and obtained best re-
sults with 60 for WEATHERGOV and 120 for WIN-
HELP. We used a trigram model for both domains,
trained on each training set.
Evaluation We compared two configurations of
our system, one with a content planning compo-
nent based on record type sequences (GRSE) and
6When horizontally markovizing, we can encode an arbi-
trary amount of context in the intermediate non-terminals that
result from this process; in our case we store h=1 horizontal
siblings plus the mother left-hand side (LHS) non-terminal, in
order to uniquely identify the Markov chain. For example,
A? B C D becomes A? B ?A . . .B?, ?A . . .B? ? C ?A . . .C?,
?A . . .C? ? D.
another one based on RST (GRST ). In both cases
content plans were extracted from (noisy) unsuper-
vised alignments. As a baseline, we used the orig-
inal model of Konstas and Lapata (2012). We also
compared our model to Angeli et al?s system (2010),
which is state of the art on WEATHERGOV.
System output was evaluated automatically, using
the BLEU modified precision score (Papineni et al,
2002) with the human-written text as reference. In
addition, we evaluated the generated text by eliciting
human judgments. Participants were presented with
a scenario and its corresponding verbalization and
were asked to rate the latter along three dimensions:
fluency (is the text grammatical?), semantic correct-
ness (does the meaning conveyed by the text corre-
spond to the database input?) and coherence (is the
text comprehensible and logically structured?). Par-
ticipants used a five point rating scale where a high
number indicates better performance. We randomly
selected 12 documents from the test set (for each do-
main) and produced output with the system of Kon-
stas and Lapata (2012) (henceforth K&L), our two
models using GRSE and GRST , respectively, and An-
geli et al (2010) (henceforth ANGELI). We also in-
cluded the original text (HUMAN) as gold-standard.
We obtained ratings for 60 (12 ? 5) scenario-text
pairs for each domain. Examples of the documents
shown to the participants are given in Table 1.
The study was conducted over the Internet us-
1510
WEATHERGOV WINHELP
G
R
SE
Showers before noon. Cloudy, with a high near
38. Southwest wind between 3 and 8 mph.
Chance of precipitation is 55 %.
Right-click my network places, and then click prop-
erties. Right-click local area connection, and click
properties. Click to select the file and printer sharing
for Microsoft networks, and then click ok.
G
R
ST
Showers likely. Mostly cloudy, with a high around
38. South wind between 1 and 8 mph. Chance of
precipitation is 55 %.
Right-click my network places, and then click proper-
ties. Right-click local area connection. Click file and
printer sharing for Microsoft networks, and click ok.
K
&
L
A chance of showers. Otherwise, cloudy, with a
high near 38. Southwest wind between 3 and 8
mph.
Right-click my network places, click properties.
Right-click local area connection. Click to select the
file and printer sharing for Microsoft networks, and
then click ok.
A
N
G
E
L
I A chance of rain or drizzle after 9am. Mostly
cloudy, with a high near 38. Southwest wind be-
tween 3 and 8 mph. Chance of precipitation is 50
%.
Right-click my network places, and then click prop-
erties on the tools menu, and then click proper-
ties. Right-click local area connection, and then click
properties. Click file and printer sharing for Microsoft
networks, and then click ok.
H
U
M
A
N A 50 percent chance of showers. Cloudy, with a
high near 38. Southwest wind between 3 and 6
mph.
Right-click my network places, and then click proper-
ties. Right-click local area connection, and then click
properties. Click to select the file and printer sharing
for Microsoft networks check box. Click ok.
Table 1: Human-authored text and system output on WEATHERGOV and WINHELP.
ing Amazon Mechanical Turk7, and involved 200
volunteers (100 for WEATHERGOV, and 100 for
WINHELP), all self reported native English speak-
ers. For WINHELP, we made sure participants were
computer-literate and familiar with the Windows op-
erating system by administering a short question-
naire prior to the experiment.
7 Results
The results of the automatic evaluation are summa-
rized in Table 2. Overall, our models outperform
K&L?s system by a wide margin on both datasets.
The two content planners (GRSE and GRST ) perform
comparably in terms of BLEU. This suggests that
document plans induced solely from data are of sim-
ilar quality to those informed by RST. This is an
encouraging result given that RST-style discourse
parsers are currently available only for English. AN-
GELI performs better on WEATHERGOV possibly due
to better output quality on the surface level. Their
system defines trigger patterns that specifically lexi-
calize record fields containing numbers. In contrast,
on WINHELP it is difficult to explicitly specify such
patterns, as none of the record fields are numeric; as
a result their system performs poorly compared to
7https://www.mturk.com
the other models.
To assess the impact of the alignment on the
content planner, we also extracted GRSE from
cleaner alignments which we obtained automat-
ically via human-crafted heuristics for each do-
main. The heuristics performed mostly anchor
matching between database records and words in the
text (e.g., the value Lkly of the field rainChance,
matches with the string rain likely in the text).
Using these alignments, GRSE obtained a BLEU
score of 39.23 on WEATHERGOV and 41.35 on WIN-
HELP. These results indicate that improved align-
ments would lead to more accurate grammar rules.
WEATHERGOV seems more sensitive to the align-
ments than WINHELP. This is probably because
the dataset shows more structural variations in the
choice of record types at the document level, and
therefore the grammar extracted from the unsuper-
vised alignments is noisier. Unfortunately, perform-
ing this kind of analysis for GRST would require gold
standard segmentation of our training corpus into
EDUs which we neither have nor can easily approx-
imate via heuristics.
The results of our human evaluation study are
shown in Table 3. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system
1511
Model WEATHERGOV WINHELP
GRSE 35.60 40.92
GRST 36.54 40.65
K&L 33.70 38.26
ANGELI 38.40 32.21
Table 2: Automatic evaluation of system output us-
ing BLEU-4.
WEATHERGOV WINHELP
Model FL SC CO FL SC CO
GRSE 4.25 3.75 4.18 3.59 3.21 3.35
GRST 4.10 3.68 4.10 3.45 3.29 3.22
K&L 3.73 3.25 3.59 3.27 2.97 2.93
ANGELI 3.90 3.44 3.82 3.44 2.79 2.97
HUMAN 4.22 3.72 4.11 4.20 4.41 4.25
Table 3: Mean ratings for fluency (FL), semantic
correctness (SC) and coherence (CO) on system out-
put elicited by humans.
type (GRSE , GRST , K&L, ANGELI, and HUMAN) on
fluency, semantic correctness and coherence ratings.
Means differences of 0.2 or more are significant at
the 0.05 level using a post-hoc Tukey test. Interest-
ingly, we observe that document planning improves
system output overall, not only in terms of coher-
ence. Across all dimensions our models are per-
ceived better than K&L and ANGELI. As far as co-
herence is concerned, the two content planners are
rated comparably (differences in the means are not
significant). Both GRSE and GRST are significantly
better than the comparison systems (ANGELI and
K&L). Table 1 illustrates examples of system out-
put along with the gold standard content selection
for reference, for the WEATHERGOV and WINHELP
domains, respectively.
In sum, we observe that integrating document
planning either via GRSE or GRST boosts perfor-
mance. Document plans induced from record
sequences exhibit similar performance, compared
to those generated using expert-derived linguistic
knowledge. Our systems are consistently better than
K&L both in terms of automatic and human eval-
uation and are close or better than the supervised
model of Angeli et al (2010). We also show that
feeding the system with a grammar of better qual-
ity can achieve state-of-the-art performance, without
further changes to the model.
8 Conclusions
In this paper, we have proposed an end-to-end sys-
tem that generates text from database input and cap-
tures all components of the traditional generation
pipeline, including document planning. Document
plans are induced automatically from training data
and are represented intuitively by PCFG rules cap-
turing the structure of the database and the way it
renders to text. We proposed two complementary
approaches to inducing content planners. In a first
linguistically naive approach, a document is mod-
elled as a sequence of sentences and each sentence
as a sequence of records. Our second approach
draws inspiration from Rhetorical Structure Theory
(Mann and Thomson, 1988) and represents a docu-
ment as a tree with intermediate nodes correspond-
ing to discourse relations, and leaf nodes to database
records.
Experiments with both approaches demonstrate
improvements over models that do not incorporate
document planning. In the future, we would like to
tackle more challenging domains, such as NFL re-
caps, financial articles and biographies (Howald et
al., 2013; Schilder et al, 2013). Our models could
also benefit from the development of more sophis-
ticated planners either via grammar refinement or
more expressive grammar formalisms (Cohn et al,
2010).
Acknowledgments
We are grateful to Percy Liang and Gabor Angeli
for providing us with their code and data. Thanks to
Giorgio Satta and Charles Sutton for helpful com-
ments and suggestions. We also thank the members
of the Probabilistic Models reading group at the Uni-
versity of Edinburgh for useful feedback.
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502?512, Cambridge, MA.
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive content selection for concept-to-text generation.
In Proceedings of Human Language Technology and
1512
Empirical Methods in Natural Language Processing,
pages 331?338, Vancouver, British Columbia.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 14(4):431?455.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
82?90, Suntec, Singapore.
L. Carlson and D. Marcu. 2001. Discourse tagging ref-
erence manual. Technical report, Univ. of Southern
California / Information Sciences Institute.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue - Volume 16, SIGDIAL ?01, pages 1?10,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128?135, Helsinki, Finland.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, 11(November):3053?
3096.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Robert Dale. 1988. Generating referring expressions in
a domain of objects and processes. Ph.D. thesis, Uni-
versity of Edinburgh.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society, se-
ries B, 39(1):1?38.
Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In Proceedings of the 39th An-
nual Meeting on Association for Computational Lin-
guistics, pages 172?179.
Pablo A. Duboue and Kathleen R. McKeown. 2002.
Content planner construction via evolutionary algo-
rithms and a corpus-based fitness function. In Pro-
ceedings of International Natural Language Genera-
tion, pages 89?96, Ramapo Mountains, NY.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 60?68, Jeju
Island, Korea.
Eduard Hovy. 1993. Automated discourse generation
using discourse structure relations. Artificial Intelli-
gence, 63:341?385.
Blake Howald, Ravikumar Kondadadi, and Frank
Schilder. 2013. Domain adaptable semantic clustering
in statistical nlg. In Proceedings of the 10th Interna-
tional Conference on Computational Semantics (IWCS
2013) ? Long Papers, pages 143?154, Potsdam, Ger-
many, March. Association for Computational Linguis-
tics.
Mark Johnson. 1998. Pcfg models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613?
632, December.
Nikiforos Karamanis. 2003. Entity Coherence for De-
scriptive Text Structuring. Ph.D. thesis, University of
Edinburgh.
Tadao Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Lab, Bedford, MA.
Rodger Kibble and Richard Power. 2004. Optimising
referential coherence in text generation. Computa-
tional Linguistics, 30(4):401?416.
Joohyun Kim and Raymond Mooney. 2010. Generative
alignment and semantic parsing for learning from am-
biguous supervision. In Proceedings of the 23rd Con-
ference on Computational Linguistics, pages 543?551,
Beijing, China.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423?430. Association for Computa-
tional Linguistics Morristown, NJ, USA.
Ioannis Konstas and Mirella Lapata. 2012. Unsupervised
concept-to-text generation with hypergraphs. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 752?
761, Montre?al, Canada.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 91?99, Suntec, Singapore.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
William C. Mann and Sandra A. Thomson. 1988.
Rhetorical structure theory. Text, 8(3):243?281.
1513
Chris Mellish, Alisdair Knott, Jon Oberlander, and Mick
O?Donnell. 1998. Experiments using stochastic
search for text planning. In Proceedings of Interna-
tional Natural Language Generation, pages 98?107,
New Brunswick, NJ.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian
Davy. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Frank Schilder, Blake Howald, and Ravi Kondadadi.
2013. Gennext: A consolidated domain adaptable nlg
system. In Proceedings of the 14th European Work-
shop on Natural Language Generation, pages 178?
182, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Donia Scott and Clarisse Sieckenius de Souza. 1990.
Getting the message across in RST-based text gener-
ation. In Robert Dale, Chris Mellish, and Michael
Zock, editors, Current Research in Natural Language
Generation, pages 47?73. Academic Press, New York.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of Association for Computational Linguis-
tics, pages 79?86, Barcelona, Spain.
Sandra Williams and Richard Power. 2008. Deriving
rhetorical complexity data from the rst-dt corpus. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), May.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172?179, Rochester, NY.
Daniel H Younger. 1967. Recognition and parsing for
context-free languages in time n3. Information and
Control, 10(2):189?208.
1514
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301?312,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Incremental Semantic Role Labeling with Tree Adjoining Grammar
Ioannis Konstas
?
, Frank Keller
?
, Vera Demberg
?
and Mirella Lapata
?
?: Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
{ikonstas,keller,mlap}@inf.ed.ac.uk
?: Cluster of Excellence Multimodal Computing and Interaction,
Saarland University
vera@coli.uni-saarland.de
Abstract
We introduce the task of incremental se-
mantic role labeling (iSRL), in which se-
mantic roles are assigned to incomplete
input (sentence prefixes). iSRL is the
semantic equivalent of incremental pars-
ing, and is useful for language model-
ing, sentence completion, machine trans-
lation, and psycholinguistic modeling. We
propose an iSRL system that combines
an incremental TAG parser with a seman-
tically enriched lexicon, a role propaga-
tion algorithm, and a cascade of classi-
fiers. Our approach achieves an SRL F-
score of 78.38% on the standard CoNLL
2009 dataset. It substantially outper-
forms a strong baseline that combines
gold-standard syntactic dependencies with
heuristic role assignment, as well as a
baseline based on Nivre?s incremental de-
pendency parser.
1 Introduction
Humans are able to assign semantic roles such as
agent, patient, and theme to an incoming sentence
before it is complete, i.e., they incrementally build
up a partial semantic representation of a sentence
prefix. As an example, consider:
(1) The athlete realized [her
goals]
PATIENT/THEME
were out of reach.
When reaching the noun phrase her goals, the hu-
man language processor is faced with a semantic
role ambiguity: her goals can either be the PA-
TIENT of the verb realize, or it can be the THEME
of a subsequent verb that has not been encoun-
tered yet. Experimental evidence shows that the
human language processor initially prefers the PA-
TIENT role, but switches its preference to the
theme role when it reaches the subordinate verb
were. Such semantic garden paths occur because
human language processing occurs word-by-word,
and are well attested in the psycholinguistic litera-
ture (e.g., Pickering et al., 2000).
Computational systems for performing seman-
tic role labeling (SRL), on the other hand, proceed
non-incrementally. They require the whole sen-
tence (typically together with its complete syntac-
tic structure) as input and assign all semantic roles
at once. The reason for this is that most features
used by current SRL systems are defined globally,
and cannot be computed on sentence prefixes.
In this paper, we propose incremental SRL
(iSRL) as a new computational task that mimics
human semantic role assignment. The aim of an
iSRL system is to determine semantic roles while
the input unfolds: given a sentence prefix and its
partial syntactic structure (typically generated by
an incremental parser), we need to (a) identify
which words in the input participate in the seman-
tic roles as arguments and predicates (the task of
role identification), and (b) assign correct seman-
tic labels to these predicate/argument pairs (the
task of role labeling). Performing these two tasks
incrementally is substantially harder than doing it
non-incrementally, as the processor needs to com-
mit to a role assignment on the basis of incom-
plete syntactic and semantic information. As an
example, take (1): on reaching athlete, the proces-
sor should assign this word the AGENT role, even
though it has not seen the corresponding predicate
yet. Similarly, upon reaching realized, the pro-
cessor can complete the AGENT role, but it should
also predict that this verb also has a PATIENT role,
even though it has not yet encountered the argu-
ment that fills this role. A system that performs
SRL in a fully incremental fashion therefore needs
to be able to assign incomplete semantic roles,
unlike existing full-sentence SRL models.
The uses of incremental SRL mirror the applica-
tions of incremental parsing: iSRL models can be
used in language modeling to assign better string
probabilities, in sentence completion systems to
301
provide semantically informed completions, in
any real time application systems, such as dia-
log processing, and to incrementalize applications
such as machine translation (e.g., in speech-to-
speech MT). Crucially, any comprehensive model
of human language understanding needs to com-
bine an incremental parser with an incremental se-
mantic processor (Pad?o et al., 2009; Keller, 2010).
The present work takes inspiration from the
psycholinguistic modeling literature by proposing
an iSRL system that is built on top of a cogni-
tively motivated incremental parser, viz., the Psy-
cholinguistically Motivated Tree Adjoining Gram-
mar parser of Demberg et al. (2013). This parser
includes a predictive component, i.e., it predicts
syntactic structure for upcoming input during in-
cremental processing. This makes PLTAG par-
ticularly suitable for iSRL, allowing it to predict
incomplete semantic roles as the input string un-
folds. Competing approaches, such as iSRL based
on an incremental dependency parser, do not share
this advantage, as we will discuss in Section 4.3.
2 Related Work
Most SRL systems to date conceptualize seman-
tic role labeling as a supervised learning prob-
lem and rely on role-annotated data for model
training. Existing models often implement a
two-stage architecture in which role identification
and role labeling are performed in sequence. Su-
pervised methods deliver reasonably good perfor-
mance with F-scores in the low eighties on stan-
dard test collections for English (M`arquez et al.,
2008; Bj?orkelund et al., 2009).
Current approaches rely primarily on syntactic
features (such as path features) in order to iden-
tify and label roles. This has been a mixed bless-
ing as the path from an argument to the predi-
cate can be very informative but is often quite
complicated, and depends on the syntactic formal-
ism used. Many paths through the parse tree are
likely to occur infrequently (or not at all), result-
ing in very sparse information for the classifier to
learn from. Moreover, as we will discuss in Sec-
tion 4.4, such path information is not always avail-
able when the input is processed incrementally.
There is previous SRL work employing Tree Ad-
joining Grammar, albeit in a non-incremental set-
ting, as a means to reduce the sparsity of syntax-
based features. Liu and Sarkar (2007) extract a
rich feature set from TAG derivations and demon-
strate that this improves SRL performance.
In contrast to incremental parsing, incremental
semantic role labeling is a novel task. Our model
builds on an incremental Tree Adjoining Gram-
mar parser (Demberg et al., 2013) which predicts
the syntactic structure of upcoming input. This al-
lows us to perform incremental parsing and incre-
mental SRL in tandem, exploiting the predictive
component of the parser to assign (potentially in-
complete) semantic roles on a word-by-word ba-
sis. Similar to work on incremental parsing that
evaluates incomplete trees (Sangati and Keller,
2013), we evaluate the incomplete semantic struc-
tures produced by our model.
3 Psycholinguistically Motivated TAG
Demberg et al. (2013) introduce Psycholin-
guistically Motivated Tree Adjoining Grammar
(PLTAG), a grammar formalism that extends stan-
dard TAG (Joshi and Schabes, 1992) in order to
enable incremental parsing. Standard TAG as-
sumes a lexicon of elementary trees, each of
which contains at least one lexical item as an an-
chor and at most one leaf node as a foot node,
marked with A?. All other leaves are marked with
A? and are called substitution nodes. Elementary
trees that contain a foot node are called auxiliary
trees; those that do not are called initial trees. Ex-
amples for TAG elementary trees are given in Fig-
ure 1a?c.
To derive a TAG parse for a sentence, we start
with the elementary tree of the head of the sen-
tence and integrate the elementary trees of the
other lexical items of the sentence using two oper-
ations: adjunction at an internal node and substi-
tution at a substitution node (the node at which the
operation applies is the integration point). Stan-
dard TAG derivations are not guaranteed to be in-
cremental, as adjunction can happen anywhere in
a sentence, possibly violating left-to-right process-
ing order. PLTAG addresses this limitation by in-
troducing prediction trees, elementary trees with-
out a lexical anchor. These can be used to predict
syntactic structure anchored by words that appear
later in an incremental derivation. The use of pre-
diction trees ensures that fully connected prefix
trees can be built for every prefix of the input sen-
tence.
Each node in a prediction tree carries mark-
ers to indicate that this node was predicted, rather
than being anchored by the current sentence pre-
fix. An example is Figure 1d, which contains a
prediction tree with marker ?1?. In PLTAG, mark-
ers are eliminated through a new operation called
verification, which matches them with the nodes
302
(a) NP
NNS
Banks
(b) S
VP
VB
open
NP?
(c) VP
VP*AP
RB
rarely
(d) S
1
VP
1
1
NP
1
?
Figure 1: PLTAG lexicon entries: (a) and (b) ini-
tial trees, (c) auxiliary tree, (d) prediction tree.
a
S
 B? 
 C? 
a
S
B
 C? 
b
a
S
 B? 
C
c
(a) valid (b) invalid
Figure 3: The current fringe (dashed line) indi-
cates where valid substitutions can occur. Other
substitutions result in an invalid prefix tree.
of non-predictive elementary trees. An example
of a PLTAG derivation is given in Figure 2. In
step 1, a prediction tree is introduced through sub-
stitution, which then allows the adjunction of an
adverb in step 2. Step 3 involves the verification
of the marker introduced by the prediction tree
against the elementary tree for open.
In order to efficiently parse PLTAG, Demberg
et al. (2013) introduce the concept of fringes.
Fringes capture the fact that in an incremental
derivation, a prefix tree can only be combined with
an elementary tree at a limited set of nodes. For
instance, the prefix tree in Figure 3 has two substi-
tution nodes, for B and C. However, only substi-
tution into B leads to a valid new prefix tree; if we
substitute into C, we obtain the tree in Figure 3b,
which is not a valid prefix tree (i.e., it represents a
non-incremental derivation).
The parsing algorithm proposed by Demberg
et al. (2013) exploits fringes to tabulate interme-
diate results. It manipulates a chart in which each
cell (i, f ) contains all the prefix trees whose first
i leaves are the first i words and whose current
fringe is f . To extend the prefix trees for i to
the prefix trees for i+ 1, the algorithm retrieves
all current fringes f such that the chart has entries
in the cell (i, f ). For each such fringe, it needs
to determine the elementary trees in the lexicon
that can be combined with f using substitution or
adjunction. In spite of the large size of a typi-
cal TAG lexicon, this can be done efficiently, as
it only requires matching the current fringes. For
each match, the parser then computes the new pre-
Banks refused to open today
A0
A1
A1
AM-TMP
nsbj aux
xcomp
tmod
?A0,Banks,refused?
?A1,to,refused?
?A1,Banks,open?
?AM-TMP,today,open?
Figure 4: Syntactic dependency graph with se-
mantic role annotation and the accompanying se-
mantic triples, for Banks refused to open today.
fix trees and its new current fringe f
?
and enters it
into cell (i+1, f
?
).
Demberg et al. (2013) convert the Penn Tree-
bank (Marcus et al., 1993) into TAG for-
mat by enriching it with head information and
argument/modifier information from Propbank
(Palmer et al., 2005). This makes it possible
to decompose the Treebank trees into elementary
trees as proposed by Xia et al. (2000). Predic-
tion trees can be learned from the converted Tree-
bank by calculating the connection path (Mazzei
et al., 2007) at each word in a tree. Intuitively,
a prediction tree for word w
n
contains the struc-
ture that is necessary to connect w
n
to the prefix
tree w
1
. . .w
n?1
, but is not part of any of the ele-
mentary trees of w
1
. . .w
n?1
. Using this lexicon, a
probabilistic model over PLTAG operations can be
estimated following Chiang (2000).
4 Model
4.1 Problem Formulation
In a typical semantic role labeling scenario, the
goal is to first identify words that are predicates
in the sentence and then identify and label all the
arguments for each predicate. This translates into
spotting specific words in a sentence that repre-
sent the predicate?s arguments, and assigning pre-
defined semantic role labels to them. Note that in
this work we focus on verb predicates only. The
output of a semantic role labeler is a set of seman-
tic dependency triples ?l,a, p?, with l ? R , and
a, p ? w, where R is a set of semantic role labels
denoting a specific relationship between a predi-
cate and an argument (e.g., ARG0, ARG1, ARGM
in Propbank), w is the list of words in the sentence,
l denotes a specific role label, a the argument, and
p the predicate. An example is shown in Figure 4.
As discussed in the introduction, standard se-
mantic role labelers make their decisions based on
evidence from the whole sentence. In contrast, our
aim is to assign semantic roles incrementally, i.e.,
303
NP
NNS
Banks
S
1
VP
1
1
NP
NNS
Banks
S
1
VP
1
VP
1
AP
RB
rarely
NP
NNS
Banks
S
VP
VP
VB
open
AP
RB
rarely
NP
NNS
Banks
1. subst
2. adj
3. verif
Figure 2: Incremental parse for Banks rarely open using the operations substitution (with a prediction
tree), adjunction, and verification.
we want to produce a set of (potentially incom-
plete) semantic dependency triples for each prefix
of the input sentence. Note that not every word
is an argument to a predicate, therefore the set of
triples will not necessarily change at every input
word. Furthermore, the triples themselves may
be incomplete, as either the predicate or the argu-
ment may not have been observed yet (predicate-
incomplete or argument-incomplete triples).
Our iSRL system relies on PLTAG, using a se-
mantically augmented lexicon. We parse an in-
put sentence incrementally, applying a novel in-
cremental role propagation algorithm (IRPA) that
creates or updates existing semantic triple candi-
dates whenever an elementary (or prediction) tree
containing role information is attached to the ex-
isting prefix tree. As soon as a triple is completed
we apply a two-stage classification process, that
first identifies whether the predicate/argument pair
is a good candidate, and then disambiguates role
labels in case there is more than one candidate.
4.2 Semantic Role Lexicon
Recall that Propbank is used to construct the
PLTAG treebank, in order to distinguish between
arguments and modifiers, which result in elemen-
tary trees with substitution nodes, and auxiliary
trees, i.e., trees with a foot node, respectively (see
Figure 1). Conveniently, we can use the same in-
formation to also enrich the extracted lexicon with
the semantic role annotations, following the pro-
cess described by Sayeed and Demberg (2013).
1
For arguments, annotations are retained on the
substitution node in the parental tree, while for
modifiers, the role annotation is displayed on the
foot node of the auxiliary tree. Note that we dis-
play role annotation on traces that are leaf nodes,
1
Contrary to Sayeed and Demberg (2013) we put role la-
bel annotations for PPs on the preposition rather than their
NP child, following of the CoNLL 2005 shared task (Carreras
and M`arquez, 2005).
which enables us to recover long-range dependen-
cies (third and fifth tree in Figure 5a). Likewise,
we annotate prediction trees with semantic roles,
which enables our system to predict upcoming in-
complete triples.
Our annotation procedure unavoidably intro-
duces some role ambiguity, especially for fre-
quently occurring trees. This can give rise to two
problems when we generate semantic triples incre-
mentally: IRPA tends to create many spurious can-
didate semantic triples for elementary trees that
correspond to high frequency words (e.g., preposi-
tions or modals). Secondly, a semantic triple may
be identified correctly but is assigned several role
labels. (See the elementary tree for refuse in Fig-
ure 5a.) We address these issues by applying clas-
sifiers for role label disambiguation at every pars-
ing operation (substitution, adjunction, or verifica-
tion), as detailed in Section 4.4.
4.3 Incremental Role Propagation Algorithm
The main idea behind IRPA is to create or up-
date existing semantic triples as soon as there is
available role information during parsing. Our al-
gorithm (lines 1?6 in Algorithm 1) is applied af-
ter every PLTAG parsing operation, i.e., when an
elementary or prediction tree T is adjoined to a
particular integration point node pi
ip
of the prefix
tree of the sentence, via substitution or adjunction
(lines 3?4).
2
In case an elementary tree T
v
verifies
a prediction tree T
pr
(lines 5?6), the same method-
ology applies, the only difference being that we
have to tackle multiple integration point nodes
T
pr,ip
, one for each prediction marker of T
pr
that
matches the corresponding nodes in T
v
.
For simplicity of presentation, we will use a
concrete example, see Figure 5. Figure 5a shows
the lexicon entries for the words of the sentence
2
Prediction tree T
pr
in our algorithm is only used during
verification, so it set to nil for substitution and adjunction op-
erations.
304
Banks refused to open. Naturally, some nodes in
the lexicon trees might have multiple candidate
role labels. For example, the substitution NP node
of the second tree takes two labels, namely A0
and A1. These stem from different role signatures
when the same elementary tree occurs in differ-
ent contexts during training (A1 only on the NP;
A0 on the NP and A1 on S). For simplicity?s sake,
we collapse different signatures, and let a classi-
fier labeller to disambiguate such cases (see Sec-
tion 4.4).
Algorithm 1 Incremental Role Propagation Alg.
1: procedure IRPA(pi
ip
, T , T
pr
)
2: ??? . ? is a dictionary of (pi
ip
, ?l,a, p?) pairs
3: if parser operation is substitution or adjunction then
4: CREATE-TRIPLES(pi
ip
, T )
5: else if parser operation is verification then
6: CREATE-TRIPLES-VERIF(pi
ip
, T , T
pr
)
return set of triples ?l,a, p? for prefix tree pi
7: procedure CREATE-TRIPLES(pi
ip
, T )
8: if HAS-ROLES(pi
ip
) then
9: UPDATE-TRIPLE(pi
ip
, T )
10: else if HAS-ROLES(T ) then
11: T
ip
? substitution or foot node of T
12: ADD-TRIPLE(pi
ip
, T
ip
, T )
13: for all remaining nodes n ? T with roles do
14: ADD-TRIPLE(pi
ip
, n, T ) . incomplete triples
15: procedure CREATE-TRIPLES-VERIF(pi
ip
, T
v
, T
pr
)
16: if HAS-ROLES(T
v
) then
17: anchor? lexeme of T
v
18: for all T
ip
? node in T
v
with role do
19: T
pr,ip
? matching node of T
ip
in T
pr
20: CREATE-TRIPLES(T
pr,ip
, T
v
)
. Process the rest of covered nodes in T
pr
with roles
21: for all remaining T
pr,ip
? node in T
pr
with role do
22: UPDATE-TRIPLE(T
pr,ip
, T
pr
)
23: function UPDATE-TRIPLE(pi
ip
, T )
24: dep? FIND-INCOMPLETE(?, T
ip
)
25: anchor? lexeme of T
26: if anchor of T is predicate then
27: SET-PREDICATE(dep, anchor)
28: else if anchor of T is argument then
29: SET-ARGUMENT(dep, anchor)
return dep
30: procedure ADD-TRIPLE(pi
ip
, T
ip
, T )
31: dep? ?[roles of T
ip
], nil, nil?
32: anchor? lexeme of T
33: if anchor of T is predicate then
34: SET-PREDICATE(dep, anchor)
35: SET-ARGUMENT(dep, head of pi
ip
)
36: else if anchor of T is argument then
37: if T is auxiliary then . adjunction
38: SET-ARGUMENT(dep, anchor)
39: else . substitution: arg is head of prefix tree
40: SET-ARGUMENT(dep, head of T
ip
)
41: pred? find dep ? ? with matching pi
ip
42: SET-PREDICATE(dep, pred)
43: ?? (pi
ip
, dep)
Once we process Banks, the prefix tree becomes
the lexical entry for this word, see the first col-
umn of Figure 5b. Next, we process refused:
the parser substitutes the prefix tree into the ele-
mentary tree T of refused;
3
the integration point
pi
ip
on the prefix tree is the topmost NP. Since
the operation is a substitution (line 3), we create
triples between T and pi
ip
via CREATE-TRIPLES
(lines 7?12). pi
ip
does not have any role infor-
mation (line 8), so we proceed to add a new se-
mantic triple between the role-labeled integration
point T
ip
, i.e., substitution NP node of T , and pi
ip
,
via ADD-TRIPLE (lines 30?43). First, we create
an incomplete semantic triple with all roles from
T
ip
(line 31). Then we set the predicate to the an-
chor of T to be the word refused, and the argu-
ment to be the head word of the prefix tree, Banks
(lines 34?35). Note that predicate identification is
a trivial task based on part-of-speech information
in the elementary tree.
4
Then, we add the pair (NP? ?{A0,A1},Banks,
refused?) to a dictionary (line 43). Storing the in-
tegration point along with the semantic triple is
essential, to be able to recover incomplete triples
in later stages of the algorithm. Finally, we re-
peat this process for all remaining nodes on T that
have roles, in our example the substitution node S
(lines 13?14). This outputs an incomplete triple,
?{A1},nil,refused?.
Next, the parser decides to substitute a predic-
tion tree (third tree in Figure 5a) into the substitu-
tion node S of the prefix tree. Since the integration
point is on the prefix tree and has role information
(line 8), the corresponding triple should already be
present in our dictionary. Upon retrieving it, we
set the nil argument to the anchor of the incoming
tree. Since it is a prediction tree, we set it to the
root of the tree, namely S
2
(phrase labels in triples
are denoted by italics), but mark the triple as yet
incomplete. This distinction allows us to fill in the
correct lexical information once it becomes avail-
able, i.e, when the tree gets verified. We also add
an incomplete triple for the trace t in the subject
position of the prediction tree, as described above.
Note that this triple contains multiple roles; this is
expected given that prediction trees are unlexical-
ized and occur in a wide variety of contexts.
When the next verb arrives, the parser success-
fully verifies it against the embedded prediction
3
PLTAG parsing operations can occur in two ways: An
elementary tree can be substituted into the substitution node
of the prefix tree, or the prefix tree can be substituted into a
node of an elementary tree. The same holds for adjunction.
4
Most predicates can be identified as anchors of non-
modifier auxiliary trees. However, there are exceptions to
this rule, i.e., modifier auxiliary trees and non-modifier non-
auxiliary trees being also verbs in our lexicon, hence the use
of the more reliable POS tags.
305
IRPA MaltParser
Banks ? ?
refused ?{A0,A1},Banks,refused?,
?A1,S
2
,refused?,
?{A0,A1,A2},t,nil?
?A0,Banks,refused?
to ? ?
open ?A1,to,refused?,
?A1,Banks,open?
?A1,to,refused?,
?A0,Banks,open?
today ?AM-TMP,today,open? ?AM-TMP,today,open?
Table 1: Complete and incomplete semantic triple
generation, comparing IRPA and a system that
maps gold-standard role labels onto MaltParser in-
cremental dependencies for Figure 4.
tree within the prefix tree (last step of Figure 5b).
Our algorithm first cycles through all nodes that
match between the verification tree T
v
and the pre-
diction tree T
pr
and will complete or create new
triples via CREATE-TRIPLES (lines 18?20). In
our example, the second semantic triple gets com-
pleted by replacing S
2
with the head of the sub-
tree rooted in S. Normally, this would be the verb
open, but in this case the verb is followed by the
infinitive marker to, hence we heuristically set it
to be the argument of the triple instead, following
Carreras and M`arquez (2005). For the last triple,
we set the predicate to the anchor of T
v
open, and
now are able to remove the excess role labels A0
and A2. This illustrated how the lexicalized veri-
fication tree disambiguates the semantic informa-
tion stored in the prediction tree. Finally, trace t is
set to the closest NP head that is below the same
phrase subtree, in this case Banks. Note that Banks
is part of two triples as shown in the last tree of
Figure 5b: it is either an A0 or an A1 for refused
and an A1 for open.
We are able to create incomplete semantic
triples after the prediction of the upcoming verb at
step 2, as shown in Figure 5b. This is not possible
using an incremental dependency parser such as
MaltParser (Nivre et al., 2007) that lacks a predic-
tive component. Table 1 illustrates this by compar-
ing the output of IRPA for Figure 5b with the out-
put of a baseline system that maps role labels onto
the syntactic dependencies in Figure 4, generated
incrementally by MaltParser (see Section 5.3 for
a description of the MaltParser baseline). Malt-
Parser has to wait for the verb open before out-
putting the relevant semantic triples. In contrast,
IRPA outputs incomplete triples as soon as the in-
formation is available, and later on updates its de-
cision. (MaltParser also incorrectly assigns A0 for
the Banks?open pair.)
4.4 Argument Identification and Role Label
Disambiguation
IRPA produces semantic triples for every role an-
notation present in the lexicon entries, which will
often overgenerate role information. Furthermore,
some triples have more than one role label at-
tached to them. During verification, we are able to
filter out the majority of labels in the correspond-
ing prediction trees; However, most triples are cre-
ated via substitution and adjunction.
In order to address these problems we adhere to
the following classification and ranking strategy:
after each semantic triple gets completed, we per-
form a binary classification that evaluates its suit-
ability as a whole, given bilexical and syntactic in-
formation. If the triple is identified as a good can-
didate, then we perform multi-class classification
over role labels: we feed the same bilexical and
syntactic information to a logistic classifier, and
get a ranked list of labels. We then use this list to
re-rank the existing ambiguous role labels in the
semantic triple, and output the top scoring ones.
The identifier is a binary L2-loss support vec-
tor classifier, and the role disambiguator an L2-
regularized logistic regression classifier, both im-
plemented using the efficient LIBLINEAR frame-
work of Fan et al. (2008). The features used are
based on Bj?orkelund et al. (2009) and Liu and
Sarkar (2007), and are listed in Table 2.
The bilexical features are: predicate POS tag,
predicate lemma, argument word form, argument
POS tag, and position. The latter indicates the po-
sition of the argument relative to the predicate, i.e.,
before, on, or after. The syntactic features are:
the predicate and argument elementary trees with-
out the anchors (to avoid sparsity), the category of
the integration point node on the prefix tree where
the elementary tree of the argument attaches to,
an alphabetically ordered set of the categories of
the fringe nodes of the prefix tree after attaching
the argument tree, and the path of PLTAG opera-
tions applied between the argument and the pred-
icate. Note that most of the original features used
by Bj?orkelund et al. (2009) and others are not ap-
plicable in our context, as they exploit information
that is not accessible incrementally. For example,
sibling information to the right of the word is not
available. Furthermore, our PLTAG parser does
not compute syntactic dependencies, hence these
cannot serve as features (and in any case not all
dependencies are available incrementally, see Fig-
ure 4). To counterbalance this, we use local syn-
tactic information stored in the fringe of the pre-
306
NP
NNS
Banks
S
VP
S?
{A1}
VP
VBD
refused
NP?
{A0,A1}
S
2
VP
2
2
VB
2
2
NP
2
1
t
1
1
{A0,A1,A2}
VP
VP?TO
to
S
VP
VB
open
NP
t
{A1}
(a) Lexicon entries
NP
NNS
Banks
S
VP
S?
{A1}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
2
{A1}
VP
2
2
VB
2
2
NP
2
1
t
1
1
{A0,A1,A2}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
2
{A1}
VP
2
VP
2
VB
2
2
TO
to
NP
2
1
t
1
1
{A0,A1,A2}
VP
VBD
refused
NP
NNS
Banks
{A0,A1}
S
VP
S
VP
VP
VB
open
TO
to
{A1}
NP
t
VP
VBD
refused
NP
NNS
Banks
{A0,A1}/{A1}
1. subst 2. subst
3. adj
4. verif
1. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,nil,refused?
2. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,S
2
,refused?
NP ? ?{A0,A1,A2},t,nil?
3. ?
4. NP ? ?{A0,A1},Banks,refused?
S ? ?A1,to,refused?
NP ? ?A1,Banks,open?
(b) Incremental parsing using PLTAG and incremental propagation of roles
Figure 5: Incremental Role Propagation Algorithm application for the sentence Banks refused to open.
Bilexical Syntactic
PredPOS PredElemTree
PredLemma ArgElemTree
ArgWord IntegrationPoint
ArgPOS PrefixFringe
Position OperationPath
Table 2: Features for argument identification and
role label disambiguation.
fix tree. We also store the series of operations ap-
plied by our parser between argument and predi-
cate, in an effort to emulate the effect of recover-
ing longer-range patterns.
5 Experimental Design
5.1 PLTAG and Classifier Training
We extracted the semantically-enriched lexicon
and trained the PLTAG parser by converting the
Wall Street Journal part of Penn Treebank to
PLTAG format. We used Propbank to retrieve
semantic role annotation, as described in Sec-
tion 4.2. We trained the PLTAG parser according
to Demberg et al. (2013) and evaluated the parser
on section 23, on sentences with 40 words or less,
given gold POS tags for each word, and achieved
a labeled bracket F
1
score of 79.41.
In order to train the argument identification and
role label disambiguation classifiers, we used the
English portion of the CoNLL 2009 Shared Task
(Haji?c et al., 2009; Surdeanu et al., 2008). It
consists of the Penn Treebank, automatically con-
verted to dependencies following Johansson and
307
Nugues (2007), accompanied by semantic role la-
bel annotation for every argument pair. The latter
is converted from Propbank based on Carreras and
M`arquez (2005). We extracted the bilexical fea-
tures for the classifiers directly from the gold stan-
dard annotation of the training set. The syntactic
features were obtained as follows: for every sen-
tence in the training set we applied IRPA using the
trained PLTAG parser, with gold standard lexicon
entries for each word of the input sentence. This
ensures near perfect parsing accuracy. Then for
each semantic triple predicted incrementally, we
extracted the relevant syntactic information in or-
der to construct training vectors. If the identified
predicate-argument pair was in the gold standard
then we assigned a positive label for the identifi-
cation classifier, otherwise we flagged it as nega-
tive. For those pairs that are not identified by IRPA
but exist in the gold standard (false negatives), we
extracted syntactic information from already iden-
tified similar triples, as follows: We first look for
correctly identified arguments, wrongly attached
to a different predicate and re-create the triple with
correct predicate/argument information. If no ar-
gument is found, we then pick the argument in the
list of identified arguments for a correct predicate
with the same POS-tag as the gold-standard argu-
ment. In the case of the role label disambigua-
tion classifier we just assign the gold label for ev-
ery correctly identified pair, and ignore the (possi-
bly ambiguous) predicted one. After tuning on the
development set, the argument identifier achieved
an accuracy of 92.18%, and the role label disam-
biguation classifier, 82.37%.
5.2 Evaluation
The focus of this paper is to build a system that is
able to output semantic role labels for predicate-
argument pairs incrementally, as soon as they be-
come available. In order to properly evaluate such
a system, we need to measure its performance in-
crementally. We propose two different cumulative
scores for assessing the (possibly incomplete) se-
mantic triples that have been created so far, as the
input is processed from left to right, per word. The
first metric is called Unlabeled Prediction Score
(UPS) and gets updated for every identified argu-
ment or predicate, even if the corresponding se-
mantic triple is incomplete. Note that UPS does
not take into account the role label, it only mea-
sures predicate and argument identification. In this
respect it is analogous to unlabeled dependency
accuracy reported in the parsing literature. We ex-
pect a model that is able to predict semantic roles
to achieve an improved UPS result compared to a
system that does not do prediction, as illustrated in
Table 1. Our second score, Combined Incremental
SRL Score (CISS), measures the identification of
complete semantic role triples (i.e., correct predi-
cate, predicate sense, argument, and role label) per
word; by the end of the sentence, CISS coincides
with standard combined SRL accuracy, as reported
in CoNLL 2009 SRL-only task. This score is anal-
ogous to labeled dependency accuracy in parsing.
Note that conventional SRL systems such as
Bj?orkelund et al. (2009) typically assume gold
standard syntactic information. In order to emu-
late this, we give our parser gold standard lexicon
entries for each word in the test set; these contain
all possible roles observed in the training set for
a given elementary tree (and all possible senses
for each predicate). This way the parser achieves
a syntactic parsing F
1
score of 94.24, thus ensur-
ing the errors of our system can be attributed to
IRPA and the classifiers. Also note that we evalu-
ate on verb predicates only, therefore trivially re-
ducing the task of predicate identification to the
simple heuristic of looking for words in the sen-
tence with a verb-related POS tag and excluding
auxiliaries and modals. Likewise, predicate sense
disambiguation on verbs presumably is trivial, as
we observed almost no ambiguity of senses among
lexicon entries of the same verb (we adhered to a
simple majority baseline, by picking the most fre-
quent sense, given the lexeme of the verb, in the
few ambiguous cases). It seems that the syntactic
information held in the elementary trees discrimi-
nates well among different senses.
5.3 System Comparison
We evaluated three configurations of our system.
The first configuration (iSRL) uses all seman-
tic roles for each PLTAG lexicon entry, applies
the PLTAG parser, IRPA, and both classifiers to
perform identification and disambiguation, as de-
scribed in Section 4. The second one (Majority-
Baseline), solves the problem of argument identifi-
cation and role disambiguation without the classi-
fiers. For the former we employ a set of heuristics
according to Lang and Lapata (2014), that rely on
gold syntactic dependency information, sourced
from CoNLL input. For the latter, we choose the
most frequent role given the gold standard depen-
dency relation label for the particular argument.
Note that dependencies have been produced in
view of the whole sentence and not incrementally.
308
System Prec Rec F1
iSRL-Oracle 91.00 80.26 85.29
iSRL 81.48 75.51 78.38
Majority-Baseline 71.05 58.10 63.92
Malt-Baseline 60.90 46.14 52.50
Table 3: Full-sentence combined SRL score
This gives the baseline a considerable advantage
especially in case of longer range dependencies.
The third configuration (iSRL-Oracle), is identical
to iSRL, but uses the gold standard roles for each
PLTAG lexicon entry, and thus provides an upper-
bound for our methodology. Finally, we evalu-
ated against Malt-Baseline, a variant of Majority-
Baseline that uses the MaltParser of Nivre et al.
(2007) to provide labeled syntactic dependencies
MaltParser is a state-of-the-art shift-reduce depen-
dency parser which uses an incremental algorithm.
Following Beuck et al. (2011), we modified the
parser to provide intermediate output at each word
by emitting the current state of the dependency
graph before each shift step. We trained Malt-
Parser using the arc-eager algorithm (which out-
performed the other parsing algorithms available
with MaltParser) on the CoNLL dataset, achiev-
ing a labeled dependency accuracy of 89.66% on
section 23.
6 Results
Figures 6 and 7 show the results on the incremen-
tal SRL task. We plot the F
1
for Unlabeled Predic-
tion Score (UPS) and Combined Incremental SRL
Score (CISS) per word, separately for sentences
of lengths 10, 20, 30, and 40 words. The task gets
harder with increasing sentence length, hence we
can only meaningfully compare the average scores
for sentence of the same length. (This approach
was proposed by Sangati and Keller 2013 for eval-
uating the performance of incremental parsers.)
The UPS results in Figure 6 clearly show that
our system (iSRL) outperforms both baselines
on unlabeled argument and predicate prediction,
across all four sentence lengths. Furthermore,
we note that the iSRL system achieves a near-
constant performance for all sentence prefixes.
Our PLTAG-based prediction/verification archi-
tecture allows us to correctly predict incomplete
semantic role triples, even at the beginning of the
sentence. Both baselines perform worse than the
iSRL system in general. Moreover, the Malt-
Baseline performs badly on the initial sentence
prefixes (up to word 10), presumably as it does
not benefit from syntactic prediction, and thus can-
not generate incomplete triples early in the sen-
tence, as illustrated in Table 1. The Majority-
Baseline also does not do prediction, but it has ac-
cess to gold-standard syntactic dependencies, and
thus outperforms the Malt-Baseline on initial sen-
tence prefixes. Note that due to prediction, our
system tends to over-generate incomplete triples
in the beginning of sentences, compared to non-
incremental output, which may inflate UPS for
the first words. However, this cancels out later
in the sentence if triples are correctly completed;
failure to do so would decrease UPS. The near-
constant performance of our output illustrates this
phenomenon. Finally, the iSRL-Oracle outper-
forms all other systems, as it benefits from correct
role labels and correct PLTAG syntax, thus provid-
ing an upper limit on performance.
The CISS results in Figure 7 present a simi-
lar picture. Again, the iSRL system outperforms
both baselines at all sentence lengths. In addition,
it shows particularly strong performance (almost
at the level of the iSRL-Oracle) at the beginning
of the sentence. This presumably is due to the
fact that our system uses prediction and is able to
identify correct semantic role triples earlier in the
sentence. The baselines also show higher perfor-
mance early in the sentence, but to a lesser degree.
Table 3 reports traditional combined SRL scores
for full sentences over all sentence lengths, as
defined for the CoNLL task. Our iSRL system
outperforms the Majority-Baseline by almost 15
points, and the Malt-Baseline by 25 points. It re-
mains seven points below the iSRL-Oracle upper
limit.
Finally, in order to test the effect of syntactic
parsing on our system, we also experimented with
a variant of our iSRL system that utilizes all lex-
icon entries for each word in the test set. This is
similar to performing the CoNLL 2009 joint task,
which is designed for systems that carry out both
syntactic parsing and semantic role labeling. This
variant achieved a full sentence F-score of 68.0%,
i.e., around 10 points lower than our iSRL system.
This drop in score correlates with the difference
in syntactic parsing F-score between the two ver-
sions of PLTAG parser (94.24 versus 79.41), and
is expected given the high ambiguity of the lex-
icon entries for each word. Note, however, that
the full-parsing version of our system still outper-
forms Malt-Baseline by 15 points.
309
2 4 6 8 10
0.2
0.4
0.6
0.8
1
words
F
1
(a) 10 words
5 10 15 20
0.2
0.4
0.6
0.8
1
words
F
1
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
(b) 20 words
5 10 15 20 25 30
0.2
0.4
0.6
0.8
1
words
F
1
(c) 30 words
10 20 30 40
0.2
0.4
0.6
0.8
1
words
F
1
(d) 40 words
Figure 6: Unlabeled Prediction Score (UPS)
2 4 6 8 10
0.2
0.4
0.6
0.8
1
words
F
1
(a) 10 words
5 10 15 20
0.2
0.4
0.6
0.8
1
words
F
1
iSRL-Oracle iSRL
Majority-Baseline Malt-Baseline
(b) 20 words
5 10 15 20 25 30
0.2
0.4
0.6
0.8
1
words
F
1
(c) 30 words
10 20 30 40
0.2
0.4
0.6
0.8
1
words
F
1
(d) 40 words
Figure 7: Combined iSRL Score (CISS)
7 Conclusions
In this paper, we introduced the new task of incre-
mental semantic role labeling and proposed a sys-
tem that solves this task by combining an incre-
mental TAG parser with a semantically enriched
lexicon, a role propagation algorithm, and a cas-
cade of classifiers. This system achieved a full-
sentence SRL F-score of 78.38% on the standard
CoNLL dataset. Not only is the full-sentence
score considerably higher than the Majority-
Baseline (which is a strong baseline, as it uses
gold-standard syntactic dependencies), but we
also observe that our iSRL system performs well
incrementally, i.e., it predicts both complete and
incomplete semantic role triples correctly early on
in the sentence. We attributed this to the fact that
our TAG-based architecture makes it possible to
predict upcoming syntactic structure together with
the corresponding semantic roles.
Acknowledgments
EPSRC support through grant EP/I032916/1 ?An
integrated model of syntactic and semantic predic-
tion in human language processing? to FK and ML
is gratefully acknowledged.
References
Beuck, Niels, Arne Khn, and Wolfgang Menzel.
2011. Incremental parsing and the evaluation
of partial dependency analyses. In Proceedings
of the 1st International Conference on Depen-
dency Linguistics. Depling 2011.
Bj?orkelund, Anders, Love Hafdell, and Pierre
Nugues. 2009. Multilingual semantic role la-
beling. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language
Learning: Shared Task. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
CoNLL ?09, pages 43?48.
Carreras, Xavier and Llu??s M`arquez. 2005. Intro-
duction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Con-
ference on Computational Natural Language
Learning. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, CONLL ?05,
pages 152?164.
Chiang, David. 2000. Statistical parsing with
an automatically-extracted tree adjoining gram-
mar. In Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics.
pages 456?463.
Demberg, Vera, Frank Keller, and Alexander
Koller. 2013. Incremental, predictive pars-
ing with psycholinguistically motivated tree-
adjoining grammar. Computational Linguistics
39(4):1025?1066.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li-
310
blinear: A library for large linear classification.
Journal of Machine Learning Research 9:1871?
1874.
Haji?c, Jan, Massimiliano Ciaramita, Richard Jo-
hansson, Daisuke Kawahara, Maria Ant`onia
Mart??, Llu??s M`arquez, Adam Meyers, Joakim
Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel
Stra?n?ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multi-
ple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language
Learning (CoNLL-2009), June 4-5. Boulder,
Colorado, USA.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion
for english. In Joakim Nivre, Heiki-Jaan
Kalep, Kadri Muischnek, and Mare Koit, edi-
tors, NODALIDA 2007 Proceedings. University
of Tartu, pages 105?112.
Joshi, Aravind K. and Yves Schabes. 1992. Tree
adjoining grammars and lexicalized grammars.
In Maurice Nivat and Andreas Podelski, editors,
Tree Automata and Languages, North-Holland,
Amsterdam, pages 409?432.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, Companion Vol-
ume: Short Papers. Uppsala, pages 60?67.
Lang, Joel and Mirella Lapata. 2014. Similarity-
driven semantic role induction via graph par-
titioning. Computational Linguistics Accepted
pages 1?62. To appear.
Liu, Yudong and Anoop Sarkar. 2007. Experimen-
tal evaluation of LTAG-based features for se-
mantic role labeling. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Prague, Czech Republic, pages 590?599.
Marcus, Mitchell P., Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a large
annotated corpus of english: The penn treebank.
Computational Linguistics 19(2):313?330.
M`arquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic Role Labeling: An Introduction to
the Special Issue. Computational Linguistics
34(2):145?159.
Mazzei, Alessandro, Vincenzo Lombardo, and
Patrick Sturt. 2007. Dynamic TAG and lexi-
cal dependencies. Research on Language and
Computation 5:309?332.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Sve-
toslav Marinov, and Erwin Marsi. 2007. Malt-
parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering 13:95?135.
Pad?o, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794?838.
Palmer, Martha, Daniel Gildea, and Paul Kings-
bury. 2005. The proposition bank: An anno-
tated corpus of semantic roles. Computational
Linguistics 31(1):71?106.
Pickering, Martin J., Matthew J. Traxler, and
Matthew W. Crocker. 2000. Ambiguity reso-
lution in sentence processing: Evidence against
frequency-based accounts. Journal of Memory
and Language 43(3):447?475.
Sangati, Federico and Frank Keller. 2013. In-
cremental tree substitution grammar for pars-
ing and word prediction. Transactions of
the Association for Computational Linguistics
1(May):111?124.
Sayeed, Asad and Vera Demberg. 2013. The se-
mantic augmentation of a psycholinguistically-
motivated syntactic formalism. In Proceed-
ings of the Fourth Annual Workshop on Cog-
nitive Modeling and Computational Linguistics
(CMCL). Association for Computational Lin-
guistics, Sofia, Bulgaria, pages 57?65.
Surdeanu, Mihai, Richard Johansson, Adam Mey-
ers, Llu??s M`arquez, and Joakim Nivre. 2008.
The CoNLL-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. In
Proceedings of the 12th Conference on Compu-
tational Natural Language Learning (CoNLL-
2008).
Witten, Ian H. and Timothy C. Bell. 1991. The
zero-frequency problem: estimating the proba-
bilities of novel events in adaptive text compres-
sion. Information Theory, IEEE Transactions
on 37(4):1085?1094.
Xia, Fei, Martha Palmer, and Aravind Joshi. 2000.
A uniform method of grammar extraction and
its applications. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
311
Natural Language Processing and Very Large
Corpora. pages 53?62.
312
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752?761,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Concept-to-text Generation with Hypergraphs
Ioannis Konstas and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
i.konstas@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Concept-to-text generation refers to the task of
automatically producing textual output from
non-linguistic input. We present a joint model
that captures content selection (?what to say?)
and surface realization (?how to say?) in
an unsupervised domain-independent fashion.
Rather than breaking up the generation pro-
cess into a sequence of local decisions, we de-
fine a probabilistic context-free grammar that
globally describes the inherent structure of the
input (a corpus of database records and text
describing some of them). We represent our
grammar compactly as a weighted hypergraph
and recast generation as the task of finding the
best derivation tree for a given input. Experi-
mental evaluation on several domains achieves
competitive results with state-of-the-art sys-
tems that use domain specific constraints, ex-
plicit feature engineering or labeled data.
1 Introduction
Concept-to-text generation broadly refers to the task
of automatically producing textual output from non-
linguistic input (Reiter and Dale, 2000). Depend-
ing on the application and the domain at hand, the
input may assume various representations includ-
ing databases of records, expert system knowledge
bases, simulations of physical systems and so on.
Figure 1 shows input examples and their correspond-
ing text for three domains, air travel, sportscasting
and weather forecast generation.
A typical concept-to-text generation system im-
plements a pipeline architecture consisting of three
core stages, namely text planning (determining the
content and structure of the target text), sentence
planning (determining the structure and lexical con-
tent of individual sentences), and surface realiza-
tion (rendering the specification chosen by the sen-
tence planner into a surface string). Traditionally,
these components are hand-engineered in order to
generate high quality text, however at the expense
of portability and scalability. It is thus no surprise
that recent years have witnessed a growing interest
in automatic methods for creating trainable genera-
tion components. Examples include learning which
database records should be present in a text (Duboue
and McKeown, 2002; Barzilay and Lapata, 2005)
and how these should be verbalized (Liang et al,
2009). Besides concentrating on isolated compo-
nents, a few approaches have emerged that tackle
concept-to-text generation end-to-end. Due to the
complexity of the task, most models simplify the
generation process, e.g., by creating output that con-
sists of a few sentences, thus obviating the need for
document planning, or by treating sentence planning
and surface realization as one component. A com-
mon modeling strategy is to break up the genera-
tion process into a sequence of local decisions, each
learned separately (Reiter et al, 2005; Belz, 2008;
Chen and Mooney, 2008; Angeli et al, 2010; Kim
and Mooney, 2010).
In this paper we describe an end-to-end gen-
eration model that performs content selection and
surface realization jointly. Given a corpus of
database records and textual descriptions (for some
of them), we define a probabilistic context-free
grammar (PCFG) that captures the structure of the
database and how it can be rendered into natural
752
Flight
From To
phoenix new york
Search
Type What
query flight
Day
Day Dep/Ar
sunday departure
List flights from phoenix to new york on sunday
Temperature
Time Min Mean Max
06:00-21:00 9 15 21
Wind Speed
Time Min Mean Max
06:00-21:00 15 20 30
Cloud Sky Cover
Time Percent (%)
06:00-09:00 25-50
09:00-12:00 50-75
Wind Direction
Time Mode
06:00-21:00 S
Cloudy, with a low around 10. South wind around 20 mph.
Pass
From To
pink3 pink7
Bad Pass
From To
pink7 purple3
Turn Over
From To
pink7 purple3
pink3 passes the ball to pink7
(b)(a)
(c)
Figure 1: Input-output examples for (a) query generation in the air travel domain, (b) weather forecast generation, and
(c) sportscasting.
language. This grammar represents a set of trees
which we encode compactly using a weighted hy-
pergraph (or packed forest), a data structure that de-
fines a probability (or weight) for each tree. Gen-
eration then boils down to finding the best deriva-
tion tree in the hypergraph which can be done effi-
ciently using the Viterbi algorithm. In order to en-
sure that our generation output is fluent, we intersect
our grammar with a language model and perform
decoding using a dynamic programming algorithm
(Huang and Chiang, 2007).
Our model is conceptually simpler than previous
approaches and encodes information about the do-
main and its structure globally, by considering the
input space simultaneously during generation. Our
only assumption is that the input must be a set of
records essentially corresponding to database-like
tables whose columns describe fields of a certain
type. Experimental evaluation on three domains ob-
tains results competitive to the state of the art with-
out using any domain specific constraints, explicit
feature engineering or labeled data.
2 Related Work
Our work is situated within the broader class of
data-driven approaches to content selection and sur-
face realization. Barzilay and Lapata (2005) focus
on the former problem which they view as an in-
stance of collective classification (Barzilay and La-
pata, 2005). Given a corpus of database records
and texts describing some of them, they learn a con-
tent selection model that simultaneously optimizes
local label assignments and their pairwise relations.
Building on this work, Liang et al (2009) present a
hierarchical hidden semi-Markov generative model
that first determines which facts to discuss and then
generates words from the predicates and arguments
of the chosen facts.
A few approaches have emerged more recently
that combine content selection and surface realiza-
tion. Kim and Mooney (2010) adopt a two-stage ap-
proach: using a generative model similar to Liang et
al. (2009), they first decide what to say and then ver-
balize the selected input with WASP?1, an existing
generation system (Wong and Mooney, 2007). In
contrast, Angeli et al (2010) propose a unified con-
tent selection and surface realization model which
also operates over the alignment output produced
by Liang et al (2009). Their model decomposes
into a sequence of discriminative local decisions.
They first determine which records in the database
to talk about, then which fields of those records
to mention, and finally which words to use to de-
scribe the chosen fields. Each of these decisions
is implemented as a log-linear model with features
learned from training data. Their surface realiza-
tion component is based on templates that are au-
tomatically extracted and smoothed with domain-
specific constraints in order to guarantee fluent out-
put. Other related work (Wong and Mooney, 2007;
Lu and Ng, 2011). has focused on generating natural
language sentences from logical form (i.e., lambda-
expressions) using mostly synchronous context-free
grammars (SCFGs).
753
Similar to Angeli et al (2010), we also present
an end-to-end system that performs content selec-
tion and surface realization. However, rather than
breaking up the generation task into a sequence of
local decisions, we optimize what to say and how
to say simultaneously. We do not learn mappings
from a logical form, but rather focus on input which
is less constrained, possibly more noisy and with a
looser structure. Our key insight is to convert the
set of database records serving as input to our gen-
erator into a PCFG that is neither hand crafted nor
domain specific but simply describes the structure
of the database. The approach is conceptually sim-
ple, does not rely on discriminative training or any
feature engineering. We represent the grammar and
its derivations compactly as a weighted hypergraph
which we intersect with a language model in order
to generate fluent output. This allows us to easily
port surface generation to different domains without
having to extract new templates or enforce domain
specific constraints.
3 Problem Formulation
We assume our generator takes as input a set of
database records d and produces text w that verbal-
izes some of these records. Each record r ? d has a
type r.t and a set of fields f associated with it. Fields
have different values f .v and types f .t (i.e., in-
teger or categorical). For example, in Figure 1b,
wind speed is a record type with four fields: time,
min, mean, and max. The values of these fields are
06:00-21:00, 15, 20, and 30, respectively; the type
of time is categorical, whereas all other fields are
integers.
During training, our algorithm is given a cor-
pus consisting of several scenarios, i.e., database
records paired with texts like those shown in Fig-
ure 1. In the weather forecast domain, a scenario cor-
responds to weather-related measurements of tem-
perature, wind, speed, and so on collected for a spe-
cific day and time (e.g., day or night). In sportscast-
ing, scenarios describe individual events in the soc-
cer game (e.g., passing or kicking the ball). In the air
travel domain, scenarios comprise of flight-related
details (e.g., origin, destination, day, time). Our goal
then is to reduce the tasks of content selection and
surface realization into a common probabilistic pars-
ing problem. We do this by abstracting the struc-
ture of the database (and accompanying texts) into
a PCFG whose probabilities are learned from train-
ing data.1 Specifically, we convert the database into
rewrite rules and represent them as a weighted di-
rected hypergraph (Gallo et al, 1993). Instead of
learning the probabilities on the PCFG, we directly
compute the weights on the hyperarcs using a dy-
namic program similar to the inside-outside algo-
rithm (Li and Eisner, 2009). During testing, we are
given a set of database records without the corre-
sponding text. Using the trained grammar we com-
pile a hypergraph specific to this test input and de-
code it approximately via cube pruning (Chiang,
2007).
The choice of the hypergraph framework is moti-
vated by at least three reasons. Firstly, hypergraphs
can be used to represent the search space of most
parsers (Klein and Manning, 2001). Secondly, they
are more efficient and faster than the common CYK
parser-based representation for PCFGs by a factor
of more than ten (Huang and Chiang, 2007). And
thirdly, the hypergraph representation allows us to
integrate an n-gram language model and perform de-
coding efficiently using k-best Viterbi search, opti-
mizing what to say and how to say at the same time.
3.1 Grammar Definition
Our model captures the inherent structure of the
database with a number of CFG rewrite rules, in
a similar way to how Liang et al (2009) define
Markov chains in the different levels of their hierar-
chical model. These rules are purely syntactic (de-
scribing the intuitive relationship between records,
records and fields, fields and corresponding words),
and could apply to any database with similar struc-
ture irrespectively of the semantics of the domain.
Our grammar is defined in Table 1 (rules (1)?(9)).
Rule weights are governed by an underlying multi-
nomial distribution and are shown in square brack-
ets. Non-terminal symbols are in capitals and de-
1An alternative would be to learn a SCFG between the
database input and the accompanying text. However, this would
involve considerable overhead in terms of alignment (as the
database and the text do not together constitute a clean parallel
corpus, but rather a noisy comparable corpus), as well as gram-
mar training and decoding using state-of-the art SMT methods,
which we manage to avoid with our simpler approach.
754
1. S? R(start) [Pr = 1]
2. R(ri.t)? FS(r j,start) R(r j.t) [P(r j.t |ri.t) ??]
3. R(ri.t)? FS(r j,start) [P(r j.t |ri.t) ??]
4. FS(r,r. fi)? F(r,r. f j) FS(r,r. f j) [P( f j | fi)]
5. FS(r,r. fi)? F(r,r. f j) [P( f j | fi)]
6. F(r,r. f )?W(r,r. f ) F(r,r. f ) [P(w |w?1,r,r. f )]
7. F(r,r. f )?W(r,r. f ) [P(w |w?1,r,r. f )]
8. W(r,r. f )? ? [P(? |r,r. f , f .t, f .v)]
9. W(r,r. f )? g( f .v)
[P(g( f .v).mode |r,r. f , f .t = int)]
Table 1: Grammar rules and their weights shown in
square brackets.
note intermediate states; the terminal symbol ?
corresponds to all words seen in the training set,
and g( f .v) is a function for generating integer num-
bers given the value of a field f . All non-terminals,
save the start symbol S, have one or more features
(shown in parentheses) that act as constraints, sim-
ilar to number and gender agreement constraints in
augmented syntactic rules.
Rule (1) denotes the expansion from the start
symbol S to record R, which has the special ?start?
record type (hence the notation R(start)). Rule (2)
defines a chain between two consecutive records,
i.e., going from a source record ri to a target r j.
Here, FS(r j,r j. f ) represents the set of fields of the
target r j, following the source record R(ri).
For example, the rule R(skyCover1.t) ?
FS(temperature1,start)R(temperature1.t) can
be interpreted as follows. Given that we have
talked about skyCover1, we will next talk about
temperature1 and thus emit its corresponding fields.
R(temperature1.t) is a non-terminal place-holder
for the continuation of the chain of records, and
start in FS is a special boundary field between
consecutive records. The weight of this rule is the
bigram probability of two records conditioned on
their record type, multiplied with a normalization
factor ?. We have also defined a null record type
i.e., a record that has no fields and acts as a
smoother for words that may not correspond to a
particular record. Rule (3) is simply an escape rule,
so that the parsing process (on the record level) can
finish.
Rule (4) is the equivalent of rule (2) at the
field level, i.e., it describes the chaining of
two consecutive fields fi and f j. Non-terminal
F(r,r. f ) refers to field f of record r. For
example, the rule FS(windSpeed1,min) ?
F(windSpeed1,max)FS(windSpeed1,max), spec-
ifies that we should talk about the field max of
record windSpeed1, after talking about the field
min. Analogously to the record level, we have also
included a special null field type for the emission
of words that do not correspond to a specific record
field. Rule (6) defines the expansion of field F to
a sequence of (binarized) words W, with a weight
equal to the bigram probability of the current word
given the previous word, the current record, and
field. This is an attempt at capturing contextual
dependencies between words over and above to
integrating a language model during decoding (see
Section 3.3).
Rules (8) and (9) define the emission of words and
integer numbers from W, given a field type and its
value. Rule (8) emits a single word from the vocabu-
lary of the training set. Its weight defines a multino-
mial distribution over all seen words, for every value
of field f , given that the field type is categorical or
the special null field. Rule (9) is identical but for
fields whose type is integer. Function g( f .v) gener-
ates an integer number given the field value, using
either of the following six ways (Liang et al, 2009):
identical to the field value, rounding up or rounding
down to a multiple of 5, rounding off to the clos-
est multiple of 5 and finally adding or subtracting
some unexplained noise.2 The weight is a multino-
mial over the six generation function modes, given
the record field f .
3.2 Hypergraph Construction
So far we have defined a probabilistic grammar
that captures the structure of a database d with
records and fields as intermediate non-terminals, and
words w (from the associated text) as terminals. Us-
ing this grammar and the CYK parsing algorithm,
we could obtain the top scoring derivation of records
and fields for a given input (i.e., a sequence of
2The noise is modeled as a geometric distribution.
755
S0,7
R0,2(start)
R0,1(start)
? ? ?
FS0,1(skyCover1,start)
R1,1(skyCover1.t)
R1,1(temp1.t)
FS0,1(temp1,start)
? ? ?
F0,1(skyCover1,%)
FS1,1(skyCover1,%)
F0,1(skyCover1,time)
FS1,1(skyCover1,time)
W0,1(skyCover1,%)
W0,1(skyCover1,time)
FS1,2(temp1,start)
R2,2(temp1.t)
FS1,2(skyCover1,start)
R2,2(skyCover1.t)
? ? ?
sunny
F1,2(temp1,min)
FS2,2(temp1,min)
W0,1(temp1,min) g0,1(min,v=10)
F1,2(temp1,max)
FS2,2(temp1,max)
W0,1(temp1,max) g0,1(max,v=20)
with
Figure 2: Partial hypergraph representation for the sentence ?Sunny with a low around 30 .? For the sake of readability,
we show a partial span on the first two words without weights on the hyperarcs.
words) as well as the optimal segmentation of the
text, provided we have a trained set of weights. The
inside-outside algorithm is commonly used for esti-
mating the weights of a PCFG. However, we first
transform the CYK parser and our grammar into
a hypergraph and then compute the weights using
inside-outside. Huang and Chiang (2005) define a
weighted directed hypergraph as follows:
Definition 1 An ordered hypergraph H is a tuple
?N,E, t,R?, where N is a finite set of nodes, E
is a finite set of hyperarcs and R is the set of
weights. Each hyperarc e ? E is a triple e =
?T (e),h(e), f (e)?, where h(e) ? N is its head node,
T (e) ? N? is a set of tail nodes and f (e) is a mono-
tonic weight function R|T (e)| to R and t ? N is a tar-
get node.
Definition 2 We impose the arity of a hyperarc to be
|e| = |T (e)| = 2, in other words, each head node is
connected with at most two tail nodes.
Given a context-free grammar G = ?N,T,P,S?
(where N is the set of variables, T the set of ter-
minals, P the set of production rules, and S ? N the
start symbol) and an input string w, we can map the
standard weighted CYK algorithm to a hypergraph
as follows. Each node [A, i, j] in the hypergraph
corresponds to non-terminal A spanning words wi
to w j of the input. Each rewrite rule A? BC in P,
with three free indices i < j < k, is mapped to
the hyperarc ?((B, i, j),(C, j,k)) ,(A, i,k), f ?, where
f = f ((B, i, j)) f ((C, j,k)) ?Pr(A? BC).3 The hy-
3Similarly, rewrite rules of type A? B are mapped to the
hyperarc ?(B, i, j),(A, i, j), f ?, with f = f ((B, i, j)) ?Pr(A? B).
pergraph can be thus viewed as a compiled lattice
of the corresponding chart graph. Figure 2 shows
an example hypergraph for a grammar defined on
database input similar to Figure (1b).
In order to learn the weights on the hyperarcs we
perform the following procedure iteratively in an
EM fashion (Li and Eisner, 2009). For each train-
ing scenario we build its hypergraph representation.
Next, we perform inference by calculating the in-
side and outside scores of the hypergraph, so as to
compute the posterior distribution over its hyperarcs
(E-step). Finally, we collectively update the posteri-
ors on the parameters-weights, i.e., rule probabilities
and emission multinomial distributions (M-step).
3.3 Decoding
In the framework outlined above, parsing an input
string w (given some learned weights) boils down
to traversing the hypergraph in a particular order.
(Note that the hypergraph should be acyclic, which
is always guaranteed by the grammar in Table 1). In
generation, our aim is to verbalize an input scenario
from a database d (see Figure 1). We thus find the
best text by maximizing:
argmax
w
P(w |d) = argmax
w
P(w) ?P(d |w) (1)
where P(d |w) is the decoding likelihood for a se-
quence of words w, P(w) is a measure of the qual-
ity of each output (given by a language model),
and P(w |d) the posterior of the best output for
database d. Note that calculating P(d |w) requires
deciding on the output length |w|. Rather than set-
756
ting w to a fixed length, we rely on a linear regres-
sion predictor that uses the counts of each record
type per scenario as features and is able to produce
variable length texts.
In order to perform decoding with an n-gram lan-
guage model, we adopt Huang and Chiang?s (2007)
dynamic-programming algorithm for SCFG-based
systems. Each node in the hypergraph is split into
a set of compound items, namely +LM items. Each
+LM item is of the form (na?b), where a and b are
boundary words of the generation string, and ? is a
place-holder symbol for an elided part of that string,
indicating a sub-generation part ranging from a to b.
An example +LM deduction of a single hyperarc of
the hypergraph in Figure 2 using bigrams is:
(2)
FS1,2(temp1,start)low : (w1,g1),
R2,2(temp1.t)around?degrees : (w2,g2)
R1,1(skyCover1.t)low?degrees : (w,g1g2)
w = w1 +w2 + ew +Plm(around | low) (3)
where w1,w2 are node weights, g1,g2 are the corre-
sponding sub-generations, ew is the weight of the hy-
perarc and w the weight of the resulting +LM item.
Plm and (na?b) are defined as in Chiang (2007) in a
generic fashion, allowing extension to an arbitrary
size of n-gram grammars.
Naive traversal of the hypergraph bottom-up
would explore all possible +LM deductions along
each hyperarc, and would increase decoding com-
plexity to an infeasible O(2nn2), assuming a trigram
model and a constant number of emissions at the ter-
minal nodes. To ensure tractability, we adopt cube
pruning, a popular approach in syntax-inspired ma-
chine translation (Chiang, 2007). The idea is to use a
beam-search over the intersection grammar coupled
with the cube-pruning heuristic. The beam limits the
number of derivations for each node, whereas cube-
pruning further limits the number of +LM items con-
sidered for inclusion in the beam. Since f (e) in Def-
inition 1 is monotonic, we can select the k-best items
without computing all possible +LM items.
Our decoder follows Huang and Chiang (2007)
but importantly differs in the treatment of leaf nodes
in the hypergraph (see rules (8) and (9)). In the
SCFG context, the Viterbi algorithm consumes ter-
minals from the source string in a bottom-up fashion
and creates sub-translations according to the CFG
rule that holds each time. In the concept-to-text
generation context, however, we do not observe the
words; instead, for each leaf node we emit the k-best
words from the underlying multinomial distribution
(see weights on rules (8) and (9)) and continue build-
ing our sub-generations bottom-up.
4 Experimental Design
Data We used our system to generate soccer com-
mentaries, weather forecasts, and spontaneous utter-
ances relevant to the air travel domain (examples
are given in Figure 1). For the first domain we
used the dataset of Chen and Mooney (2008), which
consists of 1,539 scenarios from the 2001?2004
Robocup game finals. Each scenario contains on av-
erage |d|= 2.4 records, each paired with a short sen-
tence (5.7 words). This domain has a small vocabu-
lary (214 words) and simple syntax (e.g., a transitive
verb with its subject and object). Records in this
dataset (henceforth ROBOCUP) were aligned man-
ually to their corresponding sentences (Chen and
Mooney, 2008). Given the relatively small size of
this dataset, we performed cross-validation follow-
ing previous work (Chen and Mooney, 2008; An-
geli et al, 2010). We trained our system on three
ROBOCUP games and tested on the fourth, averaging
over the four train/test splits.
For weather forecast generation, we used the
dataset of Liang et al (2009), which consists of
29,528 weather scenarios for 3,753 major US cities
(collected over four days). The vocabulary in this
domain (henceforth WEATHERGOV) is comparable
to ROBOCUP (345 words), however, the texts are
longer (|w| = 29.3) and more varied. On average,
each forecast has 4 sentences and the content selec-
tion problem is more challenging; only 5.8 out of
the 36 records per scenario are mentioned in the text
which roughly corresponds to 1.4 records per sen-
tence. We used 25,000 scenarios from WEATHER-
GOV for training, 1,000 scenarios for development
and 3,528 scenarios for testing. This is the same par-
tition used in Angeli et al (2010).
For the air travel domain we used the ATIS dataset
(Dahl et al, 1994), consisting of 5,426 scenar-
ios. These are transcriptions of spontaneous utter-
ances of users interacting with a hypothetical on-
757
WEATHERGOV ATIS ROBOCUP
1-
B
E
S
T Near 57. Near 57. Near 57. Near 57. Near
57. Near 57. Near 57. Near 57. Near 57.
Near 57. Near 57. South wind.
What what what what flights
from Denver Phoenix Pink9 to to Pink7 kicks
k-
B
E
S
T
As high as 23 mph. Chance of precipitation
is 20. Breezy, with a chance of showers.
Mostly cloudy, with a high near 57. South
wind between 3 and 9 mph.
Show me the flights from
Denver to Phoenix
Pink9 passes back to Pink7
A
N
G
E
L
I
A chance of rain or drizzle, with a high near
57. South wind between 3 and 9 mph.
Show me the flights leave
from Nashville to Phoenix
Pink9 kicks to Pink7
H
U
M
A
N
A slight chance of showers. Mostly cloudy,
with a high near 58. South wind between 3
and 9 mph, with gusts as high as 23 mph.
Chance of precipitation is 20%.
List flights from Denver to
Phoenix
Pink9 passes back to Pink7
Table 2: System output on WEATHERGOV, ATIS, and ROBOCUP (1-BEST, k-BEST, ANGELI) and corresponding
human-authored text (HUMAN).
line flight booking system. We used the dataset
introduced in Zettlemoyer and Collins (2007)4 and
automatically converted their lambda-calculus ex-
pressions to attribute-value pairs following the con-
ventions adopted by Liang et al (2009). For ex-
ample, the scenario in Figure 1(a) was initially
represented as: ?x. f light(x) ? f rom(x, phoenix) ?
to(x,new york)?day(x,sunday).5 In contrast to the
two previous datasets, ATIS has a much richer vo-
cabulary (927 words); each scenario corresponds
to a single sentence (average length is 11.2 words)
with 2.65 out of 19 record types mentioned on av-
erage. Following Zettlemoyer and Collins (2007),
we trained on 4,962 scenarios and tested on ATIS
NOV93 which contains 448 examples.
Model Parameters Our model has two parame-
ters, namely the number of k grammar derivations
considered by the decoder and the order of the
language model. We tuned k experimentally on
held-out data taken from WEATHERGOV, ROBOCUP,
and ATIS, respectively. The optimal value was k=15
for WEATHERGOV, k=25 for ROBOCUP, and k = 40
4The original corpus contains user utterances of single dia-
logue turns which would result in trivial scenarios. Zettlemoyer
and Collins (2007) concatenate all user utterances referring to
the same dialogue act, (e.g., book a flight), thus yielding more
complex scenarios with longer sentences.
5The resulting dataset and a technical report describ-
ing the mapping procedure in detail are available from
http://homepages.inf.ed.ac.uk/s0793019/index.php?
page=resources
for ATIS. For the ROBOCUP domain, we used a bi-
gram language model which was considered suffi-
cient given that the average text length is small. For
WEATHERGOV and ATIS, we used a trigram language
model.
System Comparison We evaluated two configu-
rations of our system. A baseline that uses the top
scoring derivation in each subgeneration (1-BEST)
and another version which makes better use of our
decoding algorithm and considers the best k deriva-
tions (i.e., 15 for WEATHERGOV, 40 for ATIS, and
25 for ROBOCUP). We compared our output to An-
geli et al (2010) whose approach is closest to ours
and state-of-the-art on the WEATHERGOV domain.
For ROBOCUP, we also compare against the best-
published results (Kim and Mooney, 2010).
Evaluation We evaluated system output automat-
ically, using the BLEU modified precision score
(Papineni et al, 2002) with the human-written text
as reference. In addition, we evaluated the gener-
ated text by eliciting human judgments. Participants
were presented with a scenario and its correspond-
ing verbalization and were asked to rate the latter
along two dimensions: fluency (is the text grammat-
ical and overall understandable?) and semantic cor-
rectness (does the meaning conveyed by the text cor-
respond to the database input?). The subjects used a
five point rating scale where a high number indicates
better performance. We randomly selected 12 doc-
758
ROBOCUP WEATHERGOV ATIS
System BLEU BLEU BLEU
1-BEST 10.79 8.64 11.85
k-BEST 30.90 33.70 29.30
ANGELI 28.70 38.40 26.77
KIM-MOONEY 47.27 ? ?
Table 3: BLEU scores on ROBOCUP (fixed content se-
lection), WEATHERGOV, and ATIS.
uments from the test set (for each domain) and gen-
erated output with our models (1-BEST and k-BEST)
and Angeli et al?s (2010) model (see Figure 2 for
examples of system output). We also included the
original text (HUMAN) as gold standard. We thus
obtained ratings for 48 (12 ? 4) scenario-text pairs
for each domain. The study was conducted over the
Internet using WebExp (Keller et al, 2009) and was
completed by 114 volunteers, all self reported native
English speakers.
5 Results
We conducted two experiments on the ROBOCUP do-
main. We first assessed the performance of our gen-
erator (k-BEST) on joint content selection and sur-
face realization and obtained a BLEU score of 24.88.
In comparison, the baseline?s (1-BEST) BLEU score
was 8.01. In a second experiment we forced the
generator to use the gold-standard records from the
database. This was necessary in order to compare
with previous work (Angeli et al, 2010; Kim and
Mooney, 2010).6 Our results are summarized in Ta-
ble 3. Overall, our generator performs better than
the baseline and Angeli et al (2010). We observe
a substantial increase in performance compared to
the joint content selection and surface realization
setting. This is expected as the generator is faced
with an easier task and there is less scope for error.
Our model does not outperform Kim and Mooney
(2010), however, this is not entirely surprising as
their model requires considerable more supervision
(e.g., during parameter initialization) and includes a
post-hoc re-ordering component.
6Angeli et al (2010) and Kim and Mooney (2010) fix con-
tent selection both at the record and field level. We let our gen-
erator select the appropriate fields, since these are at most two
per record type and this level of complexity can be easily tack-
led during decoding.
ROBOCUP WEATHERGOV ATIS
System F SC F SC F SC
1-BEST 2.47?? 2.33?? 1.82?? 2.05?? 2.40?? 2.46??
k-BEST 4.31? 3.96? 3.92? 3.30? 4.01 3.87
ANGELI 4.03?? 3.70?? 4.26? 3.60? 3.56?? 3.33??
HUMAN 4.47? 4.37? 4.61? 4.03? 4.10 4.01
Table 4: Mean ratings for fluency (F) and semantic cor-
rectness (SC) on system output elicited by humans on
ROBOCUP, WEATHERGOV, and ATIS (?: sig. diff. from
HUMAN; ?: sig. diff. from k-BEST.)
With regard to WEATHERGOV, our generator im-
proves over the baseline but lags behind Angeli et
al. (2010). Since our system emits words based on
a language model rather than a template, it displays
more freedom in word order and lexical choice, and
is thus penalized by BLEU when creating output that
is overly distinct from the reference. On ATIS, our
model outperforms both the baseline and Angeli et
al. This is the most challenging domain with re-
gard to surface realization with a vocabulary larger
than ROBOCUP and WEATHERGOV by factors of 2.7
and 4.3, respectively.
The results of our human evaluation study are
shown in Table 3. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system
type (1-BEST, k-BEST, ANGELI, and HUMAN) on the
fluency and semantic correctness ratings. Means
differences were compared using a post-hoc Tukey
test. On ROBOCUP, our system (k-BEST) is signif-
icantly better than the baseline (1-BEST) and AN-
GELI both in terms of fluency and semantic correct-
ness (a < 0.05). On WEATHERGOV, our generator
performs comparably to ANGELI on fluency and se-
mantic correctness (the differences in the means are
not statistically significant); 1-BEST is significantly
worse than 15-BEST and ANGELI (a < 0.05). On
ATIS, k-BEST is significantly more fluent and seman-
tically correct than 1-BEST and ANGELI (a < 0.01).
There was no statistically significant difference be-
tween the output of our system and the original ATIS
sentences.
In sum, we observe that taking the k-best deriva-
tions into account boosts performance (the 1-BEST
system is consistently worse). Our model is on par
with ANGELI on WEATHERGOV but performs better
on ROBOCUP and ATIS when evaluated both auto-
759
matically and by humans. In general, a large part of
our output resembles the human text, which demon-
strates that our simple language model yields coher-
ent sentences (without any template engineering), at
least for the domains under consideration.
6 Conclusions
We have presented an end-to-end generation system
that performs both content selection and surface re-
alization. Central to our approach is the encoding
of generation as a parsing problem. We reformulate
the input (a set of database records and text describ-
ing some of them) as a PCFG and show how to find
the best derivation using the hypergraph framework.
Despite its simplicity, our model is able to obtain
performance comparable to the state of the art. We
argue that our approach is computationally efficient
and viable in practical applications. Porting the sys-
tem to a different domain is straightforward, assum-
ing a database and corresponding (unaligned) text.
As long as the database is compatible with the struc-
ture of the grammar in Table 1, we need only retrain
to obtain the weights on the hyperarcs and a domain
specific language model.
Our model takes into account the k-best deriva-
tions at decoding time, however inspection of these
shows that it often fails to select the best one. In
the future, we plan to remedy this by using forest
reranking, a technique that approximately reranks
a packed forest of exponentially many derivations
(Huang, 2008). We would also like to scale our
model to more challenging domains (e.g., product
descriptions) and to enrich our generator with some
notion of discourse planning. An interesting ques-
tion is how to extend the PCFG-based approach ad-
vocated here so as to capture discourse-level docu-
ment structure.
Acknowledgments We are grateful to Percy Liang
and Gabor Angeli for providing us with their code
and data. We would also like to thank Luke Zettle-
moyer and Tom Kwiatkowski for sharing their ATIS
dataset with us and Frank Keller for his feedback on
an earlier version of this paper.
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502?512, Cambridge, MA.
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive content selection for concept-to-text generation.
In Proceedings of Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 331?338, Vancouver, British Columbia.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 14(4):431?455.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128?135, Helsinki, Finland.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis task:
the atis-3 corpus. In Proceedings of the Workshop on
Human Language Technology, pages 43?48, Plains-
boro, NJ.
Pablo A. Duboue and Kathleen R. McKeown. 2002.
Content planner construction via evolutionary algo-
rithms and a corpus-based fitness function. In Pro-
ceedings of International Natural Language Genera-
tion, pages 89?96, Ramapo Mountains, NY.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and appli-
cations. Discrete Applied Mathematics, 42:177?201.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International Work-
shop on Parsing Technology, pages 53?64, Vancouver,
British Columbia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio.
Frank Keller, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of Web ex-
periments: A case study using the WebExp software
package. Behavior Research Methods, 41(1):1?12.
760
Joohyun Kim and Raymond Mooney. 2010. Generative
alignment and semantic parsing for learning from am-
biguous supervision. In Proceedings of the 23rd Con-
ference on Computational Linguistics, pages 543?551,
Beijing, China.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the 7th Interna-
tional Workshop on Parsing Technologies, pages 123?
134, Beijing, China.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 40?51, Suntec, Sin-
gapore.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In roceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 91?99, Suntec, Singapore.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-
to-string model for language generation from typed
lambda calculus expressions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 1611?1622, Edinburgh,
UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian
Davy. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172?179, Rochester, NY.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 678?687, Prague, Czech Republic.
761
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 369?378,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Concept-to-text Generation via Discriminative Reranking
Ioannis Konstas and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
i.konstas@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
This paper proposes a data-driven method
for concept-to-text generation, the task of
automatically producing textual output from
non-linguistic input. A key insight in our ap-
proach is to reduce the tasks of content se-
lection (?what to say?) and surface realization
(?how to say?) into a common parsing prob-
lem. We define a probabilistic context-free
grammar that describes the structure of the in-
put (a corpus of database records and text de-
scribing some of them) and represent it com-
pactly as a weighted hypergraph. The hyper-
graph structure encodes exponentially many
derivations, which we rerank discriminatively
using local and global features. We propose a
novel decoding algorithm for finding the best
scoring derivation and generating in this set-
ting. Experimental evaluation on the ATIS do-
main shows that our model outperforms a
competitive discriminative system both using
BLEU and in a judgment elicitation study.
1 Introduction
Concept-to-text generation broadly refers to the
task of automatically producing textual output from
non-linguistic input such as databases of records,
logical form, and expert system knowledge bases
(Reiter and Dale, 2000). A variety of concept-to-
text generation systems have been engineered over
the years, with considerable success (e.g., Dale et
al. (2003), Reiter et al (2005), Green (2006), Turner
et al (2009)). Unfortunately, it is often difficult
to adapt them across different domains as they rely
mostly on handcrafted components.
In this paper we present a data-driven ap-
proach to concept-to-text generation that is domain-
independent, conceptually simple, and flexible. Our
generator learns from a set of database records and
textual descriptions (for some of them). An exam-
ple from the air travel domain is shown in Figure 1.
Here, the records provide a structured representation
of the flight details (e.g., departure and arrival time,
location), and the text renders some of this infor-
mation in natural language. Given such input, our
model determines which records to talk about (con-
tent selection) and which words to use for describing
them (surface realization). Rather than breaking up
the generation process into a sequence of local deci-
sions, we perform both tasks jointly. A key insight
in our approach is to reduce content selection and
surface realization into a common parsing problem.
Specifically, we define a probabilistic context-free
grammar (PCFG) that captures the structure of the
database and its correspondence to natural language.
This grammar represents multiple derivations which
we encode compactly using a weighted hypergraph
(or packed forest), a data structure that defines a
weight for each tree.
Following a generative approach, we could first
learn the weights of the PCFG by maximising the
joint likelihood of the model and then perform gen-
eration by finding the best derivation tree in the hy-
pergraph. The performance of this baseline system
could be potentially further improved using discrim-
inative reranking (Collins, 2000). Typically, this
method first creates a list of n-best candidates from
a generative model, and then reranks them with arbi-
trary features (both local and global) that are either
not computable or intractable to compute within the
369
Database:
Flight
from to
denver boston
Day Number
number dep/ar
9 departure
Month
month dep/ar
august departure
Condition
arg1 arg2 type
arrival time 1600 <
Search
type what
query flight
??expression:
Text:
?x. f light(x)? f rom(x,denver)? to(x,boston)?day number(x,9)?month(x,august)?
less than(arrival time(x),1600)
Give me the flights leaving Denver August ninth coming back to Boston before 4pm.
Figure 1: Example of non-linguistic input as a structured database and logical form and its corresponding text. We
omit record fields that have no value, for the sake of brevity.
baseline system.
An appealing alternative is to rerank the hyper-
graph directly (Huang, 2008). As it compactly en-
codes exponentially many derivations, we can ex-
plore a much larger hypothesis space than would
have been possible with an n-best list. Importantly,
in this framework non-local features are computed
at all internal hypergraph nodes, allowing the de-
coder to take advantage of them continuously at all
stages of the generation process. We incorporate
features that are local with respect to a span of a
sub-derivation in the packed forest; we also (approx-
imately) include features that arbitrarily exceed span
boundaries, thus capturing more global knowledge.
Experimental results on the ATIS domain (Dahl et
al., 1994) demonstrate that our model outperforms
a baseline based on the best derivation and a state-
of-the-art discriminative system (Angeli et al, 2010)
by a wide margin.
Our contributions in this paper are threefold: we
recast concept-to-text generation in a probabilistic
parsing framework that allows to jointly optimize
content selection and surface realization; we repre-
sent parse derivations compactly using hypergraphs
and illustrate the use of an algorithm for generating
(rather than parsing) in this framework; finally, the
application of discriminative reranking to concept-
to-text generation is novel to our knowledge and as
our experiments show beneficial.
2 Related Work
Early discriminative approaches to text generation
were introduced in spoken dialogue systems, and
usually tackled content selection and surface re-
alization separately. Ratnaparkhi (2002) concep-
tualized surface realization (from a fixed meaning
representation) as a classification task. Local and
non-local information (e.g., word n-grams, long-
range dependencies) was taken into account with the
use of features in a maximum entropy probability
model. More recently, Wong and Mooney (2007)
describe an approach to surface realization based on
synchronous context-free grammars. The latter are
learned using a log-linear model with minimum er-
ror rate training (Och, 2003).
Angeli et al (2010) were the first to propose a
unified approach to content selection and surface re-
alization. Their model operates over automatically
induced alignments of words to database records
(Liang et al, 2009) and decomposes into a sequence
of discriminative local decisions. They first deter-
mine which records in the database to talk about,
then which fields of those records to mention, and
finally which words to use to describe the chosen
fields. Each of these decisions is implemented as
a log-linear model with features learned from train-
ing data. Their surface realization component per-
forms decisions based on templates that are automat-
ically extracted and smoothed with domain-specific
knowledge in order to guarantee fluent output.
Discriminative reranking has been employed in
many NLP tasks such as syntactic parsing (Char-
niak and Johnson, 2005; Huang, 2008), machine
translation (Shen et al, 2004; Li and Khudanpur,
2009) and semantic parsing (Ge and Mooney, 2006).
Our model is closest to Huang (2008) who also
performs forest reranking on a hypergraph, using
both local and non-local features, whose weights
are tuned with the averaged perceptron algorithm
(Collins, 2002). We adapt forest reranking to gen-
eration and introduce several task-specific features
that boost performance. Although conceptually re-
lated to Angeli et al (2010), our model optimizes
content selection and surface realization simultane-
ously, rather than as a sequence. The discriminative
aspect of two models is also fundamentally different.
We have a single reranking component that applies
370
throughout, whereas they train different discrimina-
tive models for each local decision.
3 Problem Formulation
We assume our generator takes as input a set of
database records d and produces text w that verbal-
izes some of these records. Each record r ? d has a
type r.t and a set of fields f associated with it. Fields
have different values f .v and types f .t (i.e., integer
or categorical). For example, in Figure 1, flight is a
record type with fields from and to. The values of
these fields are denver and boston and their type is
categorical.
During training, our algorithm is given a corpus
consisting of several scenarios, i.e., database records
paired with texts like those shown in Figure 1. The
database (and accompanying texts) are next con-
verted into a PCFG whose weights are learned from
training data. PCFG derivations are represented as
a weighted directed hypergraph (Gallo et al, 1993).
The weights on the hyperarcs are defined by a vari-
ety of feature functions, which we learn via a dis-
criminative online update algorithm. During test-
ing, we are given a set of database records with-
out the corresponding text. Using the learned fea-
ture weights, we compile a hypergraph specific to
this test input and decode it approximately (Huang,
2008). The hypergraph representation allows us
to decompose the feature functions and compute
them piecemeal at each hyperarc (or sub-derivation),
rather than at the root node as in conventional n-best
list reranking. Note that the algorithm does not sep-
arate content selection from surface realization, both
subtasks are optimized jointly through the proba-
bilistic parsing formulation.
3.1 Grammar Definition
We capture the structure of the database with a num-
ber of CFG rewrite rules, in a similar way to how
Liang et al (2009) define Markov chains in their
hierarchical model. These rules are purely syn-
tactic (describing the intuitive relationship between
records, records and fields, fields and corresponding
words), and could apply to any database with sim-
ilar structure irrespectively of the semantics of the
domain.
Our grammar is defined in Table 1 (rules (1)?(9)).
Rule weights are governed by an underlying multi-
nomial distribution and are shown in square brack-
1. S? R(start) [Pr = 1]
2. R(ri.t)? FS(r j,start) R(r j.t) [P(r j.t |ri.t) ??]
3. R(ri.t)? FS(r j,start) [P(r j.t |ri.t) ??]
4. FS(r,r. fi)? F(r,r. f j) FS(r,r. f j) [P( f j | fi)]
5. FS(r,r. fi)? F(r,r. f j) [P( f j | fi)]
6. F(r,r. f )?W(r,r. f ) F(r,r. f ) [P(w |w?1,r,r. f )]
7. F(r,r. f )?W(r,r. f ) [P(w |w?1,r,r. f )]
8. W(r,r. f )? ? [P(? |r,r. f , f .t, f .v)]
9. W(r,r. f )? g( f .v)
[P(g( f .v).mode |r,r. f , f .t = int)]
Table 1: Grammar rules and their weights shown in
square brackets.
ets. Non-terminal symbols are in capitals and de-
note intermediate states; the terminal symbol ?
corresponds to all words seen in the training set,
and g( f .v) is a function for generating integer num-
bers given the value of a field f . All non-terminals,
save the start symbol S, have one or more constraints
(shown in parentheses), similar to number and gen-
der agreement constraints in augmented syntactic
rules.
Rule (1) denotes the expansion from the start
symbol S to record R, which has the special start
type (hence the notation R(start)). Rule (2) de-
fines a chain between two consecutive records ri
and r j. Here, FS(r j,start) represents the set
of fields of the target r j, following the source
record R(ri). For example, the rule R(search1.t)?
FS( f light1,start)R( f light1.t) can be interpreted as
follows. Given that we have talked about search1,
we will next talk about f light1 and thus emit its
corresponding fields. R( f light1.t) is a non-terminal
place-holder for the continuation of the chain of
records, and start in FS is a special boundary field
between consecutive records. The weight of this rule
is the bigram probability of two records conditioned
on their type, multiplied with a normalization fac-
tor ?. We have also defined a null record type i.e., a
record that has no fields and acts as a smoother for
words that may not correspond to a particular record.
Rule (3) is simply an escape rule, so that the parsing
process (on the record level) can finish.
Rule (4) is the equivalent of rule (2) at the field
371
level, i.e., it describes the chaining of two con-
secutive fields fi and f j. Non-terminal F(r,r. f )
refers to field f of record r. For example, the rule
FS( f light1, f rom) ? F( f light1, to)FS( f light1, to),
specifies that we should talk about the field to of
record f light1, after talking about the field f rom.
Analogously to the record level, we have also in-
cluded a special null field type for the emission of
words that do not correspond to a specific record
field. Rule (6) defines the expansion of field F to
a sequence of (binarized) words W, with a weight
equal to the bigram probability of the current word
given the previous word, the current record, and
field.
Rules (8) and (9) define the emission of words and
integer numbers from W, given a field type and its
value. Rule (8) emits a single word from the vocabu-
lary of the training set. Its weight defines a multino-
mial distribution over all seen words, for every value
of field f , given that the field type is categorical or
the special null field. Rule (9) is identical but for
fields whose type is integer. Function g( f .v) gener-
ates an integer number given the field value, using
either of the following six ways (Liang et al, 2009):
identical to the field value, rounding up or rounding
down to a multiple of 5, rounding off to the clos-
est multiple of 5 and finally adding or subtracting
some unexplained noise.1 The weight is a multino-
mial over the six generation function modes, given
the record field f .
The CFG in Table 1 will produce many deriva-
tions for a given input (i.e., a set of database records)
which we represent compactly using a hypergraph or
a packed forest (Klein and Manning, 2001; Huang,
2008). Simplified examples of this representation
are shown in Figure 2.
3.2 Hypergraph Reranking
For our generation task, we are given a set of
database records d, and our goal is to find the best
corresponding text w. This corresponds to the best
grammar derivation among a set of candidate deriva-
tions represented implicitly in the hypergraph struc-
ture. As shown in Table 1, the mapping from d to w
is unknown. Therefore, all the intermediate multino-
mial distributions, described in the previous section,
define a hidden correspondence structure h, between
records, fields, and their values. We find the best
1The noise is modeled as a geometric distribution.
Algorithm 1: Averaged Structured Perceptron
Input: Training scenarios: (di,w?,h+i )
N
i=1
1 ?? 0
2 for t? 1 . . .T do
3 for i? 1 . . .N do
4 (w?, h?) = argmaxw,h? ??(di,wi,hi)
5 if (w?i ,h
+
i ) 6= (w?i, h?i) then
6 ?? ?+?(di,w?i ,h
+
i )??(di, w?i, h?i)
7 return 1T ?
T
t=1
1
N ?
N
i=1?
i
t
scoring derivation (w?, h?) by maximizing over con-
figurations of h:
(w?, h?) = argmax
w,h
? ??(d,w,h)
We define the score of (w?, h?) as the dot product
between a high dimensional feature representation
?= (?1, . . . ,?m) and a weight vector ?.
We estimate the weights ? using the averaged
structured perceptron algorithm (Collins, 2002),
which is well known for its speed and good perfor-
mance in similar large-parameter NLP tasks (Liang
et al, 2006; Huang, 2008). As shown in Algo-
rithm 1, the perceptron makes several passes over
the training scenarios, and in each iteration it com-
putes the best scoring (w?, h?) among the candidate
derivations, given the current weights ?. In line 6,
the algorithm updates ? with the difference (if any)
between the feature representations of the best scor-
ing derivation (w?, h?) and the the oracle derivation
(w?,h+). Here, w? is the estimated text, w? the gold-
standard text, h? is the estimated latent configuration
of the model and h+ the oracle latent configuration.
The final weight vector ? is the average of weight
vectors over T iterations and N scenarios. This av-
eraging procedure avoids overfitting and produces
more stable results (Collins, 2002).
In the following, we first explain how we decode
in this framework, i.e., find the best scoring deriva-
tion (Section 3.3) and discuss our definition for the
oracle derivation (w?,h+) (Section 3.4). Our fea-
tures are described in Section 4.2.
3.3 Hypergraph Decoding
Following Huang (2008), we also distinguish fea-
tures into local, i.e., those that can be computed
within the confines of a single hyperedge, and non-
local, i.e., those that require the prior visit of nodes
other than their antecedents. For example, the
372
Alignment feature in Figure 2(a) is local, and thus
can be computed a priori, but the Word Trigrams
is not; in Figure 2(b) words in parentheses are sub-
generations created so far at each word node; their
combination gives rise to the trigrams serving as
input to the feature. However, this combination
may not take place at their immediate ancestors,
since these may not be adjacent nodes in the hy-
pergraph. According to the grammar in Table 1,
there is no direct hyperedge between nodes repre-
senting words (W) and nodes representing the set of
fields these correspond to (FS); rather, W and FS are
connected implicitly via individual fields (F). Note,
that in order to estimate the trigram feature at the
FS node, we need to carry word information in the
derivations of its antecedents, as we go bottom-up.2
Given these two types of features, we can then
adapt Huang?s (2008) approximate decoding algo-
rithm to find (w?, h?). Essentially, we perform bottom-
up Viterbi search, visiting the nodes in reverse topo-
logical order, and keeping the k-best derivations for
each. The score of each derivation is a linear com-
bination of local and non-local features weights. In
machine translation, a decoder that implements for-
est rescoring (Huang and Chiang, 2007) uses the lan-
guage model as an external criterion of the good-
ness of sub-translations on account of their gram-
maticality. Analogously here, non-local features in-
fluence the selection of the best combinations, by
introducing knowledge that exceeds the confines of
the node under consideration and thus depend on
the sub-derivations generated so far. (e.g., word tri-
grams spanning a field node rely on evidence from
antecedent nodes that may be arbitrarily deeper than
the field?s immediate children).
Our treatment of leaf nodes (see rules (8) and (9))
differs from the way these are usually handled in
parsing. Since in generation we must emit rather
than observe the words, for each leaf node we there-
fore output the k-best words according to the learned
weights ? of the Alignment feature (see Sec-
tion 4.2), and continue building our sub-generations
bottom-up. This generation task is far from triv-
ial: the search space on the word level is the size of
the vocabulary and each field of a record can poten-
tially generate all words. Also, note that in decoding
it is useful to have a way to score different output
2We also store field information to compute structural fea-
tures, described in Section 4.2.
lengths |w|. Rather than setting w to a fixed length,
we rely on a linear regression predictor that uses the
counts of each record type per scenario as features
and is able to produce variable length texts.
3.4 Oracle Derivation
So far we have remained agnostic with respect to
the oracle derivation (w?,h+). In other NLP tasks
such as syntactic parsing, there is a gold-standard
parse, that can be used as the oracle. In our gener-
ation setting, such information is not available. We
do not have the gold-standard alignment between the
database records and the text that verbalizes them.
Instead, we approximate it using the existing de-
coder to find the best latent configuration h+ given
the observed words in the training text w?.3 This is
similar in spirit to the generative alignment model of
Liang et al (2009).
4 Experimental Design
In this section we present our experimental setup for
assessing the performance of our model. We give
details on our dataset, model parameters and fea-
tures, the approaches used for comparison, and ex-
plain how system output was evaluated.
4.1 Dataset
We conducted our experiments on the Air Travel In-
formation System (ATIS) dataset (Dahl et al, 1994)
which consists of transcriptions of spontaneous ut-
terances of users interacting with a hypothetical on-
line flight booking system. The dataset was orig-
inally created for the development of spoken lan-
guage systems and is partitioned in individual user
turns (e.g., flights from orlando to milwaukee, show
flights from orlando to milwaukee leaving after six
o?clock) each accompanied with an SQL query to a
booking system and the results of this query. These
utterances are typically short expressing a specific
communicative goal (e.g., a question about the ori-
gin of a flight or its time of arrival). This inevitably
results in small scenarios with a few words that of-
ten unambiguously correspond to a single record. To
avoid training our model on a somewhat trivial cor-
pus, we used the dataset introduced in Zettlemoyer
3In machine translation, Huang (2008) provides a soft al-
gorithm that finds the forest oracle, i.e., the parse among the
reranked candidates with the highest Parseval F-score. How-
ever, it still relies on the gold-standard reference translation.
373
and Collins (2007) instead, which combines the ut-
terances of a single user in one scenario and con-
tains 5,426 scenarios in total; each scenario corre-
sponds to a (manually annotated) formal meaning
representation (?-expression) and its translation in
natural language.
Lambda expressions were automatically con-
verted into records, fields and values following the
conventions adopted in Liang et al (2009).4 Given
a lambda expression like the one shown in Figure 1,
we first create a record for each variable and constant
(e.g., x, 9, august). We then assign record types ac-
cording to the corresponding class types (e.g., vari-
able x has class type flight). Next, fields and val-
ues are added from predicates with two arguments
with the class type of the first argument matching
that of the record type. The name of the predicate
denotes the field, and the second argument denotes
the value. We also defined special record types, such
as condition and search. The latter is introduced for
every lambda operator and assigned the categorical
field what with the value flight which refers to the
record type of variable x.
Contrary to datasets used in previous generation
studies (e.g., ROBOCUP (Chen and Mooney, 2008)
and WEATHERGOV (Liang et al, 2009)), ATIS has a
much richer vocabulary (927 words); each scenario
corresponds to a single sentence (average length
is 11.2 words) with 2.65 out of 19 record types
mentioned on average. Following Zettlemoyer and
Collins (2007), we trained on 4,962 scenarios and
tested on ATIS NOV93 which contains 448 examples.
4.2 Features
Broadly speaking, we defined two types of features,
namely lexical and structural ones. In addition,
we used a generatively trained PCFG as a baseline
feature and an alignment feature based on the co-
occurrence of records (or fields) with words.
Baseline Feature This is the log score of a gen-
erative decoder trained on the PCFG from Table 1.
We converted the grammar into a hypergraph, and
learned its probability distributions using a dynamic
program similar to the inside-outside algorithm (Li
and Eisner, 2009). Decoding was performed approx-
4The resulting dataset and a technical report describ-
ing the mapping procedure in detail are available from
http://homepages.inf.ed.ac.uk/s0793019/index.php?
page=resources
imately via cube pruning (Chiang, 2007), by inte-
grating a trigram language model extracted from the
training set (see Konstas and Lapata (2012) for de-
tails). Intuitively, the feature refers to the overall
goodness of a specific derivation, applied locally in
every hyperedge.
Alignment Features Instances of this feature fam-
ily refer to the count of each PCFG rule from Ta-
ble 1. For example, the number of times rule
R(search1.t)? FS( f light1,start)R( f light1.t) is in-
cluded in a derivation (see Figure 2(a))
Lexical Features These features encourage gram-
matical coherence and inform lexical selection over
and above the limited horizon of the language model
captured by Rules (6)?(9). They also tackle anoma-
lies in the generated output, due to the ergodicity of
the CFG rules at the record and field level:
Word Bigrams/Trigrams This is a group of
non-local feature functions that count word n-grams
at every level in the hypergraph (see Figure 2(b)).
The integration of words in the sub-derivations is
adapted from Chiang (2007).
Number of Words per Field This feature function
counts the number of words for every field, aiming
to capture compound proper nouns and multi-word
expressions, e.g., fields from and to frequently corre-
spond to two or three words such as ?new york? and
?salt lake city? (see Figure 2(d)).
Consecutive Word/Bigram/Trigram This feature
family targets adjacent repetitions of the same word,
bigram or trigram, e.g., ?show me the show me the
flights?.
Structural Features Features in this category tar-
get primarily content selection and influence appro-
priate choice at the field level:
Field bigrams/trigrams Analogously to the lexical
features mentioned above, we introduce a series of
non-local features that capture field n-grams, given
a specific record. For example the record flight in the
air travel domain typically has the values <from to>
(see Figure 2(c)). The integration of fields in sub-
derivations is implemented in fashion similar to the
integration of words.
Number of Fields per Record This feature family
is a coarser version of the Field bigrams/trigrams
374
R(search1.t)
FS(flight1.t,start) R(flight1.t)
FS0,3(search1.t,start)
w0(search1.t,type) ? ? ? w1,2(search1.t,what)
?
?
?
show
me
what
? ? ?
?
?
?
?
?
?
me the
me f lights
the f lights
? ? ?
?
?
?
FS2,6(flight1.t,start)
F2,4(flight1.t,from) FS4,6(flight1.t,from)
F4,6(flight1.t,to) ?
| 2 words |
(b)Word Trigrams (non-local)
<show me the>, <show me flights>, etc.
(a)Alignment Features (local)
<R(srch1.t)? FS(fl1.t,st) R(fl1.t)>
(c)Field Bigrams (non-local)
<from to> | flight
(d)Number of Words per Field (local)
<2 | from>
Figure 2: Simplified hypergraph examples with corresponding local and non-local features.
feature, which is deemed to be sparse for rarely-seen
records.
Field with No Value Although records in the ATIS
database schema have many fields, only a few are
assigned a value in any given scenario. For exam-
ple, the flight record has 13 fields, of which only 1.7
(on average) have a value. Practically, in a genera-
tive model this kind of sparsity would result in very
low field recall. We thus include an identity feature
function that explicitly counts whether a particular
field has a value.
4.3 Evaluation
We evaluated three configurations of our
model. A system that only uses the top scor-
ing derivation in each sub-generation and in-
corporates only the baseline and alignment
features (1-BEST+BASE+ALIGN). Our sec-
ond system considers the k-best derivations
and additionally includes lexical features
(k-BEST+BASE+ALIGN+LEX). The number of
k-best derivations was set to 40 and estimated
experimentally on held-out data. And finally,
our third system includes the full feature set
(k-BEST+BASE+ALIGN+LEX+STR). Note, that
the second and third system incorporate non-local
features, hence the use of k-best derivation lists.5
We compared our model to Angeli et al (2010)
whose approach is closest to ours.6
We evaluated system output automatically, using
the BLEU-4 modified precision score (Papineni et
5Since the addition of these features, essentially incurs
reranking, it follows that the systems would exhibit the exact
same performance as the baseline system with 1-best lists.
6We are grateful to Gabor Angeli for providing us with the
code of his system.
al., 2002) with the human-written text as reference.
We also report results with the METEOR score
(Banerjee and Lavie, 2005), which takes into ac-
count word re-ordering and has been shown to cor-
relate better with human judgments at the sentence
level. In addition, we evaluated the generated text by
eliciting human judgments. Participants were pre-
sented with a scenario and its corresponding verbal-
ization (see Figure 3) and were asked to rate the lat-
ter along two dimensions: fluency (is the text gram-
matical and overall understandable?) and semantic
correctness (does the meaning conveyed by the text
correspond to the database input?). The subjects
used a five point rating scale where a high number
indicates better performance. We randomly selected
12 documents from the test set and generated out-
put with two of our models (1-BEST+BASE+ALIGN
and k-BEST+BASE+ALIGN+LEX+STR) and Angeli
et al?s (2010) model. We also included the original
text (HUMAN) as a gold standard. We thus obtained
ratings for 48 (12? 4) scenario-text pairs. The study
was conducted over the Internet, using Amazon Me-
chanical Turk, and was completed by 51 volunteers,
all self reported native English speakers.
5 Results
Table 2 summarizes our results. As can be seen, in-
clusion of lexical features gives our decoder an ab-
solute increase of 6.73% in BLEU over the 1-BEST
system. It also outperforms the discriminative sys-
tem of Angeli et al (2010). Our lexical features
seem more robust compared to their templates. This
is especially the case with infrequent records, where
their system struggles to learn any meaningful infor-
mation. Addition of the structural features further
boosts performance. Our model increases by 8.69%
375
System BLEU METEOR
1-BEST+BASE+ALIGN 21.93 34.01
k-BEST+BASE+ALIGN+LEX 28.66 45.18
k-BEST+BASE+ALIGN+LEX+STR 30.62 46.07
ANGELI 26.77 42.41
Table 2: BLEU-4 and METEOR results on ATIS.
over the 1-BEST system and 3.85% over ANGELI in
terms of BLEU. We observe a similar trend when
evaluating system output with METEOR. Differ-
ences in magnitude are larger with the latter metric.
The results of our human evaluation study are
shown in Table 5. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system
type (1-BEST, k-BEST, ANGELI, and HUMAN) on
the fluency and semantic correctness ratings. Means
differences were compared using a post-hoc Tukey
test. The k-BEST system is significantly better than
the 1-BEST and ANGELI (a < 0.01) both in terms
of fluency and semantic correctness. ANGELI is
significantly better than 1-BEST with regard to flu-
ency (a < 0.05) but not semantic correctness. There
is no statistically significant difference between the
k-BEST output and the original sentences (HUMAN).
Examples of system output are shown in Table 3.
They broadly convey similar meaning with the gold-
standard; ANGELI exhibits some long-range repeti-
tion, probably due to re-iteration of the same record
patterns. We tackle this issue with the inclusion of
non-local structural features. The 1-BEST system
has some grammaticality issues, which we avoid by
defining features over lexical n-grams and repeated
words. It is worth noting that both our system and
ANGELI produce output that is semantically com-
patible with but lexically different from the gold-
standard (compare please list the flights and show
me the flights against give me the flights). This is
expected given the size of the vocabulary, but raises
concerns regarding the use of automatic metrics for
the evaluation of generation output.
6 Conclusions
We presented a discriminative reranking framework
for an end-to-end generation system that performs
both content selection and surface realization. Cen-
tral to our approach is the encoding of generation
as a parsing problem. We reformulate the input (a
set of database records and text describing some of
System FluencySemCor
1-BEST+BASE+ALIGN 2.70 3.05
k-BEST+BASE+ALIGN+LEX+STR 4.02 4.04
ANGELI 3.74 3.17
HUMAN 4.18 4.02
Table 3: Mean ratings for fluency and semantic correct-
ness (SemCor) on system output elicited by humans.
Flight
from to
phoenix milwaukee
Time
when dep/ar
evening departure
Day
day dep/ar
wednesday departure
Search
type what
query flight
H
U
M
A
N
A
N
G
E
L
I
k-
B
E
S
T
1-
B
E
S
T
give me the flights from phoenix to milwaukee on
wednesday evening
show me the flights from phoenix to milwaukee on
wednesday evening flights from phoenix to milwaukee
please list the flights from phoenix to milwaukee on
wednesday evening
on wednesday evening from from phoenix to
milwaukee on wednesday evening
Figure 3: Example of scenario input and system output.
them) as a PCFG and convert it to a hypergraph. We
find the best scoring derivation via forest reranking
using both local and non-local features, that we train
using the perceptron algorithm. Experimental eval-
uation on the ATIS dataset shows that our model at-
tains significantly higher fluency and semantic cor-
rectness than any of the comparison systems. The
current model can be easily extended to incorporate,
additional, more elaborate features. Likewise, it can
port to other domains with similar database struc-
ture without modification, such as WEATHERGOV
and ROBOCUP. Finally, distributed training strate-
gies have been developed for the perceptron algo-
rithm (McDonald et al, 2010), which would allow
our generator to scale to even larger datasets.
In the future, we would also like to tackle more
challenging domains (e.g., product descriptions) and
to enrich our generator with some notion of dis-
course planning. An interesting question is how to
extend the PCFG-based approach advocated here so
as to capture discourse-level document structure.
376
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502?512, Cambridge, MA.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor, Michigan, June.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128?135, Helsinki, Finland.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the 17th In-
ternational Conference on Machine Learning, pages
175?182, Stanford, California.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing, pages 1?8, Philadelphia, Penn-
sylvania.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In Proceedings of the Work-
shop on Human Language Technology, pages 43?48,
Plainsboro, New Jersey.
Robert Dale, Sabine Geldof, and Jean-Philippe Prost.
2003. Coral: Using natural language generation for
navigational assistance. In Proceedings of the 26th
Australasian Computer Science Conference, pages
35?44, Adelaide, Australia.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and appli-
cations. Discrete Applied Mathematics, 42:177?201.
Ruifang Ge and Raymond J. Mooney. 2006. Discrimina-
tive reranking for semantic parsing. In Proceedings of
the COLING/ACL 2006 Main Conference Poster Ses-
sions, pages 263?270, Sydney, Australia.
Nancy Green. 2006. Generation of biomedical argu-
ments for lay readers. In Proceedings of the 5th In-
ternational Natural Language Generation Conference,
pages 114?121, Sydney, Australia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the 7th Interna-
tional Workshop on Parsing Technologies, pages 123?
134, Beijing, China.
Ioannis Konstas and Mirella Lapata. 2012. Unsuper-
vised concept-to-text generation with hypergraphs. To
appear in Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Montre?al, Canada.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 40?51, Suntec, Sin-
gapore.
Zhifei Li and Sanjeev Khudanpur. 2009. Forest rerank-
ing for machine translation with the perceptron algo-
rithm. In GALE Book. GALE.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 761?768,
Sydney, Australia.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 91?99, Suntec, Singapore.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 456?464, Los Angeles, CA, June. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
377
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia.
Adwait Ratnaparkhi. 2002. Trainable approaches to sur-
face natural language generation and their application
to conversational dialog systems. Computer Speech &
Language, 16(3-4):435?455.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167:137?169.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
HLT-NAACL 2004: Main Proceedings, pages 177?
184, Boston, Massachusetts.
Ross Turner, Yaji Sripada, and Ehud Reiter. 2009. Gen-
erating approximate geographic descriptions. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 42?49, Athens, Greece.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172?179, Rochester, NY.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to log-
ical form. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 678?687, Prague, Czech
Republic.
378
