Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1005?1014, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning to Merge Word Senses
Rion Snow Sushant Prakash
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{rion,sprakash}@cs.stanford.edu
Daniel Jurafsky
Linguistics Department
Stanford University
Stanford, CA 94305 USA
jurafsky@stanford.edu
Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305 USA
ang@cs.stanford.edu
Abstract
It has been widely observed that different NLP appli-
cations require different sense granularities in order to
best exploit word sense distinctions, and that for many
applications WordNet senses are too fine-grained. In
contrast to previously proposed automatic methods for
sense clustering, we formulate sense merging as a su-
pervised learning problem, exploiting human-labeled
sense clusterings as training data. We train a discrimi-
native classifier over a wide variety of features derived
from WordNet structure, corpus-based evidence, and
evidence from other lexical resources. Our learned
similarity measure outperforms previously proposed
automatic methods for sense clustering on the task of
predicting human sense merging judgments, yielding
an absolute F-score improvement of 4.1% on nouns,
13.6% on verbs, and 4.0% on adjectives. Finally, we
propose a model for clustering sense taxonomies us-
ing the outputs of our classifier, and we make avail-
able several automatically sense-clustered WordNets
of various sense granularities.
1 Introduction
Defining a discrete inventory of senses for a word is
extremely difficult (Kilgarriff, 1997; Hanks, 2000;
Palmer et al, 2005). Perhaps the greatest obstacle is
the dynamic nature of sense definition: the correct
granularity for word senses depends on the appli-
cation. For language learners, a fine-grained set of
word senses may help in learning subtle distinctions,
while coarsely-defined senses are probably more
useful in NLP tasks like information retrieval (Gon-
zalo et al, 1998), query expansion (Moldovan and
Mihalcea, 2000), and WSD (Resnik and Yarowsky,
1999; Palmer et al, 2005).
Lexical resources such as WordNet (Fellbaum,
1998) use extremely fine-grained notions of word
sense, which carefully capture even minor distinc-
tions between different possible word senses (e.g.,
the 8 noun senses of bass shown in Figure 1). Pro-
ducing sense-clustered inventories of arbitrary sense
granularity is thus crucial for tasks which depend on
lexical resources like WordNet, and is also impor-
tant for the task of automatically constructing new
WordNet-like taxonomies. A solution to this prob-
lem must also deal with the constraints of the Word-
Net taxonomy itself; for example when clustering
two senses, we need to consider the transitive effects
of merging synsets.
The state of the art in sense clustering is insuffi-
cient to meet these needs. Current sense clustering
algorithms are generally unsupervised, each relying
on a different set of useful features or hand-built
rules. But hand-written rules have little flexibility
to produce clusterings of different granularities, and
previously proposed methods offer little in the di-
rection of intelligently combining and weighting the
many proposed features.
In response to these challenges, we propose a
new algorithm for clustering large-scale sense hier-
archies like WordNet. Our algorithm is based on a
supervised classifier that learns to make graduated
judgments corresponding to the estimated probabil-
ity that each particular sense pair should be merged.
This classifier is trained on gold standard sense clus-
tering judgments using a diverse feature space. We
are able to use the outputs of our classifier to produce
a ranked list of sense merge judgments by merge
probability, and from this create sense-clustered in-
ventories of arbitrary sense granularity.1
In Section 2 we discuss past work in sense cluster-
1We have made sense-clustered Wordnets using the al-
gorithms discussed in this paper available for download at
http://ai.stanford.edu/?rion/swn.
1005
INSTRUMENT 7: ...the lowest range of a family of musical instruments
FISH
4: the lean flesh of a saltwater fish of the family Serranidae
5: any of various North American freshwater fish with lean flesh
8: nontechnical name for any of numerous... fishes
SINGER
3: an adult male singer with the lowest voice
6: the lowest adult male singing voice
PITCH
1: the lowest part of the musical range
2: the lowest part in polyphonic music
Figure 1: Sense clusters for the noun bass; the eight
WordNet senses as clustered into four groups in the
SENSEVAL-2 coarse-grained evaluation data
ing, and the gold standard datasets that we use in our
work. In Section 3 we introduce our battery of fea-
tures; in Section 4 we show how to extend our sense-
merging model to cluster full taxonomies like Word-
Net. In Section 5 we evaluate our classifier against
thirteen previously proposed methods.
2 Background
A wide number of manual and automatic techniques
have been proposed for clustering sense inventories
and mapping between sense inventories of different
granularities. Much work has gone into methods for
measuring synset similarity; early work in this direc-
tion includes (Dolan, 1994), which attempted to dis-
cover sense similarities between dictionary senses.
A variety of synset similarity measures based on
properties of WordNet itself have been proposed;
nine such measures are discussed in (Pedersen et al,
2004), including gloss-based heuristics (Lesk, 1986;
Banerjee and Pedersen, 2003), information-content
based measures (Resnik, 1995; Lin, 1998; Jiang and
Conrath, 1997), and others. Other approaches have
used specific cues from WordNet structure to inform
the construction of semantic rules; for example, (Pe-
ters et al, 1998) suggest clustering two senses based
on a wide variety of structural cues from Word-
Net, including if they are twins (if two synsets share
more than one word in their synonym list) or if
they represent an example of autohyponymy (if one
sense is the direct descendant of the other). (Mihal-
cea and Moldovan, 2001) implements six semantic
rules, using twin and autohyponym features, in addi-
tion to other WordNet-structure-based rules such as
whether two synsets share a pertainym, antonym, or
are clustered together in the same verb group.
A large body of work has attempted to capture
corpus-based estimates of word similarity (Pereira
et al, 1993; Lin, 1998); however, the lack of
large sense-tagged corpora prevent most such tech-
niques from being used effectively to compare dif-
ferent senses of the same word. Some corpus-based
attempts that are capable of estimating similarity
between word senses include the topic signatures
method; here, (Agirre and Lopez, 2003) collect con-
texts for a polysemous word based either on sense-
tagged corpora or by using a weighted agglomera-
tion of contexts of a polysemous word?s monose-
mous relatives (i.e., single-sense synsets related by
hypernym, hyponym, or other relations) from some
large untagged corpus. Other corpus-based tech-
niques developed specifically for sense clustering
include (McCarthy, 2006), which uses a combina-
tion of word-to-word distributional similarity com-
bined with the JCN WordNet-based similarity mea-
sure, and work by (Chugur et al, 2002) in find-
ing co-occurrences of senses within documents in
sense-tagged corpora. Other attempts have exploited
disagreements between WSD systems (Agirre and
Lopez, 2003) or between human labelers (Chklovski
and Mihalcea, 2003) to create synset similarity
measures; while promising, these techniques are
severely limited by the performance of the WSD
systems or the amount of available labeled data.
Some approaches for clustering have made use of
regular patterns of polysemy among words. (Pe-
ters et al, 1998) uses the COUSIN relation defined
in WordNet 1.5 to cluster hyponyms of categorically
related noun synsets, e.g., ?container/quantity? (e.g.,
for clustering senses of ?cup? or ?barrel?) or ?or-
ganization/construction? (e.g., for the building and
institution senses of ?hospital? or ?school?); other
approaches based on systematic polysemy include
the hand-constructed CORELEX database (Buite-
laar, 1998), and automatic attempts to extract pat-
terns of systematic polysemy based on minimal de-
scription length principles (Tomuro, 2001).
Another family of approaches has been to
use either manually-annotated or automatically-
constructed mappings to coarser-grained sense in-
ventories; an attempt at providing coarse-grained
sense distinctions for the SENSEVAL-1 exercise in-
cluded a mapping between WordNet and the Hec-
tor lexicon (Palmer et al, 2005). Other attempts in
1006
this vein include mappings between WordNet and
PropBank (Palmer et al, 2004) and mappings to
Levin classes (Levin, 1993; Palmer et al, 2005).
(Navigli, 2006) presents an automatic approach for
mapping between sense inventories; here similari-
ties in gloss definition and structured relations be-
tween the two sense inventories are exploited in or-
der to map between WordNet senses and distinc-
tions made within the coarser-grained Oxford En-
glish Dictionary. Other work has attempted to ex-
ploit translational equivalences of WordNet senses
in other languages, for example using foreign lan-
guage WordNet interlingual indexes (Gonzalo et al,
1998; Chugur et al, 2002).
2.1 Gold standard sense clustering data
Our approach for learning how to merge senses
relies upon the availability of labeled judgments
of sense relatedness. In this work we focus on
two datasets of hand-labeled sense groupings for
WordNet: first, a dataset of sense groupings over
nouns, verbs, and adjectives provided as part of
the SENSEVAL-2 English lexical sample WSD task
(Kilgarriff, 2001), and second, a corpus-driven map-
ping of nouns and verbs in WordNet 2.1 to the
Omega Ontology (Philpot et al, 2005), produced as
part of the ONTONOTES project (Hovy et al, 2006).
A wide variety of semantic and syntactic criteria
were used to produce the SENSEVAL-2 groupings
(Palmer et al, 2004; Palmer et al, 2005); this data
covers all senses of 411 nouns, 519 verbs, and 257
adjectives, and has been used as gold standard sense
clustering data in previous work (Agirre and Lopez,
2003; McCarthy, 2006)2. The number of judgments
within this data (after mapping to WordNet 2.1) is
displayed in Table 1.
Due to a lack of interannotator agreement data for
this dataset, (McCarthy, 2006) performed an anno-
tation study using three labelers on a 20-noun sub-
set of the SENSEVAL-2 groupings; the three label-
ers were given the task of deciding whether the 351
potentially-related sense pairs were ?Related?, ?Un-
related?, or ?Don?t Know?.3 In this task the pair-
2In order to facilitate future work in this area, we
have made cleaned versions of these groupings available at
http://ai.stanford.edu/?rion/swn along with a ?diff? with the
original files.
3McCarthy?s gold standard data is available at
SENSEVAL-2
POS Total Pairs Merged Pairs Proportion
Nouns 16403 2593 0.1581
Verbs 30688 3373 0.1099
Adjectives 8368 2209 0.2640
ONTONOTES
POS Total Pairs Merged Pairs Proportion
Nouns 3552 347 0.0977
Verbs 4663 1225 0.2627
Table 1: Gold standard datasets for sense merging;
only sense pairs that share a word in common are
included; proportion refers to the fraction of synsets
sharing a word that have been merged
POS Overlap ON-True ON-False F-Score
S-T S-F S-T S-F
Nouns 2116 121 55 181 1759 0.5063
Verbs 3297 351 503 179 2264 0.5072
Table 2: Agreement data for gold standard datasets
wise interannotator F-scores were (0.4874, 0.5454,
0.7926), for an average F-score of 0.6084.
The ONTONOTES dataset4 covers a smaller set
of nouns and verbs, but it has been created with a
more rigorous corpus-based iterative annotation pro-
cess. For each of the nouns and verbs in question, a
50-sentence sample of instances is annotated using
a preliminary set of sense distinctions; if the word
sense interannotator agreement for the sample is less
than 90%, then the sense distinctions are revised and
the sample is re-annotated, and so forth, until an in-
terannotator agreement of at least 90% is reached.
We construct a combined gold standard set from
these SENSEVAL-2 and ONTONOTES groupings,
removing disagreements. The overlap and agree-
ment/disagreement data between the two groupings
is given in Table 2; here, for example, the column
with ON-True and S-F indicates the count of senses
that ONTONOTES judged as positive examples of
sense merging, but that SENSEVAL-2 data did not
merge. We also calculate the F-score achieved by
considering only one of the datasets as a gold stan-
dard, and computing precision and recall for the
other. Since the two datasets were created indepen-
dently, with different annotation guidelines, we can-
ftp://ftp.informatics.susx.ac.uk/pub/users/dianam/relateGS/.
4The OntoNotes groupings will be available through the
LDC at http://www.ldc.upenn.edu.
1007
not consider this as a valid estimate of interannota-
tor agreement; nonetheless the F-score for the two
datasets on the overlapping set of sense judgments
(50.6% for nouns and 50.7% for verbs) is roughly
in the same range as those observed in (McCarthy,
2006).
3 Learning to merge word senses
3.1 WordNet-based features
Here we describe the feature space we construct for
classifying whether or not a pair of synsets should be
merged; first, we employ a wide variety of linguistic
features based on information derived from Word-
Net. We use eight similarity measures implemented
within the WordNet::Similarity package5, described
in (Pedersen et al, 2004); these include three mea-
sures derived from the paths between the synsets
in WordNet: HSO (Hirst and St-Onge, 1998), LCH
(Leacock and Chodorow, 1998), and WUP (Wu and
Palmer, 1994); three measures based on information
content: RES (Resnik, 1995), LIN (Lin, 1998), and
JCN (Jiang and Conrath, 1997); the gloss-based Ex-
tended Lesk Measure LESK, (Banerjee and Peder-
sen, 2003), and finally the gloss vector similarity
measure VECTOR (Patwardan, 2003). We imple-
ment the TWIN feature (Peters et al, 1998), which
counts the number of shared synonyms between
the two synsets. Additionally we produce pair-
wise features indicating whether two senses share an
ANTONYM, PERTAINYM, or derivationally-related
forms (DERIV). We also create the verb-specific
features of whether two verb synsets are linked in
a VERBGROUP (indicating semantic similarity) or
share a VERBFRAME, indicating syntactic similar-
ity. Also, we encode a generalized notion of sib-
linghood in the MN features, recording the distance
of the synset pair?s nearest least common subsumer
(i.e., closest shared hypernym) from the two synsets,
and, separately, the maximum of those distances (in
the MAXMN feature.
Previous attempts at categorizing systematic pol-
ysemy patterns within WordNet has resulted in the
COUSIN feature6; we create binary features which
5We choose not to use the PATH measure due to its negligible
difference from the LCH measure.
6This data is included in the WordNet 1.6 distribution as the
?cousin.tops? file.
indicate whether a synset pair belong to hypernym
ancestries indicated by one or more of these COUSIN
features, and the specific cousin pair(s) involved.
Finally we create sense-specific features, including
SENSECOUNT, the total number of senses associ-
ated with the shared word between the two synsets
with the highest number of senses, and SENSENUM,
the specific pairing of senses for the shared word
with the highest number of senses (which might al-
low us to learn whether the most frequent sense of a
word has a higher chance of having similar deriva-
tive senses with lower frequency).
3.2 Features derived from corpora and other
lexical resources
In addition to WordNet-based features, we use
a number of features derived from corpora and
other lexical resources. We use the publicly avail-
able topic signature data7 described in (Agirre and
Lopez, 2004), yielding representative contexts for
all nominal synsets from WordNet 1.6. These topic
signatures were obtained by weighting the contexts
of monosemous relatives of each noun synset (i.e.,
single-sense synsets related by hypernym, hyponym,
or other relations); the text for these contexts were
extracted from snippets using the Google search en-
gine. We then create a sense similarity feature by
taking a thresholded cosine similarity between pairs
of topic signatures for these noun synsets.
Additionally, we use the WordNet domain dataset
described in (Magnini and Cavaglia, 2000; Ben-
tivogli et al, 2004). This dataset contains one or
more labels indicating of 164 hierarchically orga-
nized ?domains? or ?subject fields? for each noun,
verb, and adjective synset in WordNet; we derive a
set of binary features from this data, with a single
feature indicating whether or not two synsets share
a domain, and one indicator feature per pair of do-
mains indicating respective membership of the sense
pair within those domains.
Finally, we use as a feature the mappings pro-
duced in (Navigli, 2006) of WordNet senses to Ox-
ford English Dictionary senses. This OED dataset
was used as the coarse-grained sense inventory in the
Coarse-grained English all-words task of SemEval-
7The topic signature data is available for download at
http://ixa.si.ehu.es/Ixa/resources/sensecorpus.
1008
20078; we specify a single binary feature for each
pair of synsets from this data; this feature is true if
the words are clustered in the OED mapping, and
false otherwise.
3.3 Classifier, training, and feature selection
For each part of speech, we split the merged gold
standard data into a part-of-speech-specific train-
ing set (70%) and a held-out test set (30%). For
every synset pair we use the binary ?merged? or
?not-merged? labels to train a support vector ma-
chine classifier9 (Joachims, 2002) for each POS-
specific training set. We perform feature selection
and regularization parameter optimization using 10-
fold cross-validation.
4 Clustering Senses in WordNet
The previous section describes a classifier which
predicts whether two synsets should be merged; we
would like to use the pairwise judgments of this
classifier to cluster the senses within a sense hierar-
chy. In this section we present the challenge implicit
in applying sense merging to full taxonomies, and
present our model for clustering within a taxonomy.
4.1 Challenges of clustering a sense taxonomy
The task of clustering a sense taxonomy presents
certain challenges not present in the problem of clus-
tering the senses of a word; in order to create a
consistent clustering of a sense hierarchy an algo-
rithm must consider the transitive effects of merging
synsets. This problem is compounded in sense tax-
onomies like WordNet, where each synset may have
additional structured relations, e.g., hypernym (IS-
A) or holonym (is-part-of) links. In order to consis-
tently merge two noun senses with different hyper-
nym ancestries within WordNet, for example, an al-
gorithm must decide whether to have the new sense
inherit both hypernym ancestries, or whether to in-
herit only one, and if so it must decide which ances-
try is more relevant for the merged sense.
Without strict checking, human labelers will
likely find it difficult to label a sense inventory with
8http://lcl.di.uniroma1.it/coarse-grained-aw/index.html
9We use the SV Mperf package, freely available for non-
commercial use from http://svmlight.joachims.org; we use the
default settings in v2.00, except for the regularization parameter
(set in 10-fold cross-validation).
Clusering based on ??need??
Clustering based on ??require??
need#v#1
require#v#1 require as useful, just, or proper
need#v#2
require#v#4 have need of
need#v#3 have or feel a need for
require#v#1
need#v#1 require as useful, just, or proper
require#v#4
need#v#2 have need of
require#v#2 consider obligatory; request and expect
require#v#3 make someone do something
Figure 2: Inconsistent sense clusters for the verbs
require and need from SENSEVAL-2 judgments
transitively-consistent judgments. As an example,
consider the SENSEVAL-2 clusterings of the verbs
require and need, as shown in Figure 2. In WN 2.1
require has four verb senses, of which the first has
synonyms {necessitate, ask, postulate, need, take,
involve, call for, demand}, and gloss ?require as use-
ful, just, or proper?; and the fourth has synonyms
{want, need}, and gloss ?have need of.?
Within the word require, the SENSEVAL-2 dataset
clusters senses 1 and 4, leaving the rest unclustered.
In order to make a consistent clustering with respect
to the sense inventory, however, we must enforce
the transitive closure by merging the synset corre-
sponding to the first sense (necessitate, ask, need
etc.), with the senses of want and need in the fourth
sense. In particular, these two senses correspond
to WordNet 2.1 senses need#v#1 and need#v#2, re-
spectively, which are not clustered according to
the SENSEVAL-2 word-specific labeling for need ?
need#v#1 is listed as a singleton (i.e., unclustered)
sense, though need#v#2 is clustered with need#v#3,
?have or feel a need for.?
While one might hope that such disagreements
between sense clusterings are rare, we found
178 such transitive closure disagreements in the
SENSEVAL-2 data. The ONTONOTES data is much
cleaner in this respect, most likely due to the
stricter annotation standard (Hovy et al, 2006);
we found only one transitive closure disagreement
1009
in the OntoNotes data, specifically WordNet 2.1
synsets (head#n#2, lead#n#7: ?be in charge of?) and
(head#n#3, lead#v#4: ?travel in front of?) are clus-
tered under head but not under lead.
4.2 Sense clustering within a taxonomy
As a solution to the previously mentioned chal-
lenges, in order to produce taxonomies of different
sense granularities with consistent sense distinctions
we propose to apply agglomerative clustering over
all synsets in WordNet 2.1. While one might con-
sider recalculating synset similarity features after
each synset merge operation, depending on the fea-
ture set this could be prohibitively expensive; for our
purposes we use average-link agglomerative cluster-
ing, in effect approximating the the pairwise similar-
ity score between a given synset and a merged sense
as the average of the similarity scores between the
given synset and the clustered sense?s component
synsets. Further, for the purpose of sense cluster-
ing we assume a zero sense similarity score between
synsets with no intersecting words.
Without exploiting additional hypernym or
coordinate-term evidence, our algorithm does
not distinguish between judgments about which
hypernym ancestry or other structured relationships
to keep or remove upon merging two synsets. In
lieu of additional evidence, for our experiments
we choose to retain only the hypernym ancestry of
the sense with the highest frequency in SEMCOR,
breaking frequency ties by choosing the first-listed
sense in WordNet. We add every other relationship
(meronyms, entailments, etc.) to the new merged
sense (except in the rare case where adding a
relation would cause a cycle in acyclic relations like
hypernymy or holonymy, in which case we omit
it). Using this clustering method we have produced
several sense-clustered WordNets of varying sense
granularity, which we evaluate in Section 5.3.
5 Evaluation
We evaluate our classifier in a comparison with thir-
teen previously proposed similarity measures and
automatic methods for sense clustering. We conduct
a feature ablation study to explore the relevance of
the different features in our system. Finally, we eval-
uate the sense-clustered taxonomies we create on
the problem of providing improved coarse-grained
sense distinctions for WSD evaluation.
5.1 Evaluation of automatic sense merging
We evaluate our classifier on two held-out test
sets; first, a 30% sample of the sense judgments
from the merged gold standard dataset consisting
of both the SENSEVAL-2 and ONTONOTES sense
judgments; and, second, a test set consisting of only
the ONTONOTES subset of our first held-out test set.
For comparison we implement thirteen of the meth-
ods discussed in Section 2. First, we evaluate each
of the eight WordNet::Similarity measures individu-
ally. Next, we implement cosine similarity of topic
signatures (TOPSIG) built from monosemous rela-
tives (Agirre and Lopez, 2003), which provides a
real-valued similarity score for noun synset pairs.
Additionally, we implement the two methods
proposed in (Peters et al, 1998), namely using
metonymy clusters (MetClust) and generalization
clusters (GenClust) based on the COUSIN relation-
ship in WordNet. While (Peters et al, 1998) only
considers four cousin pairs, we re-implement their
method for general purpose sense clustering by us-
ing all 226 cousin pairs defined in WordNet 1.6,
mapped to WordNet 2.1 synsets. These methods
each provide a single clustering of noun synsets.
Next, we implement the set of semantic rules de-
scribed in (Mihalcea and Moldovan, 2001) (MIMO);
this algorithm for merging senses is based on 6 se-
mantic rules, in effect using a subset of the TWIN,
MAXMN, PERTAINYM, ANTONYM, and VERB-
GROUP features; in our implementation we set the
parameter for when to cluster based on number of
twins to K = 2; this results in a single clustering
for each of nouns, verbs, and adjectives. Finally, we
compare against the mapping from WordNet to the
Oxford English Dictionary constructed in (Navigli,
2006), equivalent to clustering based solely on the
OED feature.
Considering merging senses as a binary classifi-
cation task, Table 3 gives the F-score performance
of our classifier vs. the thirteen other classifiers and
an uninformed ?merge all synsets? baseline on our
held-out gold standard test set. This table shows that
our SVM classifier outperforms all implemented
methods on the basis of F-score on both datasets
1010
SENSEVAL-2 + ONTONOTES
ONTONOTES
Method Nouns Verbs Adj Nouns Verbs
SVM 0.4228 0.4319 0.4727 0.3698 0.4545
RES 0.3817 0.2703 ? 0.2807 0.3156
WUP 0.3763 0.2782 ? 0.3036 0.3451
LCH 0.3700 0.2440 ? 0.2857 0.3396
OED 0.3310 0.2878 0.3712 0.2183 0.3962
LESK 0.3174 0.2956 0.4323 0.2914 0.3774
HSO 0.3090 0.2784 0.4312 0.3025 0.3156
TOPSIG 0.3072 ? ? 0.2581 ?
VEC 0.2960 0.2315 0.4321 0.2454 0.3420
JCN 0.2818 0.2292 ? 0.2222 0.3156
LIN 0.2759 0.2464 ? 0.2056 0.3471
Baseline 0.2587 0.2072 0.4312 0.1488 0.3156
MIMO 0.0989 0.2142 0.0759 0.1833 0.2157
GenClust 0.0973 ? ? 0.0264 ?
MetClust 0.0876 ? ? 0.0377 ?
Table 3: F-score sense merging evaluation on hand-
labeled testsets
for all parts of speech. In Figure 3 we give a pre-
cision/recall plot for noun sense merge judgments
for the SENSEVAL-2 + ONTONOTES dataset. For
sake of simplicity we plot only the two best mea-
sures (RES and WUP) of the eight WordNet-based
similarity measures; we see that our classifier, RES,
and WUP each have higher precision all levels of
recall compared to the other tested measures.
Of the methods we compare against, only the
WordNet-based similarity measures, (Mihalcea and
Moldovan, 2001), and (Navigli, 2006) provide a
method for predicting verb similarities; our learned
measure widely outperforms these methods, achiev-
ing a 13.6% F-score improvement over the LESK
similarity measure. In Figure 4 we give a pre-
cision/recall plot for verb sense merge judgments,
plotting the performance of the three best WordNet-
based similarity measures; here we see that our clas-
sifier has significantly higher precision than all other
tested measures at nearly every level of recall.
Only the measures provided by LESK, HSO,
VEC, (Mihalcea and Moldovan, 2001), and (Nav-
igli, 2006) provide a method for predicting adjective
similarities; of these, only LESK and VEC outper-
form the uninformed baseline on adjectives, while
our learned measure achieves a 4.0% improvement
over the LESK measure on adjectives.
5.2 Feature analysis
Next we analyze our feature space. Table 4 gives the
ablation analysis for all features used in our system
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Precision/Recall for Merging Nouns
SVM Classifier
Resnik Measure
Wu & Palmer Measure
Topic Signatures
OED Mapping
Generalization Clusters
Metonym Clusters
Semantic Rules
Figure 3: Precision/Recall plot for noun sense merge
judgments
as evaluated on our held-out test set; here the quan-
tity listed in the table is the F-score loss obtained by
removing that single feature from our feature space,
and retraining and retesting our classifiers, keeping
everything else the same. Here negative scores cor-
respond to an improvement in classifier performance
with the removal of the feature.
For noun classification, the three features that
yield the highest gain in testset F-score are the
topic signature, OED, and derivational link features,
yielding a 4.0%, 3.6%, and 3.5% gain, respectively.
For verb classification, we find that three features
yield more than a 5% F-score gain; by far the largest
single-feature performance gain for verb classifica-
tion found in our ablation study was the DERIV fea-
ture, i.e., the count of shared derivational links be-
tween the two synsets; this single feature improves
our maximum F-score by 9.8% on the testset. This
is a particularly interesting discovery, as none of the
referenced automatic techniques for sense clustering
presently make use of this very useful feature. We
also achieve large gains with the LIN and LESK sim-
ilarity features, with F-score improvement of 7.4%
and 5.4% gain respectively.
For adjective classification again the DERIV fea-
ture proved very helpful, with a 3.5% gain on the
testset. Interestingly, only the DERIV feature and
the SENSECNT features helped across all parts of
speech; in many cases a feature which proved to be
1011
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Recall
Pr
ec
is
io
n
Precision/Recall for Merging Verbs
SVM Classifier
Lesk Measure
Hirst & St?Onge
Wu & Palmer
OED Mapping
Semantic Rules
Figure 4: Precision/Recall plot for verb sense merge
judgments
very helpful for one part of speech actually hurt per-
formance on another part of speech (e.g., LIN on
nouns and OED on adjectives).
5.3 Evaluation of sense-clustered Wordnets
Our goal in clustering a sense taxonomy is to pro-
duce fully sense-clustered WordNets, and to be able
to produce coarse-grained Wordnets at many differ-
ent levels of resolution. In order to evaluate the en-
tire sense-clustered taxonomy, we have employed an
evaluation method inspired by Word Sense Disam-
biguation (this is similar to an evaluation used in
Navigli, 2006, however we do not remove monose-
mous clusters). Given past system responses in the
SENSEVAL-3 English all-words task, we can eval-
uate past systems on the same corpus, but using
the coarse-grained sense hierarchy provided by our
sense-clustered taxonomy. We may then compare
the scores of each system on the coarse-grained task
against their scores given a random clustering at the
same resolution. Our expectation is that, if our sense
clustering is much better than a random sense clus-
tering (and, of course, that the WSD algorithms per-
form better than random guessing), we will see a
marked improvement in the performance of WSD
algorithms using our coarse-grained sense hierarchy.
We consider the outputs of the top 3 all-
words WSD systems that participated in Senseval-3:
Gambl (Decadt et al, 2004), SenseLearner (Mihal-
cea and Faruque, 2004), and KOC University (Yuret,
Nouns Verbs Adjectives
F-SCORE 0.4228 0.4319 0.4727
Feature F-Score Ablation Difference
TOPSIG 0.0403 ? ?
OED 0.0355 0.0126 -0.0124
DERIV 0.0351 0.0977 0.0352
RES 0.0287 0.0147 ?
TWIN 0.0285 0.0109 -0.0130
MN 0.0188 0.0358 ?
LESK 0.0183 0.0541 -0.0250
SENSENUM 0.0155 0.0146 -0.0147
SENSECNT 0.0121 0.0160 0.0168
DOMAIN 0.0119 0.0082 -0.0265
LCH 0.0099 0.0068 ?
WUP 0.0036 0.0168 ?
JCN 0.0025 0.0190 ?
ANTONYM 0.0000 0.0295 0.0000
MAXMN -0.0013 0.0179 ?
VEC -0.0024 0.0371 -0.0062
HSO -0.0073 0.0112 -0.0246
LIN -0.0086 0.0742 ?
COUSIN -0.0094 ? ?
VERBGRP ? 0.0327 ?
VERBFRM ? 0.0102 ?
PERTAINYM ? ? -0.0029
Table 4: Feature ablation study; F-score difference
obtained by removal of the single feature
2004). A guess by a system is given full credit if it
was either the correct answer or if it was in the same
cluster as the correct answer.
Clearly any amount of clustering will only in-
crease WSD performance. Therefore, to account for
this natural improvement and consider only the ef-
fect of our particular clustering, we also calculate
the expected score for a random clustering of the
same granularity, as follows: Let C represent the set
of clusters over the possible N synsets containing a
given word; we then calculate the expectation that an
incorrectly-chosen sense and the actual correct sense
would be clustered together in the random clustering
as
P
c?C |c|(|c|?1)
N(N?1) .
Our sense clustering algorithm provides little im-
provement over random clustering when too few or
too many clusters are chosen; however, with an ap-
propriate threshold for average-link clustering we
find a maximum of 3.55% F-score improvement in
WSD over random clustering (averaged over the de-
cisions of the top 3 WSD algorithms). Table 5 shows
the improvement of the three top WSD algorithms
given a sense clustering created by our algorithm vs.
a random clustering at the same granularity.
1012
0 0.5 1 1.5 2 2.5 3 3.5
x 104
0.65
0.7
0.75
0.8
Sense Merge Iterations
W
SD
 F
?S
co
re
Group Average Agglomerative Clustering
Random Clustering
Figure 5: WSD Improvement with coarse-grained
sense hierarchies
System F-score Avg-link Random Impr.
Gambl 0.6516 0.7702 0.7346 0.0356
SenseLearner 0.6458 0.7536 0.7195 0.0341
KOC Univ. 0.6414 0.7521 0.7153 0.0368
Table 5: Improvement in SENSEVAL-3 WSD perfor-
mance using our average-link agglomerative cluster-
ing vs. random clustering at the same granularity
6 Conclusion
We have presented a classifier for automatic sense
merging that significantly outperforms previously
proposed automatic methods. In addition to its novel
use of supervised learning and the integration of
many previously proposed features, it is interest-
ing that one of our new features, the DERIV count
of shared derivational links between two synsets,
proved an extraordinarily useful new cue for sense-
merging, particularly for verbs.
We also show how to integrate this sense-merging
algorithm into a model for sense clustering full sense
taxonomies like WordNet, incorporating taxonomic
constraints such as the transitive effects of merging
synsets. Using this model, we have produced several
WordNet taxonomies of various sense granularities;
we hope these new lexical resources will be useful
for NLP applications that require a coarser-grained
sense hierarchy than that already found in WordNet.
Acknowledgments
Thanks to Marie-Catherine de Marneffe, Mona
Diab, Christiane Fellbaum, Thad Hughes, and Ben-
jamin Packer for useful discussions. Rion Snow is
supported by an NSF Fellowship. This work was
supported in part by the Disruptive Technology Of-
fice (DTO)?s Advanced Question Answering for In-
telligence (AQUAINT) Phase III Program.
References
Eneko Agirre and Oier Lopez de Lacalle. 2003. Cluster-
ing WordNet word senses. In Proceedings of RANLP
2003.
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nomi-
nal senses. In Proceedings of LREC 2004.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
Gloss Overlaps as a Measure of Semantic Relatedness.
In Proceedings of IJCAI 2003.
Lisa Bentivogli, Pamela Forner, Bernardo Magnini, and
Emanuele Pianta. 2004. Revising the WordNet Do-
mains Hierarchy: Semantics, Coverage, and Balanc-
ing. In Proceedings of COLING Workshop on Multi-
lingual Linguistic Resources, 2004.
Timothy Chklovski and Rada Mihalcea. 2003. Exploit-
ing Agreement and Disagreement of Human Annota-
tors for Word Sense Disambiguation. In Proceedings
of RANLP 2003.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and Sense Proximity in the Senseval-2 Test
Suite. In Proceedings of ACL 2002 WSD Workshop.
Bart Decadt, Veronique Hoste, Walter Daelemans, and
Antal van den Bosch. 2004. Gamble, genetic algo-
rithm optimization of memory-based wsd. In Proceed-
ings of ACL/SIGLEX Senseval-3.
William Dolan. 1994. Word Sense Ambiguation: Clus-
tering Related Senses. In Proceedings of ACL 1994.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Julio Gonzalo, Felia Verdejo, Irina Chugur, and Juan
Cigarran. 1998. Indexing with WordNet synsets can
improve text retrieval. In Proceedings of COLING-
ACL 1998 Workshop on WordNet in NLP Systems.
Patrick Hanks. 2000. Do word meanings exist? Com-
puters and the Humanities, 34(1-2): 171-177.
1013
Graeme Hirst and David St-Onge. 1998. Lexical chains
as representations of context for the detection and cor-
rection of malapropisms. In WordNet: An Electronic
Lexical Database.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. Proceedings of HLT-NAACL 2006.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics, 19-33.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines. Dissertation, Kluwer,
2002.
Adam Kilgariff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(1-2): 1-13.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In Proceedings of the SENSEVAL-2 work-
shop, 17-20.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. In WordNet: An Electronic Lexi-
cal Database.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of SIG-
DOC 1986.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML 1998.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998.
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating Subject Field Codes into WordNet. In Pro-
ceedings of LREC 2000.
Diana McCarthy. 2006. Relating WordNet Senses for
Word Sense Disambiguation. In Proceedings of ACL
Workshop on Making Sense of Sense, 2006.
Rada Mihalcea and Dan I. Moldovan. 2001. Automatic
Generation of a Coarse Grained WordNet. In Proceed-
ings of NAACL Workshop on WordNet and Other Lex-
ical Resources.
Rada Mihalcea and Ehsanul Faruque. 2004. Sense-
learner: Minimally supervised word sense disam-
biguation for all words in open text. In Proceedings
of ACL/SIGLEX Senseval-3.
Dan I. Moldovan and Rada Mihalcea. 2000. Using
WordNet and lexical operators to improve Internet
searches. IEEE Internet Computing, 4(1):34-43.
Roberto Navigli. 2006. Meaningful Clustering of
Senses Helps Boost Word Sense Disambiguation Per-
formance. In Proceedings of COLING-ACL 2006.
Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang.
2004. Different Sense Granularities for Different Ap-
plications. In Proceedings of Workshop on Scalable
Natural Language Understanding.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005. Making fine-grained and coarse-grained
sense distinctions. Journal of Natural Language Engi-
neering.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector measure
of semantic relatedness. Master?s thesis, Univ. of Min-
nesota, Duluth.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the
Relatedness of Concepts. In Proceedings of NAACL
2004.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional Clustering of English Words. In Pro-
ceedings of ACL 1993.
Wim Peters, Ivonne Peters, and Piek Vossen. 1998. Au-
tomatic Sense Clustering in EuroWordNet. In Pro-
ceedings of LREC 1998.
Andrew Philpot, Eduard Hovy, and Patrick Pantel. 2005.
The Omega Ontology. In Proceedings of the ON-
TOLEX Workshop at IJCNLP 2005.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the IJCAI 1995, 448-453.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(2):113-134.
Noriko Tomuro. 2001. Tree-cut and A Lexicon based
on Systematic Polysemy. In Proceedings of NAACL
2001.
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of ACL 1994.
Deniz Yuret. 2004. Some experiments with a naive
bayes wsd system. In Proceedings of ACL/SIGLEX
Senseval-3.
1014
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254?263,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Language Tasks
Rion Snow? Brendan O?Connor? Daniel Jurafsky? Andrew Y. Ng?
?Computer Science Dept.
Stanford University
Stanford, CA 94305
{rion,ang}@cs.stanford.edu
?Dolores Labs, Inc.
832 Capp St.
San Francisco, CA 94110
brendano@doloreslabs.com
?Linguistics Dept.
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Human linguistic annotation is crucial for
many natural language processing tasks but
can be expensive and time-consuming. We ex-
plore the use of Amazon?s Mechanical Turk
system, a significantly cheaper and faster
method for collecting annotations from a
broad base of paid non-expert contributors
over the Web. We investigate five tasks: af-
fect recognition, word similarity, recognizing
textual entailment, event temporal ordering,
and word sense disambiguation. For all five,
we show high agreement between Mechani-
cal Turk non-expert annotations and existing
gold standard labels provided by expert label-
ers. For the task of affect recognition, we also
show that using non-expert labels for training
machine learning algorithms can be as effec-
tive as using gold standard annotations from
experts. We propose a technique for bias
correction that significantly improves annota-
tion quality on two tasks. We conclude that
many large labeling tasks can be effectively
designed and carried out in this method at a
fraction of the usual expense.
1 Introduction
Large scale annotation projects such as TreeBank
(Marcus et al, 1993), PropBank (Palmer et
al., 2005), TimeBank (Pustejovsky et al, 2003),
FrameNet (Baker et al, 1998), SemCor (Miller et
al., 1993), and others play an important role in
natural language processing research, encouraging
the development of novel ideas, tasks, and algo-
rithms. The construction of these datasets, how-
ever, is extremely expensive in both annotator-hours
and financial cost. Since the performance of many
natural language processing tasks is limited by the
amount and quality of data available to them (Banko
and Brill, 2001), one promising alternative for some
tasks is the collection of non-expert annotations.
In this work we explore the use of Amazon Me-
chanical Turk1 (AMT) to determine whether non-
expert labelers can provide reliable natural language
annotations. We chose five natural language under-
standing tasks that we felt would be sufficiently nat-
ural and learnable for non-experts, and for which
we had gold standard labels from expert labelers,
as well as (in some cases) expert labeler agree-
ment information. The tasks are: affect recogni-
tion, word similarity, recognizing textual entailment,
event temporal ordering, and word sense disam-
biguation. For each task, we used AMT to annotate
data and measured the quality of the annotations by
comparing them with the gold standard (expert) la-
bels on the same data. Further, we compare machine
learning classifiers trained on expert annotations vs.
non-expert annotations.
In the next sections of the paper we introduce
the five tasks and the evaluation metrics, and offer
methodological insights, including a technique for
bias correction that improves annotation quality.2
1 http://mturk.com
2 Please see http://blog.doloreslabs.com/?p=109
for a condensed version of this paper, follow-ups, and on-
going public discussion. We encourage comments to be di-
rected here in addition to email when appropriate. Dolores
Labs Blog, ?AMT is fast, cheap, and good for machine learning
data,? Brendan O?Connor, Sept. 9, 2008. More related work at
http://blog.doloreslabs.com/topics/wisdom/.
254
2 Related Work
The idea of collecting annotations from volunteer
contributors has been used for a variety of tasks.
Luis von Ahn pioneered the collection of data via
online annotation tasks in the form of games, includ-
ing the ESPGame for labeling images (von Ahn and
Dabbish, 2004) and Verbosity for annotating word
relations (von Ahn et al, 2006). The Open Mind
Initiative (Stork, 1999) has taken a similar approach,
attempting to make such tasks as annotating word
sense (Chklovski and Mihalcea, 2002) and common-
sense word relations (Singh, 2002) sufficiently ?easy
and fun? to entice users into freely labeling data.
There have been an increasing number of experi-
ments using Mechanical Turk for annotation. In (Su
et al, 2007) workers provided annotations for the
tasks of hotel name entity resolution and attribute
extraction of age, product brand, and product model,
and were found to have high accuracy compared
to gold-standard labels. Kittur et al (2008) com-
pared AMT evaluations of Wikipedia article qual-
ity against experts, finding validation tests were im-
portant to ensure good results. Zaenen (Submitted)
studied the agreement of annotators on the problem
of recognizing textual entailment (a similar task and
dataset is explained in more detail in Section 4).
At least several studies have already used AMT
without external gold standard comparisons. In
(Nakov, 2008) workers generated paraphrases of
250 noun-noun compounds which were then used
as the gold standard dataset for evaluating an au-
tomatic method of noun compound paraphrasing.
Kaisser and Lowe (2008) use AMT to help build a
dataset for question answering, annotating the an-
swers to 8107 questions with the sentence contain-
ing the answer. Kaisser et al (2008) examines the
task of customizing the summary length of QA out-
put; non-experts from AMT chose a summary length
that suited their information needs for varying query
types. Dakka and Ipeirotis (2008) evaluate a docu-
ment facet generation system against AMT-supplied
facets, and also use workers for user studies of the
system. Sorokin and Forsyth (2008) collect data for
machine vision tasks and report speed and costs sim-
ilar to our findings; their summaries of worker be-
havior also corroborate with what we have found.
In general, volunteer-supplied or AMT-supplied
data is more plentiful but noisier than expert data.
It is powerful because independent annotations can
be aggregated to achieve high reliability. Sheng et
al. (2008) explore several methods for using many
noisy labels to create labeled data, how to choose
which examples should get more labels, and how to
include labels? uncertainty information when train-
ing classifiers. Since we focus on empirically val-
idating AMT as a data source, we tend to stick to
simple aggregation methods.
3 Task Design
In this section we describe Amazon Mechanical
Turk and the general design of our experiments.
3.1 Amazon Mechanical Turk
We employ the Amazon Mechanical Turk system
in order to elicit annotations from non-expert label-
ers. AMT is an online labor market where workers
are paid small amounts of money to complete small
tasks. The design of the system is as follows: one is
required to have an Amazon account to either sub-
mit tasks for annotations or to annotate submitted
tasks. These Amazon accounts are anonymous, but
are referenced by a unique Amazon ID. A Requester
can create a group of Human Intelligence Tasks (or
HITs), each of which is a form composed of an arbi-
trary number of questions. The user requesting an-
notations for the group of HITs can specify the num-
ber of unique annotations per HIT they are willing
to pay for, as well as the reward payment for each
individual HIT. While this does not guarantee that
unique people will annotate the task (since a single
person could conceivably annotate tasks using mul-
tiple accounts, in violation of the user agreement),
this does guarantee that annotations will be collected
from unique accounts. AMT also allows a requester
to restrict which workers are allowed to annotate a
task by requiring that all workers have a particular
set of qualifications, such as sufficient accuracy on
a small test set or a minimum percentage of previ-
ously accepted submissions. Annotators (variously
referred to as Workers or Turkers) may then annotate
the tasks of their choosing. Finally, after each HIT
has been annotated, the Requester has the option of
approving the work and optionally giving a bonus
to individual workers. There is a two-way commu-
255
nication channel between the task designer and the
workers mediated by Amazon, and Amazon handles
all financial transactions.
3.2 Task Design
In general we follow a few simple design principles:
we attempt to keep our task descriptions as succinct
as possible, and we attempt to give demonstrative
examples for each class wherever possible. We have
published the full experimental design and the data
we have collected for each task online3. We have
restricted our study to tasks where we require only
a multiple-choice response or numeric input within
a fixed range. For every task we collect ten inde-
pendent annotations for each unique item; this re-
dundancy allows us to perform an in-depth study of
how data quality improves with the number of inde-
pendent annotations.
4 Annotation Tasks
We analyze the quality of non-expert annotations on
five tasks: affect recognition, word similarity, rec-
ognizing textual entailment, temporal event recogni-
tion, and word sense disambiguation. In this section
we define each annotation task and the parameters
of the annotations we request using AMT. Addition-
ally we give an initial analysis of the task results,
and summarize the cost of the experiments.
4.1 Affective Text Analysis
This experiment is based on the affective text an-
notation task proposed in Strapparava and Mihalcea
(2007), wherein each annotator is presented with a
list of short headlines, and is asked to give numeric
judgments in the interval [0,100] rating the headline
for six emotions: anger, disgust, fear, joy, sadness,
and surprise, and a single numeric rating in the inter-
val [-100,100] to denote the overall positive or nega-
tive valence of the emotional content of the headline,
as in this sample headline-annotation pair:
Outcry at N Korea ?nuclear test?
(Anger, 30), (Disgust,30), (Fear,30), (Joy,0),
(Sadness,20), (Surprise,40), (Valence,-50).
3All tasks and collected data are available at
http://ai.stanford.edu/
?
rion/annotations/.
For our experiment we select a 100-headline sample
from the original SemEval test set, and collect 10
affect annotations for each of the seven label types,
for a total of 7000 affect labels.
We then performed two comparisons to evaluate
the quality of the AMT annotations. First, we asked
how well the non-experts agreed with the experts.
We did this by comparing the interannotator agree-
ment (ITA) of individual expert annotations to that
of single non-expert and averaged non-expert anno-
tations. In the original experiment ITA is measured
by calculating the Pearson correlation of one anno-
tator?s labels with the average of the labels of the
other five annotators. For each expert labeler, we
computed this ITA score of the expert against the
other five; we then average these ITA scores across
all expert annotators to compute the average expert
ITA (reported in Table 1 as ?E vs. E?. We then do the
same for individual non-expert annotations, averag-
ing Pearson correlation across all sets of the five ex-
pert labelers (?NE vs. E?). We then calculate the ITA
for each expert vs. the averaged labels from all other
experts and non-experts (marked as ?E vs. All?) and
for each non-expert vs. the pool of other non-experts
and all experts (?NE vs. All?). We compute these
ITA scores for each emotion task separately, aver-
aging the six emotion tasks as ?Avg. Emo? and the
average of all tasks as ?Avg. All?.
Emotion E vs. E E vs. All NE vs. E NE vs. All
Anger 0.459 0.503 0.444 0.573
Disgust 0.583 0.594 0.537 0.647
Fear 0.711 0.683 0.418 0.498
Joy 0.596 0.585 0.340 0.421
Sadness 0.645 0.650 0.563 0.651
Surprise 0.464 0.463 0.201 0.225
Valence 0.759 0.767 0.530 0.554
Avg. Emo 0.576 0.603 0.417 0.503
Avg. All 0.580 0.607 0.433 0.510
Table 1: Average expert and non-expert ITA on test-set
The results in Table 1 conform to the expectation
that experts are better labelers: experts agree with
experts more than non-experts agree with experts,
although the ITAs are in many cases quite close. But
we also found that adding non-experts to the gold
standard (?E vs. All?) improves agreement, suggest-
ing that non-expert annotations are good enough to
increase the overall quality of the gold labels. Our
256
first comparison showed that individual experts were
better than individual non-experts. In our next com-
parison we ask how many averaged non-experts it
would take to rival the performance of a single ex-
pert. We did this by averaging the labels of each pos-
sible subset of n non-expert annotations, for value
of n in {1, 2, . . . , 10}. We then treat this average as
though it is the output of a single ?meta-labeler?, and
compute the ITA with respect to each subset of five
of the six expert annotators. We then average the
results of these studies across each subset size; the
results of this experiment are given in Table 2 and in
Figure 1. In addition to the single meta-labeler, we
ask: what is the minimum number of non-expert an-
notations k from which we can create a meta-labeler
that has equal or better ITA than an expert annotator?
In Table 2 we give the minimum k for each emotion,
and the averaged ITA for that meta-labeler consist-
ing of k non-experts (marked ?k-NE?). In Figure 1
we plot the expert ITA correlation as the horizontal
dashed line.
Emotion 1-Expert 10-NE k k-NE
Anger 0.459 0.675 2 0.536
Disgust 0.583 0.746 2 0.627
Fear 0.711 0.689 ? ?
Joy 0.596 0.632 7 0.600
Sadness 0.645 0.776 2 0.656
Surprise 0.464 0.496 9 0.481
Valence 0.759 0.844 5 0.803
Avg. Emo. 0.576 0.669 4 0.589
Avg. All 0.603 0.694 4 0.613
Table 2: Average expert and averaged correlation over
10 non-experts on test-set. k is the minimum number of
non-experts needed to beat an average expert.
These results show that for all tasks except ?Fear?
we are able to achieve expert-level ITA with the
held-out set of experts within 9 labelers, and fre-
quently within only 2 labelers. Pooling judgments
across all 7 tasks we find that on average it re-
quires only 4 non-expert annotations per example to
achieve the equivalent ITA as a single expert anno-
tator. Given that we paid US$2.00 in order to collect
the 7000 non-expert annotations, we may interpret
our rate of 3500 non-expert labels per USD as at
least 875 expert-equivalent labels per USD.
4.2 Word Similarity
This task replicates the word similarity task used in
(Miller and Charles, 1991), following a previous
2 4 6 8 100
.4
5
0.
55
0.
65
co
rr
e
la
tio
n
anger
2 4 6 8 10
0.
55
0.
65
0.
75
co
rr
e
la
tio
n
disgust
2 4 6 8 100
.4
0
0.
50
0.
60
0.
70
co
rr
e
la
tio
n
fear
2 4 6 8 10
0.
35
0.
45
0.
55
0.
65
co
rr
e
la
tio
n
joy
2 4 6 8 100
.5
5
0.
65
0.
75
annotators
co
rr
e
la
tio
n
sadness
2 4 6 8 100
.2
0
0.
30
0.
40
0.
50
annotators
co
rr
e
la
tio
n
surprise
Figure 1: Non-expert correlation for affect recognition
task initially proposed by (Rubenstein and Good-
enough, 1965). Specifically, we ask for numeric
judgments of word similarity for 30 word pairs on
a scale of [0,10], allowing fractional responses4 .
These word pairs range from highly similar (e.g.,
{boy, lad}), to unrelated (e.g., {noon, string}). Nu-
merous expert and non-expert studies have shown
that this task typically yields very high interannota-
tor agreement as measured by Pearson correlation;
(Miller and Charles, 1991) found a 0.97 correla-
tion of the annotations of 38 subjects with the an-
notations given by 51 subjects in (Rubenstein and
Goodenough, 1965), and a following study (Resnik,
1999) with 10 subjects found a 0.958 correlation
with (Miller and Charles, 1991).
In our experiment we ask for 10 annotations each
of the full 30 word pairs, at an offered price of $0.02
for each set of 30 annotations (or, equivalently, at
the rate of 1500 annotations per USD). The most
surprising aspect of this study was the speed with
which it was completed; the task of 300 annotations
was completed by 10 annotators in less than 11 min-
4(Miller and Charles, 1991) and others originally used a
numerical score of [0,4].
257
utes from the time of submission of our task to AMT,
at the rate of 1724 annotations / hour.
As in the previous task we evaluate our non-
expert annotations by averaging the numeric re-
sponses from each possible subset of n annotators
and computing the interannotator agreement with
respect to the gold scores reported in (Miller and
Charles, 1991). Our results are displayed in Figure
2, with Resnik?s 0.958 correlation plotted as the hor-
izontal line; we find that at 10 annotators we achieve
a correlation of 0.952, well within the range of other
studies of expert and non-expert annotations.
2 4 6 8 10
0.
84
0.
90
0.
96
annotations
co
rr
e
la
tio
n
Word Similarity ITA
Figure 2: ITA for word similarity experiment
4.3 Recognizing Textual Entailment
This task replicates the recognizing textual entail-
ment task originally proposed in the PASCAL Rec-
ognizing Textual Entailment task (Dagan et al,
2006); here for each question the annotator is pre-
sented with two sentences and given a binary choice
of whether the second hypothesis sentence can be
inferred from the first. For example, the hypothesis
sentence ?Oil prices drop? would constitute a true
entailment from the text ?Crude Oil Prices Slump?,
but a false entailment from ?The government an-
nounced last week that it plans to raise oil prices?.
We gather 10 annotations each for all 800 sen-
tence pairs in the PASCAL RTE-1 dataset. For this
dataset expert interannotator agreement studies have
been reported as achieving 91% and 96% agreement
over various subsections of the corpus. When con-
sidering multiple non-expert annotations for a sen-
tence pair we use simple majority voting, breaking
ties randomly and averaging performance over all
possible ways to break ties. We collect 10 annota-
tions for each of 100 RTE sentence pairs; as dis-
played in Figure 3, we achieve a maximum accu-
racy of 89.7%, averaging over the annotations of 10
workers5.
2 4 6 8 100
.7
0
0.
80
0.
90
annotations
a
cc
u
ra
cy
RTE ITA
Figure 3: Inter-annotator agreement for RTE experiment
4.4 Event Annotation
This task is inspired by the TimeBank corpus (Puste-
jovsky et al, 2003), which includes among its anno-
tations a label for event-pairs that represents the tem-
poral relation between them, from a set of fourteen
relations (before, after, during, includes, etc.). We
implement temporal ordering as a simplified version
of the TimeBank event temporal annotation task:
rather than annotating all fourteen event types, we
restrict our consideration to the two simplest labels:
?strictly before? and ?strictly after?. Furthermore,
rather than marking both nouns and verbs in the text
as possible events, we only consider possible verb
events. We extract the 462 verb event pairs labeled
as ?strictly before? or ?strictly after? in the Time-
Bank corpus, and we present these pairs to annota-
tors with a forced binary choice on whether the event
described by the first verb occurs before or after the
second. For example, in a dialogue about a plane
explosion, we have the utterance: ?It just blew up in
the air, and then we saw two fireballs go down to the,
5It might seem pointless to consider an even number of an-
notations in this circumstance, since the majority voting mech-
anism and tie-breaking yields identical performance for 2n + 1
and 2n + 2 annotators; however, in Section 5 we will consider
methods that can make use of the even annotations.
258
to the water, and there was a big small, ah, smoke,
from ah, coming up from that?. Here for each anno-
tation we highlight the specific verb pair of interest
(e.g., go/coming, or blew/saw) and ask which event
occurs first (here, go and blew, respectively).
The results of this task are presented in Figure 4.
We achieve high agreement for this task, at a rate
of 0.94 with simple voting over 10 annotators (4620
total annotations). While an expert ITA of 0.77 was
reported for the more general task involving all four-
teen labels on both noun and verb events, no expert
ITA numbers have been reported for this simplified
temporal ordering task.
2 4 6 8 100
.7
0
0.
80
0.
90
annotators
a
cc
u
ra
cy
Temp. Ordering ITA
Figure 4: ITA for temporal ordering experiment
4.5 Word Sense Disambiguation
In this task we consider a simple problem on which
machine learning algorithms have been shown to
produce extremely good results; here we annotate
part of the SemEval Word Sense Disambiguation
Lexical Sample task (Pradhan et al, 2007); specif-
ically, we present the labeler with a paragraph of
text containing the word ?president? (e.g., a para-
graph containing ?Robert E. Lyons III...was ap-
pointed president and chief operating officer...?) and
ask the labeler which one of the following three
sense labels is most appropriate:
1) executive officer of a firm, corporation, or university
2) head of a country (other than the U.S.)
3) head of the U.S., President of the United States
We collect 10 annotations for each of 177 examples
of the noun ?president? for the three senses given in
SemEval. As shown in Figure 5, performing simple
majority voting (with random tie-breaking) over an-
notators results in a rapid accuracy plateau at a very
high rate of 0.994 accuracy. In fact, further analy-
sis reveals that there was only a single disagreement
between the averaged non-expert vote and the gold
standard; on inspection it was observed that the an-
notators voted strongly against the original gold la-
bel (9-to-1 against), and that it was in fact found to
be an error in the original gold standard annotation.6
After correcting this error, the non-expert accuracy
rate is 100% on the 177 examples in this task. This
is a specific example where non-expert annotations
can be used to correct expert annotations.
Since expert ITA was not reported per word on
this dataset, we compare instead to the performance
of the best automatic system performance for dis-
ambiguating ?president? in SemEval Task 17 (Cai et
al., 2007), with an accuracy of 0.98.
2 4 6 8 10
0.
98
0
0.
99
0
1.
00
0
annotators
a
cc
u
ra
cy
WSD ITA
Figure 5: Inter-annotator agreement for WSD experiment
4.6 Summary
Cost Time Labels Labels
Task Labels (USD) (hrs) per USD per hr
Affect 7000 $2.00 5.93 3500 1180.4
WSim 300 $0.20 0.174 1500 1724.1
RTE 8000 $8.00 89.3 1000 89.59
Event 4620 $13.86 39.9 333.3 115.85
WSD 1770 $1.76 8.59 1005.7 206.1
Total 21690 25.82 143.9 840.0 150.7
Table 3: Summary of costs for non-expert labels
6The example sentence began ?The Egyptian president said
he would visit Libya today...? and was mistakenly marked as
the ?head of a company? sense in the gold annotation (example
id 24:0@24@wsj/23/wsj 2381@wsj@en@on).
259
0 200 400 600 800
0.
4
0.
6
0.
8
1.
0
number of annotations
a
cc
u
ra
cy
Figure 6: Worker accuracies on the RTE task. Each point
is one worker. Vertical jitter has been added to points on
the left to show the large number of workers who did the
minimum amount of work (20 examples).
In Table 3 we give a summary of the costs asso-
ciated with obtaining the non-expert annotations for
each of our 5 tasks. Here Time is given as the to-
tal amount of time in hours elapsed from submitting
the group of HITs to AMT until the last assignment
is submitted by the last worker.
5 Bias correction for non-expert
annotators
The reliability of individual workers varies. Some
are very accurate, while others are more careless and
make mistakes; and a small few give very noisy re-
sponses. Furthermore, for most AMT data collec-
tion experiments, a relatively small number of work-
ers do a large portion of the task, since workers may
do as much or as little as they please. Figure 6 shows
accuracy rates for individual workers on one task.
Both the overall variability, as well as the prospect
of identifying high-volume but low-quality workers,
suggest that controlling for individual worker qual-
ity could yield higher quality overall judgments.
In general, there are at least three ways to enhance
quality in the face of worker error. More work-
ers can be used, as described in previous sections.
Another method is to use Amazon?s compensation
mechanisms to give monetary bonuses to highly-
performing workers and deny payments to unreli-
able ones; this is useful, but beyond the scope of
this paper. In this section we explore a third alterna-
tive, to model the reliability and biases of individual
workers and correct for them.
A wide number of methods have been explored to
correct for the bias of annotators. Dawid and Skene
(1979) are the first to consider the case of having
multiple annotators per example but unknown true
labels. They introduce an EM algorithm to simul-
taneously estimate annotator biases and latent label
classes. Wiebe et al (1999) analyze linguistic anno-
tator agreement statistics to find bias, and use a sim-
ilar model to correct labels. A large literature in bio-
statistics addresses this same problem for medical
diagnosis. Albert and Dodd (2004) review several
related models, but argue they have various short-
comings and emphasize instead the importance of
having a gold standard.
Here we take an approach based on gold standard
labels, using a small amount of expert-labeled train-
ing data in order to correct for the individual biases
of different non-expert annotators. The idea is to re-
calibrate worker?s responses to more closely match
expert behavior. We focus on categorical examples,
though a similar method can be used with numeric
data.
5.1 Bias correction in categorical data
Following Dawid and Skene, we model labels and
workers with a multinomial model similar to Naive
Bayes. Every example i has a true label xi. For sim-
plicity, assume two labels {Y,N}. Several differ-
ent workers give labels yi1, yi2, . . . yiW . A worker?s
conditional probability of response is modeled as
multinomial, and we model each worker?s judgment
as conditionally independent of other workers given
the true label xi, i.e.:
P (yi1, . . . , yiW , xi) =
(
?
w
P (yiw|xi)
)
p(xi)
To infer the posterior probability of the true label
for a new example, worker judgments are integrated
via Bayes rule, yielding the posterior log-odds:
log
P (xi = Y |yi1 . . . yiW )
P (xi = N |yi1 . . . yiW )
=
?
w
log
P (yiw|xi = Y )
P (yiw|xi = N)
+ log
P (xi = Y )
P (xi = N)
260
The worker response likelihoods P (yw|x = Y )
and P (yw|x = N) can be directly estimated from
frequencies of worker performance on gold standard
examples. (If we used maximum likelihood esti-
mation with no Laplace smoothing, then each yw|x
is just the worker?s empirical confusion matrix.)
For MAP label estimation, the above equation de-
scribes a weighted voting rule: each worker?s vote is
weighted by their log likelihood ratio for their given
response. Intuitively, workers who are more than
50% accurate have positive votes; workers whose
judgments are pure noise have zero votes; and an-
ticorrelated workers have negative votes. (A simpler
form of the model only considers accuracy rates,
thus weighting worker votes by log accw1?accw . But we
use the full unconstrained multinomial model here.)
5.1.1 Example tasks: RTE-1 and event
annotation
We used this model to improve accuracy on the
RTE-1 and event annotation tasks. (The other cate-
gorical task, word sense disambiguation, could not
be improved because it already had maximum accu-
racy.) First we took a sample of annotations giving
k responses per example. Within this sample, we
trained and tested via 20-fold cross-validation across
examples. Worker models were fit using Laplace
smoothing of 1 pseudocount; label priors were uni-
form, which was reasonably similar to the empirical
distribution for both tasks.
annotators
a
cc
u
ra
cy
0.
7
0.
8
0.
9
RTE
annotators
0.
7
0.
8
0.
9
before/after
Gold calibrated
Naive voting
Figure 7: Gold-calibrated labels versus raw labels
Figure 7 shows improved accuracy at different
numbers of annotators. The lowest line is for the
naive 50% majority voting rule. (This is equivalent
to the model under uniform priors and equal accu-
racies across workers and labels.) Each point is the
data set?s accuracy against the gold labels, averaged
across resamplings each of which obtains k annota-
tions per example. RTE has an average +4.0% ac-
curacy increase, averaged across 2 through 10 anno-
tators. We find a +3.4% gain on event annotation.
Finally, we experimented with a similar calibration
method for numeric data, using a Gaussian noise
model for each worker: yw|x ? N(x + ?w, ?w).
On the affect task, this yielded a small but consis-
tent increases in Pearson correlation at all numbers
of annotators, averaging a +0.6% gain.
6 Training a system with non-expert
annotations
In this section we train a supervised affect recogni-
tion system with expert vs. non-expert annotations.
6.1 Experimental Design
For the purpose of this experiment we create a sim-
ple bag-of-words unigram model for predicting af-
fect and valence, similar to the SWAT system (Katz
et al, 2007), one of the top-performing systems on
the SemEval Affective Text task.7 For each token
t in our training set, we assign t a weight for each
emotion e equal to the average emotion score ob-
served in each headline H that t participates in. i.e.,
if Ht is the set of headlines containing the token t,
then:
Score(e, t) =
?
H?Ht Score(e,H)
|Ht|
With these weights of the individual tokens we
may then compute the score for an emotion e of a
new headline H as the average score over the set of
tokens t ? H that we?ve observed in the training set
(ignoring those tokens not in the training set), i.e.:
Score(e,H) =
?
t?H
Score(e, t)
|H|
Where |H| is simply the number of tokens in
headline H , ignoring tokens not observed in the
training set.
7 Unlike the SWAT system we perform no lemmatization,
synonym expansion, or any other preprocessing of the tokens;
we simply use whitespace-separated tokens within each head-
line.
261
6.2 Experiments
We use 100 headlines as a training set (examples
500-599 from the test set of SemEval Task 14), and
we use the remaining 900 headlines as our test set.
Since we are fortunate to have the six separate ex-
pert annotations in this task, we can perform an ex-
tended systematic comparison of the performance of
the classifier trained with expert vs. non-expert data.
Emotion 1-Expert 10-NE k k-NE
Anger 0.084 0.233 1 0.172
Disgust 0.130 0.231 1 0.185
Fear 0.159 0.247 1 0.176
Joy 0.130 0.125 ? ?
Sadness 0.127 0.174 1 0.141
Surprise 0.060 0.101 1 0.061
Valence 0.159 0.229 2 0.146
Avg. Emo 0.116 0.185 1 0.135
Avg. All 0.122 0.191 1 0.137
Table 4: Performance of expert-trained and non-expert-
trained classifiers on test-set. k is the minimum number
of non-experts needed to beat an average expert.
For this evaluation we compare the performance
of systems trained on expert and non-expert annota-
tions. For each expert annotator we train a system
using only the judgments provided by that annota-
tor, and then create a gold standard test set using the
average of the responses of the remaining five label-
ers on that set. In this way we create six indepen-
dent expert-trained systems and compute the aver-
age across their performance, calculated as Pearson
correlation to the gold standard; this is reported in
the ?1-Expert? column of Table 4.
Next we train systems using non-expert labels;
for each possible subset of n annotators, for n ?
{1, 2, . . . , 10} we train a system, and evaluate by
calculating Pearson correlation with the same set of
gold standard datasets used in the expert-trained sys-
tem evaluation. Averaging the results of these stud-
ies yields the results in Table 4.
As in Table 2 we calculate the minimum number
of non-expert annotations per example k required on
average to achieve similar performance to the ex-
pert annotations; surprisingly we find that for five
of the seven tasks, the average system trained with a
single set of non-expert annotations outperforms the
average system trained with the labels from a sin-
gle expert. One possible hypothesis for the cause
of this non-intuitive result is that individual labelers
(including experts) tend to have a strong bias, and
since multiple non-expert labelers may contribute to
a single set of non-expert annotations, the annotator
diversity within the single set of labels may have the
effect of reducing annotator bias and thus increasing
system performance.
7 Conclusion
We demonstrate the effectiveness of using Amazon
Mechanical Turk for a variety of natural language
annotation tasks. Our evaluation of non-expert la-
beler data vs. expert annotations for five tasks found
that for many tasks only a small number of non-
expert annotations per item are necessary to equal
the performance of an expert annotator. In a detailed
study of expert and non-expert agreement for an af-
fect recognition task we find that we require an av-
erage of 4 non-expert labels per item in order to em-
ulate expert-level label quality. Finally, we demon-
strate significant improvement by controlling for la-
beler bias.
Acknowledgments
Thanks to Nathanael Chambers, Annie Zaenen,
Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Car-
penter, David Vickrey, William Morgan, and Lukas
Biewald for useful discussions, and for the gener-
ous support of Dolores Labs. This work was sup-
ported in part by the Disruptive Technology Office
(DTO)?s Advanced Question Answering for Intelli-
gence (AQUAINT) Phase III Program.
References
Paul S. Albert and Lori E. Dodd. 2004. A Cautionary
Note on the Robustness of Latent Class Models for
Estimating Diagnostic Error without a Gold Standard.
Biometrics, Vol. 60 (2004), pp. 427-435.
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
COLING-ACL 1998.
Michele Banko and Eric Brill. 2001. Scaling to Very
Very Large Corpora for Natural Language Disam-
biguation. In Proc. of ACL-2001.
Junfu Cai, Wee Sun Lee and Yee Whye Teh. 2007. Im-
proving Word Sense Disambiguation Using Topic Fea-
tures. In Proc. of EMNLP-2007 .
262
Timothy Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with Open Mind Word Expert.
In Proc. of the Workshop on ?Word Sense Disam-
biguation: Recent Successes and Future Directions?,
ACL 2002.
Timothy Chklovski and Yolanda Gil. 2005. Towards
Managing Knowledge Collection from Volunteer Con-
tributors. Proceedings of AAAI Spring Symposium
on Knowledge Collection from Volunteer Contributors
(KCVC05).
Ido Dagan, Oren Glickman and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. Machine Learning Challenges. Lecture
Notes in Computer Science, Vol. 3944, pp. 177-190,
Springer, 2006.
Wisam Dakka and Panagiotis G. Ipeirotis. 2008. Au-
tomatic Extraction of Useful Facet Terms from Text
Documents. In Proc. of ICDE-2008.
A. P. Dawid and A. M. Skene. 1979. Maximum Like-
lihood Estimation of Observer Error-Rates Using the
EM Algorithm. Applied Statistics, Vol. 28, No. 1
(1979), pp. 20-28.
Michael Kaisser and John B. Lowe. 2008. A Re-
search Collection of QuestionAnswer Sentence Pairs.
In Proc. of LREC-2008.
Michael Kaisser, Marti Hearst, and John B. Lowe.
2008. Evidence for Varying Search Results Summary
Lengths. In Proc. of ACL-2008.
Phil Katz, Matthew Singleton, Richard Wicentowski.
2007. SWAT-MP: The SemEval-2007 Systems for
Task 5 and Task 14. In Proc. of SemEval-2007.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proc. of CHI-2008.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics 19:2, June 1993.
George A. Miller and William G. Charles. 1991. Con-
textual Correlates of Semantic Similarity. Language
and Cognitive Processes, vol. 6, no. 1, pp. 1-28, 1991.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunke. 1993. A semantic concordance. In
Proc. of HLT-1993.
Preslav Nakov. 2008. Paraphrasing Verbs for Noun
Compound Interpretation. In Proc. of the Workshop
on Multiword Expressions, LREC-2008.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics, 31:1.
Sameer Pradhan, Edward Loper, Dmitriy Dligach and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Proc.
of SemEval-2007 .
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proc. of Corpus Linguistics 2003, 647-656.
Philip Resnik. 1999. Semantic Similarity in a Taxon-
omy: An Information-Based Measure and its Applica-
tion to Problems of Ambiguity in Natural Language.
JAIR, Volume 11, pages 95-130.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627?633.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get Another Label? Improving Data Qual-
ity and Data Mining Using Multiple, Noisy Labelers.
In Proc. of KDD-2008.
Push Singh. 2002. The public acquisition of common-
sense knowledge. In Proc. of AAAI Spring Sympo-
sium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access, 2002.
Alexander Sorokin and David Forsyth. 2008. Util-
ity data annotation with Amazon Mechanical Turk.
To appear in Proc. of First IEEE Workshop on
Internet Vision at CVPR, 2008. See also:
http://vision.cs.uiuc.edu/annotation/
David G. Stork. 1999. The Open Mind Initiative.
IEEE Expert Systems and Their Applications pp. 16-
20, May/June 1999.
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text In Proc. of SemEval-
2007.
Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C.
Baker. 2007. Internet-Scale Collection of Human-
Reviewed Data. In Proc. of WWW-2007.
Luis von Ahn and Laura Dabbish. 2004. Labeling Im-
ages with a Computer Game. In ACM Conference on
Human Factors in Computing Systems, CHI 2004.
Luis von Ahn, Mihir Kedia and Manuel Blum. 2006.
Verbosity: A Game for Collecting Common-Sense
Knowledge. In ACM Conference on Human Factors
in Computing Systems, CHI Notes 2006.
Ellen Voorhees and Hoa Trang Dang. 2006. Overview of
the TREC 2005 question answering track. In Proc. of
TREC-2005.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proc. of ACL-1999.
Annie Zaenen. Submitted. Do give a penny for their
thoughts. International Journal of Natural Language
Engineering (submitted).
263
Learning Named Entity Hyponyms for Question Answering
Paul McNamee
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
paul.mcnamee@jhuapl.edu
Rion Snow
Stanford AI Laboratory
Stanford University
Stanford, CA 94305, USA
rion@cs.stanford.edu
Patrick Schone
Department of Defense
Fort George G. Meade, MD 20755-6000
pjschon@tycho.ncsc.mil
James Mayfield
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
james.mayfield@jhuapl.edu
Abstract
Lexical mismatch is a problem that con-
founds automatic question answering sys-
tems. While existing lexical ontologies such
as WordNet have been successfully used to
match verbal synonyms (e.g., beat and de-
feat) and common nouns (tennis is-a sport),
their coverage of proper nouns is less ex-
tensive. Question answering depends sub-
stantially on processing named entities, and
thus it would be of significant benefit if
lexical ontologies could be enhanced with
additional hypernymic (i.e., is-a) relations
that include proper nouns, such as Edward
Teach is-a pirate. We demonstrate how a re-
cently developed statistical approach to min-
ing such relations can be tailored to iden-
tify named entity hyponyms, and how as a
result, superior question answering perfor-
mance can be obtained. We ranked candi-
date hyponyms on 75 categories of named
entities and attained 53% mean average pre-
cision. On TREC QA data our method pro-
duces a 9% improvement in performance.
1 Introduction
To correctly extract answers, modern question an-
swering systems depend on matching words be-
tween questions and retrieved passages containing
answers. We are interested in learning hypernymic
(i.e., is-a) relations involving named entities because
we believe these can be exploited to improve a sig-
nificant class of questions.
For example, consider the following questions:
? What island produces Blue Mountain coffee?
? In which game show do participants compete
based on their knowledge of consumer prices?
? What villain is the nemesis of Dudley Do-
Right?
Knowledge that Jamaica is an island, that The Price
is Right is a game show, and that Snidely Whiplash
is a villain, is crucial to answering these questions.
Sometimes these relations are evident in the same
context as answers to questions, for example, in
?The island of Jamaica is the only producer of Blue
Mountain coffee?; however, ?Jamaica is the only
producer of Blue Mountain coffee? should be suf-
ficient, despite the fact that Jamaica is an island is
not observable from the sentence.
The dynamic nature of named entities (NEs)
makes it difficult to enumerate all of their evolv-
ing properties; thus manual creation and curation
of this information in a lexical resource such as
WordNet (Fellbaum, 1998) is problematic. Pasca
and Harabagiu discuss how insufficient coverage of
named entities impairs QA (2001). They write:
?Because WordNet was not designed
as an encyclopedia, the hyponyms of con-
cepts such as composer or poet are illus-
trations rather than an exhaustive list of
instances. For example, only twelve com-
poser names specialize the concept com-
poser ... Consequently, the enhancement
of WordNet with NE information could
help QA.?
799
The chief contribution of this study is demonstrat-
ing that an automatically mined knowledge base,
which naturally contains errors as well as correctly
distilled knowledge, can be used to improve QA per-
formance. In Section 2 we discuss prior work in
identifying hypernymic relations. We then explain
our methods for improved NE hyponym learning
and its evaluation (Section 3) and apply the relations
that are discovered to enhance question answering
(Section 4). Finally we discuss our results (Section
5) and present our conclusions (Section 6).
2 Hyponym Induction
We review several approaches to learning is-a rela-
tions.
2.1 Hearst Patterns
The seminal work in the field of hypernym learn-
ing was done by Hearst (1992). Her approach was
to identify discriminating lexico-syntactic patterns
that suggest hypernymic relations. For example, ?X,
such as Y?, as in ?elements, such as chlorine and
fluorine?.
2.2 KnowItAll
Etzioni et al developed a system, KnowItAll, that
does not require training examples and is broadly
applicable to a variety of classes (2005). Starting
with seed examples generated from high precision
generic patterns, the system identifies class-specific
lexical and part-of-speech patterns and builds a
Bayesian classifier for each category. KnowItAll
was used to learn hundreds of thousands of class
instances and clearly has potential for improving
QA; however, it would be difficult to reproduce the
approach because of information required for each
class (i.e., specifying synonyms such as town and
village for city) and because it relies on submitting a
large number of queries to a web search engine.
2.3 Query Logs
Pasca and Van Durme looked at learning entity class
membership for five high frequency classes (com-
pany, country, city, drug, and painter), using search
engine query logs (2007). They reported precision
at 50 instances between 0.50 and 0.82.
2.4 Dependency Patterns
Snow et al have described an approach with several
desirable properties: (1) it is weakly-supervised and
only requires examples of hypernym/hyponym rela-
tions and unannotated text; (2) the method is suit-
able for both common and rare categories; and, (3)
it achieves good performance without post filtering
using the Web (2005; 2006). Their method relies
on dependency parsing, a form of shallow parsing
where each word modifies a single parent word.
Hypernym/hyponym word pairs where the words1
belong to a single WordNet synset were identified
and served to generate training data in the follow-
ing way: making the assumption that when the two
words co-occur, evidence for the is-a relation is
present, sentences containing both terms were ex-
tracted from unlabeled text. The sentences were
parsed and paths between the nouns in the depen-
dency trees were calculated and used as features in a
supervised classifier for hypernymy.
3 Learning Named Entity Hyponyms
The present work follows the technique described
by Snow et al; however, we tailor the approach in
several ways. First, we replace the logistic regres-
sion model with a support vector machine (SVM-
Light). Second, we significantly increase the size
of training corpora to increase coverage. This ben-
eficially increases the density of training and test
vectors. Third, we include additional features not
based on dependency parses (e.g., morphology and
capitalization). Fourth, because we are specifically
interested in hypernymic relations involving named
entities, we use a bootstrapping phase where train-
ing data consisting primarily of common nouns are
used to make predictions and we then manually ex-
tract named entity hyponyms to augment the train-
ing data. A second learner is then trained using the
entity-enriched data.
3.1 Data
We rely on large amounts of text; in all our exper-
iments we worked with a corpus from the sources
given in Table 1. Sentences that presented difficul-
ties in parsing were removed and those remaining
1Throughout the paper, use of the term word is intended to
include named entities and other multiword expressions.
800
Table 1: Sources used for training and learning.
Size Sentences Genre
TREC Disks 4,5 81 MB 0.70 M Newswire
AQUAINT 1464 MB 12.17 M Newswire
Wikipedia (4/04) 357 MB 3.27 M Encyclopedia
Table 2: Characteristics of training sets.
Pos. Pairs Neg. Pairs Total Features
Baseline 7975 63093 162528
+NE 9331 63093 164298
+Feat 7975 63093 162804
were parsed with MINIPAR (Lin, 1998). We ex-
tracted 17.3 million noun pairs that co-occurred in
at least one sentence. All pairs were viewed as po-
tential hyper/hyponyms.
Our three experimental conditions are summa-
rized in Table 2. The baseline model used 71068
pairs as training data; it is comparable to the
weakly-supervised hypernym classifier of Snow et
al. (2005), which used only dependency parse fea-
tures, although here the corpus is larger. The entity-
enriched data extended the baseline training set by
adding positive examples. The +Feat model uses ad-
ditional features besides dependency paths.
3.2 Bootstrapping
Our synthetic data relies on hyper/hyponym pairs
drawn from WordNet, which is generally rich in
common nouns and lacking in proper nouns. But
certain lexical and syntactic features are more likely
to be predictive for NE hyponyms. For example, it
is uncommon to precede a named entity with an in-
definite article, and certain superlative adjectives are
more likely to be used to modify classes of entities
(e.g., ?the youngest coach?, ?the highest peak?). Ac-
cordingly we wanted to enrich our training data with
NE exemplars.
By manually reviewing highly ranked predictions
of the baseline system, we identified 1356 additional
pairs to augment the training data. This annotation
took about a person-day. We then rescanned the cor-
pus to build training vectors for these co-occurring
nouns to produce the +NE model vectors.
Table 3: Features considered for +Feat model.
Feature Comment
Hypernym con-
tained in hyponym
Sands Hotel is-a hotel
Length in chars /
words
Chars: 1-4, 5-8, 9-16, 17+
Words: 1, 2, 3, 4, 5, 6, 7+
Has preposition Treaty of Paris; Statue of Liberty
Common suffixes -ation, -ment, -ology, etc...
Figurative term Such as goal, basis, or problem
Abstract category Like person, location, amount
Contains digits Usually not a good hyponym
Day of week;
month of year
Indiscriminately co-occurs with
many nouns.
Presence and depth
in WordNet graph
Shallow hypernyms are unlikely to
have entity hyponyms. Presence in
WN suggests word is not an entity.
Lexname of 1st
synset in WordNet
Root classes like person, location,
quantity, and process.
Capitalization Helps identify entities.
Binned document
frequency
Partitioned by base 10 logs
3.3 Additional Features
The +Feat model incorporated an additional 276 bi-
nary features which are listed in Table 3. We consid-
ered other features such as the frequency of patterns
on the Web, but with over 17 million noun pairs this
was computationally infeasible.
3.4 Evaluation
To compare our different models we created a test
set of 75 categories. The classes are diverse and
include personal, corporate, geographic, political,
artistic, abstract, and consumer product entities.
From the top 100 responses of the different learn-
ers, a pool of candidate hyponyms was created, ran-
domly reordered, and judged by one of the authors.
To assess the quality of purported hyponyms we
used average precision, a measure in ranked infor-
mation retrieval evaluation, which combines preci-
sion and recall.
Table 4 gives average precision values for the
three models on 15 classes of mixed difficulty2. Per-
formance varies considerably based on the hyper-
nym category, and for a given category, by classifier.
N is the number of known correct instances found in
the pool that belong to a given category.
Aggregate performance, as mean average preci-
sion, was computed over all 75 categories and is
2These are not the highest performing classes
801
Table 4: Average precision on 15 categories.
N Baseline +NE +Feat
chemical element 78 0.9096 0.9781 0.8057
african country 48 0.8581 0.8521 0.4294
prep school 26 0.6990 0.7098 0.7924
oil company 132 0.6406 0.6342 0.7808
boxer 109 0.6249 0.6487 0.6773
sculptor 95 0.6108 0.6375 0.8634
cartoonist 58 0.5988 0.6109 0.7097
volcano 119 0.5687 0.5516 0.7722
horse race 23 0.4837 0.4962 0.7322
musical 80 0.4827 0.4270 0.3690
astronaut 114 0.4723 0.5912 0.5738
word processor 26 0.4437 0.4426 0.6207
chief justice 115 0.4029 0.4630 0.5955
perfume 43 0.2482 0.2400 0.5231
pirate 10 0.1885 0.3070 0.2282
Table 5: Mean average precision over 75 categories.
Baseline +NE +Feat
MAP 0.4801 0.5001 (+4.2%) 0.5320 (+10.8%)
given in Table 5. Both the +NE and +Feat models
yielded improvements that were statistically signif-
icant at a 99% confidence level. The +Feat model
gained 11% over the baseline condition. The maxi-
mum F-score for +Feat is 0.55 at 70% recall.
Mean average precision emphasizes precision at
low ranks, so to capture the error characteristics at
multiple operating points we present a precision-
recall graph in Figure 1. The +NE and +Feat models
both attain superior performance at all but the lowest
recall levels. For question answering this is impor-
tant because it is not known which entities will be
the focus of a question, so the ability to deeply mine
various entity classes is important.
Table 6 lists top responses for four categories.
3.5 Discussion
53% mean average precision seems good, but is it
good enough? For automated taxonomy construc-
tion precision of extracted hyponyms is critically
important; however, because we want to improve
question answering we prefer high recall and can
tolerate some mistakes. This is because only a small
set of passages that are likely to contain an answer
are examined in detail, and only from this subset
of passages do we need to reason about potential
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall Level
P
r
e
c
i
s
i
o
n
Feat
Ent
Baseline
Figure 1: Precision-recall graph for three classifiers.
hyponyms. In the next section we describe an ex-
periment which confirms that our learned entity hy-
ponyms are beneficial.
4 QA Experiments
4.1 QACTIS
To evaluate the usefulness of our learned NE hy-
ponyms for question answering, we used the QAC-
TIS system (Schone et al, 2005). QACTIS was
fielded at the 2004-2006 TREC QA evaluations and
placed fifth at the 2005 workshop. We worked with
a version of the software from July 2005.
QACTIS uses WordNet to improve matching of
question and document words, and a resource, the
Semantic Forest Dictionary (SFD), which contains
many hypernym/hyponym pairs. The SFD was pop-
ulated through both automatic and manual means
(Schone et al, 2005), and was updated based on
questions asked in TREC evaluations through 2004.
4.2 Experimental Setup
We used factoid questions from the TREC 2005-
2006 QA evaluations (Voorhees and Dang, 2005)
and measured performance with mean reciprocal
rank (MRR) and percent correct at rank 1.
All runs made use of WordNet 2.0, and we ex-
amined several other sources of hypernym knowl-
802
Table 6: Top responses for four categories using the +Feat model. Starred entries were judged incorrect.
Sculptor Horse Race Astronaut Perfume
1 Evelyn Beatrice Longman Tevis Cup Mark L Polansky * Avishag
2 Nancy Schon Kenilworth Park Gold Cup Richard O Covey Ptisenbon
3 Phidias Cox Plate George D Nelson Poeme
4 Stanley Brandon Kearl Grosser Bugatti Preis Guion Bluford Jr Parfums International
5 Andy Galsworthy Melbourne Cup Stephen S Oswald Topper Schroeder
6 Alexander Collin * Great Budda Hall Eileen Collins * Baccarin
7 Rachel Feinstein Travers Stakes Leopold Eyharts Pink Lady
8 Zurab K Tsereteli English Derby Daniel M Tani Blue Waltz
9 Bertel Thorvaldsen * Contrade Ronald Grabe WCW Nitro
10 Cildo Meireles Palio * Frank Poole Jicky
Table 7: Additional knowledge sources by size.
Classes Class Instances
Baseline 76 11,066
SFD 1,140 75,647
SWN 7,327 458,370
+Feat 44,703 1,868,393
edge. The baseline condition added a small subset
of the Semantic Forest Dictionary consisting of 76
classes seen in earlier TREC test sets (e.g., nation-
alities, occupations, presidents). We also tested: (1)
the full SFD; (2) a database from the Stanford Word-
net (SWN) project (Snow et al, 2006); and, (3) the
+Feat model discussed in Section 3. The number of
classes and entries of each is given in Table 7.
4.3 Results
We observed that each source of knowledge benefit-
ted questions that were incorrectly answered in the
baseline condition. Examples include learning a me-
teorite (Q84.1), a university (Q93.3), a chief oper-
ating officer (Q108.3), a political party (Q183.3), a
pyramid (Q186.4), and a movie (Q211.5).
In Table 8 we compare performance on questions
from the 2005 and 2006 test sets. We assessed
performance primarily on test questions that were
deemed likely to benefit from hyponym knowledge
? questions that had a readily discernible category
(e.g., ?What film ...?, ?In what country ...?) ? but we
also give results on the entire test set.
The WordNet-only run suffers a large decrease
compared to the baseline. This is expected because
WordNet lacks coverage of entities and the baseline
condition specifically populates common categories
of entities that have been observed in prior TREC
evaluations. Nonetheless, WordNet is useful to the
system because it addresses lexical mismatch that
does not involve entities.
The full SFD, the SWN, and the +Feat model
achieved 17%, 2%, and 9% improvements in answer
correctness, respectively. While no model had ex-
posure to the 2005-2006 TREC questions, the SFD
database was manually updated based on training
on the TREC-8 through TREC-2004 data sets. It
approximates an upper bound on gains attributable
to addition of hyponym knowledge: it has an un-
fair advantage over the other models because recent
question sets use similar categories to those in ear-
lier TRECs. Our +Feat model, which has no bias
towards TREC questions, realizes larger gains than
the SWN. This is probably at least in part because it
produced a more diverse set of classes and a signif-
icantly larger number of class instances. Compared
to the baseline condition the +Feat model sees a 7%
improvement in mean reciprocal rank and a 9% im-
provement in correct first answers; both results rep-
resent a doubling of performance compared to the
use of WordNet alne. We believe that these results
illustrate clear improvement attributable to automat-
ically learned hyponyms.
The rightmost columns in Table 8 reveal that the
magnitude of improvements, when measured over
all questions, is less. But the drop off is consistent
with the fact that only one third of questions have
clear need for entity knowledge.
5 Discussion
Although there is a significant body of work in auto-
mated ontology construction, few researchers have
examined the relationship between their methods
803
Table 8: QA Performance on TREC 2005 & 2006 Data
Hyponym-Relevant Subset (242) All Questions (734)
MRR % Correct MRR % Correct
WN-alone 0.189 (-45.6%) 12.8 (-51.6%) 0.243 (-29.0%) 18.26 (-30.9%)
Baseline 0.348 26.4 0.342 26.4
SFD 0.405 (+16.5%) 31.0 (+17.2%) 0.362 (+5.6%) 27.9 (+5.7%)
SWN 0.351 (+1.0%) 26.9 (+1.6%) 0.343 (+0.3%) 26.6 (+0.5%)
Feat 0.373 (+7.4%) 28.9 (+9.4%) 0.351 (+2.5%) 27.3 (+3.1%)
for knowledge discovery and improved question-
answering performance. One notable study was con-
ducted by Mann (2002). Our work differs in two
ways: (1) his method for identifying hyponyms was
based on a single syntactic pattern, and (2) he looked
at a comparatively simple task ? given a question
and one answer sentence containing the answer, ex-
tract the correct named entity answer.
Other attempts to deal with lexical mismatch in
automated QA include rescoring based on syntactic
variation (Cui et al, 2005) and identification of ver-
bal paraphrases (Lin and Pantel, 2001).
The main contribution of this paper is showing
that large-scale, weakly-supervised hyponym learn-
ing is capable of producing improvements in an end-
to-end QA system. In contrast, previous studies have
generally presented algorithmic advances and show-
cased sample results, but failed to demonstrate gains
in a realistic application. While the hypothesis that
discovering is-a relations for entities would improve
factoid QA is intuitive, we believe these experiments
are important because they show that automatically
distilled knowledge, even when containing errors
that would not be introduced by human ontologists,
is effective in question answering systems.
6 Conclusion
We have shown that highly accurate statistical learn-
ing of named entity hyponyms is feasible and that
bootstrapping and feature augmentation can signif-
icantly improve classifier accuracy. Mean aver-
age precision of 53% was attained on a set of 75
categories that included many fine-grained entity
classes. We also demonstrated that mining knowl-
edge about entities can be directly applied to ques-
tion answering, and we measured the benefit on
TREC QA data. On a subset of questions for
which NE hyponyms are likely to help we found that
learned hyponyms generated a 9% improvement in
performance compared to a strong baseline.
References
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng
Chua. 2005. Question answering passage retrieval using
dependency relations. In SIGIR 2005, pages 400?407.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana M.
Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,
and Alexander Yates. 2005. Unsupervised Named-Entity
Extraction from the Web: An Experimental Study. Artificial
Intelligence, 165(1):191?134.
Christine Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In ACL 1992, pages 539?545.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of minipar.
In Workshop on the Evaluation of Parsing Systems.
Gideon S. Mann. 2002. Fine-grained proper noun ontolo-
gies for question answering. In COLING-02 on SEMANET,
pages 1?7.
Marius Pasca and Benjamin Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from query
logs. In IJCAI-07, pages 2832?2837.
Marius Pasca and Sanda M. Harabagiu. 2001. The informa-
tive role of wordnet in open-domain question answering. In
Proceedings of the NAACL 2001 Workshop on WordNet and
Other Lexical Resources.
Patrick Schone, Gary Ciany, Paul McNamee, James Mayfield,
and Thomas Smith. 2005. QACTIS-based Question An-
swering at TREC 2005. In Proceedings of the 14th Text RE-
trieval Conference.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym discovery. In
NIPS 17.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Seman-
tic taxonomy induction from heterogenous evidence. In ACL
2006, pages 801?808.
804
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 33?40,
New York, June 2006. c?2006 Association for Computational Linguistics
Effectively Using Syntax for Recognizing False Entailment
Rion Snow
Computer Science Department
Stanford University
Stanford, CA 94305
rion@cs.stanford.edu
Lucy Vanderwende and Arul Menezes
Microsoft Research
One Microsoft Way
Redmond, WA 98027
{lucyv,arulm}@microsoft.com
Abstract
Recognizing textual entailment is a chal-
lenging problem and a fundamental com-
ponent of many applications in natural
language processing. We present a novel
framework for recognizing textual entail-
ment that focuses on the use of syntactic
heuristics to recognize false entailment.
We give a thorough analysis of our sys-
tem, which demonstrates state-of-the-art
performance on a widely-used test set.
1 Introduction
Recognizing the semantic equivalence of two frag-
ments of text is a fundamental component of many
applications in natural language processing. Recog-
nizing textual entailment, as formulated in the recent
PASCAL Challenge 1, is the problem of determining
whether some text sentence T entails some hypothe-
sis sentence H .
The motivation for this formulation was to iso-
late and evaluate the application-independent com-
ponent of semantic inference shared across many ap-
plication areas, reflected in the division of the PAS-
CAL RTE dataset into seven distinct tasks: Informa-
tion Extraction (IE), Comparable Documents (CD),
Reading Comprehension (RC), Machine Translation
(MT), Information Retrieval (IR), Question Answer-
ing (QA), and Paraphrase Acquisition (PP).
1http://www.pascal-network.org/Challenges/RTE. The ex-
amples given throughout this paper are from the first PASCAL
RTE dataset, described in Section 6.
The RTE problem as presented in the PASCAL
RTE dataset is particularly attractive in that it is a
reasonably simple task for human annotators with
high inter-annotator agreement (95.1% in one inde-
pendent labeling (Bos and Markert, 2005)), but an
extremely challenging task for automated systems.
The highest accuracy systems on the RTE test set
are still much closer in performance to a random
baseline accuracy of 50% than to the inter-annotator
agreement. For example, two high-accuracy systems
are those described in (Tatu and Moldovan, 2005),
achieving 60.4% accuracy with no task-specific in-
formation, and (Bos and Markert, 2005), which
achieves 61.2% task-dependent accuracy, i.e. when
able to use the specific task labels as input.
Previous systems for RTE have attempted a wide
variety of strategies. Many previous approaches
have used a logical form representation of the text
and hypothesis sentences, focusing on deriving a
proof by which one can infer the hypothesis logical
form from the text logical form (Bayer et al, 2005;
Bos and Markert, 2005; Raina et al, 2005; Tatu and
Moldovan, 2005). These papers often cite that a ma-
jor obstacle to accurate theorem proving for the task
of textual entailment is the lack of world knowledge,
which is frequently difficult and costly to obtain and
encode. Attempts have been made to remedy this
deficit through various techniques, including model-
building (Bos and Markert, 2005) and the addition
of semantic axioms (Tatu and Moldovan, 2005).
Our system diverges from previous approaches
most strongly by focusing upon false entailments;
rather than assuming that a given entailment is false
until proven true, we make the opposite assump-
33
tion, and instead focus on applying knowledge-free
heuristics that can act locally on a subgraph of syn-
tactic dependencies to determine with high confi-
dence that the entailment is false. Our approach is
inspired by an analysis of the RTE dataset that sug-
gested a syntax-based approach should be approxi-
mately twice as effective at predicting false entail-
ment as true entailment (Vanderwende and Dolan,
2006). The analysis implied that a great deal of syn-
tactic information remained unexploited by existing
systems, but gave few explicit suggestions on how
syntactic information should be applied; this paper
provides a starting point for creating the heuristics
capable of obtaining the bound they suggest2.
2 System Description
Similar to most other syntax-based approaches to
recognizing textual entailment, we begin by rep-
resenting each text and hypothesis sentence pair
in logical forms. These logical forms are gener-
ated using NLPWIN3, a robust system for natural
language parsing and generation (Heidorn, 2000).
Our logical form representation may be consid-
ered equivalently as a set of triples of the form
RELATION(nodei, nodej), or as a graph of syntac-
tic dependencies; we use both terminologies inter-
changeably. Our algorithm proceeds as follows:
1. Parse each sentence with the NLPWIN parser,
resulting in syntactic dependency graphs for the
text and hypothesis sentences.
2. Attempt an alignment of each content node in
the dependency graph of the hypothesis sen-
tence to some node in the graph of the text sen-
tence, using a set of heuristics for alignment
(described in Section 3).
3. Using the alignment, apply a set of syntactic
heuristics for recognizing false entailment (de-
scribed in Section 4); if any match, predict that
the entailment is false.
2(Vanderwende and Dolan, 2006) suggest that the truth or
falsehood of 48% of the entailment examples in the RTE test set
could be correctly identified via syntax and a thesaurus alone;
thus by random guessing on the rest of the examples one might
hope for an accuracy level of 0.48 + 0.522 = 74%.3To aid in the replicability of our experiments, we have
published the NLPWIN logical forms for all sentences from
the development and test sets in the PASCAL RTE dataset at
http://research.microsoft.com/nlp/Projects/RTE.aspx.
lemma: freepos: Verbfeatures: Past,Pass,T1,Proposition
lemma: _Xpos: PronTsub
lemma: hostagepos: Nounfeatures: Plur,Humn,Count,Anim,Conc,Humn_sr
Tobj
lemma: sixpos: Adjfeatures: Quant,Plur,Num,Value 6Lops
lemma: Iraqpos: Nounfeatures: Sing,PrprN,Pers3,Cntry
Locn_in
Figure 1: Logical form produced by NLPWIN for
the sentence ?Six hostages in Iraq were freed.?
4. If no syntactic heuristic matches, back off to
a lexical similarity model (described in section
5.1), with an attempt to align detected para-
phrases (described in section 5.2).
In addition to the typical syntactic information pro-
vided by a dependency parser, the NLPWIN parser
provides an extensive number of semantic features
obtained from various linguistic resources, creating
a rich environment for feature engineering. For ex-
ample, Figure 1 (from Dev Ex. #616) illustrates the
dependency graph representation we use, demon-
strating the stemming, part-of-speech tagging, syn-
tactic relationship identification, and semantic fea-
ture tagging capabilities of NLPWIN.
We define a content node to be any node whose
lemma is not on a small stoplist of common stop
words. In addition to content vs. non-content nodes,
among content nodes we distinguish between en-
tities and nonentities: an entity node is any node
classified by the NLPWIN parser as being a proper
noun, quantity, or time.
Each of the features of our system were developed
from inspection of sentence pairs from the RTE de-
velopment data set, and used in the final system only
if they improved the system?s accuracy on the de-
velopment set (or improved F-score if accuracy was
unchanged); sentence pairs in the RTE test set were
left uninspected and used for testing purposes only.
3 Linguistic cues for node alignment
Our syntactic heuristics for recognizing false entail-
ment rely heavily on the correct alignment of words
and multiword units between the text and hypothesis
logical forms. In the notation below, we will con-
sider h and t to be nodes in the hypothesis H and
34
Hypothesis: ??Hepburn, who won four Oscars...??
Text: ??Hepburn, a four-time Academy Award winner...??
HepburnNoun winVerbTsub
HepburnNoun
Stringmatch
OscarNounTobj
winnerNoun
Derivationalform match
fourAdjLops
Academy_AwardNoun
Synonymmatch
four-timeAdj
Valuematch
Appostn
Attrib
Mod
Figure 2: Example of synonym, value, and deriva-
tional form alignment heuristics, Dev Ex. #767
text T logical forms, respectively. To accomplish
the task of node alignment we rely on the following
heuristics:
3.1 WordNet synonym match
As in (Herrera et al, 2005) and others, we align
a node h ? H to any node t ? T that has both
the same part of speech and belongs to the same
synset in WordNet. Our alignment considers mul-
tiword units, including compound nouns (e.g., we
align ?Oscar? to ?Academy Award? as in Figure 2),
as well as verb-particle constructions such as ?set
off? (aligned to ?trigger? in Test Ex. #1983).
3.2 Numeric value match
The NLPWIN parser assigns a normalized numeric
value feature to each piece of text inferred to cor-
respond to a numeric value; this allows us to align
?6th? to ?sixth? in Test Ex. #1175. and to align ?a
dozen? to ?twelve? in Test Ex. #1231.
3.3 Acronym match
Many acronyms are recognized using the syn-
onym match described above; nonetheless, many
acronyms are not yet in WordNet. For these cases we
have a specialized acronym match heuristic which
aligns pairs of nodes with the following properties:
if the lemma for some node h consists only of cap-
italized letters (with possible interceding periods),
and the letters correspond to the first characters of
some multiword lemma for some t ? T , then we
consider h and t to be aligned. This heuristic allows
us to align ?UNDP? to ?United Nations Develop-
ment Programme? in Dev Ex. #357 and ?ANC? to
?African National Congress? in Test Ex. #1300.
3.4 Derivational form match
We would like to align words which have the same
root form (or have a synonym with the same root
form) and which possess similar semantic meaning,
but which may belong to different syntactic cate-
gories. We perform this by using a combination of
the synonym and derivationally-related form infor-
mation contained within WordNet. Explicitly our
procedure for constructing the set of derivationally-
related forms for a node h is to take the union of all
derivationally-related forms of all the synonyms of
h (including h itself), i.e.:
DERIV(h) = ?s?WN-SYN(h)WN-DERIV(s)
In addition to the noun/verb derivationally-related
forms, we detect adjective/adverb derivationally-
related forms that differ only by the suffix ?ly?.
Unlike the previous alignment heuristics, we do
not expect that two nodes aligned via derivationally-
related forms will play the same syntactic role in
their respective sentences. Thus we consider two
nodes aligned in this way to be soft-aligned, and we
do not attempt to apply our false entailment recog-
nition heuristics to nodes aligned in this way.
3.5 Country adjectival form / demonym match
As a special case of derivational form match, we
soft-align matches from an explicit list of place
names, adjectival forms, and demonyms4; e.g.,
?Sweden? and ?Swedish? in Test Ex. #1576.
3.6 Other heuristics for alignment
In addition to these heuristics, we implemented a hy-
ponym match heuristic similar to that discussed in
(Herrera et al, 2005), and a heuristic based on the
string-edit distance of two lemmas; however, these
heuristics yielded a decrease in our system?s accu-
racy on the development set and were thus left out
of our final system.
4 Recognizing false entailment
The bulk of our system focuses on heuristics for
recognizing false entailment. For purposes of no-
tation, we define binary functions for the existence
4List of adjectival forms and demonyms based on the list at:
http://en.wikipedia.org/wiki/List of demonyms
35
Unaligned Entity: ENTITY(h) ? ?t.?ALIGN(h, t) ? False.
Negation Mismatch: ALIGN(h, t) ? NEG(t) 6= NEG(h) ? False.
Modal Mismatch: ALIGN(h, t) ? MOD(t) ? ?MOD(h) ? False.
Antonym Match: ALIGN(h1, t1) ? REL(h0, h1) ? REL(t0, t1) ? LEMMA(t0) ? ANTONYMS(h0) ? False
Argument Movement: ALIGN(h1, t1) ? ALIGN(h2, t2) ? REL(h1, h2) ? ?REL(t1, t2) ? REL ? {SUBJ, OBJ, IND} ? False
Superlative Mismatch: ?(SUPR(h1) ? (ALIGN(h1, t1) ? ALIGN(h2, t2) ? REL1(h2, h1) ? REL1(t2, t1)
??t3.(REL2(t2, t3) ? REL2 ? {MOD,POSSR,LOCN} ? REL2(h2, h3) ? ALIGN(h3, t3))) ? False
Conditional Mismatch: ALIGN(h1, t1) ? ALIGN(h2, t2) ? COND ? PATH(t1, t2) ? COND /? PATH(h1, h2) ? False
Table 1: Summary of heuristics for recognizing false entailment
of each semantic node feature recognized by NLP-
WIN; e.g., if h is negated, we state that NEG(h) =
TRUE. Similarly we assign binary functions for
the existence of each syntactic relation defined over
pairs of nodes. Finally, we define the function
ALIGN(h, t) to be true if and only if the node h ? H
has been ?hard-aligned? to the node t ? T using one
of the heuristics in Section 3. Other notation is de-
fined in the text as it is used. Table 1 summarizes all
heuristics used in our final system to recognize false
entailment.
4.1 Unaligned entity
If some node h has been recognized as an entity (i.e.,
as a proper noun, quantity, or time) but has not been
aligned to any node t, we predict that the entailment
is false. For example, we predict that Test Ex. #1863
is false because the entities ?Suwariya?, ?20 miles?,
and ?35? in H are unaligned.
4.2 Negation mismatch
If any two nodes (h, t) are aligned, and one (and
only one) of them is negated, we predict that the en-
tailment is false. Negation is conveyed by the NEG
feature in NLPWIN. This heuristic allows us to pre-
dict false entailment in the example ?Pertussis is not
very contagious? and ?...pertussis, is a highly conta-
gious bacterial infection? in Test Ex. #1144.
4.3 Modal auxiliary verb mismatch
If any two nodes (h, t) are aligned, and t is modified
by a modal auxiliary verb (e.g, can, might, should,
etc.) but h is not similarly modified, we predict that
the entailment is false. Modification by a modal aux-
iliary verb is conveyed by the MOD feature in NLP-
WIN. This heuristic allows us to predict false en-
tailment between the text phrase ?would constitute
a threat to democracy?, and the hypothesis phrase
?constitutes a democratic threat? in Test Ex. #1203.
4.4 Antonym match
If two aligned noun nodes (h1, t1) are both subjects
or both objects of verb nodes (h0, t0) in their re-
spective sentences, i.e., REL(h0, h1)? REL(t0, t1)?
REL ? {SUBJ,OBJ}, then we check for a verb
antonym match between (h0, t0). We construct
the set of verb antonyms using WordNet; we con-
sider the antonyms of h0 to be the union of the
antonyms of the first three senses of LEMMA(h0),
or of the nearest antonym-possessing hypernyms if
those senses do not themselves have antonyms in
WordNet. Explicitly our procedure for constructing
the antonym set of a node h0 is as follows:
1. ANTONYMS(h0) = {}
2. For each of the first three listed senses s of
LEMMA(h0) in WordNet:
(a) While |WN-ANTONYMS(s)| = 0
i. s ? WN-HYPERNYM(s)
(b) ANTONYMS(h0) ? ANTONYMS(h0) ?
WN-ANTONYMS(s)
3. return ANTONYMS(h0)
In addition to the verb antonyms in WordNet, we
detect the prepositional antonym pairs (before/after,
to/from, and over/under). This heuristic allows us to
predict false entailment between ?Black holes can
lose mass...? and ?Black holes can regain some of
their mass...? in Test Ex. #1445.
4.5 Argument movement
For any two aligned verb nodes (h1, t1), we con-
sider each noun child h2 of h1 possessing any of
36
Hypothesis Text
killVerb
Prime MinisterRobert MalvalNoun
Tobj
AristideNoun
Tsub
killVerb
Prime MinisterRobert MalvalNoun
AristideNoun
Tsub
conferenceNoun
Tobj
 
 
   
 
callVerb
Attrib
conferenceNoun
TobjTsub
 
 
   
 
Port-au-PrinceNoun
Locn_in
Figure 3: Example of object movement signaling
false entailment
the subject, object, or indirect object relations to
h1, i.e., there exists REL(h1, h2) such that REL ?
{SUBJ, OBJ, IND}. If there is some node t2 such that
ALIGN(h2, t2), but REL(t1, t2) 6= REL(h1, h2), then
we predict that the entailment is false.
As an example, consider Figure 3, representing
subgraphs from Dev Ex. #1916:
T : ...U.N. officials are also dismayed that Aristide killed a con-
ference called by Prime Minister Robert Malval...
H: Aristide kills Prime Minister Robert Malval.
Here let (h1, t1) correspond to the aligned verbs
with lemma kill, where the object of h1 has lemma
Prime Minister Robert Malval, and the object of t1
has lemma conference. Since h2 is aligned to some
node t2 in the text graph, but ?OBJ(t1, t2), the sen-
tence pair is rejected as a false entailment.
4.6 Superlative mismatch
If some adjective node h1 in the hypothesis is iden-
tified as a superlative, check that all of the following
conditions are satisfied:
1. h1 is aligned to some superlative t1 in the text
sentence.
2. The noun phrase h2 modified by h1 is aligned
to the noun phrase t2 modified by t1.
3. Any additional modifier t3 of the noun phrase
t2 is aligned to some modifier h3 of h2 in the
hypothesis sentence (reverse subset match).
If any of these conditions are not satisfied, we pre-
dict that the entailment is false. This heuristic allows
us to predict false entailment in (Dev Ex. #908):
T : Time Warner is the world?s largest media and Internet com-
pany.
H: Time Warner is the world?s largest company.
Here ?largest media and Internet company? in T
fails the reverse subset match (condition 3) to
?largest company? in H .
4.7 Conditional mismatch
For any pair of aligned nodes (h1, t1), if there ex-
ists a second pair of aligned nodes (h2, t2) such
that the shortest path PATH(t1, t2) in the depen-
dency graph T contains the conditional relation,
then PATH(h1, h2) must also contain the conditional
relation, or else we predict that the entailment is
false. For example, consider the following false en-
tailment (Dev Ex. #60):
T : If a Mexican approaches the border, he?s assumed to be try-
ing to illegally cross.
H: Mexicans continue to illegally cross border.
Here, ?Mexican? and ?cross? are aligned, and the
path between them in the text contains the condi-
tional relation, but does not in the hypothesis; thus
the entailment is predicted to be false.
4.8 Other heuristics for false entailment
In addition to these heuristics, we additionally im-
plemented an IS-A mismatch heuristic, which at-
tempted to discover when an IS-A relation in the hy-
pothesis sentence was not implied by a correspond-
ing IS-A relation in the text; however, this heuristic
yielded a loss in accuracy on the development set
and was therefore not included in our final system.
5 Lexical similarity and paraphrase
detection
5.1 Lexical similarity using MindNet
In case none of the preceding heuristics for rejec-
tion are applicable, we back off to a lexical sim-
ilarity model similar to that described in (Glick-
man et al, 2005). For every content node h ? H
37
not already aligned by one of the heuristics in Sec-
tion 3, we obtain a similarity score MN(h, t) from a
similarity database that is constructed automatically
from the data contained in MindNet5 as described in
(Richardson, 1997). Our similarity function is thus:
sim(h, t) =
?
??
??
1 if ANY-ALIGN(h, t)
MN(h, t) if MN(h, t) > min
min otherwise
Where the minimum score min is a parameter
tuned for maximum accuracy on the development
set; min = 0.00002 in our final system. We then
compute the entailment score:
score(H,T ) = 1|H|
?
h?H
max
t?T
sim(h, t)
This approach is identical to that used in (Glick-
man et al, 2005), except that we use alignment
heuristics and MindNet similarity scores in place
of their web-based estimation of lexical entailment
probabilities, and we take as our score the geomet-
ric mean of the component entailment scores rather
than the unnormalized product of probabilities.
5.2 Measuring phrasal similarity using the web
The methods discussed so far for alignment are lim-
ited to aligning pairs of single words or multiple-
word units constituting single syntactic categories;
these are insufficient for the problem of detecting
more complicated paraphrases. For example, con-
sider the following true entailment (Dev Ex. #496):
T : ...Muslims believe there is only one God.
H: Muslims are monotheistic.
Here we would like to align the hypothesis phrase
?are monotheistic? to the text phrase ?believe there
is only one God?; unfortunately, single-node align-
ment aligns only the nodes with lemma ?Muslim?.
In this section we describe the approach used in our
system to approximate phrasal similarity via distrib-
utional information obtained using the MSN Search
search engine.
We propose a metric for measuring phrasal simi-
larity based on a phrasal version of the distributional
hypothesis: we propose that a phrase template Ph
5http://research.microsoft.com/mnex
(e.g. ?xh are monotheistic?) has high semantic simi-
larity to a template Pt (e.g. ?xt believe there is only
one God?), with possible ?slot-fillers? xh and xt, re-
spectively, if the overlap of the sets of observed slot-
fillers Xh ?Xt for those phrase templates is high in
some sufficiently large corpus (e.g., the Web).
To measure phrasal similarity we issue the sur-
face text form of each candidate phrase template as
a query to a web-based search engine, and parse the
returned sentences in which the candidate phrase oc-
curs to determine the appropriate slot-fillers. For ex-
ample, in the above example, we observe the set of
slot-fillers Xt = {Muslims, Christians, Jews, Saiv-
ities, Sikhs, Caodaists, People}, and Xh ? Xt =
{Muslims, Christians, Jews, Sikhs, People}.
Explicitly, given the text and hypothesis logical
forms, our algorithm proceeds as follows to compute
the phrasal similarity between all phrase templates
in H and T :
1. For each pair of aligned single node and un-
aligned leaf node (t1, tl) (or pair of aligned
nodes (t1, t2)) in the text T :
(a) Use NLPWIN to generate a surface text
string S from the underlying logical form
PATH(t1, t2).
(b) Create the surface string template phrase
Pt by removing from S the lemmas corre-
sponding to t1 (and t2, if path is between
aligned nodes).
(c) Perform a web search for the string Pt.
(d) Parse the resulting sentences containing
Pt and extract all non-pronoun slot fillers
xt ? Xt that satisfy the same syntactic
roles as t1 in the original sentence.
2. Similarly, extract the slot fillers Xh for each
discovered phrase template Ph in H .
3. Calculate paraphrase similarity as a function of
the overlap between the slot-filler sets Xt and
Xh, i.e: score(Ph, Pt) = |Xh?Xt||Xt| .
We then incorporate paraphrase similarity within the
lexical similarity model by allowing, for some un-
aligned node h ? Ph, where t ? Pt:
sim(h, t) = max(MN(h, t), score(Ph, Pt))
38
Our approach to paraphrase detection is most similar
to the TE/ASE algorithm (Szpektor et al, 2004), and
bears similarity to both DIRT (Lin and Pantel, 2001)
and KnowItAll (Etzioni et al, 2004). The chief
difference in our algorithm is that we generate the
surface text search strings from the parsed logical
forms using the generation capabilities of NLPWIN
(Aikawa et al, 2001), and we verify that the syn-
tactic relations in each discovered web snippet are
isomorphic to those in the original candidate para-
phrase template.
6 Results and Discussion
In this section we present the final results of our sys-
tem on the PASCAL RTE-1 test set, and examine our
features in an ablation study. The PASCAL RTE-1
development and test sets consist of 567 and 800 ex-
amples, respectively, with the test set split equally
between true and false examples.
6.1 Results and Performance Comparison on
the PASCAL RTE-1 Test Set
Table 2 displays the accuracy and confidence-
weighted score6 (CWS) of our final system on each
of the tasks for both the development and test sets.
Our overall test set accuracy of 62.50% rep-
resents a 2.1% absolute improvement over the
task-independent system described in (Tatu and
Moldovan, 2005), and a 20.2% relative improve-
ment in accuracy over their system with respect to
an uninformed baseline accuracy of 50%.
To compute confidence scores for our judgments,
any entailment determined to be false by any heuris-
tic was assigned maximum confidence; no attempts
were made to distinguish between entailments re-
jected by different heuristics. The confidence of
all other predictions was calculated as the ab-
solute value in the difference between the output
score(H,T ) of the lexical similarity model and the
threshold t = 0.1285 as tuned for highest accu-
racy on our development set. We would expect a
higher CWS to result from learning a more appro-
priate confidence function; nonetheless our overall
6As in (Dagan et al, 2005) we compute the confidence-
weighted score (or ?average precision?) over n examples
{c1, c2, ..., cn} ranked in order of decreasing confidence as
cws = 1n
?n
i=1
(#correct-up-to-rank-i)
i
Dev Set Test Set
Task acc cws acc cws
CD 0.8061 0.8357 0.7867 0.8261
RC 0.5534 0.5885 0.6429 0.6476
IR 0.6857 0.6954 0.6000 0.6571
MT 0.7037 0.7145 0.6000 0.6350
IE 0.5857 0.6008 0.5917 0.6275
QA 0.7111 0.7121 0.5308 0.5463
PP 0.7683 0.7470 0.5200 0.5333
All 0.6878 0.6888 0.6250 0.6534
Table 2: Summary of accuracies and confidence-
weighted scores, by task
Alignment Feature Dev Test
Synonym Match 0.0106 0.0038
Derivational Form 0.0053 0.0025
Paraphrase 0.0053 0.0000
Lexical Similarity 0.0053 0.0000
Value Match 0.0017 0.0013
Acronym Match 0.0017 0.0013
Adjectival Form7 0.0000 0.0063
False Entailment Feature Dev Test
Negation Mismatch 0.0106 0.0025
Argument Movement 0.0070 0.0250
Conditional Mismatch 0.0053 0.0037
Modal Mismatch 0.0035 0.0013
Superlative Mismatch 0.0035 -0.0025
Entity Mismatch 0.0018 0.0063
Table 3: Feature ablation study; quantity is the ac-
curacy loss obtained by removal of single feature
test set CWS of 0.6534 is higher than previously-
reported task-independent systems (however, the
task-dependent system reported in (Raina et al,
2005) achieves a CWS of 0.686).
6.2 Feature analysis
Table 3 displays the results of our feature ablation
study, analyzing the individual effect of each feature.
Of the seven heuristics used in our final system
for node alignment (including lexical similarity and
paraphrase detection), our ablation study showed
7As discussed in Section 2, features with no effect on devel-
opment set accuracy were included in the system if and only if
they improved the system?s unweighted F-score.
39
that five were helpful in varying degrees on our test
set, but that removal of either MindNet similarity
scores or paraphrase detection resulted in no accu-
racy loss on the test set.
Of the six false entailment heuristics used in the
final system, five resulted in an accuracy improve-
ment on the test set (the most effective by far was
the ?Argument Movement?, resulting in a net gain
of 20 correctly-classified false examples); inclusion
of the ?Superlative Mismatch? feature resulted in a
small net loss of two examples.
We note that our heuristics for false entailment,
where applicable, were indeed significantly more ac-
curate than our final system as a whole; on the set of
examples predicted false by our heuristics we had
71.3% accuracy on the training set (112 correct out
of 157 predicted), and 72.9% accuracy on the test set
(164 correct out of 225 predicted).
7 Conclusion
In this paper we have presented and analyzed a sys-
tem for recognizing textual entailment focused pri-
marily on the recognition of false entailment, and
demonstrated higher performance than achieved by
previous approaches on the widely-used PASCAL
RTE test set. Our system achieves state-of-the-
art performance despite not exploiting a wide ar-
ray of sources of knowledge used by other high-
performance systems; we submit that the perfor-
mance of our system demonstrates the unexploited
potential in features designed specifically for the
recognition of false entailment.
Acknowledgments
We thank Chris Brockett, Michael Gamon, Gary
Kacmarick, and Chris Quirk for helpful discussion.
Also, thanks to Robert Ragno for assistance with
the MSN Search API. Rion Snow is supported by
an NDSEG Fellowship sponsored by the DOD and
AFOSR.
References
Takako Aikawa, Maite Melero, Lee Schwartz, and Andi
Wu. 2001. Multilingual Sentence Generation. In
Proc. of 8th European Workshop on Natural Language
Generation.
Samuel Bayer, John Burger, Lisa Ferro, John Henderson,
and Alexander Yeh. 2005. MITRE?s Submissions to
the EU Pascal RTE Challenge. In Proc. of the PASCAL
Challenges Workshop on RTE 2005.
Johan Bos and Katja Markert. 2005. Recognizing Tex-
tual Entailment with Logical Inference. In Proc. HLT-
EMNLP 2005.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In Proceedings of the PASCAL Challenges
Workshop on RTE 2005.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S. Weld, and Alexander Yates. 2004.
Web-scale information extraction in KnowItAll. In
Proc. WWW 2004.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
Web Based Probabilistic Textual Entailment. In Proc.
of the PASCAL Challenges Workshop on RTE 2005.
George E. Heidorn. 2000. Intelligent Writing Assis-
tance. In R. Dale, H. Moisl, and H. Somers (eds.),
A Handbook of Natural Language Processing: Tech-
niques and Applications for the Processing of Lan-
guage as Text. Marcel Dekker, New York. 181-207.
Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo. 2005.
Textual Entailment Recognision Based on Depen-
dency Analysis and WordNet. In Proc. of the PASCAL
Challenges Workshop on RTE 2005.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proc. KDD 2001.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proc. AAAI 2005.
Stephen D. Richardson. 1997. Determining Similarity
and Inferring Relations in a Lexical Knowledge Base.
Ph.D. thesis, The City University of New York.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura
Coppola. 2004. Scaling Web-based Acquisition of
Entailment Relations. In Proc. EMNLP 2004.
Marta Tatu and Dan Moldovan. 2005. A Semantic Ap-
proach to Recognizing Textual Entailment. In Proc.
HLT-EMNLP 2005.
Lucy Vanderwende and William B. Dolan. 2006. What
Syntax Can Contribute in the Entailment Task. In
MLCW 2005, LNAI 3944, pp. 205?216. J. Quinonero-
Candela et al (eds.). Springer-Verlag.
40
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801?808,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Taxonomy Induction from Heterogenous Evidence
Rion Snow
Computer Science Department
Stanford University
Stanford, CA 94305
rion@cs.stanford.edu
Daniel Jurafsky
Linguistics Department
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305
ang@cs.stanford.edu
Abstract
We propose a novel algorithm for inducing seman-
tic taxonomies. Previous algorithms for taxonomy
induction have typically focused on independent
classifiers for discovering new single relationships
based on hand-constructed or automatically discov-
ered textual patterns. By contrast, our algorithm
flexibly incorporates evidence from multiple clas-
sifiers over heterogenous relationships to optimize
the entire structure of the taxonomy, using knowl-
edge of a word?s coordinate terms to help in deter-
mining its hypernyms, and vice versa. We apply our
algorithm on the problem of sense-disambiguated
noun hyponym acquisition, where we combine the
predictions of hypernym and coordinate term clas-
sifiers with the knowledge in a preexisting seman-
tic taxonomy (WordNet 2.1). We add 10, 000 novel
synsets to WordNet 2.1 at 84% precision, a rela-
tive error reduction of 70% over a non-joint algo-
rithm using the same component classifiers. Fi-
nally, we show that a taxonomy built using our al-
gorithm shows a 23% relative F-score improvement
over WordNet 2.1 on an independent testset of hy-
pernym pairs.
1 Introduction
The goal of capturing structured relational knowl-
edge about lexical terms has been the motivating
force underlying many projects in lexical acquisi-
tion, information extraction, and the construction
of semantic taxonomies. Broad-coverage seman-
tic taxonomies such as WordNet (Fellbaum, 1998)
and CYC (Lenat, 1995) have been constructed by
hand at great cost; while a crucial source of knowl-
edge about the relations between words, these tax-
onomies still suffer from sparse coverage.
Many algorithms with the potential for auto-
matically extending lexical resources have been
proposed, including work in lexical acquisition
(Riloff and Shepherd, 1997; Roark and Charniak,
1998) and in discovering instances, named enti-
ties, and alternate glosses (Etzioni et al, 2005;
Pasc?a, 2005). Additionally, a wide variety of
relationship-specific classifiers have been pro-
posed, including pattern-based classifiers for hy-
ponyms (Hearst, 1992), meronyms (Girju, 2003),
synonyms (Lin et al, 2003), a variety of verb re-
lations (Chklovski and Pantel, 2004), and general
purpose analogy relations (Turney et al, 2003).
Such classifiers use hand-written or automatically-
induced patterns like Such NPy as NPx or NPy
like NPx to determine, for example that NPy is a
hyponym of NPx (i.e., NPy IS-A NPx). While
such classifiers have achieved some degree of suc-
cess, they frequently lack the global knowledge
necessary to integrate their predictions into a com-
plex taxonomy with multiple relations.
Past work on semantic taxonomy induction in-
cludes the noun hypernym hierarchy created in
(Caraballo, 2001), the part-whole taxonomies in
(Girju, 2003), and a great deal of recent work de-
scribed in (Buitelaar et al, 2005). Such work has
typically either focused on only inferring small
taxonomies over a single relation, or as in (Cara-
ballo, 2001), has used evidence for multiple rela-
tions independently from one another, by for ex-
ample first focusing strictly on inferring clusters
of coordinate terms, and then by inferring hyper-
nyms over those clusters.
Another major shortfall in previous techniques
for taxonomy induction has been the inability to
handle lexical ambiguity. Previous approaches
have typically sidestepped the issue of polysemy
altogether by making the assumption of only a sin-
gle sense per word, and inferring taxonomies ex-
plicitly over words and not senses. Enforcing a
false monosemy has the downside of making po-
tentially erroneous inferences; for example, col-
lapsing the polysemous term Bush into a single
sense might lead one to infer by transitivity that
a rose bush is a kind of U.S. president.
Our approach simultaneously provides a solu-
tion to the problems of jointly considering evi-
dence about multiple relationships as well as lexi-
cal ambiguity within a single probabilistic frame-
work. The key contribution of this work is to offer
a solution to two crucial problems in taxonomy in-
801
duction and hyponym acquisition: the problem of
combining heterogenous sources of evidence in a
flexible way, and the problem of correctly identi-
fying the appropriate word sense of each new word
added to the taxonomy.1
2 A Probabilistic Framework for
Taxonomy Induction
In section 2.1 we introduce our definitions for tax-
onomies, relations, and the taxonomic constraints
that enforce dependencies between relations; in
section 2.2 we give a probabilistic model for defin-
ing the conditional probability of a set of relational
evidence given a taxonomy; in section 2.3 we for-
mulate a local search algorithm to find the taxon-
omy maximizing this conditional probability; and
in section 2.4 we extend our framework to deal
with lexical ambiguity.
2.1 Taxonomies, Relations, and Taxonomic
Constraints
We define a taxonomy T as a set of pairwise re-
lations R over some domain of objects DT. For
example, the relations in WordNet include hyper-
nymy, holonymy, verb entailment, and many oth-
ers; the objects of WordNet between which these
relations hold are its word senses or synsets. We
define that each relation R ? R is a set of ordered
or unordered pairs of objects (i, j) ? DT; we de-
fine Rij ? T if relationship R holds over objects
(i, j) in T.
Relations for Hyponym Acquisition
For the case of hyponym acquisition, the ob-
jects in our taxonomy are WordNet synsets. In
this paper we focus on two of the many possible
relationships between senses: the hypernym rela-
tion and the coordinate term relation. We treat the
hypernym or ISA relation as atomic; we use the
notation Hnij if a sense j is the n-th ancestor of a
sense i in the hypernym hierarchy. We will sim-
ply use Hij to indicate that j is an ancestor of i
at some unspecified level. Two senses are typi-
cally considered to be ?coordinate terms? or ?tax-
onomic sisters? if they share an immediate parent
in the hypernym hierarchy. We generalize this no-
tion of siblinghood to state that two senses i and
j are (m,n)-cousins if their closest least common
1The taxonomies discussed in this paper are available for
download at http://ai.stanford.edu/?rion/swn.
subsumer (LCS)2 is within exactly m and n links,
respectively.3 We use the notation Cmnij to denote
that i and j are (m,n)-cousins. Thus coordinate
terms are (1, 1)-cousins; technically the hypernym
relation may also be seen as a specific case of this
representation; an immediate parent in the hyper-
nym hierarchy is a (1, 0)-cousin, and the k-th an-
cestor is a (k, 0)-cousin.
Taxonomic Constraints
A semantic taxonomy such as WordNet en-
forces certain taxonomic constraints which disal-
low particular taxonomies T. For example, the
ISA transitivity constraint in WordNet requires
that each synset inherits the hypernyms of its hy-
pernym, and the part-inheritance constraint re-
quires that each synset inherits the meronyms of
its hypernyms.
For the case of hyponym acquisition we enforce
the following two taxonomic constraints on the
hypernym and (m,n)-cousin relations:
1. ISA Transitivity:
Hmij ?Hnjk ? Hm+nik .
2. Definition of (m,n)-cousinhood:
Cmnij ? ?k.k = LCS(i, j) ?Hmik ?Hnjk.
Constraint (1) requires that the each synset inherits
the hypernyms of its direct hypernym; constraint
(2) simply defines the (m,n)-cousin relation in
terms of the atomic hypernym relation.
The addition of any new hypernym relation to a
preexisting taxonomy will usually necessitate the
addition of a set of other novel relations as implied
by the taxonomic constraints. We refer to the full
set of novel relations implied by a new link Rij as
I(Rij); we discuss the efficient computation of the
set of implied links for the purpose of hyponym
acquisition in Section 3.4.
2.2 A Probabilistic Formulation
We propose that the event Rij ? T has some
prior probability P (Rij ? T), and P (Rij ?
2A least common subsumer LCS(i, j) is defined as a
synset that is an ancestor in the hypernym hierarchy of both
i and j which has no child that is also an ancestor of both i
and j. When there is more than one LCS (due to multiple
inheritance), we refer to the closest LCS, i.e.,the LCS that
minimizes the maximum distance to i and j.
3An (m,n)-cousin for m ? 2 corresponds to the English
kinship relation ?(m?1)-th cousin |m?n|-times removed.?
802
T) + P (Rij 6? T) = 1. We define the probability
of the taxonomy as a whole as the joint probability
of its component relations; given a partition of all
possible relations R = {A,B} where A ? T and
B 6? T, we define:
P (T) = P (A ? T, B 6? T).
We assume that we have some set of observed evi-
dence E consisting of observed features over pairs
of objects in some domain DE; we?ll begin with
the assumption that our features are over pairs of
words, and that the objects in the taxonomy also
correspond directly to words.4 Given a set of fea-
tures ERij ? E, we assume we have some model
for inferring P (Rij ? T|ERij), i.e., the posterior
probability of the event Rij ? T given the corre-
sponding evidence ERij for that relation. For exam-
ple, evidence for the hypernym relation EHij might
be the set of all observed lexico-syntactic patterns
containing i and j in all sentences in some corpus.
For simplicity we make the following indepen-
dence assumptions: first, we assume that each
item of observed evidence ERij is independent of
all other observed evidence given the taxonomyT,
i.e., P (E|T) = ?ERij?E P (E
R
ij |T).
Further, we assume that each item of observed
evidence ERij depends on the taxonomy T only by
way of the corresponding relation Rij , i.e.,
P (ERij |T) =
{ P (ERij |Rij ? T) if Rij ? T
P (ERij |Rij 6? T) if Rij 6? T
For example, if our evidence EHij is a set of ob-
served lexico-syntactic patterns indicative of hy-
pernymy between two words i and j, we assume
that whatever dependence the relations in T have
on our observations may be explained entirely by
dependence on the existence or non-existence of
the single hypernym relation H(i, j).
Applying these two independence assumptions
we may express the conditional probability of our
evidence given the taxonomy:
P (E|T) =
?
Rij?T
P (ERij |Rij ? T)
?
?
Rij 6?T
P (ERij |Rij 6? T).
Rewriting the conditional probability in terms
of our estimates of the posterior probabilities
4In section 2.4 we drop this assumption, extending our
model to manage lexical ambiguity.
P (Rij |ERij) using Bayes Rule, we obtain:
P (E|T) =
?
Rij?T
P (Rij ? T|ERij)P (ERij)
P (Rij ? T)
?
?
Rij 6?T
P (Rij 6? T|ERij)P (ERij)
P (Rij 6? T) .
Within our model we define the goal of taxon-
omy induction to be to find the taxonomy T? that
maximizes the conditional probability of our ob-
servations E given the relationships of T, i.e., to
find
T? = argmax
T
P (E|T).
2.3 Local Search Over Taxonomies
We propose a search algorithm for finding T? for
the case of hyponym acquisition. We assume we
begin with some initial (possibly empty) taxon-
omy T. We restrict our consideration of possible
new taxonomies to those created by the single op-
eration ADD-RELATION(Rij ,T), which adds the
single relation Rij to T.
We define the multiplicative change ?T(Rij)
to the conditional probability P (E|T) given the
addition of a single relation Rij :
?T(Rij) = P (E|T?)/P (E|T)
= P (Rij ? T|E
R
ij)P (ERij)
P (Rij 6? T|ERij)P (ERij)
? P (Rij 6? T)P (Rij ? T)
= k
?
? P
(
Rij ? T|ERij
)
1? P
(
Rij ? T|ERij
)
?
? .
Here k is the inverse odds of the prior on the event
Rij ? T; we consider this to be a constant inde-
pendent of i, j, and the taxonomy T.
To enforce the taxonomic constraints in T, for
each application of the ADD-RELATION operator
we must add all new relations in the implied set
I(Rij) not already in T.5 Thus we define the mul-
tiplicative change of the full set of implied rela-
tions as the product over all new relations:
?T(I(Rij)) =
?
R?I(Rij)
?T(R).
5For example, in order to add the new synset
microsoft under the noun synset company#n#1
in WordNet 2.1, we must necessarily add the
new relations H2(microsoft, institution#n#1)
C11(microsoft, dotcom#n#1), and so on.
803
This definition leads to the following best-first
search algorithm for hyponym acquisition, which
at each iteration defines the new taxonomy as the
union of the previous taxonomy T and the set of
novel relations implied by the relation Rij that
maximizes ?T(I(Rij)) and thus maximizes the
conditional probability of the evidence over all
possible single relations:
WHILE max
Rij 6?T
?T(I(Rij)) > 1
T ? T ? I(arg max
Rij 6?T
?T(I(Rij))).
2.4 Extending the Model to Manage Lexical
Ambiguity
Since word senses are not directly observable, if
the objects in the taxonomy are word senses (as in
WordNet), we must extend our model to allow for
a many-to-many mapping (e.g., a word-to-sense
mapping) between DE and DT. For this setting
we assume we know the function senses(i), map-
ping from the word i to all of i?s possible corre-
sponding senses.
We assume that each set of word-pair evidence
ERij we possess is in fact sense-pair evidence ERkl
for a specific pair of senses k0 ? senses(i), l0 ?
senses(j). Further, we assume that a new relation
between two words is probable only between the
correct sense pair, i.e.:
P (Rkl|ERij) = 1{k = k0, l = l0} ? P (Rij |ERij).
When computing the conditional probability of a
specific new relation Rkl ? I(Rab), we assume
that the relevant sense pair k0, l0 is the one which
maximizes the probability of the new relation, i.e.
for k ? senses(i), l ? senses(j),
(k0, l0) = argmaxk,l P (Rkl ? T|E
R
ij).
Our independence assumptions for this exten-
sion need only to be changed slightly; we now as-
sume that the evidence ERij depends on the taxon-
omy T via only a single relation between sense-
pairs Rkl. Using this revised independence as-
sumption the derivation for best-first search over
taxonomies for hyponym acquisition remains un-
changed. One side effect of this revised indepen-
dence assumption is that the addition of the single
?sense-collapsed? relation Rkl in the taxonomy T
will explain the evidence ERij for the relation over
words i and j now that such evidence has been re-
vealed to concern only the specific senses k and l.
3 Extending WordNet
We demonstrate the ability of our model to use
evidence from multiple relations to extend Word-
Net with novel noun hyponyms. While in prin-
ciple we could use any number of relations, for
simplicity we consider two primary sources of ev-
idence: the probability of two words in WordNet
being in a hypernym relation, and the probability
of two words in WordNet being in a coordinate re-
lation.
In sections 3.1 and 3.2 we describe the construc-
tion of our hypernym and coordinate classifiers,
respectively; in section 3.3 we outline the efficient
algorithm we use to perform local search over
hyponym-extended WordNets; and in section 3.4
we give an example of the implicit structure-based
word sense disambiguation performed within our
framework.
3.1 Hyponym Classification
Our classifier for the hypernym relation is derived
from the ?hypernym-only? classifier described in
(Snow et al, 2005). The features used for pre-
dicting the hypernym relationship are obtained by
parsing a large corpus of newswire and encyclo-
pedia text with MINIPAR (Lin, 1998). From the
resulting dependency trees the evidence EHij for
each word pair (i, j) is constructed; the evidence
takes the form of a vector of counts of occurrences
that each labeled syntactic dependency path was
found as the shortest path connecting i and j in
some dependency tree. The labeled training set is
constructed by labeling the collected feature vec-
tors as positive ?known hypernym? or negative
?known non-hypernym? examples using WordNet
2.0; 49,922 feature vectors were labeled as pos-
itive training examples, and 800,828 noun pairs
were labeled as negative training examples. The
model for predicting P (Hij |EHij ) is then trained
using logistic regression, predicting the noun-pair
hypernymy label from WordNet from the feature
vector of lexico-syntactic patterns.
The hypernym classifier described above pre-
dicts the probability of the generalized hypernym-
ancestor relation over words P (Hij |EHij ). For
the purposes of taxonomy induction, we would
prefer an ancestor-distance specific set of clas-
sifiers over senses, i.e., for k ? senses(i), l ?
senses(j), the set of classifiers estimating
{P (H1kl|EHij ), P (H2kl|EHij ), . . . }.
804
One problem that arises from directly assign-
ing the probability P (Hnij |EHij ) ? P (Hij |EHij ) for
all n is the possibility of adding a novel hyponym
to an overly-specific hypernym, which might still
satisfy P (Hnij |EHij ) for a very large n. In or-
der to discourage unnecessary overspecification,
we penalize each probability P (Hkij |EHij ) by a
factor ?k?1 for some ? < 1, and renormalize:
P (Hkij |EHij ) ? ?k?1P (Hij |EHij ). In our experi-
ments we set ? = 0.95.
3.2 (m,n)-cousin Classification
The classifier for learning coordinate terms relies
on the notion of distributional similarity, i.e., the
idea that two words with similar meanings will be
used in similar contexts (Hindle, 1990). We ex-
tend this notion to suggest that words with similar
meanings should be near each other in a seman-
tic taxonomy, and in particular will likely share a
hypernym as a near parent.
Our classifier for (m,n)-cousins is derived
from the algorithm and corpus given in (Ravichan-
dran et al, 2005). In that work an efficient ran-
domized algorithm is derived for computing clus-
ters of similar nouns. We use a set of more than
1000 distinct clusters of English nouns collected
by their algorithm over 70 million webpages6,
with each noun i having a score representing its
cosine similarity to the centroid c of the cluster to
which it belongs, cos(?(i, c)).
We use the cluster scores of noun pairs as input
to our own algorithm for predicting the (m,n)-
cousin relationship between the senses of two
words i and j. If two words i and j appear in
a cluster together, with cluster centroid c, we set
our single coordinate input feature to be the mini-
mum cluster score min(cos(?(i, c)), cos(?(j, c))),
and zero otherwise. For each such noun pair fea-
ture, we construct a labeled training set of (m,n)-
cousin relation labels from WordNet 2.1. We de-
fine a noun pair (i, j) to be a ?known (m,n)-
cousin? if for some senses k ? senses(i), l ?
senses(j), Cmnij ? WordNet; if more than one
such relation exists, we assume the relation with
smallest sum m + n, breaking ties by smallest
absolute difference |m ? n|. We consider all
such labeled relationships from WordNet with 0 ?
m,n ? 7; pairs of words that have no correspond-
ing pair of synsets connected in the hypernym hi-
6As a preprocessing step we hand-edit the clusters to re-
move those containing non-English words, terms related to
adult content, and other webpage-specific clusters.
erarchy, or with min(m,n) > 7, are assigned to
a single class C?. Further, due to the symme-
try of the similarity score, we merge each class
Cmn = Cmn ? Cnm; this implies that the result-
ing classifier will predict, as expected given a sym-
metric input, P (Cmnkl |ECij ) = P (Cnmkl |ECij ).
We find 333,473 noun synset pairs in our train-
ing set with similarity score greater than 0.15. We
next apply softmax regression to learn a classifier
that predicts P (Cmnij |ECij ), predicting the Word-
Net class labels from the single similarity score
derived from the noun pair?s cluster similarity.
3.3 Details of our Implementation
Hyponym acquisition is among the simplest and
most straightforward of the possible applications
of our model; here we show how we efficiently
implement our algorithm for this problem. First,
we identify the set of all the word pairs (i, j) over
which we have hypernym and/or coordinate ev-
idence, and which might represent additions of
a novel hyponym to the WordNet 2.1 taxonomy
(i.e., that has a known noun hypernym and an un-
known hyponym, or has a known noun coordi-
nate term and an unknown coordinate term). This
yields a list of 95,000 single links over threshold
P (Rij) > 0.12.
For each unknown hyponym i we may have
several pieces of evidence; for example, for the
unknown term continental we have 21 relevant
pieces of hypernym evidence, with links to possi-
ble hypernyms {carrier, airline, unit, . . .}; and we
have 5 pieces of coordinate evidence, with links to
possible coordinate terms {airline, american ea-
gle, airbus, . . .}.
For each proposed hypernym or coordinate link
involved with the novel hyponym i, we compute
the set of candidate hypernyms for i; in practice
we consider all senses of the immediate hypernym
j for each potential novel hypernym, and all senses
of the coordinate term k and its first two hypernym
ancestors for each potential coordinate.
In the continental example, from the 26 individ-
ual pieces of evidence over words we construct the
set of 99 unique synsets that we will consider as
possible hypernyms; these include the two senses
of the word airline, the ten senses of the word car-
rier, and so forth.
Next, we iterate through each of the possi-
ble hypernym synsets l under which we might
add the new word i; for each synset l we com-
805
pute the change in taxonomy score resulting from
adding the implied relations I(H1il) required by
the taxonomic constraints of T. Since typically
our set of all evidence involving i will be much
smaller than the set of possible relations in I(H1il),
we may efficiently check whether, for each sense
s ? senses(w), for all words where we have
some evidence ERiw, whether s participates in
some relation with i in the set of implied rela-
tions I(H1il).7 If there is more than one sense
s ? senses(w), we add to I(H1il) the single re-
lationship Ris that maximizes the taxonomy like-
lihood, i.e. argmaxs?senses(w) ?T(Ris).
3.4 Hypernym Sense Disambiguation
A major strength of our model is its ability to cor-
rectly choose the sense of a hypernym to which
to add a novel hyponym, despite collecting ev-
idence over untagged word pairs. In our algo-
rithm word sense disambiguation is an implicit
side-effect of our algorithm; since our algorithm
chooses to add the single link which, with its im-
plied links, yields the most likely taxonomy, and
since each distinct synset in WordNet has a differ-
ent immediate neighborhood of relations, our al-
gorithm simply disambiguates each node based on
its surrounding structural information.
As an example of sense disambiguation in prac-
tice, consider our example of continental. Sup-
pose we are iterating through each of the 99 pos-
sible synsets under which we might add conti-
nental as a hyponym, and we come to the synset
airline#n#2 in WordNet 2.1, i.e. ?a commer-
cial organization serving as a common carrier.?
In this case we will iterate through each piece
of hypernym and coordinate evidence; we find
that the relation H(continental, carrier) is satis-
fied with high probability for the specific synset
carrier#n#5, the grandparent of airline#n#2; thus
the factor ?T(H3(continental, carrier#n#5)) is
included in the factor of the set of implied rela-
tions ?T
(I(H1(continental, airline#n#2))).
Suppose we instead evaluate the first synset
of airline, i.e., airline#n#1, with the gloss ?a
hose that carries air under pressure.? For this
synset none of the other 20 relationships di-
rectly implied by hypernym evidence or the
5 relationships implied by the coordinate ev-
7Checking whether or not Ris ? I(H1il) may be effi-
ciently computed by checking whether s is in the hypernym
ancestors of l or if it shares a least common subsumer with l
within 7 steps.
idence are implied by adding the single link
H1(continental,airline#n#1); thus the resulting
change in the set of implied links given by the cor-
rect ?carrier? sense of airline is much higher than
that of the ?hose? sense. In fact it is the largest of
all the 99 considered hypernym links for continen-
tal; H1(continental, airline#n#2) is link #18,736
added to the taxonomy by our algorithm.
4 Evaluation
In order to evaluate our framework for taxonomy
induction, we have applied hyponym acquisition
to construct several distinct taxonomies, starting
with the base of WordNet 2.1 and only adding
novel noun hyponyms. Further, we have con-
structed taxonomies using a baseline algorithm,
which uses the identical hypernym and coordinate
classifiers used in our joint algorithm, but which
does not combine the evidence of the classifiers.
In section 4.1 we describe our evaluation
methodology; in sections 4.2 and 4.3 we analyze
the fine-grained precision and disambiguation pre-
cision of our algorithm compared to the baseline;
in section 4.4 we compare the coarse-grained pre-
cision of our links (motivated by categories de-
fined by the WordNet supersenses) against the
baseline algorithm and against an ?oracle? for
named entity recognition.
Finally, in section 4.5 we evaluate the tax-
onomies inferred by our algorithm directly against
the WordNet 2.1 taxonomy; we perform this eval-
uation by testing each taxonomy on a set of human
judgments of hypernym and non-hypernym noun
pairs sampled from newswire text.
4.1 Methodology
We evaluate the quality of our acquired hy-
ponyms by direct judgment. In four sep-
arate annotation sessions, two judges labeled
{50,100,100,100} samples uniformly generated
from the first {100,1000,10000,20000} single
links added by our algorithm.
For the direct measure of fine-grained precision,
we simply ask for each link H(X,Y ) added by the
system, is X a Y ? In addition to the fine-grained
precision, we give a coarse-grained evaluation, in-
spired by the idea of supersense-tagging in (Cia-
ramita and Johnson, 2003). The 26 supersenses
used in WordNet 2.1 are listed in Table 1; we label
a hyponym link as correct in the coarse-grained
evaluation if the novel hyponym is placed under
the appropriate supersense. This evaluation task
806
1 Tops 8 communication 15 object 22 relation
2 act 9 event 16 person 23 shape
3 animal 10 feeling 17 phenomenon 24 state
4 artifact 11 food 18 plant 25 substance
5 attribute 12 group 19 possession 26 time
6 body 13 location 20 process
7 cognition 14 motive 21 quantity
Table 1: The 26 WordNet supersenses
is similar to a fine-grained Named Entity Recog-
nition (Fleischman and Hovy, 2002) task with 26
categories; for example, if our algorithm mistak-
enly inserts a novel non-capital city under the hy-
ponym state capital, it will inherit the correct su-
persense location. Finally, we evaluate the abil-
ity of our algorithm to correctly choose the ap-
propriate sense of the hypernym under which a
novel hyponym is being added. Our labelers cate-
gorize each candidate sense-disambiguated hyper-
nym synset suggested by our algorithm into the
following categories:
c1: Correct sense-disambiguated hypernym.
c2: Correct hypernym word, but incorrect sense of
that word.
c3: Incorrect hypernym, but correct supersense.
c4: Any other relation is considered incorrect.
A single hyponym/hypernym pair is allowed to be
simultaneously labeled 2 and 3.
4.2 Fine-grained evaluation
Table 2 displays the results of our evaluation of
fine-grained precision for the baseline non-joint
algorithm (Base) and our joint algorithm (Joint),
as well as the relative error reduction (ER) of our
algorithm over the baseline. We use the mini-
mum of the two judges? scores. Here we define
fine-grained precision as c1/total. We see that
our joint algorithm strongly outperforms the base-
line, and has high precision for predicting novel
hyponyms up to 10,000 links.
4.3 Hypernym sense disambiguation
Also in Table 2 we compare the sense dis-
ambiguation precision of our algorithm and the
baseline. Here we measure the precision of
sense-disambiguation among all examples where
each algorithm found a correct hyponym word;
our calculation for disambiguation precision is
c1/ (c1 + c2). Again our joint algorithm outper-
forms the baseline algorithm at all levels of re-
call. Interestingly the baseline disambiguation
precision improves with higher recall; this may
Fine-grained Pre. Disambiguation Pre.
#Links Base Joint ER Base Joint ER
100 0.60 1.00 100% 0.86 1.00 100%
1000 0.52 0.93 85% 0.84 1.00 100%
10000 0.46 0.84 70% 0.90 1.00 100%
20000 0.46 0.68 41% 0.94 0.98 68%
Table 2: Fine-grained and disambiguation preci-
sion and error reduction for hyponym acquisition
# Links NER Base Joint ER vs. ER vs.
Oracle NER Base
100 1.00 0.72 1.00 0% 100%
1000 0.69 0.68 0.99 97% 85%
10000 0.45 0.69 0.96 93% 70%
20000 0.54 0.69 0.92 83% 41%
Table 3: Coarse-grained precision and error reduc-
tion vs. Non-joint baseline and NER Oracle
be attributed to the observation that the highest-
confidence hypernyms predicted by individual
classifiers are likely to be polysemous, whereas
hypernyms of lower confidence are more fre-
quently monosemous (and thus trivially easy to
disambiguate).
4.4 Coarse-grained evaluation
We compute coarse-grained precision as (c1 +
c3)/total. Inferring the correct coarse-grained su-
persense of a novel hyponym can be viewed as a
fine-grained (26-category) Named Entity Recog-
nition task; our algorithm for taxonomy induction
can thus be viewed as performing high-accuracy
fine-grained NER. Here we compare against both
the baseline non-joint algorithm as well as an
?oracle? algorithm for Named Entity Recogni-
tion, which perfectly classifies the supersense of
all nouns that fall under the four supersenses
{person, group, location, quantity}, but works
only for those supersenses. Table 3 shows the
results of this coarse-grained evaluation. We see
that the baseline non-joint algorithm has higher
precision than the NER oracle as 10,000 and
20,000 links; however, both are significantly out-
performed by our joint algorithm, which main-
tains high coarse-grained precision (92%) even at
20,000 links.
4.5 Comparison of inferred taxonomies and
WordNet
For our final evaluation we compare our learned
taxonomies directly against the currently exist-
ing hypernym links in WordNet 2.1. In order to
compare taxonomies we use a hand-labeled test
807
WN +10K +20K +30K +40K
PRE 0.524 0.524 0.574 0.583 0.571
REC 0.165 0.165 0.203 0.211 0.211
F 0.251 0.251 0.300 0.309 0.307
Table 4: Taxonomy hypernym classification vs.
WordNet 2.1 on hand-labeled testset
set of over 5,000 noun pairs, randomly-sampled
from newswire corpora (described in (Snow et al,
2005)). We measured the performance of both our
inferred taxonomies and WordNet against this test
set.8 The performance and comparison of the best
WordNet classifier vs. our taxonomies is given in
Table 4. Our best-performing inferred taxonomy
on this test set is achieved after adding 30,000
novel hyponyms, achieving an 23% relative im-
provement in F-score over the WN2.1 classifier.
5 Conclusions
We have presented an algorithm for inducing se-
mantic taxonomies which attempts to globally
optimize the entire structure of the taxonomy.
Our probabilistic architecture also includes a new
model for learning coordinate terms based on
(m,n)-cousin classification. The model?s ability
to integrate heterogeneous evidence from different
classifiers offers a solution to the key problem of
choosing the correct word sense to which to attach
a new hypernym.
Acknowledgements
Thanks to Christiane Fellbaum, Rajat Raina, Bill
MacCartney, and Allison Buckley for useful dis-
cussions and assistance annotating data. Rion
Snow is supported by an NDSEG Fellowship
sponsored by the DOD and AFOSR. This work
was supported in part by the Disruptive Technol-
ogy Office (DTO)?s Advanced Question Answer-
ing for Intelligence (AQUAINT) Program.
References
P. Buitelaar, P. Cimiano and B. Magnini. 2005. Ontol-
ogy Learning from Text: Methods, Evaluation and
Applications. Volume 123 Frontiers in Artificial In-
telligence and Applications.
S. Caraballo. 2001. Automatic Acquisition of
a Hypernym-Labeled Noun Hierarchy from Text.
Brown University Ph.D. Thesis.
8We found that the WordNet 2.1 model achieving the
highest F-score used only the first sense of each hyponym,
and allowed a maximum distance of 4 edges between each
hyponym and its hypernym.
S. Cederberg and D. Widdows. 2003. Using LSA and
Noun Coordination Information to Improve the Pre-
cision and Recall of Automatic Hyponymy Extrac-
tion. Proc. CoNLL-2003, pp. 111?118.
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining
the Web for Fine-Grained Semantic Verb Relations.
Proc. EMNLP-2004.
M. Ciaramita and M. Johnson. 2003. Supersense
Tagging of Unknown Nouns in WordNet. Proc.
EMNLP-2003.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised Named-Entity Extraction from
the Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA: MIT Press.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning Semantic Constraints for the Automatic
Discovery of Part-Whole Relations. Proc. HLT-03.
M. Fleischman and E. Hovy. 2002. Fine grained clas-
sification of named entities. Proc. COLING-02.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. Proc. COLING-92.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. ACL-90.
D. Lenat. 1995. CYC: A Large-Scale Investment in
Knowledge Infrastructure, Communications of the
ACM, 38:11, 33?35.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain.
D. Lin, S. Zhao, L. Qin and M. Zhou. 2003. Iden-
tifying Synonyms among Distributionally Similar
Words. Proc. IJCAI-03.
M. Pasc?a. 2005. Finding Instance Names and Alter-
native Glosses on the Web: WordNet Reloaded. CI-
CLing 2005, pp. 280-292.
D. Ravichandran, P. Pantel, and E. Hovy. 2002. Ran-
domized Algorithms and NLP: Using Locality Sen-
sitive Hash Function for High Speed Noun Cluster-
ing. Proc. ACL-2002.
E. Riloff and J. Shepherd. 1997. A Corpus-Based
Approach for Building Semantic Lexicons. Proc
EMNLP-1997.
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurerence statistics for semi-automatic-semantic
lexicon construction. Proc. ACL-1998.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym dis-
covery. NIPS 2005.
P. Turney, M. Littman, J. Bigham, and V. Shnay-
der. 2003. Combining independent modules to
solve multiple-choice synonym and analogy prob-
lems. Proc. RANLP-2003, pp. 482?489.
808
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1003?1011,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Distant supervision for relation extraction without labeled data
Mike Mintz, Steven Bills, Rion Snow, Dan Jurafsky
Stanford University / Stanford, CA 94305
{mikemintz,sbills,rion,jurafsky}@cs.stanford.edu
Abstract
Modern models of relation extraction for tasks like
ACE are based on supervised learning of relations
from small hand-labeled corpora. We investigate an
alternative paradigm that does not require labeled
corpora, avoiding the domain dependence of ACE-
style algorithms, and allowing the use of corpora
of any size. Our experiments use Freebase, a large
semantic database of several thousand relations, to
provide distant supervision. For each pair of enti-
ties that appears in some Freebase relation, we find
all sentences containing those entities in a large un-
labeled corpus and extract textual features to train
a relation classifier. Our algorithm combines the
advantages of supervised IE (combining 400,000
noisy pattern features in a probabilistic classifier)
and unsupervised IE (extracting large numbers of
relations from large corpora of any domain). Our
model is able to extract 10,000 instances of 102 re-
lations at a precision of 67.6%. We also analyze
feature performance, showing that syntactic parse
features are particularly helpful for relations that are
ambiguous or lexically distant in their expression.
1 Introduction
At least three learning paradigms have been ap-
plied to the task of extracting relational facts from
text (for example, learning that a person is em-
ployed by a particular organization, or that a ge-
ographic entity is located in a particular region).
In supervised approaches, sentences in a cor-
pus are first hand-labeled for the presence of en-
tities and the relations between them. The NIST
Automatic Content Extraction (ACE) RDC 2003
and 2004 corpora, for example, include over 1,000
documents in which pairs of entities have been la-
beled with 5 to 7 major relation types and 23 to
24 subrelations, totaling 16,771 relation instances.
ACE systems then extract a wide variety of lexi-
cal, syntactic, and semantic features, and use su-
pervised classifiers to label the relation mention
holding between a given pair of entities in a test
set sentence, optionally combining relation men-
tions (Zhou et al, 2005; Zhou et al, 2007; Sur-
deanu and Ciaramita, 2007).
Supervised relation extraction suffers from a
number of problems, however. Labeled training
data is expensive to produce and thus limited in
quantity. Also, because the relations are labeled
on a particular corpus, the resulting classifiers tend
to be biased toward that text domain.
An alternative approach, purely unsupervised
information extraction, extracts strings of words
between entities in large amounts of text, and
clusters and simplifies these word strings to pro-
duce relation-strings (Shinyama and Sekine, 2006;
Banko et al, 2007). Unsupervised approaches can
use very large amounts of data and extract very
large numbers of relations, but the resulting rela-
tions may not be easy to map to relations needed
for a particular knowledge base.
A third approach has been to use a very small
number of seed instances or patterns to do boot-
strap learning (Brin, 1998; Riloff and Jones, 1999;
Agichtein and Gravano, 2000; Ravichandran and
Hovy, 2002; Etzioni et al, 2005; Pennacchiotti
and Pantel, 2006; Bunescu and Mooney, 2007;
Rozenfeld and Feldman, 2008). These seeds are
used with a large corpus to extract a new set of
patterns, which are used to extract more instances,
which are used to extract more patterns, in an it-
erative fashion. The resulting patterns often suffer
from low precision and semantic drift.
We propose an alternative paradigm, distant su-
pervision, that combines some of the advantages
of each of these approaches. Distant supervision
is an extension of the paradigm used by Snow et
al. (2005) for exploiting WordNet to extract hyper-
nym (is-a) relations between entities, and is simi-
lar to the use of weakly labeled data in bioinfor-
matics (Craven and Kumlien, 1999; Morgan et al,
1003
Relation name New instance
/location/location/contains Paris, Montmartre
/location/location/contains Ontario, Fort Erie
/music/artist/origin Mighty Wagon, Cincinnati
/people/deceased person/place of death Fyodor Kamensky, Clearwater
/people/person/nationality Marianne Yvonne Heemskerk, Netherlands
/people/person/place of birth Wavell Wayne Hinds, Kingston
/book/author/works written Upton Sinclair, Lanny Budd
/business/company/founders WWE, Vince McMahon
/people/person/profession Thomas Mellon, judge
Table 1: Ten relation instances extracted by our system that did not appear in Freebase.
2004). Our algorithm uses Freebase (Bollacker et
al., 2008), a large semantic database, to provide
distant supervision for relation extraction. Free-
base contains 116 million instances of 7,300 rela-
tions between 9 million entities. The intuition of
distant supervision is that any sentence that con-
tains a pair of entities that participate in a known
Freebase relation is likely to express that relation
in some way. Since there may be many sentences
containing a given entity pair, we can extract very
large numbers of (potentially noisy) features that
are combined in a logistic regression classifier.
Thus whereas the supervised training paradigm
uses a small labeled corpus of only 17,000 rela-
tion instances as training data, our algorithm can
use much larger amounts of data: more text, more
relations, and more instances. We use 1.2 million
Wikipedia articles and 1.8 million instances of 102
relations connecting 940,000 entities. In addition,
combining vast numbers of features in a large clas-
sifier helps obviate problems with bad features.
Because our algorithm is supervised by a
database, rather than by labeled text, it does
not suffer from the problems of overfitting and
domain-dependence that plague supervised sys-
tems. Supervision by a database also means that,
unlike in unsupervised approaches, the output of
our classifier uses canonical names for relations.
Our paradigm offers a natural way of integrating
data from multiple sentences to decide if a relation
holds between two entities. Because our algorithm
can use large amounts of unlabeled data, a pair of
entities may occur multiple times in the test set.
For each pair of entities, we aggregate the features
from the many different sentences in which that
pair appeared into a single feature vector, allowing
us to provide our classifier with more information,
resulting in more accurate labels.
Table 1 shows examples of relation instances
extracted by our system. We also use this system
to investigate the value of syntactic versus lexi-
cal (word sequence) features in relation extraction.
While syntactic features are known to improve the
performance of supervised IE, at least using clean
hand-labeled ACE data (Zhou et al, 2007; Zhou
et al, 2005), we do not know whether syntactic
features can improve the performance of unsuper-
vised or distantly supervised IE. Most previous
research in bootstrapping or unsupervised IE has
used only simple lexical features, thereby avoid-
ing the computational expense of parsing (Brin,
1998; Agichtein and Gravano, 2000; Etzioni et al,
2005), and the few systems that have used unsu-
pervised IE have not compared the performance
of these two types of feature.
2 Previous work
Except for the unsupervised algorithms discussed
above, previous supervised or bootstrapping ap-
proaches to relation extraction have typically re-
lied on relatively small datasets, or on only a small
number of distinct relations. Approaches based on
WordNet have often only looked at the hypernym
(is-a) or meronym (part-of) relation (Girju et al,
2003; Snow et al, 2005), while those based on the
ACE program (Doddington et al, 2004) have been
restricted in their evaluation to a small number of
relation instances and corpora of less than a mil-
lion words.
Many early algorithms for relation extraction
used little or no syntactic information. For ex-
ample, the DIPRE algorithm by Brin (1998) used
string-based regular expressions in order to rec-
ognize relations such as author-book, while the
SNOWBALL algorithm by Agichtein and Gravano
(2000) learned similar regular expression patterns
over words and named entity tags. Hearst (1992)
used a small number of regular expressions over
words and part-of-speech tags to find examples of
the hypernym relation. The use of these patterns
has been widely replicated in successful systems,
for example by Etzioni et al (2005). Other work
1004
Relation name Size Example
/people/person/nationality 281,107 John Dugard, South Africa
/location/location/contains 253,223 Belgium, Nijlen
/people/person/profession 208,888 Dusa McDuff, Mathematician
/people/person/place of birth 105,799 Edwin Hubble, Marshfield
/dining/restaurant/cuisine 86,213 MacAyo?s Mexican Kitchen, Mexican
/business/business chain/location 66,529 Apple Inc., Apple Inc., South Park, NC
/biology/organism classification rank 42,806 Scorpaeniformes, Order
/film/film/genre 40,658 Where the Sidewalk Ends, Film noir
/film/film/language 31,103 Enter the Phoenix, Cantonese
/biology/organism higher classification 30,052 Calopteryx, Calopterygidae
/film/film/country 27,217 Turtle Diary, United States
/film/writer/film 23,856 Irving Shulman, Rebel Without a Cause
/film/director/film 23,539 Michael Mann, Collateral
/film/producer/film 22,079 Diane Eskenazi, Aladdin
/people/deceased person/place of death 18,814 John W. Kern, Asheville
/music/artist/origin 18,619 The Octopus Project, Austin
/people/person/religion 17,582 Joseph Chartrand, Catholicism
/book/author/works written 17,278 Paul Auster, Travels in the Scriptorium
/soccer/football position/players 17,244 Midfielder, Chen Tao
/people/deceased person/cause of death 16,709 Richard Daintree, Tuberculosis
/book/book/genre 16,431 Pony Soldiers, Science fiction
/film/film/music 14,070 Stavisky, Stephen Sondheim
/business/company/industry 13,805 ATS Medical, Health care
Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation.
such as Ravichandran and Hovy (2002) and Pan-
tel and Pennacchiotti (2006) use the same formal-
ism of learning regular expressions over words and
part-of-speech tags to discover patterns indicating
a variety of relations.
More recent approaches have used deeper syn-
tactic information derived from parses of the input
sentences, including work exploiting syntactic de-
pendencies by Lin and Pantel (2001) and Snow et
al. (2005), and work in the ACE paradigm such
as Zhou et al (2005) and Zhou et al (2007).
Perhaps most similar to our distant supervision
algorithm is the effective method of Wu and Weld
(2007) who extract relations from a Wikipedia
page by using supervision from the page?s infobox.
Unlike their corpus-specific method, which is spe-
cific to a (single) Wikipedia page, our algorithm
allows us to extract evidence for a relation from
many different documents, and from any genre.
3 Freebase
Following the literature, we use the term ?rela-
tion? to refer to an ordered, binary relation be-
tween entities. We refer to individual ordered pairs
in this relation as ?relation instances?. For ex-
ample, the person-nationality relation holds be-
tween the entities named ?John Steinbeck? and
?United States?, so it has ?John Steinbeck,
United States? as an instance.
We use relations and relation instances from
Freebase, a freely available online database of
structured semantic data. Data in Freebase is
collected from a variety of sources. One major
source is text boxes and other tabular data from
Wikipedia. Data is also taken from NNDB (bio-
graphical information), MusicBrainz (music), the
SEC (financial and corporate data), as well as di-
rect, wiki-style user editing. After some basic
processing of the July 2008 link export to con-
vert Freebase?s data representation into binary re-
lations, we have 116 million instances of 7,300
relations between 9 million entities. We next fil-
ter out nameless and uninteresting entities such as
user profiles and music tracks. Freebase also con-
tains the reverses of many of its relations (book-
author v. author-book), and these are merged. Fil-
tering and removing all but the largest relations
leaves us with 1.8 million instances of 102 rela-
tions connecting 940,000 entities. Examples are
shown in Table 2.
4 Architecture
The intuition of our distant supervision approach
is to use Freebase to give us a training set of rela-
tions and entity pairs that participate in those rela-
tions. In the training step, all entities are identified
1005
in sentences using a named entity tagger that la-
bels persons, organizations and locations. If a sen-
tence contains two entities and those entities are an
instance of one of our Freebase relations, features
are extracted from that sentence and are added to
the feature vector for the relation.
The distant supervision assumption is that if two
entities participate in a relation, any sentence that
contain those two entities might express that rela-
tion. Because any individual sentence may give
an incorrect cue, our algorithm trains a multiclass
logistic regression classifier, learning weights for
each noisy feature. In training, the features for
identical tuples (relation, entity1, entity2) from
different sentences are combined, creating a richer
feature vector.
In the testing step, entities are again identified
using the named entity tagger. This time, every
pair of entities appearing together in a sentence is
considered a potential relation instance, and when-
ever those entities appear together, features are ex-
tracted on the sentence and added to a feature vec-
tor for that entity pair. For example, if a pair of
entities occurs in 10 sentences in the test set, and
each sentence has 3 features extracted from it, the
entity pair will have 30 associated features. Each
entity pair in each sentence in the test corpus is run
through feature extraction, and the regression clas-
sifier predicts a relation name for each entity pair
based on the features from all of the sentences in
which it appeared.
Consider the location-contains relation, imag-
ining that in Freebase we had two instances of
this relation: ?Virginia, Richmond? and
?France, Nantes?. As we encountered sen-
tences like ?Richmond, the capital of Virginia? and
?Henry?s Edict of Nantes helped the Protestants of
France? we would extract features from these sen-
tences. Some features would be very useful, such
as the features from the Richmond sentence, and
some would be less useful, like those from the
Nantes sentence. In testing, if we came across
a sentence like ?Vienna, the capital of Austria?,
one or more of its features would match those of
the Richmond sentence, providing evidence that
?Austria, Vienna? belongs to the location-
contains relation.
Note that one of the main advantages of our
architecture is its ability to combine informa-
tion from many different mentions of the same
relation. Consider the entity pair ?Steven
Spielberg, Saving Private Ryan?
from the following two sentences, as evidence for
the film-director relation.
[Steven Spielberg]?s film [Saving Private
Ryan] is loosely based on the brothers? story.
Allison co-produced the Academy Award-
winning [Saving Private Ryan], directed by
[Steven Spielberg]...
The first sentence, while providing evidence for
film-director, could instead be evidence for film-
writer or film-producer. The second sentence does
not mention that Saving Private Ryan is a film, and
so could instead be evidence for the CEO relation
(consider ?Robert Mueller directed the FBI?). In
isolation, neither of these features is conclusive,
but in combination, they are.
5 Features
Our features are based on standard lexical and syn-
tactic features from the literature. Each feature
describes how two entities are related in a sen-
tence, using either syntactic or non-syntactic in-
formation.
5.1 Lexical features
Our lexical features describe specific words be-
tween and surrounding the two entities in the sen-
tence in which they appear:
? The sequence of words between the two entities
? The part-of-speech tags of these words
? A flag indicating which entity came first in the sentence
? A window of k words to the left of Entity 1 and their
part-of-speech tags
? A window of k words to the right of Entity 2 and their
part-of-speech tags
Each lexical feature consists of the conjunction of
all these components. We generate a conjunctive
feature for each k ? {0, 1, 2}. Thus each lexical
row in Table 3 represents a single lexical feature.
Part-of-speech tags were assigned by a max-
imum entropy tagger trained on the Penn Tree-
bank, and then simplified into seven categories:
nouns, verbs, adverbs, adjectives, numbers, for-
eign words, and everything else.
In an attempt to approximate syntactic features,
we also tested variations on our lexical features:
(1) omitting all words that are not verbs and (2)
omitting all function words. In combination with
the other lexical features, they gave a small boost
to precision, but not large enough to justify the in-
creased demand on our computational resources.
1006
Feature type Left window NE1 Middle NE2 Right window
Lexical [] PER [was/VERB born/VERB in/CLOSED] LOC []
Lexical [Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [,]
Lexical [#PAD#, Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [, Missouri]
Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []
Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []
Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []
Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]
Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]
Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]
Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]
Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]
Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]
Table 3: Features for ?Astronomer Edwin Hubble was born in Marshfield, Missouri?.
Astronomer Edwin Hubble was born in Marshfield , Missouri
lex-mod s pred mod pcomp-n lex-mod
inside
Figure 1: Dependency parse with dependency path from ?Edwin Hubble? to ?Marshfield? highlighted in
boldface.
5.2 Syntactic features
In addition to lexical features we extract a num-
ber of features based on syntax. In order to gener-
ate these features we parse each sentence with the
broad-coverage dependency parser MINIPAR (Lin,
1998).
A dependency parse consists of a set of words
and chunks (e.g. ?Edwin Hubble?, ?Missouri?,
?born?), linked by directional dependencies (e.g.
?pred?, ?lex-mod?), as in Figure 1. For each
sentence we extract a dependency path between
each pair of entities. A dependency path con-
sists of a series of dependencies, directions and
words/chunks representing a traversal of the parse.
Part-of-speech tags are not included in the depen-
dency path.
Our syntactic features are similar to those used
in Snow et al (2005). They consist of the conjunc-
tion of:
? A dependency path between the two entities
? For each entity, one ?window? node that is not part of
the dependency path
A window node is a node connected to one of the
two entities and not part of the dependency path.
We generate one conjunctive feature for each pair
of left and right window nodes, as well as features
which omit one or both of them. Thus each syn-
tactic row in Table 3 represents a single syntactic
feature.
5.3 Named entity tag features
Every feature contains, in addition to the content
described above, named entity tags for the two en-
tities. We perform named entity tagging using the
Stanford four-class named entity tagger (Finkel et
al., 2005). The tagger provides each word with a
label from {person, location, organization, miscel-
laneous, none}.
5.4 Feature conjunction
Rather than use each of the above features in the
classifier independently, we use only conjunctive
features. Each feature consists of the conjunc-
tion of several attributes of the sentence, plus the
named entity tags. For two features to match,
all of their conjuncts must match exactly. This
yields low-recall but high-precision features. With
a small amount of data, this approach would be
problematic, since most features would only be
seen once, rendering them useless to the classifier.
Since we use large amounts of data, even complex
features appear multiple times, allowing our high-
precision features to work as intended. Features
for a sample sentence are shown in Table 3.
6 Implementation
6.1 Text
For unstructured text we use the Freebase
Wikipedia Extraction, a dump of the full text of all
Wikipedia articles (not including discussion and
1007
Relation Feature type Left window NE1 Middle NE2 Right window
/architecture/structure/architect LEXx ORG , the designer of the PER
SYN designed ?s ORG ?s designed ?by?subj by ?pcn PER ?s designed
/book/author/works written LEX PER s novel ORG
SYN PER ?pcn by ?mod story ?pred is ?s ORG
/book/book edition/author editor LEXx ORG s novel PER
SYN PER ?nn series ?gen PER
/business/company/founders LEX ORG co - founder PER
SYN ORG ?nn owner ?person PER
/business/company/place founded LEXx ORG - based LOC
SYN ORG ?s founded ?mod in ?pcn LOC
/film/film/country LEX PER , released in LOC
SYN opened ?s ORG ?s opened ?mod in ?pcn LOC ?s opened
/geography/river/mouth LEX LOC , which flows into the LOC
SYN the ?det LOC ?s is ?pred tributary ?mod of ?pcn LOC ?det the
/government/political party/country LEXx ORG politician of the LOC
SYN candidate ?nn ORG ?nn candidate ?mod for ?pcn LOC ?nn candidate
/influence/influence node/influenced LEXx PER , a student of PER
SYN of ?pcn PER ?pcn of ?mod student ?appo PER ?pcn of
/language/human language/region LEX LOC - speaking areas of LOC
SYN LOC ?lex?mod speaking areas ?mod of ?pcn LOC
/music/artist/origin LEXx ORG based band LOC
SYN is ?s ORG ?s is ?pred band ?mod from ?pcn LOC ?s is
/people/deceased person/place of death LEX PER died in LOC
SYN hanged ?s PER ?s hanged ?mod in ?pcn LOC ?s hanged
/people/person/nationality LEX PER is a citizen of LOC
SYN PER ?mod from ?pcn LOC
/people/person/parents LEX PER , son of PER
SYN father ?gen PER ?gen father ?person PER ?gen father
/people/person/place of birth LEXx PER is the birthplace of PER
SYN PER ?s born ?mod in ?pcn LOC
/people/person/religion LEX PER embraced LOC
SYN convert ?appo PER ?appo convert ?mod to ?pcn LOC ?appo convert
Table 4: Examples of high-weight features for several relations. Key: SYN = syntactic feature; LEX =
lexical feature;x = reversed; NE# = named entity tag of entity.
user pages) which has been sentence-tokenized by
Metaweb Technologies, the developers of Free-
base (Metaweb, 2008). This dump consists of
approximately 1.8 million articles, with an av-
erage of 14.3 sentences per article. The total
number of words (counting punctuation marks) is
601,600,703. For our experiments we use about
half of the articles: 800,000 for training and
400,000 for testing.
We use Wikipedia because it is relatively up-
to-date, and because its sentences tend to make
explicit many facts that might be omitted in
newswire. Much of the information in Freebase is
derived from tabular data from Wikipedia, mean-
ing that Freebase relations are more likely to ap-
pear in sentences in Wikipedia.
6.2 Parsing and chunking
Each sentence of this unstructured text is depen-
dency parsed by MINIPAR to produce a depen-
dency graph.
In preprocessing, consecutive words with the
same named entity tag are ?chunked?, so that
Edwin/PERSON Hubble/PERSON becomes
[Edwin Hubble]/PERSON. This chunking is
restricted by the dependency parse of the sentence,
however, in that chunks must be contiguous in
the parse (i.e., no chunks across subtrees). This
ensures that parse tree structure is preserved, since
the parses must be updated to reflect the chunking.
6.3 Training and testing
For held-out evaluation experiments (see section
7.1), half of the instances of each relation are not
used in training, and are later used to compare
against newly discovered instances. This means
that 900,000 Freebase relation instances are used
in training, and 900,000 are held out. These ex-
periments used 800,000 Wikipedia articles in the
training phase and 400,000 different articles in the
testing phase.
For human evaluation experiments, all 1.8 mil-
lion relation instances are used in training. Again,
we use 800,000 Wikipedia articles in the training
phase and 400,000 different articles in the testing
phase.
For all our experiments, we only extract relation
instances that do not appear in our training data,
i.e., instances that are not already in Freebase.
Our system needs negative training data for the
purposes of constructing the classifier. Towards
this end, we build a feature vector in the train-
ing phase for an ?unrelated? relation by randomly
selecting entity pairs that do not appear in any
Freebase relation and extracting features for them.
While it is possible that some of these entity pairs
1008
0?
0.1?
0.2?
0.3?
0.4?
0.5?
0.6?
0.7?
0.8?
0.9?
1?
0? 0.05? 0.1? 0.15? 0.2? 0.25? 0.3? 0.35? 0.4? 0.45?
Pr
ec
isi
on
?
Oracle?recall?
Both?
Syntax?
Surface?
Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training
on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic
features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify
most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and
10% as person-nationality.
are in fact related but are wrongly omitted from
the Freebase data, we expect that on average these
false negatives will have a small effect on the per-
formance of the classifier. For performance rea-
sons, we randomly sample 1% of such entity pairs
for use as negative training examples. By contrast,
in the actual test data, 98.7% of the entity pairs we
extract do not possess any of the top 102 relations
we consider in Freebase.
We use a multi-class logistic classifier opti-
mized using L-BFGS with Gaussian regulariza-
tion. Our classifier takes as input an entity pair
and a feature vector, and returns a relation name
and a confidence score based on the probability of
the entity pair belonging to that relation. Once all
of the entity pairs discovered during testing have
been classified, they can be ranked by confidence
score and used to generate a list of the n most
likely new relation instances.
Table 4 shows some high-weight features
learned by our system. We discuss the results in
the next section.
7 Evaluation
We evaluate labels in two ways: automatically,
by holding out part of the Freebase relation data
during training, and comparing newly discovered
relation instances against this held-out data, and
manually, having humans who look at each posi-
tively labeled entity pair and mark whether the re-
lation indeed holds between the participants. Both
evaluations allow us to calculate the precision of
the system for the best N instances.
7.1 Held-out evaluation
Figure 2 shows the performance of our classifier
on held-out Freebase relation data. While held-out
evaluation suffers from false negatives, it gives a
rough measure of precision without requiring ex-
pensive human evaluation, making it useful for pa-
rameter setting.
At most recall levels, the combination of syn-
tactic and lexical features offers a substantial im-
provement in precision over either of these feature
sets on its own.
7.2 Human evaluation
Human evaluation was performed by evaluators on
Amazon?s Mechanical Turk service, shown to be
effective for natural language annotation in Snow
et al (2008). We ran three experiments: one us-
ing only syntactic features; one using only lexical
features; and one using both syntactic and lexical
features. For each of the 10 relations that appeared
most frequently in our test data (according to our
classifier), we took samples from the first 100 and
1000 instances of this relation generated in each
experiment, and sent these to Mechanical Turk for
1009
Relation name
100 instances 1000 instances
Syn Lex Both Syn Lex Both
/film/director/film 0.49 0.43 0.44 0.49 0.41 0.46
/film/writer/film 0.70 0.60 0.65 0.71 0.61 0.69
/geography/river/basin countries 0.65 0.64 0.67 0.73 0.71 0.64
/location/country/administrative divisions 0.68 0.59 0.70 0.72 0.68 0.72
/location/location/contains 0.81 0.89 0.84 0.85 0.83 0.84
/location/us county/county seat 0.51 0.51 0.53 0.47 0.57 0.42
/music/artist/origin 0.64 0.66 0.71 0.61 0.63 0.60
/people/deceased person/place of death 0.80 0.79 0.81 0.80 0.81 0.78
/people/person/nationality 0.61 0.70 0.72 0.56 0.61 0.63
/people/person/place of birth 0.78 0.77 0.78 0.88 0.85 0.91
Average 0.67 0.66 0.69 0.68 0.67 0.67
Table 5: Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000
results per relation, using stratified samples. ?Average? gives the mean precision of the 10 relations. Key:
Syn = syntactic features only. Lex = lexical features only. We use stratified samples because of the
overabundance of location-contains instances among our high-confidence results.
human evaluation. Our sample size was 100.
Each predicted relation instance was labeled as
true or false by between 1 and 3 labelers on Me-
chanical Turk. We assigned the truth or falsehood
of each relation according to the majority vote of
the labels; in the case of a tie (one vote each way)
we assigned the relation as true or false with equal
probability. The evaluation of the syntactic, lexi-
cal, and combination of features at a recall of 100
and 1000 instances is presented in Table 5.
At a recall of 100 instances, the combination of
lexical and syntactic features has the best perfor-
mance for a majority of the relations, while at a re-
call level of 1000 instances the results are mixed.
No feature set strongly outperforms any of the oth-
ers across all relations.
8 Discussion
Our results show that the distant supervision algo-
rithm is able to extract high-precision patterns for
a reasonably large number of relations.
The held-out results in Figure 2 suggest that the
combination of syntactic and lexical features pro-
vides better performance than either feature set on
its own. In order to understand the role of syntactic
features, we examine Table 5, the human evalua-
tion of the most frequent 10 relations. For the top-
ranking 100 instances of each relation, most of the
best results use syntactic features, either alone or
in combination with lexical features. For the top-
ranking 1000 instances of each relation, the results
are more mixed, but syntactic features still helped
in most classifications.
We then examine those relations for which syn-
tactic features seem to help. For example, syn-
tactic features consistently outperform lexical fea-
tures for the director-film and writer-film relations.
As discussed in section 4, these two relations are
particularly ambiguous, suggesting that syntactic
features may help tease apart difficult relations.
Perhaps more telling, we noticed many examples
with a long string of words between the director
and the film:
Back Street is a 1932 film made by Univer-
sal Pictures, directed by John M. Stahl, and
produced by Carl Laemmle Jr.
Sentences like this have very long (and thus rare)
lexical features, but relatively short dependency
paths. Syntactic features can more easily abstract
from the syntactic modifiers that comprise the ex-
traneous parts of these strings.
Our results thus suggest that syntactic features
are indeed useful in distantly supervised informa-
tion extraction, and that the benefit of syntax oc-
curs in cases where the individual patterns are par-
ticularly ambiguous, and where they are nearby in
the dependency structure but distant in terms of
words. It remains for future work to see whether
simpler, chunk-based syntactic features might be
able to capture enough of this gain without the
overhead of full parsing, and whether coreference
resolution could improve performance.
Acknowledgments
We would like to acknowledge Sarah Spikes for
her help in developing the relation extraction sys-
tem, Christopher Manning and Mihai Surdeanu
for their invaluable advice, and Fuliang Weng
and Baoshi Yan for their guidance. Our research
was partially funded by the NSF via award IIS-
0811974 and by Robert Bosch LLC.
1010
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM Interna-
tional Conference on Digital Libraries.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In
Manuela M Veloso, editor, IJCAI-07, pages 2670?
2676.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD ?08, pages 1247?
1250, New York, NY. ACM.
Sergei Brin. 1998. Extracting patterns and relations
from the World Wide Web. In Proceedings World
Wide Web and Databases International Workshop,
Number 1590 in LNCS, pages 172?183. Springer.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
supervision. In ACL-07, pages 576?583, Prague,
Czech Republic, June.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Thomas Lengauer, Rein-
hard Schneider, Peer Bork, Douglas L. Brutlag, Jan-
ice I. Glasgow, Hans W. Mewes, and Ralf Zimmer,
editors, ISMB, pages 77?86. AAAI.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) Program?Tasks, Data, and Evaluation.
LREC-04, pages 837?840.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91?134.
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL-05, pages 363?370, Ann Arbor,
MI.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In HLT-
NAACL-03, pages 1?8, Edmonton, Canada.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING-92,
Nantes, France.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Workshop on the Evaluation of Parsing
Systems.
Metaweb. 2008. Freebase data dumps. http://
download.freebase.com/datadumps/.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh, and Jeff B. Colombe.
2004. Gene name identification and normalization
using a model organism database. J. of Biomedical
Informatics, 37(6):396?410.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for auto-
matically harvesting semantic relations. In COL-
ING/ACL 2006, pages 113?120, Sydney, Australia.
Marco Pennacchiotti and Patrick Pantel. 2006. A boot-
strapping algorithm for automatically harvesting se-
mantic relations. In in Proceedings of Inference in
Computational Semantics (ICoS-06), pages 87?96.
Deepak Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. In ACL-02, pages 41?47, Philadelphia,
PA.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI-99, pages 474?479.
Benjamin Rozenfeld and Ronen Feldman. 2008. Self-
supervised relation extraction from the web. Knowl-
edge and Information Systems, 17(1):17?33.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In HLT-NAACL-06, pages 304?311,
New York, NY.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
Le?on Bottou, editors, NIPS 17, pages 1297?1304.
MIT Press.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In EMNLP 2008, pages 254?263, Hon-
olulu, HI.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
Proceedings of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), March.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In CIKM ?07: Proceedings
of the sixteenth ACM conference on Conference on
information and knowledge management, pages 41?
50, Lisbon, Portugal.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In ACL-05, pages 427?434, Ann Arbor,
MI.
Guodong Zhou, Min Zhang, Donghong Ji, and Qiaom-
ing Zhu. 2007. Tree kernel-based relation extrac-
tion with context-sensitive structured parse tree in-
formation. In EMNLP/CoNLL 2007.
1011
