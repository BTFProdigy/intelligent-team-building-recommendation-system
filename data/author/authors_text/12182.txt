Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708?717,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Discriminative Corpus Weight Estimation for Machine Translation
Spyros Matsoukas and Antti-Veikko I. Rosti and Bing Zhang
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,arosti,bzhang}@bbn.com
Abstract
Current statistical machine translation
(SMT) systems are trained on sentence-
aligned and word-aligned parallel text col-
lected from various sources. Translation
model parameters are estimated from the
word alignments, and the quality of the
translations on a given test set depends
on the parameter estimates. There are
at least two factors affecting the parame-
ter estimation: domain match and training
data quality. This paper describes a novel
approach for automatically detecting and
down-weighing certain parts of the train-
ing corpus by assigning a weight to each
sentence in the training bitext so as to op-
timize a discriminative objective function
on a designated tuning set. This way, the
proposed method can limit the negative ef-
fects of low quality training data, and can
adapt the translation model to the domain
of interest. It is shown that such discrim-
inative corpus weights can provide sig-
nificant improvements in Arabic-English
translation on various conditions, using a
state-of-the-art SMT system.
1 Introduction
Statistical machine translation (SMT) systems rely
on a training corpus consisting of sentences in
the source language and their respective reference
translations to the target language. These paral-
lel sentences are used to perform automatic word
alignment, and extract translation rules with asso-
ciated probabilities. Typically, a parallel training
corpus is comprised of collections of varying qual-
ity and relevance to the translation problem of in-
terest. For example, an SMT system applied to
broadcast conversational data may be trained on
a corpus consisting mostly of United Nations and
newswire data, with only a very small amount of
in-domain broadcast news/conversational data. In
this case, it would be desirable to down-weigh the
out-of-domain data relative to the in-domain data
during the rule extraction and probability estima-
tion. Similarly, it would be good to assign a lower
weight to data of low quality (e.g., poorly aligned
or incorrectly translated sentences) relative to data
of high quality.
In this paper, we describe a novel discrimina-
tive training method that can be used to estimate a
weight for each sentence in the training bitext so as
to optimize an objective function ? expected trans-
lation edit rate (TER) (Snover et al, 2006) ? on a
held-out development set. The training bitext typ-
ically consists of millions of (parallel) sentences,
so in order to ensure robust estimation we express
each sentence weight as a function of sentence-
level features, and estimate the parameters of this
mapping function instead. Sentence-level fea-
tures may include the identifier of the collection or
genre that the sentence belongs to, the number of
tokens in the source or target side, alignment infor-
mation, etc. The mapping from features to weights
can be implemented via any differentiable func-
tion, but in our experiments we used a simple per-
ceptron. Sentence weights estimated in this fash-
ion are applied directly to the phrase and lexical
counts unlike any previously published method to
the author?s knowledge. The tuning framework is
developed for phrase-based SMT models, but the
tuned weights are also applicable to the training of
a hierarchical model. In cases where the tuning set
used for corpus weight estimation is a close match
to the test set, this method yields significant gains
in TER, BLEU (Papineni et al, 2002), and ME-
TEOR (Lavie and Agarwal, 2007) scores over a
state-of-the-art hierarchical baseline.
The paper is organized as follows. Related work
on data selection, data weighting, and model adap-
tation is presented in Section 2. The corpus weight
708
approach and estimation algorithm are described
in Section 3. Experimental evaluation of the ap-
proach is presented in Sections 4 and 5. Section 6
concludes the paper with a few directions for fu-
ture work.
2 Related Work
Previous work related to corpus weighting may
be split into three categories: data selection, data
weighting, and translation model adaptation. The
first two approaches may improve the quality
of the word alignment and prevent phrase-pairs
which are less useful for the domain to be learned.
The model adaptation, on the other hand, may
boost the weight of the more relevant phrase-
pairs or introduce translations for unseen source
phrases.
Resnik and Smith (2003) mined parallel text
from the web using various filters to identify likely
translations. The filtering may be viewed as a
data selection where poor quality translation are
discarded before word alignment. Yasuda et al
(2008) selected subsets of an existing parallel cor-
pus to match the domain of the test set. The dis-
carded sentence pairs may be valid translations
but they do not necessarily improve the translation
quality on the test domain. Mandal et al (2008)
used active learning to select suitable training data
for human translation. Hildebrand et al (2005) se-
lected comparable sentences from parallel corpora
using information retrieval techniques.
Lu et al (2007) proposed weighting compara-
ble portions of the parallel text before word align-
ment based on information retrieval. The relevant
portions of the parallel text were given a higher in-
teger weight in GIZA++ word alignment. Similar
effect may be achieved by replicating the relevant
subset in the training data.
Lu et al (2007) also proposed training adapted
translation models which were interpolated with a
model trained on the entire parallel text. Snover
et al (2008) used cross-lingual information re-
trieval to identify possible bias-rules to improve
the coverage on the source side. These rules may
cover source phrases for which no translations
were learned from the available parallel text.
Koehn and Schroeder (2007) described a pro-
cedure for domain adaptation that was using two
translation models in decoding, one trained on
in-domain data and the other on out-of-domain
data. Phrase translation scores from the two mod-
els where combined in a log-linear fashion, with
weights estimated based on minimum error rate
training (Och, 2003) on a designated tuning set.
The method described in this paper can also be
viewed as data filtering or (static) translation adap-
tation, but it has the following advantages over
previously published techniques:
1. The estimated corpus weights are discrim-
inative and are computed so as to directly
optimize an MT performance metric on a
pre-defined development set. Unlike the do-
main adaptation technique in (Koehn and
Schroeder, 2007), which also estimates the
adaptation parameters discriminatively, our
proposed method does not require a man-
ual specification of the in-domain and out-
of-domain training data collections. Instead,
it automatically determines which collections
are most relevant to the domain of interest,
and increases their weight while decreasing
the weight assigned to less relevant collec-
tions.
2. All sentences in the parallel corpus can in-
fluence the translation model, as opposed
to filtering/discarding data. However, the
proposed method can still assign very low
weights to parts of the corpus, if it determines
that it helps improve MT performance.
3. The framework used for estimating the cor-
pus weights can be easily extended to support
discriminative alignment link-level weights,
thus allowing the system to automatically
identify which portions of the training sen-
tences are most useful.
Naturally, as with any method, the proposed
technique has certain limitations. Specifically, it
is only concerned with influencing the translation
rule probabilities via the corpus weights; it does
not change the set of rules extracted. Thus, it is
unable to add new translation rules as in Snover
et al (2008). Also, it can potentially lead to pa-
rameter over-fitting, especially if the function that
maps sentence features to weights is complex and
based on a large number of parameters, or if the
development set used for estimating the mapping
function does not match the characteristics of the
test set.
709
3 Corpus Weights Estimation
3.1 Feature Extraction
The purpose of feature extraction is to identify,
for each sentence in the parallel training data, a
set of features that can be useful in estimating a
weight that is correlated with quality or relevance
to the MT task at hand. Starting from sentence-
aligned, word-aligned parallel training data, one
could extract various types of sentence-level fea-
tures. For example, we could specify features that
describe the two sides of the parallel data or the
alignment between them, such as collection id,
genre id, number of source tokens, number of tar-
get tokens, ratio of number of source and target
tokens, number of word alignment links, fraction
of source tokens that are unaligned, and fraction
of target tokens that are unaligned. Additionally,
we could include information retrieval (IR) related
features that reflect the relevance of a training sen-
tence to the domain of interest, e.g., by measur-
ing vector space model (VSM) distance of the sen-
tence to the current tuning set, or its log likelihhod
with respect to an in-domain language model.
Note that the collection and genre identifiers
(ids) mentioned above are bit vectors. Each col-
lection in the training set is mapped to a number.
A collection may consist of sentences from multi-
ple genres (e.g., newswire, web, broadcast news,
broadcast conversations). Genres are also mapped
to a unique number across the whole training set.
Then, given a sentence in the training bitext, we
can extract a binary vector that contains two non-
zero bits, one indicating the collection id, and an-
other denoting the genre id.
It is worth mentioning that in the experiments
reported later in this paper we made use of only the
collection and genre ids as features, although the
framework supports general sentence-level fea-
tures.
3.2 Mapping Features to Weights
As mentioned previously, one way to map a fea-
ture vector to a weight is to use a perceptron.
A multi-layer neural network may also be used,
but at the expense of slower training. In this
work, all of the experiments carried out made use
of a perceptron mapping function. However, it
is also possible to cluster the training sentences
into classes by training a Gaussian mixture model
(GMM) on their respective feature vectors1. Then,
given a feature vector we can compute the (poste-
rior) probability that it was generated by one of
the N Gaussians in the GMM, and use this N-
dimensional vector of posteriors as input to the
perceptron. This is similar to having a neural net-
work with a static hidden layer and Gaussian acti-
vation functions.
Given the many choices available in mapping
features to weights, we will describe the mapping
function in general terms. Let f
i
be the n ? 1
feature vector corresponding to sentence i. Let
?(x;?) denote a function Rn ? (0, 1) that is pa-
rameterized in terms of the parameter vector ? and
maps a feature vector x to a scalar weight in (0, 1).
The goal of the automatic corpus weight estima-
tion procedure is to estimate the parameter vector
? so as to optimize an objective function on a de-
velopment set.
3.3 Training with Weighted Corpora
Once the sentence features have been mapped to
weights, the translation rule extraction and prob-
ability estimation can proceed as usual, but with
weighted counts. For example, let w
i
= ?(f
i
;?)
be the weight assigned to sentence i. Let (s, t) be
a source-target phrase pair that can be extracted
from the corpus, and A(s) and B(t) indicating the
sets of sentences that s and t occur in. Then,
P (s|t) =
?
j?A(s)?B(t)
w
j
c
j
(s, t)
?
j?B(t)
w
j
c
j
(t)
(1)
where c
j
(?) denotes the number of occurrences of
the phrase (or phrase-pair) in sentence j.
3.4 Optimizing the Mapping Function
Estimation of the parameters ? of the mapping
function ? can be performed by directly optimiz-
ing a suitable objective function on a development
set. Ideally, we would like to estimate the param-
eters of the mapping function so as to directly op-
timize an automatic MT performance evaluation
metric, such as TER or BLEU on the full transla-
tion search space. However, this is extremely com-
putationally intensive for two reasons: (a) opti-
mizing in the full translation search space requires
a new decoding pass for each iteration of opti-
mization; and (b) a direct optimization of TER or
1Note that in order to train such a GMM it may be nec-
essary to first apply a decorrelating, dimensionality reducing,
transform (e.g., principal component analysis) to the features.
710
BLEU requires the use of a derivative free, slowly
converging optimization method such as MERT
(Och, 2003), because these objective functions are
not differentiable.
In our case, for every parameter vector update
we need to essentially retrain the translation model
(reestimate the phrase and lexical translation prob-
abilities based on the updated corpus weights), so
the cost of each iteration is significantly higher
than in a typical MERT application. For these rea-
sons, in this work we chose to minimize the ex-
pected TER over a translation N-best on a desig-
nated tuning set, which is a continuous and differ-
entiable function and can be optimized with stan-
dard gradient descent methods in a small number
of iterations. Note, that using expected TER is not
the only option here; any criterion that can be ex-
pressed as a continuous function of the phrase or
lexical translation probabilities can be used to op-
timize ?.
Given an N-best of translation hypotheses over
a development set of S sentences, we can define
the expected TER as follows
T =
?
S
s=1
?
N
s
j=1
p
sj

sj
?
S
s=1
r
s
(2)
where N
s
is the number of translation hypothe-
ses available for segment s; 
sj
is the minimum
raw edit distance between hypothesis j of seg-
ment s (or h
sj
, for short) and the reference transla-
tion(s) corresponding to segment s; r
s
is the aver-
age number of reference translation tokens in seg-
ment s, and p
sj
is the posterior probability of hy-
pothesis h
sj
in the N-best. The latter is computed
as follows
p
sj
=
e
?L
sj
?
N
s
k=1
e
?L
sk
(3)
where L
sj
is the total log likelihood of hypothe-
sis h
sj
, and ? is a tunable scaling factor that can
be used to change the dynamic range of the likeli-
hood scores and hence the distribution of posteri-
ors over the N-best. The hypothesis likelihood L
sj
is typically computed as a dot product of a decod-
ing weight vector and a vector of various ?feature?
scores, such as log phrase translation probability,
log lexical translation probability, log n-gram lan-
guage model probability, and number of tokens in
the hypothesis. However, in order to simplify this
presentation we will assume that it contains a sin-
gle translation model score, the log phrase transla-
tion probability of source given target. This score
is a sum of log conditional probabilities, similar
to the one defined in Equation 1. Therefore, L
sj
is indirectly a function of the training sentence
weights.
In order to minimize the expected TER T , we
need to compute the derivative of T with respect
to the mapping function parameters ?. Using the
chain rule, we get equations (4)-(8), where the
summation in Equation 6 is over all source-target
phrase pairs in the derivation of hypothesis h
sm
, ?
is the decoding weight assigned to the log phrase
translation score, and the summation in Equation
7 is over all training sentences2.
Thus, in order to compute the derivative of
the objective function we first need to calculate
? lnP (s
k
|t
k
)
??
for every phrase pair (s
k
, t
k
) in the
translation N-best based on Equations 7 and 8,
which requires time proportional to the number of
occurrences of these phrases in the parallel train-
ing data. After that, we can compute ?Lsm
??
for
each hypothesis h
sm
, based on Equation 6. Fi-
nally, we calculate ? ln psj
??
and ?T
??
based on Equa-
tions 5 and 4, respectively.
3.5 Implementation Issues
In our system, the corpus weights were trained
based on N-best translation hypotheses generated
by a phrase-based MT system on a designated tun-
ing set. Each translation hypothesis in the N-best
has a score that is a (linear) function of the fol-
lowing log translation probabilities: target phrase
given source phrase, source phrase given target
phrase, and lexical smoothing term. Additionally,
each hypothesis specifies information about its
derivation, i.e., which source-target phrase pairs it
consists of. Therefore, given an N-best, we can
identify the set of unique phrase pairs and use this
information in order to perform a filtered accumu-
lation of the statistics needed for calculating the
derivative in Equation 8. This reduces the storage
needed for the sufficient statistics significantly.
Minimization of the expected TER of the N-
best hypotheses was performed using the limited-
memory BFGS algorithm (Liu and Nocedal,
1989). Typically, the parameter vector ? required
about 30 iterations of LBFGS to converge.
Since the N-best provides only a limited repre-
sentation of the MT hypothesis search space, we
regenerated the N-best after every 30 iterations
2In the general case where L
sj
includes other translation
scores, e.g., lexical translation probabilities, the derivative
?L
sm
??
will have to include additional terms.
711
?T
??
=
S
?
s=1
N
s
?
j=1
?T
? ln p
sj
? ln p
sj
??
=
(
1
?
S
s=1
r
s
)
S
?
s=1
N
s
?
j=1
p
sj

sj
? ln p
sj
??
(4)
? ln p
sj
??
=
N
s
?
m=1
? ln p
sj
?L
sm
?L
sm
??
= ?
(
?L
sj
??
?
N
s
?
m=1
p
sm
?L
sm
??
)
(5)
?L
sm
??
=
?
(s
k
,t
k
)?h
sm
?L
sm
? lnP (s
k
|t
k
)
? lnP (s
k
|t
k
)
??
=
?
(s
k
,t
k
)?h
sm
?
? lnP (s
k
|t
k
)
??
(6)
? lnP (s
k
|t
k
)
??
=
?
i
? lnP (s
k
|t
k
)
?w
i
?w
i
??
(7)
? lnP (s
k
|t
k
)
?w
i
=
?
j?A(s
k
)?B(t
k
)
? (j ? i) c
j
(s
k
, t
k
)
?
j?A(s
k
)?B(t
k
)
w
j
c
j
(s
k
, t
k
)
?
?
j?B(t
k
)
? (j ? i) c
j
(t
k
)
?
j?B(t
k
)
w
j
c
j
(t
k
)
(8)
?(x) =
{
1 x = 0
0 x 6= 0
(9)
of LBFGS training, merging new hypotheses with
translations from previous iterations. The overall
training procedure is described in more detail be-
low:
1. Initialize parameter vector ? to small random
values, so that all training sentences receive
approximately equal weights.
2. Initialize phrase-based MT decoding weights
to previously tuned values.
3. Perform weighted phrase rule extraction as
described in Equation 1, to estimate the
phrase and lexical translation probabilities.
4. Decode the tuning set, generating N-best.
5. Merge N-best hypotheses from previous iter-
ations to current N-best.
6. Tune decoding weights so as to minimize
TER on merged N-best, using a derivative
free optimization method. In our case, we
used Powell?s algorithm (Powell, 1964) mod-
ified by Brent as described in (Brent, 1973) 3.
7. Identify set of unique source-target phrase
pairs in merged N-best.
8. Extract sufficient statistics from training data
for all phrases identified in step 7.
3This method was first used for N-best based parameter
optimization in (Ostendorf et al, 1991).
9. Run the LBFGS algorithm to minimize the
expected TER in the merged N-best, using
the derivative equations described previously.
10. Assign a weight to each training sentence
based on the ? values optimized in 9.
11. Go to step 3.
Typically, the corpus weights converge in about
4-5 main iterations. The calculation of the deriva-
tive is parallelized to speed up computation, re-
quiring about 10 minutes per iteration of LBFGS.
4 Experimental Setup
In this section we describe the setup that was used
for all experiments reported in this paper. Specif-
ically, we provide details about the training data,
development sets, and MT systems (phrase-based
and hierarchical).
4.1 Training Data
All MT training experiments made use of an
Arabic-English corpus of approximately 200 mil-
lion tokens (English side). Most of the collections
in this corpus are available through the Linguis-
tic Data Consortium (LDC) and are regularly part
of the resources specified for the constrained data
track of the NIST MT evaluation4.
4For a list of the NIST MT09 constrained train-
ing condition resources, see http://www.itl.
nist.gov/iad/mig/tests/mt/2009/MT09_
ConstrainedResources.pdf
712
The corpus includes data from multiple gen-
res, as shown in Table 1. The ?Sakhr? newswire
collection is a set of Arabic-to-English and
English-to-Arabic data provided by Sakhr Soft-
ware, totaling about 30.8 million tokens, and
is only available to research teams participat-
ing in the Defense Advanced Research Projects
Agency (DARPA) Global Autonomous Language
Exploitation (GALE) program. The ?LDC Giga-
word (ISI)? collection was produced by automati-
cally detecting and extracting portions of parallel
text from the monolingual LDC Arabic and En-
glish Gigaword collections, using a method devel-
oped at the Information Sciences Institute (ISI) of
the University of Southern California.
Data Origin Style Size(K tokens)
LDC pre-GALE
U. Nations 118049
Newswire 2700
Treebank 685
LDC post-GALE
Newswire 14344
Treebank 292
Web 478
Broad. News 573
Broad. Conv. 1003
Web-found text Lexicons 436Quran 406
Sakhr Newswire 30790
LDC Gigaword Newswire 29169(ISI)
Table 1: Composition of the Arabic-English par-
allel corpus used for MT training.
It is easy to see that most of the parallel train-
ing data are either newswire or from United Na-
tions. The amount of web text or broadcast
news/conversations is only a very small fraction
of the total corpus. In total, there are 31 collec-
tions in the training bitext. Some collections (es-
pecially those released recently by LDC for the
GALE project) consist of data from multiple gen-
res. The total number of unique genres (or data
types) in the training set is 10.
Besides the above bitext, we also used approxi-
mately 8 billion words of English text for language
model (LM) training (3.7B words from the LDC
Gigaword corpus, 3.3B words of web-downloaded
text, and 1.1B words of data from CNN archives).
This data was used to train two language mod-
els: an entropy-pruned trigram LM, used in decod-
ing, and an unpruned 5-gram LM used in N-best
rescoring. Kneser-Ney smoothing was applied to
the n-grams in both cases.
4.2 Development Sets
The development sets used for tuning and testing
the corpus weights and other MT settings were
comprised of documents from previous Arabic-
English NIST MT evaluation sets and from GALE
development/evaluation sets.
Specifically, the newswire Tune and Test sets
consist of documents from the following col-
lections: the newswire portion of NIST MT04,
MT05, MT06, and MT08 evaluation sets, the
GALE Phase 1 (P1) and Phase 2 (P2) evaluation
sets, and the GALE P2 and P3 development sets.
The web Tune and Test sets are made of docu-
ments from NIST MT06 and MT08, the GALE P1
and P2 evaluation sets, the GALE P2 and P3 devel-
opment sets, and a held-out portion of the GALE
year 1 quarter 4 web training data release.
The audio Tune and Test sets consist of roughly
equal parts of news and conversations broadcast
from November 2005 through May 2007 by ma-
jor Arabic-speaking television and radio stations
(e.g., Al-Jazeera, Al-Arabiya, Syrian TV), totaling
approximately 14 hours of speech. The audio was
processed through automated speech recognition
(ASR) in order to produce (errorful) transcripts
that were used as input to all MT decoding experi-
ments reported in this paper. However, the corpus
weight estimation was carried out based on N-best
MT of the Arabic audio reference transcriptions
(i.e., the transcripts had no speech recognition er-
rors, and contained full punctuation).
It is important to note that some of the docu-
ments in the above devsets have multiple reference
translations (usually 4), while others have only
one. Most of the documents in the newswire sets
have 4 references, but unfortunately the web and
audio sets have, on average, less than 2 reference
translations per segment. More details are listed in
Table 2.
Another important note is that, although the au-
dio sets consist of both broadcast news (BN) and
broadcast conversations (BC), we did not perform
BN or BC-specific tuning. Corpus weights and
MT decoding parameters were optimized based on
a single Tune set, on a mix of BN and BC data.
However, when we report speech translation re-
sults in later sections, we break down the perfor-
713
Genre Tune Test#segs #tokens #refs/seg #segs #tokens #refs/seg
Newswire 1994 72359 3.94 3149 115700 3.67
Web 3278 99280 1.69 4425 125795 2.08
Audio BN 897 32990 1.00 1530 53067 1.00
Audio BC 765 24607 1.00 1416 44435 1.00
Table 2: Characteristics of the tuning (Tune) and validation (Test) sets used for development on Arabic
newswire, web, and audio. The audio sets include material from both broadcast news and broadcast
conversations.
mance by genre.
4.3 MT Systems
Experiments were performed using two types of
statistical MT systems: a phrase-based system,
similar to Pharaoh (Koehn, 2004), and a state-
of-the-art, hierarchical string-to-dependency-tree
system, similar to (Shen et al, 2008).
The phrase-based MT system employs a pruned
3-gram LM in decoding, and can optionally gen-
erate N-best unique translation hypotheses which
are used to estimate the corpus weights, as de-
scribed in Section 3.
The hierarchical MT system performs decoding
with the same 3-gram LM, generates N-best of
unique translation hypotheses, and then rescores
them using a large, unpruned 5-gram LM in order
to select the best scoring translation. It is worth
mentioning that this hierarchical MT system pro-
vides a very strong baseline; it achieves a case-
sensitive BLEU score of 52.20 on the newswire
portion of the NIST MT08 evaluation set, which
is similar to the score of the second-best system
that participated in the unconstrained data track of
the NIST MT08 evaluation.
Both types of models were trained on the same
word alignments generated by GIZA++ (Och and
Ney, 2003).
5 Results
In this section we report results on the Arabic
newswire, web, and audio development sets, us-
ing both phrase-based and hierarchical MT sys-
tems, in terms of TER, BLEU5, and METEOR
(Lavie and Agarwal, 2007). Whenever corpus
weights are used, they were estimated on the des-
ignated Tune set using the phrase-based MT sys-
5The brevity penalty was calculated using the formula in
the original IBM paper, rather than the more recent definition
implemented in the NIST mteval-v11b.pl script.
tem. Only the collection and genre ids were used
as sentence features in order to estimate the corpus
weights. As mentioned in Section 4.1, the train-
ing bitext consists of 31 collections and 10 gen-
res, so each training sentence was assigned a 41-
dimensional binary vector indicating its particu-
lar collection/genre combination. That vector was
then mapped into a single weight using a percep-
tron.
5.1 Phrase-based MT
Results using the phrase-based MT system are
shown in Table 3. In all cases, the decoding
weights were optimized so as to minimize TER
on the designated Tune set. On newswire, the
discriminative corpus weights provide 0.8% abso-
lute gain in TER, in both Tune and Test sets. On
web, the TER gain is 0.9% absolute on Tune and
0.5% on Test. On the audio Test set, the TER gain
is 0.5% on BN and 1.4% on BC. Significant im-
provements were also obtained in the BLEU and
METEOR scores, on all sets and conditions.
5.2 Hierarchical MT
Results using the hierarchical MT system are
shown in Table 4. The hierarchical system
used different tuning criteria in each genre. On
newswire, the decoding weights were optimized
so as to maximize BLEU, while on web and audio
the tuning was based on 0.5TER+0.5(1?BLEU)
(referred to as TERBLEU in what follows). Note
that these were the criteria for tuning the decoding
weights; whenever corpus weights were used, they
were taken from the phrase-based system.
It is interesting to see that gains from discrimi-
native corpus weights carry over to the more pow-
erful hierarchical MT system. On newswire Test,
the gain in BLEU is 0.8; on web Test, the gain in
TERBLEU is 0.3. On the audio Test set, the cor-
pus weights provide 0.7 and 0.75 TERBLEU re-
duction on BN and BC, respectively. As with the
714
Set Corpus Weights Newswire WebTER BLEU MTR TER BLEU MTR
Tune No 42.3 48.2 67.5 60.0 21.9 51.3Yes 41.5 49.6 68.7 59.1 22.8 52.3
Test No 43.2 46.2 66.5 58.6 24.2 52.2Yes 42.4 47.5 67.8 58.1 25.4 52.9
(a) Results on Arabic text.
Set Corpus Weights BN BCTER BLEU MTR TER BLEU MTR
Tune No 56.0 22.9 55.5 57.3 21.7 55.0Yes 55.0 25.0 57.1 56.1 23.6 56.4
Test No 53.0 25.3 57.7 55.9 22.9 55.4Yes 52.5 26.6 58.8 54.5 24.7 56.8
(b) Results on Arabic audio.
Table 3: Phrase-based trigram decoding results on the Arabic text and audio development sets. Decoding
weights were optimized on the Tune set in order to directly minimize TER. Corpus weights were also
optimized on Tune set, but based on expected TER.
phrase-based system, all metrics improve from the
use of corpus weights, in all sets/conditions.
6 Conclusions
We have described a novel approach for estimat-
ing a weight for each sentence in a parallel train-
ing corpus so as to optimize MT performance of a
phrase-based statistical MT system. The sentence
weights influence MT performance by being ap-
plied to the phrase and lexical counts during trans-
lation rule extraction and probability estimation.
In order to ensure robust training of the weights,
we expressed them as a function of sentence-level
features. Then, we defined the process for opti-
mizing the parameters of that function based on
the expected TER of a translation hypothesis N-
best on a designated tuning set.
The proposed technique was evaluated in the
context of Arabic-English translation, on multiple
conditions. It was shown that encouraging results
were obtained by just using collection and genre
ids as features. Interestingly, the discriminative
corpus weights were found to be generally appli-
cable and provided gains in a state-of-the-art hi-
erarchical string-to-dependency-tree MT system,
even though they were trained using the phrase-
based MT system.
Next step is to include other sentence-level fea-
tures, as described in Section 3.1. Finally, the
technique described in this paper can be extended
to address the estimation of weights at the align-
ment link level, based on link-level features. We
believe that this will have a larger impact on the
lexical and phrase translation probabilities, since
there is a large number of parallel training sen-
tences that are partially correct, i.e., they contain
parts that are aligned and translated correctly, and
parts that are wrong. The current procedure tries
to assign a single weight to such sentences, so
there is no way to distinguish between the ?good?
and ?bad? portions of each sentence. Pushing the
weight estimation at the alignment link level will
alleviate this problem and will make the discrimi-
native training more targeted.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
Richard P. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
715
Set Corpus Weights Newswire WebTER BLEU MTR TER BLEU MTR
Tune No 39.5 54.4 70.3 58.2 25.2 53.8Yes 38.8 55.6 71.2 58.0 25.5 54.0
Test No 40.7 52.1 69.3 57.0 28.3 54.7Yes 40.1 52.9 69.8 56.6 28.5 55.0
(a) Results on Arabic text.
Set Corpus Weights BN BCTER BLEU MTR TER BLEU MTR
Tune No 54.9 27.3 58.0 55.8 26.1 57.4Yes 53.6 28.2 59.0 54.9 26.9 58.0
Test No 51.6 29.9 60.0 54.4 27.6 57.7Yes 50.7 30.4 60.7 53.2 27.9 58.7
(b) Results on Arabic audio.
Table 4: Hierarchical 5-gram rescoring results on the Arabic text and audio development sets. Decod-
ing/rescoring weights were optimized on the Tune set in order to directly maximize BLEU (for newswire)
or minimize TERBLEU (for web and audio). Corpus weights were the same as the ones used in the cor-
responding phrase-based decodings.
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
Annual Conference of European Association for Ma-
chine Translation, pages 133?142.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
lation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 224?227.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of the 6th Conference
of the Association for Machine Translation in the
Americas.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 343?350.
Arindam Mandal, Dimitra Vergyri, Wen Wang, Jing
Zheng, Andreas Stolcke, Gokhan Tur, Dilek
Hakkani-Tu?r, and Necip Fazil Ayan. 2008. Effi-
cient data selection for machine translation. In Pro-
ceedings of the Second IEEE/ACL Spoken Language
Technology Workshop, pages 261?264.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language, pages 83?87.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
pages 155?162.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
716
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577?585.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 857?866.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, volume II, pages 655?660.
717
Proceedings of NAACL HLT 2007, pages 228?235,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Outputs from Multiple Machine Translation Systems
Antti-Veikko I. Rosti   and Necip Fazil Ayan

and Bing Xiang   and
Spyros Matsoukas   and Richard Schwartz   and Bonnie J. Dorr

  BBN Technologies, 10 Moulton Street, Cambridge, MA 02138

arosti,bxiang,smatsouk,schwartz  @bbn.com

Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742

nfa,bonnie  @umiacs.umd.edu
Abstract
Currently there are several approaches to
machine translation (MT) based on differ-
ent paradigms; e.g., phrasal, hierarchical
and syntax-based. These three approaches
yield similar translation accuracy despite
using fairly different levels of linguistic
knowledge. The availability of such a
variety of systems has led to a growing
interest toward finding better translations
by combining outputs from multiple sys-
tems. This paper describes three differ-
ent approaches to MT system combina-
tion. These combination methods oper-
ate on sentence, phrase and word level
exploiting information from  -best lists,
system scores and target-to-source phrase
alignments. The word-level combination
provides the most robust gains but the
best results on the development test sets
(NIST MT05 and the newsgroup portion
of GALE 2006 dry-run) were achieved by
combining all three methods.
1 Introduction
In recent years, machine translation systems based
on new paradigms have emerged. These systems
employ more than just the surface-level information
used by the state-of-the-art phrase-based translation
systems. For example, hierarchical (Chiang, 2005)
and syntax-based (Galley et al, 2006) systems have
recently improved in both accuracy and scalability.
Combined with the latest advances in phrase-based
translation systems, it has become more attractive
to take advantage of the various outputs in forming
consensus translations (Frederking and Nirenburg,
1994; Bangalore et al, 2001; Jayaraman and Lavie,
2005; Matusov et al, 2006).
System combination has been successfully ap-
plied in state-of-the-art speech recognition evalua-
tion systems for several years (Fiscus, 1997). Even
though the underlying modeling techniques are sim-
ilar, many systems produce very different outputs
with approximately the same accuracy. One of the
most successful approaches is consensus network
decoding (Mangu et al, 2000) which assumes that
the confidence of a word in a certain position is
based on the sum of confidences from each system
output having the word in that position. This re-
quires aligning the system outputs to form a con-
sensus network and ? during decoding ? simply
finding the highest scoring path through this net-
work. The alignment of speech recognition outputs
is fairly straightforward due to the strict constraint in
word order. However, machine translation outputs
do not have this constraint as the word order may be
different between the source and target languages.
MT systems employ various re-ordering (distortion)
models to take this into account.
Three MT system combination methods are pre-
sented in this paper. They operate on the sentence,
phrase and word level. The sentence-level combi-
nation is based on selecting the best hypothesis out
of the merged N-best lists. This method does not
generate new hypotheses ? unlike the phrase and
word-level methods. The phrase-level combination
228
is based on extracting sentence-specific phrase trans-
lation tables from system outputs with alignments
to source and running a phrasal decoder with this
new translation table. This approach is similar to
the multi-engine MT framework proposed in (Fred-
erking and Nirenburg, 1994) which is not capable of
re-ordering. The word-level combination is based
on consensus network decoding. Translation edit
rate (TER) (Snover et al, 2006) is used to align
the hypotheses and minimum Bayes risk decoding
under TER (Sim et al, 2007) is used to select the
alignment hypothesis. All combination methods use
weights which may be tuned using Powell?s method
(Brent, 1973) on  -best lists. Both sentence and
phrase-level combination methods can generate  -
best lists which may also be used as new system out-
puts in the word-level combination.
Experiments on combining six machine transla-
tion system outputs were performed. Three sys-
tems were phrasal, two hierarchical and one syntax-
based. The systems were evaluated on NIST MT05
and the newsgroup portion of the GALE 2006 dry-
run sets. The outputs were evaluated on both TER
and BLEU. As the target evaluation metric in the
GALE program was human-mediated TER (HTER)
(Snover et al, 2006), it was found important to im-
prove both of these automatic metrics.
This paper is organized as follows. Section 2
describes the evaluation metrics and a generic dis-
criminative optimization technique used in tuning of
the various system combination weights. Sentence,
phrase and word-level system combination methods
are presented in Sections 3, 4 and 5. Experimental
results on Arabic and Chinese to English newswire
and newsgroup test data are presented in Section 6.
2 Evaluation Metrics and Discriminative
Tuning
The official metric of the 2006 DARPA GALE
evaluation was human-mediated translation edit rate
(HTER). HTER is computed as the minimum trans-
lation edit rate (TER) between a system output and
a targeted reference which preserves the meaning
and fluency of the sentence (Snover et al, 2006).
The targeted reference is generated by human post-
editors who make edits to a reference translation so
as to minimize the TER between the reference and
the MT output without changing the meaning of the
reference. Computing the HTER is very time con-
suming due to the human post-editing. It is desir-
able to have an automatic evaluation metric that cor-
relates well with the HTER to allow fast evaluation
of the MT systems during development. Correla-
tions of different evaluation metrics have been stud-
ied (Snover et al, 2006) but according to various
internal HTER experiments it is not clear whether
TER or BLEU correlates better. Therefore it is prob-
ably safest to try and not degrade either.
The TER of a translation   is computed as

 	
 
ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312?319,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Improved Word-Level System Combination for Machine Translation
Antti-Veikko I. Rosti and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street
Cambridge, MA 02138
 
arosti,smatsouk,schwartz  @bbn.com
Abstract
Recently, confusion network decoding has
been applied in machine translation system
combination. Due to errors in the hypoth-
esis alignment, decoding may result in un-
grammatical combination outputs. This pa-
per describes an improved confusion net-
work based method to combine outputs from
multiple MT systems. In this approach, ar-
bitrary features may be added log-linearly
into the objective function, thus allowing
language model expansion and re-scoring.
Also, a novel method to automatically se-
lect the hypothesis which other hypotheses
are aligned against is proposed. A generic
weight tuning algorithm may be used to op-
timize various automatic evaluation metrics
including TER, BLEU and METEOR. The
experiments using the 2005 Arabic to En-
glish and Chinese to English NIST MT eval-
uation tasks show significant improvements
in BLEU scores compared to earlier confu-
sion network decoding based methods.
1 Introduction
System combination has been shown to improve
classification performance in various tasks. There
are several approaches for combining classifiers. In
ensemble learning, a collection of simple classifiers
is used to yield better performance than any single
classifier; for example boosting (Schapire, 1990).
Another approach is to combine outputs from a few
highly specialized classifiers. The classifiers may
be based on the same basic modeling techniques
but differ by, for example, alternative feature repre-
sentations. Combination of speech recognition out-
puts is an example of this approach (Fiscus, 1997).
In speech recognition, confusion network decoding
(Mangu et al, 2000) has become widely used in sys-
tem combination.
Unlike speech recognition, current statistical ma-
chine translation (MT) systems are based on various
different paradigms; for example phrasal, hierarchi-
cal and syntax-based systems. The idea of combin-
ing outputs from different MT systems to produce
consensus translations in the hope of generating bet-
ter translations has been around for a while (Fred-
erking and Nirenburg, 1994). Recently, confusion
network decoding for MT system combination has
been proposed (Bangalore et al, 2001). To generate
confusion networks, hypotheses have to be aligned
against each other. In (Bangalore et al, 2001), Lev-
enshtein alignment was used to generate the net-
work. As opposed to speech recognition, the word
order between two correct MT outputs may be dif-
ferent and the Levenshtein alignment may not be
able to align shifted words in the hypotheses. In
(Matusov et al, 2006), different word orderings are
taken into account by training alignment models by
considering all hypothesis pairs as a parallel corpus
using GIZA++ (Och and Ney, 2003). The size of
the test set may influence the quality of these align-
ments. Thus, system outputs from development sets
may have to be added to improve the GIZA++ align-
ments. A modified Levenshtein alignment allowing
shifts as in computation of the translation edit rate
(TER) (Snover et al, 2006) was used to align hy-
312
potheses in (Sim et al, 2007). The alignments from
TER are consistent as they do not depend on the test
set size. Also, a more heuristic alignment method
has been proposed in a different system combina-
tion approach (Jayaraman and Lavie, 2005). A full
comparison of different alignment methods would
be difficult as many approaches require a significant
amount of engineering.
Confusion networks are generated by choosing
one hypothesis as the ?skeleton?, and other hypothe-
ses are aligned against it. The skeleton defines the
word order of the combination output. Minimum
Bayes risk (MBR) was used to choose the skeleton
in (Sim et al, 2007). The average TER score was
computed between each system?s   -best hypothesis
and all other hypotheses. The MBR hypothesis is
the one with the minimum average TER and thus,
may be viewed as the closest to all other hypothe-
ses in terms of TER. This work was extended in
(Rosti et al, 2007) by introducing system weights
for word confidences. However, the system weights
did not influence the skeleton selection, so a hypoth-
esis from a system with zero weight might have been
chosen as the skeleton. In this work, confusion net-
works are generated by using the   -best output from
each system as the skeleton, and prior probabili-
ties for each network are estimated from the average
TER scores between the skeleton and other hypothe-
ses. All resulting confusion networks are connected
in parallel into a joint lattice where the prior proba-
bilities are also multiplied by the system weights.
The combination outputs from confusion network
decoding may be ungrammatical due to alignment
errors. Also the word-level decoding may break
coherent phrases produced by the individual sys-
tems. In this work, log-posterior probabilities are
estimated for each confusion network arc instead of
using votes or simple word confidences. This allows
a log-linear addition of arbitrary features such as
language model (LM) scores. The LM scores should
increase the total log-posterior of more grammatical
hypotheses. Powell?s method (Brent, 1973) is used
to tune the system and feature weights simultane-
ously so as to optimize various automatic evaluation
metrics on a development set. Tuning is fully auto-
matic, as opposed to (Matusov et al, 2006) where
global system weights were set manually.
This paper is organized as follows. Three evalu-
ation metrics used in weights tuning and reporting
the test set results are reviewed in Section 2. Sec-
tion 3 describes confusion network decoding for MT
system combination. The extensions to add features
log-linearly and improve the skeleton selection are
presented in Sections 4 and 5, respectively. Section
6 details the weights optimization algorithm and the
experimental results are reported in Section 7. Con-
clusions and future work are discussed in Section 8.
2 Evaluation Metrics
Currently, the most widely used automatic MT eval-
uation metric is the NIST BLEU-4 (Papineni et al,
2002). It is computed as the geometric mean of  -
gram precisions up to  -grams between the hypoth-
esis  and reference  as follows

	
 (1)
ffProceedings of the Third Workshop on Statistical Machine Translation, pages 183?186,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Incremental Hypothesis Alignment for Building Confusion Networks with
Application to Machine Translation System Combination
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
 
arosti,bzhang,smatsouk,schwartz  @bbn.com
Abstract
Confusion network decoding has been the
most successful approach in combining out-
puts from multiple machine translation (MT)
systems in the recent DARPA GALE and
NIST Open MT evaluations. Due to the vary-
ing word order between outputs from differ-
ent MT systems, the hypothesis alignment
presents the biggest challenge in confusion
network decoding. This paper describes an
incremental alignment method to build confu-
sion networks based on the translation edit rate
(TER) algorithm. This new algorithm yields
significant BLEU score improvements over
other recent alignment methods on the GALE
test sets and was used in BBN?s submission to
the WMT08 shared translation task.
1 Introduction
Confusion network decoding has been applied in
combining outputs from multiple machine transla-
tion systems. The earliest approach in (Bangalore
et al, 2001) used edit distance based multiple string
alignment (MSA) (Durbin et al, 1988) to build the
confusion networks. The recent approaches used
pair-wise alignment algorithms based on symmetric
alignments from a HMM alignment model (Matusov
et al, 2006) or edit distance alignments allowing
shifts (Rosti et al, 2007). The alignment method
described in this paper extends the latter by incre-
mentally aligning the hypotheses as in MSA but also
allowing shifts as in the TER alignment.
The confusion networks are built around a ?skele-
ton? hypothesis. The skeleton hypothesis defines
the word order of the decoding output. Usually, the
1-best hypotheses from each system are considered
as possible skeletons. Using the pair-wise hypoth-
esis alignment, the confusion networks are built in
two steps. First, all hypotheses are aligned against
the skeleton independently. Second, the confusion
networks are created from the union of these align-
ments. The incremental hypothesis alignment algo-
rithm combines these two steps. All words from the
previously aligned hypotheses are available, even if
not present in the skeleton hypothesis, when align-
ing the following hypotheses. As in (Rosti et al,
2007), confusion networks built around all skeletons
are joined into a lattice which is expanded and re-
scored with language models. System weights and
language model weights are tuned to optimize the
quality of the decoding output on a development set.
This paper is organized as follows. The incre-
mental TER alignment algorithm is described in
Section 2. Experimental evaluation comparing the
incremental and pair-wise alignment methods are
presented in Section 3 along with results on the
WMT08 Europarl test sets. Conclusions and future
work are presented in Section 4.
2 Incremental TER Alignment
The incremental hypothesis alignment is based on
an extension of the TER algorithm (Snover et al,
2006). The extension allows using a confusion net-
work as the reference. First, the algorithm finds the
minimum edit distance between the hypothesis and
the reference network by considering all word arcs
between two consecutive nodes in the reference net-
work as possible matches for a hypothesis word at
183
1 2 3 4 5 6I (3)
NULL (2)
like (3)
NULL (2)
big blue (1)
balloons (2)
blue (1) kites (1)
Figure 1: Network after pair-wise TER alignment.
that position. Second, shifts of blocks of words that
have an exact match somewhere else in the network
are tried in order to find a new hypothesis word or-
der with a lower TER. Each shifted block is con-
sidered a single edit. These two steps are executed
iteratively as a greedy search. The final alignment
between the re-ordered hypothesis and the reference
network may include matches, substitutions, dele-
tions, and insertions.
The confusion networks are built by creating a
simple confusion network from the skeleton hypoth-
esis. If the skeleton hypothesis has   words, the
initial network has   arcs and   nodes. Each
arc has a set of system specific confidence scores.
The score for the skeleton system is set to  and
the confidences for other systems are set to zeros.
For each non-skeleton hypothesis, a TER alignment
against the current network is executed as described
above. Each match found will increase the system
specific word arc confidence by 
	 where 
is the rank of the hypothesis in that system?s   -best
list. Each substitution will generate a new word arc
at the corresponding position in the network. The
word arc confidence for the system is set to 
	
and the confidences for other systems are set to ze-
ros. Each deletion will generate a new NULL word
arc unless one exists at the corresponding position
in the network. The NULL word arc confidences are
adjusted as in the case of a match or a substitution
depending on whether the NULL word arc exists or
not. Finally, each insertion will generate a new node
and two word arcs at the corresponding position in
the network. The first word arc will have the in-
serted word with the confidence set as in the case
of a substitution and the second word arc will have
a NULL word with confidences set by assuming all
previously aligned hypotheses and the skeleton gen-
erated the NULL word arc.
After all hypotheses have been added into the con-
fusion network, the system specific word arc confi-
dences are scaled to sum to one over all arcs between
1 2 3 4 5 6I (3) like (3)
kites (1)
NULL (2) NULL (1)
big (1) blue (2)
balloons (2)
Figure 2: Network after incremental TER alignment.
each set of two consecutive nodes. Other scores for
the word arc are set as in (Rosti et al, 2007).
2.1 Benefits over Pair-Wise TER Alignment
The incremental hypothesis alignment guarantees
that insertions between a hypothesis and the cur-
rent confusion network are always considered when
aligning the following hypotheses. This is not the
case in any pair-wise hypothesis alignment algo-
rithm. During the pair-wise hypothesis alignment,
an identical word in two hypotheses may be aligned
as an insertion or a substitution in a different posi-
tion with respect to the skeleton. This will result in
undesirable repetition and lower confidence for that
word in the final confusion network. Also, multiple
insertions are not handled implicitly.
For example, three hypotheses ?I like balloons?,
?I like big blue balloons?, and ?I like blue kites?
might be aligned by the pair-wise alignment, assum-
ing the first as the skeleton, as follows:
I like NULL balloons NULL
I like big blue balloons NULL
I like NULL balloons NULL
I like NULL blue kites
which results in the confusion network shown in
Figure 1. The number of hypotheses proposing each
word is shown in parentheses. The alignment be-
tween the skeleton and the second hypothesis has
two consecutive insertions ?big blue? which are not
available for matching when the third hypothesis is
aligned against the skeleton. Therefore, the word
?blue? appears twice in the confusion network. If
many hypotheses have multiple insertions at the
same location with respect to the skeleton, they have
to be treated as phrases or a secondary alignment
process has to be applied.
Assuming the same hypotheses as above, the in-
cremental hypothesis alignment may yield the fol-
lowing alignment:
184
System TER BLEU MTR
worst 53.26 33.00 63.15
best 42.30 48.52 67.71
syscomb pw 39.85 52.00 68.73
syscomb giza 40.01 52.24 68.68
syscomb inc 39.25 52.73 68.97
oracle 21.68 64.14 78.18
Table 1: Results on the Arabic GALE Phase 2 system
combination tuning set with four reference translations.
I like NULL NULL balloons
I like big blue balloons
I like NULL blue kites
which results in the confusion network shown in
Figure 2. In this case the word ?blue? is available
for matching when the third hypothesis is aligned.
It should be noted that the final confusion network
depends on the order in which the hypotheses are
added. The experiments so far have indicated that
different alignment order does not have a significant
influence on the final combination results as mea-
sured by the automatic evaluation metrics. Usually,
aligning the system outputs in the decreasing order
of their TER scores on the development set yields
the best scores.
2.2 Confusion Network Oracle
The extended TER algorithm can also be used to
estimate an oracle TER in a confusion network by
aligning the reference translations against the con-
fusion network. The oracle hypotheses can be ex-
tracted by finding a path with the maximum number
of matches. These hypotheses give a lower bound
on the TER score for the hypotheses which can be
generated from the confusion networks.
3 Experimental Evaluation
The quality of the final combination output depends
on many factors. Combining very similar outputs
does not yield as good gains as combining out-
puts from diverse systems. It is also important that
the development set used to tune the combination
weights is as similar to the evaluation set as possi-
ble. This development set should be different from
the one used to tune the individual systems to avoid
bias toward any system that may be over-tuned. Due
System TER BLEU MTR
worst 59.09 20.74 57.24
best 48.18 31.46 62.61
syscomb pw 46.31 33.02 63.18
syscomb giza 46.03 33.39 63.21
syscomb inc 45.45 33.90 63.45
oracle 27.53 49.10 71.81
Table 2: Results on the Arabic GALE Phase 2 evaluation
set with one reference translation.
to the tight schedule for the WMT08, there was no
time to experiment with many configurations. As
more extensive experiments have been conducted in
the context of the DARPA GALE program, results
on the Arabic GALE Phase 2 evaluation setup are
first presented. The translation quality is measured
by three MT evaluation metrics: TER (Snover et al,
2006), BLEU (Papineni et al, 2002), and METEOR
(Lavie and Agarwal, 2007).
3.1 Results on Arabic GALE Outputs
For the Arabic GALE Phase 2 evaluation, nine sys-
tems were combined. Five systems were phrase-
based, two hierarchical, one syntax-based, and one
rule-based. All statistical systems were trained on
common parallel data, tuned on a common genre
specific development set, and a common English to-
kenization was used. The English bi-gram and 5-
gram language models used in the system combina-
tion were trained on about 7 billion words of English
text. Three iterations of bi-gram decoding weight
tuning were performed followed by one iteration of
5-gram re-scoring weight tuning. All weights were
tuned to minimize the sum of TER and 1-BLEU.
The final 1-best outputs were true-cased and deto-
kenized before scoring.
The results on the newswire system combination
development set and the GALE Phase 2 evaluation
set are shown in Tables 1 and 2. The first two
rows show the worst and best scores from the in-
dividual systems. The scores may be from different
systems as the best performing system in terms of
TER was not necessarily the best performing system
in terms of the other metrics. The following three
rows show the scores of three combination outputs
where the only difference was the hypothesis align-
ment method. The first, syscomb pw, corresponds
185
BLEU
System de-en fr-en
worst 11.84 16.31
best 28.30 33.13
syscomb 29.05 33.63
Table 3: NIST BLEU scores on the German-English (de-
en) and French-English (fr-en) Europarl test2008 set.
to the pair-wise TER alignment described in (Rosti
et al, 2007). The second, syscomb giza, cor-
responds to the pair-wise symmetric HMM align-
ments from GIZA++ described in (Matusov et al,
2006). The third, syscomb inc, corresponds to
the incremental TER alignment presented in this pa-
per. Finally, oracle corresponds to an estimate of
the lower bound on the translation quality obtained
by extracting the TER oracle output from the con-
fusion networks generated by the incremental TER
alignment. It is unlikely that there exists a set of
weights that would yield the oracle output after de-
coding, though. The incremental TER alignment
yields significant improvements over all individual
systems and the combination outputs using the pair-
wise alignment methods.
3.2 Results on WMT08 Europarl Outputs
On the WMT08 shared translation task, transla-
tions for two language pairs and two tasks were
provided for the system combination experiments.
Twelve systems participated in the German-English
and fourteen in the French-English translation tasks.
The translations of the Europarl test (test2008) were
provided as the development set outputs and the
translations of the News test (newstest2008) were
provided as the evaluation set outputs. An English
bi-gram, 4-gram, and true-caser language models
were trained by using all English text available for
the WMT08 shared task, including Europarl mono-
lingual and news commentary parallel training sets.
The outputs were tokenized and lower-cased before
combination, and the final combination output was
true-cased and detokenized.
The results on the Europarl test set for both lan-
guage pairs are shown in table 3. The first two rows
have the NIST BLEU scores of the worst and the
best individual systems. The last row, syscomb,
corresponds to the system combination using the in-
cremental TER alignment. The improvements in the
NIST BLEU scores are fairly modest which is prob-
ably due to low diversity of the system outputs. It is
also unlikely that these weights are optimal for the
out-of-domain News test set outputs.
4 Conclusions
This paper describes a novel hypothesis alignment
algorithm for building confusion networks from
multiple machine translation system outputs. The al-
gorithm yields significant improvements on the Ara-
bic GALE evaluation set outputs and was used in
BBN?s submission to the WMT08 shared translation
task. The hypothesis alignment may benefit from
using stemming and synonymy in matching words.
Also, special handling of punctuation may improve
the alignment further. The future work will inves-
tigate the influence of better alignment to the final
combination outputs.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program.
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In Proc. ASRU, pages 351?354.
R. Durbin, S.R. Eddy, A. Krogh, and G. Mitchison. 1988.
Biological Sequence Analysis: Probabilistic Models of
Proteins and Nucleic Acids. Cambridge Univ. Press.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for MT evaluation with high levels of cor-
relation with human judgments. In Proc. ACL/WMT,
pages 228?231.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
Proc. EACL, pages 33?40.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311?318.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. In Proc. ACL 2007, pages 312?319.
M. Snover, B. Dorr, R. Schwartz, L. Micciula, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA, pages
223?231.
186
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 61?65,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Incremental Hypothesis Alignment with Flexible Matching for Building
Confusion Networks: BBN System Description for WMT09 System
Combination Task
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
 
arosti,bzhang,smatsouk,schwartz  @bbn.com
Abstract
This paper describes the incremental hy-
pothesis alignment algorithm used in the
BBN submissions to the WMT09 system
combination task. The alignment algo-
rithm used a sentence specific alignment
order, flexible matching, and new shift
heuristics. These refinements yield more
compact confusion networks compared to
using the pair-wise or incremental TER
alignment algorithms. This should reduce
the number of spurious insertions in the
system combination output and the sys-
tem combination weight tuning converges
faster. System combination experiments
on the WMT09 test sets from five source
languages to English are presented. The
best BLEU scores were achieved by comb-
ing the English outputs of three systems
from all five source languages.
1 Introduction
Machine translation (MT) systems have different
strengths and weaknesses which can be exploited
by system combination methods resulting in an
output with a better performance than any indi-
vidual MT system output as measured by auto-
matic evaluation metrics. Confusion network de-
coding has become the most popular approach to
MT system combination. The first confusion net-
work decoding method (Bangalore et al, 2001)
was based on multiple string alignment (MSA)
(Durbin et al, 1988) borrowed from biological
sequence analysis. However, MSA does not al-
low re-ordering. The translation edit rate (TER)
(Snover et al, 2006) produces an alignment be-
tween two strings and allows shifts of blocks of
words. The availability of the TER software has
made it easy to build a high performance system
combination baseline (Rosti et al, 2007).
The pair-wise TER alignment originally de-
scribed by Sim et al (2007) has various limita-
tions. First, the hypotheses are aligned indepen-
dently against the skeleton which determines the
word order of the output. The same word from
two different hypotheses may be inserted in differ-
ent positions w.r.t. the skeleton and multiple inser-
tions require special handling. Rosti et al (2008)
described an incremental TER alignment to miti-
gate these problems. The incremental TER align-
ment used a global order in which the hypotheses
were aligned. Second, the TER software matches
words with identical surface strings. The pair-
wise alignment methods proposed by Ayan et al
(2008), He et al (2008), and Matusov et al (2006)
are able to match also synonyms and words with
identical stems. Third, the TER software uses a set
of heuristics which is not always optimal in de-
termining the block shifts. Karakos et al (2008)
proposed using inversion transduction grammars
to produce different pair-wise alignments.
This paper is organized as follows. A refined
incremental alignment algorithm is described in
Section 2. Experimental evaluation comparing
the pair-wise and incremental TER alignment al-
gorithms with the refined alignment algorithm on
WMT09 system combination task is presented in
Section 3. Conclusions and future work are pre-
sented in Section 4.
2 Incremental Hypothesis Alignment
with Flexible Matching
2.1 Sentence Specific Alignment Order
Rosti et al (2008) proposed incremental hypothe-
sis alignment using a system specific order. This
is not likely to be optimal since one MT system
may have better output on one sentence and worse
on another. More principled approach is similar to
MSA where the order is determined by the edit
distance of the hypothesis from the network for
61
17
0
1NULL(6.2e-7)
9
NULL(0.9999)
2cereal
NULL
3
thomas
4
jefferson
edison
5
says
6
eat
7
your
8
NULL
vegetables
NULL
10
eat
11
your 12cereal
NULL
13
thomas 14
edison
jefferson
15
says 16vegetables
NULL
NULL
(a) Alignment using the standard TER shift heuristics.
150
1NULL(0.5)
8
NULL(0.5)
2
thomas
3jefferson
edison
4
says
5
eat
6
your
7
vegetables
cereal NULL
9
eat
10
your
11
cereal
vegetables
12
thomas 13edison
jefferson
14
says
NULL
(b) Alignment using the modified shift heuristics.
Figure 1: Combined confusion networks using different shift heuristics. The initial NULL arcs include
the prior probability estimates in parentheses.
each sentence. The TER scores of the remaining
unaligned hypotheses using the current network as
the reference are computed. The hypothesis with
the lowest edit cost w.r.t. the network is aligned.
Given  systems, this increases the number of
alignments performed from  to 	
 .
2.2 Flexible Matching
The TER software assigns a zero cost for match-
ing tokens and a cost of one for all errors includ-
ing insertions, deletions, substitutions, and block
shifts. Ayan et al (2008) modified the TER soft-
ware to consider substitutions of synonyms with
a reduced cost. Recently, Snover et al (2009)
extended the TER algorithm in a similar fashion
to produce a new evaluation metric, TER plus
(TERp), which allows tuning of the edit costs in
order to maximize correlation with human judg-
ment. The incremental alignment with flexible
matching uses WordNet (Fellbaum, 1998) to find
all possible synonyms and words with identical
stems in a set of hypotheses. Substitutions involv-
ing synonyms and words with identical stems are
considered with a reduced cost of 0.2.
2.3 Modified Shift Heuristics
The TER is computed by trying shifts of blocks of
words that have an exact match somewhere else in
the reference in order to find a re-ordering of the
hypothesis with a lower edit distance to the refer-
ence. Karakos et al (2008) showed that the shift
heuristics in TER do not always yield an optimal
alignment. Their example used the following two
hypotheses:
1. thomas jefferson says eat your vegetables
2. eat your cereal thomas edison says
A system combination lattice using TER align-
ment is shown in Figure 1(a). The blocks
?eat your? are shifted when building both con-
fusion networks. Using the second hypothe-
sis as the skeleton seems to give a better align-
ment. The lower number of edits also results in a
higher skeleton prior shown between nodes 0 and
9. There are obviously some undesirable paths
through the lattice but it is likely that a language
model will give a higher score to the reasonable
hypotheses.
Since the flexible matching allows substitutions
with a reduced cost, the standard TER shift heuris-
tics have to be modified. A block of words may
have some words with identical matches and other
words with synonym matches. In TERp, synonym
and stem matches are considered as exact matches
for the block shifts, otherwise the TER shift con-
straints are used. In the flexible matching, the shift
heuristics were modified to allow any block shifts
62
that do not increase the edit cost. A system combi-
nation lattice using the modified shift heuristics is
shown in Figure 1(b). The optimal shifts of blocks
?eat your cereal? and ?eat your vegetables? were
found and both networks received equal skeleton
priors. TERp would yield this alignment only
if these blocks appear in the paraphrase table or
if ?cereal? and ?vegetables? are considered syn-
onyms. This example is artificial and does not
guarantee that optimal shifts are always found.
3 Experimental Evaluation
System combination experiments combining the
English WMT09 translation task outputs were per-
formed. A total of 96 English outputs were pro-
vided including primary, contrastive, and  -best
outputs. Only the primary  -best outputs were
combined due to time constraints. The numbers
of primary systems per source language were: 3
for Czech, 15 for German, 9 for Spanish, 15 for
French, and 3 for Hungarian. The English bigram
and 5-gram language models were interpolated
from four LM components trained on the English
monolingual Europarl (45M tokens) and News
(510M tokens) corpora, and the English sides of
the News Commentary (2M tokens) and Giga-
FrEn (683M tokens) parallel corpora. The interpo-
lation weights were tuned to minimize perplexity
on news-dev2009 set. The system combination
weights ? one for each system, LM weight, and
word and NULL insertion penalties ? were tuned
to maximize the BLEU (Papineni et al, 2002)
score on the tuning set (newssyscomb2009).
Since the system combination was performed on
tokenized and lower cased outputs, a trigram-
based true caser was trained on all News training
data. The tuning may be summarized as follows:
1. Tokenize and lower case the outputs;
2. Align hypotheses incrementally using each
output as a skeleton;
3. Join the confusion networks into a lattice
with skeleton specific prior estimates;
4. Extract a  -best list from the lattice given
the current weights;
5. Merge the  -best list with the hypotheses
from the previous iteration;
6. Tune new weights given the current merged
 -best list;
7. Iterate 4-6 three times;
8. Extract a  -best list from the lattice given
the best decoding weights and re-score hy-
potheses with a 5-gram;
9. Tune re-scoring weights given the final  -
best list;
10. Extract  -best hypotheses from the  -best
list given the best re-scoring weights, re-case,
and detokenize.
After tuning the system combination weights, the
outputs on a test set may be combined using the
same steps excluding 4-7 and 9. The hypothesis
scores and tuning are identical to the setup used in
(Rosti et al, 2007).
Case insensitive TER and BLEU scores for the
combination outputs using the pair-wise and in-
cremental TER alignment as well as the flexible
alignment on the tuning (dev) and test sets are
shown in Table 1. Only case insensitive scores
are reported since the re-casers used by different
systems are very different and some are trained
using larger resources than provided for WMT09.
The scores of the worst and best individual sys-
tem outputs are also shown. The best and worst
TER and BLEU scores are not necessarily from
the same system output. Both incremental
and flexible alignments used sentence spe-
cific alignment order. Combinations using the in-
cremental and flexible hypothesis alignment algo-
rithms consistently outperform the ones using the
pair-wise TER alignment. The flexible alignment
is slightly better than the incremental alignment on
Czech, Spanish, and Hungarian, and significantly
better on French to English test set scores.
Since the test sets for each language pair consist
of translations of the same documents, it is pos-
sible to combine outputs from many source lan-
guages to English. There were a total of 46 En-
glish primary  -best system outputs. Using all 46
outputs would have required too much memory in
tuning, so a subset of 11 outputs was chosen. The
11 outputs consist of google, uedin, and uka
outputs on all languages. Case insensitive TER
and BLEU scores for the xx-en combination are
shown in Table 2. In addition to incremental
and flexible alignment methods which used
sentence specific alignment order, scores for in-
cremental TER alignment with a fixed alignment
order used in the BBN submissions to WMT08
63
dev cz-en de-en es-en fr-en hu-en
System TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
worst 67.30 17.63 82.01 6.83 65.64 19.74 69.19 15.21 78.70 10.33
best 58.16 23.12 57.24 23.20 53.02 29.48 49.78 32.27 66.77 13.59
pairwise 59.60 24.01 56.35 26.04 53.11 29.49 51.03 31.65 69.58 14.60
incremental 59.22 24.31 55.73 26.73 53.05 29.72 50.72 32.09 70.15 14.85
flexible 59.38 24.18 55.51 26.71 52.62 30.24 50.22 32.58 69.83 14.88
test cz-en de-en es-en fr-en hu-en
System TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
worst 67.74 16.37 82.39 6.81 65.44 19.04 71.44 14.49 81.21 9.90
best 59.53 21.18 59.41 21.30 53.34 28.69 51.33 31.14 68.32 12.75
pairwise 61.02 21.25 58.75 23.41 53.65 28.15 53.17 29.83 71.50 13.39
incremental 60.63 21.67 58.13 23.96 53.47 28.38 52.51 30.45 71.69 13.60
flexible 60.34 21.87 58.05 23.86 53.13 28.57 51.98 31.30 71.17 13.84
Table 1: Case insensitive TER and BLEU scores on newssyscomb2009 (dev) and newstest2009
(test) for five source languages.
(Rosti et al, 2008) are marked as incr-wmt08.
The sentence specific alignment order yields about
a half BLEU point gain on the tuning set and a
one BLEU point gain on the test set. All system
combination experiments yield very good BLEU
gains on both sets. The scores are also signifi-
cantly higher than any combination from a single
source language. This shows that the outputs from
different source languages are likely to be more di-
verse than outputs from different MT systems on a
single language pair. The combination is not guar-
anteed to be the best possible as the set of outputs
was chosen arbitrarily.
The compactness of the confusion networks
may be measured by the average number of
nodes and arcs per segment. All xx-en con-
fusion networks for newssyscomb2009 and
newstest2009 after the incremental TER
alignment had on average 44.5 nodes and 112.7
arcs per segment. After the flexible hypothesis
alignment, there were on average 41.1 nodes and
104.6 arcs per segment. The number of NULL
word arcs may also be indicative of the alignment
quality. The flexible hypothesis alignment reduced
the average number of NULL word arcs from 29.0
to 24.8 per segment. The rate of convergence in
the  -best list based iterative tuning may be mon-
itored by the number of new hypotheses in the
merged  -best lists from iteration to iteration. By
the third tuning iteration, there were 10% fewer
new hypotheses in the merged  -best list when
using the flexible hypothesis alignment.
xx-en dev test
System TER BLEU TER BLEU
worst 74.21 12.80 75.84 12.05
best 49.78 32.27 51.33 31.14
pairwise 46.10 35.95 47.77 33.53
incr-wmt08 44.58 36.84 46.60 33.61
incremental 44.59 37.30 46.42 34.61
flexible 44.54 37.38 45.82 34.48
Table 2: Case insensitive TER and BLEU
scores on newssyscomb2009 (dev) and
newstest2009 (test) for xx-en combination.
4 Conclusions
This paper described a refined incremental hy-
pothesis alignment algorithm used in the BBN
submissions to the WMT09 system combination
task. The new features included sentence specific
alignment order, flexible matching, and modified
shift heuristics. The refinements yield more com-
pact confusion networks which should allow fewer
spurious insertions in the output and faster conver-
gence in tuning. The future work will investigate
tunable edit costs and methods to choose an opti-
mal subset of outputs for combination.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
64
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 33?
40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proceed-
ings of the Automatic Speech Recognition and Un-
derstanding Workshop (ASRU), pages 351?354.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1988. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107.
Damianos Karakos, Jason Eisner, Sanjeev Khundan-
pur, and Markus Dreyer. 2008. Machine trans-
lation system combination using ITG-based align-
ments. In Proceedings of ACL-08: HLT, pages 81?
84.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Proceedings of the 11th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 33?40.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine
translation system combination. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 105?108.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation.
65
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 667?673,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Combining Unsupervised and Supervised Alignments for MT:
An Empirical Study
Jinxi Xu and Antti-Veikko I. Rosti
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{jxu,arosti}@bbn.com
Abstract
Word alignment plays a central role in statisti-
cal MT (SMT) since almost all SMT systems
extract translation rules from word aligned
parallel training data. While most SMT
systems use unsupervised algorithms (e.g.
GIZA++) for training word alignment, super-
vised methods, which exploit a small amount
of human-aligned data, have become increas-
ingly popular recently. This work empirically
studies the performance of these two classes
of alignment algorithms and explores strate-
gies to combine them to improve overall sys-
tem performance. We used two unsupervised
aligners, GIZA++ and HMM, and one super-
vised aligner, ITG, in this study. To avoid lan-
guage and genre specific conclusions, we ran
experiments on test sets consisting of two lan-
guage pairs (Chinese-to-English and Arabic-
to-English) and two genres (newswire and we-
blog). Results show that the two classes of al-
gorithms achieve the same level of MT perfor-
mance. Modest improvements were achieved
by taking the union of the translation gram-
mars extracted from different alignments. Sig-
nificant improvements (around 1.0 in BLEU)
were achieved by combining outputs of differ-
ent systems trained with different alignments.
The improvements are consistent across lan-
guages and genres.
1 Introduction
Word alignment plays a central role in training sta-
tistical machine translation (SMT) systems since al-
most all SMT systems extract translation rules from
word aligned parallel training data. Until recently,
most SMT systems used GIZA++ (Och and Ney,
2003), an unsupervised algorithm, for aligning par-
allel training data. In recent years, with the availabil-
ity of human aligned training data, supervised meth-
ods (e.g. the ITG aligner (Haghighi et al, 2009))
have become increasingly popular.
The main objective of this work is to show the
two classes (unsupervised and supervised) of al-
gorithms are complementary and combining them
will improve overall system performance. The use
of human aligned training data allows supervised
methods such as ITG to more accurately align fre-
quent words, such as the alignments of Chinese par-
ticles (e.g. ?bei?, ?de?, etc) to their English equiv-
alents (e.g. ?is/are/was/..?, ?of?, etc). On the other
hand, supervised methods can be affected by sub-
optimal alignments in hand-aligned data. For exam-
ple, the hand-aligned data used in our experiments
contain some coarse-grained alignments (e.g. ?lian-
he guo? to ?United Nations?) although fine-grained
alignments (?lian-he? to ?United? and ?guo? to ?Na-
tions?) are usually more appropriate for SMT. Un-
supervised methods are less likely to be affected
by this problem. We used two well studied unsu-
pervised aligners, GIZA++ (Och and Ney, 2003)
and HMM (Liang et al, 2006) and one supervised
aligner, ITG (Haghighi et al, 2009) as representa-
tives in this work.
We explored two techniques to combine different
alignment algorithms. One is to take the union of
the translation rules extracted from alignments pro-
duced by different aligners. This is motivated by
studies that showed that the coverage of translation
rules is critical to SMT (DeNeefe et al, 2007). The
667
other method is to combine the outputs of different
MT systems trained using different aligners. As-
suming different systems make independent errors,
system combination can generate a better transla-
tion than those of individual systems through voting
(Rosti et al, 2007).
Our work differs from previous work in two ways.
Past studies of combining alternative alignments fo-
cused on minimizing alignment errors, usually by
merging alternative alignments for a sentence pair
into a single alignment with the fewest number of
incorrect alignment links (Ayan and Dorr, 2006). In
contrast, our work is based on the assumption that
perfect word alignment is impossible due to the in-
trinsic difficulty of the problem, and it is more effec-
tive to resolve translation ambiguities at later stages
of the MT pipeline. A main focus of much previous
work on word alignments is on theoretical aspects
of the proposed algorithms. In contrast, the nature
of this work is purely empirical. Our system was
trained on a large amount of training data and evalu-
ated on multiple languages (Chinese-to-English and
Arabic-to-English) and multiple genres (newswire
and weblog). Furthermore, we used a state of the art
string-to-tree decoder (Shen et al, 2008) to estab-
lish the strongest possible baseline. In comparison,
experiments in previous studies typically used one
language pair and one genre (usually newswire), a
reduced amount of training data and a phrase based
decoder.
This paper is organized as follows. Section 2 de-
scribes the three alignment algorithms. Section 3
describes the two methods used to combine these
aligners to improve MT. The experimental setup
used to compare these methods is presented in Sec-
tion 4. Section 5 shows the results including a dis-
cussion. Section 6 discusses related work. Section 7
concludes the paper.
2 Alignment Algorithms
We used three aligners in this work: GIZA++ (Och
and Ney, 2003), jointly trained HMM (Liang et al,
2006), and ITG (Haghighi et al, 2009). GIZA++
is an unsupervised method based on models 1-5 of
Brown et al (1993). Given a sentence pair e ? f ,
it seeks the alignment a that maximizes the proba-
bility P (f, a|e). As in most previous studies using
GIZA++, we ran GIZA++ in both directions, from e
to f and from f to e, and symmetrized the bidirec-
tional alignments into one, using a method similar
to the grow-diagonal-final method described in Och
and Ney (2003). We ran GIZA++ up to model 4.
The jointly trained HMM aligner, or HMM for
short, is also unsupervised but it uses a small amount
of hand-aligned data to tweak a few high level pa-
rameters. Low level parameters are estimated in an
unsupervised manner like GIZA++.
The ITG aligner is a supervised method whose pa-
rameters are tuned to optimize alignment accuracy
on hand-aligned data. It uses the inversion transduc-
tion grammar (ITG) (Wu, 1997) to narrow the space
of possible alignments. Since the ITG aligner uses
features extracted from HMM alignments, HMM
was run as a prepossessing step in our experiments.
Both the HMM and ITG aligners are publicly avail-
able1.
3 Methods of Combining Alternative
Alignments for MT
We explored two methods of combining alternative
alignments for MT. One is to extract translation rules
from the three alternative alignments and take the
union of the three sets of rules as the single transla-
tion grammar. Procedurally, this is done by concate-
nating the alignment files before extracting transla-
tion rules. We call this method unioned grammar.
This method greatly increases the coverage of the
rules, as the unioned translation grammar has about
80% more rules than the ones extracted from the in-
dividual alignment in our experiments. As such, de-
coding is also slower.
The other is to use system combination to com-
bine outputs of systems trained using different align-
ers. Due to differences in the alignment algorithms,
these systems would produce different hypotheses
with independent errors. Combining a diverse set
of hypotheses could improve overall system perfor-
mance. While system combination is a well-known
technique, to our knowledge this work is the first to
apply it to explicitly exploit complementary align-
ment algorithms on a large scale.
Since system combination is an established tech-
nique, here we only briefly discuss our system com-
1http://code.google.com/p/berkeleyaligner/
668
bination setup. The basic algorithm was described in
Rosti et al (2007). In this work, we use incremental
hypothesis alignment with flexible matching (Rosti
et al, 2009) to produce the confusion networks. 10-
best lists from all systems are collected first. All
1-best hypotheses for each segment are used as con-
fusion network skeletons, the remaining hypotheses
are aligned to the confusion networks, and the result-
ing networks are connected in parallel into a joint
lattice with skeleton specific prior probabilities es-
timated from the alignment statistics on the initial
arcs. This lattice is expanded with an unpruned bi-
gram language model and the system combination
weights are tuned directly to maximize the BLEU
score of the 1-best decoding outputs. Given the
tuned system combination weights, a 300-best list
is extracted from the lattice, the hypotheses are re-
scored using an unpruned 5-gram language model,
and a second set of system combination weights is
tuned to maximize the BLEU score of the 1-best hy-
pothesis of the re-scored 300-best list. The same re-
scoring step is also applied to the outputs of individ-
ual systems.
4 Experiment Setup
To establish strong baselines, we used a string-to-
tree SMT system (Shen et al, 2008), one of the top
performing systems in the NIST 2009 MT evalua-
tion, and trained it with very large amounts of par-
allel and language model data. The system used
large sets of discriminatively tuned features (up to
55,000 on Arabic) inspired by the work of Chiang et
al. (2009). To avoid drawing language, genre, and
metric specific conclusions, we experimented with
two language pairs, Arabic-English and Chinese-
English, and two genres, newswire and weblog, and
report both BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) scores. Systems were tuned to
maximize BLEU on the tuning set using a procedure
described in Devlin (2009).
The sizes of the parallel training corpora are
238M words (target side) for Arabic-English MT
and 265M words for Chinese-English. While the
majority of the data is publicly available from the
Linguistic Data Consortium (LDC), some of the data
is available under the DARPA GALE program. Due
to the size of the parallel corpora, we divided them
into five chunks and aligned them in parallel to save
time. Due to its running complexity, we ran ITG
only on sentences with 60 or fewer words. For
longer sentences, we used HMM alignments instead,
which were conveniently generated in the prepro-
cessing step of ITG aligner. For language model
training, we used about 9 billion words of English
text, most of which are from English Gigaword cor-
pus and GoogleNews. Each system used a 3-gram
LM for decoding and a 5-gram LM for re-scoring.
The same 5-gram LM was also used for re-scoring
system combination results.
For each combination of language pair and genre,
we used three development sets:
? Tune, which was used to tune parameters of
individual MT systems. Each system was tuned
ten iterations based on BLEU.
? SysCombTune, which was used to tune pa-
rameters of system combination. A subset of it
was also used as validation for determining the
best iteration in tuning individual systems.
? Test, which was the blind test corpus for mea-
suring performances of both individual systems
and system combination.
Test materials were drawn from two sources:
NIST MT evaluations 2004 to 2008, and develop-
ment and evaluation data for the DARPA GALE pro-
gram. Due to the mixing of different data sources,
some test sentences have four reference translations
while the rest have only one. The average num-
ber of references per test sentence varies across test
sets. For this reason, MT scores are not comparable
across test sets. Table 1 shows the size and the av-
erage number of references per sentence of the test
sets.
Two hand-aligned corpora were used to train the
ITG aligner: LDC2009E82 (Arabic-English) and
LDC2009E83 (Chinese-English). We re-tokenized
the corpora using our tokenizers and projected the
LDC alignments to our tokenization heuristically.
The projection was not perfect and sometimes cre-
ated very coarse-grained alignments. We used a set
of filters to remove such problematic data. We ended
up with 3,667 Arabic-English and 879 Chinese-
English hand-aligned sentence pairs with sufficient
quality for training automatic aligners.
669
language and genre Tune SysCombTune Test
Arabic newswire 2963 (2.9) 3223 (2.7) 2242 (2.7)
Arabic web 4597 (1.5) 4526 (1.4) 2703 (2.7)
Chinese newswire 3085 (2.6) 3001 (2.7) 2055 (1.4)
Chinese web 4221 (1.3) 4285 (1.3) 3092 (1.2)
Table 1: Numbers of sentences and average number of references (in parentheses) of test sets
5 Results
Three baseline systems were trained using the three
different aligners. Case insensitive BLEU and TER
scores for Arabic newswire, Arabic weblog, Chi-
nese newswire, and Chinese weblog are shown in
Tables 2, 3, 4, and 5, respectively2. The BLEU
scores on the Test set are fairly similar but the
ordering between different alignment algorithms is
mixed between different languages and genres. To
compare the two alignment combination strategies,
we trained a system using the union of the rules ex-
tracted from the alternative alignments (union in
the tables) and a combination of the three baseline
system outputs (3 syscomb in the tables). The
system with the unioned grammar was also added
as an additional system in the combination marked
by 4 syscomb.
As seen in the tables, unioned grammar and sys-
tem combination improve MT on both languages
(Arabic and Chinese) and both genres (newswire
and weblog). While there are improvements on
both SysCombTune and Test, the results on
SysCombTune are not totally fair since it was used
for tuning system combination weights and as val-
idation for optimizing weights of the MT systems.
Therefore our discussion will focus on results on
Test. (We did not show scores on Tune because
systems were directly tuned on it.) Statistical sig-
nificance is determined at 95% confidence level us-
ing the bootstrap method described in Koehn (2004),
and is only applied on results obtained on the blind
Test set.
For unioned grammar, the overall improvement
in BLEU is modest, ranging from 0.1 to 0.6 point
2Dagger (?) indicates statistically better results than the best
individual alignment system. Double dagger (?) indicates sta-
tistically better results than both best individual alignment and
unioned grammar. Bold indicates best Test set performance
among individual alignment systems.
compared with the best baseline system, with little
change in TER score. The improvements in BLEU
score are statistically significant for Arabic (both
genres), but not for Chinese. The improvements in
TER are not significant for either language.
System combination produces bigger improve-
ments in performance. Compared with the best base-
line system, the improvement in BLEU ranges from
0.8 to 1.6 point. There are also noticeable improve-
ments in TER, around 1.0 point. The TER improve-
ments are mostly explained by the hypothesis align-
ment algorithm which is closely related to TER scor-
ing (Rosti et al, 2009). The results are interesting
because all three baseline systems (GIZA++, HMM
and ITG) are identical except for the word align-
ments used in rule extraction. The results confirm
that the aligners are indeed complementary, as we
conjectured earlier. Also, the four-system combi-
nation yields consistent gains over the three-system
combination, suggesting that the system using the
unioned grammar is somewhat complementary to
the three baseline systems. The statistical test in-
dicates that both the three and four system combi-
nations are significantly better than the single best
alignment system for all languages and genres in
BLEU and TER. In most cases, they are also sig-
nificantly better than unioned grammar.
Somewhat surprisingly, the GIZA++ trained sys-
tem is slightly better than the ITG trained system on
all genres but Chinese weblog. However, we should
point out that such a comparison is not entirely fair.
First, we only ran ITG on short sentences. (For long
sentences, we had to settle for HMM alignments for
computing reasons.) Second, the hand-aligned data
used for ITG training are not very clean, as we said
before. The ITG results could be improved if these
problems were not present.
670
SysCombTune Test
System BLEU TER BLEU TER
GIZA++ 51.31 38.01 50.96 38.38
HMM 50.87 38.49 50.84 38.87
ITG 51.04 38.44 50.69 38.94
union 51.55 37.93 51.53? 38.32
3 syscomb 52.66 37.20 52.43? 37.69?
4 syscomb 52.80 37.05 52.55? 37.46?
Table 2: MT results on Arabic newswire (see footnote 2).
SysCombTune Test
System BLEU TER BLEU TER
GIZA++ 27.49 55.00 38.00 49.55
HMM 27.42 55.53 37.81 50.12
ITG 27.19 55.32 37.77 49.94
union 27.66 54.82 38.43? 49.43
3 syscomb 27.65 53.89 38.70? 48.72?
4 syscomb 27.83 53.68 38.82? 48.53?
Table 3: MT results on Arabic weblog (see footnote 2).
SysCombTune Test
System BLEU TER BLEU TER
GIZA++ 36.42 54.21 26.77 57.67
HMM 36.12 54.50 26.17 58.22
ITG 36.23 54.11 26.53 57.40
union 36.57 54.07 26.83 57.37
3 syscomb 37.60 53.19 27.46? 56.88?
4 syscomb 37.77 53.11 27.57? 56.57?
Table 4: MT results on Chinese newswire (see footnote
2).
SysCombTune Test
System BLEU TER BLEU TER
GIZA++ 18.71 64.10 16.94 63.46
HMM 18.35 64.66 16.66 64.02
ITG 18.76 63.67 16.97 63.29
union 18.97 63.86 17.22 63.20
3 syscomb 19.66 63.40 17.98? 62.47?
4 syscomb 19.80 63.32 18.05? 62.36?
Table 5: MT results on Chinese weblog (see footnote 2).
5.1 Discussion
Inter-aligner agreements provide additional evi-
dence about the differences between the aligners.
Suppose on a common data set, the sets of align-
ment links produced by two aligners are A and
B, we compute their agreement as (|A
?
B|/|A| +
|A
?
B|/|B|)/2. (This is the average of recall and
precision of one set by treating the other set as refer-
ence.) The agreement between GIZA++ and ITG
is around 78% on a subset of the Arabic-English
parallel data. The agreements between GIZA++
and HMM, and between HMM and ITG are slightly
higher, around 83%. Since ITG could not align long
sentences, we only used short sentences (at most 60
words in length) in our calculation.
Due to the large differences between the align-
ers, significantly more rules were extracted with
the unioned grammar method in our experiments.
On average, the size of the grammar (number of
rules) was increased by about 80% compared with
the baseline systems. The larger grammar results
in more combinations of partial theories in decod-
ing. However, for computing reasons, we kept the
beam size of the decoder constant despite the in-
crease in grammar size, potentially pruning out good
theories. Performance could be improved further if
larger beam sizes were used. We will leave this to
future work.
6 Related Work
Ayan and Dorr (2006) described a method to min-
imize alignment errors by combining alternative
alignments into a single alignment for each sentence
pair. Deng and Zhou (2009) used the number of ex-
tractable translation pairs as the objective function
for alignment combination. Och and Ney (2003) and
Koehn et al (2003) used heuristics to merge the bidi-
rectional GIZA++ alignments into a single align-
ment. Despite differences in algorithms and objec-
tive functions in these studies, they all attempted to
produce a single final alignment for each sentence
pair. In comparison, all alternative alignments are
directly used by the translation system in this work.
The unioned grammar method in this work is
very similar to Gime?nez and Ma`rquez (2005), which
combined phrase pairs extracted from different
alignments into a single phrase table. The difference
671
from that work is that our focus is to leverage com-
plementary alignment algorithms, while theirs was
to leverage alignments of different lexical units pro-
duced by the same aligner.
Some studies leveraged other types of differences
between systems to improve MT. For example, de
Gispert et al (2009) combined systems trained with
different tokenizations.
The theory behind the GIZA++ aligner was due to
Brown et al (1993). The theory of Inversion Trans-
duction Grammars (ITG) was due to Wu (1997).
The ITG aligner (Haghighi et al, 2009) used in this
work extended the original ITG to handle blocks of
words in addition to single words. The use of HMM
for word alignment can be traced as far back as to
Vogel et al (1996). The HMM aligner used in this
work was due to Liang et al (2006). It refined the
original HMM alignment algorithm by jointly train-
ing two HMMs, one in each direction. Furthermore,
it used a small amount of supervised data to tweak
some high level parameters, although it did not di-
rectly use the supervised data in training.
7 Conclusions
We explored two methods to exploit complementary
alignment algorithms. One is to extract translation
rules from all alternative alignments. The other is to
combine outputs of different MT systems trained us-
ing different aligners. Experiments on two language
pairs and two genres show consistent improvements
over the baseline systems.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program3
(Approved for Public Release, Distribution Unlim-
ited). The authors are grateful to John DeNero and
John Blitzer for their help with the Berkeley HMM
and ITG aligners.
3The views, opinions, and/or findings contained in this ar-
ticle/presentation are those of the author/presenter and should
not be interpreted as representing the official views or policies,
either expressed or implied, of the Defense Advanced Research
Projects Agency or the Department of Defense.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A maximum
entropy approach to combining word alignments. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL,
pages 96?103.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Amer-
ican Chapter of the ACL, pages 218?226.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk combi-
nation of translation hypotheses from alternative mor-
phological decompositions. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the ACL, pages 73?
76.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 755?763.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP, pages 229?232.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2005. Combining
linguistic data views for phrase-based SMT. In Pro-
ceedings of the ACL Workshop on Building and Using
Parallel Texts, pages 145?148.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
IJCNLP of the AFNLP, pages 923?931.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the 2003 Human Language Technology Conference of
the North American Chapter of the ACL, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 388?395.
672
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the ACL, pages 104?111.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the ACL, pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothe-
sis alignment with flexible matching for building con-
fusion networks: BBN system description for WMT09
system combination task. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
61?65.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In The 16th International Conference on Com-
putational Linguistics, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
673
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 321?326,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BBN System Description for WMT10 System Combination Task
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{arosti,bzhang,smatsouk,schwartz}@bbn.com
Abstract
BBN submitted system combination out-
puts for Czech-English, German-English,
Spanish-English, French-English, and All-
English language pairs. All combinations
were based on confusion network decod-
ing. An incremental hypothesis alignment
algorithm with flexible matching was used
to build the networks. The bi-gram de-
coding weights for the single source lan-
guage translations were tuned directly to
maximize the BLEU score of the decod-
ing output. Approximate expected BLEU
was used as the objective function in gra-
dient based optimization of the combina-
tion weights for a 44 system multi-source
language combination (All-English). The
system combination gained around 0.4-
2.0 BLEU points over the best individual
systems on the single source conditions.
On the multi-source condition, the system
combination gained 6.6 BLEU points.
1 Introduction
The BBN submissions to the WMT10 system
combination task were based on confusion net-
work decoding. The confusion networks were
built using the incremental hypothesis alignment
algorithm with flexible matching introduced in the
BBN submission for the WMT09 system combi-
nation task (Rosti et al, 2009). This year, the
system combination weights were tuned to max-
imize the BLEU score (Papineni et al, 2002) of
the 1-best decoding output (lattice based BLEU
tuning) using downhill simplex method (Press et
al., 2007). A 44 system multi-source combina-
tion was also submitted. Since the gradient-free
optimization algorithms do not seem to be able to
handle more than 20-30 weights, a gradient ascent
to maximize an approximate expected BLEU ob-
jective was used to optimize the larger number of
weights.
The lattice based BLEU tuning may be imple-
mented using any optimization algorithm that does
not require the gradient of the objective function.
Due to the size of the lattices, the objective func-
tion evaluation may have to be distributed to mul-
tiple servers. The optimizer client accumulates the
BLEU statistics of the 1-best hypotheses from the
servers for given search weights, computes the fi-
nal BLEU score, and passes it to the optimiza-
tion algorithm which returns a new set of search
weights. The lattice based tuning explores the en-
tire search space and does not require multiple de-
coding iterations with N -best list merging to ap-
proximate the search space as in the standard min-
imum error rate training (Och, 2003). This allows
much faster turnaround in weight tuning.
Differentiable approximations of BLEU have
been proposed for consensus decoding. Tromble
et al (2008) used a linear approximation and Pauls
et al (2009) used a closer approximation called
CoBLEU. CoBLEU is based on the BLEU for-
mula but the n-gram counts are replaced by ex-
pected counts over a translation forest. Due to the
min-functions required in converting the n-gram
counts to matches and a non-differentiable brevity
penalty, a sub-gradient ascent must be used. In
this work, an approximate expected BLEU (Exp-
BLEU) defined over N -best lists was used as a
differentiable objective function. ExpBLEU uses
expected BLEU statistics where the min-function
is not needed as the statistics are computed off-
line and the brevity penalty is replaced by a dif-
ferentiable approximation. The ExpBLEU tun-
ing yields comparable results to direct BLEU tun-
ing using gradient-free algorithms on combina-
tions of small number of systems (fewer than 20-
30 weights). Results on a 44 system combination
show that the gradient based optimization is more
robust with larger number of weights.
321
This paper is organized as follows. Section
2 reviews the incremental hypothesis alignment
algorithm used to built the confusion networks.
Decoding weight optimization using direct lattice
1-best BLEU tuning and N -best list based Exp-
BLEU tuning are presented in Section 3. Exper-
imental results on combining single source lan-
guage to English outputs and all 44 English out-
puts are detailed in Section 4. Finally, Section 5
concludes this paper with some ideas for future
work.
2 Hypothesis Alignment
The confusion networks were built by using the
incremental hypothesis alignment algorithm with
flexible matching introduced in Rosti et al (2009).
The algorithm is reviewed in more detail here. It
is loosely related to the alignment performed in
the calculation of the translation edit rate (TER)
(Snover et al, 2006) which estimates the edit
distance between two strings allowing shifts of
blocks of words in addition to insertions, dele-
tions, and substitutions. Calculating an exact TER
for strings longer than a few tokens1 is not compu-
tationally feasible, so the tercom2 software uses
heuristic shift constraints and pruning to find an
upper bound of TER. In this work, the hypothe-
ses were aligned incrementally with the confusion
network, thus using tokens from all previously
aligned hypotheses in computing the edit distance.
Lower substitution costs were assigned to tokens
considered equivalent and the heuristic shift con-
straints of tercom were relaxed3.
First, tokens from all hypotheses are put into
equivalence classes if they belong to the same
WordNet (Fellbaum, 1998) synonym set or have
the same stem. The 1-best hypothesis from each
system is used as the confusion network skeleton
which defines the final word order of the decod-
ing output. Second, a trivial confusion network
is generated from the skeleton hypothesis by gen-
erating a single arc for each token. The align-
ment algorithm explores shifts of blocks of words
that minimize the edit distance between the cur-
rent confusion network and an unaligned hypothe-
1Hypotheses are tokenized and lower-cased prior to align-
ment. Tokens generally refer to words and punctuation.
2http://www.cs.umd.edu/?snover/tercom/
current version 0.7.25.
3This algorithm is not equivalent to an incremental TER-
Plus (Snover et al, 2009) due to different shift constraints and
the lack of paraphrase matching
30 1cat(1) 2sat(1) mat(1)
(a) Skeleton hypothesis.
40 1cat(1,1) 2sat(1,1) 3on(0,1)NULL(1,0) mat(1,1)
(b) Two hypotheses (insertion).
40 1cat(1,1,0)NULL(0,0,1) 2sat(1,1,1) 3on(0,1,0)NULL(1,0,1) mat(1,1,1)
(c) Three hypotheses (deletion).
40 1cat(1,1,0,1)NULL(0,0,1,0) 2sat(1,1,1,1) 3on(0,1,0,0)NULL(1,0,1,1) mat(1,1,1,0)hat(0,0,0,1)
(d) Four hypotheses (substitution).
Figure 1: Example of incrementally aligning ?cat
sat mat?, ?cat sat on mat?, ?sat mat?, and ?cat sat
hat?.
sis. Third, the hypothesis with the lowest edit dis-
tance to the current confusion network is aligned
into the network. The heuristically selected edit
costs used in the WMT10 system were 1.0 for
insertions, deletions, and shifts, 0.2 for substitu-
tions of tokens in the same equivalence class, and
1.0001 for substitutions of non-equivalent tokens.
An insertion with respect to the network always
results in a new node and two new arcs. The first
arc contains the inserted token and the second arc
contains a NULL token representing the missing
token from all previously aligned hypotheses. A
substitution/deletion results in a new token/NULL
arc or increase in the confidence of an existing to-
ken/NULL arc. The process is repeated until all
hypotheses are aligned into the network.
For example, given the following hypotheses
from four systems: ?cat sat mat?, ?cat sat on mat?,
?sat mat?, and ?cat sat hat?, an initial network in
Figure 1(a) is generated. The following two hy-
potheses have a distance of one edit from the initial
network, so the second can be aligned next. Figure
1(b) shows the additional node created and the two
new arcs for ?on? and ?NULL? tokens. The third
hypothesis has deleted token ?cat? and matches the
322
?NULL? token between nodes 2 and 3 as seen in
Figure 1(c). The fourth hypothesis matches all but
the final token ?hat? which becomes a substitution
for ?mat? in Figure 1(d). The binary vectors in
the parentheses following each token show which
system generated the token aligned to that arc. If
the systems generated N -best hypotheses, a frac-
tional increment could be added to these vectors
as in (Rosti et al, 2007). Given these system spe-
cific scores are normalized to sum to one over all
arcs connecting two consecutive nodes, they may
be viewed as system specific word arc posterior
estimates. Note, for 1-best hypotheses the scores
sum to one without normalization.
Given system outputs E = {E1, . . . , ENs},
an algorithm to build a set of Ns confusion
networks C = {C1, . . . , CNs} may be written
as:
for n = 1 to Ns do
Cn ? Init(En) {initialize confusion net-
work from the skeleton}
E ? ? E ? En {set of unaligned hypotheses}
while E ? 6= ? do
Em ? argminE?E ? Dist(E,Cn)
{compute edit distances}
Cn ? Align(Em, Cn) {align closest hy-
pothesis}
E ? ? E ? ? Em {update set of unaligned
hypotheses}
end while
end for
The set of Ns confusion networks are expanded to
separate paths with distinct bi-gram contexts and
connected in parallel into a big lattice with com-
mon start and end nodes with NULL token arcs.
A prior probability estimate is assigned to the sys-
tem specific word arc confidences connecting the
common start node and the first node in each sub-
network. A heuristic prior is estimated as:
pn =
1
Z
exp(?100
en
Nn
) (1)
where en is the total cost of aligning all hypothe-
ses when using system n as the skeleton, Nn is
the number of nodes in the confusion network be-
fore bi-gram expansion, and Z is a scaling factor
to guarantee pn sum to one. This gives a higher
prior for a network with fewer alignment errors
and longer expected decoding output.
3 Weight Optimization
Standard search algorithms may be used to find N -
best hypotheses from the final lattice. The score
for arc l is computed as:
sl = log
( Ns?
n=1
?nsnl
)
+ ?L(wl|wP (l)) + ?S(wl)
(2)
where ?n are the system weights constrained to
sum to one, snl are the system specific arc pos-
teriors, ? is a language model (LM) scaling fac-
tor, L(wl|wP (l)) is the bi-gram log-probability for
the token wl on the arc l given the token wP (l)
on the arc P (l) preceding the arc l, ? is the word
insertion scaling factor, and S(wl) is zero if wl
is a NULL token and one otherwise. The path
with the highest total score under summation is
the 1-best decoding output. The decoding weights
? = {?1, . . . , ?Ns , ?, ?} are tuned to optimize two
objective functions described next.
3.1 Lattice Based BLEU Optimization
Powell?s method (Press et al, 2007) on N -best
lists was used in system combination weight tun-
ing in Rosti et al (2007). This requires multiple
decoding iterations and merging the N -best lists
between tuning runs to approximate the full search
space as in Och (2003). To speed up the tuning
process, a distributed optimization method can be
used. The lattices are divided into multiple chunks
each of which are loaded into memory by a server.
A client runs the optimization algorithm relying
on the servers for parallelized objective function
evaluation. The client sends a new set of search
weights to the servers which decode the chunks
of lattices and return the 1-best hypothesis BLEU
statistics back to the client. The client accumulates
the BLEU statistics from all servers and computes
the final BLEU score used as the objective func-
tion by the optimization algorithm. Results similar
to Powell?s method can be obtained with fewer it-
erations by using the downhill simplex method in
multi-dimensions (Amoeba) (Press et al, 2007).
To enforce the sum to one constraint of the sys-
tem weights ?n, the search weights are restricted
to [0, 1] by assigning a large penalty if any cor-
responding search weight breaches the limits and
these restricted search weights are scaled to sum
to one before the objective function evaluation.
After optimizing the bi-gram decoding weights
directly on the lattices, a 300-best list are gener-
323
ated. The 300-best hypotheses are re-scored using
a 5-gram LM and another set of re-scoring weights
are tuned on the development set using the stan-
dard N -best list based method. Multiple random
restarts may be used in both lattice and N-best list
based optimization to decrease chances of finding
a local minimum. Twenty sets of initial weights
(the weights from the previous tuning and 19 ran-
domly perturbed weights) were used in all experi-
ments.
3.2 Approximate Expected BLEU
Optimization
The gradient-free optimization algorithms like
Powell?s method and downhill simplex work well
for up to around 20-30 weights. When the number
of weights is larger, the algorithms often get stuck
in local optima even if multiple random restarts
are used. The BLEU score for a 1-best output is
defined as follows:
BLEU =
4?
n=1
(?
i m
n
i?
i h
n
i
) 1
4
?
(
1 ?
?
i ri
?
i h
1
i
)
(3)
where mni is the number of n-gram matches be-
tween the hypothesis and reference for segment
i, hni is the number of n-grams in the hypothesis,
ri is the reference length (or the reference length
closest to the hypothesis if multiple references are
available), and ?(x) = min(1.0, ex) is the brevity
penalty. The first term in Equation 3 is a harmonic
mean of the n-gram precisions up to n = 4. The
selection of 1-best hypotheses is discrete and the
brevity penalty is not continuous, so the BLEU
score is not differentiable and gradient based op-
timization cannot be used. Given a posterior dis-
tribution over all possible decoding outputs could
be defined, an expected BLEU could be optimized
using gradient ascent. However, this posterior dis-
tribution can only be approximated by expensive
sampling methods.
A differentiable objective function over N -best
lists to approximate the BLEU score can be de-
fined using expected BLEU statistics and a con-
tinuous approximation of the brevity penalty. The
posterior probability for hypothesis j of segment i
is simply the normalized decoder score:
pij =
e?Sij
?
k e?Sik
(4)
where ? is a posterior scaling factor and Sij is the
total score of hypothesis j of segment i. The pos-
terior scaling factor controls the shape of the pos-
terior distribution: ? > 1.0 moves the probability
mass toward the 1-best hypothesis and ? < 1.0
flattens the distribution. The BLEU statistics in
Equation 3 are replaced by the expected statistics;
for example, m?ni =
?
j pijmij , and the brevity
penalty ?(x) is approximated by:
?(x) =
ex ? 1
e1000x + 1
+ 1 (5)
ExpBLEU has a closed form solution for the gra-
dient, provided the total decoder score is differen-
tiable.
The penalty used to restrict the search weights
corresponding to the system weights ?n in
gradient-free BLEU tuning is not differentiable.
For expected BLEU tuning, the search weights ?n
are unrestricted but the system weights are ob-
tained by a sigmoid transform and normalized to
sum to one:
?n =
?(?n)
?
m ?(?m)
(6)
where ?(?n) = 1/(1 + e??n).
The expected BLEU tuning is performed on N -
best lists in similar fashion to direct BLEU tuning.
Tuned weights from one decoding iteration are
used to generate a new N -best list, the new N -best
list is merged with the N -best list from the previ-
ous tuning run, and a new set of weights are op-
timized using limited memory Broyden-Fletcher-
Goldfarb-Shanno method (lBFGS) (Liu and No-
cedal, 1989). Since the posterior distribution is
affected by the size of the N -best list and differ-
ent decoding weights, the posterior scaling factor
can be set for each tuning run so that the perplex-
ity of the posterior distribution given the merged
N -best list is constant. A target perplexity of 5.0
was used in the experiments. Four iterations of
bi-gram decoding weight tuning were performed
using 300-best lists. The final 300-best list was re-
scored with a 5-gram and another set of re-scoring
weights was tuned on the development set.
4 Experimental Evaluation
System outputs for all language pairs with En-
glish as the target were combined. Unpruned En-
glish bi-gram and 5-gram language model com-
ponents were trained using the WMT10 corpora:
EuroParl, GigaFrEn, NewsCommentary,
and News. Additional six Gigaword v4 com-
ponents were trained: AFP, APW, XIN+CNA,
324
tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 68.99 13.85 68.45 15.07 60.86 21.02 71.17 15.00
best 56.77 22.84 57.76 25.05 51.81 30.10 53.66 28.64
syscomb 57.31 25.11 54.97 27.75 50.46 31.54 51.35 31.16
test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 68.65 14.29 67.50 15.66 60.52 21.86 68.36 16.82
best 56.13 23.56 58.12 24.34 51.45 30.56 52.16 29.79
syscomb 56.89 25.12 55.60 26.38 50.33 31.59 51.36 30.16
Table 1: Case insensitive TER and BLEU scores on syscombtune (tune) and syscombtest (test)
for combinations of outputs from four source languages.
LTW, NYT, and Headlines+Datelines. In-
terpolation weights for the ten components
were tuned so as to minimize perplexity on
the newstest2009-ref.en development set.
The LMs used modified Kneser-Ney smoothing.
On the multi-source condition (xx-en) another
LM was trained from the system outputs and in-
terpolated with the general LM using an interpola-
tion weight 0.3 for the LM trained on the system
outputs. This LM is referred to as biasLM later.
A tri-gram true casing model was trained using all
available English data. This model was used to
restore the case of the lower-case system combi-
nation output.
All six 1-best system outputs on cz-en, 16
outputs on de-en, 8 outputs on es-en, and
14 outputs on fr-en were combined. The lat-
tice based BLEU tuning was used to optimize the
bi-gram decoding weights and N-best list based
BLEU tuning was used to optimize the 5-gram re-
scoring weights. Results for these single source
language experiments are shown in Table 1. The
gains on syscombtune were similar to those on
syscombtest for all but French-English. The
tuning set contained only 455 segments but ap-
peared to be well matched with the larger (2034
segments) test set. The characteristics of the indi-
vidual system outputs were probably different for
the tuning and test sets on French-English transla-
tion. In our experience, optimizing system com-
bination weights using the ExpBLEU tuning for
a small number of systems yields similar results
to lattice based BLEU tuning. The lattice based
BLEU tuning is faster as there is no need for mul-
tiple decoding and tuning iterations. Using the bi-
asLM on the single source combinations did not
xx-en tune test
System TER BLEU TER BLEU
worst 71.17 13.85 68.65 14.29
best 51.81 30.10 51.45 30.56
lattice 43.15 35.72 43.79 35.29
expBLEU 44.07 36.91 44.35 36.62
+biasLM 43.63 37.61 44.50 37.12
Table 2: Case insensitive TER and BLEU scores
on syscombtune (tune) and syscombtest
(test) for xx-en combination. Combinations us-
ing lattice BLEU tuning, expected BLEU tuning,
and after adding the system output biased LM are
shown.
yield any gains. The output for these conditions
probably did not contain enough data for biasLM
training given the small tuning set and small num-
ber of systems.
Finally, experiments combining all 44 1-best
system outputs were performed to produce a
multi-source combination output. The first experi-
ment used the lattice based BLEU tuning and gave
a 5.6 BLEU point gain on the tuning set as seen in
Table 2. The ExpBLEU tuning gave an additional
1.2 point gain which suggests that the direct lattice
based BLEU tuning got stuck in a local optimum.
Using the system output biased LM gave an addi-
tional 0.7 point gain. The gains on the test set were
similar and the best combination gave a 6.6 point
gain over the best individual system.
5 Conclusions
The BBN submissions for WMT10 system com-
bination task were described in this paper. The
combination was based on confusion network de-
325
coding. The confusion networks were built us-
ing an incremental hypothesis alignment algo-
rithm with flexible matching. The bi-gram de-
coding weights for the single source conditions
were optimized directly to maximize the BLEU
scores of the 1-best decoding outputs and the 5-
gram re-scoring weights were tuned on 300-best
lists. The BLEU gains over the best individual
system outputs were around 1.5 points on cz-en,
2.0 points on de-en, 1.0 points on es-en, and
0.4 points on fr-en. The system combination
weights on xx-en were tuned to maximize Exp-
BLEU, and a system output biased LM was used.
The BLEU gain over the best individual system
was 6.6 points. Future work will investigate tuning
of the edit costs used in the alignment. A lattice
based ExpBLEU tuning will be investigated. Also,
weights for more complicated functions with addi-
tional features may be tuned using ExpBLEU.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory method for large scale optimization. Math-
ematical Programming, 45(3):503?528.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Adam Pauls, John DeNero, and DanKlein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1418?1427.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2007. Numerical
recipes: the art of scientific computing. Cambridge
University Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hy-
pothesis alignment with flexible matching for build-
ing confusion networks: BBN system description
for WMT09 system combination task. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 61?65.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259?268.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing,
pages 620?629.
326
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 159?165,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Expected BLEU Training for Graphs: BBN System Description for
WMT11 System Combination Task
Antti-Veikko I. Rosti? and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{arosti,bzhang,smatsouk,schwartz}@bbn.com
Abstract
BBN submitted system combination outputs
for Czech-English, German-English, Spanish-
English, and French-English language pairs.
All combinations were based on confusion
network decoding. The confusion networks
were built using incremental hypothesis align-
ment algorithm with flexible matching. A
novel bi-gram count feature, which can penal-
ize bi-grams not present in the input hypothe-
ses corresponding to a source sentence, was
introduced in addition to the usual decoder
features. The system combination weights
were tuned using a graph based expected
BLEU as the objective function while incre-
mentally expanding the networks to bi-gram
and 5-gram contexts. The expected BLEU
tuning described in this paper naturally gen-
eralizes to hypergraphs and can be used to
optimize thousands of weights. The com-
bination gained about 0.5-4.0 BLEU points
over the best individual systems on the official
WMT11 language pairs. A 39 system multi-
source combination achieved an 11.1 BLEU
point gain.
1 Introduction
The confusion networks for the BBN submissions
to the WMT11 system combination task were built
using incremental hypothesis alignment algorithm
?This work was supported by DARPA/I2O Contract No.
HR0011-06-C-0022 under the GALE program (Approved for
Public Release, Distribution Unlimited). The views, opinions,
and/or findings contained in this article/presentation are those of
the author/presenter and should not be interpreted as represent-
ing the official views or policies, either expressed or implied,
of the Defense Advanced Research Projects Agency or the De-
partment of Defense.
with flexible matching (Rosti et al, 2009). A novel
bi-gram count feature was used in addition to the
standard decoder features. The N-best list based ex-
pected BLEU tuning (Rosti et al, 2010), similar to
the one proposed by Smith and Eisner (2006), was
extended to operate on word lattices. This method is
closely related to the consensus BLEU (CoBLEU)
proposed by Pauls et al (2009). The minimum oper-
ation used to compute the clipped counts (matches)
in the BLEU score (Papineni et al, 2002) was re-
placed by a differentiable function, so there was
no need to use sub-gradient ascent as in CoBLEU.
The expected BLEU (xBLEU) naturally generalizes
to hypergraphs by simply replacing the forward-
backward algorithm with inside-outside algorithm
when computing the expected n-gram counts and
sufficient statistics for the gradient.
The gradient ascent optimization of the xBLEU
appears to be more stable than the gradient-free di-
rect 1-best BLEU tuning or N -best list based min-
imum error rate training (Och, 2003), especially
when tuning a large number of weights. On the of-
ficial WMT11 language pairs with up to 30 weights,
there was no significant benefit from maximizing
xBLEU. However, on a 39 system multi-source
combination (43 weights total), it yielded a signif-
icant gain over gradient-free BLEU tuning and N -
best list based expected BLEU tuning.
2 Hypothesis Alignment and Features
The incremental hypothesis alignment with flexible
matching (Rosti et al, 2009) produces a confusion
network for each system output acting as a skele-
ton hypothesis for the ith source sentence. A con-
fusion network is a graph where all paths visit all
159
vertices. Consecutive vertices are connected by one
or more edges representing alternatives. Each edge
l is associated with a token and a set of scores. A to-
ken may be a word, punctuation symbol, or special
NULL token indicating a deletion in the alignment.
The set of scores includes a vector ofNs system spe-
cific confidences, siln, indicating whether the token
was aligned from the output of the system n.1 Other
scores may include a language model (LM) score
as well as non-NULL and NULL token indicators
(Rosti et al, 2007). As Rosti et al (2010) described,
the networks for all skeletons are connected to a start
and end vertex with NULL tokens in order to form
a joint lattice with multiple parallel networks. The
edges connecting the start vertex to the initial ver-
tices in each network have a heuristic prior estimated
from the alignment statistics at the confidence cor-
responding to the skeleton system. The edges con-
necting the final vertices of each network to the end
vertex have all system confidences set to one, so the
final edge does not change the score of any path.
A single word confidence is produced from the
confidence vector by taking an inner product with
the system weights ?n which are constrained to sum
to one,2
?
n ?n = 1. The total edge score is pro-
duced by a log-linear interpolation of the word con-
fidence with other features film:
sil = log
( Ns?
n=1
?nsiln
)
+
?
m
?mfilm (1)
The usual features film include the LM score as well
as non-NULL and NULL token indicators. Based
on an analysis of the system combination outputs, a
large number of bi-grams not present in any input
hypothesis are often produced, some of which are
clearly ungrammatical despite the LM. These novel
bi-grams are due to errors in hypothesis alignment
and the confusion network structure where any word
from the incoming edges of a vertex can be followed
by any word from the outgoing edges. After expand-
ing and re-scoring the joint lattice with a bi-gram, a
new feature indicating the presence of a novel bi-
gram may be added on the edges. A negative weight
1The confidences are binary when aligning 1-best outputs.
More elaborate confidences may be estimated fromN -best lists;
see for example Rosti et al (2007).
2See (Rosti et al, 2010) for a differentiable constraint.
for this feature discourages novel bi-grams in the
output during decoding.
3 Weight Optimization
The most common objective function used in ma-
chine translation is the BLEU-N score (Papineni et
al., 2002) defined as follows:3
BLEU =
N?
n=1
(?
i m
n
i?
i h
n
i
) 1
N
?
(
1?
?
i ri
?
i h
1
i
)
(2)
where N is the maximum n-gram order (typically
N = 4), mni is the number of n-gram matches
(clipped counts) between the hypothesis ei and ref-
erence e?i for segment i, hni is the number of n-grams
in the hypothesis, ri is the reference length,4 and
?(x) = min(1.0, ex) is the brevity penalty. Using
gn to represent an arbitrary n-gram, cign to repre-
sent the count of gn in hypothesis ei, and c?ign to
represent the count of gn in reference e?i, the BLEU
statistics can be defined as follows:
mni =
?
gn
min(cign , c?ign) (3)
hni =
?
gn
cign (4)
The unigram count h1i is simply the hypothesis
length and higher order n-gram counts can be ob-
tained by hni = h
n?1
i ? 1. The reference n-gram
counts for each sentence can be stored in an n-gram
trie for efficient scoring.5
The BLEU score is not differentiable due to the
minimum operations on the matches mni and brevity
penalty ?(x). Therefore gradient-free optimization
algorithms, such as Powell?s method or downhill
simplex (Press et al, 2007), are often employed in
weight tuning (Och, 2003). System combination
weights tuned using the downhill simplex method
to directly optimize 1-best BLEU score of the de-
coder outputs served as the first baseline in the ex-
periments. The distributed optimization approach
used here was first described in (Rosti et al, 2010).
3Superscripts indicate the n-gram order in all variables in
this paper. They are used as exponents only for the constant e.
4If multiple references are available, ri is the reference
length closest to the hypothesis length, h1i .
5If multiple references are available, the maximum n-gram
counts are stored.
160
A set of system combination weights was first tuned
for unpruned lattices re-scored with a bi-gram LM.
Another set of re-scoring weights was tuned for 300-
best lists re-scored with a 5-gram LM.
3.1 Graph expected BLEU
Gradient-free optimization algorithms work well
with a relatively small number of weights. Weight
optimization for a 44 system combination in Rosti
et al (2010) was shown to be unstable with down-
hill simplex algorithm. Instead, an N-best list based
expected BLEU tuning with gradient ascent yielded
better results. This served as the second baseline in
the experiments. The objective function is defined
by replacing the n-gram statistics with expected n-
gram counts and matches as in (Smith and Eisner,
2006), and brevity penalty with a differentiable ap-
proximation:
?(x) =
ex ? 1
1 + e1000x
+ 1 (5)
An N-best list represents a subset of the search space
and multiple decoding iterations with N-best list
merging is required to improve convergence. In this
work, expected BLEU tuning is extended for lat-
tices by replacing the minimum operation in n-gram
matches with another differentiable approximation.
The expected n-gram statistics for path j, which cor-
respond to the standard statistics in Equations 3 and
4, are defined as follows:
m?ni =
?
gn
?
( ?
j?Ji
Pijcijgn , c?ign
)
(6)
h?ni =
?
gn
?
j?Ji
Pijcijgn (7)
where Ji is the set of all paths in a lattice or all
derivations in a hypergraph for the ith source sen-
tence, Pij is the posterior of path j, and cijgn is
the count of n-grams gn in hypothesis eij on path
j. The path posterior and approximate minimum are
defined by:
Pij =
?
l?j e
?sil
?
j??Ji
?
l?j? e?sil
(8)
?(x, c) =
x? c
1 + e1000(x?c)
+ c (9)
where sil is the total score on edge l defined in Equa-
tion 1 and ? is an edge score scaling factor. The
scaling factor affects the shape of the edge posterior
distribution; ? > 1.0 makes the edge posteriors on
the 1-best path higher than edge posteriors on other
paths and ? < 1.0 makes the posteriors on all paths
more uniform.
The graph expected BLEU can be factored as
xBLEU = ePB where:
P =
1
N
N?
n=1
(
log
?
i
m?ni ? log
?
i
h?ni
)
(10)
B = ?
(
1?
?
i ri
?
i h?
1
i
)
(11)
and ri is the reference length.6 This objective func-
tion is closely related to CoBLEU (Pauls et al,
2009). Unlike CoBLEU, xBLEU is differentiable
and standard gradient ascent algorithms can be used
to find weights that maximize the objective.
Note, the expected counts can be expressed in
terms of edge posteriors as:
?
j?Ji
Pijcijgn =
?
l?Li
pil?(c
n
il, g
n) (12)
where Li is the set of all edges for the ith sentence,
pil is the edge posterior, ?(x, c) is the Kronecker
delta function which is 1 if x = c and 0 if x 6= c, and
cnil is the n-gram context of edge l. The edge posteri-
ors can be computed via standard forward-backward
algorithm for lattices or inside-outside algorithm for
hypergraphs. As with the BLEU statistics, only ex-
pected unigram counts h?1i need to be accumulated
for the hypothesis n-gram counts in Equation 7 as
h?ni = h?
n?1
i ? 1 for n > 1. Also, the expected
n-gram counts for each graph can be stored in an
n-gram trie for efficient gradient computation.
3.2 Gradient of graph expected BLEU
The gradient of the xBLEU with respect to weight ?
can be factored as:
?xBLEU
??
=
?
i
?
l?Li
?sil
??
?
j?Ji
?xBLEU
? logPij
? logPij
?sil
(13)
where the gradient of the log-path-posterior with re-
spect to the edge score is given by:
? logPij
?sil
= ?
(
?(l ? j)? pil
)
(14)
6If multiple reference are available, ri is the reference length
closest to the expected hypothesis length h?1i .
161
?xBLEU
??
= ?eP
(
B
N
N?
n=1
?
i
(
m?nik ?m
n
ik
mn
?
h?nik ? h
n
ik
hn
))
+ C??(1? C)
?
i
h?1ik ? h
1
ik
h1
(15)
and ?(l ? j) is one if edge l is on path j, and zero
otherwise. Using the factorization xBLEU = ePB,
Equation 13 can be expressed using sufficient statis-
tics as shown in Equation 15, where ??(x) is the
derivative of ?(x) with respect to x, mn =
?
i m?
n
i ,
hn =
?
i h?
n
i , C =
?
ri/
?
i h?
1
i , and the remaining
sufficient statistics are given by:
??ign = ?
?
( ?
j?Ji
Pijcijgn , c?ign
)
mnik =
(
?
l?Li
pil
?sil
??
)(
?
j?Ji
Pij
?
gn
??igncijgn
)
m?nik =
?
l?Li
?sil
??
?
j:l?Ji
Pij
?
gn
??igncijgn
hnik =
(
?
l?Li
pil
?sil
??
)(
?
j?Ji
Pij
?
gn
cijgn
)
h?nik =
?
l?Li
?sil
??
?
j:l?Ji
Pij
?
gn
cijgn
where ??(x, c) is the derivative of ?(x, c) with re-
spect to x, and the parentheses in the equations for
mnik and h
n
ik signify that the second terms do not de-
pend on the edge l.
3.3 Forward-backward algorithm under
expectation semiring
The sufficient statistics for graph expected BLEU
can be computed using expectation semirings (Li
and Eisner, 2009). Instead of computing single
forward/backward or inside/outside scores, addi-
tional n-gram elements are tracked for matches and
counts. For example in a bi-gram graph, the ele-
ments for edge l are represented by a 5-tuple7 sl =
?pl, r1lh, r
2
lh, r
1
lm, r
2
lm? where pl = e
?sil and:
rnlh =
?
gn
?(cnil, g
n)e?sil (16)
rnlm =
?
gn
??igne
?sil (17)
Assuming the lattice is topologically sorted, the for-
ward algorithm8 under expectation semiring for a 3-
7The sentence index i is dropped for brevity.
8For inside-outside algorithm, see (Li and Eisner, 2009).
tuple9 sl = ?pl, r1lh, r
1
lm? is defined by:
?0 = ?1, 0, 0? (18)
?v =
?
l?Iv
?u(l) ? sl (19)
where Iv is the set of all edges with target vertex
v and u(l) is the source vertex for edge l, and the
operations are defined by:
s1 ? s2 = ?p1 + p2, r
1
1h + r
1
2h, r
1
1m + r
1
2m?
s1 ? s2 = ?p1p2, p1r
1
2h + p2r
1
1h, p1r
1
2m + p2r
1
1m?
The backward algorithm for ?u can be implemented
via the forward algorithm in reverse through the
graph. The sufficient statistics for the gradient can
be accumulated during the backward pass noting
that:
?
j?Ji
Pij
?
gn
??igncijgn =
rnm(?0)
p(?0)
(20)
?
j?Ji
Pij
?
gn
cijgn =
rnh(?0)
p(?0)
(21)
where rnm(?) and r
n
h(?) extract the nth order r ele-
ments from the tuple for matches and counts, respec-
tively, and p(?) extracts the p element. The statistics
for the paths traveling via edge l can be computed
by:
?
j:l?Ji
Pij
?
gn
??igncijgn =
rnm(?u ? sl ? ?v)
p(?0)
(22)
?
j:l?Ji
Pij
?
gn
cijgn =
rnh(?u ? sl ? ?v)
p(?0)
(23)
where the u and v subscripts in ?u and ?v are the
start and end vertices for edge l. To avoid under-
flow, all the computations can be carried out in log
domain.
9A 3-tuple for uni-gram counts is used as an example in or-
der to save space. In a 5-tuple for bi-gram counts, all r elements
are computed independently of other r elements with the same
operations. Similarly, tri-gram counts require 7-tuples and four-
gram counts require 9-tuples.
162
tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 66.03 18.09 69.03 16.28 60.56 21.02 62.75 21.83
best 53.75 28.36 58.39 24.28 50.26 30.55 50.48 30.87
latBLEU 53.99 29.25 56.70 26.49 48.34 34.55 48.90 33.90
nbExpBLEU 54.43 29.04 56.36 27.33 48.44 34.73 48.58 34.23
latExpBLEU 53.89 29.37 56.24 27.36 48.27 34.93 48.53 34.24
test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 65.35 17.69 69.03 15.83 61.22 19.79 62.36 21.36
best 52.21 29.54 58.00 24.16 50.15 30.14 50.15 30.32
latBLEU 52.80 29.89 55.87 26.22 48.29 33.91 48.51 32.93
nbExpBLEU 52.97 29.93 55.77 26.52 48.39 33.86 48.25 32.94
latExpBLEU 52.68 29.99 55.74 26.62 48.30 34.10 48.17 32.91
Table 1: Case insensitive TER and BLEU scores on newssyscombtune (tune) and newssyscombtest (test)
for combinations of outputs from four source languages. Three tuning methods were used: lattice BLEU (latBLEU),
N-best list based expected BLEU (nbExpBLEU), and lattice expected BLEU (latExpBLEU).
3.4 Entropy on a graph
Expanding the joint lattice to n-gram orders above
n = 2 is often impractical without pruning. If the
edge posteriors are not reliable, which is usually
the case for unoptimized weights, pruning might re-
move good quality paths from the graph. As a com-
promise, an incremental expansion strategy may be
adopted by first expanding and re-scoring the lattice
with a bi-gram, optimizing weights for xBLEU-2,
and then expanding and re-scoring the lattice with
a 5-gram. Pruning should be more reliable with the
edge posteriors computed using the tuned bi-gram
weights. A second set of weights may be tuned with
the 5-gram graph to maximize xBLEU-4.
When the bi-gram weights are tuned, it may be
beneficial to increase the edge score scaling factor
to focus the edge posteriors to the 1-best path. On
the other hand, a lower scaling factor may be bene-
ficial when tuning the 5-gram weights. Rosti et al
(2010) determined the scaling factor automatically
by fixing the perplexity of the merged N -best lists
used in tuning. Similar strategy may be adopted in
incremental n-gram expansion of the lattices.
Entropy on a graph can also be computed using
the expectation semiring formalism (Li and Eisner,
2009) by defining sl = ?pl, rl? where pl = e?sil and
rl = log pl. The entropy is given by:
Hi = log p(?0)?
r(?0)
p(?0)
(24)
where p(?0) and r(?0) extract the p and r elements
from the 2-tuple ?0, respectively. The average target
entropy over all sentences was set manually to 3.0
in the experiments based on the tuning convergence
and size of the pruned 5-gram lattices.
4 Experimental Evaluation
System outputs for all language pairs with En-
glish as the target were combined (cz-en,
de-en, es-en, and fr-en). Unpruned English
bi-gram and 5-gram language model compo-
nents were trained using the WMT11 corpora:
EuroParl, GigaFrEn, UNDoc Es, UNDoc Fr,
NewsCommentary, News2007, News2008,
News2009, News2010, and News2011.
Additional six Gigaword v4 components in-
cluded: AFP, APW, XIN+CNA, LTW, NYT, and
Headlines+Datelines. The total number
of words used to train the LMs was about 6.4
billion. Interpolation weights for the sixteen
components were tuned to minimize perplexity on
the newstest2010-ref.en development set.
The modified Kneser-Ney smoothing (Chen and
163
Goodman, 1998) was used in training. Experiments
using a LM trained on the system outputs and inter-
polated with the general LM were also conducted.
The interpolation weights between 0.1 and 0.9 were
tried, and the weight yielding the highest BLEU
score on the tuning set was selected. A tri-gram true
casing model was trained on all the LM training
data. This model was used to restore the case of the
lower-case system combination output.
All twelve 1-best system outputs on cz-en, 26
outputs on de-en, 16 outputs on es-en, and 24
outputs on fr-en were combined. Three different
weight optimization methods were tried. First, lat-
tice based 1-best BLEU optimization of the bi-gram
decoding weights followed by N-best list based
BLEU optimization of 5-gram re-scoring weights
using 300-best lists, both using downhill simplex.
Second, N-best list based expected BLEU optimiza-
tion of the bi-gram and 5-gram weights using 300-
best lists with merging between bi-gram decoding
iterations. Third, lattice based expected BLEU opti-
mization of bi-gram and 5-gram decoding weights.
The L-BFGS (Liu and Nocedal, 1989) algorithm
was used in gradient ascent. Results for all four sin-
gle source experiments are shown in Table 1, includ-
ing case insensitive TER (Snover et al, 2006) and
BLEU scores for the worst and best systems, and
the system combination outputs for the three tuning
methods. The gains on tuning and test sets were con-
sistent, though relatively smaller on cz-en due to
a single system (online-B) dominating the other
systems by about 5-6 BLEU points. The tuning
method had very little influence on the test set scores
apart from de-en where the lattice BLEU opti-
mization yields slightly lower BLEU scores. This
seems to suggest that the gradient free optimization
is not as stable with a larger number of weights.10
The novel bi-gram feature did not have significant
influence on the TER or BLEU scores, but the num-
ber of novel bi-grams was reduced by up to 100%.
Finally, experiments combining 39 system out-
puts by taking the top half of the outputs from each
language pair were performed. The selection was
based on case insensitive BLEU scores on the tun-
ing set. Table 2 shows the scores for seven combi-
10A total number of 30 weights, 26 system and 4 feature
weights, were tuned for de-en.
xx-en tune test
System TER BLEU TER BLEU
worst 62.81 21.19 62.92 20.29
best 51.11 30.87 50.80 30.32
latBLEU 40.95 40.75 41.06 39.81
+biasLM 41.18 40.90 41.16 39.90
nbExpBLEU 40.81 41.36 41.05 40.15
+biasLM 40.72 41.99 40.65 40.89
latExpBLEU 40.57 41.68 40.62 40.60
+biasLM 40.42 42.23 40.52 41.38
-nBgF 40.85 41.41 40.88 40.55
Table 2: Case insensitive TER and BLEU scores on
newssyscombtune (tune) and newssyscombtest
(test) for xx-en combination. Combinations using lat-
tice BLEU tuning (latBLEU), N-best list based expected
BLEU tuning (nbExpBLEU), and lattice expected BLEU
tuning (latExpBLEU) with and without the system out-
put biased LM (biasLM) are shown. Final row, marked
nBgF, corresponds to the above tuning without the novel
bi-gram feature.
nations using the three tuning methods with or with-
out the system output biased LM, and finally without
the novel bi-gram count feature. There is a clear ad-
vantage from the expected BLEU tuning on the tun-
ing set, and lattice tuning yields better scores than
N-best list based tuning. The difference between
latBLEU and nbExpBLEU without biasLM is
not quite as large on the test set but latExpBLEU
yields significant gains over both. The biasLM also
yields significant gains on all but latBLEU tuning.
Finally, removing the novel bi-gram count feature
results in a significant loss, probably due to the large
number of input hypotheses. The number of novel
bi-grams in the test set output was reduced to zero
when using this feature.
5 Conclusions
The BBN submissions for WMT11 system combi-
nation task were described in this paper together
with a differentiable objective function, graph ex-
pected BLEU, which scales well for a large number
of weights and can be generalized to hypergraphs.
System output biased language model and a novel
bi-gram count feature also gave significant gains on
a 39 system multi-source combination.
164
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Computer Science
Group Harvard University.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 40?51.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1418?1427.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system combi-
nation for machine translation. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothe-
sis alignment with flexible matching for building con-
fusion networks: BBN system description for WMT09
system combination task. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
61?65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for WMT10 system combination task. In Proceedings
of the Fifth Workshop on Statistical Machine Transla-
tion, pages 321?326.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 787?
794.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
165
