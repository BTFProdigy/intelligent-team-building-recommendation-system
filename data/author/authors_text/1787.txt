239
240
241
242
The Automatic Generation of Formal Annotations in a Multimedia
Indexing and Searching Environment
Thierry Declerck
DFKI GmbH
Stuhlsatzenhausweg 3
D-66123 Saarbruecken
Germany
declerck@dfki.de
Peter Wittenburg
MPI for Psycholinguistics
Wundtlaan 1, PB 310
NL-6500 AH Nijmegen
The Netherlands
Peter.Wittenburg@mpi.nl
Hamish Cunningham
Dept. of Computer Science
University of Sheffield
Regent Court, 211 Portobello
GB-Sheffield S1 4DP
Great Britain
hamish@dcs.shef.ac.uk
Abstract
We describe in this paper the MU-
MIS Project (Multimedia Indexing and
Searching Environment)1 , which is
concerned with the development and in-
tegration of base technologies, demon-
strated within a laboratory prototype, to
support automated multimedia index-
ing and to facilitate search and retrieval
from multimedia databases. We stress
the role linguistically motivated annota-
tions, coupled with domain-specific in-
formation, can play within this environ-
ment. The project will demonstrate that
innovative technology components can
operate on multilingual, multisource,
and multimedia information and create
a meaningful and queryable database.
1 Introduction
MUMIS develops and integrates basic technolo-
gies, which will be demonstrated within a labora-
tory prototype, for the automatic indexing of mul-
timedia programme material. Various technology
components operating offline will generate for-
mal annotations of events in the data material pro-
cessed. These formal annotations will form the
basis for the integral online part of the MUMIS
project, consisting of a user interface allowing the
querying of videos. The indexing of the video ma-
terial with relevant events will be done along the
1MUMIS is an on-going EU-funded project within the
Information Society Program (IST) of the European Union,
section Human Language Technology (HLT). See for more
information http://parlevink.cs.utwente.nl/projects/mumis/.
line of time codes extracted from the various doc-
uments.
For this purpose the project makes use of data
from different media sources (textual documents,
radio and television broadcasts) in different lan-
guages (Dutch, English and German) to build a
specialized set of lexicons and an ontology for
the selected domain (soccer). It also digitizes
non-text data and applies speech recognition tech-
niques to extract text for the purpose of annota-
tion.
The core linguistic processing for the anno-
tation of the multimedia material consists of
advanced information extraction techniques for
identifying, collecting and normalizing signifi-
cant text elements (such as the names of players
in a team, goals scored, time points or sequences
etc.) which are critical for the appropriate anno-
tation of the multimedia material in the case of
soccer.
Due to the fact that the project is accessing and
processing distinct media in distinct languages,
there is a need for a novel type of merging tool in
order to combine the semantically related annota-
tions generated from those different data sources,
and to detect inconsistencies and/or redundancies
within the combined annotations. The merged an-
notations will be stored in a database, where they
will be combined with relevant metadata.2
Finally the project will develop a user interface
to enable professional users to query the database,
by selecting from menus based on structured an-
2We see in this process of merging extracted informa-
tions and their combination with metadata a fruitful base for
the identification and classification of content or knowledge
from distinct types of documents.
notations and metadata, and to view video frag-
ments retrieved to satisfy the query, offering thus
a tool to formulate queries about multimedia pro-
grammes and directly get interactive access to the
multimedia contents. This tool constitutes the on-
line component of the MUMIS environment.
2 State of the Art
MUMIS differs in many significant ways from ex-
isting technologies and already achieved or ad-
vanced projects3 . Most closely related to the the-
matic focus of MUMIS are the HLT projects Pop-
Eye [POP] and OLIVE [OLI]. Pop-Eye used sub-
titles to index video streams and offered time-
stamped texts to satisfy a user query, on request
displaying a storyboard or video fragment corre-
sponding to the text hit. OLIVE used automatic
speech recognition to generate transcriptions of
the sound tracks of news reports, which were then
indexed and used in ways similar to the Pop-Eye
project; both projects used fuzzy matching IR al-
gorithms to search and retrieve text, offering lim-
ited multilingual access to texts. Instead of using
IR methods to index and search the transcriptions,
MUMIS will create formal annotations to the in-
formation, and will fuse information annotations
from different media sources. The fusion result
is then used to direct retrieval, through interface
techniques such as pop-up menus, keyword lists,
and so on. Search takes the user direct to the sto-
ryboard and video clippings.
The Informedia project at Carnegie-Mellon-
University [INF] has a similar conceptual base-
line to MUMIS. The innovative contribution of
MUMIS is that it uses a variety of multilingual
information sources and fuses them on the ba-
sis of formal domain-specific annotations. Where
Informedia primarily focuses on special applica-
tions, MUMIS aims at the advancement and in-
tegratibility of HLT-enhanced modules to enable
information filtering beyond the textual domain.
Therefore, MUMIS can be seen as complemen-
tary to Informedia with extensions typical for Eu-
rope.
The THISL project [THI] is about spoken doc-
ument retrieval, i.e., automatic speech recognition
3We are aware of more related on-going projects, at least
within the IST program, but we can not compare those to
MUMIS now, since we still lack first reports.
is used to auto-transcribe news reports and then
information retrieval is carried out on this infor-
mation. One main focus of THISL is to improve
speech recognition. Compared to MUMIS it lacks
the strong language processing aspects, the fusion
of multilingual sources, and the multimedia deliv-
ery.
Columbia university is running a project [COL]
to use textual annotations of video streams to in-
dicate moments of interest, in order to limit the
scope of the video processing task which requires
extreme CPU capacities. So the focus is on find-
ing strategies to limit video processing. The Uni-
versity of Massachusetts (Amherst) is also run-
ning projects about video indexing [UMA], but
these focus on the combination of text and im-
ages. Associated text is used to facilitate indexing
of video content. Both projects are funded under
the NSF Stimulate programme [NSF].
Much work has been done on video and im-
age processing (Virage [VIR], the EUROMEDIA
project [EUR], Surfimage [SUR], the ISIS project
[ISI], IBM's Media Miner, projects funded under
the NSF Stimulate program [NSF], and many oth-
ers). Although this technology in general is in its
infancy, there is reliable technology to indicate,
for example, scene changes using very low-level
cues and to extract key frames at those instances
to form a storyboard for easy video access. Some
institutions are running projects to detect subtitles
in the video scene and create a textual annotation.
This task is very difficult, given a sequence of real
scenes with moving backgrounds and so on. Even
more ambitious tasks such as finding real patterns
in real movies (tracing the course of the ball in a
soccer match, for example) are still far from being
achieved.4
3 Formal Annotations for the Soccer
Domain
Soccer has been chosen as the domain to test and
apply the algorithms to be developed. There are a
number of reasons for this choice: availability of
people willing to help in analyzing user require-
ments, existence of many information sources in
4The URLs of the projects mentionned above are given
in the bibliography at the end of this paper.
several languages5 , and great economic and pub-
lic interest. The prototype will also be tested by
TV professionals and sport journalists, who will
report on its practicability for the creation and
management of their programme and information
material.
The principles and methods derived from this
domain can be applied to other as well. This has
been shown already in the context of text-based
Information Extraction (IE), for which method-
ologies for a fast adaptation to new domains
have been developed (see the MUC conferences
and (Neumann et al, 2000)). And generally
speaking the use of IE for automatic annotation
of multimedia document has the advantage of
providing, besides the results of the (shallow)
syntactic processing, accurate semantic (or con-
tent/conceptual) information (and thus potential
annotation) for specific predefined domains, since
a mapping from the linguistically analyzed rele-
vant text parts can be mapped onto an unambigu-
ous conceptual description6 . Thus in a sense it
can be assumed that IE is supporting the word
sense disambiguation task.
It is also commonly assumed (see among oth-
ers (Cunningham, 1999)) that IE occupies an in-
termediate place between Information Retrieval
(with few linguistic knowledge involved) and
Text Understanding (involving the full deep lin-
guistic analysis and being still not realized for the
time being.). IE being robust but offering only a
partial (but mostly accurate) syntactic and content
analysis, it can be said that this language technol-
ogy is actually filling the gap between available
low-level annotated/indexed documents and cor-
pora and the desirable full content annotation of
those documents and corpora. This is the reason
why MUMIS has chosen this technology for pro-
viding automatic annotation (at distinct linguistic
and domain-specific levels) of multimedia mate-
rial, allowing thus to add queryable ?content in-
formation? to this material.7
5We would like to thank at this place the various institu-
tions making available various textual, audio and video data.
6This topic has already been object of a workshop dis-
cussing the relations between IE and Corpus Linguistics
(McNaught, 2000).
7MUMIS was not explicitly designed for supporting
knowledge management tasks, but we assume that the mean-
ingful organization of domain-specific multimedia material
proposed by the project can be adapted to the organization of
4 The Multimedia Material in MUMIS
The MUMIS project is about automatic index-
ing of videos of soccer matches with formal an-
notations and querying that information to get
immediate access to interesting video fragments.
For this purpose the project chose the European
Football Championships 2000 in Belgium and the
Netherlands as its main database. A major project
goal is to merge the formal annotations extracted
from textual and audio material (including the au-
dio part of videos) on the EURO 2000 in three
languages: English, German, Dutch. The mate-
rial MUMIS has to process can be classified in
the following way:
1. Reports from Newspapers (reports about
specific games, general reports) which is
classified as free texts (FrT)
2. Tickers, close captions, Action-Databases
which are classified as semi-formal texts
(SFT)
3. Formal descriptions about specific games
which are classified as formal texts (FoT)
4. Audio material recorded from radio and TV
broadcasts
5. Video material recorded from TV broadcasts
1-4 will be used for automatically generating
formal annotations in order to index 5. MUMIS
is investigating the precise contribution of each
source of information for the overall goal of the
project.
Since the information contained in formal texts
can be considered as a database of true facts, they
play an important role within MUMIS. But never-
theless they contain only few information about a
game: the goals, the substitutions and some other
few events (penalties, yellow and red cards). So
there are only few time points available for in-
dexing videos. Semi-formal texts (SFT), like live
tickers on the web, are offering much more time
points sequences, related with a higher diversity
the distributed information of an enterprise and thus support
the sharing and access to companies expertise and know-
how.
of events (goals scenes, fouls etc,) and seem to of-
fer the best textual source for our purposes. Nev-
ertheless the quality of the texts of online tick-
ers is often quite poor. Free texts, like newspa-
pers articles, have a high quality but the extrac-
tion of time points and their associated events in
text is more difficult. Those texts also offer more
background information which might be interest-
ing for the users (age of the players, the clubs they
are normally playing for, etc.). Figures 1 and 2 in
section 8 show examples of (German) formal and
semi-formal texts on one and the same game.
5 Processing Steps in MUMIS
5.1 Media Pre-Processing
Media material has been delivered in various
formats (AudioDAT, AudioCassettes, Hi-8 video
cassettes, DV video cassettes etc) and qualities.
All audio signals (also those which are part of
the video recordings) are digitized and stored in
an audio archive. Audio digitization is done with
20 kHz sample frequency, the format generated is
according to the de-facto wav standard. For dig-
itization any available tool can be used such as
SoundForge.
Video information (including the audio compo-
nent) of selected games have been digitized into
MPEG1 streams first. Later it will be encoded in
MPEG2 streams. While the quality of MPEG1 is
certainly not satisfying to the end-user, its band-
width and CPU requirements are moderate for
current computer and network technology. The
mean bit rate for MPEG1 streams is about 1.5
Mbps. Current state-of-the-art computers can ren-
der MPEG1 streams in real time and many net-
work connections (Intranet and even Internet) can
support MPEG1. MPEG2 is specified for about 3
to 5 Mbps. Currently the top-end personal com-
puters can render MPEG2, but MPEG2 is not yet
supported for the most relevant player APIs such
as JavaMediaFramework or Quicktime. When
this support is given the MUMIS project will also
offer MPEG2 quality.
For all separate audio recordings as for ex-
ample from radio stations it has to be checked
whether the time base is synchronous to that one
of the corresponding video recordings. In case of
larger deviations a time base correction factor has
to be estimated and stored for later use. Given that
the annotations cannot be created with too high
accuracy a certain time base deviation will be ac-
cepted. For part of the audio signals manual tran-
scriptions have to be generated to train the speech
recognizers. These transcripts will be delivered in
XML-structured files.
Since keyframes will be needed in the user in-
terface, the MUMIS project will develop software
that easily can generate such keyframes around a
set of pre-defined time marks. Time marks will
be the result of information extraction processes,
since the corresponding formal annotations is re-
ferring to to specific moments in time. The soft-
ware to be written has to extract the set of time
marks from the XML-structured formal annota-
tion file and extract a set of keyframes from the
MPEG streams around those time marks. A set of
keyframes will be extracted around the indicated
moments in time, since the estimated times will
not be exact and since the video scenes at such
decisive moments are changing rapidly. There
is a chance to miss the interesting scene by us-
ing keyframes and just see for example specta-
tors. Taking a number of keyframes increases the
chance to grab meaningful frames.
5.2 Multilingual Automatic Speech
Recognition
Domain specific language models will be trained.
The training can be bootstrapped from written re-
ports of soccer matches, but substantial amounts
of transcribed recordings of commentaries on
matches are also required. Novel techniques
will be developed to interpolate the base-line lan-
guage models of the Automatic Speech Recogni-
tion (ASR) systems and the domain specific mod-
els. Moreover, techniques must be developed to
adapt the vocabularies and the language models
to reflect the specific conditions of a match (e.g.,
the names players have to be added to the vocabu-
lary, with the proper bias in the language model).
In addition, the acoustic models must be adapted
to cope with the background noise present in most
recordings.
Automatic speech recognition of the sound
tracks of television and (especially) radio pro-
grammes will make use of closed caption subtitle
texts and information extracted from formal texts
to help in finding interesting sequences and auto-
matically transcribing them. Further, the domain
lexicons will help with keyword and topic spot-
ting. Around such text islands ASR will be used
to transcribe the spoken soundtrack. The ASR
system will then be enriched with lexica contain-
ing more keywords, to increase the number of se-
quence types that can be identified and automati-
cally transcribed.
5.3 Multilingual Domain Lexicon Building
All the collected textual data for the soccer do-
main are used for building the multilingual do-
main lexicons. This data can be in XML, HTML,
plain text format, etc. A number of automatic
processes are used for the lexicon building, first
on a monolingual and secondly on a multilin-
gual level. Manual browsing and editing is tak-
ing place, mainly in order to provide the semantic
links to the terms, but also for the fine-tuning of
the lexicon according to the domain knowledge.
Domain lexicons are built for four lan-
guages, namely English, German, Dutch and
Swedish. The lexicons will be delivered in a
fully structured, XML-compliant, TMX-format
(Translation Memory eXchange format). For
more information about the TMX format see
http://www.lisa.org/tmx/tmx.htm.
We will also investigate how far
EUROWORDNET resources (see
http://www.hum.uva.nl/ ewn/) can be of use
for the organization of the domain-specific
terminology.
5.4 Building of Domain Ontology and Event
Table
The project is currently building an ontology for
the soccer domain, taking into consideration the
requirements of the information extraction and
merging components, as well as users require-
ments. The ontology will be delivered in an XML
format8.
8There are still on-going discussions within the
project consortium wrt the best possible encoding for-
mat for the domain ontology, the alternative being
reduced probably to RDFS, OIL and IFF, see respec-
tively, and among others, http://www.w3.org/TR/rdf-
schema/, http://www.oasis-open.org/cover/oil.html and
http://www.ontologos.org/IFF/The%20IFF%20Language.
html
In parallel to building the ontology an event ta-
ble is being described. It contains the major event
types that can occur in soccer games and their
attributes. This content of the table is matching
with the content of the ontology. The event ta-
ble is a flat structure and guides the information
extraction processes to generate the formal event
annotations. The formal event annotations build
the basis for answering user queries. The event
table is specified as an XML schema to constrain
the possibilities of annotation to what has been
agreed within the project consortium.
5.5 Generation of Formal Annotations
The formal annotations are generated by the IE
technology and are reflecting the typical output of
IE systems, i.e.instantiated domain-specific tem-
plates or event tables. The slots to be filled by
the systems are basically entities (player, teams
etc.), relations (player of, opponents etc.) and
events (goal, substitution etc.), which are all de-
rived from the current version of the domain on-
tology and can be queried for in the online com-
ponent of the MUMIS prototype. All the tem-
plates associated with an event are including a
time slot to be filled if the corresponding informa-
tion is available in a least one of the sources con-
sulted during the IE procedure. This time infor-
mation is necessary for the indexing of the video
material.
The IE systems are applying to distinct sources
(FoT, FrT etc.) but they are not concerned with
achieving consistency in the IE result on distinct
sources about the same event (game): this is the
task of the merging tools, described below.
Since the distinct textual sources are differ-
ently structured, from ?formal? to ?free? texts, the
IE systems involved have adopted a modular ap-
proach: regular expressions for the detection of
Named Entities in the case of formal texts, full
shallow parsing for the free texts. On the base of
the factual information extracted from the formal
texts, the IE systems are also building dynamic
databases on certain entities (like name and age
of the players, the clubs they are normally playing
for, etc.) or certain metadata (final score), which
can be used at the next level of processing.
5.6 The Merging Tool
The distinct formal annotations generated are
passed to a merging component, which is respon-
sible for avoiding both inconsistencies and redun-
dancies in the annotations generated on one event
(in our case a soccer game).
In a sense one can consider this merging
component as an extension of the so-called co-
reference task of IE systems to a cross-document
(and cross-lingual) reference resolution task. The
database generated during the IE process will help
here for operating reference resolution for more
?verbose? types of texts, which in the context
of soccer are quite ?poetic? with respect to the
naming of agents (the ?Kaiser? for Beckenbauer,
the ?Bomber? for Mueller etc...), which would
be quite difficult to achieve within the sole refer-
ential information available within the boundary
of one document. The project will also investi-
gate here the use of inferential mechanisms for
supporting reference resolution. So for example,
?knowing? from the formal texts the final score
of a game and the names of the scorers, follow-
ing formulation can be resolved form this kind
of formulation in a free text (in any language):
?With his decisive goal, the ?Bomber? gave the
victory to his team.?, whereas the special nam-
ing ?Bomber? can be further added to the entry
?Mueller?
The merging tools used in MUMIS will also
take into consideration some general representa-
tion of the domain-knowledge in order to filter out
some annotations generated in the former phases.
The use of general representations9 (like domain
frames), combined with inference mechanisms,
might also support a better sequential organiza-
tion of some event templates in larger scenarios.
It will also allow to induce some events which
are not explicitly mentioned in the sources under
consideration (or which the IE systems might not
have detected).
5.7 User Interface Building
The user first will interact with a web-portal to
start a MUMIS query session. An applet will be
9Like for example the Type Description Language
(TDL), a formalism supporting all kind of operations on
(typed) features as well as multiple inheritance, see (Krieger
and Schaefer, 1994).
down-line loaded in case of showing the MUMIS
demonstration. This applet mainly offers a query
interface. The user then will enter a query that
either refers to metadata, formal annotations, or
both. The MUMIS on-line system will search
for all formal annotations that meet the criteria
of the query. In doing so it will find the appro-
priate meta-information and/or moments in some
media recording. In case of meta-information it
will simply offer the information in scrollable text
widgets. This will be done in a structured way
such that different type of information can eas-
ily be detected by the user. In case that scenes of
games are the result of queries about formal anno-
tations the user interface will first present selected
video keyframes as thumbnails with a direct indi-
cation of the corresponding metadata.
The user can then ask for more metadata
about the corresponding game or for more media
data. It has still to be decided within the project
whether several layers of media data zooming in
and out are useful to satisfy the user or whether
the step directly to the corresponding video frag-
ment is offered. All can be invoked by simple
user interactions such as clicking on the presented
screen object. Playing the media means playing
the video and corresponding audio fragment in
streaming mode requested from a media server.
6 Standards for Multimedia Content
MUMIS is looking for a compliance with exist-
ing standards in the context of the processing of
multimedia content on the computer and so will
adhere to emerging standards such as MPEG4,
which defines how different media objects will be
decoded and integrated at the receiving station,
and MPEG7, which is about defining standards
for annotations which can be seen as multime-
dia objects. Further, MUMIS will also maintain
awareness of international discussions and devel-
opments in the aerea of multimedia streaming
(RTP, RTSP, JMF...), and will follow the discus-
sions within the W3C consortium and the EBU
which are also about standardizing descriptions of
media content.
7 Role of MUMIS for the Annotation of
Multimedia Content
To conclude, we would like to list the points
where we think MUMIS will, directly or indi-
rectly, contribute to extract and access multimedia
content:
  uses multimedia (MM) and multilingual in-
formation sources;
  carries out multimedia indexing by applying
information extraction to a well-delineated
domain and using already existing informa-
tion as constraints;
  uses and extends advanced language tech-
nology to automatically create formal anno-
tations for MM content;
  merges information from many sources
to improve the quality of the annotation
database;
  application of IE to the output of ASR and
the combination of this with already existing
knowledge;
  definition of a complex information annota-
tion structure, which is stored in a standard
document type definition (DTD);
  integration of new methods into a query in-
terface which is guided by domain knowl-
edge (ontology and multilingual lexica).
So in a sense MUMIS is contributing in defin-
ing semantic structures of multimedia contents,
at the level proposed by domain-specific IE anal-
ysis. The full machinery of IE, combined with
ASR (and in the future with Image Analysis)
can be used for multimedia contents development
and so efficiently support cross-media (and cross-
lingual) information retrieval and effective navi-
gation within multimedia information interfaces.
There seems thus that this technolgy can play a
highly relevant role for the purposes of knowl-
edge detection and management. This is prob-
ably specially valid for the merging component,
which is eliminating redundancies in the annota-
tions generated from sets of documents and estab-
lishing complex reference resolutions, thus sim-
plyfying the access to content (and knowledge)
distributed over multiple documents and media.
References
Doug E. Appelt. 1999. An introduction to information
extraction. AI Communications, 12.
Steven Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech Com-
munication.
K. Bontcheva, H. Brugman, A. Russel, P. Wittenburg,
and H. Cunningham. 2000. An Experiment in
Unifying Audio-Visual and Textual Infrastructures
for Language Processing R&D. In Proceedings of
the Workshop on Using Toolsets and Architectures
To Build NLP Systems at COLING-2000, Luxem-
bourg. http://gate.ac.uk/.
Daan Broeder, Hamish Cunningham, Nancy Ide,
David Roy, Henry Thompson, and Peter Witten-
burg, editors. 2000. Meta-Descriptions and An-
notation Schemes for Multimodal/Multimedia Lan-
gauge Resources LREC-2000.
H. Brugman, K. Bontcheva, P. Wittenburg, and
H. Cunningham. 1999. Integrating Multimedia and
Textual Software Architectures for Language Tech-
nology. Technical report mpi-tg-99-1, Max-Planck
Institute for Psycholinguistics, Nijmegen, Nethed-
lands.
Hamish Cunningham. 1999. An introduction to infor-
mation extraction. Research memo CS - 99 - 07.
Thierry Declerck and G. Neumann. 2000. Using a pa-
rameterisable and domain-adaptive information ex-
traction system for annotating large-scale corpora?
In Proceedings of the Workshop Information Ex-
traction meets Corpus Linguistics, LREC-2000.
Kevin Humphreys, R. Gaizauskas, S. Azzam,
C. Huyck, B. Mitchell, H. Cunningham, and
Y. Wilks. 1998. University of sheffield:
Description of the lasie-ii system as used for
muc-7. In SAIC, editor, Proceedings of the
7th Message Understanding Conference, MUC-7,
http://www.muc.saic.com/. SAIC Information Ex-
traction.
Christopher Kennedy and B. Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora res-
olution without a parser. In Proceedings of the
16th International Conference on Computational
Linguistics, COLING-96, pages 113?118, Copen-
hagen.
Hans-Ulrich Krieger and U. Schaefer. 1994.

?
a type description language for constraint-based
grammars. In Proceedings of the 15th Interna-
tional Conference on Computational Linguistics,
COLING-94, pages 893?899.
Shalom Lappin and H-H. Shih. 1996. A generalized
algorithm for ellipsis resolution. In Proceedings
of the 16th International Conference on Compu-
tational Linguistics, COLING-96, pages 687?692,
Copenhagen.
John McNaught, editor. 2000. Information Extraction
meets Corpus Linguistics, LREC-2000.
Ruslan Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In Proceedings of the 17th In-
ternational Conference on Computational Linguis-
tics, COLING-98, pages 869?875, Montreal.
MUC, editor. 1995. Sixth Message Understanding
Conference (MUC-6). Morgan Kaufmann.
MUC, editor. 1998. Seventh Message Understanding
Conference (MUC-7), http://www.muc.saic.com/.
SAIC Information Extraction.
Guenter Neumann, R. Backofen, J. Baur, M. Becker,
and C. Braun. 1997. An information extrac-
tion core system for real world german text pro-
cessing. In Proceedings of the 5th Conference on
Applied Natural Language Processing, ANLP-97,
pages 209?216.
Guenter Neumann, C. Braun, and J. Piskorski. 2000.
A divide-and-conquer strategy for shallow parsing
of german free texts. In Proceedings of the 6th Con-
ference on Applied Natural Language Processing,
ANLP-00.
Jakub Piskorski and G. Neumann. 2000. An intel-
ligent text extraction and navigation system. In
Proceedings of the 6th Conference on Recherche
d'Information Assiste?e par Ordinateur, RIAO-2000.
Project URLs:
COL:  
			ffExtending NLP Tools Repositories for the Interaction with Language
Data Resources Repositories
Thierry Declerck
DFKI GmbH
Stuhlsatzenhausweg 3
D-66123 Saarbruecken
Germany
declerck@dfki.de
Abstract
This short paper presents some mo-
tivations behind the organization of
the ACL/EACL01 ?Workshop on Shar-
ing Tools and Resources for Research
and Education?, concentrating on the
possible connection of Tools and Re-
sources repositories. Taking some pa-
pers printed in this volume and the ACL
Natural Language Software Registry as
a basis, we outline some of the steps to
be done on the side of NLP tool reposi-
tories in order to achieve this goal.
1 Introduction
The main goal of the ACL/EACL01 ?Workshop
on Sharing Tools and Resources for Research and
Education? is to discuss methods for the improve-
ment and extension of existing repositories. In
this paper we briefly address one of the central
discussion point of the workshop: how to achieve
a close interlinking between NLP tools and NL
resources repositories. We will base this discus-
sion on the ACL Natural Language Software Reg-
istry (see (Declerck et al, 2000)) and some papers
printed in these proceedings (see the list of papers
in the bibliography).
The necessity of having repositories for NLP
tools has already been clearly recognized in the
past, and recently this topic has also been ad-
dressed within the broader context of a confer-
ence on Language Resources (see (Chaudiron et
al., 2000) and (Declerck et al, 2000)). (Chaud-
iron et al, 2000) is essentially concerned with the
question of identifying the NLP supply according
to its different uses, and thus is describing a user-
oriented approach to NLP tools repositories. (De-
clerck et al, 2000) is mainly describing the func-
tionalities of the new version of the ACL Natural
Language Software Registry, also showing how
this version can overcome some of the practical
problems encountered by former repositories (a
summarized presentation of the ACL Registry is
given below in section 2). Both papers are also
discussing the problem of proposing a good tax-
onomy of NLP tools: user oriented versus de-
veloper oriented, top-down versus bottom-up ap-
proach, coarse-grained versus fine-grained classi-
fication and the way those classification strategies
could cooperate. So for sure there is also still a
need for establishing a cooperation between dis-
tinct approaches to NLP tools classification and
their implementation, and a corresponding dis-
cussion is going on.
But since NLP tools are of interest only if they
have language data they can process and trans-
form, and Language Data Resources are only of
interest if there is a clear indication on how they
can be accessed and processed, there is a also
a real need of establishing descriptive links be-
tween the two types of repositories in which tools
on the one side and language data resources on
the other side are included. This will allow people
using a certain tool to easily find the type of lan-
guage data they need. And the other way round:
people having language data can easily find the
type of tools that can produce some added-value
for their data. The successful establishment of
such a connection between these two types of
repositories will probably require as well a par-
tial reorganization of the NLP repositories on the
one hand and the language data repositories on
the other hand in order to maximally respond to
the overall requirement of what at the end will be
an infrastructure1 for discovering, accessing and
combining language related resources and tools.
This paper is specially addressing some of the
extensions the ACL Registry is undergoing in or-
der to offer a valuable contribution to this infras-
tructure.
2 The ACL Natural Language Software
Registry
The Natural Language Software Registry (NLSR)
is a concise summary of the capabilities and
sources of a large amount of natural language
processing (NLP) software available to the NLP
community.2 It comprises academic, commercial
and proprietary software with specifications and
terms on which it can be acquired clearly indi-
cated.
The visitor of the NLSR has two types of
access to the information stored in the NLSR:
browsing through the hierarchically organized list
of products (the maximal depth for browsing is
level 3) or by querying for the specifications of
the products as they are listed in the Registry.
This querying functionality is helping the visi-
tor in finding potential relevant software, since he
or she is be able to formulate standard queries,
whereas a menu allows to constrain the search to
certain aspects of the listed products. So it is pos-
sible to query for example for all freely available
morphological analyzer for Spanish running on a
specific platform. Products can be listed in dis-
tinct sections. In order to know in which sections
a product is to be found, the user can submit a
standard query to the Registry Database.
The underlying classification of the actual ver-
sion of the ACL Registry is largely based on the
book (Varile and Zampolli, 1996). But this taxon-
omy will probably have to be further specialized
and extended in order to satisfy the majority of
the visitors of the NLSR. Therefore the classifi-
cation can be enriched by the products submitted
1As (Bird and Simons, 2001) names it.
2See http://registry.dfki.de/
and/or by comments made by the visitors, intro-
ducing thus a bottom-up, developer and/or user
oriented classification.
A general goal of the most recent editions of
the NLSR was the simplification of the registra-
tion procedure, providing a short form to be filled
by the customer. We do not request anymore an
exhaustive description of the submitted product,
but concentrate on few points providing a guiding
for the visitor, who will have to consult the home
page of the institutions or authors having submit-
ted their product for getting more detailed infor-
mation. In accordance with this simplification of
the registration procedure, institutes or companies
submitting their NLP products to the ACL Natural
Language Software Registry are required to give
their URL.
3 Extending the ACL Natural Language
Software Registry
The ACL Registry was till recently a closed
world, in the sense that information encoded in
it could be accessed only by browsing or query-
ing within its web page. Obviously there is a
need for getting access to this information with-
out having to activate a web browser. Therefore
it was planned to provide for an XML export,
since XML is the standard for exchanging struc-
tured documents. And this need was getting even
more urgent after the Registry Team was asked
for permission of harvesting the ACL repository
for the purpose of creating a prototype service
provider in the context of an Open Archive Ini-
tiative for Language Resources, which is called
OLAC (Open Language Archives Community)
and described in (Bird and Simons, 2001).
This excellent initiative also requires that the
information provided by tools repositories is not
only universally available but also has to con-
form to certain standards for metadata descrip-
tion. This in order to ensure the interoperability
across all the repositories participating as meta-
data providers in OLAC.
4 XML for Tools Repositories
(Erjavec and Va?radi, 2001) are proposing a very
interesting description of the TELRI-II concerted
action for a tool catalogue specialized for cor-
pus processing tools. This ?limitation? in the
coverage of the repository TELRI repository is
allowing the authors to make extensive experi-
ments with various XML specifications and tools
for the building and display of their catalogue.
An experience which should be beneficial for
the more generic ACL Registry, as well as for
other provider of tools repositories (so for ex-
ample national initiatives, like the one described
in (Chaudiron et al, 2000)). The authors also
mention one advantage of the limitation in the
coverage of tools: the presence in the entries
of a pointer to persons or institutions being able
to offer advice on installing and using the soft-
ware. Thus addressing also one point mentioned
in (Bird and Simons, 2001), where 3 main classes
of providers are described: DATA, TOOLS and
ADVICE providers.
But (Erjavec and Va?radi, 2001) are not propos-
ing a discussion on how to integrate in the de-
scription of the tools the particular relation to
a specific corpus. Nevertheless this should be
a common task to be tackled by all providers
of tools repositories. Probably it would be the
best strategy to start with specialized repositories,
where the problems to solve can appear earlier.
5 Metadata for NLP Tools
As we saw above, the sole conformance to stan-
dards (XML) for document description and inter-
change is not enough in the context of OLAC. But
the use of metadata descriptions for tools seems to
make sense not only for such initiatives. (Lavelli
et al, 2001) show the use of metadata descrip-
tion for tools in the context of an infrastructure for
NLP application development. The role of meta-
data there is to specify the ?level of analysis ac-
complished by the source processor?. Thus the
metadata descriptions are useful for the commu-
nication between processes within an NLP chain,
and also allow to mark and identify the document
produced by such a process. In any cases, the
use of metadata description for tools (or processes
triggered by those tools) is probably a key-issue in
the modular design of complex NLP environment.
And one can see in the SiSSA approach to
metadata descriptions for NLP processes, maybe
as a side effect, a proposition for sharing anno-
tations for processes and documents (resources)
that can be handled. This might be a starting point
for the systematic connection of the descriptions
of both NLP tools and language resources.
6 Connection with
Metadata-Descriptions for
(Multimedia/Mltimodal) Language
Resources
Catalogue and repositories for Natural Language
data resources have already been working on the
topic of metadata description for their entries (See
for example LDC and ELRA). One can see OLAC
as a natural extension of the LDC, enlarging the
resources catalogue to a real infrastructure for
language resource identification.
From the side of the Language Engineering
there are initiatives for describing standards and
(Calzolari et al, 2001) present such an initia-
tive, the ISLE project, which is the continuation
of the EAGLES initiative. The main objective
of ISLE is to promote ?widely agreed and ur-
gently demanded standards and guidelines for in-
frastructural language resources ..., tools that ex-
ploit them and LE products?. The ongoing dis-
cussions within this project are thus important for
the intended extension of NLP tools repositories.
While (Calzolari et al, 2001) concentrate on
the description of the task of the ISLE compu-
tational lexicon working group and address the
topic of metadata for encoding multilingual lex-
ical resources, (Broeder and Wittenburg, 2001)
presents the work of the ISLE Metadata initiative
(IMDI), which is directly relevant for the topic
addressed here. (Broeder and Wittenburg, 2001)
give a good overview of metadata initiatives for
Language Resources and propose a contrastive
description of OLAC and IMDI, where the main
distinction can be seen in the top-down versus
bottom-up approach. The top-down approach fol-
lowed by OLAC allows an easy conformance to
the Dublin Core set, whereas the bottow-up ap-
proach requires the definition of more ?narrow
and specialized categorization schemes?.
This distinction is important for the intended
extension of the metadata description for NLP
tools, since the description of the tools will have
to connect to those distinct kinds of categorization
schemes for data resources. We think here that the
ACL Registry can easily be adapted to this situ-
ation since the actual classification of tools is a
layered one, one layer being quite general (clas-
sifying tools wrt broader application types, like
?Written Language?), and the next layer stressing
more the specific technology (for example Infor-
mation Extraction versus Text Alignment).
(Broeder and Wittenburg, 2001) is also propos-
ing a scheme for connecting the descriptions of
tools and resources. They suggest not to include a
listing of tools in the metadata description of the
resources, since this set of tools would be chang-
ing in time. Rather they suggest a detailed de-
scription of the type and the structure of the re-
sources that can be accessed by a ?browser? tool,
which on the basis of the detailed metadata de-
scription can select potential tools for handling
the resources. The tools repository would have
to include this kind of information in its metadata
description of the tools.
7 Conclusion
As we could see out of this (not exhaustive) se-
lection of papers submitted to the ACL/EACL01
?Workshop on Sharing Tools and Resources for
Research and Education?, there are a lot of very
interesting and promising, implicit or explicit,
suggestions for the goal of connecting tools and
resources repositories. The ACL Natural Lan-
guage Registry will take these suggestions as the
basis of the further work on providing extensions
to metadata descriptions in order to be as com-
pliant as possible to emerging infrastructures and
standards for language resources.
References
S. Bird and G. Simons. 2001. The OLAC Metadata
Set and Controlled Vocabularies. In This volume.
D. Broeder and P. Wittenburg. 2001. Interaction
of Tools and Metada-Descriptions for Multimedia
Language Resources. In This volume.
N. Calzolari, A. Lenci, and A. Zampolli. 2001. Inter-
national Standards for Multilingual Resource Shar-
ing: The ISLE Computational Lexicon Working
Group. In This volume.
S. Chaudiron, K. Choukri, A. Mance, and V. Mapelli.
2000. For a repository of NLP tools. In LREC 00,
pages 1273?1278.
T. Declerck, A.W. Jachmann, and H. Uszkoreit. 2000.
The new Edition of the Natural Language Software
Registry (an initiative of ACL hosted at DFKI). In
LREC 00, pages 1129?1132. http://registry.dfki.de.
T. Erjavec and T. Va?radi. 2001. The TELRI tool cata-
logue: structure and prospect. In This volume.
A. Lavelli, F. Pianesi, E. Maci, I. Prodanof, L. Dini,
and G. Mazzini. 2001. SiSSA ? An Infrastructure
for NLP Application Development. In This volume.
G.B. Varile and A. Zampolli. 1996. Survey of
the State of the Art in Human Language Tech-
nology. http://www.cse.ogi.edu/CSLU/HLTsurvey/
HLTsurvey.html.
Annotating text using the Linguistic Description Scheme of MPEG-7: 
The DIRECT-INFO Scenario 
 
 
Thierry Declerck, Stephan Busemann 
Language Technology Lab  
DFKI GmbH  
Saarbr?cken, Germany 
{declerck|busemann}@dfki.de 
Herwig Rehatschek, Gert Kienast 
Institute for Information Systems & 
Information Management,  
JRS GmbH  
Graz, Austria 
{rehatschek|kienast}@joanneum.at 
 
 
Abstract 
We describe the way we adapted a text 
analysis tool for annotating with the Lin-
guistic Description Scheme of MPEG-7 
text related to and extracted from multi-
media content. Practically applied in the 
DIRECT-INFO EC R&D project we 
show how such linguistic annotation con-
tributes to semantic annotation of multi-
modal analysis systems, demonstrating 
also the use of the XML schema of 
MPEG-7 for supporting cross-media se-
mantic content annotation. 
1 Introduction 
In the R&D project DIRECT-INFO the concrete 
business case of sponsorship tracking was tar-
geted. The scenario investigated within the pro-
ject was that sponsors want to know how often 
their brands are mentioned in connection with 
the sponsored company. The visual detection of a 
brand (e.g. in videos) is not sufficient to meet the 
requirements of this business case. Multimodal 
analysis and fusion ? as implemented within DI-
RECT-INFO ? is needed in order to fulfill these 
requirements (Rehatschek, 2004).  
Within this context text analysis has been ap-
plied to documents reporting on entities, like 
football teams, that have close relations to large 
sponsoring companies. In the text analysis com-
ponent of the system we had to detect if an entity 
was mentioned positively, negatively or neu-
trally. Besides all the processing and annotation 
issues to positive or negative mentions, we had 
to make our results available to a global MPEG-7 
document, which is encoding the annotation re-
sults of various analysis of the modalities in-
volved (logo detection, speech recognition, text 
analysis etc.). This global MPEG-7 document 
was the input for a fusion component. 
In the next sections we describe the Text 
Analysis (TA) component of DIRECT-INFO. 
We then briefly describe the linguistic descrip-
tion scheme (LDS) of MPEG-7 and show the 
annotation generated by the TA. Finally we 
briefly discuss the role the LDS, and generally 
speaking MPEG-7, can play in supporting an 
interoperable cross-media annotation strategy. It 
seems to us, that LDS is offering a good mean 
for adding semantic metadata to image/video, but 
not for a real semantic integration of text and 
media content annotation, which in the case of 
DIRECT-INFO was performed by an additional 
fusion component. 
2 The detection of positive/negative 
mentioning 
Our work in DIRECT-INFO has been dedicated 
in enhancing an already existing tool for linguis-
tic annotation. This tool, called SCHUG (Shal-
low and CHunk-based Unification Grammar 
tool), is annotating texts considering both lin-
guistic constituency and dependency structures 
(T. Declerck, M. Vela 2005). 
A first development step was dedicated in cre-
ating specialized lexicons for various types of 
lexical categories (like nouns, adjectives and 
verbs) that can bear the property of being intrin-
sically positive or negative in a specific domain, 
as can be seen just below in the case of soccer: 
 
command => {POS => Noun, INT => "positive"} 
dominate => {POS => Verb, INT => "positive"} 
weak => {POS => Adj, INT => "negative"} 
 
Considering a sentence like ?ManU takes the 
command in the game against the weak Spanish 
53
team?, the head-noun of the direct object (lin-
guistically speaking) ?the command? gets from 
the access to the specialized DIRECT-INFO 
lexicon a tag ?INTERPRETATION? with value 
?positive?. Whereas the adjective ?weak? in the 
PP-adjunct ?in the game against the weak Span-
ish team? gets an ?INTERPRETATION? tag 
with value ?negative?.  
Once the words in the sentence have been 
lexically tagged with respect to their interpreta-
tion, the computing of the pos./neg. interpreta-
tion at the level of linguistic fragments and then 
at the level of the sentences can start. For this we 
have defined heuristics along the lines of the de-
pendency structures delivered by the linguistic 
analysis. So in the case of the NP ?the weak 
Spanish team?, the head noun ?team?, as such a 
neutral expression, is getting the ?INTERPRE-
TATION? tag with the value ?negative?, since it 
is modified by a ?negative? adjective. In case the 
reference resolution algorithm of the linguistic 
tools has been able to specify that the ?Spanish 
team? is in fact ?Real Madrid? this entity gets a 
negative ?INTERPRETATION? tag. 
The head noun of the NP realizing the subject 
of the sentence, ?ManU? gets a positive mention 
tag, since it is the subject of a positive verb and 
direct object combination (the NP ?the com-
mand? having a positive reading, whereas the 
verb ?takes? has a neutral reading). 
A last aspect to be mentioned here concerns 
the treatment of the so-called polarity items. 
Specific words in natural language intrinsically 
carry a negation or position force (or scope). So 
the words not, none or no have an intrinsic nega-
tion force and negate the words and fragments in 
the context in which those specific words are 
occurring. The context that is negated by such 
words can be also called the ?scope? (or the 
range) of the negation. Consider for example the 
sentence: ?I would definitely pay ?15 million to 
get Owen, not even a decent striker, instead?? 
Our tools are able to detect that the NP ?decent 
striker? is negated, and therefore the positive 
reading of ?decent striker? is being ruled out. 
3 Metadata Description 
The different content analysis modules of the 
DIRECT-INFO system extract different types of 
metadata, ranging from low-level audiovisual 
feature descriptions to semantic metadata. The 
global metadata description must be rich and has 
to clearly interrelate the various analysis results, 
as it is the input of the fusion component. 
4.1 Using MPEG-7 for Detailed Description of 
Audiovisual Content 
In DIRECT-INFO the MPEG-7 standard is used 
for metadata description. It is an excellent choice 
for describing audiovisual content because of its 
comprehensiveness and flexibility. The compre-
hensiveness results from the fact that the stan-
dard has been designed for a broad range of ap-
plications and thus employs very general and 
widely applicable concepts. The standard con-
tains a large set of tools for diverse types of an-
notations on different semantic levels. The flexi-
bility of MPEG-7, which is provided by a high 
level of generality, makes it usable for a broad 
application area without imposing strict con-
straints on the metadata models of these applica-
tions. The flexibility is very much based on the 
structuring tools and allows the description to be 
modular and on different levels of abstraction. 
MPEG-7 supports fine grained description, and it 
is possible to attach descriptors to arbitrary seg-
ments on any level of detail of the description.  
Among the descriptive tools developed within 
the MPEG-7 framework, one is concerned with 
the use of natural language for adding metadata 
to the content description of image and video: the 
so-called Linguistic Description Scheme (LDS). 
4.2 MPEG-7: The Linguistic Description 
Scheme (LDS) 
MPEG-7 foresees four kinds of textual annota-
tion that can be attached as metadata to some 
audio-video content. The natural language ex-
pression used here is ?Spain scores a goal against 
Sweden. The scoring player is Morientes?. 
Free Text Annotation: Here only tags are put 
around the text: 
<TextAnnotation> 
   <FreeTextAnnotation xml:lang="en"> 
   Spain scores a goal against Sweden. 
   The scoring player is Morientes. 
   </FreeTextAnnotation> 
</TextAnnotation> 
 
Key Word Annotation: Key Words are ex-
tracted from text and correspondingly annotated: 
<TextAnnotation> 
   <KeywordAnnotation> 
     <Keyword>score</Keyword> 
     <Keyword>Sweden</Keyword> 
     <Keyword>Spain</Keyword> 
     <Keyword>Morientes</Keyword> 
   </KeywordAnnotation> 
</TextAnnotation> 
 
 
 
54
Structured Annotation: Question/Answering 
like semantics is associated to the text: 
<TextAnnotation> 
  <StructuredAnnotation> 
    <Who><Name>Spain</Name></Who> 
    <WhatAction><Name>score      
goal</Name></WhatAction> 
    <Where><Name>A Coru?a, 
Spain</Name></Where> 
    <When><Name>March 25, 
1998<Name></When> 
  </StructuredAnnotation> 
</TextAnnotation> 
 
Dependency Structure: Here the full linguis-
tic apparatus is used for annotating the text: 
<TextAnnotation> 
 <DependencyStructure> 
  <Sentence> 
   <Phrase operator="subject"> 
    <Head type="noun">Spain</Head> 
   </Phrase> 
   <Head type="verb" base-
Form="score">scored</Head> 
   <Phrase operator="object"> 
    <Head type="article noun">a 
goal</Head> 
   </Phrase> 
   <Phrase> 
    <Head 
type="preposition">against</Head> 
   <Phrase> 
    <Head>Sweden</Head></Phrase> 
   </Phrase> 
  </Sentence> 
 </DependencyStructure> 
</TextAnnotation>1 
4 MPEG-7 Format of the Text Analysis 
component in DIRECT-INFO 
On the base of the linguistic analysis of our de-
pendency  parser, we generate the ?structured 
annotation? of the MPEG-7 Linguistic Descrip-
tion Scheme. We think that this kind of annota-
tion is the most practical of LDS for adding se-
mantics to multimedia content, since it is proba-
bly more intuitive for the media expert as the 
underlying linguistic dependency structure. At 
the same  time it seems also straightforward to 
go first for a (internal) dependency analysis, 
since it is then relatively easy to map automati-
cally dependency units to the ?Who?, ?WhatAc-
tion? and other tags of LDS. 
The MPEG-7 output of the TA module of DI-
RECT-INFO looks like: 
 
<MediaInformation> 
 <MediaProfile> 
  <MediaFormat> 
   <Content href="http://www.direct-
info.net/mpeg7/cs/ContentCS.2004.xml/di.
content.writtenText"> 
    <Name>Written text</Name> 
                                                 
1
 These examples are taken from a former and excellent 
online tutorial on MPEG-7 by Philippe Salembier.  
   </Content> 
  </MediaFormat> 
  <MediaInstance> 
   <InstanceIdentifier/> 
   <MediaLocator> 
    <!-- essence id--> 
    <MediaUri>5543</MediaUri> 
   </MediaLocator> 
  </MediaInstance> 
 </MediaProfile> 
</MediaInformation> 
<StructuralUnit href="http://www.direct-
info.net/mpeg7/cs/StructuralUnitCS.2004.
xml/di.vis.pdf"> 
 <Name>PDF</Name> 
</StructuralUnit> 
 <!-- more than one page can be stored  
within a file --> 
<SpatialDecomposition criteria="Page"> 
 <StillRegion id="TA_PAGE1"> 
  <StructuralUnit 
href="http://www.direct- 
info.net/mpeg7/cs/StructuralUnitCS.2004.
xml/di.vis.page"> 
   <Name>Page</Name> 
  </StructuralUnit> 
 <SpatialDecomposition   crite-
ria="TextAnalysis" gap="true" over-
lap="false"> 
  <StillRegion> 
   <StructuralUnit 
href="http://www.direct-
info.net/mpeg7/cs/StructuralUnitCS.2004.
xml/di.vis.textAnal  ysisAnnotation"> 
    <Name>Text analysis annota-
tion</Name> 
   </StructuralUnit> 
   <TextAnnotation> 
    <StructuredAnnotation> 
     <WhatObject 
href="http://www.direct-
info.net/mpeg7/cs/LogoCS.2004.xml/di.ta.
object.juventus"> 
      <Name 
xml:lang="it">Juventus</Name> 
     </WhatObject 
     <WhatAction 
href="http://www.direct-
info.net/mpeg7/cs/TextAnalysisCS.2004.xm
l/di.ta.action.teamMentioned"> 
<Name xml:lang="it">mentioning of 
team</Name> 
     </WhatAction> 
     <Why> 
      <Name xml:lang="it"> 
295 771120 Con DVD Auto da Sogno Porsche 
e 10, con calendario ufficiale 2006 Ju-
ventus o Milan" o Inter o Palermo o 
Fiorentina o Totti" o Wrestling" e 6, 9 
Euro 1, Poste Italiane Sped . in A.P 
      </Name> 
     </Why> 
     <How href="http://www.direct-
info.net/mpeg7/cs/TextAnalysisCS.2004.xm
l/di.ta.mentioning.neut"> 
     <Name xml:lang="it">neut</Name> 
   </How> 
  </StructuredAnnotation> 
 </TextAnnotation> 
</StillRegion> 
 
Without going into too much detail here, it is 
enough to stress that in the first part of the anno-
tation, the link to the general multimedia and 
multimodal repository is ensured. We have to 
55
deal with a PDF document that should be proc-
essed by a Text Analysis tool. The ?essence? ID 
is giving information about the location where 
the application relevant data is stored and where 
the results of the Text Analysis should be stored. 
All this metadata is ensuring the combination of 
the results of the analysis of various modalities 
dealing with one application relevant dataset (for 
example the combination of the logo detection of 
a brand and the related positive or negative men-
tioning of a team sponsored by this brand).  For 
reason of place, we can not show and comment 
here the complete (and multimodal) MPEG-7 
annotation, but details are given in (G. Kienast, 
2005). 
The second part of the annotation gives the re-
sults of the combined linguistic and ?structured? 
analysis we are dealing with. As mentioned 
above, in the case of DIRECT-INFO, results of 
text analysis are accessed via the structured an-
notation of the Linguistic Description Schema of 
MPEG-7. 
5 Conclusions and future Work 
In the DIRECT-INFO project we managed to 
include results of text analysis in an automated 
fashion into a MPEG-7 description, which was 
dealing with the XML representation of the 
analysis of various modalities. Using correspond-
ing metadata, it was possible to ensure the en-
coding/annotation of the related results in one 
file and to facilitate the access to the separated 
annotation using XPath. As such the DIRECT-
INFO MPEG-7 annotation schema is offering a 
practicable multi-dimensional annotation 
scheme, if we consider a ?dimensions? as being 
the output of the analysis of various modalities. 
MPEG-7 proved to be generic and flexible 
enough for combining, saving and accessing 
various types of annotation.  
Limitations of MPEG-7 were encountered 
when the task was about fusion or merging of 
information encoded in the various descriptors 
(or features), and this task was addressed in a 
posterior step, whereas the encoding scheme of 
MPEG-7 was not longer helpful, in defining for 
example relations between the annotation result-
ing from the different modules or for defining 
constraints between those annotation. There 
seems to be a need for a higher level of represen-
tation for annotation resulting from the analysis 
of distinct media, being low-level features for 
images or high-level semantic features for texts.  
The need of  an ?ontologization? of multime-
dia features has been already recognized and pro-
jects are already dealing with this, like AceMe-
dia. Initial work in relating multimodal annota-
tion in DIRECT-INFO will be further developed 
in K-Space, a new Network of Excellence, which 
goal is to provide for support in semantic infer-
ence for both automatic and semi-automatic an-
notation and retrieval of multimedia content. K-
Space aims at closing the ?semantic gap? be-
tween the low-level content descriptions and the 
richness and subjectivity of semantics in high-
level human interpretations of audiovisual media. 
6 Acknowledgements 
The R&D work presented in this paper was par-
tially conducted within the DIRECT-INFO pro-
ject, funded under the 6th Framework Programme 
of the European Community within the strategic 
objective "Semantic-based knowledge manage-
ment systems" (IST FP6-506898). Actual work 
on interoperability of media, language and se-
mantic annotation is being funded by the Net-
work of Excellence K-Space (IST FP6-027026). 
References  
T. Declerck, J. Kuper, H. Saggion, A. Samiotou, P. 
Wittenburg, J. Contreras. Contribution of NLP to 
the Content Indexing of Multimedia Documents. In 
Lecture Notes in Computer Science Volume 3115 / 2004 
Pages 610-618,Springer-Verlag Heidelberg, 6 2004. 
T. Declerck, M. Vela, ?Linguistic Dependencies as a 
Basis for the Extraction of Semantic Relations?, in 
Proceedings of the ECCB'05 Workshop on Bio-
medical Ontologies and Text Processing, Madrid 
(2005) 
G. Kienast, A. Horti, Andr?s, H. Rehatschek, S.  
Busemann, T.    Declerck, V. Hahn and R. Cavet. 
?DIRECT INFO: A Media Monitoring System for 
Sponsorship Tracking.? In Proceedings of the 
ACM SIGIR Workshop on Multimedia Information 
Retrieval. 2005. 
H. Rehatschek: "DIRECT-INFO: Media monitoring 
and multimodal analysis for time critical deci-
sions". Proceedings of the 5th International Work-
shop on Image Analysis for Multimedia Interactive 
Services (WIAMIS), ISBN-972-98115-7-1, Lis-
bon, April 2004. 
AceMedia project: http://www.acemedia.org/aceMedia 
DIRECT-INFO project: http://www.direct-info.net/ 
K-Space project: http://kspace.qmul.net/ 
MPEG-7: http://www.chiariglione.org/mpeg/ 
 
56
Proceedings of the 8th International Conference on Computational Semantics, pages 346?350,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Concept and Relation Extraction in the Finance
Domain
Mihaela Vela
DFKI Saarbr?cken
Mihaela.Vela@dfki.de
Thierry Declerck
DFKI Saarbr?cken
Thierry.Declerck@dfki.de
Abstract
In this paper, we describe the state of our work on the possible deriva-
tion of ontological structures from textual analysis. We propose an ap-
proach to semi-automatic generation of domain ontologies from scratch,
on the basis of heuristic rules applied to the result of a multi-layered
processing of textual documents.
1 Introduction
In the context of the MUSING R&D European project
1
, which is dedi-
cated to the development of Business Intelligence (BI) tools and modules
founded on semantic-based knowledge and content systems, we are investi-
gating among others the integration of semantic web and human language
technologies for enhancing the technological foundations of knowledge ac-
quisition and reasoning in BI applications.
In the first phase of the project many efforts have been dedicated to
the manual creation of domain related ontologies and their integration in
upper level ontologies. The creation of those ontologies was guided by do-
main experts, who were submitting so-called competency questions to the
ontology engineers in order to support they work on ontology design and
implementation. Since this approach to ontology creation is very time and
resource consuming, we wanted to investigate the possible automation of
ontology building directly from textual documents, with a special focus on
the financial domain.
2 The Approach
We consider the semi-automatic ontology derivation from text as a linguistic
rule-based approach, which on the basis of lexical and syntactic properties
1
See www.musing.eu for more details
346
can suggest potential ontology classes and properties that can be used for
building an ontology.
We suggest a multi-layered approach, which starts with a very shallow
analysis of certain lexical properties of words or very short combination of
words, going from there to Part-of-Speech (POS) Tagging and morphological
analysis, before using, in a next step, deeper syntactic analysis and taking
into account larger parts of text, up to the level of sentences or even para-
graphs. The idea behind this: at the shallow level it is possible to detect
possible classes and relations, which can then be consolidated, refined or re-
jected at further stages of textual analysis, with the help of domain experts
and ontology engineers.
Our final goal is to clearly state what kind of ontological resource can be
extracted from financial documents (annual reports of companies, financial
newspapers) at various level of textual processing. As a data source we work
first with a corpus of economical news articles from the German newspaper
Wirtschaftswoche.
3 String-Based Processing
As a first step in the task of extracting ontology concepts and relations from
(German) textual documents, we decided to look in the corpus for words
occurring alone (and starting with a capital letter) and in the context of
larger strings (which we assume to be mostly nominal compounds). We
call the words that show this property "anchor-words". We consider them
potential labels of ontology classes and the compounds, in which they occur,
as expressing potential relations for the labels of ontology classes.
Take for example the anchor-word konzern (corporation), which is also
occurring as part of the compound medienkonzern (media corporation). At
this very shallow and pattern-based processing level, we tentatively derive
that from the compound construction PREFIX + ANCHOR we can extract
medienkonzern ISA_SUBCLASS_OF konzern. Another example of com-
pound is konzernverwaltung (corporation management). Here we derive
from the compound construction ANCHOR + SUFFIX the relation: konzern
HAS verwaltung (corporation HAS management);
Although the examples demonstrate that a string analysis can to some
extent propose some guidelines for ontology extraction, there are for sure
major limitations, due to the lack of well-defined domain and range spec-
ifications in the proposed relations, the constraint relative to the number
of extracted relations and classes, the multiple appearance of morphological
variations, the lack of textual context etc.
In order to reduce the limitations just mentioned, we started by looking
for alternative formulations of the compounds, which can help in establishing
some filters and validation steps for relation extraction. So for example the
347
expression Chef vom Konzern (chief of the corporation) is validating the
property relation Konzern HAS Chef (corporation HAS chief ), due to the
meaning we can associate with the preposition von (part-of, belonging-to).
Concerning compound reformulations expressed by genitive post- modifi-
cation, like mitarbeiter einer deutschen bank (employees of a german bank),
we can see that they validate the relation extracted from the corresponding
compounds, since the genitive constructions have here a part-of/belonging-
to meaning.
4 Morphology and Lexical Semantics for Ontology
Derivation
A way to reduce some of the limitations described in Section 3 lies in the
use of morpho-syntactic information. So for example the word Firmenchef
(the boss of the firm) would be analyzed as follows:
(1) <W INFL="[17 18 19]" POS="1" STEM="chef" COMP="firmenchef"
TC="22">Firmenchef</W>
This annotation is to read like this: the word Firmenchef has the stem
chef, has POS noun, is the result of combining the word Firmen and the
word Chef, and has certain morphological properties (here encoded with
numbers). We can then describe certain morphological constraints for filter-
ing out some suggested relations from Section 3. For example, Chemiekonz-
ern is introducing a subclass relation between Chemiekonzern and Konzern,
whereas for Grosskonzern (large corporation) the subclass relation between
Grosskonzern and Konzern does not apply. The constraint proposed for
solving this kind of ambiguities is: the compound should consists of two
nouns.
Lexical semantics can also improve the quality of relation extraction.
For the compound Chefdenker (chief thinker), we want to ensure that no
HAS-relation between an ontology class labeled by chief and thinker is sug-
gested. For this purpose we use lexical semantic resources, like WordNet,
and formulate a rule that states that if the word occurring in the prefix
position of a compound is a person, and the second part of the compound
is also denoting a person, then the HAS-relation can not be derived.
Despite of the improvements made possible by morphology and lexical
semantics a major limitation remains: ontology extraction is proposed only
on the basis of word analysis and not on the basis of phrases and sentences,
which offer more context.
348
5 Syntactic Information for the Generation of On-
tologies
By combining the processing steps described above, we were able to extract
possible relevant ontology classes, relations and properties. For further im-
provement we need to consider both the linguistic context and some infor-
mation available in ontologies so far. The syntactic analysis of the sentence
below is a good example to show how a larger linguistic context can help
improving the method described in this paper.
(2) [NP-Subj Er] [VG soll] [PP im Konzern] [NP-Ind-Obj Finanzchef [NE-Pers
Gerhard Liener] ] [VG folgen]
Through the syntactic structuring of the sentence, we can semantically
group the items, so that we can extract the fact that a financial chief is
within a corporation, since the description of job succession is within a cor-
poration (marked by the prepositional phrase im Konzern). This aspect of
ontology learning is being currently investigated and implemented.
6 Conclusions and Further Work
In this paper we have been describing a multi-layer textual analysis strat-
egy that can help in building up ontologies from scratch, or integrate new
suggested ontology classes (or relations and properties) into existing ontolo-
gies. Since this is work in progress we also intended to get a clearer picture
on what kind of ontological knowledge can be extracted from the different
layers of textual processing.
For the "shallowest" parts of our suggested approach we could see that
proposed labels for ontology classes and relations seem to be appropriate.
For sure, some evaluations of this work has to be done. Nevertheless, we
see a big potential in a combination of suggestions generated by linguistic
analysis, domain experts and ontology engineers.
References
Massimiliano Ciaramita, Aldo Gangemi, Esther Ratsch, Jasmin Saric, and Isabel
Rojas. Unsupervised learning of semantic relations for molecular biology ontolo-
gies. In Paul Buitelaar and Philipp Cimiano, editors, Ontology Learning and
Population: Bridging the Gap between Text and Knowledge, pages 91?107. IOS
Press, Amsterdam, 2008.
Philipp Cimiano, Siegfried Handschuh, and Steffen Staab. Towards the self-
annotating web. In Proceedings of the 13th World Wide Web Conference, 2004.
Thierry Declerck and Mihaela Vela. A generic nlp tool for supporting shallow
ontology building. In Proceedings of LREC, Genoa, May 2006.
349
Thierry Declerck, Hans-Ulrich Krieger, Bernd Kiefer, Marcus Spies, and Christian
Leibold. Integration of semantic resources and tools for business intelligence. In
International Workshop on Semantic-Based Software Development held at OP-
SLA 2007, 2007.
Roberto Navigli and Paola Velardi. From glossaries to Ontologies: Extracting Se-
mantic Structure from Textual Definitions, pages 71?91. IOS Press, 2008.
Patrick Pantel and Marco Pennacchiotti. Automatically Harvesting and Ontologiz-
ing Semantic Relations, pages 171?199. IOS Press, 2008.
350
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 30?34,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Ontology-Based Incremental Annotation of Characters in Folktales
Thierry Declerck
DFKI GmbH
Stuhlsatzenhausweg, 3
66123 Saarbru?cken, Germany
declerck@dfki.de
Nikolina Koleva
DFKI GmbH
Stuhlsatzenhausweg, 3
66123 Saarbru?cken, Germany
Nikolina.Koleva@dfki.de
Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg, 3
66123 Saarbru?cken, Germany
krieger@dfki.de
Abstract
We present on-going work on the auto-
mated ontology-based detection and recog-
nition of characters in folktales, restricting
ourselves for the time being to the anal-
ysis of referential nominal phrases occur-
ring in such texts. Focus of the presently
reported work was to investigate the inter-
action between an ontology and linguistic
analysis of indefinite and indefinite nomi-
nal phrase for both the incremental annota-
tion of characters in folktales text, includ-
ing some inference based co-reference res-
olution, and the incremental population of
the ontology. This in depth study was done
at this early stage using only a very small
textual base, but the demonstrated feasibil-
ity and the promising results of our small-
scale experiment are encouraging us to de-
ploy the strategy on a larger text base, cov-
ering more linguistic phenomena in a mul-
tilingual fashion.
1 Introduction
In this submission we present on-going work
dealing with the automatic annotation of charac-
ters in folktales. Focus of the investigation lies in
using an iterative approach that combines an in-
cremental ontology population and an incremen-
tal linguistic annotation. Our starting point is
given by an in-house developed ontology, which
is having as its core the description of family rela-
tions, but also some typical elements of folktales,
like supernatural entities, etc.
The use of ontologies in the field of folktales is
not new, but to our knowledge no attempt has been
done so far to use ontologies in combination with
natural language processing for automatizing the
annotation of folktales, or for automatically popu-
lating a knowledge base of characters of folktales.
The work by (Peinado et al, 2004) is dealing in
first line with the Proppian functions that charac-
ter can play and is also geared towards generation
of interactive stories. (Zoelllner-Weber, 2008) is
much closer to our aim, and in fact the author is
proposing lines of research we want to implement
in the near future, but her work is explicitly not
dealing with the automation of annotation, and
she is also not concerned with linguistic annota-
tion in particular, but with general TEI annota-
tion1 of text structures.
At the present stage of development, we re-
stricted ourselves to investigate the role indefi-
nite and definite nominal phrases (NPs) can play
for the detection of characters and their storage
as instances of ontology classes. This decision
is echoing well-established investigations on one
possible function of indefinite NPs, namely to in-
troduce a new referent in a discourse (see among
others (von Heusinger, 2000)), whereas indefinite
NPs can be used in the subsequent text to refer
back to the introduced referential entities. This
fact has also been acknowledged in the field of
folktales and narratives and (Herman, 2000), for
example, stressed the importance of analyzing se-
quences of referring expressions for achieving a
more complete and accurate view on the role of
participants in narratives.
Discourse models resulting from the sequence
of referring expressions can thus support the com-
prehension of narratives (see (Herman, 2000),p.
962). Agreeing with this study, we further think
that the automated analysis of referential expres-
1TEI stands for ?Text Encoding Initiative, see www.tei-
c.org
30
sions in folktales, delivering essential elements
for the character models used in the interpretation
of narratives, can be of help in the automated anal-
ysis of the whole folktale, and more generally for
the automated analysis of narratives.
While (Herman, 2000) treats the role of
anaphora used in transcripts of ghost stories, we
deal (for the time being) only with the relation
between characters introduced by indefinite NPs
and their subsequent enunciation by definite NPs.
In the next sections we present the main com-
ponents of the current version of our system. We
discuss also the results of a first evaluation study,
and conclude with indication on future work.
2 The Ontology
As mentioned above, our starting point is an on-
tology, developed at our lab. This ontology will
be made publicly available, after merging it with
further ontological elements relevant to the field
of narratives, as those are for example described
in (Zoelllner-Weber, 2008), and associating its
classes and relations with elements relevant for
the linguistic and semantic annotation of folk-
tales, as described for example in (Scheidel and
Declerck, 2010).
The class hierarchy and the associated relations
(or properties) are equipped with natural language
labels and comments. The labels are available in
four languages: Bulgarian, English, German and
Russian. But our work is dealing for the time be-
ing only with English.
An example of class of the ontology, with its
labels is given just below:
<owl:Class rdf:about="#BiolDaughter">
<owl:equivalentClass>
<owl:Class>
<owl:intersectionOf rdf:parseType="Collection">
<owl:Restriction>
<owl:onProperty rdf:resource="#hasBiolParent"/>
<owl:onClass rdf:resource="#BiolParent"/>
<owl:minQualifiedCardinality
rdf:datatype="&xsd;nonNegativeInteger">
1</owl:minQualifiedCardinality>
</owl:Restriction>
<owl:Restriction>
<owl:onProperty rdf:resource="#hasGender"/>
<owl:hasValue>f</owl:hasValue>
</owl:Restriction>
</owl:intersectionOf>
</owl:Class>
</owl:equivalentClass>
<rdfs:subClassOf rdf:resource="#BiolChild"/>
<rdfs:subClassOf rdf:resource="#Daughter"/>
<rdfs:comment>The class of biological daughther is
a subclass of biological child and of daughter.
This class designates all biological daughters.
Each member of this class has gender female
and at least one biological parent.
</rdfs:comment>
<dc:language xml:lang="bg">
&#1073;&#1080;&#1086;&#1083;&#1086;&#1075;&#1080;&#1095;
&#1085;&#1072; &#1044;&#1098;&#1097;&#1077;&#1088;&#1103;
</dc:language>
<dc:language xml:lang="de">
biologische Tochter
</dc:language>
<dc:language xml:lang="en">
biological Daughter
</dc:language>
<dc:language xml:lang="ru">
&#1073;&#1080;&#1086;&#1083;&#1086;&#1075;&#1080;
&#1095;&#1077;&#1089;&#1082;&#1072;&#1103; &#1044;
&#1086;&#1095;&#1100;
</dc:language>
</owl:Class>
The ontology also encodes inference rules that
allow establishing automatically family relations
between entities (characters) that have been stored
at the instance level, which is the process of on-
tology population resulting from detection in the
text.
1. hasParent(?x, ?x1), hasParent(?x, ?x2), hasParent(?y,
?x1), hasParent(?y, ?x2), hasGender(?x, ?f?), notE-
qual(?x, ?y)? Sister(?x)
2. Daughter(?d) , Father(?f) , Son(?s) ? hasBrother(?d,
?s), hasChild(?f, ?s), hasChild(?f, ?d), hasSister(?s,
?d)
The first rule is about class inference. It states
that if two different individuals in the ontology
(represented by the variables x and y) share the
same parents (represented by the variables x1 and
x2) and if the gender of one of the two individuals
is female, then this individual can be considered
as an instance of the ontology class Sister.
According to the second rule, various relations
(or properties) can be inferred from the fact that
we have three individuals (represented by the vari-
ables d, f and s) that are instances of the classes
31
Daughter, Father and Son respectively. The first
inferred relations state that the Daughter has the
Son as her Brother and the Son reciprocally has
the Daughter as his Sister. In addition, the Father
is being assigned twice the HASCHILD property,
once for the Daughter and second for the Son.
3 Processing Steps
We submit first a folktale, here the ?Magic Swan
Geese?2, to a linguistic processing engine (the
NooJ platform, see (Silberztein, 2003)), applying
to the text nominal phrases recognition rules (in-
cluding coordination), which are differentiated in
being either indefinite or definite (we do not con-
sider pronouns for the time being). All annotated
NPs are indexed with ID numbers.
In the following step, our algorithm extracts
from the whole annotated text the nominal heads
of the indefinite NPs and compares them with the
labels present in the ontology. In case a match can
be established, the nominal head of this phrase is
used for populating the corresponding ontology
class as an individual and the text is annotated
with the nominal head being a (potential) char-
acter, as can be seen in the annotation example
below, where the reader can observe that the (po-
tential) characters are also enumerated, this time
on the base of the (unique) ID they get during
the ontology population phase. In this step thus,
all candidate characters in text are automatically
marked-up with both linguistic and character in-
formation derived from the ontology.
<text>
There lived
<NPCOORD id="z_coord_ph1" Nb="p" HEAD1="man"
HEAD2="woman" Type="and">
<NP id="indef_ph1" SPEC="a" HEAD="man"
Gender="m" Num="s">
<CHAR id="ch1" TYPE="man" Gender="m"
Num="s">
an old man</CHAR>
</NP>
and
<NP id="indef_ph2" SPEC="a" HEAD="woman"
2http://en.wikipedia.org/wiki/The Magic Swan Geese
Gender="f" Num="s">
<CHAR id="ch2" TYPE="woman" Gender="f"
Num="s">
an old woman</CHAR>
</NP>
</NPCOORD>
;
they had
<NPCOORD id="z_coord_ph2" Nb="p" HEAD1=
"daughter" HEAD2="son" Type="and">
<NP id="indef_ph3" SPEC="a"
HEAD="daughter" Gender="f" Num="s">
<CHAR id="ch3" TYPE="daughter"
Gender="f" Num="s">
a daughter</CHAR>
</NP>
and
<NP id="indef_ph4" SPEC="a" HEAD="son"
Gender="m" Num="s">
<CHAR id="ch4" TYPE="son" Gender="m"
Num="s">
a little son</CHAR>
</NP>
</NPCOORD>
In the next step, the inference rules of the ontol-
ogy are applied to the candidate characters (the in-
dividuals stored so far in the knowledge base). In
the particular tale we are analysing the class asso-
ciated with the potential character ch2 (Woman)
can be equated with the class Mother and its as-
sociated string in the label (mother), so that all
occurrences of the two strings in the text can be
marked as referring to the same character.
Also some relations are established between
the individuals by the application of the inference
rules described in the ontology section above:
Wife of, Mother Of, etc. (together with the
strings listed in the labels). If the related strings
are found in definite NPs in the text, the cor-
responding segment can then be annotated by
32
our linguistic processing engine with the origi-
nal character identifier. In the example below, the
reader can see that on the base of the inference
rules, the string mother in the definite NP (with
ID DEF PH6) is referred to ch2 (see the first an-
notation example above for the first occurrence of
ch2).
<NP id="def_ph6" SPEC="the" HEAD="mother"
Gender="f" Num="s">
the mother</NP>
said: "Daughter, daughter, we are going
to work; we shall bring you back
<NP id="indef_ph7" SPEC="a" HEAD="bun"
Gender="n" Num="s">
<CHAR id="ch6" TYPE="bun" Gender="n"
Num="s">a little bun</CHAR>
</NP>
, sew you <NP id="indef_ph8" SPEC="a"
HEAD="dress" Gender="n" Num="s">
<CHAR id="ch7" TYPE="dress"
Gender="n" Num="s">
a little dress</CHAR>
</NP>
The same remark is valid for the ch3 (?a
daughter? introduced in the first sentence of the
tale). In the definite NP with ID 12, the string
(daughter) is occurring in the context of a def-
inite NP, and thus marked as referring to ch3.
The string (girl) is occurring four times in the
context of definite NPs (with IDs 18, 25, 56 and
60) and for all those 4 occurrences the inference
driven mark-up of the nominal head with ch3
turns out to be correct.
In this annotation example, the reader can also
see that the heads of all indefinite NPs are first
considered as potential characters. A preliminary
filtering of such expression like dress is not pos-
sible, since in folktales, every object can be an
actant. So for example in this tale, an oven, an
apple tree or a river of milk are play-
ing an important role, and are characters involved
in specific actions. Our filtering is rather taking
place in a post-processing phase: strings that get
only once related to a potential character ID and
which are not involved in an action are at the end
discarded.
The next steps are dealing with finding other
occurrences of the potential characters (within
definite NPs), or to exclude candidates from the
set.
4 Evaluation of the approach
In order to be able to evaluate our approach,
even considering that we are working on a very
small text base, we designed a first basic test data
and annotated manually the folktale ?The magic
swan geese? with linguistic and character anno-
tation. The linguistic annotation is including co-
referential information. In the longer term, we
plan to compare our work applied to more folk-
tales with a real gold standard, the UMIREC cor-
pus (http://dspace.mit.edu/handle/1721.1/57507)
Our evaluation study shows results in terms of
correct detection of tale characters in compari-
son with the manually annotated data. Eight of
the real characters were correctly classified by the
tool. Three of the instances are actually charac-
ters but they were not detected. One candidate is
not a character according to the manually anno-
tated data, but the system classified it as charac-
ter. Seven entities were correctly detected as non
characters. On this small basis, we calculated the
accuracy of the tool, which is 79%. We also com-
puted the precision, the recall and the F-measure.
The precision amounts to 88%; the recall to 73%;
and the value of the balanced F-measure is 80%.
So these metrics confirm what the accuracy has
been already expressing: the results are encour-
aging.
Looking at the errors made by the tool, we know
that it does not consider the characters that are
mentioned only one time. In our text, a hedge-
hog occurs only once. However, the human intu-
ition is that it is a character and differs from the
phrases a bun and a dress, which have just de-
scriptive function. In a next version of the tool,
it will be checked if the head of an indefinite NP,
which is present only once in the text, is having
an active semantic role, like Agent. In this case, it
can be considered as a character.
Another problem of our actual approach is that
we do not consider yet the possessive phrases and
pronominal expressions. Precise analysis of these
anaphoric expressions will improve the approach
33
in augmenting the number of occurrences of can-
didate characters. We also expect the availability
of related instances in the knowledge base to help
in resolving pronominal co-reference phenomena.
The applied method does not detect one of the
main characters in the sample text namely the
swan-geese. The swan-geese are introduced in the
discourse only via a definite noun phrase. If there
are some equivalent phrases, for example occur-
ring in the title of the tale, they can be annotated
as character by the tool. An additional problem
we have, is the fact that our NP grammar has ana-
lyzed the words swan and geese as separate nouns
and not as a compound noun. So that the linguis-
tic analysis for English compounds has to be im-
proved.
5 Conclusion and future work
Our in depth investigation of the interaction of an
ontology and language processing tools for the
detection of folktale characters and their use for
incrementally populating an ontology seems to be
promising, and it has allowed for example to asso-
ciate a unique character ID to occurrences of dif-
ferent nominal heads, on the base of their inferred
semantic identity. A possible result of our work
would lie in the constitution of larger database
containing characters of narratives extracted au-
tomatically from text.
We plan to tackle the processing of pronominal
and possessive expressions for completing the co-
reference task. We plan also to extend our work to
other languages, and we already started to do this
for anaother folktale in German, in which much
more complex family relationships are involved
(the German version of the tale ?Father Frost?).
But more challenging will be to deal with lan-
guages, which do not know have the difference
between indefinite and definite NPs.
Acknowledgments
The work presented in this paper has been partly
supported by the R&D project. ?Monnet?, which
is co-funded by the European Union under Grant
No. 248458.
References
Marc Cavazza and David Pizzi. 2006. Narratology for
interactive storytelling: A critical introduction. In
TIDSE, pages 72?83.
Maja Hadzic, Pornpit Wongthongtham, Tharam Dil-
lon, Elizabeth Chang, Maja Hadzic, Pornpit
Wongthongtham, Tharam Dillon, and Elizabeth
Chang. 2009. Introduction to ontology. In
Ontology-Based Multi-Agent Systems, volume 219
of Studies in Computational Intelligence, pages 37?
60. Springer Berlin / Heidelberg.
Knut Hartmann, Sandra Hartmann, and Matthias
Feustel. 2005. Motif definition and classification
to structure non-linear plots and to control the nar-
rative flow in interactive dramas. In International
Conference on Virtual Storytelling, pages 158?167.
David Herman. 2000. Pragmatic constraints on narra-
tive processing: Actants and anaphora resolution in
a corpus of north carolina ghost stories. Journal of
Pragmatics, 32(7):959 ? 1001.
Harry R. Lewis and Christos H. Papadimitriou. 1998.
Elements of the theory of computation. Prentice-
Hall.
Deborah L. McGuinness and Frank van Harmelen.
10 February 2004. OWL Web Ontology Language
Overview. W3C Recommendation.
Federico Peinado, Pablo Gerva?s, and Bele?n D??az-
Agudo. 2004. A description logic ontology for
fairy tale generation. In Language Resources for
Linguistic Creativity Workshop, 4th LREC Confer-
ence, pages 56?61.
Vladimir IA. Propp, American Folklore Society., and
Indiana University. 1968. Morphology of the
folktale / by V. Propp ; first edition translated by
Laurence Scott ; with an introduction by Svatava
Pirkova-Jakobson. University of Texas Press,
Austin :, 2nd ed. / revised and edited with a pref-
ace by louis a. wagner ; new introduction by alan
dundes. edition.
Antonia Scheidel and Thierry Declerck. 2010. Apftml
- augmented proppian fairy tale markup language.
In Sa?ndor Dara?nyi and Piroska Lendvai, editors,
First International AMICUS Workshop on Auto-
mated Motif Discovery in Cultural Heritage and
Scientific Communication Texts: Poster session. In-
ternational Workshop on Automated Motif Discov-
ery in Cultural Heritage and Scientific Communica-
tion Texts (AMICUS-10), located at Supporting the
Digital Humanities conference 2010 (SDH-2010),
October 21, Vienna, Austria. Szeged University,
Szeged, Hungary, 10.
Max Silberztein. 2003. Nooj manual. available for
download at: www.nooj4nlp.net.
Klaus von Heusinger. 2000. The reference of indefi-
nites. In K. von Heusinger and U. Egli, editors, Ref-
erence and Anaphoric Relations, pages 247?265.
Kluwer.
Amelie Zoelllner-Weber. 2008. Noctua literaria -
A Computer-Aided Approach for the Formal De-
scription of Literary Characters Using an Ontol-
ogy. Ph.D. thesis, University of Bielefeld, Biele-
feld, Germany, may.
34
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 90?95,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Integration of the Thesaurus for the Social Sciences (TheSoz)              
in an Information Extraction System  
 
 
Thierry Declerck 
DFKI GmbH, LT-Lab 
Stuhsatzenhausweg, 3 
D-66123 Saarbr?cken, Germany 
declerck@dfki.de 
 
 
Abstract 
We present current work dealing with the in-
tegration of a multilingual thesaurus for so-
cial sciences in a NLP framework for sup-
porting Knowledge-Driven Information Ex-
traction in the field of social sciences.  We 
describe the various steps that lead to a run-
ning IE system: lexicalization of the labels of 
the thesaurus and semi-automatic generation 
of domain specific IE grammars, with their 
subsequent implementation in a finite state 
engine. Finally, we outline the actual field of 
application of the IE system: analysis of so-
cial media for recognition of relevant topics 
in the context of elections.   
1 Introduction 
Within a running research project dealing with 
the automatic linguistic and semantic processing 
of social media1, we are working on a use case 
concerned with the analysis of tweets exchanged 
in the context of approaching election events. 
Besides the detection of Named Entities (name 
of politicians, political parties, locations, etc.) 
and associated opinions, we are also interested in 
identifying and classifying the topics people are 
addressing in their messages. 
There are for sure topics that are very particu-
lar to a specific election, but there are also more 
generic and recurrent topics, some of them being 
of special interest to social scientists. In order to 
be able to detect such topics in various types of 
text, we have been searching for knowledge 
sources in the field of social and political sci-
ences that can be used for the corresponding 
(both manual and automatic) semantic annotation 
                                                 
1
 The TrendMiner project, www.trendminer-project.eu,  
co-funded by the European Commission with Grant No. 
287863.  
 
of text. Our best candidate is for the time being 
the Thesaurus for the Social Sciences (TheSoz), 
developed by the GESIS institute at the Leibniz 
Institute for the Social Sciences2. This resource is 
available in the SKOS format 3 , and therefore 
adapted to the Linked Data framework4. In this 
short paper we present first in some details the 
thesaurus, before describing the steps that allow 
us to integrate the (multilingual) language data it 
includes into a NLP tools suite, for the goal of 
supporting Knowledge-Driven analysis of texts 
in the field of social sciences, with a focus on 
micro-blogs. 
2 The Thesaurus for the Social Sciences 
(TheSoz) 
The thesaurus for social sciences is a knowledge 
source under continuous development (we are 
currently using version 0.92). The list of key-
words used in TheSoz contains about 12,000 en-
tries, of which more than 8,000 are descriptors 
(or ?authorized keywords?). 
It is encoded in RDF and SKOS. While the 
main conceptual elements of the thesaurus are 
encoded in the core syntax of SKOS, the re-
source makes also use of the SKOS-XL proper-
ties5 for including labels containing natural lan-
guage expressions (authorized keywords, which 
act as domain terms) that are attached to the con-
ceptual elements., using the ?prefLabel? and 
?altLabel? annotation properties, allowing thus to 
describe main terms and their variants. The natu-
ral language expressions corresponding to the 
labels are encoding using the SKOS-XL annota-
tion property ?literalForm?.  
                                                 
2
 http://www.gesis.org/en/services/research/thesauri-und-
klassifikationen/social-science-thesaurus/ 
3
 See http://www.w3.org/TR/skos-primer/ for a concise 
introduction to SKOS. 
4
 http://linkeddata.org/ 
5
 See http://www.w3.org/TR/skos-reference/skos-xl.html 
90
In order to give a (human readable) idea of the 
content of the thesaurus6 , we extracted with a 
Perl script the main elements from the SKOS 
code and present those in a tabular fashion, an 
example of which is given below, displaying also 
the terms in the languages covered by TheSoz 
(English, French and German): 
 
concept id "10034303" 
   term ?10034303" 
? prefLabel id "10034303" 
? lang=de "Abbrecher" 
? lang=en "drop-out" 
? lang=fr "drop-out" 
? altLabel id "10034307"  
? lang=de "Studienabbrecher" 
? lang=en "university drop-out" 
? lang=fr "?tudiant qui abandonne ses ?tudes"     
   notation ?3.2.00"  
? lang=de ?Schule und Beruf (berufliche Qualifika-
tionselemente im Bereich der schulischen Ausbil-
dung)? 
? lang=en ?School and Occupation (Elements of 
Occupational Qualification in School Education)? 
? lang=fr ? ?cole et profession (?l?ments de quali-
fication professionnelle dans le domaine de 
l?enseignement scolaire) ? 
   broader notation ?3.2?     
? lang=de ?Beruf und Qualifikation? 
? lang=en ?Occupation and Qualification? 
? lang=fr ? profession et qualification ? 
   broader notation ?3?  
? lang=de ?Interdiszipin?re Anwendungsbereiche 
der Sozialwissenschaften? 
? lang=en ?Interdisciplinary Application Areas of 
Social Sciences? 
? lang=fr ? domaines interdisciplinaires d'applica-
tion des sciences sociales ?  
   
In the example above the reader can see how the 
English preferred label ?drop-out? is associated 
with the concept ?School and Occupation?, 
which is itself a subclass of the concept ?Occu-
pation and Qualification?, classified itself as a 
field of the broader concept ?Interdisciplinary 
Application Areas of Social Sciences?. All the 
language material contained in the labels or used 
for naming the ?notations? can be re-used for 
detecting and semantically annotating the related 
topics in running texts. 
3 TheSoz as Linked Data 
The encoding of TheSoz in SKOS is an impor-
tant asset, since it allows linking the data to other 
                                                 
6
 Online visualizations and access are available at 
http://lod.gesis.org/thesoz/ 
knowledge sources, like for example DBpedia7 in 
the Linked Data framework, and so to comple-
ment information contained in TheSoz, which 
remains at the terminological level, and is thus 
not giving detailed information about the in-
cluded multilingual terms for the described con-
cepts and the relations between those.  
So for example TheSoz mentions the main po-
litical parties in Germany, Austria and other 
countries, but not their actual leader, their actual 
role (in the government or in the opposition) or 
weight in the current legislation period. TheSoz 
also lists the names of important persons, like 
?Merkel, A.? or ?Brandt, W.?, but no biographi-
cal indication or relation to political parties or 
institutions are given. As such TheSoz is provid-
ing for a light-weight ontological basis, with 
multilingual labels, which allows detecting in 
text mentions of topics or entities relevant to the 
social scientists.   
The linking of concepts and associated terms 
to more elaborated knowledge sources, like 
DBpedia, is thus necessary in order to implement 
a full Knowledge Driven Information Extraction 
(KDIE) system in the field of social sciences. So 
for example the TheSoz sub-term ?university? in 
?university drop-out? can be completed by in-
formation in the DBpedia entry for ?university?, 
stating among others that ?university? is rdfs 
domain of ?numberOfPostgraduateStudents? and 
that it is a subClassOf ?EducationalInstitution. 
?http://schema.org/EducationalOrganization? is 
given as an equivalenceClass of the DBpedia 
entry for ?EducationalInstitution?. From the 
schema.org entry we can make use of additional 
relations associated to ?EducationalInstitution?, 
like for example a relation to more specific 
types, such as ?CollegeOrUniversity?, ?Elemen-
tarySchool?, ?HighSchool?, ?MiddleSchool?, 
?Preschool?,  ?School?. We can this way expand 
the terminological base of TheSoz by accessing 
the labels of the classes and concepts of other 
knowledge sources referred to by explicit seman-
tic relations like owl:equivalentClass, 
owl:sameAs or skos:exactMatch.  
   As the reader can see from the name of the 
mentioned ontology classes above, natural lan-
guage expressions associated to elements of 
knowledge sources can have different surface 
forms as the one we saw in the examples of ?lit-
eralForms? of TheSoz. Beyond the utilization of 
the annotation properties, such as rdfs:label, 
                                                 
7
 See http://dbpedia.org/About. And in fact, 5024 TheSoz 
concepts are linked to DBpedia via SKOS ?exact matches?. 
91
skosxl:prefLabel? or skosxl:literalForm, dedi-
cated to ease the understanding by human users, 
several other syntax elements of knowledge rep-
resentation systems, such as the RDF URI refer-
ences, like rdf:ID, rdf:about, or rdf:resource, may 
contain instead of numerical codes natural lan-
guage expressions, often using the CamelCase 
notation. Fu et al (2012) describes NLP tasks 
and applications using natural language expres-
sions contained in such RDF URI references. In 
our work, we  focus on natural language expres-
sions contained in the annotation properties 
rdfs:label, sxkos:label (skosxl:prefLabel and oth-
ers) and skosxl:literalForm, which typically in-
clude textual material to be consumed by human 
readers, and which can be normally directly pro-
cessed by NLP tools, without requiring prior 
transformation processes of the textual material. 
4 Integration of TheSoz  in a NLP 
Framework 
Before applying the (possibly extended) termino-
logical material of TheSoz for supporting the 
semantic annotation of running texts, it has to be 
submitted to pre-processing steps, in order to 
ensure as a minimum a possible matching to 
morpho-syntactic variations of (elements of) the 
terms that are to be expected in external text. For 
this, we need to lexicalize the labels of the the-
saurus, transforming the terms to linguistic data 
that can be used for matching linguistically proc-
essed text. A first sketch of this approach has 
been described in (Declerck & Lendvai, 2010) 
and a more elaborated methodology, encoding 
the linguistic data in RDF is presented in 
(McCrae et al 2012).  
And for ensuring a linking of linguistic data in 
text to the conceptual elements of the thesaurus 
(or other knowledge sources), the development 
of an information extraction grammar is needed. 
We present in section 3.2 below an automatized 
approach for this.   
For both steps we are using the NooJ plat-
form8 , whose finite states engine supports the 
flexible implementation of lexicons, morpho-
logical, syntactic and semantic grammars. 
4.1 Lexicalization 
The lexicalization step consists in submitting all 
the language material included in the knowledge 
source to a lexical and a syntactic analyzer, 
                                                 
8
 http://www.nooj4nlp.net/pages/nooj.html 
which in our case are lexicons and grammars 
implemented in NooJ.   
The results of such a processing can be en-
coded in the lexicon-ontology model lemon 
(McCrae et al 2012), which declaratively repre-
sents textual and linguistic information of on-
tologies as additional RDF resource linked to the 
original concepts associated to the labels. The 
lemon model decomposes multi-word expres-
sions to individual words and represents the re-
sults in a phrase structure, which can be shared 
by multiple lexical entries. Furthermore, depend-
ency relations between decomposed phrase con-
stituents can be modeled. A simplified example 
of the lemon representation of the NooJ parsed 
term ?university drop-out? is shown below: 
 
:university_drop-out [lemon:writtenRep "univer-
sity drop-out"@en] 
lemon:sense [lemon:reference ontol-
ogy:TheSoz10034307]; 
lemon:decomposition ( :university_comp 
:drop-out_comp ) ; 
lemon:phraseRoot [ lemon:constituent :NP ; 
lemon:edge [lemon:constituent :NP ; 
lemon:edge [lemon:constituent :NN ; 
lemon:leaf university_comp ] ; 
lemon:edge [lemon:constituent :NN ; 
lemon:leaf drop-out_comp ] ];  
]. 
 
For the sake of simplicity we do not display the 
lemon representation of additional analysis pro-
vided by NooJ (for example the one, which is 
decomposing ?drop-out? in two lemmas). It is 
enough to mention that lemon also supports the 
representation of preferred and alternative labels. 
This is important if one wants to consider all 
possible (linguistically annotated) term variants 
for improving the matching of TheSoz terms to 
terminological variants in text, going thus be-
yond the matching of terms to purely morpho-
syntactic variations. So for example, in TheSoz 
?drop-out? is the prefLabel, while ?university 
drop-out? is marked as altLabel of the same con-
cept. Such term variants can also be ?imported? 
in our lexicalization step from other source. Or 
one can import additional lexical material, so for 
example the corresponding WordNet synonyms 
or glosses. In the next future we also plan to 
?tap? the BabelNet9 resource, which is providing 
links to WordNet, Wikipedia and DBpedia (and 
more is planed), for extending the terminological 
                                                 
9
 See http://lcl.uniroma1.it/babelnet/ or (Navigli & 
Ponzetto, 2012). 
92
base of the (lexicalized) TheSoz labels, also with 
terms in languages not covered by TheSoz for 
now.  
4.2 Automatic Generation of Domain spe-
cific IE grammars 
On the basis of the lexicalization step described 
in section 3.1, we wrote a Perl program that gen-
erates IE grammars in the NooJ finite state en-
gine. This procedure is done in 5 steps.  
 
1) Using the Term ID of TheSoz as names 
for NooJ recognition rules. 
term10034307 = 
2) Using the corresponding lexicalised la-
bels as the expressions to be recognized 
by the NooJ rule (abstract representa-
tion): 
term10034307 = [lemma=?university? 
cat=?N?] [lemma=?drop-out? 
cat=?N?] ; 
3) Adding possible term variants to the 
rule) 10: 
term10034307 = ([lemma=?university? 
cat=?N?] [lemma=?drop-out? 
cat=?N?] | :var10034307) ;  
 
var10034307 = [lemma=?university? 
cat=?N?] [lemma=?drop? cat=?V?] 
[lemma=?out? cat=?P?] ; 
4) Linking the linguistically annotated pre-
fLabel and the altLabel(s) to the corre-
sponding Concept ID, as the basis of the 
semantic organization of the lexical ma-
terial in NooJ: 
concept10034303 = (term10034303 | 
term10034307) ; 
5) Defining the annotation generation pro-
cedure of the NooJ rules: Successful ap-
plication of  the rule  concept10034303 
can generate the following annotation: 
CLASS= TheSoz_ID=?10034303?  
altLabel_ID=?10034307? 
altLabel =?universtiy drop-out@en?  
SuperClass=TheSoz_ID_3.2  
                                                 
10
 In this simplified example we do just include as a 
term variant the decomposition of the noun ?drop-
out? in two lemmas, extending thus the lexical cover-
age of the original label. The final rule (not displayed 
here for the sake of simplicity) is also stating that the 
sub-term ?university? doesn?t have to immediately 
precede the sub-term ?drop?, accounting thus also for 
alternative word order. 
SuperClassLabel = ?Occupation and 
Qualification? 
altLabel_Translation  = ?Studienabbre-
cher@de? 
etc.11  ) 
 
This procedure has been fully implemented, us-
ing Perl scripts. The addition of term variants (in 
red color in the example above, point 3) can be 
done manually or automatically. We are also cur-
rently adding information about the context of 
such terms to be expected in running texts, like 
for example the agent of the event ?drop-out?, 
and further modifications, like date, location and 
reasons. 
At the moment we are able to semantically 
disambiguate in text for example the two senses 
of the TheSoz term ?drop-out?: one in the sense 
of ?university drop-out? and the one in the sense 
of ?resignation from occupation?. The generated 
NooJ grammars are currently being tested for a 
use case dealing with the elections in Austria. 
5 Use Case 
Our actual focus is the elections in Austria. Our 
aim is to detect which topics are of have been 
discussed in the social media, and how this re-
lates to election results obtained by candidates 
and parties.  
As such we cannot report yet on evaluation re-
sults, both at the technological and usability lev-
el, since an evaluation study is still to be per-
formed. We will be using collection of polls for 
measuring the accuracy of the detection of topics 
and the related popularity of parties/politicians 
detected in social media.  
The use case partner involved in the project 
has been designing an annotation schema and is 
performing a semi-automatic annotation of se-
lected tweets and blogs, which we will use as 
gold standard. 
A fully operational system is expected to work 
for the national elections in Austria to be held on 
the 28th September of 2013. 
6 Future Work 
Besides the evaluation work sketched in the for-
mer section, the next steps in our work will con-
sist in aggregating information from other 
                                                 
11
 An example text is: ?Mar 29, 2012 ? Record num-
bers of students quit university courses last year as the 
higher education drop-out rate soared above 30000 
for the first time?? 
93
knowledge source, not only from DBpedia but 
also from a recently developed political ontol-
ogy, which has been designed in the context of 
our project.   
We have also already conducted experiments 
in relating the linguistically annotated terms of 
TheSoz with terms available in other thesauri, 
like for example GEMET12. As GEMET is con-
taining labels in 33 languages, this linking will 
allow us to find more multilingual equivalents of 
terms in TheSoz, at least for the concepts of 
TheSoz that can be associated with concepts in 
GEMET.  
Another line of investigation will consist in 
adapting the work on correcting and comple-
menting the labels used in TheSoz, following the 
reports described in (Declerck & Gromann, 
2012), where correcting and completive patterns 
have been applied to the labels of multilingual 
taxonomies dealing with the description of indus-
try activity fields of companies listed in various 
stock exchanges. Improving the terminological 
quality of labels seems to be a good strategy for 
improving knowledge-driven information extrac-
tion.  
   Following the approaches to cross-lingual har-
monization of taxonomy labels described in (De-
clerck & Gromann, 2012; Gromann & Declerck, 
2013), we notice that in many multilingual 
knowledge sources (Thesauri, Taxonomies or 
Ontologies), the content of multilingual labels is 
not parallelized. In one of our example within the 
TheSoz, displayed in Section 2, we had the fol-
lowing concept with the labels in three lan-
guages: 
 
term "10034303" 
    concept  id "10034303" 
  ? 
       altLabel id "10034307"  
       altLabel de "Studienabbrecher" 
       altLabel en "university drop-out" 
  altLabel fr "?tudiant qui abandonne ses ?tudes" 
  ?. 
 
As the reader can see, only the French label is 
containing explicitly the fact the entity ?perform-
ing? the drop-out is a student. Although the su-
per-classes make clear that ?university drop-out? 
is in the field of ?School and Occupation?, none 
of the metadata or labels, other as the French 
?altLabel? is mentioning that a student is in-
                                                 
12
 GEMET stands for ?GEneral Multilingual Envi-
ronmental Thesaurus?. See also 
http://www.eionet.europa.eu/gemet/ 
volved in this field. The German label can lead to 
the reading that a person is involved, if adequate 
lexical semantics resources are used. The English 
label does not mention at all that an agent is in-
volved: it just names the event. The French and 
German labels are about abandoning ?studies? 
while the English label is about abandoning 
?university?.  
As suggested by Gromann & Declerck (2013), 
we can add (either manually or by automated 
process) to the English alternative labels the 
translations of the French label (in this particular 
case, the one with the richest contextual informa-
tion), like ?a student, who is dropping out his 
studies?. This is important since it improves the 
matching of the concepts of TheSoz to running 
texts. 
7 Conclusion 
We have described actual work in integrating 
multilingual knowledge sources in the field of 
social sciences into a NLP task, consisting in 
identifying relevant topics of discussion in social 
media. As it is still too early to report on results 
(due to the internal calendar of the project), we 
could only present for the time being the current 
state of implementation, which consisted in first 
lexicalizing the labels of the knowledge source 
?TheSoz?, freely available ? in the SKOS for-
mat. On the basis of the lexicalized labels, and 
their relation to conceptual element of the 
knowledge source, we implemented an automatic 
generation of knowledge-driven IE grammars, 
which have been realized as finite state transduc-
ers in the NooJ platform. Those resulting IE 
grammars are to be deployed in the context of a 
use case dealing with the detection of topics ad-
dressed in social media on approaching elections.  
 
Acknowledgments 
 
The work presented in this paper has been sup-
ported by the TrendMiner project, co-funded by 
the European Commission with Grant No. 
287863. 
The author is thanking the reviewers for their 
very helpful comments, which led to substantial 
changes brought to the final version of the paper. 
The author is also thanking Dagmar Gromann 
(Vienna University of Economics and Business).  
Intensive discussions with her on related topics 
have been heavily inspiring the work described 
in this paper. 
 
94
References  
Declerck, T., Lendvai, P. 2010. Towards a standard-
ized linguistic annotation of the textual content of 
labels in Knowledge Representation Systems. In: 
Proceedings of the seventh international confer-
ence on Language Resources and Evaluation, Va-
letta, Malta, ELRA. 
Fu, B., Brennan, R., O'Sullivan, D.: A Con_gurable 
Translation-Based Cross-Lingual Ontology Map-
ping System to Adjust Mapping Outcomes. Journal 
of Web Semantics, Vol. 15, pp.15_36 (2012) 
Declerck, T., Gromann, D. 2012. Towards the Gen-
eration of Semantically Enriched Multilingual 
Components of Ontology Labels. In: Proceedings 
of the 3rd Multilingual Semantic Web Workshop. 
Ell, B., Vrandecic, D., Simperl, E. 2011. Labels in the 
Web of Data. In Aroyo, L., Welty, C., Alani, H., 
Taylor, J., Bernstein, A. (eds.): Proceedings of the 
10th international conference on the semantic web 
- Volume Part I (ISWC'11), Vol. Part I. Springer-
Verlag, Berlin, Heidelberg, pp.162_176. 
Fu, B., Brennan, R., O'Sullivan, D.: A Configurable 
Translation-Based Cross-Lingual Ontology Map-
ping System to Adjust Mapping Outcomes. Journal 
of Web Semantics, Vol. 15, pp.15_36 (2012) 
Garcia, J., Montiel-Ponsoda, E., Cimiano, P., G?mez-
P?rez, A., Buitelaar, P., Mc- Crae, J. 2012. Chal-
lenges for the Multilingual Web of Data. Web Se-
mantics: Science, Services and Agents on the 
World Wide Web, Vol. 11, pp.63-71. 
Gromann, D., Declerck, T. 2013. Cross-Lingual Cor-
recting and Completive Patterns for Multilingual 
Ontology Labels. In Buitelaar, P. and Cimiano, P. 
(eds) Multilingual Semantic Web, Springer-Verlag 
(to appear) 
McCrae, J., Aguado-de-Cea, G., Buitelaar, P., Cimi-
ano, P., Declerck, T., G?mez-P?rez, A., Gracia, J., 
Hollink, L., Montiel-Ponsoda, E., Spohr, D., Wun-
ner, T. 2012. Interchanging lexical resources on 
the SemanticWeb. Journal of Language Resources 
and Evaluation, pp.1_19. 
Navigli, N., Ponzetto, S.P.. 2012. BabelNet: The Au-
tomatic Construction, Evaluation and Application 
of a Wide-Coverage Multilingual Semantic Net-
work. Artificial Intelligence, 193, Elsevier, pp. 
217-250. 
Silberztein, Max. 2003. NooJ manual. Available at 
the WEB site http://www.nooj4nlp.net (200 pages) 
Wimalasuriya, D. C., Dou, D. 2012. Ontology-based 
information extraction: an introduction and a sur-
vey of current approaches. Journal of Information 
Science, Vol. 36, No. 3, pp.306-_323. 
Zapilko, B., Johann Schaible, Philipp Mayr, Brigitte 
Mathiak. 2012. TheSoz. A SKOS Representation of 
the Thesaurus for the Social Sciences. Semantic-
Web Journal. 
 
 
95
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 9?12,
Gothenburg, Sweden, April 26 2014. c?2014 Association for Computational Linguistics
How to semantically relate dialectal Dictionaries 
in the Linked Data Framework 
 
 
Thierry Declerck 
University of Saarland 
 Computer Linguistics Department 
Postach 15 11 50 
D-66041 
declerck@dfki.de 
Eveline Wandl-Vogt 
Institute for Corpus Linguistics and 
Text Technology, Austrian Academy of 
Sciences. 
Sonnenfelsgasse 19/8, A-1010 Vienna 
Eveline.Wandl-Vog@ 
oeaw.ac.at 
 
 
Abstract 
We describe on-going work towards publishing 
language resources included in dialectal dictionaries 
in the Linked Open Data (LOD) cloud, and so to 
support wider access to the diverse cultural data 
associated with such dictionary entries, like the 
various historical and geographical variations of the 
use of such words. Beyond this, our approach allows 
the cross-linking of entries of dialectal dictionaries on 
the basis of the semantic representation of their 
senses, and also to link the entries of the dialectal 
dictionaries to lexical senses available in the LOD 
framework. This paper focuses on the description of 
the steps leading to a SKOS-XL and lemon encoding 
of the entries of two Austrian dialectal dictionaries, 
and how this work supports their cross-linking and 
linking to other language data in the LOD. 
1 Introduction 
The starting point for our work is given by two 
Austrian dialectal dictionaries: The Dictionary of 
Bavarian dialects of Austria (W?rterbuch der 
bairischen Mundarten in ?sterreich, WB?)1 and 
the Dictionary of the Viennese dialect 
(W?rterbuch der Wiener Mundart, WWM) 2 . 
Both dictionaries have been made available to us 
in an electronic version: WB? in a proprietary 
XML schema and WWM in Microsoft Word. We 
                                                          
1
 http://verlag.oeaw.ac.at/Woerterbuch-der-
bairischen-Mundarten-in-Oesterreich-38.-Lieferung-
WBOe  
2 See (Hornung & Gr?ner, 2002). 
used the TEI ?OxGarage?3 service to convert the 
WWM Word document into a TEI compliant 
XML representation. Table 1 below shows 
partially an example of an entry in the printed 
version of WB?. 
Table 1: An example for an entry in the WB? 
 
In a previous work we ported elements of WB? 
onto SKOS4 in order to be able to publish entries 
                                                          
3 See http://oxgarage.oucs.ox.ac.uk:8080/ege-
webclient/ 
4 ?SKOS - Simple Knowledge Organization System - 
provides a model for expressing the basic structure 
and content of concept schemes such as thesauri, 
classification schemes, subject heading lists, 
taxonomies, folksonomies, and other similar types of 
controlled vocabulary. As an application of the 
Resource Description Framework (RDF), SKOS 
allows concepts to be composed and published on the 
World Wide Web, linked with data on the Web and 
integrated into other concept schemes.? 
9
of this dictionary in the Linked Data 5  cloud 
(Wandl-Vogt & Declerck, 2013). We used 
recently a similar approach for porting the TEI 
Version of the WWM dictionary into SKOS, 
leading to few modifications in our previous 
model. 
A motivation for this additional step was to 
investigate if our SKOS-based model can support 
the (automatised) cross-linking of the dialectal 
dictionary data6. In this particular case, we can 
take advantage of a property of dialectal 
dictionaries concerning the expression of 
meanings of entries: Although conceived as 
monolingual reference works, dialectal 
dictionaries share with bilingual dictionaries the 
fact that they express the meanings of their 
entries in a different language. The meta-
language for expressing the meanings of entries 
in both WB? and WWM is Standard German, 
sometimes accompanied by Austrian German. 
This is exemplified in the WB? entry ?Puss? in 
Table 1 above, which is using both the Standard 
German ?Ku?? and the Austrian German 
?Busserl? for expressing one meaning of the 
word ?Puss? (this meaning being ?kiss?). Other 
meanings are ?Geb?ck? and ?PflN?7. Additional 
lines for the entry ?Puss? in WB?, not displayed 
in this submission due to lack of space, are 
giving more details on those meanings, pr?cising 
that in the ?Geb?ck? case we deal with a small 
sweet pastry (?Kl. s??es Geb?ck?) and in the 
?PflN? case with a ?bellis perennis? flower.8  
The related entry in WWM dictionary is 
?Bussal?, which is displayed in Table 2. 
 
 
                                                                                        
(http://www.w3.org/TR/skos-primer/) 
 
5 For more details see http://linkeddata.org/. 
6 The topic of ?cross-linking? is in fact very relevant 
to lexicographers, as can be seen in (Wandl-Vogt, 
2005). 
7 The word ?Geb?ck? (pastry) is Standard German 
and the string ?PflN? is an abbreviation for the 
German name ?Pflanzenname? (name of a plant) 
8 More details are given in (Author2 & Author1, 2013). We 
concentrate in this submission on the sense ?small sweet 
pastry? to exemplify our approach. 
 
 
 
 
 
 
 
 
 
 
We can see that this entry carries two meanings, 
which are the same as the two first meanings of 
the WB? entry ?Puss?.  Linking entries in 
distinct dialectal dictionaries can thus be 
implemented on the basis of meanings that are 
shared across the dictionaries. But, while for the 
second meaning the readers familiar with the 
German language will immediately recognize 
that both strings ?Kl. s??es Geb?ck? (WB?) and 
?kleines S??geb?ck? (WWM) have the same 
meaning, this is not evident for other readers and 
for computer program that should cross-link the 
dictionary data from those two sources.  
In order to automatically cross-link entries from 
both dictionaries, we wrote first a program for 
extracting the strings expressing the meanings 
for each entry and applied an algorithm for 
comparing the extracted strings. For this latter 
task, it is necessary to first linguistically analyse 
the strings, since pure string matching cannot 
provide accurate comparisons: lemma reduction 
and PoS tagging are giving additional indicators 
for matching strings expressing meanings.  To 
mark linguistically analysed meanings as related, 
use also semantic representation languages 
developed in the context of W3C standardization, 
more specifically SKOS-XL9 and lemon10 
2 Extraction and Linguistic Analysis of 
Strings marking Meanings 
We wrote for the extraction of strings marking 
the meanings of entries task specific Perl scripts, 
adapted to the XML schemas of WB? and 
WWM (in its converted TEI format). Second, we 
provided an automatic linguistic analysis of those 
extracted meanings, using lexical and syntactic 
analysis grammars written with the NooJ finite 
                                                          
9 http://www.w3.org/TR/skos-reference/skos-xl.html 
10 http://lemon-model.net/ and  (McCrae et al., 2012). 
Bussal, Bussi, Bussl, das, 1) Kuss (Syn.: 
Schm$tss); 2) kleines S??geb?ck; Pl. 
Bussaln; viele Komp. wie Nussbussal 
usw. ? 
 
Table 2: The related entry in the WWM 
dictionary 
10
state platform 11 . This included tokenization, 
lemmatisation, Part-of-Speech (POS) tagging 
and constituency as well as dependency analysis. 
The strings marking in both dictionaries the 
?sweet pastry? meaning are enriched with the 
following linguistic features: 
 
WB?: (NP s??es (ADJ, lemma = s??, MOD) 
Geb?ck (N, lemma = Geb?ck, HEAD)) 
 
WWM: (NP (kleines (ADJ, lemma = klein, 
MOD) S??geb?ck (N, compound: s?? (ADJ, 
lemma = s??, MOD) + Geb?ck (N, lemma = 
Geb?ck, HEAD)), HEAD)) 
 
In those examples (sweet pastry and small sweet 
pastry), we can see the distinct serializations of 
similar meanings in German. The second 
example uses a compound noun (?S??geb?ck?), 
which has the same meaning as the simple 
nominal phrase in the first example (?s??es 
Geb?ck?). In order to automatically establish this 
similarity, it is necessary to perform a 
morphological decomposition of the head noun 
in the second example. It is also necessary to 
have the lemma of the adjective in the first 
example, in order to compare it with the first 
element of the compound noun in the second 
example. 
The fact, that both linguistically analysed 
meanings (strings) share the same lemmas for 
adjectival modifiers and head nouns is the basis 
for cross-linking the entries. This cross-linking  
has to be expressed in Semantic Web standards 
(e.g. compatible to RDF) in order to be published 
in the Linked Data cloud. 
3 Porting the Dictionary Data into the 
Linked Open Data framework 
3.1 Porting the dictionaries into SKOS 
Analogue to the described SKOSification of 
WB? (see Wandl-Vogt & Declerck, 2013), the 
WWM was ported into SKOS. Departing from 
the former experiment, we decided to not encode 
anymore the whole dictionary as a SKOS 
concept scheme. Rather we introduce the listing 
of entries (each encoded as a skos:Concept) as 
being member of a skos:Collection. 
                                                          
11 See http://www.nooj4nlp.net/pages/nooj.html 
Complementary to this, extracted senses (see 
former section) are each encoded as 
skos:Concept being included in a  
skos:ConceptScheme. This decision is due to the 
fact that the senses can be organized along the 
line of (SKOS) semantic relations, whereas the 
strings marking the entries are in fact just 
member of a list, which is building the dictionary.  
The headword (string) of the dictionary entries is 
encoded as a value of the SKOS-XL prefLabel 
property. Alternative strings (like ?Bussi? in the 
WWM example in Table 2) are encoded with the 
SKOS-XL altLabel property. The use of SKOS-
XL allows us to ?reify? the value of the range of 
the label properties, and thus to have there not 
only a literal but further information, like PoS. 
Since senses are also represented in the 
dictionaries by strings, we apply the same 
procedure: a sense has skos-xl labels in which we 
can encode the lemma of the components of the 
strings, the corresponding PoS but also related 
senses, within the local concept scheme or in the 
LOD, like for example with objects in the 
DBpedia instantiation of Wiktionary12. 
3.2 Representing the meanings in lemon 
The linguistically analysed meanings cannot 
be (straightforwardly) represented in SKOS, 
and for this we opted for the lemon model, 
which has been developed specifically for 
the purpose of representing linguistic 
information of lexical entries related to 
knowledge organization systems.  The lemon 
encoding of the meanings is incorporated as 
the value of the SKOS-XL ?Label? property. 
Taking as an example the one meaning of 
?Puss? in WB? that consists of two words 
(?s??es Geb?ck?, sweet pastry), we can see 
that it is for necessary to tokenize the string 
representing the meaning of the entry 
?Puss?: the first token can then be 
lemmatized to ?s??? (sweet), while for the 
second token the lemma is identical to the 
written form used. We represent the 
                                                          
12 So for example the sense ?Kuss? for both the 
entries ?Puss? and ?Bussal? is declared as being a 
skos:exactMatch with the URL: 
http://wiktionary.dbpedia.org/page/Kuss-German-
Noun-1de. From there we can get then all multilingual 
equivalents listed in this resource. 
11
tokenization information using the lemon 
property ?decomposition?.   
4 Cross referencing of dictionary 
entries through similar meanings 
The establishment of a relation between ?Puss? 
in WB? and ?Bussal? in WWM is made possible 
on the base of the successful mapping of both the 
adjectival modifier ?s??? and the head noun 
?Geb?ck?, which are present in both the 
definitions in WB? and WWM. This similarity 
is encoded using the ?related? property of SKOS.  
Interesting is also the fact that a user searching 
the electronic version of the dictionaries could 
give the High German form ?Geb?ck? and would 
get from both dictionaries all the entries which 
have this word in their definition. The same for 
the High German adjectival form ?s???.  
Instead of the meanings we extracted from the 
dictionaries, we can use the DBpedia 
instantiation of Wiktionary as a reference for the 
senses of the entries of the dictionary, pointing 
directly to linguistic and knowledge objects that 
are already in the LOD. Using the 
?decomposition? and ?subSenses? properties of 
lemon, we link to URLs in DBpedia/Wiktionary 
representing the sense for each token. 
5 Conclusion 
We described the actual state of 
RDF/SKOS/lemon modeling of (senses of) 
entries of dialectal dictionaries, so that those 
entries can be cross-linked via their similar 
senses. We have shown that NL processing of 
the strings for marking the meanings of the 
entries is necessary in order to make them 
comparable. We further have shown that our 
encoding of the entries of the dictionaries is also 
supporting the linking to already existing lexical 
senses and other language data in the LOD. The 
model have been implemented in the TopBraider 
composer13 and all the entries of the dictionaries, 
as instances of the defined classes and properties, 
are automatically mapped onto the corresponding 
Turtle syntax14 and will be made available very 
soon as deferentiable URLs, making thus less-
                                                          
13 http://www.topquadrant.com/tools/IDE-topbraid-
composer-maestro-edition/ 
14 http://www.w3.org/TeamSubmission/turtle/ 
resourced language data available in the LOD. 
Future work will consist in applying a similar 
approach to historical and geographical contexts 
given in the entries of the dialectal dictionaries. 
Acknowledgments 
The work by University of Saarland described in 
this paper is partially supported by the PHEME 
project, funded by the European Commission 
under number 611233. 
References  
Wandl-Vogt, E. and Declerck, T. (2013) Mapping a 
Traditional Dialectal Dictionary with Linked Open 
Data. In Proc. of eLex 2013, Tallin, Estonia.  
Hornung, M., Gr?ner, S. (2002) W?rterbuch der 
Wiener Mundart; Neubearbeitung. ?bvhpt, Wien. 
McCrae, J., Aguado-de-Cea, G., Buitelaar P., 
Cimiano P., Declerck, T., G?mez-P?rez, A., Gracia, 
J., Hollink, L., Montiel-Ponsoda, E., Spohr, D., 
Wunner, T. (2012) Interchanging lexical resources 
on the Semantic Web. In: Language Resources and 
Evaluation. Vol. 46, Issue 4, Springer:701-719. 
Miles, A., Matthews, B., Wilson, M. D., Brickley, D. 
(2005) SKOS Core: Simple Knowledge 
Organisation for the Web. In Proc. International 
Conference on Dublin Core and Metadata 
Applications, Madrid, Spain,  
Moulin, C. (2010) Dialect dictionaries - traditional 
and modern. In: Auer, P., Schmidt, J.E. (2010) (eds) 
Language and Space. An International Handbook 
of Linguistic Variation. Volume 1: Theories and 
Methods. Berlin / New York. pp: 592-612.  
Romary, L. (2009) Questions & Answers for TEI 
Newcomers. Jahrbuch f?r Computerphilologie 10. 
Mentis Verlag, 
Schreibman, S. (2009) The Text Encoding Initiative: 
An Interchange Format Once Again. Jahrbuch f?r 
Computerphilologie 10. Mentis Verlag. 
Wandl-Vogt, E. (2005) From paper slips to the 
electronic archive. Cross-linking potential in 90 
years of lexicographic work at the W?rterbuch der 
bairischen Mundarten in ?sterreich (WB?). In: 
Complex 2005. Papers in computational 
lexicography. Budapest: 243-254. 
W?rterbuch der bairischen Mundarten in ?sterreich 
(WB?) (1963-). Wien.     Accessed at 
http://hw.oeaw.ac.at/wboe/31205.xml?frames=yes 
(25.5.2)
12
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 18?23,
Coling 2014, Dublin, Ireland, August 24 2014.
Harmonizing Lexical Data for their Linking to Knowledge Objects in 
the Linked Data Framework 
 
 
Thierry Declerck 
DFKI GmbH,  
Language Technology Lab 
Stuhlsatzenhausweg, 3 
D-66123 Saarbr?cken,  
Germany 
declerck@dfki.de 
 
  
 
Abstract 
In this position paper we discuss some of the experiences we made in describing lexical data 
using representation formalisms that are compatible for the publication of such data in the 
Linked Data framework. While we see a huge potential in the emerging Linguistic Linked 
Open Data, also supporting the publication of less-resourced language data on the same plat-
form as for mainstream languages, we are wondering if, parallel to the widening of linking 
language data to both other language data and encyclopaedic knowledge present in the Linked 
Data cloud, it would not be beneficial to give more focus more on harmonization and merging 
of RDF encoded lexical data, instead of establishing links between such resources in the 
Linked Data.? 
1 Introduction 
In recent years a lot of initiatives have emerged towards the RDF based representation of language 
data and the hereby opened possibility to publish those data in the Linked Open Data (LOD) cloud1. 
This development has been leading to the establishment of a specialized Linked Data (LD) cloud for 
language data. The actual diagram of this rapidly growing Linguistic Linked Open Data (LLOD) 
framework2 reflects the distinct types of language data that already exist in LOD compliant formats, 
supporting their publication in the cloud and enabling their cross-linking and their linking to other 
knowledge objects available in the LOD context.  
And to further stress the importance of this development, the main conference in the field of lan-
guage resources, LREC, has declared the LOD as one of the hot topics of its 2014 edition3 and we can 
observe from the list of accepted papers and workshops/tutorials that indeed this is really a hot topic 
for the description of language resources. 
Some projects and initiatives have been very active in this field, and we want to mention here only a 
few, like the LOD2 project4, which released among others the NIF (NLP Interchange Format) 5 speci-
fications, or the Monnet project6, which delivered the lemon model for the representation of lexical 
                                                 
?This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings foo-
ter are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
1 See http://linkeddata.org/ 
2 See http://linguistics.okfn.org/resources/llod/ 
3 http://lrec2014.lrec-conf.org/en/calls-for-papers/lrec-2014-hot-topics/ 
4 See http://lod2.eu/Welcome.html 
5 See http://nlp2rdf.org/nif-1-0 
6 http://cordis.europa.eu/fp7/ict/language-technologies/project-monnet_en.html 
18
data in ontologies7, and the current project LIDER, which is aiming at providing ?the basis for the cre-
ation of a Linguistic Linked Data cloud that can support content analytics tasks of unstructured multi-
lingual cross-media content? 8. Participants of those projects and many other researchers joined in 
standardization activities, mainly in the context of W3C, like the Ontolex community group9.  
We are also aware of works porting dialectal dictionaries (Wandl-Vogt and Declerck, 2013) or po-
larity lexicons (Buitelaar et al., 2013) onto LOD compliant representation formalisms. A benefit of 
such approaches is the fact that lexical data can be linked to meanings encoded in knowledge sources 
that are accessible via a URI, such as senses encoded in the DBpedia instantiation of Wiktioanry, and 
from there one can navigate to multilingual lexical equivalents, if those are available. 
As a concrete example, working on historical German text, we could link the old word form 
?Fegfeur? (purgatory) via its modern German lemma ?Fegefeuer? not only to a lexical sense in the 
DBpedia instantiation of Wiktionary: http://wiktionary.dbpedia.org/page/Fegefeuer-German-Noun-
1de, also with access to 7 translations of this sense, but also leading to the DBpedia page for ?purgato-
ry?, one get additional semantic information, so for example that the word is related to the categories  
?Christian_eschatology?, ?Christianity_and_death? etc.10 And, in fact, the recent release of BabelNet 
2.5 is summarizing this information in one page11 for the reader, integrating information from Word-
Net, Wiktionary and Wikipedia.  This example alone gives a very strong argument on why it is worth 
to encode language data using the same type of representation formalism as for knowledge objects 
available in the Linked Data cloud. 
2 Representation Formalisms used  
Based on the Resource Description Framework (RDF)12, SKOS (Simple Knowledge Organization 
System)13 ?provides a model for expressing the basic structure and content of concept schemes such as 
thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar 
types of controlled vocabulary.?14 This representation language is being widely used, since SKOS 
concepts can be (1) ?semantically related to each other in informal hierarchies and association net-
works?, (2) ?the SKOS vocabulary itself can be extended to suit the needs of particular communities 
of practice? and finally, because it (3) ?can also be seen as a bridging technology, providing the miss-
ing link between the rigorous logical formalism of ontology languages such as OWL and the chaotic, 
informal and weakly-structured world of Web-based collaboration tools.?15 With the use of SKOS (and 
RDF), we are also in the position to make language resources compatible with other language resource 
available in the LOD cloud, as we could see with our examples above with the DBpedia instantiation 
of Wiktionary16 or the very recent release of BabelNet. Since, contrary to most knowledge objects 
described in the LOD, we do not considers strings (encoding lemma and word forms as part of a lan-
guage) as being just literals, but in also knowledge objects, we considered the use of SKOS-XL and of 
the lemon model, which was developed in the context of the Monnet project17. lemon is also available 
as an ontology18. 
3 A concrete Exercise with (German) polarity Lexicons 
Inspired by (Buitelaar et al., 2013) we aimed at porting German polarity lexicons to a Linked Data 
compliant format, and so publish our data directly in the cloud. Our starting points are the following 
resources:  
                                                 
7 See http://lemon-model.net/ 
8 See http://www.lider-project.eu/ 
9 http://www.w3.org/community/ontolex/wiki/Main_Page 
10 Details of this work is decribed in (Resch et al., 2014) 
11 See http://babelnet.org/search.jsp?word=Fegefeuer&lang=DE 
12 http://www.w3.org/RDF/ 
13 http://www.w3.org/2004/02/skos/ 
14 http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/ 
15 Ibid. 
16 See http://dbpedia.org/Wiktionary. There, lemon is also used for the description of certain lexical properties. 
17 See http://lemon-model.net/ 
18 See http://www.monnet-project.eu/lemon 
19
? A polarity lexicon for German19 (Clematide and Klenner, 2010) 
? GermanPolarityClues20 (Waltinger, 2010) 
? GermanSentiSpin21 
? SentiWS22 (Remus et al., 2010) 
 
3.1 Pre-Processing of the lexical Data: Harmonization 
As the reader can imagine, all those resources were available in distinct formats and containing dis-
tinct types of features. Therefore, we first had first to define a pre-processing of the different lexical 
data for the purpose of their harmonisation. This point leads us to a general remark: It is by far not 
enough to transform the representation of the lexical data onto RDF and related languages for ensuring 
their semantic interoperability in the LOD cloud, but preliminary work has to be performed. Just to 
give an example of the outcome of this work, we present a harmonized entry in Figure 1 below: 
 
 
"fehler" => {             # lemma 
  "prov::GermanPC.lex" => {      # provenance info 
   "pos::N" => {      # PoS info 
    "pol_rank" => "0.783019",    # ranking in the orginal source  
    "pol_val" => "NEG",    # polarity feature in the orig souce 
   }, 
  }, 
  "prov::GermanSentiSpin.lex" => { 
   "pos::N" => { 
    "pol_rank" => "0.0087112", 
    "pol_val" => "NEG", 
   }, 
  }, 
  "prov::GermanSentiWS.lex" => { 
   "pos::N" => { 
    "pol_rank" => "0.6752", 
    "pol_val" => "NEG", 
   }, 
  }, 
  "prov::german.lex" => { 
   "pos::N" => { 
    "pol_rank" => "0.7", 
    "pol_val" => "NEG", 
   }, 
  }, 
 }, 
 
 
 
 
 
Only on the base of this harmonized lexicon, we started to model the lexical resource for publication 
on the LOD framework. But before getting onto the presentation of the model, we should note that the 
harmonized lexicon also contributed to a reduction of the lexical data: instead of originally 4 (lemma) 
entries, we have now only one. 
                                                 
19 Downloadable at http://sentimental.li/german.lex 
20 Downloadable at http://www.ulliwaltinger.de/sentiment/ 
21 SentiSpin is originally an English resource (Takamura eta., 2005), tranlsated to German by (Waltinger, 2010b). 
22 Downloadable at http://asv.informatik.uni-leipzig.de/download/sentiws.html  
Figure 1: The harmonized entry ?fehler? (error) . The remaining differences in this polarity lexicon can be 
only in the value of the features ?pos?, ?pol_val? and ?pol_rank?.  
20
3.2 The LOD compliant Representation of the harmonized polarity Lexicon 
Our work consisted in providing a representation of the lexical data using as much as possible infor-
mation that is available in external resources, like the ISOcat registry23, and an ontological model for 
the representation of polarity data, which is a slight extension of the MARL model, described in 
(Westerski et al., 2013). In below, we just display an excerpt of the description of the entry ?fehler?: 
 
 
:LexicalSense_Fehler 
      rdf:type lemon:LexicalSense ; 
      rdfs:label "fehler"@de ; 
      lemon:reference <http://wiktionary.dbpedia.org/page/Fehler-German-Noun-1de> . 
 
:Opinion_Fehler 
      rdf:type skosxl:Label , :lemma ; 
      rdfs:label "Fehler"@de ; 
      hasOpinionObject :Opinion_Fehler_2 , :Opinion_Fehler_3 , :Opinion_Fehler_4 , 
:Opinion_Fehler_1 ; 
      :hasPoS <http://www.isocat.org/rest/dc/1333> ; 
      skosxl:literalForm "fehler"@de . 
 
:Opinion_Fehler_1 
      rdf:type :Opinion_Object ; 
      rdfs:label "Fehler"^^xsd:string ; 
      op:assessedBy <http://tutorial-topbraid.com/lex_tm#german.lex> ; 
      op:hasPolarity op:Negative ; 
      op:maxPolarityValue "1.0"^^xsd:double ; 
      op:minPolarityValue "-1.0"^^xsd:double ; 
      op:polarityValue "-0.7"^^xsd:double . 
 
?? 
 
 
 
 
As the reader can see, such representation can link the lexical information to a wide range of related 
information, and what in the context of former infrastructures for language resources was represented 
by a set of external metadata can be incorporated here directly in the choice of classes and properties. 
In fact, we do not need to encode the information that the entry has PoS Noun, since this information 
is encoded in the details of the reference in Wiktionary/DBpedia we are pointing to.   
4 Some ?philosophical? Comments 
The work we described briefly in this position paper, as well as work performed by researchers for 
porting for example dialectal dictionaries onto the LOD compliant formats (see Wandl-Vogt & De-
clerck, 2013) show a real potential for publishing distinct types of lexical data in the cloud, and to 
make this data accessible for both humans and machine in a very principled way. As noted, the use of 
carefully selected (widely accepted) classes and properties in the representation of the lexical data can 
also replace the use of complex metadata sets: parts of those metadata sets being implicitly encoded in 
the semantic representation the lexical data.  
This positive aspect should not hide the fact that, at least in our opinion, the community is not think-
ing enough in providing for harmonization of the original lexical data. In many cases the data sets in 
the Linguistic Linked Open Data are redundant, repeating for example many times the lemmas of lexi-
cal entries in the different types of data set. We think that similar to the ISOcat data category we could 
aim at having a ?centralized? repository for lemmas of one language, so that this lemma is not repeat-
ed for example in Wiktionary, Lexvo24 and many other data sets in the LLOD. We are wondering if, in 
                                                 
23 See for example http://www.isocat.org/rest/dc/1333 for our selected ISOcat entry for the pos ?noun?. 
24 See http://www.lexvo.org/ 
Figure 2: The RDF, SKOS-XL and lemon representation of the entry, with a link to an ontological framework represent-
ing polarity information. The various polarities given by the various sources are represented as ?OpinionObjects?. 
21
the precise context of the LOD ? linking lexical data to other data sets in the cloud ? it would not be 
possible to have exactly one lexical data set for reach language. Figure 3 below sketches our intended 
model, taking as example terminology in the field of financial reporting. 
 
 
 
 
 
Figure 3: An example instantiation of the model we are aiming at: a unique (lemma) lexicon for one language (bottom right). 
Getting the full forms from a repository of such forms, including feature structures describing their morpho-syntactic infor-
mation. Those are linking to occurrences of terms or labels that are used in knowledge objects (domain ontologies, taxono-
mies etc.). This model allows to precisely linking information from the lexicon, the morpho-syntactic descriptions and poten-
tial grammatical patterns as those are used in labels, comments or definitions in the context of knowledge objects in the LOD 
data sets. This model for representing lexical and linguistic data would be specialized for establishing linking between lan-
guage data and representation of world knowledge. We expect from this approach an improvement in fields like domain spe-
cific machine translation and ontology-.based multilingual information extraction. 
5 Conclusions 
In this short position paper, we presented some experiences done in the context of the emerging Lin-
guistic Linked Open Data framework. This lead us to make some comments on the way we could go 
for a much more ?compressed? distribution of semantically (using LOD compliant representation lan-
guages) encoded language data, which could be more easily re-used in the context of knowledge-based 
NLP applications. The result would be a set of language specific ?centralised? repositories of lemmas 
and related full forms, all equipped with URIs, that are used in the context of knowledge objects pre-
sent in the Linked Data framework.  
 
Acknowledgments 
The research described in this paper is partly supported by the European Lider project. LIDER: 
"Linked Data as an enabler of cross-media and multilingual content analytics for enterprises across 
Europe" is a FP7 project with reference number 610782.  
 
 
 
22
References 
Buitelaar, P., Mihael Arcan, Carlos A. Iglesias, J. Fernando S?nchez, Carlo Strapparava (2013) Lin-
guistic Linked Data for Sentiment Analysis.  In: 2nd Workshop on Linked Data in Linguistics (LDL 
2013): Representing and linking lexicons, terminologies and other language data. Collocated with 
the Conference on Generative Approaches to the Lexicon, Pisa, Italy 
Clematide, S, Klenner, M. (2010). ?Evaluation and extension of a polarity lexicon for German?. In: 
Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA); Held 
in conjunction to ECAI 2010 Portugal, Lisbon, Portugal, 17 August 2010 - 17 August 2010, 7-13. 
Clematide, S., Gindl, S., Klenner, M., Petrakis, S., Remus, R., Ruppenhofer, J., Waltinger, U. and 
Wiegand, M. (2012). MLSA ? A Multi-layered Reference Corpus for German Sentiment Analysis.? 
In: Proceedings of the Eight International Conference on Language Resources and Evaluation 
(LREC'12), Istanbul, 23 May 2012 - 25 May 2012.  
Hellmann, S., Lehmann, J., Auer, A. and Br?mmer, M.: Integrating NLP using Linked Data In: 12th 
International Semantic Web Conference, 21-25 October 2013, Sydney, Australia.  
Klenner, M., Clematide, S., Petrakis, S., Luder, M. (2012). ?Compositional syntax-based phrase-level 
polarity annotation for German?. In: The 10th International Workshop on Treebanks and Linguistic 
Theories (TLT 2012), Heidelberg, 06 January 2012 - 07 January 2012, 
McCrae,J.,  Aguado-de-Cea, G., P. Buitelaar, P. Cimiano, T. Declerck, A. G?mez-P?rez, J. Gracia, L. 
Hollink, E. Montiel-Ponsoda, D. Spohr, T. Wunner.(2012) Interchanging lexical resources on the 
Semantic Web.Language Resources and Evaluation.  
Remus, R., Quasthoff, U. and Heyer, G. (2010). ?SentiWS - a Publicly Available German-language 
Resource for Sentiment Analysis.? In: Proceedings of the 7th International Language Ressources 
and Evaluation (LREC'10), 2010 
Resch, C.,  Declerck, T., M?rth, K, and Czeitschner, U. (2014) Linguistic and Semantic Annotation in 
Religious Memento Mori Literature. In Proceedings of the 2nd Workshop on Language Ressources 
and Evaluation for Religious Texts. 
Hiroya Takamura, Takashi Inui, and Manabu Okumura. (2005). Extracting semantic orientations of 
words using spin model. In Proceedings of the 43rd Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational Linguistics. 
Waltinger, U. (2010b). ?Sentiment Analysis Reloaded: A Comparative Study On Sentiment Polarity 
Identification Combining Machine Learning And Subjectivity Features?. In: Proceedings of the 6th 
International Conference on Web Information Systems and Technologies (WEBIST '10). 
Wandl-Vogt, E., Declerck, T. (2013). Mapping a Traditional Dialectal Dictionary with Linked Open 
Data. In. Kosem, I., Kallas, J., Gantar, P., Krek, S., Langemets, M., Tuulik, M. (eds.) 2013. Elec-
tronic lexicography in the 21st century: thinking outside the paper. Proceedings of the eLex 2013 
conference, 17-19 October 2013, Tallinn, Estonia. Ljubljana/Tallinn: Trojina, Institute for Applied 
Slovene Studies/Eesti Keele Instituut. 
Westerski, Adam and S?nchez-Rada, J. Fernando, Marl Ontology Specification, V1.0 May 2013, 
available at http://www.gsi.dit.upm.es/ontologies/marl 
 
 
 
23
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 30?38,
Coling 2014, Dublin, Ireland, August 24 2014.
SentiMerge: Combining Sentiment Lexicons in a Bayesian Framework
Guy Emerson and Thierry Declerck
DFKI GmbH
Universit?at Campus
66123 Saarbr?ucken
{guy.emerson, thierry.declerck}@dfki.de
Abstract
Many approaches to sentiment analysis rely on a lexicon that labels words with a prior polarity.
This is particularly true for languages other than English, where labelled training data is not
easily available. Existing efforts to produce such lexicons exist, and to avoid duplicated effort, a
principled way to combine multiple resources is required. In this paper, we introduce a Bayesian
probabilistic model, which can simultaneously combine polarity scores from several data sources
and estimate the quality of each source. We apply this algorithm to a set of four German sentiment
lexicons, to produce the SentiMerge lexicon, which we make publically available. In a simple
classification task, we show that this lexicon outperforms each of the underlying resources, as
well as a majority vote model.
1 Introduction
Wiegand (2011) describes sentiment analysis as the task of identifying and classifying opinionated con-
tent in natural language text. There are a number of subtasks within this field, such as identifying the
holder of the opinion, and the target of the opinion.
In this paper, however, we are concerned with the more specific task of identifying polar language -
that is, expressing either positive or negative opinions. Throughout the rest of this paper, we will use the
terms sentiment and polarity more or less interchangeably.
As Pang and Lee (2008) explain, sentiment analysis has become a major area of research within natural
language processing (NLP), with many established techniques, and a range of potential applications.
Indeed, in recent years there has been increasing interest in sentiment analysis for commercial purposes.
Despite the rapid growth of this area, there is a lack of gold-standard corpora which can be used to
train supervised models, particularly for languages other than English. Consequently, many algorithms
rely on sentiment lexicons, which provide prior knowledge about which lexical items might indicate
opinionated language. Such lexicons can be used directly to define features in a classifier, or can be
combined with a bootstrapping approach.
However, when presented with a number of overlapping and potentially contradictory sentiment lex-
icons, many machine learning techniques break down, and we therefore require a way to merge them
into a single resource - or else a researcher must choose between resources, and we are left with a leaky
pipeline between resource creation and application. We review methods for combining sources of infor-
mation in section 2, and then describe four German sentiment lexicons in section 3.
To merge these resources, we first want to make them match as closely as possible, and then deal with
the differences that remain. We deal with the first step in section 4, describing how to align the polarity
scores in different lexicons so that they can be directly compared. Then in section 5, we describe how to
combine these scores together.
We report results in section 6, including evaluation against a small annotated corpus, where our merged
resource outperforms both the original resources and also a majority vote baseline. Finally, we discuss
distribution of our resource in section 7, future work in section 8, and conclude in section 9.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
30
Lexicon # Entries
C&K 8714
PolarityClues 9228
SentiWS 1896
SentiSpin 95572
SentiMerge 96918
Table 1: Comparison of lexicon sizes
2 Related Work
A general problem is how to deal with missing data - in our case, we cannot expect every word to
appear in every lexicon. Schafer and Graham (2002) review techniques to deal with missing data, and
recommend two approaches: maximum likelihood estimation and Bayesian multiple imputation. The
latter is a Monte Carlo method, helpful when the marginal probability distribution cannot be calculated
analytically. The probabilistic model presented in section 5.1 is straightforward enough for marginal
probabilities to be calculated directly, and we employ maximum likelihood estimation for this reason.
A second problem is how to combine multiple sources of information, which possibly conflict, and
where some sources are more reliable than others. This becomes particularly challenging in the case
when no gold-standard data exists, and so the sources can not be evaluated directly. Raykar et al. (2010)
discusses this problem from the point of view of crowdsourcing, where there are multiple expert views
and no certain ground truth - but we can equally apply this in the context of sentiment analysis, viewing
each source as an expert. However, unlike their approach, our algorithm does not directly produce a
classifier, but rather a newly labelled resource.
Confronted with a multiplicity of data sources, some researchers have opted to link resources together
(Eckle-Kohler and Gurevych, 2013). Indeed, the lexicons we consider in section 3 have already been
compiled into a common format by Declerck and Krieger (2014). However, while linking resources
makes it easier to access a larger amount of data, it does not solve the problem of how best to process it.
To the best of our knowledge, there has not been a previous attempt to use a probabilistic model to
merge a number of sentiment lexicons into a single resource.
3 Data Sources
In the following subsections, we first describe four existing sentiment lexicons for German. These four
lexicons represent the data we have merged into a single resource, with a size comparison given in table 1,
where we count the number of distinct lemmas, not considering parts of speech. Finally, in section 3.5,
we describe the manually annotated MLSA corpus, which we use for evaluation.
3.1 Clematide and Klenner
Clematide and Klenner (2010) manually curated a lexicon
1
of around 8000 words, based on the synsets
in GermaNet, a WordNet-like database (Hamp and Feldweg, 1997). A semi-automatic approach was used
to extend the lexicon, first generating candidate polar words by searching in a corpus for coordination
with known polar words, and then presenting these words to human annotators. We will refer to this
resource as the C&K lexicon.
3.2 SentimentWortschatz
Remus et al. (2010) compiled a sentiment lexicon
2
from three data sources: a German translation of
Stone et al. (1966)?s General Inquirer lexicon, a set of rated product reviews, and a German collocation
dictionary. At this stage, words have binary polarity: positive or negative. To assign polarity weights,
they use a corpus to calculate the mutual information of a target word with a small set of seed words.
1
http://bics.sentimental.li/index.php/downloads
2
http://asv.informatik.uni-leipzig.de/download/sentiws.html
31
3.3 GermanSentiSpin
Takamura et al. (2005) produced SentiSpin, a sentiment lexicon for English. It is so named becaused
it applies the Ising Model of electron spins. The lexicon is modelled as an undirected graph, with each
word type represented by a single node. A dictionary is used to define edges: two nodes are connected if
one word appears in the other?s definition. Each word is modelled as having either positive or negative
sentiment, analogous to electrons being spin up or spin down. An energy function is defined across the
whole graph, which prefers words to have the same sentiment if they are linked together. By using a
small seed set of words which are manually assigned positive or negative sentiment, this energy function
allows us to propagate sentiment across the entire graph, assigning each word a real-valued sentiment
score in the interval [?1, 1].
Waltinger (2010b) translated the SentiSpin resource into German
3
using an online dictionary, taking
at most three translations of each English word.
3.4 GermanPolarityClues
Waltinger (2010a) utilised automatic translations of two English resources: the SentiSpin lexicon, de-
scribed in section 3.3 above; and the Subjectivity Clues lexicon, a manually annotated lexicon produced
by Wilson et al. (2005). The sentiment orientations of the German translations were then manually
assessed and corrected where necessary, to produce a new resource.
4
3.5 MLSA
To evaluate a sentiment lexicon, separately from the general task of judging the sentiment of an entire
sentence, we relied on the MLSA (Multi-Layered reference corpus for German Sentiment Analysis).
This corpus was produced by Clematide et al. (2012), independently of the above four lexicons, and
consists of 270 sentences annotated at three levels of granularity. In the first layer, annotators judged
the sentiment of whole sentences; in the second layer, the sentiment of words and phrases; and finally in
the third layer, they produced a FrameNet-like analysis of each sentence. The third layer also includes
lemmas, parts of speech, and a syntactic parse.
We extracted the sentiment judgements of individual words from the second layer, using the majority
judgement of the three annotators. Each token was mapped to its lemmatised form and part of speech,
using the information in the third layer. In some cases, the lemma was listed as ambiguous or unknown,
and in these cases, we manually added the correct lemma. Additionally, we changed the annotation of
nominalised verbs from nouns to verbs, to match the lexical entries. Finally, we kept all content words
(nouns, verbs, and adjectives) to form a set of test data. In total, there were 1001 distinct lemma types,
and 1424 tokens. Of these, 378 tokens were annotated as having positive polarity, and 399 as negative.
4 Normalising Scores
By considering positive polarity as a positive real number, and negative polarity as a negative real number,
all of the four data sources give polarity scores between ?1 and 1. However, we cannot assume that the
values directly correspond to one another. For example, does a 0.5 in one source mean the same thing
in another? An example of the kind of data we are trying to combine is given in table 2, and we can see
that the polarity strengths vary wildly between the sources.
The simplest model is to rescale scores linearly, i.e. for each source, we multiply all of its scores by
a constant factor. Intuitively, the factors should be chosen to harmonise the values - a source with large
scores should have them made smaller, and a source with small scores should have them made larger.
4.1 Linear Rescaling for Two Sources
To exemplify our method, we first restrict ourselves to the simpler case of only dealing with two lexicons.
Note that when trying to determine the normalisation factors, we only consider words in the overlap
between the two; otherwise, we would introduce a bias according to what words are considered in each
3
http://www.ulliwaltinger.de/sentiment
4
http://www.ulliwaltinger.de/sentiment
32
Lemma, POS verg?ottern, V
C&K 1.000
PolarityClues 0.333
SentiWS 0.004
SentiSpin 0.245
Table 2: An example lemma, labelled with polarity strengths from each data source
source - it is only in the overlap that we can compare them. However, once these factors have been
determined, we can use them to rescale the scores across the entire lexicon, including items that only
appear in one source.
We consider lemmas with their parts of speech, so that the same orthographic word with two possible
parts of speech is treated as two independent lexical entries, in all of the following calculations. However,
we do not distinguish homophonous or polysemous lemmas within the same part of speech, since none
of our data sources provided different sentiment scores for distinct senses.
For each word i, let u
i
and v
i
be the polarity scores for the two sources. We would like to find
positive real values ? and ? to rescale these to ?u
i
and ?v
i
respectively, minimising the loss function
?
i
(?u
i
? ?v
i
)
2
. Intuitively, we are trying to rescale the sources so that the scores are as similar as
possible. The loss function is trivially minimised when ? = ? = 0, since reducing the sizes of the scores
also reduces their difference. Hence, we can introduce the constraint that ?? = 1, so that we cannot
simultaneously make the values smaller in both sources. We would then like to minimise:
?
i
(
?u
i
?
1
?
v
i
)
2
= |u|
2
?
2
? 2u.v + |v|
2
?
?2
Note that we use vector notation, so that |u|
2
= ?
i
u
2
i
. Differentiating this with respect to ?, we get:
2? |u|
2
? 2 |v|
2
?
?3
= 0 ? ? =
?
|v|
?
|u|
However, observe that we are free to multiply both ? and ? by a constant factor, since this doesn?t
affect the relationship between the two sources, only the overall size of the polarity values. By dividing
by
?
|u| |v|, we derive the simpler expressions ? = |u|
?1
and ? = |v|
?1
, i.e. we should divide by the
root mean square. In other words, after normalising, the average squared polarity value is 1 for both
sources.
5
4.2 Rescaling for Multiple Sources
For multiple sources, the above method needs tweaking. Although we could use the overlap between all
sources, this could potentially be much smaller than the overlap between any two sources, introducing
data sparsity and making the method susceptible to noise. In the given data, 10749 lexical items appear
in at least two sources, but only 1205 appear in all four. We would like to exploit this extra information,
but the missing data means that methods such as linear regression cannot be applied.
A simple solution is to calculate the root mean square values for each pair of sources, and then average
these values for each source. These averaged values define normalisation factors, as a compromise
between the various sources.
4.3 Unspecified scores
Some lexical items in the PolarityClues dataset were not assigned a numerical score, only a polarity
direction. In these cases, the task is not to normalise the score, but to assign one. To do this, we can first
normalise the scores of all other words, as described above. Then, we can consider the words without
scores, and calculate the root mean square polarity of these words in the other sources, and assign them
this value, either positive or negative.
5
In most sentiment lexicons, polarity strengths are at most 1. This will no longer be true after this normalisation.
33
5 Combining Scores
Now that we have normalised scores, we need to calculate a combined value. Here, we take a Bayesian
approach, where we assume that there is a latent ?true? polarity value, and each source is an observation
of this value, plus some noise.
5.1 Gaussian Model
A simple model is to assume that we have a prior distribution of polarity values across the vocabulary,
distributed normally. If we further assume that a language is on average neither positive nor negative,
then this distribution has mean 0. We denote the variance as ?
2
. Each source independently introduces
a linear error term, which we also model with a normal distribution: errors from source a are distributed
with mean 0 and standard deviation ?
2
a
, which varies according to the source.
6
5.2 Hyperparameter Selection
If we observe a subset S = {a
1
, . . . , a
n
} of the sources, the marginal distribution of the observations
will be normally distributed, with mean 0 and covariance matrix as shown below. If the error variances
?
2
a
are small compared to the background variance ?
2
, then this implies a strong correlation between the
observations.
?
?
?
?
?
?
?
2
+ ?
2
a
1
?
2
. . . ?
2
?
2
?
2
+ ?
2
a
2
? ? ? ?
2
.
.
.
.
.
.
.
.
.
.
.
.
?
2
?
2
? ? ? ?
2
+ ?
2
a
n
?
?
?
?
?
?
To choose the values for ?
2
and ?
2
a
, we can aim to maximise the likelihood of the observations, i.e.
maximise the value of the above marginal distributions at the observed points. This is in line with Schafer
and Graham (2002)?s recommendations. Such an optimisation problem can be dealt with using existing
software, such as included in the SciPy
7
package for Python.
5.3 Inference
Given a model as above (whether or not the hyperparameters have been optimised), we can calculate the
posterior distribution of polarity values, given the observations x
a
i
. This again turns out to be normally
distributed, with mean ?? and variance ??
2
given by:
?? =
?
?
?2
a
i
x
a
i
?
?2
+
?
?
?2
a
i
??
?2
= ?
?2
+
?
?
?2
a
i
The mean is almost a weighted average of the observed polarity values, where each source has weight
?
?2
a
. However, there is an additional term ?
?2
in the denominator - this means we can interpret this
as a weighted average if we add an additional polarity value 0, with weight ?
?2
. This additional term
corresponds to the prior.
The weights for each source intuitively mean that we trust sources more if they have less noise. The
extra 0 term from the prior means that we interpret the observations conservatively, skewing values
towards 0 when there are fewer observations. For example, if all sources give a large positive polarity
value, we can be reasonably certain that the true value is also large and positive, but if we only have data
from one source, then we are less certain if this is true - our estimate ?? is correspondingly smaller, and
the posterior variance ??
2
correspondingly larger.
6
Because of the independence assumptions, this model can alternatively be viewed as a Markov Network, where we have
one node to represent the latent true polarity strengths, four nodes to represent observations from each source, and five nodes
to represent the hyperparameters (variances)
7
http://www.scipy.org
34
Figure 1: Gaussian kernel density estimate
6 Experiments and Results
6.1 Parameter Values
The root mean square sentiment values for the sources were: C&K 0.845; PolarityClues 0.608; SentiWS
0.267; and SentiSpin 0.560. We can see that there is a large discrepancy between the sizes of the scores
used, with SentiWS having the smallest of all. It is precisely for this reason that we need to normalise
the scores.
The optimal variances calculated during hyperparameter selection (section 5.2) were: prior 0.528;
C&K 0.328; PolarityClues 0.317; SentiWS 0.446; and SentiSpin 0.609. These values correlate with our
intuition: C&K and PolarityClues have been hand-crafted, and have smaller error variances; SentiWS
was manually finalised, and has a larger error; while finally SentiSpin was automatically generated, and
has the largest error of all, larger in fact than the variance in the prior. We would expect the polarity
values from a hand-crafted source to be more accurate, and this appears to be justified by our analysis.
6.2 Experimental Setup
The MLSA data (see section 3.5) consists of discrete polarity judgements - a word is positive, negative, or
neutral, but nothing in between.
8
To allow direct evaluation against such a resource, we need to discretise
the continuous range of polarity values; i.e. if the polarity value is above some positive threshold, we
judge it to be positive; if it is below a negative threshold, negative; and if it is between the two thresholds,
neutral. To choose this threshold before evaluation, we calculated a Gaussian kernel density estimate of
the polarity values in the entire lexicon, as shown in figure 1. There is a large density near 0, reflecting
that the bulk of the vocabulary is not strongly polar; indeed, so that the density of polar items is clearly
visible, we have chosen a scale that forces this bulk to go off the top of the chart. The high density stops
at around ?0.23, and we have accordingly set this as our threshold.
We compared the merged resource to each of the original lexicons, as well as a ?majority vote? baseline
which represents an alternative method to combine lexicons. This baseline involves considering the
polarity judgements of each lexicon (positive, negative, or neutral), and taking the most common answer.
To break ties, we took the first answer when consulting the lexicons in the following order, reflecting their
reliability: C&K, PolarityClues, SentiWS, SentiSpin.
For the automatically derived resources, we can introduce a threshold as we did for SentiMerge. How-
ever, to make these baselines as competitive as possible, we optimised them on the test data, rather than
choosing them in advance. They were chosen to maximise the macro-averaged f-score. For SentiWS,
the threshold was 0, and for SentiSpin, 0.02.
Note that a perfect score would be impossible to achieve, since 31 lemmas were annotated with more
than polarity type. These cases generally involve polysemous words which could be interpreted with
different polarities depending on the context. Indeed, two words appeared with all three labels: Span-
nung (tension) and Widerstand (resistance). In a political context, interpreting Widerstand as positive or
8
The annotation scheme also allows a further three labels: intensifier, diminisher, and shifter. While this information is
useful, we treat these values as neutral in our evaluation, since we are only concerned with words that have an inherent positive
or negative polarity.
35
Lexicon Precision Recall F-score
C&K 0.754 0.733 0.743
PolarityClues 0.705 0.564 0.626
SentiWS 0.803 0.513 0.621
SentiSpin 0.557 0.668 0.607
majority vote 0.548 0.898 0.679
SentiMerge 0.708 0.815 0.757
Table 3: Performance on MLSA, macro-averaged
negative depends very much on whose side you support. In such cases, a greater context is necessary to
decide on polarity, and a lexicon simply cannot suffice.
6.3 Evaluation on MLSA
We calculated precision, recall, and f-score (the harmonic mean of precision and recall) for both positive
and negative polarity. We report the average of these two scores in 3. We can see that in terms of f-
score, SentiMerge outperforms all four data sources, as well as the majority vote. In applications where
either precision or recall is deemed to be more important, it would be possible to adjust the threshold
accordingly. Indeed, by dropping the threshold to zero, we achieve recall of 0.894, competitive with the
majority vote method; and by increasing the threshold to 0.4, we achieve precision of 0.755, competitive
with the C&K lexicon. Furthermore, in this latter case, the f-score also increases to 0.760. We do not
report this figure in the table above because it would not be possible to predict such a judicious choice
of threshold without peeking at the test data. Nonetheless, this demonstrates that our method is robust to
changes in parameter settings.
The majority vote method performs considerably worse than SentiMerge, at least in terms of f-score.
Indeed, it actually performs worse than the C&K lexicon, with noticeably lower precision. This finding
is consistent with the results of Raykar et al. (2010), who argue against using majority voting, and who
also find that it performs poorly.
The C&K lexicon achieves almost the same level of performance as SentiMerge, so it is reasonable
to ask if there is any point in building a merged lexicon at all. We believe there are two good reasons
for doing this. Firstly, although the C&K lexicon may be the most accurate, it is also small, especially
compared to SentiSpin. SentiMerge thus manages to exploit the complementary nature of the different
lexicons, achieving the broad coverage of SentiSpin, but maintaining the precision of the C&K lexicon
for the most important lexical items.
Secondly, SentiMerge can provide much more accurate values for polarity strength than any human-
annotated resource can. As Clematide and Klenner (2010) show, inter-annotator agreement for polarity
strength is low, even when agreement for polarity direction is high. Nonetheless, some notion of po-
larity strength can still be helpful in computational applications. To demonstrate this, we calculated the
precision, recall, and f-scores again, but weighting each answer as a function of the distance from the
estimated polarity strength to the threshold. With this weighted approach, we get a macro-averaged f-
score of 0.852. This is considerably higher than the results given in table 3, which demonstrates that the
polarity scores in SentiMerge are useful as a measure of classification certainty.
6.4 Manual Inspection
In cases where all sources agree on whether a word is positive or negative, our algorithm simply serves
to assign a more accurate polarity strength. So, it is more interesting to consider those cases where the
sources disagree on polarity direction. Out of the 1205 lexemes for which we have data from all four
sources, only 22 differ between SentiMerge and the C&K lexicon, and only 16 differ between SentiMerge
and PolarityClues. One example is Beschwichtigung (appeasement). Here we can see the problem with
trying to assign a single numeric value to polarity - in a political context, Beschwichtugung could be
interpreted either as positive, since it implies an attempt to ease tension; or as negative, since it could be
36
viewed as a sign of weakness. Another example is unantastbar, which again can be interpreted positively
or negatively.
The controversial words generally denote abstract notions, or have established metaphorical senses.
In the authors? view, their polarity is heavily context-dependent, and a one-dimensional score is not
sufficient to model their contibution to sentiment.
In fact, most of these words have been assigned very small polarity values in the combined lexicon,
which reflects the conflicting evidence present in the various sources. Of the 22 items which differ in
C&K, the one with the largest value in the combined lexicon is dominieren, which has been assigned a
fairly negative combined score, but was rated positive (0.5) in C&K.
7 Distribution
We are making SentiMerge freely available for download. However, with the expanding number of
language resources, it is becoming increasingly important to link resources together, as mentioned in
section 2. For this reason, we are publishing our resource as part of the Linguistic Linked Open Data
9
initiative. In particular, we have decided to follow the specifications set forth by Buitelaar et al. (2013),
who propose a representation for sentiment resources based on Lemon (McCrae et al., 2011) and Marl
(Westerski et al., 2011). Lemon
10
is a model for resource description which builds on LMF (Lexical
Markup Framework),
11
and facilitates combination of lexicons with ontologies. Marl is an an ontology
language designed for sentiment analysis, which has been fully implemented.
12
8 Future Work
To align the disparate sources, a simple linear rescaling was used. However, in principle any monotonic
function could be considered. A more general function that would still be tractable could be u
i
7? ?u
?
i
.
Furthermore, the probabilistic model described in section 5.1 makes several simplifying assumptions,
which could be weaked or modified. For instance, we have assumed a normal distribution, with zero
mean, both for the prior distribution and for the error terms. The data is not perfectly modelled by a
normal distribution, since there are very clear bounds on the polarity scores, and some of the data takes
discrete values. Indeed, we can see in figure 1 that the data is not normally distributed. An alternative
choice of distribution might yield better results.
More generally, our method can be applied to any context where there are multiple resources to be
merged, as long as there is some real-valued property to be aligned.
9 Conclusion
We have described the merging of four sentiment lexicons into a single resource, which we have named
SentiMerge. To demonstrate the utility of the combined lexicon, we set up a word-level sentiment clas-
sification task using the MLSA corpus, in which SentiMerge outperformed all four of the underlying
resources, as well as a majority vote baseline. As a natural by-product of the merging process, we are
also able to indirectly evaluate the quality of each resource, and the results match both intuition and the
performance in the aformentioned classification task. The approach we have taken requires no parameter
setting on the part of the researcher, so we believe that the same method can be applied to other settings
where different language resources present conflicting information. This work helps to bridge the gap be-
tween resource creation efforts, which may overlap in scope, and NLP research, where researchers often
want to use all available data. Furthermore, by grounding our work in a well-defined Bayesian frame-
work, we leave scope for future improvements using more sophisticated probabilistic models. To allow
the community at large to use and build on this work, we are making SentiMerge publically available for
download, and are incorporating it into the Linguistic Linked Open Data initiative.
9
http://linguistics.okfn.org/resources/llod
10
http://lemon-model.net
11
http://www.lexicalmarkupframework.org
12
http://www.gsi.dit.upm.es/ontologies/marl
37
Acknowledgements
This work was co-financed by the European Commission, within the FP7 ICT project TrendMiner,
13
under contract number 287863. We would like to thank the authors of the all the resources mentioned
in this paper for permission to use their data. We also thank the anonymous reviewers for their helpful
comments.
References
Paul Buitelaar, Mihael Arcan, Carlos A Iglesias, J Fernando S?anchez-Rada, and Carlo Strapparava. 2013. Lin-
guistic linked data for sentiment analysis. In Proceedings of the 2nd Workshop on Linked Data in Linguistics.
Simon Clematide and Manfred Klenner. 2010. Evaluation and extension of a polarity lexicon for German. In
Proceedings of the First Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, page 7.
Simon Clematide, Stefan Gindl, Manfred Klenner, Stefanos Petrakis, Robert Remus, Josef Ruppenhofer, Ulli
Waltinger, and Michael Wiegand. 2012. MLSA ? a multi-layered reference corpus for German sentiment
analysis. pages 3551?3556. European Language Resources Association (ELRA).
Thierry Declerck and Hans-Ulrich Krieger. 2014. TMO ? the federated ontology of the TrendMiner project. In
Proceedings of the 9th International Language Resources and Evaluation Conference (LREC 2014).
Judith Eckle-Kohler and Iryna Gurevych. 2013. The practitioner?s cookbook for linked lexical resources.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet ? a lexical-semantic net for German. In Proceedings of
the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15. Association for Computational Linguistics.
John McCrae, Dennis Spohr, and Phillip Cimiano. 2011. Linking lexical resources and ontologies on the semantic
web with lemon. In Proceedings of the 8th Extended Semantic Web Conference.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information
retrieval.
Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda
Moy. 2010. Learning from crowds. The Journal of Machine Learning Research, 11:1297?1322.
Robert Remus, Uwe Quasthoff, and Gerhard Heyer. 2010. SentiWS ? a publicly available German-language
resource for sentiment analysis. In Proceedings of the 7th International Language Resources and Evaluation
Conference (LREC 2010).
Joseph L Schafer and John W Graham. 2002. Missing data: our view of the state of the art. Psychological
methods, 7(2):147.
Philip J Stone, Dexter C Dunphy, and Marshall S Smith. 1966. The general inquirer: A computer approach to
content analysis.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using
spin model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics.
Ulli Waltinger. 2010a. GermanPolarityClues: A lexical resource for German sentiment analysis. In Proceedings
of the 7th International Language Resources and Evaluation Conference (LREC 2010).
Ulli Waltinger. 2010b. Sentiment analysis reloaded - a comparative study on sentiment polarity identification
combining machine learning and subjectivity features. In Proceedings of the 6th International Conferenceon
Web Information Systems and Technologies (WEBIST 2010). INSTICC Press.
Adam Westerski, Carlos A. Iglesias, and Fernando Tapia. 2011. Linked opinions: Describing sentiments on the
structured web of data. In Proceedings of the 4th International Workshop Social Data on the Web.
Michael Wiegand. 2011. Hybrid approaches for sentiment analysis. PhD dissertation, Universit?at des Saarlandes.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sen-
timent analysis. In Proceedings of the conference on human language technology and empirical methods in
natural language processing, pages 347?354. Association for Computational Linguistics.
13
http://www.trendminer-project.eu
38
