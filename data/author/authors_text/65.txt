Coling 2008: Companion volume ? Posters and Demonstrations, pages 3?6
Manchester, August 2008
Metaphor in Textual Entailment
Rodrigo Agerri
School of Computer Science, University of Birmimgham
B15 2TT Birmingham, UK
r.agerri@cs.bham.ac.uk
Abstract
Metaphor understanding in Computational
Linguistics has largely been focused on the
development of stand-alone prototypes for
which only small-scale evaluations are car-
ried out. This has made difficult the in-
clusion of metaphor in the development
of natural language processing applica-
tions. However, dealing with metaphor
properly is ultimately crucial for any au-
tomated language technology that is to be
truly human-friendly or able to properly
appreciate utterances by humans. This pa-
per proposes to bring metaphor into the
Recognizing Textual Entailment task. By
doing so, the coverage of textual entail-
ment systems would be broadened and
metaphor research would benefit from the
textual entailment evaluation framework.
1 Introduction
Using metaphorical language is common in most
forms of everyday language, from ordinary con-
versation, ?having ideas in the back of the mind?,
through newspaper articles, ?global oil prices
clung near their highest levels?, to scientific ar-
ticles, ?the variable N goes from 1 to 100?.
Metaphor is important in part because it is an
economical and directly appealing way of talking
about many sorts of subject matter in human life,
such as time, money, relationships, emotions, poli-
tics, etc. Most importantly, metaphor can have ma-
jor effects on what can be properly inferred from
an utterance or passage.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Most of the development of natural language
processing (NLP) applications has been focused on
specific tasks such as Information Retrieval (IR)
and Question Answering (QA), largely ignoring
the question of figurative use of language. More-
over, considering the inherent difficulty in evaluat-
ing deep approaches to language in a large-scale
manner, up to date there is not a common eval-
uation framework, corpora or other resources for
metaphor processing. It certainly has not helped
that most of the computational developments on
metaphor processing have largely been stand-alone
systems that are not empirically evaluated on a
large scale (Fass, 1997; Falkenhainer et al, 1989;
Hobbs, 1992; Martin, 1990; Barnden et al, 2003).
This paper proposes to address this by adapting
the Recognizing Textual Entailment (RTE) frame-
work for metaphor interpretation. RTE aims to be
an abstract and generic task that captures major se-
mantic inference needs across applications (Dagan
et al, 2007). RTE is considered central for the
development of intelligent yet robust natural lan-
guage processing systems because most of the se-
mantic inference needed in natural language appli-
cations such as QA and IE can be characterized as
problems in RTE (Dagan et al, 2007). Intuitively,
textual entailment consists of determining whether
a hypothesis can be inferred from a given text. The
textual entailment operational definition is a direc-
tional relation between two text fragments, Text T
and Hypothesis H such that T entails H if humans
reading T and considering H will infer that H fol-
lows from T. An illustration can be given by ex-
ample 1560 of the RTE-1 dataset (which involves
a metaphor in the use of ?incubate?):
T: The technological triumph known as GPS
was incubated in the mind of Ivan Getting.
3
H: Ivan Getting invented the GPS.
As in other NLP applications, figurative language
has merely been noted as problem in the RTE field.
However, we believe that RTE provides a gen-
eral evaluation framework for semantic processing
which can be adapted for the computational test-
ing and evaluation of theories that aim to explain
the semantic inferences involved in metaphor res-
olution. Furthermore, including metaphor in the
RTE task may improve the performance and scope
of textual entailment systems, which in turn may
allow to bring metaphor into the development of
NLP systems.
2 The Role of Metaphor in Textual
Inference
Metaphorical use of language crucially affects the
inferences that can be drawn from a text. Includ-
ing metaphor in textual entailment would amount
to establish whether a hypothesis H can be in-
ferred from a text T, where (at least) T contains
a metaphorical expression whose processing is rel-
evant to judge the entailment.
2.1 Re-formulating the problem
It is usual to assume a view of metaphor under-
standing as involving some notion of properties
and relations of events that are transferred from
a source domain into a target domain. In this
view, a (declarative) metaphorical text conveys in-
formation about some target domain by means of
a number of correspondences between entities in
the source and the target domains. Lakoff and
associates argue that source to target correspon-
dences are part of more general schemes called
?conceptual metaphors? (Lakoff, 2004) which we
call ?metaphorical views?. For the GPS T-H pair
in the previous section, a metaphorical view such
as MIND AS PHYSICAL SPACE would capture
the correspondence between mind in the source to
special container or incubator in the target.
Most of the computational approaches to
metaphor processing have focused on the develop-
ment of reasoning systems which take a metaphor-
ical expression as input and perform some reason-
ing to prove the correct output ? previously given
by the researcher. The difficulties in scaling-up
and the lack of empirical evaluation have been the
main chronic problems of metaphor understanding
systems. Furthermore, a task consisting of provid-
ing an interpretation of a text such as T above ?
the GPS being incubated in the mind of Ivan Get-
ting ? is very complex because it needs to con-
sider and resolve what the ?correct interpretation?
is from the number of possible interpretations that
can be conveyed by (the use of) ?incubating?. Con-
versely, the task becomes easier if the task faced is
to judge whether H follows from T; in the GPS ex-
ample above, H sets up the context to interpret the
metaphorical expression in T, which in turn would
help to correctly judge that the GPS was invented
by Ivan Getting (?incubate? can be used instead of
?develop?, ?invent?, etc.). Thus, a slightly modi-
fied H would presumably lead to connotations of
the metaphorical use of ?incubate? previously not
considered:
T: The technological triumph known as GPS
was incubated in the mind of Ivan Getting.
H?: Ivan Getting accidentally invented the
GPS.
A reasonable judgment is that H is not entailed by
T, since an incubation process in this particular ex-
ample would seem to indicate a careful nurturing
of ideas that were brought slowly into life and so
on, within Ivan Getting?s mind. The modified H
brings extra connotations of the metaphorical use
of ?incubate? that are crucial to establish the nega-
tive entailment judgment.
2.2 Metaphor in RTE Challenges
Even though annotators aimed to filter out
metaphorical uses of language from the RTE
datasets (Zaenen et al, 2005), some metaphori-
cal texts have eluded the annotators? selection poli-
cies (Bos and Markert, 2006). Our study of RTE
datasets looking for pairs in which resolving a
metaphorical expression was relevant for the en-
tailment judgment uncovered few and mostly con-
ventional metaphors. We have focused on 10 pairs
in RTE-1 and 9 in RTE-2. Some of them are listed
here:
T1: Lyon is actually the gastronomic capital of France.
H1: Lyon is the capital of France.
T2: The upper house of the Russian parliament has ap-
proved a controversial bill to tighten state control over
non-governmental organisations (NGOs).
H2: Russian parliament closes NGOs.
T3: Convinced that pro-American officials are in the ascen-
dancy in Tokyo, they talk about turning Japan into ?the
Britain of the Far East.?
4
H3: Britain is located in the Far East.
T4: Stocks rallied for a second session Thursday, boosted
by falling oil prices and ongoing relief that the presi-
dential election has passed without incident.
H4: The falling oil prices had a positive impact on stocks.
An evaluation of the systems? accuracy for the
pairs involving metaphor was performed to test if
there was any significant difference with respect to
the overall accuracy results reported in the official
RTE challenges. The RTE-1 results are not pub-
licly available, so the study is restricted to the 7
runs which were made available ? including 4 of
the best 5 systems. Table 1 shows the official over-
all accuracy results and the results of the evaluation
over the 10 pairs involving metaphor:
Author (Group) Overall Metaphor
Bayer (MITRE) 0.586 0.4
Herrera (UNED) 0.566 0.2
0.558 0.2
Bos (Rome/Leeds) 0.563 0.2
0.555 0.1
Newman (Dublin) 0.563 0.1
0.565 0.6
Table 1: RTE-1 Accuracy Comparison.
Although the sample of metaphor pairs is fairly
small, table 1 shows that there is a trend for the ac-
curacy to be significantly lower when metaphor is
involved than for the overall results (which agrees
with Bos and Markert?s (2006) diagnostic).
RTE-2 results are publicly available and for this
study 8 runs of the best scoring systems (only
those which also submitted the average precision
results are considered) and 2 with lower accuracy
were chosen. Table 2 confirms the trend sug-
gested by table 1, namely, that the accuracy score
is lower when the judgement depends on process-
ing metaphorical uses of language. How signif-
icant are these results? For the RTE-1 pairs, a
Fisher?s test of independence establishes that for
5 out of the 7 runs the difference in performance is
statistically significant at the 0.05 level. The same
results were obtained for 7 of the 10 RTE-2 runs
compared in table 2.
3 Discussion
Although only few pairs containing fairly conven-
tional metaphors were uncovered from the RTE
Author (Group) Overall Metaphor
Hickl (LCC) 0.7538 0.4444
Tatu (LCC) 0.7375 0.5555
Zanzotto (Milan/Rome) 0.6388 0.2222
Adams (Dallas) 0.6262 0.3333
Bos (Rome/Leeds) 0.6162 0.1111
Kouylekov (Trento) 0.6050 0.1111
Vanderwende (Stanford) 0.6025 0.1111
Herrera (UNED) 0.5975 0.1111
Clarke (Sussex) 0.5275 0.1111
Newman (Dublin) 0.5250 0.4444
Table 2: RTE-2 Accuracy Comparison.
datasets, the results obtained confirm the hypoth-
esis that the ability to process metaphor would
broaden the coverage of textual entailment sys-
tems, thereby improving their overall performance.
It should also be considered that achieving statisti-
cal significance is harder when the overall results
are not that high, as shown by the fact that we get
statistical significance for Hickl?s system and not
for Newman?s and Adam?s.
Moreover, it is envisaged that the relatively good
performance of some of the systems (e.g, Hickl
and Tatu) is due to the relative lack of open-ended
metaphors in the pairs used for the analysis. This
also shows that shallow techniques can be fruitful
for processing conventional metaphor. However,
open-ended metaphors may pose more complex
problems. For example, a fairly deep analysis may
presumably be needed to extract the metaphori-
cal connotations conveyed by ?incubate? (about the
source to target transfer of carefully growing and
nurturing) to correctly judged the lack of entail-
ment for the modified hypothesis ?Ivan Getting ac-
cidentally invented the GPS?. This is also true for
metaphors about ?deepest recesses of the mind?
(in RTE-1 dataset), etc. This type of open-ended
metaphors have been subjected to a in-depth anal-
ysis (both formal and computational) within the
ATT-Meta system and approach for metaphor in-
terpretation (Agerri et al, 2007; Barnden et al,
2003). Adapting it for textual entailment may fa-
cilitate the processing of open-ended metaphor in
a textual entailment task.
Metaphor understanding systems have not
aimed to be empirically evaluated on a large-
scale, but have chosen to focus instead on the in-
depth analysis of small number of examples. As
a consequence, there are not common resources
5
such as corpora or shared task evaluation exer-
cises for metaphor resolution. In order to make
use of the RTE evaluation framework to promote
empirically-based research on metaphor under-
standing, the first task would aim to build datasets
that for the first time would allow researchers to
train and (empirically) evaluate their systems. An
obvious strategy would be to follow RTE guide-
lines with the additional requirement that at least T
should contain a metaphorical expression relevant
to judge the entailment.
The RTE evaluation framework has the advan-
tage that it is theory neutral, namely, it does not
depend on any semantic formalism and works on
open domain data. However, the RTE evaluation
framework has the disadvantage of being a ?black-
box? type of evaluation. It makes very difficult to
isolate the semantic task from the task of retriev-
ing the necessary background knowledge (Zaenen
et al, 2005; Bos, 2008). Furthermore, it is not de-
signed to measure performance on specific seman-
tic phenomena, and it is therefore difficult to know
why a system is working correctly or incorrectly.
For example, all but one of the RTE-1 runs stud-
ied incorrectly judged the T1-H1 pair to be true
(about ?gastronomic capital?). It is difficult to be
certain that this was solely due to a lack of ability
to deal with metaphor instead of a problem about
noun modifiers. However, there is not currently a
suitable alternative to RTE semantic evaluation as
trying to isolate the semantic task (e.g., metaphor)
from background knowledge usually results in us-
ing artificial examples. On the bright side, the RTE
framework will allow metaphor research to grapple
more extensively than before with the interactions
between metaphor and other language phenomena.
4 Concluding Remarks
The aim of this paper is two fold: Firstly, it pro-
vides evidence showing that the ability of process-
ing metaphor may improve the performance of tex-
tual inference systems. Secondly, it argues that
RTE may provide a much needed general semantic
framework for common evaluations and computa-
tional testing of theories that aim to explain open-
ended usages of metaphor in everyday text. The
ATT-Meta approach and system to metaphor inter-
pretation may be adapted for this particular task
(Barnden et al, 2003). Including metaphor pro-
cessing in textual entailment systems can also pro-
mote the inclusion of metaphor resolution in NLP
applications such as Question Answering, Docu-
ment Summarization or Information Retrieval.
Acknowledgments
Thanks to the RTE-1 participants that made avail-
able their results. Special thanks to John Barnden
and Alan Wallington for their comments and sug-
gestions.
References
Agerri, R., J.A. Barnden, M.G. Lee, and A.M. Walling-
ton. 2007. Metaphor, inference and domain in-
dependent mappings. In Proceedings of Research
Advances in Natural Language Processing (RANLP
2007), pages 17?24, Borovets, Bulgaria.
Barnden, J., S. Glasbey, M. Lee, and A. Wallington.
2003. Domain-transcending mappings in a system
for metaphorical reasoning. In Companion Proceed-
ings of the 10th Conference on the European Chap-
ter of the Association for Computational Linguistics
(EACL-03), pages 57?61.
Bos, J. and K. Markert. 2006. Recognizing tex-
tual entailment with robust logical inference. In
Qui?nonero-Candela, J., I. Dagan, B. Magnini, and
F. d?Alch?e Buc, editors, MLCW 2005, volume 3944
of LNAI, pages 404?426. Springer-Verlag.
Bos, J. 2008. Lets not argue about semantics. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), Marrakech, Mo-
rocco.
Dagan, I., O. Glickman, and B. Magnini. 2007. The
PASCAL Recognising Textual Entailment challenge.
In Qui?nonero-Candela, J., I. Dagan, B. Magnini, and
F. d?Alch?e Buc, editors, MLCW 2005, volume 3944
of LNAI, pages 177?190. Springer-Verlag.
Falkenhainer, B., K.D. Forbus, and D. Gentner. 1989.
The structure-mapping engine: algorithm and exam-
ples. Artificial Intelligence, 41(1):1?63.
Fass, D. 1997. Processing metaphor and metonymy.
Ablex, Greenwich, Connecticut.
Hobbs, J.R. 1992. Metaphor and abduction. In Ortony,
A., J. Slack, and O. Stock, editors, Communication
from an Artificial Intelligence Perspective, pages 35?
58. Springer-Verlag, Berlin.
Lakoff, G. 2004. Conceptual metaphor home page.
http://cogsci.berkeley.edu/lakoff/MetaphorHome.html.
Martin, J.H. 1990. A computational model of metaphor
interpretation. Academic Press, New York.
Zaenen, A., L. Karttunen, and R. Crouch. 2005. Lo-
cal textual inference: Can it be defined or circum-
scribed? In Proceedings of the ACL 05 Workshop on
Empirical Modelling of Semantic Equivalence and
Entailment, pages 31?36.
6
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 109?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
On the formalization of Invariant Mappings for Metaphor Interpretation
Rodrigo Agerri, John Barnden, Mark Lee and Alan Wallington
School of Computer Science, Univ. of Birmingham
B15 2TT Birmingham, UK
r.agerri@cs.bham.ac.uk
Abstract
In this paper we provide a formalization of
a set of default rules that we claim are re-
quired for the transfer of information such
as causation, event rate and duration in the
interpretation of metaphor. Such rules are
domain-independent and are identified as in-
variant adjuncts to any conceptual metaphor.
We also show a way of embedding the in-
variant mappings in a semantic framework.
1 Introduction
It is generally accepted that much of everyday lan-
guage shows evidence of metaphor. We assume the
general view that metaphor understanding involves
some notion of events, properties, relations, etc. that
are transferred from the source domain into the tar-
get domain. In this view, a metaphorical utterance
conveys information about the target domain. We
are particularly interested in the metaphorical utter-
ances that we call map-transcending. Consider the
following example:
(1) ?McEnroe starved Connors to death.?
We do not address in this paper the issue of
when an utterance is to be considered metaphor-
ical. Instead, we aim to offer an explanation of
how a metaphorical utterance such as (1) can be in-
terpreted. If we infer, using our knowledge about
McEnroe and Connors, that (1) is used to describe
a tennis match, it can be understood as an exam-
ple of the conceptual metaphors (or, in our termi-
nology, ?metaphorical views?) DEFEAT AS DEATH
and NECESSITIES AS FOOD. However, these
metaphorical views would not contain any relation-
ship that maps the specific manner of dying that con-
stitutes being starved to death (we say that ?starv-
ing? is a map-transcending entity). Yet one could
argue that the manner of Connors?s death is a cru-
cial part of the informational contribution of (1).
A possible solution would be to create a new
view-specific mapping that goes from the form of
killing involved in starving to death to some process
in sport, but such enrichment of mappings would be
needed for many other verbs or verbal phrases that
refer to other ways in which death is brought about,
each requiring a specific specific mapping when oc-
curring in a metaphorical utterance. Thus, finding
adequate mappings could become an endless and
computational intensive process. Moreover, there
are even cases in which we may not find a plausi-
ble mapping. Consider the following description of
the progress of a love affair:
(2) ?We?re spinning our wheels.?
It is not very clear what could be a target corre-
spondent for ?wheels?. We have developed an AI
system called ATT-Meta for metaphor interpretation
(Barnden et al, 2002) that employs reasoning within
the terms of the source domain using various sources
of information including world and linguistic knowl-
edge. The reasoning connects unmapped ideas used
by utterances, such as wheels and starving, to other
source-domain ideas for which a mapping is already
known. These known mappings may be constituents
of particular metaphorical view, but previous work
(Barnden et al, 2003; Wallington et al, 2006) has
109
shown evidence that there are metaphorical aspects
(such as causal relations between events) that, sub-
ject to being called, invariantly map from source to
target (we call these mappings View-Neutral Map-
ping Adjuncts or VNMAs) irrespective of whatever
specific metaphorical views are in play. These allow
many mapping effects, which would otherwise have
to be duplicated across all view-specific mappings,
to be factored out into separate mappings. In our
approach, source domain reasoning takes place in a
special, protected computational context that we call
the ?pretence space?. We use the term ?reality? to
refer to the space outside the pretence where propo-
sitions are about reality as the understander sees it.
Currently ATT-Meta implements the VNMAs by
including them in view-specific rules, but we plan to
make the system more modular and its view-specific
mappings more economical by implementing VN-
MAs as separate default rules. The first step to-
wards that goal is to provide a formalization of these
mappings and to show their role in metaphor in-
terpretation. In order to do so, we provide a se-
mantic representation of how these VNMAs work
by adopting Segmented Discourse Representation
Theory (Asher and Lascarides, 2003) to capture the
main aspects of the ATT-Meta approach.
2 Knowledge and Inference
If (1) is being used metaphorically to describe the
result of a tennis match, a plausible target interpre-
tation would be that McEnroe defeated Connors in a
slow manner by performing some actions to deprive
him of his usual playing style. Assuming a com-
monsensical view of the world, a within-pretence
meaning would be that McEnroe starved Connors to
death in the real, biological sense. The inferencing
within the pretence can then conclude that McEnroe
caused Connors?s death by depriving or disabling
him. Leaving some details aside, the partial logical
form (in the pretence) of the metaphorical utterance
(1) may be represented as follows (without taking
into account temporal issues):
(i) ?x, y, e(McEnroe(x) ? Connors(y)
?starve? to? death(e, x, y))
This says that there is an event e of x starving y to
death (we also use the notion of event to describe sit-
uations, processes, states, etc.). It may be suggested
that if we were trying to map the partial expression
(i), its correspondent proposition in the target could
be expressed by this formula:
(ii) ?x, y, e(McEnroe(x) ? Connors(y)?
defeat(e, x, y))
According to this, the event of x defeating y in
the reality would correspond to the event of x starv-
ing y to death in the pretence. However, by say-
ing ?McEnroe starved Connors to death? instead of
simply ?McEnroe killed Connors? the speaker is not
merely intending to convey that McEnroe defeated
Connors, but rather something related to the man-
ner in which Connors was defeated. Following this,
starving may be decomposed into the cause e1 and
its effect, namely, ?being deprived of food?:
(iii) ?x, y, z, e1, e2, e3(McEnroe(x)?
Connors(y) ? food(z) ? starve(e1, x, y) ?
death(e2, y) ? deprived(e3, y, z)?
cause(e1, e3))
Now, by means of lexical information regarding
?starving?, it can be inferred that McEnroe deprived
Connors of a necessity (see, e.g., Wordnet), namely,
of the food required for his normal functioning (the
NECESSITIES AS FOOD metaphorical view would
provide mappings to transfer food to the type of
shots that Connors needs to play his normal game).
In other words, Connors is defeated by the partic-
ular means of depriving him of a necessity (food)
which means that being deprived causes Connors?s
defeat. This fits well with the interpretation of (1)
where McEnroe?s playing deprived Connors of his
usual game. Moreover, linguistic knowledge also
provides the fact that starving someone to death is a
gradual, slow process. The result of within-pretence
inferencing may be represented as follows:
(iv) ?x, y, z, e1, e2, e3(McEnroe(x)?
Connors(y) ? food(z) ? starve(e1, x, y) ?
death(e2, y) ? deprived(e3, y, z)?
cause(e1, e3)?cause(e3, e2)?rate(e1, slow))
?Slow? refers to a commonsensical concept in the
pretence related to the progress rate of starving.
Now, the existing mapping DEFEAT AS DEATH
can be applied to derive, outside the pretence, that
McEnroe defeated Connors, but no correspondences
110
are available to account for the fact that McEnroe
caused the defeat of Connors by depriving him of
his normal play. We appear to have a problem also
to map the slow progress rate of a process like starv-
ing.
3 VNMAs in a Semantic Framework
In the ATT-Meta approach to metaphor interpreta-
tion, the mappings of caused and rate discussed
above are accomplished by a type of default map-
pings that we specify as VNMAs (the Causation
and Rate VNMAs, respectively; see (Wallington and
Barnden, 2006) for an informal but detailed de-
scription of a number of VNMAs). The idea is
that there are relationships and properties (causation,
rate, etc.) between two events or entities that iden-
tically transfer from the pretence to the reality. We
use the 7? symbol to express that this mapping is a
default. The VNMAs involved in the interpretation
of (1) can be represented as follows:
Causation: ?e1, e2(cause(e1, e2)pret 7?
cause(e1, e2)rlt)
The Rate VNMA transfers the qualitative rate of
progress of events in the source domain to the qual-
itative rate of progress of its mappee:
Rate: ?e, r(rate(e, r)pret 7? rate(e, r)rlt)
Embedding the VNMAs in a semantic framework
for metaphor interpretation is useful as a first step
towards their implementation as default rules in the
ATT-Meta system, but it is also interesting in its
own right to show the contribution that the ATT-
Meta approach can make towards the semantics of
metaphor. In the somewhat simplified discussion
on the within-pretence reasoning and mappings nec-
essary to interpret metaphorical utterances such as
(1), we have been using various sources of informa-
tion that interact in the processing of the utterance:
a) View-specific mappings provided by the relevant
metaphorical views (DEFEAT AS DEATH and NE-
CESSITIES AS FOOD); b) Linguistic and contex-
tual information necessary for reasoning in the pre-
tence; c) Relations and properties between events
such as causation and rate that are inferred in the
pretence; d) VNMAs that transfer within-pretence
event relations and properties to reality.
There are two prominent computationally-
oriented semantic approaches (Hobbs, 1996) and
(Asher and Lascarides, 2003) that take into account
contextual and linguistic information and stress the
importance of relations between text segments in
discourse interpretation. In fact, the incorporation
of the above types of information ties in well with
the SDRT (Asher and Lascarides, 2003) view of
language understanding. For example, we can think
of the pretence space as a Segmented Discourse
Representation Structure (SDRS) representing the
result of within-pretence inference which can be
mapped by using various view-specific and invariant
mappings to reality. In other words, we can see the
pretence SDRS as the input for what the ATT-Meta
system does when interpreting metaphor ? it will
reason with it, producing an output of inferred
reality facts which we may also represent by means
of an SDRS. The result of reasoning in the pretence
to interpret (1) would now looks as follows:
PRET:
?, ?, ?
?:
x, y, e1
McEnroe(x)
Connors(y)
starve(e1, x, y)
?:
e2
death(e2, y)
?:
e3 ,z
food(z)
deprived(e3, y, z)
cause(e1, e3)
cause(e3 ,e2)
rate(e1 ,slow)
7??
where ? and ? are labels for DRSs representing
events, PRET for a pretence space and 7?? map-
pings (VNMAs and central mappings) needed in the
interpretation of the metaphorical utterance. Impor-
tantly, the VNMAs would pick upon aspects such
as causation and rate from pretence to transfer them
to reality producing an output which could also be
represented as a SDRS:
RLT:
?, ?, ?
?:
x, y, e1
McEnroe(x)
Connors(y)
tennis-play(e1, x, y)
?:
e2
defeat(e2, y)
?:
e3 ,z
necessity(z)
deprived(e3, y, z)
cause(e1, e3)
cause(e3,e2)
rate(e1 ,slow)
Note that this formal representation integrates the
systematicity of mapping invariantly certain aspects
of metaphorical utterances by formulating them as
relations between events that can be represented as
111
relations and properties of DRSs. For this purpose
we need to modify the construction rules of SDRSs
to be able to infer properties and relations involving
individuals and not only DRSs? labels. In addition
to this, we have shown in the previous section how
ATT-Meta source domain reasoning captures the in-
teraction of the various sources of knowledge used
to infer causation and rate in the pretence. Further-
more, studying the interaction between VNMAs and
discourse relations may allow us to extend the study
of metaphor to discourse.
4 Concluding Remarks
Following the ATT-Meta claim metaphors often con-
vey crucial information via VNMAs, we can re-
analyze example (1) so that the effects of the NE-
CESSITIES AS FOOD mapping are obtained by
VNMAs. In the pretence, the food is something
Connors needs for proper functioning: i.e., it is nec-
essary that Connors have the food in order to func-
tion properly. The necessity here is covered by the
Modality VNMA, which maps relative degrees of
necessity, possibility, obligation, etc., from pretence
to reality. Moreover, the functioning properly would
be covered by the Function and Value-Judgement
(levels of goodness, importance, etc. map identi-
cally to levels of goodness, etc.). So all that is left is
the possession which could be covered by a STATE
AS POSSESSION mapping.
Formal semantic approaches (Asher and Las-
carides, 2003) do not account for metaphorical ut-
terances including map-transcending entities. Other
works (Carbonell, 1982; Hobbs, 1990; Martin,
1990; Narayanan, 1997) have addressed source do-
main reasoning to a limited extent, but its role in
metaphor interpretation has not previously been ad-
equately investigated. Moreover, map-transcending
entities pose a problem for analogy-based ap-
proaches to metaphor interpretation (Falkenhainer
et al, 1989), which require the discovery of an
elaborate structural similarity between the source
and target domains and/or the imposition of un-
mapped source domain structures on the target do-
main, whereas part of our approach is that the un-
mapped source domain structure introduced by the
utterance is by default not carried over.
Acknowledgements Supported by EPSRC
EP/C538943/1 and GR/M64208 grants.
References
Nicholas Asher and Alex Lascarides. 2001. The seman-
tics and pragmatics of metaphor. In P. Bouillon and F.
Busa, editors, The Language of Word Meaning, pages
262?289. Cambridge University Press.
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press.
John Barnden, Sheila Glasbey, Mark Lee, and Alan
Wallington. 2002. Reasoning in metaphor under-
standing: The att-meta approach and system. In 19th
Conference on Computational Linguistics (COLING-
2002).
John Barnden, Sheila Glasbey, Mark Lee, and
Alan Wallington. 2003. Domain-transcending
mappings in a system for metaphorical reasoning.
In Conference Companion to the 10th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL 2003), pages
57?61.
Jaime Carbonell. 1982. Metaphor: An inescapable
phenomenon in natural-language comprehension. In
W. Lehnert and M. Ringle, editors, Strategies for Nat-
ural Language Processing, pages 415?434. Lawrence
Erlbaum, Hillsdale, NJ.
BrianFalkenhainer, Kenneth Forbus, and Dedre Gentner.
1989. The structure-mapping engine: algorithm and
examples. Artificial Intelligence, 41(1):1?63.
Jerry Hobbs. 1990. Literature and Cognition. CSLI,
Lecture Notes, Stanford.
Jerry Hobbs. 1996. An approach to the structure of dis-
course. In D. Everett, editor, Discourse: Linguistic,
Computational and Philosophical Perspectives.
James Martin. 1990. A computational model of
metaphor interpretation. Academic Press, New York.
Srini Narayanan. 1997. KARMA: Knowledge-based ac-
tion representations for metaphor and aspect. Ph.D.
thesis, Computer Science Division, EECS Depart-
ment, University of California, Berkeley, August.
Alan Wallington and John Barnden. 2006. Similarity as
a basis for metaphor: Invariant transfer and the role
of VNMAs. Technical Report CSRP-06-02, School of
Computer Science, Univ. of Birmingham, December.
Alan Wallington, John Barnden, Sheila Glasbey, and
Mark Lee. 2006. Metaphorical reasoning with an eco-
nomical set of mappings. Delta, 22(1).
112
Considerations on the nature of metaphorical
meaning arising from a computational
treatment of metaphor interpetation
A.M.Wallington, R.Agerri, J.A.Barnden, S.R.Glasbey and M.G.Lee
School of Computer Science,
University of Birmingham, UK)
A. M. Wallington@ cs. bham. ac. uk
Abstract
This paper argues that there need not be a full correspondence between source and
target domains when interpreting metaphors. Instead, inference is performed in
the source domain, and conclusions transferred to the target. A description of a
computer system, ATT-Meta, that partially implements these ideas is provided.
1 Introduction
It is now generally accepted, especially since the work of Lakoff and associates
(e.g. [14,13,15]) that much of everyday discourse shows evidence of metaphor.
Consequently, the question of how metaphor should be interpreted and what
the semantic consequences are of using a metaphor is of major importance in
determining how discourse should be interpreted.
Like Stern [19], we take the position that much of the interpretation of
metaphor is highly context dependent and involves pragmatics. However, we
believe that, for metaphor, pragmatics must be informed by theories of Artifi-
cial Intelligence and psychology. Thus we have some sympathy for Levinson?s
([16] p.161) claim that:
?the interpretation of metaphor must rely on features of our general ability
to reason analogically. ... It could be claimed that linguistic pragmatics alone
should not be expected to provide such a general theory of analogy, without
considerable help from psychological theory 1 .?
We depart from Levinson with respect to analogy, and in this paper, we
shall challenge current theories of analogy (e.g. [7,8,11] and what might
1 Levinson makes it clear that psychological theory includes Artificial Intelligence.
be termed correspondence theories of metaphor (e.g, Lakoff?s Conceptual
Metaphor Theory) in which a source domain is put in correspondence with
a target domain. We shall attempt to show that there is far less parallelism
between source and target than is often assumed and that the process of inter-
preting a metaphor often requires heavy use of inferencing in order to associate
source domain entities for which there is no parallel target equivalent, what
we term ?Map-Transcending Entities? (MTEs), with information that is in-
volved in source to target transfer. Now other AI approaches to metaphor also
emphasise the role of source domain inference, Hobbs [10] and Narayanan [18]
for example. And, Martin?s MIDAS system includes a process of extending
conventional source to target mappings [17]. However, apart from important
technical differences between these systems and our own, we would wish to
stress the implications extensive inferencing has for source-target parallelism
and the repercussions this has for the semantics and pragmatics.
We do not yet have a fully developed semantics and pragmatics of metaphor
(although see [9] for some preliminary suggestions based on Stern). However,
what we do provide is an extensively developed (though informal) inference-
based model of metaphor understanding that employs an event-based formal-
ism similar to that of Hobbs [10], combined with a realization of this approach
in a fully implemented system (ATT-Meta) that effects the type of reasoning
that we claim is at the heart of much metaphor understanding (See [1,4,21]).
2 Correspondence approaches to interpreting metaphor
The work of Lakoff and Johnson e.g. [14,15] not only stressed the ubiquity of
metaphor in everyday discourse, but also noted that many metaphorical utter-
ances could be systematically related to each other, all appealing to different
aspects of the same source domain and being used to describe the same target
domain. In other words, what is involved in metaphor is the mapping of one
cognitive domain into another. For example, Lakoff [13] notes that the follow-
ing metaphors all involve a source domain of journeys being used to describe
a target domain of the progress of a love affair: Look how far we?ve come.
It?s been a long, bumpy road. We can?t turn back now. We?re at a crossroads.
The relationship isn?t going anywhere. We may have to go our separate ways.
We?re spinning our wheels. Our relationship is off the track. The marriage is
on the rocks. We may have to bail out of this relationship. To account for this
generalization, Lakoff assumes a ?LOVE-AS-JOURNEY? mapping, i.e. ?a set
of ontological correspondences that characterize epistemic correspondences by
mapping knowledge about journeys onto knowledge about love? ([13] p207).
Specifically, he assumes the following ontological correspondences:
THE LOVE-AS-JOURNEY MAPPING
i. The lovers correspond to travellers.
ii. The love relationship corresponds to the vehicle.
iii. The lovers? common goals correspond to their common destination.
iv. Difficulties in the relationship correspond to impediments to travel.
Lakoff does not spell out in any detail exactly how the epistemic corre-
spondences function, and how inferences made in the source transfer to the
target. He does however, claim that mappings ?project source domain infer-
ence patterns onto target domain inference patterns? ([13] p245, emphasis
added). However, we might turn to more formal work in analogy for a pos-
sible mechanism. Gentner (e.g. [7]) argues that complex systematic relations
between source domain entities such as causal relations also transfer, whilst
non-systematic relations such as attributes do not.
3 Map Transcending Entities
Let us return to Lakoff?s list above of travel metaphors. Consider the state-
ment that we?re spinning our wheels. How might we infer from this that the
love relationship is not progressing as it might? Plausibly, the following chain
of inferences might be entertained. If wheels are referred to, then, defeasibly 2 ,
a vehicle is involved. The spinning wheels are causing the vehicle not to move
as it should. If the vehicle is not moving as it should then it is not moving
towards its destination.
What can we make of this pattern of inferences? Both the vehicle and
the destination have correspondents in the target domain of the love affair,
namely the love relationship and the lovers? common goals. With these cor-
respondences, we might assume that the source domain conclusion can be
transferred to become a target domain conclusion. But, this is the transfer of
it conclusion. It could never have been reached without the premise that some-
thing -the spinning wheels- was causing the vehicle not to proceed. So what
is the target correspondent of the spinning wheels whose presence is needed
in order to allow the entire inference pattern as opposed to just the conclu-
sion to transfer from source to target? Nothing in the list of four ontological
correspondences would seem to be appropriate.
If we look at the other examples listed, we find similar cases where the
lack of a target domain correspondent would prevent crucial aspects of the
inference pattern mapping from source to target. For example, what is the
target domain correspondent of the rocks in the utterance our marriage is on
the rocks? A similar point can be made about the location off the track in
our relationship is off the track. It is not that these statements are uninter-
pretable. Both would seem to permit the same conclusion that was reached
about the spinning wheels, namely that the relationship/vehicle is not pro-
gressing towards the destination and hence not towards the lovers? goal.
Note that it does not seem quite right to assume that the spinning wheels,
rocks or lack of tracks are ?impediments? in the sense of the third of Lakoff?s
2 We shall henceforth assume that inferences are defeasible unless otherwise stated.
correspondences. There may be an interpretation under which the bumps in
the statement its been a long bumpy road refer to specific, listable, difficulties
in the love relationship, and similarly, the rocks may refer, for some, to a par-
ticular event, but both could be used more generally. Whatever, if anything,
wheels might refer to it is a vague and very unspecific target domain entity.
Let us now consider the utterance: We?re at a crossroads. Stating that we
are at some location on a road might be taken to implicate that we are going
somewhere along the road and hence have a destination. This would map to
a target domain ?common goal? However, there is no specific correspondent
given for crossroads. The source domain inference that if one is at a crossroads,
then there is a choice of possible destinations cannot transfer to the target
domain inference that there is a choice of possible relationship goals, if, as is
stated, inference patterns are mapped from one domain to another.
Now, a lack of target correspondents for source domain entities is not pe-
culiar to the LOVE AS A JOURNEY conceptual metaphor, but is generally
the case. In section 3, we shall give a description of our approach to metaphor-
ical reasoning and our computational system ATT-Meta that performs such
reasoning. A sentence that we shall analyse in some detail is the following:
1 In the far reaches of her mind, Mary believed Kyle was having an affair.
We assume that there is a mapping between ideas such as the idea that
Kyle was having an affair and physical, manipulable, objects. This reification
of ideas has a wide currency. We shall also assume here that the mind is often
viewed as a physical space. However, what does the far reaches refer to?
So what are we to make of these lacunae which prevent the step by step
transfer of inferences about the source becoming inferences about the tar-
get? Note that the absence of target domain correspondents of source domain
entities is not a mere technical problem in determining how source domain
implications transfer and become target domain implications, nor should we
assume that Lakoff?s claim that entire inference patterns as opposed to
certain conclusions transfer is just an imprecise form of words. If there are
no target domain correspondents of source domain entities, then we must as-
sume that there are entities in texts have no reference even indirectly via a
target domain equivalent to anything in the real world. There are entities
that can only be made sense of in terms of their literal meaning in the source
domain; a somewhat similar conclusion to Davidson?s [5] well known claim
that metaphors have only a literal meaning.
One possibility is that the four correspondences given in the LOVE AS A
JOURNEY mapping in section 2 are not intended to be exhaustive and the
mapping would if properly specified include correspondences for the entities we
have discussed. For example, the crossroads example might motivate adding
the following correspondence to the mapping.
v. A choice of goals corresponds to a choice of destinations.
However, a number of objections can be made to this view. Many concep-
tual metaphors are open-ended allowing almost any entity associated with the
source domain to be used when speaking of the target. Now, not all of these
might be conventional. They might make use of what Lakoff and Johnson
([14] p 53) call ?the unused part? of the source domain. For example, Lakoff
[13] gives an example of a creative use of the conceptual metaphor LOVE IS
A JOURNEY, the song lyric we?re riding in the fast lane on the freeway of
love. There is no correspondent listed for fast lane nor for a freeway of love.
It would seem that no finite list of correspondents would ever be enough. This
criticism would also defeat lexically based treatments of metaphor.
The last example contained very little that could plausibly correspond to
target-domain entities; perhaps just the inference that a journey and thus
a destination is involved. However, it might be conceded that whilst many
modifier phrases often do not correspond, overall statement-like constituents
of the source will correspond to statements in the target. However, consider
the following example from real discourse of a particular metaphorical view
running through several consecutive statement-like utterances:
?She was too confused to judge anything. If he?d done nothing else, he?d
at least invaded her thoughts and tossed them around until there was only
chaos in her head.?
We shall assume that the clauses he?d at least invaded her thoughts, [he
had] tossed them around and there was only chaos in her head all rely on
the metaphorical views of IDEAS AS PHYSICAL OBJECTS and MIND AS
PHYSICAL SPACE, and taken together partially convey, in target-domain
terms, that the man, ?he?, had done something that had resulted in the
thoughts of the woman, ?her?, not functioning as they ought. But is
there any need to assume a specific target-domain parallel for tossing physical
objects around? A similar point can be made about the statement he?d ...
invaded her thoughts : the invading is just mentioned as a way of emphasizing
that he had done something that had had a strong effect on her thoughts.
Now, it is possible that the mention of invading may have been used if
the man had introduced thoughts into the woman?s mind by saying things
to her; the invading corresponding to introducing. It is then these thoughts
that caused the tossing of the woman?s pre-existing thoughts. However, it is
possible to imagine the above discourse segment occurring in a context where
it is clear that the man had not communicated anything to her.
In short, even a sentence, the traditional unit of analysis of truth, might
have no meaning other than in terms of the source domain. In the rest of this
paper we shall describe our approach and implemented computational model,
ATT-Meta, for reasoning with metaphorical utterances that contain MTEs.
4 Inferencing and ATT-Meta
Recall that our proposed fifth correspondence in the previous section had a
?choice? in the target corresponding to a ?choice? in the source. If we look at the
fourth correspondence, we find something similar. A target domain ?difficulty?
corresponds to a type of difficulty or ?impediment? in the source. Note also
that Lakoff argues a-propos the ?fast lane? song lyric that driving in the fast
lane is exciting and that the excitement transfers to the target. And we could
give examples in which other emotional states transfer from the source to the
target. In the ?invasion? passage, the conclusion that the woman?s mind was
not functioning properly was likewise transferred. What all these cases have in
common is that they involve essentially ?invariant? transfers from the source
to the target. These are of a very different nature from the cross-domain
correspondences between say journeys and love affairs. We argue that such
transfers are affected by what we term ?View-Neutral Mapping Adjuncts?
and argue that they apply universally, at least by default, regardless of what
cross-domain mappings are in play and our system has made a start both
at investigating what VNMAs are required and at formalising some of them.
(See [2,3,21] for more details.)
In order to sketch our approach let us return to the Anne/Kyle example:
1 In the far reaches of her mind, Anne believed Kyle was having an affair[.]
and compare it to the following variant:
2 Anne had in her mind the belief that Kyle was having an affair.
We assume that both sentences utilize the conceptual metaphors (a term
which we replace with the more neutral ?metaphorical view? in our work):
(A) IDEAS AS PHYSICAL OBJECTS (B) MIND AS PHYSICAL SPACE.
We assume that one correspondence included under view (A) is the fol-
lowing: ?Conscious mental processing corresponds to physical manipulation.?
(See [12] for motivation). We also assume that with activities such as pro-
cessing/manipulating, which one can have the ability to perform to a greater
or lesser degree, DEGREE is a VNMA and maps over from source to target
in an invariant manner. Thus a very low ability to mentally process an idea
corresponds to a very low ability to physically manipulate an object.
We assume that accompanying metaphorical view (B) are two ancillary as-
sumptions. Firstly, that the conscious self of the mind?s possessor is metaphor-
ically cast as a person physically located in (a central part of) the mind-region.
Secondly, that when a cognitive state (such as believing) is cast as located in
a physical sub-region of a mind, then the idea or whatever that is the object
of the state is also to be thought of as physically located in that sub-region.
As we suggested in the previous section, we assume that there is no known
correspondent for the far reaches; it is a map-transcending entity.
So how does the informational contribution of (1) and (2) differ? Plausibly,
what (2) principally conveys to the reader is that Anne has the ability to
operate in a conscious mental way on the idea that Kyle was having an affair.
In brief: Anne is aware of the affair. By contrast, what (1) seems to convey
is that the ability to operate holds only to a very low degree. In brief: Anne
had very little conscious awareness of the affair.
Thus, the situation described by the ?far reaches? utterance is cast as being
one where Anne?s conscious self is a person in a central part of Anne?s mind-
region, and the idea that Kyle was having an affair is in the far reaches of
the mind-region. Now, let us assume that the understander?s common sense
knowledge of physical space and physical objects includes the following:
* things in the far reaches of a region are usually distant from things in a
central part (distance being relative to the scale of the whole region).
* if a person is physically distant from a physical object then the person usually
has only a very low degree of ability to manipulate that object physically.
Thus, the understander can reason, within the terms of the source domains
of the metaphorical views (PHYSICAL SPACE and PHYSICAL OBJECTS),
that, probably, Anne?s conscious self has only 3 a very low degree of ability to
physically manipulate the idea that Kyle was having an affair.
This conclusion can become the target-domain conclusion that Anne has
only a very low degree of ability to operate in a conscious mental way on the
idea that Kyle was having an affair, by virtue of the correspondence between
physical manipulation and conscious mental processing that was assumed as
an aspect of the IDEAS AS PHYSICAL OBJECTS mapping, and by virtue
of the VNMA invariantly mapping the very low degree from source to target.
In our approach source-target correspondences are implicit in transfer
rules. In the case of the correspondences just mentioned, English glosses of
the relevant rules include:
* IF in reality X is a person and K is an idea
* AND K is being viewed as a physical object
* AND person X?s conscious self is being viewed as being able to
operate physically on K to at least degree D
* THEN presumably in reality X can mentally operate consciously
on K to degree at least D.
This rule allows one aspect of the source-domain conclusion to lead to the
target-domain conclusion that Anne can mentally operate consciously on the
Kyle-affair idea to degree at least ?very low?.
In sum, our approach involves the following main types of processing:
* Construction of a representation of the direct, source-domain meaning of
the utterance, i.e. the meaning it has by taking only the source-domain senses
of the metaphorically-used words/phrases in the utterance. This meaning
consists of one or more propositions.
* In some cases, application of ancillary assumptions associated with the rele-
vant metaphorical views to create further propositions in source-domain terms.
* Usually, performance of source-domain reasoning on the basis of the direct
3 A very low degree of ability might implicate that Anne does not have a higher degree,
but does not entail it. Hence our addition of ?only?.
source-domain meaning, the products of ancillary assumptions, and general
knowledge relevant to the source domain meaning.
* Source-to-target transfer acts by application of transfer rules (and VNMAs).
This listing does not imply any particular temporal ordering of the types
of processing. Indeed in ATT-Meta the reasoning actually works backwards
from reasoning queries posed internally within the system and can involve any
intertwining and ordering of instances of the above types of reasoning.
An important feature of our approach that we have not yet mentioned is
that it encapsulates the source-domain reasoning based on the literal meaning
of the utterance within a special computational context we call a pretence
cocoon. Metaphorical transfer acts based on rules such as those above oper-
ate between the inside of the pretence cocoon and the reality-context outside.
Thus, for the Anne/Kyle example, the understander pretends, within the co-
coon, that Anne?s mind really is a physical space and that the believing really
does occur in the far reaches of this space. Consequences of this are inferred
in the pretence cocoon, possibly by substantial amounts of reasoning, using
ancillary assumptions and knowledge about physical objects and space. The
conclusions reached may then be able to be transmuted, via transfer rules
forming part of the relevant metaphorical views, into propositions in the re-
ality environment. However, we ought to stress that many different lines of
reasoning will be explored, many ultimately proving unsuccessful.
We should also stress that when a pretence cocoon is created, it is not
tagged as having to do with any particular metaphorical view. Only by hav-
ing the utterance?s direct source-domain meaning placed within it, such as the
mind having far-reaches, can an inference be made that that the particular
metaphorical view MIND AS PHYSICAL SPACE with its associated corre-
spondences is being used. Thus, even the question of the metaphorical views
involved in an utterance results from a possibly extensive web of inferences.
Finally note that although Anne?s mind is categorized in the pretence
as a physical region, this is in addition to its being categorized there as a
mind. (Thus, a pretence cocoon is reminiscent of a blend space in Blending
Theory: [6].) Given the existence of suitable knowledge rules, such as that
a mind is not a physical region, we can get conflicting propositions arising
within the pretence, because in general it is wrong to prevent rules about the
target domain operating within the pretence. In the present case we would
get both strong support for the mind being a physical region and for its not
being a physical region. The ATT-Meta system implements conflict-resolution
mechanisms that deal with reasoning conflicts in general, and that embody
a small number of general principles about conflict resolution in metaphor
[1,20]. In the present case, the mechanisms ensure that the proposition that
Anne?s mind is a physical region wins over the proposition that it is not.
5 Conclusion
We have provided a brief outline of some of the ideas in our implemented,
inference-based approach to metaphor. Much more detail, including the ap-
plication to other examples, can be found elsewhere [1,2,4].
The main point has been the use of inference to connect source-domain as-
pects that are raised by an utterance but not handled by known metaphorical
mappings to source-domain aspects in mappings that the understander does
know, and particularly to knowledge of what invariant aspects of metaphorical
utterances are likely to transfer. By this means, the approach can deal with
open-ended extensions of metaphorical beyond what can be readily dealt with
by known mappings by themselves, without the need for creating mappings
for the unmapped source-domain aspects.
We thus radically downplay source/target parallelism in metaphor in favour
of inference, and place great weight on the thesis that metaphors often intro-
duce source-domain aspects that do not need any correspondents in the target
domain (let alne already have any): their only purpose is to support useful
lines of source-domain inference that connect to known mappings. One of
the interesting semantic issues raised is that these unmapped aspects do not
by themselves have any meaning in target-domain terms, and it would be a
mistake to try to specify such meaning.
6 acknowledgements
This work has been supported by current and past grants: EP/C538943/1 and
GR/M64208, from the Engineering and Physical Sciences Research Council.
References
[1] Barnden, J.A. (2001) Uncertainty and conflict handling in the ATT-Meta
context-based system for metaphorical reasoning. In, V. Akman, P. Bouquet,
R. Thomason and R.A. Young (Eds), Procs. Third International Conference on
Modeling and Using Context. Lecture Notes in Artificial Intelligence, Vol. 2116.
Berlin: Springer, 15-29.
[2] Barnden, J.A. and Lee, M.G., (2001). Understanding open-ended usages of
familiar conceptual metaphors: An approach and artificial intelligence system.
Technical Report CSRP-01-05, School of Computer Science, University of
Birmingham.
[3] Barnden, J.A., Glasbey, S.R., Lee M.G. and Wallington, A.M. (2003). Domain-
transcending mappings in a system for metaphorical reasoning. In Proceedings
of the Research Note Sessions of the 10th Conference of EACL.
[4] Barnden, J.A., Glasbey, S.R., Lee, M.G. and Wallington, A.M. (2004), Varieties
and directions of inter-domain influence in metaphor. Metaphor and Symbol
19(1), 1?30.
[5] Davidson, D. (1979). What metaphors mean. In, S. Sacks (Ed.), On Metaphor.
U. Chicago Press, 29-45.
[6] Fauconnier, G and Turner, M. (2002). The Way We Think: Conceptual Blending
and the Minds Hidden Complexities. NY: Basic Books.
[7] Gentner, G. (1983). Structure-mapping: A theoretical framework for analogy.
Cognitive Science, 7(2), 155?170.
[8] Gentner, D., Falkenhainer, B. and Skorstad, J. (1988). Viewing metaphor as
analogy. In D.H. Helman (Ed.), Analogical reasoning. Dordrecht. Kluwer.
[9] Glasbey, S.R and Barnden, J.A. (submitted). Towards a situation-based
discourse semantics for metaphor. Submitted to the journal Research on
Language and Computation.
[10] Hobbs, J.R. (1990) Literature and Cognition CSLI Lecture Notes, Center for
the Study of Language and Information, Stanford University.
[11] Holyoak, K J. and Thagard, P. (1989). Analogical mapping by constraint
satisfaction. Cognitive Science, 13(3), 295-355.
[12] Jaekel, O. (1995). The Metaphorical Concept of Mind, in J.R. Taylor and R.E.
MacLaury (eds), Language and the Cognitive Construal of the World. Berlin New
York, Mouton de Gruyter. 197?229.
[13] Lakoff, G. (1993). The contemporary theory of metaphor. In A Ortony (Ed.),
Metaphor and Thought, 2nd ed. Cambridge, UK: Cambridge University Press.
[14] Lakoff, G. and Johnson, M. (1980). Metaphors We Live By. University of
Chicago Press.
[15] Lakoff, G. and Johnson, M. (1999). Philosophy in the Flesh. NY: Basic Books.
[16] Levinson, S. (1983). Pragmatics. Cambridge: Cambridge University Press.
[17] Martin, J. H. (1990). A Computational Model of Metaphor Interpretation. NY:
Academic Press.
[18] Narayanan, S. (1999). ?Moving right along: A computational model of
metaphoric reasoning about events,? Procs. National Conference on Artificial
Intelligence, pp.121?128. AAAI Press.
[19] Stern, J. (2000). Metaphor in Context. Cambridge, MA and London, UK:
Bradford Books, MIT Press.
[20] Wallington, A.M and Barnden, J.A. (2004). Uncertainty in Metaphorical
Reasoning. In Procs of the Workshop on Computational Models of Natural
Argument (CMNA) at ECAI 2004. August 2004, Valencia, Spain.
[21] Wallington, A.M., Barnden, J.A. Glasbey S.R. and Lee M. G. (2006).
Metaphorical reasoning with an economical set of mappings. Delta, 22:1.
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 88?97,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Simple, Robust and (almost) Unsupervised Generation of Polarity
Lexicons for Multiple Languages
I
?
naki San Vicente, Rodrigo Agerri, German Rigau
IXA NLP Group
University of the Basque Country (UPV/EHU)
Donostia-San Sebasti?an
{inaki.sanvicente,rodrigo.agerri,german.rigau}@ehu.es
Abstract
This paper presents a simple, robust and
(almost) unsupervised dictionary-based
method, qwn-ppv (Q-WordNet as Person-
alized PageRanking Vector) to automati-
cally generate polarity lexicons. We show
that qwn-ppv outperforms other automat-
ically generated lexicons for the four ex-
trinsic evaluations presented here. It also
shows very competitive and robust results
with respect to manually annotated ones.
Results suggest that no single lexicon is
best for every task and dataset and that
the intrinsic evaluation of polarity lexicons
is not a good performance indicator on
a Sentiment Analysis task. The qwn-ppv
method allows to easily create quality po-
larity lexicons whenever no domain-based
annotated corpora are available for a given
language.
1 Introduction
Opinion Mining and Sentiment Analysis are im-
portant for determining opinions about commer-
cial products, on companies reputation manage-
ment, brand monitoring, or to track attitudes by
mining social media, etc. Given the explosion of
information produced and shared via the Internet,
it is not possible to keep up with the constant flow
of new information by manual methods.
Sentiment Analysis often relies on the availabil-
ity of words and phrases annotated according to
the positive or negative connotations they convey.
?Beautiful?, ?wonderful?, and ?amazing? are exam-
ples of positive words whereas ?bad?, ?awful?, and
?poor? are examples of negatives.
The creation of lists of sentiment words has
generally been performed by means of manual-,
dictionary- and corpus-based methods. Manually
collecting such lists of polarity annotated words is
labor intensive and time consuming, and is thus
usually combined with automated approaches as
the final check to correct mistakes. However,
there are well known lexicons which have been
fully (Stone et al., 1966; Taboada et al., 2010) or
at least partially manually created (Hu and Liu,
2004; Riloff and Wiebe, 2003).
Dictionary-based methods rely on some dictio-
nary or lexical knowledge base (LKB) such as
WordNet (Fellbaum and Miller, 1998) that con-
tain synonyms and antonyms for each word. A
simple technique in this approach is to start with
some sentiment words as seeds which are then
used to perform some iterative propagation on the
LKB (Hu and Liu, 2004; Strapparava and Vali-
tutti, 2004; Kim and Hovy, 2004; Takamura et al.,
2005; Turney and Littman, 2003; Mohammad et
al., 2009; Agerri and Garc??a-Serrano, 2010; Bac-
cianella et al., 2010).
Corpus-based methods have usually been ap-
plied to obtain domain-specific polarity lexicons:
they have been created by either starting from a
seed list of known words and trying to find other
related words in a corpus or by attempting to di-
rectly adapt a given lexicon to a new one using
a domain-specific corpus (Hatzivassiloglou and
McKeown, 1997; Turney and Littman, 2003; Ding
et al., 2008; Choi and Cardie, 2009; Mihalcea et
al., 2007). One particular issue arising from cor-
pus methods is that for a given domain the same
word can be positive in one context but negative
in another. This is also a problem shared by man-
ual and dictionary-based methods, and that is why
qwn-ppv also produces synset-based lexicons for
approaches on Sentiment Analysis at sense level.
This paper presents a simple, robust and
(almost) unsupervised dictionary-based method,
QWordNet-PPV (QWordNet by Personalized
PageRank Vector) to automatically generate
polarity lexicons based on propagating some
automatically created seeds using a Personalized
88
PageRank algorithm (Agirre et al., 2014; Agirre
and Soroa, 2009) over a LKB projected into a
graph. We see qwn-ppv as an effective method-
ology to easily create polarity lexicons for any
language for which a WordNet is available.
This paper empirically shows that: (i) qwn-ppv
outperforms other automatically generated lexi-
cons (e.g. SentiWordNet 3.0, MSOL) on the 4
extrinsic evaluations presented here; it also dis-
plays competitive and robust results also with re-
spect to manually annotated lexicons; (ii) no single
polarity lexicon is fit for every Sentiment Analy-
sis task; depending on the text data and the task
itself, one lexicon will perform better than oth-
ers; (iii) if required, qwn-ppv efficently generates
many lexicons on demand, depending on the task
on which they will be used; (iv) intrinsic evalua-
tion is not appropriate to judge whether a polar-
ity lexicon is fit for a given Sentiment Analysis
(SA) task because good correlation with respect to
a gold-standard does not correspond with correla-
tion with respect to a SA task; (v) it is easily ap-
plicable to create qwn-ppv(s) for other languages,
and we demonstrate it here by creating many po-
larity lexicons not only for English but also for
Spanish; (vi) the method works at both word and
sense levels and it only requires the availability
of a LKB or dictionary; finally, (vii) a dictionary-
based method like qwn-ppv allows to easily cre-
ate quality polarity lexicons whenever no domain-
based annotated reviews are available for a given
language. After all, there usually is available a
dictionary for a given language; for example, the
Open Multilingual WordNet site lists WordNets
for up to 57 languages (Bond and Foster, 2013).
Although there has been previous work using
graph methods for obtaining lexicons via propa-
gation, the qwn-ppv method to combine the seed
generation and the Personalized PageRank prop-
agation is novel. Furthermore, it is considerable
simpler and obtains better and easier to reproduce
results than previous automatic approaches (Esuli
and Sebastiani, 2007; Mohammad et al., 2009;
Rao and Ravichandran, 2009).
Next section reviews previous related work, tak-
ing special interest on those that are currently
available for evaluation purposes. Section 3 de-
scribes the qwn-ppv method to automatically gen-
erate lexicons. The resulting lexical resources are
evaluated in section 4. We finish with some con-
cluding remarks and future work in section 5.
2 Related Work
There is a large amount of work on Sentiment
Analysis and Opinion Mining, and good com-
prehensive overviews are already available (Pang
and Lee, 2008; Liu, 2012), so we will review
the most representative and closest to the present
work. This means that we will not be review-
ing corpus-based approaches but rather those con-
structed manually or upon a dictionary or LKB.
We will in turn use the approaches here reviewed
for comparison with qwn-ppv in section 4.
The most popular manually-built polarity lexi-
con is part of the General Inquirer (Stone et al.,
1966), and consists of 1915 words labelled as
?positive? and 2291 as ?negative?. Taboada et al.
(2010) manually created their lexicons annotating
the polarity of 6232 words on a scale of 5 to -5.
Liu et al., starting with Hu and Liu (2004), have
along the years collected a manually corrected po-
larity lexicon which is formed by 4818 negative
and 2041 positive words. Another manually cor-
rected lexicon (Riloff and Wiebe, 2003) is the one
used by the Opinion Finder system (Wilson et al.,
2005) and contains 4903 negatively and 2718 pos-
itively annotated words respectively.
Among the automatically built lexicons, Turney
and Littman (2003) proposed a minimally super-
vised algorithm to calculate the polarity of a word
depending on whether it co-ocurred more with a
previously collected small set of positive words
rather than with a set of negative ones. Agerri and
Garc??a Serrano presented a very simple method
to extract the polarity information starting from
the quality synset in WordNet (Agerri and Garc??a-
Serrano, 2010). Mohammad et al. (2009) de-
veloped a method in which they first identify (by
means of affixes rules) a set of positive/negative
words which act as seeds, then used a Roget-like
thesaurus to mark the synonymous words for each
polarity type and to generalize from the seeds.
They produce several lexicons the best of which,
MSOL(ASL and GI) contains 51K and 76K en-
tries respectively and uses the full General Inquirer
as seeds. They performed both intrinsic and ex-
trinsic evaluations using the MPQA 1.1 corpus.
Finally, there are two approaches that are some-
what closer to us, because they are based on Word-
Net and graph-based methods. SentiWordNet 3.0
(Baccianella et al., 2010) is built in 4 steps: (i)
they select the synsets of 14 paradigmatic pos-
itive and negative words used as seeds (Turney
89
and Littman, 2003). These seeds are then it-
eratively extended following the construction of
WordNet-Affect (Strapparava and Valitutti, 2004).
(ii) They train 7 supervised classifiers with the
synsets? glosses which are used to assign polar-
ity and objectivity scores to WordNet senses. (iii)
In SentiWordNet 3.0 (Esuli and Sebastiani, 2007)
they take the output of the supervised classifiers
as input to applying PageRank to WordNet 3.0?s
graph. (iv) They intrinsically evaluate it with re-
spect to MicroWnOp-3.0 using the p-normalized
Kendall ? distance (Baccianella et al., 2010). Rao
and Ravichandran (2009) apply different semi-
supervised graph algorithms (Mincuts, Random-
ized Mincuts and Label Propagation) to a set of
seeds constructed from the General Inquirer. They
evaluate the generated lexicons intrinsically taking
the General Inquirer as the gold standard for those
words that had a match in the generated lexicons.
In this paper, we describe two methods to au-
tomatically generate seeds either by following
Agerri and Garc??a-Serrano (2010) or using Tur-
ney and Littman?s (2003) seeds. The automati-
cally obtained seeds are then fed into a Person-
alized PageRank algorithm which is applied over
a WordNet projected on a graph. This method is
fully automatic, simple and unsupervised as it only
relies on the availability of a LKB.
3 Generating qwn-ppv
The overall procedure of our approach consists of
two steps: (1) automatically creates a set of seeds
by iterating over a LKB (e.g. a WordNet) rela-
tions; and (2) uses the seeds to initialize contexts
to propagate over the LKB graph using a Personal-
ized Pagerank algorithm. The result is qwn-ppv(s):
Q-WordNets as Personalized PageRanking Vec-
tors.
3.1 Seed Generation
We generate seeds by means of two different auto-
matic procedures.
1. AG: We start at the quality synset of WordNet
and iterate over WordNet relations following
the original Q-WordNet method described in
Agerri and Garc??a Serrano (2010).
2. TL: We take a short manually created list
of 14 positive and negative words (Turney
and Littman, 2003) and iterate over Word-
Net using five relations: antonymy, similarity,
derived-from, pertains-to and also-see.
The AG method starts the propagation from
the attributes of the quality synset in WordNet.
There are five noun quality senses in WordNet,
two of which contain attribute relations (to adjec-
tives). From the quality
1
n
synset the attribute re-
lation takes us to positive
1
a
, negative
1
a
, good
1
a
and
bad
1
a
; quality
2
n
leads to the attributes superior
1
a
and
inferior
2
a
. The following step is to iterate through
every WordNet relation collecting (i.e., annotat-
ing) those synsets that are accessible from the
seeds. Both AG and TL methods to generate seeds
rely on a number of relations to obtain a more bal-
anced POS distribution in the output synsets. The
output of both methods is a list of (assumed to be)
positive and negative synsets. Depending on the
number of iterations performed a different number
of seeds to feed UKB is obtained. Seed numbers
vary from 100 hundred to 10K synsets. Both seed
creation methods can be applied to any WordNet,
not only Princeton WordNet, as we show in sec-
tion 4.
3.2 PPV generation
The second and last step to generate qwn-ppv(s)
consists of propagating over a WordNet graph to
obtain a Personalized PageRanking Vector (PPV),
one for each polarity. This step requires:
1. A LKB projected over a graph.
2. A Personalized PageRanking algorithm
which is applied over the graph.
3. Seeds to create contexts to start the propaga-
tion, either words or synsets.
Several undirected graphs based on WordNet
3.0 as represented by the MCR 3.0 (Agirre et
al., 2012) have been created for the experimenta-
tion, which correspond to 4 main sets: (G1) two
graphs consisting of every synset linked by the
synonymy and antonymy relations; (G2) a graph
with the nodes linked by every relation, includ-
ing glosses; (G3) a graph consisting of the synsets
linked by every relation except those that are
linked by antonymy; finally, (G4) a graph consist-
ing of the nodes related by every relation except
the antonymy and gloss relations.
Using the (G1) graphs, we propagate from the
seeds over each type of graph (synonymy and
antonymy) to obtain two rankings per polarity.
90
Synset Level Word level
Positives Negatives Positives Negatives
Lexicon size P R F P R F size P R F P R F
Automatically created
MSOL(ASL-GI)* 32706 .65 .45 .53 .58 .76 .66 76400 .70 .49 .58 .61 .79 .69
QWN 15508 .69 .53 .60 .62 .76 .68 11693 .64 .53 .58 .60 .70 .65
SWN 27854 .73 .57 .64 .65 .79 .71 38346 .70 .55 .62 .63 .77 .69
QWN-PPV-AG(s03 G1/w01 G1) 2589 .77 .63 .69 .69 .81 .74 5119 .68 .77 .72 .73 .64 .68
QWN-PPV-TL(s04 G1/w01 G1) 5010 .76 .66 .70 .70 .79 .74 4644 .68 .71 .69 .70 .67 .68
(Semi-) Manually created
GI* 2791 .74 .57 .64 .65 .80 .72 3376 .79 .64 .71 .70 .83 .76
OF* 4640 .77 .61 .68 .68 .81 .74 6860 .82 .71 .76 .74 .84 .79
Liu* 4127 .81 .63 .71 .70 .85 .76 6786 .85 .74 .79 .77 .87 .82
SO-CAL* 4212 .75 .57 .64 .65 .81 .72 6226 .82 .70 .76 .74 .85 .79
Table 1: Evaluation of lexicons at document level using Bespalov?s Corpus.
The graphs created in (G2), (G3) and (G4) are
used to obtain two ranks, one for each polarity by
propagating from the seeds. In all four cases the
different polarity rankings have to be combined in
order to obtain a final polarity lexicon: the polar-
ity score pol(s) of a given synset s is computed
by adding its scores in the positive rankings and
subtracting its scores in the negative rankings. If
pol(s) > 0 then s is included in the final lexicon
as positive. If pol(s) < 0 then s is included in the
final lexicon as negative. We assume that synsets
with null polarity scores have no polarity and con-
sequently they are excluded from the final lexicon.
The Personalized PageRanking propagation is
performed starting from both synsets and words
and using both AG and TL styles of seed gen-
eration, as explained in section 3.1. Combin-
ing the various possibilities will produce at least
6 different lexicons for each iteration, depending
on which decisions are taken about which graph,
seeds and word/synset to create the qwn-ppv(s). In
fact, the experiments produced hundreds of lexi-
cons, according to the different iterations for seed
generation
1
, but we will only refer to those that
obtain the best results in the extrinsic evaluations.
With respect to the algorithm to propagate over
the WordNet graph from the automatically created
seeds, we use a Personalized PageRank algorithm
(Agirre et al., 2014; Agirre and Soroa, 2009). The
famous PageRank (Brin and Page, 1998) algo-
rithm is a method to produce a rank from the ver-
tices in a graph according to their relative struc-
tural importance. PageRank has also been viewed
as the result of a Random Walk process, where the
final rank of a given node represents the probabil-
ity of a random walk over the graph which ends on
that same node. Thus, if we take the created Word-
1
The total time to generate the final 352 QWN-PPV prop-
agations amounted to around two hours of processing time in
a standard PC.
Net graph G with N vertices v
1
, . . . , v
n
and d
i
as
being the outdegree of node i, plus a N ?N tran-
sition probability matrix M where M
ji
= 1/d
i
if a link from i to j exists and 0 otherwise, then
calculating the PageRank vector over a graph G
amounts to solve the following equation (1):
Pr = cMPr + (1? c)v (1)
In the traditional PageRank, vector v is a uni-
form normalized vector whose elements values are
all 1/N , which means that all nodes in the graph
are assigned the same probabilities in case of a
random walk. Personalizing the PageRank algo-
rithm in this case means that it is possible to make
vector v non-uniform and assign stronger proba-
bilities to certain nodes, which would make the
algorithm to propagate the initial importance of
those nodes to their vicinity. Following Agirre et
al. (2014), in our approach this translates into ini-
tializing vector v with those senses obtained by the
seed generation methods described above in sec-
tion 3.1. Thus, the initialization of vector v us-
ing the seeds allows the Personalized propagation
to assign greater importance to those synsets in
the graph identified as being positive and negative,
which resuls in a PPV with the weigths skewed to-
wards those nodes initialized/personalized as pos-
itive and negative.
4 Evaluation
Previous approaches have provided intrinsic eval-
uation (Mohammad et al., 2009; Rao and
Ravichandran, 2009; Baccianella et al., 2010) us-
ing manually annotated resources such as the Gen-
eral Inquirer (Stone et al., 1966) as gold stan-
dard. To facilitate comparison, we also provide
such evaluation in section 4.3. Nevertheless, and
as demonstrated by the results of the extrinsic eval-
uations, we believe that polarity lexicons should
91
Synset Level Word level
Positives Negatives Positives Negatives
Lexicon size P R F P R F size P R F P R F
Automatically created
MSOL(ASL-GI)* 32706 .56 .37 .44 .76 .87 .81 76400 .67 .5 .57 .80 .89 .85
QWN 15508 .63 .22 .33 .73 .94 .83 11693 .58 .22 .31 .73 .93 .82
SWN 27854 .57 .33 .42 .75 .89 .81 38346 .55 .55 .55 .80 .8 .80
QWN-PPV-AG (w10 G3/s09 G4) 117485 .60 .63 .62 .83 .82 .83 144883 .65 .50 .57 .80 .88 .84
QWN-PPV-TL (s05 G4) 114698 .61 .58 .59 .82 .83 .83 144883 .66 .53 .59 .81 .88 .84
(Semi-) Manually created
GI* 2791 .70 .32 .44 .76 .94 .84 3376 .71 .56 .62 .82 .90 .86
OF* 4640 .67 .37 .48 .77 .92 .84 6860 .75 .68 .71 .87 .90 .88
Liu* 4127 .67 .33 .44 .76 .93 .83 6786 .78 .45 .57 .79 .94 .86
SO-CAL* 4212 .69 .3 .42 .75 .94 .84 6226 .73 .53 .61 .81 .91 .86
Table 2: Evaluation of lexicons using averaged ratio on the MPQA 1.2
test
Corpus.
in general be evaluated extrinsically. After all,
any polarity lexicon is as good as the results ob-
tained by using it for a particular Sentiment Anal-
ysis task.
Our goal is to evaluate the polarity lexicons
simplifying the evaluation parameters to avoid as
many external influences as possible on the re-
sults. We compare our work with most of the
lexicons reviewed in section 2, both at synset
and word level, both manually and automatically
generated: General Inquirer (GI), Opinion Finder
(OF), Liu, Taboada et al.?s (SO-CAL), Agerri
and Garc??a-Serrano (2010) (QWN), Mohammad
et al?s, (MSOL(ASL-GI)) and SentiWordNet 3.0
(SWN). The results presented in section 4.2 show
that extrinsic evaluation is more meaningful to de-
termine the adequacy of a polarity lexicon for a
specific Sentiment Analysis task.
4.1 Datasets and Evaluation System
Three different corpora were used: Bespalov et
al.?s (2011) and MPQA (Riloff and Wiebe, 2003)
for English, and HOpinion
2
in Spanish. In addi-
tion, we divided the corpus into two subsets (75%
development and 25% test) for applying our ratio
system for the phrase polarity task too. Note that
the development set is only used to set up the po-
larity classification task, and that the generation of
qwn-ppv lexicons is unsupervised.
For Spanish we tried to reproduce the English
settings with Bespalov?s corpus. Thus, both devel-
opment and test sets were created from the HOpin-
ion corpus. As it contains a much higher propor-
tion of positive reviews, we created also subsets
which contain a balanced number of positive and
negative reviews to allow for a more meaningful
comparison than that of table 6. Table 3 shows the
number of documents per polarity for Bespalov?s,
2
http://clic.ub.edu/corpus/hopinion
MPQA 1.2 and HOpinion.
Corpus POS docs NEG docs Total
Bespalov
dev
23,112 23,112 46,227
Bespalov
test
10,557 10,557 21,115
MPQA 1.2
dev
2,315 5,260 7,575
MPQA 1.2
test
771 1,753 2,524
MPQA 1.2
total
3,086 7,013 10,099
HOpinion Balanced
dev
1,582 1,582 3,164
HOpinion Balanced
test
528 528 1,056
HOpinion
dev
9,236 1,582 10,818
HOpinion
test
3,120 528 3,648
Table 3: Number of positive and negative docu-
ments in train and test sets.
We report results of 4 extrinsic evaluations or
tasks, three of them based on a simple ratio av-
erage system, inspired by Turney (2002), and an-
other one based on Mohammad et al. (2009). We
first implemented a simple average ratio classifier
which computes the average ratio of the polarity
words found in document d:
polarity(d) =
?
w?d
pol(w)
|d|
(2)
where, for each polarity, pol(w) is 1 if w is in-
cluded in the polarity lexicon and 0 otherwise.
Documents that reach a certain threshold are clas-
sified as positive, and otherwise as negative. To
setup an evaluation enviroment as fair as possi-
ble for every lexicon, the threshold is optimised by
maximising accuracy over the development data.
Second, we implemented a phrase polarity task
identification as described by Mohammad et al.
(2009). Their method consists of: (i) if any of
the words in the target phrase is contained in the
negative lexicon, then the polarity is negative; (ii)
if none of the words are negative, and at least one
word is in the positive lexicon, then is positive;
(iii) the rest are not tagged.
We chose this very simple polarity estimators
because our aim was to minimize the role other
92
Synset Level Word level
Positives Negatives Positives Negatives
Lexicon size P R F P R F size P R F P R F
Automatically created
MSOL(ASL-GI)* 32706 .52 .48 .50 .85 .62 .71 76400 .68 .56 .62 .82 .86 .84
QWN 15508 .50 .36 .42 .84 .32 .46 11693 .45 .49 .47 .78 .51 .61
SWN 27854 .50 .45 .47 .85 .48 .61 38346 .49 .52 .50 .78 .68 .73
QWN-PPV-AG (s09 G3/w02 G3) 117485 .59 .67 .63 .85 .78 .82 147194 .64 .64 .64 .84 .83 .83
QWN-PPV-TL (w02 G3/s06 G3) 117485 .59 .57 .58 .82 .81 .81 147194 .63 .67 .65 .85 .81 .83
(Semi-) Manually created
GI* 2791 .60 .40 .47 .91 .38 .54 3376 .70 .60 .65 .93 .52 .67
OF* 4640 .63 .42 .50 .93 .46 .62 6860 .75 .71 .73 .95 .66 .78
Liu* 4127 .65 .36 .47 .94 .45 .60 6786 .78 .49 .60 .97 .61 .75
SO-CAL* 4212 .65 .37 .47 .92 .45 .60 6226 .73 .57 .64 .96 .59 .73
Table 4: Evaluation of lexicons at phrase level using Mohammad et al.?s (2009) method on MPQA
1.2
total
Corpus.
aspects play in the evaluation and focus on how,
other things being equal, polarity lexicons perform
in a Sentiment Analysis task. The average ratio
is used to present results of tables 1 and 2 (with
Bespalov corpus), and 5 and 6 (with HOpinion),
whereas Mohammad et al.?s is used to report re-
sults in table 4. Mohammad et al.?s (2009) testset
based on MPQA 1.1 is smaller, but both MPQA
1.1 and 1.2 are hugely skewed towards negative
polarity (30% positive vs. 70% negative).
All datasets were POS tagged and Word
Sense Disambiguated using FreeLing (Padr?o and
Stanilovsky, 2012; Agirre and Soroa, 2009). Hav-
ing word sense annotated datasets gives us the op-
portunity to evaluate the lexicons both at word and
sense levels. For the evaluation of those lexicons
that are synset-based, such as qwn-ppv and Sen-
tiWordNet 3.0, we convert them from senses to
words by taking every word or variant contained
in each of their senses. Moreover, if a lemma ap-
pears as a variant in several synsets the most fre-
quent polarity is assigned to that lemma.
With respect to lexicons at word level, we take
the most frequent sense according to WordNet 3.0
for each of their positive and negative words. Note
that the latter conversion, for synset based evalua-
tion, is mostly done to show that the evaluation at
synset level is harder independently of the quality
of the lexicon evaluated.
4.2 Results
Although tables 1, 2 and 4 also present re-
sults at synset level, it should be noted that the
only polarity lexicons available to us for com-
parison at synset level were Q-WordNet (Agerri
and Garc??a-Serrano, 2010) and SentiWordNet 3.0
(Baccianella et al., 2010). QWN-PPV-AG refers
to the lexicon generated starting from AG?s seeds,
and QWN-PPV-TL using TL?s seeds as described
in section 3.1. Henceforth, we will use qwn-ppv to
refer to the overall method presented in this paper,
regardless of the seeds used.
For every qwn-ppv result reported in this sec-
tion, we have used every graph described in sec-
tion 3.2. The configuration of each qwn-ppv in the
results specifies which seed iteration is used as the
initialization of the Personalized PageRank algo-
rithm, and on which graph. Thus, QWN-PPV-TL
(s05 G4) in table 2 means that the 5th iteration of
synset seeds was used to propagate over graph G4.
If the configuration were (w05 G4) it would have
meant ?the 5th iteration of word seeds were used
to propagate over graph G4?. The simplicity of
our approach allows us to generate many lexicons
simply by projecting a LKB over different graphs.
The lexicons marked with an asterisk denote
those that have been converted from word to
senses using the most frequent sense of WordNet
3.0. We would like to stress again that the purpose
of such word to synset conversion is to show that
SA tasks at synset level are harder than at word
level. In addition, it should also be noted that in
the case of SO-CAL (Taboada et al., 2010), we
have reduced what is a graded lexicon with scores
ranging from 5 to -5 into a binary one.
Table 1 shows that (at least partially) manually
built lexicons obtain the best results on this eval-
uation. It also shows that qwn-ppv clearly out-
performs any other automatically built lexicons.
Moreover, manually built lexicons suffer from the
evaluation at synset level, obtaining most of them
lower scores than qwn-ppv, although Liu?s (Hu
and Liu, 2004) still obtains the best results. In any
case, for an unsupervised procedure, qwn-ppv lex-
icons obtain very competitive results with respect
to manually created lexicons and is the best among
the automatic methods. It should also be noted that
the best results of qwn-ppv are obtained with graph
93
G1 and with very few seed iterations.
Table 2 again sees the manually built lexi-
cons performing better although overall the dif-
ferences are lower with respect to automatically
built lexicons. Among these, qwn-ppv again ob-
tains the best results, both at synset and word
level, although in the latter the differences with
MSOL(ASL-GI) are not large. Finally, table 4
shows that qwn-ppv again outperforms other auto-
matic approaches and is closer to those have been
(partially at least) manually built. In both MPQA
evaluations the best graph overall to propagate the
seeds is G3 because this type of task favours high
recall.
Positives Negatives
Lexicon size P R F P R F
Automatically created
SWN 27854 .87 .99 .93 .70 .16 .27
QWN-PPV-AG
(wrd01 G1)
3306 .86 .00 .92 .67 .01 .02
QWN-PPV-TL
(s04 G1)
5010 .89 .96 .93 .58 .30 .39
Table 5: Evaluation of Spanish lexicons using the
full HOpinion corpus at synset level.
We report results on the Spanish HOpinion cor-
pus in tables 5 and 6. Mihalcea(f) is a manu-
ally revised lexicon based on the automatically
built Mihalcea(m) (P?erez-Rosas et al., 2012). Elh-
Polar (Saralegi and San Vicente, 2013) is semi-
automatically built and manually corrected. SO-
CAL is built manually. SWN and QWN-PPV have
been built via the MCR 3.0?s ILI by applying the
synset to word conversion previously described on
the Spanish dictionary of the MCR. The results for
Spanish at word level in table 6 show the same
trend as for English: qwn-ppv is the best of the
automatic approaches and it obtains competitive
although not as good as the best of the manually
created lexicons (ElhPolar). Due to the dispro-
portionate number of positive reviews, the results
for the negative polarity are not useful to draw any
meaningful conclusions. Thus, we also performed
an evaluation with HOpinion Balanced set as listed
in table 3.
The results with a balanced HOpinion, not
shown due to lack of space, also confirm the pre-
vious trend: qwn-ppv outperforms other automatic
approaches but is still worse than the best of the
manually created ones (ElhPolar).
Positives Negatives
Lexicon size P R F P R F
Automatically created
Mihalcea(m) 2496 .86 .00 .92 .00 .00 .00
SWN 9712 .88 .97 .92 .55 .19 .28
QWN-PPV-AG
(s11 G1)
1926 .89 .97 .93 .59 .26 .36
QWN-PPV-TL
(s03 G1)
939 .89 .98 .93 .71 .26 .38
(Semi-) Manually created
ElhPolar 4673 .94 .94 .94 .64 .64 .64
Mihalcea(f) 1347 .91 .96 .93 .61 .41 .49
SO-CAL 4664 .92 .96 .94 .70 .51 .59
Table 6: Evaluation of Spanish lexicons using the
full HOpinion corpus at word level.
4.3 Intrinsic evaluation
To facilitate intrinsic comparison with previous
approaches, we evaluate our automatically gener-
ated lexicons against GI. For each qwn-ppv lex-
icon shown in previous extrinsic evaluations, we
compute the intersection between the lexicon and
GI, and evaluate the words in that intersection. Ta-
ble 7 shows results for the best-performing QWN-
PPV lexicons (both using AG and TL seeds) in
the extrinsic evaluations at word level of tables 1
(first two rows), 2 (rows 3 and 4) and 4 (rows 5
and 6). We can see that QWN-PPV lexicons sys-
tematically outperform SWN in number of correct
entries. QWN-PPV-TL lexicons obtain 75.04%
of correctness on average. The best performing
lexicon contains up to 81.07% of correct entries.
Note that we did not compare the results with
MSOL(ASL-GI) because it contains the GI.
Lexicon ? wrt. GI Acc. Pos Neg
SWN 2,755 .74 .76 .73
QWN-PPV-AG (w01 G1) 849 .71 .68 .75
QWN-PPV-TL (w01 G1) 713 .78 .80 .76
QWN-PPV-AG (s09 G4) 3,328 .75 .75 .77
QWN-PPV-TL (s05 G4) 3,333 .80 .84 .77
QWN-PPV-AG (w02 G3) 3,340 .74 .71 .77
QWN-PPV-TL (s06 G3) 3,340 .77 .79 .77
Table 7: Accuracy QWN-PPV lexicons and SWN
with respect to the GI lexicon.
4.4 Discussion
QWN-PPV lexicons obtain the best results among
the evaluations for English and Spanish. Further-
more, across tasks and datasets qwn-ppv provides
a more consistent and robust behaviour than most
of the manually-built lexicons apart from OF. The
results also show that for a task requiring high
94
recall the larger graphs, e.g. G3, are preferable,
whereas for a more balanced dataset and document
level task smaller G1 graphs perform better.
These are good results considering that our
method to generate qwn-ppv is simpler, more ro-
bust and adaptable than previous automatic ap-
proaches. Furthermore, although also based on
a Personalized PageRank application, it is much
simpler than SentiWordNet 3.0, consistently out-
performed by qwn-ppv on every evaluation and
dataset. The main differences with respect to Sen-
tiWordNet?s approach are the following: (i) the
seed generation and training of 7 supervised clas-
sifiers corresponds in qwn-ppv to only one simple
step, namely, the automatic generation of seeds
as explained in section 3.1; (ii) the generation
of qwn-ppv only requires a LKB?s graph for the
Personalized PageRank propagation, no disam-
biguated glosses; (iii) the graph they use to do
the propagation also depends on disambiguated
glosses, not readily available for any language.
The fact that qwn-ppv is based on already
available WordNets projected onto simple graphs
is crucial for the robustness and adaptability of
the qwn-ppv method across evaluation tasks and
datasets: Our method can quickly create, over dif-
ferent graphs, many lexicons of diffent sizes which
can then be evaluated on a particular polarity clas-
sification task and dataset. Hence the different
configurations of the qwn-ppv lexicons, because
for some tasks a G3 graph with more AG/TL seed
iterations will obtain better recall and viceversa.
This is confirmed by the results: the tasks using
MPQA seem to clearly benefit from high recall
whereas the Bespalov?s corpus has overall, more
balanced scores. This could also be due to the size
of Bespalov?s corpus, almost 10 times larger than
MPQA 1.2.
The experiments to generate Spanish lexicons
confirm the trend showed by the English evalua-
tions: Lexicons generated by qwn-ppv consistenly
outperform other automatic approaches, although
some manual lexicon is better on a given task and
dataset (usually a different one). Nonetheless the
Spanish evaluation shows that our method is also
robust across languages as it gets quite close to
the manually corrected lexicon of Mihalcea(full)
(P?erez-Rosas et al., 2012).
The results also confirm that no single lexicon is
the most appropriate for any SA task or dataset and
domain. In this sense, the adaptability of qwn-ppv
is a desirable feature for lexicons to be employed
in SA tasks: the unsupervised qwn-ppv method
only relies on the availability of a LKB to build
hundreds of polarity lexicons which can then be
evaluated on a given task and dataset to choose the
best fit. If not annotated evaluation set is avail-
able, G3-based propagations provide the best re-
call whereas the G1-based lexicons are less noisy.
Finally, we believe that the results reported here
point out to the fact that intrinsic evaluations are
not meaningful to judge the adequacy a polarity
lexicon for a specific SA task.
5 Concluding Remarks
This paper presents an unsupervised dictionary-
based method qwn-ppv to automatically generate
polarity lexicons. Although simpler than similar
automatic approaches, it still obtains better results
on the four extrinsic evaluations presented. Be-
cause it only depends on the availability of a LKB,
we believe that this method can be valuable to gen-
erate on-demand polarity lexicons for a given lan-
guage when not sufficient annotated data is avail-
able. We demonstrate the adaptability of our ap-
proach by producing good performance polarity
lexicons for different evaluation scenarios and for
more than one language.
Further work includes investigating different
graph projections of WordNet relations to do the
propagation as well as exploiting synset weights.
We also plan to investigate the use of annotated
corpora to generate lexicons at word level to try
and close the gap with those that have been (at
least partially) manually annotated.
The qwn-ppv lexicons and graphs used in this
paper are publicly available (under CC-BY li-
cense): http://adimen.si.ehu.es/web/qwn-ppv. The
qwn-ppv tool to automatically generate polarity
lexicons given a WordNet in any language will
soon be available in the aforementioned URL.
Acknowledgements
This work has been supported by the OpeNER FP7
project under Grant No. 296451, the FP7 News-
Reader project, Grant No. 316404 and by the
Spanish MICINN project SKATER under Grant
No. TIN2012-38584-C06-01.
95
References
R. Agerri and A. Garc??a-Serrano. 2010. Q-WordNet:
extracting polarity from WordNet senses. In Seventh
Conference on International Language Resources
and Evaluation, Malta. Retrieved May, volume 25,
page 2010.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL-2009), Athens, Greece.
Aitor Gonz?alez Agirre, Egoitz Laparra, German Rigau,
and Basque Country Donostia. 2012. Multilin-
gual central repository version 3.0: upgrading a very
large lexical knowledge base. In GWC 2012 6th In-
ternational Global Wordnet Conference, page 118.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
(Early Access).
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Seventh
conference on International Language Resources
and Evaluation (LREC-2010), Malta., volume 25.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings of
the 20th ACM international conference on Informa-
tion and knowledge management, pages 375?382.
Francis Bond and Ryan Foster. 2013. Linking and ex-
tending an open multilingual wordnet. In 51st An-
nual Meeting of the Association for Computational
Linguistics: ACL-2013.
Sergey Brin and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. Computer networks and ISDN systems,
30(1):107117.
Y. Choi and C. Cardie. 2009. Adapting a polarity lexi-
con using integer linear programming for domain-
specific sentiment classification. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2-Volume 2,
pages 590?598.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings
of the international conference on Web search and
web data mining, pages 231?240.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 424?431, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
C. Fellbaum and G. Miller, editors. 1998. Wordnet:
An Electronic Lexical Database. MIT Press, Cam-
bridge (MA).
V. Hatzivassiloglou and K. R McKeown. 1997. Pre-
dicting the semantic orientation of adjectives. In
Proceedings of the eighth conference on European
chapter of the Association for Computational Lin-
guistics, pages 174?181.
M. Hu and B. Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of Coling
2004, pages 1367?1373, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual
projections. In Annual Meeting of the Associa-
tion for Computational Linguistics, volume 45, page
976.
S. Mohammad, C. Dunne, and B. Dorr. 2009. Gen-
erating high-coverage semantic orientation lexicons
from overtly marked words and a thesaurus. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 599?608.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Ver?onica P?erez-Rosas, Carmen Banea, and Rada Mi-
halcea. 2012. Learning sentiment lexicons in span-
ish. In LREC, pages 3077?3081.
D. Rao and D. Ravichandran. 2009. Semi-supervised
polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 675?
682.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
the International Conference on Empirical Methods
in Natural Language Processing (EMNLP?03).
Xabier Saralegi and I?naki San Vicente. 2013. Elhuyar
at TASS2013. In XXIX Congreso de la Sociedad Es-
paola de Procesamiento de lenguaje natural, Work-
shop on Sentiment Analysis at SEPLN (TASS2013),
pages 143?150, Madrid.
96
P. Stone, D. Dunphy, M. Smith, and D. Ogilvie. 1966.
The General Inquirer: A Computer Approach to
Content Analysis. Cambridge (MA): MIT Press.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: an affective extension of wordnet.
In Proceedings of the 4th International Conference
on Languages Resources and Evaluation (LREC
2004), pages 1083?1086, Lisbon, May.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2010. Lexicon-based methods for sen-
timent analysis. Computational Linguistics, (Early
Access):141.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), page 133140, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
P. Turney and M. Littman. 2003. Measuring praise and
criticism: Inference of semantic oreintation from as-
sociation. ACM Transaction on Information Sys-
tems, 21(4):315?346.
P.D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, page 417424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
page 347354.
97
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 5?8,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Multilingual, Efficient and Easy NLP Processing with IXA Pipeline
Rodrigo Agerri
IXA NLP Group
Univ. of the Basque Country
UPV/EHU
Donostia San-Sebasti?an
rodrigo.agerri@ehu.es
Josu Bermudez
Deusto Institute of Technology
Deustotech
Univ. of Deusto
Bilbao
josu.bermudez@deusto.es
German Rigau
IXA NLP Group
Univ. of the Basque Country
UPV/EHU
Donostia-San Sebasti?an
german.rigau@ehu.es
Abstract
IXA pipeline is a modular set of Natural Lan-
guage Processing tools (or pipes) which pro-
vide easy access to NLP technology. It aims at
lowering the barriers of using NLP technology
both for research purposes and for small indus-
trial developers and SMEs by offering robust
and efficient linguistic annotation to both re-
searchers and non-NLP experts. IXA pipeline
can be used ?as is? or exploit its modularity
to pick and change different components. This
paper describes the general data-centric archi-
tecture of IXA pipeline and presents competi-
tive results in several NLP annotations for En-
glish and Spanish.
1 Introduction
Many Natural Language Processing (NLP) applica-
tions demand some basic linguistic processing (Tok-
enization, Part of Speech (POS) tagging, Named Entity
Recognition and Classification (NER), Syntactic Pars-
ing, Coreference Resolution, etc.) to be able to further
undertake more complex tasks. Generally, NLP anno-
tation is required to be as accurate and efficient as pos-
sible and existing tools, quite righly, have mostly fo-
cused on performance. However, this generally means
that NLP suites and tools usually require researchers
to do complex compilation/installation/configuration in
order to use such tools. At the same time, in the indus-
try, there are currently many Small and Medium Enter-
prises (SMEs) offering services that one way or another
depend on NLP annotations.
In both cases, in research and industry, acquiring, de-
ploying or developing such base qualifying technolo-
gies is an expensive undertaking that redirects their
original central focus: In research, much time is spent
in the preliminaries of a particular research experiment
trying to obtain the required basic linguistic annota-
tion, whereas in an industrial environment SMEs see
their already limited resources taken away from of-
fering products and services that the market demands.
IXA pipeline provides ready to use modules to per-
form efficient and accurate linguistic annotation to al-
low users to focus on their original, central task. When
designing the architecture, we took several decisions
with respect to what IXA pipeline had to be:
Simple and ready to use: Every module of the IXA
pipeline can be up an running after two simple steps.
Portable: The modules come with ?all batteries in-
cluded? which means that no classpath configurations
or installing of any third-party dependencies is re-
quired. The modules will will run on any platform as
long as a JVM 1.7+ and/or Python 2.7 are available.
Modular: Unlike other NLP toolkits, which often are
built in a monolithic architecture, IXA pipeline is built
in a data centric architecture so that modules can be
picked and changed (even from other NLP toolkits).
The modules behave like Unix pipes, they all take stan-
dard input, do some annotation, and produce standard
output which in turn is the input for the next module.
The data-centric architecture of IXA pipeline means
that any module is highly independent and can there-
fore be used with other tools from other toolkits if re-
quired.
Efficient: Piping the tokenizer (250K words per sec-
ond) POS tagger and lemmatizer all in one process
annotates over 5K words/second. The NERC mod-
ule annotates over 5K words/second. In a multi-core
machine, these times are dramatically reduced due to
multi-threading.
Multilingual: Currently we offer NLP annotations for
both English and Spanish, but other languages are be-
ing included in the pipeline. Tokenization already
works for several languages, including Dutch, French,
Italian, German, Spanish and English.
Accurate: For example, POS tagging and NERC for
English and Spanish are comparable with other state
of the art systems, as it is the coreference resolution
module for English.
Apache License 2.0: IXA Pipeline is licensed under
the Apache License 2.0, an open-source license that fa-
cilitates source code use, distribution and integration,
also for commercial purposes.
1
Next section describes the IXA pipeline architecture,
section 3 the modules so far developed. Whenever
available, we also present empirical evaluation. Sec-
tion 4 describes the various ways of using the tools.
Finally, section 5 discusses some concluding remarks.
1
http://www.apache.org/licenses/LICENSE-2.0.html
5
2 Architecture
IXA pipeline is primarily conceived as a set of
ready to use tools that can provide efficient and
accurate linguistic annotation without any installa-
tion/configuration/compilation effort. As in Unix-like
operative systems, IXA pipeline consists of a set of pro-
cesses chained by their standard streams, in a way that
the output of each process feeds directly as input to the
next one. The Unix pipeline metaphor has been ap-
plied for NLP tools by adopting a very simple and well
known data centric architecture, in which every mod-
ule/pipe is interchangeable for another one as long as it
takes and produces the required data format.
The data format in which both the input and output of
the modules needs to be formatted to represent and fil-
ter linguistic annotations is KAF (Bosma et al., 2009).
KAF is a language neutral annotation format represent-
ing both morpho-syntactic and semantic annotation in a
structured format. KAF was originally designed in the
Kyoto European project
2
, but it has since been in con-
tinuous development
3
. Our Java modules all use kaflib
4
library for easy integration.
Every module in the IXA pipeline, except the coref-
erence resolution, is implemented in Java, and re-
quires Java JDK1.7+ to compile. The integration of
the Java modules in the IXA pipeline is performed us-
ing Maven
5
. Maven is used to take care of classpaths
configurations and third-party tool dependencies. This
means that the binaries produced and distributed will
work off-the-self. The coreference module uses pip
6
to provide an easy, one step installation. If the source
code of an ixa-pipe-$module is cloned from the remote
repository, one command to compile and have ready the
tools will suffice.
Some modules in IXA pipeline provide linguistic an-
notation based on probabilistic supervised approaches
such as POS tagging, NER and Syntactic Parsing. IXA
pipeline uses two well known machine learning algo-
rithms, namely, Maximum Entropy and the Percep-
tron. Both Perceptron (Collins, 2002; Collins, 2003)
and Maximum Entropy models (Ratnaparkhi, 1999) are
adaptable algorithms which have been successfully ap-
plied to NLP tasks such as POS tagging, NER and
Parsing with state of the art results. To avoid dupli-
cation of efforts, IXA pipeline uses the already avail-
able open-source Apache OpenNLP API
7
to train POS,
NER and parsing probabilistic models using these two
approaches.
2
http://kyoto-project.eu
3
http://www.opener-project.org/kaf/
4
https://github.com/ixa-ehu/kaflib
5
http://maven.apache.org/
6
https://pypi.python.org/pypi/pip
7
http://opennlp.apache.org
3 Pipes
IXA pipeline currently provides the following linguis-
tic annotations: Sentence segmentation, tokenization,
Part of Speech (POS) tagging, Lemmatization, Named
Entity Recognition and Classification (NER), Con-
stituent Parsing and Coreference Resolution. Every
module works for English and Spanish and is imple-
mented in Java/Maven as described above. The only
exception is the coreference resolution module, which
currently is available in Python 2.7 and for English only
(Spanish version will comme soon). We will now de-
scribe which annotation services are provided by each
module of the pipeline.
3.1 ixa-pipe-tok
This module provides rule-based Sentence Segmenta-
tion and Tokenization for French, Dutch, English, Ital-
ian and Spanish. It produces tokenized and segmented
text in KAF, running text and CoNLL formats. The
rules are originally based on the Stanford English To-
kenizer
8
, but with substantial modifications and addi-
tions. These include tokenization for other languages
such as French and Italian, normalization according
the Spanish Ancora Corpus (Taul?e et al., 2008), para-
graph treatment, and more comprehensive gazeteers
of non breaking prefixes. The tokenizer depends on
a JFlex
9
specification file which compiles in seconds
and performs at a very reasonable speed (around 250K
word/second, and much quicker with Java multithread-
ing).
3.2 ixa-pipe-pos
ixa-pipe-pos provides POS tagging and lemmatization
for English and Spanish. We have obtained the best
results so far with the same featureset as in Collins?s
(2002) paper. Perceptron models for English have been
trained and evaluated on the WSJ treebank using the
usual partitions (e.g., as explained in Toutanova et al.
(2003). We currently obtain a performance of 97.07%
vs 97.24% obtained by Toutanova et al., (2003)). For
Spanish, Maximum Entropy models have been trained
and evaluated using the Ancora corpus; it was ran-
domly divided in 90% for training and 10% for test-
ing. This corresponds to 440K words used for train-
ing and 70K words for testing. We obtain a perfor-
mance of 98.88% (the corpus partitions are available
for reproducibility). Gim?enez and Marquez (2004) re-
port 98.86%, although they train and test on a different
subset of the Ancora corpus.
Lemmatization is currently performed via 3 different
dictionary lookup methods: (i) Simple Lemmatizer: It
is based on HashMap lookups on a plain text dictionary.
Currently we use dictionaries from the LanguageTool
project
10
under their distribution licenses. The English
8
http://www-nlp.stanford.edu/software/tokenizer.shtml
9
http://jflex.de/
10
http://languagetool.org/
6
dictionary contains 300K lemmas whereas the Spanish
provides over 600K; (ii) Morfologik-stemming
11
: The
Morfologik library provides routines to produce binary
dictionaries, from dictionaries such as the one used by
the Simple Lemmatizer above, as finite state automata.
This method is convenient whenever lookups on very
large dictionaries are required because it reduces the
memory footprint to 10% of the memory required for
the equivalent plain text dictionary; and (iii) We also
provide lemmatization by lookup in WordNet-3.0 (Fell-
baum and Miller, 1998) via the JWNL API
12
. Note that
this method is only available for English.
3.3 ixa-pipe-nerc
Most of the NER systems nowdays consist of language
independent systems (sometimes enriched with gaze-
teers) based on automatic learning of statistical mod-
els. ixa-pipe-nerc provides Named Entity Recogni-
tion (NER) for English and Spanish. The named en-
tity types are based on the CONLL 2002
13
and 2003
14
tasks which were focused on language-independent su-
pervised named entity recognition (NER) for four types
of named entities: persons, locations, organizations and
names of miscellaneous entities that do not belong to
the previous three groups. We currently provide two
very fast language independent models using a rather
simple baseline featureset (e.g., similar to that of Cur-
ran and Clark (2003), except POS tag features).
For English, perceptron models have been trained
using CoNLL 2003 dataset. We currenly obtain 84.80
F1 which is coherent with other results reported with
these features (Clark and Curran, 2003; Ratinov and
Roth, 2009). The best Stanford NER model reported
on this dataset achieves 86.86 F1 (Finkel et al., 2005),
whereas the best system on this dataset achieves 90.80
F1 (Ratinov and Roth, 2009), using non local features
and substantial external knowledge.
For Spanish we currently obtain best results train-
ing Maximum Entropy models on the CoNLL 2002
dataset. Our best model obtains 79.92 F1 vs 81.39
F1 (Carreras et al., 2002), the best result so far on this
dataset. Their result uses external knowledge and with-
out it, their system obtains 79.28 F1.
3.4 ixa-pipe-parse
ixa-pipe-parse provides statistical constituent parsing
for English and Spanish. Maximum Entropy models
are trained to build shift reduce bottom up parsers (Rat-
naparkhi, 1999) as provided by the Apache OpenNLP
API. Parsing models for English have been trained us-
ing the Penn treebank and for Spanish using the Ancora
corpus (Taul?e et al., 2008).
Furthermore, ixa-pipe-parse provides two methods
of HeadWord finders: one based on Collins? head rules
11
https://github.com/morfologik/morfologik-stemming
12
http://jwordnet.sourceforge.net/
13
http://www.clips.ua.ac.be/conll2002/ner/
14
http://www.clips.ua.ac.be/conll2003/ner/
as defined in his PhD thesis (1999), and another one
based on Stanford?s parser Semantic Head Rules
15
.
The latter are a modification of Collins? head rules ac-
cording to lexical and semantic criteria. These head
rules are particularly useful for the Coreference reso-
lution module and for projecting the constituents into
dependency graphs.
As far as we know, and although previous ap-
proaches exist (Cowan and Collins, 2005), ixa-pipe-
parse provides the first publicly available statistical
parser for Spanish.
3.5 Coreference Resolution
The module of coreference resolution included in the
IXA pipeline is loosely based on the Stanford Multi
Sieve Pass system (Lee et al., 2013). The module takes
every linguistic information it requires from the KAF
layers annotated by all the previously described mod-
ules. The system consists of a number of rule-based
sieves. Each sieve pass is applied in a deterministic
manner, reusing the information generated by the pre-
vious sieve and the mention processing. The order in
which the sieves are applied favours a highest precision
approach and aims at improving the recall with the sub-
sequent application of each of the sieve passes. This
is illustrated by the evaluation results of the CoNLL
2011 Coreference Evaluation task (Lee et al., 2013), in
which the Stanford?s system obtained the best results.
So far we have evaluated our module on the CoNLL
2011 testset and we are a 5% behind the Stanford?s sys-
tem (52.8 vs 57.6 CoNLL F1), the best on that task (Lee
et al., 2013). It is interesting that in our current imple-
mentation, mention-based metrics are favoured (CEAF
and B
3
). Still, note that these results are comparable
with the results obtained by the best CoNLL 2011 par-
ticipants. Currently the module performs coreference
resolution only for English, although a Spanish version
will be coming soon.
4 Related Work
Other NLP toolkits exist providing similar or more ex-
tensive functionalities than the IXA pipeline tools, al-
though not many of them provide multilingual support.
GATE (Cunningham, 2002) is an extensive framework
supporting annotation of text. GATE has some capacity
for wrapping Apache UIMA components
16
, so should
be able to manage distributed NLP components. How-
ever, GATE is a very large and complex system, with a
corresponding steep learning curve.
Freeling (Padr?o and Stanilovsky, 2012) provides
multilingual processing for a number of languages,
incluing Spanish and English. As opposed to IXA
pipeline, Freeling is a monolithic toolkit written in C++
which needs to be compiled natively. The Stanford
15
http://www-nlp.stanford.edu/software/lex-parser.shtml
16
http://uima.apache.org/
7
CoreNLP
17
is a monolithic suite, which makes it dif-
ficult to integrate other tools in its chain.
IXA pipeline tools can easily be used piping the in-
put with the output of another too, and it is also pos-
sible to easily replace or extend the toolchain with a
third-party tool. IXA pipeline is already being used to
do extensive parallel processing in the FP7 European
projects OpeNER
18
and NewsReader
19
.
5 Conclusion and Future Work
IXA pipeline provides a simple, efficient, accurate and
ready to use set of NLP tools. Its modularity and data
centric architecture makes it flexible to pick and change
or integrate new linguistic annotators. Currently we of-
fer linguistic annotation for English and Spanish, but
more languages are being integrated. Furthermore,
other annotations such as Semantic Role Labelling and
Named Entity Disambiguation are being included in
the pipeline following the same principles.
Additionally, current integrated modules are be-
ing improved: both on the quality and variety of
the probabilistic models, and on specific issues such
as lemmatization, and treatment of time expressions.
Finally, we are adding server-mode execution into
the pipeline to provide faster processing. IXA
pipeline is publicly available under Apache 2.0 license:
http://adimen.si.ehu.es/web/ixa-pipes.
Acknowledgements
TThis work has been supported by the OpeNER FP7
project under Grant No. 296451, the FP7 NewsReader
project, Grant No. 316404, and by the SKATER Span-
ish MICINN project No TIN2012-38584-C06-01. The
work of Josu Bermudez on coreference resolution is
supported by a PhD Grant of the University of Deusto
(http://www.deusto.es).
References
Wauter Bosma, Piek Vossen, Aitor Soroa, German
Rigau, Maurizio Tesconi, Andrea Marchetti, Mon-
ica Monachini, and Carlo Aliprandi. 2009. Kaf: a
generic semantic annotation format. In Proceedings
of the GL2009 Workshop on Semantic Annotation.
X. Carreras, L. Marquez, and L. Padro. 2002. Named
entity extraction using AdaBoost. In proceedings
of the 6th conference on Natural language learning-
Volume 20, pages 1?4.
Stephen Clark and James Curran. 2003. Language In-
dependent NER using a Maximum Entropy Tagger.
In Proceedings of the Seventh Conference on Nat-
ural Language Learning (CoNLL-03), pages 164?
167, Edmonton, Canada.
17
http://nlp.stanford.edu/software/corenlp.shtml
18
http://www.opener-project.org
19
http://www.newsreader-project.eu
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Brooke Cowan and Michael Collins. 2005. Mor-
phology and reranking for the statistical parsing of
spanish. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 795?802.
Association for Computational Linguistics.
Hamish Cunningham. 2002. Gate, a general architec-
ture for text engineering. Computers and the Hu-
manities, 36(2):223?254.
C. Fellbaum and G. Miller, editors. 1998. Wordnet: An
Electronic Lexical Database. MIT Press, Cambridge
(MA).
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 363?370.
Jes?us Gim?enez and Lluis Marquez. 2004. Svmtool: A
general pos tagger generator based on support vector
machines. In In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Citeseer.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, pages 1?54, January.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, page 147155.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
learning, 34(1-3):151?175.
Mariona Taul?e, Maria Ant`onia Mart??, and Marta Re-
casens. 2008. Ancora: Multilevel annotated corpora
for catalan and spanish. In LREC.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL, pages 252?259.
8
Textual Entailment as an
Evaluation Framework for
Metaphor Resolution:
A Proposal
Rodrigo Agerri
John Barnden
Mark Lee
Alan Wallington
University of Birmingham (UK)
email: r.agerri@cs.bham.ac.uk
Abstract
We aim to address two complementary deficiencies in Natural Language
Processing (NLP) research: (i) Despite the importance and prevalence of
metaphor across many discourse genres, and metaphor?s many functions,
applied NLP has mostly not addressed metaphor understanding. But,
conversely, (ii) difficult issues in metaphor understanding have hindered
large-scale application, extensive empirical evaluation, and the handling
of the true breadth of metaphor types and interactions with other language
phenomena. In this paper, abstracted from a recent grant proposal, a new
avenue for addressing both deficiencies and for inspiring new basic re-
search on metaphor is investigated: namely, placing metaphor research
within the ?Recognizing Textual Entailment? (RTE) task framework for
evaluation of semantic processing systems.
357
358 Agerri, Barnden, Lee, and Wallington
1 Introduction
The RTE task and annual Challenges (Dagan et al, 2007), starting in 2005, have arisen
as an evaluation framework for applied semantics in response to the fact that in NLP
applications ? such as Question Answering (QA), Information Retrieval/Extraction
(IR, IE), etc. ? the development of semantic algorithms and models have been scat-
tered, or tailored to specific applications, making it difficult to compare and evaluate
them within one framework. RTE is interesting because QA, IE, etc. can all be cast
as RTE problems. In RTE, one text fragment, the Text T, is said to entail another one,
the Hypothesis H, when humans considering T and H judge that H follows from T
(perhaps only plausibly/defeasibly). Thus, entailment is a commonsense matter, not a
precise logic-based one. An example of a T/H pair is as follows (metaphor in italics):
(1) T: Lyon is actually the gastronomic capital of France.
H: Lyon is the capital of France.
Metaphor can roughly be characterized as describing something (the target) as if it
were something else (the source) to which it is perceived, or set forth, as being some-
how analogous. Metaphor has long been identified as being ubiquitous in language
(Goatly, 1997), including ordinary conversation, newspaper articles, popular novels,
popular science writing, classroom dialogue, etc. In a study (Tech. Rept. CSRP-03-05
at our School, 2003) we found one metaphorical term per 17.3 words, averaging across
various discourses of different genres. This is in line with other researchers? studies
though counts vary widely because of theory-relativity, researchers? aims, and marked
usage differences between genres. Gedigian et al (2006) note that 90% of uses of a
set of verbs of spatial motion, manipulation, and health in a Wall Street Journal corpus
were metaphorical. Some metaphor examples arising in past RTE datasets are the Ts
in T/H pairs (1?4), with human judgments No, Yes, No and No respectively.
(2) T: The technological triumph known as GPS was incubated in the mind of Ivan
Getting.
H: Ivan Getting invented the GPS.
(3) T: Convinced that pro-American officials are in the ascendancy in Tokyo, they
talk about turning Japan into the Britain of the Far East.
H: Britain is located in the Far East.
(4) T: Even today, within the deepest recesses of our mind, lies a primordial fear
that will not allow us to enter the sea without thinking about the possibility of
being attacked by a shark.
H: A shark attacked a human being.
Importantly, metaphor is often not just a matter of particular terms with particular
metaphorical senses that are entrenched (i.e., that are commonly used, default senses;
and possibly listed in dictionaries). Certainly the metaphorical senses of ?capital? and
?incubate? used in (1) and (2) are at least moderately entrenched. For ?incubate,?
Textual Entailment as an Evaluation Framework for Metaphor Resolution 359
some dictionaries list a slow, protective sense of development (in a general, possibly
non-physical sense), or for ?capital? a sense like ?a city [or place more generally]
preeminent in some special activity? [Merriam-Websters]. But (3) shows one common
type of non-entrenchedmetaphor, of the general form ?the X of Y?, where X and/or Y
are often named entities. The point of such a metaphor is typically only clear with the
help of context. Reference to recesses of a mind as in (4) is common, and a lexicon
could reasonably include a metaphorical sense of ?recess? that was directly applicable
to minds (though WordNet 3.0, e.g, does not), or include ?within the recesses of [X?s]
mind? as a stock phrase with a sense of relative inaccessibility or unmodifiability of
a thought or feeling. But the phraseology can be productively varied: e.g., ?deepest?
can be omitted or replaced by ?dim?, ?darkest?, ?foulest?, ?hidden?, etc. ? by any
compatible qualifier that emphasizes hiddenness or obstacles to accessibility. And the
fact that such access difficulties are being emphasized is a matter for general semantic
reasoning about the qualifier.
2 Why Metaphor in NLP?
Generally, in metaphor understanding research, a specialized system has been fed a
relatively small number of metaphorical inputs, and the correct outputs have been
dictated by the researcher, (e.g. Fass, 1997; Falkenhainer et al, 1989; Martin, 1990;
Barnden et al, 2003). However, metaphor in particular and figurative language in
general suffers a chronic lack of shared resources for proper evaluation of systems
(Markert and Nissim, 2007). In using the RTE evaluation framework, computational
metaphor researchers may for the first time have sizable, shared datasets, and a uni-
form evaluation method based on systematically and transparently collected human
judgments. Also, metaphor researchers will be challenged and inspired to connect
metaphor more than before to other complex linguistic phenomena.
NLP applications that RTE serves have mostly not addressed metaphor, and nei-
ther have RTE systems themselves. Indeed, despite examples (1-4), RTE datasets
have tended to avoid metaphor of more difficult types. Metaphor in general can in-
troduce additional context-sensitivity and indeterminacy in entailment, whereas RTE
Challenges have mainly concentrated on T/H pairs supporting relatively crisp, uncon-
troversial judgments (Zaenen et al (2005); RTE organizers (personal communication);
our own analysis of existing RTE datasets in Tech. Rept. CSRP-08-01, 2008). In fact,
on examples such as (1), a system must interpret ?capital? metaphorically, but as Bos
and Markert (2006) reported, the inability of their system to process metaphor meant
that it was incorrect on this example.
Further evidence is given by results on example (1) of the four RTE-1 systems avail-
able to us (out of 16 systems; the 4 include the 1st, 3rd and 4th best systems in terms of
accuracy over whole dataset). Only one reported system run was correct. The systems
mostly performed worse across (1) together with 10 other metaphor cases than on the
whole dataset, with statistical significance at 0.05 level (Fisher?s independence test);
only one system run gave a better performance. In RTE-2 (23 systems), analysis of
10 system runs shows that, across 9 metaphorical examples in the dataset, the systems
(including the most accurate one over the whole dataset) performed worse than they
did over the rest of the dataset; in 7 of the 10 RTE-2 runs compared the deficit was
statistically significant at < 0.05 level (Fisher?s test, see Tech Rept. CSRP-08-01).
360 Agerri, Barnden, Lee, and Wallington
3 RoadMap: Datasets and Metaphor Processing
Our initiative mainly consists of (A): Create a public, annotated, metaphor-focussed
text dataset suitable for RTE evaluation and testing of metaphor processing systems;
and (B): Develop a prototype RTE system centred on processing (at least) the types
of metaphor arising in (A). We will mainly address point (B) in this paper. As for
A, metaphors vary along several dimensions of interest, such as: the target subject
matter (e.g., Lyon?s food, GPS development, Japanese foreign politics, shark fear in
examples 1?4); the source subject matter; what particular, familiar metaphorical views
(e.g., Idea as Living Being, in (2)) are used; whether the meaning in context is listed
in dictionaries, WordNet, etc; whether the wording is (a variant of) a familiar idiom;
the syntax used. Based on such dimensions, we will analyse the types of metaphor
present in past RTE datasets and in the genres of text (e.g., newspapers) they drew
from. One particular source will be the 242K-word metaphor-orientated corpus that
we derived from the British National Corpus. To find metaphor examples elsewhere,
we will use known metaphorical phrases and lexical/syntactic templates as seeds for
automated search over general corpora or web. We will also investigate the use or
adaptation of other researchers? automated detection/mining techniques (e.g. Birke
and Sarkar, 2006; Gedigian et al, 2006).
4 Metaphor Processing
We will develop metaphor processing algorithms on an integrated spectrum going
from relatively ?shallow? forms of processing to relatively ?deep? forms. The deeper
are for when more inference is necessary and feasible; when less necessary or feasible,
the shallower are appropriate (but they can still involve at least partial parsing, approx-
imate semantic analysis, etc.). A significant research issue will be how to choose the
attempted depth(s) of analysis and how to choose or combine different methods? re-
sults. The deeper and shallower ends of the spectrum will take as starting points our
two previous metaphor-understanding approaches, from our ATT-Meta and E-Drama
projects respectively (Barnden et al, 2003; Wallington et al, 2008).
For detectingmetaphor, we can extendmethods from our E-drama project (Walling-
ton et al, 2008). This involves a mixture of light semantic analysis via tools such as
WordNet and recognition of lexical and syntactic signals of metaphoricity (Goatly,
1997). We aim also to recognize specific idiomatic phrases and systematic variations
of them. We will consider looking for semantic restriction violations, which some-
times accompany metaphor (cf. Fass (1997) and Mason (2004)), and using statistical
techniques borrowed from such work as Gedigian et al (2006).
Example (4) about ?recesses? is similar to metaphor examples studied in the ATT-
Meta project, and ATT-Meta-style reasoning could handle the Text. As for (2), note
that the word ?incubate? may have a directly usable lexicon-listed meaning (e.g., help
to develop much as in WordNet 3.0). However, if the system?s lexicon did not contain
such a sense, ATT-Meta-style processing would apply. ATT-Meta processing would
involve commonsense reasoning in source-domain terms (here, biological and other
physical terms): e.g., to infer that the idea was a living being, Getting?s mind was
a physical region, the idea was kept warm in that region, and the idea consequently
biologically developed there. Hence, there was a relatively protracted, continuous
Textual Entailment as an Evaluation Framework for Metaphor Resolution 361
process; the idea became more functional as a living being; and the idea needed pro-
tection from physical harm (= disenablement of function-inhibiting influences). These
default conclusions can then be translated into reality (target-based) terms: there was
a protracted, (non-physical) continuous process of change; the idea needed protection
during it; and the idea ended up being more functional. The basis for such translation
is View-Neutral Mapping Adjuncts, a special type of mappings that cover the shape
of events and processes, temporal properties and relationships, causation, functioning,
mental states, emotional states, value judgments and various other matters (Agerri
et al, 2007; Barnden et al, 2003).
RTE-2 organisers claimed there has been a trend towards using more deep inference
and that this has been beneficial provided that it is based on enough knowledge. (See
also Bos and Markert (2006) and Clark et al (2007)). The depth and knowledge needs
of ATT-Meta?s processing are like those of deeper parts of existing RTE systems, but
ATT-Meta is currently equipped only with small, hand-constructed knowledge bases.
So, our main focus in deeper processing will actually be on a shallowed, broadened
form of ATT-Meta-style reasoning. In this sense, we aim to look at common-sense
knowledge resources such as ConceptNet 3.0 which contains relationships such as
causation, function, etc. ? the types of information transferred by some of ATT-
Meta?s VNMAs ? or modified WordNets enriched by extra, web-mined knowledge
(Veale and Hao, 2008), where the extra knowledge is especially relevant to metaphor-
ical usages.
ATT-Meta?s reasoning is by backwards chaining from goals derivable from context
surrounding a metaphor. This relevance-based inference-focussingwill be key in other
processing we develop, and is highly RTE-compatible. An RTE Hypothesis can act as
(part of) a reasoning goal, with the metaphor?s within-Text context supplying further
information or goal parts. T?s own original context is of course unavailable, but this
obstacle faces human judges as well.
We will also further extend our E-Drama project?s methods, based there on robust
parsing using the Rasp system and computation over WordNet. The methods are cur-
rently largely confined to metaphors of ?X is Y? form where X is a person and Y
is some type of animal, supernatural being, artefact or natural physical object. We
will generalize to other categories for X and Y, and to cases where the categorization
is only implicit. We will make the syntactic/semantic analysis of WordNet synset-
glosses and the way the system traverses the network more advanced. We will extend
our associated treatment of metaphorically-used size adjectives (as in ?little bully?).
However, the methods are also currently confined to detecting emotional/value
judgments about X (unpleasantness, etc.), and mainly exploit metaphorical informa-
tion that is already implicitly in WordNet, e.g., ?pig? meaning a coarse, obnoxious
person, in one synset. Substantial research is needed to go beyond these limitations.
One avenue will be to apply VNMAs: when a synset gloss couches a metaphorical
sense, we could extract not just affect but other types of information that VNMAs
handle (causation, process shape, etc.); and when a gloss couches a non-metaphorical
sense, we could translate some aspects of it via VNMAs.
362 Agerri, Barnden, Lee, and Wallington
5 Concluding Remarks
This paper aims to provide an avenue for giving metaphor-understanding the promi-
nence it merits in NLP applications and in RTE, and thereby also to engender basic
and applied research advances on metaphor by ourselves and others.
RTE researchers and NLP applications developers will benefit, as systems will gain
added accuracy and coverage by addressing metaphor. Beneficiary application areas
aside from QA, IR, etc., include Knowledge Management, Information Access, and
intelligent conversation agents.
References
Agerri, R., J. Barnden, M. Lee, and A. Wallington (2007). Metaphor, inference and
domain independent mappings. In Proceedings of Research Advances in Natural
Language Processing (RANLP 2007), Borovets, Bulgaria, pp. 17?24.
Barnden, J., S. Glasbey, M. Lee, and A. Wallington (2003). Domain-transcending
mappings in a system for metaphorical reasoning. In Companion Proceedings of
the 10th Conference on the European Chapter of the Association for Computational
Linguistics (EACL-03), pp. 57?61.
Birke, J. and A. Sarkar (2006). A clustering approach for the nearly unsupervised
recognition of nonliteral language. In Proceedings of the 11th Conference on the
European Chapter of the Association for Computational Linguistics (EACL 2006),
Trento, Italy, pp. 329?336.
Bos, J. and K. Markert (2006). Recognizing textual entailment with robust logical
inference. In J. Qui?onero-Candela, I. Dagan, B. Magnini, and F. d?Alch? Buc
(Eds.), MLCW 2005, Volume 3944 of LNAI, pp. 404?426. Springer-Verlag.
Clark, P., W. Murray, J. Thompson, P. Harrison, J. Hobbs, and C. Fellbaum (2007). On
the role of lexical and world knowledge in RTE3. In Proceedings of the Workshop
on Textual Entailment and Paraphrasing, Prague, pp. 54?59. ACL 2007.
Dagan, I., O. Glickman, and B. Magnini (2007). The PASCAL Recognising Tex-
tual Entailment challenge. In J. Qui?onero-Candela, I. Dagan, B. Magnini, and
F. d?Alch? Buc (Eds.), MLCW 2005, Volume 3944 of LNAI, pp. 177?190. Springer-
Verlag.
Falkenhainer, B., K. Forbus, and D. Gentner (1989). The structure-mapping engine:
algorithm and examples. Artificial Intelligence 41(1), 1?63.
Fass, D. (1997). Processing metaphor and metonymy. Greenwich, Connecticut:
Ablex.
Gedigian, M., J. Bryant, S. Narayanan, and B. Ciric (2006). Catching metaphors. In
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding,
New York, pp. 41?48.
Goatly, A. (1997). The Language of Metaphors. Routledge.
Textual Entailment as an Evaluation Framework for Metaphor Resolution 363
Markert, K. and M. Nissim (2007, June). Semeval-2007: Metonymy resolution at
semeval-2007. In International Workshop on Semantic Evaluations (SemEval-
2007), Prague, pp. 36?41. Association for Computational Linguistics (ACL-07).
Martin, J. (1990). A computational model of metaphor interpretation. New York:
Academic Press.
Mason, Z. (2004). CorMet: a computational, corpus-based conventional metaphor
extraction system. Computational Linguistics 30(1), 23?44.
Veale, T. and Y. Hao (2008). EnrichingWordNet with folk knowledge and stereotypes.
In Proceedings of the 4th Global WordNet Conference, Szeged, Hungary.
Wallington, A., R. Agerri, J. Barnden, M. Lee, and T. Rumbell (2008, May). Affect
transfer by metaphor for an intelligent conversational agent. In Proceedings of the
LREC 2008 Workshop on Sentiment Analysis: Emotion, Metaphor, Ontology and
Terminology (EMOT 08), Marrakech, Morocco, pp. 102?107.
Zaenen, A., L. Karttunen, and R. Crouch (2005). Local textual inference: Can it be
defined or circumscribed? In Proceedings of the ACL 05 Workshop on Empirical
Modelling of Semantic Equivalence and Entailment, pp. 31?36.
