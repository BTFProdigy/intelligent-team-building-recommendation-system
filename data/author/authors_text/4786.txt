Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1086?1095,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Empirical Exploitation of Click Data for Task Specific Ranking
Anlei Dong Yi Chang Shihao Ji Ciya Liao Xin Li Zhaohui Zheng
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
{anlei,yichang,shihao,ciyaliao,xinli,zhaohui}@yahoo-inc.com
Abstract
There have been increasing needs for task
specific rankings in web search such as
rankings for specific query segments like
long queries, time-sensitive queries, navi-
gational queries, etc; or rankings for spe-
cific domains/contents like answers, blogs,
news, etc. In the spirit of ?divide-and-
conquer?, task specific ranking may have
potential advantages over generic ranking
since different tasks have task-specific fea-
tures, data distributions, as well as feature-
grade correlations. A critical problem for
the task-specific ranking is training data
insufficiency, which may be solved by us-
ing the data extracted from click log. This
paper empirically studies how to appro-
priately exploit click data to improve rank
function learning in task-specific ranking.
The main contributions are 1) the explo-
ration on the utilities of two promising ap-
proaches for click pair extraction; 2) the
analysis of the role played by the noise
information which inevitably appears in
click data extraction; 3) the appropriate
strategy for combining training data and
click data; 4) the comparison of click data
which are consistent and inconsistent with
baseline function.
1 Introduction
Learning-to-rank approaches (Liu, 2008) have
been widely applied in commercial search en-
gines, in which ranking models are learned using
labeled documents. Significant efforts have been
made in attempt to learn a generic ranking model
which can appropriately rank documents for all
queries . However, web users? query intentions are
extremely heterogeneous, which makes it difficult
for a generic ranking model to achieve best rank-
ing results for all queries. For this reason, there
have been increasing needs for task specific rank-
ings in web search such as rankings for specific
query segments like long queries, time-sensitive
queries, navigational queries, etc; or rankings
for specific domains/contents like answers, blogs,
news, etc. Therefore, a specific ranking task usu-
ally correspond to a category of queries; when
the search engine determines that a query is be-
longing to this category, it will call the ranking
function dedicated to this ranking task. The mo-
tivation of this divide-and-conquer strategy is that,
task specific ranking may have potential advan-
tages over generic ranking since different tasks
have task-specific features, data distributions, as
well as feature-grade correlations.
Such a dedicated ranking model can be trained
using the labeled data belonging to this query cat-
egory (which is called dedicated training data).
However, the amount of training data dedicated
to a specific ranking task is usually insufficient
because human labeling is expensive and time-
consuming, not to mention there are multiple rank-
ing tasks that need to be taken care of. To deal
with the training data insufficiency problem for
task-specific ranking, we propose to extract click-
through data and incorporate it with dedicated
training data to learn a dedicated model.
In order to incorporate click data to improve the
ranking for a dedicate query category, it is critical
to fully exploit click information. We empirically
explore the related approaches for the appropriate
click data exploitation in task-specific rank func-
tion learning. Figure 1 illustrates the procedures
and critical components to be studied.
1) Click data mining: the purpose is to extract
informative and reliable users? preference infor-
mation from click log. We employ two promis-
ing approaches: one is heuristic rule approach, the
other is sequential supervised learning approach.
2) Sample selection and combination: with la-
beled training data and unlabeled click data, how
1086
Generic training data
Dedicated training data
GBrank algorithm
Task-specific ranking model
Generic click data
Dedicated click data
Sample selection and combination
Click log Click data mining? Heuristic-rule-based approach
? Sequential supervised learning approach
Figure 1: Framework of incorporating click-
through data with training data to improve dedi-
cated model for task-specific ranking.
to select and combine them so that the samples
have the best utility for learning? As the data
distribution for a specific ranking task is differ-
ent from the generic data distribution, it is nat-
ural to select those labeled training samples and
unlabeled click preference pairs which belong to
this query category, so that the data distributions
of training set and testing set are consistent for
this category. On the other hand, we should keep
in mind that: a) non-dedicated data, i.e, the data
that does not belong the specific category, might
also have similar distribution as the dedicated data.
Such distribution similarity makes non-dedicated
data also useful for task-specific rank function
learning, especially for the scenario that dedicated
training samples is insufficient. b) The quality of
dedicated click data may be not as reliable as hu-
man labeled training data. In other words, there
are some extracted click preference pairs that are
inconsistent with human labeling while we regard
human labeling as correct labeling.
3) Rank function learning algorithm: we use
GBrank (Zheng et al, 2007) algorithm for rank
function learning, which has proved to be one
of the most effective up-to-date learning-to-rank
algorithms; furthermore, GBrank algorithm also
takes preference pairs as inputs, which will be il-
lustrated with more details in the paper.
2 Related work
Learning to rank has been a promising research
area which continuously improves web search rel-
evance (Burges et al, 2005) (Zha et al, 2006)
(Cao et al, 2007) (Freund et al, 1998) (Fried-
man, 2001) (Joachims, 2002) (Wang and Zhai,
2007) (Zheng et al, 2007). The ranking prob-
lem is usually formulated as learning a ranking
function from preference data. The basic idea
is to minimize the number of contradicted pairs
in the training data, and different algorithm cast
the preference learning problem from different
point of view, for example, RankSVM (Joachims,
2002) uses support vector machines; RankBoost
(Freund et al, 1998) applies the idea of boost-
ing from weak learners; GBrank (Zheng et al,
2007) uses gradient boosting with decision tree;
RankNet (Burges et al, 2005) uses gradient boost-
ing with neural net-work. In (Zha et al, 2006),
query difference is taken into consideration for
learning effective retrieval function, which leads
to a multi-task learning problem using risk mini-
mization framework.
There are a few related works to apply multi-
ple ranking models for different query categories.
However, none of them takes click-through infor-
mation into consideration. In (Kang and Kim,
2003), queries are categorized into 3 types, infor-
mational, navigational and transactional, and dif-
ferent models are applied on each query category.
a KNN method is proposed to employ different
ranking models to handle different types of queries
(Geng et al, 2008). The KNN method is unsuper-
vised, and it targets to improve the overall ranking
instead of the rank-ing for a certain query cate-
gory. In addition, the KNN method requires all
feature vector to be the same.
Quite a few research papers explore how to ob-
tain useful information from click-through data,
which could benefit search relevance (Carterette
et al, 2008) (Fox et al, 2005) (Radlinski and
Joachims, 2007) (Wang and Zhai, 2007). The in-
formation can be expressed as pair-wise prefer-
ences (Chapelle and Zhang, 2009) (Ji et al, 2009)
(Radlinski et al, 2008), or represented as rank fea-
tures (Agichtein et al, 2006). Task-specific rank-
ing relies on the accuracy of query classification.
Query classification or query intention identifica-
tion has been extensively studied in (Beitzel et al,
2007) (Lee et al, 2005) (Li et al, 2008) (Rose and
Levinson, 2004). How to combine editorial data
and click data is well discussed in (Chen et al,
2008) (Zheng et al, 2007). In addition, how to use
click data to improve ranking are also exploited
in personalized or preference-based search (Coyle
1087
Table 1: Statistics of click occurrences for heuris-
tic rule approach.
imp impression, number of occurrence of the tuple
cc number of occurrence of the tuple where two
documents both get clicked
ncc number of occurrence of the tuple where url
1
is not clicked but url
2
is clicked
cnc number of occurrence of the tuple where url
1
is clicked but url
2
is not clicked
ncnc number of occurrence of the tuple where url
1
and url
2
are not clicked
and Smyth, 2007) (Glance, 2001) (R. Jin, 2008).
3 Technical approach
This section presents the related approaches in
Figure 1. In Section 4, we will make deeper anal-
ysis based on experimental results.
3.1 Click data mining
We use two approaches for click data mining,
whose outputs are preference pairs. A preference
pair is defined as a tuple {< x
q
, y
q
> |x
q
? y
q
},
which means for the query q, the document x
q
is
more relevant than y
q
. We need to extract infor-
mative and reliable preference pairs which can be
used to improve rank function learning.
3.1.1 Heuristic rule approach
We use heuristic rules to extract skip-above pairs
and skip-next pairs, which are similar to Strategy
1 (click > skip above) and Strategy 5 (click > no-
click next) proposed in (Joachims et al, 2005). To
reduce the misleading effect of an individual click
behavior, click information from different query
sessions is aggregated before applying heuristic
rules. For a tuple (q, url
1
, url
2
, pos
1
, pos
2
) where
q is query, url
1
and url
2
are urls representing two
documents, pos
1
and pos
2
are ranking positions
for the two documents with pos
1
? pos
2
mean-
ing url
1
has higher rank than url
2
, the statistics for
this tuple are listed in Table 1.
Skip-above pair extraction: if ncc is much
larger than cnc, and
cc
imp
,
ncnc
imp
is much smaller
than 1, that means, when url
1
is ranked higher than
url
2
in query q, most users click url
2
but not click
url
1
. In this case, we extract a skip-above pair, i.e.,
url
2
is more relevant than url
1
. In order to have
highly accurate skip-above pairs, a set of thresh-
Table 2: Skip-above pairs count vs. human judge-
ments (e.g., the element in the third row and sec-
ond column means we have 40 skip-above pairs
with ?excellent? url
1
and ?perfect? url
2
). P: per-
fect; E: excellent; G: good; F: fair; B: bad.
P E G F B
P 13 13 12 4 0
E 40 44 16 2 2
G 27 53 103 29 8
F 10 15 43 27 5
B 4 4 11 20 14
Table 3: Skip-next pairs vs. human judgements
(e.g., the element in the third row and second col-
umn means we have 10 skip-next pairs with ?ex-
cellent? url
1
and ?perfect? url
2
). P: perfect; E:
excellent; G: good; F: fair; B: bad.
P E G F B
P 126 343 225 100 35
E 10 71 84 37 12
G 6 9 116 56 21
F 1 5 17 29 14
B 1 1 1 2 5
olds are applied to only extract the pairs that have
high impression and ncc is larger enough than cnc.
Skip-next pair extraction: if pos
1
= pos
2
? 1,
cnc is much larger than ncc, and
cc
imp
,
ncnc
imp
is much
smaller than 1, that means, in most of cases when
url
2
is ranked just below url
1
in query q, most
users click url
1
but not click url
2
. In this case, we
regard this tuple as a skip-next pair.
To test the accuracy of preference pairs, we
ask editors to judge some randomly selected pairs
from skip-above pairs and skip-next pairs. Edi-
tors label each query-url pair using five grades ac-
cording to relevance: perfect, excellent, good, fair,
bad. Table 2 shows skip-above pair distribution.
The diagonal elements have high values, which
are for tied pairs labeled by editors but determined
as skip-above pairs from heuristic rules. Higher
values appear in the left-bottom triangle than in
the right-top triangle, because there are more skip-
above preferences agreed with editors than dis-
agreed with editors. Summing up the tied pairs,
agreed and disagreed pairs, 44% skip-above pref-
erence judgments agree with editors, 18% skip-
above preference judgments disagree with editors,
1088
and there are 38% skip-above pairs judged as tie
pairs by editors.
Table 3 shows skip-next pair distribution. Sum-
ming up the tied pairs, agreed and disagreed pairs,
70% skip-next preference judgments agree with
editors, 4% skip-next preference judgments dis-
agree with editors, and 26% skip-next pairs judged
as tie pairs by editors.
Therefore, skip-next pairs have much higher
accuracy than skip-above. That is because in a
search engine that already has a good ranking
function, it is much easier to find a correct skip-
next pairs which are consistent with the search en-
gine than to find a correct skip-above pairs which
are contradictory to the search engine. Skip-above
and skip-next preferences provide us two kinds of
users?s feedbacks which are complementary: skip-
above preferences provide us the feedback that the
user?s vote is contradictory to the current ranking,
which implies the current relative ranking should
be reversed; skip-next preferences shows that the
user?s vote is consistent with the current ranking,
which implies the current relative ranking should
be maintained with high confidence provided by
users? vote.
3.1.2 Sequential supervised learning
The click modeling by sequential supervised
learning (SSL) was proposed in (Ji et al, 2009),
in which user?s sequential click information is
exploited to extract relevance information from
click-logs. This approach is reliable because 1)
the sequential click information embedded in an
aggregation of user clicks provides substantial rel-
evance information of the documents displayed in
the search results, and 2) the SSL is supervised
learning (i.e., human judgments are provided with
relevance labels for the training).
The SSL is formulated in the framework
of global ranking (Qin et al, 2008). Let
x
(q)
= {x
(q)
1
, x
(q)
2
, . . . , x
(q)
n
} represent the doc-
uments retrieved with a query q, and y
(q)
=
{y
(q)
1
, y
(q)
2
, . . . , y
(q)
n
} represent the relevance la-
bels assigned to the documents. Here n is the
number of documents retrieved with q. Without
loss of generality, we assume that n is fixed and
invariant with respect to different queries. The
SSL determines to find a function F in the form
of y
(q)
=F (x
(q)
) that takes all the documents as
its inputs, exploiting both local and global infor-
mation among the documents, and predict the rel-
evance labels of all the document jointly. This
is distinct to most of learning to rank methods
that optimize a ranking model defined on a sin-
gle document, i.e., in the form of y
(q)
i
=f(x
(q)
i
),
? i = 1, 2, . . . , n. This formulation of the SSL
is important in extracting relevance information
from user click data since users? click decisions
among different documents displayed in a search
session tend to rely not only on the relevance judg-
ment of a single document, but also on the relative
relevance comparison among the documents dis-
played; and the global ranking framework is well-
formulated to exploit both local and global infor-
mation from an aggregation of user clicks.
The SSL aggregates all the user sessions for
the same query into a tuple <query, n-document
list, and an aggregation of user clicks>. Fig-
ure 2 illustrates the process of feature extrac-
tion from an aggregated session, where x
(q)
=
{x
(q)
1
, x
(q)
2
, . . . , x
(q)
n
} denotes a sequence of fea-
ture vectors extracted from the aggregated ses-
sion, with x
(q)
i
representing the feature vector ex-
tracted for document i. Specifically, to form fea-
ture vector x
(q)
i
, first a feature vector x
(q)
i,j
is ex-
tracted from each user j?s click information, and
j ? {1, 2, . . . }, then x
(q)
i
is formed by averaging
over x
(q)
i,j
, ?j ? {1, 2, . . . }, i.e., x
(q)
i
is actually an
aggregated feature vector for document i. Table
4 lists all the features used in the SSL modeling.
Note that some features are statistics independent
of temporal information of the clicks, such as ?Po-
sition? and ?Frequency?, while other features re-
ply on their surrounding documents and the click
sequences. We use 90,000 query-url pairs to train
the SSL model, and 10,000 query-url pairs for best
model selection.
With the sequential click modeling discussed
above, several sequential supervised algorithms,
including the conditional random fields (CRF)
(Lafferty et al, 2001), the sliding window method
and the recurrent sliding window method (Diet-
terich, 2002), are explored to find a global ranking
function F . We omit the details but refer one to
(Ji et al, 2009). The emphasis here is on the im-
portance to adapt these algorithms to the ranking
problem.
After training, the SSL model can be used to
predict the relevance labels of all the documents in
a new aggregated session, and thus pair-wise pref-
erence data can be extracted, with the score dif-
ference representing the confidence of preference
1089
={
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 }
?
o
o
user2
doc10
odoci
oodoc2
odoc1 user1
q
?
?
Feature Extraction
x
(q)
x1
x2
xi
x10
?
?
={
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 }
y
(q)
y1
y2
yi
y10
?
?
Figure 2: An illustration of feature extraction for
an aggregated session for SSL approach. x
(q)
de-
notes an extracted sequence of feature vectors, and
y
(q)
denotes the corresponding label sequence that
is assigned by human judges for training.
Table 4: Click features used in SSL model.
Position Position of the document
in the result list
ClickRank Rank of 1st click of doc. in click seq.
Frequency Average number of clicks for this doc.
FrequencyRank Rank in the list sorted by num. of clicks
IsNextClicked 1 if next position is clicked, 0 otherwise
IsPreClicked 1 if previous position is clicked,
0 otherwise
IsAboveClicked 1 if there is a click above, 0 otherwise
IsBelowClicked 1 if there is a click below, 0 otherwise
ClickDuration Time spent on the document
prediction. For the reason of convenience, we also
call the preference pairs contradicting with pro-
duction ranking as skip-above pairs and those con-
sistent with production ranking as skip-next pairs,
so that we can analyze these two types of prefer-
ence pairs respectively.
3.2 Modeling algorithm
The basic idea of GBrank (Zheng et al, 2007)
is that if the ordering of a preference pair
by the ranking function is contradictory to this
preference, we need to modify the ranking
function along the direction by swapping this
prefence pair. Preferences pairs could be gen-
erated from labeled data, or could be extracted
from click data. For each preference pair <
x, y > in the available preference set S =
{< x
i
, y
i
> |x
i
? y
i
, i = 1, 2, ..., N}, x should
be ranked higher than y. In GBrank algorithm, the
problem of learning ranking functions is to com-
pute a ranking function h , so that h matches the
set of preference, i.e, h(x
i
) ? h(y
i
) , if x ? y,
i = 1, 2, ..., N as many as possible. The following
loss function is used to measure the risk of a given
ranking function h.
R(h) =
1
2
N
?
i=1
(max{0, h(y
i
)?h(x
i
)+?})
2
, (1)
where ? is the margin between the two documents
in the pair. To minimize the loss function, h(x) has
to be larger than h(y) with the margin ? , which can
be chosen as constant value, or as dynamic val-
ues varying with pairs. When pair-wise judgments
are extracted from editors? labels with different
grades, pair-wise judgments can include grade dif-
ference, which can further be used as margin ? .
The GBrank algorithm is illustrated in Algorithm
1, and two parameters need to be determined: the
shrinkage factor ? and the number of iteration.
Algorithm 1 GBrank algorithm.
Start with an initial guess h
0
, for m = 1, 2, ...
1. Construct a training set: for each < x
i
, y
i
>?
S, derive (x
i
,max{0, h
m?1
(y
i
) ? h
m
1
(x
i
) +
?}), and
(y
i
,?max{0, h
m?1
(y
i
)? h
m
1
(x
i
) + ?}).
2. Fit h
m
by using a base regressor with the
above training set.
3. h
m
= h
m?1
+?s
m
h
m
(x), where s
m
is found
by line search to minimize the object function,
? is shrinkage factor.
3.3 Sample selection and combination
We use a straightforward approach to learn rank-
ing model from the combined data, which is illus-
trated in Algorithm 2.
Algorithm 2 Learn ranking model by combining
editorial data and click preference pairs.
Input:
? Editorial absolute judgement data.
? Preference pairs from click data.
1. Extract preference pairs from labeled data
with absolute judgement.
2. Select and combine preference pairs from
click data and labeled data.
3. Learn GBrank model from the combined
preference pairs.
Absolute judgement on labeled data contains
(query, url) pairs with absolute grade values la-
beled by human. In Step 1, for each query with
1090
nq
query-url pairs with corresponding grades, {<
query, url
i
, grade
i
> |i = 1, 2, . . . , n
q
}, its prefer-
ence pairs are extracted as
{< query, url
i
, url
j
, grade
i
? grade
j
> |i, j =
1, 2, . . . , n
q
, i 6= j} .
When combining human-labeled pairs and click
preference pairs, we can give use different relative
weights for these two data sources. The loss func-
tion becomes
R(h) =
w
N
l
?
i?Labeled
(max{0, h(y
i
)? h(x
i
) + ?})
2
1? w
N
c
?
i?Click
(max{0, h(y
i
)? h(x
i
) + ?})
2
,(2)
where w is used to control the relative weights be-
tween labeled training data and click data, N
l
is
the number of training data pairs, and N
c
is the
number of click pairs. The margin ? can be deter-
mined as grade difference for editor pairs, and be
a constant parameter for click pairs.
Step 2 is critical for the efficacy of the approach.
A few factors need to be considered:
1) data distribution: for the application of task-
specific ranking, our purpose is to improve ranking
for the queries belonging to this category. An im-
portant observation is that the relevance patterns
for the ranking within a specific category may
have some unique characteristics, which are differ-
ent from generic relevance ranking. Thus, it is rea-
sonable to consider only using dedicated labeled
training data and dedicated click preference data
for training. The reality is that dedicated training
data is usually insufficient, while it is possible that
non-dedicated data can also help the learning.
2) click pair quality: it is inevitable there exist
some incorrect pairs in the click preference pairs.
Such incorrect pairs may mislead the learning. So
overall, can the click preference pairs still help the
learning for task-specific ranking? By our study,
skip-above pairs usually contain more incorrect
pairs compared with skip-above pairs. Does this
mean skip-next pairs are always more helpful in
improving learning than skip-above pairs?
3) click pair utility: use labeled training data as
baseline, how much complimentary information
can click pairs bring? This is determined by the
methodology of click data mining approach.
While it is possible to achieve some learning
improvement for task-specific ranking by using
click pairs by a plausible method, we attempt to
empirically explore the above interweaving fac-
tors for deeper understanding, in order to apply the
most appropriate strategy to exploit click data on
real-world applications of task-specific ranking.
4 Experiments
4.1 Data set
Query category: in the experiments, we use long
query ranking as an example of task-specific rank-
ing, because it is commonly known that long query
ranking has some unique relevance patterns com-
pared with generic ranking. We define the long
queries as the queries containing at least three to-
kens. The techniques and analysis proposed in this
paper can be applied to other ranking tasks, such
as rankings for specific query segments like time-
sensitive queries, navigational queries, or rankings
for specific domains/contents like answers, blogs,
news, as long as the tasks have their own charac-
teristics of data distributions and discriminant rank
features.
Labeled training data: we do experiments
based on a data set for a commercial search en-
gine, for which there are 16,797 query-url pairs
(with 1,123 different queries) that have been la-
beled by editors. The proportion of long queries
is about 35% of all queries. The data distribution
of such long queries may be different from gen-
eral data distribution, as it will be validated in the
experiments below.
The human labeled data is randomly split into
two sets: training set (8,831 query-url pairs, 589
queries), and testing set (7,966 query-url pairs,
534 queries). The training set will be combined
with click preference pairs for rank function learn-
ing, and the testing set will be used to evaluate the
efficacy of the ranking function. In the training set,
there are 3,842 long query-url pairs (229 queries).
At testing stage, the learned rank functions are ap-
plied only to the long queries in the testing data,
as our concern in this paper is how to improve
task-specific ranking, i.e., long query ranking in
the experiment. In the testing data, there are 3,210
query-url pairs (193 queries) are long query data,
which will be used to test rank functions.
Click preference pairs: using the two ap-
proaches of heuristic rule approach and sequen-
tial supervised approach, we extract click prefence
pairs from the click log of the search engine. Each
approach yields both skip-next and skip-above
pairs, which are sorted by confidence descending
order respectively.
1091
Table 5: Use click data by heuristic rule approach
(Data Selection: ?N?: not use; ?D?: use dedicated
data; ?G?: use generic data. Data Source: ?T?:
training data; ?C?: click data)
(a) skip-next pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.7822 0.7906 (1.2%) 0.7997(2.4%)
GC 0.7834 0.7908 (1.2%) 0.7950 (1.7%)
(b) skip-above pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.6649 0.7676 (-1.6%) 0.7748 (-0.8%)
GC 0.6792 0.7656 (-2.0%) 0.7989 (2.2%)
4.2 Setup and measurements
We try different sample selection and combination
strategies to train rank functions using GBrank al-
gorithm. For the labeled training data, we either
use generic data or dedicated data. For the click
preference pairs, we also try these two options.
Furthermore, as more click preference pairs may
bring more useful information to help the learn-
ing while on the other hand, the more incorrect
pairs may be given so that they mislead the learn-
ing, we try different amounts of these prefence
pairs: 5,000, 10,000, 30,000, 50,000, 70,000 and
100,000 pairs.
We use NDCG to evaluate ranking model,
which is defined as
NDCG
n
= Z
n
?
n
i=1
2
r(i)
?1
log(i+1)
where i is the position in the document list, r(i) is
the score of Document i, and Z
n
is a normalization
factor, which is used to make the NDCG of ideal
list be 1.
4.3 Results
Table 5 and 6 show the NDCG
5
results by using
heuristic rule approach and SSL approach respec-
tively. We do not present NDCG
1
results due to
space limitation, but NDCG
1
results have the sim-
ilar trends as NDCG
5
.
Baseline by training data: there are two base-
line functions by using training data sets 1) use
dedicated training data (DT), NDCG
5
on the test-
ing set by the rank function is 0.7736; 2) use
generic training data (GT), NDCG
5
is 0.7813. It
is reasonable that using generic training data is
Table 6: Use click data by SSL approach (Data
Selection: ?N?: not use; ?D?: use dedicated data;
?G?: use generic data. Data Source: ?T?: training
data; ?C?: click data)
(a) skip-next pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.7752 0.7933 (1.5%) 0.7936 (1.5%)
GC 0.7624 0.7844 (0.4%) 0.7914 (1.2%)
(b) skip-above pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.6756 0.7636 (-2.2%) 0.7784 (-0.3%)
GC 0.6860 0.7717 (-1.2%) 0.7774 (-0.5%)
better than only using dedicated training data, be-
cause the distributions of non-dedicated data and
dedicated data share some similarity. As the ded-
icated training data is insufficient, the adoption of
the extra non-dedicated data helps the learning.
We compare learning results with Baseline 2) (use
generic training data, the slot of NC + GT in the
tables), which is the higher baseline.
Baseline by click data: we then study the utili-
ties of click preference pairs by using them alone
for training without using labeled training data.
In Table 5 and 6, each of the NDCG
5
results us-
ing click preference pairs is the highest NDCG
5
value over the cases of using different amounts of
pairs (5000, 10,000, 30,000, 50,000, 70,000 and
100,000 pairs). The results regarding the pairs
amounts are illustrated in Figure 3, which will help
us to analyze the results more deeply.
If we only use click preference pairs for training
(the two table slots DC+NT and GC+NT, corre-
sponding to using dedicated click preference pairs
and generic click pairs respectively), the best case
is using skip-next pairs extracted by heuristic rule
approach (Table 5 (a) ). It is not surprising that
skip-next pairs outperform skip-above pairs be-
cause there are significantly lower percentage of
incorrect pairs in skip-next pairs compared with
skip-above pairs. It is a little bit surprising that
the case of DC+NT has no dominant advantage
over GC+NT as we expected. For example, in Ta-
ble 5 (a), the NDCG
5
values (0.7822 and 0.7834)
are very close to each other. However, in Figure
3, we find that with the same amount of pairs,
when we use 30,000 or fewer pairs, using dedi-
1092
1 2 3 4 5 6 7 8 9 10
x 104
0.755
0.76
0.765
0.77
0.775
0.78
0.785
0.79
0.795
0.8
click pairs
ND
CG
5
 
 
dedicate train + dedicate clickgeneric train + dedicate clickdedicate clickgeneric click
Figure 3: Incorporate different amounts of skip-
next pairs by heuristic rule approach with generic
training data.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.76
0.77
0.78
0.79
0.8
0.81
trainging sample weight
ND
CG
5
 
 
+ 5,000 dedicate click
+ 10,000 dedicate click
+ 30,000 dedicate click
Figure 4: The effects of using different combin-
ing weights. Skip-next pairs by heuristic rule ap-
proach are combined with generic training data.
cated click pairs alone is always better than using
generic click pairs alone. With more click pairs
being used (> 30, 000), the noise rates become
higher in the pairs, which makes the distribution
factor less important.
Combine training data and click data: we
compare the four table slots, DC+DT, GC+DT,
DC+GT, GC+GT, in Table 5 and 6, and there are
quite a few interesting observations:
1) Skip-next vs. skip-above: overall, incorporat-
ing skip-next pairs with training data is better than
incorporating skip-above pairs, due to the reason
that there are more incorrect pairs in skip-above
pairs, which may mislead the learning. The only
exception is the slot GC+GT in Table 5 (b), whose
NDCG
5
improvement is as high as 2.2%. We fur-
ther track this result, and find that this is the case
by using only 5,000 generic skip-above pairs. The
noise rate of these 5,000 pairs is low because they
have the highest pair extraction confidence values.
At the same time, these 5,000 pairs may provide
good complementary signals to the generic train-
ing data, so that the learning result is good. How-
ever, in general, skip-next pairs have better utilities
than skip-above pairs.
2) Dedicated training data vs. generic train-
ing data: using generic training data is gen-
erally better than only using dedicated training
data. If training data is insufficient, the extra
non-dedicated data provides useful information
for relevance pattern learning, and the distribu-
tion dissimilarity between dedicated data and non-
dedicated data is not the most important factor.
3) Dedicated click data vs. generic click data:
using dedicated click data is more effective than
using generic click data. From Figure 3, we ob-
serve that when 30,000 or fewer pairs are incorpo-
rated into training data, using dedicate click pairs
is always better than using generic click pairs.
0 0.5 1 1.5 2 2.50.784
0.786
0.788
0.79
0.792
0.794
0.796
0.798
0.8
0.802
?
ND
CG
5
 
 
+ 5,000 dedicate click
+ 10,000 dedicate click
+ 30,000 dedicate click
Figure 5: The effects of using different margin
values for click preference pairs. Skip-next pairs
by heuristic rule approach are incorporated with
generic training data.
4) Heuristic rule approach vs. SSL approach:
the preference pairs extracted by heuristic rule ap-
proach have better utilities than those extracted by
SSL approach.
5) GBrank parameters for combining training
data and click pairs: the relative weight w for
combining training data and click pairs in (2) may
also affect rank function learning. Figure 4 shows
the effects of using different combining weights,
1093
for which skip-next pairs by heuristic rule ap-
proach are combined with generic training data.
We observe that neither over-weighting training
data or over-weighting click pairs yields good re-
sults while the two data sources are best exploited
at certain weight values when there is good bal-
ance between them. Another concern is the ap-
propriate margin value ? for the click pairs in (2).
Figure 5 shows that ? = 1 consistently yields good
learning results, which suggests us that click pair
provides good information at ? = 1.
4.4 Discussions
we have defactorized the related approaches for
exploiting click data to improve task-specific rank
learning. The utility of click preference pairs de-
pends on the following factors:
1) Data distribution: if click pairs have good
quality, we should use dedicated click pairs in-
stead of generic click pairs, so that the samples
for training have similar distribution to the task of
task-specific ranking.
2) The amount of dedicated training data: the
more dedicated training data, the more reliable the
task-specific rank function is; thus, the less room
for learning improvement using click data. For the
case in the experiment that dedicated training is in-
sufficient, the non-dedicated training data can also
help the learning as non-dedicated training data
share relevance pattern similarity with the dedi-
cated data distribution.
3) The quality of click pairs: if we can extract
large amount of high-quality click pairs, the learn-
ing improvement will be significant. For example,
as shown in Figure 3, at the early stage with fewer
click pairs (5,000 and 10,000 pairs) being com-
bined with training data, the learning improvement
is best. With more click pairs are used, the noise
rate in the click pairs becomes higher so that the
learning misleading factor is more important than
information complementary factor. Thus, it is im-
portant to improve the reliability of the click pairs.
4) The utility of click pairs: by our study, the
quality of click pairs extracted by SSL approach
is comparable to those extracted by heuristic rule
approach. The possible reason that heuristic-rule-
based click pairs can bring more benefit is that
these pairs provide more complementary infor-
mation compared with SSL approach. As the
methodologies of these two click data extraction
approaches are totally different, in future we will
explore the concrete reason that causes such utility
difference.
5 Conclusions
By empirically exploring the related factors in
utilizing click-through data to improve dedicated
model learning for task-specific ranking, we have
better understood the principles of using click
preference pairs appropriately, which is impor-
tant for the real-world applications in commer-
cial search engines as using click data can sig-
nificantly save human labeling costs and makes
rank function learning more efficient. In the case
that dedicated training data is limited, while non-
dedicated training data is helpful, using dedicated
skip-next pairs is the most effective way to further
improve the learning. Heuristic rule approach pro-
vides more useful click pairs compared with se-
quential supervised learning approach. The qual-
ity of click pairs is critical for the efficacy of the
approach. Therefore, an interesting topic is how
to further reduce the inconsistency between skip-
above pairs and human labeling so that such data
may also be useful for task-specific ranking.
1094
References
E. Agichtein, E. Brill, and S. Dumais. 2006. Improv-
ing web search ranking by incorporating user behav-
ior information. Proc. of ACM SIGIR Conference.
S. M. Beitzel, E. C. Jensen, A. Chowdhury, and
O. Frieder. 2007. Varying approaches to topical
web query classification. Proceedings of ACM SI-
GIR conference.
C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005.
Learning to rank using gradient descent. Proc. of
Intl. Conf. on Machine Learning.
Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. 2007.
Learning to rank: From pairwise approach to list-
wise. Proceedings of ICML conference.
B. Carterette, P. N. Bennett, D. M. Chickering, and S. T.
Dumais. 2008. Here or there: preference judgments
for relevance. Proc. of ECIR.
O. Chapelle and Y. Zhang. 2009. A dynamic bayesian
network click model for web search ranking. Pro-
ceedings of the 18th International World Wide Web
Conference.
K. Chen, Y. Zhang, Z. Zheng, H. Zha, and G. Sun.
2008. Adapting ranking functions to user prefer-
ence. ICDE Workshops, pages 580?587.
M. Coyle and B. Smyth. 2007. Supporting intelligent
web search. ACM Transaction Internet Tech., 7(4).
T. G. Dietterich. 2002. Machine learning for sequen-
tial data: a review. Lecture Notes in Computer Sci-
ence, (2396):15?30.
S. Fox, K. Karnawat, M. Mydland, S. Dumias, and
T. White. 2005. Evaluating implicit measures to
improve web search. ACM Trans. on Information
Systems, 23(2):147?168.
Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer.
1998. An efficient boosting algorithm for combin-
ing preferences. Proceedings of International Con-
ference on Machine Learning.
J. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist., 29:1189?
1232.
X. Geng, T. Liu, T. Qin, A. Arnold, H. Li, and H. Shum.
2008. Query dependent ranking with k nearest
neighbor. Proceedings of ACM SIGIR Conference.
N. S. Glance. 2001. Community search assistant. In-
telligent User Interfaces, pages 91?96.
S. Ji, K. Zhou, C. Liao, Z. Zheng, G. Xue, O. Chapelle,
G. Sun, and H. Zha. 2009. Global ranking by ex-
ploiting user clicks. In SIGIR?09, Boston, USA, July
19-23.
T. Joachims, L. Granka, B. Pan, and G Gay. 2005.
Accurately interpreting clickthough data as implicit
feedback. Proc. of ACM SIGIR Conference.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
I. Kang and G. Kim. 2003. Query type classification
for web document retrieval. Proceedings of ACM
SIGIR Conference.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML, pages
282?289.
U. Lee, Z. Liu, and J. Cho. 2005. Automatic identifi-
cation of user goals in web search. Proceedings of
International Conference on World Wide Web.
X. Li, Y.Y Wang, and A. Acero. 2008. Learning query
intent from regularized click graphs. Proceedings of
ACM SIGIR Conference.
T. Y Liu. 2008. Learning to rank for information re-
trieval. SIGIR tutorial.
T. Qin, T. Liu, X. Zhang, D. Wang, and H. Li. 2008.
Global ranking using continuous conditional ran-
dom fields. In NIPS.
H. Li R. Jin, H. Valizadegan. 2008. Ranking re-
finement and its application to information retrieval.
Proceedings of International Conference on World
Wide Web.
F. Radlinski and T. Joachims. 2007. Active exploration
for learning rankings from clickthrough data. Proc.
of ACM SIGKDD Conference.
F. Radlinski, M. Kurup, and T. Joachims. 2008. How
does clickthrough data reflect retrieval quality? Pro-
ceedings of ACM CIKM Conference.
D. E. Rose and D. Levinson. 2004. Understanding user
goals in web search. Proceedings of International
Conference on World Wide Web.
X. Wang and C. Zhai. 2007. Learn from web search
logs to organize search results. In Proceedings of
the 30th ACM SIGIR.
H. Zha, Z. Zheng, H. Fu, and G. Sun. 2006. Incor-
porating query difference for learning retrieval func-
tions in world wide web search. Proceedings of the
15th ACM Conference on Information and Knowl-
edge Management.
Z. Zheng, H. Zhang, T. Zhang, O. Chapelle, K. Chen,
and G. Sun. 2007. A general boosting method and
its application to learning ranking functions for web
search. NIPS.
1095
Using Gene Expression Programming to Construct Sentence Ranking 
Functions for Text Summarization  
Zhuli Xie, Xin Li, Barbara Di Eugenio, 
Peter C. Nelson 
Department of Computer Science 
University of Illinois at Chicago 
Chicago, IL 60607, U.S.A. 
zxie@cs.uic.edu, xli1@cs.uic.edu,  
bdieugen@cs.uic.edu, nelson@cs.uic.edu 
Weimin Xiao, Thomas M. Tirpak 
Physical Realization Research Center of 
Motorola Labs 
Schaumburg, IL 60196, U.S.A. 
awx003@motorola.com, 
T.Tirpak@motorola.com 
  
Abstract 
In this paper, we consider the automatic text 
summarization as a challenging task of ma-
chine learning. We proposed a novel summari-
zation system architecture which employs 
Gene Expression Programming technique as its 
learning mechanism. The preliminary experi-
mental results have shown that our prototype 
system outperforms the baseline systems. 
1 Introduction 
Automatic text summarization has been studied for 
decades (Edmundson 1969) and is still a very active 
area (Salton et al 1994; Kupiec et al 1995; Bran-
dow et al 1995; Lin 1999; Aone et al 1999; Sekine 
and Nobata 2001; Mani 2001; McKeown et al 2001; 
Radev et al 2003). Only a few have tried using ma-
chine learning to accomplish this difficult task (Lin 
1999; Aone et al 1999; Neto et al 2002). Most re-
search falls into combining statistical methods with 
linguistic analysis. We regard the summarization as 
a problem of empowering a machine to learn from 
human-summarized text documents. We employ an 
evolutionary algorithm, Gene Expression Program-
ming (GEP) (Ferreira 2001), as the learning mecha-
nism in our Adaptive Text Summarization (ATS) 
system to learn sentence ranking functions. Even 
though our system generates extractive summaries, 
the sentence ranking function in use differentiates 
ours from that of (Edmundson 1969; Sekine and 
Nobata. 1999; Goldstein et al 1999) who specified it 
to be a linear function of sentence features.  We used 
GEP to generate a sentence ranking function from 
the training data and applied it to the test data, which 
also differs from (Lin 1999) who used decision tree, 
(Aone et al 1999; Kupiec et al 1995) who used 
Bayes?s rule, and (Neto et al 2002) who imple-
mented both Na?ve Bayes and decision tree C4.5. 
This paper presents our approach, details the sys-
tem architecture, and discusses preliminary experi-
mental results. Conclusions and future work are 
outlined at the end. 
2 Background 
2.1 Gene Expression Programming 
Gene Expression Programming (GEP), first intro-
duced by (Ferreira 2001), is an evolutionary algo-
rithm that evolves computer programs and predicts 
mathematical models from experimental data. The 
algorithm is similar to Genetic Programming (GP), 
but uses fixed-length character strings (called chro-
mosomes) to represent computer programs which are 
afterwards expressed as expression trees (ETs). GEP 
begins with a random population of candidate 
solutions in the form of chromosomes. The 
chromosomes are then mapped into ETs, evaluated 
based on a fitness function and selected by fitness to 
reproduce with modification via genetic operations. 
The new generation of solutions goes through the 
same process until the stop condition is satisfied. 
The fittest individual serves as the final solution. 
GEP has been used to solve symbolic regression, 
sequence induction, and classification problems effi-
ciently (Ferreira 2002; Zhou 2003). We utilized GEP 
to find the explicit form of sentence ranking func-
tions for the automatic text summarization. 
2.2 Sentence Features 
In our current system, every sentence s is repre-
sented by five normalized features: 
? Location of the Paragraph (P): 
MYP /=                               (1) 
where M is the total number of paragraphs in a 
document; Y is the index of the paragraph s belongs 
to. 
? Location of the Sentence (S): 
NXS /=                        (2) 
where N is the total number of sentences in the 
paragraph; X is the index of sentence s.   
? Length of the Sentence (L): 
The length of the sentence is the number of words it 
contained, i.e., l(s), normalized by Sigmoid function: 
   
))((
))(()(
,
1
1
slstd
slsl
e
eL
???
? ?=+
?= ?
?
                               (3) 
Where u(l(s)) is the average length of sentences, and 
std(l(s)) is the standard deviation of the sentence 
lengths. 
? Heading Sentence (H): 
H = 1, if s is a title, subtitle or heading, 0 otherwise.   
? Content-word Frequencies (F): 
))((
))(()(,
1
1)(
sCWstd
sCWsCW
e
esF ???
? ?=+
?= ?
?
         (4) 
?
=
??=
k
i
ii swwFreqsCW
1
.)],(log[)(                         (5)                              
where Freq(wi) is the frequency of wi in that docu-
ment; ?(CW(S)) is the mean of all the sentence 
scores, and std(CW(s)) is the standard deviation.  
2.3 Sentence ranking function 
We assume that for a certain type of documents, the 
mechanism to perform summarization would be the 
same. Therefore, we only need to find one algorithm 
that links a collection of documents and their corre-
sponding summaries. We process the text summari-
zation learning task in two stages: training and 
testing.  In the training stage, a set of training docu-
ments with their summaries are provided, and the 
text features are preprocessed using statistical meth-
ods and natural language processing methods as de-
fined in 2.2, then each sentence in a document is 
scored based on a sentence ranking function con-
structed by GEP.  Fitness value of the summariza-
tion task is the similarity between the summary 
produced by the machine and the summarization text 
of training document. The top n ranked sentences1 
                                                          
1 The number of sentences extracted by the GEP module can be 
a variable, which is decided by the required number of words in 
a summary. Or it can be a specified percentage of the total num-
ber of sentences in the document. 
will be returned as the summary of that document 
and presented in their nature order. In the testing 
stage, a different document set is supplied to test the 
similarity between the machine summarized text and 
the human or other system summarized text.   
3 System Architecture 
In addition to the traditional way of extracting the 
highest ranked sentences in a document to compose 
a summary as in (Edmundson 1969; Lin 1999; 
Kupiec et al 1995; Brandow 1995; Zechner 1996), 
we embedded a machine learning mechanism in our 
system. The system architecture is shown in Figure 1 
where the GEP module is highlighted. In the training 
stage, each of the training documents is passed to the 
GEP module after being preprocessed into a set of 
sentence feature vectors. The GEP runs m genera-
tions, and in each generation a population of p sen-
tence scoring functions in the form of chromosomes 
in GEP is generated. Every candidate scoring func-
tion is then applied to sentence feature vectors from 
every training document and produces a score ac-
cordingly.  Then all sentences in the same training 
document are ranked according to their scores, and n 
sentences with top scores are selected as an extract.  
The next step is to measure how similar the extract 
is to the objective summary. As discussed by 
(McLellan et al 2001; Goldstein et al 1999; McKe-
own et al 2001), evaluating the quality of a sum-
mary often requires involvement of human subjects.  
This is almost impractical in a machine learning 
procedure.  Thus we chose an alternative similarity 
measure as the approximation, i.e. a cosine function 
that is often seen in Information Retrieval to calcu-
late the relevance of two documents, to compute the 
similarity between an extract and the objective 
summary. We compute the similarity values for each 
of the obtained extracts and their objective summa-
ries respectively, and feed the results into the Fitness 
Calculation module to get a fitness measure for the 
current candidate sentence ranking function under 
consideration: 
)),(( ii OESimilarityAvgFitness = ,                   (6) 
where Ei is the extract of the i-th document in the 
training set and Oi is its objective summary.  
After the fitness value for every chromosome in 
the current generation is computed, the GEP popula-
tion undergoes all genetic operators to produce the 
next generation. After the specified number of gen-
erations has been reached, the final best chromo-
some is returned as an optimal sentence ranking 
function for the training set and is ready to use in a 
test document to produce an extractive summary.  
4 Experiments 
We randomly selected 60 documents from the 
CMPLG corpus2 for our experiments. The only re-
striction is that each document has an abstract pro-
vided which will serve as the objective summary. 
Among these 60 documents, 50 are used for training 
and the remaining 10 are used for testing. The func-
tion set for the GEP to evolve sentence ranking func-
tions includes (+, -, *, /, power, sqrt, exp, log, min, 
max, and constant 1, 2, 3, 5, 7). The length of the 
chromosome is 128.  Other GEP control parameters 
are set as follows: population, 256; probability of 
crossover, 0.5; probability of mutation, 0.2; prob-
ability of rotation, 0.2; generations, 10,000-50,000 
(in five runs).  Our system has produced a five-
sentence extractive summary for each of the testing 
documents, and calculated the similarity between the 
produced summary and the abstract coming along 
with the document.  
  Ideally, we would like to compare our system with 
other summarizers. However, due to the 
unavailability of other summarization systems to 
perform the same task, we designed three baseline 
methods, namely lead-based, randomly-selected, and 
random-lead-based, to generate summaries for per-
formance comparison, which were also adopted by 
(Brandow et al 1995; Zechner 1996; Radev et al 
2003).  The baseline methods are detailed as 
 
                                                          
2 CMPLG corpus is composed of 183 documents from the Com-
putation and Language (cmp-lg) collection, which has been 
marked up in XML. The documents are scientific papers which 
appeared in association for Computational Linguistics (ACL) 
sponsored conferences. 
follows:  
o The lead-based method selects the first sen-
tences from the first five paragraphs as the 
summary of each of the testing documents.   
o The randomly-selected method chooses five sen-
tences from a document at random to compose a 
summary.  
o The random-lead-based method chooses five 
sentences among the first sentences from all 
paragraphs in the document at random.   
We performed the random selection 1,000 times, 
and calculated the average similarity of the testing 
documents for each of the random-based methods.  
The experimental results are plotted in Figure 2, 
which have demonstrated that our system outper-
forms all three baseline methods.   
Figure 2. Similarity Comparison
0.442
0.279
0.17 0.181
0
0.1
0.2
0.3
0.4
0.5
ATS Lead-based Rand-
selected
Rand-Lead-
based
Syste m
Si
m
ila
ri
ty
 
  One sample sentence scoring function learned by the 
GEP is as follows: 
)3(
7)( +
?=
PF
Ssscore ,                 (7). 
5 Conclusions and Future Work 
In this paper, we have presented a prototype summa-
rization system which employs GEP as its learning 
mechanism for sentence ranking function.  In the 
preliminary experiments for performance testing, 
our system outperforms the baseline methods by 
58%-160% when generating summaries for 10 
documents. However, the value of the average simi-
larity gained by our system is not as high as we 
would like. The reason most likely lies in the fact 
that the styles of the objective summaries written by 
humans vary a lot or even conflict with each other.  
In other words, they do not possess many common 
features that are a must for high value of similarity 
between two texts. Using content-words and the co-
sine function to measure the similarity may not be an 
ideal evaluation metric, neither is it an ideal fitness 
measure in the GEP learning mechanism.  Our future 
           Preprocess
Objective 
Summaries 
Training 
Documents 
GEP 
Generate Scor-
ing Function 
Fitness 
Calculation
Extracts
Test 
Document 
 Post-
process 
Summarizer 
Sentence Scor-
ing Function 
 Extractive  
 Summary 
Figure 1: System Architecture 
research will further study what kinds of similarity 
measure can be obtained from raw texts without in-
volvement of human subjects.  Moreover, we plan to 
cluster collected documents to make every cluster 
contains articles summarized in a similar style. We 
will also explore other sentence features, such as 
sentence cohesion, semantic meaning, and rhetorical 
relations, for an ideal uniform sentence ranking 
function. 
6 Acknowledgements 
Our thanks go to the Physical Realization Research 
Center of Motorola Labs for their support of this 
research project. 
References  
Aone, C., Gorlinsky, J., Larsen, B., and Okurowski, 
M. E. 1999. A Trainable Summarizer with Knowl-
edge Acquried from Robust NLP Techniques, Ad-
vances in Automatic Text Summarization, pages 
71-80. The MIT Press, Cambridge, Massachusetts. 
Brandow, R., Mitze, K., and Rau, L. F.  1995. 
Automatic condensation of electronic publications 
by sentence selection.  Information Processing 
and Management, 31(5):675-685. 
Edmundson, H. 1969. New methods in automatic 
abstracting. Journal of ACM, 16(2):264-285. 
Ferreira, C. 2001. Gene Expression Programming: A 
New Adaptive Algorithm for solving problems.  
Complex Systems, 13(2):87-129. 
Ferreira, C. 2002. Gene Expression Programming: 
Mathematical Modeling by an Artificial Intelli-
gence. Angra do Heroismo, Portugal 
Goldstein, J., Kantrowitz, M., Mittal, V., and Car-
bonell, J.  1999.  Summarizing Text Documents: 
Sentence Selection and Evaluation Metrics, in 
Proc. SIGIR ?99, pages 121-128.  Berkeley, Cali-
fornia. 
Kupiec, J., Pedersen, J., and Chen, F.  1995. A train-
able document summarizer. In Proc. 18th ACM-
SIGIR Conference, pages 68-73.  Seattle, Wash-
ington. 
Lin, C. 1999. Training a Selection Function for Ex-
traction. In the 8th International Conference on 
Information and Knowledge Management (CIKM 
99), Kansa City, Missouri. 
Mani, I.  2001.  Automatic Summarization, John 
Benjamins Publishing Company, Amster-
dam/Philadelphia. 
McKeown, K. R., Barzilay, R., Evans, D., Hatzivas-
siloglou, V., Kan, M. Y., Schiffman, B., Teufel, S. 
2001.  Columbia Multi-Document Summarization: 
Approach and Evaluation, in Proceedings of the 
Document Understanding Conference (DUC01).  
Edmonton, Canada. 
McLellan, P., Tombros, A., Jose, J., Ounis, I., and 
Whitehead, M. 2001. Evaluating summarisation 
technologies: A task-oriented approach.  In Proc. 
1st International Workshop on New Developments 
in Digital Libraries (NDDL-2001), International 
Conference on Enterprise Information Systems 
(ICEIS 2001), pages 99-112. Setubal, Portugal. 
Neto, J. L., Freitas, A. A., and Kaestner, C. A. A. 
2002. Automatic Text Summarization using a Ma-
chine Learning Approach. In Proc. 16th Brazilian 
Symp. on Artificial Intelligence (SBIA-2002). Lec-
ture Notes in Artificial Intelligence 2507, pp205-
215. Springer-Verlag. 
Radev, D. R., Teufel, S., Saggion, H., Lam, W., 
Blitzer, J., Qi, H., Celebi, A., Liu, D., and Drabek, 
E. 2003. Evaluation challenges in large-scale 
document summarization, in Proc. 41st Annual 
Meeting of the Association for Computational 
Linguistics, pages 375-382.  Sapporo, Japan. 
Salton, G., Allan, J., Buckley, C., and Singhal, A.  
1994. Automatic Analysis, Theme Generation, and 
Summarization of Machine-Readable Texts.  Sci-
ence, 264(3):1421-1426. 
Sekine, S. and Nobata, C. 2001. Sentence Extraction 
with Information Extraction technique. In Proc. of 
ACM SIGIR'01 Workshop on Text Summarization. 
New Orleans. 
Zechner, K. 1996. Fast generation of abstracts from 
general domain text corpora by extracting relevant 
sentences. In Proc. COLING-96, pages 986-989. 
Copenhagen, Denmark. 
Zhou, C., Xiao, W., Tirpak, T. M., and Nelson, P. C.  
2003. Evolving Classification Rules with Gene 
Expression Programming.  IEEE Transactions on 
Evolutionary Computation, 7(6):519 ? 531. 
Question Classification using Multiple Classifiers 
LI Xin 
Computer Science 
Engineering Dep. 
FUDAN Univ., Shanghai 
lixin@fudan.edu.cn
HUANG Xuan-Jing 
Computer Science 
Engineering Dep. 
FUDAN Univ., Shanghai 
xjhuang@fudan.edu.cn
WU Li-de
Computer Science 
Engineering Dep. 
FUDAN Univ., Shanghai 
ldwu@fudan.edu.cn
Abstract
The Open-domain Question Answering 
system (QA) has been attached great 
attention for its capacity of providing 
compact and precise results for Xsers.
The question classification is an 
essential part in the system, affecting 
the accuracy of it. The paper studies 
question classification through machine 
learning approaches, namely, different 
classifiers and multiple classifier 
combination method. By using 
compositive statistic and rule classifiers, 
and by introducing dependency 
structure from Minipar and linguistic 
knowledge from Wordnet into question 
representation, the research shows high 
accuracy in question classification.  
1 Introduction 
With the rapid development of the Internet, the 
capacity of textual information has been greatly 
improved. How to acquire accurate and effective 
information has become one of the great 
concerns among Internet users. Open-Domain 
Question Answering System (QA) has gain great 
popularities among scholars who care the above 
problem (Li, et al 2002; Moldovan, et al 2003; 
Zhang, et al 2003), for QA can meet users? 
demand by offering compact and accurate 
answers, rather than text with corresponding 
answers, to the questions presented in natural 
language. Therefore, it saves users? great trouble 
to find out specific facts or figures from large 
quantity of texts.  
The study of Question Classification (QC), as 
a new field, corresponds with the research of QA. 
QC is an essential part of QA, for to correctly 
answer users? questions, the system has to know 
what the users are looking for, and it is QC that 
presents important searching clues for the 
system. QC can be defined to match a question 
to one or several classes in K category so as to 
determine the answer type. Every class presents 
some semantic restrictions on the answer 
searching, which serves QA with various 
strategies in locating the correct answer.  
The result of QC can also serve QA in the 
answer selecting and extract, which influence the 
performance of QA directly. The first reason is 
that QC minish searching space. For example, if 
the system knows that the answer type to the 
question ?Who was the first astronaut to walk in 
space?? is a person?s name, it can confine the 
answer in the names, rather than every word in 
the texts. The second reason is that QC can 
determine the searching strategies and 
knowledge base QA may need. For instance, the 
question ?What county is California in?? needs 
the name of a country as its answer, so system 
needs the knowledge of countries? name and 
name entities tagging to identify and testify the 
place name, while the question ?What is 
Teflon?? expects an answer in a sentence or a 
fragment, in the form of Teflon is <?. >. In fact, 
almost all the QA have the QC module and QC 
is the one of the most important factors what 
determines the QA system performance 
(Moldovan, et al 2003).  
64
At present the studies on QC are mainly based 
on the text classification. Though QC is similar 
to TC in some aspects, they are clearly distinct 
in that ? Question is usually shorter, and 
contains less lexicon-based information than text, 
which brings great trouble to QC. Therefore to 
obtain higher classifying accuracy, QC has to 
make further analysis of sentences, namely QC 
has to extend interrogative sentence with 
syntactic and semantic knowledge, replacing or 
extending the vocabulary of the question with 
the semantic meaning of every words.  
In QC, many systems apply machine-learning 
approaches (Hovy, et al 2002; Ittycheriah, et al 
2000; Zhang, et al 2003). The classification is 
made according to the lexical, syntactic features 
and parts of speech. Machine learning approach 
is of great adaptability, and 90.0% of classifying 
accuracy is obtained with SVM method and tree 
Kernel as features. However, there is still the 
problem that the classifying result is affected by 
the accuracy of syntactic analyzer, which need 
manually to determine the weights of different 
classifying features. 
Some other systems adopting manual-rule 
method make QC, though may have high 
classifying accuracy, lack of adaptability, 
because regulation determination involves 
manual interference to solve the conflicts 
between regulations and to form orderly 
arranged rule base.  
The paper combines statistic and rule 
classifiers, specifically statistics preceding 
regulation, to classify questions. With rule 
classifier as supplementary to statistic, the 
advantages of respective classifier can be given 
full play to, and therefore the overall 
performance of the classifier combination will 
be better than the single one. Moreover, as far as 
the QC task is concerned, the paper compares 
various classifier combinations, statistic-rule 
classifier, voting? Adaboost and ANN. To 
represent questions, the paper uses dependency 
structure from Minipar (Lin 1998) and linguistic 
knowledge from Wordnet (Miller 1995; Miller, 
et al 2003). In the following parts of the paper, 
classifying method and features is first 
introduced, and then comparisons are made 
between different type features and between 
feature combination methods. The comparisons 
are testified in experiments. The last part of the 
paper is about the conclusion of the present 
research and about the introduction of the further 
work to be done on this issue.  
2 Classifying Features  
In machine learning method, every question 
should at first be transformed into a feature 
vector. Bag-of-word is one typical way of 
transforming questions, where every feature is 
one word in a corpus, whose value can be 
Boolean, showing whether the word is present in 
questions, and which can also be an integer or a 
real number, showing the presence frequency of 
the word. In this paper, every question is 
represented as a Boolean vector.  
1. Bag-of-word: all lexical items in questions are 
taken as classifying features, because stop-word 
such as ?what? and ?is? playing a critical role in 
QC.
2. Wordnet Synsets: Wordnet was conceived as 
a machine-readable dictionary. In Wordnet, 
word form is represented by word spellings, and 
the sense is expressed by Synsets, and every 
synset stands for a concept. Wordnet shows both 
lexical and semantic relationships. The former 
exists between word forms, while the latter 
exists between concepts. Among various 
semantic relations in Wordnet, we choose 
hypernyms between nouns as our only concern. 
The classifying features are the senses of the 
nouns in the sentences and synsets of their 
hypernyms.  
3. N-gram: the model is founded on a hypothesis 
that the presence of a word is only relevant to 
the n words before it. The frequently used are 
Bi-gram and Tri-gram, and Bi-gram is chosen as 
the classifying features in the present research. 
Compared with word, Bi-gram model 
investigates two historical records, and reflects 
the partial law of language. It embodies the 
features of word order, and therefore it can 
reflect the theme of the sentence more strongly. 
4. Dependency 6tructure: Minipar is a syntactic 
analyzer, which can analyze the dependency 
relation of words in sentences.  It describes the 
syntactic relationships between words in 
sentences. Such relation is direction-oriented, 
semantically rather than spatially, namely one 
word governs, or is governed by, another 
concerning their syntactic relation. In one 
sentence (W1W2?Wn), compared with 
Bi-gram, Dependency structure concerns 
65
WiWj ? but not need limitation j= i+1. 
Obviously, Dependency Relation goes further 
than Bi-gram in language understanding. 
Dependency structure is specified by a list of 
labeled tuples. The format of a labeled tuple is as 
follows:
label (word  pos  root  governor  rel  exinfo ?)
 ?Label? is a label assigned to the tuple. If the 
tuple represents a word in the sentence, label 
should be the index of the word in the sentence. 
?Word? is a word in the input sentence. ?Pos? is 
the part of speech. ?Root? is the root form. 
?Governor? if the label of the governor of word 
(if it has one), ?rel? is type of dependency 
relationship, and ?exinfo? for extra information. 
Minipar output is represented by the word 
dependency relationship via ?governor?. Though 
only 79% of recall and some word relations fail 
to be analyzed, the accuracy reaches 89%, which 
guarantees that a large proportion of dependency 
relations from the output are correct. And the 
experiment proves that Dependency structure 
has more classify precision than Bi-gram as 
classifying feature. 
For example, as to the question ?Which 
company created the Internet browser Mosaic??
Minipar may produce the following results:   
E0 (()       fin         C    *   ) 
1 (Which  ~  Det   2  det  (gov company)) 
2 (company  ~   N  E0  whn   (gov fin)) 
3 (created   create   V   E0  i     (gov fin)) 
E2 (()  company  N  3 subj  (gov create)
 (antecedent 2)) 
??
According to the tuple, we can get 
dependency relationships between words in 
sentences. tuple 1 (Which ~  Det 2  det
 gov company) shows us the det relationship 
between ?which? and ?company? in the sentence.  
Therefore, we can get a words-pair?which
company? , and likewise other five pairs of 
words can be obtained ??company create??
? the Mosaic ? ?  (Internet Mosaic) ?
(browser Mosaic) ?  (create Mosaic), which 
will be the item of vector represented the 
question.
3 Classifying Method Description 
3.1 Support Vector Machine (SVM) 
SVM is a kind of machine learning approach 
based on statistic learning theory. SVM are 
linear functions of the form f (x) = <w?x> +b, 
where <w?x> is the inner product between the 
weight vector w and the input vector x. The 
SVM can be used as a classifier by setting the 
class to 1 if f(x) > 0 and to -1 otherwise. The 
main idea of SVM is to select a hyperplane that 
separates the positive and negative examples 
while maximizing the minimum margin, where 
the margin for example xi is yi f(x) and yi ?
>-1,1] is the target output. This corresponds to 
minimizing <w?w> subject to yi (<w?x> +b) ?
for all i. Large margin classifiers are known to 
have good generalization properties. An 
adaptation of the LIBSVM implementation 
(Chang, et al 2001) is used in the following. 
Four type of kernel function linear, polynomial, 
radial basis function, and sigmoid are provided 
by LIBSVM . 
3.2 SVM-TBL QC Algorithm 
TBL has been a part of NLP since Eric Brill?s 
breakthrough paper in 1995(Brill 1995), which 
has been as effective as any other approach on 
the Part-of-Speech Tagging problem. TBL is a 
true machine learning technique. Given a tagged 
training corpus, it produces a sequence of rules 
that serves as a model of the training data. Then, 
to derive the appropriate tags, each rule may be 
applied, in order, to each instance in an untagged 
corpus.
TBL generates all of the potential rules that 
would make at least one tag in the training 
corpus correct. For each potential rule, its 
improvement score is defined to be the number 
of correct tags in the training corpus after 
applying the rule minus the number of correct 
tags in the training corpus before applying the 
rule. The potential rule with the highest 
improvement score is output as the next rule in 
the final model and applied to the entire training 
corpus. This process repeats (using the updated 
tags on the training corpus), producing one rule 
for each pass through the training corpus until no 
rule can be found with an improvement score 
that surpasses some predefined threshold. In 
66
practice, threshold values of 1 or 2 appear to be 
effective.
Therefore, we present compositive QC 
approach with rule and statistic learning. At first, 
questions are represented by Bag-of-word, 
Wordnet Synsets, Bi-gram, and Dependency 
structure, and are classified by the same samples 
and same SVM. Then output of SVM is 
transformed to the input of TBL, and thus every 
sample in TBL training data is featured by 
four-dimensioned vectors, from which a new is 
obtained as training data of TBL. When the 
errors produced in initial marking process are 
corrected in TBL to the greatest extent, a 
final-classifier is produced as follows (Figure1). 
  Figure1 SVM-TBL QC Algorithm 
TBL is composed of three parts: unannotated 
text, transformation templates, and objective 
function. In the experiment, unannotated text is 
obtained from SVM. The transformation 
templates define the space of transformations; 
here is combination of SVM output. Suppose we 
have k basic classifiers, and each classifier may 
put questions into N types, then we 
have rule templates. 
Objective function is the precision of classifier.  
kk
kkk NCNCNC  2211
4 Results and Analysis 
The research adopts the same UIUC data and 
classifying system as (Zhang, et al 2003) shows. 
There are about 5,500 labeled questions 
randomly divided into 5 training sets of sizes 
1,000, 2,000, 3,000, 4,000 and 5,500 
respectively. The testing set contains 500 
questions from the TREC10 QA track. Only 
coarse category is test. 
4.1 SVM Classifying Result 
We experiment the QC by SVM with four kernel 
function, and the following table (Table1) is the 
illustration of classifying accuracy by using 
single-kind classifying feature. 
It is shown that as to the four type features, no 
matter what Kernel is used, using Dependency 
relation feature have more precision than others 
and feature of Synsets is better than Bag-of-word. 
Therefore it is safe to draw the conclusion that 
Synsets and dependency relationship are helpful to 
represent questions. Among the four Kernel 
function, Liner has the best classifying precision. 
That is why we use Liner in the following 
experiment. 
    Num. of Training 
Kernel & feature 1000 2000 3000 4000 5500
Bag-of-word 79.6 81.2 83.4 85.8 84.8
Wordnet 77.8 83.8 85.2 86.4 86.8
Bi-gram 73.6 80.6 83.2 87.4 88.6Liner 
Dependency 82.0 86.8 87.2 88.4 89.2
Bag-of-word 52.4 69.2 66.0 61.4 62.6
Wordnet 48.4 69.8 70.0 68.8 73.2
Bi-gram 27.6 49.2 46.4 49.6 50.8polynomial 
Dependency 73.0 78.8 81.8 82.4 85.2
Bag-of-word 68.8 73.2 80.2 81.4 83.6
Wordnet 69.0 73.2 79.8 80.2 81.0
Bi-gram 62.2 70.2 76.0 80.0 81.2RBF
Dependency 72.8 78.8 81.0 83.2 85.0
Bag-of-word 65.6 74.2 77.0 78.2 80.2
Wordnet 74.2 82.6 83.4 83.8 84.4
Bi-gram 68.6 74.4 79.8 83.2 84.8Sigmoid 
Dependency 75.2 78.0 82.4 83.4 85.2
Table1. Four kernel function Question 
Classifying Accuracy (%) 
4.2 Result of SVM multi-kind-feature 
classification
Learning Question Classifiers
Xin Li Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
fxli1,danrg@uiuc.edu
Abstract
In order to respond correctly to a free form factual ques-
tion given a large collection of texts, one needs to un-
derstand the question to a level that allows determining
some of the constraints the question imposes on a pos-
sible answer. These constraints may include a semantic
classification of the sought after answer and may even
suggest using different strategies when looking for and
verifying a candidate answer.
This paper presents a machine learning approach to
question classification. We learn a hierarchical classi-
fier that is guided by a layered semantic hierarchy of an-
swer types, and eventually classifies questions into fine-
grained classes. We show accurate results on a large col-
lection of free-form questions used in TREC 10.
1 Introduction
Open-domain question answering (Lehnert, 1986;
Harabagiu et al, 2001; Light et al, 2001) and story
comprehension (Hirschman et al, 1999) have be-
come important directions in natural language pro-
cessing. Question answering is a retrieval task more
challenging than common search engine tasks be-
cause its purpose is to find an accurate and concise
answer to a question rather than a relevant docu-
ment. The difficulty is more acute in tasks such as
story comprehension in which the target text is less
likely to overlap with the text in the questions. For
this reason, advanced natural language techniques
rather than simple key term extraction are needed.
One of the important stages in this process is analyz-
ing the question to a degree that allows determining
the ?type? of the sought after answer. In the TREC
competition (Voorhees, 2000), participants are re-
quested to build a system which, given a set of En-
glish questions, can automatically extract answers
(a short phrase) of no more than 50 bytes from a
5-gigabyte document library. Participants have re-
 Research supported by NSF grants IIS-9801638 and ITR IIS-
0085836 and an ONR MURI Award.
alized that locating an answer accurately hinges on
first filtering out a wide range of candidates (Hovy
et al, 2001; Ittycheriah et al, 2001) based on some
categorization of answer types.
This work develops a machine learning approach
to question classification (QC) (Harabagiu et al,
2001; Hermjakob, 2001). Our goal is to categorize
questions into different semantic classes that impose
constraints on potential answers, so that they can
be utilized in later stages of the question answering
process. For example, when considering the ques-
tion Q: What Canadian city has the largest popula-
tion?, the hope is to classify this question as having
answer type city, implying that only candidate an-
swers that are cities need consideration.
Based on the SNoW learning architecture, we de-
velop a hierarchical classifier that is guided by a lay-
ered semantic hierarchy of answer types and is able
to classify questions into fine-grained classes. We
suggest that it is useful to consider this classifica-
tion task as a multi-label classification and find that
it is possible to achieve good classification results
(over 90%) despite the fact that the number of dif-
ferent labels used is fairly large, 50. We observe that
local features are not sufficient to support this accu-
racy, and that inducing semantic features is crucial
for good performance.
The paper is organized as follows: Sec. 2 presents
the question classification problem; Sec. 3 discusses
the learning issues involved in QC and presents our
learning approach; Sec. 4 describes our experimen-
tal study.
2 Question Classification
We define Question Classification(QC) here to be
the task that, given a question, maps it to one of
k classes, which provide a semantic constraint on
the sought-after answer1. The intension is that this
1We do not address questions like ?Do you have a light??,
which calls for an action, but rather only factual Wh-questions.
classification, potentially with other constraints on
the answer, will be used by a downstream process
which selects a correct answer from among several
candidates.
A question classification module in a question an-
swering system has two main requirements. First, it
provides constraints on the answer types that allow
further processing to precisely locate and verify the
answer. Second, it provides information that down-
stream processes may use in determining answer se-
lection strategies that may be answer type specific,
rather than uniform. For example, given the ques-
tion ?Who was the first woman killed in the Vietnam
War?? we do not want to test every noun phrase
in a document to see whether it provides an answer.
At the very least, we would like to know that the
target of this question is a person, thereby reducing
the space of possible answers significantly. The fol-
lowing examples, taken from the TREC 10 question
collection, exhibit several aspects of this point.
Q: What is a prism? Identifying that the target of this
question is a definition, strategies that are specific for
definitions (e.g., using predefined templates) may be use-
ful. Similarly, in:
Q: Why is the sun yellow? Identifying that this question
asks for a reason, may lead to using a specific strategy
for reasons.
The above examples indicate that, given that dif-
ferent answer types may be searched using different
strategies, a good classification module may help
the question answering task. Moreover, determin-
ing the specific semantic type of the answer could
also be beneficial in locating the answer and veri-
fying it. For example, in the next two questions,
knowing that the targets are a city or country will
be more useful than just knowing that they are loca-
tions.
Q: What Canadian city has the largest population?
Q: Which country gave New York the Statue of Liberty?
However, confined by the huge amount of man-
ual work needed for constructing a classifier for a
complicated taxonomy of questions, most question
answering systems can only perform a coarse clas-
sification for no more than 20 classes. As a result,
existing approaches, as in (Singhal et al, 2000),
have adopted a small set of simple answer entity
types, which consisted of the classes: Person, Loca-
tion, Organization, Date, Quantity, Duration, Lin-
ear Measure. The rules used in the classification
were of the following forms:
? If a query starts with Who or Whom: type Person.
? If a query starts with Where: type Location.
? If a query contains Which or What, the head noun
phrase determines the class, as for What X questions.
While the rules used have large coverage and rea-
sonable accuracy, they are not sufficient to support
fine-grained classification. One difficulty in sup-
porting fine-grained classification is the need to ex-
tract from the questions finer features that require
syntactic and semantic analysis of questions, and
possibly, many of them. The approach we adopted
is a multi-level learning approach: some of our fea-
tures rely on finer analysis of the questions that are
outcomes of learned classifiers; the QC module then
applies learning with these as input features.
2.1 Classification Standard
Earlier works have suggested various standards of
classifying questions. Wendy Lehnert?s conceptual
taxonomy (Lehnert, 1986), for example, proposes
about 13 conceptual classes including causal an-
tecedent, goal orientation, enablement, causal con-
sequent, verification, disjunctive, and so on. How-
ever, in the context of factual questions that are
of interest to us here, conceptual categories do not
seem to be helpful; instead, our goal is to se-
mantically classify questions, as in earlier work on
TREC (Singhal et al, 2000; Hovy et al, 2001;
Harabagiu et al, 2001; Ittycheriah et al, 2001).
The key difference, though, is that we attempt to
do that with a significantly finer taxonomy of an-
swer types; the hope is that with the semantic an-
swer types as input, one can easily locate answer
candidates, given a reasonably accurate named en-
tity recognizer for documents.
2.2 Question Hierarchy
We define a two-layered taxonomy, which repre-
sents a natural semantic classification for typical
answers in the TREC task. The hierarchy con-
tains 6 coarse classes (ABBREVIATION, ENTITY,
DESCRIPTION, HUMAN, LOCATION and NU-
MERIC VALUE) and 50 fine classes, Table 1 shows
the distribution of these classes in the 500 ques-
tions of TREC 10. Each coarse class contains a
non-overlapping set of fine classes. The motiva-
tion behind adding a level of coarse classes is that of
compatibility with previous work?s definitions, and
comprehensibility. We also hoped that a hierarchi-
cal classifier would have a performance advantage
over a multi-class classifier; this point, however is
not fully supported by our experiments.
Class # Class #
ABBREV. 9 description 7
abb 1 manner 2
exp 8 reason 6
ENTITY 94 HUMAN 65
animal 16 group 6
body 2 individual 55
color 10 title 1
creative 0 description 3
currency 6 LOCATION 81
dis.med. 2 city 18
event 2 country 3
food 4 mountain 3
instrument 1 other 50
lang 2 state 7
letter 0 NUMERIC 113
other 12 code 0
plant 5 count 9
product 4 date 47
religion 0 distance 16
sport 1 money 3
substance 15 order 0
symbol 0 other 12
technique 1 period 8
term 7 percent 3
vehicle 4 speed 6
word 0 temp 5
DESCRIPTION 138 size 0
definition 123 weight 4
Table 1: The distribution of 500 TREC 10 questions
over the question hierarchy. Coarse classes (in bold) are
followed by their fine class refinements.
2.3 The Ambiguity Problem
One difficulty in the question classification task is
that there is no completely clear boundary between
classes. Therefore, the classification of a specific
question can be quite ambiguous. Consider
1. What is bipolar disorder?
2. What do bats eat?
3. What is the PH scale?
Question 1 could belong to definition or dis-
ease medicine; Question 2 could belong to food,
plant or animal; And Question 3 could be a nu-
meric value or a definition. It is hard to catego-
rize those questions into one single class and it is
likely that mistakes will be introduced in the down-
stream process if we do so. To avoid this problem,
we allow our classifiers to assign multiple class la-
bels for a single question. This method is better than
only allowing one label because we can apply all the
classes in the later precessing steps without any loss.
3 Learning a Question Classifier
Using machine learning methods for question clas-
sification is advantageous over manual methods for
several reasons. The construction of a manual clas-
sifier for questions is a tedious task that requires
the analysis of a large number of questions. More-
over, mapping questions into fine classes requires
the use of lexical items (specific words) and there-
fore an explicit representation of the mapping may
be very large. On the other hand, in our learning
approach one can define only a small number of
?types? of features, which are then expanded in a
data-driven way to a potentially large number of fea-
tures (Cumby and Roth, 2000), relying on the abil-
ity of the learning process to handle it. It is hard to
imagine writing explicitly a classifier that depends
on thousands or more features. Finally, a learned
classifier is more flexible to reconstruct than a man-
ual one because it can be trained on a new taxonomy
in a very short time.
One way to exhibit the difficulty in manually con-
structing a classifier is to consider reformulations of
a question:
What tourist attractions are there in Reims?
What are the names of the tourist attractions in Reims?
What do most tourists visit in Reims?
What attracts tourists to Reims?
What is worth seeing in Reims?
All these reformulations target the same answer
type Location. However, different words and syn-
tactic structures make it difficult for a manual clas-
sifier based on a small set of rules to generalize well
and map all these to the same answer type. Good
learning methods with appropriate features, on the
other hand, may not suffer from the fact that the
number of potential features (derived from words
and syntactic structures) is so large and would gen-
eralize and classify these cases correctly.
3.1 A Hierarchical Classifier
Question classification is a multi-class classifica-
tion. A question can be mapped to one of 50 pos-
sible classes (We call the set of all possible class
labels for a given question a confusion set (Golding
and Roth, 1999)). Our learned classifier is based
on the SNoW learning architecture (Carlson et al,
1999; Roth, 1998)2 where, in order to allow the
classifier to output more than one class label, we
map the classifier?s output activation into a condi-
tional probability of the class labels and threshold
it.
The question classifier makes use of a sequence
of two simple classifiers (Even-Zohar and Roth,
2001), each utilizing the Winnow algorithm within
SNoW. The first classifies questions into coarse
classes (Coarse Classifier) and the second into fine
classes (Fine Classifier). A feature extractor auto-
matically extracts the same features for each clas-
sifier. The second classifier depends on the first in
2Freely available at http://L2R.cs.uiuc.edu/cogcomp/cc-
software.html
ABBR, ENTITY,DESC,HUMAN,LOC,NUM
ABBR, 
ENTITY
ENTITY,
HUMAN
ENTITY, 
LOC,NUM DESC
Coarse Classifier
Fine Classifier
abb,exp ind, plant date
abb, animal, 
food, plant?
food,plant, 
ind,group? 
food, plant, 
city, state?
definition,
reason,?
Map coarse classes 
to fine classes
C0
C1
C2
C3 abb,def animal,food
all possible subsets 
of C0 wih size <= 5
all possible subsets 
of C2 with size <=5
Figure 1: The hierarchical classifier
that its candidate labels are generated by expanding
the set of retained coarse classes from the first into
a set of fine classes; this set is then treated as the
confusion set for the second classifier.
Figure 1 shows the basic structure of the hierar-
chical classifier. During either the training or the
testing stage, a question is processed along one path
top-down to get classified.
The initial confusion set of any question is C
0
=
fc
1
; c
2
; : : : ; c
n
g, the set of all the coarse classes.
The coarse classifier determines a set of preferred
labels, C
1
= Coarse Classifier(C
0
), C
1
 C
0
so that jC
1
j  5. Then each coarse class label
in C
1
is expanded to a fixed set of fine classes
determined by the class hierarchy. That is, sup-
pose the coarse class c
i
is mapped into the set
c
i
= ff
i1
; f
i2
; : : : ; f
im
g of fine classes, then C
2
=
S
c
i
2C
1
c
i
. The fine classifier determines a set of
preferred labels, C
3
= Fine Classifier(C
2
) so
that C
3
 C
2
and jC
3
j  5. C
1
and C
3
are the ul-
timate outputs from the whole classifier which are
used in our evaluation.
3.2 Feature Space
Each question is analyzed and represented as a list
of features to be treated as a training or test exam-
ple for learning. We use several types of features
and investigate below their contribution to the QC
accuracy.
The primitive feature types extracted for each
question include words, pos tags, chunks (non-
overlapping phrases) (Abney, 1991), named entities,
head chunks (e.g., the first noun chunk in a sen-
tence) and semantically related words (words that
often occur with a specific question class).
Over these primitive features (which we call
?sensors?) we use a set of operators to compose
more complex features, such as conjunctive (n-
grams) and relational features, as in (Cumby and
Roth, 2000; Roth and Yih, 2001). A simple script
that describes the ?types? of features used, (e.g.,
conjunction of two consecutive words and their pos
tags) is written and the features themselves are ex-
tracted in a data driven way. Only ?active? features
are listed in our representation so that despite the
large number of potential features, the size of each
example is small.
Among the 6 primitive feature types, pos tags,
chunks and head chunks are syntactic features while
named entities and semantically related words are
semantic features. Pos tags are extracted using
a SNoW-based pos tagger (Even-Zohar and Roth,
2001). Chunks are extracted using a previously
learned classifier (Punyakanok and Roth, 2001; Li
and Roth, 2001). The named entity classifier is
also learned and makes use of the same technol-
ogy developed for the chunker (Roth et al, 2002).
The ?related word? sensors were constructed semi-
automatically.
Most question classes have a semantically related
word list. Features will be extracted for this class if
a word in a question belongs to the list. For exam-
ple, when ?away?, which belongs to a list of words
semantically related to the class distance, occurs in
the sentence, the sensor Rel(distance) will be ac-
tive. We note that the features from these sensors are
different from those achieved using named entity
since they support more general ?semantic catego-
rization? and include nouns, verbs, adjectives rather
than just named entities.
For the sake of the experimental comparison, we
define six feature sets, each of which is an incre-
mental combination of the primitive feature types.
That is, Feature set 1 (denoted by Word) contains
word features; Feature set 2 (Pos) contains features
composed of words and pos tags and so on; The fi-
nal feature set, Feature set 6 (RelWord) contains all
the feature types and is the only one that contains
the related words lists. The classifiers will be exper-
imented with different feature sets to test the influ-
ence of different features. Overall, there are about
200; 000 features in the feature space of RelWord
due to the generation of complex features over sim-
ple feature types. For each question, up to a couple
of hundreds of them are active.
3.3 Decision Model
For both the coarse and fine classifiers, the same
decision model is used to choose class labels for
a question. Given a confusion set and a question,
SNoW outputs a density over the classes derived
from the activation of each class. After ranking the
classes in the decreasing order of density values, we
have the possible class labels C = fc
1
; c
2
; : : : ; c
n
g,
with their densities P = fp
1
; p
2
; : : : ; p
n
g (where,
P
n
1
p
i
= 1, 0  p
i
 1, 1  i  n). As dis-
cussed earlier, for each question we output the first
k classes (1  k  5), c
1
; c
2
; : : : c
k
where k satis-
fies,
k = min(argmin
t
(
t
X
1
p
i
 T ); 5) (1)
T is a threshold value in [0,1]. If we treat p
i
as
the probability that a question belongs to Class i,
the decision model yields a reasonable probabilistic
interpretation. We use T = 0:95 in the experiments.
4 Experimental Study
We designed two experiments to test the accuracy of
our classifier on TREC questions. The first experi-
ment evaluates the contribution of different feature
types to the quality of the classification. Our hi-
erarchical classifier is trained and tested using one
of the six feature sets defined in Sect. 3.2 (we re-
peated the experiments on several different training
and test sets). In the second experiment, we evalu-
ate the advantage we get from the hierarchical clas-
sifier. We construct a multi-class classifier only for
fine classes. This flat classifier takes all fine classes
as its initial confusion set and classifies a question
into fine classes directly. Its parameters and deci-
sion model are the same as those of the hierarchical
one. By comparing this flat classifier with our hi-
erarchical classifier in classifying fine classes, we
hope to know whether the hierarchical classifier has
any advantage in performance, in addition to the ad-
vantages it might have in downstream processing
and comprehensibility.
4.1 Data
Data are collected from four sources: 4,500 English
questions published by USC (Hovy et al, 2001),
about 500 manually constructed questions for a few
rare classes, 894 TREC 8 and TREC 9 questions,
and also 500 questions from TREC 10 which serves
as our test set3.
These questions were manually labeled accord-
ing to our question hierarchy. Although we allow
multiple labels for one question in our classifiers,
in our labeling, for simplicity, we assigned exactly
3The annotated data and experimental results are available
from http://L2R.cs.uiuc.edu/cogcomp/
one label to each question. Our annotators were re-
quested to choose the most suitable class accord-
ing to their own understanding. This methodology
might cause slight problems in training, when the
labels are ambiguous, since some questions are not
treated as positive examples for possible classes as
they should be. In training, we divide the 5,500
questions from the first three sources randomly into
5 training sets of 1,000, 2,000, 3,000, 4,000 and
5,500 questions. All 500 TREC 10 questions are
used as the test set.
4.2 Evaluation
In this paper, we count the number of correctly clas-
sified questions by two different precision standards
P
1
and P
5
. Suppose k
i
labels are output for the i-
th question (k
i
 5) and are ranked in a decreasing
order according to their density values. We define
I
ij
= f
1; if the correct label of the ith
question is output in rank j;
0; otherwise:
(2)
Then, P
1
=
P
m
i=1
I
i1
=m and P
5
=
P
m
i=1
P
k
i
j=1
I
ij
=m where m is the total number of
test examples. P
1
corresponds to the usual defini-
tion of precision which allows only one label for
each question, while P
5
allows multiple labels.
P
5
reflects the accuracy of our classifier with re-
spect to later stages in a question answering sys-
tem. As the results below show, although question
classes are still ambiguous, few mistakes are intro-
duced by our classifier in this step.
4.3 Experimental Results
Performance of the hierarchical classifier
Table 2 shows the P
5
precision of the hierarchi-
cal classifier when trained on 5,500 examples and
tested on the 500 TREC 10 questions. The re-
sults are quite encouraging; question classification
is shown to be solved effectively using machine
learning techniques. It also shows the contribution
of the feature sets we defined. Overall, we get a
98.80% precision for coarse classes with all the fea-
tures and 95% for the fine classes.
P
<=5
Word Pos Chunk NE Head RelWord
Coarse 92.00 96.60 97.00 97.00 97.80 98.80
Fine 86.00 86.60 87.60 88.60 89.40 95.00
Table 2: Classification results of the hierarchical clas-
sifier on 500 TREC 10 questions. Training is done on
5,500 questions. Columns show the performance for
difference feature sets and rows show the precision for
coarse and fine classes, resp. All the results are evalu-
ated using P
5
.
Inspecting the data carefully, we can observe the
significant contribution of the features constructed
based on semantically related words sensors. It is
interesting to observe that this improvement is even
more significant for fine classes.
No. Train Test P
1
P
<=5
1 1000 500 83.80 95.60
2 2000 500 84.80 96.40
3 3000 500 91.00 98.00
4 4000 500 90.80 98.00
5 5500 500 91.00 98.80
Table 3: Classification accuracy for coarse classes on
different training sets using the feature set RelWord. Re-
sults are evaluated using P
1
and P
5
.
No. Train Test P
1
P
<=5
1 1000 500 71.00 83.80
2 2000 500 77.80 88.20
3 3000 500 79.80 90.60
4 4000 500 80.00 91.20
5 5500 500 84.20 95.00
Table 4: Classification accuracy for fine classes on dif-
ferent training sets using the feature set RelWord. Re-
sults are evaluated using P
1
and P
5
.
Tables 3 and 4 show the P
1
and P
5
accuracy
of the hierarchical classifier on training sets of dif-
ferent sizes and exhibit the learning curve for this
problem.
We note that the average numbers of labels out-
put by the coarse and fine classifiers are 1.54 and
2.05 resp., (using the feature set RelWord and 5,500
training examples), which shows the decision model
is accurate as well as efficient.
Comparison of the hierarchical and the flat
classifier
The flat classifier consists of one classifier which is
almost the same as the fine classifier in the hierar-
chical case, except that its initial confusion set is
the whole set of fine classes. Our original hope was
that the hierarchical classifier would have a better
performance, given that its fine classifier only needs
to deal with a smaller confusion set. However, it
turns out that there is a tradeoff between this factor
and the inaccuracy, albeit small, of the coarse level
prediction. As the results show, there is no perfor-
mance advantage for using a level of coarse classes,
and the semantically appealing coarse classes do not
contribute to better performance.
Figure 2 give some more intuition on the flat vs.
hierarchical issue. We define the tendency of Class
i to be confused with Class j as follows:
D
ij
= Err
ij
 2=(N
i
+ N
j
); (3)
where (when using P
1
), Err
ij
is the number of
questions in Class i that are misclassified as belong-
P
1
Word Pos Chunk NE Head RelWord
h 77.60 78.20 77.40 78.80 78.80 84.20
f 52.40 77.20 77.00 78.40 76.80 84.00
P
<=5
Word Pos Chunk NE Head RelWord
h 86.00 86.60 87.60 88.60 89.40 95.00
f 83.20 86.80 86.60 88.40 89.80 95.60
Table 5: Comparing accuracy of the hierarchical (h) and
flat (f) classifiers on 500 TREC 10 question; training is
done on 5,500 questions. Results are shown for different
feature sets using P
1
and P
5
.
Fine Classes 1?50
Fi
ne
 C
la
ss
es
 1
?5
0
2 24 28 32 37 50
2
24
28
32
37
50
Figure 2: The gray?scale map of the matrix D[n,n]. The
color of the small box in position (i,j) denotes D
ij
. The
larger D
ij
is, the darker the color is. The dotted lines
separate the 6 coarse classes.
ing to Class j, and N
i
; N
j
are the numbers of ques-
tions in Class i and j resp.
Figure 2 is a gray-scale map of the matrix D[n,n].
D[n,n] is so sparse that most parts of the graph are
blank. We can see that there is no good cluster-
ing of fine classes mistakes within a coarse class,
which explains intuitively why the hierarchical clas-
sifier with an additional level coarse classes does not
work much better.
4.4 Discussion and Examples
We have shown that the overall accuracy of our clas-
sifier is satisfactory. Indeed, all the reformulation
questions that we exemplified in Sec. 3 have been
correctly classified. Nevertheless, it is constructive
to consider some cases in which the classifier fails.
Below are some examples misclassified by the hier-
archical classifier.
What French ruler was defeated at the battle of Water-
loo?
The correct label is individual, but the classifier,
failing to relate the word ?ruler? to a person, since
it was not in any semantic list, outputs event.
What is the speed hummingbirds fly ?
The correct label is speed, but the classifier outputs
animal. Our feature sensors fail to determine that
the focus of the question is ?speed?. This example
illustrates the necessity of identifying the question
focus by analyzing syntactic structures.
What do you call a professional map drawer ?
The classifier returns other entities instead of
equivalent term. In this case, both classes are ac-
ceptable. The ambiguity causes the classifier not to
output equivalent term as the first choice.
5 Conclusion
This paper presents a machine learning approach to
question classification. We developed a hierarchical
classifier that is guided by a layered semantic hier-
archy of answers types, and used it to classify ques-
tions into fine-grained classes. Our experimental re-
sults prove that the question classification problem
can be solved quite accurately using a learning ap-
proach, and exhibit the benefits of features based on
semantic analysis.
In future work we plan to investigate further the
application of deeper semantic analysis (including
better named entity and semantic categorization) to
feature extraction, automate the generation of the
semantic features and develop a better understand-
ing to some of the learning issues involved in the
difference between a flat and a hierarchical classi-
fier.
References
S. P. Abney. 1991. Parsing by chunks. In S. P. Abney
R. C. Berwick and C. Tenny, editors, Principle-based
parsing: Computation and Psycholinguistics, pages
257?278. Kluwer, Dordrecht.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science De-
partment, May.
C. Cumby and D. Roth. 2000. Relational representations
that facilitate learning. In Proc. of the International
Conference on the Principles of Knowledge Represen-
tation and Reasoning, pages 425?434.
Y. Even-Zohar and D. Roth. 2001. A sequential model
for multi class classification. In EMNLP-2001, the
SIGDAT Conference on Empirical Methods in Natu-
ral Language Processing, pages 10?19.
A. R. Golding and D. Roth. 1999. A Winnow based ap-
proach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and
P. Morarescu. 2001. Falcon: Boosting knowledge for
answer engines. In Proceedings of the 9th Text Re-
trieval Conference, NIST.
U. Hermjakob. 2001. Parsing and question classification
for question answering. In ACL-2001 Workshop on
Open-Domain Question Answering.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep read: A reading comprehension system. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.
E. Hovy, L. Gerber, U. Hermjakob, C. Lin, and
D. Ravichandran. 2001. Toward semantics-based an-
swer pinpointing. In Proceedings of the DARPA Hu-
man Language Technology conference (HLT). San
Diego, CA.
A. Ittycheriah, M. Franz, W-J Zhu, A. Ratnaparkhi, and
R.J. Mammone. 2001. IBM?s statistical question an-
swering system. In Proceedings of the 9th Text Re-
trieval Conference, NIST.
W. G. Lehnert. 1986. A conceptual theory of question
answering. In B. J. Grosz, K. Sparck Jones, and B. L.
Webber, editors, Natural Language Processing, pages
651?657. Kaufmann, Los Altos, CA.
X. Li and D. Roth. 2001. Exploring evidence for shal-
low parsing. In Proc. of the Annual Conference on
Computational Natural Language Learning.
M. Light, G. Mann, E. Riloff, and E. Breck. 2001.
Analyses for Elucidating Current Question Answering
Technology. Journal for Natural Language Engineer-
ing. forthcoming.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS-13; The 2000
Conference on Advances in Neural Information Pro-
cessing Systems, pages 995?1001. MIT Press.
D. Roth and W. Yih. 2001. Relational learning via
propositional algorithms: An information extraction
case study. In Proc. of the International Joint Confer-
ence on Artificial Intelligence, pages 1257?1263.
D. Roth, G. Kao, X. Li, R. Nagarajan, V. Punyakanok,
N. Rizzolo, W. Yih, C. O. Alm, and L. G. Moran.
2002. Learning components for a question answering
system. In TREC-2001.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. of the Ameri-
can Association of Artificial Intelligence, pages 806?
813.
A. Singhal, S. Abney, M. Bacchiani, M. Collins, D. Hin-
dle, and F. Pereira. 2000. AT&T at TREC-8. In Pro-
ceedings of the 8th Text Retrieval Conference, NIST.
E. Voorhees. 2000. Overview of the TREC-9 question
answering track. In The Ninth Text Retrieval Confer-
ence (TREC-9), pages 71?80. NIST SP 500-249.
Robust Reading: Identification and Tracing of Ambiguous Names
Xin Li Paul Morie Dan Roth
Department of Computer Science
University of Illinois, Urbana, IL 61801
{xli1,morie,danr}@uiuc.edu
Abstract
A given entity, representing a person, a location
or an organization, may be mentioned in text
in multiple, ambiguous ways. Understanding
natural language requires identifying whether
different mentions of a name, within and across
documents, represent the same entity.
We develop an unsupervised learning approach
that is shown to resolve accurately the name
identification and tracing problem. At the heart
of our approach is a generative model of how
documents are generated and how names are
?sprinkled? into them. In its most general form,
our model assumes: (1) a joint distribution over
entities, (2) an ?author? model, that assumes
that at least one mention of an entity in a docu-
ment is easily identifiable, and then generates
other mentions via (3) an appearance model,
governing how mentions are transformed from
the ?representative? mention. We show how to
estimate the model and do inference with it and
how this resolves several aspects of the prob-
lem from the perspective of applications such
as questions answering.
1 Introduction
Reading and understanding text is a task that requires the
ability to disambiguate at several levels, abstracting away
details and using background knowledge in a variety of
ways. One of the difficulties that humans resolve instan-
taneously and unconsciously is that of reading names.
Most names of people, locations, organizations and oth-
ers, have multiple writings that are used freely within and
across documents.
The variability in writing a given concept, along with
the fact that different concepts may have very similar
writings, poses a significant challenge to progress in nat-
ural language processing. Consider, for example, an open
domain question answering system (Voorhees, 2002) that
attempts, given a question like: ?When was President
Kennedy born?? to search a large collection of articles in
order to pinpoint the concise answer: ?on May 29, 1917.?
The sentence, and even the document that contains the
answer, may not contain the name ?President Kennedy?;
it may refer to this entity as ?Kennedy?, ?JFK? or ?John
Fitzgerald Kennedy?. Other documents may state that
?John F. Kennedy, Jr. was born on November 25, 1960?,
but this fact refers to our target entity?s son. Other men-
tions, such as ?Senator Kennedy? or ?Mrs. Kennedy?
are even ?closer? to the writing of the target entity, but
clearly refer to different entities. Even the statement
?John Kennedy, born 5-29-1941? turns out to refer to a
different entity, as one can tell observing that the doc-
ument discusses Kennedy?s batting statistics. A similar
problem exists for other entity types, such as locations,
organizations etc. Ad hoc solutions to this problem, as
we show, fail to provide a reliable and accurate solution.
This paper presents the first attempt to apply a unified
approach to all major aspects of this problem, presented
here from the perspective of the question answering task:
(1) Entity Identity - do mentions A and B (typically,
occurring in different documents, or in a question and a
document, etc.) refer to the same entity? This problem
requires both identifying when different writings refer to
the same entity, and when similar or identical writings
refer to different entities. (2) Name Expansion - given a
writing of a name (say, in a question), find other likely
writings of the same name. (3) Prominence - given
question ?What is Bush?s foreign policy??, and given that
any large collection of documents may contain several
Bush?s, there is a need to identify the most prominent, or
relevant ?Bush?, perhaps taking into account also some
contextual information.
At the heart of our approach is a global probabilistic
view on how documents are generated and how names
(of different entity types) are ?sprinkled? into them. In
its most general form, our model assumes: (1) a joint dis-
tribution over entities, so that a document that mentions
?President Kennedy? is more likely to mention ?Oswald?
or ? White House? than ?Roger Clemens?; (2) an ?au-
thor? model, that makes sure that at least one mention
of a name in a document is easily identifiable, and then
generates other mentions via (3) an appearance model,
governing how mentions are transformed from the ?rep-
resentative? mention. Our goal is to learn the model from
a large corpus and use it to support robust reading - en-
abling ?on the fly? identification and tracing of entities.
This work presents the first study of our proposed
model and several relaxations of it. Given a collection of
documents we learn the models in an unsupervised way;
that is, the system is not told during training whether two
mentions represent the same entity. We only assume the
ability to recognize names, using a named entity recog-
nizer run as a preprocessor. We define several inferences
that correspond to the solutions we seek, and evaluate the
models by performing these inferences against a large
corpus we annotated. Our experimental results suggest
that the entity identity problem can be solved accurately,
giving accuracies (F1) close to 90%, depending on the
specific task, as opposed to 80% given by state of the art
ad-hoc approaches.
Previous work in the context of question answering
has not addressed this problem. Several works in NLP
and Databases, though, have addressed some aspects of
it. From the natural language perspective, there has
been a lot of work on the related problem of corefer-
ence resolution (Soon et al, 2001; Ng and Cardie, 2003;
Kehler, 2002) - which aims at linking occurrences of
noun phrases and pronouns within a document based on
their appearance and local context. (Charniak, 2001)
presents a solution to the problem of name structure
recognition by incorporating coreference information. In
the context of databases, several works have looked at the
problem of record linkage - recognizing duplicate records
in a database (Cohen and Richman, 2002; Hernandez and
Stolfo, 1995; Bilenko and Mooney, 2003). Specifically,
(Pasula et al, 2002) considers the problem of identity un-
certainty in the context of citation matching and suggests
a probabilistic model for that. Some of very few works
we are aware of that works directly with text data and
across documents, are (Bagga and Baldwin, 1998; Mann
and Yarowsky, 2003), which consider one aspect of the
problem ? that of distinguishing occurrences of identical
names in different documents, and only of people.
The rest of this paper is organized as follows: We for-
malize the ?robust reading? problem in Sec. 2. Sec. 3
describes a generative view of documents? creation and
three practical probabilistic models designed based on it,
and discusses inference in these models. Sec. 4 illustrates
how to learn these models in an unsupervised setting, and
Sec. 5 describes the experimental study. Sec. 6 concludes.
2 Robust Reading
We consider reading a collection of documents D =
{d1, d2, . . . , dm}, each of which may contain men-
tions (i.e. real occurrences) of |T | types of enti-
ties. In the current evaluation we consider T =
{Person, Location,Organization}.
An entity refers to the ?real? concept behind a mention
and can be viewed as a unique identifier to a real-world
object. Examples might be the person ?John F. Kennedy?
who became a president, ?White House? ? the residence
of the US presidents, etc. E denotes the collection of all
possible entities in the world and Ed = {edi }l
d
1 is the set
of entities mentioned in document d. M denotes the col-
lection of all possible mentions and Md = {mdi }n
d
1 is
the set of mentions in document d. Mdi (1 ? i ? ld) is
the set of mentions that refer to entity edi ? Ed. For en-
tity ?John F. Kennedy?, the corresponding set of mentions
in a document may contain ?Kennedy?, ?J. F. Kennedy?
and ?President Kennedy?. Among all mentions of an en-
tity edi in document d we distinguish the one occurring
first, rdi ? Mdi , as the representative of edi . In practice,
rdi is usually the longest mention of edi in the document
as well, and other mentions are variations of it. Repre-
sentatives are viewed as a typical representation of an
entity mentioned in a specific time and place. For ex-
ample, ?President J.F.Kennedy? and ?Congressman John
Kennedy? may be representatives of ?John F. Kennedy?
in different documents. R denotes the collection of all
possible representatives and Rd = {rdi }l
d
1 ? Md is the
set of representatives in document d. This way, each doc-
ument is represented as the collection of its entities, rep-
resentatives and mentions d = {Ed, Rd,Md}.
Elements in the name space W = E?R?M each have
an identifying writing (denoted as wrt(n) for n ? W )1
and an ordered list of attributes, A = {a1, . . . , ap},
which depends on the entity type. Attributes used in the
current evaluation include both internal attributes, such
as, for People, {title, firstname, middlename, lastname,
gender} as well as contextual attributes such as {time, lo-
cation, proper-names}. Proper-names refer to a list of
proper names that occur around the mention in the doc-
ument. All attributes are of string value and the values
could be missing or unknown2.
The fundamental problem we address in robust read-
ing is to decide what entities are mentioned in a given
document (given the observed set Md) and what the most
likely assignment of entity to each mention is.
3 A Model of Document Generation
We define a probability distribution over documents d =
{Ed, Rd,Md}, by describing how documents are being
generated. In its most general form the model has the
following three components:
(1) A joint probability distribution P (Ed) that governs
1The observed writing of a mention is its identifying writing.
For entities, it is a standard representation of them, i.e. the full
name of a person.
2Contextual attributes are not part of the current evaluation,
and will be evaluated in the next step of this work.
EEd
Rd
Md
e
eid
Mid
rid
d
John Fitzgerald Kennedy
John Fitzgerald Kennedy
President John F. Kennedy
{President Kennedy, Kennedy, JFK}
House of Representatives
House of Representatives
House of Representatives
{House of Representatives, The House}
Figure 1: Generating a document
how entities (of different types) are distributed into a doc-
ument and reflects their co-occurrence dependencies.
(2) The number of entities in a document, size(Ed),
and the number of mentions of each entity in Ed,
size(Mdi ), need to be decided. The current evaluation
makes the simplifying assumption that these numbers are
determined uniformly over a small plausible range.
(3) The appearance probability of a name generated
(transformed) from its representative is modelled as a
product distribution over relational transformations of at-
tribute values. This model captures the similarity be-
tween appearances of two names. In the current eval-
uation the same appearance model is used to calculate
both the probability P (r|e) that generates a representa-
tive r given an entity e and the probability P (m|r) that
generates a mention m given a representative r. Attribute
transformations are relational, in the sense that the dis-
tribution is over transformation types and independent of
the specific names.
Given these, a document d is assumed to be gener-
ated as follows (see Fig. 1): A set of size(Ed) entities
Ed ? E is selected to appear in a document d, accord-
ing to P (Ed). For each entity edi ? Ed, a representative
rdi ? R is chosen according to P (rdi |edi ), generating Rd.
Then mentions Mdi of an entity are generated from each
representative rdi ? Rd ? each mention mdj ? Mdi is
independently transformed from rdi according to the ap-
pearance probability P (mdj |rdi ). Assuming conditional
independency between Md and Ed given Rd, the proba-
bility distribution over documents is therefore
P (d) = P (Ed, Rd,Md) = P (Ed)P (Rd|Ed)P (Md|Rd),
and the probability of the document collection D is:
P (D) =
?
d?D
P (d).
Given a mention m in a document d (Md is the set of
observed mentions in d), the key inference problem is to
determine the most likely entity e?m that corresponds to
it. This is done by computing:
Ed = argmaxE??EP (Ed, Rd|Md, ?) (1)
= argmaxE??EP (Ed, Rd,Md|?), (2)
where ? is the learned model?s parameters. This gives the
assignment of the most likely entity e?m for m.
3.1 Relaxations of the Model
In order to simplify model estimation and to evaluate
some assumptions, several relaxations are made to form
three simpler probabilistic models.
Model I: (the simplest model) The key relaxation here
is in losing the notion of an ?author? ? rather than first
choosing a representative for each document, mentions
are generated independently and directly given an entity.
That is, an entity ei is selected from E according to the
prior probability P (ei); then its actual mention mi is se-
lected according to P (mi|ei). Also, an entity is selected
into a document independently of other entities. In this
way, the probability of the whole document set can be
computed simply as follows:
P (D) = P ({(ei,mi)}ni=1) =
n?
i=1
P (ei)P (mi|ei),
and the inference problem for the most likely entity given
m is:
e?m = argmaxe?EP (e|m, ?) = argmaxe?EP (e)P (m|e).
(3)
Model II: (more expressive) The major relaxation
made here is in assuming a simple model of choos-
ing entities to appear in documents. Thus, in order to
generate a document d, after we decide size(Ed) and
{size(Md1 , size(Md2 ), . . . } according to uniform distri-
butions, each entity edi is selected into d independently
of others according to P (edi ). Next, the representative rdi
for each entity edi is selected according to P (rdi |edi ) and
for each representative the actual mentions are selected
independently according to P (mdj |rdj ). Here, we have in-
dividual documents along with representatives, and the
distribution over documents is:
P (d) = P (Ed, Rd,Md) = P (Ed)P (Rd|Ed)P (Md|Rd)
?
|Ed|?
i=1
[P (edi )P (rdi |edi )]
?
(rdj ,m
d
j )
P (mdj |rdj )
after we ignore the size components (they do not influ-
ence inferences). The inference problem here is the same
as in Equ. (2).
Model III: This model performs the least relaxation.
After deciding size(Ed) according to a uniform distri-
bution, instead of assuming independency among enti-
ties which does not hold in reality (For example, ?Gore?
and ?George. W. Bush? occur together frequently, but
?Gore? and ?Steve. Bush? do not), we select entities us-
ing a graph based algorithm: entities in E are viewed
as nodes in a weighted directed graph with edges (i, j)
labelled P (ej |ei) representing the probability that entity
ej is chosen into a document that contains entity ei. We
distribute entities to Ed via a random walk on this graph
starting from ed1 with a prior probability P (edi ). Repre-
sentatives and mentions are generated in the same way
as in Model II. Therefore, a more general model for the
distribution over documents is:
P (d) ? P (ed1)P (rd1 |ed1)
|Ed|?
i=2
[P (edi |edi?1)P (rdi |edi )]?
?
(rdj ,m
d
j )
P (mdj |rdj ).
The inference problem is the same as in Equ. (2).
3.2 Inference Algorithms
The fundamental problem in robust reading can be solved
as inference with the models: given a mention m, seek the
most likely entity e ? E for m according to Equ. (3) for
Model I or Equ. (2) for Model II and III. Instead of all
entities in the real world, E can be viewed without loss
as the set of entities in a closed document collection that
we use to train the model parameters and it is known after
training. The inference algorithm for Model I (with time
complexity O(|E|)) is simple and direct: just compute
P (e,m) for each candidate entity e ? E and then choose
the one with the highest value. Due to exponential num-
ber of possible assignments of Ed, Rd to Md in Model
II and III, precise inference is infeasible and approximate
algorithms are therefore designed:
In Model II, we adopt a two-step algorithm: First, we
seek the representatives Rd for the mentions Md in docu-
ment d by sequentially clustering the mentions according
to the appearance model. The first mention in each group
is chosen as the representative. Specifically, when con-
sidering a mention m ? Md, P (m|r) is computed for
each representative r that have already been created and
a fixed threshold is then used to decide whether to create a
new group for m or to add it to one of the existing groups
with the largest P (m|r). In the second step, each rep-
resentative rdi ? Rd is assigned to its most likely entity
according to e? = argmaxe?EP (e) ?P (r|e). This algo-
rithm has a time complexity of O((|Md|+ |E|) ? |Md|).
Model III has a similar algorithm as Model II. The
only difference is that we need to consider the global
dependency between entities. Thus in the second step,
instead of seeking an entity e for each representative r
separately, we determine a set of entities Ed for Rd in
a Hidden Markov Model with entities in E as hidden
states and Rd as observations. The prior probabilities,
the transitive probabilities and the observation probabil-
ities are given by P (e), P (ej |ei) and P (r|e) respec-
tively. Here we seek the most likely sequence of enti-
ties given those representatives in their appearing order
using the Viterbi algorithm. The total time complexity is
e1= George Bush e2= George W. Bush e3= Steve Bush
m1,r1=President Bush
m2=Bush
m4,r2=Steve Bush
m5=Bushm3=J. Quayle
Entities E
d1 d2
Figure 2: An conceptual example. The arrows represent
the correct assignment of entities to mentions. r1, r2 are
representatives.
O(|Md|2 + |E|2 ? |Md|). The |E|2 component can be
simplified by filtering out unlikely entities for a represen-
tative according to their appearance similarity.
3.3 Discussion
Besides different assumptions, some fundamental differ-
ences exist in inference with the models as well. In Model
I, the entity of a mention is determined completely inde-
pendently of other mentions, while in Model II, it relies
on other mentions in the same document for clustering.
In Model III, it is not only related to other mentions but
to a global dependency over entities. The following con-
ceptual example illustrates those differences as in Fig. 2.
Example 3.1 Given E = {George Bush, George W. Bush,
Steve Bush}, documents d1, d2 and 5 mentions in them, and
suppose the prior probability of entity ?George W. Bush? is
higher than those of the other two entities, the entity assign-
ments to the five mentions in the models could be as follows:
For Model I, mentions(e1) = ?, mentions(e2) =
{m1,m2,m5} and mentions(e3) = {m4}. The result is
caused by the fact that a mention tends to be assigned to the
entity with higher prior probability when the appearance simi-
larity is not distinctive.
For Model II, mentions(e1) = ?, mentions(e2) =
{m1,m2} and mentions(e3) = {m4,m5}. Local depen-
dency (appearance similarity) between mentions inside each
document enforces the constraint that they should refer to the
same entity, like ?Steve Bush? and ?Bush? in d2.
For Model III, mentions(e1) = {m1,m2}, mentions(e2)
= ?, mentions(e3) = {m4,m5}. With the help of global
dependency between entities, for example, ?George Bush? and
?J. Quayle?, an entity can be distinguished from another one
with a similar writing.
3.4 Other Tasks
Other aspects of ?Robust Reading? can be solved based
on the above inference problem.
Entity Identity: Given two mentions m1 ? d1,m2 ? d2,
determine whether they correspond to the same entity by:
m1 ? m2 ?? argmaxe?EP (e,m1) = argmaxe?EP (e,m2)
for Model I and
m1 ? m2 ?? argmaxe?EP (Ed1 , Rd1 ,Md1) =
argmaxe?EP (Ed2 , Rd2 ,Md2).
for Model II and III.
Name Expansion: Given a mention mq in a query q,
decide whether mention m in the document collection D
is a ?legal? expansion of mq:
mq ? m ?? e?mq = argmaxe?EP (Eq, Rq,Mq)
& m ? mentions(e?).
Here it?s assumed that we already know the possible
mentions of e? after training the models with D.
Prominence: Given a name n ? W , the most promi-
nent entity for n is given by (P (e) is given by the prior
distribution PE and P (n|e) is given by the appearance
model.):
e? = argmaxe?EP (e)P (n|e).
4 Learning the Models
Confined by the labor of annotating data, we learn the
probabilistic models in an unsupervised way given a col-
lection of documents; that is, the system is not told dur-
ing training whether two mentions represent the same en-
tity. A greedy search algorithm modified after the stan-
dard EM algorithm (We call it Truncated EM algorithm)
is adopted here to avoid complex computation.
Given a set of documents D to be studied and the ob-
served mentions Md in each document, this algorithm
iteratively updates the model parameter ? (several under-
lying probabilistic distributions described before) and the
structure (that is, Ed and Rd) of each document d. Dif-
ferent from the standard EM algorithm, in the E-step, it
seeks the most likely Ed and Rd for each document rather
than the expected assignment.
4.1 Truncated EM Algorithm
The basic framework of the Truncated EM algorithm to
learn Model II and III is as follows:
1. In the initial (I-) step, an initial (Ed0 , Rd0) is assigned
to each document d by an initialization algorithm.
After this step, we can assume that the documents
are annotated with D0 = {(Ed0 , Rd0,Md)}.
2. In the M-step, we seek the model parameter ?t+1
that maximizes P (Dt|?). Given the ?labels? sup-
plied in the previous I- or E-step, this amounts to the
maximum likelihood estimation. (to be described in
Sec. 4.3).
3. In the E-step, we seek (Edt+1, Rdt+1) for each
document d that maximizes P (Dt+1|?t+1) where
Dt+1 = {(Edt+1, Rdt+1,Md)}. It?s the same infer-
ence problem as in Sec. 3.2.
4. Stopping Criterion: If no increase is achieved over
P (Dt|?t), the algorithm exits. Otherwise the algo-
rithm will iterate over the M-step and E-step.
The algorithm for Model I is similar to the above one,
but much simpler in the sense that it does not have the no-
tions of documents and representatives. So in the E-step
we only seek the most likely entity e for each mention
m ? D, and this simplifies the parameter estimation in
the M-step accordingly. It usually takes 3? 10 iterations
before the algorithms stop in our experiments.
4.2 Initialization
The purpose of the initial step is to acquire an initial guess
of document structures and the set of entities E in a closed
collection of documents D. The hope is to find all entities
without loss so duplicate entities are allowed. For all the
models, we use the same algorithm:
A local clustering is performed to group mentions in-
side each document: simple heuristics are applied to cal-
culating the similarity between mentions; and pairs of
mentions with similarity above a threshold are then clus-
tered together. The first mention in each group is chosen
as the representative (only in Model II and III) and an
entity having the same writing with the representative is
created for each cluster3. For all the models, the set of
entities created in different documents become the global
entity set E in the following M- and E-steps.
4.3 Estimating the Model Parameters
In the learning process, assuming documents have al-
ready been annotated D = {(e, r,m)}n1 from previous I-
or E-step, several underlying probability distributions of
the relaxed models are estimated by maximum likelihood
estimation in each M-step. The model parameters include
a set of prior probabilities for entities PE , a set of tran-
sitive probabilities for entity pairs PE|E (only in Model
III) and the appearance probabilities PW |W of each name
in the name space W being transformed from another.
? The prior distribution PE is modelled as a multi-
nomial distribution. Given a set of labelled entity-
mention pairs {(ei,mi)}n1 ,
P (e) = freq(e)n
where freq(e) denotes the number of pairs containing
entity e.
? Given all the entities appearing in D, the transitive
probability P (e|e) is estimated by
P (e2|e1) ? P (wrt(e2)|wrt(e1)) = doc
#(wrt(e2), wrt(e1))
doc#(wrt(e1)) .
Here, the conditional probability between two real-
world entities P (e2|e1) is backed off to the one be-
tween the identifying writings of the two entities
P (wrt(e2)|wrt(e1)) in the document set D to avoid
3Note that the performance of the initialization algorithm is
97.3% precision and 10.1% recall (measures are defined later.)
sparsity problem. doc#(w1, w2, ...) denotes the num-
ber of documents having the co-occurrence of writings
w1, w2, ....
? Appearance probability, the probability of one
name being transformed from another, denoted as
P (n2|n1) (n1, n2 ? W ), is modelled as a product
of the transformation probabilities over attribute val-
ues 4. The transformation probability for each attribute
is further modelled as a multi-nomial distribution over
a set of predetermined transformation types: TT =
{copy,missing, typical, non? typical}5.
Suppose n1 = (a1 = v1, a2 = v2, ..., ap = vp) and
n2 = (a1 = v?1, a2 = v?2, ..., ap = v?p) are two names be-
longing to the same entity type, the transformation prob-
abilities PM |R, PR|E and PM |E , are all modelled as a
product distribution (naive Bayes) over attributes:
P (n2|n1) = ?pk=1P (v?k|vk).
We manually collected typical and non-typical trans-
formations for attributes such as titles, first names,
last names, organizations and locations from multiple
sources such as U.S. government census and online dic-
tionaries. For other attributes like gender, only copy
transformation is allowed. The maximum likelihood es-
timation of the transformation probability P (t, k) (t ?
TT, ak ? A) from annotated representative-mention
pairs {(r,m)}n1 is:
P (t, k) = freq(r,m) : v
r
k ?t vmk
n (4)
vrk ?t vmk denotes the transformation from attribute
ak of r to that of m is of type t. Simple smoothing is
performed here for unseen transformations.
5 Experimental Study
Our experimental study focuses on (1) evaluating the
three models on identifying three entity types (Peo-
ple, Locations, Organization); (2) comparing our in-
duced similarity measure between names (the appearance
model) with other similarity measures; (3) evaluating the
contribution of the global nature of our model, and fi-
nally, (4) evaluating our models on name expansion and
prominence ranking.
5.1 Methodology
We randomly selected 300 documents from 1998-2000
New York Times articles in the TREC corpus (Voorhees,
4The appearance probability can be modelled differently by
using other string similarity between names. We will compare
the model described here with some other non-learning similar-
ity metrics later.
5copy denotes v?k is exactly the same as vk; missing denotes
?missing value? for v?k; typical denotes v?k is a typical variation
of vk, for example, ?Prof.? for ?Professor?, ?Andy? for ?An-
drew?; non-typical denotes a non-typical transformation.
2002). The documents were annotated by a named entity
tagger for People, Locations and Organizations. The an-
notation was then corrected and each name mention was
labelled with its corresponding entity by two annotators.
In total, about 8, 000 mentions of named entities which
correspond to about 2, 000 entities were labelled. The
training process gets to see only the 300 documents and
extracts attribute values for each mention. No supervision
is supplied. These records are used to learn the proba-
bilistic models.
In the 64 million possible mention pairs, most are triv-
ial non-matching one ? the appearances of the two men-
tions are very different. Therefore, direct evaluation over
all those pairs always get alost 100% accuracy in our
experiments. To avoid this, only the 130, 000 pairs of
matching mentions that correspond to the same entity are
used to evaluate the performance of the models. Since
the probabilistic models are learned in an unsupervised
setting, testing can be viewed simply as the evaluation of
the learned model, and is thus done on the same data. The
same setting was used for all models and all comparison
performed (see below).
To evaluate the performance, we pair two mentions
iff the learned model determined that they correspond
to the same entity. The list of predicted pairs is then
compared with the annotated pairs. We measure Preci-
sion (P ) ? Percentage of correctly predicted pairs, Recall
(R) ? Percentage of correct pairs that were predicted, and
F1 = 2PRP+R .
Comparisons: The appearance model induces a ?simi-
larity? measure between names, which is estimated dur-
ing the training process. In order to understand whether
the behavior of the generative model is dominated by
the quality of the induced pairwise similarity or by the
global aspects (for example, inference with the aid of
the document structure), we (1) replace this measure by
two other ?local? similarity measures, and (2) compare
three possible decision mechanisms ? pairwise classifica-
tion, straightforward clustering over local similarity, and
our global model. To obtain the similarity required by
pairwise classification and clustering, we use this for-
mula sima(n1, n2) = P (n1|n2) to convert the appear-
ance probability described in Sec. 4.3 to it.
The first similarity measure we use is a sim-
ple baseline approach: two names are similar iff
they have identical writings (that is, simb(n1, n2) =
1 if n1, n2 are identical or 0 otherwise). The second
one is a state-of-art similarity measure sims(n1, n2) ?
[0, 1] for entity names (SoftTFIDF with Jaro-Winkler dis-
tance and ? = 0.9); it was ranked the best measure in a
recent study (Cohen et al, 2003).
Pairwise classification is done by pairing two men-
tions iff the similarity between them is above a fixed
threshold. For Clustering, a graph-based clustering al-
All(P/L/O) Identity SoftTFIDF Appearance
Pairwise 70.7 (64.7/64.1/83.7) 82.1 (79.9/77.3/89.5) 81.5 (83.6/70.9/90.7)
Clustering 70.7 (64.7/64.1/83.7) 79.8 (70.6/76.7/91.0) 79.6 (70.9/76.1/91.0)
Model II 70.7 (64.7/64.1/83.7) 82.5 (79.8/77.4/90.2) 89.0 (92.7/81.9/92.9)
Table 1: Comparison of different decision levels and sim-
ilarity measures. Three similarity measures are evaluated
(rows) across three decision levels (columns). Performance is
evaluated by the F1 values over the whole test set. The first
number averages all entity types; numbers in parentheses repre-
sent People, Location and Organization respectively.
gorithm is used. Two nodes in the graph are connected
if the similarity between the corresponding mentions is
above a threshold. In evaluation, any two mentions be-
longing to the same connected component are paired the
same way as we did in Sec. 5.1 and all those pairs are then
compared with the annotated pairs to calculate Precision,
Recall and F1.
Finally, we evaluate the baseline and the SoftTFIDF
measure in the context of Model II, where the appear-
ance model is replaced. We found that the probabil-
ities directly converted from the SoftTFIDF similarity
behave badly so we adopt this formula P (n1|n2) =
e10?sims(n1,n2)?1
e10?1 instead to acquire P (n1|n2) needed by
Model II. Those probabilities are fixed as we estimate
other model parameters in training.
5.2 Results
The bottom line result is given in Tab. 1. All the similarity
measures are compared in the context of the three levels
of decisions ? local decision (pairwise), clustering and
our probabilistic model II. Only the best results in the
experiments, achieved by trying different thresholds in
pairwise classification and clustering, are shown.
The behavior across rows indicates that, locally, our
unsupervised learning based appearance model is about
the same as the state-of-the-art SoftTFIDF similarity. The
behavior across columns, though, shows the contribu-
tion of the global model, and that the local appearance
model behaves better with it than a fixed similarity mea-
sure does. A second observation is that the Location ap-
pearance model is not as good as the one for People and
Organization, probably due to the attribute transforma-
tion types chosen.
Tab. 2 presents a more detailed evaluation of the differ-
ent approaches on the entity identity task. All the three
probabilistic models outperform the discriminatory ap-
proaches in this experiment, an indication of the effec-
tiveness of the generative model.
We note that although Model III is more expressive
and reasonable than model II, it does not always perform
better. Indeed, the global dependency among entities in
Model III achieves two-folded outcomes: it achieves bet-
ter precision, but may degrade the recall. The following
example, taken from the corpus, illustrates the advantage
of this model.
Entity Type Mod InDoc InterDoc All
F1(%) F1(%) R(%) P(%) F1(%)
All Entities B 86.0 68.8 58.5 85.5 70.7
D 86.5 78.9 66.4 95.8 79.8
I 96.3 85.0 79.0 94.1 86.2
II 96.5 88.1 85.9 92.2 89.0
III 96.5 87.9 84.4 93.6 88.9
People B 82.4 59.0 48.5 86.3 64.7
D 82.4 67.1 54.5 91.5 70.6
I 96.2 84.8 80.6 94.8 87.4
II 96.4 91.7 94.0 91.5 92.7
III 96.4 88.9 89.8 91.3 90.5
Location B 88.8 63.0 54.8 75.0 64.1
D 91.4 76.0 61.3 95.9 76.7
I 92.9 78.9 70.9 89.1 79.5
II 93.8 81.4 76.2 88.1 81.9
III 93.8 82.8 76.0 91.2 83.3
Organization B 95.3 82.8 72.6 96.4 83.7
D 95.8 90.7 83.9 98.9 91.1
I 98.8 91.8 86.5 98.5 92.3
II 98.5 92.5 88.6 97.5 92.9
III 98.8 93.0 88.5 98.6 93.4
Table 2: Performance of different approaches over all test
examples. B, D, I, II and III denote the baseline model, the
SoftTFIDF similarity model with clustering, and the three prob-
abilistic models. We distinguish between pairs of mentions that
are inside the same document (InDoc, 15% of the pairs) or not
(InterDoc).
Example 5.1 ?Sherman Williams? is mentioned along with
the baseball team ?Dallas Cowboys? in 8 out of 300 documents,
while ?Jeff Williams? is mentioned along with ?LA Dodgers?
in two documents.
In all models but Model III, ?Jeff Williams? is judged to cor-
respond to the same entity as ?Sherman Williams? since their
appearances are similar and the prior probability of the latter is
higher than the former. Only Model III, due to the co-occurring
dependency between ?Jeff Williams? and ?Dodgers?, identi-
fies it as corresponding to an entity different from ?Sherman
Williams?.
While this shows that Model III achieves better preci-
sion, the recall may go down. The reason is that global
dependencies among entities enforces restrictions over
possible grouping of similar mentions; in addition, with
a limited document set, estimating this global depen-
dency is inaccurate, especially when the entities them-
selves need to be found when training the model.
Hard Cases: To analyze the experimental results further,
we evaluated separately two types of harder cases of the
entity identity task: (1) mentions with different writings
that refer to the same entity; and (2) mentions with sim-
ilar writings that refer to different entities. Model II and
III outperform other models in those two cases as well.
Tab. 3 presents F1 performance of different approaches
in the first case. The best F1 value is only 73.1%, indicat-
ing that appearance similarity and global dependency are
not sufficient to solve this problem when the writings are
very different. Tab. 4 shows the performance of differ-
ent approaches for disambiguating similar writings that
correspond to different entities.
Both these cases exhibit the difficulty of the problem,
and that our approach provides a significant improvement
over the state of the art similarity measure ? column D
vs. column II in Tab. 4. It also shows that it is necessary
to use contextual attributes of the names, which are not
yet included in this evaluation.
Model B D I II III
Peop 0 77.9 79.2 86.0 82.6
Loc 0 30.4 55.1 58.5 61.5
Org 0 77.7 69.5 71.7 71.2
All 0 63.3 68.4 73.1 72.5
Table 3: Identifying different writings of the same entity
(F1). We filter out identical writings and report only on cases
of different writings of the same entity. The test set contains
46, 376 matching pairs (but in different writings) in the whole
data set.
Model B D I II III
Peop 75.2 83.0 60.8 89.7 88.0
Loc 86.5 80.7 80.0 90.3 90.3
Org 80.0 89.4 71.0 93.1 92.6
All 78.7 78.9 68.1 90.7 89.7
Table 4: Identifying similar writings of different
entities(F1). The test set contains 39, 837 pairs of mentions
that associated with different entities in the 300 documents and
have at least one token in common.
5.3 Other Tasks
In the following experiments, we evaluate the genera-
tive model on other tasks related to robust reading. We
present results only for Model II, the best one in previous
experiments.
Name Expansion: Given a mention m in a query, we find
the most likely entity e ? E for m using the inference al-
gorithm as described in Sec. 3.2. All unique mentions of
the entity in the documents are output as the expansions
of m. The accuracy for a given mention is defined as the
percentage of correct expansions output by the system.
The average accuracy of name expansion of Model II is
shown in Tab. 5. Here is an example:
Query: Who is Gore ?
Expansions: Vice President Al Gore, Al Gore, Gore.
Prominence Ranking: We refer to Example 3.1 and use
it to exemplify quantitatively how our system supports
prominence ranking. Given a query name n, the ranking
of the entities with regard to the value of P (e) ? P (n|e)
(shown in brackets) by Model II is as follows.
Input: George Bush
1. George Bush (0.0448) 2. George W. Bush (0.0058)
Input: Bush
1. George W. Bush (0.0047) 2. George Bush (0.0015)
3. Steve Bush (0.0002)
6 Conclusion and Future Work
This paper presents an unsupervised learning approach to
several aspects of the ?robust reading? problem ? cross-
document identification and tracing of ambiguous names.
We developed a model that describes the natural gen-
eration process of a document and the process of how
Entity Type People Location Organization
Accuracy(%) 90.6 100 100
Table 5: Accuracy of name expansion. Accuracy is averaged
over 30 randomly chosen queries for each entity type.
names are ?sprinkled? into them, taking into account de-
pendencies between entities across types and an ?author?
model. Several relaxations of this model were developed
and studied experimentally, and compared with a state-
of-the-art discriminative model that does not take a global
view. The experiments exhibit encouraging results and
the advantages of our model.
This work is a preliminary exploration of the robust
reading problem. There are several critical issues that our
model can support, but were not included in this prelimi-
nary evaluation. Some of the issues that will be included
in future steps are: (1) integration with more contextual
information (like time and place) related to the target enti-
ties, both to support a better model and to allow temporal
tracing of entities; (2) studying an incremental approach
of training the model; that is, when a new document is
observed, coming, how to update existing model param-
eters ? (3) integration of this work with other aspects of
general coreference resolution (e.g., other terms like pro-
nouns that refer to an entity) and named entity recognition
(which we now take as given); and (4) scalability issues
in applying the system to large corpora.
Acknowledgments
This research is supported by NSF grants ITR-IIS-
0085836, ITR-IIS-0085980 and IIS-9984168 and an
ONR MURI Award.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-document
coreferencing using the vector space model. In ACL.
M. Bilenko and R. Mooney. 2003. Adaptive duplicate detection
using learnable string similarity measures. In KDD.
E. Charniak. 2001. Unsupervised learning of name structure
from coreference datal. In NAACL.
W. Cohen and J. Richman. 2002. Learning to match and clus-
ter large high-dimensional data sets for data integration. In
KDD.
W. Cohen, P. Ravikumar, and S. Fienberg. 2003. A comparison
of string metrics for name-matching tasks. In IIWeb Work-
shop 2003.
M. Hernandez and S. Stolfo. 1995. The merge/purge problem
for large databases. In SIGMOD.
A. Kehler. 2002. Coherence, Reference, and the Theory of
Grammar. CSLI Publications.
G. Mann and D. Yarowsky. 2003. Unsupervised personal name
disambiguation. In CoNLL.
V. Ng and C. Cardie. 2003. Improving machine learning ap-
proaches to coreference resolution. In ACL.
H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser.
2002. Identity uncertainty and citation matching. In NIPS.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics (Special Issue on Computational Anaphora
Resolution), 27:521?544.
E. Voorhees. 2002. Overview of the TREC-2002 question an-
swering track. In Proceedings of TREC, pages 115?123.
Exploring Evidence for Shallow Parsing  
Xin Li Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
xli1@cs.uiuc.edu danr@cs.uiuc.edu
Abstract
Significant amount of work has been
devoted recently to develop learning
techniques that can be used to gener-
ate partial (shallow) analysis of natu-
ral language sentences rather than a full
parse. In this work we set out to evalu-
ate whether this direction is worthwhile
by comparing a learned shallow parser
to one of the best learned full parsers
on tasks both can perform ? identify-
ing phrases in sentences. We conclude
that directly learning to perform these
tasks as shallow parsers do is advanta-
geous over full parsers both in terms of
performance and robustness to new and
lower quality texts.
1 Introduction
Shallow parsing is studied as an alternative to
full-sentence parsing. Rather than producing a
complete analysis of sentences, the alternative
is to perform only partial analysis of the syn-
tactic structures in a text (Harris, 1957; Abney,
1991; Greffenstette, 1993). A lot of recent work
on shallow parsing has been influenced by Ab-
ney?s work (Abney, 1991), who has suggested to
?chunk? sentences to base level phrases. For ex-
ample, the sentence ?He reckons the current ac-
count deficit will narrow to only $ 1.8 billion in
September .? would be chunked as follows (Tjong
Kim Sang and Buchholz, 2000):
[NP He ] [VP reckons ] [NP the current
account deficit ] [VP will narrow ] [PP

This research is supported by NSF grants IIS-9801638,
ITR-IIS-0085836 and an ONR MURI Award.
to ] [NP only $ 1.8 billion ] [PP in ] [NP
September] .
While earlier work in this direction concen-
trated on manual construction of rules, most of
the recent work has been motivated by the obser-
vation that shallow syntactic information can be
extracted using local information ? by examin-
ing the pattern itself, its nearby context and the
local part-of-speech information. Thus, over the
past few years, along with advances in the use
of learning and statistical methods for acquisition
of full parsers (Collins, 1997; Charniak, 1997a;
Charniak, 1997b; Ratnaparkhi, 1997), significant
progress has been made on the use of statisti-
cal learning methods to recognize shallow pars-
ing patterns ? syntactic phrases or words that
participate in a syntactic relationship (Church,
1988; Ramshaw and Marcus, 1995; Argamon et
al., 1998; Cardie and Pierce, 1998; Munoz et al,
1999; Punyakanok and Roth, 2001; Buchholz et
al., 1999; Tjong Kim Sang and Buchholz, 2000).
Research on shallow parsing was inspired by
psycholinguistics arguments (Gee and Grosjean,
1983) that suggest that in many scenarios (e.g.,
conversational) full parsing is not a realistic strat-
egy for sentence processing and analysis, and was
further motivated by several arguments from a
natural language engineering viewpoint.
First, it has been noted that in many natural lan-
guage applications it is sufficient to use shallow
parsing information; information such as noun
phrases (NPs) and other syntactic sequences have
been found useful in many large-scale language
processing applications including information ex-
traction and text summarization (Grishman, 1995;
Appelt et al, 1993). Second, while training a full
parser requires a collection of fully parsed sen-
tences as training corpus, it is possible to train a
shallow parser incrementally. If all that is avail-
able is a collection of sentences annotated for
NPs, it can be used to produce this level of anal-
ysis. This can be augmented later if more infor-
mation is available. Finally, the hope behind this
research direction was that this incremental and
modular processing might result in more robust
parsing decisions, especially in cases of spoken
language or other cases in which the quality of the
natural language inputs is low ? sentences which
may have repeated words, missing words, or any
other lexical and syntactic mistakes.
Overall, the driving force behind the work on
learning shallow parsers was the desire to get bet-
ter performance and higher reliability. However,
since work in this direction has started, a sig-
nificant progress has also been made in the re-
search on statistical learning of full parsers, both
in terms of accuracy and processing time (Char-
niak, 1997b; Charniak, 1997a; Collins, 1997;
Ratnaparkhi, 1997).
This paper investigates the question of whether
work on shallow parsing is worthwhile. That
is, we attempt to evaluate quantitatively the intu-
itions described above ? that learning to perform
shallow parsing could be more accurate and more
robust than learning to generate full parses. We
do that by concentrating on the task of identify-
ing the phrase structure of sentences ? a byprod-
uct of full parsers that can also be produced by
shallow parsers. We investigate two instantiations
of this task, ?chucking? and identifying atomic
phrases. And, to study robustness, we run our
experiments both on standard Penn Treebank data
(part of which is used for training the parsers) and
on lower quality data ? the Switchboard data.
Our conclusions are quite clear. Indeed, shal-
low parsers that are specifically trained to per-
form the tasks of identifying the phrase structure
of a sentence are more accurate and more robust
than full parsers. We believe that this finding, not
only justifies work in this direction, but may even
suggest that it would be worthwhile to use this
methodology incrementally, to learn a more com-
plete parser, if needed.
2 Experimental Design
In order to run a fair comparison between full
parsers and shallow parsers ? which could pro-
duce quite different outputs ? we have chosen
the task of identifying the phrase structure of a
sentence. This structure can be easily extracted
from the outcome of a full parser and a shallow
parser can be trained specifically on this task.
There is no agreement on how to define phrases
in sentences. The definition could depend on
downstream applications and could range from
simple syntactic patterns to message units peo-
ple use in conversations. For the purpose of this
study, we chose to use two different definitions.
Both can be formally defined and they reflect dif-
ferent levels of shallow parsing patterns.
The first is the one used in the chunking com-
petition in CoNLL-2000 (Tjong Kim Sang and
Buchholz, 2000). In this case, a full parse tree
is represented in a flat form, producing a rep-
resentation as in the example above. The goal
in this case is therefore to accurately predict a
collection of  different types of phrases. The
chunk types are based on the syntactic category
part of the bracket label in the Treebank. Roughly,
a chunk contains everything to the left of and
including the syntactic head of the constituent
of the same name. The phrases are: adjective
phrase (ADJP), adverb phrase (ADVP), conjunc-
tion phrase (CONJP), interjection phrase (INTJ),
list marker (LST), noun phrase (NP), preposition
phrase (PP), particle (PRT), subordinated clause
(SBAR), unlike coordinated phrase (UCP), verb
phrase (VP). (See details in (Tjong Kim Sang and
Buchholz, 2000).)
The second definition used is that of atomic
phrases. An atomic phrase represents the most
basic phrase with no nested sub-phrases. For ex-
ample, in the parse tree,
( (S (NP (NP Pierre Vinken) , (ADJP
(NP 61 years) old) ,) (VP will (VP join
(NP the board) (PP as (NP a nonexecu-
tive director)) (NP Nov. 29))) .))
Pierre Vinken, 61 years, the board,
a nonexecutive director and Nov.
29 are atomic phrases while other higher-level
phrases are not. That is, an atomic phrase denotes
a tightly coupled message unit which is just
above the level of single words.
2.1 Parsers
We perform our comparison using two state-of-
the-art parsers. For the full parser, we use the
one developed by Michael Collins (Collins, 1996;
Collins, 1997) ? one of the most accurate full
parsers around. It represents a full parse tree as
a set of basic phrases and a set of dependency
relationships between them. Statistical learning
techniques are used to compute the probabilities
of these phrases and of candidate dependency re-
lations occurring in that sentence. After that, it
will choose the candidate parse tree with the high-
est probability as output. The experiments use
the version that was trained (by Collins) on sec-
tions 02-21 of the Penn Treebank. The reported
results for the full parse tree (on section 23) are
recall/precision of 88.1/87.5 (Collins, 1997).
The shallow parser used is the SNoW-based
CSCL parser (Punyakanok and Roth, 2001;
Munoz et al, 1999). SNoW (Carleson et al,
1999; Roth, 1998) is a multi-class classifier that
is specifically tailored for learning in domains
in which the potential number of information
sources (features) taking part in decisions is very
large, of which NLP is a principal example. It
works by learning a sparse network of linear func-
tions over a pre-defined or incrementally learned
feature space. Typically, SNoW is used as a
classifier, and predicts using a winner-take-all
mechanism over the activation value of the tar-
get classes. However, in addition to the predic-
tion, it provides a reliable confidence level in the
prediction, which enables its use in an inference
algorithm that combines predictors to produce a
coherent inference. Indeed, in CSCL (constraint
satisfaction with classifiers), SNoW is used to
learn several different classifiers ? each detects
the beginning or end of a phrase of some type
(noun phrase, verb phrase, etc.). The outcomes
of these classifiers are then combined in a way
that satisfies some constraints ? non-overlapping
constraints in this case ? using an efficient con-
straint satisfaction mechanism that makes use of
the confidence in the classifier?s outcomes.
Since earlier versions of the SNoW based
CSCL were used only to identify single
phrases (Punyakanok and Roth, 2001; Munoz
et al, 1999) and never to identify a collection
of several phrases at the same time, as we do
here, we also trained and tested it under the exact
conditions of CoNLL-2000 (Tjong Kim Sang and
Buchholz, 2000) to compare it to other shallow
parsers. Table 1 shows that it ranks among the
top shallow parsers evaluated there 1.
Table 1: Rankings of Shallow Parsers in
CoNLL-2000. See (Tjong Kim Sang and Buch-
holz, 2000) for details.
Parsers Precision( ) Recall(  )  (  )

KM00	 93.45 93.51 93.48

Hal00	 93.13 93.51 93.32

CSCL 	 * 93.41 92.64 93.02

TKS00 	 94.04 91.00 92.50

ZST00 	 91.99 92.25 92.12

Dej00	 91.87 91.31 92.09

Koe00	 92.08 91.86 91.97

Osb00	 91.65 92.23 91.94

VB00	 91.05 92.03 91.54

PMP00 	 90.63 89.65 90.14

Joh00	 86.24 88.25 87.23

VD00 	 88.82 82.91 85.76
Baseline 72.58 82.14 77.07
2.2 Data
Training was done on the Penn Treebank (Mar-
cus et al, 1993) Wall Street Journal data, sections
02-21. To train the CSCL shallow parser we had
first to convert the WSJ data to a flat format that
directly provides the phrase annotations. This is
done using the ?Chunklink? program provided for
CoNLL-2000 (Tjong Kim Sang and Buchholz,
2000).
Testing was done on two types of data. For
the first experiment, we used the WSJ section 00
(which contains about 45,000 tokens and 23,500
phrases). The goal here was simply to evaluate
the full parser and the shallow parser on text that
is similar to the one they were trained on.
1We note that some of the variations in the results are
due to variations in experimental methodology rather than
parser?s quality. For example, in [KM00], rather than learn-
ing a classifier for each of the 

 different phrases, a discrim-
inator is learned for each of the 


phrase pairs which, sta-
tistically, yields better results. [Hal00] also uses  different
parsers and reports the results of some voting mechanism on
top of these.
Our robustness test (section 3.2) makes use
of section 4 in the Switchboard (SWB) data
(which contains about 57,000 tokens and 17,000
phrases), taken from Treebank 3. The Switch-
board data contains conversation records tran-
scribed from phone calls. The goal here was two
fold. First, to evaluate the parsers on a data source
that is different from the training source. More
importantly, the goal was to evaluate the parsers
on low quality data and observe the absolute per-
formance as well as relative degradation in per-
formance.
The following sentence is a typical example of
the SWB data.
Huh/UH ,/, well/UH ,/, um/UH
,/, you/PRP know/VBP ,/, I/PRP
guess/VBP it/PRP ?s/BES pretty/RB
deep/JJ feelings/NNS ,/, uh/UH ,/,
I/PRP just/RB ,/, uh/UH ,/, went/VBD
back/RB and/CC rented/VBD ,/, uh/UH
,/, the/DT movie/NN ,/, what/WP is/VBZ
it/PRP ,/, GOOD/JJ MORNING/NN
VIET/NNP NAM/NNP ./.
The fact that it has some missing words, repeated
words and frequent interruptions makes it a suit-
able data to test robustness of parsers.
2.3 Representation
We had to do some work in order to unify the in-
put and output representations for both parsers.
Both parsers take sentences annotated with POS
tags as their input. We used the POS tags in the
WSJ and converted both the WSJ and the SWB
data into the parsers? slightly different input for-
mats. We also had to convert the outcomes of the
parsers in order to evaluate them in a fair way.
We choose CoNLL-2000?s chunking format as
our standard output format and converted Collins?
parser outcome into this format.
2.4 Performance Measure
The results are reported in terms of precision, re-
call, and  ffPhraseNet:
Towards Context Sensitive Lexical Semantics?
Xin Li?, Dan Roth?, Yuancheng Tu?
Dept. of Computer Science?
Dept. of Linguistics?
University of Illinois at Urbana-Champaign
{xli1,danr,ytu}@uiuc.edu
Abstract
This paper introduces PhraseNet, a context-
sensitive lexical semantic knowledge base sys-
tem. Based on the supposition that seman-
tic proximity is not simply a relation between
two words in isolation, but rather a relation
between them in their context, English nouns
and verbs, along with contexts they appear in,
are organized in PhraseNet into Consets; Con-
sets capture the underlying lexical concept, and
are connected with several semantic relations
that respect contextually sensitive lexical infor-
mation. PhraseNet makes use of WordNet as
an important knowledge source. It enhances
a WordNet synset with its contextual informa-
tion and refines its relational structure by main-
taining only those relations that respect con-
textual constraints. The contextual informa-
tion allows for supporting more functionali-
ties compared with those of WordNet. Nat-
ural language researchers as well as linguists
and language learners can gain from accessing
PhraseNet with a word token and its context, to
retrieve relevant semantic information.
We describe the design and construction of
PhraseNet and give preliminary experimental
evidence to its usefulness for NLP researches.
1 Introduction
Progress in natural language understanding research ne-
cessitates significant progress in lexical semantics and
the development of lexical semantics resources. In
a broad range of natural language applications, from
?Research supported by NSF grants IIS-99-84168,
ITR-IIS-00-85836 and an ONR MURI award.
Names of authors are listed alphabetically.
prepositional phrase attachment (Pantel and Lin, 2000;
Stetina and Nagao, 1997), co-reference resolution (Ng
and Cardie, 2002) to text summarization (Saggion and
Lapalme, 2002), semantic information is a necessary
component in the inference, by providing a level of ab-
straction that is necessary for robust decisions.
Inducing that the prepositional phrase in ?They ate
a cake with a fork? has the same grammatical
function as that in ?They ate a cake with a
spoon?, for example, depends on the knowledge that
?cutlery? and ?tableware? are the hypernyms of both
?fork? and ?spoon?. However, the noun ?fork? has five
senses listed in WordNet and each of them has several
different hypernyms. Choosing the correct one is a con-
text sensitive decision.
WordNet (Fellbaum, 1998), a manually constructed
lexical reference system provides a lexical database along
with semantic relations among the lexemes of English
and is widely used in NLP tasks today. However, Word-
Net is organized at the word level, and at this level, En-
glish suffers ambiguities. Stand-alone words may have
several meanings and take on relations (e.g., hypernyms,
hyponyms) that depend on their meanings. Consequently,
there are very few success stories of automatically us-
ing WordNet in natural language applications. In many
cases, reported (and unreported) problems are due to the
fact that WordNet enumerates all the senses of polyse-
mous words; attempts to use this resource automatically
often result in noisy and non-uniform information (Brill
and Resnik, 1994; Krymolowski and Roth, 1998).
PhraseNet is designed based on the assumption that,
by and large, semantic ambiguity in English disappears
when local context of words is taken into account. It
makes use of WordNet as an important knowledge source
and is generated automatically using WordNet and ma-
chine learning based processing of large English corpora.
It enhances a WordNet synset with its contextual informa-
tion and refines its relational structure, including relations
such as hypernym, hyponym, antonym and synonym, by
maintaining only those links that respect contextual con-
straints. However, PhraseNet is not just a functional ex-
tension of WordNet. It is an independent lexical semantic
system allied with proper user interfaces and access func-
tions that will allow researchers and practitioners to use
it in applications.
As stated before, PhraseNet, is built on the assumption
that linguistic context is an indispensable factor affecting
the perception of a semantic proximity between words.
In its current design, PhraseNet defines ?context? hierar-
chically with three abstraction levels: abstract syntactic
skeletons, such as
[(S)? (V )? (DO)? (IO)? (P )? (N)]
which stands for Subject, Verb, Direct Object, Indi-
rect Object, Preposition and Noun(Object) of the Prepo-
sition, respectively; syntactic skeletons whose compo-
nents are enhanced by semantic abstraction, such as
[Peop ? send ? Peop ? gift ? on ? Day] and fi-
nally concrete syntactic skeletons from real sentences as
[they ? send?mom? gift? on? Christmas].
Intuitively, while ?candle? and ?cigarette? would score
poorly on semantic similarity without any contextual in-
formation, their occurrence in sentences such as ?John
tried to light a candle/cigarette? may
highlight their connection with the process of burning.
PhraseNet captures such constraints from the contextual
structures extracted automatically from natural language
corpora and enumerates word lists with their hierarchical
contextual information. Several abstractions are made in
the process of extracting the context in order to prevent
superfluous information and support generalization.
The basic unit in PhraseNet is a conset, a word in its
context, together with all relations associated with it. In
the lexical database, consets are chained together via their
similar or hierarchical contexts. By listing every context
extracted from large corpora and all the generalized con-
texts based on those attested sentences, PhraseNet will
have much more consets than synsets in WordNet. How-
ever, the organization of PhraseNet respects the syntactic
structure together with the distinction of senses of each
word in its corresponding contexts.
For example, rather than linking all hypernyms of a
polysemous word to a single word token, PhraseNet con-
nects the hypernym of each sense to the target word in
every context that instantiates that sense. While in Word-
Net every word has an average of 5.4 hypernyms, in
PhraseNet, the average number of hypernyms of a word
in a conset is 1.51.
In addition to querying WordNet semantic relations
to disambiguate consets, PhraseNet alo maintains fre-
1The statistics is taken over 200, 000 words from a mixed
corpus of American English.
quency records of each word in its context to help dif-
ferentiate consets and makes use of defined similarity be-
tween contexts in this process 2.
Several access functions are built into PhraseNet that
allow retrieving information relevant to a word and its
context. When accessed with words and their contextual
information, the system tends to output more relevant se-
mantic information due to the constraint set by their syn-
tactic contexts.
While still in preliminary stages of development and
experimentation and with a lot of functionalities still
missing, we believe that PhraseNet is an important effort
towards building a contextually sensitive lexical semantic
resource, that will be of much value to NLP researchers
as well as linguists and language learners.
The rest of this paper is organized as follows. Sec. 2
presents the design principles of PhraseNet. Sec. 3 de-
scribes the construction of PhraseNet and the current
stage of the implementation. An application that pro-
vides a preliminary experimental evaluation is described
in Sec. 4. Sec. 5 discuses some related work on lexical se-
mantics resources and Sec. 6 discusses future directions
within PhraseNet.
2 The Design of PhraseNet
Context is one important notion in PhraseNet. While the
context may mean different things in natural language,
many previous work in statistically natural language pro-
cessing defined ?context? as an n-word window around
the target word (Gale et al, 1992; Brown et al, 1991;
Roth, 1998). In PhraseNet, ?context? has a more precise
definition that depends on the grammatical structure of a
sentence rather than simply counting surrounding words.
We define ?context? to be the syntactic structure of the
sentence in which the word of interest occurs. Specif-
ically, we define this notion at three abstraction levels.
The highest level is the abstract syntactic skeleton of the
sentence. That is, it is in the form of the different combi-
nations of six syntactic components. Some components
may be missing as long as the structure is from a legit-
imate English sentence. The most complete form of the
abstract syntactic skeleton is:
[(S)? (V )? (DO)? (IO)? (P )? (N)] (1)
which captures all of the six syntactic components such
as Subject, Verb, Direct Object, Indirect Object, Prepo-
sition and Noun(Object) of Preposition, respectively, in
the sentence. And all components are assumed to be
arranged to obey the word order in English. The low-
est level of contexts is the concrete instantiation of the
stated syntactic skeleton, such as [Mary(S)?give(V )?
John(DO) ? gift(IO) ? on(P ) ? birthday(N)] and
2See details in Sec. 3
[I(S)? eat(V )? bread(DO)? with(P )? hand(N)]
which are extracted directly from corpora with grammat-
ical lemmatization done during the process. Therefore,
all word tokens are in their lemma format. The middle
layer(s) consists of generalized formats of the syntactic
skeleton. For example, the first example given above can
be generalized as [Peop(S)?give(V )?Peop(DO)?
Possession(IO) ? on(P ) ?Day(N)] by replacing
some of its components with more abstract semantic con-
cepts.
PhraseNet organizes nouns and verbs into ?consets?
and a ?conset? is defined as a context with all its
corresponding pointers (edges) to other consets. The
context that forms a conset can be either directly ex-
tracted from the corpus, or at a certain level of ab-
straction. For example, both [Mary(S) ? eat(V ) ?
cake(DO) ? on(P ) ? birthday(N), {p1, p2, . . . , pn}]
and [Peop(S) ? eat(V ) ? Food(DO) ? on(P ) ?
Day(N), {p1, p2, . . . , pn}] are consets.
Two types of relational pointers are defined currently
in PhraseNet: Equal and Hyper. Both of these two re-
lations are based on the context of each conset. Equal
is defined among consets with same number of compo-
nents and same syntactic ordering, i.e, some contexts
under the same abstract syntactic structure (the highest
level of context as defined in this paper). It is defined
that the Equal relation exists among consets whose con-
texts are with same abstract syntactic skeleton, if there is
only one component at the same position that is differ-
ent. For example, [Mary(S)?give(V )?John(DO)?
gift(IO)?on(P )?birthday(N), {p1, p2, . . . , pn}] and
[Mary(S) ? send(V ) ? John(DO) ? gift(IO) ?
on(P ) ? birthday(N), {p1, p2, . . . , pk}] are equal be-
cause the syntactic skeleton each of them has is the
same, i.e., [(S) ? (V ) ? (DO) ? (IO) ? (P ) ? (N)]
and except one word in the verb position that is differ-
ent, i.e., ?give? and ?send?, all other five components
at the corresponding same position are the same. The
Equal relation is transitive only with regard to a spe-
cific component in the same position. For example,
to be transitive to the above two example consets, the
Equal conset should be also different from them only
by its verb. The Hyper relation is also defined for con-
sets with same abstract syntactic structure. For conset
A and conset B, if they have the same syntactic struc-
ture, and if there is at least one component of the con-
text in A that is the hypernym of the component in that
of B at the corresponding same position, and all other
components are the same respectively, A is the Hyper
conset of B. For example, both [Molly(S) ? hit(V ) ?
Body(DO), {p1, p2, . . . , pj}] and [Peop(S)?hit(V )?
Body(DO), {p1, p2, . . . , pn}] are Hyper consets of
[Molly(S)?hit(V )?nose(DO), {p1, p2, . . . , pk}]. The
intuition behind these two relations is that the Equal rela-
Figure 1: The basic organization of PhraseNet: The upward
arrow denotes the Hyper relation and the dotted two-way arrow
with a V above denotes the Equal relation that is transitive with
regard to the V component.
tion can cluster a list of words which occur in exactly the
same contextual structure and if the extreme case occurs,
namely when the same context in all these equal consets
with regard to a specific syntactic component groups vir-
tually any nouns or verbs, the Hyper relation can be used
here for further disambiguation.
To summarize, PhraseNet can be thought of as a graph
on consets. Each node is a context and edges between
nodes are relations defined by the context of each node.
They are either Equal or Hyper. Equal relation can be
derived by matching consets and it is easy to implement
while building the Hyper relation requires the assistance
of WordNet and the defined Equal relation. Semantic re-
lations among words can be generated using the two types
of defined edges. For example, it is likely that the target
words in all equal consets with transitivity have similar
meaning. If this is not true at the lowest lower of contexts,
it is more likely to be true at higher, i.e., more generalized
level. Figure 1 shows a simple example reflecting the pre-
liminary design of PhraseNet.
After we get the similar meaning lists based on their
contexts, we can build interaction from this word list to
WordNet and inherit other semantic relations from Word-
Net. However, each member of a word list can help to dis-
ambiguate other members in this list. Therefore, it is ex-
pected that with the pruning assisted by list members, i.e.,
the disambiguation by truncating semantic relations asso-
ciated with each synset in WordNet, the extract meaning
in the context together with all other semantic relations
such as hypernyms, holonyms, troponyms, antonyms can
be derived from WordNet.
In the next two sections we describe our current im-
plementation of these operations and preliminary experi-
ments we have done with them.
2.1 Accessing PhraseNet
Retrieval of information from PhraseNet is done via sev-
eral access functions that we describe below. PhraseNet
is designed to be accessed via multiple functions with
flexible input modes set by the user. These functions
may allow users to exploit several different functionali-
ties of PhraseNet, depending on their goal and amount of
resources they have.
An access function in PhraseNet has two components.
The first component is the input, which can vary from
a single word token to a word with its complete con-
text. The second component is the functionality, which
ranges over simple retrieval and several relational func-
tions, modelled after WordNet relations.
The most basic and simplest way to query PhraseNet
is with a single word. In this case, the system outputs all
contexts the word can occur in, and its related words in
each context.
PhraseNet can also be accessed with input that consists
of a single word token along with its context information.
Context information refers to any of the elements in the
syntactic skeleton defined in Eq. 1, namely, Subject(S),
Verb(V), Direct Object(DO), Indirect Object(IO), Prepo-
sition(P) and Noun(Object) of the Preposition(N). The
contextual roles S, V, DO, IO, P or N or any subset of
them, can be specified by the user or derived by an appli-
cation making use of a shallow or full parser. The more
information the user provides, the more specific the re-
trieved information is.
To ease the requirements from the user, say, in case
no information of this form is available to the user,
PhraseNet will, in the future, have functions that allow a
user to supply a word token and some context, where the
functionality of the word in the context is not specified.
See Sec. 6 for a discussion.
Function Name Input Variables Output
PN WL Word [, Context] Word List
PN RL Word [, Context] WordNet relations
PN SN Word [, Context] Sense
PN ST Context Sentence
Table 1: PhraseNet Access Functions: PhraseNet access
functions along with their input and output. [i] denotes optional
input. PN RL is a family of functions, modelled after WordNet
relations.
Table 1 lists the functionality of the access functions in
PhraseNet. If the user only input a word token without
any context, all those designed functions will return each
context the input word occurs together with the wordlist
in these contexts. Otherwise, the output is constrained by
the input context. The functions are described below:
PN WL takes the optional contextual skeleton and one
specified word in that context as inputs and returns
the corresponding wordlist occurring in that context
or a higher level of context. A parameter to this
function specifies if we want to get the complete
wordlist or those words in the list that satisfy a spe-
cific pruning criterion. (This is the function used in
the experiments in Sec. 4.)
PN RL is modelled after the WordNet access functions.
It will return all words in those contexts that are
linked in PhraseNet by their Equal or Hyper rela-
tion. Those words can help to access WordNet to
derive all lexical relations stored there.
PN SN is modelled after the semantic concordance
in (Landes et al, 1998). It takes a word token and
an optional context as input, and returns the sense
of the word in that context. Similarly to PN RL this
function is implemented by appealing to WordNet
senses and pruning the possible sense based on the
wordlist determined for the given context.
PN ST is not implemented at this point, but is designed
to output a sentence that has same structure as the
input context, but use different words. It is inspired
by the work on reformulation, e.g., (Barzilay and
McKeown, 2001).
We can envision many ways users of PhraseNet can
make use of the retrieved information. At this point in the
life of PhraseNet we focus mostly on using PhraseNet as
a way to acquire semantic features to aid learning based
natural language applications. This determines our prior-
ities in the implementation that we describe next.
3 Constructing PhraseNet
Constructing PhraseNet involves three main stages: (1)
extracting syntactic skeletons from corpora, (2) con-
structing the core element in PhraseNet: consets, and (3)
developing access functions.
The first stage makes use of fully parsed data. In
constructing the current version of PhraseNet we used
two corpora. The first, relatively small corpus of the
1.1 million-word Penn-State Treebank which consists
of American English news articles (WSJ), and is fully
parsed. The second corpus has about 5 million sentences
of the TREC-11 (Voorhees, 2002), also containing mostly
American English news articles (NYT, 1998) and parsed
with Dekang Lin?s minipar parser (Lin, 1998a).
In the near future we are planning to construct a much
larger version of PhraseNet, using Trec-10 and Trec-11
data sets, which cover about 8 GB of text. We believe that
the size is very important here, and will add significant
robustness to our results.
To reduce ostensibly different contexts, two important
abstractions take place at this stage. (1) Syntactic lemma-
tization to get the lemma for both nouns and verbs in
the context defined in Eq. 1. For data parsed via Lin?s
minipar, the lexeme of each word is already included
in the parser. (2) Sematic categorization to unify pro-
nouns, proper names of people, locations and organiza-
tion as well as numbers. This semantic abstraction cap-
tures the underlying semantic proximity by categorizing
multitudinous surface-form proper names into one repre-
senting symbol.
While the first abstraction is simple the second is not.
At this point we use an NE tagger we developed our-
selves based on the approach to phrase identification de-
veloped in (Punyakanok and Roth, 2001). Note that this
abstraction handles multiword phrases. While the accu-
racy of the NE tagger is around 90%, we have yet to ex-
periment with the implication of this additional noise on
PhraseNet.
At the end of this stage, each sentence in the original
corpora is transformed into a single context either at
the lowest level or a more generalized instantiation
(with name entity tagged). For example, ?For six
years, T. Marshall Hahn Jr. has made
corporate acquisitions in the George
Bush mode: kind and gentle.?, changes to:
[Peop?make? acquisition? in?mode].
The second stage of constructing PhraseNet concen-
trates on constructing the core element in PhraseNet:
consets.
To do that, for each context, we collect wordlists that
contain those words that we determine to be admissible in
the context(or contexts share the equal relation). The first
step in constructing the wordlists in PhraseNet is to fol-
low the most strict definition ? include those words that
actually occur in the same context in the corpus. This in-
volves all Equal consets with the transitive property to
a specific syntactic component. We then apply to the
wordlists three types of pruning operations that are based
on (1) frequency of word occurrences in identical or simi-
lar contexts; (2) categorization of words in wordlist based
on clustering all contexts they occur in, and (3) pruning
via the relational structure inherited from WordNet - we
prune from the wordlist outliers in terms of this relational
structure. Some of these operations are parameterized
and determining the optimal setting is an experimental
issue.
1. Every word in a conset wordlist has a frequency
record associated with it, which records the fre-
quency of the word in its exact context. We prune
words with a frequency below k (with the current
corpus we choose k = 3). A disadvantage of
this pruning method is that it might filter out some
appropriate words with a low frequency in reality.
For example, for the partial context [strategy ?
involve? ? ? ? ? ?], we have:
[strategy - involve - * - * - *, < DO : advertisement
4, abuse 1, campaign 2, compromise 1, everything 1,
fumigation 1, item 1, membership 1, option 3, stock-
option 1> ]
In this case,?strategy? is the subject and ?involve?
is the predicate and all words in the list serve as the
direct object. The number in the parentheses is the
frequency of the token. With k = 3 we actually get
as a wordlist only: < advertisment, option >.
2. There are several ways to prune wordlists based on
the different contexts words may occur in. This in-
volves a definition of similar contexts and threshold-
ing based on the number of such contexts a word oc-
curs in. At this point, we implement the construction
of PhraseNet using a clustering of contexts, as done
in (Pantel and Lin, 2002). An exhaustive PhraseNet
list is intersected with word lists generated based on
clustered contexts given by (Pantel and Lin, 2002).
3. We prune from the wordlist outliers in terms of the
relational structure inherited from WordNet. Cur-
rently, this is implemented only using the hypernym
relation. The hypernym shared by the highest num-
ber of words in the wordlist is kept in the database.
For example, by searching ?option? in WordNet, we
get its three senses. Then we collect the hypernyms
of ?option? from all the senses as follows:
05319492(a financial instrument whose value is
based on another security)
04869064(the cognitive process of reaching a deci-
sion)
00026065(something done)
We do this for every word in the original list and find
out the hypernym(s) shared by the highest number of
words in the original wordlist. The final pick in this
case is the synset 05319492 which is shared by both
?option? and ?stock option? as their hypernym.
The third stage is to develop the access functions. As
mentioned before, while we envision many ways users
of PhraseNet can use the retrieved information, at this
preliminary stage of PhraseNet we focus mostly on us-
ing PhraseNet as a way to supply abstract semantic fea-
tures that learning based natural language applications
can benefit from.
For this purpose, so far we have only used and evalu-
ated the function PN WL. PN WL takes as input as
specific word and (optionally) its context and returns a
lists of words which are semantically related to the target
word in the given context. For example,
PN WL ( V= protest, [peop - legislation - * - * - * ])=
[protest, resist, dissent, veto, blackball, negative, for-
bid, prohibit, interdict, proscribe, disallow ].
This function can be implemented via any of the three
pruning methods discussed earlier (see Sec. 4). This
wordlists that this function outputs, can be used to aug-
ment feature based representations for other, learning
based, NLP tasks. Other access functions of PhraseNet
can serve in other ways, e.g., expansions in information
retrieval, but we have not experimented with it yet.
With the experiments we are doing right now,
PhraseNet only takes inputs with the context information
in the format of Eq. 1. Semantic categorization and syn-
tactic lemmatization of the context is required in order to
get matched in the database. However, PhraseNet will,
in the future, have functions that allow a user to supply a
word token and more flexible contexts.
4 Evaluation and Application
In this section we provide a first evaluation of PhraseNet.
We do that in the context of a learning task.
Learning tasks in NLP are typically modelled as clas-
sification tasks, where one seeks a mapping g : X ?
c1, ..., ck, that maps an instance x ? X (e.g., a sentence)
to one of c1, ..., ck ? representing some properties of the
instance (e.g., a part-of-speech tag of a word in the con-
text of the sentence). Typically, the raw representation
? sentence or document ? are first mapped to some fea-
ture based representation, and then a learning algorithm
is applied to learn a mapping from this representation to
the desired property (Roth, 1998). It is clear that in most
cases representing the mapping g in terms of the raw rep-
resentation of the input instance ? words and their order
? is very complex. Functionally simple representations
of this mapping can only be formed if we augment the
information that is readily available in the input instance
with additional, more abstract information. For exam-
ple, it is common to augment sentence representations
with syntactic categories ? part-of-speech (POS), under
the assumption that the sought-after property, for which
we seek the classifier, depends on the syntactic role of a
word in the sentence rather than the specific word. Sim-
ilar logic can be applied to semantic categories. In many
cases, the property seems not to depend on the specific
word used in the sentence ? that could be replaced with-
out affecting this property ? but rather on its ?meaning?.
In this section we show the benefit of using PhraseNet
in doing that in the context of Question Classification.
Question classification (QC) is the task of determining
the semantic class of the answer of a given question.
For example, given the question: ?What Cuban
dictator did Fidel Castro force out
of power in 1958?? we would like to determine
that its answer should be a name of a person. Our
approach to QC follows that of (Li and Roth, 2002).
The question classifier used is a multi-class classifier
which can classify a question into one of 50 fine-grained
classes.
The baseline classifier makes use of syntactic features
like the standard POS information and information ex-
tracted by a shallow parser in addition to the words in
the sentence. The classifier is then augmented with stan-
dard WordNet or with PhraseNet information as follows.
In all cases, words in the sentence are augmented with
additional words that are supposed to be semantically re-
lated to them. The intuition, as described above, is that
this provides a level of abstract ? we could have poten-
tially seen an equivalent question, where other ?equiva-
lent? words occur.
For WordNet, for each word in a question, all its hyper-
nyms are added to its feature based representation (in ad-
dition to the syntactic features). For PhraseNet, for each
word in a question, all the words in the corresponding
conset wordlist are added (where the context is supplied
by the question).
Our experiments compare the three pruning operations
described above. Training is done on a data set of 21,500
questions. Performance is evaluated by the precision of
classifying 1,000 test questions, defined as follows:
Precison = # of correct predictions# of predictions (2)
Table 2 presents the classification precision before and
after incorporating WordNet and PhraseNet information
into the classifier. By augmenting the question classi-
fier with PhraseNet information, even in this preliminary
stage, the error rate of the classifier can be reduced by
12%, while an equivalent use of WordNet information re-
duces the error by only 5.7%.
Information Used Precision Err Reduction
Baseline 84.2% 0%
WordNet 85.1% 5.7%
PN: Freq. based Pruning 84.4% 1.3%
PN: Categ. based Pruning 85% 5.1%
PN: Relation based Pruning 86.1% 12%
Table 2: Question Classification with PhraseNet Informa-
tion Question classification precision and error rate reduction
compared with the baseline error rate(15.8%) by incorporat-
ing WordNet and PhraseNet(PN) information. ?Baseline? is
the classifier that uses only syntactic features. The classifier
is trained over 21,500 questions and tested over 1000 TREC 10
and 11 questions.
5 Related Work
In this section we point to some of the related work
on syntax, semantics interaction and lexical semantic re-
sources in computational linguistics and natural language
processing. Many current syntactic theories make the
common assumption that various aspects of syntactic al-
ternation are predicable via the meaning of the predi-
cate in the sentence (Fillmore, 1968; Jackendoff, 1990;
Levin, 1993). With the resurgence of lexical seman-
tics and corpus linguistics during the past two decades,
this so-called linking regularity triggers a broad interest
of using syntactic representations illustrated in corpora
to classify lexical meaning (Baker et al, 1998; Levin,
1993; Dorr and Jones, 1996; Lapata and Brew, 1999; Lin,
1998b; Pantel and Lin, 2002).
FrameNet (Baker et al, 1998) produces a seman-
tic dictionary that documents combinatorial properties
of English lexical items in semantic and syntactic terms
based on attestations in a very large corpus. In FrameNet,
a frame is an intuitive structure that formalizes the links
between semantics and syntax in the results of lexical
analysis. (Fillmore et al, 2001) However, instead of de-
rived via attested sentences from corpora automatically,
each conceptual frame together with all its frame ele-
ments has to be constructed via slow and labor-intensive
manual work. FrameNet is not constructed automatically
based on observed syntactic alternations. Though deep
semantic analysis is built for each frame, lack of auto-
matic derivation of the semantic roles from large corpora3
confines the usage of this network drastically.
Levin?s classes (Levin, 1993) of verbs are based on the
assumption that the semantics of a verb and its syntactic
behavior are predictably related. She defines 191 verb
classes by grouping 4183 verbs which pattern together
with respect to their diathesis alternations, namely alter-
nations in the expressions of arguments. In Levin?s clas-
sification, it is the syntactic skeletons (such as np-v-np-
pp)to classify verbs directly. Levin?s classification is val-
idated via experiments done by (Dorr and Jones, 1996)
and some counter-arguments are in (Baker and Ruppen-
hofer, 2002). Her work provides a a small knowledge
source that needs further expansion.
Lin?s work (Lin, 1998b; Pantel and Lin, 2002) makes
use of distributional syntactic contextual information to
define semantic proximity. Dekang Lin?s grouping of
similar words is a combination of the abstract syntactic
skeleton and concrete word tokens. Lin uses syntactic de-
pendencies such as ?Subj-people?, ?Modifier-red?, which
combine both abstract syntactic notations and their con-
crete word token representations. He applies this method
to classifying not only verbs, but also nouns and adjec-
tives. While no evaluation has ever been done to deter-
mine if concrete word tokens are necessary when the syn-
tactic phrase types are already presented, Lin?s work in-
directly shows that the concrete lexical representation is
effective.
WordNet (Fellbaum, 1998) by far is the most widely
used semantic database. However, this database does not
3The attempt to label these semantic roles automatically in
(Gildea and Jurafsky, 2002) assumes knowledge of the frame
and covers only 20% of them.
always work as successfully as researchers have expected
(Krymolowski and Roth, 1998; Montemagni and Pirelli,
1998). This seems to be due to lack of topical context
(Harabagiu et al, 1999; Agirre et al, 2001) as well as
local context (Fellbaum, 1998). By adding contextual in-
formation, many researchers, (e.g., (Green et al, 2001;
Lapata and Brew, 1999; Landes et al, 1998)), have al-
ready made some improvements over it.
The work on the importance of connecting syntax and
semantics in developing lexical semantic resources shows
the importance of contextual information as a step to-
wards deeper level of processing. With hierarchical sen-
tential local contexts embedded and used to categorize
word classes automatically, we believe that PhraseNet
provides the right direction for building useful lexical se-
mantic database.
6 Discussion and Further Work
We believe that progress in semantics and in develop-
ing lexical resources is a prerequisite to any signifi-
cant progress in natural language understanding. This
work makes a step in this direction by introducing a
context-sensitive lexical semantic knowledge base sys-
tem, PhraseNet. We have argued that while cur-
rent lexical resources like WordNet are invaluable, we
should move towards contextually sensitive resources.
PhraseNet is designed to fill this gap, and our preliminary
experiments with it are promising.
PhraseNet is an ongoing project and is still in its pre-
liminary stage. There are several key issues that we are
currently exploring. First, given that PhraseNet draws
part of it power from corpora, we are planning to en-
large the corpus used. We believe that the data size
is very important and will add significant robustness to
our current results. At the same time, since construct-
ing PhraseNet relies on machine learning techniques, we
need to study extensively the effect of tuning these on
the reliability of PhraseNet. Second, there are several
functionalities and access functions that we are planning
to augment PhraseNet with. Among those is the ability
of allowing a user to query PhraseNet even without ex-
plicitly specifying the role of words in the context. This
would reduce the requirement for users and applications
using PhraseNet. Finally, current PhraseNet has no lexi-
cal information about adjectives and adverbs, which may
contain important distributional information about their
modified nouns or verbs. We would like to take this in-
formation into consideration in the near future.
References
E. Agirre, O. Ansa, D. Martinez, and E. Hovy. 2001. Enriching
wordnet concepts with topic signatures.
C. Baker and J. Ruppenhofer. 2002. Framenet?s frames vs.
levin?s verb classes. In Proceedings of the 28th Annual Meet-
ing of the Berkeley Linguistics Society.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In Christian Boitet and Pete Whitelock,
editors, Proceedings of the Thirty-Sixth Annual Meeting of
the Association for Computational Linguistics and Seven-
teenth International Conference on Computational Linguis-
tics, pages 86?90, San Francisco, California. Association for
Computational Linguistics, Morgan Kaufmann Publishers.
R. Barzilay and K. R. McKeown. 2001. Extracting paraphrases
from a parallel corpus. In Proceeding of the 10th Conference
of the European Chapter of ACL.
E. Brill and P. Resnik. 1994. A rule-based approach to prepo-
sitional phrase attachment disambiguation. In Proc. of COL-
ING.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mercer.
1991. Word sense disambiguation using statistical methods.
In Proceedings of ACL-1991.
B. Dorr and D. Jones. 1996. Role of word-sense disambigua-
tion in lexical acquisition.
C. Fellbaum. 1998. In C. Fellbaum, editor, WordNet: An Elec-
tronic Lexical Database. The MIT Press.
C. J. Fillmore, C. Wooters, and C. F. Baker. 2001. Building
a large lexical databank which provides deep semantics. In
Proceedings of the Pacific Asian Conference on Language,
Information and Computation, HongKong.
C. J. Fillmore. 1968. The case for case. In Bach and Harms,
editors, Universals in Linguistic Theory, pages 1?88. Holt,
Rinehart, and Winston, New York.
W. A. Gale, K. W. Church, and D. Jarowsky. 1992. A method
for disambiguation word senses in large corpora. Computers
and the Humanities, 26(5-6):415?439.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?288,
September.
R. Green, L. Pearl, B. J. Dorr, and P. Resnik. 2001. Lexical re-
source integration across the syntax-semantics interface. In
Proceedings of WordNet and Other Lexical Resources Work-
shop, NAACL, Pittsburg, June.
S. M. Harabagiu, G. A. Miller, and D. I. Moldovan. 1999.
Wordnet2 - a morphologically and semantically enhanced re-
sources. In Proceedings of ACL-SIGLEX99: Standardizing
Lexical Resources, pages 1?8, Maryland.
R. Jackendoff. 1990. Semantic Structures. MIT Press, Cam-
bridge, MA.
Y. Krymolowski and D. Roth. 1998. Incorporating knowledge
in natural language learning: A case study. In COLING-
ACL?98 workshop on the Usage of WordNet in Natural Lan-
guage Processing Systems.
S. Landes, C. Leacock, and R. I. Tengi. 1998. Building seman-
tic concordances. In C. Fellbaum, editor, WordNet: an Elec-
toronic Lexical Database, pages 199?216. The MIT Press.
M. Lapata and C. Brew. 1999. Using subcategorization to re-
solve verb class ambiguity. In Proceedings of EMNLP, pages
266?274.
B. Levin. 1993. English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago Press,
Chicago, IL.
X. Li and D. Roth. 2002. Learning question classifiers. In
Proceedings of COLING.
D. Lin. 1998a. Dependency-based evaluation of minipar. In
In Workshop on the Evaluation of Parsing Systems Granada
Spain.
D. Lin. 1998b. An information-theoretic definition of similar-
ity. In Proc. 15th International Conf. on Machine Learning,
pages 296?304. Morgan Kaufmann, San Francisco, CA.
S. Montemagni and V. Pirelli. 1998. Augmenting WordNet-
like lexical resources with distributional evidence. an
application-oriented perspective. In S. Harabagiu, editor,
Use of WordNet in Natural Language Processing Systems:
Proceedings of the Conference, pages 87?93. Association for
Computational Linguistics.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of 40th
Annual Meeting of the ACL, TaiPei.
P. Pantel and D. Lin. 2000. An unsupervised approach to
prepositional phrase attachment using contextually similar
words. In Proceedings of Association for Computational
Linguistics, Hongkong.
P. Pantel and D. Lin. 2002. Discovering word senses from text.
In The Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.
V. Punyakanok and D. Roth. 2001. The use of classifiers in
sequential inference. In NIPS-13; The 2000 Conference on
Advances in Neural Information Processing Systems, pages
995?1001. MIT Press.
D. Roth. 1998. Learning to resolve natural language ambigu-
ities: A unified approach. In Proc. National Conference on
Artificial Intelligence, pages 806?813.
H. Saggion and G. Lapalme. 2002. Generating indicative-
informative summaries with sumum. Computational Lin-
guistics, 28(4):497?526.
J. Stetina and M. Nagao. 1997. Corpus based pp attachment
ambiguity rosolution with a semantic dictionary. In Proceed-
ings of the 5th Workshop on Very Large Corpora, Beijing and
Hongkong.
E. Voorhees. 2002. Overview of the TREC-2002 question an-
swering track. In The Eleventh TREC Conference, pages
115?123.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 64?71, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Discriminative Training of Clustering Functions:
Theory and Experiments with Entity Identification
Xin Li and Dan Roth
Department of Computer Science
University of Illinois, Urbana, IL 61801
(xli1,danr)@cs.uiuc.edu
Abstract
Clustering is an optimization procedure that
partitions a set of elements to optimize some
criteria, based on a fixed distance metric de-
fined between the elements. Clustering ap-
proaches have been widely applied in natural
language processing and it has been shown re-
peatedly that their success depends on defin-
ing a good distance metric, one that is appro-
priate for the task and the clustering algorithm
used. This paper develops a framework in
which clustering is viewed as a learning task,
and proposes a way to train a distance metric
that is appropriate for the chosen clustering al-
gorithm in the context of the given task. Ex-
periments in the context of the entity identifi-
cation problem exhibit significant performance
improvements over state-of-the-art clustering
approaches developed for this problem.
1 Introduction
Clustering approaches have been widely applied to nat-
ural language processing (NLP) problems. Typically,
natural language elements (words, phrases, sentences,
etc.) are partitioned into non-overlapping classes, based
on some distance (or similarity) metric defined between
them, in order to provide some level of syntactic or se-
mantic abstraction. A key example is that of class-based
language models (Brown et al, 1992; Dagan et al, 1999)
where clustering approaches are used in order to parti-
tion words, determined to be similar, into sets. This
enables estimating more robust statistics since these are
computed over collections of ?similar? words. A large
number of different metrics and algorithms have been ex-
perimented with these problems (Dagan et al, 1999; Lee,
1997; Weeds et al, 2004). Similarity between words was
also used as a metric in a distributional clustering algo-
rithm in (Pantel and Lin, 2002), and it shows that func-
tionally similar words can be grouped together and even
separated to smaller groups based on their senses. At a
higher level, (Mann and Yarowsky, 2003) disambiguated
personal names by clustering people?s home pages using
a TFIDF similarity, and several other researchers have ap-
plied clustering at the same level in the context of the
entity identification problem (Bilenko et al, 2003; Mc-
Callum and Wellner, 2003; Li et al, 2004). Similarly, ap-
proaches to coreference resolution (Cardie and Wagstaff,
1999) use clustering to identify groups of references to
the same entity.
Clustering is an optimization procedure that takes as
input (1) a collection of domain elements along with (2)
a distance metric between them and (3) an algorithm se-
lected to partition the data elements, with the goal of op-
timizing some form of clustering quality with respect to
the given distance metric. For example, the K-Means
clustering approach (Hartigan and Wong, 1979) seeks to
maximize a measure of tightness of the resulting clusters
based on the Euclidean distance. Clustering is typically
called an unsupervised method, since data elements are
used without labels during the clustering process and la-
bels are not used to provide feedback to the optimiza-
tion process. E.g., labels are not taken into account when
measuring the quality of the partition. However, in many
cases, supervision is used at the application level when
determining an appropriate distance metric (e.g., (Lee,
1997; Weeds et al, 2004; Bilenko et al, 2003) and more).
This scenario, however, has several setbacks. First, the
process of clustering, simply a function that partitions a
set of elements into different classes, involves no learn-
ing and thus lacks flexibility. Second, clustering quality is
typically defined with respect to a fixed distance metric,
without utilizing any direct supervision, so the practical
clustering outcome could be disparate from one?s inten-
tion. Third, when clustering with a given algorithm and
a fixed metric, one in fact makes some implicit assump-
tions on the data and the task (e.g., (Kamvar et al, 2002);
more on that below). For example, the optimal conditions
under which for K-means works are that the data is gen-
erated from a uniform mixture of Gaussian models; this
may not hold in reality.
This paper proposes a new clustering framework that
addresses all the problems discussed above. Specifically,
64
we define clustering as a learning task: in the training
stage, a partition function, parameterized by a distance
metric, is trained with respect to a specific clustering al-
gorithm, with supervision. Some of the distinct proper-
ties of this framework are that: (1) The training stage is
formalized as an optimization problem in which a parti-
tion function is learned in a way that minimizes a clus-
tering error. (2) The clustering error is well-defined and
driven by feedback from labeled data. (3) Training a
distance metric with respect to any given clustering al-
gorithm seeks to minimize the clustering error on train-
ing data that, under standard learning theory assumptions,
can be shown to imply small error also in the application
stage. (4) We develop a general learning algorithm that
can be used to learn an expressive distance metric over
the feature space (e.g., it can make use of kernels).
While our approach makes explicit use of labeled data,
we argue that, in fact, many clustering applications in nat-
ural language also exploit this information off-line, when
exploring which metrics are appropriate for the task. Our
framework makes better use of this resource by incorpo-
rating it directly into the metric training process; training
is driven by true clustering error, computed via the spe-
cific algorithm chosen to partition the data.
We study this new framework empirically on the en-
tity identification problem ? identifying whether differ-
ent mentions of real world entities, such as ?JFK? and
?John Kennedy?, within and across text documents, ac-
tually represent the same concept (McCallum and Well-
ner, 2003; Li et al, 2004). Our experimental results ex-
hibit a significant performance improvement over exist-
ing approaches (20% ? 30% F1 error reduction) on all
three types of entities we study, and indicate its promis-
ing prospective in other natural language tasks.
The rest of this paper discusses existing clustering ap-
proaches (Sec. 2) and then introduces our Supervised Dis-
criminative Clustering framework (SDC) (Sec. 3) and a
general learner for training in it (Sec. 4). Sec. 5 describes
the entity identification problem and Sec. 6 compares dif-
ferent clustering approaches on this task.
2 Clustering in Natural Language Tasks
Clustering is the task of partitioning a set of elements
S ? X into a disjoint decomposition 1 p(S) = {S1, S2,
? ? ? , SK} of S. We associate with it a partition function
p = pS : X ? C = {1, 2, . . .K} that maps each x ? S
to a class index pS(x) = k iff x ? Sk. The subscript S
in pS and pS(x) is omitted when clear from the context.
Notice that, unlike a classifier, the image x ? S under a
partition function depends on S.
In practice, a clustering algorithm A (e.g. K-Means),
and a distance metric d (e.g., Euclidean distance), are typ-
1Overlapping partitions will not be discussed here.
ically used to generate a function h to approximate the
true partition function p. Denote h(S) = Ad(S), the par-
tition of S by h. A distance (equivalently, a similarity)
function d that measures the proximity between two ele-
ments is a pairwise function X ? X ? R+, which can
be parameterized to represent a family of functions ?
metric properties are not discussed in this paper. For ex-
ample, given any two element x1 =< x(1)1 , ? ? ? , x(m)1 >
and x2 =< x(1)2 , ? ? ? , x(m)2 > in an m-dimensional space,
a linearly weighted Euclidean distance with parameters
? = {wl}m1 is defined as:
d?(x1, x2) ?
????
m?
l=1
wl ? |x(l)1 ? x(l)2 |2 (1)
When supervision (e.g. class index of elements) is un-
available, the quality of a partition function h operating
on S ? X , is measured with respect to the distance met-
ric defined over X . Suppose h partitions S into disjoint
sets h(S) = {S?k}K1 , one quality function used in the K-
Means algorithm is defined as:
qS(h) ?
K?
k=1
?
x?S?k
d(x, ??k)2, (2)
where ??k is the mean of elements in set S?k. However, this
measure can be computed irrespective of the algorithm.
2.1 What is a Good Metric?
A good metric is one in which close proximity correlates
well with the likelihood of being in the same class. When
applying clustering to some task, people typically decide
on the clustering quality measure qS(h) they want to op-
timize, and then chose a specific clustering algorithm A
and a distance metric d to generate a ?good? partition
function h. However, it is clear that without any super-
vision, the resulting function is not guaranteed to agree
with the target function p (or one?s original intention).
Given this realization, there has been some work on
selecting a good distance metric for a family of related
problems and on learning a metric for specific tasks. For
the former, the focus is on developing and selecting good
distance (similarity) metrics that reflect well pairwise
proximity between domain elements. The ?goodness?
of a metric is empirically measured when combined with
different clustering algorithms on different problems. For
example (Lee, 1997; Weeds et al, 2004) compare similar-
ity metrics such as the Cosine, Manhattan and Euclidean
distances, Kullback-Leibler divergence, Jensen-Shannon
divergence, and Jaccard?s Coefficient, that could be ap-
plied in general clustering tasks, on the task of measur-
ing distributional similarity. (Cohen et al, 2003) com-
pares a number of string and token-based similarity met-
rics on the task of matching entity names and found that,
65
overall, the best-performing method is a hybrid scheme
(SoftTFIDF) combining a TFIDF weighting scheme of
tokens with the Jaro-Winkler string-distance scheme that
is widely used for record linkage in databases.
d(x1,x2) = [(x1(1) -x2(1))2+(x1(2) -x2(2))2]1/2 d(x1,x2) = |(x1(1) +x2(1))-(x1(2) +x2(2))|
(a) Single-Linkage with Euclidean (b) K-Means with Euclidean (c) K-Means with a Linear Metric
Figure 1: Different combinations of clustering algorithms
with distance metrics. The 12 points, positioned in a two-
dimensional space < X(1), X(2) >, are clustered into two
groups containing solid and hollow points respectively.
Moreover, it is not clear whether there exists any
universal metric that is good for many different prob-
lems (or even different data sets for similar problems)
and is appropriate for any clustering algorithm. For the
word-based distributional similarity mentioned above,
this point was discussed in (Geffet and Dagan, 2004)
when it is shown that proximity metrics that are appro-
priate for class-based language models may not be ap-
propriate for other tasks. We illustrate this critical point in
Fig. 1. (a) and (b) show that even for the same data collec-
tion, different clustering algorithms with the same met-
ric could generate different outcomes. (b) and (c) show
that with the same clustering algorithm, different metrics
could also produce different outcomes. Therefore, a good
distance metric should be both domain-specific and asso-
ciated with a specific clustering algorithm.
2.2 Metric Learning via Pairwise Classification
Several works (Cohen et al, 2003; Cohen and Rich-
man, 2002; McCallum and Wellner, 2003; Li et al,
2004) have tried to remedy the aforementioned problems
by attempting to learn a distance function in a domain-
specific way via pairwise classification. In the training
stage, given a set of labeled element pairs, a function
f : X ? X ? {0, 1} is trained to classify any two el-
ements as to whether they belong to the same class (1)
or not (0), independently of other elements. The dis-
tance between the two elements is defined by converting
the prediction confidence of the pairwise classifier, and
clustering is then performed based on this distance func-
tion. Particularly, (Li et al, 2004) applied this approach
to measuring name similarity in the entity identification
problem, where a pairwise classifier (LMR) is trained us-
ing the SNoW learning architecture (Roth, 1998) based
on variations of Perceptron and Winnow, and using a col-
lection of relational features between a pair of names.
The distance between two names is defined as a softmax
over the classifier?s output. As expected, experimental
evidence (Cohen et al, 2003; Cohen and Richman, 2002;
Li et al, 2004) shows that domain-specific distance func-
tions improve over a fixed metric. This can be explained
by the flexibility provided by adapting the metric to the
domain as well as the contribution of supervision that
guides the adaptation of the metric.
A few works (Xing et al, 2002; Bar-Hillel et al, 2003;
Schultz and Joachims, 2004; Mochihashi et al, 2004)
outside the NLP domain have also pursued this general
direction, and some have tried to learn the metric with
limited amount of supervision, no supervision or by in-
corporating other information sources such as constraints
on the class memberships of the data elements. In most of
these cases, the algorithm practically used in clustering,
(e.g. K-Means), is not considered in the learning proce-
dure, or only implicitly exploited by optimizing the same
objective function. (Bach and Jordan, 2003; Bilenko et
al., 2004) indeed suggest to learn a metric directly in a
clustering task but the learning procedure is specific for
one clustering algorithm.
3 Supervised Discriminative Clustering
To solve the limitations of existing approaches, we de-
velop the Supervised Discriminative Clustering Frame-
work (SDC), that can train a distance function with re-
spect to any chosen clustering algorithm in the context of
a given task, guided by supervision.
A labeled data set S
A SupervisedLearner
Training Stage:Goal: h*=argmin errS(h,p)
A distancemetric d a clustering algorithm A+
A unlabeled data set S? A partition h(S?)
Application Stage: h(S? ) 
A partition function h(S) = Ad(S)
Figure 2: Supervised Discriminative Clustering
Fig. 2 presents this framework, in which a cluster-
ing task is explicitly split into training and application
stages, and the chosen clustering algorithm involves in
both stages. In the training stage, supervision is directly
integrated into measuring the clustering error errS(h, p)
of a partition function h by exploiting the feedback given
by the true partition p. The goal of training is to find a par-
tition function h? in a hypothesis space H that minimizes
the error. Consequently, given a new data set S? in the ap-
plication stage, under some standard learning theory as-
sumptions, the hope is that the learned partition function
66
can generalize well and achieve small error as well.
3.1 Supervised and Unsupervised Training
Let p be the target function over X , h be a function in the
hypothesis space H , and h(S) = {S?k}K1 . In principle,
given data set S ? X , if the true partition p(S) = {Sk}K1
of S is available, one can measure the deviation of h from
p over S, using an error function errS(h, p) ? R+. We
distinguish an error function from a quality function (as
in Equ. 2) as follows: an error function measures the dis-
agreement between clustering and the target partition (or
one?s intention) when supervision is given, while a qual-
ity is defined without any supervision.
For clustering, there is generally no direct way to com-
pare the true class index p(x) of each element with that
given by a hypothesis h(x), so an alternative is to mea-
sure the disagreement between p and h over pairs of el-
ements. Given a labeled data set S and p(S), one error
function, namely weighted clustering error, is defined as
a sum of the pairwise errors over any two elements in S,
weighted by the distance between them:
errS(h, p) ? 1|S|2
?
xi,xj?S
[d(xi, xj)?Aij+(D?d(xi, xj))?Bij ]
(3)
where D = maxxi,xj?S d(xi, xj) is the maximum dis-
tance between any two elements in S and I is an indica-
tor function. Aij ? I[(p(xi) = p(xj) & h(xi) 6= h(xj)]
and Bij ? I[(p(xi) 6= p(xj) & h(xi) = h(xj)] represent
two types of pairwise errors respectively.
Just like the quality defined in Equ. 2, this error is a
function of the metric d. Intuitively, the contribution of a
pair of elements that should belong to the same class but
are split by h, grows with their distance, and vice versa.
However, this measure is significantly different from the
quality, in that it does not just measure the tightness of the
partition given by h, but rather the difference between the
tightness of the partitions given by h and by p.
Given a set of observed data, the goal of training is to
learn a good partition function, parameterized by specific
clustering algorithms and distance functions. Depending
on whether training data is labeled or unlabeled, we can
further define supervised and unsupervised training.
Definition 3.1 Supervised Training: Given a labeled
data set S and p(S), a family of partition functions H ,
and the error function errS(h, p)(h ? H), the problem
is to find an optimal function h? s.t.
h? = argminh?H errS(h, p).
Definition 3.2 Unsupervised Training: Given an unla-
beled data set S (p(S) is unknown), a family of partition
functions H , and a quality function qS(h)(h ? H), the
problem is to find an optimal partition function h? s.t.
h? = argmaxh?H qS(h).
With this formalization, SDC along with supervised
training, can be distinguished clearly from (1) unsuper-
vised clustering approaches, (2) clustering over pairwise
classification; and (3) related works that exploit partial
supervision in metric learning as constraints.
3.2 Clustering via Metric Learning
By fixing the clustering algorithm in the training stage,
we can further define supervised metric learning, a spe-
cial case of supervised training.
Definition 3.3 Supervised Metric Learning: Given a la-
beled data set S and p(S), and a family of partition func-
tions H = {h} that are parameterized by a chosen clus-
tering algorithm A and a family of distance metrics d?
(? ? ?), the problem is to seek an optimal metric d??
with respect to A, s.t. for h(S) = A d? (S)
?? = argmin? errS(h, p). (4)
Learning the metric parameters ? requires parameteriz-
ing h as a function of ?, when the algorithm A is chosen
and fixed in h. In the later experiments of Sec. 5, we
try to learn weighted Manhattan distances for the single-
link algorithm and other algorithms, in the task of en-
tity identification. In this case, when pairwise features
are extracted for any elements x1, x2 ? X , (x1, x2) =<
?1, ?2, ? ? ? , ?m >, the linearly weighted Manhattan dis-
tance, parameterized by (? = {wl}m1 ) is defined as:
d(x1, x2) ?
m?
l=1
wl ? ?l(x1, x2) (5)
where wl is the weight over feature ?l(x1, x2). Since
measurement of the error is dependent on the metric,
as shown in Equ. 3, one needs to enforce some con-
straints on the parameters. One constraint is
?m
l=1 |wl| =
1, which prevents the error from being scale-dependent
(e.g., metrics giving smaller distance are always better).
4 A General Learner for SDC
In addition to the theoretical SDC framework, we also de-
velop a practical learning algorithm based on gradient de-
scent (in Fig. 3), that can train a distance function for any
chosen clustering algorithm (such as Single-Linkage and
K-Means), as in the setting of supervised metric learning.
The training procedure incorporates the clustering algo-
rithm (step 2.a) so that the metric is trained with respect
to the specific algorithm that will be applied in evalua-
tion. The convergence of this general training procedure
depends on the convexity of the error as a function of ?.
For example, since the error function we use is linear in ?,
the algorithm is guaranteed to converge to a global mini-
mum. In this case, for rate of convergence, one can appeal
to general results that typically imply, when there exists
a parameter vector with zero error, that convergence rate
67
depends on the ?separation? of the training data, which
roughly means the minimal error archived with this pa-
rameter vector. Results such as (Freund and Schapire,
1998) can be used to extend the rate of convergence re-
sult a bit beyond the separable case, when a small number
of the pairs are not separable.
Algorithm: SDC-Learner
Input: S and p(S): the labeled data set. A: the clustering
algorithm. errS(h, p): the clustering error function. ? > 0
: the learning rate. T (typically T is large) : the number of
iterations allowed.
Output: ?? : the parameters in the distance function d.
1. In the initial (I-) step, we randomly choose ?0 for d.
After this step we have the initial d0 and h0.
2. Then we iterate over t (t = 1, 2, ? ? ?),
(a) Partition S using ht?1(S) ? A dt?1(S);
(b) Compute errS(ht?1, p) and update ? using the
formula: ?t = ?t?1 ? ? ? ?errS(ht?1,p)??t?1 .
(c) Normalization: ?t = 1Z ? ?t, where Z = ||?t||.
3. Stopping Criterion: If t > T , the algorithm exits and
outputs the metric in the iteration with the least error.
Figure 3: A general training algorithm for SDC
For the weighted clustering error in Equ. 3, and linearly
weighted Manhattan distances as in Equ. 5, the update
rule in Step 2(b) becomes
wtl = wt?1l ? ? ? [?t?1l (p, S)? ?t?1l (h, S)]. (6)
where ?l(p, S) ? 1|S|2
?
xi,xj?S ?l(xi, xj) ? I[p(xi) =
p(xj)] and ?l(h, S) ? 1|S|2
?
xi,xj?S ?l(xi, xj) ?
I[h(xi) = h(xj)], and ? > 0 is the learning rate.
5 Entity Identification in Text
We conduct experimental study on the task of entity iden-
tification in text (Bilenko et al, 2003; McCallum and
Wellner, 2003; Li et al, 2004). A given entity ? rep-
resenting a person, a location or an organization ? may
be mentioned in text in multiple, ambiguous ways. Con-
sider, for example, an open domain question answering
system (Voorhees, 2002) that attempts, given a question
like: ?When was President Kennedy born?? to search a
large collection of articles in order to pinpoint the con-
cise answer: ?on May 29, 1917.? The sentence, and even
the document that contains the answer, may not contain
the name ?President Kennedy?; it may refer to this en-
tity as ?Kennedy?, ?JFK? or ?John Fitzgerald Kennedy?.
Other documents may state that ?John F. Kennedy, Jr.
was born on November 25, 1960?, but this fact refers to
our target entity?s son. Other mentions, such as ?Senator
Kennedy? or ?Mrs. Kennedy? are even ?closer? to the
writing of the target entity, but clearly refer to different
entities. Understanding natural language requires identi-
fying whether different mentions of a name, within and
across documents, represent the same entity.
We study this problem for three entity types ? People,
Location and Organization. Although deciding the coref-
erence of names within the same document might be rela-
tively easy, since within a single document identical men-
tions typically refer to the same entity, identifying coref-
erence across-document is much harder. With no stan-
dard corpora for studying the problem in a general setting
? both within and across documents, we created our own
corpus. This is done by collecting about 8, 600 names
from 300 randomly sampled 1998-2000 New York Times
articles in the TREC corpus (Voorhees, 2002). These
names are first annotated by a named entity tagger, then
manually verified and given as input to an entity identi-
fier.
Since the number of classes (entities) for names is very
large, standard multi-class classification is not feasible.
Instead, we compare SDC with several pairwise classifi-
cation and clustering approaches. Some of them (for ex-
ample, those based on SoftTFIDF similarity) do not make
use of any domain knowledge, while others do exploit su-
pervision, such as LMR and SDC. Other works (Bilenko
et al, 2003) also exploited supervision in this problem by
discriminative training of a pairwise classifier but were
shown to be inferior.
1. SoftTFIDF Classifier ? a pairwise classifier deciding
whether any two names refer to the same entity, imple-
mented by thresholding a state-of-art SoftTFIDF similar-
ity metric for string comparison (Cohen et al, 2003). Dif-
ferent thresholds have been experimented but only the best
results are reported.
2. LMR Classifier (P|W) ? a SNoW-based pairwise classi-
fier (Li et al, 2004) (described in Sec. 2.2) that learns a
linear function for each class over a collection of relational
features between two names: including string and token-
level features and structural features (listed in Table 1).
For pairwise classifiers like LMR and SoftTFIDF, predic-
tion is made over pairs of names so transitivity of predic-
tions is not guaranteed as in clustering.
3. Clustering over SoftTFIDF ? a clustering approach based
on the SoftTFIDF similarity metric.
4. Clustering over LMR (P|W) ? a clustering approach (Li et
al., 2004) by converting the LMR classifier into a similar-
ity metric (see Sec. 2.2).
5. SDC ? our new supervised clustering approach. The dis-
tance metric is represented as a linear function over a set
of pairwise features as defined in Equ. 5.
The above approaches (2), (4) and (5) learn a classifier
or a distance metric using the same feature set as in Ta-
ble 1. Different clustering algorithms 2, such as Single-
Linkage, Complete-Linkage, Graph clustering (George,
2The clustering package Cluster by Michael Eisen at Stan-
ford University is adopted for K-medoids and CLUTO by
(George, 2003) is used for other algorithms. Details of these
algorithms can be found there.
68
Honorific Equal active if both tokens are honorifics and identical.
Honorific Equivalence active if both tokens are honorifics, not identical, but equivalent.
Honorific Mismatch active for different honorifics.
Equality active if both tokens are identical.
Case-Insensitive Equal active if the tokens are case-insensitive equal.
Nickname active if tokens have a ?nickname? relation.
Prefix Equality active if the prefixes of both tokens are equal.
Substring active if one of the tokens is a substring of the other.
Abbreviation active if one of the tokens is an abbreviation of the other.
Prefix Edit Distance active if the prefixes of both tokens have an edit-distance of 1.
Edit Distance active if the tokens have an edit-distance of 1.
Initial active if one of the tokens is an initial of another.
Symbol Map active if one token is a symbolic representative of the other.
Structural recording the location of the tokens that generate other features in two names.
Table 1: Features employed by LMR and SDC.
2003) ? seeking a minimum cut of a nearest neighbor
graph, Repeated Bisections and K-medoids (Chu et al,
2001) (a variation of K-means) are experimented in (5).
The number of entities in a data set is always given.
6 Experimental Study
Our experimental study focuses on (1) evaluating the
supervised discriminative clustering approach on entity
identification; (2) comparing it with existing pairwise
classification and clustering approaches widely used in
similar tasks; and (3) further analyzing the characteris-
tics of this new framework.
We use the TREC corpus to evaluate different ap-
proaches in identifying three types of entities: People,
Locations and Organization. For each type, we generate
three pairs of training and test sets, each containing about
300 names. We note that the three entity types yield very
different data sets, exhibited by some statistical proper-
ties3. Results on each entity type will be averaged over
the three sets and ten runs of two-fold cross-validation for
each of them. For SDC, given a training set with anno-
tated name pairs, a distance function is first trained using
the algorithm in Fig. 3 (in 20 iterations) with respect to
a clustering algorithm and then be used to partition the
corresponding test set with the same algorithm.
For a comparative evaluation, the outcomes of each ap-
proach on a test set of names are converted to a classifi-
cation over all possible pairs of names (including non-
matching pairs). Only examples in the set Mp, those
that are predicated to belong to the same entity (posi-
tive predictions) are used in the evaluation, and are com-
pared with the set Ma of examples annotated as positive.
The performance of an approach is then evaluated by F1
value, defined as: F1 = 2|Mp
?
Ma|
|Mp|+|Ma| .
3The average SoftTFIDF similarity between names of the
same entity is 0.81, 0.89 and 0.95 for people, locations and or-
ganizations respectively.
6.1 Comparison of Different Approaches
Fig. 4 presents the performance of different approaches
(described in Sec. 5) on identifying the three entity types.
We experimented with different clustering algorithms but
only the results by Single-Linkage are reported for Clus-
ter over LMR (P|W) and SDC, since they are the best.
SDC works well for all three entity types in spite of
their different characteristics. The best F1 values of SDC
are 92.7%, 92.4% and 95.7% for people, locations and
organizations respectively, about 20% ? 30% error re-
duction compared with the best performance of the other
approaches. This is an indication that this new approach
which integrates metric learning and supervision in a uni-
fied framework, has significant advantages 4.
6.2 Further Analysis of SDC
In the next experiments, we will further analyze the char-
acteristics of SDC by evaluating it in different settings.
Different Training Sizes Fig. 5 reports the relationship
between the performance of SDC and different training
sizes. The learning curves for other learning-based ap-
proaches are also shown. We find that SDC exhibits good
learning ability with limited supervision. When training
examples are very limited, for example, only 10% of all
300 names, pairwise classifiers based on Perceptron and
Winnow exhibit advantages over SDC. However, when
supervision become reasonable (30%+ examples), SDC
starts to outperform all other approaches.
Different Clustering Algorithms Fig. 6 shows the
performance of applying different clustering algorithms
(see Sec. 5) in the SDC approach. Single-Linkage and
Complete-Linkage outperform all other algorithms. One
possible reason is that this task has a great number of
4We note that in this experiment, the relative comparison
between the pairwise classifiers and the clustering approaches
over them is not consistent for all entity types. This can be
partially explained by the theoretical analysis in (Li et al, 2004)
and the difference between entity types.
69
80
82
84
86
88
90
92
94
96
(a) People
F 1 (%
)
80
82
84
86
88
90
92
94
96
(b) Locations
F 1 (%
)
80
82
84
86
88
90
92
94
96
(c) Organizations
F 1 (%
)
SoftTFIDFLMR (P)LMR (W)Cluster over SoftTFIDFCluster over LMR (P)Cluster over LMR (W)SDC
Figure 4: Performance of different approaches. The results are reported for SDC with a learning rate ? = 100.0.
The Single-Linkage algorithm is applied whenever clustering is performed. Results are reported in F1 and averaged
over the three data sets for each entity type and 10 runs of two-fold cross-validation. Each training set typically
contains 300 annotated names.
10 20 30 40 50 60 70 80 90 10070
75
80
85
90
95
100
(a) Peopl
F 1 (%
)
LMR (P)LMR (W)Cluster over LMR (P)Cluster over LMR (W)SDC
10 20 30 40 50 60 70 80 90 10070
75
80
85
90
95
100
(b) Locations
F 1 (%
)
10 20 30 40 50 60 70 80 90 10070
75
80
85
90
95
100
(c) Organizations
F 1 (%
)
Figure 5: Performance for different training sizes. Five learning-based approaches are compared. Single-Linkage is
applied whenever clustering is performed. X-axis denotes different percentages of 300 names used in training. Results
are reported in F1 and averaged over the three data sets for each entity type.
People Locations Organizations40
50
60
70
80
90
Different Entity Types
F 1 (%)
GraphK?MedoidsRBComplete?LinkageSingle?Linkage
Figure 6: Different clustering algorithms. Five cluster-
ing algorithms are compared in SDC (? = 100.0). Re-
sults are averaged over the three data sets for each entity
type and 10 runs of two-fold cross-validations.
classes (100 ? 200 entities) for 300 names in each sin-
gle data set. The results indicate that the metric learn-
ing process relies on properties of the data set, as well as
the clustering algorithm. Even if a good distance metric
could be learned in SDC, choosing an appropriate algo-
rithm for the specific task is still important.
Different Learning Rates We also experimented with
different learning rates in the SDC approach as shown in
Fig. 7. It seems that SDC is not very sensitive to different
learning rates as long as it is in a reasonable range.
People Locations Organizations
86
88
90
92
94
96
Different Entity Types
F 1 (%)
?=1.0?=10.0?=100.0?=1000.0
Figure 7: Performance for different learning rates.
SDC with different learning rates (? = 1.0, 10.0, 100.0,
1000.0) compared in this setting. Single-Linkage cluster-
ing algorithm is applied.
6.3 Discussion
The reason that SDC can outperform existing clustering
approaches can be explained by the advantages of SDC ?
training the distance function with respect to the chosen
clustering algorithm, guided by supervision, but they do
not explain why it can also outperform the pairwise clas-
sifiers. One intuitive explanation is that supervision in the
entity identification task or similar tasks is typically given
on whether two names correspond to the same entity ?
entity-level annotation. Therefore it does not necessarily
mean whether they are similar in appearance. For exam-
70
ple, ?Brian? and ?Wilson? could both refer to a person
?Brian Wilson? in different contexts, and thus this name
pair is a positive example in training a pairwise classi-
fier. However, with features that only capture the appear-
ance similarity between names, such apparently different
names become training noise. This is what exactly hap-
pened when we train the LMR classifier with such name
pairs. SDC, however, can employ this entity-level anno-
tation and avoid the problem through transitivity in clus-
tering. In the above example, if there is ?Brian Wilson?
in the data set, then ?Brian? and ?Wilson? can be both
clustered into the same group with ?Brian Wilson?. Such
cases do not frequently occur for locations and organiza-
tion but still exist .
7 Conclusion
In this paper, we explicitly formalize clustering as a learn-
ing task, and propose a unified framework for training
a metric for any chosen clustering algorithm, guided by
domain-specific supervision. Our experiments exhibit the
advantage of this approach over existing approaches on
Entity Identification. Further research in this direction
will focus on (1) applying it to more NLP tasks, e.g.
coreference resolution; (2) analyzing the related theoret-
ical issues, e.g. the convergence of the algorithm; and
(3) comparing it experimentally with related approaches,
such as (Xing et al, 2002) and (McCallum and Wellner,
2003).
Acknowledgement This research is supported by
NSF grants IIS-9801638 and ITR IIS-0085836, an ONR
MURI Award and an equipment donation from AMD.
References
F. R. Bach and M. I. Jordan. 2003. Learning spectral clustering.
In NIPS-03.
A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. 2003.
Learning distance functions using equivalence relations. In
ICML-03, pages 11?18.
M. Bilenko, R. Mooney, W. Cohen, P. Ravikumar, and S. Fien-
berg. 2003. Adaptive name matching in information integra-
tion. IEEE Intelligent Systems, pages 16?23.
M Bilenko, S. Basu, and R. J. Mooney. 2004. Integrating con-
straints and metric learning in semi-supervised clustering. In
ICML-04, pages 81?88.
P. Brown, P. deSouza R. Mercer, V. Pietra, and J. Lai. 1992.
Class-based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
C. Cardie and K. Wagstaff. 1999. Noun phrase coreference as
clustering. In EMNLP-99, pages 82?89.
S. C. Chu, J. F. Roddick, and J. S. Pan. 2001. A comparative
study and extensions to k-medoids algorithms. In ICOTA-01.
W. Cohen and J. Richman. 2002. Learning to match and clus-
ter large high-dimensional data sets for data integration. In
KDD-02, pages 475?480.
W. Cohen, P. Ravikumar, and S. Fienberg. 2003. A comparison
of string metrics for name-matching tasks. In IIWeb Work-
shop 2003, pages 73?78.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based mod-
els of word cooccurrence probabilities. Machine Learning,
34(1-3):43?69.
Y. Freund and R. Schapire. 1998. Large margin classification
using the Perceptron algorithm. In COLT-98.
M. Geffet and I. Dagan. 2004. Automatic feature vector quality
and distributional similarity. In COLING-04.
K. George. 2003. Cluto: A clustering toolkit. Technical report,
Dept of Computer Science, University of Minnesota.
J. Hartigan and M. Wong. 1979. A k-means clustering algo-
rithm. Applied Statistics, 28(1):100?108.
S. Kamvar, D. Klein, and C. Manning. 2002. Interpreting and
extending classical agglomerative clustering algorithms us-
ing a model-based approach. In ICML-02, pages 283?290.
L. Lee. 1997. Similarity-Based Approaches to Natural Lan-
guage Processing. Ph.D. thesis, Harvard University, Cam-
bridge, MA.
X. Li, P. Morie, and D. Roth. 2004. Identification and trac-
ing of ambiguous names: Discriminative and generative ap-
proaches. In AAAI-04, pages 419?424.
G. Mann and D. Yarowsky. 2003. Unsupervised personal name
disambiguation. In CoNLL-03, pages 33?40.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In IJCAI Workshop on Information Integration
on the Web.
D. Mochihashi, G. Kikui, and K. Kita. 2004. Learning non-
structural distance metric by minimum cluster distortions. In
COLING-04.
P. Pantel and D. Lin. 2002. Discovering word senses from text.
In KDD-02, pages 613?619.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In AAAI-98, pages 806?813.
M. Schultz and T. Joachims. 2004. Learning a distance metric
from relative comparisons. In NIPS-04.
E. Voorhees. 2002. Overview of the TREC-2002 question an-
swering track. In TREC-02, pages 115?123.
J. Weeds, D. Weir, and D. McCarthy. 2004. Characterising
measures of lexical distributional similarity. In COLING-04.
E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. 2002.
Distance metric learning, with application to clustering with
side-information. In NIPS-02.
71
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 193?196,
Sydney, July 2006. c?2006 Association for Computational Linguistics
NetEase Automatic Chinese Word Segmentation 
 
 
Li xin                                                                       Dai shuaixiang 
NETEASE INFORMATION TECHNOLOGY (BEIJING) CO., LTD.  
SP Tower D, 26th Floor, Tsinghua Science Park Building 8, No.1 Zhongguancun East Road, 
Haidian District Beijing, 100084, PRC. 
lxin@corp.netease.com                                          ddai@corp.netease.com 
 
  
 
Abstract 
This document analyses the bakeoff re-
sults from NetEase Co. in the SIGHAN5 
Word Segmentation Task and Named En-
tity Recognition Task. The NetEase WS 
system is designed to facilitate research 
in natural language processing and in-
formation retrieval. It supports Chinese 
and English word segmentation, Chinese 
named entity recognition, Chinese part of 
speech tagging and phrase conglutination. 
Evaluation result shows our WS system 
has a passable precision in word segmen-
tation except for the unknown words rec-
ognition. 
1 Introduction 
Automatic Chinese Word Segmentation (WS) is 
the fundamental task of Chinese information 
processing [Liu, 2000].Since there are lots of 
works depending on the automatic segmentation 
of Chinese words, different Chinese NLP-
enabled applications may have different re-
quirements that call for different granularities of 
word segmentation. The key to accurate auto-
matic word identification in Chinese lies in the 
successful resolution of those ambiguities and a 
proper way to handle out-of-vocabulary (OOV) 
words (such as person names, place names and 
organization name etc.).  
We have applied corpus-based method to ex-
tracting various language phenomena from real 
texts; and have combined statistical model with 
rules in Chinese word segmentation, which has 
increased the precision of segmentation by im-
proving ambiguous phrase segmentation and out-
of-vocabulary word recognition. 
In the second section of this paper, we de-
scribe a Chinese word segmentation system de-
veloped by NetEase. And we present our strate-
gies on solving the problems of ambiguous 
phrase segmentation and identification of Chi-
nese people names and place names. The third 
section is analysis of evaluation result. 
2 Modern Chinese Automatic Segmen-
tation System  
2.1 System Structure  
The WS system of NETEASE CO. supports Chi-
nese and English word segmentation, Chinese 
named entity recognition, Chinese part of speech 
tagging and phrase conglutination. In ordering to 
processing mass data, it is designed as an effi-
cient system. The whole system includes some 
processing steps: pre-processing, num-
ber/date/time recognition, unknown words rec-
ognition, segmenting, POS tagging and post-
processing, as Fig 1 shows.  
The Prehandler module performs the pre-
processing, splits the text into sentences accord-
ing to the punctuations.    
Number/Data/Time recognition processes the 
number, date, time string and English words.  
Unknown word recognition includes personal 
name recognition and place name recognition. 
Segmenter component performs word-
segmenting task, matches all the candidate words 
and processes ambiguous lexical.  
POSTagger module performs part of speech 
tagging task and decides the optimal word seg-
mentation using hierarchical hidden Markov 
model (HHMM) [Zhang, 2003]. 
Posthandler retrieves phrases with multi-
granularities from segmentation result and de-
tects new words automatically etc. 
2.2 Ambiguous phrase segmentation  
Assume that ?AJB? are character strings and that 
W is a word list. In the field ?AJB?, if ?AJ??W, 
193
and ?JB??W, then ?AJB? is called ambiguous 
phrase of overlap type. For example, in the string 
"???", both "??" and "??" are words , so 
"???" is an ambiguous phrase of overlap type; 
and there is one ambiguous string.  
 
In the string ?AB?, if ?AB??W(word), ?A??
W, and ?B??W, then the string ?AB? is called 
ambiguous phrase of combination type. For ex-
ample, in the string "??", since "??", "?" 
and "?"are all words, so the string "??" is an 
ambiguous phrase of combination type. 
We have built an ambiguous phrase lib of 
overlap and combination type from tagged cor-
pus, which contains 200,000 phrases from 1-
gram to 4-gram. For example: ??/d ?/v ??/v, 
??/vn ?/v ?? vn? If one ambiguous phrase 
found in raw text, the potential segmentation re-
sult will be found in the lib and submit to next 
module. If not found, POS tagger module will 
disambiguate it.  
2.3 Chinese Personal Name Recognition 
At present we only consider the recognition of 
normal people name with both a family name 
and a first name. We got the statistical Character 
Set of Family Name and First Name data from 
corpus. And also consider the ability of  charac-
ter of constructing word. Some characters itself 
cannot be regarded as a word or composes a 
word with other characters, such as "?,?,?"; 
Some  name characters which can compose word 
with other characters only, e.g. ??????? 
can construct words "??????????
";Some name characters  are also a common 
words themselves, e.g. ?????. 
The recognition procedure is as follows: 
1) Find the potential Chinese personal names: 
Family name is the trigger. Whenever a family 
name is found in a text, its following word is 
taken as a first name word, or its following two 
characters as the head character and the tail char-
acter of a first name. Then the family name and 
its following make a potential people name, the 
probable largest length of which is 4 when it is 
composed of a double-character family name and 
a double-character first name. 
2) Based on the constructing word rules and 
the protective rules, sift the potential people 
names for the first time. For example, when raw 
text is ????,????, then the ??,?? were 
not family name. Because the ??,?? is number. 
3) Compute the probabilities of the potential 
name and the threshold values of corresponding 
family names, then sift the people names again 
based on the personal name probability function 
and description rules. 
4) According to the left-boundary rules and 
the right-boundary rules which base on title, for 
Fig 1 Structure and Components of WS 
194
example, ??? ,???, and name frequent of 
context, determine the boundaries of people 
names. 
5) Negate conflicting potential people names. 
6) Output the result: The output contains every 
sentence in the processed text and the start and 
the end positions and the reliability values of all 
people names in it. 
2.4 Chinese Place Name Recognition 
By collecting a large scale of place names, For 
example, (1) The names of administrative re-
gions superior to county; (2) The names of in-
habitation areas; (3) The names of geographic 
entities, such as mountain, river, lake, sea, island 
etc.; (4) Other place names, e.g. monument, ruins, 
bridge and power station etc. building the place 
name dictionary. 
Collecting words that can symbolize a place, 
e.g. ????, ????, ??? etc. 
Base on these knowledge we applied positive 
deduction mechanism. Its essence is that with 
reference to certain control strategies, a rule is 
selected; then examining whether the fact 
matches the condition of the rule, if it does, the 
rule will be triggered.  
In addition, Those words that often concurrent 
with a place name are collected , including: 
???, ???? etc. And which often concurrent 
with a people name, such as ????, ??? and 
so on,  are also considered in NER. 
WS system identifies all potential place names 
in texts by using place name base and gathers 
their context information; and through deduction, 
it utilizes rule set and knowledge base to confirm 
or negate a potential place name; hereupon, the 
remainders are recognized place name. 
2.5 Multi-granularities of word segmenta-
tion 
Whenever we deploy the segmenter for any ap-
plication, we need to customize the output of the 
segmenter according to an application specific 
standard, which is not always explicitly defined. 
However, it is often implicitly defined in a given 
amount of application data (for example, Search 
engines log, Tagged corpus) from which the spe-
cific standard can be partially learned.  
Most variability in word segmentation across 
different standards comes from those words that 
are not typically stored in the basic dictionary. 
To meet the applications of different levels, in 
our system, the standard adaptation is conducted 
by a post-processor which performs an ordered 
list of transformations on the output. For exam-
ple: When input is ????????????, 
the output will be: 
1. ????/??/ ??/ ??? ? 
2. ????/????/ ???? 
3. ????/????????  
 
Result 1 is normal segmentation, also is mini-
mum granularity of word. Result 2 and 3 is big-
ger granularity. Every application can select ap-
propriate segmentation result according to its 
purpose.  
3 Test results 
The speed of NetEase WS system is about 
1500KB/s--300KB/s in different algorithm and 
p4-2.8/512M computer. In SigHan5, the F-
MEASURE of our word segmentation is 0.924, 
the IN Recall is 0.959, but OOV Recall Rate is 
only 0.656. This indicates that our unknown 
words recognition is poor; it makes a bad impact 
on the segmented result.   It also shows our sys-
tem should be improved largely in unknown 
words recognition.  For example:  
1. Name Entity Recognize: ?????, ??
??, ???? were falsely segment to ??/??
/?), ??/?/?, ?/?/??. 
2. Name Entity Ambiguous: ??/ ??/LOC? 
are falsely recognized ????/PER?. 
3. Abbreviations of phrase: ??? (???)? 
was segment to  ??/??. 
4. New Word: ????, ??, ??? 
5. Standard of Word: we think ??????? 
and ?????? is one word, but criterion is ??
?/?/???, ???/??? etc.  
In evaluation, our system?s TOTAL INSER-
TIONS is 5292 and TOTAL DELETIONS is 
2460. The result show: our WS usually segment 
out ?shorter word?, for example,  ???????, 
and ?????? is segmented to ???/??/
??, ???/??? . But not every string is one 
word. 
Much work needs to be done to evaluate this 
WS system more thoroughly. Refined pre-
processing or post-processing steps could also 
help improve segmentation accuracy. 
For example, pre-processing will split ASCII 
string and Chinese character, so  "DD6112H6?, 
ic?, ???" will falsely segment "DD6112H6/
?, ic/?, ??/?"; In post-processing, by using 
consecutive single characters ??/?, ?/?? to 
195
detect the valid out-of-vocabulary words ???, 
??? also is good idea. 
 
References 
Kaiying Liu. Automatic Chinese Word Segmentation 
and POS Tagging. Business Publishing House. 
Beijing. 2000. 
Hua-Ping Zhang etc. Chinese Lexical Analysis Using 
Hierarchical Hidden Markov Model. Second 
SIGHAN workshop affiliated with 41th ACL, Sap-
poro Japan, July, 2003, pp. 63-70 
196
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 71?75,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Transliteration Experiments on Chinese and Arabic
Grzegorz Kondrak, Xingkai Li and Mohammad Salameh
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
{gkondrak,xingkai,msalameh}@ualberta.ca
Abstract
We report the results of our transliteration ex-
periments with language-specific adaptations
in the context of two language pairs: English
to Chinese, and Arabic to English. In particu-
lar, we investigate a syllable-based Pinyin in-
termediate representation for Chinese, and a
letter mapping for Arabic.
1 Introduction
Transliteration transforms an orthographic form of
a word in one writing script into an orthographic
form of the same word in another writing script. The
problem is challenging because the relationship be-
tween the source and target representations is often
ambiguous. The process is further complicated by
restrictions in the target phonological system.
DIRECTL+ (Jiampojamarn et al, 2010a) is an
online discriminative training system that incorpo-
rates joint n-gram features and many-to-many align-
ments, which are generated by M2M-ALIGNER (Ji-
ampojamarn et al, 2007). Our team employed vari-
ants of DIRECTL+ in the previous editions of the
Shared Task on Transliteration (Jiampojamarn et al,
2009; Jiampojamarn et al, 2010b; Bhargava et al,
2011). Recently, Bhargava and Kondrak (2012)
show significant improvement in accuracy for the
English-to-Japanese task by leveraging supplemen-
tal transliterations from other scripts.
In this edition of the Shared Task on Translitera-
tion, we experiment with language-specific adapta-
tions for the EnCh and ArEn data sets. The struc-
ture of the paper is as follows. In Section 2, we
provide details about the system parameters used in
M2M-ALIGNER and DIRECTL+. Section 3 pro-
vides details of our strategies adopted in the EnCh
task, which incorporate Chinese-specific knowledge
and system combination algorithm. In Section 4 we
elaborate on the difficulty of Arabic name transliter-
ation and propose a letter mapping scheme. In Sec-
tion 5 we present the official test results.
2 Base System
We run DIRECTL+ with all of the features described
in (Jiampojamarn et al, 2010a). System parameters
were determined during development. For the EnCh
experiments, we set the context feature size to 5, the
transition feature size to 2, and the joint n-gram fea-
ture size to 6. For the ArEn experiments, we used
the same settings, except that we set the joint n-gram
feature size to 5.
The M2M-ALIGNER parameters were set as fol-
lows. For the English-Pinyin alignment, the maxi-
mum substring length was 1 on the English side, and
2 on the Pinyin side, with empty substrings (nulls)
allowed only on the Pinyin side. For ArEn, the max-
imum substring length was 2 for both sides.
3 English to Chinese
In this section, we introduce the strategies for im-
proving DIRECTL+ performance on the EnCh task,
including the use of Chinese Pinyin for preprocess-
ing, and the combination of different models.
3.1 Data preprocessing and cleaning
In general, the preprocessing is limited to remov-
ing letter case distinctions in English names, and re-
71
placing every non-initial letter x with ks. However,
we observed that the provided development set con-
tains a number of entries (about 3%) that contain
multiple English words on the source side, but no
corresponding separators on the target side, whereas
no such entries occur in the training or testing set.
Since this discrepancy between sets may cause prob-
lems for alignment and generation, we separated
the multi-word entries into individual words (using
whitespace and apostrophes as delimiters) and man-
ually selected proper transliteration targets for them.
We also removed individual words that have no cor-
responding transliterations on the target side. The
cleaned development set contains 2483 entries.
3.2 Alignment via Pinyin
Following Jiampojamarn et al (2009; 2010b), we
utilize Pinyin as an intermediate representation of
Chinese characters during M2M alignment with the
objective of improving its quality. Pinyin is the
formally-adopted Romanization system for Stan-
dard Mandarin for the mapping of Chinese charac-
ters to Roman alphabet. It uses the 26 letters of the
English alphabet except for the letter v, with the ad-
dition of the letter u?. Every Chinese character can be
represented by a sequence of Pinyin letters accord-
ing to the way it is pronounced. Numerous freely
available online tools exist for facilitating Chinese-
Pinyin conversion1 .
In our experiments, the original Chinese charac-
ters from the target side of the training set are con-
verted to Pinyin before M2M alignment. A small
part of them (about 50 out of approximately 500
distinct Chinese characters in the Shared Task data)
have multiple pronunciations, and can thus be rep-
resented by different Pinyin sequences. For those
characters we manually select the pronunciations
that are normally used for names.
After the alignment between English and Pinyin
representation has been generated by M2M-
ALIGNER, we use it to derive the alignment between
English and Chinese characters, which is then used
for training DIRECTL+. This preprocessing step re-
sults in a more accurate alignment as it substantially
reduces the number of target symbols from around
500 distinct Chinese characters to 26 Pinyin letters.
1For instance, http://www.chinesetopinyin.com
Our approach is to utilize Pinyin only in the align-
ment phase, and converts it back to Chinese charac-
ters before the training phase. We do not incorporate
Pinyin into the generation phase in order to avoid
problems involved in converting the transliteration
results from Pinyin back to Chinese characters. For
example, a Pinyin subsequence may have multiple
Chinese character mappings because of the fact that
many Chinese characters have the same Pinyin rep-
resentation. In addition, it is not always clear how to
partition the Pinyin sequence into substrings corre-
sponding to individual Chinese characters.
The choice of the appropriate Chinese character
sequence is the problem further complicating the
conversion from Pinyin. We experimented with a tri-
gram language model trained on the target Chinese
side of the training set for the purpose of identify-
ing the correct transliteration result. However, this
approach yielded low accuracy on the development
set. In contrast, the strategy of using Pinyin only for
the alignment introduces no ambiguity because we
know the mapping between Pinyin sequences and
the target Chinese side of the training set.
3.3 Syllabic Pinyin
The Pinyin sequences representing the pronuncia-
tions of Chinese characters should not be interpreted
as combinations of individual letters. Rather, a Man-
darin phonetic syllable (the pronunciation of one
Chinese character) is composed of an optional on-
set (?initial?) followed by an obligatory rhyme (?fi-
nal?). The rhyme itself is composed of an obligatory
nucleus followed by an optional coda. Phonetically,
the onset contains a single consonant, the nucleus
contains a vowel or a diphthong, and the coda con-
tains a single consonant ([r], [n] or [N]). Both the on-
set and the rhyme can be represented by either a sin-
gle letter or sequence of two or three letters. It is the
initials and finals listed in Table 1 rather than Pinyin
letters that are the phonemic units of Pinyin for Stan-
dard Mandarin. The pronunciation of a multi-letter
initial/final is often different from the pronunciation
of the sequence of its individual letters. Treating
converted Pinyin as a sequence of separate letters
may result in an incorrect phonetic transcription.
In this paper, we further experiment with encod-
ing the converted sequences of Pinyin letters as the
sequences of initials and finals for M2M alignment.
72
Initials
b p m f d t n l
g k h j q x zh ch
sh r z c s y w
Finals
a o e i u u? ai ei
ui ao ou iu ie u?e er an
en in un u?n ang eng ing ong
Table 1: The initials and finals in Chinese Pinyin.
Although the size of the alphabet increases from 26
letters to 47 initials and finals, the original Chinese
pronunciation is represented more precisely. We re-
fer to the new model which is trained on Pinyin
initials and finals as PINYIN-SYL, and to the pre-
viously proposed model which is trained on Pinyin
letters as PINYIN-LET.
3.4 System combination
The combination of models based on different
principles may lead to improved prediction accu-
racy. We adopt the simple voting algorithm for
system combination proposed by Jiampojamarn et
al. (2009), with minor modifications. Since here
we combine only two systems (PINYIN-LET and
PINYIN-SYL), the algorithm becomes even simpler.
We first rank the participating models according to
their overall top-1 accuracy2 on the development set.
Note that the n-best list produced by DIRECTL+
may contain multiple copies of the same output
which differ only in the implied input-output align-
ment. We allow such duplicates to contribute to the
voting tally. The top-1 prediction is selected from
the set of top-1 predictions produced by the partic-
ipating models, with ties broken by voting and the
preference for the highest-ranking system. For con-
structing n-best candidate lists, we order the candi-
date transliterations according to the highest rank
assigned by either of the systems, with ties again
broken by voting and the preference for the highest-
ranking system. We refer to this combined model as
COMBINED.
Table 2 shows the results of the three discussed
approaches trained on the original training set, and
2Word accuracy in top-1 evaluates only the top translitera-
tion candidate produced by a transliteration system.
System top-1 F-score
PINYIN-LET 0.296 0.679
PINYIN-SYL 0.302 0.681
COMBINED 0.304 0.682
Table 2: Development results on EnCh.
tested on the cleaned development set. PINYIN-SYL
performs slightly better than PINYIN-LET, which
hints at the advantage of using Pinyin initials and fi-
nals over Pinyin letters as the intermediate represen-
tation during the alignment. The combination of the
two models produces a marginally higher F-score3.
The likely reason for the limited gain is the strong
similarity of the two combined models. We exper-
imented with adding a third model that is trained
directly on the original Chinese characters without
using Pinyin as the intermediate representation, but
its accuracy was lower, and the accuracy of the re-
sulting combined model was below PINYIN-SYL.
4 Arabic to English
Arabic script has 36 letters and 9 diacritics. Among
these letters, the letters Alif and Yaa can be repre-
sented in different forms (

@

@ @ @ and ?
 ? ,
respectively). The ArEn data set contains Arabic
names without diacritics, which adds ambiguity to
the transliteration task. When transliterated, such
diacritics would appear as an English vowel. For
example, it is difficult to tell whether the correct
transliteration of the two-letter name l .