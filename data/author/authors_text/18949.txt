Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 98?106,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Mapping dialectal variation by querying social media
Gabriel Doyle
Department of Linguistics
University of California, San Diego
La Jolla, CA, USA 92093-0108
gdoyle@ucsd.edu
Abstract
We propose a Bayesian method of esti-
mating a conditional distribution of data
given metadata (e.g., the usage of a di-
alectal variant given a location) based
on queries from a big data/social me-
dia source, such as Twitter. This distri-
bution is structurally equivalent to those
built from traditional experimental meth-
ods, despite lacking negative examples.
Tests using Twitter to investigate the ge-
ographic distribution of dialectal forms
show that this method can provide distri-
butions that are tightly correlated with ex-
isting gold-standard studies at a fraction of
the time, cost, and effort.
1 Introduction
Social media provides a linguist with a new data
source of unprecedented scale, opening novel av-
enues for research in empirically-driven areas,
such as corpus and sociolinguistics. Extracting the
right information from social media, though, is not
as straightforward as in traditional data sources, as
the size and format of big data makes it too un-
wieldy to observe as a whole. Researchers often
must interact with big data through queries, which
produce only positive results, those matching the
search term. At best, this can be augmented with
a set of ?absences? covering results that do not
match the search term, but explicit negative data
(e.g., confirmation that a datapoint could never
match the search term) does not exist. In addition
to the lack of explicit negative data, query-derived
data has a conditional distribution that reverses the
dependent and independent variables compared to
traditional data sources, such as sociolinguistic in-
terviews.
This paper proposes a Bayesian method for
overcoming these two difficulties, allowing query-
derived data to be applied to traditional problems
without requiring explicit negative data or the abil-
ity to view the entire dataset at once. The test case
in this paper is dialect geography, where the pos-
itive data is the presence of a dialectal word or
phrase in a tweet, and the metadata is the location
of the person tweeting it. However, the method
is general and applies to any queryable big data
source that includes metadata about the user or set-
ting that generated the data.
The key to this method lies in using an indepen-
dent query to estimate the overall distribution of
the metadata. This estimated distribution corrects
for non-uniformity in the data source, enabling the
reversal of the conditionality on the query-derived
distribution to convert it to the distribution of in-
terest.
Section 2 explains the mathematical core of the
Bayesian analysis. Section 3 implements this anal-
ysis for Twitter and introduces an open-source
program for determining the geographic distri-
bution of tweets. Section 4 tests the method on
problems in linguistic geography and shows that
its results are well-correlated with those of tradi-
tional sociolinguistic research. Section 5 addresses
potential concerns about noise or biases in the
queries.
2 Reversing the conditionality of query
data
2.1 Corpora and positive-only data
In traditional linguistic studies, the experimenter
has control over the participants? metadata, but
not over their data. For instance, a sociolinguist
may select speakers with known ages or locations,
but will not know their usages in advance. Cor-
pus queries reverse the direction of investigation;
the experimenter selects a linguistic form to search
for, but then lacks control over the metadata of the
participants who use the query. The direction of
conditionality must be reversed to get compara-
98
ble information from query-derived and traditional
data.
Queries also complicate the problem by pro-
viding only positive examples. This lack of ex-
plicit negative data is common in language ac-
quisition, as children encounter mostly grammat-
ical statements during learning, and receive few
explicitly ungrammatical examples, yet still de-
velop a consistent grammaticality classification
system as they mature. Similar positive-only prob-
lems abound in cognitive science and artificial in-
telligence, and a variety of proposals have been
offered to overcome it in different tasks. These
include biases like the Size Principle (Tenen-
baum and Griffiths, 2001), heuristics like gen-
erating pseudo-negatives from unobserved data
(Okanohara and Tsujii, 2007; Poon et al., 2009),
or innate prespecifications like Universal Gram-
mar in the Principles and Parameters framework.
For query-derived data, Bayesian reasoning can
address both problems by inverting the condi-
tionality of the distribution and implying negative
data. The key insight is that a lack of positive ex-
amples where positive examples are otherwise ex-
pected is implicit negative evidence. This method
allows a researcher to produce an estimated distri-
bution that approximates the true conditional dis-
tribution up to a normalizing factor. This condi-
tional distribution is that of data (e.g., a dialectal
form) conditioned on metadata (e.g., a location).
This distribution can be written as p(D|M),
where D and M are random variables represent-
ing the data and metadata. A query for a data value
d returns metadata values m distributed according
to p(M |D = d). All of the returned results will
have the searched-for data value, but the metadata
can take any value.
For most research, p(M |D = d) is not the dis-
tribution of interest, as it is conflated with the over-
all distribution of the metadata. For instance, if
the query results indicate that 60% of users of the
linguistic form d live in urban areas, this seems
to suggest that the linguistic form is more likely
in urban areas. But if 80% of people live in ur-
ban areas, the linguistic form is actually underrep-
resented in these areas, and positively associated
with rural areas. An example of the effect of such
misanalysis is shown in Sect. 4.2.
2.2 Reversing the conditionality
Bayesian reasoning allows a researcher to move
from the sampled p(M |D) distribution to the de-
sired p(D|M). We invoke Bayes? Rule:
p(D|M) =
p(M |D)p(D)
p(M)
In some situations, these underlying distribu-
tions will be easily obtainable. For small corpora,
p(D) and p(M) can be calculated by enumeration.
For data with explicit negative examples available,
p(D) can be estimated as the ratio of positive ex-
amples to the sum of positive and negative exam-
ples.
1
But for queries in general, neither of these
approximations is possible. Instead, we estimate
p(M) through the querying mechanism itself.
This is done by choosing a ?baseline? query
term q whose distribution is approximately inde-
pendent of the metadata ? that is, a query q such
that p(q|m) is approximately constant for all meta-
data values m ?M . If p(q|m) is constant, then by
Bayes? Rule:
p(m|q) =
p(q|m)p(m)
p(q)
? p(m), ?m ?M
Thus we can treat results from a baseline query
as though they are draws directly from p(M), and
estimate the denominator from this distribution.
The remaining unknown distribution p(d) is con-
stant for a given data value d, so combining the
above equations yields the unnormalized probabil-
ity p?(d|M):
p(d|M) ? p?(d|M) =
p(M |d)
p(M |q)
. (1)
This switch to the unnormalized distribution can
improve interpretability as well. If p?(d|m) = 1,
then p(m|d) = p(m|q), which means that the
metadata m is observed for the linguistic form d
just as often as it is for the baseline query. When
p?(d|m) > 1, the linguistic form is more common
for metadata m than average, and when p?(d|m) <
1, the form is less common for that metadata.
2
1
This can be extended to multi-class outcomes; if D has
more than two outcomes, each possible outcome is an implicit
negative example for the other possible outcomes.
2
If a normalized distribution is needed, p(d) may be es-
timable, depending on the data source. In the Twitter data pre-
sented here, tweets are sequentially numbered, so p(d) could
be estimated using these index numbers. This paper only uses
unnormalized distributions.
99
2.3 Coverage and confidence
Due to the potentially non-uniform distribution
of metadata, the amount of error in the estimate
in Eq. 1 can vary with m. Intuitively, the confi-
dence in the conditional probability estimates de-
pends on the amount of data observed for each
metadata value. Because queries estimate p(M |d)
by repeated draws from that distribution, the er-
ror in the estimate decreases as the number of
draws increases. The overall error in the estimate
of p?(d|m) decreases as the number of datapoints
observed at m increases. This suggests estimating
confidence as the square root of the count of ob-
servations of the metadata m, as the standard error
of the mean decreases in proportion to the square
root of the number of observations. More complex
Bayesian inference can be used improve error es-
timates in the future.
3 Sample Implementation: SeeTweet
This section implements the method described in
the previous section on a case study of the ge-
ographic distributions of linguistic forms, calcu-
lated from recent tweets. It is implemented as
a suite of novel open-source Python/R programs
called SeeTweet, which queries Twitter, obtains
tweet locations, performs the mathematical anal-
ysis, and maps the results. The suite is avail-
able at http://github.com/gabedoyle/
seetweet.
3.1 SeeTweet goals
Traditionally, sociolinguistic studies are highly
time-intensive, and broad coverage is difficult to
obtain at reasonable costs. Two data sources that
we compare SeeTweet to are the Atlas of North
American English (Labov et al., 2008, ANAE)
and the Harvard Dialect Survey (Vaux and Golder,
2003, HDS), both of which obtained high-quality
data, but over the course of years. Such studies
remain the gold-standard for most purposes, but
SeeTweet presents a rapid, cheap, and surprisingly
effective alternative for broad coverage on some
problems in dialect geography.
3.2 Querying Twitter
SeeTweet queries Twitter through its API, us-
ing Mike Verdone?s Python Twitter Tools
3
. The
API returns the 1000 most recent query-matching
tweets or all query-matching tweets within the
3
http://mike.verdone.ca/twitter/
last week, whichever is smaller, and can be ge-
ographically limited to tweets within a certain
radius of a center point. In theory, the contigu-
ous United States are covered by a 2500km ra-
dius (Twitter?s maximum) around the geographic
center, approximately 39.8
?
N, 98.6
?
W, near the
Kansas-Nebraska border. In practice, though, such
a query only returns tweets from a non-circular re-
gion within the Great Plains.
Through trial-and-error, four search centers
were found that span the contiguous U.S. with
minimal overlap and nearly complete coverage,
4
located near Austin, Kansas City, San Diego, and
San Francisco. All results presented here are based
on these four search centers. Tweets located out-
side the U.S. or with unmappable locations are dis-
carded.
The need for multiple queries and the API?s
tweet limit complicate the analysis. The four
searches must be balanced against each other to
avoid overrepresenting certain areas, especially in
constructing the baseline p(M). If any searches
reach the 1000-tweet limit, only the search with
the most recent 1000th tweet has all of its tweets
used. All tweets before that tweet are removed,
balancing the searches by having them all span the
same timeframe. Due to the seven-day limit for re-
cent tweets, many searches do not return 1000 hits;
if none of the searches max out, all returned tweets
are accepted.
3.3 Establishing the baseline
For the baseline query (used to estimate p(M)),
SeeTweet needs a query with approximately uni-
form usage across the country. Function or stop
words are reasonable candidates for this task. We
use the word I here, which was chosen as it is
common in all American English dialects but not
other major languages of the U.S., and it has few
obvious alternative forms. Other stop words were
tested, but the specific baseline query had little im-
pact on the learned distribution; correlations be-
tween maps with I, of, the or a baselines were all
above .97 on both baseline distributions and esti-
mated conditional distributions.
Each tweet from the target query requires its
own baseline estimate, as the true distribution of
metadata varies over time. For instance, there will
be relatively more tweets on the East Coast in
4
Northern New England has limited coverage, and the
Mountain West returns little data outside the major cities.
100
the early morning (when much of the West Coast
is still asleep). Thus, SeeTweet builds the base-
line distribution by querying the baseline term I,
and using the first 50 tweets preceding each tar-
get tweet. This query is performed for each search
center for each tweet, with the centers balanced as
discussed in the previous section.
5
3.4 Determining coordinates and mapping
A tweet?s geographic information can be specified
in many ways. These include coordinates specified
by a GPS system (?geotags?), user-specified coor-
dinates, or user specification of a home location
whose coordinates can be geocoded. Some tweets
may include more that one of these, and SeeTweet
uses this hierarchy: geotags are accepted first, fol-
lowed by user-specified coordinates, followed by
user-specified cities. This hierarchy moves from
sources with the least noise to the most.
Obtaining coordinates from user-specified loca-
tions is done in two steps. First, if the user?s loca-
tion follows a ?city, state? format, it is searched
for in the US Board on Geographic Names?s
Geographic Names Information System
6
, which
matches city names to coordinates. Locations that
do not fit the ?city, state? format are checked
against a manually compiled list of coordinates
for 100 major American cities. This second step
catches many cities that are sufficiently well-
known that a nickname is used for the city (e.g.,
Philly) and/or the state is omitted.
Tweets whose coordinates cannot be deter-
mined by these methods are discarded; this is ap-
proximately half of the returned tweets in the ex-
periments discussed here.
This process yields a database of tweet coor-
dinates for each query. To build the probability
distributions, SeeTweet uses a two-dimensional
Gaussian kernel density estimator. Gaussian distri-
butions account for local geographic dependency
and uncertainty in the exact location of a tweeter
as well as smoothing the distributions. The stan-
dard deviation (?bandwidth?) of the kernels is a
free parameter, and can be scaled to supply ap-
propriate coverage/granularity of the map. We use
5
An alternative baseline, perhaps even more intuitive,
would be to use some number of sequential tweets preced-
ing the target tweet. However, the Twitter API query mecha-
nism subsamples from the overall set of tweets, so sequential
tweets may not follow the same distribution as the queries
and would provide an inappropriate baseline.
6
http://geonames.usgs.gov/domestic/
download_data.htm
3 degrees (approximately 200 miles) of band-
width for all maps in this paper, but found con-
sistently high correlation (at least .79 by Hosmer-
Lemeshow) to the ANAE data in Sect. 4.1 with
bandwidths between 0.5 and 10 degrees.
The KDE estimates probabilities on a grid over-
laid on the map; we make each grid box a square
one-tenth of a degree on each side and calculate
p?(d|m) for each box m. SeeTweet maps plot the
value of p?(d|M) on a color gradient with approxi-
mately constant luminosity. Orange indicates high
probability of the search term, and blue low prob-
ability. Constant luminosity is used so that confi-
dence in the estimate can be represented by opac-
ity; regions with higher confidence in the esti-
mated probability appear more opaque.
7
Unfortu-
nately, this means that the maps will not be infor-
mative if printed in black and white.
4 Experiments in dialect geography
Our first goal is to test the SeeTweet results against
an existing gold standard in dialect geography;
for this, we compare SeeTweet distributions of
the needs done construction to those found by
long-term sociolinguistic studies and show that the
quick-and-dirty unsupervised SeeTweet distribu-
tions are accurate reflections of the slow-and-clean
results. Our second goal is show the importance of
using the correct conditional distribution, by com-
paring it to the unadjusted distribution. With these
points established, we then use SeeTweet to create
maps of previously uninvestigated problems.
4.1 Method verification on need + past
participle
The Atlas of North American English (Labov et
al., 2008) is the most complete linguistic atlas of
American English dialect geography. It focuses on
phonological variation, but also includes a small
set of lexical/syntactic alternations. One is the
needs + past participle construction, as in The car
needs (to be) washed. This construction has a lim-
ited geographic distribution, and ANAE provides
the first nationwide survey of its usage.
We compare SeeTweet?s conditional probabili-
ties for this construction to the ANAE responses to
see how the relatively uncontrolled Twitter source
compares to the tightly controlled telephone sur-
vey data that ANAE reports. We create a SeeTweet
7
Confidence is given by the square root of the smoothed
number of tweets in a grid box m, p(m|d) ? C(d).
101
ll l l
lll llllll
ll l
ll
l
l ll ll l
l
l ll ll l lll
l
l
l
ll l
l
lll ll l
l
l
l
lll
l lll l l
l
l
lll ll ll
l
l
ll
l
l
ll
ll ll ll lll
l
l lll l
ll
l l
l
l
lll
ll
l
l l
ll l
l
l
ll
l
l
lll l ll lll
25
30
35
40
45
50
?120 ?100 ?80Longitude
Latitude
Responsel Used byRespondentFamiliar toRespondentUnfamiliar toRespondent
ANAE/Telsur Response Data
(a) ANAE/Telsur survey responses for need+past partici-
ple.
(b) SeeTweet search for ?needs done?.
Figure 1: Comparing the SeeTweet distribution
and ANAE responses for needs done usage. Or-
ange indicates higher local usage, purple moder-
ate, and blue lower. Increased opacity indicates
more confidence (i.e., more tweets) in a region.
map and visually compare this to the ANAE map,
along with a Hosmer-Lemeshow-style analysis.
The SeeTweet map is not calibrated to the ANAE
map; they are each built independently.
The ANAE map (Fig. 1a) shows the responses
of 577 survey participants who were asked about
needs done. Three possible responses were consid-
ered: they used the construction themselves, they
did not use it but thought it was used in their area,
or they neither used it nor believed it to be used in
their area.
The SeeTweet map (Fig. 1b) is built from five
searches for the phrase ?needs done?, yielding 480
positive tweets and 32275 baseline tweets.
8
The
component distributions p(M |d) and p(M) are es-
timated by Gaussian kernels with bandwidth 3.
The log of p?(f |M), calculated as in Eq. 1, de-
termines the color of a region; orange indicates a
higher value, purple a middle (approx. 1) value,
and blue a low value. Confidence in the estimate
is reflected by opacity; higher opacity indicates
higher confidence in the estimate. Confidence val-
ues above 3 (corresponding to 9 tweets per bin) are
8
The verb do was used as it was found to be the most com-
mon verb in corpus work on needs to be [verbed] construc-
tions (Doyle and Levy, 2008), appearing almost three times
as often as the second-most common verb (replace).
fully opaque. This description holds for all other
maps in this paper.
We start with a qualitative comparison of the
maps. Both maps show the construction to be most
prominent in the area between the Plains states and
central Pennsylvania (the North Midland dialect
region), with minimal use in New England and
Northern California and limited use elsewhere.
SeeTweet lacks data in the Mountain West and
Great Plains, and ANAE lacks data for Minnesota
and surrounding states.
9
The most notable devia-
tion between the maps is that SeeTweet finds the
construction more common in the Southeast than
ANAE does.
Quantitative comparison is possible by compar-
ing SeeTweet?s estimates of the unnormalized con-
ditional probability of needs done in a location
with the ANAE informants? judgments there. Two
such comparisons are shown in Fig. 2.
The first comparison (Fig. 2a) is a violin
plot with the ANAE divided into the three re-
sponse categories. The vertical axis represents
the SeeTweet estimates, and the width of a vi-
olin is proportional to the likelihood of that
ANAE response coming from a region of the
given SeeTweet estimate. The violins? mass shifts
toward regions with lower SeeTweet estimates
(down in the graph) as the respondents report
decreasing use/familiarity with the construction
(moving left to right).
Users of the construction are most likely to
come from regions with above-average condi-
tional probability of needs done, as seen in the left-
most violin. Non-users, whether familiar with the
construction or not, are more likely to come from
regions with below-average conditional probabil-
ity. Non-users who are unfamiliar with it tend to
live in regions with the lowest conditional prob-
abilities of the three groups. This shows the ex-
pected correspondence trend between the ANAE
responses and the estimated prevalence of the con-
struction in an area; the mean SeeTweet estimates
for the three groups are 0.45, ?0.34, and ?0.61,
respectively.
The second comparison (Fig. 2b) is a Hosmer-
Lemeshow plot. The respondents are first divided
into deciles based on the SeeTweet estimate at
their location. Two mean values are calculated for
each decile: the mean SeeTweet log-probability
9
Murray et al. (1996)?s data suggest that these untested
areas would not use the construction; the SeeTweet data sug-
gests this as well.
102
ll
l
l l
l
l
l
ll
l
l l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l l
l
l
l
l
l
lll
l
l
ll ll
l
ll l
l
l
l
l
l
l
l
l
ll
l
l
l
l
ll
l l
l
ll
l
l
l
l ll l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l l
lll
ll
ll
l
l
l
l
l
l
ll
ll
l
l
l
l
l
l
l
l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
ll
lll
l
l
ll
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l l
l
l
l
l l
?2
?1
0
1
2
Used byRespondent Familiar toRespondent Unfamiliar toRespondentrespondent opinion on needs+ppt [ANAE]
log 
unn
orm
alize
d p(n
eeds
+ppt|
locati
on) [S
eeTw
eet]
(a) Violin plot of SeeTweet estimated conditional proba-
bility against ANAE response type.
l
l
l
l
l
l
l
l
l
l
?3
?2
?1
0
1
2
?2 ?1 0log proportion accepting needs+ppt [ANAE]
log 
unn
orm
alize
d p(n
eeds
+ppt|
locati
on) [S
eeTw
eet]
ANAE vs. SeeTweet, Binned Predictions
(b) Hosmer-Lemeshow plot of SeeTweet distribution
deciles against average probability of ANAE respondent
usage.
Figure 2: Quantifying the relationship between the
SeeTweet distribution and ANAE reports for needs
done.
estimate (increasing with each decile) and the log-
proportion of respondents in that decile who use
the construction.
10
If SeeTweet estimates of the
conditional distribution are an adequate reflection
of the ANAE survey data, we should see a tight
correlation between the SeeTweet and ANAE val-
ues in each decile. The correlation between the
two is R
2
= 0.90. This is an improvement over the
inappropriate conditional distribution p(M |d) that
is obtained by smoothing the tweet map without
dividing by the overall tweet distribution p(M).
Its Hosmer-Lemeshow correlation is R
2
= 0.79
These experiments verify two important points:
the SeeTweet method can generate data that is
tightly correlated with gold-standard data from
controlled surveys, and conditionality inversion
establishes a more appropriate distribution to cor-
rect for different baseline frequencies in tweeting.
This second point will be examined further with
double modals in the next section.
4.2 Double modals and the importance of the
baseline
The double modal construction provides a second
test case. While ungrammatical in Standard Amer-
ican English, forms like I might could use your
help are grammatical and common in Southern
American dialects. This construction is interesting
both for its theoretical syntax implications on the
nature of modals as well as the relationship be-
tween its sociolinguistic distribution and its prag-
matics (Hasty, 2011).
The ANAE does not have data on double
modals? distribution, but another large-scale soci-
olinguistic experiment does: the Harvard Dialect
Survey (Vaux and Golder, 2003). This online sur-
vey obtained 30788 responses to 122 dialect ques-
tions, including the use of double modals. Katz
(2013) used a nearest-neighbor model to create a
p(d|M) distribution over the contiguous U.S. for
double modal usage, mapped in Fig. 3a.
11
Lighter
colors indicate higher rates of double modal ac-
ceptance.
SeeTweet generates a similar map (Fig. 3b),
based on three searches with 928 positive and
66272 baseline tweets. As with the ANAE test, the
10
We remove all respondents who do not use the construc-
tion but report it in their area. Such respondents are fairly
rare (slightly over 10% of the population), and removing this
response converts the data to a binary classification problem
appropriate to Hosmer-Lemeshow analysis.
11
http://spark.rstudio.com/jkatz/Data/
comp-53.png
103
(a) Katz?s nearest-neighbor estimates of the double
modal?s distribution in the Harvard Dialect Survey.
(b) SeeTweet distribution for might could.
(c) Inappropriate p(M |d) distribution directly estimated
from Twitter hits.
Figure 3: Maps of the double modal?s distribution.
SeeTweet map is built independently of the HDS
data and is not calibrated to it.
The notable difference between the maps is
that SeeTweet does not localize double modals as
sharply to the Southeast, with pockets in cities
throughout the country. This may reflect the dif-
ference in the meaning of locations on Twitter and
in the HDS; Twitter locations will be a user?s cur-
rent home, whereas the HDS explicitly asks for a
respondent?s location during their formative years.
SeeTweet may partly capture the spread of dialec-
tal features due to migration.
Double modals also provide an illustration of
the importance of the Bayesian inversion in Eqn.
1, as shown in Fig. 3c. This map, based on
the inappropriate distribution p(M |d), which does
not account for the overall distribution p(M),
disagrees with general knowledge of the double
modal?s geography and the HDS map. Although
both maps find double modals to be prominent
around Atlanta, the inappropriate distribution find
New York City, Chicago, and Los Angeles to be
the next most prominent double modal regions,
with only moderate probability in the rest of the
Southeast. This is not incorrect, per se, as these
are the sources of many double modal tweets; but
these peaks are incidental, as major cities produce
more tweets than the rest of the country. This is
confirmed by their absence in the HDS map as
well as the appropriate SeeTweet map.
4.3 Extending SeeTweet to new problems
Given SeeTweet?s success in mapping needs done
and double modals, it can also be used to test new
questions. An understudied issue in past work on
the need + past participle construction is its rela-
tionship with alternative forms need to be + past
participle and need + present participle. Murray
et al. (1996) suggest that their need + past par-
ticiple users reject both alternatives, although it
is worth noting that their informants are more ac-
cepting of the to be alternative, calling it merely
?too formal?, as opposed to an ?odd? or ?ungram-
matical? opinion about the present participle form.
Their analysis of the opinions on alternative forms
does not go beyond this anecdotal evidence.
SeeTweet provides the opportunity to examine
this issue, and finds that the to be form is per-
sistent across the country (Fig. 4c), both in areas
with and without the need + past participle form,
whereas the present participle alternant (Fig. 4b)
is strongest in areas where need + past participle is
not used. Although further analysis is necessary to
see if the same people use both the past participle
forms, the current data suggests that the bare past
participle and bare present participle forms are in
complementary distribution, while the to be form
is acceptable in most locations.
We also compare the alternative constructions
to the ANAE data. Using Hosmer-Lemeshow
analysis, we find negative correlations: R
2
=
?.65 for needs doing and R
2
= ?.25 for needs
to be done. In addition, mean SeeTweet estimates
of needs doing usage were lower for regions where
respondents use needs done than for regions where
they do not: ?.93 versus ?.49.
12
Thus, SeeTweet
provides evidence that needs done and needs do-
ing are in a geographically distinct distribution,
while needs done and needs to be done are at most
weakly distinct.
12
SeeTweet estimates of needs to be done usage were com-
parable in both regions, ?.018 against .019.
104
(a) ?Needs done? distribution
(b) ?Needs doing? distribution
(c) ?Needs to be done? distribution
Figure 4: SeeTweet distributions for needs done,
needs to be done, and needs doing.
5 The appropriateness of Twitter as a
data source
A possible concern with this analysis is that Twit-
ter could be a biased and noisy dataset, inappropri-
ate for sociolinguistic investigation. Twitter skews
toward the young and slightly toward urbanites
(Duggan and Brenner, 2013). However, as young
urbanites tend to drive language change (Labov
et al., 2008), any such bias would make the re-
sults more useful for examining sociolinguistic
changes and emergent forms. The informality of
the medium also provides unedited writing data
that is more reflective of non-standard usage than
most corpora, and its large amounts of data in short
timescales offers new abilities to track emerging
linguistic change.
As for noise in the tweet data and locations, the
strong correlations between the gold-standard and
SeeTweet results show that, at least for these fea-
tures, the noise is mitigated by the size of dataset.
We examined the impact of noise on the needs
done dataset by manually inspecting the data for
false positives and re-mapping the clean data. Al-
though the false positive rate was 12%, the con-
ditional distribution learned with and without the
false positives removed remained tightly corre-
lated, at R
2
= .94. The SeeTweet method ap-
pears to be robust to false positives, although nois-
ier queries may require manual inspection.
A final point to note is that while the datasets
used in constructing these maps are relatively
small, they are crucially derived from big data. Be-
cause the needs done and double modal construc-
tions are quite rare, there would be very few ex-
amples in a standard-sized corpus. Only because
there are so many tweets are we able to get the
hundreds of examples we used in this study.
6 Conclusion
We have shown that Bayesian inversion can be
used to build conditional probability distributions
over data given metadata from the results of
queries on social media, connecting query-derived
data to traditional data sources. Tests on Twitter
show that such calculations can provide dialect
geographies that are well correlated with exist-
ing gold-standard sources at a fraction of the time,
cost, and effort.
Acknowledgments
We wish to thank Roger Levy, Dan Michel, Emily
Morgan, Mark Mysl??n, Bill Presant, Agatha Ven-
tura, and the reviewers for their advice, sugges-
tions, and testing. This work was supported in part
by NSF award 0830535.
References
Gabriel Doyle and Roger Levy. 2008. Environment
prototypicality in syntactic alternation. In Proceed-
ings of the 34th Annual Meeting of the Berkeley Lin-
guistics Society.
Maeve Duggan and Joanna Brenner. 2013. The demo-
graphics of social media users ? 2012. Pew Internet
and American Life Project.
J. Daniel Hasty. 2011. I might would not say that:
A sociolinguistic study of double modal acceptance.
In University of Pennsylvania Working Papers in
Linguistics, volume 17.
Joshua Katz. 2013. Beyond ?soda, pop, or
coke?: Regional dialect variation in the continental
US. Retrieved from http://www4.ncsu.edu/
?
jakatz2/project-dialect.html.
William Labov, Sharon Ash, and Charles Boberg.
2008. The Atlas of North American English. Pho-
netics, Phonology, and Sound Change. de Gruyter
Mouton.
105
Thomas Murray, Timothy Frazer, and Beth Lee Simon.
1996. Need + past participle in American English.
American Speech, 71:255?271.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative
samples. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Joshua Tenenbaum and Thomas Griffiths. 2001. Gen-
eralization, similiarity, and Bayesian inference. Be-
havioral and Brain Sciences, 24:629?640.
Bert Vaux and Scott Golder. 2003. Harvard dialect
survey. Available at http://www4.uwm.edu/
FLL/linguistics/dialect/index.html.
106
Proceedings of NAACL-HLT 2013, pages 117?126,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Combining multiple information types in Bayesian word segmentation
Gabriel Doyle and Roger Levy
Department of Linguistics
University of California, San Diego
La Jolla, CA 92093, USA
{gdoyle,rlevy}@ucsd.edu
Abstract
Humans identify word boundaries in continu-
ous speech by combining multiple cues; exist-
ing state-of-the-art models, though, look at a
single cue. We extend the generative model of
Goldwater et al(2006) to segment using sylla-
ble stress as well as phonemic form. Our new
model treats identification of word boundaries
and prevalent stress patterns in the language as
a joint inference task. We show that this model
improves segmentation accuracy over purely
segmental input representations, and recov-
ers the dominant stress pattern of the data.
Additionally, our model retains high perfor-
mance even without single-word utterances.
We also demonstrate a discrepancy in the per-
formance of our model and human infants on
an artificial-language task in which stress cues
and transition-probability information are pit-
ted against one another. We argue that this dis-
crepancy indicates a bound on rationality in
the mechanisms of human segmentation.
1 Introduction
For an adult speaker of a language, word segmen-
tation from fluid speech may seem so easy that
it barely needed to be learned. However, pauses
in speech and word boundaries are not well cor-
related (Cole & Jakimik, 1980), word boundaries
are marked by a conspiracy of partially-informative
cues (Johnson & Jusczyk, 2001), and different lan-
guages mark their boundaries differently (Cutler &
Carter, 1987). This makes the problem of unsuper-
vised word segmentation acquisition, whether by a
computational model or an infant, a daunting task.
Effective segmentation relies on the flexible in-
tegration of multiple types of segmentation cues,
among them statistical regularities in phonemes and
prosody, coarticulation, and allophonic variation. In-
fants begin using multiple segmentation cues within
their first year of life (Johnson & Jusczyk, 2001).
Despite this, many state-of-the-art models look at
only one type of information: phonemes.
In this study, we expand an existing model to
incorporate multiple cues, leading to an improve-
ment in segmentation performance and opening new
ways of investigating human segmentation acquisi-
tion. On the latter point, we show that rational learn-
ers can learn to segment without encountering words
in isolation, and that human learners deviate from ra-
tionality in certain segmentation tasks.
2 Previous work
The prevailing unsupervised word segmentation sys-
tems (e.g., Brent, 1999; Goldwater, Griffiths, &
Johnson, 2006; Blanchard & Heinz, 2008) use only
phonemic information to segment speech. However,
human segmenters use additional information types,
notably stress information, in their segmentation.
We present an overview of these phonemic mod-
els here before discussing the prosodic model ex-
pansion. A more complete review is available in
Goldwater (2007).
2.1 Goldwater et al(2006)
The Goldwater et almodel is related to Brent
(1999)?s model, both of which use strictly phone-
mic information to segment. The model assumes that
the corpus is generated by a Dirichlet process over
117
word bigrams.1 We present a basic overview here,
based on Sect. 5.5 of Goldwater, 2007. To generate
the word wi given the preceding word wi?1:
1. Decide if bigram bi = ?wi?1, wi? is novel
2. If bi non-novel, draw bi from bigram lexicon
3. If bi novel, decide whether wi is novel
a. If wi non-novel, draw wi from word lexicon
b. If wi novel, draw wi from word-generating
distribution P0.
The Dirichlet process first decides whether to
draw a non-novel (?nn?) bigram, with probability
proportional to the number of times the previous
word has appeared in the corpus:
p(?wi?1, wi? nn|wi?1) =
n?wi?1,??
n?wi?1,?? + ?1
, (1)
where n?x,y? is the token count for bigram ?x, y?.
If the bigram is non-novel, word wi is drawn in
proportion to the number of times it has appeared
after wi?1 in the corpus:
p(wi = x|?wi?1, wi? nn) =
n?wi?1,x?
n?wi?1,??
(2)
If the bigram is novel, this could either be due to
wi being a novel word or due to wi being an existing
word that had not appeared with wi?1 before. The
probability of wi being a non-novel word x is
p(wi = x,wi nn|
?wi?1, wi?
novel
) =
b??,wi?
(b??,?? + ?0)
,
(3)
where b?.,.? is the count of word bigram types.
Finally, if wi is a new word, its phonemic form is
generated from a distribution P0. In the Goldwater
et almodel, this distribution is simply the product of
the unigram probabilities of the phonemes, P (?j),
times the probability of a word boundary, p#, to end
the word:
p(wi = ?1 ? ? ??M |
wi
novel
) = p#(1? p#)M?1
?
P (?j)
(4)
1We will only discuss the bigram model here because it is
more appropriate from both a cognitive perspective (it posits la-
tent hierarchical structure) and engineering perspective (it seg-
ments more accurately) than the unigram model.
To segment an observed corpus, the model Gibbs
samples over the possible word boundaries (utter-
ance boundaries are assumed to be word bound-
aries).2 The exchangability of draws from a Dirichlet
process allows for Gibbs sampling of each possible
boundary given all the others.
2.2 A cognitively-plausible variant
Phillips and Pearl (2012) make these Bayesian seg-
mentation models more cognitively plausible in two
ways. The first is to move from phonemes to syl-
lables as the base representational unit from which
words are constructed, as infants learn to categorize
syllables before phonemes (Eimas, 1999). The sec-
ond is to add memory and processing constraints on
the learner. They find that syllable-based segmen-
tation is better than phoneme-based segmentation
in the bigram model (though worse in the unigram
model), and that, counter-intuitively, the constrained
learner outperforms the unconstrained learner. This
improvement appears to be driven by better perfor-
mance in segmenting more common words. In this
work, we adopt the syllabified representation but re-
tain the unconstrained rational learner assumption.
2.3 Other multiple-cue models
Some previous models have incorporated multiple
cues, specifically the phonemic and stress infor-
mation that our model will use. Two prominent
examples are Christiansen, Allen, and Seidenberg
(1998)?s connectionist model and Gambell and Yang
(2006)?s algebraic model. The connectionist model
places word boundaries where the combination of
phonemic and stress information predict likely ut-
terance boundaries, but does not include an explicit
sense of ?word?, and performs only modestly on
the segmentation task (boundary F-scores of .40-
.45). The algebraic model also underperforms the
Bayesian model (Phillips & Pearl, 2012) unless it
includes the heuristic that there is a word bound-
ary between any two stressed syllables. Our model
presents a more general and completely unsuper-
vised approach to segmentation with multiple cue-
types.
2The model assumes that utterance boundaries are generated
just like other words, and includes an adjustable parameter p$
to account for their frequency.
118
In general, joint inference is becoming more com-
mon in language acquisition problems and has been
shown to improve performance over single-feature
inference. Examples include joint inference of a
lexicon and phonetic categories (Feldman, Grif-
fiths, & Morgan, 2009), joint inference of syntactic
word order and word reference (Maurits, Perfors, &
Navarro, 2009), and joint inference of word mean-
ings and speaker intentions in child-directed speech
(Frank, Goodman, & Tenenbaum, 2009).
3 Model design
Our model changes P0 from a single-cue distribu-
tion, generating only phonemes, to a multiple-cue
distribution that generates a stress form as well. This
can improve segmentation performance and allows
the investigation of rational segmentation behavior
in a multiple-cue world.
In the original model, P0(wi = ?1 ? ? ??M ) ??
j P (?j), where P (?j) is the frequency of the
phoneme ?j . In the multiple-cue model, we first
generate a phonemic form wi, then assign a stress
pattern si to it.
P0(wi, si) = PW (wi)PS(si|M)
= p#(1? p#)
M?1
M?
j
P (?j)PS(si|M) (5)
The phonemic form wi has the same product-of-
segments probability as the Goldwater et almodel,
but ?j are now syllables instead of phonemes. We
discuss the rationale behind this change in the next
section.
The phonemic form is generated first, and the
stress form is then drawn as a multinomial over all
possible stress patterns with the same number of syl-
lables as wi. The stress distribution PS is a multino-
mial distribution over word-length stress templates.
PS can be learned by the model based on a Dirich-
let prior, but for simplicity in the present implemen-
tation, we estimate PS as the plus-one-smoothed
frequency of the stress patterns in the current seg-
mentation. There are two stress levels (stressed or
unstressed), and 2M possible stress templates for a
word of length M .3
3We do not assume that each word has one and only one
Unlike phonemic forms, stress patterns are drawn
as a whole word. This allows the model to capture
a wide range of stress biases, although it prevents
the model from generalizing biases across different
word lengths. A potential future change to PS that
would allow for better generalization is discussed in
Section 6.
3.1 On syllabification and stress
We change from segmenting on phonemes to seg-
menting on syllables in order to more easily imple-
ment stress information, which is a supersegmental
feature most appropriately located on syllables. Syl-
labified data has been used in some previous mod-
els of segmentation, especially those using stress
information or syllable-level transition probabilities
(Christiansen et al, 1998; Swingley, 2005; Gambell
& Yang, 2006; Phillips & Pearl, 2012).
For studying human word segmentation, Phillips
and Pearl argue syllabified speech may be a
more cognitively plausible testing ground. 3-month-
old infants appear to have categorical representa-
tions of syllables (Eimas, 1999), three months be-
fore word segmentation appears (Borfeld, Morgan,
Golinkoff, & Rathbun, 2005), and seven months
before phoneme categorization (Werker & Tees,
1984). In addition, syllabification is assumed in
much work on human word segmentation, especially
in artificial-language studies (e.g., Thiessen & Saf-
fran, 2003), which calculate statistical cues at the
syllable level.
The assumption that syllable boundaries are
known affects the baseline performance of the
model, as it reduces the number of possible word
boundary locations (since a word boundary is nec-
essarily a syllable boundary). As such performance
over syllabified data cannot be directly compared to
performance on non-syllabified data.
It may seem that syllabification is so closely tied
to word segmentation that including the former in a
model of the latter leaves little to the model. How-
ever, the determinants of syllable boundaries are not
the same as those for word boundaries. The prob-
stressed syllable, which would reduce the number of possible
stress templates to M , for two reasons. First, in the current cor-
pus, some words have citation forms with multiple stressed syl-
lables. Second, in actual speech this assumption will not hold
(e.g., many function words go unstressed).
119
lem of assigning syllable boundaries is a question of
deciding where a boundary goes between two syl-
lable nuclei, with the assumption that there must be
a boundary there. The problem of assigning word
boundaries is a question of deciding whether there
is a boundary between two syllable nuclei, and if so,
where it is. Knowing the syllable boundaries reduces
the set of possible word boundaries, but does not di-
rectly address the question of how likely a boundary
is. The difference in these tasks is supported by the
three-month gap between syllable and word identifi-
cation in infants.
4 Data
We use the Korman (1984) training corpus, as com-
piled by Christiansen et al (1998), in this study. This
is a 24493-word corpus of English spoken by adults
to infants aged 6?16 weeks.4 Phonemes, stresses,
and syllable boundaries are the same as those used
by Christiansen et al which were based on citation
forms in the MRC Psycholinguistic Database. All
monosyllabic words were coded as stressed. Only
utterances for which all words had citation forms
were included.
This corpus is largely monosyllabic (87.3% of all
word tokens), and heavily biased toward initial stress
(89.2% of all multisyllable word tokens). No word
is longer than three syllables, and most words have
only one stressed syllable. A breakdown of the cor-
pus by stress pattern is given in Table 1. This mono-
syllabic bias is an inherent property of English, not
idiosyncratic to this corpus. The Bernstein-Ratner
child-directed corpus is also over 80% monosyl-
labic. We expect that the results of segmentation on
child-directed data will extend to adult speech, as
the adult-directed corpus used by Gambell and Yang
(2006) has an average word length of 1.17 syllables.
5 Experiments
We test the model on three problems. First, we show
that the addition of stress information improves seg-
mentation performance compared to a stress-less
model. Next, we apply the model to a question in
human segmentation acquisition. Finally, we look at
4Approximately 150 word tokens from the original corpus
were omitted in our version of the corpus due to a disparity
between recorded number of syllables and number of stresses.
Types Tokens
Stress pattern Count Stress pattern Count
S 21402 S 523
SW 2231 SW 208
SS 389 WS 40
WS 284 SWW 24
SWW 182 SS 7
WSW 33 WSW 7
Other 5 Other 2
Table 1: Corpus stress patterns by types and tokens,
showing an initial-stress bias in all lengths.
a task where the rational model deviates from human
performance.
5.1 Parameter setting
The model has four free parameters: ?0 and ?1,
which affect the likelihood of new words and bi-
grams, respectively, and p# and p$, which affect the
expected likelihood of word and utterance bound-
aries. Following Goldwater, Griffiths, and Johnson
(2009), we set ?0 = 20, ?1 = 100, p# = 0.8 and
p$ = 0.5 in all experiments.
5
In all cases, the model performed five indepen-
dent runs of 20000 iterations of Gibbs sampling the
boundaries for the full corpus. Simulated annealing
was performed during the burn-in period to improve
convergence. All performance measures are reported
as the mean of these five runs.
Performance is measured as word, boundary, and
lexicon precision, recall, and F-scores. A word is
matched iff both of its true boundaries are marked
as boundaries and no internal boundaries are marked
as word boundaries. Boundary counts omit utterance
boundaries, which are assumed to be word bound-
aries. Lexical counts are based on word type counts.
5.2 Stress improves performance
We begin by showing that including a second
cue type improves segmentation performance. We
compare segmentation on a corpus with the at-
tested stress patterns to that of a corpus with-
out stress. With stress information included in
the model, word/boundary/lexicon F-scores are
5Performance was similar for a range of settings between 1
and 100 for ?0 and between 10 and 200 for ?1.
120
With stress Without stress
Word Bnd Lex Word Bnd Lex
Prec .76 .99 .75 .76 .99 .72
Rec .61 .70 .87 .60 .69 .84
F .68 .82 .80 .67 .82 .77
Table 2: Precision, recall, and F-score over corpora with
and without stress information available. Stress informa-
tion especially improves lexical performance.
.68/.82/.80. Without stress, performance drops to
.67/.82/.77.6 Full results are given in Table 2.
Stress information primarily improves lexicon
performance, along with a small improvement in
token segmentation. Accounting for stress reduces
both false positives and negatives in the lexicon; the
fact that the lexical improvement is greater than that
for words or boundaries suggests that much of the
improvement rests is on rare words.
These effects are small but significant. For word
token performance, we performed a paired t-test
on utterance token F-scores between the with- and
without-stress models. This difference was signif-
icant (t = 11.28, df = 8125, p < .001). We
performed a similar utterance-by-utterance test on
boundaries; again a small singificant improvement
was found (t = 8.92, df = 6084, p < .001). To
assess lexicon performance, we calculated for each
word type in the gold-standard lexicon the propor-
tion of the five trials in which that word appeared
in the learned lexicon for the two models. We then
examined the words where the proportions differed
between the models. 89 true words appeared more
often in the with-stress lexicons; 40 appeared more
often in the without-stress lexicons. (683 appeared
equally often in both.) By a sign test, this is signif-
icant at p < .001. We also tested lexicon perfor-
mance with a binomial test on the two models? lexi-
con accuracy; this result was marginal (p = .06).
The explicit tracking of stress information also
improves the model?s acquisition of the stress bias of
the language. Acquisition of the stress bias is poten-
tially useful for generalization; stress patterns can be
used for an initial segmentation if few or none of the
words are familiar. In practice, we see children use
6Recall that due to the syllabified data, these results are not
directly comparable to unsyllabified results in previous work.
their stress biases to segment new words from En-
glish speech (Jusczyk, Houston, & Newsome, 1999)
as well as artificial languages (Thiessen & Saffran,
2003).
We assess the learned stress bias by dividing up
the corpus as the model has segmented it, and count
the number of tokens with SW versus WS stress pat-
terns.7 With stress representation, the learned stress
bias is 6.77:1, and without stress representation, the
stress bias is lower, at 6.33:1. Although these are
both underestimates of the corpus?s true stress bias
(7.86:1), the stressed model is stronger and a better
estimate of the true value.
The model?s performance can be compared to
various baselines, but perhaps the strongest is one
with every syllable boundary being a word bound-
ary. This baseline represents a shift from boundary
precision being at ceiling (as in the model) to bound-
ary recall being at ceiling. In fact, due to the pre-
ponderance of monosyllabic words in English child-
directed speech, this baseline outperforms the model
on word and boundary F-scores (.68 and .82 in the
model, .82 and .91 in the baseline). However, the
baseline?s lexicon is much worse than the model?s
(F=.80 in the with-stress model, F=.64 in the base-
line), and the baseline fails to learn anything about
the language?s stress biases. In addition, the base-
line oversegments, whereas both the model and in-
fant segmenters undersegment (Peters, 1983). This
raises an important question about what the model
should seek to optimize: though the baseline is more
accurate by token, no structure is learned; type per-
formance is more important if we want to learn the
underlying structure.
5.3 Are isolated words necessary?
We next use this model to test the necessity of iso-
lated words in rational word segmentation. It is not
immediately obvious how human learners begin to
segment words from fluid speech. Stress biases and
other phonological cues are dominant in all but the
earliest of infant word segmentation (Johnson &
Jusczyk, 2001). This raises a chicken-and-egg prob-
lem; if the cues infants favor to segment words, such
as stress biases, are dependent on the words of the
7Note this defines a stress bias for the stressless model as
well.
121
language, how do they learn enough words to deter-
mine the cues? biases?
One existing proposal is that human learners de-
velop their stress biases based on words frequently
heard in isolation (Jusczyk et al, 1999). In En-
glish, these include names and common diminutives
(e.g., mommy, kitty) that generally have initial stress.
These single-word utterances could offer the seg-
menter an initial guess of the stress bias, by suppos-
ing that short utterances are single words and record-
ing their stress patterns. The most common stress
patterns in short utterances could then be used as
an initial guess at the stress bias to bootstrap other
words and thereby improve the learned stress bias.
We test the rational learner?s need for such ex-
plicit bootstrapping by learning to segment a corpus
with all single-word utterances removed. The corpus
is produced by excising all single-word utterances
from the Korman corpus. This results in a 22081-
word corpus, 10% fewer tokens than in the original.
However, it does not substantially change the lexi-
con; the number of distinct word types only drops
from 811 to 806.
We compare performance only on ambiguous
boundaries and lexicon, as these are comparable
between the corpora, and find that the model per-
forms almost equally well. Without single-word ut-
terances, boundary and lexical F-scores are .81 and
.80, compared to .82 and .80 with single-word utter-
ances. This shows that rational learners are able to
segment even without the possibility of bootstrap-
ping stress patterns from single-word utterances.
5.4 Bounded rationality in human
segmentation
Lastly, we use this model to examine rational per-
formance in a multiple-cue segmentation task. We
show that humans? segmentation does not adhere to
these predictions, suggesting a bound on human ra-
tionality in word segmentation.
We consider an artificial language study by
Thiessen and Saffran (2003). In this study, infants
are exposed to an artificial language consisting of
four bisyllabic word types uttered repeatedly with-
out pauses. Each syllable appears in only one word
type, so within-word transition probabilities are al-
ways 1, while across-word transition probabilities
are less than 0.5. Segmentation strategies that hy-
Against bias, with TP
AB CD CD AB
WS WS WS WS
With bias, against TP
A BC DC DA B
W SW SW SW S
Table 3: Examples of segmenting an artificial language
according to transition probabilities (top) or stress bias
(bottom), when the true words have weak-strong stress.
Vertical lines represent word boundaries. The top seg-
mentation produces a smaller lexicon, but the bottom seg-
mentation produces primarily words with the preferred
stress pattern.
pothesize word boundaries at low transition proba-
bilities or that seek to minimize the lexicon size will
segment out the four word types as expected.
Segmentation in the experiment is complicated by
the presence of stress in the artificial language. De-
pending on the condition, the words are either all
strong-weak or all weak-strong. In the first condi-
tion, segmenting according to transition probabili-
ties, lexicon size, or English stress bias favors the
same segmentation. In the second condition, though,
segmenting by the English stress bias to yield a lex-
icon of strong-weak words requires boundaries in
the middle of the words. The segmenter must decide
whether transition probabilities or preferred stress
patterns are more important in segmentation. This
situation is illustrated in Table 3, with a corpus con-
sisting of two word types, AB and CD, each with
weak-strong stress.
Thiessen and Saffran found that seven-month-
old English-learning infants consistently segmented
according to the transition probabilities, regardless
of stress. However, nine-month-olds segmented ac-
cording to the English stress bias, even if this meant
going against the transition probabilities.
Intuitively, this could be rational behavior accord-
ing to our model. A child?s increasing age means
more exposure to data, potentially leading the child
to develop more confidence in the stress bias. As
confidence in the stress bias increases, the cost of
segmenting against it increases as well. A suffi-
ciently strong stress preference could lead the seg-
menter to accept a large lexicon, all of whose words
have the preferred stress pattern, over a small lexi-
122
con, all of whose words have the dispreferred stress
pattern.
To judge by the Korman corpus, English has a
stress bias of approximately 7:1 in favor of SW bi-
syllabic stress over WS.8 If human segmentation be-
havior follows the rational model, the model should
predict segmentation to favor strong-weak words
over the transition probabilities when the stress bias
is approximately this strong.
We test this rationality hypothesis with a smaller
version of the Thiessen and Saffran artificial lan-
guage, consisting of 48 tokens.9 In one version,
all tokens have the preferred SW pattern, and in
the other all tokens have the dispreferred WS pat-
tern. We then adjust the PS distribution such that
PS(SW |M = 2) = b ? PS(WS|M = 2), where
b is the bias ratio. We run the model otherwise the
same as in the previous experiments, except with 10
runs instead of 5.
Contrary to this hypothesis, the model?s segmen-
tation with b = 7 was the same whether the true
words were strong-weak or weak-strong. In all ten
runs, transition probabilities dictated the segmenta-
tion. To switch to stress-based segmentation, the bias
must be orders of magnitude greater than the English
bias. Figure 1 shows the proportions of runs in the
weak-strong condition that show segmentation ac-
cording to the stress bias, as the bias increases by
factors of 10. When b = 10000, three of the ten runs
segmented according to the stress bias; below that,
the stress bias did not affect the rational model?s seg-
mentation.
Why is this? In the Bayesian model, the stress bias
of a language affects only the PS(si|M) term in the
P0 distribution, so non-novel words are not penal-
ized for their stress pattern. The model pays only
once to create a word; once the word is generated,
no matter how a priori implausible the word was,
it may be cheaply drawn again as a non-novel word.
This effect can be illustrated with a brief calculation.
Consider a corpus built from four bisyllabic word
types (AB, CD, EF, GH), each appearingN times. If
8The specific bias varies from corpus to corpus, but this ap-
pears to be a representative value.
9The 48 tokens come from four word types, with two types
appearing 16 times and the other two appearing 8 times, mim-
icking the relative frequencies of Thiessen and Saffran?s lan-
guages. Their test language had 270 tokens.
bias
Pe
rc
en
t S
W 
seg
me
ntio
n
0
20
40
60
80
100
100 101 102 103 104 105 106
Figure 1: Percentage of runs segmented with the stress
bias, against transition probabilities, as bias varies. At
English-level biases, the rational model still overrules the
stress bias when segmenting.
the corpus is segmented against the transition proba-
bilities, the resulting lexicon will have 16 bisyllabic
word types (BA, BC, BE, BG, DA, etc.), each occur-
ring approximately N4 times.
The probability of the against-bias corpus (CWS)
is proportional to the probability of generating the
four word types, and then drawing them non-novelly
from the lexicon.10 (To simplify the calculations,
we use the unigram version of the Goldwater et al
model.)
p(CWS) ? P
4
WPS(WS)
4(N !)4
1
4N !
(6)
The first two terms are the probability of gen-
erating the four word types (Eqn. 5);11 the second
two terms are the Dirichlet process draws from
the existing lexicon N times each (Eqn. 2). By
comparison, the probability of the with-bias corpus
CSW depends on generating the 16 word types, and
drawing each non-novelly N4 times.
p(CSW ) ? P
16
W PS(SW )
16
(
N
4
!
)16 1
4N !
(7)
Given an SW bias b and a uniform distribution
over syllables (so PW = 164 ), we find:
p(CWS)
p(CSW )
= 6412
(b+ 1)12
b16
(N !)4
(N4 !)
16
(8)
10It is also possible to generate this corpus by re-drawing the
words novelly, but this is much less likely than non-novel draws.
11Because all syllables have equal unigram probabilities, the
probability of all words? phonemic forms are equal, and will be
written as PW .
123
This equation shows that the rational model is
heavily biased toward the segmentation that fits the
transition probabilities. Increasing the stress bias b
or decreasing the number of observed word tokens
makes the rational model more likely to segment
with the stress bias (against transition probabilities),
but as we see in the experimental results, the stress
bias must be very strong to overcome the efficient
lexicon that the transition probability segmentation
provides.
Since humans do not show this same inherent
bias (or quickly lose it as they acquire the stress
bias), we can ask how humans deviate from ratio-
nality. One possibility is that humans simply do not
segment in this Bayesian manner. However, previ-
ous work (Frank, Goldwater, Griffiths, & Tenen-
baum, 2010) has shown that human word segmen-
tation shows similar behavior to a resource-limited
Bayesian model. Equation 8 suggests that human
segmentation could deviate from rationality by hav-
ing an effectively stronger bias than English would
suggest (reducing the first fraction)12 or, as with
Phillips and Pearl?s constrained learners, by having
effectively less input than the model assumes (reduc-
ing the second fraction).
6 Future work
Introducing stress into the Bayesian segmentation
model suggests a few additional expansions. One
possibility is to add other cues into the genera-
tive model via P0. Any cue that is based on the
word itself can be added in this way, with little
change to the general model structure. Phonotactics
can be added using an n-gram distribution for P0
(Blanchard & Heinz, 2008). Coarticulation between
adjacent phonemes is also used in human segmen-
tation (Johnson & Jusczyk, 2001), so the P0 distri-
bution could predict higher within-word coarticula-
tion. Integrating additional cues used by human seg-
menters extends the investigation of the bounds on
rationality in human segmentation and in balancing
multiple conflicting cues.
12A potential source of an inflated bias is infants? preference
for strong-weak patterns. Jusczyk, Cutler, and Redanz (1993)
found English-hearing infants listened longer to strong-weak
patterns than weak-strong. This could lead to overestimation of
the stress bias by making possible strong-weak segmentations
more prominent in the segmenter?s mind.
A more complex view of the stress system of a
language may also be useful. One possibility is to
place a Dirichlet prior over the stress templates and
allow PS to be learned as a latent variable in the
model. Another possibility is to treat the stress tem-
plates more generally; in the present implementa-
tion, knowledge of the preferred stress patterns for
word of one length tells the segmenter nothing about
preferred stress patterns in another length. Cross-
linguistically common stress rules (e.g., those that
place stress a certain number of syllables from the
left or right edge of a word) can be coded into PS to
improve generalization. Each rule dictates a specific
stress pattern for each word length. When a word
is generated in the Dirichlet process, the generative
model would decide whether to assign stress accord-
ing to one of these rules or to assign lexical stress
from a default multinomial distribution. (This ?de-
fault? distribution would handle idiosyncratic stress
assignments, as one might see with names or mor-
phologically complex words, like Spanish reflexive
verbs.) A sparse prior over these rules, asymmetri-
cally weighted against the default category, will en-
courage the model to explain as much of the ob-
served stress patterns as possible with a few domi-
nant rules, improving the phonological structure that
the segmenter learns.
Improving the realism of the data is also impor-
tant. The corpora used in much of segmentation re-
search are idealized representations of the true data,
and the dictionary-based phoneme and stress pat-
terns used in this study are no exception. This ideal
setting may paint a skewed picture of the segmen-
tation problem, by providing a more consistent and
learnable data source than humans actually receive.
Elsner, Goldwater, and Eisenstein (2012)?s model
unifying lexical and phonetic acquisition takes a sig-
nificant step in showing that a rational segmenter
can handle noisy input by recognizing phonetic vari-
ants of a base form. In terms of stress representa-
tions, dictionary-based stress has been standard in
previous work (Christiansen et al, 1998; Gambell &
Yang, 2006; Rytting, Brew, & Fosler-Lussier, 2010),
but it is important to confirm such results against a
(currently nonexistent) corpus with stresses based on
the actual utterances. Effective use of stress in a less
idealized setting may require a more complex repre-
sentation of stress in the model.
124
7 Conclusion
Effective word segmentation combines multiple fac-
tors to make predictions about word boundaries. We
extended an existing Bayesian segmentation model
to account for two factors, phonemes and stress,
when segmenting. This improves segmentation per-
formance and opens up new possibilities for compar-
ing rational segmentation and human segmentation.
Acknowledgments
This research was partially supported by an Alfred P.
Sloan Fellowship to RL and by NSF award 0830535.
We also appreciate the feedback of the reviewers
and the members of the UCSD Computational Psy-
cholinguistics Lab.
References
Blanchard, D., & Heinz, J. (2008). Improving
word segmentation by simultaneously learn-
ing phonotactics. In Proceedings of CoNLL
(pp. 65?72).
Borfeld, H., Morgan, J., Golinkoff, R., & Rath-
bun, K. (2005). Mommy and me: familiar
names help launch babies into speech-stream
segmentation. Psychological Science, 16(4),
298?304.
Brent, M. R. (1999). An efficient, probabilistically
sound algorithm for segmentation and word
discovery. Machine Learning, 34, 71?105.
Christiansen, M. H., Allen, J., & Seidenberg, M. S.
(1998). Learning to segment speech using
multiple cues: A connectionist model. Lan-
guage and Cognitive Processes, 13, 221?268.
Cole, R., & Jakimik, J. (1980). A model of speech
perception. In Perception and production of
fluent speech (pp. 136?163). Hillsdale, NJ:
Erlbaum.
Cutler, A., & Carter, D. (1987). The predominance
of strong initial syllables in the English vocab-
ulary. Comp. Speech Lang., 2, 133?142.
Eimas, P. (1999). Segmental and syllabic representa-
tions in the perception of speech by young in-
fants. Journal of the Acoustic Society of Amer-
ica, 105, 1901?1911.
Elsner, M., Goldwater, S., & Eisenstein, J. (2012).
Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the
50th annual meeting of the ACL.
Feldman, N., Griffiths, T., & Morgan, J. (2009).
Learning phonetic categories by learning a
lexicon. In Proceedings of the 31st annual
conference on cognitive science.
Frank, M., Goldwater, S., Griffiths, T., & Tenen-
baum, J. (2010). Modeling human perfor-
mance in statistical word segmentation. Cog-
nition.
Frank, M., Goodman, N., & Tenenbaum, J. (2009).
Using speakers? referential intentions to
model early cross-situational word learning.
Psychological Science, 20, 579?585.
Gambell, T., & Yang, C. (2006). Word segmen-
tation: Quick but not dirty. (Unpublished
manuscript)
Goldwater, S. (2007). Nonparametric Bayesian
models of lexical acquisition. Unpublished
doctoral dissertation, Brown Univ.
Goldwater, S., Griffiths, T., & Johnson, M. (2006).
Contextual dependencies in unsupervised
word segmentation. In Proceedings of Col-
ing/ACL.
Goldwater, S., Griffiths, T. L., & Johnson, M.
(2009). A Bayesian framework for word seg-
mentation: Exploring the effects of context.
Cognition, 112, 21?54.
Johnson, E., & Jusczyk, P. (2001). Word segmen-
tation by 8-month-olds: When speech cues
count more than statistics. J. of Memory and
Language, 44, 548?567.
Jusczyk, P., Cutler, A., & Redanz, N. (1993). Pref-
erence for predominant stress patterns of En-
glish words. Child Development, 64, 675?
687.
Jusczyk, P., Houston, D., & Newsome, M. (1999).
The beginnings of word segmentation in
English-learning infants. Cognitive Psychol-
ogy, 39, 159?207.
Korman, M. (1984). Adaptive aspects of mater-
nal vocalizations in differing contexts at ten
weeks. First language, 5, 44?45.
Maurits, L., Perfors, A., & Navarro, D. (2009). Joint
acquisition of word order and word reference.
In Proceedings of 31st annual conference of
the Cognitive Science Society.
Peters, A. (1983). The units of language acqui-
125
sition: Monographs in applied psycholinguis-
tics. Cambridge Univ. Press.
Phillips, L., & Pearl, L. (2012). ?less is more?
in Bayesian word segmentation: When cogni-
tively plausible learners outperform the ideal.
In Proceedings of the 34th annual conference
of the cognitive science society.
Rytting, C. A., Brew, C., & Fosler-Lussier, E.
(2010). Segmenting words from natural
speech: subsegmental variation in segmental
cues. Journal of Child Language, 37, 513?
543.
Swingley, D. (2005). Statistical clustering and the
contents of the infant vocabulary. Cognitive
Psychology, 50, 86?132.
Thiessen, E. D., & Saffran, J. R. (2003). When cues
collide: Use of stress and statistical cues to
word boundaries by 7- to 9-month-old infants.
Developmental Psychology, 39(4), 706?716.
Werker, J., & Tees, R. (1984). Cross-language
speech perception: Evidence for perceptual
reorganization during the first year of life. In-
fant Behavior and Development, 7, 49?63.
126
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1094?1103,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Nonparametric Learning of Phonological Constraints in Optimality
Theory
Gabriel Doyle
Department of Linguistics
UC San Diego
La Jolla, CA, USA 92093
gdoyle@ucsd.edu
Klinton Bicknell
Department of Linguistics
Northwestern University
Evanston, IL, USA 60208
kbicknell@northwestern.edu
Roger Levy
Department of Linguistics
UC San Diego
La Jolla, CA, USA 92093
rlevy@ucsd.edu
Abstract
We present a method to jointly learn fea-
tures and weights directly from distri-
butional data in a log-linear framework.
Specifically, we propose a non-parametric
Bayesian model for learning phonologi-
cal markedness constraints directly from
the distribution of input-output mappings
in an Optimality Theory (OT) setting. The
model uses an Indian Buffet Process prior
to learn the feature values used in the log-
linear method, and is the first algorithm
for learning phonological constraints with-
out presupposing constraint structure. The
model learns a system of constraints that
explains observed data as well as the
phonologically-grounded constraints of a
standard analysis, with a violation struc-
ture corresponding to the standard con-
straints. These results suggest an alterna-
tive data-driven source for constraints in-
stead of a fully innate constraint set.
1 Introduction
Many aspects of human cognition involve the in-
teraction of constraints that push a decision-maker
toward different options, whether in something so
trivial as choosing a movie or so important as
a fight-or-flight response. These constraint-driven
decisions can be modeled with a log-linear system.
In these models, a set of constraints is weighted
and their violations are used to determine a prob-
ability distribution over outcomes. But where do
these constraints come from?
We consider this question by examining the
dominant framework in modern phonology, Opti-
mality Theory (Prince and Smolensky, 1993, OT),
implemented in a log-linear framework, MaxEnt
OT (Goldwater and Johnson, 2003), with output
forms? probabilities based on a weighted sum of
constraint violations. OT analyses generally as-
sume that the constraints are innate and univer-
sal, both to obviate the problem of learning con-
straints? identities and to limit the set of possible
languages.
We propose a new approach: to learn con-
straints with limited innate phonological knowl-
edge by identifying sets of constraint violations
that explain the observed distributional data, in-
stead of selecting constraints from an innate set
of constraint definitions. Because the constraints
are identified as sets of violations, this also per-
mits constraints specific to a given language to
be learned. This method, which we call IBPOT,
uses an Indian Buffet Process (IBP) prior to define
the space of possible constraint violation matri-
ces, and uses Bayesian reasoning to identify con-
straint matrices likely to have generated the ob-
served data. In identifying constraints solely by
their extensional violation profiles, this method
does not directly identify the intensional defini-
tions of the identified constraints, but to the extent
that the resulting violation profiles are phonologi-
cally interpretable, we may conclude that the data
themselves guide constraint identification. We test
IBPOT on tongue-root vowel harmony in Wolof, a
West African language.
The set of constraints learned by the model sat-
isfy two major goals: they explain the data as well
as the standard phonological analysis, and their vi-
olation structures correspond to the standard con-
straints. This suggests an alternative data-driven
genesis for constraints, rather than the traditional
assumption of fully innate constraints.
2 Phonology and Optimality Theory
2.1 OT structure
Optimality Theory has been used for constraint-
based analysis of many areas of language, but we
focus on its most successful application: phonol-
ogy. We consider an OT analysis of the mappings
1094
between underlying forms and their phonological
manifestations ? i.e., mappings between forms in
the mental lexicon and the actual vocalized forms
of the words.
1
Stated generally, an OT system takes some in-
put, generates a set of candidate outputs, deter-
mines what constraints each output violates, and
then selects a candidate output with a relatively
unobjectionable violation profile. To do this, an
OT system contains four major components: a
generator GEN, which generates candidate out-
put forms for the input; a set of constraints CON,
which penalize candidates; a evaluation method
EVAL, which selects an winning candidate; and
H , a language-particular weighting of constraints
that EVAL uses to determine the winning candi-
date. Previous OT work has focused on identifying
the appropriate formulation of EVAL and the val-
ues and acquisition of H , while taking GEN and
CON as given. Here, we expand the learning task
by proposing an acquisition method for CON.
To learn CON, we propose a data-driven
markedness constraint learning system that avoids
both innateness and tractability issues. Unlike pre-
vious OT learning methods, which assume known
constraint definitions and only learn the relative
strength of these constraints, the IBPOT learns
constraint violation profiles and weights for them
simultaneously. The constraints are derived from
sets of violations that effectively explain the ob-
served data, rather than being selected from a pre-
existing set of possible constraints.
2.2 OT as a weighted-constraint method
Although all OT systems share the same core
structure, different choices of EVAL lead to dif-
ferent behaviors. In IBPOT, we use the log-
linear EVAL developed by Goldwater and John-
son (2003) in their MaxEnt OT system. MEOT
extends traditional OT to account for variation
(cases in which multiple candidates can be the
winner), as well as gradient/probabilistic produc-
tions (Anttila, 1997) and other constraint interac-
tions (e.g., cumulativity) that traditional OT can-
not handle (Keller, 2000). MEOT also is motivated
by the general MaxEnt framework, whereas most
other OT formulations are ad hoc constructions
specific to phonology.
In MEOT, each constraint C
i
is associated with
1
Although phonology is usually framed in terms of sound,
sign languages also have components that serve equivalent
roles in the physical realization of signs (Stokoe, 1960).
a weight w
i
< 0. (Weights are always negative
in OT; a constraint violation can never make a
candidate more likely to win.) For a given input-
candidate pair (x, y), f
i
(y, x) is the number of vi-
olations of constraint C
i
by the pair. As a maxi-
mum entropy model, the probability of y given x
is proportional to the exponential of the weighted
sum of violations,
?
i
w
i
f
i
(y, x). If Y(x) is the
set of all output candidates for the input x, then
the probability of y as the winning output is:
p(y|x) =
exp (
?
i
w
i
f
i
(y, x))
?
z?Y(x)
exp (
?
i
w
i
f
i
(z, x))
(1)
This formulation represents a probabilistic
extension of the traditional formulation of
OT (Prince and Smolensky, 1993). Traditionally,
constraints form a strict hierarchy, where a single
violation of a high-ranked constraint is worse than
any number of violations of lower-ranked con-
straints. Traditional OT is also deterministic, with
the optimal candidate always selected. In MEOT,
the constraint weights define hierarchies of vary-
ing strictness, and some probability is assigned to
all candidates. If constraints? weights are close to-
gether, multiple violations of lower-weighted con-
straints can reduce a candidate?s probability below
that of a competitor with a single high-weight vio-
lation. As the distance between weights in MEOT
increases, the probability of a suboptimal candi-
date being chosen approaches zero; thus the tradi-
tional formulation is a limit case of MEOT.
2.3 OT in practice
Figure 1 shows tableaux, a visualization for
OT, applied in Wolof (Archangeli and Pulley-
blank, 1994; Boersma, 1999). We are interested
in four Wolof constraints that combine to induce
vowel harmony: *I, PARSE[rtr], HARMONY, and
PARSE[atr]. The meaning of these constraints will
be discussed in Sect. 4.1; for now, we will only
consider their violation profiles. Each column rep-
resents a constraint, with weights decreasing left-
to-right. Each tableau looks at a single input form,
noted in the top-left cell: ete, EtE, Ite, or itE.
Each row is a candidate output form. A black
cell indicates that the candidate, or input-candidate
pair, violates the constraint in that column.
2
A
white cell indicates no violation. Grey stripes are
2
In general, a constraint can be violated multiple times
by a given candidate, but we will be using binary constraints
(violated or not) in this work. See Sect. 5.2 for further discus-
sion.
1095
ete *? Parse(rtr) Harmony Parse(atr) Score ?te *? Parse(rtr) Harmony Parse(atr) Score
ete 0 ite -32
?te -24 ?te -80
et? -24 it? -56
?t? -8 ?t? -72
?t? *? Parse(rtr) Harmony Parse(atr) Score it? *? Parse(rtr) Harmony Parse(atr) Score
ete -32 ite -32
?te -48 ?te -120
et? -48 it? -16
?t? 0 ?t? -72
Figure 1: Tableaux for the Wolof input forms ete, EtE, Ite, and itE. Black indicates violation, white no
violation. Scores are calculated for a MaxEnt OT system with constraint weights of -64, -32, -16, and -8,
approximating a traditional hierarchical OT design. Values of grey-striped cells have negligible effects
on the distribution (see Sect. 4.3).
overlaid on cells whose value will have a negligi-
ble impact on the distribution due to the values of
higher-ranked constraint.
Constraints fall into two categories, faithful-
ness and markedness, which differ in what infor-
mation they use to assign violations. Faithfulness
constraints penalize mismatches between the in-
put and output, while markedness constraints con-
sider only the output. Faithfulness violations in-
clude phoneme additions or deletions between the
input and output; markedness violations include
penalizing specific phonemes in the output form,
regardless of whether the phoneme is present in
the input.
In MaxEnt OT, each constraint has a weight,
and the candidates? scores are the sums of the
weights of violated constraints. In the ete tableau
at top left, output ete has no violations, and there-
fore a score of zero. Outputs Ete and etE vio-
late both HARMONY (weight 16) and PARSE[atr]
(weight 8), so their scores are 24. Output EtE vi-
olates PARSE[atr], and has score 8. Thus the log-
probability of output EtE is 1/8 that of ete, and the
log-probability of disharmonious Ete and etE are
each 1/24 that of ete. As the ratio between scores
increases, the log-probability ratios can become
arbitrarily close to zero, approximating the deter-
ministic situation of traditional OT.
2.4 Learning Constraints
Choosing a winning candidate presumes that a
set of constraints CON is available, but where do
these constraints come from? The standard as-
sumption within OT is that CON is innate and
universal. But in the absence of direct evidence
of innate constraints, we should prefer a method
that can derive the constraints from cognitively-
general learning over one that assumes they are
pre-specified. Learning appropriate model features
has been an important idea in the development of
constraint-based models (Della Pietra et al, 1997).
The innateness assumption can induce tractabil-
ity issues as well. The strictest formulation of in-
nateness posits that virtually all constraints are
shared across all languages, even when there is
no evidence for the constraint in a particular lan-
guage (Tesar and Smolensky, 2000). Strict uni-
versality is undermined by the extremely large
set of constraints it must weight, as well as
the possible existence of language-particular con-
straints (Smith, 2004).
A looser version of universality supposes that
constraints are built compositionally from a set
of constraint templates or primitives or phono-
logical features (Hayes, 1999; Smith, 2004; Id-
sardi, 2006; Riggle, 2009). This version allows
language-particular constraints, but it comes with
a computational cost, as the learner must be able
to generate and evaluate possible constraints while
learning the language?s phonology. Even with rel-
atively simple constraint templates, such as the
phonological constraint learner of Hayes and Wil-
son (2008), the number of possible constraints ex-
pands exponentially. Depending on the specific
formulation of the constraints, the constraint iden-
tification problem may even be NP-hard (Idsardi,
2006; Heinz et al, 2009). Our approach of casting
the learning problem as one of identifying viola-
tion profiles is an attempt to determine the amount
that can be learned about the active constraints in a
paradigm without hypothesizing intensional con-
straint definitions. The violation profile informa-
1096
tion used by our model could then be used to nar-
row the search space for intensional constraints,
either by performing post-hoc analysis of the con-
straints identified by our model or by combining
intensional constraint search into the learning pro-
cess. We discuss each of these possibilities in Sec-
tion 5.2.
Innateness is less of a concern for faithfulness
than markedness constraints. Faithfulness viola-
tions are determined by the changes between an
input form and a candidate, yielding an indepen-
dent motivation for a universal set of faithfulness
constraints (McCarthy, 2008). Some markedness
constraints can also be motivated in a universal
manner (Hayes, 1999), but many markedness con-
straints lack such grounding.
3
As such, it is un-
clear where a universal set of markedness con-
straints would come from.
3 The IBPOT Model
3.1 Structure
The IBPOT model defines a generative process for
mappings between input and output forms based
on three latent variables: the constraint violation
matrices F (faithfulness) and M (markedness),
and the weight vector w. The cells of the violation
matrices correspond to the number of violations of
a constraint by a given input-output mapping. F
ijk
is the number of violations of faithfulness con-
straint F
k
by input-output pair type (x
i
, y
j
);M
jl
is
the number of violations of markedness constraint
M
?l
by output candidate y
j
. Note that M is shared
across inputs, as M
jl
has the same value for all
input-output pairs with output y
j
. The weight vec-
tor w provides weight for both F and M . Proba-
bilities of output forms are given by a log-linear
function:
p(y
j
|x
i
) =
exp (
?
k
w
k
F
ijk
+
?
l
w
l
M
jl
)
?
y
z
?Y(x
i
)
exp (
?
k
w
k
F
izk
+
?
l
w
l
M
zl
)
(2)
Note that this is the same structure as Eq. 1
but with faithfulness and markedness constraints
listed separately. As discussed in Sect. 2.4, we as-
sume that F is known as part of the output of GEN
(Riggle, 2009). The goal of the IBPOT model is to
3
McCarthy (2008, ?4.8) gives examples of ?ad hoc? in-
tersegmental constraints. Even well-known constraint types,
such as generalized alignment, can have disputed structures
(Hyde, 2012).
learn the markedness matrix M and weights w for
both the markedness and faithfulness constraints.
As for M , we need a non-parametric prior, as
there is no inherent limit to the number of marked-
ness constraints a language will use. We use the
Indian Buffet Process (Griffiths and Ghahramani,
2005), which defines a proper probability distri-
bution over binary feature matrices with an un-
bounded number of columns. The IBP can be
thought of as representing the set of dishes that
diners eat at an infinite buffet table. Each diner
(i.e., output form) first draws dishes (i.e., con-
straint violations) with probability proportional
to the number of previous diners who drew it:
p(M
jl
= 1|{M
zl
}
z<j
) = n
l
/j. After choosing
from the previously taken dishes, the diner can
try additional dishes that no previous diner has
had. The number of new dishes that the j-th cus-
tomer draws follows a Poisson(?/j) distribution.
The complete specification of the model is then:
M ? IBP (?); Y(x
i
) = Gen(x
i
)
w ? ??(1, 1); y|x
i
? LogLin(M,F,w,Y(x
i
))
3.2 Inference
To perform inference in this model, we adopt a
common Markov chain Monte Carlo estimation
procedure for IBPs (G?or?ur et al, 2006; Navarro
and Griffiths, 2007). We alternate approximate
Gibbs sampling over the constraint matrix M ,
using the IBP prior, with a Metropolis-Hastings
method to sample constraint weights w.
We initialize the model with a randomly-drawn
markedness violation matrix M and weight vector
w. To learn, we iterate through the output forms
y
j
; for each, we splitM
?j?
into ?represented? con-
straints (those that are violated by at least one
output form other than y
j
) and ?non-represented?
constraints (those violated only by y
j
). For each
represented constraintM
?l
, we re-sample the value
for the cell M
jl
. All non-represented constraints
are removed, and we propose new constraints, vi-
olated only by y
j
, to replace them. After each it-
eration throughM , we use Metropolis-Hastings to
update the weight vector w.
Represented constraint sampling We begin by
resampling M
jl
for all represented constraints
M
?l
, conditioned on the rest of the violations
(M
?(jl)
, F ) and the weights w. This is the sam-
pling counterpart of drawing existing features in
the IBP generative process. By Bayes? Rule, the
1097
posterior probability of a violation is propor-
tional to product of the likelihood p(Y |M
jl
=
1,M
?jl
, F, w) from Eq. 2 and the IBP prior prob-
ability p(M
jl
= 1|M
?jl
) = n
?jl
/n, where n
?jl
is the number of outputs other than y
j
that violate
constraint M
?l
.
Non-represented constraint sampling After
sampling the represented constraints for y
j
, we
consider the addition of new constraints that are
violated only by y
j
. This is the sampling coun-
terpart to the Poisson draw for new features in
the IBP generative process. Ideally, this would
draw new constraints from the infinite feature ma-
trix; however, this requires marginalizing the like-
lihood over possible weights, and we lack an ap-
propriate conjugate prior for doing so. We approx-
imate the infinite matrix with a truncated Bernoulli
draw over unrepresented constraints (G?or?ur et al,
2006). We consider in each sample at most K
?
new constraints, with weights based on the auxil-
iary vector w
?
. This approximation retains the un-
bounded feature set of the IBP, as repeated sam-
pling can add more and more constraints without
limit.
The auxiliary vector w
?
contains the weights
of all the constraints that have been removed in
the previous step. If the number of constraints
removed is less than K
?
, w
?
is filled out with
draws from the prior distribution over weights. We
then consider adding any subset of these new con-
straints to M , each of which would be violated
only by y
j
. Let M
?
represent a (possibly empty)
set of constraints paired with a subset of w
?
. The
posterior probability of drawingM
?
from the trun-
cated Bernoulli distribution is the product of the
prior probability of M
?
(
?
K
?
N
Y
+
?
K
?
)
and the like-
lihood p(Y |M
?
, w
?
,M,w, F ), including the new
constraints M
?
.
Weight sampling After sampling through
all candidates, we use Metropolis-Hastings
to estimate new weights for both con-
straint matrices. Our proposal distribution is
Gamma(w
k
2
/?, ?/w
k
), with mean w
k
and
mode w
k
?
?
w
k
(for w
k
> 1). Unlike Gibbs
sampling on the constraints, which occurs only on
markedness constraints, weights are sampled for
both markedness and faithfulness features.
4 Experiment
4.1 Wolof vowel harmony
We test the model by learning the markedness con-
straints driving Wolof vowel harmony (Archangeli
and Pulleyblank, 1994). Vowel harmony in gen-
eral refers to a phonological phenomenon wherein
the vowels of a word share certain features in the
output form even if they do not share them in the
input. In the case of Wolof, harmony encourages
forms that have consistent tongue root positions.
The Wolof vowel system has two relevant fea-
tures, tongue root position and vowel height. The
tongue root can either be advanced (ATR) or re-
tracted (RTR), and the body of the tongue can be in
the high, middle, or low part of the mouth. These
features define six vowels:
high mid low
ATR i e @
RTR I E a
We test IBPOT on the harmony system provided
in the Praat program (Boersma, 1999), previ-
ously used as a test case by Goldwater and John-
son (2003) for MEOT learning with known con-
straints. This system has four constraints:
4
? Markedness:
? *I: do not have I (high RTR vowel)
? HARMONY: do not have RTR and ATR
vowels in the same word
? Faithfulness:
? PARSE[rtr]: do not change RTR input to
ATR output
? PARSE[atr]: do not change ATR input to
RTR output
These constraints define the phonological stan-
dard that we will compare IBPOT to, with a rank-
ing from strongest to weakest of *I>> PARSE[rtr]
>> HARMONY >> PARSE[atr]. Under this rank-
ing, Wolof harmony is achieved by changing a
disharmonious ATR to an RTR, unless this cre-
ates an I vowel. We see this in Figure 1, where
three of the four winners are harmonic, but with
input itE, harmony would require violating one
of the two higher-ranked constraints. As in previ-
ous MEOT work, all Wolof candidates are faithful
4
The version in Praat includes a fifth constraint, but its
value never affects the choice of output in our data and is
omitted in this analysis.
1098
with respect to vowel height, either because height
changes are not considered by GEN, or because
of a high-ranked faithfulness constraint blocking
height changes.
5
The Wolof constraints provide an interesting
testing ground for the model, because it is a small
set of constraints to be learned, but contains the
HARMONY constraint, which can be violated by
non-adjacent segments. Non-adjacent constraints
are difficult for string-based approaches because
of the exponential number of possible relation-
ships across non-adjacent segments. However, the
Wolof results show that by learning violations di-
rectly, IBPOT does not encounter problems with
non-adjacent constraints.
The Wolof data has 36 input forms, each of the
form V
1
tV
2
, where V
1
and V
2
are vowels that agree
in height. Each input form has four candidate out-
puts, with one output always winning. The outputs
appear for multiple inputs, as shown in Figure 1.
The candidate outputs are the four combinations
of tongue-roots for the given vowel heights; the
inputs and candidates are known to the learner.
We generate simulated data by observing 1000 in-
stances of the winning output for each input.
6
The
model must learn the markedness constraints *I
and HARMONY, as well as the weights for all four
constraints.
We make a small modification to the constraints
for the test data: all constraints are limited to bi-
nary values. For constraints that can be violated
multiple times by an output (e.g., *I twice by ItI),
we use only a single violation. This is necessary in
the current model definition because the IBP pro-
duces a prior over binary matrices. We generate
the simulated data using only single violations of
each constraint by each output form. Overcoming
the binarity restriction is discussed in Sect. 5.2.
4.2 Experiment Design
We run the model for 10000 iterations, using de-
terministic annealing through the first 2500 it-
5
In the present experiment, we assume that GEN does not
generate candidates with unfaithful vowel heights. If unfaith-
ful vowel heights were allowed by GEN, these unfaithful can-
didates would incur a violation approximately as strong as *I,
as neither unfaithful-height candidates nor I candidates are at-
tested in the Wolof data.
6
Since data, matrix, and weight likelihoods all shape the
learned constraints, there must be enough data for the model
to avoid settling for a simple matrix that poorly explains the
data. This represents a similar training set size to previous
work (Goldwater and Johnson, 2003; Boersma and Hayes,
2001).
erations. The model is initialized with a ran-
dom markedness matrix drawn from the IBP and
weights from the exponential prior. We ran ver-
sions of the model with parameter settings be-
tween 0.01 and 1 for ?, 0.05 and 0.5 for ?, and
2 and 5 for K
?
. All these produced quantitatively
similar results; we report values for ? = 1, ? =
0.5, and K
?
= 5, which provides the least bias
toward small constraint sets.
To establish performance for the phonological
standard, we use the IBPOT learner to find con-
straint weights but do not update M . The resultant
learner is essentially MaxEnt OT with the weights
estimated through Metropolis sampling instead of
gradient ascent. This is done so that the IBPOT
weights and phonological standard weights are
learned by the same process and can be compared.
We use the same parameters for this baseline as
for the IBPOT tests. The results in this section are
based on nine runs each of IBPOT and MEOT; ten
MEOT runs were performed but one failed to con-
verge and was removed from analysis.
4.3 Results
A successful set of learned constraints will satisfy
two criteria: achieving good data likelihood (no
worse than the phonological-standard constraints)
and acquiring constraint violation profiles that are
phonologically interpretable. We find that both of
these criteria are met by IBPOT on Wolof.
Likelihood comparison First, we calculate the
joint probability of the data and model given the
priors, p(Y,M,w|F, ?), which is proportional to
the product of three terms: the data likelihood
p(Y |M,F,w), the markedness matrix probabil-
ity p(M |?), and the weight probability p(w). We
present both the mean and MAP values for these
over the final 1000 iterations of each run. Results
are shown in Table 1.
All eight differences are significant according
to t-tests over the nine runs. In all cases but mean
M , the IBPOT method has a better log-probability.
The most important differences are those in the
data probabilities, as the matrix and weight prob-
abilities are reflective primarily of the choice of
prior. By both measures, the IBPOT constraints
explain the observed data better than the phono-
logically standard constraints.
Interestingly, the mean M probability is lower
for IBPOT than for the phonological standard.
Though the phonologically standard constraints
1099
MAP Mean
IBPOT PS IBPOT PS
Data -1.52 -3.94 -5.48 -9.23
M -51.7 -53.3 -54.7 -53.3
w -44.2 -71.1 -50.6 -78.1
Joint -97.4 -128.4 -110.6 -140.6
Table 1: Data, markedness matrix, weight vec-
tor, and joint log-probabilities for the IBPOT and
the phonological standard constraints. MAP and
mean estimates over the final 1000 iterations for
each run. All IBPOT/PS differences are significant
(p < .005 for MAP M ; p < .001 for others).
exist independently of the IBP prior, they fit the
prior better than the average IBPOT constraints do.
This shows that the IBP?s prior preferences can be
overcome in order to have constraints that better
explain the data.
Constraint comparison Our second criterion
is the acquisition of meaningful constraints,
that is, ones whose violation profiles have
phonologically-grounded explanations. IBPOT
learns the same number of markedness constraints
as the phonological standard (two); over the final
1000 iterations of the model runs, 99.2% of the it-
erations had two markedness constraints, and the
rest had three.
Turning to the form of these constraints, Figure
2 shows violation profiles from the last iteration
of a representative IBPOT run.
7
Because vowel
heights must be faithful between input and out-
put, the Wolof data is divided into nine separate
paradigms, each containing the four candidates
(ATR/RTR ? ATR/RTR) for the vowel heights in
the input.
The violations on a given output form only
affect probabilities within its paradigm. As a
result, learned constraints are consistent within
paradigms, but across paradigms, the same con-
straint may serve different purposes.
For instance, the strongest learned markedness
constraint, shown as M1 in Figure 2, has the same
violations as the top-ranked constraint that ac-
tively distinguishes between candidates in each
paradigm. For the five paradigms with at least
one high vowel (the top row and left column),
M1 has the same violations as *I, as *I penal-
izes some but not all of the candidates. In the
7
Specifically, from the run with the median joint posterior.
other four paradigms, *I penalizes none of the
candidates, and the IBPOT learner has no rea-
son to learn it. Instead, it learns that M1 has
the same violations as HARMONY, which is the
highest-weighted constraint that distinguishes be-
tween candidates in these paradigms. Thus in the
high-vowel paradigms, M1 serves as *I, while in
the low/mid-vowel paradigms, it serves as HAR-
MONY.
The lower-weighted M2 is defined noisily, as
the higher-ranked M1 makes some values of M2
inconsequential. Consider the top-left paradigm of
Figure 2, the high-high input, in which only one
candidate does not violate M1 (*I). Because M1
has a much higher weight than M2, a violation of
M2 has a negligible effect on a candidate?s prob-
ability.
8
In such cells, the constraint?s value is in-
fluenced more by the prior than by the data. These
inconsequential cells are overlaid with grey stripes
in Figure 2.
The meaning of M2, then, depends only on the
consequential cells. In the high-vowel paradigms,
M2 matches HARMONY, and the learned and stan-
dard constraints agree on all consequential viola-
tions, despite being essentially at chance on the in-
distinguishable violations (58%). On the non-high
paradigms, the meaning of M2 is unclear, as HAR-
MONY is handled by M1 and *I is unviolated. In
all four paradigms, the model learns that the RTR-
RTR candidate violates M2 and the ATR-ATR can-
didate does not; this appears to be the model?s at-
tempt to reinforce a pattern in the lowest-ranked
faithfulness constraint (PARSE[atr]), which the
ATR-ATR candidate never violates.
Thus, while the IBPOT constraints are not
identical to the phonologically standard ones,
they reflect a version of the standard constraints
that is consistent with the IBPOT framework.
9
In paradigms where each markedness constraint
distinguishes candidates, the learned constraints
match the standard constraints. In paradigms
where only one constraint distinguishes candi-
dates, the top learned constraint matches it and the
second learned constraint exhibits a pattern con-
sistent with a low-ranked faithfulness constraint.
8
Given the learned weights in Fig. 2, if the losing candi-
date violates M1, its probability changes from 10
?12
when
the preferred candidate does not violate M2 to 10
?8
when it
does.
9
In fact, it appears this constraint organization is favored
by IBPOT as it allows for lower weights, hence the large dif-
ference in w log-probability in Table 1.
1100
*? Harmony M1 M2 *? Harmony M1 M2 *? Harmony M1 M2
iti eti ?ti
?ti ?ti ati
it? et? ?t?
?t? ?t? at?
ite ete ?te
?te ?te ate
it? et? ?t?
?t? ?t? at?
it? et? ?t?
?t? ?t? at?
ita eta ?ta
?ta ?ta ata
LearnedPhono. Std.
hi
hi
hi
mid
hi
lo
Phono. Std. Learned
mid
lo
mid
mid
mid
hi
Phono. Std. Learned
lo
hi
lo
mid
lo
lo
Figure 2: Phonologically standard (*I, HARMONY) and learned (M1,M2) constraint violation profiles for
the output forms. Learned weights for the standard constraints are -32.8 and -15.3; for M1 and M2, they
are -26.5 and -8.4. Black indicates violation, white no violation. Grey stripes indicate cells whose values
have negligible effects on the probability distribution.
5 Discussion and Future Work
5.1 Relation to phonotactic learning
Our primary finding from IBPOT is that it is possi-
ble to identify constraints that are both effective at
explaining the data and representative of theorized
phonologically-grounded constraints, given only
input-output mappings and faithfulness violations.
Furthermore, these constraints are successfully ac-
quired without any knowledge of the phonological
structure of the data beyond the faithfulness vio-
lation profiles. The model?s ability to infer con-
straint violation profiles without theoretical con-
straint structure provides an alternative solution to
the problems of the traditionally innate and univer-
sal OT constraint set.
As it jointly learns constraints and weights,
the IBPOT model calls to mind Hayes and
Wilson?s (2008) joint phonotactic learner. Their
learner also jointly learns weights and constraints,
but directly selects its constraints from a composi-
tional grammar of constraint definitions. This lim-
its their learner in practice by the rapid explosion
in the number of constraints as the maximum con-
straint definition size grows. By directly learning
violation profiles, the IBPOT model avoids this ex-
plosion, and the violation profiles can be automat-
ically parsed to identify the constraint definitions
that are consistent with the learned profile. The
inference method of the two models is different
as well; the phonotactic learner selects constraints
greedily, whereas the sampling on M in IBPOT
asymptotically approaches the posterior.
The two learners also address related but dif-
ferent phonological problems. The phonotactic
learner considers phonotactic problems, in which
only output matters. The constraints learned by
Hayes and Wilson?s learner are essentially OT
markedness constraints, but their learner does not
have to account for varied inputs or effects of faith-
fulness constraints.
5.2 Extending the learning model
IBPOT, as proposed here, learns constraints based
on binary violation profiles, defined extensionally.
A complete model of constraint acquisition should
provide intensional definitions that are phonolog-
ically grounded and cover potentially non-binary
constraints. We discuss how to extend the model
toward these goals.
IBPOT currently learns extensional constraints,
defined by which candidates do or do not violate
the constraint. Intensional definitions are needed
to extend constraints to unseen forms. Post hoc vi-
olation profile analysis, as in Sect. 4.3, provides
a first step toward this goal. Such analysis can
be integrated into the learning process using the
Rational Rules model (Goodman et al, 2008) to
identify likely constraint definitions composition-
ally. Alternately, phonological knowledge could
be integrated into a joint constraint learning pro-
cess in the form of a naturalness bias on the con-
straint weights or a phonologically-motivated re-
placement for the IBP prior.
The results presented here use binary con-
straints, where each candidate violates each con-
straint only once, a result of the IBP?s restriction
to binary matrices. Non-binarity can be handled
by using the binary matrix M to indicate whether
a candidate violates a constraint, with a second
1101
distribution determining the number of violations.
Alternately, a binary matrix can directly capture
non-binary constraints; Frank and Satta (1998)
converted existing non-binary constraints into a
binary OT system by representing non-binary con-
straints as a set of equally-weighted overlapping
constraints, each accounting for one violation. The
non-binary harmony constraint, for instance, be-
comes a set {*(at least one disharmony), *(at least
two disharmonies), etc.}.
Lastly, the Wolof vowel harmony problem pro-
vides a test case with overlaps in the candidate sets
for different inputs. This candidate overlap helps
the model find appropriate constraint structures.
Analyzing other phenomena may require the iden-
tification of appropriate abstractions to find this
same structural overlap. English regular plurals,
for instance, fall into broad categories depending
on the features of the stem-final phoneme. IBPOT
learning in such settings may require learning an
appropriate abstraction as well.
6 Conclusion
A central assumption of Optimality Theory has
been the existence of a fixed inventory of uni-
versal markedness constraints innately available to
the learner, an assumption by arguments regarding
the computational complexity of constraint iden-
tification. However, our results show for the first
time that nonparametric, data-driven learning can
identify sparse constraint inventories that both ac-
curately predict the data and are phonologically
meaningful, providing a serious alternative to the
strong nativist view of the OT constraint inventory.
Acknowledgments
We wish to thank Eric Bakovi?c, Emily Mor-
gan, Mark Mysl??n, the UCSD Computational Psy-
cholinguistics Lab, the Phon Company, and the re-
viewers for their discussions and feedback on this
work. This research was supported by NSF award
IIS-0830535 and an Alfred P. Sloan Foundation
Research Fellowship to RL.
References
Arto Anttila. 1997. Variation in Finnish phonology
and morphology. Ph.D. thesis, Stanford U.
Diana Archangeli and Douglas Pulleyblank. 1994.
Grounded phonology. MIT Press.
Paul Boersma. 1999. Empirical tests of the Gradual
Learning Algorithm. Linguistic Inquiry, 32:45?86.
Paul Boersma and Bruce Hayes. 2001. Optimality-
theoretic learning in the Praat program. In Proceed-
ings of the Institute of Phonetic Sciences of the Uni-
versity of Amsterdam.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 19:380?393.
Robert Frank and Giorgio Satta. 1998. Optimality the-
ory and the generative complexity of constraint vio-
lability. Computational Linguistics, 24:307?315.
Sharon Goldwater and Mark Johnson. 2003. Learning
OT constraint rankings using a Maximum Entropy
model. In Proceedings of the Workshop on Variation
within Optimality Theory.
Noah Goodman, Joshua Tenebaum, Jacob Feldman,
and Tom Griffiths. 2008. A rational analysis of rule-
based concept learning. Cognitive Science, 32:108?
154.
Dilan G?or?ur, Frank J?akel, and Carl Rasmussen. 2006.
A choice model with infinitely many latent features.
In Proceedings of the 23rd International Conference
on Machine Learning.
Thomas Griffiths and Zoubin Ghahramani. 2005. Infi-
nite latent feature models and the Indian buffet pro-
cess. Technical Report 2005-001, Gatsby Computa-
tional Neuroscience Unit.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379?440.
Bruce Hayes. 1999. Phonetically driven phonology:
the role of optimality theory and inductive ground-
ing. In Darnell et al editor, Formalism and Func-
tionalism in Linguistics, vol. 1. Benjamins.
Jeffrey Heinz, Gregory Kobele, and Jason Riggle.
2009. Evaluating the complexity of Optimality The-
ory. Linguistic Inquiry.
Brett Hyde. 2012. Alignment constraints. Natural
Language and Linguistic Theory, 30:789?836.
William Idsardi. 2006. A simple proof that Optimal-
ity Theory is computationally intractable. Linguistic
Inquiry, 37:271?275.
Frank Keller. 2000. Gradience in grammar: Ex-
perimental and computational aspects of degrees of
grammaticality. Ph.D. thesis, U. of Edinburgh.
John McCarthy. 2008. Doing Optimality Theory.
Blackwell.
Daniel Navarro and Tom Griffiths. 2007. A nonpara-
metric Bayesian method for inferring features from
similarity judgments. In Advances in Neural Infor-
mation Processing Systems 19.
1102
Alan Prince and Paul Smolensky. 1993. Optimality
theory: Constraint interaction in generative gram-
mar. Technical report, Rutgers Center for Cognitive
Science.
Jason Riggle. 2009. Generating contenders. Rutgers
Optimality Archive, 1044.
Jennifer Smith. 2004. Making constraints composi-
tional: toward a compositional model of Con. Lin-
gua, 114:1433?1464.
William Stokoe. 1960. Sign Language Structure. Lin-
stok Press.
Bruce Tesar and Paul Smolensky. 2000. Learnability
in Optimality Theory. MIT Press.
1103
