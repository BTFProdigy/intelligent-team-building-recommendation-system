Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1427?1432,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Detecting Compositionality of Multi-Word Expressions using Nearest
Neighbours in Vector Space Models
Douwe Kiela
University of Cambridge
Computer Laboratory
douwe.kiela@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
stephen.clark@cl.cam.ac.uk
Abstract
We present a novel unsupervised approach to
detecting the compositionality of multi-word
expressions. We compute the compositional-
ity of a phrase through substituting the con-
stituent words with their ?neighbours? in a se-
mantic vector space and averaging over the
distance between the original phrase and the
substituted neighbour phrases. Several meth-
ods of obtaining neighbours are presented.
The results are compared to existing super-
vised results and achieve state-of-the-art per-
formance on a verb-object dataset of human
compositionality ratings.
1 Introduction
Multi-word expressions (MWEs) are defined as ?id-
iosyncratic interpretations that cross word bound-
aries? (Sag et al, 2002). They tend to have a
standard syntactic structure but are often semanti-
cally non-compositional; i.e. their meaning is not
fully determined by their syntactic structure and the
meanings of their constituents. A classic example
is kick the bucket, which means to die rather than to
hit a bucket with the foot. These types of expres-
sions account for a large proportion of day-to-day
language interactions (Schuler and Joshi, 2011) and
present a significant problem for natural language
processing systems (Sag et al, 2002).
This paper presents a novel unsupervised ap-
proach to detecting the compositionality of MWEs,
specifically of verb-noun collocations. The idea is
that we can recognize compositional phrases by sub-
stituting related words for constituent words in the
phrase: if the result of a substitution yields a mean-
ingful phrase, its individual constituents are likely to
contribute toward the overall meaning of the phrase.
Conversely, if a substitution yields a non-sensical
phrase, its constituents are likely to contribute less
or not at all to the overall meaning of the phrase.
For the phrase eat her hat, for example, we might
consider the following substituted phrases:
1. consume her hat
2. eat her trousers
Both phrases are semantically anomalous, implying
that eat hat is a highly non-compositional verb-noun
collocation. Following a similar procedure for eat
apple, however, would not lead to an anomaly: con-
sume apple and eat pear are perfectly meaningful,
leading us to believe that eat apple is compositional.
In the context of distributional models, this idea
can be formalised in terms of vector spaces:
the average distance between a phrase
vector and its substituted phrase vectors is
related to its compositionality.
Since we are relying on the relative distances of
phrases in semantic space, we require a method
for computing vectors for phrases. We experi-
mented with a number of composition operators
from Mitchell and Lapata (2010), in order to com-
pose constituent word vectors into phrase vectors.
The relation between phrase vectors and substituted
phrase vectors is most pronounced in the case of
1427
pointwise multiplication, which has the effect of
placing semantically anomalous phrases relatively
close together in space (since the vectors for the con-
stituent words have little in common), whereas the
semantically meaningful phrases are further apart.
This implies that compositional phrases are less sim-
ilar to their neighbours, which is to say that the
greater the average distance between a phrase vec-
tor and its substituted phrase vectors, the greater its
compositionality.
The contribution of this short focused research pa-
per is a novel approach to detecting the composition-
ality of multi-word expressions that makes full use
of the ability of semantic vector space models to cal-
culate distances between words and phrases. Using
this unsupervised approach, we achieve state-of-the-
art performance in a direct comparison with existing
supervised methods.
2 Dataset and Vectors
The verb-noun collocation dataset from Venkatapa-
thy and Joshi (2005), which consists of 765 verb-
object pairs with human compositionality ratings,
was used for evaluation. Venkatapathy & Joshi used
a support vector machine (SVM) to obtain a Spear-
man ?s correlation of 0.448. They employed a va-
riety of features ranging from frequency to LSA-
derived similarity measures and used 10% of the
dataset as training data with tenfold cross-validation.
McCarthy et al (2007) used the same dataset and ex-
panded on the original approach by adding WordNet
and distributional prototypes to the SVM, achieving
a ?s correlation of 0.454.
The distributional vectors for our experiments
were constructed from the ukWaC corpus (Baroni
et al, 2009). Vectors were obtained using a stan-
dard window method (with a window size of 5) and
the 50,000 most frequent context words as features,
with stopwords removed. We also experimented
with syntax-based co-occurrence features extracted
from a dependency-parsed version of ukWaC, but
in agreement with results obtained by Schulte im
Walde et al (2013) for predicting compositional-
ity in German, the window-based co-occurrence
method produced better results.
We tried several weighting schemes from the liter-
ature, such as t-test (Curran, 2004), positive mutual
information (Bullinaria and Levy, 2012) and the ra-
tio of the probability of the context word given the
target word1 to the context word?s overall probabil-
ity (Mitchell and Lapata, 2010). We found that a
tf-idf variant called LTU yielded the best results, de-
fined as follows (Reed et al, 2006):
wij =
(log(fij) + 1.0) log(Nnj )
0.8 + 0.2? |context word|
|avg context word|
where fij is the number of times that the target word
and context word co-occur in the same window, nj
is the context word frequency, N is the total fre-
quency and |context word| is the total number of oc-
currences of a context word. Distance is calculated
using the standard cosine measure:
dist(v1, v2) = 1?
v1 ? v2
|v1||v2|
where v1 and v2 are vectors in the semantic vector
space model.
3 Finding Neighbours and Computing
Compositionality
We experimented with two different ways of obtain-
ing neighbours for the constituent words in a phrase.
Since vector space models lend themselves naturally
to similarity computations, one way to get neigh-
bours is to take the k-most similar vectors from a
similarity matrix. This approach is straightforward,
but has some potential drawbacks: it assumes that
we have a large number of vectors to select neigh-
bours from, and becomes computationally expensive
when the number of neighbours is increased.
An alternative source for obtaining neighbours is
the lexical database WordNet (Fellbaum, 1998). We
define neighbours as siblings in the hypernym hier-
archy, so that the neighbours of a word can be found
by taking the hyponyms of its hypernyms. Word-
Net alo allows us to extract only neighbours of the
same grammatical type (yielding noun neighbours
for nouns and verb neighbours for verbs, for exam-
ple). Since not every word has the same number
of neighbours in WordNet, we use only the first k
1We use target word to refer to the word for which a vector
is being constructed.
1428
neighbours, which means that the neighbours have
to be ranked. An obvious ranking method is to use
the frequency with which each neighbour co-occurs
with the other constituent(s) of the same phrase. For
example, for all the WordNet neighbours of eat (for
all senses of eat), we count the co-occurrences with
hat in a given window size and rank them accord-
ingly. This ranking method also has the desirable
side-effect of performing some word sense disam-
biguation, at least in some cases. For example, the
highly ranked neighbours of apple for eat apple are
likely to be items of food, and not (inedible) trees
(apple is also a tree in WordNet).
In order to obtain frequency-ranked neighbours,
we used the ukWaC corpus with a window size of
5. One reason for having multiple neighbours is that
it allows us to correct for word sense disambigua-
tion errors (as mentioned above), since averaging
over results for several neighbours reduces the im-
pact of including incorrect senses. For example, the
first 20 neighbours of eat, ranked by co-occurrence
frequency with all the objects of eat in the dataset,
are:
eat use consume drink sample smoke
swallow spend break hit save afford burn
partake dine breakfast worry damage de-
plete drug
One problem with the evaluation dataset is that
it does not solely consist of verb-noun pairs: 84
phrases contain pronouns, while there are also sev-
eral examples containing words that WordNet con-
siders to be adjectives rather than nouns. This prob-
lem was mitigated by part-of-speech tagging the
dataset. As neighbours for pronouns (which are not
included in WordNet), we used the other pronouns
present in the dataset. For the remaining words,
we included the part-of-speech when looking up the
word in WordNet.
3.1 Average distance compositionality score
We considered several different ways of construct-
ing phrasal vectors. We chose not to use the com-
positional models of Baroni and Zamparelli (2010)
and Socher et al (2011) because we believe that it is
important that our methods are completely unsuper-
vised and do not require any initial learning phase.
Hence, we experimented with different ways of con-
structing phrasal vectors according to Mitchell and
Lapata (2010) and found that pointwise multiplica-
tion  worked best in our experiments. Thus, we
define the composed vector
?????
eat hat as:
??
eat
??
hat
We can now compute a compositionality score sc by
averaging the distance between the original phrase
vector and its substituted neighbour phrase vectors
via the following formula:
sc(
?????
eat hat) =
1
2k
(
k?
i=1
dist(
??
eat
??
hat,
??
eat
????????
neighbouri) +
k?
j=1
dist(
??
eat
??
hat,
????????
neighbourj 
??
hat))
We also experimented with substituting only for
the noun or the verb, and in fact found that only tak-
ing neighbours for the verb yields better results:
sc(
?????
eat hat) =
1
k
k?
j=1
dist(
??
eat
??
hat,
????????
neighbourj 
??
hat)
To illustrate the method, consider the collocations
take breath and lend money. The annotators as-
signed these phrases a compositionality score of 1
out of 6 and 6 out of 6, respectively, meaning that the
former is non-compositional and the latter is com-
positional. The distances between the first ten verb-
substituted phrases and the original phrase, together
with the average distance, are shown in Table 1 and
Table 2.
Substituting the verb in the non-compositional
phrase yields semantically anomalous vectors,
which leads to very small changes in the distance
between it and the original phrase vector. This is a
result of using pointwise multiplication, where over-
lapping components are stressed: since the vectors
for take and breath have little overlap outside of
1429
Neighbour Dist
get breath 0.049
find breath 0.051
use breath 0.050
work breath 0.060
hold breath 0.094
run breath 0.079
carry breath 0.076
look breath 0.065
play breath 0.071
buy breath 0.100
AvgDist 0.069
Table 1: Example take breath
Neighbour Dist
pay money 0.446
put money 0.432
bring money 0.405
provide money 0.442
owe money 0.559
sell money 0.404
cost money 0.482
look money 0.425
distribute money 0.544
offer money 0.428
AvgDist 0.457
Table 2: Example lend money
the idiomatic sense in take breath, its neighbour-
substituted phrases also have little overlap, result-
ing in a smaller change in distance upon substitu-
tion. Conversely, substituting the verb in the com-
positional phrase yields meaningful vectors, putting
them in locations in semantic vector space which are
sufficiently far apart to distinguish them from the
non-compositional cases.
4 Results
Results are given for the two methods of obtaining
neighbours: via frequency-ranked WordNet neigh-
bours and via vector space neighbours. The com-
positionality score was computed by using only the
verb, only the noun, or both constituent neighbours
in the substituted phrase vectors.
System ?s
Venkatapathy and Joshi (2005) 0.447
McCarthy et al (2007) 0.454
AvgDist VSM neighbours-both 0.131
AvgDist VSM neighbours-verb 0.420
AvgDist VSM neighbours-noun 0.245
AvgDist WN-ranked neighbours-both 0.165
AvgDist WN-ranked neighbours-verb 0.461
AvgDist WN-ranked neighbours-noun 0.169
Table 3: Spearman ?s results
The results are compared with the scores reported
in Venkatapathy and Joshi (2005) and McCarthy et
al. (2007), which were achieved using SVMs with a
wide variety of features. Values of 1 ? k ? 20 were
tried. If a phrase has fewer than k neighbours be-
cause not enough neighbours have been found to co-
occur with the other constituent, we use all of them.
The results for k = 20 are reported here because
that gave the best overall score. The dataset has an
inter-annotator agreement of Kendall?s ? of 0.61 and
a Spearman ?s of 0.71 and all reported differences
in values are highly significant. Table 3 gives the
results.
Note that, even though the current approach is un-
supervised (in terms of not having access to compo-
sitionality ratings during training, although it does
rely on WordNet), it outperforms SVMs that require
an ensemble of complex feature sets (some of which
are also based on WordNet).
It is interesting to observe that the state-of-the-art
performance is reached when only using the verb?s
neighbours to compute substituted phrase vectors.
One might initially expect this not to be the case,
since e.g. eat trousers, where the noun has been
substituted, does not make a lot of sense either ?
which we would expect to be informative for de-
termining compositionality. There are two possi-
ble explanations for this, which might be at play
simultaneously: since our dataset consists of verb-
object pairs, the verb constituent is always the head
word of the phrase, and the dataset contains several
so-called ?light verbs?, which have little semantic
content of their own. Head words have been found
to have a higher impact on compositionality scores
for compound nouns: Reddy et al (2011) weighted
1430
the contribution of individual constituents in such a
way that the modifier?s contribution is included but
is weighted less highly than the head?s contribution,
which led to an improvement in performance. Our
results might be improved by weighting the contri-
bution of constituent words in a similar fashion, and
by more closely examining the impact of light verbs
for the compositionality of a phrase.
5 Related Work
The past decade has seen extensive work on compu-
tational and statistical methods in detecting the com-
positionality of MWEs (Lin, 1999; Schone and Ju-
rafsky, 2001; Katz and Giesbrecht, 2006; Sporleder
and Li, 2009; Biemann and Giesbrecht, 2011).
Many of these methods rely on distributional mod-
els and vector space models (Schu?tze, 1993; Tur-
ney and Pantel, 2010; Erk, 2012). Work has been
done on different types of phrases, including work
on particle verbs (McCarthy et al, 2003; Bannard
et al, 2003), verb-noun collocations (Venkatapathy
and Joshi, 2005; McCarthy et al, 2007), adjective-
noun combinations (Vecchi et al, 2011) and noun-
noun compounds (Reddy et al, 2011), as well as on
languages other than English (Schulte im Walde et
al., 2013). Recent developments in distributional
compositional models (Widdows, 2008; Mitchell
and Lapata, 2010; Baroni and Zamparelli, 2010; Co-
ecke et al, 2010; Socher et al, 2011) have opened
up a number of possibilities for constructing vectors
for phrases, which have also been applied to com-
positionality tests (Giesbrecht, 2009; Kochmar and
Briscoe, 2013).
This paper takes that work a step further: by con-
structing phrase vectors and evaluating these vectors
on a dataset of human compositionality ratings, we
show that existing compositional models allow us to
detect compositionality of multi-word expressions
in a straightforward and intuitive manner.
6 Conclusion
We have presented a novel unsupervised approach
that can be used to detect the compositionality of
multi-word expressions. Our results show that the
underlying intuition appears to be sound: substitut-
ing neighbours may lead to meaningful or meaning-
less phrases depending on whether or not the phrase
is compositional. This can be formalized in vec-
tor space models to obtain compositionality scores
by computing the average distance to the original
phrase?s substituted neighbour phrases. In this short
focused research paper, we show that, depending on
how we obtain neighbours, we are able to achieve
a higher performance than that achieved by super-
vised methods which rely on a complex feature set
and support vector machines.
Acknowledgments
This work has been supported by EPSRC grant
EP/I037512/1. The authors would like to thank Di-
ana McCarthy for providing the dataset; and Ed
Grefenstette, Eva Maria Vecchi, Laura Rimell and
Tamara Polajnar and the anonymous reviewers for
their helpful comments.
References
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 Workshop
on Multiword expressions: analysis, acquisition and
treatment, MWE 03.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices: Representing adjective-noun
constructions in semantic space. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1183?1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Chris Biemann and Eugenie Giesbrecht. 2011. Disco-
11: Proceedings of the workshop on distributional se-
mantics and compositionality.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
Semantic Representations from Word Co-occurrence
Statistics: Stop-lists, Stemming and SVD. Behavior
Research Methods, 44:890?907.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguistic
Analysis (Lambek Festschrift), volume 36, pages 345?
384.
James Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
1431
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635?653.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Eugenie Giesbrecht. 2009. In search of semantic com-
positionality in vector spaces. In Sebastian Rudolph,
Frithjof Dau, and SergeiO. Kuznetsov, editors, Con-
ceptual Structures: Leveraging Semantic Technolo-
gies, volume 5662 of Lecture Notes in Computer Sci-
ence, pages 173?184. Springer Berlin Heidelberg.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19.
Ekaterina Kochmar and Ted Briscoe. 2013. Capturing
Anomalies in the Choice of Content Words in Com-
positional Distributional Semantic Space. In Recent
Advances in Natural Language Processing.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ?99,
pages 317?324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 2003 workshop
on Multiword expressions: analysis, acquisition and
treatment - Volume 18, MWE ?03, pages 73?80.
Diana McCarthy, Sriram Venkatapathy, and Aravind
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369?379.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of The 5th Interna-
tional Joint Conference on Natural Language Process-
ing 2011 (IJCNLP 2011), Thailand.
J.W. Reed, Y. Jiao, T.E. Potok, B.A. Klump, M.T. El-
more, and A.R. Hurson. 2006. TF-ICF: A new term
weighting scheme for clustering dynamic data streams.
In Machine Learning and Applications, 2006. ICMLA
?06. 5th International Conference on, pages 258?263.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A Pain in the Neck for NLP. In Proceed-
ings of the Third International Conference on Com-
putational Linguistics and Intelligent Text Processing,
CICLing ?02, pages 1?15.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
Empirical Methods in Natural Language Processing,
EMNLP ?01.
William Schuler and Aravind K. Joshi. 2011. Tree-
rewriting models of multi-word expressions. In Pro-
ceedings of the Workshop on Multiword Expressions:
from Parsing and Generation to the Real World, MWE
?11, pages 25?30.
Sabine Schulte im Walde, Stefan Mu?ller, and Stephen
Roller. 2013. Exploring Vector Space Models to
Predict the Compositionality of German Noun-Noun
Compounds. In Proceedings of the 2nd Joint Confer-
ence on Lexical and Computational Semantics, pages
255?265, Atlanta, GA.
Hinrich Schu?tze. 1993. Word space. In Advances in
Neural Information Processing Systems 5, pages 895?
902. Morgan Kaufmann.
Richard Socher, Cliff Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011. Parsing Natural Scenes
and Natural Language with Recursive Neural Net-
works. In The 28th International Conference on Ma-
chine Learning, ICML 2011.
Caroline Sporleder and Linlin Li. 2009. 2009. unsuper-
vised recognition of literal and non-literal use of id-
iomatic expressions. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL, EACL
?09.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141?188, January.
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (linear) maps of the impossible: Capturing
semantic anomalies in distributional space. In Pro-
ceedings of the Workshop on Distributional Seman-
tics and Compositionality, pages 1?9, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sriram Venkatapathy and Aravind K. Joshi. 2005. Mea-
suring the relative compositionality of verb-noun (v-n)
collocations by integrating features. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
HLT ?05, pages 899?906.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sympo-
sium on Quantum Interaction, Oxford.
1432
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 36?45,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning Image Embeddings using Convolutional Neural Networks for
Improved Multi-Modal Semantics
Douwe Kiela
?
University of Cambridge
Computer Laboratory
douwe.kiela@cl.cam.ac.uk
L
?
eon Bottou
Microsoft Research
New York
leon@bottou.org
Abstract
We construct multi-modal concept repre-
sentations by concatenating a skip-gram
linguistic representation vector with a vi-
sual concept representation vector com-
puted using the feature extraction layers
of a deep convolutional neural network
(CNN) trained on a large labeled object
recognition dataset. This transfer learn-
ing approach brings a clear performance
gain over features based on the traditional
bag-of-visual-word approach. Experimen-
tal results are reported on the WordSim353
and MEN semantic relatedness evaluation
tasks. We use visual features computed us-
ing either ImageNet or ESP Game images.
1 Introduction
Recent works have shown that multi-modal se-
mantic representation models outperform uni-
modal linguistic models on a variety of tasks, in-
cluding modeling semantic relatedness and pre-
dicting compositionality (Feng and Lapata, 2010;
Leong and Mihalcea, 2011; Bruni et al., 2012;
Roller and Schulte im Walde, 2013; Kiela et al.,
2014). These results were obtained by combin-
ing linguistic feature representations with robust
visual features extracted from a set of images as-
sociated with the concept in question. This extrac-
tion of visual features usually follows the popular
computer vision approach consisting of comput-
ing local features, such as SIFT features (Lowe,
1999), and aggregating them as bags of visual
words (Sivic and Zisserman, 2003).
Meanwhile, deep transfer learning techniques
have gained considerable attention in the com-
puter vision community. First, a deep convolu-
tional neural network (CNN) is trained on a large
?
This work was carried out while Douwe Kiela was an
intern at Microsoft Research, New York.
labeled dataset (Krizhevsky et al., 2012). The
convolutional layers are then used as mid-level
feature extractors on a variety of computer vi-
sion tasks (Oquab et al., 2014; Girshick et al.,
2013; Zeiler and Fergus, 2013; Donahue et al.,
2014). Although transferring convolutional net-
work features is not a new idea (Driancourt and
Bottou, 1990), the simultaneous availability of
large datasets and cheap GPU co-processors has
contributed to the achievement of considerable
performance gains on a variety computer vision
benchmarks: ?SIFT and HOG descriptors pro-
duced big performance gains a decade ago, and
now deep convolutional features are providing a
similar breakthrough? (Razavian et al., 2014).
This work reports on results obtained by using
CNN-extracted features in multi-modal semantic
representation models. These results are interest-
ing in several respects. First, these superior fea-
tures provide the opportunity to increase the per-
formance gap achieved by augmenting linguistic
features with multi-modal features. Second, this
increased performance confirms that the multi-
modal performance improvement results from the
information contained in the images and not the
information used to select which images to use
to represent a concept. Third, our evaluation re-
veals an intriguing property of the CNN-extracted
features. Finally, since we use the skip-gram ap-
proach of Mikolov et al. (2013) to generate our
linguistic features, we believe that this work rep-
resents the first approach to multimodal distribu-
tional semantics that exclusively relies on deep
learning for both its linguistic and visual compo-
nents.
2 Related work
2.1 Multi-Modal Distributional Semantics
Multi-modal models are motivated by parallels
with human concept acquisition. Standard se-
36
mantic space models extract meanings solely from
linguistic data, even though we know that hu-
man semantic knowledge relies heavily on percep-
tual information (Louwerse, 2011). That is, there
exists substantial evidence that many concepts
are grounded in the perceptual system (Barsalou,
2008). One way to do this grounding in the context
of distributional semantics is to obtain represen-
tations that combine information from linguistic
corpora with information from another modality,
obtained from e.g. property norming experiments
(Silberer and Lapata, 2012; Roller and Schulte im
Walde, 2013) or from processing and extracting
features from images (Feng and Lapata, 2010;
Leong and Mihalcea, 2011; Bruni et al., 2012).
This approach has met with quite some success
(Bruni et al., 2014).
2.2 Multi-modal Deep Learning
Other examples that apply multi-modal deep
learning use restricted Boltzmann machines (Sri-
vastava and Salakhutdinov, 2012; Feng et al.,
2013), auto-encoders (Wu et al., 2013) or recur-
sive neural networks (Socher et al., 2014). Multi-
modal models with deep learning components
have also successfully been employed in cross-
modal tasks (Lazaridou et al., 2014). Work that is
closely related in spirit to ours is by Silberer and
Lapata (2014). They use a stacked auto-encoder
to learn combined embeddings of textual and vi-
sual input. Their visual inputs consist of vectors
of visual attributes obtained from learning SVM
classifiers on attribute prediction tasks. In con-
trast, our work keeps the modalities separate and
follows the standard multi-modal approach of con-
catenating linguistic and visual representations in
a single semantic space model. This has the advan-
tage that it allows for separate data sources for the
individual modalities. We also learn visual repre-
sentations directly from the images (i.e., we apply
deep learning directly to the images), as opposed
to taking a higher-level representation as a start-
ing point. Frome et al. (2013) jointly learn multi-
modal representations as well, but apply them to
a visual object recognition task instead of concept
meaning.
2.3 Deep Convolutional Neural Networks
A flurry of recent results indicates that image de-
scriptors extracted from deep convolutional neu-
ral networks (CNNs) are very powerful and con-
sistently outperform highly tuned state-of-the-art
systems on a variety of visual recognition tasks
(Razavian et al., 2014). Embeddings from state-
of-the-art CNNs (such as Krizhevsky et al. (2012))
have been applied successfully to a number of
problems in computer vision (Girshick et al.,
2013; Zeiler and Fergus, 2013; Donahue et al.,
2014). This contribution follows the approach de-
scribed by Oquab et al. (2014): they train a CNN
on 1512 ImageNet synsets (Deng et al., 2009),
use the first seven layers of the trained network as
feature extractors on the Pascal VOC dataset, and
achieve state-of-the-art performance on the Pascal
VOC classification task.
3 Improving Multi-Modal
Representations
Figure 1 illustrates how our system computes
multi-modal semantic representations.
3.1 Perceptual Representations
The perceptual component of standard multi-
modal models that rely on visual data is often
an instance of the bag-of-visual-words (BOVW)
representation (Sivic and Zisserman, 2003). This
approach takes a collection of images associated
with words or tags representing the concept in
question. For each image, keypoints are laid out
as a dense grid. Each keypoint is represented by
a vector of robust local visual features such as
SIFT (Lowe, 1999), SURF (Bay et al., 2008) and
HOG (Dalal and Triggs, 2005), as well as pyra-
midal variants of these descriptors such as PHOW
(Bosch et al., 2007). These descriptors are sub-
sequently clustered into a discrete set of ?visual
words? using a standard clustering algorithm like
k-means and quantized into vector representations
by comparing the local descriptors with the cluster
centroids. Visual representations are obtained by
taking the average of the BOVW vectors for the
images that correspond to a given word. We use
BOVW as a baseline.
Our approach similarly makes use of a collec-
tion of images associated with words or tags rep-
resenting a particular concept. Each image is pro-
cessed by the first seven layers of the convolu-
tional network defined by Krizhevsky et al. (2012)
and adapted by Oquab et al. (2014)
1
. This net-
work takes 224 ? 224 pixel RGB images and ap-
plies five successive convolutional layers followed
by three fully connected layers. Its eighth and last
1
http://www.di.ens.fr/willow/research/cnn/
37
Training linguistic features (after Mikolov et al., 2013)
C1-C2-C3-C4-C5
Training visual features (after Oquab et al., 2014) 
Convolutional layers Fully-connected layers
6144-dim
feature 
vector
African elephant
Wall clock
Imagenet labels
?
FC6 FC7 FC8
100-dim word projections
w(t) w(t+1) w(t+2)w(t-2)w(t-2)
C1-C2-C3-C4-C5 FC7FC6
100-dim word projections
Word
Select images
from ImageNet or ESP
Aggregate
6144-dim feature vectors
Multimodal word vector
Figure 1: Computing word feature vectors.
layer produces a vector of 1512 scores associated
with 1000 categories of the ILSVRC-2012 chal-
lenge and the 512 additional categories selected by
Oquab et al. (2014). This network was trained us-
ing about 1.6 million ImageNet images associated
with these 1512 categories. We then freeze the
trained parameters, chop the last network layer,
and use the remaining seventh layer as a filter to
compute a 6144-dimensional feature vector on ar-
bitrary 224? 224 input images.
We consider two ways to aggregate the feature
vectors representing each image.
1. The first method (CNN-Mean) simply com-
putes the average of all feature vectors.
2. The second method (CNN-Max) computes
the component-wise maximum of all feature
vectors. This approach makes sense because
the feature vectors extracted from this par-
ticular network are quite sparse (about 22%
non-zero coefficients) and can be interpreted
as bags of visual properties.
3.2 Linguistic representations
For our linguistic representations we extract 100-
dimensional continuous vector representations us-
ing the log-linear skip-gram model of Mikolov
et al. (2013) trained on a corpus consisting of
the 400M word Text8 corpus of Wikipedia text
2
together with the 100M word British National
Corpus (Leech et al., 1994). We also experi-
mented with dependency-based skip-grams (Levy
and Goldberg, 2014) but this did not improve re-
sults. The skip-gram model learns high quality se-
mantic representations based on the distributional
properties of words in text, and outperforms stan-
dard distributional models on a variety of semantic
similarity and relatedness tasks. However we note
that Bruni et al. (2014) have recently reported an
even better performance for their linguistic com-
ponent using a standard distributional model, al-
though this may have been tuned to the task.
3.3 Multi-modal Representations
Following Bruni et al. (2014), we construct multi-
modal semantic representations by concatenating
the centered and L
2
-normalized linguistic and per-
ceptual feature vectors ~v
ling
and ~v
vis
,
~v
concept
= ?? ~v
ling
|| (1? ?)? ~v
vis
, (1)
where || denotes the concatenation operator and ?
is an optional tuning parameter.
2
http://mattmahoney.net/dc/textdata.html
38
Figure 2: Examples of dog in the ESP Game dataset.
Figure 3: Examples of golden retriever in ImageNet.
4 Experimental Setup
We carried out experiments using visual repre-
sentations computed using two canonical image
datasets. The resulting multi-modal concept rep-
resentations were evaluated using two well-known
semantic relatedness datasets.
4.1 Visual Data
We carried out experiments using two distinct
sources of images to compute the visual represen-
tations.
The ImageNet dataset (Deng et al., 2009) is
a large-scale ontology of images organized ac-
cording to the hierarchy of WordNet (Fellbaum,
1999). The dataset was constructed by manually
re-labelling candidate images collected using web
searches for each WordNet synset. The images
tend to be of high quality with the designated ob-
ject roughly centered in the image. Our copy of
ImageNet contains about 12.5 million images or-
ganized in 22K synsets. This implies that Ima-
geNet covers only a small fraction of the existing
117K WordNet synsets.
The ESP Game dataset (Von Ahn and Dabbish,
2004) was famously collected as a ?game with
a purpose?, in which two players must indepen-
dently and rapidly agree on a correct word label
for randomly selected images. Once a word label
has been used sufficiently frequently for a given
image, that word is added to the image?s tags. This
dataset contains 100K images, but with every im-
age having on average 14 tags, that amounts to a
coverage of 20,515 words. Since players are en-
couraged to produce as many terms per image, the
dataset?s increased coverage is at the expense of
accuracy in the word-to-image mapping: a dog in
a field with a house in the background might be a
golden retriever in ImageNet and could have tags
dog, golden retriever, grass, field, house, door in
the ESP Dataset. In other words, images in the
ESP dataset do not make a distinction between ob-
jects in the foreground and in the background, or
between the relative size of the objects (tags for
images are provided in a random order, so the top
tag is not necessarily the best one).
Figures 2 and 3 show typical examples of im-
ages belonging to these datasets. Both datasets
have attractive properties. On the one hand, Ima-
geNet has higher quality images with better labels.
On the other hand, the ESP dataset has an interest-
ing coverage because the MEN task (see section
4.4) was specifically designed to be covered by the
ESP dataset.
4.2 Image Selection
Since ImageNet follows the WordNet hierarchy,
we would have to include almost all images in
the dataset to obtain representations for high-level
concepts such as entity, object and animal. Doing
so is both computationally expensive and unlikely
to improve the results. For this reason, we ran-
domly sample up to N distinct images from the
subtree associated with each concept. When this
returns less thanN images, we attempt to increase
coverage by sampling images from the subtree of
the concept?s hypernym instead. In order to allow
for a fair comparison, we apply the same method
of sampling up to N on the ESP Game dataset. In
all following experiments, N = 1.000. We used
the WordNet lemmatizer from NLTK (Bird et al.,
2009) to lemmatize tags and concept words so as
to further improve the dataset?s coverage.
4.3 Image Processing
The ImageNet images were preprocessed as de-
scribed by (Krizhevsky et al., 2012). The largest
centered square contained in each image is resam-
39
pled to form a 256 ? 256 image. The CNN input
is then formed by cropping 16 pixels off each bor-
der and subtracting 128 to the image components.
The ESP Game images were preprocessed slightly
differently because we do not expect the objects
to be centered. Each image was rescaled to fit in-
side a 224?224 rectangle. The CNN input is then
formed by centering this image into the 224? 224
input field, subtracting 128 to the image compo-
nents, and zero padding.
The BOVW features were obtained by comput-
ing DSIFT descriptors using VLFeat (Vedaldi and
Fulkerson, 2008). These descriptors were subse-
quently clustered using mini-batch k-means (Scul-
ley, 2010) with 100 clusters. Each image is then
represented by a bag of clusters (visual words)
quantized as a 100-dimensional feature vector.
These vectors were then combined into visual con-
cept representations by taking their mean.
4.4 Evaluation
We evaluate our multi-modal word representations
using two semantic relatedness datasets widely
used in distributional semantics (Agirre et al.,
2009; Feng and Lapata, 2010; Bruni et al., 2012;
Kiela and Clark, 2014; Bruni et al., 2014).
WordSim353 (Finkelstein et al., 2001) is a se-
lection of 353 concept pairs with a similarity rat-
ing provided by human annotators. Since this is
probably the most widely used evaluation dataset
for distributional semantics, we include it for com-
parison with other approaches. WordSim353 has
some known idiosyncracies: it includes named en-
tities, such as OPEC, Arafat, and Maradona, as
well as abstract words, such as antecedent and
credibility, for which it may be hard to find cor-
responding images. Multi-modal representations
are often evaluated on an unspecified subset of
WordSim353 (Feng and Lapata, 2010; Bruni et
al., 2012; Bruni et al., 2014), making it impossi-
ble to compare the reported scores. In this work,
we report scores on the full WordSim353 dataset
(W353) by setting the visual vector ~v
vis
to zero for
concepts without images. We also report scores
on the subset (W353-Relevant) of pairs for which
both concepts have both ImageNet and ESP Game
images using the aforementioned selection proce-
dure.
MEN (Bruni et al., 2012) was in part designed
to alleviate the WordSim353 problems. It was con-
structed in such a way that only frequent words
with at least 50 images in the ESP Game dataset
were included in the evaluation pairs. The MEN
dataset has been found to mirror the aggregate
score over a variety of tasks and similarity datasets
(Kiela and Clark, 2014). It is also much larger,
with 3000 words pairs consisting of 751 individual
words. Although MEN was constructed so as to
have at least a minimum amount of images avail-
able in the ESP Game dataset for each concept,
this is not the case for ImageNet. Hence, simi-
larly to WordSim353, we also evaluate on a subset
(MEN-Relevant) for which images are available
in both datasets.
We evaluate the models in terms of their Spear-
man ? correlation with the human relatedness rat-
ings. The similarity between the representations
associated with a pair of words is calculated using
the cosine similarity:
cos(v
1
, v
2
) =
v
1
? v
2
?v
1
? ?v
2
?
(2)
5 Results
We evaluate on the two semantic relatedness
datasets using solely linguistic, solely visual and
multi-modal representations. In the case of MEN-
Relevant and W353-Relevant, we report scores for
BOVW, CNN-Mean and CNN-Max visual repre-
sentations. For all datasets we report the scores
obtained by BOVW, CNN-Mean and CNN-Max
multi-modal representations. Since we have full
coverage with the ESP Game dataset on MEN, we
are able to report visual representation scores for
the entire dataset as well. The results can be seen
in Table 1.
There are a number of questions to ask. First
of all, do CNNs yield better visual representa-
tions? Second, do CNNs yield better multi-modal
representations? And third, is there a difference
between the high-quality low-coverage ImageNet
and the low-quality higher-coverage ESP Game
dataset representations?
5.1 Visual Representations
In all cases, CNN-generated visual representations
perform better or as good as BOVW representa-
tions (we report results for BOVW-Mean, which
performs slightly better than taking the element-
wise maximum). This confirms the motivation
outlined in the introduction: by applying state-of-
the-art approaches from computer vision to multi-
modal semantics, we obtain a signficant perfor-
40
Dataset Linguistic Visual Multi-modal
BOVW CNN-Mean CNN-Max BOVW CNN-Mean CNN-Max
ImageNet visual features
MEN 0.64 - - - 0.64 0.70 0.67
MEN-Relevant 0.62 0.40 0.64 0.63 0.64 0.72 0.71
W353 0.57 - - - 0.58 0.59 0.60
W353-Relevant 0.51 0.30 0.32 0.30 0.55 0.56 0.57
ESP game visual features
MEN 0.64 0.17 0.51 0.20 0.64 0.71 0.65
MEN-Relevant 0.62 0.35 0.58 0.57 0.63 0.69 0.70
W353 0.57 - - - 0.58 0.59 0.60
W353-Relevant 0.51 0.38 0.44 0.56 0.52 0.55 0.61
Table 1: Results (see sections 4 and 5).
mance increase over standard multi-modal mod-
els.
5.2 Multi-modal Representations
Higher-quality perceptual input leads to better-
performing multi-modal representations. In all
cases multi-modal models with CNNs outperform
multi-modal models with BOVW, occasionally by
quite a margin. In all cases, multi-modal rep-
resentations outperform purely linguistic vectors
that were obtained using a state-of-the-art system.
This re-affirms the importance of multi-modal rep-
resentations for distributional semantics.
5.3 The Contribution of Images
Since the ESP Game images come with a multi-
tude of word labels, one could question whether
a performance increase of multi-modal models
based on that dataset comes from the images them-
selves, or from overlapping word labels. It might
also be possible that similar concepts are more
likely to occur in the same image, which encodes
relatedness information without necessarily tak-
ing the image data itself into account. In short,
it is a natural question to ask whether the perfor-
mance gain is due to image data or due to word
label associations? We conclusively show that the
image data matters in two ways: (a) using a dif-
ferent dataset (ImageNet) also results in a perfor-
mance boost, and (b) using higher-quality image
features on the ESP game images increases the
performance boost without changing the associa-
tion between word labels.
5.4 Image Datasets
It is important to ask whether the source im-
age dataset has a large impact on performance.
Although the scores for the visual representa-
tion in some cases differ, performance of multi-
modal representations remains close for both im-
age datasets. This implies that our method is ro-
bust over different datasets. It also suggests that it
is beneficial to train on high-quality datasets like
ImageNet and to subsequently generate embed-
dings for other sets of images like the ESP Game
dataset that are more noisy but have better cover-
age. The results show the benefit of transfering
convolutional network features, corroborating re-
cent results in computer vision.
5.5 Semantic Similarity/Relatedness Datasets
There is an interesting discrepancy between the
two types of network with respect to dataset per-
formance: CNN-Mean multi-modal models tend
to perform best on MEN and MEN-Relevant,
while CNN-Max multi-modal models perform
better on W353 and W353-Relevant. There also
appears to be some interplay between the source
corpus, the evaluation dataset and the best per-
forming CNN: the performance leap on W353-
41
Figure 4: Varying the ? parameter for MEN, MEN-Relevant, WordSim353 and WordSim353-Relevant,
respectively.
Relevant for CNN-Max is much larger using ESP
Game images than with ImageNet images.
We speculate that this is because CNN-Max per-
forms better than CNN-Mean on a somewhat dif-
ferent type of similarity. It has been noted (Agirre
et al., 2009) that WordSim353 captures both sim-
ilarity (as in tiger-cat, with a score of 7.35) as
well as relatedness (as in Maradona-football, with
a score of 8.62). MEN, however, is explicitly de-
signed to capture semantic relatedness only (Bruni
et al., 2012). CNN-Max using sparse feature vec-
tors means that we treat the dominant components
as definitive of the concept class, which is more
suited to similarity. CNN-Mean averages over
all the feature components, and as such might be
more suited to relatedness. We conjecture that the
performance increase on WordSim353 is due to
increased performance on the similarity subset of
that dataset.
5.6 Tuning
The concatenation scheme in Equation 1 allows
for a tuning parameter ? to weight the relative
contribution of the respective modalities. Previous
work on MEN has found that the optimal param-
eter for that dataset is close to 0.5 (Bruni et al.,
2014). We have found that this is indeed the case.
On WordSim353, however, we have found the pa-
rameter for optimal performance to be shifted to
the right, meaning that optimal performance is
achieved when we include less of the visual input
compared to the linguistic input. Figure 4 shows
what happens when we vary alpha over the four
datasets. There are a number of observations to be
made here.
First of all, we can see that the performance
peak for the MEN datastes is much higher than
for the WordSim353 ones, and that its peak is rel-
atively higher as well. This indicates that MEN is
in a sense a more balanced dataset. There are two
possible explanations: as indicated earlier, Word-
Sim353 contains slightly idiosyncratic word pairs
which may have a detrimental effect on perfor-
mance; or, WordSim353 was not constructed with
multi-modal semantics in mind, and contains a
substantial amount of abstract words that would
not benefit at all from including visual informa-
tion.
Due to the nature of the datasets and the tasks
at hand, it is arguably much more important that
CNNs beat standard bag-of-visual-words repre-
sentations on MEN than on W353, and indeed we
see that there exists no ? for which BOVW would
beat any of the CNN networks.
6 Error Analysis
Table 2 shows the top 5 best and top 5 worst scor-
ing word pairs for the two datasets using CNN-
42
W353-Relevant
ImageNet ESP Game
word1 word2 system score gold standard word1 word2 system score gold standard
tiger tiger 1.00 1.00 tiger tiger 1.00 1.00
man governor 0.53 0.53 man governor 0.53 0.53
stock phone 0.15 0.16 stock phone 0.15 0.16
football tennis 0.68 0.66 football tennis 0.68 0.66
man woman 0.85 0.83 man woman 0.85 0.83
cell phone 0.27 0.78 law lawyer 0.33 0.84
discovery space 0.10 0.63 monk slave 0.58 0.09
closet clothes 0.22 0.80 gem jewel 0.41 0.90
king queen 0.26 0.86 stock market 0.33 0.81
wood forest 0.13 0.77 planet space 0.32 0.79
MEN-Relevant
ImageNet ESP Game
word1 word2 system score gold standard word1 word2 system score gold standard
beef potatoes 0.35 0.35 beef potatoes 0.35 0.35
art work 0.35 0.35 art work 0.35 0.35
grass stop 0.06 0.06 grass stop 0.06 0.06
shade tree 0.45 0.45 shade tree 0.45 0.45
blonde rock 0.07 0.07 blonde rock 0.07 0.07
bread potatoes 0.88 0.34 bread dessert 0.78 0.24
fruit potatoes 0.80 0.26 jacket shirt 0.89 0.34
dessert sandwich 0.76 0.23 fruit nuts 0.88 0.33
pepper tomato 0.79 0.27 dinner lunch 0.93 0.37
dessert tomato 0.66 0.14 dessert soup 0.81 0.23
Table 2: The top 5 best and top 5 worst scoring pairs with respect to the gold standard.
Mean multi-modal vectors. The most accurate
pairs are consistently the same across the two im-
age datasets. There are some clear differences
between the least accurate pairs, however. The
MEN words potatoes and tomato probably have
low quality ImageNet-derived representations, be-
cause they occur often in the bottom pairs for that
dataset. The MEN words dessert, bread and fruit
occur in the bottom 5 for both image datasets,
which implies that their linguistic representations
are probably not very good. For WordSim353, the
bottom pairs on ImageNet could be said to be sim-
ilarity mistakes; while the ESP Game dataset con-
tains more relatedness mistakes (king and queen
would evaluate similarity, while stock and market
would evaluate relatedness). It is difficult to say
anything conclusive about this discrepancy, but it
is clearly a direction for future research.
7 Image embeddings
To facilitate further research on image embed-
dings and multi-modal semantics, we publicly re-
lease embeddings for all the image labels occur-
ring in the ESP Game dataset. Please see the fol-
lowing web page: http://www.cl.cam.ac.
uk/
?
dk427/imgembed.html
8 Conclusion
We presented a novel approach to improving
multi-modal representations using deep convo-
lutional neural network-extracted features. We
reported high results on two well-known and
widely-used semantic relatedness benchmarks,
with increased performance both in the separate
visual representations and in the combined multi-
modal representations. Our results indicate that
such multi-modal representations outperform both
linguistic and standard bag-of-visual-words multi-
modal representations. We have shown that our
approach is robust and that CNN-extracted fea-
tures from separate image datasets can succesfully
be applied to semantic relatedness.
In addition to improving multi-modal represen-
tations, we have shown that the source of this im-
provement is due to image data and is not simply a
result of word label associations. We have shown
this by obtaining performance improvements on
two different image datasets, and by obtaining
43
higher performance with higher-quality image fea-
tures on the ESP game images, without changing
the association between word labels.
In future work, we will investigate whether our
system can be further improved by including con-
creteness information or a substitute metric such
as image dispersion, as has been suggested by
other work on multi-modal semantics (Kiela et al.,
2014). Furthermore, a logical next step to increase
performance would be to jointly learn multi-modal
representations or to learn weighting parameters.
Another interesting possibility would be to exam-
ine multi-modal distributional compositional se-
mantics, where multi-modal representations are
composed to obtain phrasal representations.
Acknowledgments
We would like to thank Maxime Oquab for pro-
viding the feature extraction code.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 19?27, Boulder, Colorado.
Lawrence W. Barsalou. 2008. Grounded cognition.
Annual Review of Psychology, 59:617?845.
Herbert Bay, Andreas Ess, Tinne Tuytelaars, and
Luc Van Gool. 2008. SURF: Speeded Up Robust
Features. In Computer Vision and Image Under-
standing (CVIU), volume 110, pages 346?359.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O?Reilly Media Inc.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image classification using random forests and
ferns. In Proceedings of ICCV.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
Navneet Dalal and Bill Triggs. 2005. Histograms
of oriented gradients for human detection. In Pro-
ceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR?05) - Volume 1 - Volume 01, CVPR ?05,
pages 886?893.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248?255. IEEE.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. DeCAF: A Deep Convolutional Activation
Feature for Generic Visual Recognition. In Inter-
national Conference on Machine Learning (ICML
2014).
Xavier Driancourt and L?eon Bottou. 1990. TDNN-
extracted features. In Proceedings of Neuro Nimes
90, Nimes, France. EC2.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99. Asso-
ciation for Computational Linguistics.
Fangxiang Feng, Ruifan Li, and Xiaojie Wang. 2013.
Constructing hierarchical image-tags bimodal repre-
sentations for word tags alternative choice. CoRR.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406?
414. ACM.
Andrea Frome, Greg Corrado, Jonathon Shlens, Samy
Bengio, Jeffrey Dean, Marc
?
Aurelio Ranzato, and
Tomas Mikolov. 2013. DeViSE: A Deep Visual-
Semantic Embedding Model. In NIPS.
R. Girshick, J. Donahue, T. Darrell, and J. Malik.
2013. Rich feature hierarchies for accurate ob-
ject detection and semantic segmentation. arXiv
preprint:1311.2524, November.
Douwe Kiela and Stephen Clark. 2014. A Systematic
Study of Semantic Vector Space Model Parameters.
In Proceedings of EACL 2014, Workshop on Contin-
uous Vector Space Models and their Compositional-
ity (CVSC).
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving Multi-Modal Representa-
tions Using Image Dispersion: Why Less is Some-
times More. In Proceedings of ACL 2014.
44
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In NIPS, pages 1106?
1114.
Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of ACL 2014.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the British National
Corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Ben Leong and Rada Mihalcea. 2011. Going Beyond
Text: A Hybrid Image-Text Approach for Measuring
Word Relatedness. In Proceedings of Joint Interna-
tional Conference on Natural Language Processing
(IJCNLP), Chiang Mai, Thailand.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of ACL
2014.
M. M. Louwerse. 2011. Symbol interdependency in
symbolic and embodied cognition. TopiCS in Cog-
nitive Science, 3:273?302.
David G. Lowe. 1999. Object recognition from local
scale-invariant features. In Proceedings of the Inter-
national Conference on Computer Vision-Volume 2 -
Volume 2, ICCV ?99.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
M. Oquab, L. Bottou, I. Laptev, and J. Sivic. 2014.
Learning and transferring mid-level image represen-
tations using convolutional neural networks. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition.
A.S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. 2014. CNN features off-the-shelf:
an astounding baseline for recognition. arXiv
preprint:1403.6382.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
D Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th international conference on
World wide web, pages 1177?1178. ACM.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433. As-
sociation for Computational Linguistics.
Carina Silberer and Mirella Lapata. 2014. Learning
Grounded Meaning Representations with Autoen-
coders. In Proceedings of ACL 2014, Baltimore,
MD.
J. Sivic and A. Zisserman. 2003. Video Google: a text
retrieval approach to object matching in videos. In
Proceedings of the Ninth IEEE International Con-
ference on Computer Vision, volume 2, pages 1470?
1477, Oct.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded Compositional Semantics for Finding and
Describing Images with Sentences. Transactions
of the Association for Computational Linguistics
(TACL 2014).
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In F. Pereira, C.J.C. Burges, L. Bottou, and
K.Q. Weinberger, editors, Advances in Neural Infor-
mation Processing Systems 25, pages 2222?2230.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319?326. ACM.
Pengcheng Wu, Steven C.H. Hoi, Hao Xia, Peilin Zhao,
Dayong Wang, and Chunyan Miao. 2013. Online
multimodal deep similarity learning with application
to image retrieval. In Proceedings of the 21st ACM
International Conference on Multimedia, MM ?13,
pages 153?162.
Matthew D. Zeiler and Rob Fergus. 2013. Visualizing
and understanding convolutional networks. CoRR.
45
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835?841,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving Multi-Modal Representations Using Image Dispersion:
Why Less is Sometimes More
Douwe Kiela*, Felix Hill*, Anna Korhonen and Stephen Clark
University of Cambridge
Computer Laboratory
{douwe.kiela|felix.hill|anna.korhonen|stephen.clark}@cl.cam.ac.uk
Abstract
Models that learn semantic representations
from both linguistic and perceptual in-
put outperform text-only models in many
contexts and better reflect human concept
acquisition. However, experiments sug-
gest that while the inclusion of perceptual
input improves representations of certain
concepts, it degrades the representations
of others. We propose an unsupervised
method to determine whether to include
perceptual input for a concept, and show
that it significantly improves the ability of
multi-modal models to learn and represent
word meanings. The method relies solely
on image data, and can be applied to a va-
riety of other NLP tasks.
1 Introduction
Multi-modal models that learn semantic concept
representations from both linguistic and percep-
tual input were originally motivated by parallels
with human concept acquisition, and evidence that
many concepts are grounded in the perceptual sys-
tem (Barsalou et al, 2003). Such models extract
information about the perceptible characteristics
of words from data collected in property norming
experiments (Roller and Schulte im Walde, 2013;
Silberer and Lapata, 2012) or directly from ?raw?
data sources such as images (Feng and Lapata,
2010; Bruni et al, 2012). This input is combined
with information from linguistic corpora to pro-
duce enhanced representations of concept mean-
ing. Multi-modal models outperform language-
only models on a range of tasks, including mod-
elling conceptual association and predicting com-
positionality (Bruni et al, 2012; Silberer and Lap-
ata, 2012; Roller and Schulte im Walde, 2013).
Despite these results, the advantage of multi-
modal over linguistic-only models has only been
demonstrated on concrete concepts, such as
chocolate or cheeseburger, as opposed to abstract
concepts such as such as guilt or obesity. Indeed,
experiments indicate that while the addition of
perceptual input is generally beneficial for repre-
sentations of concrete concepts (Hill et al, 2013a;
Bruni et al, 2014), it can in fact be detrimental
to representations of abstract concepts (Hill et al,
2013a). Further, while the theoretical importance
of the perceptual modalities to concrete represen-
tations is well known, evidence suggests this is not
the case for more abstract concepts (Paivio, 1990;
Hill et al, 2013b). Indeed, perhaps the most influ-
ential characterization of the abstract/concrete dis-
tinction, the Dual Coding Theory (Paivio, 1990),
posits that concrete representations are encoded
in both the linguistic and perceptual modalities
whereas abstract concepts are encoded only in the
linguistic modality.
Existing multi-modal architectures generally
extract and process all the information from their
specified sources of perceptual input. Since per-
ceptual data sources typically contain information
about both abstract and concrete concepts, such in-
formation is included for both concept types. The
potential effect of this design decision on perfor-
mance is significant because the vast majority of
meaning-bearing words in everyday language cor-
respond to abstract concepts. For instance, 72% of
word tokens in the British National Corpus (Leech
et al, 1994) were rated by contributors to the Uni-
versity of South Florida dataset (USF) (Nelson et
al., 2004) as more abstract than the noun war, a
concept that many would consider quite abstract.
In light of these considerations, we propose
a novel algorithm for approximating conceptual
concreteness. Multi-modal models in which per-
ceptual input is filtered according to our algorithm
learn higher-quality semantic representations than
previous approaches, resulting in a significant per-
formance improvement of up to 17% in captur-
835
ing the semantic similarity of concepts. Further,
our algorithm constitutes the first means of quan-
tifying conceptual concreteness that does not rely
on labor-intensive experimental studies or annota-
tors. Finally, we demonstrate the application of
this unsupervised concreteness metric to the se-
mantic classification of adjective-noun pairs, an
existing NLP task to which concreteness data has
proved valuable previously.
2 Experimental Approach
Our experiments focus on multi-modal models
that extract their perceptual input automatically
from images. Image-based models more natu-
rally mirror the process of human concept acquisi-
tion than those whose input derives from exper-
imental datasets or expert annotation. They are
also more scalable since high-quality tagged im-
ages are freely available in several web-scale im-
age datasets.
We use Google Images as our image source,
and extract the first n image results for each con-
cept word. It has been shown that images from
Google yield higher-quality representations than
comparable sources such as Flickr (Bergsma and
Goebel, 2011). Other potential sources, such as
ImageNet (Deng et al, 2009) or the ESP Game
Dataset (Von Ahn and Dabbish, 2004), either do
not contain images for abstract concepts or do not
contain sufficient images for the concepts in our
evaluation sets.
2.1 Image Dispersion-Based Filtering
Following the motivation outlined in Section 1, we
aim to distinguish visual input corresponding to
concrete concepts from visual input correspond-
ing to abstract concepts. Our algorithm is moti-
vated by the intuition that the diversity of images
returned for a particular concept depends on its
concreteness (see Figure 1). Specifically, we an-
ticipate greater congruence or similarity among a
set of images for, say, elephant than among im-
ages for happiness. By exploiting this connection,
the method approximates the concreteness of con-
cepts, and provides a basis to filter the correspond-
ing perceptual information.
Formally, we propose a measure, image disper-
sion d of a concept word w, defined as the aver-
age pairwise cosine distance between all the image
representations { ~w
1
. . . ~w
n
} in the set of images
for that concept:
Figure 1: Example images for a concrete (elephant
? little diversity, low dispersion) and an abstract
concept (happiness ? greater diversity, high dis-
persion).
Figure 2: Computation of PHOW descriptors us-
ing dense SIFT for levels l = 0 to l = 2 and the
corresponding histogram representations (Bosch
et al, 2007).
d(w) =
1
2n(n? 1)
?
i<j?n
1?
~w
i
? ~w
j
| ~w
i
|| ~w
j
|
(1)
We use an average pairwise distance-based met-
ric because this emphasizes the total variation
more than e.g. the mean distance from the cen-
troid. In all experiments we set n = 50.
Generating Visual Representations Visual
vector representations for each image were ob-
tained using the well-known bag of visual words
(BoVW) approach (Sivic and Zisserman, 2003).
BoVW obtains a vector representation for an
836
image by mapping each of its local descriptors
to a cluster histogram using a standard clustering
algorithm such as k-means.
Previous NLP-related work uses SIFT (Feng
and Lapata, 2010; Bruni et al, 2012) or SURF
(Roller and Schulte im Walde, 2013) descriptors
for identifying points of interest in an image,
quantified by 128-dimensional local descriptors.
We apply Pyramid Histogram Of visual Words
(PHOW) descriptors, which are particularly well-
suited for object categorization, a key component
of image similarity and thus dispersion (Bosch et
al., 2007). PHOW is roughly equivalent to run-
ning SIFT on a dense grid of locations at a fixed
scale and orientation and at multiple scales (see
Fig 2), but is both more efficient and more accu-
rate than regular (dense) SIFT approaches (Bosch
et al, 2007). We resize the images in our dataset
to 100x100 pixels and compute PHOW descriptors
using VLFeat (Vedaldi and Fulkerson, 2008).
The descriptors for the images were subse-
quently clustered using mini-batch k-means (Scul-
ley, 2010) with k = 50 to obtain histograms of
visual words, yielding 50-dimensional visual vec-
tors for each of the images.
Generating Linguistic Representations We
extract continuous vector representations (also of
50 dimensions) for concepts using the continu-
ous log-linear skipgram model of Mikolov et al
(2013a), trained on the 100M word British Na-
tional Corpus (Leech et al, 1994). This model
learns high quality lexical semantic representa-
tions based on the distributional properties of
words in text, and has been shown to outperform
simple distributional models on applications such
as semantic composition and analogical mapping
(Mikolov et al, 2013b).
2.2 Evaluation Gold-standards
We evaluate models by measuring the Spearman
correlation of model output with two well-known
gold-standards reflecting semantic proximity ? a
standard measure for evaluating the quality of rep-
resentations (see e.g. Agirre et al (2009)).
To test the ability of our model to capture
concept similarity, we measure correlations with
WordSim353 (Finkelstein et al, 2001), a selec-
tion of 353 concept pairs together with a similar-
ity rating provided by human annotators. Word-
Sim has been used as a benchmark for distribu-
tional semantic models in numerous studies (see
e.g. (Huang et al, 2012; Bruni et al, 2012)).
As a complementary gold-standard, we use the
University of South Florida Norms (USF) (Nelson
et al, 2004). This dataset contains scores for free
association, an experimental measure of cognitive
association, between over 40,000 concept pairs.
The USF norms have been used in many previous
studies to evaluate semantic representations (An-
drews et al, 2009; Feng and Lapata, 2010; Sil-
berer and Lapata, 2012; Roller and Schulte im
Walde, 2013). The USF evaluation set is partic-
ularly appropriate in the present context because
concepts in the dataset are also rated for concep-
tual concreteness by at least 10 human annotators.
We create a representative evaluation set of USF
pairs as follows. We randomly sample 100 con-
cepts from the upper quartile and 100 concepts
from the lower quartile of a list of all USF con-
cepts ranked by concreteness. We denote these
sets C, for concrete, and A for abstract respec-
tively. We then extract all pairs (w
1
, w
2
) in the
USF dataset such that bothw
1
andw
2
are inA?C.
This yields an evaluation set of 903 pairs, of which
304 are such that w
1
, w
2
? C and 317 are such
that w
1
, w
2
? A.
The images used in our experiments and
the evaluation gold-standards can be down-
loaded from http://www.cl.cam.ac.uk/
?
dk427/dispersion.html.
3 Improving Multi-Modal
Representations
We apply image dispersion-based filtering as fol-
lows: if both concepts in an evaluation pair have
an image dispersion below a given threshold, both
the linguistic and the visual representations are in-
cluded. If not, in accordance with the Dual Cod-
ing Theory of human concept processing (Paivio,
1990), only the linguistic representation is used.
For both datasets, we set the threshold as the
median image dispersion, although performance
could in principle be improved by adjusting this
parameter. We compare dispersion filtered rep-
resentations with linguistic, perceptual and stan-
dard multi-modal representations (concatenated
linguistic and perceptual representations). Sim-
ilarity between concept pairs is calculated using
cosine similarity.
As Figure 3 shows, dispersion-filtered multi-
modal representations significantly outperform
837
0.145
0.532
0.477
0.542
0.189
0.229
0.203
0.247
0.0
0.1
0.2
0.3
0.4
0.5
Similarity ? WordSim 353 Free association ? USF (903)Evaluation Set
Cor
rela
tion
Model Representations
Linguistic onlyImage onlyStandard multi?modalDispersion filtered
Figure 3: Performance of conventional multi-
modal (visual input included for all concepts) vs.
image dispersion-based filtering models (visual in-
put only for concepts classified as concrete) on the
two evaluation gold-standards.
standard multi-modal representations on both
evaluation datasets. We observe a 17% increase in
Spearman correlation on WordSim353 and a 22%
increase on the USF norms. Based on the corre-
lation comparison method of Steiger (1980), both
represent significant improvements (WordSim353,
t = 2.42, p < 0.05; USF, t = 1.86, p < 0.1). In
both cases, models with the dispersion-based filter
also outperform the purely linguistic model, which
is not the case for other multi-modal approaches
that evaluate on WordSim353 (e.g. Bruni et al
(2012)).
4 Concreteness and Image Dispersion
The filtering approach described thus far improves
multi-modal representations because image dis-
persion provides a means to distinguish concrete
concepts from more abstract concepts. Since re-
search has demonstrated the applicability of con-
creteness to a range of other NLP tasks (Turney et
al., 2011; Kwong, 2008), it is important to exam-
ine the connection between image dispersion and
concreteness in more detail.
4.1 Quantifying Concreteness
To evaluate the effectiveness of image dispersion
as a proxy for concreteness we evaluated our al-
gorithm on a binary classification task based on
the set of 100 concrete and 100 abstract concepts
A?C introduced in Section 2. By classifying con-
0.184
0.257
0.29
0.054
0.189
0.167
0.0
0.1
0.2
0.3
0.4
'concrete' pairs (304) 'abstract' pairs (317)Concept Type
Cor
rela
tion
Representation Modality
LinguisticVisualLinguistic+Visual
Figure 4: Visual input is valuable for representing
concepts that are classified as concrete by the im-
age dispersion algorithm, but not so for concepts
classified as abstract. All correlations are with the
USF gold-standard.
cepts with image dispersion below the median as
concrete and concepts above this threshold as ab-
stract we achieved an abstract-concrete prediction
accuracy of 81%.
While well-understood intuitively, concreteness is
not a formally defined notion. Quantities such as
the USF concreteness score depend on the sub-
jective judgement of raters and the particular an-
notation guidelines. According to the Dual Cod-
ing Theory, however, concrete concepts are pre-
cisely those with a salient perceptual representa-
tion. As illustrated in Figure 4, our binary clas-
sification conforms to this characterization. The
importance of the visual modality is significantly
greater when evaluating on pairs for which both
concepts are classified as concrete than on pairs of
two abstract concepts.
Image dispersion is also an effective predic-
tor of concreteness on samples for which the ab-
stract/concrete distinction is less clear. On a differ-
ent set of 200 concepts extracted by random sam-
pling from the USF dataset stratified by concrete-
ness rating (including concepts across the con-
creteness spectrum), we observed a high correla-
tion between abstractness and dispersion (Spear-
man ? = 0.61, p < 0.001). On this more diverse
sample, which reflects the range of concepts typi-
cally found in linguistic corpora, image dispersion
is a particularly useful diagnostic for identifying
838
Concept Image Dispersion Conc. (USF)
shirt .488 6.05
bed .495 5.91
knife .560 6.08
dress .578 6.59
car .580 6.35
ego 1.000 1.93
nonsense .999 1.90
memory .999 1.78
potential .997 1.90
know .996 2.70
Table 1: Concepts with highest and lowest image
dispersion scores in our evaluation set, and con-
creteness ratings from the USF dataset.
the very abstract or very concrete concepts. As
Table 1 illustrates, the concepts with the lowest
dispersion in this sample are, without exception,
highly concrete, and the concepts of highest dis-
persion are clearly very abstract.
It should be noted that all previous approaches
to the automatic measurement of concreteness rely
on annotator ratings, dictionaries or manually-
constructed resources. Kwong (2008) proposes
a method based on the presence of hard-coded
phrasal features in dictionary entries correspond-
ing to each concept. By contrast, S?anchez et al
(2011) present an approach based on the position
of word senses corresponding to each concept in
the WordNet ontology (Fellbaum, 1999). Turney
et al (2011) propose a method that extends a large
set of concreteness ratings similar to those in the
USF dataset. The Turney et al algorithm quanti-
fies the concreteness of concepts that lack such a
rating based on their proximity to rated concepts
in a semantic vector space. In contrast to each of
these approaches, the image dispersion approach
requires no hand-coded resources. It is therefore
more scalable, and instantly applicable to a wide
range of languages.
4.2 Classifying Adjective-Noun Pairs
Finally, we explored whether image dispersion
can be applied to specific NLP tasks as an effec-
tive proxy for concreteness. Turney et al (2011)
showed that concreteness is applicable to the clas-
sification of adjective-noun modification as either
literal or non-literal. By applying a logistic regres-
sion with noun concreteness as the predictor vari-
able, Turney et al achieved a classification accu-
racy of 79% on this task. This model relies on sig-
nificant supervision in the form of over 4,000 hu-
man lexical concreteness ratings.
1
Applying im-
age dispersion in place of concreteness in an iden-
tical classifier on the same dataset, our entirely un-
supervised approach achieves an accuracy of 63%.
This is a notable improvement on the largest-class
baseline of 55%.
5 Conclusions
We presented a novel method, image dispersion-
based filtering, that improves multi-modal repre-
sentations by approximating conceptual concrete-
ness from images and filtering model input. The
results clearly show that including more percep-
tual input in multi-modal models is not always bet-
ter. Motivated by this fact, our approach provides
an intuitive and straightforward metric to deter-
mine whether or not to include such information.
In addition to improving multi-modal represen-
tations, we have shown the applicability of the im-
age dispersion metric to several other tasks. To
our knowledge, our algorithm constitutes the first
unsupervised method for quantifying conceptual
concreteness as applied to NLP, although it does,
of course, rely on the Google Images retrieval al-
gorithm. Moreover, we presented a method to
classify adjective-noun pairs according to modi-
fication type that exploits the link between image
dispersion and concreteness. It is striking that this
apparently linguistic problem can be addressed
solely using the raw data encoded in images.
In future work, we will investigate the precise
quantity of perceptual information to be included
for best performance, as well as the optimal filter-
ing threshold. In addition, we will explore whether
the application of image data, and the interaction
between images and language, can yield improve-
ments on other tasks in semantic processing and
representation.
Acknowledgments
DK is supported by EPSRC grant EP/I037512/1.
FH is supported by St John?s College, Cambridge.
AK is supported by The Royal Society. SC is sup-
ported by ERC Starting Grant DisCoTex (306920)
and EPSRC grant EP/I037512/1. We thank the
anonymous reviewers for their helpful comments.
1
The MRC Psycholinguistics concreteness ratings (Colt-
heart, 1981) used by Turney et al (2011) are a subset of those
included in the USF dataset.
839
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 19?27, Boulder, Colorado.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological review, 116(3):463.
Lawrence W Barsalou, W Kyle Simmons, Aron K Bar-
bey, and Christine D Wilson. 2003. Grounding
conceptual knowledge in modality-specific systems.
Trends in cognitive sciences, 7(2):84?91.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
RANLP, pages 399?405.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image classification using random forests and
ferns. In Proceedings of ICCV.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
Max Coltheart. 1981. The MRC psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497?505.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248?255. IEEE.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99. Asso-
ciation for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406?
414. ACM.
Felix Hill, Douwe Kiela, and Anna Korhonen. 2013a.
Concreteness and corpora: A theoretical and practi-
cal analysis. CMCL 2013.
Felix Hill, Anna Korhonen, and Christian Bentz.
2013b. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive science, 38(1).
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Oi Yee Kwong. 2008. A preliminary study on the im-
pact of lexical concreteness on word sense disam-
biguation. In PACLIC, pages 235?244.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, & Com-
puters, 36(3):402?407.
Allan Paivio. 1990. Mental representations: A dual
coding approach. Oxford University Press.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
David S?anchez, Montserrat Batet, and David Isern.
2011. Ontology-based information content compu-
tation. Knowledge-Based Systems, 24(2):297?303.
D Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th international conference on
World wide web, pages 1177?1178. ACM.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
840
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433. As-
sociation for Computational Linguistics.
J. Sivic and A. Zisserman. 2003. Video Google: a text
retrieval approach to object matching in videos. In
Proceedings of the Ninth IEEE International Con-
ference on Computer Vision, volume 2, pages 1470?
1477, Oct.
James H Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empiri-
cal Methods in Natural Language Processing, pages
680?690.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319?326. ACM.
841
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 85?89, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UCAM-CORE: Incorporating structured distributional similarity into STS
Tamara Polajnar Laura Rimell Douwe Kiela
Computer Laboratory
University of Cambridge
Cambridge CB3 0FD, UK
{tamara.polajnar,laura.rimell,douwe.kiela}@cl.cam.ac.uk
Abstract
This paper describes methods that were sub-
mitted as part of the *SEM shared task on
Semantic Textual Similarity. Multiple kernels
provide different views of syntactic structure,
from both tree and dependency parses. The
kernels are then combined with simple lex-
ical features using Gaussian process regres-
sion, which is trained on different subsets of
training data for each run. We found that the
simplest combination has the highest consis-
tency across the different data sets, while in-
troduction of more training data and models
requires training and test data with matching
qualities.
1 Introduction
The Semantic Textual Similarity (STS) shared task
consists of several data sets of paired passages of
text. The aim is to predict the similarity that hu-
man annotators have assigned to these aligned pairs.
Text length and grammatical quality vary between
the data sets, so our submissions to the task aimed to
investigate whether models that incorporate syntac-
tic structure in similarity calculation can be consis-
tently applied to diverse and noisy data.
We model the problem as a combination of ker-
nels (Shawe-Taylor and Cristianini, 2004), each of
which calculates similarity based on a different view
of the text. State-of-the-art results on text classifi-
cation have been achieved with kernel-based classi-
fication algorithms, such as the support vector ma-
chine (SVM) (Joachims, 1998), and the methods
here can be adapted for use in multiple kernel classi-
fication, as in Polajnar et al (2011). The kernels are
combined using Gaussian process regression (GPR)
(Rasmussen and Williams, 2006). It is important
to note that the combination strategy described here
is only a different way of viewing the regression-
combined mixture of similarity measures approach
that is already popular in STS systems, including
several that participated in previous SemEval tasks
(Croce et al, 2012; Ba?r et al, 2012). Likewise, oth-
ers, such as Croce et al (2012), have used tree and
dependency parse information as part of their sys-
tems; however, we use a tree kernel approach based
on a novel encoding method introduced by Zanzotto
et al (2011) and from there derive two dependency-
based methods.
In the rest of this paper we will describe our sys-
tem, which consists of distributional similarity (Sec-
tion 2.1), several kernel measures (Section 2.2), and
a combination method (Section 2.3). This will be
followed by the description of our three submissions
(Section 3), and a discussion of the results (Sec-
tion 4).
2 Methods
At the core of all the kernel methods is either sur-
face, distributional, or syntactic similarity between
sentence constituents. The methods themselves en-
code sentences into vectors or sets of vectors, while
the similarity between any two vectors is calculated
using cosine.
2.1 Distributional Similarity
Target words are the non-stopwords that occur
within our training and test data. The two distri-
butional methods we use here both represent target
85
words as vectors that encode word occurrence within
a set of contexts. The first method is a variation on
BEAGLE (Jones and Mewhort, 2007), which con-
siders contexts to be words that surround targets.
The second method is based on ESA (Gabrilovich
and Markovitch, 2007), which considers contexts to
be Wikipedia documents that contain target words.
To gather the distributional data with both of
these approaches we used 316,305 documents from
the September 2012 snapshot of Wikipedia. The
training corpus for BEAGLE is generated by pool-
ing the top 20 documents retrieved by querying the
Wikipedia snapshot index for each target word in the
training and test data sets.
2.1.1 BEAGLE
Random indexing (Kaski, 1998) is a technique for
dimensionality reduction where pseudo-orthogonal
bases are generated by randomly sampling a distri-
bution. BEAGLE is a model where random indexing
is used to represent word co-occurrence vectors in a
distributional model.
Each context word is represented as a D-
dimensional vector of normally distributed random
values drawn from the Gaussian distribution
N (0, ?2), where ? =
1
?
D
and D = 4096 (1)
A target word is represented as the sum of the
vectors of all the context words that occur within a
certain context window around the target word. In
BEAGLE this window is considered to be the sen-
tence in which the target word occurs; however, to
avoid segmenting the entire corpus, we assume the
window to include 5 words to either side of the tar-
get. This method has the advantage of keeping the
dimensionality of the context space constant even
if more context words are added, but we limit the
context words to the top 10,000 most frequent non-
stopwords in the corpus.
2.1.2 ESA
ESA represents a target word as a weighted
ranked list of the top N documents that contain the
word, retrieved from a high quality collection. We
used the BM25F (Robertson et al, 2004) weighting
function and the topN = 700 documents. These pa-
rameters were chosen by testing on the WordSim353
dataset.1 The list of retrieved documents can be rep-
resented as a very sparse vector whose dimensions
match the number of documents in the collection,
or in a more computationally efficient manner as
a hash map linking document identifiers to the re-
trieval weights. Similarity between lists was calcu-
lated using the cosine measure augmented to work
on the hash map data type.
2.2 Kernel Measures
In our experiments we use six basic kernel types,
which are described below. Effectively we have
eight kernels, because we also use the tree and de-
pendency kernels with and without distributional in-
formation. Each kernel is a function which is passed
a pair of short texts, which it then encodes into a spe-
cific format and compares using a defined similarity
function. LK uses the regular cosine similarity func-
tion, but LEK, TK, DK, MDK, DGK use the follow-
ing cosine similarity redefined for sets of vectors. If
the texts are represented as sets of vectors X and Y ,
the set similarity kernel function is:
?set(X,Y ) =
?
i
?
j
cos(~xi, ~yj) (2)
and normalisation is accomplished in the standard
way for kernels by:
?set?n(X,Y ) =
?set(X,Y )
?
(?set(X,X)?set(Y, Y ))
(3)
LK - The lexical kernel calculates the overlap be-
tween the tokens that occur in each of the paired
texts, where the tokens consist of Porter stemmed
(Porter, 1980) non-stopwords. Each text is repre-
sented as a frequency vector of tokens that occur
within it and the similarity between the pair is cal-
culated using cosine.
LEK - The lexical ESA kernel represents each
example in the pair as the set of words that do not
occur in the intersection of the two texts. The simi-
larity is calculated as in Equation (3) with X and Y
being the ESA vectors of each word from the first
and second text representations, respectively.
TK - The tree kernel representation is based on
the definition by Zanzotto et al (2011). Briefly,
1http://www.cs.technion.ac.il/?gabr/resources/
data/wordsim353/
86
each piece of text is parsed2; the non-terminal
nodes of the parse tree, stopwords, and out-of-
dictionary terms are all assigned a new random vec-
tor (Equation 1); while the leaves that occurred
in the BEAGLE training corpus are assigned their
learned distributional vectors (Section 2.1.1).
Each subtree of a tree is encoded recursively as
a vector, where the distributional vectors represent-
ing each node are combined using the circular con-
volution operator (Plate, 1994; Jones and Mewhort,
2007). The whole tree is represented as a set of vec-
tors, one for each subtree.
DK - The dependency kernel representation en-
codes each dependency pair as a separate vector, dis-
counting the labels. The non-stopword terminals are
represented as their distributional vectors, while the
stopwords and out-of-dictionary terms are given a
unique random vector. The vector for the depen-
dency pair is obtained via a circular convolution of
the individual word vectors.
MDK - The multiple dependency kernel is con-
structed like the dependency kernel, but similarity is
calculated separately between all the the pairs that
share the same dependency label. The combined
similarity for all dependency labels in the parse is
then calculated using least squares linear regression.
While at the later stage we use GPR to combine all
of the different kernels, for MDK we found that lin-
ear regression provided better performance.
DGK - The depgram kernel represents each de-
pendency pair as an ESA vector obtained by search-
ing the ESA collection for the two words in the
dependency pair joined by the AND operator. The
DGK representation only contains the dependencies
that occur in one similarity text or the other, but not
in both.
2.3 Regression
Each of the kernel measures above is used to calcu-
late a similarity score between a pair of texts. The
different similarity scores are then combined using
2Because many of the datasets contained incomplete or un-
grammatical sentences, we had to approximate some parses.
The parsing was done using the Stanford parser (Klein and
Manning, 2003), which failed on some overly long sentences,
which we therefore segmented at conjunctions or commas.
Since our methods only compared subtrees of parses, we simply
took the union of all the partial parses for a given sentence.
Gaussian process regression (GPR) (Rasmussen and
Williams, 2006). GPR is a probabilistic regression
method where the weights are modelled as Gaussian
random variables. GPR is defined by a covariance
function, which is akin to the kernel function in the
support vector machine. We used the squared expo-
nential isotropic covariance function (also known as
the radial basis function):
cov(xi, xj) = p
2
1e
(xi?xj)
T ?(p2?I)
?1?(xi?xj)
2 + p23?ij
with parameters p1 = 1, p2 = 1, and p3 = 0.01. We
found that training for parameters increased overfit-
ting and produced worse results in validation exper-
iments.
3 Submitted Runs
We submitted three runs. This is not sufficient for
a full evaluation of the new methods we proposed
here, but it gives us an inkling of general trends. To
choose the composition of the submissions, we used
STS 2012 training data for training, and STS 2012
test data for validation (Agirre et al, 2012). The
final submitted runs also used some of the STS 2012
test data for training.
Basic - With this run we were examining if a sim-
ple introduction of syntactic structure can improve
over the baseline performance. We trained a GPR
combination of the linear and tree kernels (LK-TK)
on the MSRpar training data. In validation experi-
ments we found that this data set in general gave the
most consistent performance for regression training.
Custom - Here we tried to approximate the best
training setup for each type of data. We only had
training data for OnWN and for this dataset we were
able to improve over the LK-TK setup; however, the
settings for the rest of the data sets were guesses
based on observations from the validation experi-
ments and overall performed poorly. OnWN was
trained on MSRpar train with LK and DK. The head-
lines model was trained on MSRpar train and Eu-
roparl test, with LK-LEK-TK-DK-TKND-DKND-
MDK (trained on Europarl).3 FNWN was trained on
MSRpar train and OnWN test with LK-LEK-DGK-
TK-DK-TKND-DKND. Finally, the SMT model
3TKND and DKND are the versions of the tree and depen-
dency kernels where no distributional vectors were used.
87
010
20
30
40
50
60
0 1 2 3 4 5
Number 
of STS p
airs
Score
Gold Standard
0
5
10
15
20
25
0 1 2 3 4 5
Number 
of STS p
airs
Score
Basic
0
5
10
15
20
25
30
35
0 1 2 3 4 5
Number 
of STS p
airs
Score
Custom
0
5
10
15
20
2 2.5 3 3.5 4 4.5 5
Number 
of STS p
airs
Score
All
Figure 1: Score distributions of different runs on the OnWN dataset
was trained on MSRpar train and Europarl test with
LK-LEK-TK-DK-TKND-DKND-MDK (trained on
MSRpar).
All - As in the LK-TK experiment, we used
the same model on all of the data sets. It was
trained on all of the training data except MSRvid,
using all eight kernel types defined above. In sum-
mary we used the LK-LEK-TK-TKND-DK-DKND-
MDK-DGK kernel combination. MDK was trained
on the 2012 training portion of MSRpar.
4 Discussion
From the shared task results in Table 1, we can see
that Basic is our highest ranked run. It has also
achieved the best performance on all data sets. The
LK on its own improves slightly on the task baseline
by removing stop words and using stemming, while
the introduction of TK contributes syntactic and dis-
tributional information. With the Custom run, we
were trying to manually estimate which training data
would best reflect properties of particular test data,
and to customise the kernel combination through
validation experiments. The only data set for which
this led to an improvement is OnWN, indicating
that customised settings can be beneficial, but that
a more scientific method for matching of training
and test data properties is required. In the All run,
we were examining the effects that maximising the
amount of training data and the number of kernel
hdlns OnWN FNWN SMT mean rank
BL 0.5399 0.2828 0.2146 0.2861 0.3639 71
Basic 0.6399 0.4440 0.3995 0.3400 0.4709 51
Cstm 0.4962 0.5639 0.1724 0.3006 0.4207 60
All 0.5510 0.3099 0.2385 0.1171 0.3200 78
Table 1: Shared task results: Pearson correlation with the
gold standard
measures has on the output predictions. The results
show that swamping the regression with models and
training data leads to overly normalised output and
a decrease in performance.
While the evaluation measure, Pearson correla-
tion, does not take into account the shape of the out-
put distribution, Figure 1 shows that this informa-
tion may be a useful indicator of model quality and
behaviour. In particular, the role of the regression
component in our approach is to learn a transforma-
tion from the output distributions of the models to
the distribution of the training data gold standard.
This makes it sensitive to the choice of training data,
which ideally would have similar characteristics to
the individual kernels, as well as a similar gold stan-
dard distribution to the test data. We can see in Fig-
ure 1 that the training data and choice of kernels in-
fluence the output distribution.
Analysis of the minimum, first quartile, median,
third quartile, and maximum statistics of the distri-
butions in Figure 1 demonstrates that, while it is dif-
ficult to visually evaluate the similarities of the dif-
ferent distributions, the smallest squared error is be-
tween the gold standard and the Custom run. This
suggests that properties other than the rank order
may also be good indicators in training and testing
of STS methods.
Acknowledgments
Tamara Polajnar is supported by the ERC Starting
Grant, DisCoTex, awarded to Stephen Clark, and
Laura Rimell and Douwe Kiela by EPSRC grant
EP/I037512/1: A Unified Model of Compositional
and Distributional Semantics: Theory and Applica-
tions.
88
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montre?al, Canada,
June. Association for Computational Linguistics.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. 2012. UNITOR: Combining semantic text
similarity functions through sv regression. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation, held in conjunction with the 1st Joint
Conference on Lexical and Computational Semantics,
pages 597?602, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In Proceedings of the 10th European Con-
ference on Machine Learning, ECML ?98, pages 137?
142, London, UK, UK. Springer-Verlag.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information in
a composite holographic lexicon. Psychological Re-
view, 114:1?37.
S. Kaski. 1998. Dimensionality reduction by random
mapping: fast similarity computation for clustering.
In Proceedings of the 1998 IEEE International Joint
Conference on Neural Networks, volume 1, pages
413?418 vol.1, May.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
T. A. Plate. 1994. Distributed Representations and
Nested Compositional Structure. Ph.D. thesis, Univer-
sity of Toronto.
T Polajnar, T Damoulas, and M Girolami. 2011. Protein
interaction sentence detection using multiple semantic
kernels. J Biomed Semantics, 2(1):1?1.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137, July.
C. E. Rasmussen and C. K. I. Williams. 2006. Gaussian
Processes for Machine Learning. MIT Press.
Stephen Robertson, Hugo Zaragoza, and Michael Taylor.
2004. Simple BM25 extension to multiple weighted
fields. In Proceedings of the thirteenth ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?04, pages 42?49, New York, NY,
USA. ACM.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.
2011. Distributed structures and distributional mean-
ing. In Proceedings of the Workshop on Distributional
Semantics and Compositionality, DiSCo ?11, pages
10?15, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
89
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 75?83,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Concreteness and Corpora: A Theoretical and Practical Analysis  
 
Felix Hill 
Computer Laboratory 
University of Cambridge 
fh295@cam.ac.uk 
Douwe Kiela 
Computer Laboratory 
University of Cambridge 
dlk427@cam.ac.uk 
Anna Korhonen 
Computer Laboratory 
University of Cambridge 
alk23@cam.ac.uk 
 
  
Abstract 
An increasing body of empirical evidence 
suggests that concreteness is a fundamental 
dimension of semantic representation. By im-
plementing both a vector space model and a 
Latent Dirichlet Allocation (LDA) Model, we 
explore the extent to which concreteness is re-
flected in the distributional patterns in corpora.  
In one experiment, we show that that vector 
space models can be tailored to better model 
semantic domains of particular degrees of 
concreteness.   In a second experiment, we 
show that the quality of the representations of 
abstract words in LDA models can be im-
proved by supplementing the training data 
with information on the physical properties of 
concrete concepts.  We conclude by discussing 
the implications for computational systems 
and also for how concrete and abstract con-
cepts are represented in the mind 
1 Introduction 
A growing body of theoretical evidence empha-
sizes the importance of concreteness to semantic 
representations.  This fact has not been widely 
exploited in NLP systems, despite its clear theo-
retical relevance to tasks such as word-sense in-
duction and compositionality modeling.  In this 
paper, we take a first step towards integrating 
concreteness into NLP by testing the extent to 
which it is reflected by the superficial (distribu-
tional) patterns in corpora.  The motivation is 
both theoretical and practical: We consider the 
implications for the development of computa-
tional systems and also for how concrete and ab-
stract concepts are represented in the human 
mind.  Experimenting with two popular methods 
of extracting lexical representations from text, 
we show both that these approaches are sensitive 
to concreteness and that their performance can be 
improved by adapting their implementation to 
the concreteness of the domain of application.  In 
addition, our findings offer varying degrees of 
support to several recent proposals about concep-
tual representation.   
In the following section we review recent 
theoretical and practical work. In Section 3 we 
explore the extent to which concreteness is re-
flected by Vector-Space Models of meaning 
(VSMs), and in Section 4 we conduct a similar 
analysis for (Bayesian) Latent Dirichlet Alloca-
tion (LDA) models.   We conclude, in Section 5, 
by discussing practical and theoretical implica-
tions.     
2 Related work 
2.1 Concreteness 
Empirical evidence indicates important cognitive 
differences between abstract concepts, such as 
guilt or obesity, and concrete concepts, such as 
chocolate or cheeseburger.  It has been shown 
that concrete concepts are more easily learned 
and remembered than abstract concepts, and that 
language referring to concrete concepts is more 
easily processed (Schwanenflugel, 1991).  There 
are cases of brain damage in which either ab-
stract or concrete concepts appear to be specifi-
cally impaired (Warrington, 1975), and function-
al magnetic resonance imaging (fMRI) studies 
implicate overlapping but partly distinct neural 
systems in the processing of the two concept 
types (Binder et al, 2005).  Further, there is in-
creasing evidence that concrete concepts are 
represented via intrinsic properties whereas ab-
stract representations encode extrinsic relations 
to other concepts (Hill et al, in press). However, 
while these studies together suggest that con-
creteness is fundamental to human conceptual 
representation, much remains to be understood 
about the precise cognitive basis of the ab-
stract/concrete distinction.  Indeed, the majority 
of theoretically motivated studies of conceptual 
representation focus on concrete domains, and 
75
comparatively little has been established empiri-
cally about abstract concepts. 
Despite this support for the cognitive impor-
tance of concreteness, its application to computa-
tional semantics has been limited to date.  One 
possible reason for this is the difficulty in mea-
suring lexical concreteness using corpora alone 
(Kwong, 2008).  Turney et al (2011) overcome 
this hurdle by applying a semi-supervised me-
thod to quantify noun concreteness.  Using this 
data, they show that a disparity in the concrete-
ness between elements of a construction can faci-
litate metaphor identification. For instance, in the 
expressions kill the process or black comedy, a 
verb or adjective that generally occurs with a 
concrete argument takes an abstract argument. 
Turney et al show that a supervised classifier 
can exploit this effect to correctly identify 79% 
of adjective-noun and verb-object constructions 
as literal or metaphorical.  Although these results 
are clearly promising, to our knowledge Turney 
et al?s paper is unique in integrating corpus-
based methods and concreteness in NLP systems.   
1.2 Association / similarity 
A proposed distinction between abstract and 
concrete concepts that is particularly important 
for the present work relates to the semantic rela-
tions association and (semantic) similarity (see 
e.g. Crutch et al 2009; Resnik, 1995). The dif-
ference between these relations is exemplified by 
the concept pairs {car, petrol} and {car, van}.  
Car is said to be (semantically) similar to van, 
and associated with (but not similar to) petrol.  
Intuitively, the basis for the similarity of car and 
bike may be their common physical features 
(wheels) or the fact that they fall within a clearly 
definable category (modes of transport).  In con-
trast, the basis for the association between car 
and petrol may be that they are often found to-
gether or the clear functional relationship be-
tween them.  The two relations are neither mu-
tually exclusive nor independent; bike and car 
are related to some degree by both association 
and similarity.  
Based on fresults of behavioral experiments, 
Crutch et al (2009) make the following proposal 
concerning how association and similarity inte-
ract with concreteness: 
 
(C) The conceptual organization of abstract con-
cepts is governed by association, whereas the 
organization of concrete concepts is governed by 
similarity.   
 
Crutch et al?s hypothesis derives from experi-
ments in which participants selected the odd-one-
out from lists of five words appearing on a 
screen. The lists comprised either concrete or 
abstract words (based on ratings of six infor-
mants) connected either by similarity (e.g. dog, 
wolf, fox etc.; theft, robbery, stealing etc.) or 
association (dog, bone, collar etc.; theft, law, vic-
tim etc.), with an unrelated odd-one-out item in 
each list. Controlling for frequency and position, 
subjects were both significantly faster and more 
accurate if the related words were either abstract 
and associated or concrete and similar. These 
results support (C) on the basis that decision 
times are faster when the related items form a 
more coherent group, rendering the odd-one out 
more salient.  Hill et al (in press) tested the same 
hypothesis on a larger scale, analyzing over 
18,000 concept pairs scored by human annotators 
for concreteness as well as the strength of associ-
ation between them.  They found a moderate in-
teraction between concreteness and the correla-
tion between association strength and similarity 
(as measured using WordNet), but concluded 
that the strength of the effect was not sufficiently 
strong to either confirm or refute (C). 
Against this backdrop, the present work ex-
amines how association, similarity and concrete-
ness are reflected in LDA models and, first, 
VSMs.  In both cases we test Hypothesis (C) and 
related theoretical proposals, and discuss whether 
these findings can lead to better performing se-
mantic models.   
3 Vector Space Models 
Vector space models (VSMs) are perhaps the 
most common general method of extracting se-
mantic representations from corpora (Sahlgren, 
2006; Turney & Pantel, 2010).  Words are 
represented in VSMs as points in a (geometric) 
vector space. The dimensions of the space cor-
respond to the model features, which in the sim-
plest case are high frequency words from the 
corpus.  In such models, the position of a word 
representation along a given feature dimension 
depends on how often that word occurs within a 
specified proximity to tokens of the feature word 
in the corpus.  The exact proximity required is an 
important parameter for model implementation, 
and is referred to as the context window.  Finally, 
the degree to which two word representations are 
related can be calculated as some function of the 
distance between the corresponding points in the 
semantic space.   
76
3.1 Motivation 
VSMs are well established as a method of quan-
tifying relations between word concepts and have 
achieved impressive performance in related NLP 
tasks (Sahlgren, 2006; Turney & Pantel, 2010).  
In these studies, however, it is not always clear 
exactly which semantic relation is best reflected 
by the implemented models.  Indeed, research 
has shown that by changing certain parameter 
settings in the standard VSM architecture, mod-
els can be adapted to better reflect one relation 
type or another.  Specifically, models with 
smaller context windows are reportedly better at 
reflecting similarity, whereas models with larger 
windows better reflect association. (Agirre et al, 
2009; Peirsman et al, 2008) 
Our experiments in this section aim first to 
corroborate these findings by testing how models 
of varying context window sizes perform on em-
pirical data of both association and similarity.  
We then test if this effect differentially affects 
performance on concrete and abstract words.   
3.2 Method  
We employ a conventional VSM design, extract-
ing representations from the (unlemmatised) 
British National Corpus (Leech et al, 1994) with 
stopwords removed.   In the vector representation 
of each noun, our dimension features are the 
50,000 most frequently occurring (non-
stopword) words in the corpus.    We experiment 
with window sizes of three, five and nine (one, 
two and four words either side of the noun, 
counting stopwords).  Finally, we apply point-
wise mutual information (PMI) weighting of our 
co-occurrence frequencies, and measure similari-
ty between weighted noun vectors by the cosine 
of the angle between them in the vector space.    
To evaluate modeling of association, we use 
the University of South Florida (USF) Free-
association Norms (Nelson & McEvoy, 2012).  
The USF data consist of over 5,000 words paired 
with their free associates.  To elicit free asso-
ciates, more than 6,000 participants were pre-
sented with cue words and asked to ?write the 
first word that comes to mind that is meaningful-
ly related or strongly associated to the presented 
word?.  For a cue word c and an associate a, the 
forward association strength (association) from 
c to a is the proportion of participants who pro-
duced a when presented with c.  association is 
thus a measure of the strength of an associate 
relative to other associates of that cue.  The USF 
data is well suited to our purpose because many 
cues and associates in the data have a concrete-
ness score, taken from either the norms of Paivio, 
Yuille and Madigan (1968) or Toglia and Battig 
(1978).  In both cases contributors were asked to 
rate words based on a scale of 1 (very abstract) to 
7 (very concrete).1  We extracted the all 2,230 
nouns from the USF data for which concreteness 
scores were known, yielding a total of 15,195 
noun-noun pairs together with concreteness and 
association values.   
Although some empirical word-similarity da-
tasets are publically available, they contain few if 
any abstract words (Finkelstein et al, 2002; Ru-
benstein & Goodenough, 1965).  Therefore to 
evaluate similarity modeling, we use Wu-Palmer 
Similarity (similarity) (Wu & Palmer, 1994), a 
word similarity metric based on the position of 
the senses of two words in the WordNet taxono-
my (Felbaum, 1998).  similarity can be applied to 
both abstract and concrete nouns and achieves a 
high correlation, with human similarity judg-
ments (Wu & Palmer, 1994).2     
3.3 Results 
In line with previous studies, we observed that 
VSMs with smaller window sizes were better 
able to predict similarity.  The model with win-
dow size 3 achieves a higher correlation with 
similarity (Spearman rank rs  = -0.29) than the 
model with window size 9 (rs  = -0.25).  Howev-
er, the converse effect for association was not 
observed: Model correlation with association 
was approximately constant over all window siz-
es.  These effects are illustrated in Fig. 1.  
 
                                                 
1Although concreteness is well understood intuitively, it 
lacks a universally accepted definition.  It is often described 
in terms of reference to sensory experience (Paivio et al, 
1968), but also connected to specificity; rose is often consi-
dered more concrete than flora.  The present work does not 
address this ambiguity.     
2 similarity achieves a Pearson correlation of r  = .80 on 
the  30 concrete word pairs in the Miller & Charles (1991) 
data.   
0.123 0.125
0.12
0.286
0.29
0.241
0.0
0.1
0.2
0.3
3 5 9
Window
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
77
Figure 1:  Spearman correlations between VSM out-
put and association and similarity for different win-
dow sizes. 
 
In addressing the theoretical Hypothesis (C) we 
focused on the output of our VSM of window 
size five, although the same trends were ob-
served over all three models.  Over all 18,195 
noun-noun pairs the correlation between the 
model output and association was significant (rs  
= 0.13, p < 0.001) but notably lower than the cor-
relation with similarity (rs  = -0.29, p < 0.001).  
To investigate the effect of concreteness, we 
ranked each pair in our sample by the total con-
creteness of both nouns, and restricted our analy-
sis to the 1000 most concrete and 1000 most ab-
stract pairs.  The models captured association 
better over the abstract pairs than concrete con-
cepts, but reflected similarity better over the con-
crete concepts.  The strength of this effect is illu-
strated in Fig. 2.   
 
 
Figure 2: Spearman correlation values between VSM 
output and similarity and association over subsets of 
concrete and abstract pairs. 
 
Given that small window sizes are optimal for 
modeling similarity, and that WSMs appear to 
model similarity better over concrete concepts 
than over abstract concepts, we explored whether 
different window sizes were optimal for either 
abstract or concrete word pairs. When comparing 
the model output to association, no interaction 
between window size and concreteness was ob-
served.  However, there was a notable interaction 
when considering performance in modeling simi-
larity.  As illustrated in Fig. 3, performance on 
concrete word pairs is better for smaller window 
sizes, whereas with abstract word pairs a larger 
window size is preferable.   
 
 
Figure 3:  Spearman correlation values between VSM 
output and similarity and association for different 
window sizes over abstract and concrete word pair 
subsets 
3.4 Conclusion 
Our results corroborate the body of VSM re-
search that reports better performance from small 
window sizes in modeling similarity.  A likely 
explanation for this finding is that similarity is a 
paradigmatic relation: Two similar entities can 
be plausibly exchanged in most linguistic con-
texts.  Small context windows emphasize prox-
imity, which loosely reflects structural relation-
ships such as verb-object, ensuring that paradig-
matically related entities score highly.  Models 
with larger context windows cannot discern pa-
radigmatically and syntagmatically related enti-
ties in this way.  The performance of our models 
on the association dataset did not support the 
converse conclusion that larger window sizes 
perform better.  Overall, each of the three models 
was notably better at capturing similarity than 
association.  This suggests that the core architec-
ture of WSMs is not well suited to modeling as-
sociation.  Indeed, ?first order? models that di-
rectly measure word co-occurrences, rather than 
connecting them via features, seem to perform 
better at this task (Chaudhari et al, 2011).  This 
fact is consistent with the view that association is 
a more basic or fundamental semantic relation 
from which other more structured relations are 
derived.  
The fact that the USF association data re-
flects the instinctive first response of participants 
when presented with a cue word is important for 
interpreting the results with respect to Hypothe-
sis (C).  Our findings suggest that VSMs are bet-
ter able to model this data for abstract word pairs 
than for concrete word pairs.  This is consistent 
with the idea that language fundamentally deter-
mines which abstract concepts come to be asso-
ciated or connected in the mind.  Conversely, the 
0.125
0.108
0.215
0.297
0.0
0.1
0.2
0.3
Association Similarity
Relation Type
C
o
r
r
e
l
a
t
i
o
n
Abstract
Concrete
0.202
0.272
0.223
0.254
0.14
0.086
0.104
0.099
0.0
0.1
0.2
3 - Similarity 9 - Similarity 3 - Assoc. 9 - Assoc.
Window size - Relation type
C
o
r
r
e
l
a
t
i
o
n
Abstract
Concrete
78
fact that the model reflects associations between 
concrete words less well suggests that the impor-
tance of extra-linguistic information is lower for 
connecting concrete concepts in this instinctive 
way.  Indeed, it seems plausible that the process 
by which concrete concepts become associated 
involves visualization or some other form of per-
ceptual reconstruction. Consistent with Hypothe-
sis (C), this reconstruction, which is not possible 
for abstract concepts, would naturally reflect si-
milarity to a greater extent than linguistic context 
alone.   
Finally, when modeling similarity, the ad-
vantage of a small window increases as the 
words become more concrete.  Similarity be-
tween concrete concepts is fundamental to cogni-
tive theories involving the well studied notions 
of prototype and categorization (Rosch, 1975; 
Rogers & McClelland, 2003).   In contrast, the 
computation of abstract similarity is intuitively a 
more complex cognitive operation.  Although the 
accurate quantification of abstract similarity may 
be beyond existing corpus-based methods, our 
results suggest that a larger context window 
could in fact be marginally preferable should 
VSMs be applied to this task.   
Overall, our findings show that the design of 
VSMs can be tailored to reflect particular seman-
tic relations and that this in turn can affect their 
performance on different semantic domains, par-
ticularly with respect to concreteness.  In the 
next section, we investigate whether the same 
conclusions should apply to a different class of 
distributional model.        
4 Latent Dirichlet Allocation Models 
LDA models are trained on corpora that are di-
vided into sections (typically documents), ex-
ploiting the principle that words appearing in the 
same document are likely to have similar mean-
ings.  In an LDA model, the sections are viewed 
as having been generated by random sampling 
from unknown latent dimensions, which are 
represented as probability distributions (Dirichlet 
distributions) over words.  Each document can 
then be represented by a probability distribution 
over these dimensions, and by considering the 
meaning of the dimensions, the meaning of the 
document can be effectively characterized.  More 
importantly, because each latent dimension clus-
ters words of a similar meaning, the output of 
such models can be exploited to provide high 
quality lexical representations (Griffiths et al, 
2007).  Such a word representation encodes the 
extent to which each of the latent dimensions 
influences the meaning of that word, and takes 
the form of a probability distribution over these 
dimensions.  The degree to which two words are 
related can then be approximated by any function 
that measures the similarity or difference be-
tween distributions.        
4.1 Motivation 
In recent work, Andrews et al (2009) explore 
ways in which LSA models can be modified to 
improve the quality of their lexical representa-
tions.  They propose that concepts are acquired 
via two distinct information sources: experiential 
data ? the perceptible properties of objects, and 
distributional data ? the superficial patterns of 
language.  To test this hypothesis, Andrews et al 
construct three different LDA models, one 
trained on experiential data, one trained in the 
conventional manner on running text, and one 
trained on the same text but with the experiential 
data appended.  They evaluate the quality of the 
lexical representations in the three models by 
calculating the Kulback-Leibler divergence be-
tween the representation distributions to measure 
how closely related two words are (Kullback & 
Leibler, 1951).  When this data was compared 
with the USF association data, the combined 
model performed better than the corpus-based 
model, which in turn performed better than the 
features-only model.  Andrews et al concluded 
that both experiential and distributional data are 
necessary for the acquisition of good quality lex-
ical representations. 
     As well as suggesting a way to improve the 
performance of LDA models on NLP tasks by 
supplementing the training data, the approach 
taken by Andrews et al may be useful for better 
understanding the nature of the abstract/concrete 
distinction.  In recent work, Hill et al (in press) 
present empirical evidence that concrete con-
cepts are represented in terms of intrinsic fea-
tures or properties whereas abstract concepts are 
represented in terms of connections to other 
(concrete and abstract) concepts.  For example, 
the features [legs], [tail], [fur], [barks] are all 
central aspects of the concrete representation of 
dog, whereas the representation of the abstract 
concept love encodes connections to other con-
cepts such as heart, rose, commitment and hap-
piness etc.  If a feature-based representation is 
understood to be constructed from physical or 
perceptible properties (which themselves may be 
basic or fundamental concrete representations), 
Hill et al?s characterization of concreteness can 
be summarized as follows:  
79
 
(H) Concreteness correlates with the degree to 
which conceptual representations are feature-
based 
 
Because such differences in representation struc-
ture would in turn entail differences in the com-
putation of similarity, (H) is closely related to a 
proposal of Markman and Stilwell (2001; see 
also Gentner & Markman, 2007):  
 
(M) Computing similarity among concrete con-
cepts involves a feature-comparison operation, 
whereas similarity between abstract concepts is 
a structural, analogy-like, comparison.   
 
The findings of Andrews et al do not address 
(H) or (M) directly, for two reasons. Firstly, they 
evaluate their model on a set that includes no 
abstract concepts.  Secondly, they compare their 
model output to association data without testing 
how well it reflects similarity.  In this section we 
therefore reconstruct the Andrews models and 
evaluate how well they reflect both association 
and similarity across a larger set of abstract and 
concrete concepts.   
4.2  Method/materials 
We reconstruct two of the three models devel-
oped by Andrews et al (2009), excluding the 
features-only model because of the present focus 
on corpus-based approaches.  However, while 
the experiential data applied in the Andrews et 
al. combined model was that collected by Vig-
liocco et al (2004), we use the publicly available 
McRae feature production norms (McRae et al, 
2005).  The McRae data consist of 541 concrete 
noun concepts together with features for each 
elicited from 725 participants.  In the data collec-
tion, feature was understood in a very loose 
sense, so that participants were asked to list both 
physical and functional properties of the nouns in 
addition to encyclopedic facts.  However, for the 
present work, we filter out those features that 
were not perceptual properties using McRae et 
al.?s feature classes, leaving a total of 1,285 fea-
ture types, such as [has_claws] and 
[made_of_brass].  The importance of each fea-
ture to the representation of a given concept is 
reflected by the proportion of participants who 
named that feature in the elicitation experiment.  
For each noun concept we therefore extract a 
corresponding probability distribution over fea-
tures. 
The model design and inference are identical 
to those applied by Andrews et al  Our distribu-
tional model contains 250 latent dimensions and 
was trained using a Gibbs Sampling algorithm on 
approximately 7,500 sections of the BNC with 
stopwords removed.3  The combined model con-
tains 350 latent dimensions, and was trained on 
the same BNC data.  However, for each instance 
of one of the 541 McRae concept words, a fea-
ture is drawn at random from the probability dis-
tribution corresponding to that word and ap-
pended to the training data.  The latent dimen-
sions in the combined model therefore corres-
pond to probability distributions both over words 
and over features. This leads to an important dif-
ference between how words come to be related in 
the distributional model and in the combined 
model.  Both models infer connections between 
words by virtue of their occurrence either in the 
same document or in pairs of documents for 
which the same latent dimensions are prominent.  
In the distributional model, it is the words in a 
document that determines which latent dimen-
sions are ultimately prominent, whereas the in 
combined model it is both the words and the fea-
tures in that document.  Therefore, in the com-
bined model, two words can come to be related 
because they occur not only in documents whose 
words are related, but also in documents whose 
features are related.  For words in the McRae 
data, this has the effect of strengthening the rela-
tionship between words with common features.  
More interestingly, because it alters which latent 
dimensions are most prominent for each docu-
ment, it should also influence the relationship 
between words not in the McRae data.   
We evaluate the performance of our models in 
reflecting free association (association) and simi-
larity (similarity).  To obtain test items we rank 
the 18,195 noun-noun pairs from the USF data 
by the product of the two (BNC) word frequen-
cies and select the 5,000 highest frequency pairs.   
4.3 Results 
As expected, the correlation of the combined 
model output with association was greater than 
the correlation of the distributional model output.  
Notably, however, as illustrated in Fig. 4, we 
observed far greater differences between the 
combined and the distributional models when 
comparing to similarity.  Over all noun pairs, the 
addition of features in the combined model im-
                                                 
3 Code for model implementation was taken from Mark 
Andrews : http://www.mjandrews.net/code/index.html  
80
proved the correlation with similarity from 
Spearman rs  =  0.09  to  rs  =  0.15.   
 
Figure 4:  Spearman correlations between distribu-
tional and combined model outputs, similarity and 
association  
 
In order to address Hypothesis (C) (Section 2.2), 
we analyzed the output of the combined model 
on subsets of the 1000 most abstract and concrete 
word pairs in our data as before.  Perhaps surpri-
singly, as shown in Fig. 5, when comparing with 
similarity, the model performed better over ab-
stract pairs, whereas when comparing with asso-
ciation the model performed better over concrete 
pairs.  However, when these concrete pairs were 
restricted to those for which at least one of the 
two words was in the McRae data, and hence to 
which features had been appended in the corpus, 
the ability of the model to reflect similarity in-
creased significantly.          
 
Figure 5:  Spearman correlations between combined 
model output and similarity and association on differ-
ent word pair subsets  
 
Finally, to address hypotheses (H) and (M) we 
compared the previous analysis of the combined 
model output to the equivalent output from the 
distributional model.  Surprisingly, as shown in 
Fig. 6, the ability of the model to reflect associa-
tion over abstract pairs seemed to reduce with the 
addition of features to the training data.  Never-
theless, in all other cases the combined model 
outperformed the distributional model.  Interes-
tingly, the combined model advantage when 
comparing with similarity was roughly the same 
over both abstract and concrete pairs.  However, 
when these pairs contained at least one word 
from the McRae data, the combined model was 
indeed significantly better at modeling similarity, 
consistent with Hypotheses (M) and (H). 
 
Figure 6:  Comparison between distributional 
model and combined model output correlations with 
similarity and association over different word pair 
subsets 
4.4 Conclusion 
Our findings corroborate the main conclusion of 
Andrews et al, that the addition of experiential 
data improves the performance of the LDA mod-
el in reflecting association.  However, they also 
indicate that the advantage of feature-based LDA 
models is far more significant when the objective 
is to model similarity. 
 The findings are also consistent with, if 
not suggestive of, the theoretical hypotheses (H) 
and (M).  Clearly, the property features in the 
combined model training data enable it to better 
model both similarity and association between 
those concepts to which the features correspond.  
However, this benefit is greater when modeling 
similarity than when modeling association.  This 
suggests that the similarity operation is indeed 
based on features to a greater extent than associa-
tion.  Moreover, this effect is far greater for the 
concrete words for which the features were add-
ed than over the other words pairs we tested.  
Whilst this is not a sound test of hypothesis (H) 
(no attempt was made to add ?features? of ab-
stract concepts to the model), it is certainly con-
sistent with the idea that features or properties 
are a more important aspect of concrete represen-
tations than of abstract representations. 
0.13
0.09
0.14
0.15
0.00
0.05
0.10
0.15
Distributional Combined
Model type
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
0.01
0.16
0.2
0.08
0.14
0.36
0.0
0.1
0.2
0.3
Abstract Concrete McRae
Word pair category
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
0.03
0.093
0.15
0.018
0.11
0.025
0.01
0.16
0.2
0.08
0.14
0.36
0.0
0.1
0.2
0.3
Abstract Concrete 
 Distributional model
McRae Abstract_ Concrete_
 Combined model
McRae_
Word pair category
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
81
Perhaps the most interesting aspect of the 
combined model is how the addition of feature 
information in the training data for certain words 
influences performance on words for which fea-
tures were not added.  In this case, our findings 
suggest that the benefit when modeling similarity 
is marginally greater than when modeling associ-
ation, an observation consistent with Hypothesis 
(M).  A less expected observation is that, be-
tween words for which features were not added, 
the advantage of the combined model over the 
distributional model in modeling similarity was 
equal if not greater for abstract than for concrete 
concepts.  We hypothesize that this is because 
abstract representations naturally inherit any re-
liance on feature information from the concrete 
concepts with which they participate.  In con-
trast, highly concrete representations do not en-
code relations to other concepts and therefore 
cannot inherit relevant feature information in the 
same way.  Under this interpretation, the con-
crete information from the McRae words would 
propagate more naturally to abstract concepts 
than to other concrete concepts.  As a result, the 
highest quality representations in the combined 
model would be those of the McRae words, fol-
lowed by those of the abstract concepts to which 
they closely relate.    
5 Discussion  
This study has investigated how concreteness is 
reflected in the distributional patterns found in 
running text corpora. Our results add to the body 
of evidence that abstract and concrete concepts 
are represented differently in the mind.  The fact 
that VSMs with small windows are particularly 
adept at modeling relations between concrete 
concepts supports the view that similarity go-
verns the conceptual organization of concrete 
concepts to a greater extent than for abstract con-
cepts.  Further, the performance of our LSA 
models on different tasks and across different 
word pairs is consistent with the idea that con-
crete representations are built around features, 
whereas abstract concepts are not.  
More practically, we have demonstrated that 
vector space models can be tailored to reflect 
either similarity or association by adjusting the 
size of the context window.  This in turn indi-
cates a way in which VSMs might be optimized 
to either abstract or concrete domains.  Our expe-
riments with Latent Dirichlet Allocation corrobo-
rate a recent proposal that appending training 
data with perceptible feature or property infor-
mation for a subset of concrete nouns can signif-
icantly improve the quality of the model?s lexical 
representations.  As expected, this effect was 
particularly salient for representations of words 
for which features were appended to the training 
data.  However, the results show that this infor-
mation can propagate to words for which fea-
tures were not appended, in particular to abstract 
words.   
The fact that certain perceptible aspects of 
meaning are not exhaustively reflected in linguis-
tic data is a potentially critical obstacle for cor-
pus-based semantic models.  Our findings sug-
gest that existing machine learning techniques 
may be able to overcome this by adding the re-
quired information for words that refer to con-
crete entities and allowing this information to 
propagate to other elements of language.  In fu-
ture work we aim to investigate specifically 
whether this hypothesis holds for particular parts 
of speech.  For example, we would hypothesize 
that verbs inherit a good degree of their meaning 
from their prototypical nominal arguments.     
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J. 
Pasca, K,. & Soroa,A. 2009. A Study on Similarity 
and Relatedness Using Distributional and WordNet-
based Approaches. In Proceedings of NAACL-HLT 
2009. 
Andrews, M., Vigliocco, G. & Vinson, D. 2009. 
Integrating experiential and distributional data to 
learn semantic represenations. Psychological Review, 
116(3), 463-498. 
Barsalou, L. 1999. Perceptual symbol systems. Be-
havioral and Brain Sciences, 22, 577-609. 
Binder, J., Westbury, C., McKiernan, K., Possing, 
E., & Medler, D. 2005. Distinct brain systems for 
processing concrete and abstract concepts. Journal of 
Cognitive Neuroscience 17(6), 905-917. 
Chaudhari, D., Damani, O., & Laxman, S. 2011. 
Lexical Co-occurrence, Statistical Significance, and 
Word Association. EMNLP 2011, 1058-1068. 
Crutch, S., Connell, S., & Warrington, E. 2009. 
The different representational frameworks underpin-
ning abstract and concrete knowledge: evidence from 
odd-one-out judgments. Quarterly Journal of Experi-
mental Psychology, 62(7), 1377-1388. 
Felbaum, C. 1998. WordNet: An Electronic Lexical 
Database. Cambridge, MA: MIT Press. 
Finkelstein, L., Gabrilovich, Matias, Rivlin, Solan, 
Wolfman & Ruppin. 2002. Placing Search in Context: 
The Concept Revisited. ACM Transactions on Infor-
mation Systems, 20(1):116-131. 
Gentner, D., & Markman, A. 1997. Structure map-
ping in analogy and similarity. American Psycholo-
gist, 52. 45-56. 
82
Griffiths, T., Steyvers, M., & Tenembaum, J. 2007. 
Topics in semantic representation. Psychological Re-
view, 114 (2), 211-244. 
Hill, F., Korhonen, A., & Bentz, C. A quantitative 
empricial analysis of the abstract/concrete distinction. 
Cognitive Science. In press.  
Kullback, S., & Leibler, R.A. 1951. On Informa-
tion and Sufficiency. Annals of Mathematical Statis-
tics 22 (1): 79?86. 
Kwong, O, Y. 2008. A Preliminary study on induc-
ing lexical concreteness from dictionary definitions. 
22nd Pacific Asia Conference on Language, In-
formation and Computation, 235?244. 
Leech, G., Garside, R. & Bryant, R. 1994. Claws4: 
The tagging of the British National Corpus. COL-ING 
94, Lancaster: UK. 
Markman, A, & Stilwell, C. 2001. Role-governed 
categories. Journal of Theoretical and Experimental 
Artificial Intelligence, 13, 329-358. 
McRae, K., Cree, G. S., Seidenberg, M. S., & 
McNorgan, C. 2005. Semantic feature production 
norms for a large set of living and nonliving things. 
Behavior Research Methods, 37, 547-559 
Miller, G., & Charles, W. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive 
Processes, 6(1). 
Nelson, D., & McEvoy, C. 2012. The University of 
South Florida Word Association, Rhyme and Word 
Fragment Norms. Retrieved online from: 
http://web.usf.edu/FreeAssociation/Intro.html. 
Paivio, A., Yuille, J., & Madigan, S. 1968. Con-
creteness, imagery, and meaningfulness values for 
925 nouns. Journal of Experimental Psychology Mo-
nograph Supplement, 76(1, Pt. 2).  
Peirsman, y., Heylen, K. & Geeraerts, D. 2008. 
Size Matters. Tight and Loose Context Definitions in 
English Word Space Models. In Proceedings of the 
ESSLLI Workshop on Distributional Lexical Seman-
tics, Hamburg, Germany 
Resnik, P. 1995. Using Information Content to 
Evaluate Semantic Similarity in a Taxonomy. Pro-
ceedings of IJCAI-95. 
Rogers, T., & McLelland, J. 2003. Semantic Cog-
nition. Cambridge, Mass: MIT Press. 
Rosch, E. 1975. Cognitive representations of se-
mantic categories. Journal of Experimental Psycholo-
gy: General, 104(3), (September 1975), pp. 192?233. 
Rubenstein, H., & Goodenough, J. 1965. Contex-
tual correlates of synonymy. Communications of the 
ACM 8(10), 627-633. 
Sahlgren, M. 2006. The Word-Space Model: Using 
distributional analysis to represent syntagmatic and 
paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, De-
partment of Linguistics, Stockholm University.  
Schwanenflugel, P. 1991.  Why are abstract con-
cepts hard to understand? In P.  Schwanenflugel.  
The psychology of word meanings (pp.  223-250).  
Hillsdale, NJ: Erlbaum. 
Toglia, M., & Battig, W. 1978. Handbook of se-
mantic word norms. Hillsdale, N.J: Erlbaum. 
Turney, P, & Pantel, P. 2010. From frequency to 
meaning: Vector space models of semantics. Journal 
of Artificial Intelligence Research (JAIR), 37, 141-
188. 
Turney,P., Neuman, Y., Assaf,.D, Cohen, Y. 2011. 
Literal and Metaphorical Sense Identification through 
Concrete and Abstract Context. EMNLP 2011: 680-
690 
Vigliocco, G., Vinson, D. P., Lewis, W., & Garrett, 
M. F. 2004. Reprssenting the meanings of object and 
action words: The featural and unitary semantic space 
hypothesis. Cognitive Psychology, 48, 422?488. 
Warrington, E. (1975). The selective impairment of 
semantic memory. Quarterly Journal of Experimental 
Psychology 27(4), 635-657. 
Wu, Z., Palmer, M. 1994. Verb semantics and lexi-
cal selection. In: Proceedings of the 32nd Annual 
Meeting of the Associations for Computational Lin-
guistics. 133?138. 
 
 
83
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 21?30,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Systematic Study of Semantic Vector Space Model Parameters
Douwe Kiela
University of Cambridge
Computer Laboratory
douwe.kiela@cl.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Abstract
We present a systematic study of parame-
ters used in the construction of semantic
vector space models. Evaluation is car-
ried out on a variety of similarity tasks, in-
cluding a compositionality dataset, using
several source corpora. In addition to rec-
ommendations for optimal parameters, we
present some novel findings, including a
similarity metric that outperforms the al-
ternatives on all tasks considered.
1 Introduction
Vector space models (VSMs) represent the mean-
ings of lexical items as vectors in a ?semantic
space?. The benefit of VSMs is that they can eas-
ily be manipulated using linear algebra, allowing
a degree of similarity between vectors to be com-
puted. They rely on the distributional hypothesis
(Harris, 1954): the idea that ?words that occur in
similar contexts tend to have similar meanings?
(Turney and Pantel, 2010; Erk, 2012). The con-
struction of a suitable VSM for a particular task is
highly parameterised, and there appears to be little
consensus over which parameter settings to use.
This paper presents a systematic study of the
following parameters:
? vector size;
? window size;
? window-based or dependency-based context;
? feature granularity;
? similarity metric;
? weighting scheme;
? stopwords and high frequency cut-off.
A representative set of semantic similarity
datasets has been selected from the literature, in-
cluding a phrasal similarity dataset for evaluating
compositionality. The choice of source corpus is
likely to influence the quality of the VSM, and so
we use a selection of source corpora. Hence there
are two additional ?superparameters?:
? dataset for evaluation;
? source corpus.
Previous studies have been limited to investigat-
ing only a small number of parameters, and us-
ing a limited set of source corpora and tasks for
evaluation (Curran and Moens, 2002a; Curran and
Moens, 2002b; Curran, 2004; Grefenstette, 1994;
Pado and Lapata, 2007; Sahlgren, 2006; Turney
and Pantel, 2010; Schulte im Walde et al., 2013).
Rohde et al. (2006) considered several weighting
schemes for a large variety of tasks, while Weeds
et al. (2004) did the same for similarity metrics.
Stone et al. (2008) investigated the effectiveness
of sub-spacing corpora, where a larger corpus is
queried in order to construct a smaller sub-spaced
corpus (Zelikovitz and Kogan, 2006). Blacoe and
Lapata (2012) compare several types of vector rep-
resentations for semantic composition tasks. The
most comprehensive existing studies of VSM pa-
rameters ? encompassing window sizes, feature
granularity, stopwords and dimensionality reduc-
tion ? are by Bullinaria and Levy (2007; 2012)
and Lapesa and Evert (2013).
Section 2 introduces the various parameters of
vector space model construction. We then attempt,
in Section 3, to answer some of the fundamen-
tal questions for building VSMs through a number
of experiments that consider each of the selected
parameters. In Section 4 we examine how these
findings relate to the recent development of dis-
tributional compositional semantics (Baroni et al.,
2013; Clark, 2014), where vectors for words are
combined into vectors for phrases.
2 Data and Parameters
Two datasets have dominated the literature with
respect to VSM parameters: WordSim353 (Finkel-
stein et al., 2002) and the TOEFL synonym dataset
21
Dataset Pairings Words
RG 65 48
MC 30 39
W353 353 437
MEN 3000 751
TOEFL 80 400
M&L10 324 314
Table 1: Datasets for evaluation
(Landauer and Dumais, 1997). There is a risk
that semantic similarity studies have been overfit-
ting to their idiosyncracies, so in this study we
evaluate on a variety of datasets: in addition to
WordSim353 (W353) and TOEFL, we also use
the Rubenstein & Goodenough (RG) (1965) and
Miller & Charles (MC) (1991) data, as well as
a much larger set of similarity ratings: the MEN
dataset (Bruni et al., 2012). All these datasets con-
sist of human similarity ratings for word pairings,
except TOEFL, which consists of multiple choice
questions where the task is to select the correct
synonym for a target word. In Section 4 we ex-
amine our parameters in the context of distribu-
tional compositional semantics, using the evalua-
tion dataset from Mitchell and Lapata (2010). Ta-
ble 1 gives statistics for the number of words and
word pairings in each of the datasets.
As well as using a variety of datasets, we also
consider three different corpora from which to
build the vectors, varying in size and domain.
These include the BNC (Burnard, 2007) (10
6
word types, 10
8
tokens) and the larger ukWaC
(Baroni et al., 2009) (10
7
types, 10
9
tokens).
We also include a sub-spaced Wikipedia corpus
(Stone et al., 2008): for all words in the eval-
uation datasets, we build a subcorpus by query-
ing the top 10-ranked Wikipedia documents using
the words as search terms, resulting in a corpus
with 10
6
word types and 10
7
tokens. For examin-
ing the dependency-based contexts, we include the
Google Syntactic N-gram corpus (Goldberg and
Orwant, 2013), with 10
7
types and 10
11
tokens.
2.1 Parameters
We selected the following set of parameters for in-
vestigation, all of which are fundamental to vector
space model construction
1
.
1
Another obvious parameter would be dimensionality re-
duction, which we chose not to include because it does not
represent a fundamental aspect of VSM construction: di-
mensionality reduction relies on some original non-reduced
model, and directly depends on its quality.
Vector size Each component of a vector repre-
sents a context (or perhaps more accurately a ?con-
textual element?, such as second word to the left
of the target word).
2
The number of components
varies hugely in the literature, but a typical value
is in the low thousands. Here we consider vec-
tor sizes ranging from 50,000 to 500,000, to see
whether larger vectors lead to better performance.
Context There are two main approaches to mod-
elling context: window-based and dependency-
based. For window-based methods, contexts are
determined by word co-occurrences within a win-
dow of a given size, where the window simply
spans a number of words occurring around in-
stances of a target word. For dependency-based
methods, the contexts are determined by word
co-occurrences in a particular syntactic relation
with a target word (e.g. target word dog is the
subject of run, where run subj is the context).
We consider different window sizes and compare
window-based and dependency-based methods.
Feature granularity Context words, or ?fea-
tures?, are often stemmed or lemmatised. We in-
vestigate the effect of stemming and lemmatisa-
tion, in particular to see whether the effect varies
with corpus size. We also consider more fine-
grained features in which each context word is
paired with a POS tag or a lexical category from
CCG (Steedman, 2000).
Similarity metric A variety of metrics can be
used to calculate the similarity between two vec-
tors. We consider the similarity metrics in Table 2.
Weighting Weighting schemes increase the im-
portance of contexts that are more indicative of the
meaning of the target word: the fact that cat co-
occurs with purr is much more informative than
its co-occurrence with the. Table 3 gives defini-
tions of the weighting schemes considered.
Stopwords, high frequency cut-off Function
words and stopwords are often considered too un-
informative to be suitable context words. Ignor-
ing them not only leads to a reduction in model
size and computational effort, but also to a more
informative distributional vector. Hence we fol-
lowed standard practice and did not use stopwords
as context words (using the stoplist in NLTK (Bird
et al., 2009)). The question we investigated is
2
We will use the term ?feature? or ?context? or ?context
word? to refer to contextual elements.
22
Measure Definition
Euclidean
1
1+
??
n
i=1
(u
i
?v
i
)
2
Cityblock
1
1+
?
n
i=1
|u
i
?v
i
|
Chebyshev
1
1+max
i
|u
i
?v
i
|
Cosine
u?v
|u||v|
Correlation
(u??
u
)?(v??
v
)
|u||v|
Dice
2
?
n
i=0
min(u
i
,v
i
)
?
n
i=0
u
i
+v
i
Jaccard
u?v?
n
i=0
u
i
+v
i
Jaccard2
?
n
i=0
min(u
i
,v
i
)
?
n
i=0
max(u
i
,v
i
)
Lin
?
n
i=0
u
i
+v
i
|u|+|v|
Tanimoto
u?v
|u|+|v|?u?v
Jensen-Shannon Div 1?
1
2
(D(u||
u+v
2
)+D(v||
u+v
2
))
?
2 log 2
?-skew 1?
D(u||?v+(1??)u)
?
2 log 2
Table 2: Similarity measures between vectors v
and u, where v
i
is the ith component of v
whether removing more context words, based on
a frequency cut-off, can improve performance.
3 Experiments
The parameter space is too large to analyse ex-
haustively, and so we adopted a strategy for how
to navigate through it, selecting certain parame-
ters to investigate first, which then get fixed or
?clamped? in the remaining experiments. Unless
specified otherwise, vectors are generated with the
following restrictions and transformations on fea-
tures: stopwords are removed, numbers mapped
to ?NUM?, and only strings consisting of alphanu-
meric characters are allowed. In all experiments,
the features consist of the frequency-ranked first n
words in the given source corpus.
Four of the five similarity datasets (RG, MC,
W353, MEN) contain continuous scales of sim-
ilarity ratings for word pairs; hence we follow
standard practice in using a Spearman correlation
coefficient ?
s
for evaluation. The fifth dataset
(TOEFL) is a set of multiple-choice questions,
for which an accuracy measure is appropriate.
Calculating an aggregate score over all datasets
is non-trivial, since taking the mean of correla-
tion scores leads to an under-estimation of per-
formance; hence for the aggregate score we use
the Fisher-transformed z-variable of the correla-
Scheme Definition
None w
ij
= f
ij
TF-IDF w
ij
= log(f
ij
)? log(
N
n
j
)
TF-ICF w
ij
= log(f
ij
)? log(
N
f
j
)
Okapi BM25 w
ij
=
f
ij
0.5+1.5?
f
j
f
j
j
+f
ij
log
N?n
j
+0.5
f
ij
+0.5
ATC w
ij
=
(0.5+0.5?
f
ij
max
f
) log(
N
n
j
)
?
?
N
i=1
[(0.5+0.5?
f
ij
max
f
) log(
N
n
j
)]
2
LTU w
ij
=
(log(f
ij
)+1.0) log(
N
n
j
)
0.8+0.2?f
j
?
j
f
j
MI w
ij
= log
P (t
ij
|c
j
)
P (t
ij
)P (c
j
)
PosMI max(0,MI)
T-Test w
ij
=
P (t
ij
|c
j
)?P (t
ij
)P (c
j
)
?
P (t
ij
)P (c
j
)
?
2
see (Curran, 2004, p. 83)
Lin98a w
ij
=
f
ij
?f
f
i
?f
j
Lin98b w
ij
= ?1? log
n
j
N
Gref94 w
ij
=
log f
ij
+1
logn
j
+1
Table 3: Term weighting schemes. f
ij
denotes the
target word frequency in a particular context, f
i
the total target word frequency, f
j
the total context
frequency, N the total of all frequencies, n
j
the
number of non-zero contexts. P (t
ij
|c
j
) is defined
as
f
ij
f
j
and P (t
ij
) as
f
ij
N
.
tion datasets, and take the weighted average of
its inverse over the correlation datasets and the
TOEFL accuracy score (Silver and Dunlap, 1987).
3.1 Vector size
The first parameter we investigate is vector size,
measured by the number of features. Vectors are
constructed from the BNC using a window-based
method, with a window size of 5 (2 words either
side of the target word). We experiment with vec-
tor sizes up to 0.5M features, which is close to the
total number of context words present in the en-
tire BNC according to our preprocessing scheme.
Features are added according to frequency in the
BNC, with increasingly more rare features being
added. For weighting we consider both Positive
Mutual Information and T-Test, which have been
found to work best in previous research (Bullinaria
and Levy, 2012; Curran, 2004). Similarity is com-
puted using Cosine.
23
Figure 1: Impact of vector size on performance
across different datasets
The results in Figure 1 show a clear trend: for
both weighting schemes, performance no longer
improves after around 50,000 features; in fact, for
T-test weighting, and some of the datasets, perfor-
mance initially declines with an increase in fea-
tures. Hence we conclude that continuing to add
more rare features is detrimental to performance,
and that 50,000 features or less will give good per-
formance. An added benefit of smaller vectors is
the reduction in computational cost.
3.2 Window size
Recent studies have found that the best window
size depends on the task at hand. For example,
Hill et al. (2013) found that smaller windows work
best for measuring similarity of concrete nouns,
whereas larger window sizes work better for ab-
stract nouns. Schulte im Walde et al. (2013) found
that a large window size worked best for a com-
positionality dataset of German noun-noun com-
pounds. Similar relations between window size
and performance have been found for similar ver-
sus related words, as well as for similar versus as-
sociated words (Turney and Pantel, 2010).
We experiment with window sizes of 3, 5, 7, 9
and a full sentence. (A window size of n implies
n?1
2
words either side of the target word.) We
use Positive Mutual Information weighting, Co-
sine similarity, and vectors of size 50,000 (based
on the results from Section 3.1). Figure 2 shows
the results for all the similarity datasets, with the
aggregated score at the bottom right.
Performance was evaluated on three corpora,
in order to answer three questions: Does win-
dow size affect performance? Does corpus size
interact with window size? Does corpus sub-
Figure 2: Impact of window size across three cor-
pora
spacing interact with window size? Figure 2
clearly shows the answer to all three questions is
?yes?. First, ukWaC consistently outperforms the
BNC, across all window sizes, indicating that a
larger source corpus leads to better performance.
Second, we see that the larger ukWaC performs
better with smaller window sizes compared to the
BNC, with the best ukWaC performance typically
being found with a window size of only 3. For
the BNC, it appears that a larger window is able to
offset the smaller size of corpus to some extent.
We also evaluated on a sub-spaced Wikipedia
source corpus similar to Stone et al. (2008), which
performs much better with larger window sizes
than the BNC or ukWaC. Our explanation for this
result is that sub-spacing, resulting from search-
ing for Wikipedia pages with the appropriate tar-
get terms, provides a focused, less noisy corpus in
which context words some distance from the target
word are still relevant to its meaning.
In summary, the highest score is typically
achieved with the largest source corpora and
smallest window size, with the exception of the
much smaller sub-spaced Wikipedia corpus.
3.3 Context
The notion of context plays a key role in VSMs.
Pado and Lapata (2007) present a comparison of
window-based versus dependency-based methods
and conclude that dependency-based contexts give
better results. We also compare window-based and
dependency-based models.
Dependency-parsed versions of the BNC and
ukWaC were used to construct syntactically-
informed vectors, with a single, labelled arc be-
24
Figure 3: Window versus dependency contexts
tween the target word and context word.
3
Since
this effectively provides a window size of 3, we
also use a window size of 3 for the window-based
method (which provided the best results in Sec-
tion 3.2 with the ukWaC corpus). As well as
the ukWaC and BNC source corpora, we use the
Google syntactic N-gram corpus (Goldberg and
Orwant, 2013), which is one of the largest cor-
pora to date, and which consists of syntactic n-
grams as opposed to window-based n-grams. We
use vectors of size 50,000 with Positive Mutual In-
formation weighting and Cosine similarity. Due
to its size and associated computational cost, we
used only 10,000 contexts for the vectors gener-
ated from the syntactic N-gram corpus. The re-
sults are shown in Figure 3.
In contrast to the idea that dependency-based
methods outperform window-based methods, we
find that the window-based models outperform
dependency-based models when they are con-
structed from the same corpus using the small
window size. However, Google?s syntactic N-
gram corpus does indeed outperform window-
based methods, even though smaller vectors were
used for the Google models (10,000 vs. 50,000
features). We observe large variations across
datasets, with window-based methods performing
particularly well on some, but not all. In partic-
ular, window-based methods clearly outperform
dependency-based methods on the RG dataset (for
the same source corpus), whereas the opposite
trend is observed for the TOEFL synonym dataset.
The summary is that the model built from the syn-
tactic N-grams is the overall winner, but when we
3
The Clark and Curran (2007) parser was used to provide
the dependencies.
compare both methods on the same corpus, the
window-based method on a large corpus appears
to work best (given the small window size).
3.4 Feature granularity
Stemming and lemmatisation are standard tech-
niques in NLP and IR to reduce data sparsity.
However, with large enough corpora it may be
that the loss of information through generalisa-
tion hurts performance. In fact, it may be that in-
creased granularity ? through the use of grammat-
ical tags ? can lead to improved performance. We
test these hypotheses by comparing four types of
processed context words: lemmatised, stemmed,
POS-tagged, and tagged with CCG lexical cate-
gories (which can be thought of as fine-grained
POS tags (Clark and Curran, 2007)).
4
The source
corpora are BNC and ukWaC, using a window-
based method with windows of size 5, Positive
Mutual Information weighting, vectors of size
50,000 and Cosine similarity. The results are re-
ported in Figure 4.
The ukWaC-generated vectors outperform the
BNC-generated ones on all but a single instance
for each of the granularities. Stemming yields
the best overall performance, and increasing the
granularity does not lead to better results. Even
with a very large corpus like ukWaC, stemming
yields signficantly better results than not reduc-
ing the feature granularity at all. Conversely, apart
from the results on the TOEFL synonym dataset,
increasing the feature granularity of contexts by
including POS tags or CCG categories does not
yield any improvement.
3.5 Similarity-weighting combination
There is contrasting evidence in the literature re-
garding which combination of similarity metric
and weighting scheme works best. Here we inves-
tigate this question using vectors of size 50,000,
no processing of the context features (i.e., ?nor-
mal? feature granularity), and a window-based
method with a window size of 5. Aggregated
scores across the datasets are reported in Tables
4 and 5 for the BNC and ukWaC, respectively.
There are some clear messages to be taken from
these large tables of results. First, two weighting
schemes perform better than the others: Positive
Mutual Information (PosMI) and T-Test. On the
BNC, the former yields the best results. There are
4
Using NLTK?s Porter stemmer and WordNet lemmatiser.
25
Figure 4: Feature granularity: stemmed (S), lem-
matised (L), normal (N), POS-tagged (T) and
CCG-tagged (C)
RG MC W353 MEN TOEFL
P+COS 0.74 0.64 0.50 0.66 0.76
P+COR 0.74 0.65 0.58 0.71 0.83
T+COS 0.78 0.69 0.54 0.68 0.78
T+COR 0.78 0.71 0.54 0.68 0.78
Table 6: Similarity scores on individual datasets
for positive mutual information (P) and T-test
(T) weighting, with cosine (COS) and correlation
(COR) similarity
three similarity metrics that perform particularly
well: Cosine, Correlation and the Tanimoto coef-
ficient (the latter also being similar to Cosine; see
Table 2). The Correlation similarity metric has the
most consistent performance across the different
weighting schemes, and yields the highest score
for both corpora. The most consistent weighting
scheme across the two source corpora and similar-
ity metrics appears to be PosMI.
The highest combined aggregate score is that of
PosMI with the Correlation metric, in line with
the conclusion of Bullinaria and Levy (2012) that
PosMI is the best weighting scheme
5
. However,
for the large ukWaC corpus, T-Test achieves sim-
ilarly high aggregate scores, in line with the work
of Curran (2004). When we look at these two
weighting schemes in more detail, we see that T-
Test works best for the RG and MC datasets, while
PosMI works best for the others; see Table 6. Cor-
relation is the best similarity metric in all cases.
5
In some cases, the combination of weighting scheme and
similarity metric results in a division by zero or leads to tak-
ing the logarithm of a negative number, in which cases we
report the aggregate scores as nan (not-a-number).
Figure 5: Finding the optimal ?contiguous subvec-
tor? of size 10,000
3.6 Optimal subvector
Stopwords are typically removed from vectors and
not used as features. However, Bullinaria and
Levy (2012) find that removing stopwords has no
effect on performance. A possible explanation
is that, since they are using a weighting scheme,
the weights of stopwords are low enough that
they have effectively been removed anyhow. This
raises the question: are we removing stopwords
because they contribute little towards the meaning
of the target word, or are we removing them be-
cause they have high frequency?
The experiment used ukWaC, with a window-
based method and window size of 5, normal fea-
ture granularity, Cosine similarity and a sliding
vector of size 10,000. Having a sliding vector im-
plies that we throw away up to the first 40,000 con-
texts as we slide across to the 50,000 mark (replac-
ing the higher frequency contexts with lower fre-
quency ones). In effect, we are trying to find the
cut-off point where the 10,000-component ?con-
tiguous subvector? of the target word vector is
optimal (where the features are ordered by fre-
quency). Results are given for PosMI, T-Test and
no weighting at all.
The results are shown in Figure 5. T-test outper-
forms PosMI at the higher frequency ranges (to the
left of the plots) but PosMI gives better results for
some of the datasets further to the right. For both
weighting schemes the performance decreases as
high frequency contexts are replaced with lower
frequency contexts.
A different picture emerges when no weight-
ing is used, however. Here the performance can
increase as high-frequency contexts are replaced
26
British National Corpus
COS COR DIC JC1 JC2 TAN LIN EUC CIB CHS JSD ASK
none 0.49 0.50 0.34 0.35 0.27 0.22 0.30 0.09 0.11 0.08 0.45 0.36
tfidf 0.43 0.44 0.33 0.34 0.22 0.16 0.27 0.13 0.12 0.16 0.38 0.32
tficf 0.47 0.48 0.34 0.36 0.23 0.16 0.27 0.13 0.12 0.15 0.40 0.33
okapi 0.40 0.42 0.37 0.42 0.22 0.23 0.26 0.25 0.15 0.14 0.37 0.26
atc 0.40 0.43 0.25 0.24 0.16 0.34 0.30 0.10 0.13 0.08 0.33 0.23
ltu 0.44 0.45 0.35 0.36 0.22 0.23 0.26 0.22 0.13 0.21 0.37 0.27
mi 0.58 0.61 0.31 0.56 0.29 -0.07 0.45 0.15 0.10 0.09 0.16 -0.04
posmi 0.63 0.66 0.52 0.58 0.35 -0.08 0.45 0.15 0.11 0.06 0.54 0.46
ttest 0.63 0.62 0.11 0.34 0.08 0.63 0.17 0.18 0.14 0.11 nan nan
chisquared 0.50 0.50 0.46 0.42 0.42 0.42 nan 0.06 0.07 0.08 0.57 0.52
lin98b 0.47 0.52 0.35 0.40 0.21 -0.10 0.29 0.10 0.11 nan 0.38 0.29
gref94 0.46 0.49 0.35 0.37 0.23 0.06 0.28 0.12 0.11 0.09 0.41 0.30
Table 4: Aggregated scores for combinations of weighting schemes and similarity metrics using the BNC.
The similarity metrics are Cosine (COS), Correlation (COR), Dice (DIC), Jaccard (JC1), Jaccard2 (JC2),
Tanimoto (TAN), Lin (LIN), Euclidean (EUC), CityBlock (CIB), Chebyshev (CHS), Jensen-Shannon
Divergence (JSD) and ?-skew (ASK)
ukWaC
COS COR DIC JC1 JC2 TAN LIN EUC CIB CHS JSD ASK
none 0.55 0.55 0.28 0.35 0.24 0.41 0.31 0.06 0.09 0.08 0.56 0.49
tfidf 0.45 0.47 0.26 0.30 0.20 0.28 0.22 0.14 0.12 0.16 0.37 0.27
tficf 0.45 0.49 0.27 0.33 0.20 0.29 0.24 0.13 0.11 0.09 0.37 0.28
okapi 0.37 0.42 0.33 0.37 0.18 0.27 0.26 0.26 0.17 0.12 0.34 0.20
atc 0.34 0.42 0.13 0.13 0.08 0.15 0.28 0.10 0.09 0.07 0.28 0.15
ltu 0.43 0.48 0.30 0.34 0.19 0.26 0.25 0.26 0.16 0.24 0.36 0.23
mi 0.51 0.53 0.18 0.51 0.16 0.28 0.37 0.18 0.10 0.09 0.12 nan
posmi 0.67 0.70 0.56 0.62 0.42 0.59 0.52 0.23 0.15 0.06 0.60 0.49
ttest 0.70 0.70 0.16 0.48 0.10 0.70 0.22 0.16 0.11 0.15 nan nan
chisquared 0.57 0.58 0.52 0.56 0.44 0.52 nan 0.08 0.06 0.10 0.63 0.60
lin98b 0.43 0.63 0.31 0.37 0.20 0.23 0.26 0.09 0.10 nan 0.34 0.24
gref94 0.48 0.54 0.27 0.33 0.20 0.17 0.23 0.13 0.11 0.09 0.38 0.25
Table 5: Aggregated scores for combinations of weighting schemes and similarity metrics using ukWaC
with lower-frequency ones, with optimal perfor-
mance comparable to when weighting is used.
There are some scenarios where it may be ad-
vantageous not to use weighting, for example in
an online setting where the total set of vectors is
not fixed; in situations where use of a dimension-
ality reduction technique does not directly allow
for weighting, such as random indexing (Sahlgren,
2006); as well as in settings where calculating
weights is too expensive. Where to stop the slid-
ing window varies with the datasets, however, and
so our conclusion is that the default scheme should
be weighting plus high frequency contexts.
4 Compositionality
In order to examine whether optimal parame-
ters carry over to vectors that are combined into
phrasal vectors using a composition operator, we
perform a subset of our experiments on the canoni-
cal compositionality dataset from Mitchell and La-
pata (2010), using vector addition and pointwise
multiplication (the best performing operators in
the original study).
We evaluate using two source corpora (the BNC
and ukWaC) and two window sizes (small, with
a window size of 3; and big, where the full sen-
tence is the window). In addition to the weight-
ing schemes from the previous experiment, we in-
clude Mitchell & Lapata?s own weighting scheme,
which (in our notation) is defined as w
ij
=
f
ij
?N
f
i
?f
j
.
While all weighting schemes and similarity met-
rics were tested, we report only the best perform-
ing ones: correlations below 0.5 were ommitted
for the sake of brevity. Table 7 shows the results.
We find that many of our findings continue to
hold. PosMI and T-Test are the best performing
weighting schemes, together with Mitchell & La-
pata?s own weighting scheme. We find that ad-
dition outperforms multiplication (contrary to the
original study) and that small window sizes work
best, except in the VO case. Performance across
corpora is comparable. The best performing simi-
larity metrics are Cosine and Correlation, with the
latter having a slight edge over the former.
27
BNC - Small window
AN NN VO ALL
add-posmi-cosine 0.57 0.56 0.52 0.55
add-posmi-correlation 0.66 0.60 0.53 0.60
add-ttest-cosine 0.59 0.54 0.53 0.56
add-ttest-correlation 0.60 0.54 0.53 0.56
add-mila-correlation 0.64 0.38 0.51 0.51
ukWaC - Small window
AN NN VO ALL
add-posmi-correlation 0.64 0.59 0.56 0.59
add-ttest-cosine 0.61 0.55 0.53 0.56
add-ttest-correlation 0.61 0.55 0.53 0.56
add-mila-correlation 0.64 0.48 0.57 0.56
mult-mila-correlation 0.52 0.44 0.63 0.53
BNC - Large window
AN NN VO ALL
add-posmi-correlation 0.47 0.49 0.57 0.51
add-ttest-cosine 0.50 0.53 0.60 0.54
add-ttest-correlation 0.50 0.53 0.60 0.54
add-mila-correlation 0.51 0.49 0.61 0.54
mult-posmi-correlation 0.48 0.48 0.66 0.54
mult-mila-correlation 0.53 0.51 0.67 0.57
ukWaC - Large window
AN NN VO ALL
add-posmi-correlation 0.46 0.44 0.60 0.50
add-ttest-cosine 0.46 0.46 0.59 0.50
add-ttest-correlation 0.47 0.46 0.60 0.51
add-mila-correlation 0.47 0.46 0.64 0.52
mult-posmi-correlation 0.44 0.46 0.65 0.52
mult-mila-correlation 0.56 0.49 0.70 0.58
Table 7: Selected Spearman ? scores on the
Mitchell & Lapata 2010 compositionality dataset
5 Conclusion
Our experiments were designed to investigate a
wide range of VSM parameters, using a variety
of evaluation tasks and several source corpora.
Across each of the experiments, results are com-
petitive with the state of the art. Some important
messages can be taken away from this study:
Experiment 1 Larger vectors do not always lead
to better performance. As vector size increases,
performance stabilises, and a vector size of around
50,000 appears to be optimal.
Experiment 2 The size of the window has a
clear impact on performance: a large corpus with
a small window size performs best, but high per-
formance can be achieved on a small subspaced
corpus, if the window size is large.
Experiment 3 The size of the source corpus
is more important than whether the model is
window- or dependency-based. Window-based
methods with a window size of 3 yield better re-
sults than dependency-based methods with a win-
dow of 3 (i.e. having a single arc). The Google
Syntactic N-gram corpus yields very good perfor-
mance, but it is unclear whether this is due to being
dependency-based or being very large.
Experiment 4 The granularity of the context
words has a relatively low impact on performance,
but stemming yields the best results.
Experiment 5 The optimal combination of
weighting scheme and similarity metric is Posi-
tive Mutual Information with a mean-adjusted ver-
sion of Cosine that we have called Correlation.
Another high-performing weighting scheme is T-
Test, which works better for smaller vector sizes.
The Correlation similarity metric consistently out-
performs Cosine, and we recommend its use.
Experiment 6 Use of a weighting scheme ob-
viates the need for removing high-frequency fea-
tures. Without weighting, many of the high-
frequency features should be removed. However,
if weighting is an option we recommend its use.
Compositionality The best parameters for
individual vectors generally carry over to a com-
positional similarity task where phrasal similarity
is evaluated by combining vectors into phrasal
vectors.
Furthermore, we observe that in general perfor-
mance increases as source corpus size increases,
so we recommend using a corpus such as ukWaC
over smaller corpora like the BNC. Likewise,
since the MEN dataset is the largest similarity
dataset available and mirrors our aggregate score
the best across the various experiments, we rec-
ommend evaluating on that similarity task if only
a single dataset is used for evaluation.
Obvious extensions include an analysis of the
performance of the various dimensionality reduc-
tion techniques, examining the importance of win-
dow size and feature granularity for dependency-
based methods, and further exploring the relation
between the size and frequency distribution of a
corpus together with the optimal characteristics
(such as the high-frequency cut-off point) of vec-
tors generated from that source.
Acknowledgments
This work has been supported by EPSRC grant
EP/I037512/1. We would like to thank Laura
Rimell, Tamara Polajnar and Felix Hill for help-
ful comments and suggestions.
28
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A collection of very large linguistically processed
Web-crawled corpora. Language Resources and
Evaluation, 43(3):209?226.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2013. Frege in Space: A program for com-
positional distributional semantics. Linguistic Is-
sues in Language Technologies (LiLT).
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556, Jeju Island, Korea,
July. Association for Computational Linguistics.
Elia Bruni, Gemma Boleda, Marco Baroni, and N. K.
Tran. 2012. Distributional Semantics in Techni-
color. In Proceedings of the ACL 2012.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting Semantic Representations from Word Co-
occurrence Statistics: A computational study. Be-
havior Research Methods, 39:510?526.
John A. Bullinaria and Joseph P. Levy. 2012. Ex-
tracting Semantic Representations from Word Co-
occurrence Statistics: Stop-lists, Stemming and
SVD. Behavior Research Methods, 44:890?907.
L. Burnard. 2007. Reference Guide
for the British National Corpus.
http://www.natcorp.ox.ac.uk/docs/URG/.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Stephen Clark. 2014. Vector Space Models of Lexical
Meaning (to appear). In Shalom Lappin and Chris
Fox, editors, Handbook of Contemporary Semantics.
Wiley-Blackwell, Oxford.
James R. Curran and Marc Moens. 2002a. Improve-
ments in Automatic Thesaurus Extraction. In Pro-
ceedings of the ACL-02 workshop on Unsupervised
lexical acquisition-Volume 9, pages 59?66. Associa-
tion for Computational Linguistics.
James R. Curran and Marc Moens. 2002b. Scaling
Context Space. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 231?238. Association for Computational
Linguistics.
James R. Curran. 2004. FromDistributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk. 2012. Vector Space Models of Word
Meaning and Phrase Meaning: A Survey. Language
and Linguistics Compass, 6(10):635?653.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing Search in Context: The
Concept Revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Yoav Goldberg and Jon Orwant. 2013. A Dataset
of Syntactic-Ngrams over Time from a Very Large
Corpus of English Books. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Con-
ference and the Shared Task: Semantic Textual Simi-
larity, pages 241?247, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA, USA.
Z. Harris. 1954. Distributional Structure. Word,
10(23):146?162.
F. Hill, D. Kiela, and A. Korhonen. 2013. Con-
creteness and Corpora: A Theoretical and Practical
Analysis. In Proceedings of ACL 2013, Workshop
on Cognitive Modelling and Computational Linguis-
tics, Sofia, Bulgaria.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Platos problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Gabriella Lapesa and Stefan Evert. 2013. Evaluat-
ing neighbor rank and distance measures as predic-
tors of semantic priming. In In Proceedings of the
ACL Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL 2013), Sofia, Bulgaria.
G.A. Miller and W.G. Charles. 1991. Contextual Cor-
relates of Semantic Similarity. Language and Cog-
nitive Processes, 6(1):1?28.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1429.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based Construction of Semantic
Space Models. Computational Linguistics,
33(2):161?199.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2006. An Improved Model of Se-
mantic Similarity based on Lexical Co-occurence.
Communciations of the ACM, 8:627?633.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Commun.
ACM, 8(10):627?633, October.
29
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Depart-
ment of Linguistics, Stockholm University.
Sabine Schulte im Walde, Stefan M?uller, and Stephen
Roller. 2013. Exploring Vector Space Models to
Predict the Compositionality of German Noun-Noun
Compounds. In Proceedings of the 2nd Joint Con-
ference on Lexical and Computational Semantics,
pages 255?265, Atlanta, GA.
N. Clayton Silver and William P. Dunlap. 1987. Av-
eraging Correlation Coefficients: Should Fisher?s z
Transformation Be Used? Journal of Applied Psy-
chology, 72(1):146?148, February.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Benjamin P. Stone, Simon J. Dennis, and Peter J.
Kwantes. 2008. A Systematic Comparison of Se-
mantic Models on Human Similarity Rating Data:
The Effectiveness of Subspacing. In The Proceed-
ings of the Thirtieth Conference of the Cognitive Sci-
ence Society.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: vector space models of seman-
tics. J. Artif. Int. Res., 37(1):141?188, January.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of Coling 2004, pages
1015?1021, Geneva, Switzerland, Aug 23?Aug 27.
COLING.
S. Zelikovitz and M. Kogan. 2006. Using Web
Searches on Important Words to create Background
Sets for LSI Classification. In In Proceedings of
the 19th International FLAIRS Conference, pages
598?603, Menlo Park, CA. AAAI Press.
30
