Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1103?1111, Prague, June 2007. c?2007 Association for Computational Linguistics
Building Domain-Specific Taggers without Annotated (Domain) Data 
John E. Miller1 Manabu Torii2 K. Vijay-Shanker1 
1Computer & Information Sciences University of Delaware Newark, DE 19716 {jmiller,vijay}@cis.udel.edu 
2Biostatistics, Bioinformatics and Biomathematics  Georgetown University Medical Center  Washington, DC 20057 mt352@georgetown.edu  Abstract Part of speech tagging is a fundamental component in many NLP systems. When taggers developed in one domain are used in another domain, the performance can degrade considerably. We present a method for developing taggers for new domains without requiring POS annotated text in the new domain. Our method involves using raw domain text and identifying related words to form a domain specific lexicon. This lexicon provides the initial lexical probabilities for EM training of an HMM model. We evaluate the method by apply-ing it in the Biology domain and show that we achieve results that are comparable with some taggers developed for this domain.  1 Introduction As Natural Language Processing (NLP) technol-ogy advances and more text becomes available, it is being applied more and often in specialized do-mains. Part of Speech (POS) tagging is often a fundamental component to these NLP applications and hence its accuracy can have a significant im-pact on the application?s success. The success that the taggers have attained is often not replicated when the domain is changed. Degradation of accu-racy in a new domain can be overcome by devel-oping an annotated corpus for that specific domain, e.g., as in the Biology domain. However, this solu-tion is feasible only if there is sufficient interest in the use of NLP technology in that domain, and there are sufficient funding and resources. In con-trast, our approach is to use existing resources, and 
rapidly develop taggers for new domains without using the time and effort to develop annotated data. In this work, we use the Wall Street Journal (WSJ) corpus (Marcus et al 1993) and large amounts of domain-specific raw text to develop taggers. We evaluate our methodology in the Biol-ogy domain and show the resulting performance is competitive with some taggers built with super-vised learning for that domain. Also, we note that the accuracy of taggers trained on the WSJ corpus drops off considerably when applied to this domain. Smith et al (2005) report that the Brill tagger (1995) has an accuracy of 86.8% on 1000 sen-tences taken from Medline, and that the Xerox tag-ger (Cutting et al.1992) has an accuracy of 93.1% on the same sentences. They attribute this drop off to the fact that only 57.8% of the 10,000 most fre-quent words can be found in WSJ corpus. This ob-servation provides further impetus to developing lexicon for taggers in the new domains.  In the next section, we discuss our general ap-proach. The details of the EM training of the HMM tagger are given in Section 3. Section 4 provides details of how a domain specific lexicon is created. Next, we discuss the evaluation of our models and analysis based on the results. Section 6 discusses related work and those works from which we have taken some ideas. Section 7 has some concluding remarks.  2 Basic Methodology Inadequate treatment of domain-specific vocabu-lary is often the primary cause in the degradation of performance when a tagger trained in one genre of text is ported to a new domain. The significance of out-of-vocabulary words has been noted in re-duced accuracy of NLP components in the Biology 
1103
domain (e.g., Lease and Charniak, 2005; Smith et al 2004). The handling of domain-specific vocabu-lary is the focus of our approach.  It is quite common to use suffix information in the prediction of POS tags for occurrences of new words. However, its effectiveness may be limited in English, which is not a highly inflected language. However, even for English, we find that not only can suffix information be used online during tag-ging, but also the presence or absence of morpho-logically related words can provide considerable information to pre-build a lexicon that associates possible tags with words.  Consider the example of the word ?broaden?. While the suffix ?en? may be utilized to predict the likelihood of verbal tags (VB and VBP) for the word during tagging, if we were to build a lexicon offline, the existence of the words ?broadened?, ?broadening?, ?broadens? and ?broad? give further evidence to treat ?broaden? as a verb. This type of information has been used before in (Cucerzan and Yarowsky, 2000).  In the above example, the presence or absence of words with the suffix morphemes suggests POS tag information in two ways: 1) The presence of a suffix morpheme in a word suggests a POS tag or a small set of POS tags for the word. This is the type of information most taggers use to predict tags for unknown words during the tagging process; 2) The presence of the morpheme can also indicate possi-ble tags for the words it attaches to. For example, the derivational morpheme ?ment? indicates ?gov-ernment? is likely to be an NN and also that the word it attaches to, ?govern? is likely to be a verb. Inflectional and derivational morphemes don?t at-tach to words of just any POS category; they are particular. Thus, we can propose the possibility of JJ (adjective) to ?broad? and VB or VBP to ?gov-ern? (based on the fact the derivational morphemes ?en? and ?ment? attach to them) even though by themselves they don?t have any suffix information that might be indicative of JJ and VB or VBP.  Additional suffixes (that may or may not be taken from a standard list of English inflectional and derivational morphemes) can also be used. As an example, the suffix ?ate? can be associated with a small set of tags: VB or VBP (?educate?, ?cre-ate?), JJ (?adequate?, ?appropriate?), and NN (?candidate?, ?climate?). Note the possibility or impossibility of the addition of ?tion? and ?ly? can help distinguish between the verbal and adjectival 
situations. In contrast, most taggers that use just suffix information during the tagging process will need strong contextual information (i.e., tags of nearby words) in making their prediction for each occurrence, as such suffixes can be associated with multiple tags. To utilize such information, we need a diction-ary of words in the domain for which we are inter-ested in building a tagger. Such a dictionary will allow us to propose possible tags for a domain word such as ?phosphorylate?. If we can verify whether words like ?phosphorylation?, ?phos-phorylates?, and ?phosphorylately,? are available in the domain then we can obtain considerable in-formation regarding the possible tags that can be associated with ?phosphorylate?. But we cannot assume the availability of a dictionary of words in the domain. However, it would suffice to have a large text corpus, which we call Text-Lex. We use it as a proxy for a domain dictionary by obtaining a list of words and their relative frequency of ap-pearance in the domain.  Rather than using manually developed rules that assign possible tags for words based on the pres-ence or absence of related words, we wish to apply a more empirical methodology. Since this sort of information is specific to a language rather than a domain, we can use an annotated corpus in another domain to provide exemplars. We use the WSJ (Marcus et al 1993) corpus, a POS annotated cor-pus, for this purpose.  For example, we can see that ?phosphorylate? in the Biology domain and ?cre-ate? in the WSJ corpus are similar in the sense both take on ?tion?, ?ed?, and ?ing? suffixes but not ?ly? for instance. Since the WSJ corpus would provide POS tag information for ?create?, we can use it to inform us for ?phosphorylate?. The above method forms the basis for our de-termination of the set of tags that are to be associ-ated with the domain words. However, the actual tag to be assigned for an occurrence in text de-pends on the context of use. We capture this in-formation by using a first-order HMM tagger model. For the transitional probabilities, we begin by using WSJ-based probabilities as a starting point and then adjust to the new domain by using a domain specific text and using EM training. EM also allows for adjusting lexical probabilities de-rived using WSJ words as exemplars. We call the domain specific text used for training of our HMM tagger as Text-EM. While this could be the same 
1104
as Text-Lex, we distinguish the two since Text-EM could be smaller than Text-Lex. From Text-Lex, we only extract a list of words and their frequency of occurrences. In contrast, we use Text-EM as a text and hence as a sequence of words.  In this work, the set of suffixes that we use is adapted those found in a GRE preparation web-page (DeForest, 2000). A few additional suffixes were obtained from the online English Dictionary AllWords.com (2005). In the future we expect to consider automatic mining of useful suffixes from a domain. Furthermore, prefixes are also useful for our purposes. However apart from a few prefixes used in hyphenated words, we haven?t yet incorpo-rated prefix information in a systematic way into our framework.  In this paper, our evaluation domain is molecu-lar biology. Large amounts of text are easily avail-able in the form of Medline abstracts. We use only about 1% of the Medline text database for Text-Lex. Another reason for selecting this evaluation domain is that we have a considerable amount POS-annotated text in this domain, and the most recent techniques of supervised POS tag learning have been used in developing taggers for this do-main. This allows us to evaluate our tagger using the annotated text for evaluation as well as to com-pare our tagger with others developed for this do-main. The POS-annotated text we use is the well-known GENIA (Tateisi et al 2003) corpus that was developed at University of Tokyo.  3 Expectation Maximization Training Our tagger is a first-order Hidden Markov Model (HMM) tagger that is trained using Expectation Maximization (EM) since we do not assume exis-tence of annotated data in the new domain.1 Al-though we use the GENIA corpus, we take only the raw text and strip off the annotated information for obtaining the Text-EM. Our HMM is based on bi-gram modeling and hence our transitional prob-abilities correspond to P(t | t?) where t and t? are POS tags. The emissions that label the transition edges will be discussed in the next section and in-clude domain words as well as certain types of ?coded words?.                                                   1 We considered a 2nd order model as well, but early work showed negligible advantage predicting to the same training set. Following Wang and Schuurmans (2005) we chose to focus on quality of estimation over model complexity. 
The initial transitional probabilities are not ran-domly chosen but rather taken from the WSJ cor-pus. If we take the transitional probabilities as a representation of syntactic preferences, then EM learning using Text-EM may be taken as adjust-ment of the grammatical preferences in the WSJ corpus to those in the new domain. In order to ad-just the grammatical preference to the new domain, we start from smoothed WSJ bigram probabilities. If we started from unsmoothed WSJ bigram prob-abilities, then EM would not allow us to account for transitions that are not observed in the WSJ corpus. For example, in scientific text, transition from RRB (the right round bracket) to VBG may be possible, while it does not occur in the WSJ corpus. Hence, we smooth the WSJ bigram prob-abilities with WSJ unigram probabilities.  We compute smoothed initial bigram probabili-ties as P(t | t?) = ? PWSJ(t | t?) + (1-?) PWSJ(t), where ?=0.9. We felt employing techniques sug-gested in (Brants, 2000) gave too high a preference for unigram probabilities.  The initial emit probability is obtained from the domain text Text-Lex. The process is described in the next section. This information is derived purely from suffix and suffix distribution, or from ortho-graphic information and does not account for the actual context of occurrences in the domain text. We take this suffix-based (and orthographic-based) emit probabilities as reasonable initial lexical probabilities. EM training will adjust them as nec-essary.  We made one minor modification to the stan-dard forward-backward EM algorithm. We dampen the change in transitional and emit probabilities for each iteration. Significant differences in lexical probabilities between the new domain and WSJ can make undue changes in transitional probabili-ties and this in turn can further lead the lexical probabilities to head in the wrong direction. By adding a damping factor, we can prevent the unsu-pervised training to spiral out of control. Hence we let the new transitional probability be given by  P(t | t?) = ? PNEW(t | t?) + (1-?) POLD(t | t?)  where POLD represents the transitional probability in the previous iteration and PNEW represents the probability by standard use of forward-backward algorithm. We use a damping factor of 0.5 for both transitional and emit probabilities. For the emit probabilities, this has the effect of moderating POS 
1105
preferences derived from the training data and pre-serving words and POSes from the lexicon for use in the test set. Even with the damping factor, EM learning fol-lowed the pattern of ?Early Maximum? described by Elworthy (1994), where with good initial esti-mates EM learning only improves accuracy for a few iterations.  For our EM training, we fixed it-eration 2 as our ?best? EM trained model. 4 Development of the Lexicon and Initial Probabilities As noted earlier, we use a domain text, Text-Lex, to develop the initial lexical probabilities for the HMM. The essential process is as follows. Let a word w appear a sufficient number of times in Text-Lex (at least 5 times). We look in Text-Lex for related words in order to assign a feature vector with this word. Each feature is written as ?x+y, where x and y represent suffixes or the empty string (here represented as _).  Features: The feature ?x+y represents the word formed by replacing some suffix x in w by some suffix y. Consider the word ?creation?. ??ion+_? corresponds to the stem word ?create? and ??ion+ion? corresponds to the word ?creation? itself. The feature ??ion+ed? captures information about the word ?created? whereas the feature ?-_+s? cor-responds to word ?creations?. Now consider a word like ?history?. While this might have non-zero values for ?-y+ic? (historic) or ?-_+s? (histories), we are likely to set zero value for ??ory+_? (unless ?hist? or ?histe? is found in Text-Lex). This zero value represents the fact that although ?history? has ?ory? as a suffix, it has no stem. Such a distinction (whether or not there is a stem) bears much information for suffixes like ?ate? and ?ory?.  We use suffix classes rather than actual suffixes as we believe this provides a more appropriate level of abstraction. Given a word w with a suffix x (for a word with no suffix from our list of suf-fixes, x is taken to be _. i.e., empty string), we ex-amine whether removal of x from w leads to an-other word by using a few basic variations that can be found in any rudimentary exposition on English morphology. For example, for the suffix ?ed?, we attempt to replace ?ied? with ?y? which relates ?purified? with ?purify? and recognizes the spell-ing alternation of i/y. Thus for the word ?purify? 
the feature ?-+ed? represents the presence of ?pu-rified? since ?+ed? represents the suffix class rather than the actual suffix. Similarly, we also consider removal of a suffix and, if necessary, add-ing an ?e? to see if such a word exists. This allows us to relate ?creation? with ?create? or ?activate? with ?active?. Also doubling of a few consonants is attempted to relate ?occurrence? and ?occur?. Finally, when a word could have two suffixes, the word is considered to always have the longer func-tional suffix. Hence, we consider ?government? to have ?ment? suffix rather than ?ent? suffix.  Feature Vectors: There are two different types of vectors we use for any word, one called Bin (for binary count) and other called RFreq (for relative frequency). In the Bin vector associated with ?creation?, all these four features will get the value one, assuming that the four corresponding words are found in Text-Lex. On the other hand, assuming ?creatory? is not found in Text-Lex, the feature ?-ion+ory? would get a zero value.  For RFreq vector, instead of ones and zeros, we first start with the frequency of occurrences of each word and then normalize so that the sum of all fea-ture values is one. Thus, for example, a word with 4 features having non-zero frequencies of 10, 20, 30 and 40 will have the respective values set to 0.1, 0.2, 0.3 and 0.4. A word with four features having non-zero frequency, which are 1, 2, 3 and 4, will also have same 4 relative frequency values.  Our intuition is that the Bin vector is helpful in determining the set of tags that can be associated with a word and that the RFreq vector can aug-ment this information regarding the likelihood of these tags. For example, a one for the ?-ing+_? feature in a Bin vector (thus disqualifying a word like ?during?) may be sufficient to predict VBG, JJ and NN tags. However, this may not suffice to provide the ordering of likelihood among these tags for this word. On the other hand, it seems to be the case that when the ?ing? form appears far more often than the ?ed? form, then the NN tag is most likely. But if the ?ed? form is more frequent, then VBG is most likely. Examples in the WSJ corpus include ?smoking?, ?marketing?, ?index-ing?, and ?restructuring? for the first kind, and ?calling?, ?counting?, ?advising?, and ?noting? for the second kind.  Exemplars in WSJ: Given a word w from Text-Lex, we look for similar words from the WSJ cor-pus. Even though the set of words used in this cor-
1106
pus may differ substantially from the domain text, our hypothesis is that words with similar suffix distribution will have similar POS tag assignments regardless of the domain. We follow Cucerzan and Yarowsky (2000) in using the kNN method for finding similar words, but we differ in details of the construction of the feature vectors and distance computation. For the word w we create the Bin and RFreq vectors based on distribution of words in Text-Lex. Following the same method, we create the Bin and RFreq vectors for a word v in the WSJ corpus by using the distributions in the WSJ cor-pus. Then we compute BinDist(w,v) as the number of features in which the two Bin vectors  differ. A similar RFDist is defined as a weighted sum of two distances: the first distance is L1-norm dis-tance based on values of features for which both words have non-zero values for and the second distance is based on values of features for which one word has a zero value and other does not. Thus, if the two words? RFreq vectors are 
? 
< w
1
,...,w
n
>  and 
? 
< v
1
,...,v
n
>respectively then  
? 
RFsame(w,v) = w
i
? v
i
w
i
?0?v
i
?0
?
  
? 
RFdiff (w,v) =
w
i
? v
i
w
i
=0? v
i
?0
?
+ w
i
? v
i
w
i
?0? v
i
=0
?
 and,   
? 
RFDist ( w,v ) = RFsame( w,v ) +?RFdiff ( w,v ) For RFDist(.), we used ? =2. Given a word w, we find the 5 nearest neighbors from the WSJ cor-pus and use their average lexical probabilities to obtain the lexical probabilities for w. We investi-gate the use of Bin vector information and RFreq vector information for computing the distances (i.e., BinDist(.) and RFDist(.)) as well as a hybrid measure that combines these two distances.  We also considered smoothing the lexical prob-abilities obtained in the above fashion. Let w be a word for which the above method suggests tags t1,?,tn in order of likelihood (t1 is most probable).  Then we consider sqrt-score(ti)= 
? 
n +1? i . We then assign probabilities based on this score after normalizing them so that the probabilities for the n tags will sum to 1. Thus, for example, if a word w has three possible tags, no matter what the original lexical probabilities were determined to be, if t1 is 
determined to be most probable, then P(t1|w) will be 0.418 by this method. The second most prob-able tag will be assigned 0.341.   The intuition behind this square root smoothing method is that this smoothing may be appropriate for low frequency words, where empirical prob-abilities based purely on a kNN basis may not be entirely appropriate if the new domain is very dif-ferent. The drawback of course is that if there is sufficient information, we lose useful information by such flattening. And when a tag is significantly more probable for a word then we lose this vital information. For example, the word ?high? is mostly annotated as JJ in WSJ corpus but RB and NN are also possible. Square root smoothing will flatten this distribution considerably. Nevertheless, we wish to investigate whether this method of smoothing the distribution is enough in conjunc-tion with EM. EM adjusts the probability from observing the number and context of occurrences in the domain text.2  Coded Words: No matter how large Text-Lex is, there will be words that do not appear a suffi-cient number of times (we take this number to be 5). We aggregate such words according to their suffixes, if they correspond to one of the prede-fined suffixes. Then each word with suffix x is considered to be an instance of a ?coded? word SFX-x. If a word does not have any of these suf-fixes then they fall into the coded class unknown. For each such coded word, we assign the tags and probabilities based on similarly aggregated words in the WSJ corpus. We have two other broad classes of words that we treat differently. Coded words are formed based on orthographic characteristics, which include but are not limited to Greek letters, Roman numerals, digits, upper or lower case single letters, upper case letter sequences, cardinals, certain prefix words, and their combinations.  Since they are rela-tively easy to tag, we do not use the WSJ corpus for them but handle it programmatically. Finally, if a word occurs often in WSJ or is assigned tags such as CD, FW, MD, PRP, DT, WDT, etc. (tags which can?t be predicted by means of suffix or suf-fix-related words), we add this word together with the tags and probability into the domain lexicon that we are building.                                                   2 We also considered linear and square functions for smooth-ing while reporting only the sqrt results in section 5. 
1107
5 Evaluation and Analysis As noted earlier, our evaluation is on molecular biology text. For Text-Lex, we used 133,666 ti-tles/abstracts of research papers, a small fraction of the Medline database available from the National Library of Medicine. These abstracts were con-tained in just five of the 500 compressed data files in the 2006 version of the Medline database. These abstracts cover topics more broadly in Biomedicine and not just molecular biology.  On the other hand, we use for Text-EM, text which can be regarded to be in a subfield of molecular biology.  Text-EM is the text from the GENIA corpus (version 3.02) described in (Tateisi et al 2003). This corresponds to about 2000 abstracts, which are annotated with POS tag information (using the same tags used in the WSJ corpus). We use a 5-fold cross-validation, i.e., 5 partitions are formed and experiments conducted 5 times and results av-eraged. For each test partition, the remainder parti-tions are used for ?training?. In our case, this is unsupervised since we use EM and hence we to-tally disregard the POS tag information that is as-sociated with the words. We note that both the text for EM training as well as for testing come from the same domain.   We first evaluate the process of building the lexicon. This time we consider the entire GENIA corpus and not any partition. We first considered all words in the GENIA corpus for which we can expect our kNN method to assign a tag. Hence all words that would be treated as coded words are ignored. For each such word, we consider the tags assigned to them in the GENIA corpus and form pairs <w,t>.  We are interested in the word type and not token and hence we will not have any mul-tiple occurrences of a pair <w,t>. Our kNN method identifies 96.3% of these pairs; we can think of this as recall. This makes our approach effective, espe-cially given the fact that the kNN method only as-signs 1.92 tags on an average to these words in the GENIA corpus. Next considering all words appear-ing in the GENIA corpus, our lexicon includes a correct tag in 99.0% of the cases on a word-token basis. These results are summarized below.  Characteristic Statistic kNN Recall (word-type) 96.3% Average Number Tags/Word 1.92 tags Lexicon Recall (word-token) 99.0% 
We now turn to the evaluation of the accuracy of our HMM. As mentioned earlier, these results are based on 5-fold cross-validation experiments. The best results (95.77%) were obtained for the case where we took the lexical probabilities directly from kNN using only RFDist and by discarding all tags assigned with probability less than 0.02.3  These results compare favorably to other taggers developed for the Biology domain. The MedPost tagger (see Section 6) achieved an accuracy of 94.1% when we applied it to the GENIA abstracts. The PennBioIE tagger (see Section 6) achieved an accuracy of 95.1%. Note that output from the PennBioIE tagger is not fully compatible with GENIA annotation due to some differences in its tokenization.  Even if the differences in accuracies can be discounted due to tokenization or even sys-tematic differences in annotation between the train-ing and test corpora, our main point is that our re-sults compare favorably (our tagger competitive) with taggers that were developed for the Biomedi-cine domain using supervised training.   These results are summarized in the table below.  POS Tagger %Accuracy Our HMM (5-fold) 95.77% MedPost 94.1% PennBioIE 95.1% GENIA supervised 98.26%  MedPost seems intended to cover all of Bio-medicine, since its lexicon is based on the 10,000 most frequently occurring words from Medline and for which the set of possible tags were manually specified. The PennBioIE tagger was developed using 315 Medline abstracts using another subfield of molecular biology. None of these accuracies however are as high as those of the GENIA tagger (Tsuruoka et al 2005) which was trained (supervised) using GENIA cor-pus and uses a machine learning model more so-phisticated than the simple first-order HMM tagger we use. This model considers more features includ-ing words to the right. The best results (98.26%) were obtained when lexicon from three different sources were aggregated.                                                    3 Banko and Moore (2004) showed only slight improvement in tag accuracy between .01 and .1 cutoffs with a lexicon built from annotated data. We opted for the .02 cutoff because of our ?noisier? lexicon. 
1108
Returning to the results for our taggers, we also tried BinDist in the kNN method, with and without square root smoothing. These results were typi-cally less than the above-mentioned result. We also compared using a square root smooth on RFDist obtaining results approximately 1% lower than without the square root smooth. We next present some examples that illustrate strengths and weaknesses of the current model. An example that shows that EM training makes good adjustment to the domain is the improvement in tagging of verbal categories. We conducted a de-tailed error analysis on one of the cross-validation partitions and noted that the accuracy on all verbal POS tags improved after EM training. A notewor-thy case is the improvement in tagging of VBP originally misclassified as VB. Since most English words that are VB can also be VBP, and since they are annotated more frequently in WSJ as VB, the initial lexicon usually has a higher probability as-signed to VB for most words. As EM training pro-gresses, we noted that the frequency of VBP mistagged as VB decreases. Similarly, misclassifi-cations of VBG as NN also drops in the final model (by 40.3% on Text-EM) as compared to the initial model based on WSJ transitional probabili-ties and initial lexicon derived using WSJ words as exemplars. Previously, in the context of parsing Biomedical text, Lease and Charniak (2004) mention the oc-currences of sequences of multiple NN is more frequent in the GENIA corpus than in the WSJ corpus and that it could lead to parsing errors. We didn?t observe this problem here, but rather the contrary situation where many JJs were initially mistagged as NN.  About 22% of these misclassifi-cations are corrected after EM training.  While our model adjusts well in these cases to the new domain, sometimes the drift leads to worse performance. An example is in the misclassifica-tion of VBN as JJ. The most frequent word for which this misclassification occurs in the word ?activated?. These misclassifications occur in the context such as ?the activated cells?. The use of VBN rather than JJ is hard to determine on basis of just surface features and perhaps has to do more with the meaning of the word. In supervised set-ting, if sufficient such cases were annotated then this would be learned. But in an unsupervised set-ting this turns out to be a problem case. Despite the fact that RFDist predicted VBN as most probable 
tag for ?activated?, EM training makes this situa-tion worse.  Analysis of words with most frequent errors re-vealed many cases from orthographic coded words. Many occurrences of single lower case letters (which could have LS, SYM or NN tags) were la-beled as LS whereas the GENIA tagging used NN. Our model tagged ?+/-? always as SYM whereas because of the context of use, GENIA annotations were CC. (In fact, GENIA does not appear to use the SYM tag.) Similarly, ?<? and ?>? were often mistagged as SYM by our model whereas based on context they are annotated as JJR.  6 Related Work The impact of out-of-vocabulary words on NLP applications has been noted before. The degrada-tion in performance of components, which were trained on the WSJ corpus, but used on biomedical text has been noted (Lease and Charniak, 2004, Smith et al 2005). Smith et al (2005) use this ob-servation in the design of their POS tagger, Med-Post, by building a Markov model with a lexicon containing the 10,000 most frequent words from Medline, and using annotated text from the Bio-medical text for supervised training.  There are many unsupervised approaches to POS tagging. We focus now on those that are most closely related to our work and contain ideas that have influenced this work. There have been many uses of EM training to build HMM taggers (Kupiec, 1992; Elworthy, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Banko and Moore (2004) achieved better accuracy by restrict-ing the set of possible tags that are associated with words. By eliminating possibilities that may appear rarely with a word, they reduce the chances of un-supervised training spiraling along an unlikely path.  We believe by using our approach we con-siderably reduce the set of tags to what is appropri-ate for each word. Further, we too remove any tag associated with low probability by kNN method.  Usually these tags are noise introduced by some inappropriate exemplar.  Wang and Schuurman (2005) suggest that EM algorithm be modified such that at any iteration the unigram tag probability be held constant to the true probability for each tag. Again, this might serve to stop a drift in unsupervised methods towards mak-ing a tag?s probability become larger than it should 
1109
be. However, the true probability cannot be known ahead of time and certainly not in a new domain. While a WSJ bigram probability need not reflect the corresponding preferences in the new domain, our use of starting from WSJ probabilities and then damping changes to transition probabilities was motivated by a similar concern of not letting a drift towards making some (bigram) tags too frequent during EM iterations.  Using suffixation patterns for purposes of pre-dicting POS tags has been considered before. Al-though as far as we know, we are the first to apply it for domain adaptation purposes. Schone and Ju-rafsky (2001) consider clusters of words (obtained by some ?perfect? clustering algorithm) and then compute a measure of how ?affixy? a cluster is. For example, a cluster containing words ?climb? and ?jump? may be related by suffixing operation +s to a cluster that contains words ?climbs? and ?jumps?. The percentage of words in a cluster that are so related provides a measure of how ?affixy? a cluster. This together with five other attributes of clusters (such as whether words in a cluster pre-cede those of another cluster, optionality) and lan-guage universals induce POS tags for these clusters from corpora. This method does not use POS tagged corpora (although in the reported experi-ment the initial ?perfect? clusters were obtained from the Brown corpus using the POS tag informa-tion). In contrast, we use the POS tagged WSJ cor-pus to assist in the induction of tag information for our lexicon. In this respect, our method is closer to the approach of Cucerzan and Yarowsky (2000). Our use of the kNN method to identify tags and their probabilities for words was inspired by this work.  However, their use of kNN method was in the context of supervised learning. The method was applied for handling words unseen in the train-ing data. The estimated probabilities were used during the tagging process. Instead of just applying the method for unknown words, i.e., words not present in the training data, our approach is to cre-ate the entire lexicon in the new domain. As Lease and Charniak (2004), among others, have noted, the distribution of NN tag sequences as well as tag distributions in the Biomedical domain could differ from WSJ text. Since our aim is to adjust to the new domain, we employed unsupervised learning in the form of EM training, unlike the supervised tagging model development approach of Cucerzan and Yarowsky. Another significant difference is 
that their method determines nearest neighbors not only on the basis of suffix-related words but also on the basis of nearby words context. Since our motivation, on the other hand, is to move to a new domain, we didn?t consider detection of similarity on the basis of word contexts. In contrast, we have shown that the approach of identifying words on the basis of suffixation patterns and using them as exemplars can be applied effectively even when the domain of application is substantially different from the text (the WSJ corpus) providing the ex-emplars. 7 Conclusions As NLP technology continues to be applied in new domains, it becomes more important to consider the issue of portability to new domains. To cope with domain-specific vocabulary and also different use of vocabulary in a new domain, we exploited suffix information of words. While use of suffix information per se has been employed in many ex-isting POS taggers, its use is often limited to an online manner, where each word is examined inde-pendently from the existence of its morphologi-cally related words. As shown in (Cucerzan and Yarowsky, 2000), such information can provide considerable information to build a lexicon that associates possible tags with words. However, we use this information only to provide the initial val-ues. We apply EM algorithm to adjust these initial probabilities to the new domain.  The results in Section 5 show that we achieve good performance in the evaluation domain, which is comparable with two recently developed taggers for this domain. We also show in section 5 exam-ples of how EM unlearns some WSJ bias and ad-justs to the new domain. While we introduce a damping factor to slow down changes in iterations of EM training, we believe there is scope for fur-ther improvement to minimize drift. Furthermore, there is scope to improve our kNN method as dis-cussed at the end of Section 5. In the future, we also expect to consider methods that may auto-matically mine suffixes in a new domain and use these domain-specific suffixes. We used the kNN method to associate words in the new domain with possible POS tags.  Despite the often-stated notion that English is not morphologically rich, we find that suffix-based methods can still help make significant inroads. 
1110
Our method offers the chance to develop good tag-gers for specialized domains. For example, the GENIA corpus and PennBioIE corpus are speciali-zations within molecular biology, but taggers de-veloped on one corpus degrades in performance on the other. Using our method, we could use differ-ent Text-EM for these specializations even if we retain Medline as Text-Lex. In the same way, we could develop a tagger for the medical domain, which has a distinct vocabulary from biology.  References Banko, M. and Moore, R.C. 2004.  Part of Speech Tag-ging in Context. In Proceedings, 20th International Conference on Computational Linguistics (Coling 2004), Geneva, Switzerland, pp.556-561. Baum, L. 1972. An inequality and associated maximiza-tion technique in statistical estimation for probabilis-tic functions of a Markov process. Inequalities 3:1-8.  Brants, T. 2000. TNT - a statistical part-of-speech tag-ger.  In Proceedings of the Sixth Applied Natural Language Processing Conference ANLP-2000.   Brill, E. 1995. Transformation-based error-driven learn-ing and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565.   Cucerzan, S. and Yarowsky, D. 2000. Language inde-pendent minimally supervised induction of lexical probabilities. Proceedings of ACL-2000, Hong Kong, pages 270-277. Cutting, D., Kupiec, J., Pedersen, J., and Sibun, P. 1992. A practical part of sppech tagger. Proceedings of 3rd Conference on Applied Natural Language Process-ing, 53-58.  DeForest, J. 2000. Graduate Record Exam Suffixed web page. Michigan State University. http://www.msu.edu/~defores1/gre/sufx/gre_suffx.htm Elworthy, D. 1994. Does Baum-Welch re-estimation help taggers. In Proceedings of the Fourth Confer-ence on Applied Natural Language Processing, ACL. Kulick, S., Bies, A., Liberman, M., Mandel, M.,  McDonald, R., Palmer, M., Schein, A. and Ungar, L. Integrated Annotation for Biomedical Information Extraction. HLT/NAACL, 2004. Kupiec, J. 1992. Robust Part-of-speech Tagging Using a Hidden Markov Model. Computer Speech and Lan-guage, 6. 
Lease, M. and Charniak, E. 2005. Parsing Biomedical Literature.  IJCNLP-2005: 58-69.   Marcus, M., Santorini, B., Marcinkiewicz, M.A. 1993. Building a large annotated corpus of English: The Penn Treebank.  Computational Linguistics, 19:313-330.   PennBioIE. 2005. Mining The Bibliome Project. http://bioie.ldc.upenn.edu/. Schone, P. and Jurafsky, D. 2001, Language Independ-ent Induction of Part of Speech Class Labels Using Language Universals. IJCAI Workshop on Text Learning: Beyond Supervision.  Smith, L., Rindflesch, T., Wilbur, W.J. 2004. MedPost: a part-of-speech tagger for biological text.  Bioin-formatics 20 (14):2320-2321.   Smith, L., Rindflesch, T., Wilbur, W.J. 2005. The im-portance of the lexicon in tagging biomedical text. Natural Language Engineering 12(2) 1-17. Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsujii, J. 2003. The GENIA corpus: Medline ab-stracts annotated with linguistic information.  In: Third meeting of SIG on Text Mining, Intelligent Systems for Molecular Biology (ISMB).   Tsuruoka, Y., Tateishi, Y., Kim, J. D., Ohta, T., McNaught, J., Ananiadou, S., and Tsujii, J.. 2005. Developing a Robust Part-of-Speech Tagger for Biomedical Text, Advances in Informatics - 10th Panhellenic Conference on Informatics, LNCS 3746: 382-392. Wang, Q. and Schuurmans, D. 2005. Improved estima-tion for unsupervised part-of-speech tagging. In IEEE NLP-KE www.AllWords.com. 2005. English Dictionary and Language Guide. AllSites LLC. www.AllSitesllc.com  
1111
D-Tree Substitution Grammars 
Owen Rambow* 
AT&T Labs-Research 
Dav id  Weir~ 
University of Sussex 
K. V i jay -Shanker t  
University of Delaware 
There is considerable interest among computational linguists in lexicalized grammatical frame- 
works; lexicalized tree adjoining rammar (LTAG) is one widely studied example. In this paper, 
we investigate how derivations in LTAG can be viewed not as manipulations of trees but as 
manipulations of tree descriptions. Changing the way the lexicalized formalism is viewed raises 
questions as to the desirability of certain aspects of the formalism. We present anew formalism, 
d-tree substitution grammar (DSG). Derivations in DSG involve the composition of d-trees, 
special kinds of tree descriptions. Trees are read off rom derived -trees. We show how the DSG 
formalism, which is designed to inherit many of the characterestics of LTAG, can be used to express 
a variety of linguistic analyses not available in LTAG. 
1. Introduction 
There is considerable interest among computational linguists in lexicalized grammati- 
cal frameworks. From a theoretical perspective, this interest is motivated by the widely 
held assumption that grammatical structure is projected from the lexicon. From a prac- 
tical perspective, the interest stems from the growing importance of word-based cor- 
pora in natural anguage processing. Schabes (1990) defines a lexicalized grammar as 
a grammar in which every elementary structure (rules, trees, etc.) is associated with a 
lexical item and every lexical item is associated with a finite set of elementary struc- 
tures of the grammar. Lexicalized tree adjoining grammar (LTAG) (Joshi and Schabes 
1991) is a widely studied example of a lexicalized grammatical formalism. 1 
In LTAG, the elementary structures of the grammar are phrase structure trees. 
Because of the extended omain of locality of a tree (as compared to a context-free 
string rewriting rule), the elementary trees of an LTAG can provide possible syntactic 
contexts for the lexical item or items that anchor the tree, i.e., from which the syntactic 
structure in the tree is projected. LTAG provides two operations for combining trees: 
substitution and adjunction. The substitution operation appends one tree at a frontier 
node of another tree. The adjunction operation is more powerful: it can be used to 
insert one tree within another. This property of adjoining has been widely used in the 
LTAG literature to provide an account for long-distance dependencies. For example, 
* ATT Labs-Research, B233 180 Park Ave, PO Box 971, Florham Park, NJ 07932-0971, USA. E-mail: 
rambow@research.att.com 
t Department ofComputer and Information Science University of Delaware Newark, Delaware 19716. 
E-mail: vijay@udel.edu 
School of Cognitive and Computing Sciences University of Sussex Brighton, BN1 6QH E. Sussex UK. 
E-mail: david.weir@cogs.susx.ac.uk 
1 Other examples of lexicalized grammar formalisms include different varieties of categorial grammars 
and dependency grammars. Neither HPSG nor LFG are lexicalized in the sense of Schabes (1990). 
Computational Linguistics Volume 27, Number 1 
NP S 
Peter NP VP 
John V NP 
I I 
saw e 
fl: S 
NP VP 
you V S 
I 
thought 
NP S 
Peter NP VP 
you V S 
thought NP VP 
I /N  
John V NP 
I I 
saw e 
Figure 1 
Example of adjunction. 
Figure I shows a typical analysis of topicalization. 2 The related nodes for the filler and 
the gap in the elementary tree c~ are moved further apart when the tree 7 is obtained 
by adjoining the auxiliary tree fl within ~. This shows that adjunction changes the 
structural relationship between some of the nodes in the tree into which adjunction 
occurs. 
In LTAG, the lexicalized elementary objects are defined in such a way that the 
structural relationships between the anchor and each of its dependents change during 
the course of a derivation through the operation of adjunction, as just illustrated. This 
approach is not the only possibility. An alternative would be to define the relationships 
between the nodes of the elementary objects in such a way that these relationships 
hold throughout the derivation, regardless of how the derivation proceeds. 
This perspective on the LTAG formalism was explored in Vijay-Shanker (1992) 
where, following the principles of d-theory parsing (Marcus, Hindle, and Fleck 1983), 
LTAG was seen as a system manipulating descriptions of trees rather than as a tree 
2 The same analysis holds for wh-movement, but we use topicalization as an example in order to avoid 
the superficial complication of the auxiliary needed in English questions. Sometimes, topicalized 
sentences sound somewhat less natural than the corresponding wh-questions, which are always 
structurally equivalent. 
88 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
fl': 
NP S 
I 
Peter 
I 
I 
S 
NP VP 
I ' 
John 
I 
t 
VP 
V NP 
I I 
saw e 
S O d : 
NP VP 
I ' 
I 
you VP 
V S 
I 
thought 
Figure 2 
Adjunction example revisited. 
NP S 
Peter NP VP 
I ' ! 
you VP 
V S 
thought NP VP 
I ' 
I 
John VP 
V NP 
I I 
saw e 
rewriting formalism. Elementary objects are descriptions of possible syntactic ontexts 
for the anchor, formalized in a logic for describing nodes and the relationships (dom- 
inance, immediate dominance, linear precedence) that hold between them. 
From this perspective, instead of positing the elementary tree ~ in Figure 1, we can 
describe the projection of syntactic structure from the transitive verb. This description is
presented pictorially as c~  in Figure 2. The solid lines indicate immediate domination, 
whereas the dashed lines indicate a domination of arbitrary length. The description 
a ~ not only partially describes the tree c~ (by taking the dominations to be those of 
length 0) but also any tree (such as "~) that can be derived by using the operations 
of adjunction and substitution starting from c~. In fact, (~t describes exactly what is 
common among these trees. 
By expressing elementary objects in terms of tree descriptions, we can describe 
syntactic structure projected from a lexical item in a way that is independent of the 
derivations in which it is used. This is achieved by employing composition operations 
that produce descriptions that are compatible with the descriptions being combined. 
For instance, adjoining, seen from this perspective, serves to further specify the un- 
derspecified ominations. In Figure 2, the description -y~ is obtained by additionally 
stating that the domination between the two nodes labeled S in c~  is now given by 
the domination relation between the two nodes labeled S in fl~. 
As we will explore in this paper, changing the way the lexicalized formalism is 
viewed, from tree rewriting to tree descriptions, raises questions as to the desirability 
89 
Computational Linguistics Volume 27, Number 1 
S 
NP~ S NP VP 
' I ' ! ! 
many of us VP John VP 
to meet NPi V VP 
I I 
e hopes 
Figure 3 
A problem for LTAG. 
of certain aspects of the formalism. Specifically, we claim that the following two aspects 
of LTAG appear unnecessarily restrictive from the perspective of tree description: 
. 
. 
In LTAG, the root and foot of auxiliary trees must be labeled by the same 
nonterminal symbol. This is not a minor issue since it derives from one 
of the most fundamental principles of LTAG, factoring of recursion. This 
principle states that auxiliary trees express factored out recursion, which 
can be reintroduced via the adjunction operation. It has had a profound 
influence on the way that the formalism has been applied linguistically. 3 
An example of how this can create problems is shown in Figure 3. In this 
case, the "adjoined" tree has a root labeled S and a foot labeled VP, 
something that is not permissible in LTAG. Note that without his 
constraint, he combination would appear to be exactly like adjoining. 
We consider this aspect in more detail in Section 4.1. 
The adjunction operation embeds all of the adjoined tree within that part 
of the tree at which adjunction occurs. This is illustrated in -y' (Figure 2) 
where both parts (separated by domination) of fl~ appear within one 
underspecified domination relationship in c~'. 
The foot node of tree fl in Figure 1 corresponds to a required argument 
of the lexical anchor, thought. The adjunction operation accomplishes the 
role of expanding this argument node. Unlike the substitution operation, 
where an entire tree is inserted below the argument node, with 
adjunction, only a subtree of ~ appears below the argument node; the 
remainder appears in its entirety above the root node of ft. However, if 
we view the trees as descriptions, as in Figure 2, and if we take the 
expansion of the foot node as the main goal served by adjunction, it is 
not clear why the composition should have anything to say about the 
domination relationship between the other parts of the two objects being 
combined. In the description approach, in order to obtain 3/we (in a 
3 Note that in feature-based LTAG there is no restriction that the two feature structures be the same, or 
even that they be compatible. 
90 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
PP S , 
I 
i 
To some of us VP 
V PP 
I I 
appears e 
Figure 4 
Another problem for LTAG. 
VP 
S 
NP VP 
I ' I i 
John VP 
to be happy 
sense to be made precise later) substitute the second component of ~ 
(rooted in S) at the foot node of fl'. This operation does not itself entail 
any further domination constraints between the components of ~P and fl~ 
that are not directly involved in the substitution, specifically, the top 
components of o /and fl'. In the trees described it is possible for either 
one to dominate the other. 4However, adjunction further stipulates that 
the rest of ~' will appear above all of fl'. This additional constraint 
makes certain analyses unavailable for the LTAG formalism (as is well 
known). For instance, given the two lexical projections in Figure 4, the 
subtrees must be interleaved in a fashion not available with adjoining to 
produce the desired result. This aspect of adjoining is the focus of the 
discussion in Section 4.2. 
In this paper, we describe a formalism based on tree descriptions called d-tree 
substitution grammars (DSG). 5 The elementary tree descriptions in DSG can be used 
to describe lexical items and the grammatical structure they project. Each elementary 
tree description can be seen as describing two aspects of the tree structure: one part of 
the description specifies phrase structure rules for lexical projections, and a second part 
of the description states domination relationships between pairs of nodes. DSG inherits 
from LTAG the extended omain of locality of its elementary structures, and, in DSG as 
in LTAG, this extended omain of locality allows us to develop a lexicalized grammar 
in which lexical items project grammatical structure, including positions for arguments. 
But DSG departs from LTAG in that it does not include factoring of recursion as 
a constraint on the makeup of the grammatical projections. Furthermore, in DSG, 
arguments are added to their head by a single operation that we call generalized 
substitution, whereas in LTAG two operations are used: adjunction and substitution. 
DSG is intended to be a simple framework with which it is possible to provide 
analyses for those cases described with LTAG as well as for various cases in which 
extensions of LTAG have been needed, such as different versions of multicomponent 
4 Of course, the node labels further estrict possible dominance in this case. 
5 This paper is based on Rarnbow, Vijay-Shanker, and Weir (1995), where DSG was called DTG (d-tree 
grammar). 
91 
Computational Linguistics Volume 27, Number 1 
Z% {Xl ~ X2, {Ul ~ U2, Xl /k X3, ~3 U3 ~1 /k U3~ X2 -~ 3:3~ U2 "~ ~3~ 
X3 '/~ Y l ,  I I 
yl /k y2, I I Zl /~ Z2, 
Yl /X Y3' O~ O~ z l  /X za' y2 -~ Y3} z2 -~ z3} 
Y3 z3 
Figure 5 
A pair of tree descriptions (which are also d-trees). 
LTAG. Furthermore, because the elementary objects are expressed in terms of logical 
descriptions, it has been possible to investigate the characteristics of the underspecifi- 
cation that is used in these descriptions (Vijay-Shanker and Weir 1999). 
In Section 2, we give some formal definitions and in Section 3 discuss some of 
the formal properties of DSG. In Section 4, we present analyses in DSG for various 
linguistic constructions in several languages, and compare them to the corresponding 
LTAG analyses. In Section 5, we discuss the particular problem of modeling syntactic 
dependency. We conclude with a discussion of some related work and summary. 
2. Def in i t ion of DSG 
D-trees are the primitive elements of a DSG. D-trees are descriptions of trees, in partic- 
ular, certain types of expressions in a tree description language such as that of Rogers 
and Vijay-Shanker (1992). In this section we define tree descriptions and substitution 
of tree descriptions (Section 2.1) and d-trees (Section 2.2) together with some associ- 
ated terminology and the graphical representation (Section 2.3). We then define d-tree 
substitution grammars, along with derivations of d-tree substitution grammars (Sec- 
tion 2.4) and languages generated by these grammars (Section 2.5), and close with an 
informal discussion of path constraints (Section 2.6). 
2.1 Tree Descr ipt ions and Subst i tut ion 
In the following, we are interested in a tree description language that provides at least 
the following binary predicate symbols: A, /~, and -~. These three predicate sym- 
bols are intended to be interpreted as the immediate domination, domination, and 
precedence r lations, respectively. That is, in a tree model, the literal x/~ y would be 
interpreted as node (referred to by the variable) x immediately dominates node (re- 
ferred to by) y, the literal x/~ y would be interpreted such that x dominates y, and 
x -~ y indicates that x is to the left of y. In addition to these predicate symbols, we 
assume there is a finite set of unary function symbols, such as label, which are to be 
used to describe node labeling. Finally, we assume the language includes the equality 
symbol. 
We will now introduce the notion of tree description. 
Def in i t ion 
A tree descr ipt ion is a finite set (conjunction) of positive literals in a tree description 
language. 
In order to make the presentation more readable, tree descriptions are usually pre- 
sented graphically rather than as logical expressions. Figure 5 gives two tree descrip- 
tions, each presented both graphically and in terms of tree descriptions. We introduce 
92 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
x x3 
,p 
/ 
Z3 
Y5 
Figure 6 
A tree description (which is also a d-tree) with three components. 
the conventions used in the graphical representations in more detail in Section 2.3. 
Note that with a functor for each feature, feature structure labels can be specified as 
required. Although feature structures will be used in the linguistic examples presented 
in Section 4, for the remainder of this section we will assume that each node is labeled 
with a symbol by the function label. Furthermore, we assume that these symbols come 
from two pairwise distinct sets of symbols, the terminal and nonterminal labels. (Note 
that the examples in this section do not show labels for nodes, but rather their names, 
while the examples in subsequent sections how the labels.) 
In the following, we consider a tree description to be satisfiable if it is satisfied 
by a finite tree model. For our current purposes, we assume that a tree model will be 
defined as a finite universe (the set of nodes) and will interpret the predicate symbols: 
G, ,~,, and -~ as the immediate domination, domination, and precedence relations, 
respectively. For more details on the notion of satisfiability and the definition of tree 
models, see Backofen, Rogers, and Vijay-Shanker (1995), where the axiomatization of
their theory is also discussed. 6 
We use d ~ d ~ to indicate that the description d~ logically follows from d, in other 
words, that d ~ is known in d. 7 Given a tree description d, we say x dominates y in d if 
d ~ x ,~ y (similarly for the immediate domination and precedes relations). 
We use vars(d) to denote the set of variables involved in the description d. For 
convenience, we will also call the variables in vars(d) the nodes of description d. For a 
tree description d, a node x E vars(d) is a frontier node of d if for all y E vars(d) such 
that x # y, it is not the case that d =~ x ~ y. Only frontier nodes of the tree description 
can be labeled with terminals. A frontier node labeled with a nonterminal is called a 
subst i tut ion node. 
A useful notion for tree description is the notion of components. Given a tree 
description d, consider the binary relation on vars(d) corresponding to the immediate 
domination relations pecified in d; i.e., the relation {(x, Y/ I x, Y E vars(d), d =~ x A y}. 
The transitive, symmetric, reflexive closure of this relation partitions vars(d) into equiv- 
alence classes that we call components. For example, the nodes in the tree descrip- 
tion in Figure 6 fall into the three components: { Xl, x2, x3, x4, x5 }, { yl, y2, y3, y4, Y5 }, 
and { zl,z2, Zg, Z4,Z5 }- In particular, note that y4 and Zl (likewise x3 and z2) are not 
in the same components despite the fact that y4 dominates zl is known in that de- 
6 Note that the symbol /~ in this paper replaces the symbol/~* used in Backofen, Rogers, and 
Vijay-Shanker (1995). 
7 In other words, d =~ d ~ iff d A ~d ~ is not satisfied by any tree model. 
93 
Computational Linguistics Volume 27, Number 1 
0 3 : ~  
{Xl  t 2:2~ 
x l  /k x3, OFX2 ~ X 3 
3:2 "~ X3~ I 
x3/h Yl, I 
Yl /k y2, 
y2 "< Ul, 
Ul \]~ U2~ I?1 
Ul t u3,  ~U 2 0153 
I/,2 "~ u3 
'//'2 \]~' Zl ~ ! I 
Zl \]~ Z2 ~ I 
Zl t z3, Z~Z 1 
z2 "~ Z3} 
Z3 
Figure 7 
Result of substitution by tree description root. 
scription. This is because the reflexive, symmetric, and transitive closure of the im- 
mediate domination relation known in the description will not include these pairs 
of nodes. 
We say that x is the root of a component  if it dominates every node in its com- 
ponent, and we say that x is on the frontier of a component if the only node in its 
component that it dominates i  itself. Note that x can be on the frontier of a component 
of d without being a frontier node of a tree description. For example, in Figure 6, x3 
is a frontier of a component but not a frontier of the tree description. In contrast, z3 is 
both a frontier of a component as well as a frontier of the tree description. We say that 
x is the root of a tree descr ipt ion if it dominates every node in the tree description. 
Note that it need not be the case that every tree description has a root. For example, 
according to the definition of tree descriptions, the description in Figure 6 is a tree de- 
scription and does not have a root. Although we know that either xl or Yl dominates 
all nodes in a tree model of the tree description, we don't know which. 
We can now define the subst i tut ion operat ion on tree descr ipt ions that will be 
used in DSG. We use dl \[y/x\] to denote the description obtained from dl by replacing 
all instances in dl of the variable x by y. 
Def in i t ion 
Let dl and d2 be two tree descriptions. Without loss of generality, we assume that 
vars(dl) N vars(d2) = ?. Let x E vars(dl) be a root of a component of dl and 
y E vars(d2) be a substitution ode in the frontier of d2. Let d be the description 
dl kJ d2\[x/y\]. We say that d is obtained from dl and d2 by subst i tut ing x at y. 
Note that in addition, we may place restrictions on the values of the labeling 
functions for x and y in the above definition. Typically, for a node labeling function such 
as label we require label(x) = label(y), and for functions that return feature structures 
we require unifiability (with the unification being the new value of the feature function 
for y). 
Figure 7 shows the result of substituting the root Ul of the tree description on 
the right of Figure 5 at the substitution ode Y3 of the tree description on the left of 
Figure 5. 
Figure 8 shows the result of substituting a node that is not the root of the tree 
94 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
Xl \]~ .T2~ 
Xl /'x X3, 
x3 ~ y~, ~X2 N~ x3 c~t  ,
Yl A y~, . / 'c 4 Yl fk Zl, I q) U 2 O '//3 
Y2 "~ Zl, I I 
Ul \]k U2~ ~ Y l  i 
Ul ~ U3~ 
u2 -~ ua, (5 Y2 ~Y3:  zl  
U2 /~ Zl~ 
Zl /k Z2, (9 Z2 (9 Z3 
zl /k z3, 
z2 ~ z3} 
Figure 8 
Result of substitution by component root. 
description but the root Zl of a component of the tree description on the right of Figure 5 
at the substitution ode y3 of the tree description on the left of Figure 5. 
2.2 D-Trees 
D-trees are certain types of tree descriptions: not all tree descriptions are d-trees. In 
describing syntactic structure, we are interested in two kinds of primitive tree de- 
scriptions. The first kind of primitive tree description, which we call parent-child 
descriptions, involves n + 1 (n _> 1) variables, say x, xl,. ?., Xn, and in addition to spec- 
ifying categorial information associated with these variables, specifies tree structure of 
the form 
{X & I1 . . . . .  X A Xn, Xl "g X2 . . . . .  Xn-1 "?, Xn} 
A parent-child escription corresponds to a phrase structure rule in a context-free 
grammar, and by extension, to a phrase structure rule in X-bar theory, to the instanti- 
ation of a rule schema in HPSG, or to a c-structure rule in LFG. As in a context-free 
grammar, in DSG we assume the siblings Xl , . . . ,  Xn are totally ordered by precedence, s 
The second kind of primitive description, which we call a dominat ion description, 
has the form {x & y}, where x and y are variables. In projecting from a lexical item to 
obtain the elementary objects of a grammar, this underspecified domination statement 
allows for structures projected from other lexical items to be interspersed during the 
derivation process. 
Definit ion 
A d-tree is a satisfiable description in the smallest class of tree descriptions obtained 
by closing the primitive tree descriptions under the substitution operation. 
For example, Figure 9 shows how the d-tree in Figure 6 is produced by using 
six parent-child escriptions and two domination descriptions. The ovals show cases 
of substitution; the circle represents a case of two successive substitutions. Figure 10 
shows a tree description that is not a d-tree: it is not a parent-child escription, nor 
8 One could, of course, relax this constraint and assume that they are only partially ordered. However, 
for now, we do not consider such an extension. See Section 4.4 for a discussion. 
95 
Computational Linguistics Volume 27, Number 1 
~g4 
X5 I 
X3 
Y2 
Y5 
,s 
J 
J 
2:4 
| f 
Z3 
Figure 9 
Derivation of an elementary d-tree. 
{x & y, o.x 
6y "o z 
y -~ z} 
Figure 10 
A description that is not a d-tree. 
Y~ 
can it be derived from two domination descriptions by substitution, since substitution 
can only occur at the frontier nodes. 
A d-tree d is complete if it does not contain any substitution odes, i.e., all the 
frontier nodes of the description d are labeled by terminals. Given a d-tree d, we say 
that a pair of nodes, x and y (variables in vars(d)), are related by an i-edge if d ::~ xAy .  
We say that x is an i-parent and y is an i-child. Given a d-tree d, we say that a pair of 
nodes, x and y, are related by a d-edge if it is known from d that x dominates y, it is 
not known from d that x immediately dominates y, and there is no other variable in d 
that is known to be between them. That is, a pair of nodes x and y, x ~ y, are related 
byad-edge i fd  ~ x~y,d  ~ xGy,  and for a l l z  E vars(d), i fd ~ (x&zAz~y)  
then z = x or z = y. If x and y are related by a d-edge, then we say that they are 
d-parent and d-child, respectively. Note that a node in a d-tree (unlike a node in a 
tree description) cannot be both an i-parent and a d-parent at the same time. 
2.3 Graphical Presentation of a D-Tree 
We usually find it more convenient to present d-trees graphically. When presenting a 
d-tree graphically, i-edges are represented with a solid line, while d-edges are repre- 
sented with a broken line. All immediate dominance relations are always represented 
graphically, but only the domination relations corresponding to d-edges are shown 
explicitly in graphical presentations. 
By definition of d-trees, each component of a d-tree is fully specified with respect 
to immediate domination. Thus, all immediate domination relations between nodes 
in a component are indicated by i-edges. Also, by definition, components must be 
fully specified with respect o precedence. That is, for any two nodes u and v within 
96 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
a component we must know whether u precedes v or vice versa. In fact, all prece- 
dence information derives from precedence among siblings (two nodes immediately 
dominated by a common node). This means that all the precedence in a description 
can be expressed graphically simply by using the normal left-to-right ordering among 
siblings. 
Another important restriction on d-trees has to do with how components are re- 
lated to one another. As we said above, a frontier node of a component of a d-tree 
can be a d-parent but not an i-parent, and only frontier nodes of a component can 
serve as d-parents. However, by definition, a frontier node of a d-tree can neither be a 
d-parent nor an i-parent. Graphically, this restriction can be characterized as follows: 
edges specifying domination (d-edges) must connect a node on the frontier of a com- 
ponent with a node of another component. Furthermore, nodes on the frontier of a 
component can have at most one d-child. 
Recall that not every set of positive literals involving A, /~, and -~ is a legal d-tree. 
In particular, we can show that a description is a d-tree if and only if it is logically 
equivalent to descriptions that, when written graphically, would have the appearance 
described above. 
2.4 D-Tree Substitution Grammars 
We can now define d-tree substitution grammars. 
Definition 
A d-tree substitution grammar (DSG) G is a 4-tuple (VT, VN, T, ds), where VT and 
VN are pairwise distinct terminal and nonterminal alphabets, respectively, T is a 
finite set of elementary d-trees such that the functor label assigns each node in 
each d-tree in T a label in VT U VN and such that only d-tree frontier nodes take 
labels in VT, and ds is a characterization f the labels that can appear at the root 
of a derived tree. 
Derivations in DSG are defined as follows. Let G = (VT, VN, T, ds) be a DSG. 
Furthermore: 
? Let T0(G) --- T. 
? Let Ti+I = Ti U {d\[d is satisfiable and d results from combining a pair of 
d-trees in Ti by substitution at a node x such that label(x) c VN}. 
The d-tree language T(G) generated by G is defined as follows. 
T (G)={dcT i l i>0 ,  d i scomplete}  
In a lexicalized DSG, there is at least one terminal node on the frontier of every 
d-tree; this terminal is (these terminals are) designated the anchor(s) of the d-tree. 
The remaining frontier nodes (of the description) and all internal nodes are labeled by 
nonterminals. Nonterminal nodes on the frontier of a description are called substitution 
nodes because these are nodes at which a substitution must occur (see below). Finally, 
we say that a d-tree d is sentential if d has a single component and the label of the 
root of d is compatible with ds. 
2.5 Reading D-Trees 
A description d is a tree if and only if it has a single component (i.e., it does not have 
any d-edges). Therefore, the process of reading off trees from d-trees can be viewed as 
97 
Computational Linguistics Volume 27, Number 1 
a nondeterministic process that involves repeatedly removing d-edges until a d-tree 
with a single component results. 
In defining the process of removing a d-edge, we require first, that no i-edges 
be added which are not already specified in the components, and second, that those 
i-edges that are distinct prior to the process of reading off remain distinct after the 
removal of the d-edges. This means that each removal of a d-edge results in equating 
exactly one pair of nodes. These requirements are motivated by the observation that 
the i-edges represent linguistically determined structures embodied in the elementary 
d-trees that cannot be created or reduced uring a derivation. 
We now define the d-edge removal algorithm. A d-edge represents a domination 
relation of length zero or more. Given the above requirements, at the end of the 
composition process, we can, when possible, get a minimal reading of a d-edge to 
be a domination relation of length zero. Thus, we obtain the following procedure for 
removing d-edges: Consider ad-edge with a node x as the d-parent and with a d-child 
y. By definition of d-trees, x is on the frontier of a component. The d-child y can either 
be a root of a component or not. Let us first consider the case in which y is a root of 
a component. To remove this d-edge, we equate x with y.9 This gives us the minimal 
reading that meets the above requirement ( hat no i-edges are added which are not 
already specified in the components, and that those i-edges that are distinct prior to 
the process of reading off remain distinct after the removal of the d-edge). Now we 
consider the alternate case in which the d-child is not the root of its component. Let z 
be the root of the component containing y. Now both z and x are known to dominate 
y and hence in any model of the description, either z will dominate x or vice versa. 
Equating x with y (the two nodes in the d-edge under consideration) has the potential 
of requiring the collapsing of i-edges (e.g., i-edges between x and its parent and y and 
its parent in the component including z). As a consequence of our requirement, the 
only way to remove the d-edge is by equating the nodes x and z. If we equated x
with any other node dominated by z (such as y), we would also be collapsing i-edges 
from two distinct components and equating more than one pair of nodes, contrary to 
our requirement. The removal of the d-edge by equating x and z can also be viewed 
as adding a d-edge from x to z (which, as mentioned, is compatible with the given 
description and does not have the potential for collapsing i-edges). Now since this d- 
edge is between a frontier of a component and the root of another, it can be removed 
by equating the two nodes. 
Definition 
A tree t can be read off from a d-tree d iff t is obtained from d by removing the d-edges 
of d in any order using the d-edge removal algorithm. 
By selecting d-edges for removal in different orders, different rees can be pro- 
duced. Thus, in general, we can read off several trees from each d-tree in T(G). For 
example, the d-tree in Figure 6 can produce two trees: one rooted in xl (if we choose 
to collapse the edge between y4 and zl first) and one rooted in yl (if we choose to col- 
lapse the edge between x3 and z2 first). The fact that a d-tree can have several minimal 
readings can be exploited to underspecify different word orderings (see Section 4.4). 
9 This additional equality to obtain the minimal  readings is similar to unification of the so-called top and 
bottom feature structures associated with a node in tree adjoining grammars,  which happens at the end 
of a derivation. In DSG, if the labeling specifications on x and y are incompatible, then the addit ional 
equality statement does not lead to any minimal  tree model, just as in TAG, a derivation cannot 
terminate if the top and bottom feature structures associated with a node do not unify. 
98 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
Thus, while a single d-tree may describe several trees, only some of these trees 
will be read off in this way. This is because of our assumptions about what is being 
implicitly stated in a d-tree--for example, our requirement that i-edges can be neither 
destroyed nor created in a derivation. Assumptions uch as these about the implicit 
content of d-trees constitute a theory of how to read off from d-trees. Variants of the 
DSG formalism can be defined, which differ with respect o this theory. 
We now define the tree and string languages of a DSG. 
Definition 
Let G be a DSG. The tree language T(G) generated by G is the set of trees that can be 
read off from sentential d-trees in T(G). 
Definition 
The string language generated by G is the set of terminal strings on the frontier of 
trees in T(G). 
2.6 DSG with Path Constraints 
In DSG, domination statements are used to express domination paths of arbitrary 
length. There is no requirement placed on the nodes that appear on such paths. In this 
section, we informally define an extension to DSG that allows for additional statements 
constraining the paths. 
Path constraints can be associated with domination statements o constrain which 
nodes, in terms of their labels, can or cannot appear within a path instantiating a
d-edge. 1? Path constraints do not directly constrain the length of the domination path, 
which still remains underspecified. Path constraints are specified in DSG by associat- 
ing with domination statements a set of labels that defines which nodes cannot appear 
within this path. u Suppose we have a statement x A y with an associated path con- 
straint set, P, then logically this pair can be understood as x A y A Vz(z ~ x A z y~ 
y AxAzAzA y) ~ label(z) ~P.  
Note that during the process of derivation involving substitution, the domination 
statements in the two descriptions being composed continue to exist and do not play 
any role in the composition operation itself. The domination statements only affect 
the reading off process. For this reason, we can capture the effect of path constraints 
by merely defining how they affect the reading off process. Recall that the reading off 
process is essentially the elimination of d-edges to arrive at a single component d-tree. 
If there is a d-edge between x and y, we consider two situations: is the d-child y the 
root of a component, or not? When y is the root of a component, then x and y are 
collapsed. Clearly any path constraint on this d-edge has no effect. However, when y 
is not the root of a component, and z is the root of the component containing y, then 
the tree we obtain from the reading off process is one where x dominates z and not 
where z properly dominates x. That is, in this case, we replace the d-edge between 
x and y with a d-edge between x and z, which we then eliminate in the reading off 
process by equating x and z. But in order to replace the d-edge between x and y with 
a d-edge between x and z, we need to make sure that the path between z and y does 
not violate the path constraint associated with the d-edge between x and y. 
10 In Rambow, Vijay-Shanker, and Weir (1995), path constraints are called "subsertion-insertion 
constraints." 
11 Rambow (1996) uses regular expressions to specify path constraints. 
99 
Computational Linguistics Volume 27, Number 1 
A A 
a A a A 
I I 
I I 
, o 
B B 
b B b B 
I I 
! I 
i i 
C C 
e 6' 
Figure 11 
Counting to three: A derivation. 
A 
a B 
I 
I 
i 
B 
/N  
b C 
i 
I 
i 
C 
C 
3. Properties of the Languages of DSG 
It is clear that any context-free language can be generated by DSG (a context-free 
grammar can simply be reinterpreted as a DSG). It is also easy to show that the weak 
generative capacity of DSG exceeds that of context-free grammars. Figure 11 shows 
three d-trees (including two copies of the same d-tree) that generate the non-context- 
free language { anb'c" In > 1 }. Figure 12 shows the result of performing the first of 
two substitutions indicated by the arrows (top) and the result of performing both 
substitutions (bottom). Note that although there are various ways that the domination 
edges can be collapsed when reading off trees from this d-tree, the order in which 
we collapse domination edges is constrained by the need to consistently label nodes 
being equated. This is what gives us the correct order of terminals. 
Figure 13 shows a grammar for the language 
{ w E { a, b, c }* \] w has an equal nonzero number of a's, b's and c's }, 
which we call Mix. This grammar is very similar to the previous one. The only differ- 
ence is that node labels are no longer used to constrain word order. Thus the domi- 
nation edges can be collapsed in any order. 
Both of the previous two examples can be extended to give a grammar for strings 
containing an equal number of any number of symbols imply by including additional 
components in the elementary d-trees for each symbol to be counted. Hence, DSG 
generates not only non-context-free languages but also non-tree adjoining languages, 
since LTAG cannot generate the language { anbncndnen i n _~ 1 } (Vijay-Shanker 1987). 
However, it appears that DSG cannot generate all of the tree adjoining languages, 
and we conjecture that the classes are therefore incomparable (we offer no proof of 
this claim in this paper). It does not appear to be possible for DSG to generate the 
copy language { ww \[ w c { a, b }* }. Intuitively, this claim can be motivated by the 
observation that nonterminal labels can be used to control the ordering of a bounded 
number of terminals (as in Figure 12), but this cannot be done in an unbounded way, 
as would be required for the copy language (since the label alphabet is finite). 
100 
Rainbow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
A A 
a A a A 
t I 
I I 
o n 
B B 
b B b B 
! / 
o / 
c 6' 
c 6' 
A A A 
a A a A a B 
I I ! 
I I I 
i n i 
B B B 
b B b B b C 
| d / / t 
I / . , . I  
I / 
C / ., / t 
/ 1 1 / 
f / 
s j 
i 
i 
e C 
I 
C 
Figure 12 
Counting to three: After substituting one tree (above) and the derived d-tree (below). 
DSG is closely related (and weakly equivalent) to two equivalent string rewriting 
systems, UVG-DL and {}-LIG (Rainbow 1994a, 1994b). In UVG-DL, several context- 
free rewrite rules are grouped into a set, and dominance links may hold between 
101 
Computational Linguistics Volume 27, Number 1 
S S S S 
a S b S c S a S 
S < 
S S 
b S c S 
:" S 
I 
? 
S 
a S 
S S S S S 
b S c S a S b S c S 
S 
I 
Figure 13 
A grammar for Mix. 
right-hand-side nonterminals and left-hand-side nonterminals of different rules from 
the same set. In a derivation, the context-free rules are applied as usual, except hat all 
rules from an instance of a set must be used in the derivation, and at the end of the 
derivation, the dominance links must correspond to dominance relations in the deriva- 
tion tree. {}-LIG is a multisebvalued variant of Linear Index Grammar (Gazdar 1988). 
UVG-DL and {}-LIG, when lexicalized, generate only context-sensitive languages. 
Finally, Vijay-Shanker, Weir, and Rainbow (1995), using techniques developed for 
UVG-DL (Rainbow 1994a; Becker and Rambow 1995), show that the languages gen- 
erated by lexicalized DSG can be recognized in polynomial time. This can be shown 
with a straightforward extension to the usual bottom-up dynamic programming algo- 
rithm for context-free grammars. In the DSG case, the nonterminals in the chart are 
paired with multisets. The nonterminals are used to verify that the immediate dom- 
inance relations (i.e., the parent-child escriptions) hold, just as in the case of CFG. 
The multisets record the domination descriptions whose lower (dominated) node has 
been found but whose upper (dominating) node still needs to be found in order for 
the parse to find a valid derivation of the input string (so-called open domination 
descriptions). The key to the complexity result is that the size of the multisets is lin- 
early bounded by the length of the input string if the grammar is lexicalized, and the 
number of multisets of size n is polynomial in n. Furthermore, if the number of open 
domination descriptions in any chart entry is bounded by some constant independent 
102 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
of the length of the input string (as is plausible for many natural anguages including 
English), the parser performs in cubic time. 
4. Some Linguistic Analyses with DSG 
In Section 1, we saw that the extended omain of locality of the elementary structures 
of DSG--which DSG shares with LTAG--allows us to develop lexicalized grammars 
in which the elementary structures contain lexical items and the syntactic structure 
they project. There has been considerable research in the context of LTAG on the is- 
sue of how to use the formalism for modeling natural anguage syntax--we mention 
as salient examples XTAG-Group (1999), a wide-coverage grammar for English, and 
Frank (1992, forthcoming), an extensive investigation from the point of view of theo- 
retical syntax. Since DSG shares the same extended omain of locality as LTAG, much 
of this research carries over to DSG. In this section, we will be presenting linguis- 
tic analyses in DSG that follow some of the elementary principles developed in the 
context of LTAG. We will call these conventions the standard LTAG practices and 
summarize them here for convenience. 
? Each elementary structure contains a lexical item (which can be 
multiword) and the syntactic structure it projects. 
? Each elementary structure for a syntactic head contains yntactic 
positions for its arguments. (In LTAG, this means substitution or foot 
nodes; in DSG, this means substitution odes.) 
? When combining two elementary structures, a syntactic relation between 
their lexical heads is established. For example, when substituting the 
elementary structure for lexical item ll into an argument position of the 
elementary structure for lexical item 12, then ll is in fact an argument 
of 12. 
In Section 1 we also saw that the adjoining operation of LTAG has two properties 
that appear arbitrary from a tree description perspective. The first property is the 
recursion requirement, which states that the root and foot of an auxiliary tree must 
be identically labeled. This requirement embodies the principle that auxiliary trees 
are seen as factoring recursion. The second property, which we will refer to as the 
nesting property of adjunction, follows from the fact that the adjoining operation 
is not symmetrical. All the structural components projected from one lexical item 
(corresponding to the auxiliary tree used in an adjoining step) are included entirely 
between two components in the other projected structure. That is, components of only 
one of the lexically projected structures can get separated in an adjoining step. 
In this section, we examine some of the ramifications of these two constraints 
by giving a number of linguistic examples for which they appear to preclude the 
formulation of an attractive analysis. We show that the additional flexibility inherent in 
the generalized substitution operation is useful in overcoming the problems that arise. 
4.1 Factoring of Recursion 
We begin by explaining why, in LTAG, the availability of analyses for long-distance 
dependencies is limited by the recursion requirement. Normally, substitution is used 
in LTAG to associate a complement to its head, and adjunction is used to associate 
a modifier. However, adjunction rather than substitution must be used with com- 
plements involving long-distance dependencies, e.g., in wh-dependencies and raising 
103 
Computational Linguistics Volume 27, Number 1 
S 
NP~ S NP VP 
I I 
many of us S John VP 
NP VP V S 
PRO VP hopes 
to meet NPi 
I 
e 
Figure 14 
S-analysis for extraction from infinitival complements. 
constructions. Such auxiliary trees are called predicative auxiliary trees. 12 In a pred- 
icative auxiliary tree, the foot node should be one of the nonterminal nodes on the 
frontier that is included ue to argument requirements of the lexical anchor of the tree 
(as determined by its active valency). However, the recursion requirement means that 
all frontier nonterminal nodes that do not have the same label as the root node must 
be designated as substitution nodes, which may mean that no well-formed auxiliary 
tree can be formed. 
Let us consider again the topicalized sentence used as an example in Section 1, 
repeated here for convenience: 
(1) Many of us, John hopes to meet 
A possible analysis is shown in Figure 3 in Section 1. We will refer to this anal- 
ysis as the VP-complement analysis. Note that the individual pieces of the structures 
projected from lexical items follow standard LTAG practices. Because of the recursion 
requirement, the tree on the right is not (a description of) an auxiliary tree. To obtain 
an auxiliary tree in order to give a usual TAG-style account of long-distance depen- 
dencies, the complement of the equi-verb (control verb) hopes must be given an S label, 
which in turn imposes a linguistic analysis using an empty (PRO) subject as shown 
in Figure 14 (or, at any rate, an analysis in which the infinitival to meet projects to S). 
The VP-complement analysis has been proposed within different frameworks, and 
has been adopted as the standard analysis in HPSG (Pollard and Sag 1994). However, 
because this would require an auxiliary tree rooted in S with a VP foot node, the 
recursion requirement precludes the adoption of such an analysis in LTAG. We are 
12 This term is from Schabes and Shieber (1994). Kroch (1987) calls such trees complement auxiliary trees. 
104 
Rainbow, Vijay-Shanker, and Weir D-Tree Substitution Grammars  
NPi S 
/ / x , ,x  ' I I 
i i 
John PP VP 
P NPi V NP PP 
I I 
to e gave the book 
Figure 15 
HPSG analysis of give expressed as trees. 
S 
NP VP 
I 
Peter 
not suggesting that one linguistic analysis is better than another, but instead we point 
out that the formal mechanism ofLTAG precludes the adoption of certain linguistically 
motivated analyses. Furthermore, this mechanism akes it difficult to express entire 
grammars originally formulated in other formalisms in LTAG; for example, when 
compiling a fragment of HPSG into TAG (Kasper et al 1995). In fact, the compilation 
produces tructures just like those (described) in Figure 3. Kasper et al (1995) consider 
the tree on the right of Figure 3 to be an auxiliary tree with the VP sibling of the anchor 
determined tobe the foot node. Technically, the tree on the right of Figure 3 caimot be 
an auxiliary tree. Kasper et al (1995) overcome the problem by making the node label 
a feature (with all nodes having a default label of no significance). This determination 
of the foot node is independent of the node labels of the frontier nodes. Instead, the 
foot node is chosen because it shares certain crucial features (other than label!) with 
the root node. These shared features are extracted from the HPSG rule schema nd 
are used to define the localization of dependencies in the compiled TAG grammar. See 
Kasper et al (1995) for details. 
A similar example involves analyses for sentences such as (2), which involve ex- 
traction from argument PPs. 
(2) John, Peter gave the book to 
Figure 15 shows the structures obtained by using the method of Kasper et al (1995) 
for compiling an HPSG fragment to TAG-like structures. In contrast to traditional TAG 
analyses (in which the elementary tree contains the preposition and its PP, with the 
NP complement of the preposition as a substitution ode), the PP argument of the 
ditransitive verb is not expanded. ~3Instead the PP tree anchored by the preposition 
is substituted. However, because of the extraction, DSG's notion of substitution rather 
than LTAG substitution would need to be used. 
These examples uggest hat the method for compiling an HPSG fragment into 
TAG-like structures discussed in Kasper et al (1995) can be simplified by compiling 
HPSG to a DSG-like framework. 
13 Recall that we are not, in this section, advocating one analysis over another; rather, we are discussing 
the range of options available to the syntactician working in the TAG framework. 
105 
Computational Linguistics Volume 27, Number 1 
NPi S / , , ,  , 
I 
i 
This painting NP 
DET KI 
a N PP 
copy P 
I 
ol 
Figure 16 
Extraction from picture-NPs. 
S 
NP VP 
I ' I i 
John VP 
V NP 
I 
bought 
NP~ 
I 
e 
We have shown a number of examples where some, but not all, of the possible 
linguistic analyses can be expressed in LTAG. It could be claimed that a formal frame- 
work limiting the range of possible analyses constitutes a methodological dvantage 
rather than a disadvantage. However, as is well known, there are several other exam- 
ples in English syntax where the factoring of recursion requirement in fact eliminates 
all plausible LTAG analyses. The only constraint assumed here is that extraction is 
localized in elementary trees. One such example in English is extraction out of a "pic- 
ture-NP" (a noun which takes a prepositional complement from which extraction i to 
the main sentence is possible), as illustrated in the following example: 
(3) This painting, John bought a copy of 
Following the standard LTAG practices, we would obtain the structures described 
in Figure 16. As these descriptions show, the recursion constraint means that adjoining 
cannot be used to provide this analysis of extraction out of NPs. See Kroch (1989) for 
various examples of such constructions in English and their treatment using an exten- 
sion of TAG called multicomponent tree adjoining rammars. (We return to analyses 
using multicomponent TAG in Section 4.2.) 
However, we now show that all of these cases can be captured uniformly with 
generalized substitution (see Figure 17). The node labeled X in fl arises due to the 
argument requirements of the anchor (the verb) and when X = S, fl is a predica- 
tive auxiliary tree in LTAG. The required derived phrase structure in these cases is 
described by 7. To obtain these trees, it would suffice to simply substitute the compo- 
nent rooted in X of c~ at the node labeled X in ft. While in general, such a substitution 
would not constrain the placement of the upper component of t ,  because of the labels 
of the relevant nodes, this substitution will always result in % The use of substitution 
at argument nodes not only captures the situations where adjoining or multicompo- 
106 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
g t :  S v g 
YPi S NP VP YPi S 
! I 
i i 
X VP NP VP 
V X V X 
J 
e 
e 
Figure 17 
General case of extraction. 
nent adjoining is used for these examples, it also allows the DSG treatment to be 
uniform, and is applicable ven in cases where there is no extraction (e.g., the upper 
component of a is not present). 
We end this discussion of the nature of foot nodes by addressing the question 
of how the choice of foot nodes limits illicit extractions. In the TAG approach, the 
designation of a foot node specifically rules out extraction from any structure that 
gets attached to any other frontier node (other arguments), or from structures that 
are adjoined in (adjuncts). However, as has been pointed out before (Abeill6 1991), the 
choice of foot nodes is not always determined by node labels alone, for example in the 
presence of sentential subjects or verbs such as ddduire, which can be argued to have 
two sentential objects. In these cases some additional linguistic riteria are needed in 
order to designate the foot node. These same linguistic riteria can be used to designate 
frontier nodes from which extraction is possible; extraction can be regulated through 
the use of features. We also note that in moving to a multicomponent TAG analysis, an 
additional regulatory mechanism becomes necessary in any case to avoid extractions 
out of subjects (and, to a lesser degree, out of adjuncts). We refer the interested reader 
to Rainbow, Vijay-Shanker, and Weir (1995) and Rambow and Vijay-Shanker (1998) for 
a fuller discussion. 
4.2 Interspersing of Components 
We now consider how the nesting constraint of LTAG limits the TAG formalism as a 
descriptive device for natural language syntax. We contrast this with the case of DSG, 
which, through the use of domination in describing elementary structures projected 
from a lexical item, allows for the interleaving of components projected from lexical 
items during a derivation. 
Consider the raising example introduced in Section 1 repeated here as (4a), along 
with its nontopicalized version (4b), which indicates a possible original position for 
107 
Computational Linguistics Volume 27, Number 1 
g 
PPi S /, , , ,  , 
I 
i 
To many of us VP 
S 
I ' I t
John VP 
V PP VP to be happy 
I I 
appears e 
Figure 18 
Topicalization out of the clause of a raising verb. 
the topicalized phrase) 4
(4) a. To many of us, John appears to be happy 
b. John appears to many of us to be happy 
Following standard LTAG practices of localizing argument structure (even in the 
presence of topicalization) and the standard LTAG analysis for the raising verb appear, 
the descriptions shown in Figure 18 could be proposed. Because of the nesting property 
of adjunction, the interleaving required to obtain the relevant phrase structure for the 
sentence (4a) cannot be realized using LTAG with the assumed lexical projections (or 
any other reasonable structures where the topicalized PP and the verb appear are in 
the same projected structure). In contrast, with these projections, using generalized 
substitution in DSG (i.e., equating the VP argument node of the verb and the root of 
the infinitival VP), the only possible derived tree is the desired one. 
We will now consider an example that does not involve a wh-type dependency: 
(5) Didn't John seem to like the gift? 
Following the principles laid out in Frank (1992) for constructing the elementary 
trees of TAG, we would obtain the projections described in Figure 19 (except for the 
node labels). Note in particular the inclusion of the auxiliary node with the cliticized 
negation marker in the projection of the raising verb seem. Clearly the TAG opera- 
tions could never yield the necessary phrase structure given this localization. Once 
again, the use of generalized substitution in DSG would result in the desired phrase 
structure. 
An alternative to the treatment in Frank (1992) is implemented in the XTAG gram- 
mar for English (XTAG-Group 1999) developed at the University of Pennsylvania. The 
XTAG grammar does not presuppose the inclusion of the auxiliary in the projection 
of the main verb. Rather, the auxiliary gets included by separately adjoining a tree 
14 Throughout this section, we underline the embedded clause with all of its arguments, such as here, the 
raised subject. 
108 
Rainbow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
I 
Didn't 
V VP 
I 
seem 
Figure 19 
Raising verb with a fronted auxiliary. 
S S 
I I I i i 
VP John VP 
to like the gift 
projected from the auxiliary verb. The adjunction of the auxiliary is forced through 
a linguistically motivated system of features. A treatment such as this is needed to 
avoid using multicomponent adjoining. In our example, the auxiliary, along with the 
negation marker, is adjoined into the tree projected by the embedded verb l ike, which 
may be considered undesirable since semantically, it is the matrix verb seem that is 
negated. We take this example to show once more that TAG imposes restrictions on 
the linguistic analyses that can be expressed in it. Specifically, there are constructions 
(which do not involve long-distance phenomena) for which one of the most widely 
developed and comprehensive theories for determining the nature of localization in 
elementary trees--that of Frank (1992)---calmot be used because of the nature of the 
TAG operation of adjunction. In contrast, the operations of DSG allow this theory of 
elementary lexical projections to be used. 
In English, the finite verb appears before the subject only in questions (and in 
some other contexts uch as neg-inversion), but in other languages, this word order 
is routine, leading to similar problems for an LTAG analysis. In V1 languages uch 
as Welsh, the subject appears in second position after the finite verb in the standard 
declarative sentence. The raised subject behaves in the same manner as the matrix 
subject, as observed in Harley and Kulick (1998) and illustrated in (6), from Hen- 
drick (1988): 
(6) a. Mae 
Is 
John 
b. Mae 
Is 
John 
Si6n yn gweld Mair 
John seeing Mary 
is seeing Mary 
Si6n yn digwydd bod yn gweld Mair 
John happening be seeing Mary 
happens to be seeing Mary 
In German, a V2 language, the finite verb appears in second position in matrix 
clauses. The first position may be occupied by any constituent (not necessarily the 
subject). When the subject is not in initial position, it follows the finite verb, both in 
109 
Computational Linguistics Volume 27, Number 1 
S 
NP VP 
I ' 
John 
NPi S 
I 
i 
, Which bridge VP 
u 
I VP PP 
v 
I P NPi 
sleep I I 
Figure 20 
Licit extraction from an adjunct in English. 
under e 
simplex sentences and in raising constructions: 
(7) a. Leider wird es standig regnen 
unfortunately will itNOM continually rain 
Unfortunately, it will rain continually 
b. Oft schien e_~s uns st~indig zu regnen 
often seemed itNo M USDA w continually to rain 
Often it seemed to us to rain continually 
In the German example, a separate adjunction of the tensed verb (as in the XTAG 
analysis of the English auxiliary) is not a viable analysis at all, since the tensed verb 
is not an auxiliary but the main (raising) verb of the matrix clause. 
We now return to examples that do not include raising, but only wh-dependencies. 
(8) a. John slept under the bridge 
b. Which bridge did John sleep under? 
Most LTAG analyses would treat he prepositional phrase in (8a) as an adjunct and 
use an intransitive frame for the verb. However, the related sentence (8b) cannot be 
analyzed with TAG operations in the same way, because the projected structures from 
the verb and the preposition would have to be as shown in Figure 20. The interspersing 
of components from these projections to obtain the desired tree cannot be obtained 
using adjoining. Clearly, with the appropriate generalized substitutions in DSG, this 
tree alone will be derived with these lexical projections. 
Related problems arise in languages in which a wh-moved element does not in- 
variably appear in sentence-initial position, as it does in English. For example, in 
110 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
Kashmiri, the wh-element ends up in second position in the presence of a topic. This 
is the case even if the wh-element comes from the embedded clause and the topic from 
the matrix clause. (The data is from Bhatt, \[1994\].) 
(9) a. rameshan kyaa dyutnay tse 
RameShERc whatNoM gave yOUDA T 
What did you give Ramesh? 
b. rameshan kyaai chu baasaan ki me kor ti 
RameShERc what is belieVeNPERF that IERC do 
What does Ramesh believe that I did? 
Another example comes from Rumanian. Rumanian differs from English in that it 
allows multiple fronted wh-elements in the same clause. Leahu (1998) illustrates this 
point with the examples in (10) (her (8a) and (11a)); (10a) shows multiple wh-movement 
in the same clause, while (10b) shows multiple wh-words in one clause that originate 
from different clauses, resulting again in an interspersed order. 
(10) a. Cinei cuij ti promite o masina tj? 
who to whom promises a car 
Who promises a car to whom? 
b. Cinei pe cinej a zis ti ca a vazut tj? 
who whom has said that has seen 
Who has said he has seen whom? 
The examples discussed in this section show a range of syntactic phenomena in
English and in other languages that cannot be analyzed using the operations of TAG. 
We conclude that complex interspersing is a fairly common phenomenon i natu- 
ral language. As in the case of factoring of recursion, sometimes we find that the 
definition of adjunction precludes certain linguistically plausible analyses but allows 
others; in other cases, TAG does not seem to allow any linguistically plausible anal- 
ysis at all. However, in each case, we can use standard LTAG practices for projecting 
structures from lexical items and combine the resulting structures using the general- 
ized substitution operation of DSG to obtain the desired analyses, thus bringing out 
the underlying similarity of related constructions both within languages and cross- 
linguistically. 
4.3 Linguistic Use of Path Constraints 
In the examples discussed so far, we have not had the need to use path constraints. 
The d-edges een so far express any domination path. Recall that path constraints can 
be associated with a d-edge to express certain constraints on what nodes, in terms of 
their labels, cannot appear within a path instantiating a d-edge. 
111 
Computational Linguistics Volume 27, Number 1 
S 
NP VP 
I ' 
I 
I 
wood I 
I 
I 
I 
I 
I 
I 
I 
" ' " ' " ' . . .  ok 
? ' ' ? ' . o  ? o 
I path cgnstr,aint: I ', 
no ~ nocle 
%. 
S 
! 
i 
I 
....." ~ ' " ' . . .ok .  
V S 
' l Seem8 
S 
? ,~ .  I 
I "~"  
I 
VP 
V Vp 
' T 
appears 
VP 
NP 
I 
it 
~tc ??  
S 
VP 
to float 
Figure 21 
Path constraints are needed to rule out ungrammatical super-raising. 
As an example of the use of path constraints, let us consider the well-known case 
of "super-raising": 
(11) a. It seems wood appears to float 
b. *Wood seems it appears to float 
c. Wood seems to appear to float 
In (11a), the subject of float, wood, has raised to the appears clause, while the raising 
verb seem does not trigger raising and has an expletive it as its subject? In (11b), 
wood has raised further, and appear now has an expletive subject; (11b) is completely 
ungrammatical. If we make the intermediate raising verb appear nonfinite (and hence 
without a subject), as in (11c), the sentence is again grammatical. 
Now consider the DSG analysis for (11a) shown in Figure 21. The d-tree for seem 
has an S substitution node, since seems takes a finite complement with a subject? Appear, 
112 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
since it is finite, projects to S, but takes a VP complement since its complement, he 
float clause, is nonfinite and has no overt subject, is We furthermore assume that the 
raising verbs seem and appear do not select for subjects, but that the expletive subject 
it is freely available for inclusion in their d-trees, since expletive it is semantically 
vacuous and merely fulfills syntactic requirements ( uch as subject-verb agreement), 
not semantic ones. We substitute the float d-tree into the appear d-tree, and the result 
into the seem d-tree, as indicated by the solid arrows in Figure 21. Given the reading 
off process, this derived d-tree can be seen to express two possibilities, depending 
on where the wood component and the expletive it end up. These two possibilities 
correspond to (11a) and (11b). 
To exclude the ungrammatical result, we use the path constraints discussed in 
Section 2.6. Let us make the uncontroversial ssumption that as we project from a 
verb, we will project to a VP before projecting to an S. But we will interpret his 
notion of projection as also applying to the d-edges between nodes labeled VP: we 
annotate the d-edge between the VP nodes in the float tree (and in fact in all trees, of 
course) as having a path constraint that does not allow an S node on this path. This 
is, after all, what we would expect in claiming that the float tree represents a structure 
lexically projected from the verb float. 16 Given this additional grammatical expression, 
after the substitution at the S node of the seems tree, it is no longer possible to read off 
from the d-tree in Figure 21 a tree whose yield is the ungrammatical (11b). The only 
possible way of reading off from the derived d-tree yields (11a). 
What is striking is that this particular path constraint disallowing S nodes between 
VP nodes in structures projected from a verb can be used in other cases as well. In fact, 
this same path constraint on its own, when applied to the English examples considered 
so far, predicts the correct arrangement of all components among the two d-trees 
being combined, regardless of whether the nesting constraint of adjoining must be met 
(extraction out of clausal or VP complements, extraction from NP or PP complements), 
or not (extraction from the clause of a raising verb, raising verb with fronted auxiliary, 
or extraction from an adjunct). For example, in Figure 18, after substituting the to be 
happy component at the VP node of the appears d-tree, a path constraint on the d-edge 
between the two VP nodes of the to be happy tree makes it impossible for the to any 
of us component to intervene, thus leaving the interspersed tree as the only possible 
result of the reading off process, even if we relaxed the requirement on label equality 
for the removal of d-edges during the reading off process. 
Note that while the same path constraints apply in all cases, in LTAG, as we have 
seen, the nesting constraint of adjoining precludes deriving the correct order in some 
cases, and the use of extensions such as mult icomponent adjoining has been suggested. 
In fact, because there are both situations in which the arrangement of components of 
the lexically projected structures corresponds to adjoining and situations in which 
this arrangement is inappropriate, Vijay-Shanker (1992) raises the question of whether 
the definition of the formalism should limit the arrangement of components of the 
lexically projected structures, or whether the possible arrangements should be derived 
from the linguistic theory and from intuitions about the nature of the elementary 
objects of a grammar. This subsection partially addresses this question and shows 
15 The point we are making in this section relies on there being some distinction between the labels of the 
roots of the appear and float clauses, a linguistically uncontroversial assumption. Here, we use the 
categorial distinction between Sand VP for convenience only; we could also have assumed a difference 
in feature content. 
16 Bleam (2000) uses informal path constraints inmuch the same way in order to restrict Spanish clitic 
climbing in an LTAG analysis. 
113 
Computational Linguistics Volume 27, Number 1 
how the path constraint expressing the nature of projection from a lexical item can be 
used to derive the arrangements of components corresponding to adjoining in some 
cases as well as predict when the nesting condition of adjoining is too limiting in the 
others. 
4.4 Underspecification of Linear Precedence 
In our proposed tree description language, we provide for underspecified dominance 
but not for underspecified linear precedence. As a consequence, in the graphical repre- 
sentations of d-trees, we assume that sister nodes are always ordered as shown. This 
may seem arbitrary at first glance, especially since in many linguistic frameworks 
and theories it is common to specify linear precedence (LP) separately from syntactic 
structure (GPSG, HPSG, LFG, ID/LP-TAG \[Joshi 1987\] and FO-TAG \[Becker, Joshi, 
and Rambow 1991\], various dependency-based formalisms, and so on). This separate 
specification of LP rules allows for underspecified LP rules, which is useful in cases 
in which word order is not fully fixed. 
In principle, an underspecification f LP could easily be added to DSG without 
profoundly changing its character or formal properties. The reason we have not done 
so is that in all cases, the same effect can be achieved using underspecified dominance 
alone, though at the cost of forcing a linguistic analysis that uses binary branching 
phrase structure trees rather than n-ary branching ones. We will illustrate the point 
using examples from German, which allows for scrambling of the arguments. 
Consider the following German examples. 17
(12) a. dat~ die Kinder dem Lehrer das Buch geben 
that \[the children\]NOM \[the teacher\]DAT \[the book\]Acc give 
that the children give the teacher the book 
b. dat~ dem Lehrer die Kinder das Buch geben 
c. dat~ dem Lehrer das Buch die Kinder geben 
All orders of the three arguments are possible, resulting in six possible sentences 
(three of which are shown in (12)). In DSG, we can express this by giving the lexical 
entry for geben shown in Figure 22. TM The arguments of the verb have no dominance 
specified among them, so that when using this d-tree (which is of course not yet a 
tree) in a derivation, we can choose whichever dominance relations we want when 
we read off a tree at the end of the derivation. As a result, we obtain any ordering of 
the arguments. 
As mentioned previously, while we can derive any ordering, we cannot, in DSG, 
obtain a flat VP structure. However, our analysis has an advantage when we consider 
"long scrambling," in which arguments from two lexical verbs intersperse. (In German, 
only certain matrix verbs allow long scrambling.) If we have the subject-control verb 
versuchen 'to try', the nominative argument is the overt subject of the matrix clause, 
while the dative and accusative arguments are arguments of the embedded clause. 
Nonetheless, the same six word orders are possible (we again underline the embedded 
17 We give embedded clauses tarting with the complementizer in order to avoid the problem of V2. For 
a discussion of V2 in a framework like DSG, see Rambow (1994a) and Rambow and Santorini (1995). 
18 We label all projections from the verb (except the immediate preterminal) VP. We assume that relevant 
levels of projection are distinguished by the feature content of the nodes. This choice has mainly been 
made in order to allow us to derive verb-second matrix clause order using the same d-trees, which is 
also why the verb is in a component of its own. 
114 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
VP VP VP 
NPNoM VP NPAcc VP NPDAT VP VP 
VP 
I 
V 
I 
e 
Figure 22 
D-tree for German verb geben 'to give'. 
VP 
geben 
SUBJ XCOMP 
VP VP 
NP VP VP VP 
VP 
I 
e 
Figure 23 
D-tree for German verb versuchen 'to try'. 
VP 
VP V 
versuchen 
clause material): 
(13) a. daf~ die Kinder dem Lehrer das Buch zu geben 
that \[the children\]NOM \[the teacher\]DAT \[the book\]ncc to give 
that the children try to give the teacher the book 
b. daf~ dem Lehrer die Kinder das Buch zu geben versuchen 
c. daf~ dem Lehrer das Buch die Kinder zu geben versuchen 
versuchen 
try 
We can represent the matrix verb as shown in Figure 23, and a derivat ion as shown 
in Figure 24. It is clear that we  can still obtain all possible word  orders, and that this 
115 
Computational Linguistics Volume 27, Number 1 
OBJ INDOBJ XGOMP 
VP VP VP VP 
NP VP NP VP VP VP VP V 
SUBJ 
VP 
NP VP 
- -  - , i x ,  ' ' I " 
\ I .#  
VP V , ~versuchen.  - 
I 
VP zu geben VP 
I I 
e e 
Figure 24 
DSG derivation for a complex sentence. 
wou ld  be imposs ib le  us ing  s imple  LP rules that order  s ister nodes.  19 (It wou ld  also 
be imposs ib le  in LTAG, but  see Joshi, Becker, and  Rambow \[2000\] for an a l ternate 
d iscuss ion of long sc rambl ing  in LTAG.) 
5. Modeling Syntactic Dependency 
In the prev ious  sections, we  have presented  DSG and have  shown how it can be used  to 
prov ide  ana lyses  for a range of l inguist ic  phenomena.  In this section, we  conc lude  our  
in t roduct ion  of DSG by  d iscuss ing  the re lat ionsh ip  between der ivat ions  in DSG and 
syntactic dependency. Recently, syntact ic  dependency  has emerged as an impor tant  
factor for app l icat ions  in natura l  language process ing.  
In lex ica l ized fo rmal i sms such as LTAG, the operat ions  of the fo rmal i sm (i.e., 
in the case of LTAG, subst i tut ion and  adjunct ion)  relate structures assoc iated w i th  
two lexical i tems. It is therefore natura l  to interpret  hese operat ions  as estab l ish ing 
a direct syntact ic re lat ion between the two lexical i tems,  i.e., a re lat ion of syntact ic  
dependency.  There are at least two  types  of syntact ic  dependency :  a re lat ion of com- 
p lementat ion  (pred icate -argument  relat ion) and  a re lat ion of modi f i cat ion  (predicate-  
ad junct  relat ion).  2? Syntact ic dependency  represents  an impor tant  l inguist ic  intui t ion,  
p rov ides  a un i fo rm interface to semant ics ,  and  is, as Schabes and  Shieber (1994) argue,  
impor tant  in order  to suppor t  stat ist ical  parameters  in stochast ic f rameworks .  In fact, 
19 This kind of construction has been extensively analyzed in the Germanic syntax literature. Following 
the descriptive notion of "coherent construction" proposed by Bech (1955), Evers (1975) proposes that 
in German (and in Dutch, but not in English) a biclausal structure undergoes a special process to 
produce a monoclausal structure, in which the argument lists of the two verbs are merged and the 
verbs form a morphological unit. This analysis has been widely adopted (in one form or another) in 
the formal and computational syntax literature by introducing special mechanisms into the underlying 
formal system. If the special mechanism produces a single (flat) VP for the new argument list, then LP 
rules for the simplex case can also apply to the complex case. However, the DSG analysis has the 
advantage that it does not involve a special mechanism, and the difference between German and 
English complex clauses is related simply to the difference in word orders allowable in the simplex 
case (i.e., German but not English allows scrambling). Furthermore, the DSG analysis correctly predicts 
some "interleaved" word orders to be grammatical. See Rambow (1995) for details. 
20 In addition, we may want to identify the relation between a function word and its lexical headword 
(e.g., between a determiner and a noun) as a third type of relation. 
116 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
adore claim 
he seem 
Mary / OBJ \ seems I COMP 
I 
hotdog claim adore 
I SUB J SUBJ~~~ BJ 
he Mary hotdog 
Figure 25 
LTAG derivation tree for (14) (left); dependency tree for (14) (right). 
recent advances in parsing technology are due to the explicit stochastic modeling of 
dependency information (Collins 1997). 
Purely CFG-based approaches do not represent syntactic dependency, but other 
frameworks do, e.g. the f-structure (functional structure) of LFG (Kaplan and Bresnan 
1982), and dependency grammars (see, for example, Mel'~uk \[1988\]), for which syn- 
tactic dependency is the sole basis for representation. As observed by Rambow and 
Joshi (1997), for LTAG, we can see the derivation structure as a dependency structure, 
since in it lexemes are related directly. 
However, as we have pointed out in Section 4.1, the LTAG composition operations 
are not used uniformly: while substitution is used only to add a (nominal) complement, 
adjunction is used both for modification and (clausal) complementation. 21 Furthermore, 
there is an inconsistency in the directionality of the substitution operation and those 
uses of adjunction for clausal complementation: i  LTAG, nominal complements are 
substituted into their governing verb's tree, while the governing verb's tree is ad- 
joined into its own clausal complement. The fact that adjunction and substitution are 
used in a linguistically heterogeneous manner means that (standard) LTAG derivation 
trees do not provide a direct representation f the dependencies between the words 
of the sentence, i.e., of the predicate-argument a d modification structure. In DSG, 
this problem is overcome straightforwardly, since DSG uses generalized substitution 
for all complementation (be it nominal or clausal), while still allowing long-distance 
effects. =
There is a second, more serious problem with modeling syntactic dependency in
LTAG, as can be seen from the following example: 
(14) Hot dogs he claims Mary seems to adore 
The problem is that in the standard LTAG derivation, we adjoin both the trees 
for claim and seem into the tree for adore (Figure 25, left), while in the (commonly 
assumed) dependency structure, seem depends on claim, and adore depends on seem 
(Figure 25, right). The problem is in fact related to the interleaving problem discussed 
in Section 4.2, and can easily be solved in DSG by proposing a structure such as that 
21 Clausal complementation ca not be handled nniformly by substitution because ofthe existence of
syntactic phenomena such as long-distance wh-movement i  English. 
22 Modification can be handled by some other operation, such as sister adjunction (Rainbow, 
Vijay-Shanker, and Weir 1995), and is thus distinguished from complementation. We do not discuss 
modification i  this paper. 
117 
Computational Linguistics Volume 27, Number 1 
S 
I 
I 
i 
VP 
V VP 
I 
seems 
Figure 26 
Elementary d-tree for finite seems. 
in Figure 26 for seems, which we have already seen in Figure 21. (This structure can 
be justified on linguistic grounds independently from the dependency considerations, 
by assuming that all finite verbs--whether raising or not--project to at least S \[= IP\]. 
Raising verbs simply lack a subject of their own, but the S node is justified by the 
finiteness of the verb, not by the presence or absence of a subject.) Thus, DSG can be 
used to develop grammars in which the derivation faithfully and straightforwardly 
reflects yntactic dependency. 23 
6. Related Work 
In this section, we mention some related theoretical work and some application- 
oriented work that is based on DSG. 
On the theoretical side, Kallmeyer (1996, 1999) presents an independently con- 
ceived formalism called tree description grammar (TDG). TDG is similar to DSG: 
in both formalisms, descriptions of trees are composed uring derivations through 
conjunction and equation of nodes. Furthermore, like DSG, TDG does not allow the 
conflation of immediate dominance structure specified in elementary structures. How- 
ever, TDG allows for more than one node to be equated in a derivation step: nodes 
are "marked" and all marked nodes are required to be equated with other nodes in a 
derivation step. (Equating more than one pair of nodes in each derivation step shifts 
some of the work done in reading off in DSG to the derivation in TDG.) In DSG, 
we have designed a simple generative system based on tree descriptions involving 
dominance, using an operation that directly correspond to the linguistic notion of 
complementation. Additional mechanisms, uch as the marking of nodes and their 
simultaneous involvement in a derivation step, are not available in DSG. 
Hepple (1998) relates DSG to a system he has previously proposed in which de- 
ductions in implicational linear logic are recast as deductions involving only first-order 
formulas (Hepple 1996). He shows how this relation can be exploited to give deriva- 
tions in DSG a functional semantics. 
There is an ongoing effort to evaluate the theoretical proposals presented in this 
paper through the development of a wide-coverage DSG-based parsing system that 
provides analysis in a broadly HPSG style (Carroll et al 2000). One aspect of this work 
involves exploiting the extended omain of locality that DSG shares with TAG in order 
23 Candito and Kahane (1998) propose to use derivations in DSG to model semantic (rather than 
syntactic) dependency. 
118 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
to maximize localization of syntactic dependencies within elementary tree descriptions, 
thereby avoiding the need for unification during parsing (Carroll et al 1999). 
Nicolov and Mellish (2000) use DSG as the formalism in a generation application. 
The principal motivation for using DSG is that DSG is a lexicalized formalism which 
can provide derivations that correspond to the traditional notion of (deep) syntactic 
dependency (see Section 5), which is often considered to be the input to the syntactic 
component of a generation system. 
7. Conc lus ions  
We have introduced the grammar formalism of d-tree substitution grammars by show- 
ing how it emerges from a tree-description-theoretic analysis of tree adjoining ram- 
mars. Derivations in DSG involve the composition of d-trees, special kinds of tree 
descriptions. Trees are read off from derived d-trees. 
We have shown that the DSG formalism can be used to express a variety of lin- 
guistic analyses, including styles of analysis that do not appear to be available with 
the LTAG approach, and analyses for constructions that appear to be beyond the de- 
scriptive capacity of LTAG. Furthermore, linguistic analyses of syntactic phenomena 
are uniform, both language-internally nd cross-linguistically. Finally, DSG allows for 
a consistent modeling of syntactic dependency. 
References 
AbeillG Anne. 1991. Une grammaire l xicalisde 
d'arbres adjoints pour le fran?ais. Ph.D. 
thesis, Universit~ Paris 7. 
Backofen, Roll James Rogers, and K. 
Vijay-Shanker. 1995. A first-order 
axiomatization f the theory of finite 
trees. Journal of Language, Logic, and 
Information, 4(1):5-39. 
Bech, Gunnar. 1955. Studien fiber das deutsche 
Verbum infinitum. Det Kongelige Danske 
videnskabernes selskab. 
Historisk-Filosofiske M ddelelser, bd. 35, 
nr. 2 (1955) and bd. 36, nr. 6 (1957). 
Munksgaard, Kopenhagen. Second 
unrevised edition published 1983 by Max 
Niemeyer Verlag, Tiibingen (Linguistische 
Arbeiten 139). 
Becker, Tilman, Aravind Joshi, and Owen 
Rainbow. 1991. Long distance scrambling 
and tree adjoining rammars. In Fifth 
Conference ofthe European Chapter of the 
Association for Computational Linguistics 
(EACL'91), pages 21-26. 
Becker, Tilman and Owen Rainbow. 1995. 
Parsing non-immediate dominance 
relations. In Proceedings ofthe Fourth 
International Workshop on Parsing 
Technologies, pages 26-33, Prague. 
Bhatt, Rakesh. 1994. Word Order and Case in 
Kashmiri. Ph.D. thesis, University of 
Illinois, Urbana-Champaign. 
Bleam, Tonia. 2000. Clitic climbing and the 
power of tree adjoining rammar. In 
Anne AbeilM and Owen Rambow, editors, 
Tree Adjoining Grammars: Formalisms, 
Linguistic Analyses and Processing. CSLI 
Publications, pages 193-220. Paper 
initially presented in 1995. 
Candito, Marie-He'l~ne and Sylvain Kahane. 
1998. Defining DTG derivations to get 
semantic graphs. In Proceedings ofthe 
Fourth International Workshop on Tree 
Adjoining Grammars and Related Frameworks 
(TAG+4), IRCS Report 98-12, pages 25-28. 
Institute for Research in Cognitive 
Science, University of Pennsylvania. 
Carroll, John, Nicolas Nicolov, Olga 
Shaumyan, Martine Smets, and David 
Weir. 1999. Parsing with an extended 
domain of locality. In Ninth Conference of
the European Chapter of the Association for 
Computational Linguistics (EACL'99), 
pages 217-224. 
Carroll, John, Nicolas Nicolov, Olga 
Shaumyan, Martine Smets, and David 
Weir. 2000. Engineering a wide-coverage 
lexicalized grammar. In Proceedings ofthe 
Fifth International Workshop on Tree 
Adjoining Grammars and Related 
Frameworks, pages 55-60. 
Collins, Michael. 1997. Three generative, 
lexicalised models for statistical parsing. 
In Proceedings ofthe 35th Annual Meeting, 
Madrid, Spain, July. Association for 
Computational Linguistics. 
Evers, Arnold. 1975. The Transformational 
Cycle in Dutch and German. Ph.D. thesis, 
University of Utrecht. Distributed by the 
Indiana University Linguistics Club. 
119 
Computational Linguistics Volume 27, Number 1 
Frank, Robert. 1992. Syntactic Locality and 
Tree Adjoining Grammar: Grammatical, 
Acquisition and Processing Perspectives. 
Ph.D. thesis, Department of Computer 
and Information Science, University of 
Pennsylvania. 
Frank, Robert. Forthcoming. Phrase Structure 
Composition and Syntactic Dependencies. 
MIT Press, Cambridge. 
Gazdar, G. 1988. Applicability of indexed 
grammars to natural anguages. In U. 
Reyle and C. Rohrer, editors, Natural 
Language Parsing and Linguistic Theories. D. 
Reidel, Dordrecht, pages 69-94. 
Harley, Heidi and Seth Kulick. 1998. TAG 
and raising in VSO languages. In 
Proceedings ofthe Fourth International 
Workshop on Tree Adjoining Grammars and 
Related Frameworks (TAG+4), IRCS Report 
98-12, pages 62-65. Institute for Research 
in Cognitive Science, University of 
Pennsylvania. 
Hendrick, R. 1988. Anaphora in Celtic and 
Universal Grammar. Kluwer Academic 
Publishers, Dordrecht. 
Hepple, Mark. 1996. A compilation-chart 
method for linear categorical deduction. 
In Proceedings ofthe 16th International 
Conference on Computational Linguistics 
(COLING'96), pages 537-542. 
Hepple, Mark. 1998. On same similarities 
between D-Tree Grammars and 
type-logical grammars. In Proceedings of
the Fourth International Workshop on Tree 
Adjoining Grammars and Related Frameworks 
(TAG+4), IRCS Report 98-12, pages 66-69. 
Institute for Research in Cognitive 
Science, University of Pennsylvania. 
Joshi, Aravind K. 1987. Word-order 
variation in natural anguage generation. 
Technical Report, Department of
Computer and Information Science, 
University of Pennsylvania. 
Joshi, Aravind K., Tilman Becker, and Owen 
Rambow. 2000. A new twist on the 
competence/performance distinction. In 
Anne Abeill4 and Owen Rambow, editors, 
Tree Adjoining Grammars: Formalisms, 
Linguistic Analysis, and Processing. CSLI 
Publications, pages 167-182. 
Joshi, Aravind K. and Yves Schabes. 1991. 
Tree-adjoining grammars and lexicalized 
grammars. In Maurice Nivat and Andreas 
Podelski, editors, Definability and 
Recognizability of Sets of Trees. Elsevier. 
Kallmeyer, Laura. 1996. Tree description 
grammars. In D. Gibbon, editor, Natural 
Language Processing and Speech Technology. 
Results of the 3rd KONVENS Conference, 
pages 332-341, Berlin. Mouton de 
Gruyter. 
KaUmeyer, Laura. 1999. Tree Description 
Grammars and Underspecified 
Representations. Ph D. thesis, University of 
Tiibingen. Available as Technical Report 
No. 99-08 from the Institute for Research 
in Cognitive Science at the University of 
Pennsylvania. 
Kaplan, Ronald M. and Joan W. Bresnan. 
1982. Lexical-functional grammar: A
formal system for grammatical 
representation. I  J. W. Bresnan, editor, 
The Mental Representation of Grammatical 
Relations. MIT Press, Cambridge, MA. 
Kasper, Robert, Bernd Kiefer, Klaus Netter, 
and K. Vijay-Shanker. 1995. Compilation 
of HPSG and TAG. In Proceedings ofthe 
Annual Meeting, pages 92-99. Association 
for Computational Linguistics. 
Kroch, Anthony. 1987. Subjacency in a tree 
adjoining grammar. In Alexis 
Manaster-Ramer, ditor, Mathematics of
Language. John Benjamins, Amsterdam, 
pages 143-172. 
Kroch, Anthony. 1989. Asymmetries in long 
distance xtraction i  a Tree Adjoining 
Grammar. In Mark Baltin and Anthony 
Kroch, editors, Alternative Conceptions of
Phrase Structure. University of Chicago 
Press, Chicago, pages 66-98. 
Leahu, Manuela. 1998. Wh-dependencies n 
Romanian and TAG. In Proceedings ofthe 
Fourth International Workshop on Tree 
Adjoining Grammars and Related Frameworks 
(TAG+4), pages 92-95, IRCS Report 98-12, 
Institute for Research in Cognitive 
Science, University of Pennsylvania. 
Marcus, Mitchell, Donald Hindle, and 
Margaret Fleck. 1983. D-theory: Talking 
about talking about rees. In Proceedings of
the 21st Annual Meeting, Cambridge, MA. 
Association for Computational 
Linguistics. 
Mel'~uk, Igor A. 1988. Dependency S ntax: 
Theory and Practice. State University of 
New York Press, New York. 
Nicolov, Nicolas and Christopher Mellish. 
2000. Protector: Efficient generation with 
lexicalized grammars. In Ruslan Mitkov 
and Nicolas Nicolov, editors, Recent 
Advances in Natural Language Processing 
(RANLP vol. II). John Benjamins, 
Amsterdam and Philadelphia, 
pages 221-243. 
Pollard, Carl and Ivan Sag. 1994. 
Head-Driven Phrase Structure Grammar. 
University of Chicago Press, Chicago. 
Rambow, Owen. 1994a. Formal and 
Computational Aspects of Natural Language 
Syntax. Ph.D. thesis, Department of
Computer and Information Science, 
University of Pennsylvania, Philadelphia. 
120 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
Available as Tectmical Report 94-08 from 
the Institute for Research in Cognitive 
Science (IRCS) and also at ftp://ftp.cis. 
upenn.edu/pub/rambow/thesis.ps.Z. 
Rambow, Owen. 1994b. Multiset-valued 
linear index grammars. In Proceedings of
the 32nd Annual Meeting, pages 263-270. 
Association for Computational 
Linguistics. 
Rambow, Owen. 1995. Coherent 
constructions in German: Lexicon or 
syntax? In Glyn Morrill and Richard 
Oehrle, editors, Formal Grammar: 
Proceedings ofthe Conference ofthe European 
Summer School in Logic, Language, and 
Information, pages 213-226, Barcelona. 
Rambow, Owen. 1996. Word order, clause 
union, and the formal machinery of 
syntax. In Miriam Butt and Tracy 
Holloway King, editors, Proceedings ofthe 
First LFG Conference. On-line version at 
http://www-csli.stanford.edu/ 
publications/LFG/lfgl.html. 
Rainbow, Owen and Aravind Joshi. 1997. A 
formal ook at dependency gran~nars and 
phrase-structure grammars, with special 
consideration of word-order phenomena. 
In Leo Wanner, editor, Recent Trends in 
Meaning-Text Theory. John Benjamins, 
Amsterdam and Philadelphia. 
Rainbow, Owen and Beatrice Santorini. 
1995. Incremental phrase structure 
generation and a universal theory of V2. 
In J. N. Beckman, editor, Proceedings of
NELS 25, pages 373-387, Amherst, MA. 
GSLA. 
Rambow, Owen and K. Vijay-Shanker. 1998. 
Wh-islands in TAG and related 
formalisms. In Proceedings ofthe Fourth 
International Workshop on Tree Adjoining 
Grammars and Related Frameworks (TAG+4), 
pages 147-150, IRCS Report, 98-12. 
Institute for Research in Cognitive 
Science, University of Pennsylvania. 
Rambow, Owen, K. Vijay-Shanker, and 
David Weir. 1995. D-Tree Grammars. In 
Proceedings ofthe 33rd Annual Meeting, 
pages 151-158. Association for 
Computational Linguistics. 
Rogers, James and K. Vijay-Shanker. 1992. 
Reasoning with descriptions of trees. In 
Proceedings ofthe 30th Annual Meeting, 
pages 72-80. Association for 
Computational Linguistics. 
Schabes, Yves. 1990. Mathematical nd 
Computational Aspects of Lexicalized 
Grammars. Ph.D. thesis, Department of
Computer and Information Science, 
University of Pennsylvania. 
Schabes, Yves and Stuart Shieber. 1994. An 
alternative conception of tree-adjoining 
derivation. Computational Linguistics, 
20(1):91-124. 
Vijay-Shanker, K. 1987. A Study of Tree 
Adjoining Grammars. Ph.D. thesis, 
Department of Computer and Information 
Science, University of Pennsylvania, 
Philadelphia, PA, December. 
Vijay-Shanker, K. 1992. Using descriptions 
of trees in a Tree Adjoining Grammar. 
Computational Linguistics, 18(4):481-518. 
Vijay-Shanker, K. and David Weir. 1999. 
Exploring the underspecified world of 
Lexicalized Tree Adjoining Grammars. In 
Proceedings ofthe Sixth Meeting on 
Mathematics of Language. 
Vijay-Shanker, K., David Weir, and Owen 
Rainbow. 1995. Parsing D-Tree Grammars. 
In Proceedings ofthe Fourth International 
Workshop on Parsing Technologies, 
pages 252-259. ACL/SIGPARSE. 
XTAG-Group, The. 1999. A lexicalized Tree 
Adjoining Grammar for English. 
Technical Report. The Institute for 
Research in Cognitive Science, University 
of Pennsylvania. Available at: 
http://www.cis.upenn.edu/~xtag/tech- 
report/tech-report.html. 
121 

Proceedings of NAACL HLT 2009: Short Papers, pages 137?140,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Taking into Account the Differences between Actively and Passively
Acquired Data: The Case of Active Learning with Support Vector Machines
for Imbalanced Datasets
Michael Bloodgood?
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211 USA
bloodgood@jhu.edu
K. Vijay-Shanker
Computer and Information
Sciences Department
University of Delaware
Newark, DE 19716 USA
vijay@cis.udel.edu
Abstract
Actively sampled data can have very different
characteristics than passively sampled data.
Therefore, it?s promising to investigate using
different inference procedures during AL than
are used during passive learning (PL). This
general idea is explored in detail for the fo-
cused case of AL with cost-weighted SVMs
for imbalanced data, a situation that arises for
many HLT tasks. The key idea behind the
proposed InitPA method for addressing im-
balance is to base cost models during AL on
an estimate of overall corpus imbalance com-
puted via a small unbiased sample rather than
the imbalance in the labeled training data,
which is the leading method used during PL.
1 Introduction
Recently there has been considerable interest in us-
ing active learning (AL) to reduce HLT annotation
burdens. Actively sampled data can have differ-
ent characteristics than passively sampled data and
therefore, this paper proposes modifying algorithms
used to infer models during AL. Since most AL re-
search assumes the same learning algorithms will be
used during AL as during passive learning1 (PL),
this paper opens up a new thread of AL research that
accounts for the differences between passively and
actively sampled data.
The specific case focused on in this paper is
that of AL with SVMs (AL-SVM) for imbalanced
?This research was conducted while the first author was a
PhD student at the University of Delaware.
1Passive learning refers to the typical supervised learning
setup where the learner does not actively select its training data.
datasets2. Collectively, the factors: interest in AL,
widespread class imbalance for many HLT tasks, in-
terest in using SVMs, and PL research showing that
SVM performance can be improved substantially by
addressing imbalance, indicate the importance of the
case of AL with SVMs with imbalanced data.
Extensive PL research has shown that learning
algorithms? performance degrades for imbalanced
datasets and techniques have been developed that
prevent this degradation. However, to date, rela-
tively little work has addressed imbalance during AL
(see Section 2). In contrast to previous work, this
paper advocates that the AL scenario brings out the
need to modify PL approaches to dealing with im-
balance. In particular, a new method is developed
for cost-weighted SVMs that estimates a cost model
based on overall corpus imbalance rather than the
imbalance in the so far labeled training data. Sec-
tion 2 discusses related work, Section 3 discusses
the experimental setup, Section 4 presents the new
method called InitPA, Section 5 evaluates InitPA,
and Section 6 concludes.
2 Related Work
A problem with imbalanced data is that the class
boundary (hyperplane) learned by SVMs can be too
close to the positive (pos) examples and then recall
suffers. Many approaches have been presented for
overcoming this problem in the PL setting. Many
require substantially longer training times or ex-
2This paper focuses on the fundamental case of binary clas-
sification where class imbalance arises because the positive ex-
amples are rarer than the negative examples, a situation that nat-
urally arises for many HLT tasks.
137
tra training data to tune parameters and thus are
not ideal for use during AL. Cost-weighted SVMs
(cwSVMs), on the other hand, are a promising ap-
proach for use with AL: they impose no extra train-
ing overhead. cwSVMs introduce unequal cost fac-
tors so the optimization problem solved becomes:
Minimize:
1
2?~w?
2 + C+
?
i:yi=+1
?i + C?
?
i:yi=?1
?i (1)
Subject to:
?k : yk [~w ? ~xk + b] ? 1? ?k, (2)
where (~w, b) represents the learned hyperplane, ~xk
is the feature vector for example k, yk is the label
for example k, ?k = max(0, 1 ? yk(~wk ? ~xk + b))
is the slack variable for example k, and C+ and C?
are user-defined cost factors.
The most important part for this paper are the cost
factors C+ and C?. The ratio C+C? quantifies the
importance of reducing slack error on pos train ex-
amples relative to reducing slack error on negative
(neg) train examples. The value of the ratio is cru-
cial for balancing the precision recall tradeoff well.
(Morik et al, 1999) showed that during PL, set-
ting C+C? = # of neg training examples# of pos training examples is an effec-
tive heuristic. Section 4 explores using this heuris-
tic during AL and explains a modified heuristic that
could work better during AL.
(Ertekin et al, 2007) propose using the balancing
of training data that occurs as a result of AL-SVM
to handle imbalance and do not use any further mea-
sures to address imbalance. (Zhu and Hovy, 2007)
used resampling to address imbalance and based the
amount of resampling, which is the analog of our
cost model, on the amount of imbalance in the cur-
rent set of labeled train data, as PL approaches do.
In contrast, the InitPA approach in Section 4 bases
its cost models on overall (unlabeled) corpus imbal-
ance rather than the amount of imbalance in the cur-
rent set of labeled data.
3 Experimental Setup
We use relation extraction (RE) and text classifica-
tion (TC) datasets and SVMlight (Joachims, 1999)
for training the SVMs. For RE, we use AImed,
previously used to train protein interaction extrac-
tion systems ((Giuliano et al, 2006)). As in previ-
ous work, we cast RE as a binary classification task
Figure 1: Hyperplane B was trained with a higher C+C?
ratio than hyperplane A was trained with.
(14.94% of the examples in AImed are positive). We
use the KGC kernel from (Giuliano et al, 2006), one
of the highest-performing systems on AImed to date
and perform 10-fold cross validation. For TC, we
use the Reuters-21578 ModApte split. Since a doc-
ument may belong to more than one category, each
category is treated as a separate binary classification
problem, as in (Joachims, 1998). As in (Joachims,
1998), we use the ten largest categories, which have
imbalances ranging from 1.88% to 29.96%.
4 AL-SVM Methods for Addressing Class
Imbalance
The key question when using cwSVMs is how to set
the ratio C+C? . Increasing it will typically shift the
learned hyperplane so recall is increased and preci-
sion is decreased (see Figure 1 for a hypothetical ex-
ample). Let PA= C+C? .3 How should the PA be set
during AL-SVM?
We propose two approaches: one sets the PA
based on the level of imbalance in the labeled train-
ing data and one aims to set the PA based on an es-
timate of overall corpus imbalance, which can dras-
tically differ from the level of imbalance in actively
sampled training data. The first method is called
CurrentPA, depicted in Figure 2. Note that in step
0 of the loop, PA is set based on the distribution of
positive and negative examples in the current set of
labeled data. However, observe that during AL the
ratio # neg labeled examples# pos labeled examples in the current set of la-
beled data gets skewed from the ratio in the entire
3PA stands for positive amplification and gives us a concise
way to denote the fraction C+C? , which doesn?t have a standard
name.
138
Input:
L = small initial set of labeled data
U = large pool of unlabeled data
Loop until stopping criterion is met:
0. Set PA = |{x?Labeled:f(x)=?1}||{x?L:f(x)=+1}|
where f is the function we desire to learn.
1. Train an SVM with C+ and C? set such
that C+C? = PA and obtain hyperplane h .4
2. batch? select k points from U that are
closest to h and request their labels.5
3. U = U ? batch .
4. L = L ? batch .
End Loop
Figure 2: The CurrentPA algorithm
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
1.5
2
2.5
3
3.5
4
4.5
5
5.5
Empirical Evidence of CurrentPA creating a Skewed Distribution (Fold Avg)
Number of Points for which Annotations Have Been Requested
Ra
tio
 of
 # 
of 
Ne
ga
tiv
e t
o #
 of
 Po
sit
ive
 Po
int
s
 
 
Ratio with CurrentPA
Ratio with Entire Set
Figure 3: Illustration of AL skewing the distribution of
pos/neg points on AImed.
corpus because AL systematically selects the exam-
ples that are closest to the current model?s hyper-
plane and this tends to select more positive exam-
ples than random selection would select (see also
(Ertekin et al, 2007)).
Empirical evidence of this distribution skew is il-
lustrated in Figure 3. The trend toward balanced
datasets during AL could mislead and cause us to
underestimate the PA.
Therefore, our next algorithm aims to set the PA
based on the ratio of neg to pos instances in the en-
tire corpus. However, since we don?t have labels for
the entire corpus, we don?t know this ratio. But by
using a small initial sample of labeled data, we can
4We use SVMlight?s default value for C?.
5In our experiments, batch size is 20.
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
20
25
30
35
40
45
50
55
60
AImed Average F Measure versus Number of Annotations
Number of Points for which Annotations Have Been Requested
Pe
rfo
rm
an
ce
 (F 
Me
asu
re)
 
 
InitPA
Oversampling(Zhu and Hovy,2007)
CurrentPA
EHG2007(Ertekin et al 2007)
Figure 4: AImed learning curves. y-axis is from 20% to
60%.
estimate this ratio with high confidence. This esti-
mate can then be used for setting the PA throughout
the AL process. We call this method of setting the
PA based on a small initial set of labeled data the
InitPA method. It is like CurrentPA except we move
Step 0 to be executed one time before the loop and
then use that same PA value on each iteration of the
AL loop.
To guide what size to make the initial set of la-
beled data, one can determine the sample size re-
quired to estimate the proportion of positives in a
finite population to within sampling error e with a
desired level of confidence using standard statisti-
cal techniques found in many college-level statistics
references such as (Berenson et al, 1988). For ex-
ample, carrying out the computations on the AImed
dataset shows that a size of 100 enables us to be
95% confident that our proportion estimate is within
0.0739 of the true proportion. In our experiments,
we used an initial labeled set of size 100.
5 Evaluation
In addition to InitPA and CurrentPA, we also imple-
mented the methods from (Ertekin et al, 2007; Zhu
and Hovy, 2007). We implemented oversampling by
duplicating points and by BootOS (Zhu and Hovy,
2007). To avoid cluttering the graphs, we only show
the highest-performing oversampling variant, which
was by duplicating points. Learning curves are pre-
sented in Figures 4 and 5.
Note InitPA is the highest-performing method for
all datasets, especially in the practically important
area of where the learning curves begin to plateau.
139
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
76
77
78
79
80
81
82
83
Reuters Average F Measure versus Number of Annotations
Number of Points for which Annotations Have Been Requested
Pe
rfo
rm
an
ce
 (F
 Me
asu
re)
 
 
InitPA
Oversampling(Zhu and Hovy,2007)
CurrentPA
EHG2007(Ertekin et al 2007)
Figure 5: Reuters learning curves. y-axis is from 76% to
83%.
This area is important because this is around where
we would want to stop AL (Bloodgood and Vijay-
Shanker, 2009).
Observe that the gains of InitPA over CurrentPA
are smaller for Reuters. For some Reuters cate-
gories, InitPA and CurrentPA have nearly identical
performance. Applying the models learned by Cur-
rentPA at each round of AL on the data used to
train the model reveals that the recall on the train-
ing data is nearly 100% for those categories where
InitPA/CurrentPA perform similarly. Increasing the
relative penalty for slack error on positive training
points will not have much impact if (nearly) all of
the pos train points are already classified correctly.
Thus, in situations where models are already achiev-
ing nearly 100% recall on their train data, InitPA is
not expected to outperform CurrentPA.
The hyperplanes learned during AL-SVM serve
two purposes: sampling - they govern which unla-
beled points will be selected for human annotation,
and predicting - when AL stops, the most recently
learned hyperplane is used for classifying test data.
Although all AL-SVM approaches we?re aware of
use the same hyperplane at each round of AL for
both of these purposes, this is not required. We com-
pared InitPA with hybrid approaches where hyper-
planes trained using an InitPA cost model are used
for sampling and hyperplanes trained using a Cur-
rentPA cost model are used for predicting, and vice-
versa, and found that InitPA performed better than
both of these hybrid approaches. This indicates that
the InitPA cost model yields hyperplanes that are
better for both sampling and predicting.
6 Conclusions
We?ve made the case for the importance of AL-SVM
for imbalanced datasets and showed that the AL sce-
nario calls for modifications to PL approaches to ad-
dressing imbalance. For AL-SVM, the key idea be-
hind InitPA is to base cost models on an estimate of
overall corpus imbalance rather than the class imbal-
ance in the so far labeled data. The practical utility
of the InitPA method was demonstrated empirically;
situations where InitPA won?t help that much were
made clear; and analysis showed that the sources of
InitPA?s gains were from both better sampling and
better predictive models.
InitPA is an instantiation of a more general idea
of not using the same inference algorithms during
AL as during PL but instead modifying inference al-
gorithms to suit esoteric characteristics of actively
sampled data. This is an idea that has seen relatively
little exploration and is ripe for further investigation.
References
Mark L. Berenson, David M. Levine, and David Rind-
skopf. 1988. Applied Statistics. Prentice-Hall, Engle-
wood Cliffs, NJ.
Michael Bloodgood and K. Vijay-Shanker. 2009. A
method for stopping active learning based on stabiliz-
ing predictions and the need for user-adjustable stop-
ping. In CoNLL.
Seyda Ertekin, Jian Huang, Le?on Bottou, and C. Lee
Giles. 2007. Learning on the border: active learning
in imbalanced data classification. In CIKM.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In EACL.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In ECML, pages 137?142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods ?
Support Vector Learning, pages 169?184.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning with a
knowledge-based approach - a case study in intensive
care monitoring. In ICML, pages 268?277.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In EMNLP-CoNLL.
140
An Investigation of Various Information Sources for Classifying Biological
Names
Manabu Torii, Sachin Kamboj and K. Vijay-Shanker
Department of Computer and Information Sciences
University of Delaware
Newark, DE 19716
ftorii,kamboj,vijayg@mail.eecis.udel.edu
Abstract
The classification task is an integral part of
named entity extraction. This task has not
received much attention in the biomedi-
cal setting, partly due to the fact that pro-
tein name recognition has been the focus
of the majority of the work in this field.
We study this problem and focus on dif-
ferent sources of information that can be
utilized for the classification task and in-
vestigate the extent of their contributions
for classification in this domain. However,
while developing a specific algorithm for
the classification of the names is not our
main focus, we make use of some simple
techniques to investigate different sources
of information and verify our intuitions
about their usefulness.
1 Introduction
In this paper, we investigate the extent to which
different sources of information contribute towards
the task of classifying the type of biological en-
tity a phrase might refer to. The classification task
is an integral part of named entity extraction. For
this reason, name classification has been studied in
solving the named entity extraction task in the NLP
and information extraction communities (see, for
example, (Collins and Singer, 1999; Cucerzan and
Yarowsky, 1999) and various approaches reported in
the MUC conferences (MUC-6, 1995)). However,
many of these approaches do not distinguish the de-
tection of the names (i.e., identifying a sequence of
characters and words in text as a name) from that of
its classification as separate phases. Yet, we believe
that we will gain from examining the two as sepa-
rate tasks as the classification task, the focus of this
work, is sufficiently distinct from the name identifi-
cation task. More importantly, from the perspective
of the current work, we hope to show that the sources
of information that help in solving the two tasks are
quite distinct.
Similar to the approaches of name classifica-
tion of (Collins and Singer, 1999; Cucerzan and
Yarowsky, 1999), we investigate both name internal
and external clues. However, we believe that the sit-
uation in the specialized domain of biomedicine is
sufficiently distinct, that the clues for this domain
need further investigation and that the classification
task has not received the similar attention deserved.
A large number of name extraction methods pro-
posed in this specialized domain have focused on
extracting protein names only (Fukuda et al, 1998;
Franzen et al, 2002; Tanabe et al, 2002). Since only
one class is recognized, the only task these methods
directly address is that of identifying a string of char-
acters and/or words that constitute a protein name.
These methods do not, at least in an explicit manner,
have to consider the classification task.
There are some important reasons to consider the
detection of names of other types of entities of bio-
logical relevance. Information extraction need not
be limited to protein-protein interactions, and ex-
tracting names of other types of entities will be re-
quired for other types of interactions. Secondly,
classification of names can help improve the preci-
sion of the methods. For example, KEX (Fukuda
et al, 1998) is a protein name recognizer and hence
labels each name it detects as a protein. However,
names of different types of entities share similar sur-
face characteristics (including use of digits, special
characters, and capitalizations). Due to this reason,
KEX and other protein name recognizers can pick
names of entities other than proteins (and label them
as proteins). (Narayanaswamy et al, 2003) reports
that by recognizing that some of these names as not
those of proteins allows their method to improve the
precision of protein name detection. Thirdly detect-
ing names of different classes will help in corefer-
ence resolution, the importance of which is well rec-
ognized in the IE domain. In such specialized do-
mains, the sortal/class information will play an im-
portant role for this task. In fact, the coreference res-
olution method described in (Castan?o et al, 2002)
seeks to use such information by using the UMLS
system1 and by applying type coercion. Finally,
many information extraction methods are based on
identifying or inducing patterns by which informa-
tion (of the kind being extracted) is expressed in nat-
ural language text. If we can tag the text with occur-
rences of various types of names (or phrases that re-
fer to biological entities) then better generalizations
of patterns can be induced.
There are at least two efforts (Narayanaswamy et
al., 2003; Kazama et al, 2002) that consider the
recognition of names of different classes of biomed-
ical relevance. Work reported in (Pustejovsky et al,
2002; Castan?o et al, 2002) also seeks to classify or
find the sortal information of phrases that refer to
biological entities. However, classification was not
the primary focus of these papers and hence the de-
tails and accuracy of the classification methods are
not described in much detail. Other related works
include those of (Hatzivassiloglou et al, 2001; Liu
et al, 2001) that use external or contextual clues to
disambiguate ambiguous expressions. While these
works maybe viewed as similar to word sense dis-
ambiguation (WSD), the one reported in (Hatzivas-
siloglou et al, 2001) in particular is close to classifi-
cation as well. In this work, using context of individ-
ual occurrences, names are disambiguated between
gene, protein and RNA senses.
1The Unified Medical Language System (UMLS) was de-
veloped at National Library of Medicine, a National Institutes
of Health at Bethesda, USA.
While our interest is in classification of phrases
that refer to entities of biomedical significance, in
this work we limit ourselves to name classification.
In our investigations, we wish to use an annotated
corpus for both inducing and evaluating features.
We are unaware of any large corpus where phrases
are annotated with their classes. However, large cor-
pora for named entity extraction in this domain are
being developed, and fortunately, corpora such as
GENIA being developed at University of Tokyo are
freely available. We make use of this corpus and
hence investigate the classification of names only.
However, we believe that the conclusions we draw
in this regard will apply equally to classification of
phrases other than names as well.
2 Sources of Information for Name
Classification
To classify a name we consider both the words
within the name (i.e., name internal) as well as the
nearby words, the context of occurrences.
2.1 Using Name Internal Information
Methods for learning to identify names try to in-
duce patterns of words and special characters that
might constitute names. Hence the entire sequence
of words in a name is important and necessary for
name identification purposes. In contrast, for classi-
fication purposes, some parts of the names are more
important than the others and some may play no role
at all. For example, in the name cyclic AMP re-
sponse element-binding protein, the last word, pro-
tein, is sufficient for its classification. Similarly,
Adherence-isolated monocytes, can be classified on
the basis of its last word, monocytes.
The fact that the last word of a name often bears
the most information about the class of the name is
not surprising. In English, often the type of object
referred by a noun phrase is given by the head noun.
Viewing a name as a noun phrase, the head noun is
likely to determine its class. And in English noun
phrases, the head noun is often the rightmost word
because of the right-branching structure of English
noun phrases. Quite often the nouns correspond to
concepts (or classes) in an ontology. In such cases,
we call these nouns functional terms or f-terms, fol-
lowing the terminology used in some name recog-
nizers proposed for the biomedical domain.
2.1.1 F-terms
The notion of f-terms, was first introduced in the
design of KEX (Fukuda et al, 1998). In this work,
a set of words such as proteins and receptors, were
manually selected as f-terms. In this protein name
recognition system, as well as in Yapex (Franzen et
al., 2002), f-terms are only used for locating names
in text. On the other hand, the system reported
in (Narayanaswamy et al, 2003), which identifies
the names of other classes as well, generalizes them
to also classify names as well. Thus, f-terms are
identified with types/classes.
The existing methods that use f-terms rely on a
manually selected list of f-terms. However, manual
selection methods are usually susceptible to errors of
omission. In Section 4.1, we consider a method that
tries to automatically select a list of f-terms and the
resultant word classes based on the GENIA corpus.
We then use this generated list to test our intuitions
about f-terms.
We also consider f-terms extended to consist of
two consecutive words. We refer to these as bigram
f-terms to differentiate them from single word only
(unigram) f-terms. The use of bigrams will help us
to classify names when the last word is not an f-term,
but the last two words together can uniquely clas-
sify the name. For example, Allergen -specific T cell
clones cannot be classified using the last word alone.
However, a name ending with cell clones as the last
bigram is likely to be a ?Source?.
2.1.2 Suffixes
Often the information about the class designated
by a noun can be found in its suffix, particularly
in a scientific domain. If f-terms can be viewed
as words that designate a class of entities then note
that suffixes also play the same role. For example,
words ending with the suffix -amine are nitrogen
compounds and those ending with -cytes are cells.
Thus using suffixes results in a generalization at the
word level. A method of selecting a list of suffixes
and associating classes with them is described in
Section 4.1.
2.1.3 Example-based Classification
Of course, not all names can be classified on the
basis of f-terms and suffixes only. Sometimes names
are chosen on a more ad hoc manner and do not re-
flect any underlying meaning. In such cases, match-
ing with names found in a dictionary would be the
only name-internal method possible.
We cannot simply use an ?exact matching? algo-
rithm since such a method would only work if the
name was already present in our dictionary. As it is
not reasonable at this time to have a dictionary that
contains all possible names, we can attempt to use
approximate matches to find similar names in the
dictionary and use them for classification purposes.
Such a method then can be thought of finding a way
to generalize from the names in a dictionary, instead
of relying on simple memorization.
However, assuming a large dictionary is not feasi-
ble at this time especially for all the classes. So our
alternate is to look at examples from GENIA corpus.
The candidate examples that we will use for classi-
fication would be the ones that most closely match
a given name that needs to be classified. Hence, the
method we are following here essentially becomes
an example-based classification method such as k-
nearest neighbor method. One approach to this task
is described in Section 4.3.
2.2 Using Context
We now turn our attention to looking at clues that
are outside the name being classified. Using context
has been widely used for WSD and has also been ap-
plied to name classification (for example, in (Collins
and Singer, 1999; Cucerzan and Yarowsky, 1999)).
This approach has also been adopted for the biomed-
ical domain as illustrated in the work of (Hatzivas-
siloglou et al, 2001; Narayanaswamy et al, 2003;
Castan?o et al, 2002)2.
In the WSD work involving the use of context, we
can find two approaches: one that uses few strong
contextual evidences for disambiguation purposes,
as exemplified by (Yarowsky, 1995); and the other
that uses weaker evidences but considers a combi-
nation of a number of them, as exemplified by (Gale
et al, 1992). We explore both the methods. In Sec-
tion 4.4, we discuss our formulation and present a
simple way of extracting contextual clues.
2(Castan?o et al, 2002) can be seen as using context in its
type coercion rules.
3 Experimental Setup
3.1 Division of the corpus
We divided the name-annotated GENIA corpus
(consisting of 2000 abstracts) into two parts?1500
abstracts were used to derive all the clues: f-terms,
suffixes, examples (for matching) and finally contex-
tual features. These derived sources of information
were then used to classify the names found in the re-
maining 500 abstracts. The keys from the annotated
corpus were then used to compute the precision and
recall figures. We will call these two parts the train-
ing and test sections.
Since we pick the names from the test section
and classify them, we are entirely avoiding the name
identification task. Of course, this means that we do
not account for errors in classification that might re-
sult from errors in identifying names. However, we
believe that this is appropriate for two reasons. Our
investigation focuses on how useful the above men-
tioned features are for classification and we felt that
this might be slanted based on the name identifier
we use and its characteristics. Secondly, most of the
errors are due to not finding the correct extent of the
name, either because additional neighboring words
are included or because some words/characters are
not included. In our experience, most of these errors
happen at the beginning part of the name and, hence,
should not unduly affect the classification.
3.2 Classes of Names
In our method, we classify names into one of the
five classes that we call Protein, Protein Part, Chem-
ical, Source and Others. We don?t have any partic-
ularly strong reasons for this set of classes although
we wish to point out that the first four in this choice
corresponds to the classes used by the name recog-
nizer of (Narayanaswamy et al, 2003). It must be
noted that the class proteins not only include pro-
teins but also protein families, and genes; all of
which are recognized by many protein name recog-
nizers. The GENIA class names were then mapped
onto our class names.
3.3 Tokenization
After the assignment of classes, all the extracted
names were tokenized. Noting that changing a digit
by another, a Greek character by another, a Ro-
man numeral by another rarely ever results in ob-
taining another name of a different class, our name
tokenization marks these occurrences accordingly.
To remove variability in naming, hyphens and ex-
tra spaces were removed. Also, as acronyms are not
useful for detecting types, their presence is identi-
fied (in our case we use a simplistic heuristic that
acronyms are words with 2 or more consecutive up-
per case characters).
3.4 Evaluation Methodology
We used an n-fold cross-validation to verify that the
results and conclusions we draw are not slanted by a
particular division of the 2000 abstracts. The corpus
was divided into sets of 500 abstracts - the composi-
tion of each set being random - thus obtaining 4 dif-
ferent partitions. In the first partition, the first three
sets were combined to form the Training Set and the
last was used as the Test Set. In the second partition,
the second, third and fourth sets formed the Training
Set and the first was used as the Test Set and so on.
The overall results that we report in Section 5
were the average of results on the four partitions.
However, the first partition was used for more de-
tailed investigation.
4 Classification Method
Given an unclassified name, we first tried to clas-
sify it on the basis of the f-terms and the suffixes. If
that failed, we applied our string matcher to try to
find a match and assign a category to the unknown
name. Finally, we used context to assign classes to
the names that were still left unclassified.
4.1 F-Term and Suffix Extraction
Since we consider f-terms to be nouns that appear
at the end of a name and denote a type of entity,
their presence in the name suffices for its classifi-
cation. Hence, we use the last words of names
found in the training set to see if they can uniquely
identify the class. To generate a list of f-terms and
their respective classes, we count each word or pair
of words that is found at the end of any name. A
unigram or bigram, w, was selected as an f-term if it
appeared at least 5 times and if the conditional prob-
ability P(classj w) for any class exceeds a threshold
which we set at 0.95.
In the counting to estimate this conditional prob-
ability we ignore the presence of digits, Greek char-
acters and Roman numerals as discussed in the Sec-
tion 3.3. For example, in latent membrane protein
1 the ?1? at the end is ignored and ?protein? will be
selected as the unigram for the count.
The number of f-terms selected for chemicals was
the lowest. This is not surprising considering chem-
ical names have few words defining subtypes of
chemicals. acetate was an example chosen for this
class. Some other examples of extracted f-terms and
their associated classes are: cell, tissue, virus (for
Source); kinase, plasmid and protein (for Proteins);
subunit, site and chain (for Protein Parts) and bind-
ings and defects (for Others). A couple of surprising
words were selected. Due to the limitations of our
method, we do not check if a last name indeed de-
notes a class of entities but merely note that the name
is strongly associated with a class. Hence, protein
names like Ras and Tax were also selected.
For suffix extraction, we considered suffixes of
length three, four and five. Since we argued ear-
lier that the suffixes that we are considering play
the same role as f-terms, we only consider the suf-
fixes of the last word. This prevents the classifica-
tion of cortisol- dependent BA patients (a ?Source?)
as a ?Chemical? on the basis of the suffix -isol. Also,
like in the case of f-terms, digits, Greek characters
etc at the end of a name were ignored. However,
unlike f-terms, if the last word is an acronym the
whole name is dropped, as taking the suffix of an
acronym wouldn?t result in any generalization. The
probability of a class given a suffix is then calculated
and only those suffixes which had a probability of
greater than the probability threshold were selected.
When generating the list of suffixes, we have two
possibilities. We could choose to consider names
which ended with an f-term that was selected or not
consider these names under the assumption that f-
terms would be sufficient to classify such names. We
found that considering the suffixes of the f-terms re-
sults in a significant increase in the recall with lit-
tle or no change in precision. This rather surprising
result can be understood if we consider the kinds
of names that show up in the class Others. A suf-
fix such as ation was selected because a number of
names ending with selected f-terms like transplan-
tation, transformation, and association. This suffix
allows us to classify AP-1 translocation on the basis
of the suffix despite the fact that translocation was
not chosen as an f-term.
4.2 Classification based on f-terms and suffixes
Given a set of f-terms and suffixes, along with their
associated classes, selected from the training part,
names in the test portion were classified by looking
at the words that end the names. If a name ended
with a selected f-term, then the name was tagged as
belonging to the corresponding class. If a match was
not found, the suffix of the last word of the name was
extracted and a match was attempted with the known
list of suffixes. If no match was found, the name was
left unclassified.
4.3 Classifying Names using Similar Examples
We had discussed earlier the use of similar examples
to classify a new occurrence of a name. To find sim-
ilar examples, standard string matching algorithms
are often used which produce a similarity score that
varies inversely with the number of edit operations
needed to match two strings identically. However,
we abandoned the use of standard string matching
programs as their performance for classification pur-
poses was rather poor. Primarily this was due to
the fact that these algorithms do not distinguish be-
tween matches at the beginning and at the end of the
name strings. As discussed before, for classification
purposes the position of words is important and we
noticed that matches at the beginning of the strings
were hardly ever relevant unlike the case with those
at the end. For this reason, we developed our own
matching algorithm.
Given a name in the test corpus, we try to find
how similar it is to candidate examples taken from
the training portion. For each pair of names, we first
try to pair together the individual words that make
up the names allowing for some partial matching.
These partial matches allow for certain kinds of sub-
stitutions that we do not believe will affect the classi-
fication. These include dropping a plural ?s?, substi-
tuting one Greek character by another, changing an
uppercase character by the same character in lower
case, changing an Arabic/Roman single digit by an-
other, changing a Roman numeral by an Arabic one,
and dropping digits. Each substitution draws a small
penalty (although dropping digits incurs a slightly
greater penalty) and only a perfect match receives a
score of 1 for matching of individual words. Com-
plete mismatches receive a score of 0.
We then try to assign a score to the whole pair of
names. We begin by assigning position numbers to
each pair of words (including matches, mismatches
and drops) starting from the rightmost match which
is assigned a position of zero. Mismatches to the
right of the first match, if any, are assigned negative
positions. We then use a weight table that gives more
weightage to lower position numbers (i.e., towards
the end of the strings rather than the beginning) to
assign a weight to each pair of words depending on
the position. Then the score of the entire match is
given by a weighted sum of the match scores, nor-
malized for length of the string. Assigning a score
of 0 for a mismatch is tantamount to saying that a
mismatch does not contribute towards the similarity
score. A negative score for a mismatch would result
in assigning a penalty.
We only consider those strings as candidate exam-
ples if their similarity score is greater than a thresh-
old . To assign a class to a name instance, we look
at the k nearest neighbors, as determined by their
similarity scores to the name being classified. To
assign a class to the name, we weight the voting of
each of the k (or fewer) candidates by their similarity
score. A class is assigned only if the the ratio of the
scores of the top two candidates exceeds a thresh-
old, . The precision should tend to increase with
this ratio. 3
4.4 Classifying Based on Context
To identify the best sources of contextual informa-
tion for classifying names, we considered two pos-
sibilities ? the use of a single strong piece of ev-
idence and the use of a combination of weak evi-
dences. For the former we made use Decision Lists
similar to Yarowsky?s method for Word Sense Dis-
ambiguation (WSD) (Yarowsky, 1995). However,
we found that this method had a poor recall.4
3As always, the reason for using a threshold is that it allows
us to find the appropriate level of compromise between preci-
sion and recall. Given that there are different sources of infor-
mation, there is no need to insist that particular method assign a
class tag if we are not comfortable with the level of confidence
that we have in such an assignment.
4Due to space limitations, we don?t discuss why we might
have obtained the poor recall that we got for the decision list
Hence, we decided to use a combination of weak
evidences and employ the Naive-Bayes assumption
of independence between evidences, similar to the
method described in (Gale et al, 1992). To do
this, the words that occurred within a window and
that matched some template pattern were selected
as features if their scores 5 exceeded some thresh-
old (which we name a). Also, unlike Decision Lists,
all the features presented in the context of a name
instance were involved in its classification and the
probability that a name instance has a certain class
was calculated by multiplying probabilities associ-
ated with all the features. As some of the evi-
dences might be fairly weak, we wanted to classify
only those cases where the combination of features
strongly indicated a particular class. This is done
by comparing the two probabilities associated with
the best two classes for an instance. A class was
assigned to a particular name instance only when
the ratio of the two probabilities was beyond a cer-
tain threshold (which will call b). Together with
the threshold, a for the feature selection, choice of
this threshold could allow trade-off between preci-
sion and recall for classification accuracies.
5 Results and Evaluation
5.1 F-Terms and Suffixes
Table 1 gives the precision and recall values for the
first partition for both f-terms and suffixes.6 As
can be seen, the recall for ?Chemical? is very low
as compared to the other classes. This is due to
two reasons?firstly most chemical names consist of
only one word and secondly we found that chemical
names do not end with an indicative word.
The number of f-terms and suffixes extracted by
our program was considerably less for Chemicals
and Protein Parts as compared to Proteins and Oth-
ers. While this is consistent with the the explana-
tion of poor recall for chemicals, it can be noted that
the low number of f-terms and suffixes extracted for
protein parts did not affect its recall in the same man-
ner. As expected the precision remains high for all
classes.
method.
5The scores were simply the conditional probability of a
class given a word
6The suffix list includes f-terms.
F-Term and suffix String Matching Context
F-Term Suffix Alone After Suffix a = 5, b = 2 a = 2, b = 5
Class Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec.
Chemical 0.97 0.05 0.98 0.19 0.89 0.54 0.90 0.59 0.85 0.06 0.55 0.10
Protein 0.97 0.35 0.98 0.55 0.92 0.81 0.93 0.81 0.70 0.31 0.53 0.76
Protein Part 0.98 0.40 0.98 0.33 0.86 0.75 0.85 0.76 0.75 0.05 0.37 0.12
Source 0.98 0.61 0.97 0.62 0.95 0.87 0.94 0.89 0.83 0.10 0.78 0.10
Others 0.99 0.69 0.97 0.71 0.96 0.87 0.96 0.91 0.80 0.05 0.74 0.03
Total 0.98 0.49 0.98 0.57 0.93 0.81 0.93 0.84 0.72 0.17 0.53 0.36
Table 1: Results for the various stages of our method.
Figure 1: Precision-Recall Tradeoff
5.2 Using Examples
For the string matching, we tried three different set
of values for the parameters ,  and k,that is (0.3,
2, 3), (0.7, 2, 1) and (0.7, 2, 5). We found that the
results were marginally better for the set (0.3,2,3)
and, hence, show the results for this set only. Table
1 shows the results of applying the string matching
to the first partition ? all by itself and on names not
classified after the suffix stage. As can be seen, the
recall is higher than the previous stages but with a
slight reduction in precision.
5.3 Results for Context
We ran the context classifier for different values of
the parameters f, a and b but finally chose a value of
5 for f because choosing a higher frequency thresh-
old does not improve the precision but hurts the re-
call. Figure 1 shows the precision plotted against the
recall for different choice of a and b.
The values of the precision and recall on the first
Class Precision Recall
Chemical 0.87 0.62
Protein 0.84 0.90
Protein Part 0.86 0.79
Source 0.94 0.87
Others 0.96 0.90
Total 0.90 0.87
Table 2: Overall Results
partition for each individual class and the two sets
of thresholds are shown in Table 1. The first set,
that considers stronger evidences (since a is higher),
achieves higher precision but recall is not satisfac-
tory. Most of the word evidences chosen tended to
indicate a classification of proteins and hence the
higher recall for this class. Allowing weaker evi-
dences (because a = 2) means more contextual ev-
idences were selected and hence a higher recall was
obtained (particularly for protein). But precision is
lowered except for Source and Others (which inci-
dentally don?t show an increase in recall).
5.4 Overall Results
Table 2 shows the precision and recall for all the dif-
ferent classes, averaging it out for the 4 different par-
titions. We observed very little variance between the
results for the different partitions.
6 Conclusions and Future Work
We have considered a few name internal and exter-
nal sources of information that help in the classifica-
tion of names. Despite using fairly simple methods
to classify the names, we have obtained encourag-
ing results which we take to suggest that that our
intuitions about them are on the right track. We
feel that the effectiveness of f-terms and suffixes that
generalize the idea of f-terms, the matching algo-
rithm that places more emphasis on partial matches
of words to the right vindicates our stance that the
classification of names is a task sufficiently distinct
from the name identification process and warrants
an independent investigation. Even the use of con-
text is different for the two tasks as in the latter task
only the immediately neighboring words are used
and that too for purpose of demarking the extrem-
ities of the name string.
While the high precision of f-terms and suffix
based classification was expected, the recall of these
methods was higher than expected. It is also clear
that these methods do not help much with the chem-
icals class. We believe that in addition to suffix,
the knowledge of other chemical root forms (such as
?meth?), e.g., used in (Narayanaswamy et al, 2003),
would be useful.
We would like to focus more on the matching part
of the work. In particular, rather than hand-coding
our intuitions in terms of weights for the different
parameters, we would like to automatically, e.g., us-
ing a held-out validation set, have these set and see
to what extent the automated choice of parameters
show the bias for the rightmost words in the match-
ing. We would also like to generalize our work fur-
ther by not limiting the classes to the ones chosen
here but allow a wider range of classes. To do this,
we would like to consider the GENIA classes and
collapse classes at various levels of their ontology
and try to see at what level of fine-grained distinc-
tions can classification still be done satisfactorily. In
regards to the use of the contextual method, while
we have some preliminary ideas, we need to inves-
tigate further why the use of a single strong clue,
as exemplified by the decision list method, does not
work as well as it seems to for the WSD task.
References
J. Castan?o, M. Zhang, and J. Pustejovsky. 2002.
Anaphora Resolution in Biomedical Literature. In
Proc. of International Symposium on Reference Res-
olution.
M. Collins and Y. Singer. 1999. Unsupervised Models
for Named Entity Classification. In Proc. of EMNLP
1999.
S. Cucerzan and D. Yarowsky 1999. Language Inde-
pendent Named Entity Recognition Combining Mor-
phological and Contextual Evidence. In Proc. of Joint
SIGDAT Conference on Empirical Methods in NLP
and Very Large Corpora, 90?99.
W. Gale, K. W. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large cor-
pus. Computers and the Humanities, 26:415?439.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. 1998.
Toward information extraction: identifying protein
names from biological papers. Proc. of ISMB 1998,
707?18.
K. Franze?n, G. Eriksson, F. Olsson, L. Asker, P. Lide?n,
and J. Co?ster. 2002. Protein names and how to
find them. International Journal of Medical Informat-
ics special issue on Natural Language Processing in
Biomedical Applications, 67:49?61.
V. Hatzivassiloglou, P. A. Duboue, and A. Rzhetsky.
2001. Disambiguating proteins, genes, and RNA in
text: a machine learning approach. Bioinformatics, 17
Suppl 1: S97?S106.
J. Kazama, T. Makino, Y. Ota, and J. Tsujii. 2002. Tun-
ing Support Vector Machines for Biomedical Named
Entity Recognition. In Proc. of the ACL-02 Workshop
on Natural Language Processing in the Biomedical
Domain, 1?8.
H. Liu, Y. Lussier, and C. Friedman. 2001. Disambigut-
ing Biomedical Terms in Biomedical Narrative Text:
an Unsupervised. Journal of Biomedical Informatics,
34 (4): 249-61.
Proc. of the Sixth Message Understanding Conference
(MUC-6). 1995. Morgan Kaufmann.
M. Narayanaswamy, K. E. Ravikumar, and K. Vijay-
Shanker. 2003. A Biological Named Entity Recog-
nizer. In Proc. of PSB 2003. 8.
J. Pustejovsky, J. Castan?o, J. Zhang, M. Kotecki, and
B. Cochran. 2002. Robust Relational Parsing Over
Biomedical Literature: Extracting Inhibit Relations.
In Proc. of PSB 2002, 7:362?373.
L. Tanabe and W. J. Wilbur 2002. Tagging gene and
protein names in full text articles. In Proc. of the ACL-
02 Workshop on Natural Language Processing in the
Biomedical Domain, 9?13.
D. Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proc. of
ACL 1995, 189?196.
Generation of single-sentence paraphrases from
predicate/argument structure using lexico-grammatical resources
Raymond Kozlowski, Kathleen F. McCoy, and K. Vijay-Shanker
Department of Computer and Information Sciences
University of Delaware
Newark, DE 19716, USA
kozlowsk,mccoy,vijay@cis.udel.edu
Abstract
Paraphrases, which stem from the va-
riety of lexical and grammatical means
of expressing meaning available in a
language, pose challenges for a sen-
tence generation system. In this
paper, we discuss the generation of
paraphrases from predicate/argument
structure using a simple, uniform gen-
eration methodology. Central to our
approach are lexico-grammatical re-
sources which pair elementary seman-
tic structures with their syntactic re-
alization and a simple but powerful
mechanism for combining resources.
1 Introduction
In natural language generation, producing some
realization of the input semantics is not the only
goal. The same meaning can often be expressed
in various ways using dierent lexical and syn-
tactic means. These dierent realizations, called
paraphrases, vary considerably in appropriate-
ness based on pragmatic factors and commu-
nicative goals. If a generator is to come up with
the most appropriate realization, it must be ca-
pable of generating all paraphrases that realize
the input semantics. Even if it makes choices on
pragmatic grounds during generation and pro-
duces a single realization, the ability to generate
them all must still exist.
Variety of lexical and grammatical forms
of expression pose challenges to a generator
((Stede, 1999); (Elhadad et al, 1997); (Nicolov
et al, 1995)). In this paper, we discuss the gen-
eration of single-sentence paraphrases realizing
the same semantics in a uniform fashion using a
simple sentence generation architecture.
In order to handle the various ways of realiz-
ing meaning in a simple manner, we believe that
the generation architecture should not be aware
of the variety and not have any special mech-
anisms to handle the dierent types of realiza-
tions
1
. Instead, we want all lexical and gram-
matical variety to follow automatically from the
variety of the elementary building blocks of gen-
eration, lexico-grammatical resources.
We have developed a fully-operational proto-
type of our generation system capable of gen-
erating the examples presented in this paper,
which illustrate a wide range of paraphrases.
As we shall see, the paraphrases that are pro-
duced by the system depend entirely on the
actual lexicon used in the particular applica-
tion. Determining the range of alternate forms
that constitute paraphrases is not the focus of
this work. Instead, we describe a framework in
which lexico-grammatical resources, if properly
dened, can be used to generate paraphrases.
2 Typical generation methodology
Sentence generation takes as input some seman-
tic representation of the meaning to be conveyed
in a sentence. We make the assumption that
1
Ability to handle variety in a uniform manner is also
important in multilingual generation as some forms avail-
able in one language may not be available in another.
ENJOY
EXPERIENCER THEME
AMY INTERACTION
Figure 1: The semantics underlying (2a-2b)
the input is a hierarchical predicate/argument
structure such as that shown in Fig. 1. The
output of this process should be a set of gram-
matical sentences whose meaning matches the
original semantic input.
One standard approach to sentence genera-
tion from predicate/argument structure (like the
semantic-head-driven generation in (Shieber et
al., 1990)) involves a simple algorithm.
1. decompose the input into the top predicate
(to be realized by a (single) lexical item that
serves as the syntactic head) and identify
the arguments and modiers
2. recursively realize arguments, then modi-
ers
3. combine the realizations in step 2 with the
head in step 1
In realizing the input in Fig. 1, the input can be
decomposed into the top predicate which can be
realized by a syntactic head (a transitive verb)
and its two arguments, the experiencer and the
theme. Suppose that the verb enjoy is chosen
to realize the top predicate. The two arguments
can then be independently realized as Amy and
the interaction. Finally, the realization of the
experiencer, Amy, can be placed in the subject
position and that of the theme, the interaction,
in the complement position, yielding (2a).
Our architecture is very similar but we argue
for a more central role of lexico-grammatical re-
sources driving the realization process.
3 Challenges in generating
paraphrases
Paraphrases come from various sources. In this
section, we give examples of some types of para-
phrases we handle and discuss the challenges
they pose to other generators. We also identify
types of paraphrases we do not consider.
3.1 Paraphrases we handle
Simple synonymy The simplest source of
paraphrases is simple synonymy. We take sim-
ple synonyms to be dierent words that have
the same meaning and are of the same syntactic
category and set up the same syntactic context.
(1a) Booth killed Lincoln.
(1b) Booth assassinated Lincoln.
A generation system must be able to allow
the same semantic input to be realized in dif-
ferent ways. Notice that the words kill and as-
sassinate are not always interchangeable, e.g.,
assassinate is only appropriate when the victim
is a famous person. Such constraints need to be
captured with selectional restrictions lest inap-
propriate realizations be produced.
Dierent placement of argument realiza-
tions Sometimes dierent synonyms, like the
verbs enjoy and please, place argument realiza-
tions dierently with respect to the head, as il-
lustrated in (2a-2b).
(2a) Amy enjoyed the interaction.
(2b) The interaction pleased Amy.
To handle this variety, a uniform generation
methodology should not assume a xed map-
ping between thematic and syntactic roles but
let each lexical item determine the placement of
argument realizations. Generation systems that
use such a xed mapping must override it for
the divergent cases (e.g., (Dorr, 1993)).
Words with overlapping meaning There
are often cases of dierent words that realize dif-
ferent but overlapping semantic pieces. The eas-
iest way to see this is in what has been termed
incorporation, where a word not only realizes a
predicate but also one or more of its arguments.
Dierent words may incorporate dierent argu-
ments or none at all, which may lead to para-
phrases, as illustrated in (3a-3c).
(3a) Charles ew across the ocean.
(3b) Charles crossed the ocean by plane.
(3c) Charles went across the ocean by plane.
Notice that the verb y realizes not only go-
ing but also the mode of transportation being a
plane, the verb cross with its complement real-
ize going whose path is across the object realized
by the complement, and the verb go only real-
izes going. For all of these verbs, the remaining
arguments are realized by modiers.
Incorporation shows that a uniform genera-
tor should use the word choices to determine 1)
what portion of the semantics they realize, 2)
what portions are to be realized as arguments
of the realized semantics, and 3) what portions
remain to be realized and attached as modiers.
Generation systems that assume a one-to-one
mapping between semantic and syntactic units
(e.g., (Dorr, 1993)) must use special processing
for cases of overlapping semantics.
Dierent syntactic categories Predicates
can often be realized by words of dierent syn-
tactic categories, e.g., the verb found and the
noun founding, as in (4a-4b).
(4a) I know that Olds founded GM.
(4b) I know about the founding of GM by Olds.
Words of dierent syntactic categories usu-
ally have dierent syntactic consequences. One
such consequence is the presence of additional
syntactic material. Notice that (4b) contains
the prepositions of and by while (4a) does not.
These prepositions might be considered a syn-
tactic consequence of the use of the noun found-
ing in this conguration. Another syntactic con-
sequence is a dierent placement of argument re-
alizations. The realization of the founder is the
subject of the verb found in (4a) while in (4b)
the use of founding leads to its placement in the
object position of the preposition by.
Grammatical alternations Words can be
put in a variety of grammatical alternations such
as the active and passive voice, as in (5a-5b), the
topicalized form, the it-cleft form, etc.
(5a) Oswald killed Kennedy.
(5b) Kennedy was killed by Oswald.
The choice of dierent grammatical alterna-
tions has dierent syntactic consequences which
must be enforced in generation, such as the pres-
ence or absence of the copula and the dierent
placement of argument realizations. In some
systems such as ones based on Tree-Adjoining
Grammars (TAG), including ours, these con-
sequences are encapsulated within elementary
structures of the grammar. Thus, such systems
do not have to specically reason about these
consequences, as do some other systems.
More complex alternations The same con-
tent of excelling at an activity can be realized by
the verb excel, the adverb well, and the adjective
good, as illustrated in (6a-6c).
(6a) Barbara excels at teaching.
(6b) Barbara teaches well.
(6c) Barbara is a good teacher.
This variety of expression, often called head
switching, poses a considerable diculty for
most existing sentence generators. The di-
culty stems from the fact that the realization
of a phrase (sentence) typically starts with the
syntactic head (verb) which sets up a syntactic
context into which other constituents are t. If
the top predicate is the excelling, we have to be
able to start generation not only with the verb
excel but also with the adverb well and the ad-
jective good, typically not seen as setting up an
appropriate syntactic context into which the re-
maining arguments can be t. Existing genera-
tion systems that handle this variety do so using
special assumptions or exceptional processing,
all in order to start the generation of a phrase
with the syntactic head (e.g., (Stede, 1999), (El-
hadad et al, 1997), (Nicolov et al, 1995), (Dorr,
1993)). Our system does not require that the se-
mantic head map to the syntactic head.
Dierent grammatical forms realizing se-
mantic content Finally, we consider a case,
which to our knowledge is not handled by other
generation systems, where grammatical forms
realize content independently of the lexical item
on which they act, as in (7a-7b).
(7a) Who rules Jordan?
(7b) Identify the ruler of Jordan!
The wh-question form, as used in (7a), real-
izes a request for identication by the listener
(in this case, the ruler of Jordan). Likewise, the
imperative structure (used in (7b)) realizes a re-
quest or a command to the listener (in this case,
to identify the ruler of Jordan).
3.2 Paraphrases we do not consider
Since our focus is on sentence generation and not
sentence planning, we only consider the genera-
tion of single-sentence paraphrases. Hence, we
do not have the ability to generate (8a-8b) from
the same input.
(8a) CS1 has a programming lab.
(8b) CS1 has a lab. It involves programming.
Since we do not reason about the semantic
input, including deriving entailment relations,
we cannot generate (9a-9b) from the same input.
(9a) Oslo is the capital of Norway.
(9b) Oslo is located in Norway.
4 Our generation methodology
Generation in our system is driven by the
semantic input, realized by selecting lexico-
grammatical resources matching pieces of it,
starting with the top predicate. The realization
of a piece containing the top predicate provides
the syntactic context into which the realizations
of the remaining pieces can be t (their place-
ment being determined by the resource).
The key to our ability to handle paraphrases
in a uniform manner is that our processing is
driven by our lexicon and thus we do not make
any a priori assumptions about 1) the amount
of the input realized by a lexical unit, 2) the re-
lationship between semantic and syntactic types
(and thus the syntactic rank or category of the
realization of the top piece), 3) the nature of
the mapping between thematic roles and syn-
tactic positions, and 4) the grammatical alter-
nation (e.g., there are dierent resources for the
same verb in dierent alternations: the active,
passive, topicalized, etc.). Because this informa-
tion is contained in each lexico-grammatical re-
source, generation can proceed no matter what
choices are specied about these in each indi-
vidual resource. Our approach is fundamen-
tally dierent from systems that reason directly
about syntax and build realizations by syntactic
rank ((Bateman, 1997), (Elhadad et al, 1997);
(Nicolov et al, 1995); (Stone and Doran, 1997)).
4.1 Our algorithm
Our generation algorithm is a simple, recursive,
semantic-head-driven generation process, con-
sistent with the approach described in section 2,
but one driven by the semantic input and the
lexico-grammatical resources.
1. given an unrealized input, nd a lexico-
grammatical resource that matches a por-
tion including the top predicate and satis-
es any selectional restrictions
2. recursively realize arguments, then modi-
ers
3. combine the realizations in step 2 with the
resource in step 1, as determined by the re-
source in step 1
Notice the prominence of lexico-grammatical re-
sources in steps 1 and 3 of this algorithm. The
standard approach in section 2 need not be
driven by resources.
4.2 Lexico-grammatical resources
The key to the simplicity of our algorithm lies in
the lexico-grammatical resources, which encap-
sulate information necessary to carry through
generation. These consist of three parts:
 the semantic side: the portion of seman-
tics realized by the resource (including the
predicate and any arguments; this part is
matched against the input semantics)
 the syntactic side: either word(s) in a syn-
tactic conguration or a grammatical form
without words, and syntactic consequences
 a mapping between semantic and syntactic
constituents indicating which constituent
on the semantic side is realized by which
constituent on the syntactic side
Consider the resources for the verbs enjoy and
please in Fig. 2. The semantic sides indicate
that these resources realize the predicate ENJOY
and the thematic roles EXPERIENCER and THEME.
The arguments lling those roles (which must be
realized separately, as indicated by dashed out-
lines) appear as variables X and Y which will be
matched against actual arguments. The syntac-
tic sides contain the verbs enjoy and please in
the active voice conguration. The mappings
include links between ENJOY and its realization
as well as links between the unrealized agent (X)
or theme (Y) and the subject or the complement.
Our mapping between semantic and syntactic
constituents bears resemblance to the pairings in
Synchronous TAG (Shieber and Schabes, 1990).
Just like in Synchronous TAG, the mapping is
VPNP
VP
NPV
enjoy
u
S
0 0
1
ENJOY
EXPERIENCER THEME
X Y
1
VPNP
VP
NPV
please
u
S
0 0
1
ENJOY
EXPERIENCER THEME
X Y
1
Figure 2: Two dierent resources for ENJOY
critical for combining realizations (in step 3 of
our algorithm in section 4.1). There are, how-
ever, advantages that our approach has. For
one, we are not constrained by the isomorphism
requirement in a Synchronous TAG derivation.
Also, the DSG formalism that we use aords
greater exibility, signicant in our approach, as
discussed later in this paper (and in more detail
in (Kozlowski, 2002b)).
4.3 The grammatical formalism
Both step 3 of our algorithm (putting re-
alizations together) and the needs of lexico-
grammatical resources (the encapsulation of
syntactic consequences such as the position
of argument realizations) place signicant de-
mands on the grammatical formalism to be used
in the implementation of the architecture. One
grammatical formalism that is well-suited for
our purposes is the D-Tree Substitution Gram-
mars (DSG, (Rambow et al, 2001)), a variant
of Tree-Adjoining Grammars (TAG). This for-
malism features an extended domain of locality
and exibility in encapsulation of syntactic con-
sequences, crucial in our architecture.
Consider the elementary DSG structures on
the right-hand-side of the resources for enjoy
and please in Fig. 2. Note that nodes marked
with # are substitution nodes corresponding to
syntactic positions into which the realizations of
S
u
0 0
1
1
VPNP
VP
please
NPVu
S
0 0
1
1
NP
the interactionAmy
VPNP
VP
V
enjoy
Figure 3: Combining argument realizations with
the resources for enjoy and please
arguments will be substituted. The positions of
both the subject and the complement are en-
capsulated in these elementary structures. This
allows the mapping between semantic and syn-
tactic constituents to be dened locally within
the resources. Dotted lines indicate domination
of length zero or more where syntactic material
(e.g., modiers) may end up.
4.4 Using resources in our algorithm
Step 1 of our algorithm requires matching the se-
mantic side of a resource against the top of the
input and testing selectional restrictions. A se-
mantic side matches if it can be overlaid against
the input. Details of this process are given
in (Kozlowski, 2002a). Selectional restrictions
(type restrictions on arguments) are associated
with nodes on the semantic side of resources.
In their evaluation, the appropriate knowledge
base instance is accessed and its type is tested.
More details about using selectional restrictions
in generation and in our architecture are given
in (Kozlowski et al, 2002).
Resources for enjoy and please which match
the top of the input in Fig. 1 are shown in
Fig. 2. In doing the matching, the arguments
AMY and INTERACTION are unied with X and
Y. The dashed outlines around X and Y indicate
that the resource does not realize them. Our al-
gorithm calls for the independent recursive real-
ization of these arguments and then putting to-
gether those realizations with the syntactic side
of the resource, as indicated by the mapping.
0u
fly
VPNP
VP
S
0
V
GO
1
PLANE
AGENT MODE
X
ACROSS
VPNP
VP
NPV
cross
u
S
0 0
1
GO
AGENT PATH
X
1
THEME
Y
Figure 4: Two dierent resources for GO
PATH MODE
ACROSS
OCEAN
THEME
AGENT
GO
PLANECHARLES
Figure 5: The semantics underlying (3a-3c) with
portion realized by cross in bold
This is shown in Fig. 3. The argument realiza-
tions, Amy and the interaction, are placed in the
subject and complement positions of enjoy and
please, according to the mapping in the corre-
sponding resources.
4.5 Driving decomposition by resources
The semantic side of a resource determines
which arguments, if any, are realized by the re-
source, while the matching done in step 1 of our
algorithm determines the portions that must be
realized by modiers. This is always done the
same way regardless of the resources selected
and how much of the input they realize, such
as the two resources realizing the predicate GO
shown in Fig. 4, one for y which incorporates
MODE PLANE and another for cross which incor-
porates PATH ACROSS.
YX
AGENT THEME
NP
FOUND
Y
1
X
THEMEAGENT
1
00
S
u
found
V
VP
NP VP
u
N?
the
u 0
NP
D N?
1
FOUND
2
PP
1by
P NP
founding
u
of
P NP
u PP
2
N
N?
Figure 6: Two dierent resources for FOUND
Suppose the semantic input underlying (3a-
3c) is as given in Fig. 5. The portion shown
in bold is realized by the resource for cross in
Fig. 4. The agent of GO and the theme of ACROSS
are to be realized as arguments. The remaining
thematic role MODE with the argument PLANE ll-
ing it, is to be realized by a modier.
4.6 Encapsulation of syntactic
consequences
All syntactic information should be encapsu-
lated within resources and transparent to the
algorithm. This includes the identication of ar-
guments, including their placement with respect
to the realization. Another example of a syn-
tactic consequence is the presence of additional
syntactic material required by the lexical item in
the particular syntactic conguration. The verb
found in the active conguration, as in (4a), does
not require any additional syntactic material.
On the other hand, the noun founding in the
conguration with prepositional phrases headed
by of and by, as in (4b), may be said to require
the use of the prepositions. The resources for
found and founding are shown in Fig. 6. Encap-
sulation of such consequences allows us to avoid
special mechanisms to keep track of and enforce
EXPERIENCER
EXCEL
THEME
[0]:
1
uV
VP
u
at
P
PP
[0]:
excel
VP
[0]:
[0]:
0
u
well
Adv
0Adv?
1Adv?
1
VP
AdvP
P
AGENT
NP
1
PRO
NP
S
P
THEMEEXPERIENCER
EXCEL
00
S
VP
Figure 7: Two dierent resources for EXCEL
them for individual resources.
4.7 Syntactic rank and category
No assumptions are made about the realization
of a piece of input semantics, including its syn-
tactic rank and category. For instance, the pred-
icate EXCEL can be realized by the verb excel,
the adverb well, and the adjective good, as illus-
trated in (6a-6c). The processing is the same:
a resource is selected and any argument realiza-
tions are attached to the resource.
Fig. 7 shows a resource for the predicate
EXCEL realized by the verb excel. What is in-
teresting about this case is that the DSG for-
malism we chose allows us to encapsulate the
PRO in the subject position of the complement
as a syntactic consequence of the verb excel in
this conguration. The other resource for EXCEL
shown in Fig. 7 is unusual in that the predicate
is realized by an adverb, well. Note the link be-
tween the uninstantiated theme on the semantic
side and the position for its corresponding syn-
tactic realization, the substitution node VP
1
2
.
Suppose the semantic input underlying (6a-
2
Also notice that the experiencer of EXCEL is consid-
ered realized by the well resource and coindexed with the
agent of the theme of EXCEL, to be realized by a separate
resource.
[1]
[1]: BARBARA
TEACH
[1]
AGENT
EXCEL
THEMEEXPERIENCER
Figure 8: The semantics underlying (6a-6c)
6c) is as given in Fig. 8 and the well resource in
Fig. 7 is selected to realize the top of the seman-
tics. The matching in step 1 of our algorithm
determines that the subtree of the input rooted
at TEACH must be recursively realized. The re-
alization of this subtree yields Barbara teaches.
Because of the link between the theme of EXCEL
and the VP
1
node of well, the realization Bar-
bara teaches is substituted to the VP
1
node of
well. This is a more complex substitution than
in regular TAG (where the substitution node is
identied with the root of the argument realiza-
tion), and is equivalent to the adjunction of well
to Barbara teaches. In DSG, we are able to treat
structures such as the well structure as initial
and not auxiliary, as TAG would. Thus, argu-
ment realizations are combined with all struc-
tures in a uniform fashion.
4.8 Grammatical forms
As discussed before, grammatical forms them-
selves can realize a piece of semantics. For in-
stance, the imperative syntactic form realizes a
request or a command to the listener, as shown
in Fig. 9. Likewise, the wh-question form real-
izes a request to identify, also shown in Fig. 9.
In our system, whether the realization has any
lexical items is not relevant.
4.9 The role of DSG
We believe that the choice of the DSG formal-
ism plays a crucial role in maintaining our sim-
ple methodology. Like TAG, DSG allows cap-
turing syntactic consequences in one elementary
structure. DSG, however, allows even greater
exibility in what is included in an elementary
structure. Note that in DSG we may have non-
immediate domination links between nodes of
[empty:+]
[subj?empty:+]
[0]:
[0]:
REQUEST
ACTION
ACTION
REQUEST
P
YOU
S
NP
(you)
IDENTIFY
THEME
S 1NP [inv:+]
S
NP
?who
uN
YOU SET?OF
THEME SUCH?THAT
P
AGENT
Figure 9: Two dierent resources for REQUEST
dierent syntactic categories (e.g., between the S
and NP in Fig. 9 and also in the excel at structure
in Fig. 7). DSG also allows uniform treatment
of complementation and modication using the
operations of substitution (regardless of the re-
alization of the predicate, e.g., the structures in
Fig. 7) and adjunction, respectively.
5 Conclusions
Although we only consider paraphrases with the
same semantics, there is still a wide variety of
expression which poses challenges to any genera-
tion system. In overcoming those challenges and
generating in a simple manner in our architec-
ture, our lexico-grammatical resources play an
important role in each phase of generation. En-
capsulation of syntactic consequences within ele-
mentary syntactic structures keeps our method-
ology modular. Whatever those consequences,
often very dierent for dierent paraphrases,
generation always proceeds in the same manner.
Both the algorithm and the constraints on
our lexico-grammatical resources place signif-
icant demands on the grammatical formalism
used for the architecture. We nd that the DSG
formalism meets those demands well.
References
John Bateman. 1997. Enabling technology for mul-
tilingual natural language generation: the KPML
development environment. Natural Language En-
gineering, 3(1):15{55.
Bonnie J. Dorr. 1993. Interlingual machine transla-
tion: a parametrized approach. Articial Intelli-
gence, 63(1):429{492.
Michael Elhadad, Kathleen McKeown, and Jacques
Robin. 1997. Floating constraints in lexical
choice. Computational Intelligence, 23:195{239.
Raymond Kozlowski, Kathleen F. McCoy, and
K. Vijay-Shanker. 2002. Selectional restrictions
in natural language sentence generation. In Pro-
ceedings of the 6th World Multiconference on Sys-
temics, Cybernetics, and Informatics (SCI'02).
Raymond Kozlowski. 2002a. Driving multilingual
sentence generation with lexico-grammatical re-
sources. In Proceedings of the Second Interna-
tional Natural Language Generation Conference
(INLG'02) - Student Session.
Raymond Kozlowski. 2002b. DSG/TAG - An appro-
priate grammatical formalism for exible sentence
generation. In Proceedings of the Student Research
Workshop at the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL'02).
Nicolas Nicolov, Chris Mellish, and Graeme Ritchie.
1995. Sentence Generation from Conceptual
Graphs. In Proceedings of the 3rd International
Conference on Conceptual Structures (ICCS'95).
Owen Rambow, K. Vijay-Shanker, and David Weir.
2001. D-Tree Substitution Grammars. Computa-
tional Linguistics, 27(1):87{122.
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous Tree-Adjoining Grammars. In Proceed-
ings of the 13th International Conference on Com-
putational Linguistics.
Stuart M. Shieber, Gertjan van Noord, Fernando
C. N. Pereira, and Robert C. Moore. 1990.
Semantic-Head-Driven Generation. Computa-
tional Linguistics, 16(1):30{42.
Manfred Stede. 1999. Lexical semantics and knowl-
edge representation in multilingual text genera-
tion. Kluwer Academic Publishers, Boston.
Matthew Stone and Christine Doran. 1997. Sen-
tence Planning as Description Using Tree Adjoin-
ing Grammar. In Proceedings of the 35th Annual
Meeting of the Association for Computational Lin-
guistics (ACL'97).
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 118?119,
New York City, June 2006. c?2006 Association for Computational Linguistics
Rapid Adaptation of POS Tagging for Domain Specific Uses John E. Miller1 Michael Bloodgood1 Manabu Torii2 K. Vijay-Shanker1 1Computer & Information Sciences 2Biostatistics, Bioinformatics and Biomathematics University of Delaware Georgetown University Medical Center Newark, DE 19716 Washington, DC 20057 {jmiller,bloodgoo,vijay}@cis.udel.edu mt352@georgetown.edu 
1 Introduction Part-of-speech (POS) tagging is a fundamental component for performing natural language tasks such as parsing, information extraction, and ques-tion answering.  When POS taggers are trained in one domain and applied in significantly different domains, their performance can degrade dramati-cally.  We present a methodology for rapid adapta-tion of POS taggers to new domains.  Our technique is unsupervised in that a manually anno-tated corpus for the new domain is not necessary.  We use suffix information gathered from large amounts of raw text as well as orthographic infor-mation to increase the lexical coverage. We present an experiment in the Biological domain where our POS tagger achieves results comparable to POS taggers specifically trained to this domain.   Many machine-learning and statistical tech-niques employed for POS tagging train a model on an annotated corpus, such as the Penn Treebank (Marcus et al 1993). Most state-of-the-art POS taggers use two main sources of information: 1) Information about neighboring tags, and 2) Infor-mation about the word itself. Methods using both sources of information for tagging are: Hidden Markov Modeling, Maximum Entropy modeling, and Transformation Based Learning (Brill, 1995).  In moving to a new domain, performance can degrade dramatically because of the increase in the unknown word rate as well as domain-specific word use. We improve tagging performance by attacking these problems. Since our goal is to em-ploy minimal manual effort or domain-specific knowledge, we consider only orthographic, inflec-tional and derivational information in deriving POS. We bypass the time, cost, resource, and con-tent expert intensive approach of annotating a cor-pus for a new domain. 
2 Methodology and Experiment The initial components in our POS tagging process are a lexicon and part of speech (POS) tagger trained on a generic domain corpus. The lexicon is updated to include domain specific information based on suffix rules applied to an un-annotated corpus. Documents in the new domain are POS tagged using the updated lexicon and orthographic information. So, the POS tagger uses the domain specific updated lexicon, along with what it knows from generic training, to process domain specific text and output POS tags. In demonstrating feasibility of the approach, we used the fnTBL-1.0 POS tagger (Ngai and Florian, 2001) based on Brill?s Transformation Based Learning (Brill, 1995) along with its lexicon and contextual rules trained on the Wall Street Journal corpus.  To update the lexicon, we processed 104,322 abstracts from five of the 500 compressed data files in the 2005 PubMed/Medline database (Smith et al 2004). As a result of this update, coverage of words with POS tags from the lexicon increased from 73.0% to 89.6% in our test corpus. Suffix rules were composed based on informa-tion from Michigan State University?s Suffixes and Parts of Speech web page for Graduate Record Exams (DeForest, 2000). The suffix endings indi-cate the POS used for new words. However, as seen in the table of suffix examples below, there can be significant lack of precision in assigning POS based just on suffixes.  Suffix POS #uses/ %acc ize; izes VB VBP; VBZ 23/100% ous JJ 195/100% er, or; ers, ors NN; NNS 1471/99.5% ate; ates VB VBP 576/55.7% 
118
Most suffixes did well in determining the actual POS assigned to the word. Some such as ?-er? and ?-or? had very broad use as well. ?-ate? typically forms a verb from a noun or adjective in a generic domain. However in scientific domains it often indicates a noun or adjective word form. (In work just begun, we add POS assignment confirmation tests to suffix rules so as to confirm POS tags while maintaining our domain independent and unsupervised analysis of un-annotated corpora.) Since the fnTBL POS tagger gives preliminary assignment of POS tags based on the first POS listed for that word in the lexicon, it is vital that the first POS tag for a common word be correct. Words ending in ?-ing? can be used in a verbal (VBG), adjectival (JJ) or noun (NN) sense. Our intuition is that the ?-ed? form should also appear often when the verbal sense dominates. In contrast, if the ratio heavily favors the ?-ing? form then we expect the noun sense to dominate.  We incorporated this reasoning into a computa-tionally defined process which assigned the NN tag first to the following words: binding, imaging, learning, nursing, processing, screening, signal-ing, smoking, training, and underlying. Only un-derlying seems out of place in this list.  In addition to inflectional and derivational suf-fixes, we used rules based on orthographic charac-teristics. These rules defined proper noun and number or code categories. 3 Results and Conclusion For testing purposes, we used approximately half the abstracts of the GENIA corpus (version 3.02) described in (Tateisi et al 2003). As the GENIA corpus does not distinguish between common and proper nouns we dropped that distinction in evalu-ating tagger performance.  POS tagging accuracy on our GENIA test set (second half of abstracts) consisting of 243,577 words is shown in the table below. Source Accuracy Original fnTBL lexicon 92.58% Adapted lexicon (Rapid) 94.13% MedPost 94.04% PennBioIE1 93.98%                                                            1 Note that output from the tagger is not fully compatible with GENIA annotation. 
The original fnTBL tagger has an accuracy of 92.58% on the GENIA test corpus showing that it deals well with unknown words from this domain. Our rapid adaptation tagger achieves a modest 1.55% absolute improvement in accuracy, which equates to a 21% error reduction.   There is little difference in performance be-tween our rapid adaptation tagger and the MedPost (Smith et al 2004) and PennBioIE (Kulick et al 2004) taggers. The PennBioIE tagger employs maximum entropy modeling and was developed using 315 manually annotated Medline abstracts. The MedPost tagger also used domain-specific annotated corpora and a 10,000 word lexicon, manually updated with POS tags. We have improved the accuracy of the fnTBL-1.0 tagger for a new domain by adding words and POS tags to its lexicon via unsupervised methods of processing raw text from the new domain. The accuracy of the resulting tagger compares well to those that have been trained to this domain using annotation effort and domain-specific knowledge.  References Brill, E. 1995. Transformation-based error-driven learn-ing and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565.   DeForest, Jessica. 2000. Graduate Record Exam Suffix web page. Michigan State University. http:// www.msu.edu/~defores1/gre/sufx/gre_suffx.htm.  Kulick, S., Bies, A., Liberman, M., Mandel, M., McDonald, R., Palmer, M., Schein, A., Ungar, L. 2004. Integrated annotation for biomedical informa-tion extraction. HLT/NAACL-2004: 61-68. Marcus, M., Santorini, B., Marcinkiewicz, M.A. 1993. Building a large annotated corpus of English: The Penn Treebank.  Computational Linguistics, 19:313-330.   Ngai, G. and Florian, R. 2001.  Transformation-based learning in the fast lane.  In Proceedings of North America ACL 2001(June): 40-47.   Smith, L., Rindflesch, T., Wilbur, W.J. 2004. MedPost: a part-of-speech tagger for bioMedical text.  Bioin-formatics 20 (14):2320-2321.   Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsujii, J. 2003. The GENIA corpus: Medline ab-stracts annotated with linguistic information.  In: Third meeting of SIG on Text Mining, Intelligent Systems for Molecular Biology (ISMB).   
119
BioNLP 2007: Biological, translational, and clinical language processing, pages 179?180,
Prague, June 2007. c?2007 Association for Computational Linguistics
 Adaptation of POS Tagging for Multiple BioMedical Domains John E. Miller1 Manabu Torii2 K. Vijay-Shanker1 1Computer & Information Sciences University of Delaware Newark, DE 19716 {jmiller,vijay}@cis.udel.edu 2Biostatistics, Bioinformatics and Biomathematics Georgetown University Medical Center  Washington, DC 20057 mt352@georgetown.edu   1 Introduction Part of Speech (POS) tagging is often a prerequi-site for tasks such as partial parsing and informa-tion extraction. However, when a POS tagger is simply ported to another domain the tagger?s accu-racy drops. This problem can be addressed through hand annotation of a corpus in the new domain and supervised training of a new tagger. In our meth-odology, we use existing raw text and a generic POS annotated corpus to develop taggers for new domains without hand annotation or supervised training. We focus in particular on out-of-vocabulary words since they reduce accuracy (Lease and Charniak. 2005; Smith et al 2005).  There is substantial information in the deriva-tional suffixes and few inflectional suffixes of English.  We look at individual words and their suffixes along with the morphologically related words to build a domain specific lexicon contain-ing POS tags and probabilities for each word.  2 Adaptation Methodology Our methodology is described in detail in Miller et al(2007) and summarized here: 1) Process ge-neric POS annotated text to obtain state and lexical POS tag probabilities. 2) Obtain a frequency table of words from a large corpus of raw sub-domain text. 3) Construct a partial sub-domain lexicon matching relative frequencies of morphologically related words with words from the generic anno-tated text averaging POS probabilities of the k nearest neighbors. 4) Combine common generic words and orthographic word categories with the partial lexicon making the sub-domain lexicon. 5) Train a first order Hidden Markov Model (HMM) by Expectation Maximization (EM). 6) Apply the Viterbi algorithm with the HMM to tag sub-domain text. 
3 Adaptation to Multiple Domains Molecular Biology Domain: We used the Wall Street Journal corpus (WSJ) (Marcus et al 1993) as our generic POS annotated corpus. For our raw un-annotated text we used 133,666 abstracts from the MEDLINE distribution covering molecular biology and biomedicine sub-domains. We split the GENIA database  (Tateisi et al 2003) into training and test portions and ignored the POS tags for training. We ran a 5-fold cross validation study and obtained an average accuracy of 95.77%.  Medical Domain: Again we used the WSJ as our generic POS annotated corpus. For our raw un-annotated text we used 164,670 abstracts from the MEDLINE distribution with selection based on 83 journals from the medical domain. For our HMM EM training we selected 1966 abstracts (same journals). For evaluation purposes, we selected 1932 POS annotated sentences from the MedPost (Smith et al 2004) distribution (same journals). The MedPost tag set coding was converted to the Penn Treebank tag set using the utilities provided with the MedPost tagger distribution. We obtained an accuracy of 93.17% on the single medical test corpus, a substantial drop from the 95.77% average accuracy obtained in the GENIA corpus.  4 Coding Differences We looked at high frequency tagging errors in the medical test set and found that many errors resulted directly from the differences in the coding styles between GENIA and MedPost. Our model reflects the coding style of the WSJ, used for our generic POS annotated text. GENIA largely fol-lowed the WSJ coding conventions. Annotation in the 1932 sentences taken from MedPost had some systematic differences in coding style from this.  
179
Identified Differences: Lexical differences: 1) Words such as ?more? and ?less? are JJR or RBR in WSJ/GENIA but JJ or RB in MedPost. 2) Tokens such as %, =, /, <, > are typically NN or JJ in WSJ/GENIA but SYM in MedPost. 3)?be? is VB in WSJ/GENIA but VB or VBP in MedPost. 4) Some orthographic categories are JJ in WSJ/GENIA but NN in MedPost. Transition discrepancies: 1) Verbs are tagged VB following a TO or MD in WSJ/GENIA but only following a TO in MedPost. 2) MedPost prefers NN and NN-NN sequences. Ad Hoc Adjustments: We constructed a new lexicon accounting for some of the lexical differ-ences and attained an accuracy of 94.15% versus the previous 93.17%. Next we biased a few initial state transition probabilities, changing P(VB|MD) from very high to a very low and increasing P(NN|NN), and attained an accuracy of 94.63%.  As the coding differences had nothing to do with suffixes and suffix distributions, the central part of our methodology, we tried some ad hoc fixes to determine what our performance might have been. We suffered at least a 1.46% drop in accuracy due to differences in coding, not language use. 5 Evaluation The table shows the accuracy of our tagger and a few well-known taggers in our target biomedical sub-domains.   Molecular Biology %Accuracy - Our  tagger (5-fold) 95.8% - MedPost  94.1% - Penn BioIE1 95.1% - GENIA supervised 98.3% Medical Domain  - Our  tagger  93.17% - Our  tagger (+ lex bias) 94.15% - Our tagger (+ lex & trans bias) 94.63% - MedPost supervised2 96.9% The MedPost and Penn BioIE taggers used an-notated text and supervised training in other bio-medical domains, but they were not trained spe-cifically for the GENIA Molecular Biology sub-domain. Our tagger seems competitive with these                                                  1 PennBioIE. 2005. Mining The Bibliome Project. http://bioie.ldc.upenn.edu/. 2 Based on Medpost test set of 1000 sentences, not on our test set of 1932 sentences.  
taggers.  We cannot claim superior accuracy as these taggers may suffer the same coding bias ef-fects we have noted. The superior performance of the GENIA tagger (Tsuruoka et al 2005) in the Molecular Biology/GENIA domain and the Med-Post tagger (Smith et al 2004) in its biomedical domain owes to their use of supervised training on an annotated training set with evaluation on a test set from the same domain. The approximate 1.5% bias effect due to coding differences is attributable to organizational differences in POS.  6 Conclusions  To cope with domain specific vocabulary and uses of vocabulary, we exploited the suffix information of words and related words to build domain spe-cific lexicons. We trained our HMM using EM and un-annotated text from the specialized domains. We assessed accuracy versus annotated test sets in the specialized domains, noting discrepancies in our results across specialized domains, and con-cluding that our methodology performs competi-tively versus well-known taggers that used anno-tated text and supervised training in other biomedi-cal domains. References M. Lease and E. Charniak. 2005. Parsing Biomedical Literature.  IJCNLP-05: 58-69.   M. Marcus, B. Santorini, M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank.  Comp. Ling., 19:313-330.   J.E. Miller, M. Torii, K. Vijay-Shanker. 2007. Building Domain-Specific Taggers Without Annotated (Do-main) Data. EMNLP-07.   L. Smith, T. Rindflesch, W.J. Wilbur. 2004. MedPost: a part-of-speech tagger for bioMedical text.  Bioinfor-matics 20 (14):2320-2321.   L. Smith, T. Rindflesch, W.J. Wilbur. 2005. The impor-tance of the lexicon in tagging biomedical text. Natu-ral Language Engineering 12(2) 1-17. Y. Tateisi, T. Ohta, J. Dong Kim, H. Hong, S. Jian, J. Tsujii. 2003. The GENIA corpus: Medline abstracts annotated with linguistic information. Third meeting of SIG on Text Mining, ISMB.   Y. Tsuruoka, Y. Tateishi, J.D. Kim, T. Ohta, J. McNaught, S. Ananiadou, J. Tsujii. 2005. Developing a Robust Part-of-Speech Tagger for Biomedical Text, Advances in Informatics, LNCS 3746: 382-392. 
180
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 28?29,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mining the Biomedical Literature for Genic Information
Catalina O. Tudor ? K. Vijay-Shanker ? Carl J. Schmidt ?
Department of Computer and Information Sciences ?
Department of Animal and Food Sciences ?
University of Delaware, Newark, DE 19716
{tudor,vijay}@cis.udel.edu schmidtc@udel.edu
Abstract
eGIFT (Extracting Gene Information From
Text) is an intelligent system which is in-
tended to aid scientists in surveying litera-
ture relevant to genes of interest. From a
gene specific set of abstracts retrieved from
PubMed, eGIFT determines the most impor-
tant terms associated with the given gene.
Annotators using eGIFT can quickly find ar-
ticles describing gene functions and individ-
uals scientists surveying the results of high-
throughput experiments can quickly extract
information important to their hits.
1 Introduction
Given the huge number of articles from the biomed-
ical domain, it has become very difficult for scien-
tists to quickly search and find the information they
need. Systems to facilitate literature search are being
built. E.g. GoPubMed (Doms and Schroeder, 2005)
clusters abstracts retrieved from PubMed based on
GO and MeSH terms, iHOP (Hoffman and Valen-
cia, 2005) connects biomedical literature based on
genes, EBIMed (Rebholz-Schuhmann et al, 2006)
displays sentences containing GO terms, drugs, and
species.
In contrast to these systems, eGIFT automatically
identifies the most relevant terms associated with a
given gene. We believe that such a retrieval of terms
could itself enable the scientists to form a reason-
able good idea about the gene. For example, some
of the top key phrases associated with Groucho (En-
trez Gene ID 43162) by eGIFT are: transcriptional
corepressor, segmentation, neurogenesis and wd40.
This might immediately inform a user that Grou-
cho is probably a transcriptional corepressor, that
it might be involved in the processes of segmenta-
tion and neurogenesis and that it might contain the
wd40 domain, which allows them to draw further in-
ferences about the gene. To enable the scientists to
get a deeper understanding, eGIFT further allows the
retrieval of all sentences from this gene?s literature
containing the key phrase in question. The sentences
can be displayed in isolation or in the context of the
abstract in which they appear.
2 Ranking Key Terms
(Andrade and Valencia, 1998) automatically ex-
tracted keywords from scientific text by computing
scores for each word in a given protein family, based
on the frequency of the word in the family, the aver-
age frequency of the word and the deviation of word
distribution over all families. (Liu et al, 2004) ex-
tended this method to statistically mine functional
keywords associated with genes.
Our application is somewhat similar in that we
compare the distribution of phrases in the abstracts
about the gene from some background set. We use
statistical methods to identify the situations where
the different frequencies of appearance of a term
in two sets of the literature are statistically interest-
ing. We differ from the above work by choosing a
broader range of background information. Our moti-
vation is to retrieve any type of phrases, thus not lim-
iting ourselves to only functional terms or terms that
might differentiate the selected set of protein fami-
lies. Since we no longer have several sets of litera-
28
ture, our approach differs from the above method in
that we cannot base the score on average frequencies
and term deviation in the same way.
Background Set (BSet): In order to capture a wide
range of information about genes in general, we
downloaded from PubMed all the abstracts for the
following boolean query: gene[tiab] OR genes[tiab]
OR protein[tiab] OR proteins[tiab]. Approximately
640,000 non-empty abstracts were found.
Query Set (QSet): We download from PubMed the
abstracts that mention a given gene name and its syn-
onyms. We obtained the latter from BioThesaurus
(Liu et al, 2005).
Key Term Scores: We considered many different
statistical tests to identify significant key phrases,
but eventually settled on the following score:
st = (
dctq
Nq
?
dctb
Nb
) ? ln
(
Nb
dctb
)
where dctb and dctq are the background and query
document counts of term t, and Nb and Nq are the
total number of documents from the BSet and QSet.
The difference in frequencies (dctqNq ?
dctb
Nb
) gives
preference to terms that appear more frequently in
the QSet than in the BSet. This way, we would
like to capture terms that are common to the given
gene but not to genes and proteins in general. The
difference itself is not sufficient to eliminate com-
mon words. To address this problem, similar to the
use of IDF in IR, we add a global frequency term
(ln
(
Nb
dctb
)
) to further penalize common terms, such
as protein.
To better understand how the score is computed,
consider the gene Groucho and its key term core-
pressor, which was mentioned in 66% of the QSet
and only in 0.1% of the BSet. The huge difference
in frequencies, together with the low background
frequency, helped the key term corepressor score
4.3617, while most of the terms score below 0.25.
Enhancements to Basic Method: First, we ex-
tended our method to include unigrams, bigrams,
and multi-word terms where previously identified.
We observed that some words are not meaningful
when presented alone. For instance, the words de-
velopment and embryonic taken separately are not
as informative as when put together into embryonic
development, a term which was ranked much higher
than the two words.
Next, we applied morphological grouping on
terms, based on manually developed rules, after ob-
serving variances within the same concept. In writ-
ing, we can say corepressor, co-repressor, or co-
repressors. In order to capture the concept, we com-
puted frequencies on morphological groups and not
on each individual term.
Last, we divided key terms into categories by
using morphological information to separate terms
such as descriptors, and by consulting publicly avail-
able controlled vocabularies (such as NCBI Con-
served Domains, NCBI Taxonomy, MedlinePlus,
DrugBank, and MeSH category A01).
3 Assessment
Our method has been applied on 55 different genes
selected by annotators for a public resource. The
initial feedback has been encouraging. Also pre-
liminary investigations suggest we get far more key-
words associated with some genes in resources such
as GenBank, SwissProt and Gene Ontology than the
system of (Liu et al, 2004). Our next goal is to do a
thorough evaluation of our system.
References
Miguel A Andrade and Alfonso Valencia. 1998. Auto-
matic extraction of keywords from scientific text: ap-
plication to the knowledge domain of protein families.
Bioinformatics, 14(7):600?607.
Andreas Doms and Michael Schroeder. 2005. GoP-
ubMed: exploring PubMed with the Gene Ontology.
Nucleid Acid Research, 33:w783?w786.
Robert Hoffman and Alfonso Valencia. 2005. Imple-
menting the iHOP concept for navigation of biomedi-
cal literature. Bioinformatics, 21:ii252?ii258.
Ying Liu, Martin Brandon, Shamkant Navathe, Ray Din-
gledine, and Brian J. Ciliax. 2004. Text mining
functional keywords associated with genes. MedInfo,
11:292?296.
Hongfang Liu, Zhang-Zhiu Hu, Jian Zhang, and Cathy
Wu. 2005. Biothesaurus: a web-based thesaurus of
protein and gene names. Bioinformatics, 22(1):103?
105.
Dietrich Rebholz-Schuhmann, Harald Kirsch, Miguel
Arregui, Sylvain Gaudan, Mark Riethoven, and Peter
Stoehr. 2006. EBIMed - text crunching to gather facts
for proteins from Medline. Bioinformatics, 23:e237?
e244.
29
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 104?105,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Approach to Reducing Annotation Costs for BioNLP 
Michael Bloodgood 
Computer and Information Sciences 
University of Delaware 
Newark, DE 19716 
bloodgoo@cis.udel.edu 
K. Vijay-Shanker 
Computer and Information Sciences 
University of Delaware 
Newark, DE 19716 
vijay@cis.udel.edu 
1 Introduction 
There is a broad range of BioNLP tasks for which 
active learning (AL) can significantly reduce anno-
tation costs and a specific AL algorithm we have 
developed is particularly effective in reducing an-
notation costs for these tasks. We have previously 
developed an AL algorithm called ClosestInitPA 
that works best with tasks that have the following 
characteristics: redundancy in training material, 
burdensome annotation costs, Support Vector Ma-
chines (SVMs) work well for the task, and imbal-
anced datasets (i.e. when set up as a binary 
classification problem, one class is substantially 
rarer than the other). Many BioNLP tasks have 
these characteristics and thus our AL algorithm is a 
natural approach to apply to BioNLP tasks.   
2 Active Learning Algorithm 
ClosestInitPA uses SVMs as its base learner. This 
fits well with many BioNLP tasks where SVMs 
deliver high performance (Giuliano et al, 2006; 
Lee et al, 2004). ClosestInitPA is based on the 
strategy of selecting the points which are closest to 
the current model?s hyperplane (Tong and Koller, 
2002) for human annotation. ClosestInitPA works 
best in situations with imbalanced data, which is 
often the case for BioNLP tasks. For example, in 
the AIMed dataset annotated with protein-protein 
interactions, the percentage of pairs of proteins in 
the same sentence that are annotated as interacting 
is only 17.6%.  
SVMs (Vapnik, 1998) are learning systems that 
learn linear functions for classification. A state-
ment of the optimization problem solved by soft-
margin SVMs that enables the use of asymmetric 
cost factors is the following: 
Minimize: ??
?=
?
+=
+
++
1:1:
2||||
2
1
yy
w
ji
j
j
i
i CC ??r  (1) 
Subject to: kkk bxwyk ???+?? 1][: rr          (2) 
where ),( bwr  represents the hyperplane that is 
learned, kx
r
is the feature vector for example k, yk 
in {+1,-1} is the label for example k, 
])[1,0max( bxwy kkk +??=
rr?
 
is the slack vari-
able for example k, and C+ and C- are user-defined 
cost factors that trade off separating the data with a 
large margin and misclassifying training examples. 
Let PA=C+/C-. PA stands for ?positive amplifi-
cation.? We use this term because as the PA is in-
creased, the importance of positive examples is 
amplified. ClosestInitPA is described in Figure 3. 
We have previously shown that setting PA based 
on a small initial set of data outperforms the more 
obvious approach of using the current labeled data 
to estimate PA. 
 
 
Figure 3. ClosestInitPA algorithm. 
 
We have previously developed a stopping crite-
rion called staticPredictions that is based on stop-
ping when we detect that the predictions of our 
models on some unlabeled data have stabilized. All 
of the automatic stopping points in our results are 
determined using staticPredictions.  
Initialization: 
? L = small initial set of labeled data 
? U = large pool of unlabeled data 
 
L
LPA
in  examples pos #
in  examples neg #
=  
Loop until stopping criterion is met: 
1. Train an SVM with parameters C+ 
and C
-
 set such that C+/C- = PA. 
2. batch = select k points from U that 
are closest to the hyperplane learned 
in step 1. 
U = U ? batch 
L = L U batch 
104
3 Experiments 
Protein-Protein Interaction Extraction: We used 
the AImed corpus, which was previously used for 
training protein interaction extraction systems in 
(Giuliano et al, 2006). We cast RE as a binary 
classification task as in (Giuliano et al, 2006). 
We do 10-fold cross validation and use what is 
referred to in (Giuliano et al, 2006) as the KGC 
kernel with SVMlight (Joachims, 1999) in our ex-
periments. Table 1 reports the results. 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 1012 48.33 54.34 
30% 1516 49.76 54.52 
40% 2022 53.11 56.39 
100% 5060 57.54 57.54 
AutoStopPoint 1562 51.25 55.34 
Table 1. AImed Stopping Point Performance. ?AutoS-
topPoint? is when the stopping criterion says to stop. 
 
Medline Text Classification: We use the Oh-
sumed corpus (Hersh, 1994) and a linear kernel 
with SVMlight with binary features for each word 
that occurs in the training data at least three times. 
Results for the five largest categories for one ver-
sus the rest classification are in Table 2. 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 1260 49.99 61.49 
30% 1880 54.18 62.72 
40% 2500 57.46 63.75 
100% 6260 65.75 65.75 
AutoStopPoint 1204 47.06 60.73 
Table 2. Ohsumed stopping point performance. ?AutoS-
topPoint? is when the stopping criterion says to stop. 
 
GENIA NER: We assume a two-phase model 
(Lee et al, 2004) where boundary identification of 
named entities is performed in the first phase and 
the entities are classified in the second phase. As in 
the semantic classification evaluation of (Lee et al, 
2004), we assume that boundary identification has 
been performed. We use features based on those 
from (Lee et al, 2004), a one versus the rest setup 
and 10-fold cross validation. Tables 3-5 show the 
results for the three most common types in 
GENIA. 
 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 13440 86.78 90.16 
30% 20120 87.81 90.27 
40% 26900 88.55 90.32 
100% 67220 90.28 90.28 
AutoStopPoint 8720 85.41 89.24 
Table 3. Protein stopping points performance. ?AutoS-
topPoint? is when the stopping criterion says to stop. 
 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 13440 79.85 82.06 
30% 20120 80.40 81.98 
40% 26900 80.85 81.84 
100% 67220 81.68 81.68 
AutoStopPoint 7060 78.35 82.29 
Table 4. DNA stopping points performance. ?AutoS-
topPoint? is when the stopping criterion says to stop. 
 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 13440 84.01 86.76 
30% 20120 84.62 86.63 
40% 26900 85.25 86.45 
100% 67220 86.08 86.08 
AutoStopPoint 4200 81.32 86.31 
 Table 5. Cell Type stopping points performance. ?Au-
toStopPoint? is when the stopping criterion says to stop. 
4 Conclusions 
ClosestInitPA is well suited to many BioNLP 
tasks. In experiments, the annotation savings are 
practically significant for extracting protein-protein 
interactions, classifying Medline text, and perform-
ing biomedical named entity recognition.  
References 
 
Claudio Giuliano, Alberto Lavelli, and Lorenza Roma-
no. 2006. Exploiting Shallow Linguistic Information 
for Relation Extraction from Biomedical Literature. 
In Proceedings of the EACL, 401-408. 
 William Hersh, Buckley, C., Leone, T.J., and Hickman, 
D. (1994). Ohsumed: an interactive retrieval evalua-
tion and new large text collection for research. ACM 
SIGIR.  
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, MIT-Press, 169-184. 
Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, and 
Hae-Chang Rim. 2004. Biomedical named entity 
recognition using two-phase model based on SVMs. 
Journal of Biomedical Informatics, Vol 37, 436?447. 
Simon Tong and Daphne Koller. 2002. Support vector 
machine active learning with applications to text 
classification. JMLR 2: 45-66. 
Vladimir Vapnik. 1998. Statistical Learning Theory. 
John Wiley & Sons, New York, NY, USA.  
 
105
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 39?47,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Method for Stopping Active Learning Based on Stabilizing Predictions
and the Need for User-Adjustable Stopping
Michael Bloodgood?
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211 USA
bloodgood@jhu.edu
K. Vijay-Shanker
Computer and Information
Sciences Department
University of Delaware
Newark, DE 19716 USA
vijay@cis.udel.edu
Abstract
A survey of existing methods for stopping ac-
tive learning (AL) reveals the needs for meth-
ods that are: more widely applicable; more ag-
gressive in saving annotations; and more sta-
ble across changing datasets. A new method
for stopping AL based on stabilizing predic-
tions is presented that addresses these needs.
Furthermore, stopping methods are required
to handle a broad range of different annota-
tion/performance tradeoff valuations. Despite
this, the existing body of work is dominated
by conservative methods with little (if any) at-
tention paid to providing users with control
over the behavior of stopping methods. The
proposed method is shown to fill a gap in the
level of aggressiveness available for stopping
AL and supports providing users with control
over stopping behavior.
1 Introduction
The use of Active Learning (AL) to reduce NLP an-
notation costs has generated considerable interest re-
cently (e.g. (Bloodgood and Vijay-Shanker, 2009;
Baldridge and Osborne, 2008; Zhu et al, 2008a)).
To realize the savings in annotation efforts that AL
enables, we must have a mechanism for knowing
when to stop the annotation process.
Figure 1 is intended to motivate the value of stop-
ping at the right time. The x-axis measures the num-
ber of human annotations that have been requested
and ranges from 0 to 70,000. The y-axis measures
? This research was conducted while the first author was a
PhD student at the University of Delaware.
0 1 2 3 4 5 6 7
x 104
65
70
75
80
85
90
Active Learning Curve (F Measure vs Number of Annotations)
Number of Points for which     
Annotations Have Been Requested
Pe
rfo
rm
an
ce
 (F
 M
ea
su
re)
 
 
stop point 1: 
stops too early;
results in lower 
performing model
stop point 2: 
good place to stop
stop point 3: 
stops too late; 
wastes around 
30,000 human 
annotations
Figure 1: Hypothetical Active Learning Curve with hy-
pothetical stopping points.
performance in terms of F-Measure. As can be seen
from the figure, the issue is that if we stop too early
while useful generalizations are still being made, we
wind up with a lower performing system but if we
stop too late after all the useful generalizations have
been made, we just wind up wasting human annota-
tion effort.
The terms aggressive and conservative will be
used throughout the rest of this paper to describe the
behavior of stopping methods. Conservative meth-
ods tend to stop further to the right in Figure 1.
They are conservative in the sense that they?re very
careful not to risk losing significant amounts of F-
measure, even if it means annotating many more ex-
amples than necessary. Aggressive methods, on the
other hand, tend to stop further to the left in Figure 1.
They are aggressively trying to reduce unnecessary
annotations.
There has been a flurry of recent work tackling the
39
problem of automatically determining when to stop
AL (see Section 2). There are three areas where this
body of work can be improved:
applicability Several of the leading methods are re-
stricted to only being used in certain situations,
e.g., they can?t be used with some base learn-
ers, they have to select points in certain batch
sizes during AL, etc. (See Section 2 for dis-
cussion of the exact applicability constraints of
existing methods.)
lack of aggressive stopping The leading methods
tend to find stop points that are too far to the
right in Figure 1. (See Section 4 for empirical
confirmation of this.)
instability Some of the leading methods work well
on some datasets but then can completely break
down on other datasets, either stopping way too
late and wasting enormous amounts of annota-
tion effort or stopping way too early and losing
large amounts of F-measure. (See Section 4 for
empirical confirmation of this.)
This paper presents a new stopping method based
on stabilizing predictions that addresses each of
these areas and provides user-adjustable stopping
behavior. The essential idea behind the new method
is to test the predictions of the recently learned mod-
els (during AL) on examples which don?t have to
be labeled and stop when the predictions have sta-
bilized. Some of the main advantages of the new
method are that: it requires no additional labeled
data, it?s widely applicable, it fills a need for a
method which can aggressively save annotations, it
has stable performance, and it provides users with
control over how aggressively/conservatively to stop
AL.
Section 2 discusses related work. Section 3 ex-
plains our Stabilizing Predictions (SP) stopping cri-
terion in detail. Section 4 evaluates the SP method
and discusses results. Section 5 concludes.
2 Related Work
Laws and Schu?tze (2008) present stopping criteria
based on the gradient of performance estimates and
the gradient of confidence estimates. Their tech-
nique with gradient of performance estimates is only
applicable when probabilistic base learners are used.
The gradient of confidence estimates method is more
generally applicable (e.g., it can be applied with
our experiments where we use SVMs as the base
learner). This method, denoted by LS2008 in Tables
and Figures, measures the rate of change of model
confidence over a window of recent points and when
the gradient falls below a threshold, AL is stopped.
The margin exhaustion stopping criterion was de-
veloped for AL with SVMs (AL-SVM). It says to
stop when all of the remaining unlabeled examples
are outside of the current model?s margin (Schohn
and Cohn, 2000) and is denoted as SC2000 in Ta-
bles and Figures. Ertekin et al (2007) developed a
similar technique that stops when the number of sup-
port vectors saturates. This is equivalent to margin
exhaustion in all of our experiments so this method
is not shown explicitly in Tables and Figures. Since
we use AL with SVMs, we will compare with mar-
gin exhaustion in our evaluation section. Unlike our
SP method, margin exhaustion is only applicable for
use with margin-based methods such as SVMs and
can?t be used with other base learners such as Maxi-
mum Entropy, Naive Bayes, and others. Schohn and
Cohn (2000) show in their experiments that margin
exhaustion has a tendency to stop late. This is fur-
ther confirmed in our experiments in Section 4.
The confidence-based stopping criterion (here-
after, V2008) in (Vlachos, 2008) says to stop when
model confidence consistently drops. As pointed out
by (Vlachos, 2008), this stopping criterion is based
on the assumption that the learner/feature represen-
tation is incapable of fully explaining all the exam-
ples. However, this assumption is often violated and
then the performance of the method suffers (see Sec-
tion 4).
Two stopping criteria (max-conf and min-err) are
reported in (Zhu and Hovy, 2007). The max-conf
method indicates to stop when the confidence of the
model on each unlabeled example exceeds a thresh-
old. In the context of margin-based methods, max-
conf boils down to be simply a generalization of the
margin exhaustion method. Min-err, reported to be
superior to max-conf, says to stop when the accu-
racy of the most recent model on the current batch of
queried examples exceeds some threshold (they use
0.9). Zhu et al (2008b) proposes the use of multi-
criteria-based stopping to handle setting the thresh-
40
old for min-err. They refuse to stop and they raise
the min-err threshold if there have been any classi-
fication changes on the remaining unlabeled data by
consecutive actively learned models when the cur-
rent min-err threshold is satisfied. We denote this
multi-criteria-based strategy, reported to work better
than min-err in isolation, by ZWH2008. As seen in
(Zhu et al, 2008a), sometimes min-err indeed stops
later than desired and ZWH2008 must (by nature
of how it operates) stop at least as late as min-err
does. The susceptibility of ZWH2008 to stopping
late is further shown emprically in Section 4. Also,
ZWH2008 is not applicable for use with AL setups
that select examples in small batches.
3 A Method for Stopping Active Learning
Based on Stabilizing Predictions
To stop active learning at the point when annotations
stop providing increases in performance, perhaps the
most straightforward way is to use a separate set of
labeled data and stop when performance begins to
level off on that set. But the problem with this is that
it requires additional labeled data which is counter
to our original reason for using AL in the first place.
Our hypothesis is that we can sense when to stop AL
by looking at (only) the predictions of consecutively
learned models on examples that don?t have to be
labeled. We won?t know if the predictions are cor-
rect or not but we can see if they have stabilized. If
the predictions have stabilized, we hypothesize that
the performance of the models will have stabilized
and vice-versa, which will ensure a (much-needed)
aggressive approach to saving annotations.
SP checks for stabilization of predictions on a set
of examples, called the stop set, that don?t have to
be labeled. Since stabilizing predictions on the stop
set is going to be used as an indication that model
stabilization has occurred, the stop set ought to be
representative of the types of examples that will be
encountered at application time. There are two con-
flicting factors in deciding upon the size of the stop
set to use. On the one hand, a small set is desir-
able because then SP can be checked quickly. On
the other hand, a large set is desired to ensure we
don?t make a decision based on a set that isn?t repre-
sentative of the application space. As a compromise
between these factors, we chose a size of 2000. In
Section 4, sensitivity analysis to stop set size is per-
formed and more principled methods for determin-
ing stop set size and makeup are discussed.
It?s important to allow the examples in the stop
set to be queried if the active learner selects them
because they may be highly informative and ruling
them out could hurt performance. In preliminary ex-
periments we had made the stop set distinct from the
set of unlabeled points made available for querying
and we saw performance was qualitatively the same
but the AL curve was translated down by a few F-
measure points. Therefore, we allow the points in
the stop set to be selected during AL.1
The essential idea is to compare successive mod-
els? predictions on the stop set to see if they have
stabilized. A simple way to define agreement be-
tween two models would be to measure the percent-
age of points on which the models make the same
predictions. However, experimental results on a sep-
arate development dataset show then that the cutoff
agreement at which to stop is sensitive to the dataset
being used. This is because different datasets have
different levels of agreement that can be expected by
chance and simple percent agreement doesn?t adjust
for this.
Measurement of agreement between human anno-
tators has received significant attention and in that
context, the drawbacks of using percent agreement
have been recognized (Artstein and Poesio, 2008).
Alternative metrics have been proposed that take
chance agreement into account. In (Artstein and
Poesio, 2008), a survey of several agreement met-
rics is presented. Most of the agreement metrics are
of the form:
agreement = Ao ? Ae1 ? Ae , (1)
where Ao = observed agreement, and Ae = agree-
ment expected by chance. The different metrics dif-
fer in how they compute Ae.
The Kappa statistic (Cohen, 1960) measures
agreement expected by chance by modeling each
coder (in our case model) with a separate distribu-
tion governing their likelihood of assigning a partic-
ular category. Formally, Kappa is defined by Equa-
1They remain in the stop set if they?re selected.
41
tion 1 with Ae computed as follows:
Ae =
?
k?{+1,?1}
P (k|c1) ? P (k|c2), (2)
where each ci is one of the coders (in our case,
models), and P (k|ci) is the probability that coder
(model) ci labels an instance as being in category k.
Kappa estimates P (k|ci) based on the proportion of
observed instances that coder (model) ci labeled as
being in category k.
We have found Kappa to be a robust parameter
that doesn?t require tuning when moving to a new
dataset. On a separate development dataset, a Kappa
cutoff of 0.99 worked well. All of the experiments
(except those in Table 2) in the current paper used an
agreement cutoff of Kappa = 0.99 with zero tuning
performed. We will see in Section 4 that this cutoff
delivers robust results across all of the folds for all
of the datasets.
The Kappa cutoff captures the intensity of the
agreement that must occur before SP will conclude
to stop. Though an intensity cutoff of K=0.99 is
an excellent default (as seen by the results in Sec-
tion 4), one of the advantages of the SP method is
that by giving users the option to vary the intensity
cutoff, users can control how aggressive SP will be-
have. This is explored further in Section 4.
Another way to give users control over stopping
behavior is to give them control over the longevity
for which agreement (at the specified intensity) must
be maintained before SP concludes to stop. The sim-
plest implementation would be to check the most
recent model with the previous model and stop if
their agreement exceeds the intensity cutoff. How-
ever, independent of wanting to provide users with
a longevity control, this is not an ideal approach be-
cause there?s a risk that these two models could hap-
pen to highly agree but then the next model will not
highly agree with them. Therefore, we propose us-
ing the average of the agreements from a window
of the k most recent pairs of models. If we call the
most recent model Mn, the previous model Mn?1
and so on, with a window size of 3, we average the
agreements between Mn and Mn?1, between Mn?1
and Mn?2, and between Mn?2 and Mn?3. On sepa-
rate development data a window size of k=3 worked
well. All of the experiments (except those in Ta-
ble 3) in the current paper used a longevity window
size of k=3 with zero tuning performed. We will
see in Section 4 that this longevity default delivers
robust results across all of the folds for all of the
datasets. Furthermore, Section 4 shows that varying
the longevity requirement provides users with an-
other lever for controlling how aggressively SP will
behave.
4 Evaluation and Discussion
4.1 Experimental Setup
We evaluate the Stabilizing Predictions (SP) stop-
ping method on multiple datasets for Text Classifi-
cation (TC) and Named Entity Recognition (NER)
tasks. All of the datasets are freely and publicly
available and have been used in many past works.
For Text Classification, we use two publicly avail-
able spam corpora: the spamassassin corpus used in
(Sculley, 2007) and the TREC spam corpus trec05p-
1/ham25 described in (Cormack and Lynam, 2005).
For both of these corpora, the task is a binary clas-
sification task and we perform 10-fold cross valida-
tion. We also use the Reuters dataset, in particular
the Reuters-21578 Distribution 1.0 ModApte split2.
Since a document may belong to more than one cat-
egory, each category is treated as a separate binary
classification problem, as in (Joachims, 1998; Du-
mais et al, 1998). Consistent with (Joachims, 1998;
Dumais et al, 1998), results are reported for the ten
largest categories. Other TC datasets we use are the
20Newsgroups3 newsgroup article classification and
the WebKB web page classification datasets. For
WebKB, as in (McCallum and Nigam, 1998; Zhu et
al., 2008a; Zhu et al, 2008b) we use the four largest
categories. For all of our TC datasets, we use binary
features for every word that occurs in the training
data at least three times.
For NER, we use the publicly available GENIA
corpus4. Our features, based on those from (Lee et
al., 2004), are surface features such as the words in
2http://www.daviddlewis.com/resources/
testcollections/reuters21578
3We used the ?bydate? version of the dataset downloaded
from http://people.csail.mit.edu/jrennie/20Newsgroups/. This
version is recommended since it makes cross-experiment com-
parison easier since there is no randomness in the selection of
train/test splits.
4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/
wiki.cgi?page=GENIA+Project
42
the named entity and two words on each side, suf-
fix information, and positional information. We as-
sume a two-phase model where boundary identifica-
tion has already been performed, as in (Lee et al,
2004).
SVMs deliver high performance for the datasets
we use so we employ SVMs as our base learner
in the bulk of our experiments (maximum entropy
models are used in Subsection 4.3). For selection of
points to query, we use the approach that was used
in (Tong and Koller, 2002; Schohn and Cohn, 2000;
Campbell et al, 2000) of selecting the points that are
closest to the current hyperplane. We use SVMlight
(Joachims, 1999) for training the SVMs. For the
smaller datasets (less than 50,000 examples in total),
a batch size of 20 was used with an initial training
set of size 100 and for the larger datasets (greater
than 50,000 examples in total), a batch size of 200
was used with an initial training set of size 1000.
4.2 Main Results
Table 1 shows the results for all of our datasets. For
each dataset, we report the average number of anno-
tations5 requested by each of the stopping methods
as well as the average F-measure achieved by each
of the stopping methods.6
There are two facts worth keeping in mind. First,
the numbers in Table 1 are averages and therefore,
sometimes two methods could have very similar
average numbers of annotations but wildly differ-
ent average F-measures (because one of the meth-
ods was consistently stopping around its average
whereas the other was stopping way too early and
way too late). Second, sometimes a method with a
higher average number of annotations has a lower
5Better evaluation metrics would use more refined measures
of annotation effort than the number of annotations because not
all annotations require the same amount of effort to annotate but
lacking such a refined model for our datasets, we use number of
annotations in these experiments.
6Tests of statistical significance are performed using
matched pairs t tests at a 95% confidence level.
7(Vlachos, 2008) suggests using three drops in a row to de-
tect a consistent drop in confidence so we do the same in our
implementation of the method from (Vlachos, 2008).
8Following (Zhu et al, 2008b), we set the starting accuracy
threshold to 0.9 when reimplementing their method.
9(Laws and Schu?tze, 2008) uses a window of size 100
and a threshold of 0.00005 so we do the same in our re-
implementation of their method.
average F-measure than a method with a lower aver-
age number of annotations. This can be caused be-
cause of the first fact just mentioned about the num-
bers being averages and/or this can also be caused
by the ?less is more? phenomenon in active learn-
ing where often with less data, a higher-performing
model is learned than with all the data; this was
first reported in (Schohn and Cohn, 2000) and sub-
sequently observed by many others (e.g., (Vlachos,
2008; Laws and Schu?tze, 2008)).
There are a few observations to highlight regard-
ing the performance of the various stopping meth-
ods:
? SP is the most parsimonious method in terms
of annotations. It stops the earliest and remark-
ably it is able to do so largely without sacrific-
ing F-measure.
? All the methods except for SP and SC2000 are
unstable in the sense that on at least one dataset
they have a major failure, either stopping way
too late and wasting large numbers of anno-
tations (e.g. ZWH2008 and V2008 on TREC
Spam) or stopping way too early and losing
large amounts of F-measure (e.g. LS2008 on
NER-Protein) .
? It?s not always clear how to evaluate stopping
methods because the tradeoff between the value
of extra F-measure versus saving annotations is
not clearly known and will be different for dif-
ferent applications and users.
This last point deserves some more discussion. In
some cases it is clear that one stopping method is
the best. For example, on WKB-Project, the SP
method saves the most annotations and has the high-
est F-measure. But which method performs the
best on NER-DNA? Arguments can reasonably be
made for SP, SC2000, or ZWH2008 being the best
in this case depending on what exactly the anno-
tation/performance tradeoff is. A promising direc-
tion for research on AL stopping methods is to de-
velop user-adjustable stopping methods that stop as
aggressively as the user?s annotation/performance
preferences dictate.
One avenue of providing user-adjustable stopping
is that if some methods are known to perform con-
sistently in an aggressive manner against annotating
43
Task-Dataset SP V20087 SC2000 ZWH20088 LS20089 All
TREC-SPAM 2100 56000 3900 29220 3160 56000
(10-fold AVG) 98.33 98.47 98.41 98.44 96.63 98.47
20Newsgroups 678 181 1984 1340 1669 11280
(20-cat AVG) 60.85 18.06 55.43 60.72 54.79 54.81
Spamassassin 326 4362 862 398 1176 5400
(10-fold AVG) 94.57 95.00 95.53 95.94 95.62 95.63
NER-protein 8720 67220 17680 18580 2360 67220
(10-fold AVG) 89.48 90.28 90.38 90.31 76.47 90.28
NER-DNA 4020 67220 10640 7200 3900 67220
(10-fold AVG) 82.40 84.31 84.73 84.51 74.74 84.31
NER-cellType 3840 29600 5540 11580 4580 67220
(10-fold AVG) 86.15 86.87 87.19 87.32 85.65 87.83
Reuters 484 6762 1196 650 1272 9580
(10-cat AVG) 74.29 65.81 73.88 76.77 74.00 75.64
WKB-Course 790 184 1752 912 1740 7420
(10-fold AVG) 83.12 30.34 80.47 83.16 80.55 80.19
WKB-Faculty 808 892 1932 1062 1818 7420
(10-fold AVG) 81.53 40.14 81.79 81.64 81.99 82.36
WKB-Project 646 916 1358 794 1482 7420
(10-fold AVG) 63.30 25.33 58.11 61.82 59.30 61.19
WKB-Student 1258 894 2400 1468 2150 7420
(10-fold AVG) 84.70 50.66 83.46 84.39 83.19 83.30
Average 2152 21294 4477 6655 2301 28509
(macro-avg) 81.70 62.30 80.85 82.27 78.45 81.27
Table 1: Methods for stopping AL. For each dataset, the average number of annotations at the automatically determined
stopping points and the average F-measure at the automatically determined stopping points are displayed. Bold entries
are statistically significantly different than SP (and non-bold entries are not). The Average row is simply an unweighted
macro-average over all the datasets. The final column (labeled ?All?) represents standard fully supervised passive
learning with the entire set of training data.
too much while others are known to perform consis-
tently in a conservative manner, then users can pick
the stopping criterion that?s more suitable for their
particular annotation/performance valuation. For
this purpose, SP fills a gap as the other stopping cri-
teria seem to be conservative in the sense defined
in Section 1. SP, on the other hand, is more of an
aggressive stopping criterion and is less likely to an-
notate data that is not needed.
A second avenue for providing user-adjustable
stopping is a single stopping method that is itself ad-
justable. To this end, Section 4.3 shows how inten-
sity and longevity provide levers that can be used to
control the behavior of SP in a controlled fashion.
Sometimes viewing the stopping points of the var-
ious criteria on a graph with the active learning curve
can help one visualize how the methods perform.
Figure 2 shows the graph for a representative fold.10
The x-axis measures the number of human annota-
tions that have been requested so far. The y-axis
measures performance in terms of F-Measure. The
vertical lines are where the various stopping meth-
ods would have stopped AL if we hadn?t continued
the simulation. The figure reinforces and illustrates
what we have seen in Table 1, namely that SP stops
more aggressively than existing criteria and is able
10It doesn?t make sense to show a graph for the average over
cross validation because the average number of annotations at
the stopping point may cross the learning curve at a completely
misleading point. Consider a method that stops way too early
and way too late at times.
44
0 1 2 3 4 5 6 7
x 104
60
65
70
75
80
85
90
Number of Human Annotations Requested
Pe
rfo
rm
an
ce
 (F
?M
eas
ure
)
DNA Fold 1
SC2000
LS2008
ZWH2008
V2008
SP
Figure 2: Graphic with stopping criteria in action for fold
1 of NER of DNA from the GENIA corpus. The x-axis
ranges from 0 to 70,000.
to do so without sacrificing performance.
4.3 Additional Experiments
All of the additional experiments in this subsection
were conducted on our least computationally de-
manding dataset, Spamassassin. The results in Ta-
bles 2 and 3 show how varying the intensity cut-
off and the longevity requirement, respectively, of
SP enable a user to control stopping behavior. Both
methods enable a user to adjust stopping in a con-
trolled fashion (without radical changes in behav-
ior). Areas of future work include: combining the
intensity and longevity methods for controlling be-
havior; and developing precise expectations on the
change in behavior corresponding to changes in the
intensity and longevity settings.
The results in Table 4 show results for different
stop set sizes. Even with random selection of a stop
set as small as 500, SP?s performance holds fairly
steady. This plus the fact that random selection of
stop sets of size 2000 worked across all the folds of
all the datasets in Table 1 show that in practice per-
haps the simple heuristic of choosing a fairly large
random set of points works well. Nonetheless, we
think the size necessary will depend on the dataset
and other factors such as the feature representation
so more principled methods of determining the size
and/or the makeup of the stop set are an area for
future work. For example, construction techniques
Intensity Annotations F-Measure
K=99.5 364 96.01
K=99.0 326 94.57
K=98.5 304 95.59
K=98.0 262 93.75
K=97.5 242 93.35
K=97.0 224 90.91
Table 2: Controlling the behavior of stopping through the
use of intensity. For Kappa intensity levels in {97.0, 97.5,
98.0, 98.5, 99.0, 99.5}, the 10-fold average number of an-
notations at the automatically determined stopping points
and the 10-fold average F-measure at the automatically
determined stopping points are displayed for the Spamas-
sassin dataset.
Longevity Annotations F-Measure
k=1 284 95.17
k=2 318 94.95
k=3 326 94.57
k=4 336 95.40
k=5 346 96.41
k=6 366 94.53
Table 3: Controlling the behavior of stopping through the
use of longevity. For window length k longevity levels in
{1, 2, 3, 4, 5, 6}, the 10-fold average number of annota-
tions at the automatically determined stopping points and
the 10-fold average F-measure at the automatically deter-
mined stopping points are displayed for the Spamassassin
dataset.
could be developed to create stop sets with high rep-
resentativeness (in terms of feature space) density
(meaning representativeness of stop set divided by
size of stop set). For example, a possibility is to
cluster examples before AL begins and then make
sure the stop set contains examples from each of the
clusters. Another possibility is to use a greedy algo-
rithm where the stop set is iteratively grown where
on each iteration the center of mass of the stop set
in feature space is computed and an example in the
unlabeled pool that is maximally far in feature space
from this center of mass is selected for inclusion in
the stop set. This could be useful for efficiency (in
terms of getting the same stopping performance with
a smaller stop set as could be achieved with a larger
stop set) and also as a way to ensure adequate repre-
sentation of the task space. The latter can be accom-
45
Task-Dataset SP V2008 ZWH2008 LS2008 All
Spamassassin 286 1208 386 756 5400
(10-fold AVG) 94.92 89.89 95.31 96.40 91.74
Table 5: Methods for stopping AL with maximum entropy as the base learner. For each stopping method, the average
number of annotations at the automatically determined stopping point and the average F-measure at the automatically
determined stopping point are displayed. Bold entries are statistically significantly different than SP (and non-bold
entries are not). SC2000, the margin exhaustion method, is not shown since it can?t be used with a non-margin-based
learner. The final column (labeled ?All?) represents standard fully supervised passive learning with the entire set of
training data.
Stop Set Size Annotations F-Measure
2500 326 95.58
2000 326 94.57
1500 314 95.00
1000 328 95.73
500 314 94.57
Table 4: Investigating the sensitivity to stop set size. For
stop set sizes in {2500, 2000, 1500, 1000, 500}, the 10-
fold average number of annotations at the automatically
determined stopping points and the 10-fold average F-
measure at the automatically determined stopping points
are displayed for the Spamassassin dataset.
plished by perhaps continuing to add examples to
the stop set until adding new examples is no longer
increasing the representativeness of the stop set.
As one of the advantages of SP is that it?s widely
applicable, Table 5 shows the results when using
maximum entropy models as the base learner dur-
ing AL (the query points selected are those which
the model is most uncertain about). The results re-
inforce our conclusions from the SVM experiments,
with SP performing aggressively and all statistically
significant differences in performance being in SP?s
favor. Figure 3 shows the graph for a representative
fold.
5 Conclusions
Effective methods for stopping AL are crucial for re-
alizing the potential annotation savings enabled by
AL. A survey of existing stopping methods identi-
fied three areas where improvements are called for.
The new stopping method based on Stabilizing Pre-
dictions (SP) addresses all three areas: SP is widely
applicable, stable, and aggressive in saving annota-
tions.
0 1000 2000 3000 4000 5000 6000
50
60
70
80
90
100
Number of Human Annotations Requested
Pe
rf
or
m
an
ce
 (F
?M
ea
su
re)
AL?MaxEnt: Spamassassin Fold 5
SP
ZWH2008
LS2008
V2008
Figure 3: Graphic with stopping criteria in action for fold
5 of TC of the spamassassin corpus. The x-axis ranges
from 0 to 6,000.
The empirical evaluation of SP and the existing
methods was informative for evaluating the crite-
ria but it was also informative for demonstrating the
difficulties for rigorous objective evaluation of stop-
ping criteria due to different annotation/performance
tradeoff valuations. This opens up a future area for
work on user-adjustable stopping. Two potential
avenues for enabling user-adjustable stopping are a
single criterion that is itself adjustable or a suite of
methods with consistent differing levels of aggres-
siveness/conservativeness from which users can pick
the one(s) that suit their annotation/performance
tradeoff valuation. SP substantially widens the range
of behaviors of existing methods that users can
choose from. Also, SP?s behavior itself can be ad-
justed through user-controllable parameters.
46
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for hpsg parse se-
lection. Nat. Lang. Eng., 14(2):191?222.
Michael Bloodgood and K. Vijay-Shanker. 2009. Taking
into account the differences between actively and pas-
sively acquired data: The case of active learning with
support vector machines for imbalanced datasets. In
NAACL.
Colin Campbell, Nello Cristianini, and Alex J. Smola.
2000. Query learning with large margin classifiers.
In ICML ?00: Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning, pages 111?
118, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
Gordon Cormack and Thomas Lynam. 2005. Trec 2005
spam track overview. In TREC-14.
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algorithms
and representations for text categorization. In CIKM
?98: Proceedings of the seventh international con-
ference on Information and knowledge management,
pages 148?155, New York, NY, USA. ACM.
Seyda Ertekin, Jian Huang, Le?on Bottou, and C. Lee
Giles. 2007. Learning on the border: active learn-
ing in imbalanced data classification. In Ma?rio J.
Silva, Alberto H. F. Laender, Ricardo A. Baeza-Yates,
Deborah L. McGuinness, Bj?rn Olstad, ?ystein Haug
Olsen, and Andre? O. Falca?o, editors, Proceedings of
the Sixteenth ACM Conference on Information and
Knowledge Management, CIKM 2007, Lisbon, Portu-
gal, November 6-10, 2007, pages 127?136. ACM.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In ECML, pages 137?142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods ?
Support Vector Learning, pages 169?184.
Florian Laws and Hinrich Schu?tze. 2008. Stopping crite-
ria for active learning of named entity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 465?
472, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, and
Hae-Chang Rim. 2004. Biomedical named entity
recognition using two-phase model based on svms.
Journal of Biomedical Informatics, 37(6):436?447.
Andrew McCallum and Kamal Nigam. 1998. A compar-
ison of event models for naive bayes text classification.
In Proceedings of AAAI-98, Workshop on Learning for
Text Categorization.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In Proc.
17th International Conf. on Machine Learning, pages
839?846. Morgan Kaufmann, San Francisco, CA.
D. Sculley. 2007. Online active learning methods for fast
label-efficient spam filtering. In Conference on Email
and Anti-Spam (CEAS), Mountain View, CA, USA.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Research
(JMLR), 2:45?66.
Andreas Vlachos. 2008. A stopping criterion for active
learning. Computer Speech and Language, 22(3):295?
312.
Jingbo Zhu and Eduard Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
783?790.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008a.
Learning a stopping criterion for active learning for
word sense disambiguation and text classification. In
IJCNLP.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008b.
Multi-criteria-based strategy to stop active learning for
data annotation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 1129?1136, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
47
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 163?171,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
RankPref: Ranking Sentences Describing Relations
between Biomedical Entities with an Application
Catalina O Tudor K Vijay-Shanker
Department of Computer and Information Sciences
University of Delaware, Newark, DE, USA
tudor@cis.udel.edu vijay@cis.udel.edu
Abstract
This paper presents a machine learning ap-
proach that selects and, more generally, ranks
sentences containing clear relations between
genes and terms that are related to them. This
is treated as a binary classification task, where
preference judgments are used to learn how to
choose a sentence from a pair of sentences.
Features to capture how the relationship is de-
scribed textually, as well as how central the
relationship is in the sentence, are used in the
learning process. Simplification of complex
sentences into simple structures is also applied
for the extraction of the features. We show that
such simplification improves the results by up
to 13%. We conducted three different evalu-
ations and we found that the system signifi-
cantly outperforms the baselines.
1 Introduction
Life scientists, doctors and clinicians often search
for information relating biological concepts. For ex-
ample, a doctor might be interested to know the im-
pact of a drug on some disease. One source of infor-
mation is the knowledge bases and ontologies that
are manually curated with facts from scientific arti-
cles. However, the curation process is slow and can-
not keep up with ongoing publications. Moreover,
not all associations between biological concepts can
be found in these databases.
Another source of information is the scientific
literature itself. However, searching for biological
facts and how they might be related is often cumber-
some. The work presented in this paper tries to au-
tomate the process of finding sentences that clearly
describe relationships between biological concepts.
We rank all sentences mentioning two concepts and
pick the top one to show to the user. In this paper, we
focused on certain specific types of concepts (i.e.,
genes1 and terms believed to be related to them), al-
though our approach can be generalized.
Systems to facilitate knowledge exploration of
genes are being built for the biomedical domain.
One of them, eGIFT (Tudor et al, 2010), tries to
identify iTerms (informative terms) for a gene based
on frequency of co-occurrence (see Figure 1 for top
15 terms selected for gene Groucho). iTerms are
unigrams, bigrams, and exact matches of biomedi-
cal terms gathered from various controlled vocabu-
laries. Thus, iTerms can be of any type (e.g., pro-
cesses, domains, drugs, other genes, etc.), the types
being determined by what is being described about
the gene in the literature. The iTerms for a gene
are ranked based on a score that compares their fre-
quencies of occurrence in publications mentioning
the gene in question with their frequencies in a back-
ground set of articles about a wide variety of genes.
Previous evaluation of eGIFT by life scientists
suggested that there is almost always some kind of
relationship between a gene and its iTerms. These
relationships can be many and varied from one gene-
term pair to another. Sometimes a user might make
an erroneous assumption about a gene-term asso-
ciation if sentences supporting the association are
not immediately inspected. For example, upon see-
ing ?co-repressor? in connection to gene Groucho,
eGIFT users might correctly assume that Groucho is
1Throughout the paper, the word ?gene? will be used for
both the gene and its products.
163
Figure 1: Top iTerms for gene Groucho, and sentences picked by RankPref for various iTerms.
a co-repressor (i.e., a protein that binds to transcrip-
tion factors). However, upon seeing ?wrpw motif?,
a user might assume that this is a motif contained
within gene Groucho, as this is typically the asso-
ciation that we make between genes and informa-
tion annotated for them in knowledge bases. But
this would be a wrong assumption, since in actuality
the wrpw motif is contained within other genes that
interact with Groucho, fact which is evident from
reading sentences containing the gene and the mo-
tif. To get a quick overall understanding of a gene?s
functionalities, users of eGIFT could be presented
with terms extracted for the gene, as well as sen-
tences clearly describing how they are related.
Our method selects sentences using a model that
is trained on preference judgments provided by biol-
ogists. Example sentences chosen by our method are
shown in Figure 1. While we evaluate our approach
on sentences from eGIFT, this work could have
equally applied on other similar systems (Smal-
heiser et al, 2008; Gladki et al, 2008; Kim et al,
2008; Kaczanowski et al, 2009). These systems
also identify ?important terms? from a set of doc-
uments retrieved for a given search (either a gene
name or other biomedical concept).
The main contributions of this work are: (1) a
method for ranking sentences by employing ma-
chine learning; (2) the use of preference judgments;
(3) features to capture whether two terms are clearly
related and in focus in a sentence; (4) another appli-
cation of sentence simplification, showing a signifi-
cant gain in performance when utilized.
We continue with a description of our approach,
which includes the use of preference judgments to
learn the models, how the features are extracted, and
how the sentence simplifier is used for this task. The
evaluation of the trained model and the system?s re-
sults are presented in the following section. Re-
lated work, conclusions, and future directions are
provided at the end of the manuscript.
2 Methods
Rather than pre-judging what is important for this
task and manually determining a weighting schema
to automatically score sentences for a gene-term
pair, we approached this task using machine learn-
ing. We asked a group of annotators to rank sen-
tences relating genes and iTerms, and we used their
annotations, together with features described in Sec-
tion 2.3, to learn how to rank sentences.
164
2.1 Preference Judgments
For the annotation task, we presented biologists with
sentences containing a gene-term pair and asked
them to specify which sentence they prefer. One
way to do this is by employing the pointwise ap-
proach, which requires absolute judgments (i.e. the
annotator scores each sentence in a list or ranks the
sentences based on their relevance to the given task).
A second approach is the pairwise approach, which
requires the iteration of preference judgments (i.e.,
the annotator is presented with two sentences at a
time, and is asked to chose one as more relevant to
the task than the other).
In order to simplify the annotator?s task, as well
as construct a more reliable training set, we used the
pairwise approach. Our decision was influenced by
Carterette et al (2008), who showed that preference
judgments are faster and easier to make than abso-
lute judgments. Thus, we can obtain many annotated
instances in a relatively short amount of time. More-
over, since there are only two possible outcomes in
choosing one sentence, we need at most three judges
for a majority vote. This will also ensure consistency
in the annotations. We discuss the model trained on
preference judgments in Section 2.2.
2.2 Learned Models: PrefSVM and RankPref
We used the preference judgments to learn a model,
PrefSVM, that picks one sentence from a pair of sen-
tences. This model was built using SVMLight with
a linear kernel. The examples used in the learning
process correspond to pairs of sentences. For each
pair, we constructed a vector of feature values, by
subtracting the feature values corresponding to the
first sentence from the feature values corresponding
to the second sentence. We assigned a positive value
to a pair vector if the first sentence was preferred and
a negative value if the second one was preferred.
We can also use PrefSVM to design a system that
can rank all the sentences containing a gene and
an iTerm, by performing comparisons between sen-
tences in the list. We call RankPref the system that
picks one sentence from a group of sentences, and
which also ranks the entire set of sentences. This
method recursively applies PrefSVM in the following
manner: Two sentences are randomly picked from
a given list of sentences. PrefSVM chooses one sen-
tence and discards the other. A third sentence is then
randomly picked from the list, and PrefSVM makes
its choice by comparing it to the sentence kept in the
previous step. This process of picking, comparing
and discarding sentences is continued until there is
only one sentence left. We keep track of comparison
results and apply transitivity, in order to speed up the
process of ranking all the sentences.
2.3 Features
Each sentence is first chunked into base phrases. We
used Genia Tagger (Tsuruoka et al, 2005), which
provides part-of-speech tags for every word in the
sentence. We trained a chunker (i.e., shallow parser
that identifies base NPs) using the Genia corpus.
We considered typical features that are used in
machine learning approaches, such as distance be-
tween gene and iTerm, length of sentence, etc.
Moreover, we included additional groups of features
that we felt might be important for this task: one
group to capture how the relationship is described
textually, another group to capture how central the
relationship is in terms of what is being described in
the sentence, and the last to capture whether the re-
lation is stated as a conjecture or a fact. The weights
for these features will be determined automatically
during the learning process and they will be depen-
dent on whether or not the features were effective,
given the annotation set.
The first type of features is to capture how the
relationship is described textually. As an example,
consider the sentence ?Bmp2 stimulates osteoblas-
tic differentiation?2, where the gene and the iTerm
are in subject and object (direct object or otherwise)
positions, and the verb is a common biological verb.
Thus, we constructed a set of lexico-syntactic pat-
terns to capture the different kinds of argument re-
lations served by the two concepts. We grouped 25
lexico-syntactic patterns into 8 groups, correspond-
ing to different relational constructions that can ex-
ist between a gene and an iTerm. Example patterns
are shown in Table 1 for each group, and the sym-
bols used in these patterns are explained in Table 2.
When a sentence matches a pattern group, the corre-
sponding value is set to 1 for that feature.
2In our examples, the gene will be marked in italics and the
iTerm will be marked in bold.
165
Group Example Pattern
G1 G VG+ I
G2 G/I via/by/through I/G
G3 G VG+ (NP/PP)* by/in VBG I
G4 G/I by/in VBG I/G
G5 G/I VB I/G
G6 G/I of I/G
G7 G/I other preposition I/G
G8 including/such as/etc. G/I and I/G
Table 1: Examples of lexico-syntactic patterns
For example, the following sentence, in which
the gene is Lmo2 and the iTerm is ?erythropoiesis?,
matches the pattern in G1: [G VG+ I].
While Tal1 has been shown to induce ery-
throid differentiation , Lmo2 appears to sup-
press fetal erythropoiesis.
where ?Lmo2? matches G, ?appears to suppress?
matches VG+, and ?fetal erythropoiesis? matches I.
Notice how the verb plays an important role in
the patterns of groups G1, G3, G4, and G5. We also
have a verb type feature which differentiates groups
of verbs having the gene and the iTerm as arguments
(e.g., ?activates?, ?is involved in?, ?plays a role?,
etc. are treated as different types).
The second type of features captures how cen-
tral the relationship is in terms of what is being de-
scribed in the sentence. The subject feature records
whether the gene and iTerm appear in the subject
position, as this will tell us if they are in focus in
the sentence. While we do not parse the sentence,
we take a simplified sentence (see Section 2.4) and
see if the gene/term appear in a noun phrase pre-
ceding the first tensed verb. Another feature, the
gene-iTerm position, measures how close the gene
and the term are to each other and to the beginning
of the sentence, as this makes it easier for a reader
to grasp the relation between them. For this, we add
the number of words occurring to the left of the seg-
ment spanning the gene and iTerm, and half of the
number of words occurring between them. Finally,
we included a headedness feature. The idea here is
that if the gene/term are not the head of the noun
group, but rather embedded inside, then this poten-
tially makes the relation less straightforward. These
Symb Definition
NP a base noun phrase
PP a preposition followed by a base noun phrase
VG+ a series of one or more verb groups
VBG a verb group in which the head is a gerund verb
VBN a verb group in which the head is a participle verb
VB a verb group in which the head is a base verb
G, I base noun phrases, with 0 or more prepositional
phrases, containing the gene/iTerm
Table 2: Symbols used in the pattern notation
groups are denoted by G and I in the patterns shown
in Table 1.
The third type of features captures information
about the sentence itself. The sentence complexity
feature is measured in terms of the number of verbs,
conjunctions, commas, and parentheticals that oc-
cur in the sentence. We use a conjecture feature for
detecting whether the sentence involves a hypothe-
sis. We have a simple rule for this feature, by see-
ing if words such as ?may?, ?could?, ?probably?,
?potentially?, etc., appear in proximity of the gene
and iTerm. Additionally, we have a negation feature
to detect whether the relationship is mentioned in a
negative way. We look for words such as ?not?, ?nei-
ther?, etc., within proximity of the gene and iTerm.
Although the features and lexico-syntactic pat-
terns were determined by analyzing a development
set of sentences containing genes and their iTerms,
we believe that these features and patterns can be
used to rank sentences involving other biomedical
entities, not just genes.
2.4 Sentence Simplification
Notice that the lexico-syntactic patterns are written
as sequences of chunks and lexical tags. If a sen-
tence matches a pattern, then the sentence expresses
a relation between the gene and the iTerm. However,
sometimes it is not possible to match a pattern if the
sentence is complex.
For example, consider sentence A in Table 3, for
gene Cd63. Let us assume that the iTerm is ?prota-
somes?. Clearly, there is a relationship between the
gene and the iTerm, namely that Cd63 was found in
pc-3 cell-derived protasomes. However, none of the
lexico-syntactic patterns is able to capture this rela-
tion, because of all the extra information between
166
A Cd63, an integral membrane protein found
in multivesicular lysosomes and secretory
granules, was also found in pc-3 cell-
derived protasomes.
S1 Cd63 was found in pc-3 cell-derived pro-
tasomes.
S2 Cd63 is an integral membrane protein.
CS1 Cd63 is found in multivesicular lyso-
somes.
CS2 Cd63 is found in secretory granules.
Table 3: Simplified sentences for gene Cd63. Example
iTerms: ?protasomes? and ?secretory granules?.
the gene and the term. While we may have multi-
ple patterns in each group, we cannot necessarily ac-
count for each lexical variation at this level of gran-
ularity.
We are using a sentence simplifier, built in-house,
to ensure a match where applicable. The simpli-
fier identifies appositions, relative clauses, and con-
junctions/lists of different types, using regular ex-
pressions to match chunked tags. In the sentence
of Table 3, the simplifier recognizes the apposition
?an integral membrane protein?, the reduced relative
clause ?found in multivesicular bodies/lysosomes
and secretory granules? and the noun conjunction
?multivesicular bodies/lysosome and secretory gran-
ules?. It then produces several simplified sentences
containing the gene. S1 and S2, shown in Table 3,
are simplified sentences obtained from the simpli-
fier. CS1 and CS2 are additional simplified sen-
tences, which required the combination of multiple
simplifications: the appositive, the relative clause,
and the noun conjunction.
Notice how each of the simplified sentences
shown in Table 3 is now matching a pattern group.
If we are interested in the relationship between Cd63
and ?protasomes?, we can look at S1. Likewise, if
we are interested in the relationship between Cd63
and ?secretory granules?, we can look at CS2.
We have a matching feature that tells whether the
pattern was matched in the original sentence, a sim-
plified sentence, or a combined sentence, and this
feature is taken into account in the learning process.
3 Results and Discussion
We evaluated both PrefSVM and RankPref. Each re-
quired a different set of annotated data. For the
evaluation of PrefSVM, we used the preference judg-
ments and leave-one-out cross validation. And for
the evaluation of RankPref, we asked the annota-
tors to order a group of sentences mentioning gene-
iTerm pairs. Six life science researchers, with grad-
uate degrees, annotated both sets.
3.1 Evaluation of PrefSVM
First, we evaluated the performance of PrefSVM us-
ing leave-one-out cross validation.
3.1.1 Annotation of Preference Judgements
We started by selecting a group of pairs of sen-
tences. We randomly picked gene-iTerm combi-
nations, and for each combination, we randomly
picked two sentences containing both the gene and
the term. To alleviate bias, the order of the sentences
was chosen randomly before displaying them to the
annotators. In our guidelines, we asked the annota-
tors to choose sentences that clearly state the rela-
tionship between the gene and the iTerm. Because
the focus here is on the relationship between the two
terms, we also asked them to refrain from choos-
ing sentences that describe additional information or
other aspects. It is conceivable that, for other appli-
cations, extra information might be an important de-
termining factor, but for our task we wanted to focus
on the relationship only.
For each pair of sentences, we wanted to have
three opinions so that we can have a majority vote.
To alleviate the burden on the annotators, we started
by giving each pair of sentences to two annotators,
and asked for an extra opinion only when they did
not agree. Each biologist was given an initial set
of 75 pairs of sentences to annotate, and shared the
same amount of annotations (15) with each of the
other biologists. 225 unique pairs of sentences were
thus annotated, but six were discarded after the an-
notators informed us that they did not contain the
gene in question.
In 34 out of 219 pairs of sentences, the two biol-
ogists disagreed on their annotations. These cases
included pairs of similar sentences, or pairs of sen-
tences that did not describe any relationship between
167
System Performance Correct
Baseline 1 65.75% 144
Baseline 2 71.69% 157
PrefSVM without Simp 72.14% 158
PrefSVM with Simp 83.10% 182
Table 4: Results for PrefSVM
the gene and the iTerm. An example of sentences for
which the annotators could not agree is:
1. The tle proteins are the mammalian ho-
mologues of gro, a member of the drosophila
notch signaling pathway.
2. In drosophila, gro is one of the neurogenic
genes that participates in the notch signalling
pathway .
For these 34 pairs, we randomly selected another
annotator and considered the majority vote.
3.1.2 Baselines
We chose two baselines against which to com-
pare PrefSVM. The first baseline always chooses
the shortest sentence. For the second baseline, we
looked at the proximity of the gene/term to the be-
ginning of the sentence, as well as the proximity of
the two to each other, and chose the sentence that
had the lowest accumulated proximity. The reason
for this second baseline is because the proximity of
the gene/term to the beginning of the sentence could
mean that the sentence focuses on the gene/term and
their relation. Furthermore, the proximity of the
gene to the iTerm could mean a clearer relation be-
tween them.
3.1.3 Results
We evaluated PrefSVM by performing leave-one-
out cross validation on the set of 219 pairs of sen-
tences. Each pair of sentences was tested by using
the model trained on the remaining 218 pairs. The
results are shown in Table 4.
The first baseline performed at 65.75%, correctly
choosing 144 of 219 sentences. The second base-
line performed slightly better, at 71.69%. PrefSVM
outperformed both baselines, especially when the
sentence simplifier was used, as this facilitated the
match of the lexico-syntactic patterns used as fea-
tures. PrefSVM performed at 83.10%, which is
17.35% better than the first baseline, and 11.41%
better than the second baseline.
3.2 Evaluation of RankPref
The previous evaluation showed how PrefSVM per-
forms at picking a sentence from a pair of sentences.
But ultimately, for the intended eGIFT application,
the system needs to choose one sentence from many.
We evaluated RankPref for this task.
3.2.1 Annotating Data for Sentence Selection
For this evaluation, we needed to create a different
set of annotated data that reflects the selection of one
sentence from a group of sentences.
Since a gene and an iTerm can appear in many
sentences, it is too onerous a task for a human anno-
tator to choose one out of tens or hundreds of sen-
tences. For this reason, we limited the set of sen-
tences mentioning a gene and an iTerm to only 10.
We randomly picked 100 gene-term pairs and for the
pairs that contained more than ten sentences, we ran-
domly chose ten of them. On average, there were 9.4
sentences per set.
We asked the same annotators as in the previous
evaluation to participate in this annotation task. Be-
cause the task is very time consuming, and because
it is hard to decide how to combine the results from
multiple annotators, we assigned each set of sen-
tences to only one annotator. We showed the sen-
tences in a random order so that biasing them would
not be an issue.
We initially asked the annotators to order the sen-
tences in the set. However, this task proved to be im-
possible, since many sentences were alike. Instead,
we asked the annotators to assign them one of three
categories:
(Cat.1) Any sentence in this category could be
considered the ?best? among the choices provided;
(Cat.2) These sentences are good, but there are
other sentences that are slightly better;
(Cat.3) These sentences are not good or at least
there are other sentences in this set that are much
better.
Classifying the sentences into these categories
was less cumbersome, fact which was confirmed by
our evaluators after a trial annotation.
Out of the total of 936 sentences, 322 (34.4%)
were placed in the first category, 332 (35.5%) were
168
System Cat.1 Cat.2 Cat.3
Baseline 1 58 30 12
Baseline 2 61 24 15
RankPref without Simp 67 21 12
RankPref with Simp 80 17 3
Table 5: Results for RankPref
placed in the second category, and 282 (30.1%) were
placed in the third category. On average, it took
about 15 minutes for an annotator to group a set?s
sentences into these three categories. So each anno-
tator volunteered approximately 5 hours of annota-
tion time.
3.2.2 Results
Table 5 shows how the top sentences picked for
the 100 gene-term pairs by the four systems matched
with the annotations. 80 of 100 sentences that
RankPref picked were placed in Cat.1 by the anno-
tators, 17 were placed in Cat.2, and 3 sentences
were placed in Cat.3. These results compare favor-
ably with results obtained for the two baselines and
RankPref without the use of the simplifier.
Furthermore, instead of just focussing on the top
choice sentence, we also considered the ranking of
the entire set of sentences. We looked at how the
ranked lists agree with the categories assigned by
the annotators. We used the normalized discounted
cumulative gain (nDCG) (Jarvelin and Kekalainen,
2002), a standard metric used in information re-
trieval to evaluate the quality of the ranked lists.
DCG at rank p is defined as:
DCGp = rel1 +
p?
i=2
reli
log2i
where reli is the relevance of the item at position i.
We normalize DCG by dividing it by an ideal gain
(i.e., DCG of same list, when ordered from highest
to lowest relevance).
For our task, we took the relevance score to be 1
for a sentence placed in Cat.1, a relevance score of
0.5 for a sentence placed in Cat.2, and a relevance
score of 0 for a sentence placed in Cat.3. We report
a normalized discounted cumulative gain of 77.19%.
This result compares favorably with results re-
ported for the two baselines (68.36% for B1 and
Figure 2: Distribution of nDCG for different relevance
scores assigned to sentences placed in category Cat.2.
68.32% for B2) as well as for when the sentence
simplifier was removed (69.45%).
Figure 2 shows different results for nDCG when
the relevance score for Cat.2 is varied between 0
(same as sentences placed in Cat.1) and 1 (same as
sentences placed in Cat.3).
4 Related Work
To the best of our knowledge, no one has attempted
to rank sentences from the biomedical literature,
using machine learning on a set of data marked
with preference judgments. However, different ap-
proaches have been described in the literature that
use preference judgments to learn ranked lists. For
example, Radlinski and Joachims (2005) used pref-
erence judgments to learn ranked retrieval functions
for web search results. These judgments were gen-
erated automatically from search engine logs. Their
learned rankings outperformed a static ranking func-
tion. Similar approaches in IR are those of Cohen et
al. (1999) and Freund et al (2003).
Ranking of text passages and documents has
been done previously in BioNLP for other purposes.
Suomela and Andrade (2005) proposed a way to
rank the entire PubMed database, given a large train-
ing set for a specific topic. Goldberg et al (2008)
and Lu et al (2009) describe in detail how they iden-
tified and ranked passages for the 2006 Trec Ge-
nomics Track (Hersh et al, 2006). Yeganova et
al. (2011) present a method for ranking positively la-
beled data within large sets of data, and this method
was applied by Neveol et al (2011) to rank sen-
tences containing deposition relationships between
biological data and public repositories.
169
Extraction of sentences describing gene functions
has also been applied for creating gene summaries
(Ling et al, 2007; Jin et al, 2009; Yang et al, 2009).
However, these methods differ in that their goal is
not to look for sentences containing specific terms
and their relations with genes, but rather for sen-
tences that fall into some predefined categories of
sentences typically observed in gene summaries.
Sentence simplification has been used to aid pars-
ing (Chandrasekar et al, 1996; Jonnalagadda et
al., 2009). Devlin and Tait (1998) and Carroll et
al. (1998) use it to help people with aphasia. Sid-
dharthan (2004) was concerned with cohesion and
suggested some applications.
The idea of using lexico-syntactic patterns to
identify relation candidates has also been applied in
the work of Banko et al (2007), although their pat-
terns are not used in the learning process.
5 Conclusion and Future Directions
We have developed a system which aims to identify
sentences that clearly and succinctly describe the re-
lation between two entities. We used a set of prefer-
ence judgements, as provided by biologists, to learn
an SVM model that could make a choice between
any two sentences mentioning these entities.
The model compares favorably with baselines on
both the task of choosing between two sentences, as
well as ranking a set of sentences. The performance
for choosing between two sentences was 83.10%, as
compared to 65.75% and 71.69% for the two base-
lines, respectively. For choosing one sentence from
a list of sentences, the performance was 80%, as
compared to 58% and 61%. Furthermore, when the
entire list of ranked sentences was evaluated, the
system reported a nDCG of 77.19%, compared to
68.36% and 68.32% for the two baselines.
The model?s performance was also shown to
be significantly better when sentence simplification
was used. We were able to match relation patterns
on complex sentences, and observed an increase of
10.96%, 13%, and 7.74% for the three evaluations
afore-mentioned, respectively. It is noteworthy that,
without the simplification, the performance is only
slightly better than the second baseline. This is be-
cause the second baseline uses information that is
also used by our system, although this does not in-
clude the lexico-syntactic patterns that identify the
type of relation between the gene and the term.
Given that the full system?s performance is much
better than both baselines, and that the system?s per-
formance without simplification is only slightly bet-
ter than the second baseline, we believe that: (1) the
pattern and type of relation determination are impor-
tant, and (2) sentence simplification is crucial for the
determination of the relationship type.
We are currently pursuing summaries for genes.
Since iTerms have been shown in previous evalua-
tions to represent important aspects of a gene?s func-
tionality and behavior, we are investigating whether
they are represented in gene summaries found in En-
trezGene and UniProtKB. If so, an extractive sum-
mary can be produced by choosing sentences for the
gene and its iTerms. We are also considering de-
veloping abstractive summaries. Our use of lexico-
syntactic patterns can be extended to pick the exact
relation between a gene and the iTerm. For exam-
ple, by using the lexico-syntactic patterns, coupled
with simplification, we can extract the following ex-
act relations from the four sentences shown in Fig-
ure 1: ?Groucho is a corepressor?, ?The wrpw motif
recruits groucho?, ?Groucho is implicated in notch
signaling?, and ?The eh1 repression domain binds
groucho?. With these relations extracted, using text
generation algorithms for textual realization and co-
hesion, we can produce abstractive summaries.
We would also like to investigate how to general-
ize this work to other pairs of entities, as well as how
to generalize this work for other applications which
may or may not require the same features as the ones
we used.
Acknowledgments
This work has been supported in part by USDA
Grant 2008-35205-18734 and the Agriculture and
Food Research Initiative Competitive USDA Grant
2011-67015-3032. We thank Cecilia Arighi, Kevin
Bullaughey, Teresia Buza, Fiona McCarthy, Lak-
shmi Pillai, Carl Schmidt, Liang Sun, Hui Wang,
and Qinghua Wang for participating in the anno-
tation task and/or for various discussions. We
also thank the anonymous reviewers for their com-
ments and suggestions, which helped us improve the
manuscript.
170
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open In-
formation Extraction from the Web. In Proceedings of
IJCAI.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of English newspaper text to assist aphasic readers.
Proceedings of the AAAI98 Workshop on Integrating
AI and Assistive Technology, pages 7?10.
Ben Carterette, Paul N Bennett, David Maxwell Chicker-
ing, and Susan T Dumais. 2008. Here or there: Pref-
erence judgments for relevance. In Proceedings of the
IR research, 30th European conference on Adv. in IR.
R Chandrasekar, Christine Doran, and B Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of the 16th conference on Computational
linguistics, volume 2, pages 1041?1044. Association
for Computational Linguistics.
Wiliam W Cohen, Robert E Schapire, and Yoram Singer.
1999. Learning to order things. Journal of Artificial
Intelligence Research, 10:243?270.
Siobhan Devlin and John Tait. 1998. The use of a psy-
cholinguistic database in the simplification of text for
aphasic readers. Linguistic Databases, pages 161?
173.
Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning
Research, 4:933?969.
Arek Gladki, Pawel Siedlecki, Szymon Kaczanowski,
and Piotr Zielenkewicz. 2008. e-LiSe?an online tool
for finding needles in the ?Medline haystack?. Bioin-
formatics, 24(8):1115?1117.
Andrew B Goldberg, David Andrzejewski, Jurgen Van
Gael, Burr Settles, Xiaojin Zhu, and Mark Craven.
2008. Ranking biomedical passages for relevance and
diversity. In Proceedings of TREC.
William Hersh, Aaron M Cohen, Phoebe Roberts, and
Hari Krishna Rekapalli. 2006. TREC 2006 Genomics
Track Overview.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20(4):422?446.
Feng Jin, Minlie Huang, Zhiyong Lu, and Xiaoyan Zhu.
2009. Towards automatic generation of gene sum-
mary. In Proceedings of the BioNLP 2009 Work-
shop, pages 97?105. Association for Computational
Linguistics, June.
Siddhartha Jonnalagadda, Luis Tari, Jorg Hakenberg,
Chitta Baral, and Graciela Gonzalez. 2009. Towards
effective sentence simplification for automatic pro-
cessing of biomedical text. In Proceedings of NAACL
HLT 2009: Short Papers, pages 177?180.
Szymon Kaczanowski, Pawel Siedlecki, and Piotr Zie-
lenkewicz. 2009. The high throughput sequence
annotation service (HT-SAS) - the shortcut from se-
quence to true medline words. BMC Bioinformatics,
10:148?154, May.
Jung-Jae Kim, Piotr Pezik, and Dietrich Rebholz-
Schuhmann. 2008. MedEvi: Retrieving textual evi-
dence of relations between biomedical concepts from
medline. Bioinformatics, 24(11):1410?1412.
Xu Ling, Jing Jiang, Xin He, Qiaozhu Mei, Chengxi-
ang Zhai, and Bruce Schatz. 2007. Generating gene
summaries from biomedical literature: A study of
semi-structured summarization. Information Process-
ing and Management, 43:1777?1791, March.
Yue Lu, Hui Fang, and Chengxiang Zhai. 2009. An
empirical study of gene synonym query expansion
in biomedical information retrieval. Information Re-
trieval, 12:51?68, February.
Aure?lie Ne?ve?ol, W John Wilbur, and Zhiyong Lu. 2011.
Extraction of data deposition statements from the lit-
erature: a method for automatically tracking research
results. Bioinformatics, 27(23):3306?3312.
Filip Radlinski and Thorsten Joachims. 2005. Query
chains: Learning to rank from implicit feedback. In
Proceedings of KDD?05.
Advaith Siddharthan. 2004. Syntactic Simplification and
Text Cohesion. Ph.D. thesis, University of Cambridge.
Neil R Smalheiser, Wei Zhou, and Vetle I Torvik. 2008.
Anne O?Tate: A tool to support user-driven summa-
rization, drill-down and browsing of PubMed search
results. Journal of Biomedical Discovery and Collab-
oration, 3(1):2?11.
Brian P Suomela and Miguel A Andrade. 2005. Rank-
ing the whole MEDLINE database according to a large
training set using text indexing. BMC Bioinformatics,
6(75), March.
Yoshimasa Tsuruoka, Yuka Tateishi, Jing-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics ? 10th Panhellenic Conference on Infor-
matics, LNCS 3746, pages 382?392.
Catalina O Tudor, Carl J Schmidt, and K Vijay-Shanker.
2010. eGIFT: Mining Gene Information from the Lit-
erature. BMC Bioinformatics, 11:418.
Jianji Yang, Aaron Cohen, and William Hersh. 2009.
Evaluation of a gene information summarization sys-
tem by users during the analysis process of microarray
datasets. BMC Bioinformatics, 10(Suppl 2):S5.
Lana Yeganova, Donald C Comeau, Won Kim, and
W John Wilbur. 2011. Text Mining Techniques for
Leveraging Positively Labeled Data. In Proceedings
of ACL Workshop BioNLP, pages 155?163.
171
