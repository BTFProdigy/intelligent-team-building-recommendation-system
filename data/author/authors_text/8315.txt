Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 764?773, Prague, June 2007. c?2007 Association for Computational Linguistics
Online Large-Margin Training for Statistical Machine Translation
Taro Watanabe Jun Suzuki Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
{taro,jun,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
We achieved a state of the art performance
in statistical machine translation by using
a large number of features with an online
large-margin training algorithm. The mil-
lions of parameters were tuned only on a
small development set consisting of less than
1K sentences. Experiments on Arabic-to-
English translation indicated that a model
trained with sparse binary features outper-
formed a conventional SMT system with a
small number of features.
1 Introduction
The recent advances in statistical machine transla-
tion have been achieved by discriminatively train-
ing a small number of real-valued features based ei-
ther on (hierarchical) phrase-based translation (Och
and Ney, 2004; Koehn et al, 2003; Chiang, 2005) or
syntax-based translation (Galley et al, 2006). How-
ever, it does not scale well with a large number of
features of the order of millions.
Tillmann and Zhang (2006), Liang et al (2006)
and Bangalore et al (2006) introduced sparse binary
features for statistical machine translation trained on
a large training corpus. In this framework, the prob-
lem of translation is regarded as a sequential labeling
problem, in the same way as part-of-speech tagging,
chunking or shallow parsing. However, the use of a
large number of features did not provide any signifi-
cant improvements over a conventional small feature
set.
Bangalore et al (2006) trained the lexical choice
model by using Conditional Random Fields (CRF)
realized on a WFST. Their modeling was reduced to
Maximum Entropy Markov Model (MEMM) to han-
dle a large number of features which, in turn, faced
the labeling bias problem (Lafferty et al, 2001).
Tillmann and Zhang (2006) trained their feature set
using an online discriminative algorithm. Since the
decoding is still expensive, their online training ap-
proach is approximated by enlarging a merged k-
best list one-by-one with a 1-best output. Liang
et al (2006) introduced an averaged perceptron al-
gorithm, but employed only 1-best translation. In
Watanabe et al (2006a), binary features were trained
only on a small development set using a variant of
voted perceptron for reranking k-best translations.
Thus, the improvement is merely relative to the
baseline translation system, namely whether or not
there is a good translation in their k-best.
We present a method to estimate a large num-
ber of parameters ? of the order of millions ?
using an online training algorithm. Although it
was intuitively considered to be prone to overfit-
ting, training on a small development set ? less
than 1K sentences ? was sufficient to achieve im-
proved performance. In this method, each train-
ing sentence is decoded and weights are updated at
every iteration (Liang et al, 2006). When updat-
ing model parameters, we employ a memorization-
variant of a local updating strategy (Liang et al,
2006) in which parameters are optimized toward
a set of good translations found in the k-best list
across iterations. The objective function is an ap-
proximated BLEU (Watanabe et al, 2006a) that
scales the loss of a sentence BLEU to a document-
wise loss. The parameters are trained using the
764
Margin Infused Relaxed Algorithm (MIRA) (Cram-
mer et al, 2006). MIRA is successfully employed
in dependency parsing (McDonald et al, 2005) or
the joint-labeling/chunking task (Shimizu and Haas,
2006). Experiments were carried out on an Arabic-
to-English translation task, and we achieved signif-
icant improvements over conventional minimum er-
ror training with a small number of features.
This paper is organized as follows: First, Sec-
tion 2 introduces the framework of statistical ma-
chine translation. As a baseline SMT system, we
use the hierarchical phrase-based translation with
an efficient left-to-right generation (Watanabe et al,
2006b) originally proposed by Chiang (2005). In
Section 3, a set of binary sparse features are defined
including numeric features for our baseline system.
Section 4 introduces an online large-margin training
algorithm using MIRA with our key components.
The experiments are presented in Section 5 followed
by discussion in Section 6.
2 Statistical Machine Translation
We use a log-linear approach (Och, 2003) in which
a foreign language sentence f is translated into an-
other language, for example English, e, by seeking a
maximum solution:
e? = argmax
e
wT ? h( f , e) (1)
where h( f , e) is a large-dimension feature vector. w
is a weight vector that scales the contribution from
each feature. Each feature can take any real value,
such as the log of the n-gram language model to
represent fluency, or a lexicon model to capture the
word or phrase-wise correspondence.
2.1 Hierarchical Phrase-based SMT
Chiang (2005) introduced the hierarchical phrase-
based translation approach, in which non-terminals
are embedded in each phrase. A translation is gener-
ated by hierarchically combining phrases using the
non-terminals. Such a quasi-syntactic structure can
naturally capture the reordering of phrases that is not
directly modeled by a conventional phrase-based ap-
proach (Koehn et al, 2003). The non-terminal em-
bedded phrases are learned from a bilingual corpus
without a linguistically motivated syntactic struc-
ture.
Based on hierarchical phrase-based modeling, we
adopted the left-to-right target generation method
(Watanabe et al, 2006b). This method is able to
generate translations efficiently, first, by simplifying
the grammar so that the target side takes a phrase-
prefixed form, namely a target normalized form.
Second, a translation is generated in a left-to-right
manner, similar to the phrase-based approach using
Earley-style top-down parsing on the source side.
Coupled with the target normalized form, n-gram
language models are efficiently integrated during the
search even with a higher order of n.
2.2 Target Normalized Form
In Chiang (2005), each production rule is restricted
to a rank-2 or binarized form in which each rule con-
tains at most two non-terminals. The target normal-
ized form (Watanabe et al, 2006b) further imposes
a constraint whereby the target side of the aligned
right-hand side is restricted to a Greibach Normal
Form like structure:
X ?
?
?, ?b?,?
?
(2)
where X is a non-terminal, ? is a source side string of
arbitrary terminals and/or non-terminals. ?b? is a cor-
responding target side where ?b is a string of termi-
nals, or a phrase, and ? is a (possibly empty) string
of non-terminals. ? defines one-to-one mapping be-
tween non-terminals in ? and ?. The use of phrase
?b as a prefix maintains the strength of the phrase-
base framework. A contiguous English side with a
(possibly) discontiguous foreign language side pre-
serves phrase-bounded local word reordering. At
the same time, the target normalized framework still
combines phrases hierarchically in a restricted man-
ner.
2.3 Left-to-Right Target Generation
Decoding is performed by parsing on the source side
and by combining the projected target side. We
applied an Earley-style top-down parsing approach
(Wu and Wong, 1998; Watanabe et al, 2006b; Zoll-
mann and Venugopal, 2006). The basic idea is
to perform top-down parsing so that the projected
target side is generated in a left-to-right manner.
The search is guided with a push-down automaton,
which keeps track of the span of uncovered source
765
word positions. Combined with the rest-cost esti-
mation aggregated in a bottom-up way, our decoder
efficiently searches for the most likely translation.
The use of a target normalized form further sim-
plifies the decoding procedure. Since the rule form
does not allow any holes for the target side, the inte-
gration with an n-gram language model is straight-
forward: the prefixed phrases are simply concate-
nated and intersected with n-gram.
3 Features
3.1 Baseline Features
The hierarchical phrase-based translation system
employs standard numeric value features:
? n-gram language model to capture the fluency
of the target side.
? Hierarchical phrase translation probabilities in
both directions, h(?|?b?) and h(?b?|?), estimated
by relative counts, count(?, ?b?).
? Word-based lexically weighted models of
hlex(?|?b?) and hlex(?b?|?) using lexical transla-
tion models.
? Word-based insertion/deletion penalties that
penalize through the low probabilities of the
lexical translation models (Bender et al, 2004).
? Word/hierarchical-phrase length penalties.
? Backtrack-based penalties inspired by the dis-
tortion penalties in phrase-based modeling
(Watanabe et al, 2006b).
3.2 Sparse Features
In addition to the baseline features, a large number
of binary features are integrated in our MT system.
We may use any binary features, such as
h( f , e) =
?
?
?
?
?
?
?
?
?
1 English word ?violate? and Arabic
word ?tnthk? appeared in e and f .
0 otherwise.
The features are designed by considering the decod-
ing efficiency and are based on the word alignment
structure preserved in hierarchical phrase transla-
tion pairs (Zens and Ney, 2006). When hierarchi-
cal phrases are extracted, the word alignment is pre-
served. If multiple word alignments are observed
ei?1 ei ei+1 ei+2 ei+3 ei+4
f j?1 f j f j+1 f j+2 f j+3
Figure 1: An example of sparse features for a phrase
translation.
with the same source and target sides, only the fre-
quently observed word alignment is kept to reduce
the grammar size.
3.2.1 Word Pair Features
Word pair features reflect the word correspon-
dence in a hierarchical phrase. Figure 1 illustrates
an example of sparse features for a phrase trans-
lation pair f j, ..., f j+2 and ei, ..., ei+3 1. From the
word alignment encoded in this phrase, we can ex-
tract word pair features of (ei, f j+1), (ei+2, f j+2) and
(ei+3, f j).
The bigrams of word pairs are also used to
capture the contextual dependency. We assume
that the word pairs follow the target side order-
ing. For instance, we define ((ei?1, f j?1), (ei, f j+1)),
((ei, f j+1), (ei+2, f j+2)) and ((ei+2, f j+2), (ei+3, f j)) in-
dicated by the arrows in Figure 1.
Extracting bigram word pair features following
the target side ordering implies that the correspond-
ing source side is reordered according to the tar-
get side. The reordering of hierarchical phrases is
represented by using contextually dependent word
pairs across their boundaries, as with the feature
((ei?1, f j?1), (ei, f j+1)) in Figure 1.
3.2.2 Insertion Features
The above features are insufficient to capture the
translation because spurious words are sometimes
inserted in the target side. Therefore, insertion fea-
tures are integrated in which no word alignment is
associated in the target. The inserted words are asso-
ciated with all the words in the source sentence, such
as (ei+1, f1), ..., (ei+1, fJ) for the non-aligned word
ei+1 with the source sentence f J1 in Figure 1. In the
1For simplicity, we show an example of phrase translation
pairs, but it is trivial to define the features over hierarchical
phrases.
766
f j?1
f j f j+1
f j+2
f j+3
X 1
X 2
X 3
Figure 2: Example hierarchical features.
same way, we will be able to include deletion fea-
tures where a non-aligned source word is associated
with the target sentence. However, this would lead to
complex decoding in which all the translated words
are memorized for each hypothesis, and thus not in-
tegrated in our feature set.
3.2.3 Target Bigram Features
Target side bigram features are also included to
directly capture the fluency as in the n-gram lan-
guage model (Roark et al, 2004). For instance, bi-
gram features of (ei?1, ei), (ei, ei+1), (ei+1, ei+2)... are
observed in Figure 1.
3.2.4 Hierarchical Features
In addition to the phrase motivated features, we
included features inspired by the hierarchical struc-
ture. Figure 2 shows an example of hierarchical
phrases in the source side, consisting of X 1 ?
?
f j?1X 2 f j+3
?
, X 2 ?
?
f j f j+1X 3
?
and X 3 ?
?
f j+2
?
.
Hierarchical features capture the dependency of
the source words in a parent phrase to the source
words in child phrases, such as ( f j?1, f j), ( f j?1, f j+1),
( f j+3, f j), ( f j+3, f j+1), ( f j, f j+2) and ( f j+1, f j+2) as in-
dicated by the arrows in Figure 2. The hierarchical
features are extracted only for those source words
that are aligned with the target side to limit the fea-
ture size.
3.3 Normalization
In order to achieve the generalization capability, the
following normalized tokens are introduced for each
surface form:
? Word class or POS.
? 4-letter prefix and suffix. For instance, the word
Algorithm 1 Online Training Algorithm
Training data: T = {( f t, et)}Tt=1
m-best oracles: O = {}Tt=1
i = 0
1: for n = 1, ..., N do
2: for t = 1, ..., T do
3: Ct ? bestk( f t; wi)
4: Ot ? oraclem(Ot ? Ct; et)
5: wi+1 = update wi using Ct w.r.t. Ot
6: i = i + 1
7: end for
8: end for
9: return
?NT
i=1 w
i
NT
?violate? is normalized to ?viol+? and ?+late?
by taking the prefix and suffix, respectively.
? Digits replaced by a sequence of ?@?. For ex-
ample, the word ?2007/6/27? is represented as
?@@@@/@/@@?.
We consider all possible combination of those to-
ken types. For example, the word pair feature (vi-
olate, tnthk) is normalized and expanded to (viol+,
tnthk), (viol+, tnth+), (violate, tnth+), etc. using the
4-letter prefix token type.
4 Online Large-Margin Training
Algorithm 1 is our generic online training algo-
rithm. The algorithm is slightly different from other
online training algorithms (Tillmann and Zhang,
2006; Liang et al, 2006) in that we keep and up-
date oracle translations, which is a set of good trans-
lations reachable by a decoder according to a met-
ric, i.e. BLEU (Papineni et al, 2002). In line 3,
a k-best list is generated by bestk(?) using the cur-
rent weight vector wi for the training instance of
( f t, et). Each training instance has multiple (or, pos-
sibly one) reference translations et for the source
sentence f t. Using the k-best list, m-best oracle
translations Ot is updated by oraclem(?) for every it-
eration (line 4). Usually, a decoder cannot generate
translations that exactly match the reference transla-
tions due to its beam search pruning and OOV. Thus,
we cannot always assign scores for each reference
translation. Therefore, possible oracle translations
are maintained according to an objective function,
767
i.e. BLEU. Tillmann and Zhang (2006) avoided the
problem by precomputing the oracle translations in
advance. Liang et al (2006) presented a similar up-
dating strategy in which parameters were updated
toward an oracle translation found in Ct, but ignored
potentially better translations discovered in the past
iterations.
New wi+1 is computed using the k-best list Ct with
respect to the oracle translations Ot (line 5). After N
iterations, the algorithm returns an averaged weight
vector to avoid overfitting (line 9). The key to this
online training algorithm is the selection of the up-
dating scheme in line 5.
4.1 Margin Infused Relaxed Algorithm
The Margin Infused Relaxed Algorithm (MIRA)
(Crammer et al, 2006) is an online version of the
large-margin training algorithm for structured clas-
sification (Taskar et al, 2004) that has been suc-
cessfully used for dependency parsing (McDonald et
al., 2005) and joint-labeling/chunking (Shimizu and
Haas, 2006). The basic idea is to keep the norm of
the updates to the weight vector as small as possible,
considering a margin at least as large as the loss of
the incorrect classification.
Line 5 of the weight vector update procedure in
Algorithm 1 is replaced by the solution of:
w?i+1 = argmin
wi+1
||wi+1 ? wi|| + C
?
e?,e?
?(e?, e?)
subject to
si+1( f t, e?) ? si+1( f t, e?) + ?(e?, e?) ? L(e?, e?; et)
?(e?, e?) ? 0
?e? ? Ot,?e? ? Ct (3)
where si( f t, e) =
{
wi
}T ? h( f t, e). ?(?) is a non-
negative slack variable and C ? 0 is a constant to
control the influence to the objective function. A
larger C implies larger updates to the weight vec-
tor. L(?) is a loss function, for instance difference of
BLEU, that measures the difference between e? and
e? according to the reference translations et. In this
update, a margin is created for each correct and in-
correct translation at least as large as the loss of the
incorrect translation. A larger error means a larger
distance between the scores of the correct and incor-
rect translations. Following McDonald et al (2005),
only k-best translations are used to form the margins
in order to reduce the number of constraints in Eq. 3.
In the translation task, multiple translations are ac-
ceptable. Thus, margins for m-oracle translation are
created, which amount to m ? k large-margin con-
straints. In this online training, only active features
constrained by Eq. 3 are kept and updated, unlike
offline training in which all possible features have to
be extracted and selected in advance.
The Lagrange dual form of Eq. 3 is:
max?(?)?0 ?
1
2
||
?
e?,e?
?(e?, e?)
(
h( f t, e?) ? h( f t, e?)
)
||2
+
?
e?,e?
?(e?, e?)L(e?, e?; et)
?
?
e?,e?
?(e?, e?)
(
si( f t, e?) ? si( f t, e?)
)
subject to
?
e?,e?
?(e?, e?) ? C (4)
with the weight vector update:
wi+1 = wi +
?
e?,e?
?(e?, e?)
(
h( f t, e?) ? h( f t, e?)
)
(5)
Equation 4 is solved using a QP-solver, such as a co-
ordinate ascent algorithm, by heuristically selecting
(e?, e?) and by updating ?(?) iteratively:
?(e?, e?) = max (0, ?(e?, e?) + ?(e?, e?)) (6)
?(e?, e?) =
L(e?, e?; et) ?
(
si( f t, e?) ? si( f t, e?)
)
||h( f t, e?) ? h( f t, e?)||2
C is used to clip the amount of updates.
A single oracle with 1-best translation is analyti-
cally solved without a QP-solver and is represented
as the following perceptron-like update (Shimizu
and Haas, 2006):
? = max
?
?
?
?
?
?
?
?
0, min
?
?
?
?
?
?
?
?
C,
L(e?, e?; et) ?
(
si( f t, e?) ? si( f t, e?)
)
||h( f t, e?) ? h( f t, e?)||2
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Intuitively, the update amount is controlled by the
margin and the loss between the correct and incor-
rect translations and by the closeness of two transla-
tions in terms of feature vectors. Indeed, Liang et al
(2006) employed an averaged perceptron algorithm
in which ? value was always set to one. Tillmann
and Zhang (2006) used a different update style based
on a convex loss function:
? = ?L(e?, e?; et) ?max
(
0, 1 ?
(
si( f t, e?) ? si( f t, e?)
))
768
Table 1: Experimental results obtained by varying normalized tokens used with surface form.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
surface form 492K 11.32 54.11 10.57 49.01 10.77 48.05
w/ prefix/suffix 4,204K 12.38 63.87 10.42 48.74 10.58 47.18
w/ word class 2,689K 10.87 49.59 10.63 49.55 10.89 48.79
w/ digits 576K 11.01 50.72 10.66 49.67 10.84 48.39
all token types 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
where ? > 0 is a learning rate for controlling the
convergence.
4.2 Approximated BLEU
We used the BLEU score (Papineni et al, 2002) as
the loss function computed by:
BLEU(E; E) = exp
?
?
?
?
?
?
?
?
1
N
N
?
n=1
log pn(E, E)
?
?
?
?
?
?
?
?
? BP(E, E)
(7)
where pn(?) is the n-gram precision of hypothesized
translations E = {et}Tt=1 given reference translations
E = {et}Tt=1 and BP(?) ? 1 is a brevity penalty. BLEU
is computed for a set of sentences, not for a sin-
gle sentence. Our algorithm requires frequent up-
dates on the weight vector, which implies higher cost
in computing the document-wise BLEU. Tillmann
and Zhang (2006) and Liang et al (2006) solved
the problem by introducing a sentence-wise BLEU.
However, the use of the sentence-wise scoring does
not translate directly into the document-wise score
because of the n-gram precision statistics and the
brevity penalty statistics aggregated for a sentence
set. Thus, we use an approximated BLEU score
that basically computes BLEU for a sentence set, but
accumulates the difference for a particular sentence
(Watanabe et al, 2006a).
The approximated BLEU is computed as follows:
Given oracle translations O for T , we maintain the
best oracle translations OT1 =
{
e?1, ..., e?T
}
. The ap-
proximated BLEU for a hypothesized translation e?
for the training instance ( f t, et) is computed over OT1
except for e?t, which is replaced by e?:
BLEU({e?1, ..., e?t?1, e?, e?t+1, ..., e?T }; E)
The loss computed by the approximated BLEU mea-
sures the document-wise loss of substituting the cor-
rect translation e?t into an incorrect translation e?.
The score can be regarded as a normalization which
scales a sentence-wise score into a document-wise
score.
5 Experiments
We employed our online large-margin training pro-
cedure for an Arabic-to-English translation task.
The training data were extracted from the Ara-
bic/English news/UN bilingual corpora supplied by
LDC. The data amount to nearly 3.8M sentences.
The Arabic part of the bilingual data is tokenized by
isolating Arabic scripts and punctuation marks. The
development set comes from the MT2003 Arabic-
English NIST evaluation test set consisting of 663
sentences in the news domain with four reference
translations. The performance is evaluated by the
news domain MT2004/MT2005 test set consisting
of 707 and 1,056 sentences, respectively.
The hierarchical phrase translation pairs are ex-
tracted in a standard way (Chiang, 2005): First,
the bilingual data are word alignment annotated by
running GIZA++ (Och and Ney, 2003) in two di-
rections. Second, the word alignment is refined
by a grow-diag-final heuristic (Koehn et al, 2003).
Third, phrase translation pairs are extracted together
with hierarchical phrases by considering holes. In
the last step, the hierarchical phrases are constrained
so that they follow the target normalized form con-
straint. A 5-gram language model is trained on the
English side of the bilingual data combined with the
English Gigaword from LDC.
First, the use of normalized token types in Sec-
tion 3.3 is evaluated in Table 1. In this setting, all
the structural features in Section 3.2 are used, but
differentiated by the normalized tokens combined
with surface forms. Our online large-margin train-
ing algorithm performed 50 iterations constrained
769
Table 2: Experimental results obtained by incrementally adding structural features.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
word pairs 11,042K 11.05 51.63 10.43 48.69 10.73 47.72
+ target bigram 11,230K 11.19 53.49 10.40 48.60 10.66 47.47
+ insertion 13,489K 11.21 52.20 10.77 50.33 10.93 48.08
+ hierarchical 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
Table 3: Experimental results for varying k-best and m-oracle translations.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
baseline 10.64 46.47 10.83 49.33 10.90 47.03
1-oracle 1-best 8,735K 11.25 52.63 10.82 50.77 10.93 48.11
1-oracle 10-best 10,480K 11.24 53.45 10.55 49.10 10.82 48.49
10-oracle 1-best 8,416K 10.70 47.63 10.83 48.88 10.76 46.00
10-oracle 10-best 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
sentence-BLEU 14,587K 11.10 51.17 10.82 49.97 10.86 47.04
by 10-oracle and 10-best list. When decoding, a
1000-best list is generated to achieve better oracle
translations. The training took nearly 1 day using 8
cores of Opteron. The translation quality is eval-
uated by case-sensitive NIST (Doddington, 2002)
and BLEU (Papineni et al, 2002)2. The table also
shows the number of active features in which non-
zero values were assigned as weights. The addition
of prefix/suffix tokens greatly increased the number
of active features. The setting severely overfit to the
development data, and therefore resulted in worse
results in open tests. The word class3 with surface
form avoided the overfitting problem. The digit se-
quence normalization provides a similar generaliza-
tion capability despite of the moderate increase in
the active feature size. By including all token types,
we achieved better NIST/BLEU scores for the 2004
and 2005 test sets. This set of experiments indi-
cates that a token normalization is useful especially
trained on a small data.
Second, we used all the normalized token types,
but incrementally added structural features in Ta-
ble 2. Target bigram features account for only the
fluency of the target side without considering the
source/target correspondence. Therefore, the in-
2We used the tool available at http://www.nist.gov/
speech/tests/mt/
3We induced 50 classes each for English and Arabic.
clusion of target bigram features clearly overfit to
the development data. The problem is resolved by
adding insertion features which can take into ac-
count an agreement with the source side that is not
directly captured by word pair features. Hierarchi-
cal features are somewhat effective in the 2005 test
set by considering the dependency structure of the
source side.
Finally, we compared our online training algo-
rithm with sparse features with a baseline system
in Table 3. The baseline hierarchical phrase-based
system is trained using standard max-BLEU training
(MERT) without sparse features (Och, 2003). Table
3 shows the results obtained by varying the m-oracle
and k-best size (k, m = 1, 10) using all structural
features and all token types. We also experimented
sentence-wise BLEU as an objective function con-
strained by 10-oracle and 10-best list. Even the 1-
oracle 1-best configuration achieved significant im-
provements over the baseline system. The use of
a larger k-best list further optimizes to the devel-
opment set, but at the cost of degraded translation
quality in the 2004 test set. The larger m-oracle size
seems to be harmful if coupled with the 1-best list.
As indicated by the reduced active feature size, 1-
best translation seems to be updated toward worse
translations in 10-oracles that are ?close? in terms
of features. We achieved significant improvements
770
Table 4: Two-fold cross validation experiments.
closed test open test
NIST BLEU NIST BLEU
[%] [%]
baseline 10.71 44.79 10.68 44.44
online 11.58 53.42 10.90 47.64
when the k-best list size was also increased. The
use of sentence-wise BLEU as an objective provides
almost no improvement in the 2005 test set, but is
comparable for the 2004 test set.
As observed in three experiments, the 2004/2005
test sets behaved differently, probably because of
the domain mismatch. Thus, we conducted a two-
fold cross validation using the 2003/2004/2005 test
sets to observe the effect of optimization as shown
in Table 44. The MERT baseline system performed
similarly both in closed and open tests. Our on-
line large-margin training with 10-oracle and 10-
best constraints and the approximated BLEU loss
function significantly outperformed the baseline sys-
tem in the open test. The development data is almost
doubled in this setting. The MERT approach seems
to be confused with the slightly larger data and with
the mixed domains from different epochs.
6 Discussion
In this work, the translation model consisting of mil-
lions of features are successfully integrated. In or-
der to avoid poor overfitting, features are limited to
word-based features, but are designed to reflect the
structures inside hierarchical phrases. One of the
benefit of MIRA is its flexibility. We may include
as many constraints as possible, like m-oracle con-
straints in our experiments. Although we described
experiments on the hierarchical phrase-based trans-
lation, the online training algorithm is applicable to
any translation systems, such as phrase-based trans-
lations and syntax-based translations.
Online discriminative training has already been
studied by Tillmann and Zhang (2006) and Liang
et al (2006). In their approach, training was per-
formed on a large corpus using the sparse features of
phrase translation pairs, target n-grams and/or bag-
of-word pairs inside phrases. In Tillmann and Zhang
4We split data by document, not by sentence.
(2006), k-best list generation is approximated by a
step-by-step one-best merging method that separates
the decoding and training steps. The weight vector
update scheme is very similar to MIRA but based
on a convex loss function. Our method directly em-
ploys the k-best list generated by the fast decoding
method (Watanabe et al, 2006b) at every iteration.
One of the benefits is that we avoid the rather expen-
sive cost of merging the k-best list especially when
handling millions of features.
Liang et al (2006) employed an averaged percep-
tron algorithm. They decoded each training instance
and performed a perceptron update to the weight
vector. An incorrect translation was updated toward
an oracle translation found in a k-best list, but dis-
carded potentially better translations in the past iter-
ations.
An experiment has been undertaken using a small
development set together with sparse features for the
reranking of a k-best translation (Watanabe et al,
2006a). They relied on a variant of a voted percep-
tron, and achieved significant improvements. How-
ever, their work was limited to reranking, thus the
improvement was relative to the performance of the
baseline system, whether or not there was a good
translation in a list. In our work, the sparse features
are directly integrated into the DP-based search.
The design of the sparse features was inspired
by Zens and Ney (2006). They exploited the
word alignment structure inside the phrase trans-
lation pairs for discriminatively training a reorder-
ing model in their phrase-based translation. The re-
ordering model simply classifies whether to perform
monotone decoding or not. The trained model is
treated as a single feature function integrated in Eq.
1. Our approach differs in that each sparse feature is
individually integrated in Eq. 1.
7 Conclusion
We exploited a large number of binary features
for statistical machine translation. The model was
trained on a small development set. The optimiza-
tion was carried out by MIRA, which is an online
version of the large-margin training algorithm. Mil-
lions of sparse features are intuitively considered
prone to overfitting, especially when trained on a
small development set. However, our algorithm with
771
millions of features achieved very significant im-
provements over a conventional method with a small
number of features. This result indicates that we
can easily experiment many alternative features even
with a small data set, but we believe that our ap-
proach can scale well to a larger data set for further
improved performance. Future work involves scal-
ing up to larger data and more features.
Acknowledgements
We would like to thank reviewers and our colleagues
for useful comment and discussion.
References
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2006. Sequence classification for machine trans-
lation. In Proc. of Interspeech 2006, pages 1157?
1160, Pittsburgh.
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system?. In Proc. of IWSLT 2004, pages 79?84,
Kyoto, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL
2005, pages 263?270, Ann Arbor, Michigan, June.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, March.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In In Proc. ARPA Workshop on Human Lan-
guage Technology.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of COLING/ACL 2006, pages 961?968, Sydney, Aus-
tralia, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of COL-
ING/ACL 2006, pages 761?768, Sydney, Australia,
July.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc. of ACL 2005, pages 91?98, Ann Ar-
bor, Michigan, June.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318, Philadelphia, Pennsylvania.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language model-
ing with conditional random fields and the percep-
tron algorithm. In Proc. of ACL 2004, pages 47?54,
Barcelona, Spain, July.
Nobuyuki Shimizu and Andrew Haas. 2006. Exact de-
coding for jointly labeling and chunking sequences.
In Proc. of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 763?770, Sydney, Australia,
July.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. of EMNLP 2004, pages 1?8, Barcelona, Spain,
July.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. of COLING/ACL 2006, pages 721?728, Sydney,
Australia, July.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2006a. NTT Statistical Machine Translation
for IWSLT 2006. In Proc. of IWSLT 2006, pages 95?
102, Kyoto, Japan.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006b. Left-to-right target generation for hierarchi-
cal phrase-based translation. In Proc. of COLING/ACL
2006, pages 777?784, Sydney, Australia, July.
772
Dekai Wu and Hongsing Wong. 1998. Machine transla-
tion with a stochastic grammatical channel. In Proc.
of COLING 98, pages 1408?1415, Montreal, Quebec,
Canada.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proc. of WSMT 2006, pages 55?63, New York City,
June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of WSMT 2006, pages 138?141, New York City,
June.
773
Instance-Based Generation for Interactive
Restricted Domain Question Answering Systems
Matthias Denecke and Hajime Tsukada
NTT Communication Science Laboratories,
2-4 Hikaridai, Seika-Cho, Soraku-gun, Kyoto
{denecke, tsukada}@cslab.kecl.ntt.co.jp
Abstract. One important component of interactive systems is the gen-
eration component. While template-based generation is appropriate in
many cases (for example, task oriented spoken dialogue systems), inter-
active question answering systems require a more sophisticated approach.
In this paper, we propose and compare two example-based methods for
generation of information seeking questions.
1 Introduction
Question answering is the task of providing natural language answers to natural
language questions using an information retrieval engine. Due to the unrestricted
nature of the problem, shallow and statistical methods are paramount.
Spoken dialogue systems address the problem of accessing information from a
structured database (such as time table information) or controlling appliances by
voice. Due to the fact that the scope of the application defined by the back-end,
the domain of the system is well-defined. Therefore, in the presence of vague,
ill-defined or misrecognized input from the user, dialogue management, relying
on the domain restrictions as given by the application, can interactively request
more information from the user until the users? intent has been determined. In
this paper, we are interested in generation of information seeking questions in
interactive question-answering systems.
1.1 Our System
We implemented a system that combines features of question answering systems
with those of spoken dialogue systems. We integrated the following two features
in an interactive restricted domain question answering system: (1) As in question
answering systems, the system draws its knowledge from a database of unstruc-
tured text. (2) As in spoken dialogue systems, the system can interactively query
for more information in the case of vague or ill-defined user queries.
1.2 Problem Addressed in This Paper
Restricteddomain question answering systems canbe deployed in interactive prob-
lem solving solutions, for example, software trouble shooting. In these scenarios,
interactivity becomes a necessity. This is because it is highly unlikely that all facts
relevant to retrieving the appropriate response are stated in the query. For exam-
ple, in the software trouble shooting task described in [5], a frequent system gen-
erated information seeking question is for the version of the software. Therefore,
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 486?497, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Instance-Based Generation 487
there is a need to inquire additional problem relevant information from the user,
depending on the interaction history and the problem to be solved.
In this paper, we specifically address the problem of how to generate in-
formation seeking questions in the case of ambiguous, vague or ill-defined user
questions. We assume that the decision of whether an information seeking ques-
tion is needed is made outside of the module described here. More formally, the
problem we address can be described as follows:
Given 1. A representation of the previous interaction history, consisting
of user and system utterances, and retrieval results from the IR
subsystem,
2. A decision for a information seeking question
Produce An information seeking question.
Problems of this kind have appeared traditionally in task oriented spoken
dialogue systems, where missing information needs to be prompted. However,
in the case of spoken dialogue systems, question generation is typically not a
substantial problem: the fact that the back-end is well-structured allows for
simple template-based generation in many cases. For example, missing values
for database queries or remote method invocations can be queried that way.
(But see also Oh and Rudnicky [7] or Walker et al[12] for more elaborated
approaches to generation for spoken dialogue systems).
In our case, however, a template-based approach is unrealistic. This is due
to the unstructured back-end application. Unlike as spoken dialogue systems,
we cannot make assumptions over what kind of questions to ask as this is de-
termined by the result set of articles as returned by the information retrieval
engine. Existing interactive question-answering systems (see section 7.1 for a
more detailed description) either use canned text on dialogue cards [5], break
down the dialogue representation into frames and then techniques from spo-
ken dialogue systems [8], or make simplifying assumptions to the extent that
generation essentially becomes equivalent to template-based generation.
1.3 Proposed Solution
For reasons discussed above, we propose an example-based approach to genera-
tion. More specifically, we use an existing dialogue corpus to retrieve appropriate
questions and modify in order to fit the situation at hand. We describe two algo-
rithms for instance-based natural language questions generation by first selecting
appropriate candidates from the corpus, then modifying the candidates to fit the
situation at hand, and finally re-rank the candidates. This is an example of a
memory-based learning approach, which in turn is a kind of a case-based reason-
ing. To the best of our knowledge, this is the first work addressing the problem
of example-based generation information seeking questions in the absence of a
structured back-end application.
2 Instance Based Natural Language Generation
In this section, we review the background in memory-based learning and its
application in natural language generation.
488 M. Denecke and H. Tsukada
2.1 Memory-Based Reasoning
Memory-based reasoning (MBR) is often considered a subtype of Case-based
reasoning. Case-based reasoning was proposed in the 80?s as an alternative to
rule-based approaches. Instead of expressing regularities about the domain to
be modeled in rules, the primary knowledge source in case-based reasoning is
a memory of cases representing episodes of encountered problems. Generating
a solution to a given problem consists of retrieving an appropriate case from
memory and adapting it to the problem at hand.
MBR solves problems by retrieving stored precedents as a starting point for
new problem-solving (e.g., [9]). However, its primary focus is on the retrieval pro-
cess, and in particular on the use of parallel retrieval schemes to enable retrieval
without conventional index selection. One aspect of memory-based systems is to
choose a distance that appropriately selects candidate exemplars.
Memory-based reasoning has been applied to machine translation, parsing,
unit selection text-to-speech synthesis, part-of-speech tagging, and others. An
overview of memory-based approaches to natural language processing can be
found in the introduction to the special issue [2].
2.2 Statistical and Instance-Based Generation
The most prominent example for statistical generation is Nitrogen [6]. This
system has been designed to allows large scale generation while requiring only
a minimal knowledge base. An abstract meaning representation is turned into
a lattice of surface sentences using a simple keyword based grammar. Using
statistical information acquired from a corpus, the sentences in the lattices are
re-ranked to determine the optimal surface string.
More recently, example-based natural language generation using a corpus
was proposed [11]. It is assumed in this work that content determination has
already taken place and the input has been broken down to sentence-size pieces.
The approach is to use a learned grammar to generate a list of candidates using
a traditional chart based generation algorithm. The grammar is learned using
statistical methods. During generation, edges that are added to the chart are
ranked depending on their distance to the closest instance in the example base.
This is where the memory-based approach comes into play. In order to allow for
careful generalization in the instance base, the authors propose to add a list of tag
(?slots?) with which the corpus is annotated. Based on this annotated corpus,
a semantic grammar is learned. For ranking the edge based on the instances,
the authors propose the well-known tf-idf scheme with the difference that those
words that are annotated with a semantic tag are replaced by their tag.
3 Kernels
Memory-based learning requires a distance metric in order to identify instances
similar to the problem at hand. We propose to use convolution kernels as distance
metric. A kernel K can be seen as a generalized form of a distance metric that
performs the following calculation
K(x, y) = ??(x), ?(y)?,
Instance-Based Generation 489
where ? is a non-linear mapping from the input space into some higher di-
mensional feature space, and ??, ?? is the inner product in the feature space.
Calculating the inner product in some space of higher dimension than the in-
put space is desirable for classifiers because non linearly separable sets can be
linearly separated in the higher dimensional feature space. Kernel methods are
computationally attractive because the kernel can calculate the mapping and
the inner product implicitly rather than explicitly determining the image under
? of the input.
While Bag-of-Words techniques can be employed as an approximation to
derive feature vectors for classifiers, the loss of structure is not desirable. To
address this problem, Haussler [3] proposed Convolution Kernels that are capable
of processing structured objects x and y. The structured objects x and y consist
of components x1, . . . , xm and y1, . . . , yn. The convolution kernel of x and y is
given by the sum of the products of the components? convolution kernels. This
approach can be applied to structured objects of various kinds, and results have
been reported for string kernels and tree kernels.
3.1 Hierarchical Tree Kernel
The idea behind Convolution Kernels is that the kernel of two structures is
defined as the sum of the kernels of their parts. Formally, let D be a positive
integer and X, X1, . . . , XD separable metric spaces. Furthermore, let x and y
be two structured objects, and x = x1, . . . , xD and y = y1, . . . , yD their parts.
The relation R ? X1 ? . . . ? XD ? X holds for x and x if x are the parts of
x. The inverse R?1 maps each structured object onto its parts, i.e. R?1(x) =
{x : R(x, x)}. Then the kernel of x and y is given by the following generalized
convolution:
K(x, y) =
?
x?R?1(x)
?
y?R?1(y)
D
?
1
Kd(xd, yd)
Informally, the value of a convolution kernel for two objects X and Y is given
by the sum of the kernel value for each of the substructures, i.e. their convolution.
Suzuki et al[10] proposed Hierarchical Directed Acyclic Graph kernels in
which the substructures contain nodes which can contain graphs themselves. The
hierarchy of graphs allows extended information from multiple components to be
represented and used in classification. In addition, nodes may be annotated with
attributes, such as part of speech tags, in order to add information. For example,
in a Question-Answering system, components such as Named Entity Extraction,
Question Classification, Chunking and so on may each add to the graph.
4 Corpus
We collected a corpus for our instance based generation system as follows. We
set up communications between a wizard and users. The wizard was instructed
to ?act like the system? we intend to build, that is, she was required to interact
with the user either by prompting for more information or give the user the
information she thought he wanted. Altogether, 20 users participated in the
490 M. Denecke and H. Tsukada
Fig. 1. Extract from the dialogue corpus
data collection effort. Each user contributed to 8 to 15 dialogues. The length
of the dialogues varies between 11 and 84 turns, the median being 34 turns.
Altogether, the corpus consists of 201 dialogues. The corpus consists of 6785
turns, 3299 of which are user turns and the remaining 3486 are wizard turns.
Due to the strict dialogue regiment prescribed in the onset of the data collection,
each dialogue consists either of an equal number of user and wizard turns (in
case the user ends the dialogue; 14 cases) or one wizard turn more than user
turn in case the wizard ends the dialogue (187 cases). Figure 1 shows the first
part of a dialogue from the corpus.
5 Generation Algorithm
5.1 Overview of the Algorithm
We now describe our algorithm informally. Given the dialogue history up until
now, the last user utterance and the result list as a response to the last user
utterance, it is the task of the algorithm to generate an appropriate question to
elicit more information from the user. Recall an external dialogue module (not
described in this paper) decides whether an information seeking question should
be generated (as opposed to, say, turning the information found in the highest
ranking article into an answer).
Informally, the algorithm works as follows. Initially, the dialogue corpus is
preprocessed, including word segmentation and part-of-speech labeling (see sec-
tion 5.2). In step 1, a ranked list of question candidates is generated (see section
5.3). In step 2, for each of the candidates, a list of change positions is deter-
mined (see section 5.4). These indicate the part of the questions that need to
be adapted to the current situation. Subsequently, the portions indicated by the
change positions are replaced by appropriate constituents. In the step 3, the
candidates generated in the previous step are re-ranked (see section 5.5). Re-
ranking takes place by using the same distance as the one in step 1. The highest
ranking candidate is then presented to the user.
Instance-Based Generation 491
5.2 Corpus Preprocessing
Since Japanese does not provide word segmentation, we need to preprocess the
corpus. The corpus consists of a set of dialogues. Each dialogue consists of a set
of utterances. Each utterance is annotated for speaker and utterance type. In a
dialogue, wizard and user utterance strictly alternate, with no interjections.
Preprocessing is done as follows. Each utterance is stripped of its annotations
and presented to the part-of-speech tagger Chasen [1]. Chasen segments the input
sentence, reduces inflected words to their base forms and assigns part of speech
tags to the base forms. We use the notation cw(u) to designate the content words
in utterance, sentence or newspaper article u. For our purposes, content words
are adjectives, nouns and verbs, de-inflected to their base form, if necessary. A
subsequent processing step assigns semantic labels and named entity classes to
the de-inflected word forms.
5.3 Sentence Selection
In order to understand the motivation for our approaches to sentence selection,
it is necessary to recall the context in which sentences are selected. We would like
to find a information seeking question similar to the one we want to generate.
The question to be generated is determined by the dialogue context. A natural
approach is to choose a bag-of-word distance measure for sentences, define a
distance for partial dialogues based on this distance and then choose the dialogue,
and a sentence from that dialogue with the lowest distance.
It turns out, however, that this approach does not work too well. One problem
is that in the beginning of a dialogue not many informative words are contained in
the utterances, therefore making an informed selection of utterances difficult. The
point of this paper is to determine how to overcome this problem. In the following
two sections, we propose two approaches. The first uses additional information in
the retrieved documents, and the second uses additional syntactic and semantic in-
formationwhen calculating the distance between sentences.Bothmethods consists
of calculating a score for candidate sentences and selecting the highest ranking one.
Method 1. Information retrieval over large corpora works well due to the redun-
dancy in the document data, a fact that for example Latent Semantic Indexing
exploits. The principal idea of the first method is to use the redundancy in the
unrestricted document corpus when scoring sentence candidates. Instead of de-
termining the bag-of-word score between a candidate sentence and the query
sentence, we submit the information extracted from the candidate dialogue and
the current dialogue to the information retrieval engine, resulting in two n best
lists of articles L and L?. In order to score the degree of similarity, we determine
the the intersection of content words in the retrieved articles. The larger the in-
tersection, the higher the score is to be ranked. In order to take relevance in the
result set into account, the scores are discounted by the position of the article
in the n best list. More specifically, we calculate the similarity score between
the current dialogue and an example dialogue as follows. Let d be the currently
developing dialogue consisting of t user utterances and u1, . . . ut be the user ut-
terances in the current dialogue up until now. Furthermore, let d? be an example
dialogue from the corpus and let u?1, . . . u
?
t? be the first t
? user utterances in the
example dialogue. Then:
492 M. Denecke and H. Tsukada
1. Form the union of content words CW =
?
t cw(ut), CW
? =
?
t? cw(u
?
t? )
2. Submit two queries to the information retrieval engine consisting of CW and
CW ?, respectively and obtain two article n best lists L and L?.
3. Calculate the similarity score according to
sim(ut, u
?
t?) =
?
l?L
?
l??L?
cw(l) ? cw(l?)
rank(l) + rank(l?)
Method 2. In the first method described above, we seek to overcome poor
scoring function by adding redundancy from the information retrieval engine.
The second method we propose attempts to improve scoring by adding syntactic
and semantic structure to the distance metric. More specifically, we directly
compare the last user utterance in the current dialogue with the last utterance
in the example dialogue, but do so in a more detailed manner. To this end, we
determine the similarity score as the output of the hierarchical directed acyclic
graph kernel. The similarity is thus defined as sim(ut, u?t?) = K(ut, u
?
t?).
5.4 Sentence Adaptation
The adaptation of the highest ranking question to the current dialogue consists of
four steps. First, we determine the location(s) where change should take place.
Second, we determine constraints for the substituting constituent. Third, we
determine a list of substituents for each location of change. Fourth, we replace
the phrase(s) at the location(s) of change with the highest ranking element from
the corresponding list of substituents.
Determining Locations of Change. After the example sentences have been
retrieved from the corpus, we need to determine where and how the questions
need to be adapted to the current dialogue. We determine the locations of change
li by identifying suitable head words of phrase to be exchanged. What are the
criteria for suitable head words? Recall that the example sentences are drawn
from dialogue similar in topics but in which the content words are exchanged.
This limits the part-of-speech of the words to be exchanged to nouns and verbs.
Therefore, we construct a list l of nouns and verbs that are part of the retrieved
sentence but cannot be found in the current user query. Second, since we are
interested in replacing those content words that are specific to the retrieved
dialogue with those specific to the current dialogue, we would like to incorporate
some measure of informativeness. For that reason, we determine the unigram
count for all content words in l. High ranking candidates for change are those
words that are specific (i.e., have a low unigram count above a certain threshold).
Constraints for Substituents. The constraints for the substituents are given
by the semantic and syntactic information of the phrase at the change location.
More specifically, the constraints include the following features: Part of speech,
type of named entity, if applicable (the type includes location, state, person
name and so on), and semantic class.
Determining Substituents. After having determined the change locations
and constraints of the substituents, we proceed to determine the substituents.
Instance-Based Generation 493
The primary source for substituents are the retrieved newspaper articles. How-
ever, since we wish to apply the generation component in a dialogue system, we
need to take implicit confirmation into account as well. For this reason, we deter-
mine whether a phrase matching the phrase at change location li occurs before
li previously in the dialogue. If this is the case, the source for the substituent is
to be the current dialogue.
Given the constraints for a change location determined in the previous step,
we add all content words from the highest ranking article to the candidate list
for that change location. The score for a content word is given by the number of
constraints it fulfills. Ties are broken by unigram counts so that rare words get
a higher score due to their informativeness.
Application of Change. Applying the change simply consists of removing the
phrase whose head word is located at the change location and replacing it with
the highest ranking word from the candidate list for that score.
5.5 Reranking
The previous steps produce a list of sentence candidates. For each of the sentence
candidates, we calculate the similarity between the generated sentence with the
sentences from a small corpus of desirable sentences. Finally, the sentence with
the highest score is presented to the user. Examples of generated sentences are
shown in figure 2. The complete algorithm is given in figure 3.
Fig. 2. Generated questions. The substituent in the first question comes from the
dialogue context, while the other substituents come from retrieved articles.
6 Evaluation
The evaluation was done as follows. We divided the corpus in a example base and
a test set. The example base consists of 151 randomly selected dialogues, the test
set consists of the remaining 50 dialogues. From each of the test examples, we
supplied the initial wizard greeting and the initial user utterance as context for
the dialogue. Given this context, each method generated an n best list consisting
of 3 information seeking questions.
The generated lists were labeled by three annotators according to the follow-
ing criteria. For each of the three questions in the n best lists, the annotators
had to determine a syntactic, a semantic and an overall score. The scores range
over the labels poor, acceptable, good. The same score could be assigned more
494 M. Denecke and H. Tsukada
Input: Preprocessed dialogue corpus C = {d?1, . . . , d?n}
Current dialogue d with user utterances u1, . . . , ut
Output: Information seeking question
Step 1: Determine sim(ut, u?t?) for all user utterances u
?
t? from the dialogue corpus
Select the w?1, . . . w
?
k wizard utterances directly following the k
highest ranking utterances
Step 2: for each w?i ? {w?1, . . . , w?k}:
Determine change locations l1, . . . , ll
for each lj ? {l1, . . . , ll}
Determine list of substituents s1ij , . . . , s
p
ij
Generate modified sentence list v1, . . . , vm by replacing substituents
at change locations
Step 3: Determine and return highest ranking vi? .
Fig. 3. Generation algorithm
than once, for example, in case the sentence selection algorithm produced an
unreliable candidate, the overall score for all three sentence candidates could
be bad. Furthermore, the evaluators had to re-arrange the 3 best list according
to the quality of the generated questions. Finally, the annotators had provide
a sentence they consider good. For easy comparison, the symbolic scores poor,
acceptable, good translate to 0,0.5 and 1, respectively, in the tables below.
6.1 Scoring Results
The results of the three best syntactic and semantic sentence scoring are shown
in table 1 (a) and 1 (b). The inter-annotator agreement is given by their kappa
scores for each method separately. Table 1 (c) shows the average of syntactic
and semantic scores. The kappa coefficient for the inter-annotator agreement for
these scores are 0.68, 0.72, and 0.71, respectively.
The syntactic scores rank higher than the semantic scores. This is explained
by the fact that the corpus contains syntactically relatively well-formed example
sentences, and the replacement operator, in addition to being constrained by
Table 1. Average of syntactic and semantic scores
Method 1 Method 2
1 0.796 0.800
2 0.657 0.790
3 0.787 0.780
(a)
Method 1 Method 2
1 0.573 0.393
2 0.393 0.426
3 0.416 0.376
(b)
Method 1 Method 2
1 0.685 0.596
2 0.525 0.608
3 0.602 0.578
(c)
Instance-Based Generation 495
part-of-speech as well as semantic information, does not have much opportunity
to create a syntactically malformed sentence. Furthermore, method 1 produces
sentences that are semantically more accurate than method 2.
6.2 Ranking Results
In order to determine the quality of the ranking, the annotators had to rerank the
generated questions. We determine the distance between two rankings according
to the Edit distance. Since the generated lists are only of length 3, there are only
three possibilties: the lists are equal (edit distance 0), one element in both lists is
the same (edit distance 2), and no element in the lists is the same, (edit distance
3). In order to allow easy comparison with the table above, we award scores of 1,
0.5 and 0 for edit distances of 0, 2 and 3, respectively (i.e., 1 is best, 0 is worst).
The annotators were asked to rank the questions according to syntactic criteria
alone, semantic criteria alone and all criteria. The results are shown in Table 2.
Table 2. Comparison of ranking: Syntactic, semantic and overall
Method 1 Method 2
1 0.493 0.893
2 0.813 0.860
3 0.767 0.227
(a)
Method 1 Method 2
1 0.720 0.873
2 0.760 0.780
3 0.567 0.353
(b)
Method 1 Method 2
1 0.766 0.853
2 0.740 0.726
3 0.573 0.213
(c)
It can be seen that method 2 ranks the example sentences in a way that is
more in line with the choices of the annotators than method 1.
6.3 Quality of Ranking
We hypothesize that the differences in the performance of the algorithms is due
to the different selection mechanisms. In order to validate this point, we asked
the three annotators to each provide one utterance they would rank highest for
each system question (called gold standard). Then, we formed a list of 6 sentences
u?1, . . . u
?
6 (3 generated by the generation algorithm and 3 by the annotators) and
compared for each dialogue context the scores sim(ut, u?i) for those 6 sentences
where ut is the user utterance from the corresponding test case. We expect a
perfect ranking algorithm to value the gold standard as least as high as any
sentence from the corpus, and to value the gold standard higher every time the
annotators found the generated sentences faulty. It turns out that method 1
places the sentences of the gold standard in the top 3 in 42.3% of the cases while
method 2 does this in 59.3% of the cases.
7 Discussion
It can be seen that in general, method 1 produces higher quality sentences while
method 2 ranks the sentences better. We interpret this as follows. For sentence
selection, the redundancy as provided by the IR engine is helpful, whereas for
ranking of example sentences, the additional structure as expressed in the ker-
nel helps.
496 M. Denecke and H. Tsukada
7.1 Related Work
Kiyota and colleagues [5] describe an interactive restricted domain question an-
swering system where users can interactively retrieve causes for problems with
a computers? operating system. Here, the problem of missing structure is solved
by providing so-called dialogue cards which provide the knowledge necessary for
dialogue processing. A dialogue card contains keywords, a question as asked by
the user in natural language (for example ?Windows does not boot?), an infor-
mation seeking question to be issued by the system (for example ?Which version
of Windows do you use?) and a list of options associated with actions. The ac-
tions are executed in function of the users? answer to the question. Dialogue
processing takes place by retrieving relevant dialogue cards, where relevance is
determined by matching the users? question and keywords with the question and
keywords noted on the dialogue card. Compared to our method, this method re-
quires substantially more structure to be represented in the dialogue cards and
is therefore more expensive to develop. Furthermore, the absence of any sort of
change operators to adapt the question from the dialogue card to the current sit-
uation does not provide as much flexibility as our method. On the other hand,
the highly structured dialogue cards give the developers more control (at the
price of a higher development cost) over the systems behavior than our method
and is therefore less risky in situations where failure is expensive.
In Small et al[8], retrieved documents are forced into frame structures. Mis-
matches or between the fillers of the frame structures or missing fillers trigger
information seeking questions to the user. While the generation as it is actually
used is not described in the paper, we believe that the frames provide sufficient
structure for template-based approaches.
Hori and coworkers [4] developed an interactive question answering system
based on a Japanese newspaper corpus. The purpose of information seeking ques-
tions is to prompt the user for missing or disambiguating information. From a
generation point of view, strong assumptions are made on the surface form of
the generated information seeking question. More specifically, ambiguous key-
words are combined with disambiguating options by means of the Japanese par-
ticle ?no?.
7.2 Summary
To summarize, the presented approaches attempt in different ways to compen-
sate for the lack of structure in an question answering system. Structure can
be provided explicitly as in the case of the dialogue cards, can be introduced
during processing as in the case of the frame-based document representations,
and can be assumed in the target expression as in the case of the generation
templates. In contrast to the described methods, our method does not require
an explicit representation of structure. Rather, the structure is given by what-
ever structure the kernel and the change operators construct during generation.
In other words, the structure our approach uses is (1) restricted to the question
to be generated and does not apply to the document level, and (2) in tradition
with the lazy learning characteristics of memory-based approaches is generated
on the fly on an as-needed basis, as opposed to being dictated from the outset
at design time.
Instance-Based Generation 497
Acknowledgements
We acknowledge the help of Takuya Suzuki with the implementation. Jun Suzuki
provided the implementation of the HDAG kernel. We would like to thank
Hideki Isozaki and our colleagues at NTT CS labs for discussion and
encouragement.
References
1. M. Asahara and Y. Matsumoto. 2000. Extended Models and Tools for High-
Performance Part-of-Speech Tagger. In Proceedings of The 18th International
Conference on Computational Linguistics, Coling 2000, Saarbru?cken, Germany.
2. W. Daelemans. 1999. Introduction to the Special Issue on Memory-Based Language
Processing. Journal of Experimental and Theoretical Artificial Intelligence.
3. D. Haussler. 1999. Convolution kernels on discrete structures. Technical report,
UC Santa Cruz.
4. C. Hori, T. Hori, H. Tsukada, H. Isozaki, Y. Sasaki, and E. Maeda. 2003. Spoken
interactive odqa system: Spiqa. In Proc. of the 41th Annual Meeting of Association
for Computational Linguistics (ACL-2003), Sapporo, Japan.
5. K. Kiyota, S. Kurohashi, and F. Kido. 2002. ?Dialog Navigator?: A Question
Answering System based on Large Text Knowledge Base. In Proceedings of The
19th International Conference on Computational Linguistics, Coling 2002,Taipei,
Taiwan.
6. I. Langkilde and K. Knight. 1998. Generation that exploits Corpus-Based Statis-
tical Knowledge. In Proceedings of the Conference of the Association for Compu-
tational Linguistics (COLING/ACL).
7. A.H. Oh and A. Rudnicky. 2000. Stochastic Language Generation for Spoken
Dialogue Systems. In ANLP/NAACL 2000 Workshop on Conversational Systems,
pages 27?32.
8. S. Small and T. Strzalkowski. 2004. Hitiqa: Towards analytical question answering.
In Proceedings of The 20th International Conference on Computational Linguistics,
Coling 2004,Geneva Switzerland.
9. C. Stanfill and D. Waltz. 1986. Toward Memory-based Reasoning. Communications
of the ACM, vol. 29, pages 1213-1228.
10. J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda. 2003. Hierarchical directed acyclic
graph kernel: Methods for structured natural language data. In Proc. of the 41th
Annual Meeting of Association for Computational Linguistics (ACL-2003), Sap-
poro, Japan, pages 32?39.
11. S. Varges and C. Mellish. 2001. Instance-based natural language generation. In
Proceedings of the 2nd Meeting of the North American Chapter of the Association
for Computational Linguistics, pages 1?8.
12. M. Walker, O. Rambow, and M. Rogati. 2001. SPoT: A Trainable Sentence
Planner. In Proceedings of the North American Meeting of the Association for
Computational Linguistics.
Spoken Interactive ODQA System: SPIQA
Chiori Hori, Takaaki Hori, Hajime Tsukada,
Hideki Isozaki, Yutaka Sasaki and Eisaku Maeda
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
We have been investigating an interactive
approach for Open-domain QA (ODQA)
and have constructed a spoken interactive
ODQA system, SPIQA. The system de-
rives disambiguating queries (DQs) that
draw out additional information. To test
the efficiency of additional information re-
quested by the DQs, the system recon-
structs the user?s initial question by com-
bining the addition information with ques-
tion. The combination is then used for an-
swer extraction. Experimental results re-
vealed the potential of the generated DQs.
1 Introduction
Open-domain QA (ODQA), which extracts answers
from large text corpora, such as newspaper texts, has
been intensively investigated in the Text REtrieval
Conference (TREC). ODQA systems return an ac-
tual answer in response to a question written in a
natural language. However, the information in the
first question input by a user is not usually sufficient
to yield the desired answer. Interactions for col-
lecting additional information to accomplish QA are
needed. To construct more precise and user-friendly
ODQA systems, a speech interface is used for the
interaction between human beings and machines.
Our goal is to construct a spoken interactive
ODQA system that includes an automatic speech
recognition (ASR) system and an ODQA system.
To clarify the problems presented in building such
a system, the QA systems constructed so far have
been classified into a number of groups, depending
on their target domains, interfaces, and interactions
to draw out additional information from users to ac-
complish set tasks, as is shown in Table 1. In this
table, text and speech denote text input and speech
input, respectively. The term ?addition? represents
additional information queried by the QA systems.
This additional information is separate to that de-
rived from the user?s initial questions.
Table 1: Domain and data structure for QA systems
target domain specific open
data structure knowledge DB unstructured text
without addition CHAT-80 SAIQAtext
with addition MYCIN (SPIQA?)
without addition Harpy VAQA
speech
with addition JUPITER (SPIQA?)
? SPIQA is our system.
To construct spoken interactive ODQA systems,
the following problems must be overcome: 1. Sys-
tem queries for additional information to extract an-
swers and effective interaction strategies using such
queries cannot be prepared before the user inputs the
question. 2. Recognition errors degrade the perfor-
mance of QA systems. Some information indispens-
able for extracting answers is deleted or substituted
with other words.
Our spoken interactive ODQA system, SPIQA,
copes with the first problem by adopting disam-
biguating users? questions using system queries. In
addition, a speech summarization technique is ap-
plied to handle recognition errors.
2 Spoken Interactive QA system: SPIQA
Figure 1 shows the components of our system, and
the data that flows through it. This system com-
prises an ASR system (SOLON), a screening filter
that uses a summarization method, and ODQA en-
gine (SAIQA) for a Japanese newspaper text corpus,
a Deriving Disambiguating Queries (DDQ) module,
and a Text-to-Speech Synthesis (TTS) engine (Fi-
nalFluet).
ASR
TTS
Screening
filter
ODQA engine
(SAIQA)
DDQ
module
Answer
derived?
Answer
sentence generator
Question
reconstructor
No
Yes
Additional
info. New question
First
question
Question/
Additional info.
User Answer/
DDQ speech
Answer
sentence
DDQ
sentence
Recognition
result
Answer
Figure 1: Components and data flow in SPIQA.
ASR system
Our ASR system is based on the Weighted Finite-
State Transducers (WFST) approach that is becom-
ing a promising alternative formulation for the tra-
ditional decoding approach. The WFST approach
offers a unified framework representing various
knowledge sources in addition to producing an op-
timized search network of HMM states. We com-
bined cross-word triphones and trigrams into a sin-
gle WFST and applied a one-pass search algorithm
to it.
Screening filter
To alleviate degradation of the QA?s perfor-
mance by recognition errors, fillers, word fragments,
and other distractors in the transcribed question, a
screening filter that removes these redundant and
irrelevant information and extracts meaningful in-
formation is required. The speech summarization
approach (C. Hori et. al., 2003) is applied to the
screening process, wherein a set of words maximiz-
ing a summarization score that indicates the appro-
priateness of summarization is extracted automati-
cally from a transcribed question, and these words
are then concatenated together. The extraction pro-
cess is performed using a Dynamic Programming
(DP) technique.
ODQA engine
The ODQA engine, SAIQA, has four compo-
nents: question analysis, text retrieval, answer hy-
pothesis extraction, and answer selection.
DDQ module
When the ODQA engine cannot extract an appro-
priate answer to a user?s question, the question is
considered to be ?ambiguous.? To disambiguate the
initial questions, the DDQ module automatically de-
rives disambiguating queries (DQs) that require in-
formation indispensable for answer extraction. The
situations in which a question is considered ambigu-
ous are those when users? questions exclude indis-
pensable information or indispensable information
is lost through ASR errors. These instances of miss-
ing information should be compensated for by the
users.
To disambiguate a question, ambiguous phrases
within it should be identified. The ambiguity of
each phrase can be measured by using the struc-
tural ambiguity and generality score for the phrase.
The structural ambiguity is based on the dependency
structure of the sentence; phrase that is not modified
by other phrases is considered to be highly ambigu-
ous. Figure 2 has an example of a dependency struc-
ture, where the question is separated into phrases.
Each arrow represents the dependency between two
phrases. In this example, ?the World Cup? has no
Which  country won the  world  cupin Southeast Asia ?
Figure 2: Example of dependency structure.
modifiers and needs more information to be identi-
fied. ?Southeast Asia? also has no modifiers. How-
ever, since ?the World Cup?appears more frequently
than ?Southeast Asia? in the retrieved corpus, ?the
World Cup? is more difficult to identify. In other
words, words that frequently occur in a corpus rarely
help to extract answers in ODQA systems. There-
fore, it is adequate for the DDQ module to generate
questions relating to ?World Cup? in this example,
such as ?What kind of World Cup?? , ?What year
was the World Cup held??.
The structural ambiguity of the n-th phrase is de-
fined as
A
D
(P
n
) = log
{
1 ?
?
N
i=1:i=n
D(P
i
, P
n
)
}
,
where the complete question is separated into N
phrases, and D(P
i
, P
n
) is the probability that phrase
P
n
will be modified by phrase P
i
, which can be cal-
culated using Stochastic Dependency Context-Free
Grammar (SDCFG) (C. Hori et. al., 2003).
Using this SDCFG, only the number of non-
terminal symbols is determined and all combina-
tions of rules are applied recursively. The non-
terminal symbol has no specific function, such as
a noun phrase. All the probabilities of rules are
stochastically estimated based on data. Probabilities
for frequently used rules become greater, and those
for rarely used rules become smaller. Even though
transcription results given by a speech recognizer are
ill-formed, the dependency structure can be robustly
estimated by our SDCFG.
The generality score is defined as
A
G
(P
n
) =
?
w?P
n
:w=cont log P (w),
where P (w) is the unigram probability of w based
on the corpus to be retrieved. Thus, ?w = cont?
means that w is a content word such as a noun, verb
or adjective.
We generate the DQs using templates of interrog-
ative sentences. These templates contain an inter-
rogative and a phrase taken from the user?s question,
i.e., ?What kind of * ??, ?What year was * held??
and ?Where is * ??.
The DDQ module selects the best DQ based on its
linguistic appropriateness and the ambiguity of the
phrase. The linguistic appropriateness of DQs can
be measured by using a language model, N-gram.
Let S
mn
be a DQ generated by inserting the n-th
phrase into the m-th template. The DDQ module
selects the DQ that maximizes the DQ score:
H(S
mn
) = ?
L
L(S
mn
)+?
D
A
D
(P
n
)+?
G
A
G
(P
n
),
where L(?) is a linguistic score such as the loga-
rithm for trigram probability, and ?
L
, ?
D
, and ?
G
are weighting factors to balance the scores.
Hence, the module can generate a sentence that
is linguistically appropriate and asks the user to dis-
ambiguate the most ambiguous phrase in his or her
question.
3 Evaluation Experiments
Questions consisting of 69 sentences read aloud by
seven male speakers were transcribed by our ASR
system. The question transcriptions were processed
with a screening filter and input into the ODQA
engine. Each question consisted of about 19 mor-
phemes on average. The sentences were grammat-
ically correct, formally structured, and had enough
information for the ODQA engine to extract the cor-
rect answers. The mean word recognition accuracy
obtained by the ASR system was 76%.
3.1 Screening filter
Screening was performed by removing recognition
errors using a confidence measure as a threshold and
then summarizing it within an 80% to 100% com-
paction ratio. In this summarization technique, the
word significance and linguistic score for summa-
rization were calculated using text from Mainichi
newspapers published from 1994 to 2001, compris-
ing 13.6M sentences with 232M words. The SD-
CFG for the word concatenation score was calcu-
lated using the manually parsed corpus of Mainichi
newspapers published from 1996 to 1998, consist-
ing of approximately 4M sentences with 68M words.
The number of non-terminal symbols was 100. The
posterior probability of each transcribed word in a
word graph obtained by ASR was used as the confi-
dence score.
3.2 DDQ module
The word generality score A
G
was computed using
the same Mainichi newspaper text described above,
while the SDCFG for the dependency ambiguity
score A
D
for each phrase was the same as that used
in (C. Hori et. al., 2003). Eighty-two types of inter-
rogative sentences were created as disambiguating
queries for each noun and noun-phrase in each ques-
tion and evaluated by the DDQ module. The linguis-
tic score L indicating the appropriateness of inter-
rogative sentences was calculated using 1000 ques-
tions and newspaper text extracted for three years.
The structural ambiguity score A
D
was calculated
based on the SDCFG, which was used for the screen-
ing filter.
3.3 Evaluation method
The DQs generated by the DDQ module were eval-
uated in comparison with manual disambiguation
queries. Although the questions read by the seven
speakers had sufficient information to extract ex-
act answers, some recognition errors resulted in a
loss of information that was indispensable for ob-
taining the correct answers. The manual DQs were
made by five subjects based on a comparison of
the original written questions and the transcription
results given by the ASR system. The automatic
DQs were categorized into two classes: APPRO-
PRIATE when they had the same meaning as at
least one of the five manual DQs, and INAPPRO-
PRIATE when there was no match. The QA per-
formance in using recognized (REC) and screened
questions (SCRN) were evaluated by MRR (Mean
Reciprocal Rank) (http://trec.nist.gov/data/qa.html).
SCRN was compared with the transcribed question
that just had recognition errors removed (DEL). In
addition, the questions reconstructed manually by
merging these questions and additional information
requested the DQs generated by using SCRN, (DQ)
were also evaluated. The additional information was
extracted from the original users? question without
recognition errors. In this study, adding information
by using the DQs was performed only once.
3.4 Evaluation results
Table 2 shows the evaluation results in terms of
the appropriateness of the DQs and the QA-system
MRRs. The results indicate that roughly 50% of the
DQs generated by the DDQ module based on the
screened results were APPROPRIATE. The MRR
for manual transcription (TRS) with no recognition
errors was 0.43. In addition, we could improve the
MRR from 0.25 (REC) to 0.28 (DQ) by using the
DQs only once. Experimental results revealed the
potential of the generated DQs in compensating for
the degradation of the QA performance due to recog-
nition errors.
4 Conclusion
The proposed spoken interactive ODQA system,
SPIQA copes with missing information by adopt-
ing disambiguation of users? questions by system
queries. In addition, a speech summarization tech-
nique was applied for handling recognition errors.
Although adding information was performed using
DQs only once, experimental results revealed the
potential of the generated DQs to acquire indispens-
able information that was lacking for extracting an-
swers. In addition, the screening filter helped to gen-
erate the appropriate DQs. Future research will in-
Table 2: Evaluation results of disambiguating
queries generated by the DDQ module.
Word MRR w/o IN-SPK
acc. REC DEL SCRN DQ errors APP APP
A 70% 0.19 0.16 0.17 0.23 4 32 33
B 76% 0.31 0.24 0.29 0.31 8 36 25
C 79% 0.26 0.18 0.26 0.30 10 34 25
D 73% 0.27 0.21 0.24 0.30 4 35 30
E 78% 0.24 0.21 0.24 0.27 7 31 31
F 80% 0.28 0.25 0.30 0.33 8 34 27
G 74% 0.22 0.19 0.19 0.22 3 35 31
AVG 76% 0.25 0.21 0.24 0.28 9% 49% 42%
An integer without a % other than MRRs indicates number of
sentences. Word acc.:word accuracy, SPK:speaker, AVG: aver-
aged values, w/o errors: transcribed sentences without recog-
nition errors, APP: appropriate DQs and InAPP: inappropriate
DQs.
clude an evaluation of the appropriateness of DQs
derived repeatedly to obtain the final answers. In
addition, the interaction strategy automatically gen-
erated by the DDQ module should be evaluated in
terms of how much the DQs improve QA?s total per-
formance.
References
F. Pereira et. al., ?Definite Clause Grammars for Language
Analysis ?a Survey of the Formalism and a Comparison with
Augmented Transition Networks,? Artificial Intelligence, 13:
231-278, 1980.
E. H. Shortliffe, ?Computer-Based Medical Consultations:
MYCIN,? Elsevier/North Holland, New York NY, 1976.
B. Lowerre et. al., ?The Harpy speech understanding system,?
W. A. Lea (Ed.), Trends in Speech recognition, pp. 340, Pren-
tice Hall.
L. D. Erman et. al., ?The Hearsay-II Speech-Understanding
System: Integrating Knowledge to Resolve Uncertainty,?
ACM computing Survays, Vol. 12, No. 2, pp. 213 ? 253,
1980.
V. Zue, et al, ?JUPITER: A Telephone-Based Conversational
Interface for Weather Information,? IEEE Transactions on
Speech and Audio Processing, Vol. 8, No. 1, 2000.
S. Harabagiu et. al., ?Open-Domain Voice-Activated Ques-
tion Answering,? COLING2002, Vol.I, pp. 321?327, Taipei,
2002.
C. Hori et. al., ?A Statistical Approach for Automatic Speech
Summarization,? EURASIP Journal on Applied Signal Pro-
cessing (EURASIP), pp128?139, 2003.
Y. Sasaki et. al., ?NTT?s QA Systems for NTCIR QAC-1,?
Working Notes of the Third NTCIR Workshop Meeting,
pp.63?70, 2002.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 617?624,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Incorporating speech recognition confidence into
discriminative named entity recognition of speech data
Katsuhito Sudoh Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan
{sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a named entity recog-
nition (NER) method for speech recogni-
tion results that uses confidence on auto-
matic speech recognition (ASR) as a fea-
ture. The ASR confidence feature indi-
cates whether each word has been cor-
rectly recognized. The NER model is
trained using ASR results with named en-
tity (NE) labels as well as the correspond-
ing transcriptions with NE labels. In ex-
periments using support vector machines
(SVMs) and speech data from Japanese
newspaper articles, the proposed method
outperformed a simple application of text-
based NER to ASR results in NER F-
measure by improving precision. These
results show that the proposed method is
effective in NER for noisy inputs.
1 Introduction
As network bandwidths and storage capacities
continue to grow, a large volume of speech data
including broadcast news and PodCasts is becom-
ing available. These data are important informa-
tion sources as well as such text data as newspaper
articles and WWW pages. Speech data as infor-
mation sources are attracting a great deal of inter-
est, such as DARPA?s global autonomous language
exploitation (GALE) program. We also aim to use
them for information extraction (IE), question an-
swering, and indexing.
Named entity recognition (NER) is a key tech-
nique for IE and other natural language process-
ing tasks. Named entities (NEs) are the proper ex-
pressions for things such as peoples? names, loca-
tions? names, and dates, and NER identifies those
expressions and their categories. Unlike text data,
speech data introduce automatic speech recogni-
tion (ASR) error problems to NER. Although im-
provements to ASR are needed, developing a ro-
bust NER for noisy word sequences is also impor-
tant. In this paper, we focus on the NER of ASR
results and discuss the suppression of ASR error
problems in NER.
Most previous studies of the NER of speech
data used generative models such as hidden
Markov models (HMMs) (Miller et al, 1999;
Palmer and Ostendorf, 2001; Horlock and King,
2003b; Be?chet et al, 2004; Favre et al, 2005).
On the other hand, in text-based NER, better re-
sults are obtained using discriminative schemes
such as maximum entropy (ME) models (Borth-
wick, 1999; Chieu and Ng, 2003), support vec-
tor machines (SVMs) (Isozaki and Kazawa, 2002),
and conditional random fields (CRFs) (McCal-
lum and Li, 2003). Zhai et al (2004) applied a
text-level ME-based NER to ASR results. These
models have an advantage in utilizing various fea-
tures, such as part-of-speech information, charac-
ter types, and surrounding words, which may be
overlapped, while overlapping features are hard to
use in HMM-based models.
To deal with ASR error problems in NER,
Palmer and Ostendorf (2001) proposed an HMM-
based NER method that explicitly models ASR er-
rors using ASR confidence and rejects erroneous
word hypotheses in the ASR results. Such rejec-
tion is especially effective when ASR accuracy is
relatively low because many misrecognized words
may be extracted as NEs, which would decrease
NER precision.
Motivated by these issues, we extended their ap-
proach to discriminative models and propose an
NER method that deals with ASR errors as fea-
617
tures. We use NE-labeled ASR results for training
to incorporate the features into the NER model as
well as the corresponding transcriptions with NE
labels. In testing, ASR errors are identified by
ASR confidence scores and are used for the NER.
In experiments using SVM-based NER and speech
data from Japanese newspaper articles, the pro-
posed method increased the NER F-measure, es-
pecially in precision, compared to simply applying
text-based NER to the ASR results.
2 SVM-based NER
NER is a kind of chunking problem that can
be solved by classifying words into NE classes
that consist of name categories and such chunk-
ing states as PERSON-BEGIN (the beginning of
a person?s name) and LOCATION-MIDDLE (the
middle of a location?s name). Many discrimi-
native methods have been applied to NER, such
as decision trees (Sekine et al, 1998), ME mod-
els (Borthwick, 1999; Chieu and Ng, 2003), and
CRFs (McCallum and Li, 2003). In this paper, we
employ an SVM-based NER method in the follow-
ing way that showed good NER performance in
Japanese (Isozaki and Kazawa, 2002).
We define three features for each word: the
word itself, its part-of-speech tag, and its charac-
ter type. We also use those features for the two
preceding and succeeding words for context de-
pendence and use 15 features when classifying a
word. Each feature is represented by a binary
value (1 or 0), for example, ?whether the previous
word is Japan,? and each word is classified based
on a long binary vector where only 15 elements
are 1.
We have two problems when solving NER
using SVMs. One, SVMs can solve only a
two-class problem. We reduce multi-class prob-
lems of NER to a group of two-class problems
using the one-against-all approach, where each
SVM is trained to distinguish members of a
class (e.g., PERSON-BEGIN) from non-members
(PERSON-MIDDLE, MONEY-BEGIN, ... ). In this
approach, two or more classes may be assigned to
a word or no class may be assigned to a word. To
avoid these situations, we choose class c that has
the largest SVM output score gc(x) among all oth-
ers.
The other is that the NE label sequence must be
consistent; for example, ARTIFACT-END
must follow ARTIFACT-BEGIN or
Speech data
NE-labeled
transcriptions
Transcriptions ASR results
ASR-based
training data
Text-based
training data
           Manual
transcription ASR
NE labeling
Setting ASR
confidence
feature to 1
Alignment
&
identifying
ASR errors
and NEs
Figure 1: Procedure for preparing training data.
ARTIFACT-MIDDLE. We use a Viterbi search to
obtain the best and consistent NE label sequence
after classifying all words in a sentence, based
on probability-like values obtained by applying
sigmoid function sn(x) = 1/(1 + exp(??nx)) to
SVM output score gc(x).
3 Proposed method
3.1 Incorporating ASR confidence into NER
In the NER of ASR results, ASR errors cause NEs
to be missed and erroneous NEs to be recognized.
If one or more words constituting an NE are mis-
recognized, we cannot recognize the correct NE.
Even if all words constituting an NE are correctly
recognized, we may not recognize the correct NE
due to ASR errors on context words. To avoid
this problem, we model ASR errors using addi-
tional features that indicate whether each word is
correctly recognized. Our NER model is trained
using ASR results with a feature, where feature
values are obtained through alignment to the cor-
responding transcriptions. In testing, we estimate
feature values using ASR confidence scores. In
this paper, this feature is called the ASR confidence
feature.
Note that we only aim to identify NEs that are
correctly recognized by ASR, and NEs containing
ASR errors are not regarded as NEs. Utilizing er-
roneous NEs is a more difficult problem that is be-
yond the scope of this paper.
3.2 Training NER model
Figure 1 illustrates the procedure for preparing
training data from speech data. First, the speech
618
data are manually transcribed and automatically
recognized by the ASR. Second, we label NEs
in the transcriptions and then set the ASR con-
fidence feature values to 1 because the words in
the transcriptions are regarded as correctly recog-
nized words. Finally, we align the ASR results to
the transcriptions to identify ASR errors for the
ASR confidence feature values and to label cor-
rectly recognized NEs in the ASR results. Note
that we label the NEs in the ASR results that exist
in the same positions as the transcriptions. If a part
of an NE is misrecognized, the NE is ignored, and
all words for the NE are labeled as non-NE words
(OTHER). Examples of text-based and ASR-based
training data are shown in Tables 1 and 2. Since
the name Murayama Tomiichi in Table 1 is mis-
recognized in ASR, the correctly recognized word
Murayama is also labeled OTHER in Table 2. An-
other approach can be considered, where misrec-
ognized words are replaced by word error symbols
such as those shown in Table 3. In this case, those
words are rejected, and those part-of-speech and
character type features are not used in NER.
3.3 ASR confidence scoring for using the
proposed NER model
ASR confidence scoring is an important technique
in many ASR applications, and many methods
have been proposed including using word poste-
rior probabilities on word graphs (Wessel et al,
2001), integrating several confidence measures us-
ing neural networks (Schaaf and Kemp, 1997),
using linear discriminant analysis (Kamppari and
Hazen, 2000), and using SVMs (Zhang and Rud-
nicky, 2001).
Word posterior probability is a commonly used
and effective ASR confidence measure. Word pos-
terior probability p([w; ?, t]|X) of word w at time
interval [?, t] for speech signal X is calculated as
follows (Wessel et al, 2001):
p([w; ?, t]|X)
=
?
W?W [w;?,t]
{
p(X|W ) (p(W ))?
}?
p(X) , (1)
where W is a sentence hypothesis, W [w; ?, t] is
the set of sentence hypotheses that include w in
[?, t], p(X|W ) is a acoustic model score, p(W )
is a language model score, ? is a scaling param-
eter (?<1), and ? is a language model weight.
? is used for scaling the large dynamic range of
Word Confidence NE label
Murayama 1 PERSON-BEGIN
Tomiichi 1 PERSON-END
shusho 1 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 1: An example of text-based training data.
Word Confidence NE label
Murayama 1 OTHER
shi 0 OTHER
ni 0 OTHER
ichi 0 OTHER
shiyo 0 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 2: An example of ASR-based training data.
Word Confidence NE label
Murayama 1 OTHER
(error) 0 OTHER
(error) 0 OTHER
(error) 0 OTHER
(error) 0 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 3: An example of ASR-based training data
with word error symbols.
p(X|W )(p(W ))? to avoid a few of the top hy-
potheses dominating posterior probabilities. p(X)
is approximated by the sum over all sentence hy-
potheses and is denoted as
p(X) =
?
W
{
p(X|W ) (p(W ))?
}? . (2)
p([w; ?, t]|X) can be efficiently calculated using a
forward-backward algorithm.
In this paper, we use SVMs for ASR confidence
scoring to achieve a better performance than when
using word posterior probabilities as ASR confi-
dence scores. SVMs are trained using ASR re-
sults, whose errors are known through their align-
ment to their reference transcriptions. The follow-
ing features are used for confidence scoring: the
word itself, its part-of-speech tag, and its word
posterior probability; those of the two preceding
and succeeding words are also used. The word
itself and its part-of-speech are also represented
619
by a set of binary values, the same as with an
SVM-based NER. Since all other features are bi-
nary, we reduce real-valued word posterior prob-
ability p to ten binary features for simplicity: (if
0 < p ? 0.1, if 0.1 < p ? 0.2, ... , and if
0.9 < p ? 1.0). To normalize SVMs? output
scores for ASR confidence, we use a sigmoid func-
tion sw(x) = 1/(1 + exp(??wx)). We use these
normalized scores as ASR confidence scores. Al-
though a large variety of features have been pro-
posed in previous studies, we use only these sim-
ple features and reserve the other features for fur-
ther studies.
Using the ASR confidence scores, we estimate
whether each word is correctly recognized. If the
ASR confidence score of a word is greater than
threshold tw, the word is estimated as correct, and
we set the ASR confidence feature value to 1; oth-
erwise we set it to 0.
3.4 Rejection at the NER level
We use the ASR confidence feature to suppress
ASR error problems; however, even text-based
NERs sometimes make errors. NER performance
is a trade-off between missing correct NEs and
accepting erroneous NEs, and requirements dif-
fer by task. Although we can tune the parame-
ters in training SVMs to control the trade-off, it
seems very hard to find appropriate values for all
the SVMs. We use a simple NER-level rejection
by modifying the SVM output scores for the non-
NE class (OTHER). We add constant offset value to
to each SVM output score for OTHER. With a large
to, OTHER becomes more desirable than the other
NE classes, and many words are classified as non-
NE words and vice versa. Therefore, to works as a
parameter for NER-level rejection. This approach
can also be applied to text-based NER.
4 Experiments
We conducted the following experiments related
to the NER of speech data to investigate the per-
formance of the proposed method.
4.1 Setup
In the experiment, we simulated the procedure
shown in Figure 1 using speech data from the
NE-labeled text corpus. We used the training
data of the Information Retrieval and Extraction
Exercise (IREX) workshop (Sekine and Eriguchi,
2000) as the text corpus, which consisted of 1,174
Japanese newspaper articles (10,718 sentences)
and 18,200 NEs in eight categories (artifact, or-
ganization, location, person, date, time, money,
and percent). The sentences were read by 106
speakers (about 100 sentences per speaker), and
the recorded speech data were used for the exper-
iments. The experiments were conducted with 5-
fold cross validation, using 80% of the 1,174 ar-
ticles and the ASR results of the corresponding
speech data for training SVMs (both for ASR con-
fidence scoring and for NER) and the rest for the
test.
We tokenized the sentences into words and
tagged the part-of-speech information using the
Japanese morphological analyzer ChaSen 1 2.3.3
and then labeled the NEs. Unreadable to-
kens such as parentheses were removed in to-
kenization. After tokenization, the text cor-
pus had 264,388 words of 60 part-of-speech
types. Since three different kinds of charac-
ters are used in Japanese, the character types
used as features included: single-kanji
(words written in a single Chinese charac-
ter), all-kanji (longer words written in Chi-
nese characters), hiragana (words written
in hiragana Japanese phonograms), katakana
(words written in katakana Japanese phono-
grams), number, single-capital (words
with a single capitalized letter), all-capital,
capitalized (only the first letter is capital-
ized), roman (other roman character words), and
others (all other words). We used all the fea-
tures that appeared in each training set (no feature
selection was performed). The chunking states in-
cluded in the NE classes were: BEGIN (beginning
of a NE), MIDDLE (middle of a NE), END (ending
of a NE), and SINGLE (a single-word NE). There
were 33 NE classes (eight categories * four chunk-
ing states + OTHER), and therefore we trained 33
SVMs to distinguish words of a class from words
of other classes. For NER, we used an SVM-based
chunk annotator YamCha 2 0.33 with a quadratic
kernel (1 + ~x ? ~y)2 and a soft margin parameter
of SVMs C=0.1 for training and applied sigmoid
function sn(x) with ?n=1.0 and Viterbi search to
the SVMs? outputs. These parameters were exper-
imentally chosen using the test set.
We used an ASR engine (Hori et al, 2004) with
a speaker-independent acoustic model. The lan-
1http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)
2http://www.chasen.org/?taku/software/yamcha/
620
guage model was a word 3-gram model, trained
using other Japanese newspaper articles (about
340 M words) that were also tokenized using
ChaSen. The vocabulary size of the word 3-gram
model was 426,023. The test-set perplexity over
the text corpus was 76.928. The number of out-
of-vocabulary words was 1,551 (0.587%). 223
(1.23%) NEs in the text corpus contained such out-
of-vocabulary words, so those NEs could not be
correctly recognized by ASR. The scaling param-
eter ? was set to 0.01, which showed the best ASR
error estimation results using word posterior prob-
abilities in the test set in terms of receiver operator
characteristic (ROC) curves. The language model
weight ? was set to 15, which is a commonly used
value in our ASR system. The word accuracy ob-
tained using our ASR engine for the overall dataset
was 79.45%. In the ASR results, 82.00% of the
NEs in the text corpus remained. Figure 2 shows
the ROC curves of ASR error estimation for the
overall five cross-validation test sets, using SVM-
based ASR confidence scoring and word posterior
probabilities as ASR confidence scores, where
True positive rate
= # correctly recognized words estimated as correct
# correctly recognized words
False positive rate
= # misrecognized words estimated as correct
# misrecognized words .
In SVM-based ASR confidence scoring, we used
the quadratic kernel and C=0.01. Parameter ?w of
sigmoid function sw(x) was set to 1.0. These pa-
rameters were also experimentally chosen. SVM-
based ASR confidence scoring showed better per-
formance in ASR error estimation than simple
word posterior probabilities by integrating mul-
tiple features. Five values of ASR confidence
threshold tw were tested in the following experi-
ments: 0.2, 0.3, 0.4, 0.5, and 0.6 (shown by black
dots in Figure 2).
4.2 Evaluation metrics
Evaluation was based on an averaged NER F-
measure, which is the harmonic mean of NER pre-
cision and recall:
NER precision = # correctly recognized NEs
# recognized NEs
NER recall = # correctly recognized NEs
# NEs in original text
.
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Tr
ue
 p
os
itv
e 
ra
te
 (%
)
False positive rate (%)
=0.3
=0.4
SVM-based
confidence
scoring
Word posterior probability
tw
t
t
t
w
=0.2tw
=0.6w
=0.5w
Figure 2: SVM-based confidence scoring outper-
forms word posterior probability for ASR error es-
timation.
A recognized NE was accepted as correct if and
only if it appeared in the same position as its refer-
ence NE through alignment, in addition to having
the correct NE surface and category, because the
same NEs might appear more than once. Compar-
isons of NE surfaces did not include differences
in word segmentation because of the segmentation
ambiguity in Japanese. Note that NER recall with
ASR results could not exceed the rate of the re-
maining NEs after ASR (about 82%) because NEs
containing ASR errors were always lost.
In addition, we also evaluated the NER perfor-
mance in NER precision and recall with NER-
level rejection using the procedure in Section 3.4,
by modifying the non-NE class scores using offset
value to.
4.3 Compared methods
We compared several combinations of features
and training conditions for evaluating the effect of
incorporating the ASR confidence feature and in-
vestigating differences among training data: text-
based, ASR-based, and both.
Baseline does not use the ASR confidence fea-
ture and is trained using text-based training data
only.
NoConf-A does not use the ASR confidence
feature and is trained using ASR-based training
data only.
621
Method Confidence Training Test F-measure (%) Precision (%) Recall (%)
Baseline Text ASR 67.00 70.67 63.70
NoConf-A Not used ASR ASR 65.52 78.86 56.05
NoConf-TA Text+ASR ASR 66.95 77.55 58.91
Conf-A ASR ASR? 67.69 76.69 60.59
Proposed Used Text+ASR ASR? 69.02 78.13 61.81
Conf-Reject Used? Text+ASR ASR? 68.77 77.57 61.78
Conf-UB Used Text+ASR ASR?? 73.14 87.51 62.83
Transcription Not used Text Text 84.04 86.27 81.93
Table 4: NER results in averaged NER F-measure, precision, and recall without considering NER-level
rejection (to = 0). ASR word accuracy was 79.45%, and 82.00% of NEs remained in ASR results.
(?Unconfident words were rejected and replaced by word error symbols, ?tw = 0.4, ??ASR errors were
known.)
NoConf-TA does not use the ASR confidence
feature and is trained using both text-based and
ASR-based training data.
Conf-A uses the ASR confidence feature and is
trained using ASR-based training data only.
Proposed uses the ASR confidence feature and
is trained using both text-based and ASR-based
training data.
Conf-Reject is almost the same as Proposed,
but misrecognized words are rejected and replaced
with word error symbols, as described at the end
of Section 3.2.
The following two methods are for reference.
Conf-UB assumes perfect ASR confidence scor-
ing, so the ASR errors in the test set are known.
The NER model, which is identical to Proposed,
is regarded as the upper-boundary of Proposed.
Transcription applies the same model as Base-
line to reference transcriptions, assuming word ac-
curacy is 100%.
4.4 NER Results
In the NER experiments, Proposed achieved the
best results among the above methods. Table
4 shows the NER results obtained by the meth-
ods without considering NER-level rejection (i.e.,
to = 0), using threshold tw = 0.4 for Conf-A,
Proposed, and Conf-Reject, which resulted in the
best NER F-measures (see Table 5). Proposed
showed the best F-measure, 69.02%. It outper-
formed Baseline by 2.0%, with a 7.5% improve-
ment in precision, instead of a recall decrease of
1.9%. Conf-Reject showed slightly worse results
Method tw F (%) P (%) R (%)
0.2 66.72 71.28 62.71
0.3 67.32 73.68 61.98
Conf-A 0.4 67.69 76.69 60.59
0.5 67.04 79.64 57.89
0.6 64.48 81.90 53.14
0.2 68.08 72.54 64.14
0.3 68.70 75.11 63.31
Proposed 0.4 69.02 78.13 61.81
0.5 68.17 80.88 58.93
0.6 65.39 83.00 53.96
0.2 68.06 72.49 64.14
0.3 68.61 74.88 63.31
Conf-Reject 0.4 68.77 77.57 61.78
0.5 67.93 80.23 58.91
0.6 64.93 82.05 53.73
Table 5: NER results with varying ASR confi-
dence score threshold tw for Conf-A, Proposed,
and Conf-Reject. (F: F-measure, P: precision, R:
recall)
than Proposed. Conf-A resulted in 1.3% worse F-
measure than Proposed. NoConf-A and NoConf-
TA achieved 7-8% higher precision than Base-
line; however, their F-measure results were worse
than Baseline because of the large drop of recall.
The upper-bound results of the proposed method
(Conf-UB) in F-measure was 73.14%, which was
4% higher than Proposed.
Figure 3 shows NER precision and recall with
NER-level rejection by to for Baseline, NoConf-
TA, Proposed, Conf-UB, and Transcription. In the
figure, black dots represent results with to = 0,
as shown in Table 4. By all five methods, we
622
 0
 20
 40
 60
 80
 100
 50  60  70  80  90  100
Reca
ll (%)
Precision (%)
Baseline
NoConf-TA
ProposedConf-UB
Transcription
Figure 3: NER precision and recall with NER-
level rejection by to
obtained higher precision with to > 0. Pro-
posed achieved more than 5% higher precision
than Baseline on most recall ranges and showed
higher precision than NoConf-TA on recall ranges
higher than about 35%.
5 Discussion
The proposed method effectively improves NER
performance, as shown by the difference between
Proposed and Baseline in Tables 4 and 5. Improve-
ment comes from two factors: using both text-
based and ASR-based training data and incorpo-
rating ASR confidence feature. As shown by the
difference between Baseline and the methods us-
ing ASR-based training data (NoConf-A, NoConf-
TA, Conf-A, Proposed, Conf-Reject), ASR-based
training data increases precision and decreases
recall. In ASR-based training data, all words
constituting NEs that contain ASR errors are re-
garded as non-NE words, and those NE exam-
ples are lost in training, which emphasizes NER
precision. When text-based training data are also
available, they compensate for the loss of NE
examples and recover NER recall, as shown by
the difference between the methods without text-
based training data (NoConf-A, Conf-A) and those
with (NoConf-TA, Proposed). The ASR confi-
dence feature also increases NER recall, as shown
by the difference between the methods without
it (NoConf-A, NoConf-TA) and with it (Conf-A,
Proposed). This suggests that the ASR confidence
feature helps distinguish whether ASR error influ-
ences NER and suppresses excessive rejection of
NEs around ASR errors.
With respect to the ASR confidence feature, the
small difference between Conf-Reject and Pro-
posed suggests that ASR confidence is a more
dominant feature in misrecognized words than the
other features: the word itself, its part-of-speech
tag, and its character type. In addition, the dif-
ference between Conf-UB and Proposed indicated
that there is room to improve NER performance
with better ASR confidence scoring.
NER-level rejection also increased precision, as
shown in Figure 3. We can control the trade-
off between precision and recall with to accord-
ing to the task requirements, even in text-based
NER. In the NER of speech data, we can ob-
tain much higher precision using both ASR-based
training data and NER-level rejection than using
either one.
6 Related work
Recent studies on the NER of speech data consider
more than 1-best ASR results in the form of N-best
lists and word lattices. Using many ASR hypothe-
ses helps recover the ASR errors of NE words in
1-best ASR results and improves NER accuracy.
Our method can be extended to multiple ASR hy-
potheses.
Generative NER models were used for multi-
pass ASR and NER searches using word lattices
(Horlock and King, 2003b; Be?chet et al, 2004;
Favre et al, 2005). Horlock and King (2003a)
also proposed discriminative training of their NER
models. These studies showed the advantage of
using multiple ASR hypotheses, but they do not
use overlapping features.
Discriminative NER models were also applied
to multiple ASR hypotheses. Zhai et al (2004) ap-
plied text-based NER to N-best ASR results, and
merged the N-best NER results by weighted vot-
ing based on several sentence-level results such as
ASR and NER scores. Using the ASR confidence
feature does not depend on SVMs and can be used
with their method and other discriminative mod-
els.
7 Conclusion
We proposed a method for NER of speech data
that incorporates ASR confidence as a feature
of discriminative NER, where the NER model
623
is trained using both text-based and ASR-based
training data. In experiments using SVMs,
the proposed method showed a higher NER F-
measure, especially in terms of improving pre-
cision, than simply applying text-based NER to
ASR results. The method effectively rejected erro-
neous NEs due to ASR errors with a small drop of
recall, thanks to both the ASR confidence feature
and ASR-based training data. NER-level rejection
also effectively increased precision.
Our approach can also be used in other tasks
in spoken language processing, and we expect it
to be effective. Since confidence itself is not lim-
ited to speech, our approach can also be applied to
other noisy inputs, such as optical character recog-
nition (OCR). For further improvement, we will
consider N-best ASR results or word lattices as in-
puts and introduce more speech-specific features
such as word durations and prosodic features.
Acknowledgments We would like to thank
anonymous reviewers for their helpful comments.
References
Fre?de?ric Be?chet, Allen L. Gorin, Jeremy H. Wright,
and Dilek Hakkani-Tu?r. 2004. Detecting and ex-
tracting named entities from spontaneous speech in a
mixed-initiative spoken dialogue context: How May
I Help You? Speech Communication, 42(2):207?
225.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proc. CoNLL, pages 160?163.
Beno??t Favre, Fre?de?ric Be?chet, and Pascal Noce?ra.
2005. Robust named entity extraction from large
spoken archives. In Proc. HLT-EMNLP, pages 491?
498.
Takaaki Hori, Chiori Hori, and Yasuhiro Minami.
2004. Fast on-the-fly composition for weighted
finite-state transducers in 1.8 million-word vocab-
ulary continuous-speech recognition. In Proc. IC-
SLP, volume 1, pages 289?292.
James Horlock and Simon King. 2003a. Discrimi-
native methods for improving named entity extrac-
tion on speech data. In Proc. EUROSPEECH, pages
2765?2768.
James Horlock and Simon King. 2003b. Named en-
tity extraction from word lattices. In Proc. EU-
ROSPEECH, pages 1265?1268.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING, pages 390?396.
Simo O. Kamppari and Timothy J. Hazen. 2000. Word
and phone level acoustic confidence scoring. In
Proc. ICASSP, volume 3, pages 1799?1802.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proc. CoNLL, pages 188?191.
David Miller, Richard Schwartz, Ralph Weischedel,
and Rebecca Stone. 1999. Named entity extraction
from broadcast news. In Proceedings of the DARPA
Broadcast News Workshop, pages 37?40.
David D. Palmer and Mari Ostendorf. 2001. Im-
proving information extraction by modeling errors
in speech recognizer output. In Proc. HLT, pages
156?160.
Thomas Schaaf and Thomas Kemp. 1997. Confidence
measures for spontaneous speech recognition. In
Proc. ICASSP, volume II, pages 875?878.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation - analysis of re-
sults. In Proc. COLING, pages 25?30.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A decision tree method for finding and
classifying names in Japanese texts. In Proc. the
Sixth Workshop on Very Large Corpora, pages 171?
178.
Frank Wessel, Ralf Schlu?ter, Klaus Macherey, and
Hermann Ney. 2001. Confidence measures for
large vocabulary continuous speech recognition.
IEEE Transactions on Speech and Audio Process-
ing, 9(3):288?298.
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine
Carpuat, and Dekai Wu. 2004. Using N-best lists
for named entity recognition from chinese speech.
In Proc. HLT-NAACL, pages 37?40.
Rong Zhang and Alexander I. Rudnicky. 2001. Word
level confidence annotation using combinations of
features. In Proc. EUROSPEECH, pages 2105?
2108.
624
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 777?784,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Left-to-Right Target Generation for Hierarchical Phrase-based
Translation
Taro Watanabe Hajime Tsukada Hideki Isozaki
2-4, Hikaridai, Seika-cho, Soraku-gun,
Kyoto, JAPAN 619-0237
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
We present a hierarchical phrase-based
statistical machine translation in which a
target sentence is efficiently generated in
left-to-right order. The model is a class
of synchronous-CFG with a Greibach Nor-
mal Form-like structure for the projected
production rule: The paired target-side
of a production rule takes a phrase pre-
fixed form. The decoder for the target-
normalized form is based on an Early-
style top down parser on the source side.
The target-normalized form coupled with
our top down parser implies a left-to-
right generation of translations which en-
ables us a straightforward integration with
ngram language models. Our model was
experimented on a Japanese-to-English
newswire translation task, and showed sta-
tistically significant performance improve-
ments against a phrase-based translation
system.
1 Introduction
In a classical statistical machine translation, a for-
eign language sentence f J1 = f1, f2, ... fJ is trans-
lated into another language, i.e. English, eI1 =
e1, e2, ..., eI by seeking a maximum likely solution
of:
e?I1 = argmax
eI1
Pr(eI1| f J1 ) (1)
= argmax
eI1
Pr( f J1 |eI1)Pr(eI1) (2)
The source channel approach in Equation 2 inde-
pendently decomposes translation knowledge into
a translation model and a language model, respec-
tively (Brown et al, 1993). The former repre-
sents the correspondence between two languages
and the latter contributes to the fluency of English.
In the state of the art statistical machine transla-
tion, the posterior probability Pr(eI1| f J1 ) is directly
maximized using a log-linear combination of fea-
ture functions (Och and Ney, 2002):
e?I1 = argmax
eI1
exp
(
?M
m=1 ?mhm(eI1, f J1 )
)
?
e? I
?
1
exp
(
?M
m=1 ?mhm(e? I
?
1 , f J1 )
) (3)
where hm(eI1, f J1 ) is a feature function, such as
a ngram language model or a translation model.
When decoding, the denominator is dropped since
it depends only on f J1 . Feature function scaling
factors ?m are optimized based on a maximum
likely approach (Och and Ney, 2002) or on a direct
error minimization approach (Och, 2003). This
modeling allows the integration of various fea-
ture functions depending on the scenario of how
a translation is constituted.
A phrase-based translation model is one of the
modern approaches which exploits a phrase, a
contiguous sequence of words, as a unit of transla-
tion (Koehn et al, 2003; Zens and Ney, 2003; Till-
man, 2004). The idea is based on a word-based
source channel modeling of Brown et al (1993):
It assumes that eI1 is segmented into a sequence
of K phrases e?K1 . Each phrase e?k is transformed
into ?fk. The translated phrases are reordered to
form f J1 . One of the benefits of the modeling is
that the phrase translation unit preserves localized
word reordering. However, it cannot hypothesize
a long-distance reordering required for linguisti-
cally divergent language pairs. For instance, when
translating Japanese to English, a Japanese SOV
structure has to be reordered to match with an En-
777
glish SVO structure. Such a sentence-wise move-
ment cannot be realized within the phrase-based
modeling.
Chiang (2005) introduced a hierarchical phrase-
based translation model that combined the
strength of the phrase-based approach and a
synchronous-CFG formalism (Aho and Ullman,
1969): A rewrite system initiated from a start
symbol which synchronously rewrites paired non-
terminals. Their translation model is a binarized
synchronous-CFG, or a rank-2 of synchronous-
CFG, in which the right-hand side of a production
rule contains at most two non-terminals. The form
can be regarded as a phrase translation pair with
at most two holes instantiated with other phrases.
The hierarchically combined phrases provide a
sort of reordering constraints that is not directly
modeled by a phrase-based model.
Rules are induced from a bilingual corpus with-
out linguistic clues first by extracting phrase trans-
lation pairs, and then by generalizing extracted
phrases with holes (Chiang, 2005). Even in a
phrase-based model, the number of phrases ex-
tracted from a bilingual corpus is quadratic to
the length of bilingual sentences. The grammar
size for the hierarchical phrase-based model will
be further exploded, since there exists numerous
combination of inserting holes to each rule. The
spuriously increasing grammar size will be prob-
lematic for decoding without certain heuristics,
such as a length based thresholding.
The integration with a ngram language model
further increases the cost of decoding especially
when incorporating a higher order ngram, such as
5-gram. In the hierarchical phrase-based model
(Chiang, 2005), and an inversion transduction
grammar (ITG) (Wu, 1997), the problem is re-
solved by restricting to a binarized form where at
most two non-terminals are allowed in the right-
hand side. However, Huang et al (2005) reported
that the computational complexity for decoding
amounted to O(J3+3(n?1)) with n-gram even using
a hook technique. The complexity lies in mem-
orizing the ngram?s context for each constituent.
The order of ngram would be a dominant factor
for higher order ngrams.
As an alternative to a binarized form, we
present a target-normalized hierarchical phrase-
based translation model. The model is a class of a
hierarchical phrase-based model, but constrained
so that the English part of the right-hand side
is restricted to a Greibach Normal Form (GNF)-
like structure: A contiguous sequence of termi-
nals, or a phrase, is followed by a string of non-
terminals. The target-normalized form reduces the
number of rules extracted from a bilingual corpus,
but still preserves the strength of the phrase-based
approach. An integration with ngram language
model is straightforward, since the model gener-
ates a translation in left-to-right order. Our de-
coder is based on an Earley-style top down pars-
ing on the foreign language side. The projected
English-side is generated in left-to-right order syn-
chronized with the derivation of the foreign lan-
guage side. The decoder?s implementation is taken
after a decoder for an existing phrase-based model
with a simple modification to account for produc-
tion rules. Experimental results on a Japanese-to-
English newswire translation task showed signif-
icant improvement against a phrase-based model-
ing.
2 Translation Model
A weighted synchronous-CFG is a rewrite system
consisting of production rules whose right-hand
side is paired (Aho and Ullman, 1969):
X ? ??, ?,?? (4)
where X is a non-terminal, ? and ? are strings of
terminals and non-terminals. For notational sim-
plicity, we assume that ? and ? correspond to the
foreign language side and the English side, re-
spectively. ? is a one-to-one correspondence for
the non-terminals appeared in ? and ?. Starting
from an initial non-terminal, each rule rewrites
non-terminals in ? and ? that are associated with
?.
Chiang (2005) proposed a hierarchical phrase-
based translation model, a binary synchronous-
CFG, which restricted the form of production rules
as follows:
? Only two types of non-terminals allowed: S
and X.
? Both of the strings ? and ? must contain at
least one terminal item.
? Rules may have at most two non-terminals
but non-terminals cannot be adjacent for the
foreign language side ?.
The production rules are induced from a bilingual
corpus with the help of word alignments. To al-
leviate a data sparseness problem, glue rules are
778
added that prefer combining hierarchical phrases
in a serial manner:
S ?
?
S 1 X2 , S 1 X2
?
(5)
S ?
?
X 1 , X1
?
(6)
where boxed indices indicate non-terminal?s link-
ages represented in ?.
Our model is based on Chiang (2005)?s frame-
work, but further restricts the form of production
rules so that the aligned right-hand side ? follows
a GNF-like structure:
X ?
?
?, ?b?,?
?
(7)
where ?b is a string of terminals, or a phrase,
and beta is a (possibly empty) string of non-
terminals. The foreign language at right-hand side
? still takes an arbitrary string of terminals and
non-terminals. The use of a phrase ?b as a pre-
fix keeps the strength of the phrase-base frame-
work. A contiguous English side coupled with
a (possibly) discontiguous foreign language side
preserves a phrase-bounded local word reordering.
At the same time, the target-normalized frame-
work still combines phrases hierarchically in a re-
stricted manner.
The target-normalized form can be regarded as
a type of rule in which certain non-terminals are
always instantiated with phrase translation pairs.
Thus, we will be able to reduce the number of rules
induced from a bilingual corpus, which, in turn,
help reducing the decoding complexity.
The contiguous phrase-prefixed form generates
English in left-to-right order. Therefore, a decoder
can easily hypothesize a derivation tree integrated
with a ngram language model even with higher or-
der.
Note that we do not imply arbitrary
synchronous-CFGs are transformed into the
target normalized form. The form simply restricts
the grammar extracted from a bilingual corpus
explained in the next section.
2.1 Rule Extraction
We present an algorithm to extract production
rules from a bilingual corpus. The procedure is
based on those for the hierarchical phrase-based
translation model (Chiang, 2005).
First, a bilingual corpus is annotated with word
alignments using the method of Koehn et al
(2003). Many-to-many word alignments are in-
duced by running a one-to-many word alignment
model, such as GIZA++ (Och and Ney, 2003), in
both directions and by combining the results based
on a heuristic (Koehn et al, 2003).
Second, phrase translation pairs are extracted
from the word alignment corpus (Koehn et al,
2003). The method exhaustively extracts phrase
pairs ( f j+mj , ei+ni ) from a sentence pair ( f J1 , eI1) that
do not violate the word alignment constraints a:
?(i?, j?) ? a : j? ? [ j, j + m], i? ? [i, i + n]
?(i?, j?) ? a : j? ? [ j, j + m], i? < [i, i + n]
?(i?, j?) ? a : j? < [ j, j + m], i? ? [i, i + n]
Third, based on the extracted phrases, production
rules are accumulated by computing the ?holes?
for contiguous phrases (Chiang, 2005):
1. A phrase pair ( ?f , e?) constitutes a rule
X ?
?
?f , e?
?
2. A rule X ? ??, ?? and a phrase pair ( ?f , e?) s.t.
? = ?? ?f??? and ? = e??e?? constitutes a rule
X ?
?
?? X k ?
??, e?? X k ?
?
Following Chiang (2005), we applied constraints
when inducing rules with non-terminals:
? At least one foreign word must be aligned to
an English word.
? Adjacent non-terminals are not allowed for
the foreign language side.
2.2 Phrase-based Rules
The rule extraction procedure described in Section
2.1 is a corpus-based, therefore will be easily suf-
fered from a data sparseness problem. The hier-
archical phrase-based model avoided this problem
by introducing the glue rules 5 and 6 that com-
bined hierarchical phrases sequentially (Chiang,
2005).
We use a different method of generalizing pro-
duction rules. When production rules without non-
terminals are extracted in step 1 of Section 2.1,
X ?
?
?f , e?
?
(8)
then, we also add production rules as follows:
X ?
?
?f X 1 , e? X 1
?
(9)
X ?
?
X 1 ?f , e? X 1
?
(10)
X ?
?
X 1 ?f X 2 , e? X 1 X 2
?
(11)
X ?
?
X 2 ?f X 1 , e? X 1 X 2
?
(12)
779
The international terrorism also is a possible threat in Japan
Reference translation: ?International terrorism is a threat
even to Japan?
(a) Translation by a phrase-based model. (b) A derivation tree representation for Figure 1(a).Indices in
non-terminal X represent the order to perform rewriting.
Figure 1: An example of Japanese-to-English translation by a phrase-based model.
We call them phrase-based rules, since four types
of rules are generalized directly from phrase trans-
lation pairs.
The class of rules roughly corresponds to the re-
ordering constraints used in a phrase-based model
during decoding. Rules 8 and 9 are sufficient to re-
alize a monotone decoding in which phrase trans-
lation pairs are simply combined sequentially.
With rules 10 and 11, the non-terminal X 1 behaves
as a place holder where certain number of foreign
words are skipped. Therefore, those rules real-
ize a window size constraint used in many phrase-
based models (Koehn et al, 2003). The rule 12
further gives an extra freedom for the phrase pair
reordering. The rules 8 through 12 can be in-
terpreted as ITG-constraints where phrase trans-
lation pairs are hierarchically combined either in
a monotonic way or in an inverted manner (Zens
and Ney, 2003; Wu, 1997). Thus, by controlling
what types of phrase-based rules employed in a
grammar, we will be able to simulate a phrase-
based translation model with various constraints.
This reduction is rather natural in that a finite state
transducer, or a phrase-based model, is a subclass
of a synchronous-CFG.
Figure 1(a) shows an example Japanese-to-
English translation by a phrase-based model de-
scribed in Section 5. Using the phrase-based rules,
the translation results is represented as a derivation
tree in Figure 1(b).
3 Decoding
Our decoder is an Earley-style top down parser on
the foreign language side with a beam search strat-
egy. Given an input sentence f J1 , the decoder seeks
for the best English according to Equation 3 us-
ing the feature functions described in Section 4.
The English output sentence is generated in left-
to-right order in accordance with the derivation of
the foreign language side synchronized with the
cardinality of already translated foreign word po-
sitions.
The decoding process is very similar to those
described in (Koehn et al, 2003): It starts from an
initial empty hypothesis. From an existing hypoth-
esis, new hypothesis is generated by consuming
a production rule that covers untranslated foreign
word positions. The score for the newly generated
hypothesis is updated by combining the scores of
feature functions described in Section 4. The En-
glish side of the rule is simply concatenated to
form a new prefix of English sentence. Hypothe-
ses that consumed m foreign words are stored in a
priority queue Qm.
Hypotheses in Qm undergo two types of prun-
ing: A histogram pruning preserves at most M hy-
potheses inQm. A threshold pruning discards a hy-
potheses whose score is below the maximum score
of Qm multiplied with a threshold value ?. Rules
are constrained by their foreign word span of a
non-terminal. For a rule consisting of more than
two non-terminals, we constrained so that at least
one non-terminal should span at most ? words.
The decoder is characterized as a weighted
synchronous-CFG implemented with a push-down
automaton rather a weighted finite state transducer
(Aho and Ullman, 1969). Each hypothesis main-
tains following knowledge:
? A prefix of English sentence. For space ef-
ficiency, the prefix is represented as a word
graph.
? Partial contexts for each feature function.
For instance, to compute a 5-gram language
model feature, we keep the consecutive last
four words of an English prefix.
780
? A stack that keeps track of the uncovered for-
eign word spans. The stack for an initial hy-
pothesis is initialized with span [1, J].
When extending a hypothesis, the associated stack
structure is popped. The popped foreign word
span [ jl, jr] is used to locate the rules for uncov-
ered foreign word positions. We assume that the
decoder accumulates all the applicable rules from
a large database and stores the extracted rules in a
chart structure. The decoder identifies what rules
to consume when extending a hypothesis using the
chart structure. A new hypothesis is created with
an updated stack by pushing foreign non-terminal
spans: For each rule spanning [ jl, jr] at foreign-
side with non-terminal spans of [kl1, kr1], [kl2, kr2], ...,
the non-terminal spans are pushed in the reverse
order of the projected English side. For example,
A rule with foreign word non-terminal spans:
X ?
?
X 2 : [kl2, kr2] ?f X 1 : [kl1, kr1], e? X 1 X 2
?
will update a stack by pushing the foreign word
spans [kl2, kr2] and [kl1, kr1] in order. This ordering
assures that, when popped, the English-side will
be generated in left-to-right order. A hypothesis
with an empty stack implies that the hypothesis
has covered all the foreign words.
Figure 2 illustrates the decoding process for the
derivation tree in Figure 1(b). Starting from the
initial hypothesis of [1, 11], the stack is updated in
accordance with non-terminal?s spans. The span
is popped and the rule with the foreign word pan
[1, 11] is looked up from the chart structure. The
stack structure for the newly created hypothesis is
updated by pushing non-terminal spans [4, 11] and
[1, 2].
Our decoder is based on an in-house devel-
oped phrase-based decoder which uses a bit vec-
tor to represent uncovered foreign word positions
for each hypothesis. We basically replaced the
bit vector structure to the stack structure: Al-
most no modification was required for the word
graph structure and the beam search strategy im-
plemented for a phrase-based modeling. The use
of a stack structure directly models a synchronous-
CFG formalism realized as a push-down automa-
tion, while the bit vector implementation is con-
ceptualized as a finite state transducer. The cost
of decoding with the proposed model is cubic to
foreign language sentence length.
Rules Stack
[1, 11]
X : [1, 11]?
?
X 1 : [1, 2] X 2 : [4, 11], The X 1 X 2
? [1, 2]
[4, 11]
X : [1, 2]?
?
X 1 : [2, 2], international X 1
? [2, 2]
[4, 11]
X : [2, 2]? ? , terrorism? [4, 11]
X : [4, 11]?
?
X 2 : [4, 5] X 1 : [7, 11], also X 1 X 2
? [7, 11]
[4, 5]
X : [7, 11]?
?
X 1 : [7, 9] , is a X 1
? [7, 9]
[4, 5]
X : [7, 9]?
?
X 1 : [9, 9], possible X 1
? [9, 9]
[4, 5]
X : [9, 9]? ? , threat? [4, 5]
X : [4, 5]?
?
X 1 : [4, 4] , in X 1
?
[4, 4]
X : [4, 4]? ? , Japan?
Figure 2: An example decoding process of Fig-
ure 1(b) with a stack to keep track of foreign word
spans.
4 Feature Functions
The decoder for our translation model uses a log-
linear combination of feature functions, or sub-
models, to seek for the maximum likely translation
according to Equation 3. This section describes
the models experimented in Section 5, mainly
consisting of count-based models, lexicon-based
models, a language model, reordering models and
length-based models.
4.1 Count-based Models
Main feature functions h?( f J1 |eI1,D) and
h?(eI1| f J1 ,D) estimate the likelihood of two
sentences f J1 and eI1 over a derivation tree D.
We assume that the production rules in D are
independent of each other:
h?( f J1 |eI1,D) = log
?
??,???D
?(?|?) (13)
?(?|?) is estimated through the relative frequency
on a given bilingual corpus.
?(?|?) = count(?, ?)?
? count(?, ?)
(14)
where count(?) represents the cooccurrence fre-
quency of rules ? and ?.
The relative count-based probabilities for the
phrase-based rules are simply adopted from the
original probabilities of phrase translation pairs.
4.2 Lexicon-based Models
We define lexically weighted feature functions
hw( f J1 |eI1,D) and hw(eI1| f J1 ,D) applying the inde-
pendence assumption of production rules as in
781
Equation 13.
hw( f J1 |eI1,D) = log
?
??,???D
pw(?|?) (15)
The lexical weight pw(?|?) is computed from word
alignments a inside ? and ? (Koehn et al, 2003):
pw(?|?, a) =
|?|
?
i=1
1
|{ j|(i, j) ? a}|
?
?(i, j)?a
t(? j|?i)
(16)
where t(?) is a lexicon model trained from the word
alignment annotated bilingual corpus discussed in
Section 2.1. The alignment a also includes non-
terminal correspondence with t(X k |X k ) = 1. If we
observed multiple alignment instances for ? and ?,
then, we take the maximum of the weights.
pw(?|?) = max
a
pw(?|?, a) (17)
4.3 Language Model
We used mixed-cased n-gram language model. In
case of 5-gram language model, the feature func-
tion is expressed as follows:
hlm(eI1) = log
?
i
pn(ei|ei?4ei?3ei?2ei?1) (18)
4.4 Reordering Models
In order to limit the reorderings, two feature func-
tions are employed based on the backtracking of
rules during the top-down parsing on foreign lan-
guage side.
hh(eI1, f J1 ,D) =
?
Di?back(D)
height(Di) (19)
hw(eI1, f J1 ,D) =
?
Di?back(D)
width(Di) (20)
where back(D) is a set of subtrees backtracked
during the derivation of D, and height(Di) and
width(Di) refer the height and width of subtreeDi,
respectively. In Figure 1(b), for instance, a rule of
X 1 with non-terminals X 2 and X 4 , two rules X 2
and X 3 spanning two terminal symbols should be
backtracked to proceed to X 4 . The rationale is that
positive scaling factors prefer a deeper structure
whereby negative scaling factors prefer a mono-
tonized structure.
4.5 Length-based Models
Three trivial length-based feature functions were
used in our experiment.
hl(eI1) = I (21)
hr(D) = rule(D) (22)
hp(D) = phrase(D) (23)
Table 1: Japanese/English news corpus
Japanese English
train sentence 175,384
dictionary + 1,329,519
words 8,373,478 7,222,726
vocabulary 297,646 397,592
dev. sentence 1,500
words 47,081 39,117
OOV 45 149
test sentence 1,500
words 47,033 38,707
OOV 51 127
Table 2: Phrases/rules extracted from the
Japanese/English bilingual corpus. Figures do not
include phrase-based rules.
# rules/phrases
Phrase 5,433,091
Normalized-2 6,225,630
Normalized-3 6,233,294
Hierarchical 12,824,387
where rule(D) and phrase(D) are the number
of production rules extracted in Section 2.1 and
phrase-based rules generalized in Section 2.2, re-
spectively. The English length feature function
controls the length of output sentence. Two feature
functions based on rule?s counts are hypothesized
to control whether to incorporate a production rule
or a phrase-based rule into D.
5 Experiments
The bilingual corpus used for our experiments was
obtained from an automatically sentence aligned
Japanese/English Yomiuri newspaper corpus con-
sisting of 180K sentence pairs (refer to Table
1) (Utiyama and Isahara, 2003). From one-to-
one aligned sentences, 1,500 sentence pairs were
sampled for a development set and a test set1.
Since the bilingual corpus is rather small, es-
pecially for the newspaper translation domain,
Japanese/English dictionaries consisting of 1.3M
entries were added into a training set to alleviate
an OOV problem2.
Word alignments were annotated by a HMM
translation model (Och and Ney, 2003). After
1Japanese sentences were segmented by MeCab available
from http://mecab.sourceforge.jp.
2The dictionary entries were compiled from JE-
DICT/JNAMEDICT and an in-house developed dictionary.
782
the annotation via Viterbi alignments with refine-
ments, phrases translation pairs and production
rules were extracted (refer to Table 2). We per-
formed the rule extraction using the hierarchi-
cal phrase-based constraint (Hierarchical) and our
proposed target-normalized form with 2 and 3
non-terminals (Normalized-2 and Normalized-3).
Phrase translation pairs were also extracted for
comparison (Phrase). We did not threshold the
extracted phrases or rules by their length. Ta-
ble 2 shows that Normalized-2 extracted slightly
larger number of rules than those for phrase-
based model. Including three non-terminals did
not increase the grammar size. The hierarchical
phrase-based translation model extracts twice as
large as our target-normalized formalism. The
target-normalized form is restrictive in that non-
terminals should be consecutive for the English-
side. This property prohibits spuriously extracted
production rules.
Mixed-casing 3-gram/5-gram language models
were estimated from LDC English GigaWord 2 to-
gether with the 100K English articles of Yomiuri
newspaper that were used neither for development
nor test sets 3.
We run the decoder for the target-normalized
hierarchical phrase-based model consisting of at
most two non-terminals, since adding rules with
three non-terminals did not increase the grammar
size. ITG-constraint simulated phrase-based rules
were also included into our grammar. The foreign
word span size was thresholded so that at least one
non-terminal should span at most 7 words.
Our phrase-based model employed all feature
functions for the hierarchical phrase-based system
with additional feature functions:
? A distortion model that penalizes the re-
ordering of phrases by the number of words
skipped | j ? ( j? + m?) ? 1|, where j is the for-
eign word position for a phrase f j+mj trans-
lated immediately after a phrase for f j?+m?j?
(Koehn et al, 2003).
? Lexicalized reordering models constrain the
reordering of phrases whether to favor mono-
tone, swap or discontinuous positions (Till-
man, 2004).
The phrase-based decoder?s reordering was con-
strained by ITG-constraints with a window size of
3We used SRI ngram language modeling toolkit with lim-
ited vocabulary size.
Table 3: Results for the Japanese-to-English
newswire translation task.
BLEU NIST
[%]
Phrase 3-gram 7.14 3.21
5-gram 7.33 3.19
Normalized-2 3-gram 10.00 4.11
5-gram 10.26 4.20
7.
The translation results are summarized in Table
3. Two systems were contrasted by 3-gram and 5-
gram language models. Results were evaluated by
ngram precision based metrics, BLEU and NIST,
on the casing preserved single reference test set.
Feature function scaling factors for each system
were optimized on BLEU score under the devel-
opment set using a downhill simplex method. The
differences of translation qualities are statistically
significant at the 95% confidence level (Koehn,
2004). Although the figures presented in Table
3 are rather low, we found that Normalized-2 re-
sulted in statistically significant improvement over
Phrase. Figure 3 shows some translation results
from the test set.
6 Conclusion
The target-normalized hierarchical phrase-based
model is based on a more general hierarchical
phrase-based model (Chiang, 2005). The hier-
archically combined phrases can be regarded as
an instance of phrase-based model with a place
holder to constraint reordering. Such reorder-
ing was realized either by an additional constraint
for decoding, such as window constraints, IBM
constraints or ITG-constraints (Zens and Ney,
2003), or by lexicalized reordering feature func-
tions (Tillman, 2004). In the hierarchical phrase-
based model, such reordering is explicitly repre-
sented in each rule.
As experimented in Section 5, the use of the
target-normalized form reduced the grammar size,
but still outperformed a phrase-based system.
Furthermore, the target-normalized form coupled
with our top down parsing on the foreign lan-
guage side allows an easier integration with ngram
language model. A decoder can be implemented
based on a phrase-based model by employing a
stack structure to keep track of untranslated for-
eign word spans.
The target-normalized form can be interpreted
783
Reference: Japan needs to learn a lesson from history to ensure that it not repeat its mistakes .
Phrase: At the same time , it never mistakes that it is necessary to learn lessons from the history of criminal .
Normalized-2: It is necessary to learn lessons from history so as not to repeat similar mistakes in the future .
Reference: The ministries will dispatch design and construction experts to China to train local engineers and to
research technology that is appropriate to China?s economic situation .
Phrase: Japan sent specialists to train local technicians to the project , in addition to the situation in China and
its design methods by exception of study .
Normalized-2: Japan will send experts to study the situation in China , and train Chinese engineers , construction
design and construction methods of the recipient from .
Reference: The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending relief
measures to the village and the city of Niigata .
Phrase: The Health and Welfare Ministry in that the Japanese people in the village are made law .
Normalized-2: The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in Niigata .
Figure 3: Sample translations from two systems: Phrase and Normalized-2
as a set of rules that reorders the foreign lan-
guage to match with English language sequen-
tially. Collins et al (2005) presented a method
with hand-coded rules. Our method directly learns
such serialization rules from a bilingual corpus
without linguistic clues.
The translation quality presented in Section 5
are rather low due to the limited size of the bilin-
gual corpus, and also because of the linguistic dif-
ference of two languages. As our future work,
we are in the process of experimenting our model
for other languages with rich resources, such as
Chinese and Arabic, as well as similar language
pairs, such as French and English. Additional
feature functions will be also investigated that
were proved successful for phrase-based models
together with feature functions useful for a tree-
based modeling.
Acknowledgement
We would like to thank to our colleagues, espe-
cially to Hideto Kazawa and Jun Suzuki, for useful
discussions on the hierarchical phrase-based trans-
lation.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37?56.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL 2005, pages 263?270, Ann Arbor, Michigan,
June.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL 2005, pages 531?540,
Ann Arbor, Michigan, June.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with
hooks. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 65?73,
Vancouver, British Columbia, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388?395, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL 2002,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL
2003, pages 160?167.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts, USA, May 2 - May 7.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proc. of ACL 2003, pages
72?79.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proc. of ACL 2003, pages 144?151.
784
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 341?344,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Succinct N-gram Language Model
Taro Watanabe Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
Efficient processing of tera-scale text data
is an important research topic. This pa-
per proposes lossless compression of N -
gram language models based on LOUDS,
a succinct data structure. LOUDS suc-
cinctly represents a trie with M nodes as a
2M + 1 bit string. We compress it further
for the N -gram language model structure.
We also use ?variable length coding? and
?block-wise compression? to compress val-
ues associated with nodes. Experimental
results for three large-scale N -gram com-
pression tasks achieved a significant com-
pression rate without any loss.
1 Introduction
There has been an increase in available N -gram
data and a large amount of web-scaled N -gram
data has been successfully deployed in statistical
machine translation. However, we need either a
machine with hundreds of gigabytes of memory
or a large computer cluster to handle them.
Either pruning (Stolcke, 1998; Church et al,
2007) or lossy randomizing approaches (Talbot
and Brants, 2008) may result in a compact repre-
sentation for the application run-time. However,
the lossy approaches may reduce accuracy, and
tuning is necessary. A lossless approach is obvi-
ously better than a lossy one if other conditions
are the same. In addtion, a lossless approach can
easly combined with pruning. Therefore, lossless
representation of N -gram is a key issue even for
lossy approaches.
Raj and Whittaker (2003) showed a general N -
gram language model structure and introduced a
lossless algorithm that compressed a sorted integer
vector by recursively shifting a certain number of
bits and by emitting index-value inverted vectors.
However, we need more compact representation.
In this work, we propose a succinct way to
represent the N -gram language model structure
based on LOUDS (Jacobson, 1989; Delpratt et
al., 2006). It was first introduced by Jacobson
(1989) and requires only a small space close to
the information-theoretic lower bound. For an M
node ordinal trie, its information-theoretical lower
bound is 2M ? O(lg M) bits (lg(x) = log
2
(x))
1-gram 2-gram 3-gram
probability
back-off
pointer
word idprobabilityback-offpointer
word id
probability
back-off
pointer
Figure 1: Data structure for language model
and LOUDS succinctly represents it by a 2M + 1
bit string. The space is further reduced by consid-
ering the N -gram structure. We also use variable
length coding and block-wise compression to com-
press the values associated with each node, such as
word ids, probabilities or counts.
We experimented with English Web 1T 5-gram
from LDC consisting of 25 GB of gzipped raw
text N -gram counts. By using 8-bit floating point
quantization 1, N -gram language models are com-
pressed into 10 GB, which is comparable to a lossy
representation (Talbot and Brants, 2008).
2 N -gram Language Model
We assume a back-off N -gram language model in
which the conditional probability Pr(w
n
|w
n?1
1
)
for an arbitrary N -gram wn
1
= (w
1
, ..., w
n
) is re-
cursively computed as follows.
?(w
n
1
) if wn
1
exists.
?(w
n?1
1
)Pr(w
n
|w
n?1
2
) if wn?1
1
exists.
Pr(w
n
|w
n?1
2
) otherwise.
?(w
n
1
) and ?(wn
1
) are smoothed probabilities and
back-off coefficients, respectively.
The N -grams are stored in a trie structure as
shown in Figure 1. N -grams of different orders
are stored in different tables and each row corre-
sponds to a particular wn
1
, consisting of a word id
for w
n
, ?(w
n
1
), ?(w
n
1
) and a pointer to the first po-
sition of the succeeding (n + 1)-grams that share
the same prefix wn
1
. The succeeding (n+1)-grams
are stored in a contiguous region and sorted by the
word id of w
n+1
. The boundary of the region is de-
termined by the pointer of the next N -gram in the
1The compact representation of the floating point is out of
the scope of this paper. Therefore, we use the term lossless
even when using floating point quantization.
341
0
1 2 3 4
5 6 7 8 9 10
11 12 13 14 15
(a) Trie structure
node id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
bit position 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
LOUDS bit 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0
(b) Corresponding LOUDS bit string
0 1 2 3
4 5 6 7 8 9
10 11 12 13 14
(c) Trie structure for N -gram
node id 0 1 2 3 4 5 6 7 8 9
bit position 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
LOUDS bit 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0
(d) Corresponding N -gram optimized LOUDS bit string
Figure 2: Optimization of LOUDS bit string for N -gram data
row. When an N -gram is traversed, binary search
is performed N times. If each word id corresponds
to its node position in the unigram table, we can
remove the word ids for the first order.
Our implementation merges across different or-
ders of N -grams, then separates into multiple ta-
bles such as word ids, smoothed probabilities,
back-off coefficients, and pointers. The starting
positions of different orders are memorized to al-
low access to arbitrary orders. To store N -gram
counts, we use three tables for word ids, counts
and pointers. We share the same tables for word
ids and pointers with additional probability and
back-off coefficient tables.
To support distributed computation (Brants et
al., 2007), we further split the N -gram data into
?shards? by hash values of the first bigram. Uni-
gram data are shared across shards for efficiency.
3 Succinct N -gram Structure
The table of pointers described in the previous
section represents a trie. We use a succinct data
structure LOUDS (Jacobson, 1989; Delpratt et al,
2006) for compact representation of the trie.
For an M node ordinal trie, there exist
1
2M+1
(
2M+1
M
)
different tries. Therefore,
its information-theoretical lower bound is
lg
?
1
2M+1
(
2M+1
M
)
?
? 2M ? O(lg M) bits.
LOUDS represents a trie with M nodes as a
2M + O(M) bit string.
The LOUDS bit string is constructed as follows.
Starting from the root node, we traverse a trie in
level order. For each node with d ? 0 children, the
bit string 1d0 is emitted. In addition, 10 is prefixed
to the bit string emitted by an imaginary super-root
node pointing to the root node. Figure 2(a) shows
an example trie structure. The nodes are numbered
in level order, and from left to right. The cor-
responding LOUDS bit string is shown in Figure
2(b). Since the root node 0 has four child nodes,
it emits four 1s followed by 0, which marks the
end of the node. Before the root node, we assume
an imaginary super root node emits 10 for its only
child, i.e., the root node. After the root node, its
first child or node 1 follows. Since (M + 1)0s and
M1s are emitted for a trie with M nodes, LOUDS
occupies 2M + 1 bits.
We define a basic operation on the bit string.
sel
1
(i) returns the position of the i-th 1. We can
also define similar operations over zero bit strings,
sel
0
(i). Given sel
b
, we define two operations for
a node x. parent(x) gives x?s parent node and
firstch(x) gives x?s first child node:
parent(x) = sel
1
(x + 1) ? x ? 1, (1)
firstch(x) = sel
0
(x + 1) ? x. (2)
To test whether a child node exists, we sim-
ply check firstch(x) 6= firstch(x + 1). Sim-
ilarly, the child node range is determined by
[firstch(x),firstch(x + 1)).
3.1 Optimizing N -gram Structure for Space
We propose removing redundant bits from the
baseline LOUDS representation assuming N -
gram structures. Since we do not store any infor-
mation in the root node, we can safely remove the
root so that the imaginary super-root node directly
points to unigram nodes. The node ids are renum-
bered and the first unigram is 0. In this way, 2 bits
are saved.
The N -gram data structure has a fixed depth N
and takes a flat structure. Since the highest or-
der N -grams have no child nodes, they emit 0NN
in the tail of the bit stream, where N
n
stands for
the number of n-grams. By memorizing the start-
ing position of the highest order N -grams, we can
completely remove N
N
bits.
The imaginary super-root emits 1N10 at the be-
ginning of the bit stream. By memorizing the bi-
gram starting position, we can remove the N
1
+ 1
bits.
Finally, parent(x) and firstch(x) are rewritten as
342
integer seq. 52 156 260 364
coding 0x34 0x9c 0x01 0x04 0x01 0x6c
boundary 1 1 0 1 0 1
Figure 3: Example of variable length coding
follows:
parent(x) = sel
1
(x + 1 ?N
1
) + N
1
? x, (3)
firstch(x) = sel
0
(x) + N
1
+ 1 ? x. (4)
Figure 2(c) shows the N -gram optimized trie
structure (N = 3) from Figure 2 with N
1
= 4
and N
3
= 5. The parent of node 8 is found by
sel
1
(8+1?4) = 5 and 5+4?8 = 1. The first child
is located by sel
0
(8) = 16 and 16+4+1?8 = 13.
When accessing the N -gram data structure,
sel
b
(i) operations are used extensively. We use an
auxiliary dictionary structure proposed by Kim et
al. (2005) and Jacobson (1989) that supports an
efficient sel
1
(i) (sel
0
(i)) with the dictionary. We
omit the details due to lack of space.
3.2 Variable Length Coding
The above method compactly represents pointers,
but not associated values, such as word ids or
counts. Raj and Whittaker (2003) proposed in-
teger compression on each range of the word id
sequence that shared the same N -gram prefix.
Here, we introduce a simple but more effec-
tive variable length coding for integer sequences
of word ids and counts. The basic idea comes from
encoding each integer by the smallest number of
required bytes. Specifically, an integer within the
range of 0 to 255 is coded as a 1-byte integer,
the integers within the range of 256 to 65,535 are
stored as 2-byte integers, and so on. We use an ad-
ditional bit vector to indicate the boundary of the
byte sequences. Figure 3 presents an example in-
teger sequence, 52, 156, 260 and 364 with coded
integers in hex decimals with boundary bits.
In spite of the length variability, the system
can directly access a value at index i as bytes
in [sel
1
(i) + 1, sel
1
(i + 1) + 1) by the efficient
sel
1
operation assuming that sel
1
(0) yields ?1.
For example, the value 260 at index 2 in Figure
3 is mapped onto the byte range of [sel
1
(2) +
1, sel
1
(3) + 1) = [2, 4).
3.3 Block-wise Compression
We further compress every 8K-byte data block of
all tables in N -grams by using a generic com-
pression library, zlib, employed in UNIX gzip.
We treat a sequence of 4-byte floats in the prob-
ability table as a byte stream, and compress ev-
ery 8K-byte block. To facilitate random access to
the compressed block, we keep track of the com-
pressed block?s starting offsets. Since the offsets
are in sorted order, we can apply sorted integer
compression (Raj and Whittaker, 2003). Since N -
gram language model access preserves some local-
ity, N -gram with block compression is still practi-
cal enough to be usable in our system.
4 Experiments
We applied the proposed representation to 5-gram
trained by ?English Gigaword 3rd Edition,? ?En-
glish Web 1T 5-gram? from LDC, and ?Japanese
Web 1T 7-gram? from GSK. Since their tendencies
are the same, we only report in this paper the re-
sults on English Web 1T 5-gram, where the size
of the count data in gzipped raw text format is
25GB, the number of N-grams is 3.8G, the vocab-
ulary size is 13.6M words, and the number of the
highest order N-grams is 1.2G.
We implemented an N -gram indexer/estimator
using MPI inspired by the MapReduce imple-
mentation of N -gram language model index-
ing/estimation pipeline (Brants et al, 2007).
Table 1 summarizes the overall results. We
show the initial indexed counts and the final lan-
guage model size by differentiating compression
strategies for the pointers, namely the 4-byte raw
value (Trie), the sorted integer compression (In-
teger) and our succinct representation (Succinct).
The ?block? indicates block compression. For the
sake of implementation simplicity, the sorted in-
teger compression used a fixed 8-bit shift amount,
although the original paper proposed recursively
determined optimum shift amounts (Raj and Whit-
taker, 2003). 8-bit quantization was performed
for probabilities and back-off coefficients using a
simple binning approach (Federico and Cettolo,
2007).
N -gram counts were reduced from 23.59GB
to 10.57GB by our succinct representation with
block compression. N -gram language models of
42.65GB were compressed to 18.37GB. Finally,
the 8-bit quantized N -gram language models are
represented by 9.83GB of space.
Table 2 shows the compression ratio for the
pointer table alone. Block compression employed
on raw 4-byte pointers attained a large reduc-
tion that was almost comparable to sorted inte-
ger compression. Since large pointer value tables
are sorted, even a generic compression algorithm
could achieve better compression. Using our suc-
cinct representation, 2.4 bits are required for each
N -gram. By using the ?flat? trie structure, we
approach closer to its information-theoretic lower
bound beyond the LOUDS baseline. With block
compression, we achieved 1.8 bits per N -gram.
Table 3 shows the effect of variable length
coding and block compression for the word ids,
counts, probabilities and back-off coefficients. Af-
ter variable-length coding, the word id is almost
half its original size. We assign a word id for each
343
w/o block w/ block
Counts Trie 23.59 GB 12.21 GB
Integer 14.59 GB 11.18 GB
Succinct 12.62 GB 10.57 GB
Language Trie 42.65 GB 20.01 GB
model Integer 33.65 GB 18.98 GB
Succinct 31.67 GB 18.37 GB
Quantized Trie 24.73 GB 11.47 GB
language Integer 15.73 GB 10.44 GB
model Succinct 13.75 GB 9.83 GB
Table 1: Summary of N -gram compression
total per N -gram
4-byte Pointer 12.04 GB 27.24 bits
+block compression 2.42 GB 5.48 bits
Sorted Integer 3.04 GB 6.87 bits
+block compression 1.39 GB 3.15 bits
Succinct 1.06 GB 2.40 bits
+block compression 0.78 GB 1.76 bits
Table 2: Compression ratio for pointers
word according to its reverse sorted order of fre-
quency. Therefore, highly frequent words are as-
signed smaller values, which in turn occupies less
space in our variable length coding. With block
compression, we achieved further 1 GB reduction
in space. Since the word id sequence preserves
local ordering for a certain range, even a generic
compression algorithm is effective.
The most frequently observed count in N -gram
data is one. Therefore, we can reduce the space
by the variable length coding. Large compression
rates are achieved for both probabilities and back-
off coefficients.
5 Conclusion
We provided a succinct representation of the N -
gram language model without any loss. Our
method approaches closer to the information-
theoretic lower bound beyond the LOUDS base-
line. Experimental results showed our succinct
representation drastically reduces the space for
the pointers compared to the sorted integer com-
pression approach. Furthermore, the space of
N -grams was significantly reduced by variable
total per N -gram
word id size (4 bytes) 14.09 GB 31.89 bits
+variable length 6.72 GB 15.20 bits
+block compression 5.57 GB 12.60 bits
count size (8 bytes) 28.28 GB 64.00 bits
+variable length 4.85 GB 10.96 bits
+block compression 4.22 GB 9.56 bits
probability size (4 bytes) 14.14 GB 32.00 bits
+block compression 9.55 GB 21.61 bits
8-bit quantization 3.54 GB 8.00 bits
+block compression 2.64 GB 5.97 bits
backoff size (4 bytes) 9.76 GB 22.08 bits
+block compression 2.48 GB 5.61 bits
8-bit quantization 2.44 GB 5.52 bits
+block compression 0.85 GB 1.92 bits
Table 3: Effects of block compression
length coding and block compression. A large
amount of N -gram data is reduced from unin-
dexed gzipped 25 GB text counts to 10 GB of
indexed language models. Our representation is
practical enough though we did not experimen-
tally investigate the runtime efficiency in this pa-
per. The proposed representation enables us to
utilize a web-scaled N -gram in our MT compe-
tition system (Watanabe et al, 2008). Our suc-
cinct representation will encourage new research
on web-scaled N -gram data without requiring a
larger computer cluster or hundreds of gigabytes
of memory.
Acknowledgments
We would like to thank Daisuke Okanohara for his
open source implementation and extensive docu-
mentation of LOUDS, which helped our original
coding.
References
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In Proc. of EMNLP-CoNLL 2007.
K. Church, T. Hart, and J. Gao. 2007. Compressing
trigram language models with Golomb coding. In
Proc. of EMNLP-CoNLL 2007.
O. Delpratt, N. Rahman, and R. Raman. 2006. Engi-
neering the LOUDS succinct tree representation. In
Proc. of the 5th International Workshop on Experi-
mental Algorithms.
M. Federico and M. Cettolo. 2007. Efficient handling
of n-gram language models for statistical machine
translation. In Proc. of the 2nd Workshop on Statis-
tical Machine Translation.
G. Jacobson. 1989. Space-efficient static trees and
graphs. In 30th Annual Symposium on Foundations
of Computer Science, Nov.
D. K. Kim, J. C. Na, J. E. Kim, and K. Park. 2005. Ef-
ficient implementation of rank and select functions
for succinct representation. In Proc. of the 5th Inter-
national Workshop on Experimental Algorithms.
B. Raj and E. W. D. Whittaker. 2003. Lossless com-
pression of language model structure and word iden-
tifiers. In Proc. of ICASSP 2003, volume 1.
A. Stolcke. 1998. Entropy-based pruning of backoff
language models. In Proc. of the ARPA Workshop
on Human Language Technology.
D. Talbot and T. Brants. 2008. Randomized language
models via perfect hash functions. In Proc. of ACL-
08: HLT.
T. Watanabe, H. Tsukada, and H. Isozaki. 2008. NTT
SMT system 2008 at NTCIR-7. In Proc. of the 7th
NTCIR Workshop, pages 420?422.
344
Efficient Decoding for Statistical Machine Translation
with a Fully Expanded WFST Model
Hajime Tsukada
NTT Communication Science Labs.
2-4 Hikaridai Seika-cho Soraku-gun
Kyoto 619-0237
Japan
tsukada@cslab.kecl.ntt.co.jp
Masaaki Nagata
NTT Cyber Space Labs.
1-1 Hikari-no-Oka Yokosuka-shi
Kanagawa 239-0847
Japan
nagata.masaaki@lab.ntt.co.jp
Abstract
This paper proposes a novel method to compile sta-
tistical models for machine translation to achieve
efficient decoding. In our method, each statistical
submodel is represented by a weighted finite-state
transducer (WFST), and all of the submodels are ex-
panded into a composition model beforehand. Fur-
thermore, the ambiguity of the composition model
is reduced by the statistics of hypotheses while de-
coding. The experimental results show that the pro-
posed model representation drastically improves the
efficiency of decoding compared to the dynamic
composition of the submodels, which corresponds
to conventional approaches.
1 Introduction
Recently, research on statistical machine translation
has grown along with the increase in computational
power as well as the amount of bilingual corpora.
The basic idea of modeling machine translation was
proposed by Brown et al (1993), who assumed that
machine translation can be modeled on noisy chan-
nels. The source language is encoded from a target
language by a noisy channel, and translation is per-
formed as a decoding process from source language
to target language.
Knight (1999) showed that the translation prob-
lem defined by Brown et al (1993) is NP-
complete. Therefore, with this model it is al-
most impossible to search for optimal solutions in
the decoding process. Several studies have pro-
posed methods for searching suboptimal solutions.
Berger et al (1996) and Och et al (2001) pro-
posed such depth-first search methods as stack de-
coders. Wand and Waibel (1997) and Tillmann and
Ney (2003) proposed breadth-first search methods,
i.e. beam search. Germann (2001) and Watanabe
and Sumita (2003) proposed greedy type decoding
methods. In all of these search algorithms, better
representation of the statistical model in systems
can improve the search efficiency.
For model representation, a search method based
on weighted finite-state transducer (WFST) (Mohri
et al, 2002) has achieved great success in the speech
recognition field. The basic idea is that each statis-
tical model is represented by a WFST and they are
composed beforehand; the composed model is op-
timized by WFST operations such as determiniza-
tion and minimization. This fully expanded model
permits efficient searches. Our motivation is to ap-
ply this approach to machine translation. However,
WFST optimization operations such as determiniza-
tion are nearly impossible to apply to WFSTs in ma-
chine translation because they are much more am-
biguous than speech recognition. To reduce the am-
biguity, we propose a WFST optimization method
that considers the statistics of hypotheses while de-
coding.
Some approaches have applied WFST to sta-
tistical machine translation. Knight and Al-
Onaizan (1998) proposed the representation of
IBM model 3 with WFSTs; Bangalore and Ric-
cardi (2001) studied WFST models in call-routing
tasks, and Kumar and Byrne (2003) modeled
phrase-based translation by WFSTs. All of these
studies mainly focused on the representation of each
submodel used in machine translation. However,
few studies have focued on the integration of each
WFST submodel to improve the decoding efficiency
of machine translation.
To this end, we propose a method that expands
all of the submodels into a composition model, re-
ducing the ambiguity of the expanded model by the
statistics of hypotheses while decoding. First, we
explain the translation model (Brown et al, 1993;
Knight and Al-Onaizan, 1998) that we used as a
base for our decoding research. Second, our pro-
posed method is introduced. Finally, experimental
results show that our proposed method drastically
improves decoding efficiency.
2 IBM Model
For our decoding research, we assume the IBM-
style modeling for translation proposed in Brown et
al. (1993). In this model, translation from Japanese
 to English  attempts to find the  that maximizes


 
. Using Bayes? rule,


 
is rewritten as
	
	


 
	
	

 
 




where



is referred to as a language model and

 
 

is referred to as a translation model. In this
paper, we use word trigram for a language model
and IBM model 3 for a translation model.
The translation model is represented as follows
considering all possible word alignments.

 
 

Proceedings of the Workshop on Statistical Machine Translation, pages 122?125,
New York City, June 2006. c?2006 Association for Computational Linguistics
NTT System Description for the WMT2006 Shared Task
Taro Watanabe Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun,
Kyoto, Japan 619-0237
{taro,tsukada,isozaki}@kecl.ntt.co.jp
Abstract
We present two translation systems ex-
perimented for the shared-task of ?Work-
shop on Statistical Machine Translation,?
a phrase-based model and a hierarchical
phrase-based model. The former uses a
phrasal unit for translation, whereas the
latter is conceptualized as a synchronous-
CFG in which phrases are hierarchically
combined using non-terminals. Experi-
ments showed that the hierarchical phrase-
based model performed very comparable
to the phrase-based model. We also report
a phrase/rule extraction technique differ-
entiating tokenization of corpora.
1 Introduction
We contrasted two translation methods for the
Workshop on Statistical Machine Translation
(WMT2006) shared-task. One is a phrase-based
translation in which a phrasal unit is employed
for translation (Koehn et al, 2003). The other is
a hierarchical phrase-based translation in which
translation is realized as a set of paired production
rules (Chiang, 2005). Section 2 discusses those two
models and details extraction algorithms, decoding
algorithms and feature functions.
We also explored three types of corpus pre-
processing in Section 3. As expected, different
tokenization would lead to different word align-
ments which, in turn, resulted in the divergence
of the extracted phrase/rule size. In our method,
phrase/rule translation pairs extracted from three
distinctly word-aligned corpora are aggregated into
one large phrase/rule translation table. The experi-
ments and the final translation results are presented
in Section 4.
2 Translation Models
We used a log-linear approach (Och and Ney,
2002) in which a foreign language sentence f J1 =f1, f2, ... fJ is translated into another language, i.e.
English, eI1 = e1, e2, ..., eI by seeking a maximum
likelihood solution of
e?I1 = argmax
eI1
Pr(eI1| f J1 ) (1)
= argmax
eI1
exp
(
?M
m=1 ?mhm(eI1, f J1 )
)
?
e? I
?
1
exp
(
?M
m=1 ?mhm(e? I
?
1 , f J1 )
)(2)
In this framework, the posterior probability
Pr(eI1| f J1 ) is directly maximized using a log-linear
combination of feature functions hm(eI1, f J1 ), such
as a ngram language model or a translation model.
When decoding, the denominator is dropped since it
depends only on f J1 . Feature function scaling factors
?m are optimized based on a maximum likelihood
approach (Och and Ney, 2002) or on a direct error
minimization approach (Och, 2003). This modeling
allows the integration of various feature functions
depending on the scenario of how a translation is
constituted.
In a phrase-based statistical translation (Koehn
et al, 2003), a bilingual text is decomposed as K
phrase translation pairs (e?1, ?fa?1), (e?2, ?fa?2 ), ...: The in-
put foreign sentence is segmented into phrases ?f K1 ,
122
mapped into corresponding English e?K1 , then, re-
ordered to form the output English sentence accord-
ing to a phrase alignment index mapping a?.
In a hierarchical phrase-based translation (Chi-
ang, 2005), translation is modeled after a weighted
synchronous-CFG consisting of production rules
whose right-hand side is paired (Aho and Ullman,
1969):
X ? ??, ?,??
where X is a non-terminal, ? and ? are strings of ter-
minals and non-terminals. ? is a one-to-one corre-
spondence for the non-terminals appeared in ? and
?. Starting from an initial non-terminal, each rule
rewrites non-terminals in ? and ? that are associated
with ?.
2.1 Phrase/Rule Extraction
The phrase extraction algorithm is based on those
presented by Koehn et al (2003). First, many-
to-many word alignments are induced by running
a one-to-many word alignment model, such as
GIZA++ (Och and Ney, 2003), in both directions
and by combining the results based on a heuristic
(Och and Ney, 2004). Second, phrase translation
pairs are extracted from the word aligned corpus
(Koehn et al, 2003). The method exhaustively ex-
tracts phrase pairs ( f j+mj , ei+ni ) from a sentence pair
( f J1 , eI1) that do not violate the word alignment con-
straints a.
In the hierarchical phrase-based model, produc-
tion rules are accumulated by computing ?holes? for
extracted contiguous phrases (Chiang, 2005):
1. A phrase pair ( ?f , e?) constitutes a rule:
X ?
?
?f , e?
?
2. A rule X ? ??, ?? and a phrase pair ( ?f , e?) s.t.
? = ?? ?f??? and ? = ??e???? constitutes a rule:
X ?
?
?? X k ?
??, ?? X k ?
??
?
2.2 Decoding
The decoder for the phrase-based model is a left-to-
right generation decoder with a beam search strategy
synchronized with the cardinality of already trans-
lated foreign words. The decoding process is very
similar to those described in (Koehn et al, 2003):
It starts from an initial empty hypothesis. From an
existing hypothesis, new hypothesis is generated by
consuming a phrase translation pair that covers un-
translated foreign word positions. The score for the
newly generated hypothesis is updated by combin-
ing the scores of feature functions described in Sec-
tion 2.3. The English side of the phrase is simply
concatenated to form a new prefix of English sen-
tence.
In the hierarchical phrase-based model, decoding
is realized as an Earley-style top-down parser on the
foreign language side with a beam search strategy
synchronized with the cardinality of already trans-
lated foreign words (Watanabe et al, 2006). The ma-
jor difference to the phrase-based model?s decoder is
the handling of non-terminals, or holes, in each rule.
2.3 Feature Functions
Our phrase-based model uses a standard pharaoh
feature functions listed as follows (Koehn et al,
2003):
? Relative-count based phrase translation proba-
bilities in both directions.
? Lexically weighted feature functions in both di-
rections.
? The supplied trigram language model.
? Distortion model that counts the number of
words skipped.
? The number of words in English-side and the
number of phrases that constitute translation.
For details, please refer to Koehn et al (2003).
In addition, we added three feature functions to
restrict reorderings and to represent globalized in-
sertion/deletion of words:
? Lexicalized reordering feature function scores
whether a phrase translation pair is monotoni-
cally translated or not (Och et al, 2004):
hlex(a?K1 | ?f K1 , e?K1 ) = log
K
?
k=1
pr(?k | ?fa?k , e?k) (3)
where ?k = 1 iff a?k ? a?k?1 = 1 otherwise ?k = 0.
? Deletion feature function penalizes words that
do not constitute a translation according to a
123
Table 1: Number of word alignment by different preprocessings.
de-en es-en fr-en en-de en-es en-fr
lower 17,660,187 17,221,890 16,176,075 17,596,764 17,237,723 16,220,520
stem 17,110,890 16,601,306 15,635,900 17,052,808 16,597,274 15,658,940
prefix4 16,975,398 16,540,767 15,610,319 16,936,710 16,530,810 15,613,755
intersection 12,203,979 12,677,192 11,645,404 12,218,997 12,688,773 11,653,242
union 23,186,379 21,709,212 20,760,539 23,066,052 21,698,267 20,789,570
Table 2: Number of phrases extracted from differently preprocessed corpora.
de-en es-en fr-en en-de en-es en-fr
lower 37,711,217 61,161,868 56,025,918 38,142,663 60,619,435 55,198,497
stem 46,550,101 75,610,696 68,210,968 46,749,195 75,473,313 67,733,045
prefix4 53,429,522 78,193,818 70,514,377 53,647,033 78,223,236 70,378,947
merged 80,260,191 111,153,303 103,523,206 80,666,414 110,787,982 102,940,840
lexicon model t( f |e) (Bender et al, 2004):
hdel(eI1, f J1 ) =
J
?
j=1
[
max
0?i?I
t( f j|ei) < ?del
]
(4)
The deletion model simply counts the number
of words whose lexicon model probability is
lower than a threshold ?del. Likewise, we also
added an insertion model hins(eI1, f J1 ) that pe-
nalizes the spuriously inserted English words
using a lexicon model t(e| f ).
For the hierarchical phrase-based model, we em-
ployed the same feature set except for the distortion
model and the lexicalized reordering model.
3 Phrase Extraction from Different Word
Alignment
We prepared three kinds of corpora differentiated
by tokenization methods. First, the simplest pre-
processing is lower-casing (lower). Second, corpora
were transformed by a Porter?s algorithm based mul-
tilingual stemmer (stem) 1. Third, mixed-cased cor-
pora were truncated to the prefix of four letters of
each word (prefix4). For each differently tokenized
corpus, we computed word alignments by a HMM
translation model (Och and Ney, 2003) and by a
word alignment refinement heuristic of ?grow-diag-
final? (Koehn et al, 2003). Different preprocessing
yields quite divergent alignment points as illustrated
in Table 1. The table also shows the numbers for
the intersection and union of three alignment anno-
tations.
The (hierarchical) phrase translation pairs are ex-
tracted from three distinctly word aligned corpora.
1We used the Snowball stemmer from http://snowball.
tartarus.org
In this process, each word is recovered into its lower-
cased form. The associated counts are aggregated
to constitute relative count-based feature functions.
Table 2 summarizes the size of phrase tables in-
duced from the corpora. The number of rules ex-
tracted for the hierarchical phrase-based model was
roughly twice as large as those for the phrase-based
model. Fewer word alignments resulted in larger
phrase translation table size as observed in the ?pre-
fix4? corpus. The size is further increased by our
aggregation step (merged).
Different induction/refinement algorithms or pre-
processings of a corpus bias word alignment. We
found that some word alignments were consistent
even with different preprocessings, though we could
not justify whether such alignments would match
against human intuition. If we could trust such
consistently aligned words, reliable (hierarchical)
phrase translation pairs would be extracted, which,
in turn, would result in better estimates for relative
count-based feature functions. At the same time, dif-
ferently biased word alignment annotations suggest
alternative phrase translation pairs that is useful for
increasing the coverage of translations.
4 Results
Table 3 shows the open test translation results on
2005 and 2006 test set (the development-test set and
the final test set) 2. We used the merged (hierar-
chical) phrase tables for decoding. Feature function
scaling factors were optimized on BLEU score us-
ing the supplied development set that is identical to
the 2005?s development set. We observed that our
2We did not differetiated in-domain or out-of-domain for
2006 test set.
124
Table 3: Open test on the 2005/2006 test sets (BLEU [%]).
de-en es-en fr-en en-de en-es en-fr
test2005 Phrase 25.72 30.97 30.97 18.08 30.48 32.14
Rule 25.14 30.11 30.31 17.96 27.96 31.04
2005?s best 24.77 30.95 30.27
test2006 Phrase 23.16 29.90 27.89 15.79 29.54 29.19
Rule 22.74 28.80 27.28 15.99 26.56 27.86
results are very comparable to the last year?s best re-
sults in test2005. Also found that our hierarchical
phrase-based translation (Rule) performed slightly
inferior to the phrase-based translation (Phrase) in
both test sets. The hierarchically combined phrases
seem to be too flexible to represent the relationship
of similar language pairs. Note that our hierarchical
phrase-based model performed better in the English-
to-German translation task. Those language pair re-
quires rather distorted reordering, which could be
represented by hierarchically combined phrases.
We also conducted additional studies on how
differently aligned corpora might affect the trans-
lation quality on Spanish-to-English task for the
2005 test set. Using our phrase-based model,
the BLEU scores for lower/stem/prefix4 were
30.90/30.89/30.76, respectively. The differences of
translation qualities were statistically significant at
the 95% confidence level. Our phrase translation
pairs aggregated from all the differently prepro-
cessed corpora improved the translation quality.
5 Conclusion
We presented two translation models, a phrase-
based model and a hierarchical phrase-based model.
The former performed as well as the last year?s best
system, whereas the latter performed comparable to
our phrase-based model. We are going to experi-
ment new feature functions to restrict the too flexible
reordering represented by our hierarchical phrase-
based model.
We also investigated different word alignment an-
notations, first using lower-cased corpus, second
performing stemming, and third retaining only 4-
letter prefix. Differently preprocessed corpora re-
sulted in quite divergent word alignment. Large
phrase/rule translation tables were accumulated
from three distinctly aligned corpora, which in turn,
increased the translation quality.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37?56.
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system?. In Proc. of IWSLT 2004, pages 79?84,
Kyoto, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL
2005, pages 263?270, Ann Arbor, Michigan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL 2002, pages
295?302.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Shankar Fraser, Alex a
nd Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In HLT-NAACL 2004: Main Proceedings, pages
161?168, Boston, Massachusetts, USA, May 2 - May
7.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING-ACL
2006 (to appear), Sydney, Australia, July.
125
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 439?446,
Beijing, August 2010
Hierarchical Phrase-based Machine Translation with Word-based
Reordering Model
Katsuhiko Hayashi*, Hajime Tsukada**
Katsuhito Sudoh**, Kevin Duh**, Seiichi Yamamoto*
*Doshisha University
katsuhiko-h@is.naist.jp, seyamamo@mail.doshisha.ac.jp
**NTT Communication Science Laboratories
tsukada, sudoh, kevinduh@cslab.kecl.ntt.co.jp
Abstract
Hierarchical phrase-based machine trans-
lation can capture global reordering with
synchronous context-free grammar, but
has little ability to evaluate the correctness
of word orderings during decoding. We
propose a method to integrate word-based
reordering model into hierarchical phrase-
based machine translation to overcome
this weakness. Our approach extends the
synchronous context-free grammar rules
of hierarchical phrase-based model to in-
clude reordered source strings, allowing
efficient calculation of reordering model
scores during decoding. Our experimen-
tal results on Japanese-to-English basic
travel expression corpus showed that the
BLEU scores obtained by our proposed
system were better than those obtained by
a standard hierarchical phrase-based ma-
chine translation system.
1 Introduction
Hierarchical phrase-based machine translation
(Chiang, 2007; Watanabe et al, 2006) is one of
the promising statistical machine translation ap-
proaches (Brown et al, 1993). Its model is for-
mulated by a synchronous context-free grammar
(SCFG) which captures the syntactic information
between source and target languages. Although
the model captures global reordering by SCFG,
it does not explicitly introduce reordering model
to constrain word order. In contrast, lexicalized
reordering models (Tillman, 2004; Koehn et al,
2005; Nagata et al, 2006) are extensively used
for phrase-based translation. These lexicalized re-
ordering models cannot be directly applied to hi-
erarchical phrased-based translation since the hi-
erarchical phrase representation uses nonterminal
symbols.
To handle global reordering in phrase-based
translation, various preprocessing approaches
have been proposed, where the source sentence
is reordered to target language order beforehand
(Xia and McCord, 2004; Collins et al, 2005; Li et
al., 2007; Tromble and Eisner, 2009). However,
preprocessing approaches cannot utilize other in-
formation in the translation model and target lan-
guage model, which has been proven helpful in
decoding.
This paper proposes a method that incorpo-
rates word-based reordering model into hierarchi-
cal phrase-based translation to constrain word or-
der. In this paper, we adopt the reordering model
originally proposed by Tromble and Eisner (2009)
for the preprocessing approach in phrase-based
translation. To integrate the word-based reorder-
ing model, we added a reordered source string
into the right-hand-side of SCFG?s rules. By this
extension, our system can generate the reordered
source sentence as well as target sentence and is
able to efficiently calculate the score of the re-
ordering model. Our method utilizes the transla-
tion model and target language model as well as
the reordering model during decoding. This is an
advantage of our method over the preprocessing
approach.
The remainder of this paper is organized as
follows. Section 2 describes the concept of our
approach. Section 3 briefly reviews our pro-
posed method on hierarchical phrase-based ma-
439
Standard SCFG X ?< X1 wa jinsei no X2 da , X1 is X2 of life>
SCFG (move-to-front) X ?< X1 wa jinsei no X2 da , wa X1 da X2 no jinsei , X1 is X2 of life>
SCFG (attach) X ?< X1 wa jinsei no X2 da , X1 wa da X2 no jinsei , X1 is X2 of life>
Table 1: A Japanese-to-English example of various SCFG?s rule representations. Japanese words are
romanized. Our proposed representation of rules has reordered source string to generate reordered
source sentence S? as well as target sentence T . The ?move-to-front? means Tromble and Eisner (2009)
?s algorithm and the ?attach? means Al-Onaizan and Papineni (2006) ?s algorithm.
chine translation model. We experimentally com-
pare our proposed system to a standard hierarchi-
cal phrase-based system on Japanese-to-English
translation task in Section 4. Then we discuss on
related work in Section 5 and conclude this paper
in Section 6.
2 The Concept of Our Approach
The preprocessing approach (Xia and McCord,
2004; Collins et al, 2005; Li et al, 2007; Tromble
and Eisner, 2009) splits translation procedure into
two stages:
S ? S? ? T (1)
where S is a source sentence, S? is a reordered
source sentence with respect to the word order of
target sentence T . Preprocessing approach has the
very deterministic and hard decision in reorder-
ing. To overcome the problem, Li et al (2007)
proposed k-best appoach. However, even with a
k-best approach, it is difficult to generate good hy-
potheses S? by using only a reordering model.
In this paper, we directly integrated the reorder-
ing model into the decoder in order to use the
reordering model together with other information
in the hierarchical phrase-based translation model
and target language model. Our approach is ex-
pressed as the following equation.
S ? (S? , T ). (2)
Our proposed method generates the reordered
source sentence S? by SCFG and evaluates the
correctness of the reorderings using a word-based
reordering model of S? which will be introduced
in section 3.4.
Figure 1: A derivation tree for Japanse-to-English
translation.
3 Hierarchical Phrase-based Model
Extension
3.1 Hierarchical Phrase-based Model
Hierarchical phrase-based model (Chiang, 2007)
induces rules of the form
X ?< ?, ?,?, w > (3)
where X is a non-terminal symbol, ? is a se-
quence string of non-terminals and source termi-
nals, ? is a sequence string of non-terminals and
target terminals. ? is a one-to-one correspon-
dence for the non-terminals appeared in ? and ?.
Given a source sentence S, the translation task
under this model can be expressed as
T? = T
(
argmax
D:S(D)=S
w(D)
)
(4)
where D is a derivation and w(D) is a score of
the derivation. Decoder seeks a target sentence
440
Figure 2: Reordered source sentence generated by
our proposed system.
T (D) which has the highest score w(D). S(D)
is a source sentence under a derivation D. Fig-
ure 1 shows the example of Japanese-to-English
translation by hierarchical phrase-based machine
translation model.
3.2 Rule Extension
To generate reordered source sentence S? as well
as target sentence T , we extend hierarchical
phrase rule expressed in Equation 3 to
X ?< ?, ?? , ?,?, w > (5)
where ?? is a sequence string of non-terminals and
source terminals, which is reordered ? with re-
spect to the word order of target string ?. The
reason why we add ?? to rules is to efficiently cal-
culate the reordering model scores. If each rule
does not have ?? , the decoder need to keep word
alignments because we cannot know word order
of S? without them. The calculation of reorder-
ing model scores using word alignments is very
wasteful when decoding.
The translation task under our model extends
Equation 4 to the following equation:
T? = (S?? , T? ) = (S? , T )
(
argmax
D:S(D)=S
w(D)
)
. (6)
Our system generates the reordered source sen-
tence S? as well as target sentence T . Figure 2
shows the generated reordered source sentence S?
Uni-gram Features
sr, s-posr
sr
s-posr
sl, s-posl
sl
s-posl
Bi-gram Features
sr, s-posr, sl, s-posl
s-posr, sl, s-posl
sr, sl, s-posl
sr, s-posr, s-posl
sr, s-posr, sl
sr, sl
s-posr, s-posl
Table 2: Features used by Word-based Reordering
Model. pos means part-of-speech tag.
when translating the example of Figure 1. Note
that the structure of S? is the same as that of target
sentence T . The decoder generates both Figure 2
and the right hand side of Figure 1, allowing us to
score both global and local word reorderings.
To add ?? to rules, we permuted ? into ?? after
rule extraction based on Grow-diag-final (Koehn
et al, 2005) alignment by GIZA++ (Och and Ney,
2003). To do this permutation on rules, we ap-
plied two methods. One is the same algorithm
as Tromble and Eisner (2009), which reorders
aligned source terminals and nonterminals in the
same order as that of target side and moves un-
aligned source terminals to the front of aligned
terminals or nonterminals (move-to-front). The
other is the same algorithm as AI-Onaizan and
Papineni (2006), which differs from Tromble and
Eisner?s approach in attaching unaligned source
terminals to the closest prealigned source termi-
nals or nonterminals (attach). This extension of
adding ?? does not increase the number of rules.
Table 1 shows a Japanese-to-English example
of the representation of rules for our proposed sys-
tem. Japanese words are romanized. Suppose that
source-side string is (X1 wa jinsei no X2 da) and
target-side string is (X1 is X2 of life) and their
word alignments are a=((jinsei , life) , (no , of)
, (da , is)). Source-side aligned words and non-
terminal symbols are sorted into the same order of
target string. Source-side unaligned word (wa) is
moved to the front or right of the prealigned sym-
bol (X1).
441
Surrounding Word Pos Features
s-posr, s-posr + 1, s-posl ? 1, s-posl
s-posr ? 1, s-posr, s-posl ? 1, s-posl
s-posr, s-posr + 1, s-posl, s-posl + 1
s-posr ? 1, s-posr, s-posl, s-posl + 1
Table 3: The Example of Context Features
3.3 Word-based Reordering Model
We utilize the following score(S?) as a feature for
the word-based reordering model. This is incor-
polated into the log-linear model (Och and Ney,
2002) of statistical machine translation.
score(S?) =
?
i,j:1?i<j?n
B[s?i, s
?
j ] (7)
B[s?l, s
?
r] = ? ? ?(s
?
l, s
?
r) (8)
where n is the length of reordered source sen-
tence S? (= (s?1 . . . s
?
n)), ? is a weight vector and
? is a vector of features. This reordering model,
which is originally proposed by Tromble and Eis-
ner (2009), can assign a score to any possible per-
mutation of source sentences. Intuitively B[s?l, s
?
r]
represents the score of ordering s?l before s
?
r; the
higher the value, the more we prefer word s?l oc-
curs before s?r. Whether S
?
l should occur before S
?
r
depends on how often this reordering occurs when
we reorder the source to target sentence order.
To train B, we used binary feature functions
? as used in (Tromble and Eisner, 2009), which
were introduced for dependency parsing by Mc-
Donald et al (2005). Table 2 shows the kind
of features we used in our experiments. We did
not use context features like surrounding word pos
features in Table 3 because they were not useful in
our preliminary experiments and propose an effi-
cient implementation described in the next section
in order to calculate this reordering model when
decoding. To train the parameter ?, we used the
perceptron algorithm following Tromble and Eis-
ner (2009).
3.4 Integration to Cube Pruning
CKY parsing and cube-pruning are used for de-
coding of hierarchical phrase-based model (Chi-
ang, 2007). Figure 3 displays that hierarchical
phrase-based decoder seeks new span [1,7] items
Figure 3: Creating new items from subitems and
rules, that have a span [1,7] in source sentence.
with rules, utilizing subspan [1,3] items and sub-
span [4,7] items. In this example, we use 2-gram
language model and +LM decoding. uni(?) means
1-gram language model cost for heuristics and in-
teraction usually means language model cost that
cannot be calculated offline. Here, we introduce
our two implementations to calculate word-based
reordering model scores in this decoding algo-
rithm.
First, we explain a naive implementation shown
in the left side of Figure 4. This algorithm per-
forms the same calculation of reordering model as
that of language model. Each item keeps a part of
reordered source sentence. The reordering score
of new item can be calculated as interaction cost
when combining subitems with the rule.
The right side of Figure 4 shows our pro-
posed implementation. This implementation can
be adopted to decoding only when we do not use
context features like surrounding word pos fea-
tures in Table 3 (and consider a distance between
words in features). If a span is given, the reorder-
ing scores of new item can be calculated for each
rule, being independent from the word order of
reordered source segment of a subitem. So, the
reordering model scores can be calculated for all
rules with spans by using a part of the input source
sentence before sorting them for cube pruning.
We expect this sorting of rules with reordering
442
Figure 4: The ?naive? and ?proposed? implementation to calculate the reordering cost of new items.
model scores will have good influence on cube
pruning. The right hand side of Figure 4 shows
the diffrence between naive and proposed imple-
mentation (S? is not shown to allow for a clear pre-
sentation). Note the difference is in where/when
the reordering scores are inserted: together with
the N -gram scores in the case of naive implemen-
tation; incorpolated into sorted rules for the pro-
posed implementation.
4 Experiment
4.1 Purpose
To reveal the effectiveness of integrating the re-
ordering model into decoder, we compared the
following setups:
? baseline: a standard hierarchical phrase-
based machine translation (Hiero) system.
? preprocessing: applied Tromble and Eisner?s
approach, then translate by Hiero system.
? Hiero system + reordering model: integrated
reordering model into Hiero system.
We used the Joshua Decoder (Li and Khudanpur,
2008) as the baseline Hiero system. This decoder
uses a log-linear model with seven features, which
consist of N -gram language model PLM (T ), lex-
ical translation model Pw(?|?), Pw(?|?), rule
translation model P (?|?), P (?|?), word penalty
and arity penalty.
The ?Hiero + Reordering model? system has
word-based reordering model as an additional fea-
ture to baseline features. For this approach, we
use two systems. One has ?move-to-front? sys-
tem and the other is ?attach? system explained in
Section 3.2. We implemented our proposed algo-
rithm in Section 3.4 to both ?Hiero + Reordering
model? systems. As for beam width, we use the
same setups for each system.
4.2 Data Set
Data Sent. Word. Avg. leng
Training ja 200.8K 2.4M 12.0
en 200.8K 2.3M 11.5
Development ja 1.0K 10.3K 10.3
en 1.0K 9.8K 9.8
Test ja 1.0K 14.2K 14.2
en 1.0K 13.5K 13.5
Table 4: The Data statistics
For experiments we used a Japanese-English
basic travel expression corpus (BTEC). Japanese
word order is linguistically very different from
English and we think Japanese-English pair is
a very good test bed for evaluating reordering
model.
443
XXXXXXXXXXXSystem
Metrics BLEU PER
Baseline (Hiero) 28.09 39.68
Preprocessing 17.32 45.27
Hiero + move-to-front 28.85 39.89
Hiero + attach 29.25 39.43
Table 5: BLEU and PER scores on the test set.
Our training corpus contains about 200.8k sen-
tences. Using the training corpus, we extracted
hierarchical phrase rules and trained 4-gram lan-
guage model and word-based reordering model.
Parameters were tuned over 1.0k sentences (devel-
opment data) with single reference by minimum
error rate training (MERT) (Och, 2003). Test data
consisted of 1.0k sentences with single reference.
Table 4 shows the condition of corpus in detail.
4.3 Results
Table 5 shows the BLEU (Papineni et al, 2001)
and PER (Niesen et al, 2000) scores obtained by
each system. The results clearly indicated that
our proposed system with word-based reorder-
ing model (move-to-front or attach) outperformed
baseline system on BLEU scores. In contrast,
there is no significant improvement from baseline
on PER. This suggests that the improvement of
BLEU mainly comes from reordering. In our ex-
periment, preprocessing approach resulted in very
poor scores.
4.4 Discussion
Table 6 displays examples showing the cause of
the improvements of our system with reordering
model (attach) comparing to baseline system. We
can see that the outputs of our system are more
fluent than those of baseline system because of re-
ordering model.
As a further analysis, we calculated the BLEU
scores of Japanese S? predicted from reorder-
ing model against true Japanese S? made from
GIZA++ alignments, were only 26.2 points on de-
velopment data. We think the poorness mainly
comes from unaligned words since they are un-
tractable for the word-based reordering model.
Actually, Japanese sentences in our training data
include 34.7% unaligned words. In spite of the
poorness, our proposed method effectively utilize
this reordering model in contrast to preprocessing
approach.
5 Related Work
Our approach is similar to preprocessing approach
(Xia and McCord, 2004; Collins et al, 2005; Li
et al, 2007; Tromble and Eisner, 2009) in that it
reorders source sentence in target order. The dif-
ference is this sentence reordering is done in de-
coding rather than in preprocessing.
A lot of studies on lexicalized reordering (Till-
man, 2004; Koehn et al, 2005; Nagata et al,
2006) focus on the phrase-based model. These
works cannnot be directly applied to hierarchi-
cal phrase-based model because of the difference
between normal phrases and hierarchical phrases
that includes nonterminal symbols.
Shen et al (2008,2009) proposed a way to inte-
grate dependency structure into target and source
side string on hierarchical phrase rules. This ap-
proach is similar to our approach in extending the
formalism of rules on hierarchical phrase-based
model in order to consider the constraint of word
order. But, our approach differs from (Shen et al,
2008; Shen et al, 2009) in that syntax annotation
is not necessary.
6 Conclusion and Future Work
We proposed a method to integrate word-based
reordering model into hierarchical phrase-based
machine translation system. We add ?? into the
hiero rules, but this does not increase the num-
ber of rules. So, this extension itself does not af-
fect the search space of decoding. In this paper
we used Tromble and Eisner?s reordering model
for our method, but various reordering model can
be incorporated to our method, for example S?
N -gram language model. Our experimental re-
sults on Japanese-to-English task showed that our
system outperformed baseline system and prepro-
cessing approach.
In this paper we utilize ?? only for reorder-
ing model. However, it is possible to use ?? for
other modeling, for example we can use it for
rule translation probabilities P (?? |?), P (?|??) for
additional feature functions. Of course, we can
444
S america de seihin no hanbai wo hajimeru keikaku ga ari masu ka . kono tegami wa koukuubin de nihon made ikura kakari masu ka .
TB sales of product in america are you planning to start ? this letter by airmail to japan . how much is it ?
TP are you planning to start products in the u.s. ? how much does it cost to this letter by airmail to japan ?
R do you plan to begin selling your products in the u.s. ? how much will it cost to send this letter by air mail to japan ?
Table 6: Examples of outputs for input sentence S from baseline system TB and our proposed sys-
tem (attach) TP . R is a reference. The underlined portions have equivalent meanings and show the
reordering differences.
also utilize reordered target sentence T ? for vari-
ous modeling as well. Addtionally we plan to use
S? for MERT because we hypothesize the fluent
S? leads to fluent T .
References
AI-Onaizan, Y. and K. Papineni. 2006. Distortion
models for statistical machine translation. In Proc.
the 44th ACL, pages 529?536.
Brown, P. F., S. A. D. Pietra, V. D. J. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguitics, 19:263?312.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proc. NAACL, pages 216?226.
Chiang, D. 2007. Hierachical phrase-based transla-
tion. Computational Linguitics, 33:201?228.
Collins, M., P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proc. the 43th ACL, pages 531?540.
Collins, M. 2002. Discriminative training methods for
hidden markov models. In Proc. of EMNLP.
Freund, Y. and R. E. Schapire. 1996. Experiments
with a new boosting algorithm. In Proc. of the 13th
ICML, pages 148?156.
Koehn, P., A. Axelrod, A-B. Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Ed-
inburgh system description for 2005 iwslt speech
translation evaluation. In Proc. the 2nd IWSLT.
Li, Z. and S. Khudanpur. 2008. A scalable decoder
for parsing-based machine translation with equiv-
alent language model state maintenance. In Proc.
ACL SSST.
Li, C-H., D. Zhang, M. Li, M. Zhou, K. Li, and
Y. Guan. 2007. A probabilistic approach to syntax-
based reordering for statistical machine translation.
In Proc. the 45th ACL, pages 720?727.
McDonald, R., K. Crammer, and F. Pereira. 2005.
Spanning tree methods for discriminative training of
dependency parsers. In Thechnical Report MS-CIS-
05-11, UPenn CIS.
Nagata, M., K. Saito, K. Yamamoto, and K. Ohashi.
2006. A clustered global phrase reordering model
for statistical machine translation. In COLING-
ACL, pages 713?720.
Niesen, S., F.J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast
evaluation for mt research. In Proc. the 2nd In-
ternational Conference on Language Resources and
Evaluation.
Och, F. J. and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. In Proc. the 40th ACL, pages 295?
302.
Och, F. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Och, F. J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. the 41th ACL,
pages 160?167.
Papineni, K. A., S. Roukos, T. Ward, and W-J. Zhu.
2001. Bleu: a method for automatic evaluation of
machine translation. In Proc. the 39th ACL, pages
311?318.
Shen, L., J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
ACL, pages 577?585.
Shen, L., J. Xu, B. Zhang, S. Matsoukas, and
R. Weischedel. 2009. Effective use of linguistic and
contextual information for statistical machine trans-
lation. In Proc. EMNLP, pages 72?80.
Tillman, C. 2004. A unigram orientation model
for statistical machine translation. In Proc. HLT-
NAACL, pages 101?104.
Tromble, R. and J. Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
445
Watanabe, T., H. Tsukada, and H. Isozaki. 2006. Left-
to-right target generation for hierarchical phrase-
based translation. In Proc. COLING-ACL, pages
777?784.
Xia, F. and M. McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite
patterns. In Proc. the 18th ICON, pages 508?514.
446
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944?952,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Translation Quality for Distant Language Pairs
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,hirao,kevinduh,sudoh,tsukada}@cslab.kecl.ntt.co.jp
Abstract
Automatic evaluation of Machine Translation
(MT) quality is essential to developing high-
quality MT systems. Various evaluation met-
rics have been proposed, and BLEU is now
used as the de facto standard metric. How-
ever, when we consider translation between
distant language pairs such as Japanese and
English, most popular metrics (e.g., BLEU,
NIST, PER, and TER) do not work well. It
is well known that Japanese and English have
completely different word orders, and special
care must be paid to word order in transla-
tion. Otherwise, translations with wrong word
order often lead to misunderstanding and in-
comprehensibility. For instance, SMT-based
Japanese-to-English translators tend to trans-
late ?A because B? as ?B because A.? Thus,
word order is the most important problem
for distant language translation. However,
conventional evaluation metrics do not sig-
nificantly penalize such word order mistakes.
Therefore, locally optimizing these metrics
leads to inadequate translations. In this pa-
per, we propose an automatic evaluation met-
ric based on rank correlation coefficients mod-
ified with precision. Our meta-evaluation of
the NTCIR-7 PATMT JE task data shows that
this metric outperforms conventional metrics.
1 Introduction
Automatic evaluation of machine translation (MT)
quality is essential to developing high-quality ma-
chine translation systems because human evaluation
is time consuming, expensive, and irreproducible. If
we have a perfect automatic evaluation metric, we
can tune our translation system for the metric.
BLEU (Papineni et al, 2002b; Papineni et al,
2002a) showed high correlation with human judg-
ments and is still used as the de facto standard au-
tomatic evaluation metric. However, Callison-Burch
et al (2006) argued that the MT community is overly
reliant on BLEU by showing examples of poor per-
formance. For Japanese-to-English (JE) translation,
Echizen-ya et al (2009) showed that the popular
BLEU and NIST do not work well by using the sys-
tem outputs of the NTCIR-7 PATMT (patent transla-
tion) JE task (Fujii et al, 2008). On the other hand,
ROUGE-L (Lin and Hovy, 2003), Word Error Rate
(WER), and IMPACT (Echizen-ya and Araki, 2007)
worked better.
In these studies, Pearson?s correlation coefficient
and Spearman?s rank correlation ? with human eval-
uation scores are used to measure how closely an
automatic evaluation method correlates with human
evaluation. This evaluation of automatic evaluation
methods is called meta-evaluation. In human eval-
uation, people judge the adequacy and the fluency of
each translation.
Denoual and Lepage (2005) pointed out that
BLEU assumes word boundaries, which is ambigu-
ous in Japanese and Chinese. Here, we assume
the word boundaries given by ChaSen, one of the
standard morphological analyzers (http://chasen-
legacy.sourceforge.jp/) following Fujii et al
(2008)
In JE translation, most Statistical Machine Trans-
lation (SMT) systems translate the Japanese sen-
tence
(J0) kare wa sono hon wo yonda node
sekaishi ni kyoumi ga atta
which means
944
(R0) he was interested in world
history because he read the book
into an English sentence such as
(H0) he read the book because he was
interested in world history
in which the cause and the effect are swapped. Why
does this happen? The former half of (J0) means ?He
read the book,? and the latter half means ?(he) was
interested in world history.? The middle word
?node? between them corresponds to ?because.?
Therefore, SMT systems output sentences like (H0).
On the other hand, Rule-based Machine Translation
(RBMT) systems correctly give (R0).
In order to find (R0), SMT systems have to search
a very large space because we cannot restrict its
search space with a small distortion limit. Most
SMT systems thus fail to find (R0).
Consequently, the global word order is essential
for translation between distant language pairs, and
wrong word order can easily lead to misunderstand-
ing or incomprehensibility. Perhaps, some readers
do not understand why we emphasize word order
from this example alone. A few more examples
will clarify what happens when SMT is applied to
Japanese-to-English translation. Even the most fa-
mous SMT service available on the web failed to
translate the following very simple sentence at the
time of writing this paper.
Japanese: meari wa jon wo koroshita.
Reference: Mary killed John.
SMT output: John killed Mary.
Since it cannot translate such a simple sentence, it
obviously cannot translate more complex sentences
correctly.
Japanese: bobu ga katta hon wo jon wa yonda.
Reference: John read a book that Bob bought.
SMT output: Bob read the book John bought.
Another example is:
Japanese: bobu wa meari ni yubiwa wo kau
tameni, jon no mise ni itta.
Reference: Bob went to John?s store to buy a
ring for Mary.
SMT output: Bob Mary to buy the ring, John
went to the store.
In this way, this SMT service usually gives incom-
prehensible or misleading translations, and thus peo-
ple prefer RBMT services. Other SMT systems also
tend to make similar word order mistakes, and spe-
cial care should be paid to the translation between
distant language pairs such as Japanese and English.
Even Japanese people cannot solve this word or-
der problem easily: It is well known that Japanese
people are not good at speaking English.
From this point of view, conventional automatic
evaluation metrics of translation quality disregard
word order mistakes too much. Single-reference
BLEU is defined by a geometrical mean of n-gram
precisions pn and is modified by Brevity Penalty
(BP) min(1, exp(1? r/h)), where r is the length of
the reference and h is the length of the hypothesis.
BLEU = BP? (p1p2p3p4)
1/4.
Its range is [0, 1]. The BLEU score of (H0) with ref-
erence (R0) is 1.0?(11/11?9/10?6/9?4/8)1/4 =
0.740. Therefore, BLEU gives a very good score to
this inadequate translation because it checks only n-
grams and does not regard global word order.
Since (R0) and (H0) look similar in terms of flu-
ency, adequacy is more important than fluency in
the translation between distant language pairs.
Similarly, other popular scores such as NIST,
PER, and TER (Snover et al, 2006) also give
relatively good scores to this translation. NIST
also considers only local word orders (n-grams).
PER (Position-Independent Word Error Rate) was
designed to disregard word order completely.
TER (Snover et al, 2006) was designed to allow
phrase movements without large penalties. There-
fore, these standard metrics are not optimal for eval-
uating translation between distant language pairs.
In this paper, we propose an alternative automatic
evaluation metric appropriate for distant language
pairs. Our method is based on rank correlation co-
efficients. We use them to compare the word ranks
in the reference with those in the hypothesis.
There are two popular rank correlation coeffi-
cients: Spearman?s ? and Kendall?s ? (Kendall,
1975). In Isozaki et al (2010), we used Kendall?s ?
to measure the effectiveness of our Head Finaliza-
tion rule as a preprocessor for English-to-Japanese
translation, but we measured the quality of transla-
tion by using conventional metrics.
945
It is not clear how well ? works as an automatic
evaluation metric of translation quality. Moreover,
Spearman?s ? might work better than Kendall?s ? .
As we discuss later, ? considers only the direction
of the rank change, whereas ? considers the distance
of the change.
The first objective of this paper is to examine
which is the better metric for distant language pairs.
The second objective is to find improvements of
these rank correlation-metrics.
Spearman?s ? is based on Pearson?s correlation
coefficients. Suppose we have two lists of numbers
x = [0.1, 0.4, 0.2, 0.6],
y = [0.9, 0.6, 0.2, 0.7].
To obtain Pearson?s coefficients between x and y,
we use the raw values in these lists. If we substitute
their ranks for their raw values, we get
x? = [1, 3, 2, 4] and y? = [4, 2, 1, 3].
Then, Spearman?s ? between x and y is given by
Pearson?s coefficients between x? and y?. This ?
can be rewritten as follows when there is no tie:
? = 1?
?
i d
2
i
n+1C3
.
Here, di indicates the difference in the ranks of the
i-th element. Rank distances are squared in this
formula. Because of this square, we expect that ?
decreases drastically when there is an element that
significantly changes in rank. But we are also afraid
that ? may be too severe for alternative good trans-
lations.
Since Pearson?s correlation metric assumes lin-
earity, nonlinear monotonic functions can change
its score. On the other hand, Spearman?s ? and
Kendall?s ? uses ranks instead of raw evaluation
scores, and simple application of monotonic func-
tions cannot change them (use of other operations
such as averaging sentence scores can change them).
2 Methodology
2.1 Word alignment for rank correlations
We have to determine word ranks to obtain rank cor-
relation coefficients. Suppose we have:
(R1) John hit Bob yesterday
(H1) Bob hit John yesterday
The 1st word ?John? in R1 becomes the 3rd word
in H1. The 2nd word ?hit? in R1 becomes the 2nd
word in H1. The 3rd word ?Bob? in R1 becomes the
1st word in H1. The 4th word ?yesterday? in R1 be-
comes the 4th word in H1. Thus, we get H1?s word
order list [3, 2, 1, 4]. The number of all pairs of in-
tegers in this list is 4C2 = 6. It has three increasing
pairs: (3,4), (2,4), and (1,4). Since Kendall?s ? is
given by:
? = 2?
the number of increasing pairs
the number of all pairs
? 1,
H1?s ? is 2? 3/6? 1 = 0.0.
In this case, we can obtain Spearman?s ? as fol-
lows: ?John? moved by d1 = 2 words, ?hit? moved
by d2 = 0 words, ?Bob? moved by d3 = 2 words,
and ?yesterday? moved by d4 = 0 words. Therefore,
H1?s ? is 1? (22 + 02 + 22 + 02)/5C3 = 0.2.
Thus, ? considers only the direction of the move-
ment, whereas ? considers the distance of the move-
ment. Both ? and ? have the same range [?1, 1]. The
main objective of this paper is to clarify which rank
correlation is closer to human evaluation scores.
We have to consider the limitation of the rank cor-
relation metrics. They are defined only when there
is one-to-one correspondence. However, a refer-
ence sentence and a hypothesis sentence may have
different numbers of words. They may have two or
more occurrences of the same word in one sentence.
Sometimes, a word in the reference does not appear
in the hypothesis, or a word in the hypothesis does
not appear in the reference. Therefore, we cannot
calculate ? and ? following the above definitions in
general.
Here, we determine the correspondence of words
between hypotheses and references as follows. First,
we find one-to-one corresponding words. That is,
we find words that appear in both sentences and only
once in each sentence. Suppose we have:
(R2) the boy read the book
(H2) the book was read by the boy
By removing non-aligned words by one-to-one cor-
respondence, we get:
946
(R3) boy read book
(H3) book read boy
Thus, we lost ?the.? We relax this one-to-one cor-
respondence constraint by using one-to-one corre-
sponding bigrams. (R2) and (H2) share ?the boy?
and ?the book,? and we can align these instances of
?the? correctly.
(R4) the1 boy2 read3 the4 book5
(H4) the4 book5 read3 the1 boy2
Now, we have five aligned words, and H4?s word
order is represented by [4, 5, 3, 1, 2].
In returning to H0 and R0, we find that each of
these sentences has eleven words. Almost all words
are aligned by one-to-one correspondence but ?he?
is not aligned because it appears twice in each sen-
tence. By considering one-to-one corresponding bi-
grams (?he was? and ?he read?), ?he? is aligned as
follows.
(R5) he1 was2 interested3 in4 world5
history6 because7 he8 read9 the10
book11
(H5) he8 read9 the10 book11 because7
he1 was2 interested3 in4 world5
history6
H5?s word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6].
The number of increasing pairs is: 4C2 = 6 pairs in
[8, 9, 10, 11] and 6C2 = 15 pairs in [1, 2, 3, 4, 5,
6]. Then we obtain ? = 2 ? (6 + 15)/11C2 ? 1 =
?0.236. On the other hand,
?
i d
2
i = 5
2 ? 6 + 22 +
72 ? 4 = 350, and we obtain ? = 1 ? 350/12C3 =
?0.591.
Therefore, both Spearman?s ? and Kendall?s ?
give very bad scores to the misleading translation
H0. This fact implies they are much better metrics
than BLEU, which gave a good score to it. ? is much
lower than ? as we expected.
In general, we can use higher-order n-grams for
this alignment, but here we use only unigrams and
bigrams for simplicity. This algnment algorithm is
given in Figure 1. Since some hypothesis words do
not have corresponding reference words, the output
integer list worder is sometimes shorter than the
evaluated sentence. Therefore, we should not use
worder[i] ? i as di directly. We have to renumber
the list by rank as we did in Section 1.
Read a hypothesis sentence h = h1h2 . . . hm
and its reference sentence r = r1r2 . . . rn.
Initialize worder with an empty list.
For each word hi in h:
? If hi appears only once each in h and r, append j
s.t. rj = hi to worder.
? Otherwise, if the bigram hihi+1 appears only once
each in h and r, append j s.t. rjrj+1 = hihi+1 to
worder.
? Otherwise, if the bigram hi?1hi appears only once
each in h and r, append j s.t. rj?1rj = hi?1hi to
worder.
Return worder.
Figure 1: Word alignment algorithm for rank correlation
2.2 Word order metrics and meta-evaluation
metrics
These rank correlation metrics sometimes have neg-
ative values. In order to make them just like other
automatic evaluation metrics, we normalize them as
follows.
? Normalized Kendall?s ? : NKT = (? + 1)/2.
? Normalized Spearman?s ?: NSR = (?+ 1)/2.
Accordingly, NKT is 0.382 and NSR is 0.205.
These metrics are defined only when the number
of aligned words is two or more. We define both
NKT and NSR as zero when the number is one or
less. Consequently, these normalized metrics have
the same range [0, 1].
In order to avoid confusion, we use these abbre-
viations (NKT and NSR) when we use rank corre-
lations as word order metrics, because these cor-
relation metrics are also used in the machine trans-
lation community for meta-evaluation. For meta-
evaluation, we use Spearman?s ? and Pearson?s cor-
relation coefficient and call them ?Spearman? and
?Pearson,? respectively.
2.3 Overestimation problem
Since we measure the rank correlation of only cor-
responding words, these metrics will overestimate
the correlation. For instance, a hypothesis sentence
might have only two corresponding words among
947
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
BP (brevity penalty)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
? ?
?
?
? ?
?
?
?
?
?
?
?
??
?
?
?
?? ??
?
?
?
??
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
??
?
?
?
??
?
? ?
?
??
?
?
?
?
? ? ?
? ? ? ??
?
?
?
? ?
?
?
?
?? ??
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
? ?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
P (precision)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?? ?
?
?
?
?
?
?
?
??
?
? ?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
??
?
???
?
??? ?
?
?
? ?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
??
?
?
?
?
??
?
?
?
?
?
?
?
? ?
? ?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
?? ?
?
???
?
?
??
?
??
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?? ?
?
?
?
?
?
??
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
?
?
?
?
? ??
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
??
?
? ?
??
?
?
?
?
?
Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right).
(Each ? corresponds to one sentence generated by one MT system)
dozens of words. In this case, these two words
determine the score of the whole sentence. If the
two words appear in their order in the reference,
the whole sentence obtains the best score, NSR =
NKT = 1.0, in spite of the fact that only two words
matched.
Solving this overestimation problem is the second
objective of this paper. BLEU uses ?Brevity Penalty
(BP)? (Section 1) to reduce the scores of too-short
sentences. We can combine the above word order
metrics with BP, e.g., NKT? BP and NSR? BP.
However, we cannot very much expect from this
solution because BP scores do not correlate with
human judgments well. The left graph of Figure
2 shows a scatter plot of BP and ?normalized av-
erage adequacy.? This graph has 15 (systems) ?
100 (sentences) dots. Each dot (?) corresponds to
one sentence from one translation system.
In the NTCIR-7 data, three human judges gave
five-point scores (1, 2, 3, 4, 5) for ?adequacy? and
?fluency? of each translated sentence. Although
each system translated 1,381 sentences, only 100
sentences were evaluated by the judges.
For each translated sentence, we averaged three
judges? adequacy scores and normalized this aver-
age x by (x?1)/4. This is our ?normalized average
adequacy,? and the dots appears only at multiples of
1/3? 1/4.
This graph shows that BP has very little correla-
tion with adequacy, and we cannot expect BP to im-
prove the meta-evaluation performance very much.
Perhaps, BP?s poor performance was caused by the
fact that most MT systems output almost the same
number of words, and if the number exceeds the
length of the reference, BP=1.0 holds.
Therefore, we have to consider other modifiers
for this overestimation problem. We can use other
common metrics such as precision, recall, and F-
measure to reduce the overestimation of NSR and
NKT.
? Precision: P = c/|h|, where c is the number of
corresponding words and |h| is the number of
words in the hypothesis sentence h.
? Recall: R = c/r, where |r| is the number of
words in the reference sentence r.
? F-measure: F? = (1 + ?2)PR/(?2P + R),
where ? is a parameter.
In (R2)&(H2)?s case, precision is 5/7 = 0.714 and
recall is 5/5 = 1.000.
Which metric should we use? Our preliminary
experiments with NTCIR-7 data showed that preci-
sion correlated best with adequacy among these
three metrics (P , R, and F?=1). In addition, BLEU
is essentially made for precision. Therefore, preci-
sion seems the most promising modifier.
The right graph of Figure 2 shows a scatter plot
of precision and normalized average adequacy. The
graph shows that precision has more correlation with
adequacy than BP. We can observe that sentences
with very small P values usually obtain very low
adequacy scores but those with mediocre P values
often obtain good adequacy scores.
948
If we multiply P directly by NSR or NKT, those
sentences with mediocre P values will lose too
much of their scores. The use of
?
x will miti-
gate this problem. Since
?
P is closer to 1.0 than
P itself, multiplication of
?
P instead of P itself
will save these sentences. If we apply
?
x twice
(
??
P = 4
?
P ), it will further save them. There-
fore, we expect?
?
P and? 4
?
P to work better than
?P . Now, we propose two new metrics:
NSRP? and NKTP?,
where ? is a parameter (0 ? ? ? 1).
3 Experiments
3.1 Meta-evaluation with NTCIR-7 data
In order to compare automatic translation evalua-
tion methods, we use submissions to the NTCIR-7
Patent Translation (PATMT) task (Fujii et al, 2008).
Fourteen MT systems participated in the Japanese-
English intrinsic evaluation. There were two Rule-
Based MT (RMBT) systems and one Example-
based MT (EBMT) system. All other systems were
Statistical MT (SMT) systems. The task organiz-
ers provided a baseline SMT system. These 15 sys-
tems translated 1,381 Japanese sentences into En-
glish. The organizers evaluated these translations by
using BLEU and human judgments. In the human
judgements, three experts independently evaluated
100 selected sentences in terms of ?adequacy? and
?fluency.?
For automatic evaluation, we used a single refer-
ence sentence for each of these 100 manually evalu-
ated sentences. Echizen-ya et al (2009) used multi-
reference data, but their data is not publicly available
yet.
For this meta-evaluation, we measured the
corpus-level correlation between the human evalua-
tion scores and the automatic evaluation scores. We
simply averaged scores of 100 sentences for the pro-
posed metrics. For existing metrics such as BLEU,
we followed their definitions for corpus-level eval-
uation instead of simple averages of sentence-level
scores. We used default settings for conventional
metrics, but we tuned GTM (Melamed et al, 2007)
with -e option. This option controls preferences
on longer word runs. We also used the para-
phrase database TERp (http://www.umiacs.umd.
edu/?snover/terp) for METEOR (Banerjee and
Lavie, 2005).
3.2 Meta-evaluation with WMT-07 data
We developed our metric mainly for automatic eval-
uation of translation quality for distant language
pairs such as Japanese-English, but we also want
to know how well the metric works for similar lan-
guage pairs. Therefore, we also use the WMT-
07 data (Callison-Burch et al, 2007) that covers
only European language pairs. Callison-Burch et al
(2007) tried different human evaluation methods and
showed detailed evaluation scores. The Europarl test
set has 2,000 sentences, and The News Commentary
test set has 2,007 sentences.
This data has different language pairs: Spanish,
French, German ? English. We exclude Czech-
English because there were so few systems (See the
footnote of p. 146 in their paper).
4 Results
4.1 Meta-evaluation with NTCIR-7 data
Table 1 shows the main results of this paper. The
left part has corpus-level meta-evaluation with ade-
quacy. Error metrics, WER, PER, and TER, have
negative correlation coefficients, but we did not
show their minus signs here.
Both NSR-based metrics and NKT-based metrics
perform better than conventional metrics for this NT-
CIR PATMT JE translation data. As we expected,
?BP and ?P (1/1) performed badly. Spearman of
BP itself is zero.
NKT performed slightly better than NSR. Per-
haps, NSR penalized alternative good translations
too much. However, one of the NSR-based metrics,
NSRP 1/4, gave the best Spearman score of 0.947,
and the difference between NSRP? and NKTP?
was small. Modification with P led to this improve-
ment.
NKT gave the best Pearson score of 0.922. How-
ever, Pearson measures linearity and we can change
its score through a nonlinear monotonic function
without changing Spearman very much. For in-
stance, (NSRP 1/4)1.5 also has Spearman of 0.947
but its Pearson is 0.931, which is better than NKT?s
0.922. Thus, we think Spearman is a better meta-
evaluation metric than Pearson.
949
Table 1: NTCIR-7 Meta-evaluation: correlation with hu-
man judgments (Spm = Spearman, Prs = Pearson)
human judge Adequacy Fluency
eval\ meta-eval Spm Prs Spm Prs
P 0.615 0.704 0.672 0.876
R 0.436 0.669 0.461 0.854
F?=1 0.525 0.692 0.543 0.871
BP 0.000 0.515 -0.007 0.742
NSR 0.904 0.906 0.869 0.910
NSRP 1/8 0.937 0.905 0.890 0.934
NSRP 1/4 0.947 0.900 0.901 0.944
NSRP 1/2 0.937 0.890 0.926 0.949
NSRP 1/1 0.883 0.872 0.883 0.939
NSR ? BP 0.851 0.874 0.769 0.910
NKT 0.940 0.922 0.887 0.931
NKTP 1/8 0.940 0.913 0.908 0.944
NKTP 1/4 0.940 0.904 0.908 0.949
NKTP 1/2 0.929 0.890 0.897 0.949
NKTP 1/1 0.897 0.869 0.879 0.936
NKT ? BP 0.829 0.878 0.726 0.918
ROUGE-L 0.903 0.874 0.889 0.932
ROUGE-S(4) 0.593 0.757 0.640 0.869
IMPACT 0.797 0.813 0.751 0.932
WER 0.894 0.822 0.836 0.926
TER 0.854 0.806 0.372 0.856
PER 0.375 0.642 0.393 0.842
METEOR(TERp) 0.490 0.708 0.508 0.878
GTM(-e 12) 0.618 0.723 0.601 0.850
NIST 0.343 0.661 0.372 0.856
BLEU 0.515 0.653 0.500 0.795
The right part of Table 1 shows correlation with
fluency, but adequacy is more important, because
our motivation is to provide a metric that is useful to
reduce incomprehensible or misunderstanding out-
puts of MT systems. Again, the correlation-based
metrics gave better scores than conventional metrics,
and BP performed badly. NSR-based metrics proved
to be as good as NKT-based metrics.
Meta-evaluation scores of the de facto standard
BLEU is much lower than those of other metrics.
Echizen-ya et al (2009) reported that IMPACT per-
formed very well for sentence-level evaluation of
NTCIR-7 PATMT JE data. This corpus-level result
also shows that IMPACT works better than BLEU,
but ROUGE-L, WER, and our methods give better
scores than IMPACT.
Table 2: WMT-07 meta-evaluation: Each source lan-
guage has two columns: the left one is News Corpus and
the right one is Europarl.
Spearman?s ? with human ?rank?
source French Spanish German
NSR 0.775 0.837 0.523 0.766 0.700 0.593
NSRP 1/8 0.821 0.857 0.786 0.595 0.400 0.685
NSRP 1/4 0.821 0.857 0.786 0.455 0.400 0.714
NSRP 1/2 0.821 0.857 0.786 0.347 0.400 0.714
NKT 0.845 0.857 0.607 0.838 0.700 0.630
NKTP 1/8 0.793 0.857 0.786 0.595 0.400 0.714
NKTP 1/4 0.793 0.857 0.786 0.524 0.400 0.714
NKTP 1/2 0.793 0.857 0.786 0.347 0.400 0.714
BLEU 0.786 0.679 0.750 0.595 0.400 0.821
WER 0.607 0.857 0.750 0.429 0.000 0.500
ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857
ROUGES 0.883 0.679 0.786 0.690 0.400 0.929
4.2 Meta-evaluation with WMT-07 data
Callison-Burch et al (2007) have performed differ-
ent human evaluation methods for different language
pairs and different corpora. Their Table 5 shows
inter-annotator agreements for the human evaluation
methods. According to their table, the ?sentence
ranking? (or ?rank?) method obtained better agree-
ment than ?adequacy.? Therefore, we show Spear-
man?s ? for ?rank.? We used the scores given in
their Tables 9, 10, and 11. (The ?constituent? meth-
ods obtained the best inter-annotator agreement, but
these methods focus on local translation quality and
have nothing to do with global word order, which we
are discussing here.)
Table 2 shows that our metrics designed for
distant language pairs are comparable to conven-
tional methods even for similar language pairs, but
ROUGE-L and ROUGE-S performed better than
ours for French News Corpus and German Europarl.
BLEU scores in this table agree with those in Table
17 of Callison-Burch et al (2007) within rounding
errors.
After some experiments, we noticed that the use
ofR instead of P often gives better scores for WMT-
07, but it degrades NTCIR-7 scores. We can extend
our metric by F? , weighted harmonic mean of P and
R, or any other interpolation, but the introduction
of new parameters into our metric makes it difficult
950
to control. Improvement without new parameters is
beyond the scope of this paper.
5 Discussion
It has come to our attention that Birch et al (2010)
has independently proposed an automatic evaluation
method based on Kendall?s ? . First, they started
with Kendall?s ? distance, which can be written as
?1?NKT? in our terminology, and then subtracted
it from one. Thus, their metric is nothing but NKT.
Then, they proposed application of the square root
to get better Pearson by improving ?the sensitivity
to small reorderings.? Since they used ?Kendall?s ??
and ?Kendall?s ? distance? interchangeably, it is not
clear what they mean by ?
?
Kendall?s ? ,? but per-
haps they mean 1 ?
?
1?NKT because
?
NKT is
more insensitive to small reorderings. Table 3 shows
the performance of these metrics for NTCIR-7 data.
Pearson?s correlation coefficient with adequacy was
improved by 1 ?
?
1? NKT, but other scores were
degraded in this experiment.
The difference between our method and Birch et
al. (2010)?s method comes from the fact that we
used Japanese-English translation data and Spear-
man?s correlation for meta-evaluation, whereas they
used Chinese-English translation data and only Pear-
son?s correlation for meta-evaluation. Chinese word
order is different from English, but Chinese is a
Subject-Verb-Object (SVO) language and thus is
much closer to English word order than Japanese,
a typical SOV language.
We preferred NSR because it penalizes global
word order mistakes much more than does NKT, and
as discussed above, global word order mistakes of-
ten lead to incomprehensibility and misunderstand-
ing.
On the other hand, they also tried Hamming dis-
tance, and summarized their experiments as follows:
However, the Hamming distance seems to
be more informative than Kendall?s tau for
small amounts of reordering.
This sentence and the introduction of the square root
to NKT imply that Chinese word order is close to
that of English, and they have to measure subtle
word order mistakes.
Table 3: NTCIR-7 meta-evaluation: Effects of square
root (b(x) = 1?
?
1? x)
NKT
?
NKT b(NKT)
Spearman w/ adequacy 0.940 0.940 0.922
Pearson w/ adequacy 0.922 0.817 0.941
Spearman w/ fluency 0.887 0.865 0.858
Pearson w/ fluency 0.931 0.917 0.833
In spite of these differences, the two groups inde-
pendently recognized the usefulness of rank correla-
tions for automatic evaluation of translation quality
for distant language pairs.
In their WMT-2010 paper (Birch and Osborne,
2010), they multiplied NKT with the brevity penalty
and interpolated it with BLEU for the WMT-2010
shared task. This fact implies that incomprehensible
or misleading word order mistakes are rare in trans-
lation among European languages.
6 Conclusions
When Statistical Machine Translation is applied to
distant language pairs such as Japanese and English,
word order becomes an important problem. SMT
systems often fail to find an appropriate translation
because of a large search space. Therefore, they
often output misleading or incomprehensible sen-
tences such as ?A because B? vs. ?B because A.? To
penalize such inadequate translations, we presented
an automatic evaluation method based on rank corre-
lation. There were two questions for this approach.
First, which correlation coefficient should we use:
Spearman?s ? or Kendall?s ?? Second, how should
we solve the overestimation problem caused by the
nature of one-to-one correspondence?
We answered these questions through our exper-
iments using the NTCIR-7 PATMT JE translation
data. For the first question, ? was slightly better
than ?, but ? was improved by precision. For the
second question, it turned out that BLEU?s Brevity
Penalty was counter-productive. A precision-based
penalty gave a better solution. With this precision-
based penalty, both ? and ? worked well and they
outperformed conventional methods for NTCIR-7
data. For similar language pairs, our method was
comparable to conventional evaluation methods. Fu-
951
ture work includes extension of the method so that it
can outperform conventional methods even for sim-
ilar language pairs.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgements. In Proc. of ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and Summarization, pages 65?72.
Alexandra Birch and Miles Osborne. 2010. LRscore for
evaluating lexical and reordering quality in MT. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 327?
332.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluatiing the role of Bleu in ma-
chine translation research. In Proc. of the Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Chrstof Monz, and Josh Schroeder. 2007.
(Meta-)Evaluation of machine translation. In Proc. of
the Workshop on Machine Translation (WMT), pages
136?158.
Etienne Denoual and Yves Lepage. 2005. BLEU in char-
acters: towards automatic MT evaluation in languages
without word delimiters. In Companion Volume to the
Proceedings of the Second International Joint Confer-
ence on Natural Language Processing, pages 81?86.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In Proceedings of MT Summit XII Workshop on Patent
Translation, pages 151?158.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata,
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, and Noriko Kando. 2009. Meta-
evaluation of automatic evaluation methods for ma-
chine translation using patent translation data in ntcir-
7. In Proceedings of the 3rd Workshop on Patent
Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 250?257.
Maurice G. Kendall. 1975. Rank Correlation Methods.
Charles Griffin.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proc. of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 71?78.
Dan Melamed, Ryan Green, and Joseph P. Turian. 2007.
Precision and recall of machine translation. In Proc.
of NAACL-HLT, pages 61?63.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002a. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish Results. In
Proc. of the International Conference on Human Lan-
guage Technology Research (HLT), pages 132?136.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002b. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
952
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1382?1386,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Shift-Reduce Word Reordering for Machine Translation
Katsuhiko Hayashi?, Katsuhito Sudoh, Hajime Tsukada, Jun Suzuki, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
?hayashi.katsuhiko@lab.ntt.co.jp
Abstract
This paper presents a novel word reordering
model that employs a shift-reduce parser for
inversion transduction grammars. Our model
uses rich syntax parsing features for word re-
ordering and runs in linear time. We apply it to
postordering of phrase-based machine trans-
lation (PBMT) for Japanese-to-English patent
tasks. Our experimental results show that our
method achieves a significant improvement
of +3.1 BLEU scores against 30.15 BLEU
scores of the baseline PBMT system.
1 Introduction
Even though phrase-based machine translation
(PBMT) (Koehn et al, 2007) and tree-based MT
(Graehl and Knight, 2004; Chiang, 2005; Galley
et al, 2006) systems have achieved great success,
many problems remain for distinct language pairs,
including long-distant word reordering.
To improve such word reordering, one promis-
ing way is to separate it from the translation pro-
cess as preordering (Collins et al, 2005; DeNero
and Uszkoreit, 2011) or postordering (Sudoh et al,
2011; Goto et al, 2012). Many studies utilize a rule-
based or a probabilistic model to perform a reorder-
ing decision at each node of a syntactic parse tree.
This paper presents a parser-based word reorder-
ing model that employs a shift-reduce parser for in-
version transduction grammars (ITG) (Wu, 1997).
To the best of our knowledge, this is the first study
on a shift-reduce parser for word reordering.
The parser-based reordering approach uses rich
syntax parsing features for reordering decisions.
Our propoesd method can also easily define such
.
Source-
ordered Target
Sentence (HFE)
Source Sen-
tence (J)
Target Sen-
tence (E)
reordering
Figure 1: A description of the postordering MT system.
non-local features as theN -gram words of reordered
strings. Even when using these non-local features,
the complexity of the shift-reduce parser does not
increase at all due to give up achieving an optimal
solution. Therefore, it works much more efficient.
In our experiments, we apply our proposed
method to postordering for J-to-E patent tasks be-
cause their training data for reordering have little
noise and they are ideal for evaluating reordering
methods. Although our used J-to-E setups need
a language-dependent scheme and we describe our
proposed method as a J-to-E postordering method,
the key algorithm is language-independent and it can
be applicable to preordering as well as postordering
if the training data for reordering are available.
2 Postordering by Parsing
As shown in Fig.1, postordering (Sudoh et al, 2011)
has two steps; the first is a translation step that trans-
lates an input sentence into source-ordered transla-
tions. The second is a reordering step in which the
translations are reordered in the target language or-
der. The key to postordering is the second step.
Goto et al (2012) modeled the second step by
parsing and created training data for a postordering
parser using a language-dependent rule called head-
finalization. The rule moves syntactic heads of a
lexicalized parse tree of an English sentence to the
1382
.S(saw)
. .VP(saw)
. .PP(with)
. .NP(telescope)
. .N(telescope)
.telescope.
D(a)
.a
.
PR(with)
.with
.
VP(saw)
. .NP(girl)
. .N(girl)
.girl.
D(a)
.a
.
V(saw)
.saw
.
NP(I)
.N(I)
.I
.
. .mita. .wo.shoujyo.de.bouenkyo. .wawatashi
.S(saw)
. .VP#(saw)
. .VP#(saw)
. .V(saw)
.saw
.
NP(wo)?a/an?
. .WO(wo)
.wo
N(girl)
.girl.
PP#(with)
. .PR(with)
.with
NP(telescope)?a/an?
.N(telescope)
.telescope
.
NP(wa)?no articles?
. .WA(wa)
.wa.
N(I)
.I
.
. .mita. .wo.shoujyo.de.bouenkyo. .wawatashi
Figure 2: An example of the head-finzalizaton process for an English-Japanese sentence pair: the left-hand side tree
is the original English tree, and the right-hand side tree is its head-final English tree.
end of the corresponding syntactic constituents. As
a result, the terminal symbols of the English tree are
sorted in a Japanese-like order. In Fig.2, we show an
example of head-finalization and a tree on the right-
hand side is a head-finalized English (HFE) tree of
an English tree on the left-hand side. We annotate
each parent node of the swapped edge with # sym-
bol. For example, a nonterminal symbol PP#(with)
shows that a noun phrase ?a/an telescope? and a
word ?with? are inverted.
For better word alignments, Isozaki et al (2012)
also deleted articles ?the? ?a? ?an? from English be-
cause Japanese has no articles, and inserted Japanese
particles ?ga? ?wo? ?wa? into English sentences.
We privilege the nonterminals of a phrase modified
by a deleted article to determine which ?the? ?a/an?
or ?no articles? should be inserted at the front of the
phrase. Note that an original English sentence can
be recovered from its HFE tree by using # symbols
and annotated articles and deleting Japanese parti-
cles.
As well as Goto et al (2012), we solve postorder-
ing by a parser whose model is trained with a set
of HFE trees. The main difference between Goto et
al. (2012)?s model and ours is that while the former
simply used the Berkeley parser (Petrov and Klein,
2007), our shift-reduce parsing model can use such
non-local task specific features as theN -gram words
of reordered strings without sacrificing efficiency.
Our method integrates postediting (Knight and
Chander, 1994) with reordering and inserts articles
into English translations by learning an additional
?insert? action of the parser. Goto et al (2012)
solved the article generation problem by using an
N -gram language model, but this somewhat compli-
cates their approach. Compared with other parsers,
one advantage of the shift-reduce parser is to easily
define such additional operations as ?insert?.
HFE trees can be defined as monolingual ITG
trees (DeNero and Uszkoreit, 2011). Our monolin-
gual ITG G is a tuple G = (V, T, P, I, S) where V
is a set of nonterminals, T is a set of terminals, P
is a set of production rules, I is a set of nontermi-
nals on which ?the? ?a/an? or ?no articles? must be
determined, and S is the start symbol.
Set P consists of terminal production rules that
are responsible for generating word w(? T ):
X ? w
and binary production rules in two forms:
X ? YZ
X# ? YZ
where X, X#, Y and Z are nonterminals. On
the right-hand side, the second rule generates two
phrases Y and Z in the reverse order. In our experi-
ments, we removed all unary production rules.
3 Shift-Reduce Parsing
Given an input sentence w1 . . . wn, the shift-reduce
parser uses a stack of partial derivations, a buffer of
input words, and a set of actions to build a parse tree.
The following is the parser?s configuration:
? : ?i, j, S? : pi
where ? is the step size, S is a stack of elements
s0, s1, . . . , i is the leftmost span index of the stack
1383
top element s0, j is an index of the next input word
of the buffer, and pi is a set of predictor states1.
Each stack element has at least the following com-
ponents of its partial derivation tree:
s = {H, h, wleft, wright, a}
where H is a root nonterminal or a part-of-speech tag
of the subtree, h is a head index of H, a is a variable
to which ?the? ?a/an? ?no articles? or null are as-
signed, and wleft, wright are the leftmost and right-
most words of phrase H. When referring to compo-
nent ?, we use a s.? notation.
Our proposed system has 4 actions shift-X, insert-
x, reduce-MR-X and reduce-SR-X.
The shift-X action pushes the next input word
onto the stack and assigns a part-of-speech tag X to
the word. The deduction step is as follows:
X ? wj ? P
p
? ?? ?
? : ?i, j, S|s?0? : pi
? + 1 : ?j, j + 1, S|s?0|s0)? : {p}
where s0 is {X, j, wj , wj , null}.
The insert-x action determines whether to gener-
ate ?the? ?a/an? or ?no articles? (= x):
s?0.X ? I ? (s?0.a ?= ?the? ? s?0.a ?= ?a/an?)
? : ?i, j, S|s?0)? : pi
? + 1 : ?i, j, S|s0? : pi
where s?0 is {X, h, wleft, wright, a} and s0 is
{X, h, wleft, wright, x} (i ? h, left, right < j).
The side condition prevents the parser from inserting
articles into phrase X more than twice. During pars-
ing, articles are not explicitly inserted into the input
string: they are inserted into it when backtracking to
generate a reordered string after parsing.
The reduce-MR-X action has a deduction rule:
X ? Y Z ? P ? q ? pi
q
? ?? ?
: ?k, i, S|s?2|s?1? : pi? ? : ?i, j, S|s?1|s?0? : pi
? + 1 : ?k, j, S|s?2|s0? : pi?
1Since our notion of predictor states is identical to that in
(Huang and Sagae, 2010), we omit the details here.
s0.wh ? s0.th s0.H s0.H ? s0.th s0.wh ? s0.H
s1.wh ? s1.th s1.H s1.H ? s1.th s1.wh ? s1.H
s2.th ? s2.H s2.wh ? s2.H q0.w q1.w q2.w
s0.tl ? s0.L s0.wl ? s0.L s1.tl ? s1.L s1.wl ? s1.L
s0.wh ? s0.H ? s1.wh ? s1.H s0.H ? s1.wh s0.wh ? s1.H
s0.H ? s1.H s0.wh ? s0.H ? q0.w s0.H ? q0.w
s1.wh ? s1.H ? q0.w s1.H ? q0.w s1.th ? q0.w ? q1.w
s0.wh ? s0.H ? s1.H ? q0.w s0.H ? s1.wh ? s1.H ? q0.w
s0.H ? s1.H ? q0.w s0.th ? s1.th ? q0.w
s0.wh ? s1.H ? q0.w ? q1.w s0.H ? q0.w ? q1.w
s0.th ? q0.w ? q1.w s0.wh ? s0.H ? s1.H ? s2.H
s0.H ? s1.wh ? s1.H ? s2.H s0.H ? s1.H ? s2.wh ? s2.H
s0.H ? s1.H ? s2.H s0.th ? s1.th ? s2.th
s0.H ? s0.R ? s0.L s1.H ? s1.R ? s1.L s0.H ? s0.R ? q0.w
s0.H ? s0.L ? s1.H s0.H ? s0.L ? s1.wh s0.H ? s1.H ? s1.L
s0.wh ? s1.H ? s1.R
s0.wleft ? s1.wright s0.tleft ? s1.tright
s0.wright ? s1.wleft s0.tright ? s1.tleft
s0.a ? s0.wleft s0.a ? s0.tleft s0.a ? s0.wleft ? s1.wright
s0.a ? s0.tleft ? s1.tright s0.a ? s0.wh s0.a ? s0.th
Table 1: Feature templates: s.L and s.R denote the left
and right subnodes of s. l and r are head indices of L and
R. q denotes a buffer element. t is a part-of-speech tag.
where s?0 is {Z, h0, wleft0, wright0, a0} and s?1 is
{Y, h1, wleft1, wright1, a1}. The action generates s0
by combining s?0 and s?1 with binary rule X?Y Z:
s0 = {X, h0, wleft1, wright0, a1}.
New nonterminal X is lexicalized with head word
wh0 of right nonterminal Z. This action expands Y
and Z in a straight order. The leftmost word of
phrase X is set to leftmost word wleft1 of Y, and the
rightmost word of phrase X is set to rightmost word
wright0 of Z. Variable a is set to a1 of Y.
The difference between reduce-MR-X and
reduce-SR-X actions is new stack element s0. The
reduce-SR-X action generates s0 by combining s?0
and s?1 with binary rule X# ?Y Z:
s0 = {X#, h0, wleft0, wright1, a0}.
This action expands Y and Z in a reverse order, and
the leftmost word of X# is set to wleft0 of Z, and the
rightmost word of X# is set to wright1 of Y. Variable
a is set to a0 of Z.
We use a linear model that is discriminatively
trained with the averaged perceptron (Collins and
Roark, 2004). Table 1 shows the feature templates
used in our experiments and we call the features in
the bottom two rows ?non-local? features.
1384
train dev test9 test10
# of sent. 3,191,228 2,000 2,000 2,300
ave. leng. (J) 36.4 36.6 37.0 43.1
ave. leng. (E) 33.3 33.3 33.7 39.6
Table 2: NTCIR-9 and 10 data statistics.
4 Experiments
4.1 Experimental Setups
We conducted experiments for NTCIR-9 and 10
patent data using a Japanese-English language pair.
Mecab2 was used for the Japanese morphological
analysis. The data are summarized in Table 2.
We used Enju (Miyao and Tsujii, 2008) for pars-
ing the English training data and converted parse
trees into HFE trees by a head-finalization scheme.
We extracted grammar rules from all the HFE trees
and randomly selected 500,000 HFE trees to train
the shift-reduce parser.
We used Moses (Koehn et al, 2007) with lexical-
ized reordering and a 6-gram language model (LM)
trained using SRILM (Stolcke et al, 2011) to trans-
late the Japanese sentences into HFE sentences.
To recover the English sentences, our shift-reduce
parser reordered only the 1-best HFE sentence. Our
strategy is much simpler than Goto et al (2012)?s
because they used a linear inteporation of MT cost,
parser cost and N -gram LM cost to generate the best
English sentence from the n-best HFE sentences.
4.2 Main Results
The main results in Table 3 indicate our method was
significantly better and faster than the conventional
PBMT system. Our method also ourperformed Goto
et al (2012)?s reported systems as well as a tree-
based (moses-chart) system3. Our proposed model
with ?non-local? features (w/ nf.) achieved gains
against that without the features (w/o nf.). Further
feature engineering may improve the accuracy more.
4.3 Analysis
We show N -gram precisions of PBMT (dist=6,
dist=20) and proposed systems in Table 5. The re-
sults clearly show that improvements of 1-gram pre-
2https://code.google.com/p/mecab/
3All the data and the MT toolkits used in our experiments
are the same as theirs.
test9 test10
BLEU RIBES BLEU RIBES
HFE w/ art. 28.86 73.45 29.9 73.52
proposed 32.93 76.68 33.25 76.74
w/o art. 19.86 75.62 20.17 75.63
N -gram 32.15 76.52 32.28 76.46
Table 4: The effects of article generation: ?w/o art.? de-
notes evaluation scores for translations of the best system
(?proposed?) in Table 3 from which articles are removed.
?HFE w/ art.? system used HFE data with articles and
generated them by MT system and the shift-reduce parser
performed only reordering. ?N -gram? system inserted
articles into the translations of ?w/o art.? by Goto et al
(2012)?s article generation method.
(1?4)-gram precision
moses (dist=6) 67.1 / 36.9 / 20.7 / 11.5
moses (dist=20) 67.7 / 38.9 / 23.0 / 13.7
proposed 68.9 / 40.6 / 25.7 / 16.7
Table 5: N -gram precisions of moses (dist=6, dist=20)
and proposed systems for test9 data.
cisions are the main factors that contribute to bet-
ter performance of our proposed system than PBMT
systems. It seems that the gains of 1-gram presicions
come from postediting (article generation).
In table 4, we show the effectiveness of our joint
reordering and postediting approach (?proposed?).
The ?w/o art.? results clearly show that generating
articles has great effects on MT evaluations espe-
cially for BLEU metric. Comparing ?proposed? and
?HFEw/ art.? systems, these results show that poste-
diting is much more effective than generating arti-
cles by MT. Our joint approach also outperformed
?N -gram? postediting system.
5 Conclusion
We proposed a shift-reduce word ordering model
and applied it to J-to-E postordering. Our experi-
mental results indicate our method can significantly
improve the performance of a PBMT system.
Future work will investigate our method?s use-
fulness on various language datasets. We plan to
study more general methods that use word align-
ments to embed swap information in trees (Galley
et al, 2006).
1385
test9 test10
BLEU RIBES time (sec.) BLEU RIBES time (sec.)
PBMT (dist=6) 27.1 67.76 2.66 27.92 68.13 3.18
PBMT (dist=12) 29.55 69.84 4.15 30.03 69.88 4.93
PBMT (dist=20) 29.98 69.87 6.22 30.15 69.43 7.19
Tree-based MT** (Goto et al, 2012) 29.53 69.22 ? ? ? ?
PBMT (dist=20)** (Goto et al, 2012) 30.13 68.86 ? ? ? ?
Goto et al (2012)** 31.75 72.57 ? ? ? ?
PBMT (dist=0) + proposed w/o nf. (beam=12) 32.59 76.35 1.46 + 0.01 32.83 76.44 1.7 + 0.01
PBMT (dist=0) + proposed w/o nf. (beam=48) 32.61 76.58 1.46 + 0.06 32.86 76.6 1.7 + 0.06
PBMT (dist=0) + proposed w/ nf. (beam=12) 32.91 76.38 1.46 + 0.01 33.15 76.53 1.7 + 0.02
PBMT (dist=0) + proposed w/ nf. (beam=48) 32.93 76.68 1.46 + 0.07 33.25 76.74 1.7 + 0.07
Table 3: System comparison: time represents the average second per sentence. ** denotes ?not our experiments?.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 111.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531?540.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 193?203.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
961?968.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statisti-
cal machine translation. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 311?316.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086.
Hideki Isozaki, Jun Suzuki, Hajime Tsukada, Masaaki
Nagata, Sho Hoshino, and Yusuke Miyao. 2012.
HPSG-based preprocessing for English-to-Japanese
translation. ACM Transactions on Asian Language In-
formation Processing (TALIP), 11(3).
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
National Conference on Artificial Intelligence, pages
779?779.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177?180.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North American
chapter of the Association for Computational Linguis-
tics, pages 404?411.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition
and Understanding Workshop.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering in
statistical machine translation. In Proc. MT Summit.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
1386
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning to Translate with Multiple Objectives
Kevin Duh? Katsuhito Sudoh Xianchao Wu Hajime Tsukada Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
kevinduh@is.naist.jp, lastname.firstname@lab.ntt.co.jp
Abstract
We introduce an approach to optimize a ma-
chine translation (MT) system on multiple
metrics simultaneously. Different metrics
(e.g. BLEU, TER) focus on different aspects
of translation quality; our multi-objective ap-
proach leverages these diverse aspects to im-
prove overall quality.
Our approach is based on the theory of Pareto
Optimality. It is simple to implement on top of
existing single-objective optimization meth-
ods (e.g. MERT, PRO) and outperforms ad
hoc alternatives based on linear-combination
of metrics. We also discuss the issue of metric
tunability and show that our Pareto approach
is more effective in incorporating new metrics
from MT evaluation for MT optimization.
1 Introduction
Weight optimization is an important step in build-
ing machine translation (MT) systems. Discrimi-
native optimization methods such as MERT (Och,
2003), MIRA (Crammer et al, 2006), PRO (Hop-
kins and May, 2011), and Downhill-Simplex (Nelder
and Mead, 1965) have been influential in improving
MT systems in recent years. These methods are ef-
fective because they tune the system to maximize an
automatic evaluation metric such as BLEU, which
serve as surrogate objective for translation quality.
However, we know that a single metric such as
BLEU is not enough. Ideally, we want to tune to-
wards an automatic metric that has perfect corre-
lation with human judgments of translation quality.
?*Now at Nara Institute of Science & Technology (NAIST)
While many alternatives have been proposed, such a
perfect evaluation metric remains elusive.
As a result, many MT evaluation campaigns now
report multiple evaluation metrics (Callison-Burch
et al, 2011; Paul, 2010). Different evaluation met-
rics focus on different aspects of translation quality.
For example, while BLEU (Papineni et al, 2002)
focuses on word-based n-gram precision, METEOR
(Lavie and Agarwal, 2007) allows for stem/synonym
matching and incorporates recall. TER (Snover
et al, 2006) allows arbitrary chunk movements,
while permutation metrics like RIBES (Isozaki et
al., 2010; Birch et al, 2010) measure deviation in
word order. Syntax (Owczarzak et al, 2007) and se-
mantics (Pado et al, 2009) also help. Arguably, all
these metrics correspond to our intuitions on what is
a good translation.
The current approach of optimizing MT towards
a single metric runs the risk of sacrificing other met-
rics. Can we really claim that a system is good if
it has high BLEU, but very low METEOR? Simi-
larly, is a high-METEOR low-BLEU system desir-
able? Our goal is to propose a multi-objective op-
timization method that avoids ?overfitting to a sin-
gle metric?. We want to build a MT system that
does well with respect to many aspects of transla-
tion quality.
In general, we cannot expect to improve multi-
ple metrics jointly if there are some inherent trade-
offs. We therefore need to define the notion of Pareto
Optimality (Pareto, 1906), which characterizes this
tradeoff in a rigorous way and distinguishes the set
of equally good solutions. We will describe Pareto
Optimality in detail later, but roughly speaking, a
1
hypothesis is pareto-optimal if there exist no other
hypothesis better in all metrics. The contribution of
this paper is two-fold:
? We introduce PMO (Pareto-based Multi-
objective Optimization), a general approach for
learning with multiple metrics. Existing single-
objective methods can be easily extended to
multi-objective using PMO.
? We show that PMO outperforms the alterna-
tive (single-objective optimization of linearly-
combined metrics) in multi-objective space,
and especially obtains stronger results for met-
rics that may be difficult to tune individually.
In the following, we first explain the theory of
Pareto Optimality (Section 2), and then use it to
build up our proposed PMO approach (Section 3).
Experiments on NIST Chinese-English and PubMed
English-Japanese translation using BLEU, TER, and
RIBES are presented in Section 4. We conclude by
discussing related work (Section 5) and opportuni-
ties/limitations (Section 6).
2 Theory of Pareto Optimality
2.1 Definitions and Concepts
The idea of Pareto optimality comes originally from
economics (Pareto, 1906), where the goal is to char-
acterize situations when a change in allocation of
goods does not make anybody worse off. Here, we
will explain it in terms of MT:
Let h ? L be a hypothesis from an N-best list L.
We have a total of K different metrics Mk(h) for
evaluating the quality of h. Without loss of gen-
erality, we assume metric scores are bounded be-
tween 0 and 1, with 1 being perfect. Each hypoth-
esis h can be mapped to a K-dimensional vector
M(h) = [M1(h);M2(h); ...;MK(h)]. For exam-
ple, suppose K = 2, M1(h) computes the BLEU
score, and M2(h) gives the METEOR score of h.
Figure 1 illustrates the set of vectors {M(h)} in a
10-best list.
For two hypotheses h1, h2, we write M(h1) >
M(h2) if h1 is better than h2 in all metrics, and
M(h1) ? M(h2) if h1 is better than or equal
to h2 in all metrics. When M(h1) ? M(h2) and
Mk(h1) > Mk(h2) for at least one metric k, we say
that h1 dominates h2 and write M(h1) . M(h2).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
metric1
me
tric2
Figure 1: Illustration of Pareto Frontier. Ten hypotheses
are plotted by their scores in two metrics. Hypotheses
indicated by a circle (o) are pareto-optimal, while those
indicated by a plus (+) are not. The line shows the convex
hull, which attains only a subset of pareto-optimal points.
The triangle (4) is a point that is weakly pareto-optimal
but not pareto-optimal.
Definition 1. Pareto Optimal: A hypothesis h? ?
L is pareto-optimal iff there does not exist another
hypothesis h ? L such that M(h) . M(h?).
In Figure 1, the hypotheses indicated by circle
(o) are pareto-optimal, while those with plus (+) are
not. To visualize this, take for instance the pareto-
optimal point (0.4,0.7). There is no other point with
either (metric1 > 0.4 and metric2 ? 0.7), or (met-
ric1 ? 0.4 and metric2 > 0.7). On the other hand,
the non-pareto point (0.6,0.4) is ?dominated? by an-
other point (0.7,0.6), because for metric1: 0.7 > 0.6
and for metric2: 0.6 > 0.4.
There is another definition of optimality, which
disregards ties and may be easier to visualize:
Definition 2. Weakly Pareto Optimal: A hypothesis
h? ? L is weakly pareto-optimal iff there is no other
hypothesis h ? L such that M(h) > M(h?).
Weakly pareto-optimal points are a superset of
pareto-optimal points. A hypothesis is weakly
pareto-optimal if there is no other hypothesis that
improves all the metrics; a hypothesis is pareto-
optimal if there is no other hypothesis that improves
at least one metric without detriment to other met-
rics. In Figure 1, point (0.1,0.8) is weakly pareto-
optimal but not pareto-optimal, because of the com-
peting point (0.3,0.8). Here we focus on pareto-
optimality, but note our algorithms can be easily
2
modified for weakly pareto-optimality. Finally, we
can introduce the key concept used in our proposed
PMO approach:
Definition 3. Pareto Frontier: Given an N-best list
L, the set of all pareto-optimal hypotheses h ? L is
called the Pareto Frontier.
The Pareto Frontier has two desirable properties
from the multi-objective optimization perspective:
1. Hypotheses on the Frontier are equivalently
good in the Pareto sense.
2. For each hypothesis not on the Frontier, there
is always a better (pareto-optimal) hypothesis.
This provides a principled approach to optimiza-
tion: i.e. optimizing towards points on the Frontier
and away from those that are not, and giving no pref-
erence to different pareto-optimal hypotheses.
2.2 Reduction to Linear Combination
Multi-objective problems can be formulated as:
arg max
w
[M1(h);M2(h); . . . ;Mk(h)] (1)
where h = Decode(w, f)
Here, the MT system?s Decode function, parame-
terized by weight vector w, takes in a foreign sen-
tence f and returns a translated hypothesis h. The
argmax operates in vector space and our goal is to
find w leading to hypotheses on the Pareto Frontier.
In the study of Pareto Optimality, one central
question is: To what extent can multi-objective prob-
lems be solved by single-objective methods? Equa-
tion 1 can be reduced to a single-objective problem
by scalarizing the vector [M1(h); . . . ;Mk(h)] with
a linear combination:
arg max
w
K?
k=1
pkMk(h) (2)
where h = Decode(w, f)
Here, pk are positive real numbers indicating the rel-
ative importance of each metric (without loss of gen-
erality, assume
?
k pk = 1). Are the solutions to
Eq. 2 also solutions to Eq. 1 (i.e. pareto-optimal)
and vice-versa? The theory says:
Theorem 1. Sufficient Condition: If w? is solution
to Eq. 2, then it is weakly pareto-optimal. Further,
if w? is unique, then it is pareto-optimal.
Theorem 2. No Necessary Condition: There may
exist solutions to Eq. 1 that cannot be achieved by
Eq. 2, irregardless of any setting of {pk}.
Theorem 1 is a positive result asserting that lin-
ear combination can give pareto-optimal solutions.
However, Theorem 2 states the limits: in partic-
ular, Eq. 2 attains only pareto-optimal points that
are on the convex hull. This is illustrated in Fig-
ure 1: imagine sweeping all values of p1 = [0, 1]
and p2 = 1? p1 and recording the set of hypotheses
that maximizes
?
k pkMk(h). For 0.6 < p1 ? 1 we
get h = (0.9, 0.1), for p1 = 0.6 we get (0.7, 0.6),
and for 0 < p1 < 0.6 we get (0.4, 0.8). At no
setting of p1 do we attain h = (0.4, 0.7) which
is also pareto-optimal but not on the convex hull.1
This may have ramifications for issues like metric
tunability and local optima. To summarize, linear-
combination is reasonable but has limitations. Our
proposed approach will instead directly solve Eq. 1.
Pareto Optimality and multi-objective optimiza-
tion is a deep field with active inquiry in engineer-
ing, operations research, economics, etc. For the in-
terested reader, we recommend the survey by Mar-
ler and Arora (2004) and books by (Sawaragi et al,
1985; Miettinen, 1998).
3 Multi-objective Algorithms
3.1 Computing the Pareto Frontier
Our PMO approach will need to compute the Pareto
Frontier for potentially large sets of points, so we
first describe how this can be done efficiently. Given
a set of N vectors {M(h)} from an N-best list L,
our goal is extract the subset that are pareto-optimal.
Here we present an algorithm based on iterative
filtering, in our opinion the simplest algorithm to
understand and implement. The strategy is to loop
through the list L, keeping track of any dominant
points. Given a dominant point, it is easy to filter
out many points that are dominated by it. After suc-
cessive rounds, any remaining points that are not fil-
1We note that scalarization by exponentiated-combination
?
k pkMk(h)
q , for a suitable q > 0, does satisfy necessary
conditions for pareto optimality. However the proper tuning of q
is not known a priori. See (Miettinen, 1998) for theorem proofs.
3
Algorithm 1 FindParetoFrontier
Input: {M(h)}, h ? L
Output: All pareto-optimal points of {M(h)}
1: F = ?
2: while L is not empty do
3: h? = shift(L)
4: for each h in L do
5: if (M(h?) . M(h)): remove h from L
6: else if (M(h) . M(h?)): remove h from L; set
h? = h
7: end for
8: Add h? to Frontier Set F
9: for each h in L do
10: if (M(h?) . M(h)): remove h from L
11: end for
12: end while
13: Return F
tered are necessarily pareto-optimal. Algorithm 1
shows the pseudocode. In line 3, we take a point h?
and check if it is dominating or dominated in the for-
loop (lines 4-8). At least one pareto-optimal point
will be found by line 8. The second loop (lines 9-11)
further filters the list for points that are dominated by
h? but iterated before h? in the first for-loop.
The outer while-loop stops exactly after P iter-
ations, where P is the actual number of pareto-
optimal points in L. Each inner loop costs O(KN)
so the total complexity is O(PKN). Since P ? N
with the actual value depending on the probability
distribution of {M(h)}, the worst-case run-time is
O(KN2). For a survey of various Pareto algorithms,
refer to (Godfrey et al, 2007). The algorithm we de-
scribed here is borrowed from the database literature
in what is known as skyline operators.2
3.2 PMO-PRO Algorithm
We are now ready to present an algorithm for multi-
objective optimization. As we will see, it can be seen
as a generalization of the pairwise ranking optimiza-
tion (PRO) of (Hopkins and May, 2011), so we call
it PMO-PRO. PMO-PRO approach works by itera-
tively decoding-and-optimizing on the devset, sim-
2The inquisitive reader may wonder how is Pareto related
to databases. The motivation is to incorporate preferences into
relational queries(Bo?rzso?nyi et al, 2001). For K = 2 metrics,
they also present an alternative faster O(N logN) algorithm by
first topologically sorting along the 2 dimensions. All domi-
nated points can be filtered by one-pass by comparing with the
most-recent dominating point.
ilar to many MT optimization methods. The main
difference is that rather than trying to maximize a
single metric, we maximize the number of pareto
points, in order to expand the Pareto Frontier
We will explain PMO-PRO in terms of the
pseudo-code shown in Algorithm 2. For each sen-
tence pair (f, e) in the devset, we first generate an
N-best list L ? {h} using the current weight vector
w (line 5). In line 6, we evaluate each hypothesis
h with respect to the K metrics, giving a set of K-
dimensional vectors {M(h)}.
Lines 7-8 is the critical part: it gives a ?la-
bel? to each hypothesis, based on whether it is
in the Pareto Frontier. In particular, first we call
FindParetoFrontier (Algorithm 1), which re-
turns a set of pareto hypotheses; pareto-optimal hy-
potheses will get label 1 while non-optimal hypothe-
ses will get label 0. This information is added to
the training set T (line 8), which is then optimized
by any conventional subroutine in line 10. We will
follow PRO in using a pairwise classifier in line 10,
which finds w? that separates hypotheses with labels
1 vs. 0. In essence, this is the trick we employ to
directly optimize on the Pareto Frontier. If we had
used BLEU scores rather than the {0, 1} labels in
line 8, the entire PMO-PRO algorithm would revert
to single-objective PRO.
By definition, there is no single ?best? result
for multi-objective optimization, so we collect all
weights and return the Pareto-optimal set. In line 13
we evaluate each weight w on K metrics across the
entire corpus and call FindParetoFrontier
in line 14.3 This choice highlights an interesting
change of philosophy: While setting {pk} in linear-
combination forces the designer to make an a priori
preference among metrics prior to optimization, the
PMO strategy is to optimize first agnostically and
a posteriori let the designer choose among a set of
weights. Arguably it is easier to choose among so-
lutions based on their evaluation scores rather than
devising exact values for {pk}.
3.3 Discussion
Variants: In practice we find that a slight modifi-
cation of line 8 in Algorithm 2 leads to more sta-
3Note this is the same FindParetoFrontier algorithm as used
in line 7. Both operate on sets of points in K-dimensional
space, induced from either weights {w} or hypotheses {h}.
4
Algorithm 2 Proposed PMO-PRO algorithm
Input: Devset, max number of iterations I
Output: A set of (pareto-optimal) weight vectors
1: Initialize w. LetW = ?.
2: for i = 1 to I do
3: Let T = ?.
4: for each (f, e) in devset do
5: {h} =DecodeNbest(w,f )
6: {M(h)}=EvalMetricsOnSentence({h}, e)
7: {f} =FindParetoFrontier({M(h)})
8: foreach h ? {h}:
if h ? {f}, set l=1, else l=0; Add (l, h) to T
9: end for
10: w?=OptimizationSubroutine(T , w)
11: Add w? toW; Set w = w?.
12: end for
13: M(w) =EvalMetricsOnCorpus(w,devset) ?w ? W
14: Return FindParetoFrontier({M(w)})
ble results for PMO-PRO: for non-pareto hypothe-
ses h /? {f}, we set label l =
?
kMk(h)/K in-
stead of l= 0, so the method not only learns to dis-
criminate pareto vs. non-pareto but also also learns
to discriminate among competing non-pareto points.
Also, like other MT works, in line 5 the N-best list is
concatenated to N-best lists from previous iterations,
so {h} is a set with i ?N elements.
General PMO Approach: The strategy we out-
lined in Section 3.2 can be easily applied to other
MT optimization techniques. For example, by re-
placing the optimization subroutine (line 10, Algo-
rithm 2) with a Powell search (Och, 2003), one can
get PMO-MERT4. Alternatively, by using the large-
margin optimizer in (Chiang et al, 2009) and mov-
ing it into the for-each loop (lines 4-9), one can
get an online algorithm such PMO-MIRA. Virtually
all MT optimization algorithms have a place where
metric scores feedback into the optimization proce-
dure; the idea of PMO is to replace these raw scores
with labels derived from Pareto optimality.
4 Experiments
4.1 Evaluation Methodology
We experiment with two datasets: (1) The PubMed
task is English-to-Japanese translation of scientific
4A difference with traditional MERT is the necessity of
sentence-BLEU (Liang et al, 2006) in line 6. We use sentence-
BLEU for optimization but corpus-BLEU for evaluation here.
abstracts. As metrics we use BLEU and RIBES
(which demonstrated good human correlation in
this language pair (Goto et al, 2011)). (2) The
NIST task is Chinese-to-English translation with
OpenMT08 training data and MT06 as devset. As
metrics we use BLEU and NTER.
? BLEU = BP ? (?precn)1/4. BP is brevity
penality. precn is precision of n-gram matches.
? RIBES = (? + 1)/2 ? prec1/41 , with Kendall?s
? computed by measuring permutation between
matching words in reference and hypothesis5.
? NTER=max(1?TER, 0), which normalizes
Translation Edit Rate6 so that NTER=1 is best.
We compare two multi-objective approaches:
1. Linear-Combination of metrics (Eq. 2),
optimized with PRO. We search a range
of combination settings: (p1, p2) =
{(0, 1), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3), (1, 0)}.
Note (1, 0) reduces to standard single-metric
optimization of e.g. BLEU.
2. Proposed Pareto approach (PMO-PRO).
Evaluation of multi-objective problems can be
tricky because there is no single figure-of-merit.
We thus adopted the following methodology: We
run both methods 5 times (i.e. using the 5 differ-
ent (p1, p2) setting each time) and I = 20 iterations
each. For each method, this generates 5x20=100 re-
sults, and we plot the Pareto Frontier of these points
in a 2-dimensional metric space (e.g. see Figure 2).
A method is deemed better if its final Pareto Fron-
tier curve is strictly dominating the other. We report
devset results here; testset trends are similar but not
included due to space constraints.7
5from www.kecl.ntt.co.jp/icl/lirg/ribes
6from www.umd.edu/?snover/tercom
7An aside: For comparing optimization methods, we believe
devset comparison is preferable to testset since data mismatch
may confound results. If one worries about generalization, we
advocate to re-decode the devset with final weights and evaluate
its 1-best output (which is done here). This is preferable to sim-
ply reporting the achieved scores on devset N-best (as done in
some open-source scripts) since the learned weight may pick
out good hypotheses in the N-best but perform poorly when
re-decoding the same devset. The re-decode devset approach
avoids being overly optimistic while accurately measuring op-
timization performance.
5
Train Devset #Feat Metrics
PubMed 0.2M 2k 14 BLEU, RIBES
NIST 7M 1.6k 8 BLEU, NTER
Table 1: Task characteristics: #sentences in Train/Dev, #
of features, and metrics used. Our MT models are trained
with standard phrase-based Moses software (Koehn and
others, 2007), with IBM M4 alignments, 4gram SRILM,
lexical ordering for PubMed and distance ordering for the
NIST system. The decoder generates 50-best lists each
iteration. We use SVMRank (Joachims, 2006) as opti-
mization subroutine for PRO, which efficiently handle all
pairwise samples without the need for sampling.
4.2 Results
Figures 2 and 3 show the results for PubMed and
NIST, respectively. A method is better if its Pareto
Frontier lies more towards the upper-right hand cor-
ner of the graph. Our observations are:
1. PMO-PRO generally outperforms Linear-
Combination with any setting of (p1, p2).
The Pareto Frontier of PMO-PRO dominates
that of Linear-Combination. This implies
PMO is effective in optimizing towards Pareto
hypotheses.
2. For both methods, trading-off between met-
rics is necessary. For example in PubMed,
the designer would need to make a choice be-
tween picking the best weight according to
BLEU (BLEU=.265,RIBES=.665) vs. another
weight with higher RIBES but poorer BLEU,
e.g. (.255,.675). Nevertheless, both the PMO
and Linear-Combination with various (p1, p2)
samples this joint-objective space broadly.
3. Interestingly, a multi-objective approach can
sometimes outperform a single-objective opti-
mizer in its own metric. In Figure 2, single-
objective PRO focusing on optimizing RIBES
only achieves 0.68, but PMO-PRO using both
BLEU and RIBES outperforms with 0.685.
The third observation relates to the issue of metric
tunability (Liu et al, 2011). We found that RIBES
can be difficult to tune directly. It is an extremely
non-smooth objective with many local optima?slight
changes in word ordering causes large changes in
RIBES. So the best way to improve RIBES is to
0.2 0.21 0.22 0.23 0.24 0.25 0.26 0.270.665
0.67
0.675
0.68
0.685
0.69
0.695
bleu
ribe
s
 
 Linear CombinationPareto (PMO?PRO)
Figure 2: PubMed Results. The curve represents the
Pareto Frontier of all results collected after multiple runs.
0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0.1640.694
0.695
0.696
0.697
0.698
0.699
0.7
0.701
0.702
0.703
0.704
bleu
nte
r
 
 Linear CombinationPareto (PMO?PRO)
Figure 3: NIST Results
not to optimize it directly, but jointly with a more
tunable metric BLEU. The learning curve in Fig-
ure 4 show that single-objective optimization of
RIBES quickly falls into local optimum (at iteration
3) whereas PMO can zigzag and sacrifice RIBES in
intermediate iterations (e.g. iteration 2, 15) leading
to a stronger result ultimately. The reason is the
diversity of solutions provided by the Pareto Fron-
tier. This finding suggests that multi-objective ap-
proaches may be preferred, especially when dealing
with new metrics that may be difficult to tune.
4.3 Additional Analysis and Discussions
What is the training time? The Pareto approach
does not add much overhead to PMO-PRO. While
FindParetoFrontier scales quadratically by size of
N-best list, Figure 5 shows that the runtime is triv-
6
0 2 4 6 8 10 12 14 16 18 200.63
0.64
0.65
0.66
0.67
0.68
0.69
iteration
ribe
s
 
 
Single?Objective RIBES
Pareto (PMO?PRO)
Figure 4: Learning Curve on RIBES: comparing single-
objective optimization and PMO.
0 100 200 300 400 500 600 700 800 900 10000
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Set size |L|
Run
time
 (sec
onds
)
 
 
Algorithm 1
TopologicalSort (footnote 2)
Figure 5: Avg. runtime per sentence of FindPareto
ial (0.3 seconds for 1000-best). Table 2 shows
the time usage breakdown in different iterations for
PubMed. We see it is mostly dominated by decod-
ing time (constant per iteration at 40 minutes on
single 3.33GHz processor). At later iterations, Opt
takes more time due to larger file I/O in SVMRank.
Note Decode and Pareto can be ?embarrasingly par-
allelized.?
Iter Time Decode Pareto Opt Misc.
(line 5) (line 7) (line 10) (line 6,8)
1 47m 85% 1% 1% 13%
10 62m 67% 6% 8% 19%
20 91m 47% 15% 22% 16%
Table 2: Training time usage in PMO-PRO (Algo 2).
How many Pareto points? The number of pareto
0 2 4 6 8 10 12 14 16 185
10
15
20
25
30
35
Iterations
Num
ber 
of P
aret
o P
oint
s
 
 
NIST
PubMed
Figure 6: Average number of Pareto points
hypotheses gives a rough indication of the diversity
of hypotheses that can be exploited by PMO. Fig-
ure 6 shows that this number increases gradually per
iteration. This perhaps gives PMO-PRO more direc-
tions for optimizing around potential local optimal.
Nevertheless, we note that tens of Pareto points is far
few compared to the large size of N-best lists used
at later iterations of PMO-PRO. This may explain
why the differences between methods in Figure 3
are not more substantial. Theoretically, the num-
ber will eventually level off as it gets increasingly
harder to generate new Pareto points in a crowded
space (Bentley et al, 1978).
Practical recommendation: We present the
Pareto approach as a way to agnostically optimize
multiple metrics jointly. However, in practice, one
may have intuitions about metric tradeoffs even if
one cannot specify {pk}. For example, we might
believe that approximately 1-point BLEU degra-
dation is acceptable only if RIBES improves by
at least 3-points. In this case, we recommend
the following trick: Set up a multi-objective prob-
lem where one metric is BLEU and the other is
3/4BLEU+1/4RIBES. This encourages PMO to ex-
plore the joint metric space but avoid solutions that
sacrifice too much BLEU, and should also outper-
form Linear Combination that searches only on the
(3/4,1/4) direction.
5 Related Work
Multi-objective optimization for MT is a relatively
new area. Linear-combination of BLEU/TER is
7
the most common technique (Zaidan, 2009), some-
times achieving good results in evaluation cam-
paigns (Dyer et al, 2009). As far as we known, the
only work that directly proposes a multi-objective
technique is (He and Way, 2009), which modifies
MERT to optimize a single metric subject to the
constraint that it does not degrade others. These
approaches all require some setting of constraint
strength or combination weights {pk}. Recent work
in MT evaluation has examined combining metrics
using machine learning for better correlation with
human judgments (Liu and Gildea, 2007; Albrecht
and Hwa, 2007; Gimnez and Ma`rquez, 2008) and
may give insights for setting {pk}. We view our
Pareto-based approach as orthogonal to these efforts.
The tunability of metrics is a problem that is gain-
ing recognition (Liu et al, 2011). If a good evalu-
ation metric could not be used for tuning, it would
be a pity. The Tunable Metrics task at WMT2011
concluded that BLEU is still the easiest to tune
(Callison-Burch et al, 2011). (Mauser et al, 2008;
Cer et al, 2010) report similar observations, in ad-
dition citing WER being difficult and BLEU-TER
being amenable. One unsolved question is whether
metric tunability is a problem inherent to the metric
only, or depends also on the underlying optimization
algorithm. Our positive results with PMO suggest
that the choice of optimization algorithm can help.
Multi-objective ideas are being explored in other
NLP areas. (Spitkovsky et al, 2011) describe a tech-
nique that alternates between hard and soft EM ob-
jectives in order to achieve better local optimum in
grammar induction. (Hall et al, 2011) investigates
joint optimization of a supervised parsing objective
and some extrinsic objectives based on downstream
applications. (Agarwal et al, 2011) considers us-
ing multiple signals (of varying quality) from online
users to train recommendation models. (Eisner and
Daume? III, 2011) trades off speed and accuracy of
a parser with reinforcement learning. None of the
techniques in NLP use Pareto concepts, however.
6 Opportunities and Limitations
We introduce a new approach (PMO) for training
MT systems on multiple metrics. Leveraging the
diverse perspectives of different evaluation metrics
has the potential to improve overall quality. Based
on Pareto Optimality, PMO is easy to implement
and achieves better solutions compared to linear-
combination baselines, for any setting of combi-
nation weights. Further we observe that multi-
objective approaches can be helpful for optimiz-
ing difficult-to-tune metrics; this is beneficial for
quickly introducing new metrics developed in MT
evaluation into MT optimization, especially when
good {pk} are not yet known. We conclude by draw-
ing attention to some limitations and opportunities
raised by this work:
Limitations: (1) The performance of PMO is
limited by the size of the Pareto set. Small N-best
lists lead to sparsely-sampled Pareto Frontiers, and
a much better approach would be to enlarge the hy-
pothesis space using lattices (Macherey et al, 2008).
How to compute Pareto points directly from lattices
is an interesting open research question. (2) The
binary distinction between pareto vs. non-pareto
points ignores the fact that 2nd-place non-pareto
points may also lead to good practical solutions. A
better approach may be to adopt a graded definition
of Pareto optimality as done in some multi-objective
works (Deb et al, 2002). (3) A robust evaluation
methodology that enables significance testing for
multi-objective problems is sorely needed. This will
make it possible to compare multi-objective meth-
ods on more than 2 metrics. We also need to follow
up with human evaluation.
Opportunities: (1) There is still much we do
not understand about metric tunability; we can learn
much by looking at joint metric-spaces and exam-
ining how new metrics correlate with established
ones. (2) Pareto is just one approach among many
in multi-objective optimization. A wealth of meth-
ods are available (Marler and Arora, 2004) and more
experimentation in this space will definitely lead to
new insights. (3) Finally, it would be interesting to
explore other creative uses of multiple-objectives in
MT beyond multiple metrics. For example: Can we
learn to translate faster while sacrificing little on ac-
curacy? Can we learn to jointly optimize cascaded
systems, such as as speech translation or pivot trans-
lation? Life is full of multiple competing objectives.
Acknowledgments
We thank the reviewers for insightful feedback.
8
References
Deepak Agarwal, Bee-Chung Chen, Pradheep Elango,
and Xuanhui Wang. 2011. Click shaping to optimize
multiple objectives. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?11, pages 132?140,
New York, NY, USA. ACM.
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level mt evalu-
ation. In ACL.
J. L. Bentley, H. T. Kung, M. Schkolnick, and C. D.
Thompson. 1978. On the average number of max-
ima in a set of vectors and applications. Journal of the
Association for Computing Machinery (JACM), 25(4).
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing. Machine Translation, 24(1).
S. Bo?rzso?nyi, D. Kossmann, and K. Stocker. 2001. The
skyline operator. In Proceedings of the 17th Interna-
tional Conference on Data Engineering (ICDE).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Daniel Cer, Christopher Manning, and Daniel Jurafsky.
2010. The best lexical metric for phrase-based statis-
tical MT system optimization. In NAACL HLT.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passiveag-
gressive algorithms. Journal of Machine Learning Re-
search, 7.
Kalyanmoy Deb, Amrit Pratap, Sammer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2).
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Jason Eisner and Hal Daume? III. 2011. Learning speed-
accuracy tradeoffs in nondeterministic inference algo-
rithms. In COST: NIPS 2011 Workshop on Computa-
tional Trade-offs in Statistical Learning.
Jesu?s Gimnez and Llu??s Ma`rquez. 2008. Heterogeneous
automatic mt evaluation through non-parametric met-
ric combinations. In ICJNLP.
Parke Godfrey, Ryan Shipley, and Jarek Gyrz. 2007. Al-
gorithms and analyses for maximal vector computa-
tion. VLDB Journal, 16.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of the NTCIR-9 Workshop Meeting.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1489?1499, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010. Automatic evaluation of translation quality for
distant language pairs. In EMNLP.
T. Joachims. 2006. Training linear SVMs in linear time.
In KDD.
P. Koehn et al 2007. Moses: open source toolkit for
statistical machine translation. In ACL.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for mt evaluation with high levels of cor-
relation with human judgments. In Workshop on Sta-
tistical Machine Translation.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In NAACL.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineering.
Structural and Multidisciplinary Optimization, 26.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
9
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
Kaisa Miettinen. 1998. Nonlinear Multiobjective Opti-
mization. Springer.
J.A. Nelder and R. Mead. 1965. The downhill simplex
method. Computer Journal, 7(308).
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL.
Vilfredo Pareto. 1906. Manuale di Economica Politica,
(Translated into English by A.S. Schwier as Manual of
Political Economy, 1971). Societa Editrice Libraria,
Milan.
Michael Paul. 2010. Overview of the iwslt 2010 evalua-
tion campaign. In IWSLT.
Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo
Tanino, editors. 1985. Theory of Multiobjective Opti-
mization. Academic Press.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen em: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1269?1280, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. In The Prague Bulletin of Mathe-
matical Linguistics.
10
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 100?104,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparative Study of Target Dependency Structures
for Statistical Machine Translation
Xianchao Wu?, Katsuhito Sudoh, Kevin Duh?, Hajime Tsukada, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan
wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp,
kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp
Abstract
This paper presents a comparative study of
target dependency structures yielded by sev-
eral state-of-the-art linguistic parsers. Our ap-
proach is to measure the impact of these non-
isomorphic dependency structures to be used
for string-to-dependency translation. Besides
using traditional dependency parsers, we also
use the dependency structures transformed
from PCFG trees and predicate-argument
structures (PASs) which are generated by an
HPSG parser and a CCG parser. The experi-
ments on Chinese-to-English translation show
that the HPSG parser?s PASs achieved the best
dependency and translation accuracies.
1 Introduction
Target language side dependency structures have
been successfully used in statistical machine trans-
lation (SMT) by Shen et al (2008) and achieved
state-of-the-art results as reported in the NIST 2008
Open MT Evaluation workshop and the NTCIR-9
Chinese-to-English patent translation task (Goto et
al., 2011; Ma and Matsoukas, 2011). A primary ad-
vantage of dependency representations is that they
have a natural mechanism for representing discon-
tinuous constructions, which arise due to long-
distance dependencies or in languages where gram-
matical relations are often signaled by morphology
instead of word order (McDonald and Nivre, 2011).
It is known that dependency-style structures can
be transformed from a number of linguistic struc-
?Now at Baidu Inc.
?Now at Nara Institute of Science & Technology (NAIST)
tures. For example, using the constituent-to-
dependency conversion approach proposed by Jo-
hansson and Nugues (2007), we can easily yield de-
pendency trees from PCFG style trees. A seman-
tic dependency representation of a whole sentence,
predicate-argument structures (PASs), are also in-
cluded in the output trees of (1) a state-of-the-art
head-driven phrase structure grammar (HPSG) (Pol-
lard and Sag, 1994; Sag et al, 2003) parser, Enju1
(Miyao and Tsujii, 2008) and (2) a state-of-the-art
CCG parser2 (Clark and Curran, 2007). The moti-
vation of this paper is to investigate the impact of
these non-isomorphic dependency structures to be
used for SMT. That is, we would like to provide a
comparative evaluation of these dependencies in a
string-to-dependency decoder (Shen et al, 2008).
2 Gaining Dependency Structures
2.1 Dependency tree
We follow the definition of dependency graph and
dependency tree as given in (McDonald and Nivre,
2011). A dependency graph G for sentence s is
called a dependency tree when it satisfies, (1) the
nodes cover all the words in s besides the ROOT;
(2) one node can have one and only one head (word)
with a determined syntactic role; and (3) the ROOT
of the graph is reachable from all other nodes.
For extracting string-to-dependency transfer
rules, we use well-formed dependency structures,
either fixed or floating, as defined in (Shen et al,
2008). Similarly, we ignore the syntactic roles
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
2http://groups.inf.ed.ac.uk/ccg/software.html
100
 when the fluid pressure cylinder 31 is used , fluid is gradually applied . 
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 
c2 c5 c7 c9 c11 c12 c14 c15 c17 c20 c22 c24 c25 
c3 
c4 
c6 
c8 
c10 c13 
c18 
c19 
c21 
c23 
c16 
c1 
c0 
conj_ 
arg12 
det_ 
arg1 
adj_ 
arg1 
noun_ 
arg1 
noun_ 
arg0 
adj_ 
arg1 
aux_ 
arg12 
verb_ 
arg12 
punct_ 
arg1 
noun_ 
arg0 
aux_ 
arg12 
adj_ 
arg1 
verb_ 
arg12 
* + 
* + 
* + 
* 
+ 
* + 
* + 
* + 
*  
* + 
* + 
* + 
* + 
* + 
+ 
Figure 1: HPSG tree of an example sentence. ?*?/
?+?=syntactic/semantic heads. Arrows in red (upper)=
PASs, orange (bottom)=word-level dependencies gener-
ated from PASs, blue=newly appended dependencies.
both during rule extracting and target dependency
language model (LM) training.
2.2 Dependency parsing
Graph-based and transition-based are two predom-
inant paradigms for data-driven dependency pars-
ing. The MST parser (McDonald et al, 2005) and
the Malt parser (Nivre, 2003) stand for two typical
parsers, respectively. Parsing accuracy comparison
and error analysis under the CoNLL-X dependency
shared task data (Buchholz and Marsi, 2006) have
been performed by McDonald and Nivre (2011).
Here, we compare them on the SMT tasks through
parsing the real-world SMT data.
2.3 PCFG parsing
For PCFG parsing, we select the Berkeley parser
(Petrov and Klein, 2007). In order to generate word-
level dependency trees from the PCFG tree, we use
the LTH constituent-to-dependency conversion tool3
written by Johansson and Nugues (2007). The head
finding rules4 are according to Magerman (1995)
and Collins (1997). Similar approach has been orig-
inally used by Shen et al (2008).
2.4 HPSG parsing
In the Enju English HPSG grammar (Miyao et al,
2003) used in this paper, the semantic content of
3http://nlp.cs.lth.se/software/treebank converter/
4http://www.cs.columbia.edu/ mcollins/papers/heads
a sentence/phrase is represented by a PAS. In an
HPSG tree, each leaf node generally introduces a
predicate, which is represented by the pair made up
of the lexical entry feature and predicate type fea-
ture. The arguments of a predicate are designated by
the arrows from the argument features in a leaf node
to non-terminal nodes (e.g., t0?c3, t0?c16).
Since the PASs use the non-terminal nodes in the
HPSG tree (Figure 1), this prevents their direct us-
age in a string-to-dependency decoder. We thus need
an algorithm to transform these phrasal predicate-
argument dependencies into a word-to-word depen-
dency tree. Our algorithm (refer to Figure 1 for an
example) for changing PASs into word-based depen-
dency trees is as follows:
1. finding, i.e., find the syntactic/semantic head
word of each argument node through a bottom-
up traversal of the tree;
2. mapping, i.e., determine the arc directions
(among a predicate word and the syntac-
tic/semantic head words of the argument nodes)
for each predicate type according to Table 1.
Then, a dependency graph will be generated;
3. checking, i.e., post modifying the dependency
graph according to the definition of dependency
tree (Section 2.1).
Table 1 lists the mapping from HPSG?s PAS types
to word-level dependency arcs. Since a non-terminal
node in an HPSG tree has two kinds of heads, syn-
tactic or semantic, we will generate two dependency
graphs after mapping. We use ?PAS+syn? to repre-
sent the dependency trees generated from the HPSG
PASs guided by the syntactic heads. For semantic
heads, we use ?PAS+sem?.
For example, refer to t0 = when in Figure 1.
Its arg1 = c16 (with syntactic head t10), arg2
= c3 (with syntactic head t6), and PAS type =
conj arg12. In Table 1, this PAS type corresponds
to arg2?pred?arg1, then the result word-level de-
pendency is t6(is)?t0(when)?t10(is).
We need to post modify the dependency graph af-
ter applying the mapping, since it is not guaranteed
to be a dependency tree. Referring to the definition
of dependency tree (Section 2.1), we need the strat-
egy for (1) selecting only one head from multiple
101
PAS Type Dependency Relation
adj arg1[2] [arg2 ?] pred ? arg1
adj mod arg1[2] [arg2 ?] pred ? arg1 ? mod
aux[ mod] arg12 arg1/pred ? arg2 [? mod]
conj arg1[2[3]] [arg2[/arg3]] ? pred ? arg1
comp arg1[2] pred ? arg1 [? arg2]
comp mod arg1 arg1 ? pred ? mod
noun arg1 pred ? arg1
noun arg[1]2 arg2 ? pred [? arg1]
poss arg[1]2 pred ? arg2 [? arg1]
prep arg12[3] arg2[/arg3] ? pred ? arg1
prep mod arg12[3] arg2[/arg3] ? pred ? arg1 ? mod
quote arg[1]2 [arg1 ?] pred ? arg2
quote arg[1]23 [arg1/]arg3 ? pred ? arg2
lparen arg123 pred/arg2 ? arg3 ? arg1
relative arg1[2] [arg2 ?] pred ? arg1
verb arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]] ? pred
verb mod arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]]?pred?mod
app arg12,coord arg12 arg2/pred ? arg1
det arg1,it arg1,punct arg1 pred ? arg1
dtv arg2 pred ? arg2
lgs arg2 arg2 ? pred
Table 1: Mapping fromHPSG?s PAS types to dependency
relations. Dependent(s)? head(s), / = and, [] = optional.
heads and (2) appending dependency relations for
those words/punctuation that do not have any head.
When one word has multiple heads, we only keep
one. The selection strategy is that, if this arc was
deleted, it will cause the biggest number of words
that can not reach to the root word anymore. In case
of a tie, we greedily pack the arc that connect two
words wi and wj where |i? j| is the biggest. For all
the words and punctuation that do not have a head,
we greedily take the root word of the sentence as
their heads. In order to fully use the training data,
if there are directed cycles in the result dependency
graph, we still use the graph in our experiments,
where only partial dependency arcs, i.e., those target
flat/hierarchical phrases attached with well-formed
dependency structures, can be used during transla-
tion rule extraction.
2.5 CCG parsing
We also use the predicate-argument dependencies
generated by the CCG parser developed by Clark
and Curran (2007). The algorithm for generating
word-level dependency tree is easier than processing
the PASs included in the HPSG trees, since the word
level predicate-argument relations have already been
included in the output of CCG parser. The mapping
from predicate types to the gold-standard grammat-
ical relations can be found in Table 13 in (Clark and
Curran, 2007). The post-processing is like that de-
scribed for HPSG parsing, except we greedily use
the MST?s sentence root when we can not determine
it based on the CCG parser?s PASs.
3 Experiments
3.1 Setup
We re-implemented the string-to-dependency de-
coder described in (Shen et al, 2008). Dependency
structures from non-isomorphic syntactic/semantic
parsers are separately used to train the transfer
rules as well as target dependency LMs. For intu-
itive comparison, an outside SMT system is Moses
(Koehn et al, 2007).
For Chinese-to-English translation, we use the
parallel data from NIST Open Machine Translation
Evaluation tasks. The training data contains 353,796
sentence pairs, 8.7M Chinese words and 10.4M En-
glish words. The NIST 2003 and 2005 test data
are respectively taken as the development and test
set. We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al, 2007) to obtain word alignments. The
Berkeley Language Modeling Toolkit, berkeleylm-
1.0b35 (Pauls and Klein, 2011), was employed to
train (1) a five-gram LM on the Xinhua portion of
LDC English Gigaword corpus v3 (LDC2007T07)
and (2) a tri-gram dependency LM on the English
dependency structures of the training data. We re-
port the translation quality using the case-insensitive
BLEU-4 metric (Papineni et al, 2002).
3.2 Statistics of dependencies
We compare the similarity of the dependencies with
each other, as shown in Table 2. Basically, we in-
vestigate (1) if two dependency graphs of one sen-
tence share the same root word and (2) if the head of
one word in one sentence are identical in two depen-
dency graphs. In terms of root word comparison, we
observe that MST and CCG share 87.3% of iden-
tical root words, caused by borrowing roots from
MST to CCG. Then, it is interesting that Berkeley
and PAS+syn share 74.8% of identical root words.
Note that the Berkeley parser is trained on the Penn
treebank (Marcus et al, 1994) yet the HPSG parser
is trained on the HPSG treebank (Miyao and Tsujii,
5http://code.google.com/p/berkeleylm/
102
Dependency Precision Recall BLEU-Dev BLEU-Test # phrases # hier rules # illegal dep trees # directed cycles
Moses-1 - - 0.3349 0.3207 5.4M - - -
Moses-2 - - 0.3445 0.3262 0.7M 4.5M - -
MST 0.744 0.750 0.3520 0.3291 2.4M 2.1M 251 0
Malt 0.732 0.738 0.3423 0.3203 1.5M 1.3M 130,960 0
Berkeley 0.800 0.806 0.3475 0.3312 2.4M 2.2M 282 0
PAS+syn 0.818 0.824 0.3499 0.3376 2.2M 1.9M 10,411 5,853
PAS+sem 0.777 0.782 0.3484 0.3343 2.1M 1.6M 14,271 9,747
CCG 0.701 0.705 0.3442 0.3283 1.7M 1.3M 61,015 49,955
Table 3: Comparison of dependency and translation accuracies. Moses-1 = phrasal, Moses-2 = hierarchical.
Malt Berkeley PAS PAS CCG
+syn +sem
MST 70.5 62.5 69.2 53.3 87.3
(77.3) (64.6) (58.5) (58.1) (61.7)
Malt 66.2 73.0 46.8 62.9
(63.2) (57.7) (56.6) (58.1)
Berkeley 74.8 44.2 56.5
(64.3) (56.0) (59.2)
PAS+ 59.3 62.9
syn (79.1) (61.0)
PAS+ 60.0
sem (58.8)
Table 2: Comparison of the dependencies of the English
sentences in the training data. Without () = % of similar
root words; with () = % of similar head words.
2008). In terms of head word comparison, PAS+syn
and PAS+sem share 79.1% of identical head words.
This is basically due to that we used the similar
PASs of the HPSG trees. Interestingly, there are only
59.3% identical root words shared by PAS+syn and
PAS+sem. This reflects the significant difference be-
tween syntactic and semantic heads.
We also manually created the golden dependency
trees for the first 200 English sentences in the train-
ing data. The precision/recall (P/R) are shown in
Table 3. We observe that (1) the translation accura-
cies approximately follow the P/R scores yet are not
that sensitive to their large variances, and (2) it is
still tough for domain-adapting from the treebank-
trained parsers to parse the real-world SMT data.
PAS+syn performed the best by avoiding the errors
of missing of arguments for a predicate, wrongly
identified head words for a linguistic phrase, and in-
consistency dependencies inside relatively long co-
ordinate structures. These errors significantly influ-
ence the number of extractable translation rules and
the final translation accuracies.
Note that, these P/R scores on the first 200 sen-
tences (all from less than 20 newswire documents)
shall only be taken as an approximation of the total
training data and not necessarily exactly follow the
tendency of the final BLEU scores. For example,
CCG is worse than Malt in terms of P/R yet with a
higher BLEU score. We argue this is mainly due to
that the number of illegal dependency trees gener-
ated by Malt is the highest. Consequently, the num-
ber of flat/hierarchical rules generated by using Malt
trees is the lowest. Also, PAS+sem has a lower P/R
than Berkeley, yet their final BLEU scores are not
statistically different.
3.3 Results
Table 3 also shows the BLEU scores, the number of
flat phrases and hierarchical rules (both integrated
with target dependency structures), and the num-
ber of illegal dependency trees generated by each
parser. From the table, we have the following ob-
servations: (1) all the dependency structures (except
Malt) achieved a significant better BLEU score than
the phrasal Moses; (2) PAS+syn performed the best
in the test set (0.3376), and it is significantly better
than phrasal/hierarchical Moses (p < 0.01), MST
(p < 0.05), Malt (p < 0.01), Berkeley (p < 0.05),
and CCG (p < 0.05); and (3) CCG performed as
well as MST and Berkeley. These results lead us to
argue that the robustness of deep syntactic parsers
can be advantageous in SMT compared with tradi-
tional dependency parsers.
4 Conclusion
We have constructed a string-to-dependency trans-
lation platform for comparing non-isomorphic tar-
get dependency structures. Specially, we proposed
an algorithm for generating word-based dependency
trees from PASs which are generated by a state-of-
the-art HPSG parser. We found that dependency
trees transformed from these HPSG PASs achieved
the best dependency/translation accuracies.
103
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments and suggestions.
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain, July.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of NTCIR-9, pages 559?578.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
In Proceedings of NODALIDA, Tartu, Estonia, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Jeff Ma and Spyros Matsoukas. 2011. Bbn?s systems
for the chinese-english sub-task of the ntcir-9 patentmt
evaluation. In Proceedings of NTCIR-9, pages 579?
584.
David Magerman. 1995. Statistical decision-tree models
for parsing. In In Proceedings of of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276?283.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on HLT, pages 114?119,
Plainsboro.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197?230.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 91?98, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35?80.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285?
291, Borovets.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT,
pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08:HLT, pages 577?585, Colum-
bus, Ohio.
104
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678?683,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adaptation Data Selection using Neural Language Models:
Experiments in Machine Translation
Kevin Duh, Graham Neubig
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Japan
kevinduh@is.naist.jp
neubig@is.naist.jp
Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Labs.
NTT Corporation
2-4 Hikaridai, Seika, Kyoto, Japan
sudoh.katsuhito@lab.ntt.co.jp
tsukada.hajime@lab.ntt.co.jp
Abstract
Data selection is an effective approach
to domain adaptation in statistical ma-
chine translation. The idea is to use lan-
guage models trained on small in-domain
text to select similar sentences from large
general-domain corpora, which are then
incorporated into the training data. Sub-
stantial gains have been demonstrated in
previous works, which employ standard n-
gram language models. Here, we explore
the use of neural language models for data
selection. We hypothesize that the con-
tinuous vector representation of words in
neural language models makes them more
effective than n-grams for modeling un-
known word contexts, which are prevalent
in general-domain text. In a comprehen-
sive evaluation of 4 language pairs (En-
glish to German, French, Russian, Span-
ish), we found that neural language mod-
els are indeed viable tools for data se-
lection: while the improvements are var-
ied (i.e. 0.1 to 1.7 gains in BLEU), they
are fast to train on small in-domain data
and can sometimes substantially outper-
form conventional n-grams.
1 Introduction
A perennial challenge in building Statistical Ma-
chine Translation (SMT) systems is the dearth
of high-quality bitext in the domain of interest.
An effective and practical solution is adaptation
data selection: the idea is to use language models
(LMs) trained on in-domain text to select similar
sentences from large general-domain corpora. The
selected sentences are then incorporated into the
SMT training data. Analyses have shown that this
augmented data can lead to better statistical esti-
mation or word coverage (Duh et al, 2010; Had-
dow and Koehn, 2012).
Although previous works in data selection (Ax-
elrod et al, 2011; Koehn and Haddow, 2012; Ya-
suda et al, 2008) have shown substantial gains, we
suspect that the commonly-used n-gram LMs may
be sub-optimal. The small size of the in-domain
text implies that a large percentage of general-
domain sentences will contain words not observed
in the LM training data. In fact, as many as 60% of
general-domain sentences contain at least one un-
known word in our experiments. Although the LM
probabilities of these sentences could still be com-
puted by resorting to back-off and other smoothing
techniques, a natural question remains: will alter-
native, more robust LMs do better?
We hypothesize that the neural language model
(Bengio et al, 2003) is a viable alternative, since
its continuous vector representation of words is
well-suited for modeling sentences with frequent
unknown words, providing smooth probability es-
timates of unseen but similar contexts. Neu-
ral LMs have achieved positive results in speech
recognition and SMT reranking (Schwenk et al,
2012; Mikolov et al, 2011a). To the best of our
knowledge, this paper is the first work that exam-
ines neural LMs for adaptation data selection.
2 Data Selection Method
We employ the data selection method of (Ax-
elrod et al, 2011), which builds upon (Moore
and Lewis, 2010). The intuition is to select
general-domain sentences that are similar to in-
domain text, while being dis-similar to the average
general-domain text.
To do so, one defines the score of an general-
domain sentence pair (e, f) as:
[INE(e)?GENE(e)] + [INF (f)?GENF (f)]
(1)
where INE(e) is the length-normalized cross-
entropy of e on the English in-domain LM.
GENE(e) is the length-normalized cross-entropy
678
Figure 1: Recurrent neural LM.
of e on the English general-domain LM, which
is built from a sub-sample of the general-domain
text. Similarly, INF (f) and GENF (f) are the
cross-entropies of f on Foreign-side LM. Finally,
sentence pairs are ranked according to Eq. 1 and
those with scores lower than some empirically-
chosen threshold are added to the bitext for trans-
lation model training.
2.1 Neural Language Models
The four LMs used to compute Eq. 1 have con-
ventionally been n-grams. N-grams of the form
p(w(t)|w(t ? 1), w(t ? 2), . . .) predict words by
using multinomial distributions conditioned on the
context (w(t?1), w(t?2), . . .). But when the con-
text is rare or contains unknown words, n-grams
are forced to back-off to lower-order models, e.g.
p(w(t)|w(t ? 1)). These backoffs are unfortu-
nately very frequent in adaptation data selection.
Neural LMs, in contrast, model word probabili-
ties using continuous vector representations. Fig-
ure 1 shows a type of neural LMs called recurrent
neural networks (Mikolov et al, 2011b).1 Rather
than representing context as an identity (n-gram
hit-or-miss) function on [w(t ? 1), w(t ? 2), . . .],
neural LMs summarize the context by a hidden
state vector s(t). This is a continuous vector of
dimension |S| whose elements are predicted by
the previous word w(t ? 1) and previous state
s(t ? 1). This is robust to rare contexts because
continuous representations enable sharing of sta-
tistical strength between similar contexts. Bengio
(2009) shows that such representations are better
than multinomials in alleviating sparsity issues.
1Another major type of neural LMs are the so-called
feed-forward networks (Bengio et al, 2003; Schwenk, 2007;
Nakamura et al, 1990). Both types of neural LMs have seen
many improvements recently, in terms of computational scal-
ability (Le et al, 2011) and modeling power (Arisoy et al,
2012; Wu et al, 2012; Alexandrescu and Kirchhoff, 2006).
We focus on recurrent networks here since there are fewer
hyper-parameters and its ability to model infinite context us-
ing recursion is theoretically attractive. But we note that feed-
forward networks are just as viable.
Now, given state vector s(t), we can predict the
probability of the current word. Figure 1 is ex-
pressed formally in the following equations:
w(t) = [w0(t), . . . , wk(t), . . . w|W |(t)] (2)
wk(t) = g
?
?
|S|?
j=0
sj(t)Vkj
?
? (3)
sj(t)=f
?
?
|W |?
i=0
wi(t? 1)Uji +
|S|?
i?=0
si?(t? 1)Aji?
?
?
(4)
Here, w(t) is viewed as a vector of dimension
|W | (vocabulary size) where each element wk(t)
represents the probability of the k-th vocabulary
item at sentence position t. The function g(zk) =
ezk/?k ezk is a softmax function that ensures the
neural LM outputs are proper probabilities, and
f(z) = 1/(1 + e?z) is a sigmoid activation that
induces the non-linearity critical to the neural net-
work?s expressive power. The matrices V , U , and
A are trained by maximizing likelihood on train-
ing data using a ?backpropagation-through-time?
method.2 Intuitively, U and A compress the con-
text (|S| < |W |) such that contexts predictive of
the same word w(t) are close together.
Since proper modeling of unknown contexts is
important in our problem, training text for both n-
gram and neural LM is pre-processed by convert-
ing all low-frequency words in the training data
(frequency=1 in our case) to a special ?unknown?
token. This is used only in Eq. 1 for selecting
general-domain sentences; these words retain their
surface forms in the SMT train pipeline.
3 Experiment Setup
We experimented with four language pairs in the
WIT3 corpus (Cettolo et al, 2012), with English
(en) as source and German (de), Spanish (es),
French (fr), Russian (ru) as target. This is the
in-domain corpus, and consists of TED Talk tran-
scripts covering topics in technology, entertain-
ment, and design. As general-domain corpora,
we collected bitext from the WMT2013 campaign,
including CommonCrawl and NewsCommentary
for all 4 languages, Europarl for de/es/fr, UN for
es/fr, Gigaword for fr, and Yandex for ru. The in-
domain data is divided into a training set (for SMT
2The recurrent states are unrolled for several time-steps,
then stochastic gradient descent is applied.
679
en-de en-es en-fr en-ru
In-domain Training Set
#sentence 129k 140k 139k 117k
#token (en) 2.5M 2.7M 2.7M 2.3M
#vocab (en) 26k 27k 27k 25k
#vocab (f) 42k 39k 34k 58k
General-domain Bitext
#sentence 4.4M 14.7M 38.9M 2.0M
#token (en) 113M 385M 1012M 51M
%unknown 60% 58% 64% 65%
Table 1: Data statistics. ?%unknown?=fraction of
general-domain sentences with unknown words.
pipeline and neural LM training), a tuning set (for
MERT), a validation set (for choosing the optimal
threshold in data selection), and finally a testset of
1616 sentences.3 Table 1 lists data statistics.
For each language pair, we built a baseline in-
data SMT system trained only on in-domain data,
and an alldata system using combined in-domain
and general-domain data.4 We then built 3 systems
from augmented data selected by different LMs:
? ngram: Data selection by 4-gram LMs with
Kneser-Ney smoothing (Axelrod et al, 2011)
? neuralnet: Data selection by Recurrent neu-
ral LM, with the RNNLM Toolkit.5
? combine: Data selection by interpolated LM
using n-gram & neuralnet (equal weight).
All systems are built using standard settings in
the Moses toolkit (GIZA++ alignment, grow-diag-
final-and, lexical reordering models, and SRILM).
Note that standard n-grams are used as LMs for
SMT; neural LMs are only used for data selection.
Multiple SMT systems are trained by thresholding
on {10k,50k,100k,500k,1M} general-domain sen-
tence subsets, and we empirically determine the
single system for testing based on results on a sep-
arate validation set (in practice, 500k was chosen
for fr and 1M for es, de, ru.).
3The original data are provided by http://wit3.fbk.eu and
http://www.statmt.org/wmt13/. Our domain adaptation sce-
nario is similar to the IWSLT2012 campaign but we used our
own random train/test splits, since we wanted to ensure the
testset for all languages had identical source sentences for
comparison purposes. For replicability, our software is avail-
able at http://cl.naist.jp/?kevinduh/a/acl2013.
4More advanced phrase table adaptation methods are pos-
sible. but our interest is in comparing data selection methods.
The conclusions should transfer to advanced methods such as
(Foster et al, 2010; Niehues and Waibel, 2012).
5http://www.fit.vutbr.cz/?imikolov/rnnlm/
4 Results
4.1 LM Perplexity and Training Time
First, we measured perplexity to check the gen-
eralization ability of our neural LMs as language
models. Recall that we train four LMs to com-
pute each of the components of Eq. 1. In Table 2,
we compared each of the four versions of ngram,
neuralnet, and combine LMs on in-domain test
sets or general-domain held-out sets. It re-affirms
previous positive results (Mikolov et al, 2011a),
with neuralnet outperforming ngram by 20-30%
perplexity across all tasks. Also, combine slightly
improves the perplexity of neuralnet.
Task ngram neuralnet combine
In-Domain Test Set
en-de de 157 110 (29%) 110 (29%)
en-de en 102 81 (20%) 78 (24%)
en-es es 129 102 (20%) 98 (24%)
en-es en 101 80 (21%) 77 (24%)
en-fr fr 90 67 (25%) 65 (27%)
en-fr en 102 80 (21%) 77 (24%)
en-ru ru 208 167 (19%) 155 (26%)
en-ru en 103 83 (19%) 79 (23%)
General-Domain Held-out Set
en-de de 234 174 (25%) 161 (31%)
en-de en 218 168 (23%) 155 (29%)
en-es es 62 43 (31%) 43 (31%)
en-es en 84 61 (27%) 59 (30%)
en-fr fr 64 43 (33%) 43 (33%)
en-fr en 95 67 (30%) 65 (32%)
en-ru ru 242 199 (18%) 176 (27%)
en-ru en 191 153 (20%) 142 (26%)
Table 2: Perplexity of various LMs. Number in
parenthesis is percentage improvement vs. ngram.
Second, we show that the usual concern of neu-
ral LM training time is not so critical for the in-
domain data sizes used domain adaptation. The
complexity of training Figure 1 is dominated by
computing Eq. 3 and scales as O(|W | ? |S|) in
the number of tokens. Since |W | can be large, one
practical trick is to cluster the vocabulary so that
the output dimension is reduced. Table 3 shows
the training times on a 3.3GHz XeonE5 CPU by
varying these two main hyper-parameters (|S| and
cluster size). Note that the setting |S| = 200 and
cluster size of 100 already gives good perplexity
in reasonable training time. All neural LMs in this
paper use this setting, without additional tuning.
680
|S| Cluster Time Perplexity
200 100 198m 110
100 |W | 12915m 110
200 400 208m 113
100 100 52m 118
100 400 71m 120
Table 3: Training time (in minutes) for various
neural LM architectures (Task: en-de de).
4.2 End-to-end SMT Evaluation
Table 4 shows translation results in terms of BLEU
(Papineni et al, 2002), RIBES (Isozaki et al,
2010), and TER (Snover et al, 2006). We observe
that all three data selection methods essentially
outperform alldata and indata for all language
pairs, and neuralnet tend to be the best in all met-
rics. E.g., BLEU improvements over ngram are
in the range of 0.4 for en-de, 0.5 for en-es, 0.1
for en-fr, and 1.7 for en-ru. Although not all im-
provements are large in absolute terms, many are
statistically significant (95% confidence).
We therefore believe that neural LMs are gen-
erally worthwhile to try for data selection, as it
rarely underperform n-grams. The open question
is: what can explain the significant improvements
in, for example Russian, Spanish, German, but the
lack thereof in French? One conjecture is that
neural LMs succeeded in lowering testset out-of-
vocabulary (OOV) rate, but we found that OOV
reduction is similar across all selection methods.
The improvements appear to be due to better
probability estimates of the translation/reordering
models. We performed a diagnostic by decoding
the testset using LMs trained on the same test-
set, while varying the translation/reordering ta-
bles with those of ngram and neuralnet; this is a
kind of pseudo forced-decoding that can inform us
about which table has better coverage. We found
that across all language pairs, BLEU differences of
translations under this diagnostic become insignif-
icant, implying that the raw probability value is
the differentiating factor between ngram and neu-
ralnet. Manual inspection of en-de revealed that
many improvements come from lexical choice in
morphological variants (?meinen Sohn? vs. ?mein
Sohn?), segmentation changes (?baking soda? ?
?Backpulver? vs. ?baken Soda?), and handling of
unaligned words at phrase boundaries.
Finally, we measured the intersection between
the sentence set selected by ngram vs neural-
Task System BLEU RIBES TER
en-de indata 20.8 80.1 59.0
alldata 21.5 80.1 59.1
ngram 21.5 80.3 58.9
neuralnet 21.9+ 80.5+ 58.4
combine 21.5 80.2 58.8
en-es indata 30.4 83.5 48.7
alldata 31.2 83.2 49.9
ngram 32.0 83.7 48.4
neuralnet 32.5+ 83.7 48.3+
combine 32.5+ 83.8 48.3+
en-fr indata 31.4 83.9 51.2
alldata 31.5 83.5 51.4
ngram 32.7 83.7 50.4
neuralnet 32.8 84.2+ 50.3
combine 32.5 84.0 50.5
en-ru indata 14.8 72.5 69.5
alldata 23.4 75.0 62.3
ngram 24.0 75.7 61.4
neuralnet 25.7+ 76.1 60.0+
combine 23.7 75.9 61.9?
Table 4: End-to-end Translation Results. The best
results are bold-faced. We also compare neural
LMs to ngram using pairwise bootstrap (Koehn,
2004): ?+? means statistically significant im-
provement and ??? means significant degradation.
net. They share 60-75% of the augmented train-
ing data. This high overlap means that ngram
and neuralnet are actually not drastically different
systems, and neuralnet with its slightly better se-
lections represent an incremental improvement.6
5 Conclusions
We perform an evaluation of neural LMs for
adaptation data selection, based on the hypothe-
sis that their continuous vector representations are
effective at comparing general-domain sentences,
which contain frequent unknown words. Com-
pared to conventional n-grams, we observed end-
to-end translation improvements from 0.1 to 1.7
BLEU. Since neural LMs are fast to train in the
small in-domain data setting and achieve equal or
incrementally better results, we conclude that they
are an worthwhile option to include in the arsenal
of adaptation data selection techniques.
6This is corroborated by another analysis: taking the
union of sentences found by ngram and neuralnet gives sim-
ilar BLEU scores as neuralnet.
681
Acknowledgments
We thank Amittai Axelrod for discussions about
data selection implementation details, and an
anonymous reviewer for suggesting the union idea
for results analysis. K. D. would like to credit Spy-
ros Matsoukas (personal communication, 2010)
for the trick of using LM-based pseudo forced-
decoding for error analysis.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2006.
Factored neural language models. In Proceed-
ings of the Human Language Technology Confer-
ence of the NAACL, Companion Volume: Short Pa-
pers, NAACL-Short ?06, pages 1?4, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20?28, Montre?al, Canada,
June. Association for Computational Linguistics.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage models. JMLR.
Yoshua Bengio. 2009. Learning Deep Architectures
for AI, volume Foundations and Trends in Machine
Learning. NOW Publishers.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261?268, Trento, Italy,
May.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation for
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT) - Technical Papers Track.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on smt systems. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 422?432, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 944?952, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In WMT.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural net-
work language model. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for
training large scale neural network language model.
In ASRU.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model.
In Proceedings of the 2011 IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Masami Nakamura, Katsuteru Maruyama, Takeshi
Kawabata, and Kiyohiro Shikano. 1990. Neural
network approach to word category prediction for
english texts. In Proceedings of the 13th conference
on Computational linguistics - Volume 3, COLING
?90, pages 213?218, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jan Niehues and Alex Waibel. 2012. Detailed analysis
of different strategies for phrase table adaptation in
SMT. In AMTA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
682
HLT, pages 11?19, Montre?al, Canada, June. Associ-
ation for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518,
July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Youzheng Wu, Xugang Lu, Hitoshi Yamamoto,
Shigeki Matsuda, Chiori Hori, and Hideki Kashioka.
2012. Factored language model based on recurrent
neural network. In Proceedings of COLING 2012,
pages 2835?2850, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In ICJNLP.
683
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 244?251,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Head Finalization: A Simple Reordering Rule for SOV Languages
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, Kevin Duh
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,sudoh,tsukada,kevinduh}@cslab.kecl.ntt.co.jp
Abstract
English is a typical SVO (Subject-Verb-
Object) language, while Japanese is a typ-
ical SOV language. Conventional Statis-
tical Machine Translation (SMT) systems
work well within each of these language
families. However, SMT-based translation
from an SVO language to an SOV lan-
guage does not work well because their
word orders are completely different. Re-
cently, a few groups have proposed rule-
based preprocessing methods to mitigate
this problem (Xu et al, 2009; Hong et al,
2009). These methods rewrite SVO sen-
tences to derive more SOV-like sentences
by using a set of handcrafted rules. In this
paper, we propose an alternative single re-
ordering rule: Head Finalization. This
is a syntax-based preprocessing approach
that offers the advantage of simplicity. We
do not have to be concerned about part-
of-speech tags or rule weights because the
powerful Enju parser allows us to imple-
ment the rule at a general level. Our ex-
periments show that its result, Head Final
English (HFE), follows almost the same
order as Japanese. We also show that this
rule improves automatic evaluation scores.
1 Introduction
Statistical Machine Translation (SMT) is useful
for building a machine translator between a pair of
languages that follow similar word orders. How-
ever, SMT does not work well for distant language
pairs such as English and Japanese, since English
is an SVO language and Japanese is an SOV lan-
guage.
Some existing methods try to solve this word-
order problem in language-independent ways.
They usually parse input sentences and learn a re-
ordering decision at each node of the parse trees.
For example, Yamada and Knight (2001), Quirk et
al. (2005), Xia and McCord (2004), and Li et al
(2007) proposed such methods.
Other methods tackle this problem in language-
dependent ways (Katz-Brown and Collins, 2008;
Collins et al, 2005; Nguyen and Shimazu, 2006).
Recently, Xu et al (2009) and Hong et al (2009)
proposed rule-based preprocessing methods for
SOV languages. These methods parse input sen-
tences and reorder the words using a set of hand-
crafted rules to get SOV-like sentences.
If we could completely reorder the words in in-
put sentences by preprocessing to match the word
order of the target language, we would be able to
greatly reduce the computational cost of SMT sys-
tems.
In this paper, we introduce a single reordering
rule: Head Finalization. We simply move syntac-
tic heads to the end of the corresponding syntactic
constituents (e.g., phrases and clauses). We use
only this reordering rule, and we do not have to
consider part-of-speech tags or rule weights be-
cause the powerful Enju parser allows us to im-
plement the rule at a general level.
Why do we think this works? The reason is
simple: Japanese is a typical head-final language.
That is, a syntactic head word comes after non-
head (dependent) words. SOV is just one as-
pect of head-final languages. In order to imple-
ment this idea, we need a parser that outputs syn-
tactic heads. Enju is such a parser from the
University of Tokyo (http://www-tsujii.is.s.
u-tokyo.ac.jp/enju). We discuss other parsers
in section 5.
There is another kind of head: semantic heads.
Hong et al (2009) used Stanford parser (de Marn-
effe et al, 2006), which outputs semantic head-
based dependencies; Xu et al (2009) also used the
same representation.
The use of syntactic heads and the number
of dependents are essential for the simplicity of
244
Head Finalization (See Discussion). Our method
simply checks whether a tree node is a syntactic
head. We do not have to consider what we are
moving and how to move it. On the other hand, Xu
et al had to introduce dozens of weighted rules,
probably because they used the semantic head-
based dependency representation without restric-
tion on the number of dependents.
The major difference between our method and
the above conventional methods, other than its
simplicity, is that our method moves not only verbs
and adjectives but also functional words such as
prepositions.
2 Head Finalization
Figure 1 shows Enju?s XML output for the simple
sentence: ?John hit a ball.? The tag <cons>
indicates a nonterminal node and <tok> indicates
a terminal node or a word (token). Each node has
a unique id. Head information is given by the
node?s head attribute. For instance, node c0?s head
is node c3, and c3 is a VP, or verb phrase. Thus,
Enju treats not only words but also non-terminal
nodes as heads.
Enju outputs at most two child nodes for each
node. One child is a head and the other is a depen-
dent. c3?s head is c4, which is VX, or a fragment of
a verb phrase. c4?s head is t1 or hit, which is VBD
or a past-tense verb. The upper picture of Figure 2
shows the parse tree graphically. Here, ? indicates
an edge that is linked from a ?head.?
Our Head Finalization rule simply swaps two
children when the head child appears before the
dependent child. In the upper picture of Fig. 2, c3
has two children c4 and c5. Here, c3?s head c4
appears before c5, so c4 and c5 are swapped.
The lower picture shows the swapped result.
Then we get John a ball hit, which has the
same word order as its Japanese translation jon wa
bohru wo utta except for the functional words a,
wa, and wo.
We have to add Japanese particles wa (topic
marker) or ga (nominative case marker) for John
and wo (objective case marker) for ball to get an
acceptable Japanese sentence.
It is well known that SMT is not good at gen-
erating appropriate particles from English, whitch
does not have particles. Particle generation was
tackled by a few research groups (Toutanova and
Suzuki, 2007; Hong et al, 2009).
Here, we use Enju?s output to generate seeds
?sentence id=?s0? parse status=?success??
?cons id=?c0? cat=?S? xcat=?? head=?c3??
?cons id=?c1? cat=?NP? xcat=?? head=?c2??
?cons id=?c2? cat=?NX? xcat=?? head=?t0??
?tok id=?t0? cat=?N? pos=?NNP?
base=?john??John?/tok?
?/cons?
?/cons?
?cons id=?c3? cat=?VP? xcat=?? head=?c4??
?cons id=?c4? cat=?VX? xcat=?? head=?t1??
?tok id=?t1? cat=?V? pos=?VBD? base=?hit?
arg1=?c1? arg2=?c5??hit?/tok?
?/cons?
?cons id=?c5? cat=?NP? xcat=?? head=?c7??
?cons id=?c6? cat=?DP? xcat=?? head=?t2?
?tok id=?t2? cat=?D? pos=?DT? base=?a?
arg1=?c7??a?/tok?
?/cons?
?cons id=?c7? cat=?NX? xcat=?? head=?t3??
?tok id=?t3? cat=?N? pos=?NN?
base=?ball??ball?/tok?
?/cons?
?/cons?
?/cons?
?/cons?
.?/sentence?
Figure 1: Enju?s XML output (some attributes are
removed for readability).
t0
John
t1
hit
t2
a
t3
ball
c7?c6?
c5?
c4?
c3?
c2?
c1?
c0 Original English?
t0
John
jon (wa)
t1
hit
utta
t2
a
?
t3
ball
bohru (wo)
c7?c6?
c5?
c4?
c3?
c2?
c1?
c0 Head Final English?
Figure 2: Head Finalization of a simple sentence
(? indicates a head).
245
2
John
5
went
7
to
9
the
10
police
12
because
15
Mary
17
lost
19
his
20
wallet
1? 14?8? 18?
6?
4? 16?
13?
11?
3?
0 Original English?
2
John
jon (wa)
5
Mary
meari (ga)
19
his
kare no
20
wallet
saifu (wo)
17
lost
nakushita
12
because
node
9
the
?
10
police
keisatsu
7
to
ni
5
went
itta
1? 14? 8?18?
6?
4?16?
13?
11?
3 ?
0 Head Final English?
Figure 3: Head-Finalizing a complex sentence.
for particles. As Fig. 1 shows, the verb hit has
arg1="c1" and arg2="c5". This indicates that c1
(John) is the subject of hit and c5 (a ball) is
the object of hit. We add seed words va1 after
arg1 and va2 after arg2. Then, we obtain John
va1 a ball va2 hit. We do not have to add
arg2 for be because be?s arg2 is not an object but
a complement. We introduced the idea of particle
seed words independently but found that it is very
similar to Hong et al (2009)?s method for Korean.
Figure 3 shows Enju?s parse tree for a
more complicated sentence ?John went to the
police because Mary lost his wallet.? For
brevity, we hide the terminal nodes, and we re-
moved the nonterminal nodes? prefix c.
Conventional Rule-Based Machine Translation
(RBMT) systems swap X and Y of ?X because Y?
and move verbs to the end of each clause. Then we
get ?Mary his wallet lost because John the police
to went.? Its word-to-word translation is a fluent
Japanese sentence: meari (ga) kare no saifu (wo)
nakushita node jon (wa) keisatsu ni itta.
On the other hand, our Head Finalization with
particle seed words yields a slightly different word
order ?John va1 Mary va1 his wallet va2 lost
because the police to went.? Its word-to-word
translation is jon wa meari ga kare no saifu wo
nakushita node keisatsu ni itta. This is also an ac-
ceptable Japanese sentence.
This difference comes from the syntactic role
of ?because.? In our method, Enju states that
because is a dependent of went, whereas RBMT
systems treat because as a clause conjunction.
When we use Xu et al?s preprocessing method,
?because? moves to the beginning of the sentence.
We do not know a good monotonic translation of
the result.
Preliminary experiments show that HFE looks
good as a first approximiation of Japanese word
order. However, we can make it better by intro-
ducing some heuristic rules. (We did not see the
test set to develop these heuristic rules.)
From a preliminary experiment, we found that
coordination expressions such as A and B and A
or B are reordered as B and A and B or A. Al-
though A and B have syntactically equal positions,
the order of these elements sometimes matters.
Therefore, we decided to stop swapping them at
coordination nodes, which are indicated cat and
xcat attributes of the Enju output. We call this
the coordination exception rule. In addition,
we avoid Enju?s splitting of numerical expressions
such as ?12,345? and ?(1)? because this splitting
leads to inappropriate word orders.
246
3 Experiments
In order to show how closely our Head Finaliza-
tion makes English follow Japanese word order,
we measured Kendall?s ? , a rank correlation co-
efficient. We also measured BLEU (Papineni et
al., 2002) and other automatic evaluation scores to
show that Head Finalization can actually improve
the translation quality.
We used NTCIR7 PAT-MT?s Patent corpus (Fu-
jii et al, 2008). Its training corpus has 1.8 mil-
lion sentence pairs. We used MeCab (http://
mecab.sourceforge.net/) to segment Japanese
sentences.
3.1 Rough evaluation of reordering
First, we examined rank correlation between Head
Final English sentences produced by the Head Fi-
nalization rule and Japanese reference sentences.
Since we do not have handcrafted word alignment
data for an English-to-Japanese bilingual corpus,
we used GIZA++ (Och and Ney, 2003) to get au-
tomatic word alignment.
Based on this automatic word alignment, we
measured Kendall?s ? for the word order between
HFE sentences and Japanese sentences. Kendall?s
? is a kind of rank correlation measure defined as
follows. Suppose a list of integers such as L = [2,
1, 3, 4]. The number of all integer pairs in this list
is 4C2 = 4 ? 3/(2 ? 1) = 6. The number of in-
creasing pairs is five: (2, 3), (2, 4), (1, 3), (1, 4),
and (3, 4). Kendall?s ? is defined by
? = #increasing pairs
#all pairs
? 2? 1.
In this case, we get ? = 5/6? 2? 1 = 0.667.
For each sentence in the training data,
we calculate ? based on a GIZA++ align-
ment file, en-ja.A3.final. (We also tried
ja-en.A3.final, but we got similar results.) It
looks something like this:
John hit a ball .
NULL ({3}) jon ({1}) wa ({}) bohru ({4})
wo ({}) utta ({2}) . ({5})
Numbers in ({ }) indicate corresponding En-
glish words. The article ?a? has no correspond-
ing word in Japanese, and such words are listed
in NULL ({ }). From this alignment information,
we get an integer list [1, 4, 2, 5]. Then, we get
? = 5/4C2 ? 2? 1 = 0.667.
For HFE in Figure 2, we will get the following
alignment.
John va1 a ball va2 hit .
NULL ({3}) jon ({1}) wa ({2}) bohru ({4})
wo ({5}) utta ({6}) . ({7})
Then, we get [1, 2, 4, 5, 6, 7] and ? = 1.0. We
use ? or the average of ? over all training sentences
to observe the tendency.
Sometimes, one Japanese word corresponds to
an English phrase:
John went to Costa Rica .
NULL ({}) jon ({1}) wa ({}) kosutarika ({4 5})
ni ({3}) itta ({2}) . ({6})
We get [1, 4, 5, 3, 2, 6] from this alignment.
When the same word (or derivative words) ap-
pears twice or more in a single English sentence,
two or more non-consecutive words in the English
sentence are aligned to a single Japanese word:
rate of change of speed
NULL ({}) sokudo ({5}) henka ({3})
no ({2 4}) wariai ({1})
We excluded the ambiguously aligned words (2
4) from the calculation of ? . We use only [5, 3,
1] and get ? = ?1.0. The exclusion of these
words will be criticized by statisticians, but even
this rough calculation of ? sheds light on the weak
points of Head Finalization.
Because of this exclusion, the best value ? =
1.0 does not mean that we obtained the perfect
word ordering, but low ? values imply failures. In
section 4, we use ? to analyze failures.
By examining low ? sentences, we found that
patent documents have a lot of expressions such
as ?motor 2.? These are reordered (2 motor) and
slightly degrade ? . We did not notice this problem
until we handled the patent corpus because these
expressions are rare in other documents such as
news articles. Here, we added a rule to keep these
expressions.
We did not use any dictionary in our experi-
ment, but if we add dictionary entries to the train-
ing data, it raises ? because most entries are short.
One-word entries do not affect ? because we can-
not calculate ? . Most multi-word entries are short
noun phrases that are not reordered (? = 1.0).
Therefore, we should exclude dictionary entries
from the calculation of ? .
3.2 Quality of translation
It must be noted that the rank correlation does not
directly measure the quality of translation. There-
fore, we also measured BLEU and other automatic
evaluation scores of the translated sentences. We
used Moses (Koehn, 2010) for Minimum Error
Rate Training and decoding.
247
0%
5%
10%
15%
20%
-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0
? of English sentences
0%
5%
10%
15%
20%
-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0
? of Head Finalized English sentences
Figure 4: Distribution of ?
We used the development set (915 sentences) in
the NTCIR7 PAT-MT PSD data as well as the for-
mal run test set (1,381 sentences).
In the NTCIR7 PAT-MT workshop held in 2008,
its participants used different methods such as hi-
erarchical phrase-based SMT, RBMT, and EBMT
(Example-Based Machine Translation). However,
the organizers? Moses-based baseline system ob-
tained the best BLEU score.
4 Results
First, we show ? values to evaluate word order,
and then we show BLEU and other automatic eval-
uation scores.
4.1 Rank correlation
The original English sentences have ? = 0.451.
Head Finalization improved it to 0.722. Figure
4 shows the distribution of ? for all training sen-
tences. HFE reduces the percentage of low ? sen-
tences: 49.6% of the 1.8 million HFE sentences
have ? ? 0.8 and 15.1% have ? = 1.0.
We also implemented Xu et al?s method with
the Stanford parser 1.6.2. Its ? was 0.624. The
rate of the sentences with ? ? 0.8 was 30.6% and
the rate of ? = 1.0 was 4.3%.
We examined low ? sentences of our method
and found the following reasons for low ? values.
? The sentence pair is not an exact one-to-one
translation. A Japanese reference sentence
for ?I bought the cake.? can be some-
thing like ?The cake I bought.? or ?The
person who bought the cake is me.?
? Mistakes in Enju?s tagging or parsing. We
encountered certain POS tag mistakes:
? VBZ/NNS mistake: ?advances? of ?. . .
device advances along . . .? is VBZ,
main cause count
tagging/parsing mistakes 12
VBN/VBD mistake (4)
VBZ/NNS mistake (2)
comma or and (2)
inexact translation 7
wrong alignment 1
Table 1: Main causes of 20 worst sentences
but NNS is assigned.
? VBN/VBD mistake: ?encoded? of
?. . . the error correction encoded
data is supplied . . .? is VBN, but
VBD is assigned.
These tagging mistakes lead to global parsing
mistakes. In addition, just like other parsers,
Enju tends to make mistakes when a sentence
has a comma or ?and.?
? Mistakes/Ambiguity of GIZA++ automatic
word alignment. Ambiguity happens when
a single sentence has two or more occur-
rences of a word or derivatives of a word
(e.g., difference/different/differential). As we
described above, ambiguously aligned words
are removed from calculation of ? , and small
reordering mistakes in other words are em-
phasized.
We analyzed the 20 worst sentences with ? <
?0.5 when we used only 400,000 sentences for
GIZA++. Their causes are summarized in Table
1. In general, low ? sentences have two or more
causes, but here we show only the most influen-
tial cause for each sentence. This table shows that
mistakes in tagging and parsing are major causes
of low ? values. When we used all of 1.8 million
248
Method BLEU WER TER
proposed (0) 30.79 0.663 0.554
proposed (3) 30.97 0.665 0.554
proposed (6) 31.21 0.660 0.549
proposed (9) 31.11 0.661 0.549
proposed (12) 30.98 0.662 0.551
proposed (15) 31.00 0.662 0.552
no va (6) 30.99 0.669 0.559
Organizer 30.58 0.755 0.592
Table 2: Automatic Evaluation of Translation
Quality (Numbers in parentheses indicate distor-
tion limits).
sentence pairs, only 11 sentences had ? < ?0.5
among the 1.8 million sentences.
4.2 Automatic Evaluation of Translation
Quality
In general, it is believed that translation between
English and Japanese requires a large distortion
limit (dl), which restricts how far a phrase can
move. SMT reasearchers working on E-J or J-
E translation often use dl=?1 (unlimited) as a
default value, and this takes a long translation
time.
For PATMT J-E translation, Katz-Brown and
Collins (2008) showed that dl=unlimited is the
best and it requires a very long translation time.
For PATMT E-J translation, Kumai et al (2008)
claimed that they achieved the best result ?when
the distortion limit was 20 instead of ?1.?
Table 2 compares the single-reference BLEU
score of the proposed method and that of the
Moses-based system by the NTCIR-7 PATMT
organizers. This organizers? system was better
than all participants (Fujii et al, 2008) in terms
of BLEU. Here, we used Bleu Kit (http://
www.mibel.cs.tsukuba.ac.jp/norimatsu/
bleu kit/) following the PATMT?s overview
paper (Fujii et al, 2008). The table shows that
dl=6 gives the best result, and even dl=0 (no
reordering in Moses) gives better scores than the
organizers? Moses.
Table 2 also shows Word Error Rates (WER)
and Translation Error Rates (TER) (Snover et al,
2006). Since they are error rates, smaller is better.
Although the improvement of BLEU is not very
impressive, the score of WER is greatly reduced.
This difference comes from the fact that BLEU
measures only local word order, while WER mea-
Method ROUGE-L IMPACT PER
proposed (6) 0.480 0.369 0.390
no va (6) 0.475 0.368 0.398
Organizer 0.403 0.339 0.384
Table 3: Improvement in word order
sures global word order. Another line ?no va?
stands for our method without vas or particle
seeds. Without particle seeds, all scores slightly
drop.
Echizen-ya et al (2009) showed that IMPACT
and ROUGE-L are highly correlated to human
evaluation in evaluating J-E patent translation.
Therefore, we also used these evaluation methods
here for E-J translation. Table 3 shows that the
proposed method is also much better than the or-
ganizers? Moses in terms of these measures. With-
out particle seeds, these scores also drop slightly.
On the other hand, Position-independent Word
Error Rate (PER), which completely disregards
word order, does not change very much. These
facts indicate that our method improves word or-
der, which is the most important problem in E-J
translation.
The organizers? Moses uses dl=unlimited, and
it has been reported that its MERT training took
two weeks. On the other hand, our MERT training
with dl=6 took only eight hours on a PC: Xeon
X5570 2.93 GHz. Our method takes extra time to
parse sentences by Enju, but it is easy to run the
parser in parallel.
5 Discussion
Our method used an HPSG parser, which gives
rich information, but it is not easy to build such a
parser. It is much easier to build word dependency
parsers and Penn Treebank-style parsers. In order
use these parsers, we have to add some heuristic
rules.
5.1 Word Dependency Parsers
At first, we thought that we could substitute a word
dependency parser for Enju by simply rephrasing
a head with a modified word. Xu et al (2009)
used a semantic head-based dependency parser for
a similar purpose. Even when we use a syntac-
tic head-based dependency parser instead, we en-
countered their ?excessive movement? problem.
A straightforward application of their rules
changes
249
3
John
5
hit
7
the
8
ball
10
but
13
Sam
15
threw
17
the
18
ball
16?
14?
12?
11?
9?
6?
4?
2?
1?
0?
xcat="COOD"
cat="COOD"
Figure 5: Head Finilization does not mix up
clauses
(0) John hit the ball but Sam threw the ball.
to
(1) John the ball but Sam the ball threw hit.
Here, the two clauses are mixed up. To prevent
this, they disallow any movement across punctua-
tion and conjunctions. Then they get a better re-
sult:
(2) John the ball hit but Sam the ball threw.
When we used Enju, these clauses were not
mixed up. Enju-based Head Finalization gave the
same word order as (2):
(3) John va1 ball va2 hit but Sam va1 ball va2
throw.
Figure 5 shows Enju?s parse tree. When Head Fi-
nalization swaps the children of a mother node,
the children do not move beyond the range of
the mother node. Therefore, Head Finalization
based on Enju does not mix up the first clause
John hit the ball covered by Node 1 with the
second clause Sam threw the ball covered by
Node 11. Moreover, our coordination exception
rule keeps the order of these clauses. Thus, non-
terminal nodes in Enju?s output are useful to pro-
tect clauses.
When we use a word-dependency parser, we as-
sume that the modified words are heads. Further-
more, the Head Finalization rule is rephrased as
?move modified words after modifiers.? There-
fore, hit is moved after threw just like (2), and
the two clauses become mixed up. Consequently,
we need a heuristic rule like Xu?s.
5.2 Penn Treebank-style parsers
We also tried Charniak-Johnson?s parser (Char-
niak and Johnson, 2005). PyInputTree
(http://www.cs.brown.edu/?dmcc/software/
PyInputTree/) gives heads. Enju outputs at
most two children for a mother node, but Penn
Treebank-style parsers do not have such a limita-
tion on the number of children. This fact causes a
problem.
When we use Enju, ?This toy is popular in
Japan? is reordered as ?This toy va1 Japan in
popular is.? Its monotonic translation is fluent:
kono omocha wa nihon de ninki ga aru.
On the other hand, Charniak-Johnson?s parser
outputs the following S-expression for this sen-
tence (we added asterisks (*) to indicate heads).
(S (NP (DT This) (NN* toy))
(VP* (AUX* is)
(ADJP (JJ* popular))
(PP (IN* in) (NP (NNP* Japan)))))
Simply moving heads to the end introduces
?Japan in? between ?is? and ?popular?: this toy
va1 popular Japan in is. It is difficult to translate
this monotonically because of this interruption.
Reversing the children order (Xu et al, 2009)
reconnects is and popular. We get ?This toy
(va1) Japan in popular is? from the follow-
ing reversed S-expression.
(S (NP (DT This) (NN* toy))
(VP* (PP (IN* in) (NP (NNP* Japan)))
(ADJP (JJ* popular))
(AUX* is)))
5.3 Limitation of Head Finalization
Head Finalization gives a good first approximation
of Japanese word order in spite of its simplicity.
However, it is not perfect. In fact, a small distor-
tion limit improved the performance.
Sometimes, the Japanese language does not
have an appropriate word for monotonic transla-
tion. For instance, ?I have no time? becomes
?I va1 no time va2 have.? Its monotonic trans-
lation is ?watashi wa nai jikan wo motteiru,?
but this sentence is not acceptable. An acceptable
literal translation is ?watashi wa jikan ga nai.?
Here, ?no? corresponds to ?nai? at the end of the
sentence.
6 Conclusion
To solve the word-order problem between SVO
languages and SOV langugages, we introduced
a new reordering rule called Head Finalization.
This rule is simple, and we do not have to consider
POS tags or rule weights. We also showed that this
reordering improved automatic evaluation scores
of English-to-Japanese translation. Improvement
of the BLEU score is not very impressive, but
other evaluation scores (WER, TER, LOUGE-L,
and IMPACT) are greatly improved.
250
However, Head Finalization requires a sophis-
ticated HPSG tagger such as Enju. We showed
that severe failures are caused by Enju?s POS tag-
ging mistakes. We discussed the problems of other
parsers and how to solve them.
Our future work is to build our own parser that
makes fewer errors and to apply Head Finalization
to other SOV languages such as Korean.
Acknowledgements
We would like to thank Dr. Yusuke Miyao for
his useful advice on the usage of Enju. We also
thank anonymous reviewers for their valuable sug-
gestions.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
173?180.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of the Language Resources and Evaluation
Conference (LREC), pages 449?454.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimo-
hata, Atsushi Fujii, Masao Utiyama, Mikio Ya-
mamoto, Takehito Utsuro, and Noriko Kando. 2009.
Meta-evaluation of automatic evaluation methods
for machine translation using patent translation data
in NTCIR-7. In Proceedings of the 3rd Workshop on
Patent Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging morpho-syntactic gap be-
tween source and target sentences for English-
Korean statistical machine translation. In Proc. of
ACL-IJCNLP, pages 233?236.
Jason Katz-Brown and Michael Collins. 2008. Syn-
tactic reordering in preprocessing for Japanese ?
English translation: MIT system description for
NTCIR-7 patent translation task. In Working Notes
of the NTCIR Workshop Meeting (NTCIR).
Philipp Koehn, 2010. MOSES, Statistical Machine
Translation System, User Manual and Code Guide.
Hiroyuki Kumai, Hirohiko Segawa, and Yasutsugu
Morimoto. 2008. NTCIR-7 patent translation ex-
periments at Hitachi. In Working Notes of the NT-
CIR Workshop Meeting (NTCIR), pages 441?444.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 720?727.
Thai Phuong Nguyen and Akira Shimazu. 2006.
Improving phrase-based statistical machine transla-
tion with morphosyntactic transformation. Machine
Translation, 20(3):147?166.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 271?279.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Kristina Toutanova and Hisami Suzuki. 2007. Gener-
ating case markers in machine translation. In Proc.
of NAACL-HLT, pages 49?56.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. of the International Con-
ference on Computational Linguistics (COLING),
pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
of NAACL-HLT, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 523?530.
251
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 375?383,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
N-best Reranking by Multitask Learning
Kevin Duh Katsuhito Sudoh Hajime Tsukada Hideki Isozaki Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{kevinduh,sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
Abstract
We propose a new framework for N-best
reranking on sparse feature sets. The idea
is to reformulate the reranking problem as
a Multitask Learning problem, where each
N-best list corresponds to a distinct task.
This is motivated by the observation that
N-best lists often show significant differ-
ences in feature distributions. Training a
single reranker directly on this heteroge-
nous data can be difficult.
Our proposed meta-algorithm solves this
challenge by using multitask learning
(such as ?1/?2 regularization) to discover
common feature representations across N-
best lists. This meta-algorithm is simple to
implement, and its modular approach al-
lows one to plug-in different learning algo-
rithms from existing literature. As a proof
of concept, we show statistically signifi-
cant improvements on a machine transla-
tion system involving millions of features.
1 Introduction
Many natural language processing applications,
such as machine translation (MT), parsing, and
language modeling, benefit from the N-best
reranking framework (Shen et al, 2004; Collins
and Koo, 2005; Roark et al, 2007). The advan-
tage of N-best reranking is that it abstracts away
the complexities of first-pass decoding, allowing
the researcher to try new features and learning al-
gorithms with fast experimental turnover.
In the N-best reranking scenario, the training
data consists of sets of hypotheses (i.e. N-best
lists) generated by a first-pass system, along with
their labels. Given a new N-best list, the goal is
to rerank it such that the best hypothesis appears
near the top of the list. Existing research have fo-
cused on training a single reranker directly on the
entire data. This approach is reasonable if the data
is homogenous, but it fails when features vary sig-
nificantly across different N-best lists. In partic-
ular, when one employs sparse feature sets, one
seldom finds features that are simultaneously ac-
tive on multiple N-best lists.
In this case, we believe it is more advantageous
to view the N-best reranking problem as a multi-
task learning problem, where each N-best list cor-
responds to a distinct task. Multitask learning, a
subfield of machine learning, focuses on how to
effectively train on a set of different but related
datasets (tasks). Our heterogenous N-best list data
fits nicely with this assumption.
The contribution of this work is three-fold:
1. We introduce the idea of viewing N-best
reranking as a multitask learning problem.
This view is particularly apt to any general
reranking problem with sparse feature sets.
2. We propose a simple meta-algorithm that
first discovers common feature representa-
tions across N-bests (via multitask learning)
before training a conventional reranker. Thus
it is easily applicable to existing systems.
3. We demonstrate that our proposed method
outperforms the conventional reranking ap-
proach on a English-Japanese biomedical
machine translation task involving millions
of features.
The paper is organized as follows: Section 2 de-
scribes the feature sparsity problem and Section 3
presents our multitask solution. The effectiveness
of our proposed approach is validated by experi-
ments demonstrated in Section 4. Finally, Sections
5 and 6 discuss related work and conclusions.
2 The Problem of Sparse Feature Sets
For concreteness, we will describe N-best rerank-
ing in terms of machine translation (MT), though
375
our approach is agnostic to the application. In MT
reranking, the goal is to translate a foreign lan-
guage sentence f into an English sentence e by
picking from a set of likely translations. A stan-
dard approach is to use a linear model:
e? = argmax
e?N(f)
wT ? h(e, f) (1)
where h(e, f) is a D-dimensional feature vector,
w is the weight vector to be trained, and N(f) is
the set of likely translations of f , i.e. the N-best
list. The feature h(e, f) can be any quantity de-
fined in terms of the sentence pair, such as transla-
tion model and language model probabilities.
Here we are interested in situations where the
feature definitions can be quite sparse. A com-
mon methodology in reranking is to first design
feature templates based on linguistic intuition and
domain knowledge. Then, numerous features are
instantiated based on the training data seen. For
example, the work of (Watanabe et al, 2007) de-
fines feature templates based on bilingual word
alignments, which lead to extraction of heavily-
lexicalized features of the form:
h(e, f) =
?
?
?
?
?
?
?
1 if foreign word ?Monsieur?
and English word ?Mr.?
co-occur in e,f
0 otherwise
(2)
One can imagine that such features are sparse
because it may only fire for input sentences that
contain the word ?Monsieur?. For all other input
sentences, it is an useless, inactive feature.
Another common feature involves word ngram
templates, for example:
h(e, f) =
?
?
?
1 if English trigram
?Mr. Smith said? occurs in e
0 otherwise
(3)
In this case, all possible trigrams seen in the N-
best list are extracted as features. One can see
that this kind of feature can be very sensitive to
the first-pass decoder: if the decoder has loose re-
ordering constraints, then we may extract expo-
nentially many nonsense ngram features such as
?Smith said Mr.? and ?said Smith Mr.?. Granted,
the reranker training algorithm may learn that
these nonsense ngrams are indicative of poor hy-
potheses, but it is unlikely that the exact same non-
sense ngrams will appear given a different test sen-
tence.
In summary, the following issues compound to
create extremely sparse feature sets:
1. Feature templates are heavily-lexicalized,
which causes the number of features to grow
unbounded as the the amount of data in-
creases.
2. The input (f ) has high variability (e.g. large
vocabulary size), so that features for different
inputs are rarely shared.
3. The N-best list output also exhibits high vari-
ability (e.g. many different word reorder-
ings). Larger N may improve reranking per-
formance, but may also increase feature spar-
sity.
When the number of features is too large, even
popular reranking algorithms such as SVM (Shen
et al, 2004) and MIRA (Watanabe et al, 2007;
Chiang et al, 2009) may fail. Our goal here is to
address this situation.
3 Proposed Reranking Framework
In the following, we first give an intuitive com-
parison between single vs. multiple task learning
(Section 3.1), before presenting the general meta-
algorithm (Section 3.2) and particular instantia-
tions (Section 3.3).
3.1 Single vs. Multiple Tasks
Given a set of I input sentences {f i}, the training
data for reranking consists of a set of I N-best lists
{(Hi,yi)}i=1,...,I , where Hi are features and yi
are labels.
To clarify the notation:1 for an input sentence
f i, there is a N-best list N(f i). For a N-best list
N(f i), there are N feature vectors corresponding
to the N hypotheses, each with dimension D. The
collection of feature vectors for N(f i) is repre-
sented by Hi, which can be seen as a D ? N
matrix. Finally, the N -dimensional vector of la-
bels yi indicates the translation quality of each hy-
pothesis in N(f i). The purpose of the reranker
training algorithm is to find good parameters from
{(Hi,yi)}.
1Generally we use bold font h to represent a vector, bold-
capital font H to represent a matrix. Script h and h(?) may
be scalar, function, or sentence (depends on context).
376
The conventional method of training a single
reranker (single task formulation) involves opti-
mizing a generic objective such as:
argmin
w
I
?
i=1
L(w,Hi,yi) + ??(w) (4)
where w ? RD is the reranker trained on all lists,
and L(?) is some loss function. ?(w) is an op-
tional regularizer, whose effect is traded-off by the
constant ?. For example, the SVM reranker for
MT (Shen et al, 2004) defines L(?) to be some
function of sentence-level BLEU score, and ?(w)
to be the large margin regularizer.2
On the other hand, multitask learning involves
solving for multiple weights, w1,w2, . . . ,wI ,
one for each N-best list. One class of multitask
learning algorithms, Joint Regularization, solves
the following objective:
arg min
w1,..,wI
I
?
i=1
L(wi,Hi,yi) + ??(w1, ..,wI )
(5)
The loss decomposes by task but the joint regu-
larizer ?(w1, ..,wI) couples together the different
weight parameters. The key is to note that multi-
ple weights allow the algorithm to fit the heteroge-
nous data better, compared to a single weight vec-
tor. Yet these weights are still tied together so that
some information can be shared across N-best lists
(tasks).
One instantiation of Eq. 5 is ?1/?2 regular-
ization: ?(w1, ..,wI) , ||W||1,2, where W =
[w1|w2| . . . |wI ]T is a I-by-D matrix of stacked
weight vectors. The norm is computed by first tak-
ing the 2-norm on columns of W, then taking a
1-norm on the resulting D-length vector. This en-
courages the optimizer to choose a small subset of
features that are useful across all tasks.
For example, suppose two different sets of
weight vectors Wa and Wb for a 2 lists, 4 fea-
tures reranking problem. The ?1/?2 norm for Wa
is 14; the ?1/?2 norm for Wb is 12. If both have
the same loss L(?) in Eq. 5, the multitask opti-
mizer would prefer Wb since more features are
shared:
Wa :
?
4 0 0 3
0 4 3 0
?
Wb :
?
4 3 0 0
0 4 3 0
?
4 4 3 3 ? 14 4 5 3 0 ? 12
2In MT, evaluation metrics like BLEU do not exactly de-
compose across sentences, so for some training algorithms
this loss is an approximation.
3.2 Proposed Meta-algorithm
We are now ready to present our general reranking
meta-algorithm (see Algorithm 1), termed Rerank-
ing by Multitask Learning (RML).
Algorithm 1 Reranking by Multitask Learning
Input: N-best data {(Hi,yi)}i=1,...,I
Output: Common feature representation hc(e, f)
and weight vector wc
1: [optional] RandomHashing({Hi})
2: W = MultitaskLearn({(Hi ,yi)})
3: hc = ExtractCommonFeature(W)
4: {Hic} = RemapFeature({Hi}, hc)
5: wc = ConventionalReranker({(Hic ,yi)})
The first step, random hashing, is optional. Ran-
dom hashing is an effective trick for reducing the
dimension of sparse feature sets without suffer-
ing losses in fidelity (Weinberger et al, 2009;
Ganchev and Dredze, 2008). It works by collaps-
ing random subsets of features. This step can be
performed to speed-up multitask learning later. In
some cases, the original feature dimension may be
so large that hashed representations may be neces-
sary.
The next two steps are key. A multitask learn-
ing algorithm is run on the N-best lists, and a com-
mon feature space shared by all lists is extracted.
For example, if one uses the multitask objective
of Eq. 5, the result of step 2 is a set of weights
W. ExtractCommonFeature(W) then returns the
feature id?s (either from original or hashed repre-
sentation) that receive nonzero weight in any of
W.3 The new features hc(e, f) are expected to
have lower dimension than the original features
h(e, f). Section 3.3 describes in detail different
multitask methods that can be plugged-in to this
step.
The final two steps involve a conventional
reranker. In step 4, we remap the N-best list
data according to the new feature representations
hc(e, f). In step 5, we train a conventional
reranker on this common representation, which by
now should have overcome sparsity issues. Us-
ing a conventional reranker at the end allows us
to exploit existing rerankers designed for specific
NLP applications. In a sense, our meta-algorithm
simply involves a change of representation for
the conventional reranking scenario, where the
3For example in Wb, features 1-3 have nonzero weights
and are extracted. Feature 4 is discarded.
377
new representation is found by multitask methods
which are well-suited to heterogenous data.
3.3 Multitask Objective Functions
Here, we describe various multitask methods that
can be plugged in Step 2 of Algorithm 1. Our
goal is to demonstrate that a wide range of existing
methods from the multitask learning literature can
be brought to our problem. We categorize multi-
task methods into two major approaches:
1. Joint Regularization: Eq. 5 is an exam-
ple of joint regularization, with ?1/?2 norm being
a particular regularizer. The idea is to use the reg-
ularizer to ensure that the learned functions of re-
lated tasks are close to each other. The popular
?1/?2 objective can be optimized by various meth-
ods, such as boosting (Obozinski et al, 2009) and
convex programming (Argyriou et al, 2008). Yet
another regularizer is the ?1/?? norm (Quattoni et
al., 2009), which replaces the 2-norm with a max.
One could also define a regularizer to ensure
that each task-specific wi is close to some average
parameter, e.g.
?
i ||wi ? wavg||2. If we inter-
pret wavg as a prior, we begin to see links to Hier-
archical Bayesian methods for multitask learning
(Finkel and Manning, 2009; Daume, 2009).
2. Shared Subspace: This approach assumes
that there is an underlying feature subspace that
is common to all tasks. Early works on multi-
task learning implement this by neural networks,
where different tasks have different output layers
but share the same hidden layer (Caruana, 1997).
Another method is to write the weight vector
as two parts w = [u;v] and let the task-specific
function be uT ? h(e, f) + vT ? ? ? h(e, f) (Ando
and Zhang, 2005). ? is a D??D matrix that maps
the original features to a subspace common to all
tasks. The new feature representation is computed
by the projection hc(e, f) , ? ? h(e, f).
Multitask learning is a vast field and relates to
areas like collaborative filtering (Yu and Tresp,
2005) and domain adaptation. Most methods as-
sume some common representation and is thus ap-
plicable to our framework. The reader is urged to
refer to citations in, e.g. (Argyriou et al, 2008) for
a survey.
4 Experiments and Results
As a proof of concept, we perform experiments
on a MT system with millions of features. We
use a hierarchical phrase-based system (Chiang,
100 101 102 103 104
10?7
10?6
10?5
10?4
10?3
10?2
10?1
100
P(
fea
tur
e o
cc
urs
 in
 x 
lis
ts)
x
Figure 1: This log-log plot shows that there are
many rare features and few common features. The
probability that a feature occurs in x number of N-
best lists behaves according to the power-law x??,
where ? = 2.28.
2007) to generate N-best lists (N=100). Sparse
features used in reranking are extracted according
to (Watanabe et al, 2007). Specifically, the major-
ity are lexical features involving joint occurrences
of words within the N-best lists and source sen-
tences.
It is worth noting that the fact that the first pass
system is a hierarchical system is not essential to
the feature extraction step; similar features can be
extracted with other systems as first-pass, e.g. a
phrase-based system. That said, the extent of the
feature sparsity problem may depend on the per-
formance of the first-pass system.
We experiment with medical domain MT, where
large numbers of technical vocabulary cause spar-
sity challenges. Our corpora consists of English
abstracts from PubMed4 with their Japanese trans-
lations. The first-pass system is built on hierarchi-
cal phrases extracted from 17k sentence pairs and
target (Japanese) language models trained on 800k
medical-domain sentences. For our reranking ex-
periments, we used 500 lists as the training set5,
500 lists as held-out, and another 500 for test.
4.1 Data Characteristics
We present some statistics to illustrate the feature
sparsity problem: From 500 N-best lists, we ex-
tracted a total of 2.4 million distinct features. By
type, 75% of these features occur in only one N-
best list in the dataset. Less than 3% of features
4A database of the U.S. National Library of Medicine.
5In MT, training data for reranking is sometimes referred
to as ?dev set? to distinguish from the data used in first-pass.
Also, while the 17k bitext may seem small compared to other
MT work, we note that 1st pass translation quality (around 28
BLEU) is high enough to evaluate reranking methods.
378
occur in ten or more lists. The distribution of fea-
ture occurrence is clearly Zipfian, as seen in the
power-law plot in Figure 1.
We can also observe the feature growth rate (Ta-
ble 1). This is the number of new features intro-
duced when an additional N-best list is seen. It is
important to note that on average, 2599 new fea-
tures are added everytime a new N-best list is seen.
This is as much as 2599/4188 = 62% of the ac-
tive features. Imagine an online training algorithm
(e.g. MIRA or perceptron) on this kind of data:
whenever a loss occurs and we update the weight
vector, less than half of the weight vector update
applies to data we have seen thus far. Herein lies
the potential for overfitting.
From observing the feature grow rate, one may
hypothesize that adding large numbers of N-best
lists to the training set (500 in the experiments
here) may not necessarily improve results. While
adding data potentially improves the estimation
process, it also increases the feature space dramat-
ically. Thus we see the need for a feature extrac-
tion procedure.
(Watanabe et al, 2007) also reports the possibil-
ity of overfitting in their dataset (Arabic-English
newswire translation), especially when domain
differences are present. Here we observe this ten-
dency already on the same domain, which is likely
due to the highly-specialized vocabulary and the
complex sentence structures common in research
paper abstracts.
4.2 MT Results
Our goal is to compare different feature represen-
tations in reranking: The baseline reranker uses
the original sparse feature representation. This is
compared to feature representations discovered by
three different multitask learning methods:
? Joint Regularization (Obozinski et al, 2009)
? Shared Subspace (Ando and Zhang, 2005)
? Unsupervised Multitask Feature Selection
(Abernethy et al, 2007).6
We use existing implementations of the above
methods.7 The conventional reranker (Step 5, Al-
6This is not a standard multitask algorithm since most
multitask algorithms are supervised. We include it to see
if unsupervised or semi-supervised multitask algorithms is
promising. Intuitively, the method tries to select subsets of
features that are correlated across multiple tasks using ran-
dom sampling (MCMC). Features that co-occur in different
tasks form a high probability path.
7Available at http://multitask.cs.berkeley.edu
Nbest id #NewFt #SoFar #Active
1 3900 3900 3900
2 7535 11435 7913
3 6078 17513 7087
4 3868 21381 4747
5 1896 23277 2645
6 3542 26819 4747
....
100 2440 289118 4299
101 1639 290757 2390
102 3468 294225 4755
103 2350 296575 3824
Average 2599 ? 4188
Table 1: Feature growth rate: For N-best list i in
the table, we have (#NewFt = number of new fea-
tures introduced since N-best i ? 1) ; (#SoFar =
Total number of features defined so far); and (#Ac-
tive = number of active features for N-best i). E.g.,
we extracted 7535 new features from N-best 2;
combined with the 3900 from N-best 1, the total
features so far is 11435.
gorithm 1) used in all cases is SVMrank.8 Our
initial experiments show that the SVM baseline
performance is comparable to MIRA training, so
we use SVM throughout. The labels for the SVM
are derived as in (Shen et al, 2004), where top
10% of hypotheses by smoothed sentence-BLEU
is ranked before the bottom 90%. All multitask
learning methods work on hashed features of di-
mension 4000 (Step 1, Algorithm 1). This speeds
up the training process.
All hyperparameters of the multitask method
are tuned on the held-out set. In particular, the
most important is the number of common features
to extract, which we pick from {250, 500, 1000}.
Table 2 shows the results by BLEU (Papineni
et al, 2002) and PER. The Oracle results are ob-
tained by choosing the best hypothesis per N-best
list by sentence-level BLEU, which achieved 36.9
BLEU in both Train and Test. A summary of our
observations is:
1. The baseline (All sparse features) overfits. It
achieves the oracle BLEU score on the train
set (36.9) but performs poorly on the test
(28.6).
2. Similar overfitting occurs when traditional ?1
regularization is used to select features on
8Available at http://svmlight.joachims.org
379
the sparse feature representation9 . ?1 reg-
ularization is a good method of handling
sparse features for classification problems,
but in reranking the lack of tying between
lists makes this regularizer inappropriate. A
small set of around 1200 features are chosen:
they perform well independently on each task
in the training data, but there is little sharing
with the test data.
3. All three multitask methods obtained features
that outperformed the baseline. The BLEU
scores are 28.8, 28.9, 29.1 for Unsupervised
Feature Selection, Joint Regularization, and
Shared Subspace, respectively, which all out-
perform the 28.6 baseline. All improvements
are statistically significant by bootstrap sam-
pling test (1000 samples, p < 0.05) (Zhang
et al, 2004).
4. Shared Subspace performed the best. We
conjecture this is because its feature projec-
tion can create new feature combinations that
is more expressive than the feature selection
used by the two other methods.
5. PER results are qualitatively similar to BLEU
results.
6. As a further analysis, we are interested in see-
ing whether multitask learning extracts novel
features, especially those that have low fre-
quency. Thus, we tried an additional feature
representation (feature threshold) which only
keeps features that occur in more than x N-
bests, and concatenate these high-frequency
features to the multitask features. The fea-
ture threshold alone achieves nice BLEU re-
sults (29.0 for x > 10), but the combination
outperforms it by statistically significant mar-
gins (29.3-29.6). This implies that multitask
learning is extracting features that comple-
ment well with high frequency features.
For the multitask features, improvements of 0.2
to 1.0 BLEU are modest but consistent. Figure
2 shows the BLEU of bootstrap samples obtained
as part of the statistical significance test. We see
that multitask almost never underperform base-
line in any random sampling of the data. This im-
plies that the proposed meta-algorithm is very sta-
9Optimized by the Vowpal Wabbit toolkit:
http://hunch.net/vw/
ble, i.e. it is not a method that sometimes improves
and sometimes degrades.
Finally, a potential question to ask is: what
kinds of features are being selected by the
multitask learning algorithms? We found that
that two kinds of features are usually selected:
one is general features that are not lexicalized,
such as ?count of phrases?, ?count of dele-
tions/insertions?, ?number of punctuation marks?.
The other kind is lexicalized features, such as
those in Equations 2 and 3, but involving functions
words (like the Japanese characters ?wa?, ?ga?,
?ni?, ?de?) or special characters (such as numeral
symbol and punctuation). These are features that
can be expected to be widely applicable, and it is
promising that multitask learning is able to recover
these from the millions of potential features. 10
?0.2 0 0.2 0.4 0.6 0.8 1 1.2
0
50
100
150
200
250
300
BLEU(shared subspace)?BLEU(baseline sparse feature)
Bo
ot
st
ra
p 
sa
m
pl
es
Figure 2: BLEU difference of 1000 bootstrap sam-
ples. 95% confidence interval is [.15, .90] The
proposed approach therefore seems to be a stable
method.
5 Related Work in NLP
Previous reranking work in NLP can be classified
into two different research focuses:
1. Engineering better features: In MT, (Och
and others, 2004) investigates features extracted
from a wide variety of syntactic representations,
such as parse tree probability on the outputs. Al-
though their results show that the proposed syntac-
tic features gave little improvements, they point to
some potential reasons, such as domain mismatch
for the parser and overfitting by the reranking
10Note: In order to do this analysis, we needed to run Joint
Regularization on the original feature representation, since
the hashed representations are less interpretable. This turns
out to be computationally prohibitive in the time being so we
only ran on a smaller data set of 50 lists. Recently new op-
timization methods that are orders of magnitude faster have
been developed (Liu et al, 2009), which makes larger-scale
experiments possible.
380
Train Test Test
Feature Representation #Feature BLEU BLEU PER
(baselines)
First pass 20 29.5 28.5 38.3
All sparse features (Main baseline) 2.4M 36.9 28.6 38.2
All sparse features w/ ?1 regularization 1200 36.5 28.5 38.6
Random hash representation 4000 33.0 28.5 38.2
(multitask learning)
Unsupervised FeatureSelect 500 32.0 28.8 37.7
Joint Regularization 250 31.8 28.9 37.5
Shared Subspace 1000 32.9 29.1 37.3
(combination w/ high-frequency features)
(a) Feature threshold x > 100 3k 31.7 27.9 38.2
(b) Feature threshold x > 10 60k 35.8 29.0 37.9
Unsupervised FeatureSelect + (b) 60.5k 36.2 29.3 37.6
Joint Regularization + (b) 60.25k 36.1 29.4 37.5
Shared Subspace + (b) 61k 36.2 29.6 37.3
Oracle (best possible) ? 36.9 36.9 33.1
Table 2: Results for different feature sets, with corresponding feature size and train/test BLEU/PER. All
multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared
Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency
features also give significant improvements over the high frequency features alone.
method. Recent work by (Chiang et al, 2009) de-
scribes new features for hierarchical phrase-based
MT, while (Collins and Koo, 2005) describes
features for parsing. Evaluation campaigns like
WMT (Callison-Burch et al, 2009) and IWSLT
(Paul, 2009) also contains a wealth of information
for feature engineering in various MT tasks.
2. Designing better training algorithms: N-
best reranking can be seen as a subproblem of
structured prediction, so many general structured
prediction algorithms (c.f. (Bakir et al, 2007))
can be applied. In fact, some structured predic-
tion algorithms, such as the MIRA algorithm used
in dependency parsing (McDonald et al, 2005)
and MT (Watanabe et al, 2007) uses iterative
sets of N-best lists in its training process. Other
training algorithms include perceptron-style algo-
rithms (Liang et al, 2006), MaxEnt (Charniak and
Johnson, 2005), and boosting variants (Kudo et al,
2005).
The division into two research focuses is conve-
nient, but may be suboptimal if the training algo-
rithm and features do not match well together. Our
work can be seen as re-connecting the two focuses,
where the training algorithm is explicitly used to
help discover better features.
Multitask learning is currently an active subfield
within machine learning. There has already been
some applications in NLP: For example, (Col-
lobert and Weston, 2008) uses a deep neural net-
work architecture for multitask learning on part-
of-speech tagging, chunking, semantic role label-
ing, etc. They showed that jointly learning these
related tasks lead to overall improvements. (De-
selaers et al, 2009) applies similar methods for
machine transliteration. In information extraction,
learning different relation types can be naturally
cast as a multitask problem (Jiang, 2009; Carlson
et al, 2009). Our work can be seen as following
the same philosophy, but applied to N-best lists.
In other areas, (Reichart et al, 2008) introduced
an active learning strategy for annotating multitask
linguistic data. (Blitzer et al, 2006) applies the
multitask algorithm of (Ando and Zhang, 2005)
to domain adaptation problems in NLP. We expect
that more novel applications of multitask learning
will appear in NLP as the techniques become scal-
able and standard.
6 Discussion and Conclusion
N-best reranking is a beneficial framework for ex-
perimenting with large feature sets, but unfortu-
nately feature sparsity leads to overfitting. We ad-
dressed this by re-casting N-best lists as multitask
381
learning data. Our MT experiments show consis-
tent statistically significant improvements.
From the Bayesian view, multitask formulation
of N-best lists is actually very natural: Each N-
best is generated by a different data-generating
distribution since the input sentences are different,
i.e. p(e|f1) 6= p(e|f2). Yet these N-bests are re-
lated since the general p(e|f) distribution depends
on the same first-pass models.
The multitask learning perspective opens up
interesting new possibilities for future work, e.g.:
? Different ways to partition data into tasks,
e.g. clustering lists by document structure, or
hierarchical clustering of data
? Multitask learning on lattices or N-best lists
with larger N. It is possible that a larger hy-
pothesis space may improve the estimation of
task-specific weights.
? Comparing multitask learning to sparse on-
line learning of batch data, e.g. (Tsuruoka et
al., 2009).
? Modifying the multitask objective to incorpo-
rate application-specific loss/decoding, such
as Minimum Bayes Risk (Kumar and Byrne,
2004)
? Using multitask learning to aid large-scale
feature engineering and visualization.
Acknowledgments
We have received numerous helpful comments
throughout the course of this work. In partic-
ular, we would like to thank Albert Au Yeung,
Jun Suzuki, Shinji Watanabe, and the three anony-
mous reviewers for their valuable suggestions.
References
Jacob Abernethy, Peter Bartlett, and Alexander
Rakhlin. 2007. Multitask learning with expert ad-
vice. In COLT.
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. JMLR.
Andreas Argyriou, Theodoros Evgeniou, and Massim-
iliano Pontil. 2008. Convex multitask feature learn-
ing. Machine Learning, 73(3).
G. Bakir, T. Hofmann, B. Scholkopf, A. Smola,
B. Taskar, and S. V. N. Vishwanathan, editors. 2007.
Predicting structured data. MIT Press.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning.
In EMNLP.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In
WMT.
Andrew Carlson, Justin Betteridge, Estevam Hruschka,
and Tom Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In NAACL
Workshop on Semi-supervised learning for NLP
(SSLNLP).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural langauge parsing. Computa-
tional Linguistics, 31(1).
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In ICML.
Hal Daume. 2009. Bayesian multitask learning with
latent hierarchies. In UAI.
Thomas Deselaers, Sasa Hasan, Oliver Bender, and
Hermann Ney. 2009. A deep learning approach to
machine transliteration. In WMT.
Jenny Rose Finkel and Chris Manning. 2009. Hier-
archical Bayesian domain adaptation. In NAACL-
HLT.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In ACL-
2008 Workshop on Mobile Language Processing.
Jing Jiang. 2009. Multitask transfer learning for
weakly-supervised relation extraction. In ACL.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In ACL.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
382
J. Liu, S. Ji, and J. Ye. 2009. Multi-task feature learn-
ing via efficient l2,1-norm minimization. In UAI.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large margin training of de-
pendency parsers. In ACL.
Guillaume Obozinski, Ben Taskar, and Michael Jor-
dan. 2009. Joint covariate selection and joint sub-
space selection for multiple classification problems.
Statistics and Computing.
F.J. Och et al 2004. A smorgasbord of features for
statistical machine translation. In HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Michael Paul. 2009. Overview of the iwslt 2009 eval-
uation campaign. In IWSLT.
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projection
for L1-Linfinity regularization. In ICML.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for lin-
guistic annotations. In ACL.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech and Language, 21(2).
Libin Shen, Anoop Sarkar, and Franz Och. 2004. Dis-
criminative reranking for machine translation. In
HLT-NAACL.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In ACL-IJCNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In EMNLP-
CoNLL.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In ICML.
Kai Yu and Volker Tresp. 2005. Learning to learn and
collaborative filtering. In NIPS-2005 Workshop on
Inductive Transfer.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
LREC.
383
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418?427,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Divide and Translate: Improving Long Distance Reordering in Statistical
Machine Translation
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
sudoh@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a novel method
for long distance, clause-level reordering
in statistical machine translation (SMT).
The proposed method separately translates
clauses in the source sentence and recon-
structs the target sentence using the clause
translations with non-terminals. The non-
terminals are placeholders of embedded
clauses, by which we reduce complicated
clause-level reordering into simple word-
level reordering. Its translation model
is trained using a bilingual corpus with
clause-level alignment, which can be au-
tomatically annotated by our alignment
algorithm with a syntactic parser in the
source language. We achieved signifi-
cant improvements of 1.4% in BLEU and
1.3% in TER by using Moses, and 2.2%
in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT, for
the English-to-Japanese translation of re-
search paper abstracts in the medical do-
main.
1 Introduction
One of the common problems of statistical ma-
chine translation (SMT) is to overcome the differ-
ences in word order between the source and target
languages. This reordering problem is especially
serious for language pairs with very different word
orders, such as English-Japanese. Many previous
studies on SMT have addressed the problem by
incorporating probabilistic models into SMT re-
ordering. This approach faces the very large com-
putational cost of searching over many possibili-
ties, especially for long sentences. In practice the
search can be made tractable by limiting its re-
ordering distance, but this also renders long dis-
tance movements impossible. Some recent stud-
ies avoid the problem by reordering source words
prior to decoding. This approach faces difficul-
ties when the input phrases are long and require
significant word reordering, mainly because their
reordering model is not very accurate.
In this paper, we propose a novel method for
translating long sentences that is different from
the above approaches. Problematic long sentences
often include embedded clauses1 such as rela-
tive clauses. Such an embedded (subordinate)
clause can usually be translated almost indepen-
dently of words outside the clause. From this
viewpoint, we propose a divide-and-conquer ap-
proach: we aim to translate the clauses sepa-
rately and reconstruct the target sentence using the
clause translations. We first segment a source sen-
tence into clauses using a syntactic parser. The
clauses can include non-terminals as placeholders
for nested clauses. Then we translate the clauses
with a standard SMT method, in which the non-
terminals are reordered as words. Finally we re-
construct the target sentence by replacing the non-
terminals with their corresponding clause transla-
tions. With this method, clause-level reordering is
reduced to word-level reordering and can be dealt
with efficiently. The models for clause translation
are trained using a bilingual corpus with clause-
level alignment. We also present an automatic
clause alignment algorithm that can be applied to
sentence-aligned bilingual corpora.
In our experiment on the English-to-Japanese
translation of multi-clause sentences, the proposed
method improved the translation performance by
1.4% in BLEU and 1.3% in TER by using Moses,
and by 2.2% in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT.
The main contribution of this paper is two-fold:
1Although various definitions of a clause can be
considered, this paper follows the definition of ?S?
(sentence) in Enju. It basically follows the Penn Tree-
bank II scheme but also includes SINV, SQ, SBAR. See
http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enju-
output-spec.html#correspondence for details.
418
1. We introduce the idea of explicit separa-
tion of in-clause and outside-clause reorder-
ing and reduction of outside-clause reorder-
ing into common word-level reordering.
2. We propose an automatic clause alignment
algorithm, by which our approach can be
used without manual clause-level alignment.
This paper is organized as follows. The next
section reviews related studies on reordering. Sec-
tion 3 describes the proposed method in detail.
Section 4 presents and discusses our experimen-
tal results. Finally, we conclude this paper with
our thoughts on future studies.
2 Related Work
Reordering in SMT can be roughly classified into
two approaches, namely a search in SMT decod-
ing and preprocessing.
The former approach is a straightforward way
that models reordering in noisy channel transla-
tion, and has been studied from the early period
of SMT research. Distance-based reordering is a
typical approach used in many previous studies re-
lated to word-based SMT (Brown et al, 1993) and
phrase-based SMT (Koehn et al, 2003). Along
with the advances in phrase-based SMT, lexical-
ized reordering with a block orientation model was
proposed (Tillmann, 2004; Koehn et al, 2005).
This kind of reordering is suitable and commonly
used in phrase-based SMT. On the other hand,
a syntax-based SMT naturally includes reorder-
ing in its translation model. A lot of research
work undertaken in this decade has used syntac-
tic parsing for linguistically-motivated translation.
(Yamada and Knight, 2001; Graehl and Knight,
2004; Galley et al, 2004; Liu et al, 2006). Wu
(1997) and Chiang (2007) focus on formal struc-
tures that can be extracted from parallel corpora,
instead of a syntactic parser trained using tree-
banks. These syntactic approaches can theoret-
ically model reordering over an arbitrary length,
however, long distance reordering still faces the
difficulty of searching over an extremely large
search space.
The preprocessing approach employs deter-
ministic reordering so that the following trans-
lation process requires only short distance re-
ordering (or even a monotone). Several previ-
ous studies have proposed syntax-driven reorder-
ing based on source-side parse trees. Xia and
McCord (2004) extracted reordering rules auto-
matically from bilingual corpora for English-to-
French translation; Collins et al (2005) used
linguistically-motivated clause restructuring rules
for German-to-English translation; Li et al (2007)
modeled reordering on parse tree nodes by us-
ing a maximum entropy model with surface and
syntactic features for Chinese-to-English trans-
lation; Katz-Brown and Collins (2008) applied
a very simple reverse ordering to Japanese-to-
English translation, which reversed the word order
in Japanese segments separated by a few simple
cues; Xu et al (2009) utilized a dependency parser
with several hand-labeled precedence rules for re-
ordering English to subject-object-verb order like
Korean and Japanese. Tromble and Eisner (2009)
proposed another reordering approach based on a
linear ordering problem over source words with-
out a linguistically syntactic structure. These pre-
processing methods reorder source words close
to the target-side order by employing language-
dependent rules or statistical reordering models
based on automatic word alignment. Although
the use of language-dependent rules is a natural
and promising way of bridging gaps between lan-
guages with large syntactic differences, the rules
are usually unsuitable for other language groups.
On the other hand, statistical methods can be ap-
plied to any language pairs. However, it is very
difficult to reorder all source words so that they are
monotonic with the target words. This is because
automatic word alignment is not usually reliable
owing to data sparseness and the weak modeling
of many-to-many word alignments. Since such
a reordering is not complete or may even harm
word ordering consistency in the source language,
these previous methods further applied reordering
in their decoding. Li et al (2007) used N-best
reordering hypotheses to overcome the reordering
ambiguity.
Our approach is different from those of previous
studies that aim to perform both short and long dis-
tance reordering at the same time. The proposed
method distinguishes the reordering of embedded
clauses from others and efficiently accomplishes it
by using a divide-and-conquer framework. The re-
maining (relatively short distance) reordering can
be realized in decoding and preprocessing by the
methods described above. The proposed frame-
work itself does not depend on a certain language
pair. It is based on the assumption that a source
419
language clause is translated to the corresponding
target language clause as a continuous segment.
The only language-dependent resource we need is
a syntactic parser of the source language. Note
that clause translation in the proposed method is a
standardMT problem and therefore any reordering
method can be employed for further improvement.
This work is inspired by syntax-based meth-
ods with respect to the use of non-terminals. Our
method can be seen as a variant of tree-to-string
translation that focuses only on the clause struc-
ture in parse trees and independently translates the
clauses. Although previous syntax-based methods
can theoretically model this kind of derivation, it
is practically difficult to decode long multi-clause
sentences as described above.
Our approach is also related to sentence sim-
plification and is intended to obtain simple and
short source sentences for better translation. Kim
and Ehara (1994) proposed a rule-based method
for splitting long Japanese sentences for Japanese-
to-English translation; Furuse et al (1998) used
a syntactic structure to split ill-formed inputs in
speech translation. Their splitting approach splits
a sentence sequentially to obtain short segments,
and does not undertake their reordering.
Another related field is clause identification
(Tjong et al, 2001). The proposed method is not
limited to a specific clause identification method
and any method can be employed, if their clause
definition matches the proposed method where
clauses are independently translated.
3 Proposed Method
The proposed method consists of the following
steps illustrated in Figure 1.
During training:
1) clause segmentation of source sentences with
a syntactic parser (section 3.1)
2) alignment of target words with source clauses
to develop a clause-level aligned corpus (section
3.2)
3) training the clause translation models using
the corpus (section 3.3)
During testing:
1) clause translation with the clause translation
models (section 3.4)
2) sentence reconstruction based on non-
terminals (section 3.5)
Bilingual
Corpus
(Training)
source
target
parse & clause
segmentation
parse &
clause
segmen-
tation
Source Sentences
(clause-segmented)
Word Alignment
Model
Target Word Bigram
Language Model
LM training
word
alignment
Bilingual Corpus
(clause-aligned)
automatic clause alignment
Clause
Translation Models
(Phrase Table, N-gram LMs, ...)
training from scratch
Bilingual
Corpus
(Development)
(clause-segmented)
MERT
Test Sentence
Sentence
Translation
clause
clause
clause
clause
translation
clause
translation
clause
translation
sentence reconstruction
based on non-terminals
translation
Original (sentence-aligned)
corpus can also be used
Figure 1: Overview of proposed method.
3.1 Clause Segmentation of Source Sentences
Clauses in source sentences are identified by a
syntactic parser. Figure 2 shows a parse tree for
the example sentence below. The example sen-
tence has a relative clause modifying the noun
book. Figure 3 shows the word alignment of this
example.
English: John lost the book that was borrowed
last week from Mary.
Japanese: john wa (topic marker) senshu (last
week) mary kara (from) kari (borrow) ta
(past tense marker) hon (book) o (direct ob-
ject marker) nakushi (lose) ta (past tense
marker) .
We segment the source sentence at the clause level
and the example is rewritten with two clauses as
follows.
? John lost the book s0 .
? that was borrowed last week from Mary
s0 is a non-terminal symbol the serves as a place-
holder of the relative clause. We allow an arbitrary
420
SS
John
lost
the
book
that
was
borrowed
from Mary
last week
Figure 2: Parse tree for example English sentence.
Node labels are omitted except S.
John
lo
st
the
book
that
w
as
borrow
ed
from
M
ary
last
w
eek
john
wa
ta
nakushi
o
hon
ta
kari
kara
mary
senshu
Figure 3: Word alignment for example bilingual
sentence.
number of non-terminals in each clause2. A nested
clause structure can be represented in the same
manner using such non-terminals recursively.
3.2 Alignment of Target Words with Source
Clauses
To translate source clauses with non-terminal sym-
bols, we need models trained using a clause-level
aligned bilingual corpus. A clause-level aligned
corpus is defined as a set of parallel, bilingual
clause pairs including non-terminals that represent
embedded clauses.
We assume that a sentence-aligned bilingual
corpus is available and consider the alignment of
target words with source clauses. We can manu-
ally align these Japanese words with the English
clauses as follows.
? john wa s0 hon o nakushi ta .
2In practice not so many clauses are embedded in a single
sentence but we found some examples with nine embedded
clauses for coordination in our corpora.
John lost the book s0 .
? senshu mary kara kari ta
that was borrowed last week from Mary
Since the cost of manual clause alignment is
high especially for a large-scale corpus, a natu-
ral question to ask is whether this resource can be
obtained from a sentence-aligned bilingual corpus
automatically with no human input. To answer
this, we now describe a simple method for deal-
ing with clause alignment data from scratch, us-
ing only the word alignment and language model
probabilities inferred from bilingual and monolin-
gual corpora.
Our method is based on the idea that automatic
clause alignment can be viewed as a classification
problem: for an English sentence with N words (e
= (e1, e2, . . . , eN )) andK clauses (e?1,e?2,. . . ,e?K),
and its Japanese translation with M words (f
= (f1, f2, . . . , fM )), the goal is to classify each
Japanese word into one of {1, . . . ,K} classes. In-
tuitively, the probability that a Japanese word fm
is assigned to class k ? {1, . . . ,K} depends on
two factors:
1. The probability of translating fm into the En-
glish words of clause k (i.e.
?
e?e?k p(e|fm)).
We expect fm to be assigned to a clause
where this value is high.
2. The language model probability
(i.e. p(fm|fm?1)). If this value is high,
we expect fm and fm?1 to be assigned to the
same clause.
We implement this intuition using a graph-
based method. For each English-Japanese sen-
tence pair, we construct a graph with K clause
nodes (representing English clauses) and M word
nodes (representing Japanese words). The edge
weights between word and clause nodes are de-
fined as the sum of lexical translation probabilities
?
e?e?k p(e|fm). The edge weights between words
are defined as the bigram probability p(fm|fm?1).
Each clause node is labeled with a class ID k ?
{1, . . . ,K}. We then propagate these K labels
along the graph to label the M word nodes. Fig-
ure 4 shows the graph for the example sentence.
Many label propagation algorithms are avail-
able. The important thing is to use an algo-
rithm that encourages node pairs with strong edge
weights to receive the same label. We use the label
propagation algorithm of (Zhu et al, 2003). If we
421
John  lost  the  book  that  was  borrowed ...
clause(1) clause(2)
John Mary fromlast weektopicmarker
p(John |           )
+ p(lost |           )
+ ...
p(that |        )
+ p(was |        )
+ ...
p(     |         ) p(         |            ) p(        |         )p(            |     )
john kara
karajohn
john wa senshu mary kara
wa  john senshu  wa mary  senshu kara mary
Figure 4: Graph-based representation of the ex-
ample sentence. We propagate the clause labels to
the Japanese word nodes on this graph to form the
clause alignments.
assume the labels are binary, the following objec-
tive is minimized:
argmin
l?RK+M
?
i,j
wij(li ? lj)2 (1)
where wij is the edge weight between nodes i
and j (1 ? i ? K + M , 1 ? j ? K +
M ), and l (li ? {0, 1}) is a vector of labels
on the nodes. The first K elements of l, lc =
(l1, l2, ..., lK)T , are constant because the clause
nodes are pre-labeled. The remaining M ele-
ments, lf = (lK+1, lK+2, ..., lK+M )T , are un-
known and to be determined. Here, we consider
the decomposition of the weight matrixW = [wij ]
into four blocks after the K-th row and column as
follows:
W =
[
W cc W cf
W fc W ff
]
(2)
The solution of eqn. (1), namely lf , is given by the
following equation:
lf = (Dff ?W ff )?1W fc lc (3)
where D is the diagonal matrix with di =
?
j wij
and is decomposed similarly to W . Each element
of lf is in the interval (0, 1) and can be regarded
as the label propagation probability. A detailed ex-
planation of this solution can be found in Section 2
of (Zhu et al, 2003). For our multi-label problem
with K labels, we slightly modified the algorithm
by expanding the vector l to an (M + K) ? K
binary matrix L = [ l1 l2 ... lK ].
After the optimization, we can normalize Lf
to obtain the clause alignment scores t(lm =
k|fm) between each Japanese word fm and En-
glish clause k. Theoretically, we can simply out-
put the clause id k? for each fm by finding k? =
argmaxk t(lm = k|fm). In practice, this may
sometimes lead to Japanese clauses that have too
many gaps, so we employ a two-stage procedure
to extract clauses that are more contiguous.
First, we segment the Japanese sentence into K
clauses based on a dynamic programming algo-
rithm proposed by Malioutov and Barzilay (2006).
We define an M ? M similarity matrix S = [sij ]
with sij = exp(?||li?lj ||) where li is (K + i)-th
row vector in the label matrix L. sij represents
the similarity between the i-th and j-th Japanese
words with respect to their clause alignment score
distributions; if the score distributions are sim-
ilar then sij is large. The details of this algo-
rithm can be found in (Malioutov and Barzilay,
2006). The clause segmentation gives us contigu-
ous Japanese clauses f?1, f?2, ..., f?K , thus min-
imizing inter-segment similarity and maximizing
intra-segment similarity. Second, we determine
the clause labels of the segmented clauses, based
on clause alignment scores T = [Tkk? ] for English
and automatically-segmented Japanese clauses:
Tkk? =
?
fm?f? k?
t(lm = k|fm) (4)
where f?k? is the j?-th Japanese clause. In descend-
ing order of the clause alignment score, we greed-
ily determine the clause label 3.
3.3 Training Clause Translation Models
We train clause translation models using the
clause-level aligned corpus. In addition we can
also include the original sentence-aligned corpus.
We emphasize that we can use standard techniques
for heuristically extracted phrase tables, word n-
gram language models, and so on.
3.4 Clause Translation
By using the source language parser, a multi-
clause source sentence is reduced to a set of
clauses. We translate these clauses with a common
SMT method using the clause translation models.
Here we present another English example I
bought the magazine which Tom recommended
yesterday. This sentence is segmented into clauses
as follows.
3Although a full search is available when the number of
clauses is small, we employ a greedy search in this paper.
422
? I bought the magazine s0 .
? which Tom recommended yersterday
These clauses are translated into Japanese:
? watashi (I) wa (topic marker) s0
zasshi (magazine) o (direct object marker)
kat (buy) ta (past tense marker).
? tom ga (subject marker) kino (yesterday)
susume (recommend) ta (past tense marker)
3.5 Sentence Reconstruction
We reconstruct the target sentence from the clause
translations, based on non-terminals. Starting
from the clause translation of the top clause, we re-
cursively replace non-terminal symbols with their
corresponding clause translations. Here, if a non-
terminal is eventually deleted in SMT decoding,
we simply concatenate the translation behind its
parent clause.
Using the example above, we replace the non-
terminal symbol s0 with the second clause and
obtain the Japanese sentence:
watashi wa tom ga kino susume ta zasshi o kat ta .
4 Experiment
We conducted the following experiments on the
English-to-Japanese translation of research paper
abstracts in the medical domain. Such techni-
cal documents are logically and formally writ-
ten, and sentences are often so long and syntac-
tically complex that their translation needs long
distance reordering. We believe that the medical
domain is suitable as regards evaluating the pro-
posed method.
4.1 Resources
Our bilingual resources were taken from the med-
ical domain. The parallel corpus consisted of
research paper abstracts in English taken from
PubMed4 and the corresponding Japanese transla-
tions.
The training portion consisted of 25,500 sen-
tences (no-clause-seg.; original sentences with-
out clause segmentation). 4,132 English sen-
tences in the corpus were composed of multi-
ple clauses and were separated at the clause level
4http://www.ncbi.nlm.nih.gov/pubmed/
by the procedure in section 3.1. As the syntac-
tic parser, we used the Enju5 (Miyao and Tsu-
jii, 2008) English HPSG parser. For these train-
ing sentences, we automatically aligned Japanese
words with each English clause as described in
section 3.2 and developed a clause-level aligned
corpus, called auto-aligned corpus. We prepared
manually-aligned (oracle) clauses for reference,
called oracle-aligned clauses. The clause align-
ment error rate of the auto-aligned corpus was
14% (number of wrong clause assignments di-
vided by total number of words). The develop-
ment and test portions each consisted of 1,032
multi-clause sentences. because this paper focuses
only on multi-clause sentences. Their English-
side was segmented into clauses in the same man-
ner as the training sentences, and the development
sentences had oracle clause alignment for MERT.
We also used the Life Science Dictionary6 for
training. We extracted 100,606 unique English
entries from the dictionary including entries with
multiple translation options, which we expanded
to one-to-one entries, and finally we obtained
155,692 entries.
English-side tokenization was obtained using
Enju, and we applied a simple preprocessing that
removed articles (a, an, the) and normalized plu-
ral forms to singular ones. Japanese-side tokeniza-
tion was obtained using MeCab7 with ComeJisyo8
(dictionary for Japanese medical document tok-
enization). Our resource statistics are summarized
in Table 1.
4.2 Model and Decoder
We used two decoders in the experiments,
Moses9 (Koehn et al, 2007) and our in-
house hierarchical phrase-based SMT (almost
equivalent to Hiero (Chiang, 2007)). Moses
used a phrase table with a maximum phrase
length of 7, a lexicalized reordering model with
msd-bidirectional-fe, and a distortion
limit of 1210. Our hierarchical phrase-based SMT
used a phrase table with a maximum rule length of
7 and a window size (Hiero?s ?) of 12 11. Both
5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
6http://lsd.pharm.kyoto-u.ac.jp/en/index.html
7http://mecab.sourceforge.net/
8http://sourceforge.jp/projects/comedic/ (in Japanese)
9http://www.statmt.org/moses/
10Unlimited distortion was also tested but the results were
worse.
11A larger window size could not be used due to its mem-
ory requirements.
423
Table 1: Data statistics on training, development,
and test sets. All development and test sentences
are multi-clause sentences.
Training
Corpus Type #words #sentences
Parallel E 690,536
(no-clause-seg.) J 942,913
25,550
Parallel E 135,698
(auto-aligned) J 183,043
4,132
(oracle-aligned) J 183,147
(10,766 clauses)
E 263,175 155.692Dictionary
J 291,455 (entries)
Development
Corpus Type #words #sentences
Parallel E 34,417 1,032
(oracle-aligned) J 46,480 (2,683 clauses)
Test
Corpus Type #words #sentences
Parallel E 34,433 1,032
(clause-seg.) J 45,975 (2,737 clauses)
decoders employed two language models: a word
5-gram language model from the Japanese sen-
tences in the parallel corpus and a word 4-gram
language model from the Japanese entries in the
dictionary. The feature weights were optimized
for BLEU (Papineni et al, 2002) by MERT, using
the development sentences.
4.3 Compared Methods
We compared four different training and test con-
ditions with respect to the use of clauses in training
and testing. The development (i.e., MERT) condi-
tions followed the test conditions. Two additional
conditions with oracle clause alignment were also
tested for reference.
Table 2 lists the compared methods. First,
the proposed method (proposed) used the auto-
aligned corpus in training and clause segmen-
tation in testing. Second, the baseline method
(baseline) did not use clause segmentation in ei-
ther training or testing. Using this standard base-
line method, we focused on the advantages of the
divide-and-conquer translation itself. Third, we
tested the same translation models as used with
the proposed method for test sentences without
clause segmentation, (comp.(1)). Although this
comparison method cannot employ the proposed
clause-level reordering, it was expected to be bet-
ter than the baseline method because its transla-
tion model can be trained more precisely using the
finely aligned clause-level corpus. Finally, the sec-
ond comparison method (comp.(2)) translated seg-
mented clauses with the baseline (without clause
segmentation) model, as if each of them was a sin-
gle sentence. Its translation of each clause was
expected to be better than that of the baseline be-
cause of the efficient search over shortened inputs,
while its reordering of clauses (non-terminals) was
unreliable due to the lack of clause information
in training. Its sentence reconstruction based on
non-terminals was the same as with the proposed
method. Although non-terminals in the second
comparison method were out-of-vocabulary words
and may be deleted in decoding, all of them sur-
vived and we could reconstruct sentences from
translated clauses throughout the experiments. In
addition, two other conditions were tested: us-
ing oracle-aligned clauses in training: the pro-
posed method trained using oracle-aligned (ora-
cle) clauses and the first comparison method using
oracle-aligned (oracle-comp.) clauses.
4.4 Results
Table 3 shows the results in BLEU, Transla-
tion Edit Rate (TER) (Snover et al, 2006),
and Position-independent Word-error Rate (PER)
(Och et al, 2001), obtained with Moses and our
hierarchical phrase-based SMT, respectively. Bold
face results indicate the best scores obtained with
the compared methods (excluding oracles).
The proposed method consistently outper-
formed the baseline. The BLEU improve-
ments with the proposed method over the base-
line and comparison methods were statistically
significant according to the bootstrap sampling
test (p < 0.05, 1,000 samples) (Zhang et al,
2004). With Moses, the improvement when us-
ing the proposed method was 1.4% (33.19% to
34.60%) in BLEU and 1.3% (57.83% to 56.50%)
in TER, with a slight improvement in PER
(35.84% to 35.61%). We observed: oracle ?
proposed ? comp.(1) ? baseline ? comp.(2)
by the Bonferroni method, where the symbol
A ? B means ?A?s improvement over B is
statistically significant.? With the hierarchical
phrase-based SMT, the improvement was 2.2%
(32.39% to 34.55%) in BLEU, 3.5% (58.36% to
54.87%) in TER, and 1.5% in PER (36.42% to
34.79%). We observed: oracle ? proposed ?
424
Table 2: Compared methods.
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. proposed comp.(2) oracle
no-clause-seg. comp.(1) baseline oracle-comp.
{comp.(1), comp.(2)} ? baseline by the Bon-
ferroni method. The oracle results were better than
these obtained with the proposed method but the
differences were not very large.
4.5 Discussion
We think the advantage of the proposed method
arises from three possibilities: 1) better translation
model training using the fine-aligned corpus, 2) an
efficient decoder search over shortened inputs, and
3) an effective clause-level reordering model real-
ized by using non-terminals.
First, the results of the first comparison method
(comp.(1)) indicate an advantage of the transla-
tion models trained using the auto-aligned corpus.
The training of the translation models, namely
word alignment and phrase extraction, is difficult
for long sentences due to their large ambiguity.
This result suggests that the use of clause-level
alignment provides fine-grained word alignments
and precise translation models. We can also ex-
pect that the model of the proposed method will
work better for the translation of single-clause sen-
tences.
Second, the average and median lengths (in-
cluding non-terminals) of the clause-seg. test set
were 13.2 and 10 words, respectively. They were
much smaller than those of no-clause-seg. at 33.4
and 30 words and are expected to help realize
an efficient SMT search. Another observation is
the relationship between the number of clauses
and translation performance, as shown in Fig-
ure 5. The proposed method achieved a greater im-
provement in sentences with a greater number of
clauses. This suggests that our divide-and-conquer
approach works effectively for multi-clause sen-
tences. Here, the results of the second comparison
method (comp.(2)) with Moses were worse than
the baseline results, while there was an improve-
ment with our hierarchical phrase-based SMT.
This probably arose from the difference between
the decoders when translating out-of-vocabulary
words. The non-terminals were handled as out-of-
vocabulary words under the comp.(2) condition.
52
54
56
58
60
62
64
66
2 4 53
TE
R
 (%
)
The number of clauses
baseline
proposed
comp.(2)
Figure 5: Relationship between TER and number
of clauses for proposed, baseline, and comp.(2)
when using our hierarchical phrase-based SMT.
Moses generated erroneous translations around
such non-terminals that can be identified at a
glance, while our hierarchical phrase-based SMT
generated relatively good translations. This may
be a decoder-dependent issue and is not an essen-
tial problem.
Third, the results obtained with the proposed
method reveal an advantage in reordering in ad-
dition to the previous two advantages. The differ-
ence between the PERs with the proposed method
and the baseline with Moses was small (0.2%)
in spite of the large differences in BLEU and
TER (about 1.5%). This suggests that the pro-
posed method is better in word ordering and im-
plies our method is also effective in reordering.
With the hierarchical phrase-based SMT, the pro-
posed method showed a large improvement from
the baseline and comparison methods, especially
in TER which was better than the best Moses
configuration (proposed). This suggests that the
decoding of long sentences with long-distance
reordering is not easy even for the hierarchical
phrase-based SMT due to its limited window size,
while the hierarchical framework itself can natu-
rally model a long-distance reordering. If we try to
find a derivation with such long-distance reorder-
ing, we will probably be faced with an intractable
search space and computation time. Therefore,
we can conclude that the proposed divide-and-
425
Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,
TER, and PER.
Moses : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.60 / 56.50 / 35.61 32.14 / 58.78 / 36.08 35.31 / 55.12 / 34.42
no-clause-seg. 34.22 / 56.90 / 35.20 33.19 / 57.83 / 35.84 34.24 / 56.67 / 35.03
Hierarchical : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.55 / 54.87 / 34.79 33.03 / 56.70 / 36.03 35.08 / 54.22 / 34.77
no-clause-seg. 33.41 / 57.02 / 35.86 32.39 / 58.36 / 36.42 33.83 / 56.26 / 34.96
conquer approach provides more practical long-
distance reordering at the clause level.
We also analyzed the difference between auto-
matic and manual clause alignment. Since auto-
aligned corpus had many obvious alignment er-
rors, we suspected these noisy clauses hurt the
clause translation model. However, they were not
serious in terms of final translation performance.
So we can conclude that our proposed divide-and-
conquer approach is promising for long sentence
translation. Although we aimed to see whether we
could bootstrap using existing bilingual corpora in
this paper, we imagine better clause alignment can
be obtained with some supervised classifiers.
One problem with the divide-and-conquer ap-
proach is that its independently-translated clauses
potentially cause disfluencies in final sentence
translations, mainly due to wrong inflections. A
promising solution is to optimize a whole sentence
translation by integrating search of each clause
translation but this may require a much larger
search space for decoding. More simply, we may
be able to approximate it using n-best clause trans-
lations. This problem should be addressed for fur-
ther improvement in future studies.
5 Conclusion
In this paper we proposed a clause-based divide-
and-conquer approach for SMT that can re-
duce complicated clause-level reordering to sim-
ple word-level reordering. The proposed method
separately translates clauses with non-terminals by
using a well-known SMT method and reconstructs
a sentence based on the non-terminals, to reorder
long clauses. The clause translation models are
trained using a bilingual corpus with clause-level
alignment, which can be obtained with an un-
supervised graph-based method using sentence-
aligned corpora. The proposed method improves
the translation of long, multi-clause sentences and
is especially effective for language pairs with
large word order differences, such as English-to-
Japanese.
This paper focused only on clauses as segments
for division. However, other long segments such
as prepositional phrases are similarly difficult to
reorder correctly. The divide-and-conquer ap-
proach itself can be applied to long phrases, and
it is worth pursuing such an extension. As another
future direction, we must develop a more sophis-
ticated method for automatic clause alignment if
we are to use the proposed method for various lan-
guage pairs and domains.
Acknowledgments
We thank the U. S. National Library of Medicine
for the use of PubMed abstracts and Prof. Shuji
Kaneko of Kyoto University for the use of Life
Science Dictionary. We also thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL, pages 531?540.
426
Osamu Furuse, Setsuo Yamada, and Kazuhide Ya-
mamoto. 1998. Splitting long or ill-formed in-
put for robust spoken-language translation. In Proc.
COLING-ACL, pages 421?427.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. NAACL, pages 273?280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT-NAACL, pages 105?
112.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic reordering in preprocessing for Japanese-English
translation: MIT system description for NTCIR-7
patent translation task. In Proc. NTCIR-7, pages
409?414.
Yeun-Bae Kim and Terumasa Ehara. 1994. A method
for partitioning of long Japanese sentences with sub-
ject resolution in J/E machine translation. In Proc.
International Conference on Computer Processing
of Oriental Languages, pages 467?473.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 263?270.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. ACL, pages 720?727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String alignment template for statistical machine
translation. In Proc. Coling-ACL, pages 609?616.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Coling-ACL, pages 25?32.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statis-
tical machine translation. In Proc. the ACL Work-
shop on Data-Driven Methods in Machine Transla-
tion, pages 55?62.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proc.
HLT-NAACL, pages 101?104.
Erik F. Tjong, Kim Sang, and Herve? De?jean. 2001. In-
troduction to the CoNLL-2001 shared task: Clause
identification. In Proc. CoNLL, pages 53?57.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. COLING, pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
HLT-NAACL, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL,
pages 523?530.
Ying Zhang, Stephan Vogel, and Alex Weibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proc. LREC, pages 2051?2054.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proc. ICML, pages
912?919.
427
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57?66,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Head Finalization Reordering for Chinese-to-Japanese
Machine Translation
Han Dan+ Katsuhito Sudoh? Xianchao Wu??
Kevin Duh?? Hajime Tsukada? Masaaki Nagata?
+The Graduate University For Advanced Studies, Tokyo, Japan
?NTT Communication Science Laboratories, NTT Corporation
+handan@nii.ac.jp, ?wuxianchao@baidu.com, ?kevinduh@is.naist.jp
?{sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp
Abstract
In Statistical Machine Translation, reorder-
ing rules have proved useful in extracting
bilingual phrases and in decoding during
translation between languages that are struc-
turally different. Linguistically motivated
rules have been incorporated into Chinese-
to-English (Wang et al, 2007) and English-
to-Japanese (Isozaki et al, 2010b) transla-
tion with significant gains to the statistical
translation system. Here, we carry out a lin-
guistic analysis of the Chinese-to-Japanese
translation problem and propose one of the
first reordering rules for this language pair.
Experimental results show substantially im-
provements (from 20.70 to 23.17 BLEU)
when head-finalization rules based on HPSG
parses are used, and further gains (to 24.14
BLEU) were obtained using more refined
rules.
1 Introduction
In state-of-the-art Statistical Machine Translation
(SMT) systems, bilingual phrases are the main
building blocks for constructing a translation given
a sentence from a source language. To extract
those bilingual phrases from a parallel corpus,
the first step is to discover the implicit word-
to-word correspondences between bilingual sen-
tences (Brown et al, 1993). Then, a symmetriza-
tion matrix is built (Och and Ney, 2004) by us-
ing word-to-word alignments, and a wide variety
?Now at Baidu Japan Inc.
? Now at Nara Institute of Science and Technology
(NAIST)
of heuristics can be used to extract the bilingual
phrases (Zens et al, 2002; Koehn et al, 2003).
This method performs relatively well when the
source and the target languages have similar word
order, as in the case of French, Spanish, and En-
glish. However, when translating between lan-
guages with very different structures, as in the case
of English and Japanese, or Japanese and Chinese,
the quality of extracted bilingual phrases and the
overall translation quality diminishes.
In the latter scenario, a simple but effective strat-
egy to cope with this problem is to reorder the
words of sentences in one language so that it re-
sembles the word order of another language (Wu
et al, 2011; Isozaki et al, 2010b). The advan-
tages of this strategy are two fold. The first ad-
vantage is at the decoding stage, since it enables
the translation to be constructed almost monoton-
ically. The second advantage is at the training
stage, since automatically estimated word-to-word
alignments are likely to be more accurate and sym-
metrization matrices reveal more evident bilingual
phrases, leading to the extraction of better quality
bilingual phrases and cleaner phrase tables.
In this work, we focus on Chinese-to-Japanese
translation, motivated by the increasing interaction
between these two countries and the need to im-
prove direct machine translation without using a
pivot language. Despite the countries? close cul-
tural relationship, their languages significantly dif-
fer in terms of syntax, which poses a severe diffi-
culty in statistical machine translation. The syntac-
tic relationship of this language pair has not been
carefully studied before in the machine translation
57
field, and our work aims to contribute in this direc-
tion as follows:
? We present a detailed syntactic analysis of
several reordering issues in Chinese-Japanese
translation using the information provided by
an HPSG-based deep parser.
? We introduce novel reordering rules based on
head-finalization and linguistically inspired
refinements to make words in Chinese sen-
tences resemble Japanese word order. We em-
pirically show its effectiveness (e.g. 20.70 to
24.23 BLEU improvement).
The paper is structured as follows. Section 2 in-
troduces the background and gives an overview of
similar techniques related to this work. Section 3
describes the proposed method in detail. Exper-
imental evaluation of the performance of the pro-
posed method is described in section 4. There is an
error analysis on the obtained results in section 5.
Conclusions and a short description on future work
derived from this research are given in the final
section.
2 Background
2.1 Head Finalization
The structure of languages can be characterized
by phrase structures. The head of a phrase is the
word that determines the syntactic category of the
phrase, and its modifiers (also called dependents)
are the rest of the words within the phrase. In En-
glish, the head of a phrase can be usually found
before its modifiers. For that reason, English is
called a head-initial language (Cook and Newson,
1988). Japanese, on the other hand, is head-final
language (Fukui, 1992), since the head of a phrase
always appears after its modifiers.
In certain applications, as in the case of ma-
chine translation, word reordering can be a promis-
ing strategy to ease the task when working with
languages with different phrase structures like En-
glish and Japanese. Head Finalization is a success-
ful syntax-based reordering method designed to re-
order sentences from a head-initial language to re-
semble the word order in sentences from a head-
final language (Isozaki et al, 2010b). The essence
of this rule is to move the syntactic heads to the
end of its dependency by swapping child nodes in
a phrase structure tree when the head child appears
before the dependent child.
Isozaki et al (2010b) proposed a simple method
of Head Finalization, by using an HPSG-based
deep parser for English (Miyao and Tsujii, 2008)
to obtain phrase structures and head information.
The score results from several mainstream evalua-
tion methods indicated that the translation quality
had been improved; the scores of Word Error Rate
(WER) and Translation Edit Rate (TER) (Snover
et al, 2006) had especially been greatly reduced.
2.2 Chinese Deep Parsing
Syntax-based reordering methods need parsed sen-
tences as input. Isozaki et al (2010b) used Enju,
an HPSG-based deep parser for English, but they
also discussed using other types of parsers, such
as word dependency parsers and Penn Treebank-
style parsers. However, to use word dependency
parsers, they needed an additional heuristic rule to
recover phrase structures, and Penn Treebank-style
parsers are problematic because they output flat
phrase structures (i.e. a phrase may have multiple
dependents, which causes a problem of reorder-
ing within a phrase). Consequently, compared to
different types of parsers, Head-Final English per-
forms the best on the basis of English Enju?s pars-
ing result.
In this paper, we follow their observation, and
use the HPSG-based parser for Chinese (Chinese
Enju) (Yu et al, 2011) for Chinese syntactic pars-
ing. Since Chinese Enju is based on the same pars-
ing model as English Enju, it provides rich syn-
tactic information including phrase structures and
syntactic/semantic heads.
Figure 1 shows an example of an XML output
from Chinese Enju for the sentence ?wo (I) qu (go
to) dongjing (Tokyo) he (and) jingdu (Ky-
oto).? The label <cons> and <tok> represent
the non-terminal nodes and terminal nodes, respec-
tively. Each node is identified by a unique ?id?
and has several attributes. The attribute ?head?
indicates which child node is the syntactic head.
In this figure, <head=?c4? id=?c3?> means that
the node that has id=?c4? is the syntactic head of
the node that has id=?c3?.
58
Figure 1: An XML output for a Chinese sentence from
Chinese Enju. For clarity, we only draw information
related to the phrase structure and the heads.
2.3 Related Work
Reordering is a popular strategy for improving
machine translation quality when source and tar-
get languages are structurally very different. Re-
searchers have approached the reordering problem
in multiple ways. The most basic idea is pre-
ordering (Xia and McCord, 2004; Collins et al,
2005), that is, to do reordering during preprocess-
ing time, where the source side of the training and
development data and sentences from a source lan-
guage that have to be translated are first reordered
to ease the training and the translation, respec-
tively. In (Xu et al, 2009), authors used a depen-
dency parser to introduce manually created pre-
ordering rules to reorder English sentences when
translating into five different SOV(Subject-Object-
Verb) languages. Other authors (Genzel, 2010; Wu
et al, 2011) use automatically generated rules in-
duced from parallel data. Tillmann (2004) used a
lexical reordering model, and Galley et al (2004)
followed a syntactic-based model.
In this work, however, we are centered in the
design of manual rules inspired by the Head Final-
ization (HF) reordering (Isozaki et al, 2010b). HF
reordering is one of the simplest methods for pre-
ordering that significantly improves word align-
ments and leads to a better translation quality. Al-
though the method is limited to translation where
the target language is head-final, it requires neither
training data nor fine-tuning. To our knowledge,
HF is the best method to reorder languages when
translating into head-final languages like Japanese.
The implementation of HF method for English-
to-Japanese translation appears to work well. A
reasonable explanation for this is the close match
between the concept of ?head? in this language
pair. However, for Chinese-to-Japanese, there are
differences in the definitions of numbers of impor-
tant syntactic concepts, including the definition of
the syntactic head. We concluded that the diffi-
culties we encountered in using HF to Chinese-to-
Japanese translation were the result of these differ-
ences in the definition of ?head?. As we believe
that such differences are also likely to be observed
in other language pairs, the present work is gener-
ally important for head-initial to head-final trans-
lation as it shows a systematic linguistic analysis
that consistently improves the effectivity of the HF
method.
3 Syntax-based Reordering Rules
This section describes our method for syntax-
based reordering for Chinese-to-Japanese transla-
tion. We start by introducing Head Finalization
for Chinese (HFC), which is a simple adaptation
of Isozaki et al (2010b)?s method for English-to-
Japanese translation. However, we found that this
simple method has problems when applied to Chi-
nese, due to peculiarities in Chinese syntax. In
Section 3.2, we analyze several distinctive cases of
the problem in detail. And following this analysis,
Section 3.3 proposes a refinement of the original
HFC, with a couple of exception rules for reorder-
ing.
3.1 Head Finalization for Chinese (HFC)
Since Chinese and English are both known to be
head-initial languages1, the reordering rule intro-
duced in (Isozaki et al, 2010b) ideally would re-
order Chinese sentences to follow the word order
1As Gao (2008) summarized, whether Chinese is a head-
initial or a head-final language is open for debate. Neverthe-
less, we take the view that most Chinese sentence structures
are head-initial since the written form of Chinese mainly be-
haves as an head-initial language.
59
Figure 2: Simple example for Head-Final Chinese. The left figure shows the parsing tree of the original sentence
and its English translation. The right figure shows the reordered sentence along with its Japanese translation.
( ?*? indicate the syntactic head).
of their Japanese counterparts.
Figure 2 shows an example of a head finalized
Chinese sentence based on the output from Chi-
nese Enju shown in Figure 1. Notice that the
coordination exception rule described in (Isozaki
et al, 2010b) also applies to Chinese reordering.
This exception rule says that child nodes are not
swapped if the node is a coordination2. Another
exception rule is for punctuation symbols, which
are also preserved in their original order. In this
case, as can be seen in the example in Figure 2, the
nodes of c3, c6, and c8 had not been swapped with
their dependency. In this account, only the verb
?qu? had been moved to the end of the sentence,
following the same word order as its Japanese
translation.
3.2 Discrepancies in Head Definition
Head Finalization relies on the idea that head-
dependent relations are largely consistent among
different languages while word orders are differ-
ent. However, in Chinese, there has been much
debate on the definition of head3, possibly because
Chinese has fewer surface syntactic features than
other languages like English and Japanese. This
causes some discrepancies between the definitions
2Coordination is easily detected in the output of
Enju; it is marked by the attributes xcat="COOD" or
schema="coord-left/right" as shown in Figure 1.
3In this paper, we only consider the syntactic head.
of the head in Chinese and Japanese, which leads
to undesirable reordering of Chinese sentences.
Specifically, in preliminary experiments we ob-
served unexpected reorderings that are caused by
the differences in the head definitions, which we
describe below.
3.2.1 Aspect Particle
Although Chinese has no syntactic tense marker,
three aspect particles following verbs can be used
to identify the tense semantically. They are ?le0?
(did), ?zhe0? (doing), and ?guo4? (done), and
their counterparts in Japanese are ?ta?, ?teiru?,
and ?ta?, respectively. Both the first word and
third word can represent the past tense, but the
third one is more often used in the past perfect.
The Chinese parser4 treated aspect particles as
dependents of verbs, whereas their Japanese coun-
terparts are identified as the head. For exam-
ple in Table 15, ?qu? (go) and ?guo? (done)
aligned with ?i? and ?tta?, respectively. How-
ever, since ?guo? is treated as a dependent of
?qu?, by directly implementing the Head Final
Chinese (HFC), the sentence will be reordered like
4The discussions in this section presuppose the syntactic
analysis done by Chinese Enju, but most of the analysis is
consistent with the common explanation for Chinese syntax.
5English translation (En); Chinese original sentence
(Ch); reordered Chinese by Head-Final Chinese (HFC); re-
ordered Chinese by Refined Head-Final Chinese (R-HFC)
and Japanese translation (Ja).
60
HFC in Table 1, which does not follow the word
order of the Japanese (Ja) translation. In contrast,
the reordered sentence from refined-HFC (R-HFC)
can be translated monotonically.
En I have been to Tokyo.
Ch wo qu guo dongjing.
HFC wo dongjing guo qu.
R-HFC wo dongjing qu guo.
Ja watashi (wa) Tokyo (ni) i tta.
Table 1: An example for Aspect Particle. Best word
alignment Ja-Ch (En): ?watashi? ? ?wo?(I); ?Tokyo? ?
?dongjing? (Tokyo); ?i? ? ?qu? (been); ?tta? ? ?guo?
(have).
3.2.2 Adverbial Modifier ?bu4?
Both in Chinese and Japanese, verb phrase mod-
ifiers typically occur in pre-verbal positions, espe-
cially when the modifiers are adverbs. Since ad-
verbial modifiers are dependents in both Chinese
and Japanese, head finalization works perfectly for
them. However, there is an exceptional adverb,
?bu4?, which means negation and is usually trans-
lated into ?nai?, which is always at the end of the
sentence in Japanese and thus is the head. For ex-
ample in Table 2, the word ?kan? (watch) will be
identified as the head and the word ?bu? is its de-
pendent; on the contrary, in the Japanese transla-
tion (Ja), the word ?nai?, which is aligned with
?bu?, will be identified as the head. Therefore,
the Head Final Chinese is not in the same order,
but the reordered sentence by R-HFC obtained the
same order with the Japanese translation.
En I do not watch TV.
Ch wo bu kan dianshi.
HFC wo dianshi bu kan.
R-HFC wo dianshi kan bu.
Ja watashi (wa) terebi (wo) mi nai.
Table 2: An example for Adverbial Modifier bu4.
Best word alignment Ja-Ch (En): ?watashi? ? ?wo? (I);
?terebi? ? ?dianshi? (TV); ?mi? ? ?kan? (watch); ?nai?
? ?bu? (do not).
3.2.3 Sentence-final Particle
Sentence-final particles often appear at the end
of a sentence to express a speaker?s attitude:
e.g. ?ba0, a0? in Chinese, and ?naa, nee? in
Japanese. Although they appear in the same posi-
tion in both Chinese and Japanese, in accordance
with the differences of head definition, they are
identified as the dependent in Chinese while they
are the head in Japanese. For example in Table 3,
since ?a0? was identified as the dependent, it had
been reordered to the beginning of the sentence
while its Japanese translation ?nee? is at the end
of the sentence as the head. Likewise, by refining
the HFC, we can improve the word alignment.
En It is good weather.
Ch tianqi zhenhao a.
HFC a tianqi zhenhao.
R-HFC tianqi zhenhao a.
Ja ii tennki desu nee.
Table 3: An example for Sentence-final Particle.
Best word alignment Ja-Ch (En): ?tennki? ? ?tianqi?
(weather); ?ii? ? ?zhenhao? (good); ?nee? ? ?a? (None).
3.2.4 Et cetera
In Chinese, there are two expressions for rep-
resenting the meaning of ?and other things? with
one Chinese character: ?deng3? and ?deng3
deng3?, which are both identified as dependent
of a noun. In contrast, in Japanese, ?nado? is al-
ways the head because it appears as the right-most
word in a noun phrase. Table 4 shows an example.
En Fruits include apples, etc.
Ch shuiguo baokuo pingguo deng.
HFC shuiguo deng pingguo baokuo.
R-HFC shuiguo pingguo deng baokuo.
Ja kudamono (wa) ringo nado (wo)
fukunde iru.
Table 4: An example for Et cetera. Best word alignment
Ja-Ch (En): ?kudamono? ? ?shuiguo? (Fruits); ?ringo?
? ?pingguo? (apples); ?nado? ? ?deng? (etc.); ?fukunde
iru? ? ?baokuo? (include).
61
AS Aspect particle
SP Sentence-final particle
ETC et cetera (i.e. deng3 and deng3 deng3)
IJ Interjection
PU Punctuation
CC Coordinating conjunction
Table 5: The list of POSs for exception reordering rules
3.3 Refinement of HFC
In the preceding sections, we have discussed syn-
tactic constructions that cause wrong application
of Head Finalization to Chinese sentences. Fol-
lowing the observations, we propose a method to
improve the original Head Finalization reordering
rule to obtain better alignment with Japanese.
The idea is simple: we define a list of POSs,
and when we find one of them as a dependent
child of the node, we do not apply reordering. Ta-
ble 5 shows the list of POSs we define in the cur-
rent implementation6. While interjections are not
discussed in detail, we should obviously not re-
order to interjections because they are position-
independent. The rules for PU and CC are ba-
sically equivalent to the exception rules proposed
by (Isozaki et al, 2010b).
4 Experiments
The corpus we used as training data comes
from the China Workshop on Machine Transla-
tion (CWMT) (Zhao et al, 2011). This is a
Japanese-Chinese parallel corpus in the news do-
main, containing 281, 322 sentence pairs. We also
collected another Japanese-Chinese parallel cor-
pus from news containing 529, 769 sentences and
merged it with the CWMT corpus to create an ex-
tended version of the CWMT corpus. We will re-
fer to this corpus as ?CWMT ext.? We split an in-
verted multi-reference set into a development and a
test set containing 1, 000 sentences each. In these
two sets, the Chinese input was different, but the
Japanese reference was identical. We think that
this split does not pose any severe problem to the
comparison fairness of the experiment, since no
new phrases are added during tuning and the ex-
perimental conditions remain equal for all tested
6The POSs are from Penn Chinese Treebank.
Ch Ja
CWMT
Sentences 282K
Run. words 2.5M 3.2M
Avg. sent. leng. 8.8 11.5
Vocabulary 102K 42K
CWMT ext.
Sentences 811K
Run. words 14.7M 17M
Avg. sent. leng. 18.1 20.9
Vocabulary 249K 95K
Dev.
Sentences 1000
Run. words 29.9K 35.7K
Avg. sent. leng. 29.9 35.7
OoV w.r.t. CWMT 485 106
OoV w.r.t. CWMT ext. 244 53
Test
Sentences 1000
Run. words 25.8K 35.7K
Avg. sent. leng. 25.8 35.7
OoV w.r.t. CWMT 456 106
OoV w.r.t. CWMT ext. 228 53
Table 6: Characteristics of CWMT and extended
CWMT Chinese-Japanese corpus. Dev. stands for De-
velopment, OoV for ?Out of Vocabulary? words, K for
thousands of elements, and M for millions of elements.
Data statistics were collected after tokenizing.
methods. Detailed Corpus statistics can be found
in Table 6.
To parse Chinese sentences, we used Chinese
Enju (Yu et al, 2010), an HPSG-based parser
trained with the Chinese HPSG treebank converted
from Penn Chinese Treebank. Chinese Enju re-
quires segmented and POS-tagged sentences to
do parsing. We used the Stanford Chinese seg-
menter (Chang et al, 2008) and Stanford POS-
tagger (Toutanova et al, 2003) to obtain the seg-
mentation and POS-tagging of the Chinese side of
the training, development, and test sets.
The baseline system was trained following
the instructions of recent SMT evaluation cam-
paigns (Callison-Burch et al, 2010) by using the
MT toolkit Moses (Koehn et al, 2007) in its de-
fault configuration. Phrase pairs were extracted
from symmetrized word alignments and distor-
tions generated by GIZA++ (Och and Ney, 2003)
using the combination of heuristics ?grow-diag-
final-and? and ?msd-bidirectional-fe?. The lan-
guage model was a 5-gram language model es-
timated on the target side of the parallel cor-
pora by using the modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999) implemented in
62
the SRILM (Stolcke, 2002) toolkit. The weights
of the log-linear combination of feature functions
were estimated by using MERT (Och, 2003) on the
development set described in Table 6.
The effectiveness of the reorderings proposed
in Section 3.3 was assessed by using two preci-
sion metrics and two error metrics on translation
quality. The first evaluation metric is BLEU (Pap-
ineni et al, 2002), a very common accuracy metric
in SMT that measures N -gram precision, with a
penalty for too short sentences. The second eval-
uation metric was RIBES (Isozaki et al, 2010a), a
recent precision metric used to evaluate translation
quality between structurally different languages. It
uses notions on rank correlation coefficients and
precision measures. The third evaluation metric is
TER (Snover et al, 2006), another error metric that
computes the minimum number of edits required
to convert translated sentences into its correspond-
ing references. Possible edits include insertion,
deletion, substitution of single words, and shifts of
word sequences. The fourth evaluation metric is
WER, an error metric inspired in the Levenshtein
distance at word level. BLEU, WER, and TER
were used to provide a sense of comparison but
they do not significantly penalize long-range word
order errors. For this reason, RIBES was used to
account for this aspect of translation quality.
The baseline system was trained and tuned us-
ing the same configuration setup described in this
section, but no reordering rule was implemented at
the preprocessing stage.
Three systems have been run to translate the test
set for comparison when the systems were trained
using the two training data sets. They are the
baseline system, the system consisting in the na??ve
implementation of HF reordering, and the system
with refined HFC reordering rules. Assessment of
translation quality can be found in Table 7.
As can be observed in Table 7, the translation
quality, as measured by precision and error met-
rics, was consistently and significantly increased
when the HFC reordering rule was used and was
significantly improved further when the refinement
proposed in this work was used. Specifically, the
BLEU score increased from 19.94 to 20.79 when
the CWMT corpus was used, and from 23.17 to
24.14 when the extended CWMT corpus was used.
AS SP ETC IJ PU COOD
3.8% 0.8% 1.3% 0.0%* 21.0% 38.3%
Table 8: Weighted recall of each exception rule during
reordering on CWMT ext. training data, dev data, and
test data. (* actual value 0.0016%.)
Table 8 shows the recall of each exception rule
listed in Section 3, and was computed by counting
the times an exception rule was triggered divided
by the number of times the head finalization rule
applied. Data was collected for CWMT ext. train-
ing, dev and test sets. Although the exception rules
related to aspect particles, Et cetera, sentence-final
particles and interjections have a comparatively
lower frequency of application than punctuation
or coordination exception rules, the improvements
they led to are significant.
5 Error Analysis
In Section 3 we have analyzed syntactic differ-
ences between Chinese and Japanese that led to
the design of an effective refinement. A manual
error analysis of the results of our refined reorder-
ing rules showed that some more reordering issues
remain and, although they are not side effects of
our proposed rule, they are worth mentioning in
this separate section.
5.1 Serial Verb Construction
Serial verb construction is a phenomenon occur-
ring in Chinese, where several verbs are put to-
gether as one unit without any conjunction be-
tween them. The relationship between these
verbs can be progressive or parallel. Apparently,
Japanese has a largely corresponding construc-
tion, which indicates that no reordering should
be applied. An example to illustrate this fact in
Chinese is ?weishi (maintain) shenhua (deepen)
zhongriguanxi (Japan-China relations) de
(of) gaishan (improvement) jidiao (basic
tone).?7 The two verbs ?weishi? (in Japanese,
iji) and ?shenhua? (in Japanese, shinka) are
used together, and they follow the same order as
in Japanese: ?nicchukankei (Japan-China re-
7English translation: Maintain and deepen the improved
basic tone of Japan-China relations.
63
CWMT CWMT ext.
BLEU RIBES TER WER BLEU RIBES TER WER
baseline 16.74 71.24 70.86 77.45 20.70 74.21 66.10 72.36
HFC 19.94 73.49 65.19 71.39 23.17 75.35 61.38 67.74
refined HFC 20.79 75.09 64.91 70.39 24.14 77.17 59.67 65.31
Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for
training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed
refinement of head finalization reordering rules.
lations) no (of) kaizan (improvement) kityo
(basic tone) wo iji (maintain) shinka (deepen)
suru (do).?
5.2 Complementizer
A ?complementizer? is a particle used to intro-
duce a complement. In English, a very common
complementizer is the word ?that? when making a
clausal complement, while in Chinese it can de-
note other types of word, such as verbs, adjec-
tives or quantifiers. The complementizer is iden-
tified as the dependent of the verb that it modi-
fies. For instance, a Chinese sentence: ?wo (I)
mang wan le (have finished the work).? This
can be translated into Japanese: ?watashi (I) wa
shigoto (work) wo owa tta (have finished).? In
Chinese, the verb ?mang? is the head while ?wan?
is the complementizer, and its Japanese counter-
part ?owa tta? has the same word order.
However, during the reordering, ?mang? will be
placed at the end of the sentence and ?wan? in the
beginning, leading to an inconsistency with respect
to the Japanese translation where the complemen-
tizer ?tta? is the head.
5.3 Verbal Nominalization and Nounal
Verbalization
As discussed by Guo (2009), compared to English
and Japanese, Chinese has little inflectional mor-
phology, that is, no inflection to denote tense, case,
etc. Thus, words are extremely flexible, making
verb nominalization and noun verbalization appear
frequently and commonly without any conjugation
or declension. As a result, it is difficult to do dis-
ambiguation during POS tagging and parsing. For
example, the Chinese word ?kaifa? may have
two syntactic functions: verb (develop) and noun
(development). Thus, it is difficult to reliably tag
without considering the context. In contrast, in
Japanese, ?suru? can be used to identify verbs.
For example, ?kaihatu suru? (develop) is a
verb and ?kaihatu? (development) is a noun.
This ambiguity is prone to not only POS tagging
error but also parsing error, and thus affects the
identification of heads, which may lead to incor-
rect reordering.
5.4 Adverbial Modifier
Unlike the adverb ?bu4? we discussed in Sec-
tion 3.2, the ordinary adverbial modifier comes
directly before the verb it modifies both in Chi-
nese and Japanese, but not in English. Nev-
ertheless, in accordance with the principle of
identifying the head for Chinese, the adverb
will be treated as the dependent and it will
not be reordered following the verb it modi-
fied. As a result, the alignment between adverbs
and verbs is non-monotonic. This can be ob-
served in the Chinese sentence ?guojia (coun-
try) yanli (severely) chufa (penalize) jiage
(price) weifa (violation) xingwei (behavior)?8,
and its Japanese translation: ?kuni (country) wa
kakaku (price) no ihou (violation) koui (be-
havior) wo kibisiku (severely) syobatu (penal-
ize).? Both in Chinese and Japanese, the adverbial
modifier ?yanli? and ?kibisiku? are directly
in front of the verb ?chufa? and ?syobatu?, re-
spectively. However, the verb in Chinese is identi-
fied as the head and will be reordered to the end of
the sentence without the adverb.
8English translation: The country severely penalizes vio-
lations of price restrictions.
64
5.5 POS tagging and Parsing Errors
There were word reordering issues not caused
solely by differences in syntactic structures. Here
we summarize two that are difficult to remedy dur-
ing reordering and that are hard to avoid since re-
ordering rules are highly dependent on the tagger
and parser.
? POS tagging errors
In Chinese, for example, the word ?Iran?
was tagged as ?VV? or ?JJ? instead of ?NR?.
This led to identifying ?Iran? as a head in
accordance with the head definition in Chi-
nese, and it was reordered undesirably.
? Parsing errors
For example, in the Chinese verb phrase
?touzi (invest) 20 yi (200 million)
meiyuan (dollars)?, ?20? and ?yi? were
identified as dependent of ?touzi? and
?meiyuan?, respectively, which led to an
unsuitable reordering for posterior word
alignment.
6 Conclusion and Future Work
In the present work, we have proposed novel
Chinese-to-Japanese reordering rules inspired
in (Isozaki et al, 2010b) based on linguistic analy-
sis on Chinese HPSG and differences among Chi-
nese and Japanese. Although a simple implemen-
tation of HF to reorder Chinese sentences per-
forms well, translation quality was substantially
improved further by including linguistic knowl-
edge into the refinement of the reordering rules.
In Section 5, we found more patterns on reorder-
ing issues when reordering Chinese sentences to
resemble Japanese word order. The extraction of
those patterns and their effective implementation
may lead to further improvements in translation
quality, so we are planning to explore this possi-
bility.
In this work, syntactic information from a deep
parser has been used to reorder words better. We
believe that using semantic information can fur-
ther increase the expressive power of reordering
rules. With that objective, Chinese Enju can be
used since it provides the semantic head of nodes
and can interpret sentences by using their semantic
dependency.
Acknowledgments
This work was mainly developed during an intern-
ship at NTT Communication Science Laborato-
ries. We would like to thank Prof. Yusuke Miyao
for his invaluable support on this work.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of ma-
chine translation. In Computational Linguistics, vol-
ume 19, pages 263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the joint 5th workshop on Statistical Ma-
chine Translation and MetricsMATR. Association
for Computational Linguistics, July.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word seg-
mentation for machine translation performance. In
Proceedings of the 3rd Workshop on SMT, pages
224?232, Columbus, Ohio. Association for Compu-
tational Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Vivian James Cook and Mark Newson. 1988. Chom-
sky?s Universal Grammar: An introduction. Oxford:
Basil Blackwell.
Naoki Fukui. 1992. Theory of Projection in Syntax.
CSLI Publisher and Kuroshio Publisher.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. Whats in a translation rule?
In Proceedings of HLT-NAACL.
Qian Gao. 2008. Word order in mandarin: Reading and
speaking. In Proceedings of the 20th North Ameri-
can Conference on Chinese Linguistics (NACCL-20),
volume 2, pages 611?626.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
65
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuqing Guo. 2009. Treebank-based acquisition of
Chinese LFG resources for parsing and generation.
Ph.D. thesis, Dublin City University.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of Empirical Methods on Nat-
ural Language Processing (EMNLP).
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMTMetricsMATR, pages 244?251.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings
HLT/NAACL?03, pages 48?54.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL Demo and Poster Sessions, 2007, pages
177?180, June 25?27.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34:35?80, March.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st annual conference of the Association for Com-
putational Linguistics, 2003, pages 160?167, July 7?
12.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual conference of the Association for Com-
putational Linguistics, 2002, pages 311?318, July 6?
12.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on Spoken Language Pro-
cessing, 2002, pages 901?904, September 16?20.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings OF HLT-NAACL, pages 252?259.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 737?
745, Prague, Czech Republic, June. Association for
Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 29?
37, Chiang Mai, Thailand, November. Asian Feder-
ation of Natural Language Processing.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 245?253, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Kun Yu, Yusuke Miyao, Xiangli Wang, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2010. Semi-
automatically developing chinese hpsg grammar
from the penn chinese treebank for deep parsing. In
COLING (Posters)?10, pages 1417?1425.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
the 12th International Conference on Parsing Tech-
nologies, pages 48?57.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of
KI?02, pages 18?32.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th china workshop on machine translation
(cwmt2011). The 7th China Workshop on Machine
Translation (CWMT2011).
66
