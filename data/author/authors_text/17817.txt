Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 1?12,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multi-document multilingual summarization corpus preparation, Part 1:
Arabic, English, Greek, Chinese, Romanian
Lei Li
BUPT, China
leili@bupt.edu.cn
Corina Forascu
RACAI, Romania
UAIC, Romania
corinfor@info.uaic.ro
Mahmoud El-Haj
Lancaster Univ., UK
m.el-haj@lancaster.ac.uk
George Giannakopoulos
NCSR Demokritos, Greece
SciFY NPC, Greece
ggianna@iit.demokritos.gr
Abstract
This document overviews the strategy, ef-
fort and aftermath of the MultiLing 2013
multilingual summarization data collec-
tion. We describe how the Data Contrib-
utors of MultiLing collected and gener-
ated a multilingual multi-document sum-
marization corpus on 10 different lan-
guages: Arabic, Chinese, Czech, English,
French, Greek, Hebrew, Hindi, Romanian
and Spanish. We discuss the rationale be-
hind the main decisions of the collection,
the methodology used to generate the mul-
tilingual corpus, as well as challenges and
problems faced per language. This paper
overviews the work on Arabic, Chinese,
English, Greek, and Romanian languages.
A second part, covering the remaining lan-
guages, is available as a distinct paper in
the MultiLing 2013 proceedings.
1 Introduction
Summarization has recently received the focus
of media attention (Cahan, 2013; Shih, 2013), due
to a set of corporate buy-outs related to summariza-
tion technology companies. This trend of applying
summarization is the result of a long research effort
related to summarization. Previously, especially
within the Text Analysis Conference (TAC) series
of workshops (Dang, 2005; Dang, 2006; Dang and
Owczarzak, 2008), multi-document summariza-
tion has covered aspects of summarization such
as update summarization, guided summarization
and cross-lingual summarization. In TAC 2011
the MultiLing Pilot (Giannakopoulos et al, 2011)
was introduced: a combined community effort to
present and promote multi-document summariza-
tion apporaches that are (fully or partly) language-
neutral. To support this effort an organizing com-
mittee across more than six countries was assigned
to create a multi-lingual corpus on news texts, cov-
ering seven different languages: Arabic, Czech,
English, French, Greek, Hebrew, Hindi.
The Pilot gave birth to an active community of
researchers, who provided the effort and know-
how to realize a continuation of the original ef-
fort: MultiLing 2013. The MultiLing 2013 Work-
shop, taking place within ACL 2013, built upon
the existing corpus of MultiLing 2011 to provide
additional languages and challenges for summa-
rization systems. This year 3 new languages were
added: Chinese, Romanian and Spanish. Further-
more, more texts were added to most existing cor-
pus languages (with the exception of French and
Hindi).
In the following paragraphs we first overview
theMultiLing tasks, for which the corpus was built
(Section 2). We then describe the rationale and
strategy applied for the corpus collection and cre-
ation (Section 3). We continue with special com-
ments for the English, Greek, Chinese and Roma-
nian languages (Section 4). Finally, we summarize
the findings at the end of this paper (Section 5). We
note that a second paper (Elhadad et al, 2013) de-
scribes the language-specific notes related to the
rest of the MultiLing 2013 language contributions
(Czech, Hebrew, Spanish).
2 The MultiLing tasks
There are two main tasks (and a single-
document multilingual summarization pilot de-
scribed in a separate paper) in MultiLing 2013:
Summarization Task This MultiLing task aims
to evaluate the application of (partially or
fully) language-independent summarization
algorithms on a variety of languages. Each
system participating in the task was called
to provide summaries for a range of differ-
ent languages, based on corresponding cor-
pora. In the MultiLing Pilot of 2011 the lan-
1
guages used were 7, while this year systems
were called to summarize texts in 10 differ-
ent languages: Arabic, Chinese, Czech, En-
glish, French, Greek, Hebrew, Hindi, Roma-
nian, Spanish. Participating systems were re-
quired to apply their methods to a minimum
of two languages.
The task was aiming at the real problem of
summarizing news topics, parts of which may
be described or may happen in different mo-
ments in time. We consider, similarly to Mul-
tiLing 2011(Giannakopoulos et al, 2011) that
news topics can be seen as event sequences:
Definition 1 An event sequence is a set of
atomic (self-sufficient) event descriptions, se-
quenced in time, that share main actors, lo-
cation of occurence or some other important
factor. Event sequences may refer to topics
such as a natural disaster, a crime investiga-
tion, a set of negotiations focused on a single
political issue, a sports event.
The summarization task requires to generate
a single, fluent, representative summary from
a set of documents describing an event se-
quence. The language of the document set
will be within the given range of 10 languages
and all documents in a set share the same lan-
guage. The output summary should be of the
same language as its source documents. The
output summary should be between 240 and
250 words.
Evaluation Task This task aims to examine how
well automated systems can evaluate sum-
maries from different languages. This task
takes as input the summaries generated from
automatic systems and humans in the Sum-
marization Task. The output should be a grad-
ing of the summaries. Ideally, we would want
the automatic evaluation to maximally corre-
late to human judgement.
The first task was aiming at the real problem of
summarizing news topics, parts of which may be
described or happen in different moments in time.
The implications of including multiple aspects of
the same event, as well as time relations at a vary-
ing level (from consequtive days to years), are still
difficult to tackle in a summarization context. Fur-
thermore, the requirement for multilingual appli-
cability of the methods, further accentuates the dif-
ficulty of the task.
The second task, summarization evaluation has
come to be a prominent research problem, based on
the difficulty of the summary evaluation process.
While commonly used methods build upon a few
human summaries to be able to judge automatic
summaries (e.g., (Lin, 2004; Hovy et al, 2005)),
there also exist works on fully automatic evalua-
tion of summaries, without human?model? sum-
maries (Louis and Nenkova, 2012; Saggion et al,
2010). The Text Analysis Conference has a sepa-
rate track, named AESOP (Dang and Owczarzak,
2009) aiming to test and evaluate different auto-
matic evaluation methods of summarization sys-
tems.
Given the tasks, a corpus needed to be gener-
ated, that would be able to:
? provide input texts in different languages to
summarization systems.
? provide model summaries in different lan-
guages as gold standard summaries, to also
allow for automatic evaluation using model-
dependent methods.
? provide human grades to automatic and hu-
man summaries in different languages, to
support the testing of summary evaluation
systems.
In the following section we show how these re-
quirements were met in MultiLing 2013.
3 Corpus collection and generation
The overall process of creating the corpus of
MultiLing 2013 was, similarly to MultiLing 2011,
based on a community effort. The main processes
consisting of the generation of the corpus are as
follows:
? Selection of a source corpus in a single lan-
guage (see Section 3.1).
? Translation of the source corpus to different
languages (see Section 3.2).
? Human summarization of corpus topics per
language (see Section 3.3).
? Evaluation of human summaries, as well as of
submitted system runs (see Section 3.4).
2
We should note here that the translation is meant
to provide a parallel corpus of texts across differ-
ent languages. The main ideas behind this first ap-
proach are that:
? the corpus will allow performing secondary
studies, related to the human summarization
effort in different languages. Having a paral-
lel corpus is such cases can prove critical, in
that it provides a common working base.
? we may be able to study topic-related
or domain-related summarization difficulty
across languages.
? the parallel corpus highlights language-
specific problems (such as ambiguity in word
meaning, named entity representation across
languages).
? the parallel corpus fixes the setting in which
methods can show their cross-language ap-
plicability. Examining significantly varying
results in different languages over a parallel
corpus offers some background on how to im-
prove existing methods and may highlight the
need for language-specific resources.
On the other hand, the significant organizational
and implementaion effort required for the transla-
tion (please see per language notes in the corre-
sponding sections) may lead to a comparable (vs.
parallel) corpus in future MultiLing endeavours.
Given the tasks at hand, the Contributors first
performed the selection of the texts that would be
used for the MultiLing tracks, as described below.
3.1 Selecting the corpus
To support the summarization task, we needed
a dataset of freely available news texts (to allow
reuse), covering news topics that would contain
event sequences. Based on the ? apparently good
? decisions of the MultiLing 2011 Pilot, we de-
termined that each event sequence in the corpus
should contain at least three distinct atomic events,
to imply an underlying story.
The dataset created was based on the WikiNews
site1, which covers a variety of news topics,
while allowing the reuse of the texts based on the
Creative Commons Licence. An example topic
with two sample texts derived from the original
WikiNews documents is provided in Figure 1. It
1See http://www.wikinews.org.
can be seen clearly that the event in the example
has significantly different aspects, since an earth-
quake caused a radiation leak, via a series of inter-
actions in the real world. Systems would normally
be expected to express both aspects of the event
with adequate information.
During the selection of the source texts, we
first gathered an English corpus of 15 topics (10
of which were already available from MultiLing
2011), each containing 10 texts. Wemade sure that
each topic contained at least one event sequence.
From the original HTML text we only kept unfor-
matted content text, without any images, tables or
links.
While choosing topics we made sure that there
existed topics:
? with varying time granularity. Some top-
ics happen within days (e.g., sports events),
while others within years (e.g., Iranian nu-
clear policy and international negotiations).
? covering various domains. There existed top-
ics related to international politics, sports,
natural disasters, political campaigns and
elections.
? with a varying number of apparent actors.
Some topics focus on specific individuals
(e.g., campaign of Barack Obama) while oth-
ers refer to numerous participants (e.g., para-
Olympics and participating athletes).
? with numeric aspects, that would change over
time. Such examples are natural disasters
(with the number of estimated victims, or
the estimated magnitude of earthquakes) and
sports events (number of medals per country).
? with an important time dimension. For ex-
ample during the Egyptian riots, the order of
events is non-trivial to determine from text.
Determining the order of events is also very
challenging while following multi-day sports
events. Ignoring the time dimension in such
topics is expected to worsen the performance
of summarization systems.
Given the English texts, we now needed to pro-
vide corresponding texts in all the languages used
in MultiLing. To this end, we organized a transla-
tion process, which is elaborated below.
3
Fukushima reactor suffers multiple fires, radiation leak confirmed
Tuesday, March 15, 2011
Fires broke out at the Fukushima Daiichi plant's No. 4 reactor in Japan on
Tuesday, according to the Tokyo Electric Power Company. The first fire caused
a leak of concentrated radioactive material, according to the Japanese prime
minister, Naoto Kan.
The first fire broke out at 9:40 a.m. local time on Tuesday, and was thought
to have been put out, but another fire was discovered early on Wednesday,
believed to have started because the earlier one had not been fully
extinguished.
In a televised statement, the prime minister told residents near the plant
that "I sincerely ask all citizens within the 20 km distance from the reactor
to leave this zone." He went on to say that "[t]he radiation level has risen
substantially. The risk that radiation will leak from now on has risen."
Kan warned residents to remain indoors and to shut windows and doors to avoid
radiation poisoning.
The French Embassy in Japan reports that the radiation will reach Tokyo in 10
hours, with current wind speeds.
Death toll rises from Japan quake
Sunday, March 13, 2011
The death toll from the earthquake and subsequent tsunami that hit Japan on
Friday has risen to more than a thousand, with many people still missing,
according to reports issued over the weekend.
While Japan's police says that only 637 are confirmed dead, media reports say
that over a thousand people have been killed, with several hundred bodies
still being transported. Thousands more are still unaccounted for; in the town
of Minamisanriku, Miyagi Prefecture alone, up to 10,000 people are missing.
Four trains that were on the coast have yet to be located.
In the aftermath of the disaster, evacuations of around 300,000 people have
taken place; more evacuations are likely in the wake of concerns over a
damaged nuclear power plant. According to Prime Minister Naoto Kan, around
3,000 people have been rescued thus far. 50,000 troops from the Japanese
military have been deployed to assist in rescue efforts.
The tsunami generated by the quake has destroyed communities along Japan's
Pacific coast, with up to 90% of the houses in some towns having been
destroyed; at least 3,400 structures have been destroyed in total. Fires have
also sprung up among the impacted areas.
Figure 1: Topic Sample (Japan Earthquake and Nuclear Threat)
4
3.2 Translating the corpus
The English texts selected in the selection step
were translated using a sentence-by-sentence ap-
proach to each of the other languages: Arabic, Chi-
nese, Czech, French, Greek, Hebrew, Hindi, Ro-
manian, Spanish. This year there was no support
for the Hindi and French languages, which still
contain 10 topics. Also the Chinese language cov-
ers 10 topics. All the remaining languages cover
15 topics.
During the translation process, the guidelines
were minimal:
Given the source language text A,
the translator is requested to translate
each sentence in A, into the target lan-
guage. Each target sentence should keep
the meaning from the source language.
Some additional, optional guidelines (provided
in the Appendix) were provided by the Romanian
language Contributors, proposing ways to react to
date formatting, name translations, etc.
During the translation process, the translators
were also asked to keep track of the time spent on
different stages of the process: first full reading of
the source document, translation and verification.
The whole set of translated documents together
with the original English document set will be re-
ferred to as the Source Document Set. Given the
creation process, the Source Document Set con-
tains a total of 1350 texts (vs. 700 from MultiLing
2011): 7 languages with 15 topics per language, 10
texts per topic for a total of 1050 texts; 3 languages
with 10 topics per language, 10 texts per topic for
a total of 300 texts.
This Source Document Set was provided to par-
ticipating systems as input for their summarization
systems. It was also provided to human summa-
rizers, so that they would provide human, model
summaries on each topic and each language. The
human summarization process is described in the
following section.
3.3 Summarizing topics
In the summarization step of the corpus creation
different summarizers were asked to generate one
summary per topic in each language. The follow-
ing guidelines were provided to help the summa-
rizers:
The summarizer will read the whole
set of texts at least once. Then, the sum-
marizer should compose a summary,
with a minimum size of 240 and a maxi-
mum size of 250 words. The summary
should be in the same language as the
texts in the set. The aim is to create a
summary that covers all the major points
of the document set (what is major is
left to summarizer discretion). The sum-
mary should be written using fluent, eas-
ily readable language. No formatting or
other markup should be included in the
text. The output summary should be a
self-sufficient, clearly written text, pro-
viding no other information than what is
included in the source documents.
After summarization, human evaluation was
performed. The evaluation covered human sum-
maries, but also summarization system submis-
sions. The details are provided in the following
paragraphs.
3.4 Evaluating the summaries
The evaluation of summaries was performed
both automatically and manually. The manual
evaluation was based on the Overall Responsive-
ness (Dang and Owczarzak, 2008) of a text, as de-
scribed below, and the automatic evaluation used
the ROUGE (Lin, 2004) and AutoSummENG-
MeMoG (Giannakopoulos et al, 2008; Gian-
nakopoulos and Karkaletsis, 2011) and NPowER
(Giannakopoulos and Karkaletsis, 2013) methods
to provide a grading of performance.
For the manual evaluation the human evaluators
were provided the following guidelines:
Each summary is to be assigned an
integer grade from 1 to 5, related to the
overall responsiveness of the summary.
We consider a text to be worth a 5, if
it appears to cover all the important as-
pects of the corresponding document set
using fluent, readable language. A text
should be assigned a 1, if it is either un-
readable, nonsensical, or contains only
trivial information from the document
set. We consider the content and the
quality of the language to be equally im-
portant in the grading.
As indicated in the task, the acceptable limits for
the word count of a summary were between 240
5
and 250 words2 (inclusive). In the case of Chi-
nese there was a problem determining the number
of words. Based on the model summaries gathered
we (arbitrarily) set the upper limit of length in bytes
of the UTF8-encoded summary files to 750 bytes.
4 Language specific notes
In the following paragraphs we provide
language-specific overviews related to the corpus
contribution effort. The aim of these overviews is
to provide a reusable pool of knowledge for future
similar efforts.
In this document we elaborate on Arabic, En-
glish, Greek, Chinese and Romanian languages. A
second document (Elhadad et al, 2013) elaborates
on the rest of the languages.
4.1 Arabic language
The preparation of the Arabic corpus for the
2013 MultiLing Summarization tasks was organ-
ised jointly by Lancaster University and the Uni-
versity of Essex in the United Kingdom. 20 people
participated in translating the English corpus into
Arabic, validating the translation and summarising
the set of related Arabic articles. The participants
are studying, or have finished a university degree
in an Arabic speaking country. The participants?
age ranged between 21 and 32 years old.
The participants translated the English dataset
into Arabic. For each translated article another
translator validated the translation and fixed any
errors. For each of the translated articles, three
manual summaries were created by three different
participants (human peers). Amid the summarisa-
tion process the participants evaluated the quality
of the generated summary by assigning a score be-
tween one (unreadable summary) and five (fluent
and readable summary). No self evaluation was
allowed.
The average time for reading the English news
articles by the Arabic native speaker participants
was 5.58minutes. The average time it took them to
translate these articles into Arabic was 42.18 min-
utes and to validate each of the translated Arabic
articles the participants took 5.25 minutes on aver-
age.
For the summarisation task the average time for
reading the set of related articles (10 articles per
2The count of words was provided by thewc -w linux com-
mand.
each set) was 34.44 minutes. The average time for
summarising each set was 25.41 minutes.
4.1.1 Problems and Challenges
Many difficulties arose during the creation of
the gold-standard summaries. Some are language-
dependent and relate to the complexity of the Ara-
bic language. This required a special attention to
be paid while creating the summaries.
One problem concerns the handling of month
names in Arabic. There are twoways of translating
month names into Arabic:
? using the Arabic transliteration of the
Aramic (Syriac) month names (e.g. ?May?,
?PA


K




@?, ?Ayyar?).
? using the Arabic transliteration of the
English month names (e.g. ?May?,
??K


A

??, ?Mayo?).
Some of the participants found it difficult to
translate sentences where they believe they contain
an ambiguous structure. For example: ?She said
Iranian security Chief Saeed Jalili had requested a
meeting in a telephone call?. The translators (who
are Native Arabic speakers) found it a bit hard to
choose between two translations:
? ?Saeed Jalili asked to schedule a telephone
meeting?
? ?Saeed Jalili phoned to request a meeting?.
Arabic sentence structure is highly complex and
therefore great attention must be paid when mov-
ing forward or pushing back phrases within a sen-
tence, as such shifts are likely to change the over-
all meaning. In addition, the use of passive voice,
metaphors and idioms in the original English text
has captured the translators attention, as the mean-
ing in such cases takes precedence over the literal
translation.
During the summarisation process, a sum-
mariser found that ordering a set of related articles
(discussing the same topic) in chronological order
simplifies the summarisation process.
Many participants found it difficult to meet the
250 summary word-limit as they believe 250 is not
enough to cover all the essential information de-
rived from a given set of documents.
Another problem concerns ?proper nouns? when
translating into Arabic. The Arabic electronic dis-
course would sometimes show two variants of one
6
English proper noun, as in the case with the name
?Francois Hollande?. Mostly in such cases, the
variant used in popular websites such as the Arabic
version Wikipedia was adopted.
Finally, there were many questions by the par-
ticipants on whether to create abstractive or extrac-
tive summaries.
4.2 Chinese language
Below we provide an overview of the organiza-
tional effort and comments on a variety of prob-
lems related to the preparation of the Chinese cor-
pus for MultiLing 2013.
4.2.1 Organization
First, the Chinese language team translated two
texts from English to Chinese together in order to
make an original unified example for each trans-
lator, including file format, title format, date for-
mat, named entity translation, etc. Second, we as-
signed different set of news texts as specific task
for each translator. For each news topic, we usu-
ally split the ten texts to two different translators at
least, so as to bring more thoughts from different
viewers and prepare enough for later discussion.
During the process of each translator, they were
asked to note any problems in a ?problem file?, in-
cluding the source English part and the target Chi-
nese part. Third, we summed up a big problem
file from each translator. After a series of discus-
sions, we classified the problems into different cat-
egories and solved some of the problems success-
fully. The remaining problems were noted down in
a detailed report to the organizer of the MultiLing
2013Workshop of ACL 2013, as a knowledge pool
for future efforts. Fourth, we performed the verifi-
cation task. During the process, we made sure that
for each text, the verifier was different from the
translator. Also each verifier was demanded to log
any problems. Fifth, we did another discussion for
new problems coming from the verification phase.
Some problems were solved; others were added to
the detailed report. Sixth, we generated the needed
result files and made sure that they were in the re-
quested format (e.g., UTF8, no-BOM, plain text
files for summaries).
For the process of summarization and human
evaluation, first, we assigned three summarizers,
each of which needed to read all the ten topics and
write a summary for each topic. Second, we as-
signed three evaluators, making sure that for each
summary, the evaluator was different from the
summarizer. Third, we made a discussion about
the process of summarization and evaluation. All
agreed that summarization and evaluation were
much easier than translation.
There were mainly two common problems. One
was about the summary length. So we set a uni-
fied method for length checking. The other prob-
lem was more complex, which was that there
were many different information in the original ten
texts, but the result summary was limited to 250
words, so it was very difficult to choose the most
important information. As a result, some infor-
mation could be lost in final summaries. At the
same time, we also found minor problems regard-
ing the translation, improved the translation files
and updated the detailed report about the problems
we faced.
4.2.2 Problems and proposed solutions
In fact, related problems mainly came up from
the task of translation. Most of them were com-
mon questions of the translators and language-
dependent problems that needed special care. Here
we only list the main categories of problems 3.
First, there were problems with the translation of
person names. There are several sub-problems
here:
? There are some person names which are not
so popular, we could not find a result, so
we finally keep the unknown English words
among Chinese words.
? There is no specific separator between first
name, middle name and family name in En-
glish, only normal space. But in Chinese, we
usually add a separator ?? ? between them.
?
? There is also some ambiguity in person name
to us, since we may be not quite familiar with
some specific knowledge of news related do-
main. ?
? There are also some person names which
seem to contain non-English characters.
These names are more difficult for us, so we
just keep most of them as the original format
in English news. ?
? There are some person names with only one
capitalized character and a dot in the middle
3A more detailed report has been submitted to the orga-
nizer of the Workshop.
7
part. It?s really difficult for us to find a cor-
responding Chinese translation for it, so we
just keep it as the original English format in
the Chinese translations and keep the original
English name in the following brackets.
Second, the translation for the English name of
some websites, companies, organizations, etc, can
cause problems. Since the full name may be too
long for news reports, most of them also have oc-
curred in corresponding simple format of abbre-
viation. Some of them are famous enough that
we have a popular Chinese translation for them,
while others are not so popular. So we decided
that for unknown ones, we just reserve the English
name, but for those known ones, we add the Chi-
nese translation and keep some of the English ab-
breviation.
Third, the translation of time expressions is non-
trivial. In English, the order usually used is: Week-
day, Month Day, Year. But according to Chinese
habit, we mention time usually in the following or-
der: Year Month Day, Weekday.
Fourth, translation of locations names may not
exist. There are many location names in these
news texts. We tried to find their Chinese transla-
tion from many resources, but there are still some
difficult ones left.
Fifth, there are someEnglishwords in the source
texts which seem to be unrelated to other sentences
in the news text (these may be text captions of pho-
tos in the source WikiNews articles). We just left
them as they were.
Sixth, there are some sentences which are diffi-
cult to understand clearly because the context and
structure are ambiguous. In these cases, we made
a Chinese translation which seems best to us.
The above problems conclude the Chinese lan-
guage contribution language-specific notes.
4.3 English and Greek languages
The effort related to the organization of the En-
glish and Greek languages was essentially equiva-
lent to the MultiLing 2011 pilot (Giannakopoulos
et al, 2011). This year 5 new topics were added
to the two languages. The effort for English was
reduced because no translation was needed. In the
following subsections we elaborate on the organi-
zation details and the problems faced during the
different subprocesses of the corpus creation.
4.3.1 Organization
A total of 7 people (being either MSc students,
or researchers, all with fluency in English and
Greek) were recruited for the two languages. An
initial meeting was held to provide the basic guide-
lines and discuss questions on the translation pro-
cess. Subsequently, e-mail communication and
periodic conferences were used to assign the next
tasks, related to summarization and evaluation.
For the purposes of meaningful assignment we
created and used an automatic assignment script,
that allows pre-allocating specific texts to workers
(for any of the required tasks), while it automati-
cally distributes work according to the availability
of workers. The script avoids assigning workers to
texts/tasks more than once.
In the evaluation process, we made sure
(through pre-assignments) that no human would
judge their own summary. It would have increased
efficiency, if we had ascertained that human sum-
marization would occur right after the translation
of the texts.
The average time for reading the English news
articles by the Greek native speaker participants
was around 8 minutes. The average time it took
them to translate these articles into Greek was
around 48 minutes on average (with a couple of
extreme cases exceeding 100 minutes, due to tech-
nical terminology, which was difficult to trans-
late). The summarization time of the new topics
in English was around 24 minutes per topic (plus
an average of 8 minutes allocated to reading the
source texts). For Greek the summary time was
around 50 minutes per topic (we note that the sum-
marizers? groups for English and Greek were only
minimally overlapping). In the Greek case, some
deeper search showed that a single summarizer
heavily biased the distribution of times to higher
values.
To follow the progress of tasks, a generic project
management tool was used. However, the tool
proved insufficient in the micro-planning of the ef-
fort (individual assignments tracking). It would
clearly make sense to use an ad-hoc designed sys-
tem for planning and implementation of the effort.
4.3.2 Problems and proposed solutions
The main problems identified by contributors
for Greek and English translation were related to
well-known translation problems: named entity
translation, date formatting, highly technical or
domain specific terminology, ambiguous terms in
8
the source text. Additional effort from translators
provided solutions to these problems according to
common practice in the translation domain.
The summarization effort indicated a few inter-
esting points. Even though summarizers have their
individual method for summarizing, some com-
mon practices and notes arise:
? A non-thorough glimpse of the source texts
helps determine the overall topic.
? Time ordering is important in several cases,
thus time ordering of the source texts is ap-
plied before the summarization process itself.
The process is non-trivial even for humans.
? An initial summary which may be longer than
the target size is created and several reductive
transformations are applied. The 250 word
limit proved critical and challenging, in that it
forced summarizers to carefully choose infor-
mation, essentially not covering the whole set
of information from the source documents.
? Syntactic compression and rewriting is the
last line of summarization, when it is obvious
that more compression is needed.
As related to the evaluation process, we noted
that there exists an inherent tendency for evalua-
tors to determine whether a human or a machine
performed the summarization. There were cases
where evaluators altered their grading, because
they inferred that not all texts were from humans
or not all were from machines. We had noted this
phenomenon also in MultiLing 2011. There are
several cases where the evaluator also tries to de-
termine the strategy of the system and, when one
understands the underlying strategy, this may bias
the grade. It would be interesting to evaluate this
bias in the future.
Some additional notes are related to problems
with the organization of the effort:
? A distributed work environment that would
help track the progress of individuals and
assignment of new tasks without significant
communication effort, would have been very
helpful.
? The assignment script was really critical in
facilitating the organization of the effort and
we plan to make it publicly available to allow
reuse.
Overall, the collection and generation of the cor-
pus was a very challenging effort, both in terms
of organization and individual questions arising.
However, next steps can build upon the lessons
learnt, if the effort is well documented and the doc-
uments are freely and openly shared.
4.4 Romanian language
AtMultiLing 2013, Romanian was addressed as
a language for the first time. Following the Call
for Contributors launched by the MultiLing orga-
nizers and based on the experience in the QA @
CLEF4 evaluation campaign (Pe?as et al, 2012),
we started the data collection process workingwith
a group of ten MSc students in Computational
Linguistics from our Faculty, later adding another
MSc student to the working group. Below we pro-
vide some notes on the translation and generation
of human summaries processes:
? The translation, including verification, of
the 150 WikiNews text documents from En-
glish into Romanian, was performed in a dis-
tributed context, theoretically based on an ar-
chitecture like the one described in (Alboaie
et al, 2003). Each student received one topic
(10 documents) to be translated, based on a
set of guidelines. We devised guidelines to
tackle any language-dependent problems that
need special care, and they were improved af-
ter each solution received from the students
and based also on their questions. The full
guidelines are provided in the Appendix of
this document.
We started with the following workflow: stu-
dent A receives 10 English documents to be
translated and summarized and sends the re-
sults to the organizer; another student, B, re-
ceives the English documents and the Roma-
nian translations (made by student A) and s/he
verifies the translations and prepares another
summary. Finally, another student, C, re-
ceives from the organizer the 10 Romanian
documents and s/he prepares the third sum-
mary of a given topic.
Since the task proved to be very time-
consuming for the students, all the last five
topics (the ones introduced this year) were
given to one student and then the translations
were verified by the organizer.
4See http://celct.fbk.eu/ResPubliQA/index.
php for more information.
9
? The generation of human summaries was per-
formed immediately after the translation. For
each topic, the aim was to create a summary
that covers all the major points of the topic
(what is major was left to summarizer?s dis-
cretion), being a self-sufficient, clearly writ-
ten text, providing no other information than
what is included in the source documents.
The students were given no specific recom-
mendations regarding the type of summary
they should produce, e.g. an abstract ver-
sus an extract (Mani and Maybury, 1999),
but they were specifically instructed to under-
stand the main aspects of summarization.
5 Conclusions and lessons learnt
The corpus generated throughout the MultiLing
corpus preparation provides a benchmark dataset
for multilingual summarization. It tries to cap-
tured interesting, representative events, covering
a variety of well-known news events around the
world. The recent corporate interest in summa-
rization, in conjunction with the ever-present in-
crease of information flow from the Web and in-
formation redundancy, show that having a scien-
tifically plausible set of evaluation tools for sys-
tems can help bring useful summarization systems
to a wide audience. MultiLing functions as a fo-
cus point for multilingual summarization research
and this document described the methods used to
create a commonly accepted multilingual, multi-
document summarization corpus.
Concerning thoughts on the future work of Mul-
tiLing, there are some points that have been raised
by Contributors that we reproduce in the following
sentences:
? In the translation phase, it would be useful to
have translators for different languages dis-
cuss directly about some difficult cases, such
as some ambiguous words, phrases and sen-
tences, especially when they are expressed in
some language-specific way.
? It would be very interesting to exploit the po-
tential of comparable corpora, and not only
of the parallel ones, especially if we consider
the multilingual setting of MultiLing 2013.
This means that the data should be collected
starting from a given topic and each language
contributor should find 10 documents on that
given topic in his/her language.
? Creating a collaborative platform for build-
ing and improving summarization corpora
could significantly facilitate the corpus build-
ing process for future efforts.
We remind the reader that a second paper (El-
hadad et al, 2013) addresses the problems and
challenges faced in the remaining languages ac-
tively contributed to in MultiLing 2013 (Czech,
Hebrew and Spanish), thus completing the lessons
learnt from theMultiLing 2013 contribution effort.
Extended technical reports recapitulating discus-
sions and findings from the MultiLing Workshop
will be available after the workshop at the Multi-
Ling Community website5, as an addenum to the
proceedings.
Acknowledgments
MultiLing is a community effort and this com-
munity is what keeps it alive and interesting. We
would like to thank Contributors for their organi-
zational effort, which made MultiLing possible in
so many languages and all volunteers, helpers and
researchers that helped realize individual steps of
the process. A more detailed reference of the con-
tributor teams can be found in Appendix A.
The MultiLing 2013 organization has been par-
tially supported by the NOMAD FP7 EU Project
(cf. http://www.nomad-project.eu).
References
[Alboaie et al2003] Lenuta Alboaie, Sabin C Buraga,
and S?nica Alboaie. 2003. tuBiG?a layered infras-
tructure to provide support for grid functionalities.
Omega, 2:3.
[Cahan2013] Adam Cahan. 2013. Yahoo! To Acquire
Summly http://yodel.yahoo.com/blogs/
general/yahoo-acquire-summly-13171.
html, March 25th.
[Dang and Owczarzak2008] H. T. Dang and
K. Owczarzak. 2008. Overview of the TAC
2008 update summarization task. In TAC 2008
Workshop - Notebook papers and results, pages
10?23, Maryland MD, USA, November.
[Dang and Owczarzak2009] Hoa Trang Dang and
K. Owczarzak. 2009. Overview of the tac 2009
summarization track, Nov.
[Dang2005] H. T. Dang. 2005. Overview of DUC
2005. In Proceedings of the Document Under-
standing Conf. Wksp. 2005 (DUC 2005) at the
5See http://multiling.iit.demokritos.gr/
pages/view/1256/proceedings-addenum)
10
Human Language Technology Conf./Conf. on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP 2005).
[Dang2006] H. T. Dang. 2006. Overview of DUC
2006. In Proceedings of HLT-NAACL 2006.
[Elhadad et al2013] Michael Elhadad, Sabino
Miranda-Jim?nez, Josef Steinberger, and George
Giannakopoulos. 2013. Multi-document multi-
lingual summarization corpus preparation, part 2:
Czech, hebrew and spanish. In MultiLing 2013
Workshop in ACL 2013, Sofia, Bulgaria, August.
[Giannakopoulos and Karkaletsis2011] George Gi-
annakopoulos and Vangelis Karkaletsis. 2011.
Autosummeng and memog in evaluating guided
summaries. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Giannakopoulos and Karkaletsis2013] George Gi-
annakopoulos and Vangelis Karkaletsis. 2013.
Summary evaluation: Together we stand npower-ed.
In Computational Linguistics and Intelligent Text
Processing, pages 436?450. Springer.
[Giannakopoulos et al2008] George Giannakopoulos,
Vangelis Karkaletsis, George Vouros, and Panagio-
tis Stamatopoulos. 2008. Summarization system
evaluation revisited: N-gram graphs. ACM Trans.
Speech Lang. Process., 5(3):1?39.
[Giannakopoulos et al2011] G. Giannakopoulos,
M. El-Haj, B. Favre, M. Litvak, J. Steinberger,
and V. Varma. 2011. TAC 2011 MultiLing pilot
overview. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Hovy et al2005] E. Hovy, C. Y. Lin, L. Zhou, and
J. Fukumoto. 2005. Basic elements.
[Lin2004] C. Y. Lin. 2004. Rouge: A package for
automatic evaluation of summaries. Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26.
[Louis and Nenkova2012] Annie Louis and Ani
Nenkova. 2012. Automatically assessing ma-
chine summary content without a gold standard.
Computational Linguistics, 39(2):267?300, Aug.
[Mani and Maybury1999] Inderjeet Mani and Mark T
Maybury. 1999. Advances in automatic text sum-
marization. the MIT Press.
[Pe?as et al2012] Anselmo Pe?as, Eduard H. Hovy,
Pamela Forner, ?lvaro Rodrigo, Richard F. E. Sut-
cliffe, Caroline Sporleder, Corina Forascu, Yassine
Benajiba, and Petya Osenova. 2012. Overview of
qa4mre at clef 2012: Question answering for ma-
chine reading evaluation. In CLEF (Online Working
Notes/Labs/Workshop).
[Saggion et al2010] H. Saggion, J. M. Torres-Moreno,
I. Cunha, and E. SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, page 1059?
1067.
[Shih2013] Gerry Shih. 2013. Sound Famil-
iar? After Yahoo Buys Summly, Google
Buys News Summarization App Wavii
http://www.huffingtonpost.com/2013/
04/24/google-wavii_n_3143116.html, April
23rd.
[Tufis et al2004] Dan Tufis, DanCristea, and Sofia Sta-
mou. 2004. Balkanet: Aims, methods, results and
perspectives. a general overview. Romanian Journal
of Information science and technology, 7(1-2):9?43.
Appendix A: Contributor teams
Arabic language team
Team members Mahmoud El-Haj (Lancaster
University, UK); Ans Alghamdi, Maha
Althobaiti (Essex University, UK); Ahmad
Alharthi (King Saud University, Saudi
Arabia)
Contact e-mail m.el-haj@lancaster.ac.uk
Chinese language team
Team members Lei Li, Wei Heng, Jia Yu, Yu Liu,
Qian Li
Team affiliation Center for Intelligence Science
and Technology (CIST), School of Com-
puter Science,Beijing University of Posts and
Telecommunications,
Postal Address P.O.Box 310, Beijing University
of Posts and Telecommunications, Xitucheng
Road 10, Haidian District, Beijing, China
Contact e-mail leili@bupt.edu.cn
English and Greek languages team
Team members Zoe Angelou, Argyro
Mavridakis, Valentini Mellas, Efrosini
Zacharopoulou, George Kiomourtzis,
George Petasis, George Giannakopoulos
Team affiliation NCSR?Demokritos?
Postal Address Institute of Informatics and
Telecommunications, Patriarchou Grigoriou
and Neapoleos Str., Aghia Paraskevi Attikis,
Athens, Greece
Contact e-mail ggianna@iit.demokritos.gr
11
Romanian language team
Team members Corina Forascu, Raluca Moi-
seanu; Ana Maria Timofciuc, Alexandra
Cristea, Alexandrina Sbiera, Bogdan Puiu,
and Tudor Popoiu; other contributors to the
task were Monica Ancu?a, Romic? Iarca,
Claudiu Popa, and Cosmin Vl?du?u
Team affiliation UAIC, Romania
Contact e-mail corinfor@info.uaic.ro
Appendix B: Romanian guidelines
1. Translation equivalents belonging to the same
part of speech should be used. The Romanian
words should be as?closest?as possible to
their English equivalents: If the English word
has as equivalent a cognate in Romanian, this
one should be used. The Romanian wordnet6
(Tufis et al, 2004) should be used for prob-
lematic situations. If the English word doesn?
t have a Romanian cognate, then the transla-
tor should not try to paraphrase it. Example:
The English ?sporadic?will be translated
into?sporadic?, even though the translator
would be tempted to use instead?izolat?or
?rar?. It is not recommended to give trans-
lations such as ?mai pu?in?or ?mai rar?
.
2. English words should not be omitted and
words which are not in the original English
text should not be added because of stylistic
reasons. Example:?The Telegraph?will be
not translated when it refers to the newspa-
per and, moreover, the translators will not in-
troduce an explanation, like?cotidianul The
Telegraph?[English: The Telegraph newspa-
per].
3. The Romanian diacritics have to be used, in
UTF-8 encoding.
4. The translators must preserve as much as pos-
sible the tenses of the English verbs. Any dis-
agreement from the English tense is allowed
for linguistic reasons only (Romanian spe-
cific constructions), and not for stylistic ones.
5. The translators will preserve the format of
dates, times, numbers. For example, for the
issuing date of an article being ?March 25,
6See http://www.racai.ro/wnbrowser/.
2010?, the Romanian translation will be?25
martie 2010?and NOT ?Martie, 25, 2010?
OR?25 Martie, 2010?.
6. The format of the numbers should follow the
Romanian convention with respect to the dec-
imal separator, which is comma (,), and not
the period (.), like in English-speaking coun-
tries.
7. The unclear or unsure situations encountered
by the translators will be separately recorded
in a file, indicating the provenance of the doc-
ument, the ID used for the problematic sen-
tence and the commentaries/suggestions.
12
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 13?19,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multi-document multilingual summarization corpus preparation, Part 2:
Czech, Hebrew and Spanish
Michael Elhadad
Ben-Gurion Univ.
in the Negev, Israel
elhadad@cs.bgu.ac.il
Sabino Miranda-Jim?nez
Instituto Polit?cnico
Nacional, Mexico
sabino_m@hotmail.com
Josef Steinberger
Univ. of
West Bohemia,
Czech Republic
jstein@kiv.zcu.cz
George Giannakopoulos
NCSR Demokritos, Greece
SciFY NPC, Greece
ggianna@iit.demokritos.gr
Abstract
This document overviews the strategy, ef-
fort and aftermath of the MultiLing 2013
multilingual summarization data collec-
tion. We describe how the Data Contrib-
utors of MultiLing collected and gener-
ated a multilingual multi-document sum-
marization corpus on 10 different lan-
guages: Arabic, Chinese, Czech, English,
French, Greek, Hebrew, Hindi, Romanian
and Spanish. We discuss the rationale be-
hind the main decisions of the collection,
the methodology used to generate the mul-
tilingual corpus, as well as challenges and
problems faced per language. This paper
overviews the work on Czech, Hebrew and
Spanish languages.
1 Introduction
In this document we present the language-
specific problems and challenges faced by Con-
tributors during the corpus creation process. To
facilitate the reader we repeat some information
found in the first part of the overview (Li et al,
2013): the MultiLing tasks and the main steps of
the corpus creation process.
2 The MultiLing tasks
There are two main tasks (and a single-
document multilingual summarization pilot de-
scribed in a separate paper) in MultiLing 2013:
Summarization Task This MultiLing task aims
to evaluate the application of (partially or
fully) language-independent summarization
algorithms on a variety of languages. Each
system participating in the task was called
to provide summaries for a range of differ-
ent languages, based on corresponding cor-
pora. In the MultiLing Pilot of 2011 the lan-
guages used were 7, while this year systems
were called to summarize texts in 10 differ-
ent languages: Arabic, Chinese, Czech, En-
glish, French, Greek, Hebrew, Hindi, Roma-
nian, Spanish. Participating systems were re-
quired to apply their methods to a minimum
of two languages.
The task was aiming at the real problem of
summarizing news topics, parts of which may
be described or may happen in different mo-
ments in time. We consider, similarly to Mul-
tiLing 2011(Giannakopoulos et al, 2011) that
news topics can be seen as event sequences:
Definition 1 An event sequence is a set of
atomic (self-sufficient) event descriptions, se-
quenced in time, that share main actors, lo-
cation of occurence or some other important
factor. Event sequences may refer to topics
such as a natural disaster, a crime investiga-
tion, a set of negotiations focused on a single
political issue, a sports event.
The summarization task requires to generate
a single, fluent, representative summary from
a set of documents describing an event se-
quence. The language of the document set
will be within the given range of 10 languages
and all documents in a set share the same lan-
guage. The output summary should be of the
same language as its source documents. The
output summary should be between 240 and
250 words.
Evaluation Task This task aims to examine how
well automated systems can evaluate sum-
maries from different languages. This task
takes as input the summaries generated from
automatic systems and humans in the Sum-
marization Task. The output should be a grad-
ing of the summaries. Ideally, we would want
the automatic evaluation to maximally corre-
late to human judgement.
13
The first task was aiming at the real problem of
summarizing news topics, parts of which may be
described or happen in different moments in time.
The implications of including multiple aspects of
the same event, as well as time relations at a vary-
ing level (from consequtive days to years), are still
difficult to tackle in a summarization context. Fur-
thermore, the requirement for multilingual appli-
cability of the methods, further accentuates the dif-
ficulty of the task.
The second task, summarization evaluation has
come to be a prominent research problem, based on
the difficulty of the summary evaluation process.
While commonly used methods build upon a few
human summaries to be able to judge automatic
summaries (e.g., (Lin, 2004; Hovy et al, 2005)),
there also exist works on fully automatic evalua-
tion of summaries, without human?model? sum-
maries (Louis and Nenkova, 2012; Saggion et al,
2010). The Text Analysis Conference has a sepa-
rate track, named AESOP (Dang and Owczarzak,
2009) aiming to test and evaluate different auto-
matic evaluation methods of summarization sys-
tems.
Given the tasks, a corpus needed to be gener-
ated, that would be able to:
? provide input texts in different languages to
summarization systems.
? provide model summaries in different lan-
guages as gold standard summaries, to also
allow for automatic evaluation using model-
dependent methods.
? provide human grades to automatic and hu-
man summaries in different languages, to
support the testing of summary evaluation
systems.
In the following section we show how these re-
quirements were met in MultiLing 2013.
3 Corpus collection and generation
The overall process of creating the corpus of
MultiLing 2013 was, similarly to MultiLing 2011,
based on a community effort. The main processes
consisting the generation of the corpus are as fol-
lows:
? Selection of a source corpus in a single lan-
guage.
? Translation of the source corpus to different
languages.
? Human summarization of corpus topics per
language.
? Evaluation of human summaries, as well as of
submitted system runs.
4 Language specific notes
In the following paragraphs we provide
language-specific overviews related to the corpus
contribution effort. The aim of these overviews is
to provide a reusable pool of knowledge for future
similar efforts.
In this document we elaborate on Czech, He-
brew, and Spanish languages. A second document
(Elhadad et al, 2013) elaborates on the rest of the
languages.
4.1 Czech language
The first part of the Czech subcorpus (10 top-
ics) was created for the multilingual pilot task at
TAC 2011. Five new topics were added for Mul-
tiling 2013. In total, 14 annotators participated in
the Czech corpus creation.
The most time consuming part of the annota-
tion work was the translation of the articles. The
annotators were not professional translators and
many topics required domain knowledge for cor-
rect translation. To be able to translate a per-
son name, the translator needs to know its correct
spelling in Czech, which is usually different from
English. The gender also plays an important role
in the translation, because a suffix ?ov?? must be
added to female surnames.
Translation of organisation names or person?s
functions within an organisation needs some do-
main knowledge as well. Complicated morphol-
ogy and word order in Czech (more free but some-
times very different fromEnglish) makes the trans-
lation even more difficult.
For the creation of model summaries the anno-
tator needed to analyse the topic well in order to
decide what is important and what is redundant.
Sometimes, it was very difficult, mainly in the
case of topics which covered a long period (even
5 years) and which contained articles sharing very
little information.
The main question of the evaluation part was
how to evaluate a summary which contains a read-
able, continuous text ? mainly the case of the
14
Group SysID Avg Perf
a B 4.75
a A 4.63
ab C 4.61
b D 4.21
b E 4.10
Table 1: Czech: Tukey?s HSD test groups for hu-
man summarizers
baseline system with ID6) ? however not impor-
tant information from the article cluster point of
view.
An overview of the Overall Responsiveness and
the corresponding average grades of the human
summarizers can be seen in Table 1. We note
that on average the human summaries are consid-
ered excellent (graded above 4 out of 5), but that
there exist statistically significant differences be-
tween summarizers, essentially forming two dis-
tinct groups.
4.2 Hebrew language
This section describes the process of preparing
the dataset for MultiLing 2013 in Hebrew: transla-
tion of source texts from English, and the summa-
rization for the translated texts, by the Ben Gurion
University Natural Language Processing team.
4.2.1 Translation Process
Four people participated in the translation and
the summarization of the dataset of the 50 news
articles: three graduate students, one a native En-
glish speaker with fluent Hebrew and the other two
with Hebrew as a mother tongue and very good
English skills. The process was supervised by a
professional translator with a doctoral degree with
experience in translation and scientific editing.
The average times to read an article was 2.5min-
utes (std. dev 1.2min), the average translation time
was 30 minutes (std. dev 15min), and the average
proofing time was 18.5min (std. dev 10.5min).
4.2.2 Translation Methodology
We tested two translation methodologies by dif-
ferent translators. In some of the cases, translation
was aided with Google Translate1, while in other
cases, translation was performed from scratch.
In the cases where texts were first translated
using Google Translate, the translator reviewed
1See http://translate.google.com/.
the text and edited changes according to her judg-
ment. Relying on the time that was reported for the
proofreading of each translation, we could tell that
texts that were translated using this method, re-
quired longer periods of proofreading (and some-
times more time was required to proofread than to
translate). This is most likely because once the au-
tomatic translation was available, the human trans-
lator was biased by the automatic outcome, re-
maining anchored? to the given text with reduced
criticism and creativity.
Translating the text manually, aided with online
or offline dictionaries, Wikipedia and news site on
the subject that was translated, showed better qual-
ity as analysis of time shows, where the ratio be-
tween the time needed to proofread was less than
half.
In addition, we found, that inmost cases the time
that the translation took for the first texts of a given
subject (for each article cluster), tends to be signif-
icantly longer than the subsequent articles in the
same cluster. This reflects the ?learning phase? ex-
perienced by the translators who approached each
cluster, getting to know the vocabulary of each
subject.
4.2.3 Topic Clusters
The text collection includes five clusters of ten
articles each. Some of the topics were very famil-
iar to the Hebrew-speaking readers, and some sub-
jects were less familiar or relevant. The Iranian
Nuclear issue is very common in the local news
and terminology is well known. Moreover, it was
possible to track the articles from the news as they
were published in Hebrew news websites at that
time; this was important for the usage of actual
and correct news-wise terminology. The hardest
batch to translate was on the Paralympics champi-
onship, which had no publicity in Hebrew, and the
terminology of winter sports is culturally foreign
to native Hebrew speakers.
4.2.4 Special Issues in Hebrew
A couple of issues have surfaced during the
translation and should be noted. Many words in
Hebrew have a foreign transliterated usage and an
original Hebrew word as well. For instance, the
Latin word Atomic is very common in Hebrew
and, therefore, it will be equally acceptable to use
it in the Hebrew form, ????? / ?atomi?but also
the Hebrew word ?????? (?gar? ini? / nuclear).
Traditional HebrewNews Agencies have for many
15
Summarizer Reading time Summarization
A 43 min 49 min
B 22 min 84 min
C 35 min 62 min
Table 2: Summarization process times (averaged)
years adopted an editorial line which strongly en-
courages using original Hebrew words whenever
possible. In recent years, however, this approach
is relaxed, and both registers are equally accepted.
We have tried to use a ?common notion? in all texts
using the way terms are written inWikipedia as the
voice of majority. In most cases, this meant using
many transliterations.
Another issue in Hebrew concerns the orthog-
raphy variations of plene vs. deficient spelling.
Since Hebrew can be written with or without vo-
calization, words may be written with variations.
For instance, the vocalized version of the word
?air? is ?????? (?avir? ) while the non-vocalized
version is ????? (?avvir?). The rules of spelling
related to these variations are complicated and are
not common knowledge. Even educated people
write words with high variability, and in many
cases, usage is skewed by the rules embedded in
the Microsoft Word editor. We did not make any
specific effort to enforce standard spelling in the
dataset.
4.2.5 Summarization Process
Each cluster of articles was summarized by three
persons, and each summary was proof-read by the
other summarizers. Most of the summarizers read
the texts before summarization, while translating
or proofreading them, and, therefore, the time that
was required to read all texts was reduced.
The time spent reading and summarizing was
extremely different for each of the three summa-
rizers, reflecting widely different summarization
strategies, as indicated in the Table 2 (average
times over the 5 new clusters of MultiLing 2013):
The trend indicates that investing more time up
front reading the clusters pays off later in summa-
rization time.
The instructions did not explicitly recommend
abstractive vs. extractive summarization. Two
summarizers applied abstractive methods, one
tended to use mostly extractive (C). The extractive
method did not take markedly less time than the
abstractive one. In the evaluation, the extractive
Group SysID Avg Perf
a A 4.80
ab B 4.40
b C 4.13
Table 3: Hebrew: Tukey?s HSD test groups for hu-
man summarizers
summary was found markedly less fluent.
As the best technique to summarize efficiently,
all summarizers found that ordering the texts by
date of publication was the best way to conduct the
summaries in the most fluent manner.
However, it was not completely a linear process,
since it was often found that general information,
which should be located at the beginning of the
summary as background information, appeared in
a later text. In such cases, summarizers changed
their usual strategy and consciously moved infor-
mation from a later text to the beginning of the
summary. This was felt as a distinct deviation ?
as the dominant strategy was to keep track of the
story told across the chronology of the cluster, and
to only add new and important information to the
summary that was collected so far.
The most difficult subject to summarize was
the set on Paralympic winter sports championship
which was a collection of anecdotal descriptions
which were not necessarily a developing or a se-
quential story and had no natural coherence as a
cluster.
4.2.6 Human evaluation
The results of human evaluation over the human
summarizers are provided in Table 3. It is inter-
esting to note that even between humans there ex-
ist two groups with statistically significant differ-
ences in their grades. On the other hand, the hu-
man grades are high enough to show high quality
summaries (over 4 on a 5 point scale).
4.3 Spanish language
Thirty undergraduate students, from National
Institute Polytechnic and Autonomous University
of the State of Mexico, were involved in creating
of Spanish corpus for MultiLing 2013.
The Spanish corpus built upon the Text Analy-
sis Conference (TAC) MultiLing Corpus of 2011.
The source documents were news fromWikiNews
website, in English language. The source corpus
for translating consisted of 15 topics and 10 docu-
ments per topic. In the following paragraphs, we
16
show the measured times for each stage and prob-
lems that people had to face during the generation
of corpus that includes translation of documents,
multi-document summarization, and evaluation of
human (manual) summaries.
At the translation step, people had to translate
sentence by sentence or paraphrase a sentence up
to completing the whole document. When a docu-
ment was translated, it was sent to another person
to verify the quality of the translated document.
The effort was measured by three different time
measurements: reading time, translation time, and
verification time.
The reading average at document level was 7.6
minutes (with a standard deviation of 3.4 minutes),
the average translation of each document was 19.2
minutes (with a standard deviation of 7.8 min-
utes), and the average verification was 14.9 min-
utes (with a standard deviation of 7.7 minutes).
The translation stage took 104.5 man-hours.
At summarization step, people had to read the
whole set of translated documents (topic) and cre-
ate a summary per each set of documents. The
length of a summary is between 240 and 250
words. Three summaries were created for each
topic. Also, reading time of the topic and time of
writing the summary were measured.
The average reading of a set of documents was
31.6 minutes (with a standard deviation of 10.2
minutes), and the average time to generate a sum-
mary was 27.7 minutes (with a standard deviation
of 6.5 minutes). This stage took 44.5 man-hours.
At evaluation step, people had to read the whole
set of translated documents and assess its corre-
sponding summary. The summary quality was
evaluated. Three evaluations were done for each
summary. The human judges assessed the overall
responsiveness of the summary based on covering
all important aspects of the document set, fluent
and readable language. The human summary qual-
ity average was 3.8 (on a scale 1 to 5) (with a stan-
dard deviation of 0.81). The results are detailed in
Table 4. It is interesting to note that all humans
have no statistically significant differences in their
grades. On the other hand, the human grades are
not excellent on average (i.e. exceeding 4 out of 5)
which shows that the evaluators considered human
summaries non-optimal.
Group SysID Avg Perf
a C 3.867
a B 3.778
a A 3.667
Table 4: Spanish: Tukey?s HSD test groups for hu-
man summarizers
4.3.1 Problems during Generation of Spanish
Corpus
During the translation step, translators had to
face problems related to proper names, acronyms,
abbreviations, and specific themes. For instance,
the proper name?United States?can be depicted
with different Spanish words such as ?EE. UU.?
2,?Estados Unidos?, and?EUA??all of them
are valid words. Even though translators know
all the correct translations, they decided to use the
frequent terms in a context of news (the first two
terms are frequently used).
In relation to acronyms, well-known acronyms
were translated into equivalent well-known (or fre-
quent) Spanish translations such as UN (United
Nations) became into ONU (Organizaci?n de las
Naciones Unidas), or they were kept in the source
language, because they are frequently used in
Spanish, for example, UNICEF, BBC, AP (the
news agency, Associated Press), etc.
On the contrary, for not well-known acronyms
of agencies, monitoring centers, etc., translators
looked for the common translation of the proper
name on Spanish news websites in order to cre-
ate the acronym based on the name. Other trans-
lators chose to translate the proper name, but they
kept the acronym from the source document beside
the translated name. In cases where acronyms ap-
peared alone, they kept the acronym from source
language. It is a serious problem because a set of
translated documents has a mix of acronyms.
Abbreviations were mainly faced with ranks
such as lieutenant (Lt.), Colonel (Col.), etc. Trans-
lators used an equivalent rank in Spanish. For in-
stance, lieutenant (Lt.) is translated into?teniente
(Tte.)?; however, translators preferred to use the
complete word rather than the abbreviation.
In case of specific topics, translators used Span-
ish websites related to the topic in order to know
the particular vocabulary and to decide what (tech-
2The double E and double U indicate that the letter rep-
resents a plural: e.g. EE. may stand for Asuntos Exteriores
(Foreign Affairs).
17
nical) words should be translated and how they
should be expressed.
As regards at text summarization step, sum-
marizers dealt with how to organize the sum-
mary because there were ten documents per topic,
and all documents involved dates. Two strategies
were employed to solve the problem: generating
the summary according to representative dates, or
starting the summary based on a particular date.
In the first case, summarizers took the chain
of events and wrote the summary considering the
dates of events. They gathered important events
and put together under one date, typically, the lat-
est date according to a part of the chain of events.
They grouped all events in several dates; thus, the
summary is a sequence of dates that gather events.
However, the dates are chosen arbitrary according
to the summarizers.
In the second case, summarizers started the sum-
mary based on a specific date, and continued writ-
ing the sequence of important events. The se-
quence of events represents the temporality start-
ing from a specific point of time (usually, the
first date in the set of documents). Finally, in
most cases, evaluators think that human sum-
maries meet the requirements of covering all im-
portant aspects of the document set, fluent and
readable language.
5 Conclusions and lessons learnt
The findings from the languages presented in
this paper appear to second the claims found in the
rest of the languages (Li et al, 2013):
? Translation is a non-trivial process, often re-
quiring expert know-how to be performed.
? The distribution of time in summarization can
significantly vary among human summariz-
ers: it essentially sketches different strate-
gies of summarization. It would be interest-
ing to follow different strategies and record
their effectiveness in the multilingual setting,
similarly to previous works on human-style
summarization (Endres-Niggemeyer, 2000;
Endres-Niggemeyer and Wansorra, 2004).
Our find may be related to the (implied) ef-
fort of taking notes while reading, which can
be a difficult cognitive process (Piolat et al,
2005).
? The time aspect is important when generat-
ing a summary. The exact use of time (a sim-
ple timeline? a grouping of events based on
time?) is apparently arbitrary.
We remind the reader that extended technical re-
ports recapitulating discussions and findings from
the MultiLingWorkshop will be available after the
workshop at the MultiLing Community website3,
as an addenum to the proceedings.
What can definitely be derived from all the ef-
fort and discussion related to the gathering of sum-
marization corpora is that it is a research challenge
in itself. If the future we plan to broaden the scope
of the MultiLing effort, integrating all the findings
in tools that will support the whole process and al-
low quantifying the apparent problems in the dif-
ferent stages of corpus creation. We have also been
considering to generate comparable corpora (e.g.,
see (Saggion and Szasz, 2012)) for future Multi-
Ling efforts. We examine this course of action
to avoid the significant overhead by the transla-
tion process required for parallel corpus genera-
tion. We should note here that so far we have been
using parallel corpora to:
? allow for secondary studies, related to the
human summarization effort in different lan-
guages. Having a parallel corpus is such cases
can prove critical, in that it provides a com-
mon working base.
? be able to study topic-related or domain-
related summarization difficulty across lan-
guages.
? highlight language-specific problems (such
as ambiguity in word meaning, named entity
representation across languages).
? fixes the setting in which methods can show
their cross-language applicability. Exam-
ining significantly varying results in differ-
ent languages over a parallel corpus offers
some background on how to improve exist-
ing methods and may highlight the need for
language-specific resources.
On the other hand, the significant organizational
and implementaion effort required for the transla-
tion may turn the balance towards comparable cor-
pora for future MultiLing endeavours.
3See http://multiling.iit.demokritos.gr/
pages/view/1256/proceedings-addenum)
18
Acknowledgments
MultiLing is a community effort and this com-
munity is what keeps it alive and interesting. We
would like to thank contributors for their organi-
zational effort, which made MultiLing possible in
so many languages and all volunteers, helpers and
researchers that helped realize individual steps of
the process. A more detailed reference of the con-
tributor teams can be found in the Appendix.
The MultiLing 2013 organization has been par-
tially supported by the NOMAD FP7 EU Project
(cf. http://www.nomad-project.eu).
References
[Dang and Owczarzak2009] Hoa Trang Dang and
K. Owczarzak. 2009. Overview of the tac 2009
summarization track, Nov.
[Elhadad et al2013] Michael Elhadad, Sabino
Miranda-Jim?nez, Josef Steinberger, and George
Giannakopoulos. 2013. Multi-document multi-
lingual summarization corpus preparation, part 2:
Czech, hebrew and spanish. In MultiLing 2013
Workshop in ACL 2013, Sofia, Bulgaria, August.
[Endres-Niggemeyer and Wansorra2004] Brigitte
Endres-Niggemeyer and Elisabeth Wansorra. 2004.
Making cognitive summarization agents work in
a real-world domain. In Proceedings of NLUCS
Workshop, pages 86?96. Citeseer.
[Endres-Niggemeyer2000] Brigitte Endres-
Niggemeyer. 2000. Human-style WWW sum-
marization. Technical report.
[Giannakopoulos et al2011] G. Giannakopoulos,
M. El-Haj, B. Favre, M. Litvak, J. Steinberger,
and V. Varma. 2011. TAC 2011 MultiLing pilot
overview. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Hovy et al2005] E. Hovy, C. Y. Lin, L. Zhou, and
J. Fukumoto. 2005. Basic elements.
[Li et al2013] Lei Li, Corina Forascu, Mahmoud El-
Haj, and George Giannakopoulos. 2013. Multi-
document multilingual summarization corpus prepa-
ration, part 1: Arabic, english, greek, chinese, ro-
manian. In MultiLing 2013 Workshop in ACL 2013,
Sofia, Bulgaria, August.
[Lin2004] C. Y. Lin. 2004. Rouge: A package for
automatic evaluation of summaries. Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26.
[Louis and Nenkova2012] Annie Louis and Ani
Nenkova. 2012. Automatically assessing ma-
chine summary content without a gold standard.
Computational Linguistics, 39(2):267?300, Aug.
[Piolat et al2005] Annie Piolat, Thierry Olive, and
Ronald T Kellogg. 2005. Cognitive effort dur-
ing note taking. Applied Cognitive Psychology,
19(3):291?312.
[Saggion and Szasz2012] Horacio Saggion and Sandra
Szasz. 2012. The concisus corpus of event sum-
maries. In LREC, pages 2031?2037.
[Saggion et al2010] H. Saggion, J. M. Torres-Moreno,
I. Cunha, and E. SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, page 1059?
1067.
Appendix: Contributor teams
Czech language team
Team members Brychc?n Tom??, Campr Michal,
Fiala Dalibor, Habernal Ivan, Habernalov?
Anna, Je?ek Karel, Konkol Michal, Konop?k
Miloslav, Kr?m?? Lubom?r, Nejezchlebov?
Pavla, Pelechov? Blanka, Pt??ek Tom??,
Steinberger Josef, Z?ma Martin.
Team affiliation University of West Bohemia,
Czech Republic
Contact e-mail jstein@kiv.zcu.cz
Hebrew language team
Team members Tal Baumel, Raphael Cohen,
Michael Elhadad, Sagit Fried, Avi Hayoun,
Yael Netzer
Team affiliation Computer Science Dept. Ben-
Gurion University in the Negev, Israel
Contact e-mail elhadad@cs.bgu.ac.il
Spanish language team
Team members Sabino Miranda-Jim?nez, Grig-
ori Sidorov, Alexander Gelbukh (Natural
Language and Text Processing Laboratory,
Center for Computing Research, National In-
stitute Polytechnic, Mexico City, Mexico)
Obdulia Pichardo-Lagunas (Interdisciplinary
Professional Unit on Engineering and Ad-
vanced Technologies (UPIITA), National In-
stitute Polytechnic, Mexico City, Mexico)
Contact e-mail sabino_m@hotmail.com
19
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 20?28,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multi-document multilingual summarization and evaluation tracks in
ACL 2013 MultiLing Workshop
George Giannakopoulos
NCSR Demokritos, Greece
SciFY NPC, Greece
ggianna@iit.demokritos.gr
Abstract
The MultiLing 2013 Workshop of ACL
2013 posed a multi-lingual, multi-
document summarization task to the
summarization community, aiming to
quantify and measure the performance of
multi-lingual, multi-document summa-
rization systems across languages. The
task was to create a 240?250 word sum-
mary from 10 news articles, describing
a given topic. The texts of each topic
were provided in 10 languages (Arabic,
Chinese, Czech, English, French, Greek,
Hebrew, Hindi, Romanian, Spanish) and
each participant generated summaries
for at least 2 languages. The evaluation
of the summaries was performed using
automatic and manual processes. The
participating systems submitted over 15
runs, some providing summaries across
all languages. An automatic evaluation
task was also added to this year?s set
of tasks. The evaluation task meant to
determine whether automatic measures
of evaluation can function well in the
multi-lingual domain. This paper provides
a brief description related to the data of
both tasks, the evaluation methodology, as
well as an overview of participation and
corresponding results.
1 Introduction
The MultiLing Pilot introduced in TAC 2011
was a combined community effort to present
and promote multi-document summarization ap-
poraches that are (fully or partly) language-neutral.
This year, in the MultiLing 2013 Workshop of
ACL 2013, the effort grew to include a total of
10 languages in a multi-lingual, multi-document
summarization corpus: Arabic, Czech, English,
French, Greek, Hebrew, Hindi from the old cor-
pus, plus Chinese, Romanian and Spanish as new
additions. Furthermore, the document set in exist-
ing languages was extended by 5 new topics. We
also added a new track aiming to work on evalu-
ation measures related to multi-document summa-
rization, similarly to the AESOP task of the recent
Text Analysis Conferences.
This document describes:
? the tasks and the data of the multi-document
multilingual summarization track;
? the evaluation methodology of the participat-
ing systems (Section 2.3);
? the evaluation track of MultiLing (Section 3).
? The document is concluded (Section 4) with a
summary and future steps related to this spe-
cific task.
The first track aims at the real problem of sum-
marizing news topics, parts of which may be de-
scribed or happen in different moments in time.
The implications of including multiple aspects of
the same event, as well as time relations at a vary-
ing level (from consequtive days to years), are still
difficult to tackle in a summarization context. Fur-
thermore, the requirement for multilingual appli-
cability of the methods, further accentuates the dif-
ficulty of the task.
The second track, summarization evaluation,
is related the corresponding, prominent research
problem of how to automatically evaluate a sum-
mary. While commonly used methods build upon
a few human summaries to be able to judge au-
tomatic summaries (e.g., (Lin, 2004; Hovy et al,
2005)), there also exist works on fully automatic
evaluation of summaries, without human?model?
summaries (Louis and Nenkova, 2012; Saggion
et al, 2010). The Text Analysis Conference has
a separate track, named AESOP (e.g. see (Dang
20
and Owczarzak, 2009)) aiming to test and evaluate
different automatic evaluation methods of summa-
rization systems. We perform a similar task, but in
a multilingual setting.
2 Multi-document multi-lingual
summarization track
In the next paragraphs we describe the task, the
corpus, the evaluation methodology and the results
related to the summarization track of MultiLing
2013.
2.1 The summarization task
This MultiLing task aims to evaluate the appli-
cation of (partially or fully) language-independent
summarization algorithms on a variety of lan-
guages. Each system participating in the task was
called to provide summaries for a range of different
languages, based on corresponding corpora. In the
MultiLing Pilot of 2011 the languages used were 7,
while this year systems were called to summarize
texts in 10 different languages: Arabic, Chinese,
Czech, English, French, Greek, Hebrew, Hindi,
Romanian, Spanish. Participating systems were
required to apply their methods to a minimum of
two languages.
The task was aiming at the real problem of sum-
marizing news topics, parts of which may be de-
scribed or may happen in different moments in
time. We consider, similarly to MultiLing 2011
(Giannakopoulos et al, 2011) that news topics can
be seen as event sequences:
Definition 1 An event sequence is a set of atomic
(self-sufficient) event descriptions, sequenced in
time, that share main actors, location of occurence
or some other important factor. Event sequences
may refer to topics such as a natural disaster, a
crime investigation, a set of negotiations focused
on a single political issue, a sports event.
The summarization task requires to generate a
single, fluent, representative summary from a set
of documents describing an event sequence. The
language of the document set will be within the
given range of 10 languages and all documents in a
set share the same language. The output summary
should be of the same language as its source doc-
uments. The output summary should be between
240 and 250 words.
2.2 Summarization Corpus
The summarization corpus is based on a gath-
ered English corpus of 15 topics (10 of which
were already available fromMultiLing 2011), each
containing 10 texts. Each topic contains at least
one event sequence. The English corpus was then
translated to all other languages (see also (Li et
al., 2013; Elhadad et al, 2013)), trying to gener-
ate sentence-parallel translations.
The input documents generated are UTF8-
encoded, plain text files. The whole set of trans-
lated documents together with the original English
document set will be referred to as the Source Doc-
ument Set. Given the creation process, the Source
Document Set contains a total of 1350 texts (650
more than the corpus of the MultiLing 2011 Pilot):
7 languages (Arabic, Czech, English, Greek, ) with
15 topics per language and 10 texts per topic for a
total of 1050 texts; 3 languages (Chinese, French,
Hindi) with 10 topics per language and 10 texts per
topic for a total of 300 texts.
The non-Chinese texts had an average word
length of approximately 350 words (and a standard
deviation of 224 words). Since words in Chinese
cannot be counted easily, the Chinese text length
was based on the byte length of the correspond-
ing files. Thus, Chinese texts had an average byte
length of 1984 bytes (and a standard deviation of
1366 bytes). The ratio of average words in non-
Chinese texts to average bytes in Chinese texts
shows that on average one may (simplisticly) ex-
pect that 6 bytes of Chinese text are adequate to
express one word from a European language.
We note that the measurement of Chinese text
length in words proved a very difficult endeavour.
In the future we plan to use specialized Chinese
tokenizers, which have an adequately high perfor-
mance that will allowmeasuring text and summary
lengths in words more accurately.
2.3 Evaluation Methodology
The evaluation of results was perfromed both
automatically and manually. The manual evalu-
ation was based on the Overall Responsiveness
(Dang and Owczarzak, 2008) of a text. For the
manual evaluation the human evaluators were pro-
vided the following guidelines:
Each summary is to be assigned an
integer grade from 1 to 5, related to the
overall responsiveness of the summary.
We consider a text to be worth a 5, if
21
it appears to cover all the important as-
pects of the corresponding document set
using fluent, readable language. A text
should be assigned a 1, if it is either un-
readable, nonsensical, or contains only
trivial information from the document
set. We consider the content and the
quality of the language to be equally im-
portant in the grading.
The automatic evaluation was based on human,
model summaries provided by fluent speakers of
each corresponding language (native speakers in
the general case). ROUGE variations (ROUGE-
1, ROUGE-2, ROUGE-3, ROUGE-4) (Lin, 2004)
and the AutoSummENG-MeMoG (Giannakopou-
los et al, 2008; Giannakopoulos and Karkalet-
sis, 2011) and NPowER (Giannakopoulos and
Karkaletsis, 2013) methods were used to automat-
ically evaluate the summarization systems. Within
this paper we provide results based on ROUGE-2
and MeMoG methods.
2.4 Participation and Overview of Results
This section provides a per-language overview
of participation and of the evaluation results.
For an overview of participation information see
Table 1. In the table, one can find the mapping
between participant teams and IDs, as well as per
language information. An asterisk in a cell indi-
cates systems of co-organizers for the specific lan-
guage. These systems had early access to the cor-
pus for their language and, thus, had an advantage
over others on that specific language.
Moreover, for the MultiLing pilot we created
two systems, one acting as a global baseline (Sys-
tem ID6) and the other as a global topline (System
ID61). These two systems are described briefly in
the following paragraphs.
2.5 Baseline/Topline Systems
The two systems devised as pointers of a stan-
dard, simplistic approach and of an approach tak-
ing into account human summaries were imple-
mented as follows.
The global baseline system?ID6? represents
the documents of a topic in vector space using a
bag-of-words approach. Then it determines the
centroidC of the document set in that space. Given
the centroid, the system gets the text T that is most
similar to the centroid (based on the cosine simi-
larity) and uses it in the summary. If the text ex-
ceeds the summary word limit, then only a part of
it is used to provide the summary. Otherwise, the
whole text is added as summary text. If the sum-
mary is below the lower word limit, the process is
repeated iteratively adding the next most similar
document to the centroid.
The global topline system ? ID61 ? uses the
(human) model summaries as a given (thus cheat-
ing). These documents are represented in the vec-
tor space similarly to the global baseline. Then,
an algorithm produces random summaries by com-
bining sentences from the original texts. The sum-
maries are evaluated by their cosine similarity to
the centroid of the model summaries.
We use the centroid score as a fitness measure
in a genetic algorithm process. The genetic algo-
rithm fitness function also penalizes summaries of
out-of-limit length. Thus, what we do is that we
search, using a genetic algorithm process, through
the space of possible summaries, to produce one
that mostly matches (an average representation of)
the model summaries. Of course, using an in-
termediate, centroid representation, loses part of
the information in the original text. Through this
method we want to see how well we can create
summaries by knowing a priori what (on average)
must be included.
Unfortunately, the sentence splitting module of
the topline, based on the Apache OpenNLP li-
brary1 statistical sentence splitted failed due to a
bug in our code. This resulted in an interesting
phenomenon: the system would maximize sim-
ilarity to the centroid, using fragments of sen-
tences. This is actually an excellent way to ex-
amine what types of text can cheat n-gram based
methods that they are good, while remaining just-
not-good-enough from a human perspective. In the
system performance analysis sections we will see
that this expectation holds.
In the Tables of the following sectionwe provide
MeMoG and Overall Responsiveness (OR) statis-
tics per system and language. We also provide in-
formation on statistically significant performance
differences (based on Tukey HSD tests).
2.6 Language-specific Tables
The tables below illustrate the system per-
formances per language. Each table contains
three columns: ?Group?, ?SysID? and ?Avg Perf?.
The Group column indicates to which statistically
1See http://opennlp.apache.org/.
22
Participant Run IDs Arabic Chinese Czech English French Greek Hebrew Hindi Romanian Spanish
Maryland ID1, ID11, ID21 ? ? ? ? ? ? ? ? ? ?
CIST ID2 ? ? ? ? ? ? ? ? ? ? ?
Lancaster ID3 ? ? ?
WBU ID4 ? ? ?? ? ? ? ? ? ? ?
Shamoon ID5, ID51 ? ? ??
Baseline ID6 Centroid baseline for all languages
Topline ID61 Using model summaries for all languages
Table 1: Participation per language. An asterisk indicates a contributor system, with early access to
corpus data.
Group SysID Avg Perf
a ID61 0.2488
ab ID4 0.2235
abc ID1 0.2190
abc ID11 0.2054
abc ID21 0.1875
abc ID2 0.1587
abc ID5 0.1520
bc ID51 0.1450
bc ID6 0.1376
c ID3 0.1230
Table 2: Arabic: Tukey?s HSD test MeMoG
groups
equivalent groups of performance a system be-
longs. If two systems belong to the same group,
they do not have statistically significant differ-
ences in their performance (95% confidence level
of Tukey?s HSD test). The SysID column indicates
the system ID and the ?Avg Perf? column the av-
erage performance of the system in the given lan-
guage. The caption of each table indicates what
measure was used to grade performance. In the
Overall Responsiveness (OR) tables we also pro-
vide the grades assigned to human summarizers.
We note that for two of the languages ? French,
Hindi ? there were no human evaluations this
year, thus there are no OR tables for these lan-
guages. At the time of writing of this paper, there
were also no evaluations for human summaries for
the Hebrew and the Romanian languages. These
data are planned to be included in an extended
technical report, whichwill bemade available after
the workshop at the MultiLing Community web-
site2, as an addenum to the proceedings.
There are several notable findings in the tables:
? In several languages (e.g., Arabic, Spanish)
there were systems (notable system ID4) that
2See http://multiling.iit.demokritos.gr/
pages/view/1256/proceedings-addenum)
Group SysID Avg Perf
a B 4.07
ab C 3.93
ab A 3.80
ab ID6 3.71
ab ID2 3.58
ab ID3 3.58
ab ID4 3.49
ab ID1 3.47
abc ID11 3.33
bcd ID21 3.11
cde ID51 2.78
de ID5 2.71
e ID61 2.49
Table 3: Arabic: Tukey?s HSD test OR groups
Group SysID Avg Perf
a ID4 0.1019
ab ID61 0.0927
bc ID2 0.0589
bc ID1 0.0540
bc ID11 0.0537
c ID21 0.0256
c ID6 0.0200
Table 4: Chinese: Tukey?s HSD test MeMoG
groups
Group SysID Avg Perf
a B 4.47
a C 4.30
a A 4.03
b ID2 3.40
c ID4 2.43
c ID61 2.33
c ID21 2.13
c ID11 2.13
c ID1 2.07
d ID6 1.07
Table 5: Chinese: Tukey?s HSD test OR groups
23
Group SysID Avg Perf
a ID61 0.2500
a ID4 0.2312
ab ID11 0.2139
ab ID21 0.2120
ab ID1 0.2026
b ID2 0.1565
b ID6 0.1489
Table 6: Czech: Tukey?s HSD testMeMoG groups
Group SysID Avg Perf
a B 4.75
ab A 4.633
ab C 4.613
ab D 4.215
b E 4.1
c ID4 3.129
d ID1 2.642
d ID11 2.604
de ID21 2.453
e ID61 2.178
e ID2 2.067
f ID6 1.651
Table 7: Czech: Tukey?s HSD test OR groups
Group SysID Avg Perf
a ID4 0.2220
a ID11 0.2129
a ID61 0.2103
ab ID1 0.2085
ab ID21 0.1903
ab ID6 0.1798
ab ID2 0.1751
ab ID5 0.1728
b ID3 0.1590
b ID51 0.1588
Table 8: English: Tukey?s HSD test MeMoG
groups
Group SysID Avg Perf
a A 4.5
a C 4.467
a B 4.25
ab D 4.167
ab ID4 3.547
b ID11 3.013
b ID6 2.776
bc ID21 2.639
bc ID51 2.571
bc ID61 2.388
bc ID5 2.245
bc ID1 2.244
bc ID3 2.208
c ID2 1.893
Table 9: English: Tukey?s HSD test OR groups
Group SysID Avg Perf
a ID4 0.2661
ab ID61 0.2585
ab ID1 0.2390
ab ID11 0.2353
ab ID21 0.2180
ab ID6 0.1956
b ID2 0.1844
Table 10: French: Tukey?s HSD test MeMoG
groups
Group SysID Avg Perf
a ID61 0.2179
ab ID11 0.1825
ab ID1 0.1783
ab ID21 0.1783
ab ID4 0.1727
b ID2 0.1521
b ID6 0.1393
Table 11: Greek: Tukey?s HSD test MeMoG
groups
24
Group SysID Avg Perf
a A 3.889
a ID4 3.833
a B 3.792
a C 3.792
a D 3.583
ab ID11 2.878
ab ID6 2.795
ab ID1 2.762
ab ID21 2.744
ab ID61 2.717
b ID2 2.389
Table 12: Greek: Tukey?s HSD test OR groups
Group SysID Avg Perf
a ID61 0.219
ab ID11 0.1888
ab ID4 0.1832
ab ID21 0.1668
ab ID51 0.1659
ab ID1 0.1633
ab ID5 0.1631
b ID6 0.1411
b ID2 0.1320
Table 13: Hebrew: Tukey?s HSD test MeMoG
groups
Group SysID Avg Perf
a ID11 0.1490
a ID4 0.1472
a ID2 0.1421
a ID21 0.1402
a ID61 0.1401
a ID1 0.1365
a ID6 0.1208
Table 14: Hindi: Tukey?s HSD test MeMoG
groups
Group SysID Avg Perf
a ID61 0.2308
a ID4 0.2100
a ID1 0.2096
a ID21 0.1989
a ID11 0.1959
a ID6 0.1676
a ID2 0.1629
Table 15: Romanian: Tukey?s HSD test MeMoG
groups
Group SysID Avg Perf
a ID4 4.336
ab ID6 4.033
bc ID11 3.433
c ID1 3.329
c ID21 3.207
c ID61 3.051
c ID2 2.822
Table 16: Romanian: Tukey?s HSD test OR groups
Group SysID Avg Perf
a ID4 0.2516
a ID61 0.2491
ab ID11 0.2399
ab ID1 0.2261
ab ID21 0.2083
ab ID2 0.2075
b ID6 0.187
Table 17: Spanish: Tukey?s HSD test MeMoG
groups
Group SysID Avg Perf
a C 3.867
a ID4 3.844
a B 3.778
ab A 3.667
abc ID6 3.444
bc ID2 3.067
c ID11 3.022
c ID1 2.978
c ID21 2.956
c ID61 2.844
Table 18: Spanish: Tukey?s HSD test OR groups
25
reached human level performance.
? The centroid baseline performed very well in
several cases (e.g., Spanish, Arabic), while
rather badly in others (e.g., Czech).
? The cheating topline system did indeed man-
age to reveal a blind-spot of automatic evalu-
ation, achieving high MeMoG grades, while
performing badly in terms of OR grade.
We note that detailed results related to the per-
formances of the participants will be made avail-
able via the MultiLing website3.
3 Automatic Evaluation track
In the next paragraphs we describe the task, the
corpus and the evaluation methodology related to
the automatic summary evaluation track of Multi-
Ling 2013.
3.1 The Evaluation Task
This task aims to examine how well automated
systems can evaluate summaries from different
languages. This task takes as input the summaries
generated from automatic systems and humans in
the Summarization Task. The output should be a
grading of the summaries. Ideally, we would want
the automatic evaluation to maximally correlate to
human judgement.
3.2 Evaluation Corpus
Based on the Source Document Set, a number
of human summarizers and several automatic sys-
tems submitted summaries for the different top-
ics in different languages. The human summaries
were considered model summaries and were pro-
vided, together with the source texts and the auto-
matic summaries, as input to summary evaluation
systems. There were a total of 405 model sum-
maries and 929 automatic summaries (one system
did not submit summaries for all the topics). Each
topic in each language was mapped to 3 model
summaries.
The question posed in the multi-lingual con-
text is whether an automatic measure is enough
to provide a ranking of systems. In order to an-
swer this question we used the ROUGE-2 score,
as well as the ?n-gram graph?-based methods (Au-
toSummENG, MeMoG, NPowER) to grade sum-
maries. We used ROUGE-2 because it has been ro-
bust and highly used for several years in the DUC
3See http://multiling.iit.demokritos.gr
and TAC communities. There was only one ad-
ditional participating measure for the evaluation
track ? namely the Coverage measure ? in ad-
dition to the above methods.
In order to measure correlation we used
Kendall?s Tau, to see whether grading with the au-
tomatic or the manual grades would cause differ-
ent rankings (and how different). The results of
the correlation per language are indicated in Ta-
ble 19. Unfortunately, the Hebrew evaluation data
were not fully available at the time of writing and,
thus, they could not be used. Please check the tech-
nical report tha twill be available after the comple-
tion of the Workshop for more information4.
4 Summary and Future Directions
Overall, the MultiLing 2013 multi-document
summarization and summary evaluation tasks
aimed to provide a scientifically acceptable bench-
mark setting for summarization systems. Building
upon previous community effort we managed to
achieve two main aims of the MultiLing Pilot of
2011 (Giannakopoulos et al, 2011): we managed
to increase the number of languages included to 10
and increase the number of topics per language.
We should also note that the addition of Chinese
topics offered a fresh set of requirements, related
to the differences of writing in this specific lan-
guage from writing in the rest of the languages in
the corpus: not even tokenization is easy to transfer
to Chinese from other,e.g. European languages.
The main lessons learned from the multi-
document and evaluation tracks were the follow-
ing:
? multi-document summarization is an active
domain of research.
? current systems seem to performwell-enough
to provide more than basic, acceptable ser-
vices to humans in a variety of languages.
However, there still exist challenging lan-
guages.
? there are languages where systems achieved
human-grade performance.
? automatic evaluation of summaries in differ-
ent languages in far from an easy task. Much
more effort must be put in this direction, to
facilitate summarization research.
4See http://multiling.iit.demokritos.gr/
pages/view/1256/proceedings-addenum)
26
Language R2 to OR MeMoG to OR Coverage to OR
Arabic -0.11 0.00 -0.07
Chinese -0.38 0.46 0.41
Czech 0.38 0.30 0.26
English 0.22 0.24 0.26
Greek 0.07 0.07 0.03
Romanian 0.15 0.16 0.12
Spanish 0.01 0.05 0.04
All languages 0.12 0.18 0.14
Table 19: Correlation (Kendall?s Tau) Between Gradings. Note: statistically significant results, with
p-value < 0.05, in bold.
The main steps we plan to take, based also on
the future steps inherited from the MultiLing Pilot
of 2011 are:
? to find the funds required for the evaluation
process, in order to support the quality of the
endeavour.
? to use the top performing evaluation system
as the main evaluationmeasure in futureMul-
tiLing workshops.
? to create a piece of support software that will
help implement and track all corpus genera-
tion processes.
? to study the possibility of breaking down the
summarization process and asking systems
to make individual components available as
(web) services to other systems. This prac-
tice aims to allow combinations of different
components into new methods.
? to check the possibility of using the corpus for
cross-language summarization. We can either
have the task of generating a summary in a
different language than the source documents,
or/and use multi-language source documents
on a single topic to provide a summary in one
target language.
? to start a track aiming to measure the effec-
tiveness of multi-lingual summarization as a
commercial service to all the world. This
track would need a common interface, hid-
ing the underlying mechanics from the user.
The user, in turn, will be requested to judge a
summary based on its extrinsic value. Much
conversation needs to be conducted in order
for this task to provide a meaningful compar-
ison between systems. The aim of the track
would be to illustrate the current applicability
of multilingual multi-document summariza-
tion systems in a real-world task, aiming at
non-expert users.
Overall, the MultiLing effort enjoys the con-
tribution of a flourishing research community on
multi-lingual summarization research. We need to
continue building on this contribution, inviting and
challenging more researchers to participate in the
community. So far we have seen the MultiLing ef-
fort grow from a pilot to a workshop, encompass-
ing more and more languages and research groups
under a common aim: providing a commonly ac-
cepted benchmark setting for current and future
multi-lingual summarization systems.
References
H. T. Dang and K. Owczarzak. 2008. Overview of the
TAC 2008 update summarization task. In TAC 2008
Workshop - Notebook papers and results, pages 10?
23, Maryland MD, USA, November.
Hoa Trang Dang and K. Owczarzak. 2009. Overview
of the tac 2009 summarization track, Nov.
Michael Elhadad, Sabino Miranda-Jim?nez, Josef
Steinberger, and George Giannakopoulos. 2013.
Multi-document multilingual summarization corpus
preparation, part 2: Czech, hebrew and spanish. In
MultiLing 2013 Workshop in ACL 2013, Sofia, Bul-
garia, August.
George Giannakopoulos and Vangelis Karkaletsis.
2011. Autosummeng and memog in evaluating
guided summaries. In TAC 2011 Workshop, Mary-
land MD, USA, November.
George Giannakopoulos and Vangelis Karkaletsis.
2013. Summary evaluation: Together we stand
npower-ed. In Computational Linguistics and Intel-
ligent Text Processing, pages 436?450. Springer.
27
George Giannakopoulos, Vangelis Karkaletsis, George
Vouros, and Panagiotis Stamatopoulos. 2008. Sum-
marization system evaluation revisited: N-gram
graphs. ACM Trans. Speech Lang. Process., 5(3):1?
39.
G. Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,
J. Steinberger, and V. Varma. 2011. TAC 2011
MultiLing pilot overview. In TAC 2011 Workshop,
Maryland MD, USA, November.
E. Hovy, C. Y. Lin, L. Zhou, and J. Fukumoto. 2005.
Basic elements.
Lei Li, Corina Forascu, Mahmoud El-Haj, and George
Giannakopoulos. 2013. Multi-document multilin-
gual summarization corpus preparation, part 1: Ara-
bic, english, greek, chinese, romanian. InMultiLing
2013 Workshop in ACL 2013, Sofia, Bulgaria, Au-
gust.
C. Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. Proceedings of the Workshop
on Text Summarization Branches Out (WAS 2004),
pages 25?26.
Annie Louis and Ani Nenkova. 2012. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39(2):267?
300, Aug.
H. Saggion, J. M. Torres-Moreno, I. Cunha, and E. San-
Juan. 2010. Multilingual summarization evalu-
ation without human models. In Proceedings of
the 23rd International Conference on Computational
Linguistics: Posters, page 1059?1067.
28
