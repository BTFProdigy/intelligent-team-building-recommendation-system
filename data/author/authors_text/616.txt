Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1119?1123,
Prague, June 2007. c?2007 Association for Computational Linguistics
Hybrid?Ways?to?Improve?Domain?Independence?
in?an?ML?Dependency?Parser
Eckhard?Bick
Institute?of?Language?and?Communication
University?of?Southern?Denmark
5230?Odense??M,?Denmark
eckhard.bick@mail.dk
Abstract
This ? paper ? reports ? a ? hybridization ? experi?
ment,?where?a?baseline?ML?dependency?pars?
er, ? LingPars, ? was ? allowed ? access ? to ? Con?
straint?Grammar?analyses?provided?by?a?rule?
based?parser?(EngGram)?for?the?same?data.?
Descriptive?compatibility?issues?and?their?in?
fluence ?on ?performance ?are ?discussed. ?The?
hybrid?system?performed?considerably?better?
than? its ?ML?baseline,?and?proved?more?ro?
bust?than?the?latter?in?the?domain?adaptation?
task,?where?it?was?the?best?scoring?system?in?
the?open?class?for?the?chemical?test?data,?and?
the?best?overall?system?for?the?CHILDES?test?
data.
1 Introduction
LingPars,?a?language?independent?treebank?learner?
developed ? in ? the ?context ?of ? the ?CoNLL?X?2006?
shared?task?(http://nextens.uvt.nl/~conll/),?was??in?
spired ?by ? the ?Constraint ?Grammar ? (CG) ?parsing?
approach?(Karlsson?et?al.?1995)?in?the?sense?that?it?
prioritized?the?identification?of?syntactic?function?
over?syntactic?form,?basing?the?dependency?poten?
tial?of?a?word?on?"edge"?labels?like?subject,?object?
etc.?rather?than?the?other?way?around.?The?system?
also ? used ? other ? features ? typical ? of ? CG? systems,?
such?as?BARRIER?conditions,?tag?chains?of?vari?
able?length,?implicit?clause?boundaries?and?tag?sets?
(Bick?2006).?For?the?2007?task?only?one?such?fea?
ture?was?newly?introduced???a?directedness?marker?
for?a?few?major?functions,?splitting?subject,?adver?
bial ?and?adnominal ? labels ? into?pairs?of ? left? ?and?
right?attaching?labels?(e.g.?SBJ?L,?SBJ?R,?NMOD?
L,?NMOD?R).?Even?this?small?addition,?however,?
increased? the ?memory?space? requirements ?of ? the?
model?to?such?a?degree?that?only?runs?with?50?75%?
of?the?training?data?were?possible?on?the?available?
hardware.
The?main?purpose?of?the?LingPars?architecture?
changes?for?CoNLL2007?(Nivre?et?al.?2007),?how?
ever,?was?to?test?two?core?hypotheses:
? Can ? an ? independent, ? rule?based ? parser ? be?
made? to ?conform?to?different, ?data?imposed?
descriptive ? conventions ?without ? too ?great ? a?
loss?in?accuracy?
? Does?a?rules?based?dependency?parser?have?a?
better?chance?than?a?machine?learned?one?to?
identify ? long?distance ? relations ? and ? global?
sentence ? structure, ? thus ? providing ? valuable?
arbiter?information?to?the?latter?
Obviously, ? both ?points ? rule ?out ? a ? test ? involving?
many?languages?with?the?same?parser?(CoNLL?task?
1).?The?domain?adaptation?task?(task?2),?however,?
satisfied ? the ? single?language ? condition ? and ? also?
adressed?the?descriptive?adaptation?problem?(sec?
ond?hypothesis),?involving?three?English?treebanks?
??Wall?Street?Journal?data?from?the?Penn?treebank?
(PTB, ?Marcus ? et ? al. ? 1993) ? for ? training, ? and ? the?
Pchem?(Kulick?et?al.?2004)?and?CHILDES?(Brown?
1973 ? and ? MacWhinney ? 2000) ? treebanks ? with?
biomedical?and?spoken?language?data,?respectively.
2 Developing?and?adapting?EngGram
A ? parser ? with ? hand?written ? rules ? pays ? a ? high?
"labour?price"?to?arrive?at?deep,?linguistically?pre?
1119
dictable?and?versatile?analyses.?For?CG?systems?as?
employed?by?the?author,?the?cost,?from?lexicon?to?
dependency,?is?usually?several?man?years,?and?re?
sults?are?not?language?independent.?One?way?of?in?
creasing ? development ? efficiency ? is ? to ? combine?
modules ? for ? different ? levels ? of ? analysis ? while?
reusing?or?adapting?the?less?language?independent?
ones.?Thus,?the?development?of?a?new?English?de?
pendency?parser, ?EngGram,?under?way?for ?some?
time, ?was ? accelerated ? for ? the ?present ? project ? by?
seeding ? the ? syntactic ? disambiguation ? grammar?
with ?Danish ?rules?from?the?well?established?Dan?
Gram ? parser ? (http://beta.visl.sdu.dk/?
constraint_grammar.html). ? By ? maintaining ? an?
identical?set?of?syntactic?function?tags,?it?was?even?
possible ? to ? use ? the ? Danish ? dependency ? module?
(Bick?2005)?with?only?minor?adaptations?(mainly?
concerning?noun?chains?and?proper?nouns).
In?order?to?integrate?the?output?of?a?CG?parser?
into?an?ML?parser?for?the?shared?task?data,?several?
levels?of?compatibility?issues?have?to?be?addressed.?
On? the ? input ?side, ? (1) ?PTB?tokenization?and? (2)?
word?classes?(PoS) ? ?have? to?be?fed? into? the?CG?
parser?bypassing?its?own?modules?of?morphologi?
cal ? analysis ? and ? disambiguation. ? On ? the ? output?
side,?(3)?CG?function?categories?and?(4)?attachment?
conventions ? have ? to ? be ? adapted ? to ? match ? PTB?
ones.
For?example,?the?manual?rules?were?tuned?to?a?
tokenization?system??that?handles?expressions?such?
as ? "a=few", ? "at=least" ? and ? "such=as" ? as ? units.?
Though ? amounting ? to ? only ? 1% ? of ? running ? text,?
they ? constitute ? syntactically ? crucial ? words, ? and?
misanalysis ? leads ? to ?numerous ? secondary ? errors.?
Even?worse?is?the?case?of?the?genitive?s?(also?with?
a?frequency?of?1%),?tokenised?in?the?PTB?conven?
tion,?but?regarded?a?morpheme?in?EngGram.?Since?
EngGram?does?not?have?a?word?class?for?the?isolat?
ed?'s',?and?since?ordinary?rules?disfavour?postnomi?
nal?singel?word?attachment,?the?'s'?had?to?be?fused?
in ? PTB?to?CG ? input, ? creating ? fewer ? tokens ? and?
thus?problems?in?re?aligning?the?analysed?output.?
Also?relevant?for?a?full?structure?parser?is?the?parse?
window. ? Here, ? in ? order ? to ? match ? PTB ? window?
size,?EngGram?had?to?be?forced?not?to?regard?;?(?)?
and?:?as?delimiters,?with?an?arguable?loss?in?annota?
tion?accuracy?due?to?rules?with?global?NOT?con?
texts?designed?for?smaller?windows.
Finally, ? PTB ? convention ? fuses ? certain ? word?
classes,?like?subordinating?conjunctions?and?prepo?
sitions ? ? (IN), ? and ? the ? infititive ? marker ? and ? the?
preposition?"to"?(TO).?Though?these?cases?can?be?
treated?by?letting?CG?disambiguation?override?the?
CoNLL ? input's ? pos ? tag, ? input ? pos ? can ? then ? no?
longer?be?said?to?be?"known",?with?some?deteriora?
tion?in?recall ?as?a?consequence.?Open?class?cate?
gories?matched?well?even?at?a?word?by?word?level,?
closed?class?tokens?were?found?to?sometimes?differ?
for?individual?words,?an?error?source? left ? largely?
unchecked.
Treebank?error?rate?is?another?factor?to?be?con?
sidered???in?cases?where?the?PoS?accuracy?of?the?
human?revised ? treebanks ? is ? lower ? than ? that ?of ? a?
CG?system,?the?latter?should?be?allowed?to?always ?
assign?its?own?tags,?rather?than?follow?the?suppos?
edly?fixed?input?pos.?In?the?domain?adaptation?task,?
the?CHILDES?data?were?a?case?in?point.?A?separate?
CG ? run ? indicated ? 6.6% ? differences ? in ? PoS, ? and?
manual? inspection?of?part?of ? the?cases?suggested?
that ?while ? some ?cases ?were ? irrelevant ? variations?
(e.g.??adjective?vs.?participle),?most?were?real?error?
on ? the ?part ?of ? the ? treebank, ? and ? the ?parser ?was?
therefore?set?to?ignore?test?data?annotation?and?to?
treat?it?as?pure?text.
Errors?appeared?to?be?rarer?in?the?training?data,?
but?inconsistencies?between?pos?and?function?label?
(e.g. ? IN?preposition ? and ? SBJ?subject ? for ? "that")?
prove ? that ? errors ? aren't ? unknown ? here ? either ? ??
which ? is ?why?a ?hybrid ? system?with ? independent?
analysis?has?the?potential?benefit?of?compensating?
for?"mis?learned"?patterns?in?the?ML?system.
Output?conversion?from?CG?to?PTB/CoNLL?for?
mat?had?to?address,?besides?realignment?of?tokens?
(e.g.?genitive?s),?the?disparity?in?edge?(function)?la?
bels.?However,?since?the?PTB?set?was?more?coarse?
grained, ? it ? was ? possible ? to ? simply ? lump ? several?
EngGram?labels?into?one?PTB?label,?for?instance:
SC,?OC,?SUB,?INFM???>?VMOD
ADVL,?SA,?OA,?PIV,?PRED??>?ADV
Some?idiosyncrasies?had?to?be?observed?here,?for?
instance?the?treatment?of?SC?(subject?complement)?
1120
as?VMOD?for?words,?but?ADV?for?clauses,?or?the?
descriptive ?decision ? to ? tag ?direct ?objects ? in ?ACI?
constructions ? with ? OA?clausal ? complements ? as?
subjects.?Some?cases?of?label?variation,?however,?
could?not?be?solved?in?a?systematic?way.?Thus,?ad?
verbs?within?verb?chains, ?always?ADVL?in?Eng?
Gram,?could?not?systematically?be?mapped,?since?
PTB?uses?both?VMOD?and?ADV?in?this?position.?
A?certain?percentage?of?mismatches?in?spite?of?a?
correct ?analysis ?must ? therefore?be? taken? into?ac?
count?as?part?of?the?"price"?for?letting?the?CG?sys?
tem?advise?the?machine?learner.
Dependencies?were?generally?used?in?the?same?
way?in?both?systems,?but?multi?word?expressions?
were ?problematic, ? since ?PTB ? ? ? without ?marking?
them?as?MWE???appears?to?attach?all?elements?to?a?
common?head?even?where?internal?structure?(e.g.?a?
PP)?is?present.?No?reliable?way?was?found?to?pre?
dict ? this ?behaviour ? from?CG?dependency?output.?
Finally,?PTB?often?uses?the?adverbial?modifier?tag?
(AMOD)?for?what?would?logically?be?the ?head?of?
an?expression:
about?(head)?1,200?(AMOD)
so?(head)?totally?(AMOD)
herbicide?(head)?resistant?(AMOD)
EngGram?in?these?examples?regards?the?first?ele?
ment?as?AMOD?modifier,?and?the?second?as?head.?
Since?the?inversion?was?so?common,?it?was?accept?
ed ? as ? either ? intentional ? or ? systematically ? erro?
neous,?and?the?CG?output?inverted?accordingly.?It?
is?an?open?question,? for?future?research,?whether?
the?CG?and?ML?systems?could?have?been?harmo?
nized?better,?had?the?training?data?been?an?original?
dependency?treebank?rather?than?a?constituent?tree?
bank,???or?at?least?linguistically?revised?at?the?de?
pendency?level.?Making?the?constituent?dependen?
cy ? conversion ? principles ? (Johansson ? & ? Nugues?
2007,?forthcoming)?public ?before? rather?than?after?
the?shared? task?might ?also?have?contributed? to ?a?
better?CG?annotation?transfer.
3 System?architecture
As?described?in?(Bick?2006),?the?LingPars?system?
uses ? the ? fine?grained ? part ? of ? speech ? (PoS) ? tags?
(POSTAG)?and?? ? for ?words ?above?a ?certain ? fre?
quency ? threshold ? ? ? the ? LEMMA ? or, ? if ? absent,?
FORM?tag.?In?a?first?round,?LingPars?calculates?a?
preference?list ?of ?functions?and?dependencies?for?
each?word,?examining?all?possible?mother?daughter?
pairs?and?n?grams?in?the?sentence?(or?paragraph).?
Next,?dependencies?are?adjusted?for?function,?basi?
cally?summing?up?the?frequency?,?distance??and?di?
rection?calibrated?function?>PoS?attachment?prob?
abilities?for?all?contextually?allowed?functions?for?a?
given?word.?Finally,?dependency?probabilities?are?
weighted ? using ? linked ? probabilities ? for ? possible?
mother?,?daughter??and?sister?tags?in?a?second?pass.
The?result?are?2?arrays,?one?for?possible?daugh?
ter?>mother ? pairs, ? one ? for ? word:function ? pairs.?
LingPars?then?attempts?to?"effectuate"?the?depen?
dency?(daughter?>mother)?array,?starting?with?the???
in?normalized?terms???highest?value.?If?the?daughter?
candidate?is?as?yet?unattached,?and?the?dependency?
does?not?produce?circularities?or?crossing?branches,?
the?corresponding?part?of?the?(ordered)?word:func?
tion?array?is?calibrated?for?the?suggested?dependen?
cy,?and?the?top?ranking?function?chosen.
One?of?the?major?problems?in?the?original?sys?
tem?was?uniqueness?clashes,?and?as?a?special?case,?
root ?attachment?ambiguity,? resulting?from?a?con?
flict?between?the?current?best?attachment?candidate?
in?the?pipe?and?an?earlier?chosen?attachment?to?the?
same?head.?Originally,?the?parser?tried?to?resolve?
these?conflicts?by?assigning?penalties?to?the?attach?
ments?in?question?and?recalculating?"second?best"?
attachments?for?the?tokens?in?question.?While?solv?
ing?some?cases,?this?method?often?timed?out?with?
out?finding?a?globally?compatible?solution.
In?the?new?version?of?LingPars, ?with?open?re?
sources,?the?attachment?and?function?label?rankings?
were?calibrated?using?the?analysis?suggested?by?the?
EngGram?CG?system?for?the?same?data,?assigning?
extra ? weights ? to ? readings ? supported ? by ? the ? rule?
based?analysis,?using?addition?of?a?weight?constant?
for?function,?and?multiplication?with?a?weight?con?
stant?for?attachments,?thus?integrating?CG?informa?
tion?on?par?with?statistical?information1.?This?was?
1Experiments?suggested?that?there?is?a?limit?beyond?which?an?
increase?of?these?weighting?constants,?for?both?function?and?
dependency,?will?actually?lead?to?a?decrease?in?performance,?
because?the?positive?effect?of?long?distance?attachments?from?
the?CG?system?will?be?cancelled?out?by?the?negative?effect?of?
1121
not, ? however, ? thought ? sufficient ? to ? resolve ? the?
global?syntactic?problem?of?root?attachment?where?
(wrong)?statistical?preferences?could?be?so?strong?
that?even?20?rounds?of?penalties?could?not?weaken?
them?sufficient?to?be?ruled?out.?Therefore,?root?and?
root?attachments?supported?by?the?CG?trees?were?
fixed? in ? the?first ?pass, ?without ?reruns. ?The?same?
method?was?used?for?another?source?of?global?er?
rors???coordination.?Here,?the?probabilistic?system?
had?difficulties?learning?patterns,?because?a?specif?
ic?function?label?(SBJ?or?OBJ?etc)?would?be?associ?
ated?with?a ?non?specific ?word?class ? (CC), ?and?a?
non?specific?function?(COORD)?with?a?host?of?dif?
ferent ? word ? classes. ? Again, ? adding ? a ? first?pass?
override??based?on?CG?provided?coordination?links?
solved?many?of?these?cases.
Though?limited?to?2?types?of?global?dependency?
(root?and?coordination),?the?help?provided?by?the?
rule?based?analysis, ?also?had?indirect ?benefits?by?
providing?a?better?point?of?departure?for?other?at?
tachments, ?among?other ? things ?because?LingPars?
exaggerated?both?good?and?bad?analyses:?Good?at?
tachments ? would ? help ? weight ? other ? attachments?
through ? correct ? n?gram?, ? mother?, ? daughter? ? and?
sibling ? contexts, ? but ? isolated ? bad ? attachments?
would?lead?to?even?worse?attachments?by?trigger?
ing,?for?instance,?incorrect?BARRIER?or?crossing?
branch ? constraints. ? These ? adverse ? effects ? were?
moderated?by?getting?a?larger?percentage?of?global?
dependencies?right?in?the?first?place,?and?also?by?a?
new?addition?to?the?crossing?and?BARRIER?sub?
routine?invalidating?it?in?the?case?of?CG?supported?
attachments.
4 Evaluation
The?hybrid?LingPars?was?the?best?scoring?system?
in ? the ? open ? section ? of ? both ? domain ? adaptation?
tasks2?(Nivre?et?al.?2007),?outperforming?its?proba?
bilistic?core?system?on?all?scores,?with?an?improve?
ment ? of ? 6.57 ? LAS ? percentage ? points ? for ? the?
disturbing?the?application?of?machine?learned?local?dependen?
cies.
2?During?the?test?phase,?the?data?set?for?one?of?the?originally?2?
test?domains,?CHILDES,?was?withdrawn?from?the?official?
ranking,?though?its?scores?were?still?computed?and?admissible?
for?evaluation.
pchemtb ? corpus ? (table ? 1), ? and ? 3.42 ? for ? the?
CHILDES?attachment?score?(table?2). ?In?the?for?
mer,? the?effect?was?slightly?more?marked?for?at?
tachment?than?for?label?accuracy.?
However,?whereas?results?also?surpassed? those?
of?the?top?closed?class?system?in?the?CHILDES?do?
main?(by?1.12?percentage?points),?they?fell?short?of?
this ?mark?for?the?pchemtb?corpus???by?1.26?per?
centage?points?for?label?accuracy?and?1.80?for?at?
tachment.?
Top?score
pchemtb
average
pchemtb
System
pchemtb
System
train
Closed
??LAS
??UAS
??LS
81.06
83.42
88.28
73.03
76.42
81.74
71.81
74.71
80.78
(75.01)3
(76.71)
(84.12)
Open
??LAS
??UAS
??LS
78.48
81.62
87.02
65.11
70.24
77.14
78.48
81.62
87.02
(79.04)
(80.82)
(88.07)
Table?1:?Performance,?Pchemtb?data
UAS Top?score average System
CHILDES?closed 61.37 57.89 58.07
CHILDES?open 62.49 56.12 62.49
Table?2:?Performance,?CHILDES?data
When?compared?with?runs?on?(unknown)?data?from?
the?training?domain,?cross?domain?performance?of?
the?closed?system?was?2?percentage?points?lower?
for ? attachment ?and ?3.5 ? lower ? for ? label ? accuracy?
(LA?scores?of?71.81?and?58.07?for?the?pchemtb?and?
CHILDES?corpus,?respectively).
Interestingly,?hybrid?results?for?the?pchemtb?data?
were?only?marginally? lower?than?for?the? training?
domain? ?(in?fact, ?higher? for?attachment),?suggest?
ing?a?higher?domain?robustness?for?the?hybrid?than?
for??the?probabilistic?approach.
3This?is?the?accuracy?for?the?test?data?used?during?develop?
ment.?For?the?PTB?gold?test?data?from?track?1,?LAS?was?high?
er?(76.21).
1122
References?
E.?Bick.?2006,?LingPars,?a?Linguistically?Inspired,?Lan?
guage?Independent?Machine?Learner?for?Dependency?
Treebanks,?In:?M?rquez,?Llu?s?&?Klein,?Dan?(eds.),?
Proceedings ? of ? the ? Tenth ? Conference ? on ? Natural ?
Language?Learning?(CoNLL?X,?New?York,?June?8?9, ?
2006)
E.?Bick.?2005.?Turning?Constraint?Grammar?Data?into?
Running?Dependency?Treebanks.?In:?Civit,?Montser?
rat ?&?K?bler, ?Sandra?&?Mart?, ?Ma.?Ant?nia? (red.),?
Proceedings?of?TLT?2005,?Barcelona.?pp.19?2
R.?Brown.?1973.?A?First?Language:?The?Early?Stages.?
Harvard?University?Press
R. ? Johansson ? and ? P. ? Nugues. ? 2007. ? Extended ? Con?
stituent?to?Dependency ? Conversion ? for ? English. ? In:?
Proceedings?of?NoDaLiDa?16.?Forthcoming
F.?Karlsson,?A.?Vouitilainen,?J.?Heikkil??and?A.?Anttila.?
1995. ? Constraint ? Grammar ? ? ? A ? Language?Indepen?
dent?System?for?Parsing?Unrestricted?Text. ?Mouton?
de?Gruyter:?Berlin.
S. ?Kulick, ?A.?Bies,?M.?Liberman,?M.?Mandel,?R.?Mc?
Donald,?M.?Palmer,?A.?Schein?and?L.?Ungar.?2004.?
Integrated?Annotation?for?biomedical?Information?Ex?
tractions.?In:?Proceedings?of?HLT?NAACL?2004.
B. ?MacWhinney. ?2000. ?The?CHILDES?Project: ?Tools?
for?Analyzing?Talk.?Lawrence?Erlbaum
M.?Marcus,?B.?Santorini?and?M.?Marcinkiewicz.?1993.?
Building?a?Large?Annotated?Corpus?of?English:?The?
Penn?Treebank. ? In: ?Computational ?Linguistics ?Vol. ?
19,2.?pp.?313?330
J.?Nivre,?J.?Hall,?S.?K?bler,?R.?McDonald,?J.?Nilsson,?S.?
Riedel?and?D.?Yuret.?2007.?The?CoNLL?2007?Shared?
Task?on?Dependency?Parsing.?In:?Proceedings?of?the ?
CoNLL?2007?Shared?Task.? ?Joint?Conf.?on?Empirical ?
Methods ? in ? Natural ? Language ? Processing ? and ?
Computational ? Natural ? Language ? Learning ?
(EMNLP?CoNLL).
1123
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 76?79,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic tagging for resolution of indirect anaphora
R. Vieira1, E. Bick2, J. Coelho1, V. Muller1, S. Collovini1, J. Souza1, L. Rino3
UNISINOS1, University of Denmark2, UFSCAR3
renatav@unisinos.br, eckhard.bick@mail.dk, lucia@dc.ufscar.br
Abstract
This paper presents an evaluation of indi-
rect anaphor resolution which considers as
lexical resource the semantic tagging pro-
vided by the PALAVRAS parser. We de-
scribe the semantic tagging process and a
corpus experiment.
1 Introduction
Bridging anaphora represents a special part of the
general problem of anaphor resolution. As a spe-
cial case of anaphora, it has been studied and dis-
cussed by different authors and for various lan-
guages. There are many problems in develop-
ing such studies. First, bridging is not a regu-
lar class, it seldom contains cases of associative
and indirect anaphora (defined in the sequence);
lexical resources such as Wordnet are not avail-
able for every language, and even when available
such resources have proven to be insufficient for
the problem. In fact, different sources of lexi-
cal knowledge have been evaluated for anaphora
resolution (Poesio et al, 2002; Markert and Nis-
sim, 2005; Bunescu, 2003). At last, corpus stud-
ies of bridging anaphora usually report results
on a reduced number of examples, because this
kind of data is scarce. Usually bridging anaphora
considers two types: Associative anaphors are
NPs that have an antecedent that is necessary
to their interpretation (the relation between the
anaphor and its antecedent is different from iden-
tity); and Indirect anaphor are those that have
an identity relation with their antecedents but the
anaphor and its antecedent have different head-
nouns. In both associative and indirect anaphora,
the semantic relation holding between the anaphor
and its antecedent play an essential role for res-
olution. However, here we present an evalu-
ation of the semantic tagging provided by the
Portuguese parser PALAVRAS (Bick, 2000)
(http://visl.sdu.dk/visl/pt/parsing/automatic) as a
lexical resource for indirect anaphora resolution.
We focus on indirect anaphors for two reasons,
they are greater in number and they present better
agreement features concerning human annotation.
2 Semantic Annotation with Prototype
Tags
As a Constraint Grammar system, PALAVRAS
encodes all annotational information as word
based tags. A distinction is made between mor-
phological, syntactic, valency and semantic tags,
and for a given rule module (or level of analysis),
one tag type will be regarded as primary (= flagged
for disambiguation), while tags from lower lev-
els provide unambiguous context, and tags from
higher levels ambiguous lexical potentialities.
Thus, semantic tags are regarded as secondary
help tags at the syntactic level, but will have un-
dergone some disambiguation at the anaphora res-
olution level. The semantic noun classes were
conceived as distinctors rather than semantic de-
finitions, the goal being on the one hand to cap-
ture semantically motivated regularities and rela-
tions in syntax, on the other hand to allow to dis-
tinguish between different senses, or to chose dif-
ferent translation equivalents in MT applications.
A limited set of semantic prototype classes was
deamed ideal for both purposes, since it allows at
the same time similarity-based lumping of words
(useful in structural analysis, IR, anaphora reso-
lution) and context based polysemy resolution for
an individual word (useful in MT, lexicography,
alignment). Though we define class hypernyms
as prototypes in the Roschian sense (Rosch, 1978)
76
as an (idealized) best instance of a given class of
entities, we avoided low level prototypes, using
<Azo> for four-legged land-animals rather than
<dog> and <cat> for dog and cat races etc.).
Where possible, systematic sub-classes were es-
tablished. Semiotic artifacts <sem>, for instance
are sub-divided into ?readables? <sem-r> (book-
prototype: book, paper, magazine), ?watchables?
<sem-w> (film, show, spectacle), ?listenables?
etc. The final category inventory, though devel-
oped independently, resembles the ontology used
in the multilingual European SIMPLE project
(http://www.ub.es/ gilcub/SIMPLE/simple.html).
For the sake of rule based inheritance reasoning,
semantic prototype classes were bundled using a
matrix of 16 atomic semantic features. Thus,
the atomic feature +MOVE is shared by the dif-
ferent human and animal prototypes as well as
the vehicle prototype, but the vehicle prototype
lacks the +ANIM feature, and only the bun-
dle on human prototypes (<Hprof>, <Hfam>,
<Hideo>,...) shares the +HUM feature (human
professional, human family, human follower of a
theory/belief/conviction/ideology). In the parser,
a rule selecting the +MOVE feature (e.g. for sub-
jects of movement verbs) will help discard com-
peting senses from lemmas with the above proto-
types, since they will all inherit choices based on
the shared atomic feature. Furthermore, atomic
features can themselves be subjected to inheri-
tance rules, e.g. +HUM ?> +ANIM ?> +CON-
CRETE, or +MOVE?> +MOVABLE. In Table 1,
which contains examples of polysemic institution
nouns, positive features are marked with capital
letters, negative features with small letters1. The
words in the Table 1 are ambiguous with regard
to the feature H, and since it is only the <inst>
prototype that contributes the +HUM feature po-
tential, it can be singled out by a rule selecting
?H? or by discarding ?h?. The parser?s about 140
prototypes have been manually implemented for a
lexicon of about 35.000 nouns. In addition, the
?HUM category was also introduced as a selec-
tion restriction for 2.000 verb senses (subject re-
striction) and 1.300 adjective senses (head restric-
tion).
While the semantic annotation of common
nouns is carried out by disambiguating a given
lemma?s lexicon-listed prototype potential, this
strategy is not sufficient for proper nouns, due
1furn=furniture, con=container, inst=institution
Ee = entities (?CONCRETE)
Jj = ?MOVABLE
Hh = ?HUMAN ENTITY
Mm = ?MAS
Ll = ?LOCATION
polysemy spectrum
Ee j Hh m Ll faculdade
E H L <inst> univ. faculty
e h l <f-c> property
Ee j Hh m Ll fundo
e h L <Labs> bottom
E H L <inst> foundation
e h l <ac> <smP> funds
Ee j Hh Mm Ll indu?stria
E H m L <inst> industry
e h M l <am> diligence
E Jj Hh m L rede
J h <con> net
j H <inst> <+n> network
J h <furn> hammock
Table 1: Feature bundles in prototype based poly-
semy
to the productive nature of this word class. In
two recent NER projects, the parser was aug-
mented with a pattern recognition module and a
rule-based module for identifying and classify-
ing names. In the first project (Bick, 2003),
6 main classes with about 15 subclasses were
used in a lexeme-based approach, while the
second adopted the 41 largely functional cate-
gories of Linguateca?s joint HAREM evaluation
in 2005 (http://www.linguateca.com). A lexicon-
registered name like Berlin would have a stable
tag (<civ> = civitas) in the first version, while
it would be tagged as either <hum>, <top> or
<org> in the second, dependent on context. At
the time of writing, we have not yet tagged our
anaphora corpus with name type tags, and it is
unclear which approach, lexematic or functional,
will work best for the resolution of indirect and
associative anaphora.
3 Indirect Anaphora Resolution
Our work was based on a corpus formed by 31
newspaper articles, from Folha de Sa?o Paulo, writ-
ten in Brazilian Portuguese. The corpus was au-
tomatically parsed using the parser PALAVRAS,
and manually annotated for anaphoricity using
the MMAX tool(http://mmax.eml-research.de/) .
Four subjects annotated the corpus. All annota-
tors agreed on the antecedent in 73% of the cases,
in other 22% of the cases there was agreement be-
tween three annotators and in 5% of the cases only
two annotators agreed. There were 133 cases of
77
definite Indirect anaphors (NPs starting with def-
inite articles) from the total of 1454 definite de-
scriptions (near to 10%) and 2267 NPs.
The parser gives to each noun of the text (or to
most of them) a semantic tag. For instance, the
noun japone?s [japanese] has the following seman-
tic tags ling and Hnat, representing the features:
human nationality and language respectively.
<word id="word_28">
<n can="japone?s" gender="M" number="S">
<secondary_n tag="Hnat"/>
<secondary_n tag="ling"/>
</n>
</word>
The approach consists in finding relationships
with previous nouns through the semantic tags.
The chosen antecedent will be the nearest expres-
sion with the largest number of equal semantic
tags. For instance, in the example below, the
anaphor is resolved by applying this resolution
principle, to japone?s - a l??ngua.
O Eurocenter oferece cursos de japone?s em Kanazawa.
Apo?s um me?s, o aluno falara? modestamente a l??ngua.
The Eurocenter offers Japanese courses in Kanazawa. Af-
ter one month, a student can modestly speak the language.
As both expressions (japanese and language)
hold the semantic tag ?ling? the anaphor is re-
solved. For the experiments, we considered as cor-
rect the cases where the antecedent found automat-
ically was the same as in the manual annotation
(same), and also the cases in which the antecedent
of the manual annotation was found further up in
the chain identified automatically (in-chain). We
also counted those cases in which the antecedent
of the manual annotation was among the group of
candidates sharing the same tags (in-candidates),
but was not the chosen one (the chosen being the
nearest with greater number of equal tags).
Indirect anaphora
Results # % of Total
Same 25 19%
In-chain 15 11%
Total Correct 40 30%
In-candidates 9 7%
Unsolved 40 30%
Error 44 33%
Total 133 100%
Table 2: Indirect anaphor resolution
Table 2 shows the results of the indirect anaphor
resolution. In 19% of the cases, the system found
the same antecedent as marked in the manual an-
notation. Considering the chain identified by the
system the correct cases go up to 30%. The great
number of unsolved cases were related to the fact
that proper names were not tagged. Considering
mainly the tagged nouns (about 93 cases), the cor-
rect cases amount to 43%). This gives us an idea
of the quality of the tags for the task. We further
tested if increasing the weight of more specific
features in opposition to the more general ones
would help in the antecedent decision process. A
semantic tag that is more specific receives a higher
weight The semantic tag set has three levels, level
1, which is more general receives weight 1, level 2
receives 5, and level 3 receives 10. See the exam-
ple below.
<A> 1 Animal, umbrella tag
<AA> 5 Group of animals
<Adom> 10 Domestic animal
In this experiment the chosen candidate is the
nearest one whose sum of equal tag values has
higher weight. Table 3 shows just a small im-
provement in the correct cases. If we do not
consider unsolved cases, mostly related to proper
names, indirect anaphors were correctly identified
in 46% of the cases (43/96).
Indirect anaphora
Results # % of Total
Same 24 18%
In-chain 19 14%
Total Correct 43 32%
In-candidates 6 5%
Unsolved 40 30%
Error 44 33%
Total 133 100%
Table 3: Indirect anaphor - weighting schema
Since there is no semantic tagging for proper
names as yet, the relationship between pairs such
as Sa?o Carlos - a cidade [Sa?o Carlos - the city]
could not be found. Regarding wrong antecedents,
we have seen that some semantic relationships are
weaker, having no semantic tags in common, for
instance: a proposta - o aumento [the proposal -
the rise]. In some cases the antecedent is not a
previous noun phrase but a whole sentence, para-
graph or disjoint parts of the text. As we con-
sider only relations holding between noun phrases,
these cases could not be resolved. Finally, there
are cases of plain heuristic failure. For instance,
establishing a relationship between os professores
78
[the teachers], with the semantic tags H and Hprof,
and os politicos [the politicians], with the seman-
tic tags H and Hprof, when the correct antecedent
was os docentes [the docents], with the semantic
tags HH (group of humans) and Hprof.
4 Final Remarks
Previous work on nominal anaphor resolution has
used lexical knowledge in different ways. (Poe-
sio et al, 1997) presented results concerning the
resolution of bridging definitions, using the Word-
Net (Fellbaum, 1998), where bridging DDs en-
close our Indirect and Associative anaphora. Poe-
sio et al reported 35% of recall for synonymy,
56% for hypernymy and 38% for meronymy.
(Schulte im Walde, 1997) evaluated the bridg-
ing cases presented in (Poesio et al, 1997), on
the basis of lexical acquisition from the British
National Corpus. She reported a recall of 33%
for synonymy, 15% for hypernymy and 18% for
meronymy. (Poesio et al, 2002) considering syn-
tactic patterns for lexical knowledge acquisition,
obtained better results for resolving meronymy
(66% of recall). (Gasperin and Vieira, 2004)
tested the use of word similarity lists on resolv-
ing indirect anaphora, reporting 33% of recall.
(Markert and Nissim, 2005) presented two ways
(WordNet and Web) of obtaining lexical knowl-
edge for antecedent selection in coreferent DDs
(Direct and Indirect anaphora). Markert and
Nissim achieved 71% of recall using Web-based
method and 65% of recall using WordNet-based
method. We can say that our results are very sat-
isfactory, considering the related work. Note that
usually evaluation of bridging anaphora is made
on the basis of a limited number of cases, because
the data is sparse. Our study was based on 133
examples, which is not much but surpasses some
of the previous related work. Mainly, our results
indicate that the semantic tagging provided by the
parser is a good resource for dealing with the prob-
lem, if compared to other lexical resources such as
WordNet and acquired similarity lists. We believe
that the results will improve significantly once se-
mantic tags for proper names are provided by the
parser. This evaluation is planned as future work.
Acknowledgments
This work was partially funded by CNPq.
References
Eckhard Bick. 2000. The Parsing System PALAVRAS:
Automatic Grammatical Analysis of Protuguese in
a Constraint Grammar Framework. Ph.D. thesis,
Arhus University, Arhus.
Eckhard Bick. 2003. Multi-level ner for portuguese in
a cg framework. In Nuno J. et al Mamede, editor,
Computational Processing of the Portuguese Lan-
guage (Procedings of the 6th International Work-
shop, PROPOR 2003), number 2721 in Lecture
Notes in Computer Science, pages 118?125, Faro,
Portugal. Springer.
Razvan Bunescu. 2003. Associative anaphora reso-
lution: A web-based approach. In Proceedings of
the Workshop on The Computational Treatment of
Anaphora - EACL 2003, Budapest.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Caroline Gasperin and Renata Vieira. 2004. Us-
ing word similarity lists for resolving indirect
anaphora. In Proceedings of ACL Workshop on Ref-
erence Resolution and its Applications, pages 40?
46, Barcelona.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?401.
Massimo Poesio, Renata Vieira, and Simone Teufel.
1997. Resolving bridging descriptions in un-
restricted texts. In Proceedings of the Work-
shop on Operational Factors In Practical, Robust,
Anaphora Resolution for Unrestricted Texts, pages
1?6, Madrid.
Masimo Poesio, Ishikawa Tomonori, Sabine Shulte im
Walde, and Renata Vieira. 2002. Acquiring lexical
knowledge for anaphora resolution. In Proceedings
of 3rd Language resources and evaluation confer-
ence LREC 2002, Las Palmas.
Eleanor Rosch. 1978. Principles of categorization.
In E. Rosch and B. Lloyd, editors, Cognition and
Categorization, pages 27?48. Hillsdale, New Jersey:
Lawrence Erlbaum Associate.
Sabine Schulte im Walde. 1997. Resolving Bridging
Descriptions in High-Dimensional Space Resolving
Bridging Descriptions in High-Dimensional Space.
Ph.D. thesis, Institut fu?r Maschinelle Sprachverar-
beitung, Universita?t Stuttgart, and Center for Cogni-
tive Science, University of Edinburgh, Edinburgh.
79
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 171?175, New York City, June 2006. c?2006 Association for Computational Linguistics
LingPars, a Linguistically Inspired, Language-Independent Machine 
Learner for Dependency Treebanks
Eckhard Bick
Institute of Language and Communication
University of Southern Denmark
5230 Odense  M, Denmark
eckhard.bick@mail.dk
Abstract
This paper presents a Constraint Grammar-
inspired machine learner and parser,  Ling?
Pars, that assigns dependencies to morpho?
logically annotated treebanks in a function-
centred way. The system not only bases at?
tachment probabilities for PoS, case, mood, 
lemma on those features' function probabili?
ties, but also uses topological features like 
function/PoS  n-grams,  barrier  tags  and 
daughter-sequences.  In  the  CoNLL shared 
task, performance was below average on at?
tachment  scores,  but  a  relatively  higher 
score for  function tags/deprels  in  isolation 
suggests that the system's strengths were not 
fully exploited in the current architecture.
1 Introduction
This paper describes LingPars, a Constraint Gram?
mar-inspired language-independent treebank-learn?
er developed from scratch between January 9th and 
March  9th 2006  in  the  context  of  the  CoNLL-X 
2006 shared task (http://nextens.uvt.nl/~conll/), or?
ganized by Sabine Buchholz,  Erwin Marsi,  Yval 
Krymolowski and Amit Dubey. Training treebanks 
and test data were provided for  13 different  lan?
guages: Arabic (Smr? et al 2002), Chinese (Chen 
et  al.  2003),  Czech  (Haji?  et  al.  2001), Danish 
(Kromann 2003), Dutch (van der Beek et al 2002), 
German (Brants et.al 2002), Japanese (Kawata and 
Bartels), Portuguese (Afonso et al 2002), Slovene 
(D?erosky  et  al.  2006),  Spanish  (Palomar  et  al. 
2004),  Swedish  (Nilsson  et  al.  2005),  Turkish 
(Oflazer et al 2003 and Nart et.al 2003), Bulgarian 
(Simov et al 2005). A number of these treebanks 
were not originally annotated in dependency style, 
but transformed from constituent tree style for the 
task, and all differ widely in terms of tag granulari?
ty (21-302  part-of-speech tags, 7-82 function la?
bels). Also, not all treebanks included morphologi?
cal  information,  and  only  half  offered  a  lemma 
field.  Such  descriptive  variation  proved  to  be  a 
considerable  constraint  for  our  parser  design,  as 
will  be  explained  in  chapter  2.  No  external  re?
sources and no structural preprocessing were used1.
2 Language  independence  versus  theory 
independence
While  manual  annotation  and/or  linguistic,  rule-
based parsers are necessary for the creation of its 
training data, only a machine learning based parser 
(as targeted in the CoNNL shared task) can hope to 
be  truly language independent  in  its  design.  The 
question is, however, if this necessarily implies in?
dependence of linguistic/descriptive theory.
In our own approach, LingPars, we thus depart?
ed from the Constraint Grammar descriptive model 
(Karlsson  et  al.  2005),  where  syntactic  function 
tags (called DEPREL or dependency relations  in 
the shared task) rank higher than dependency/con?
stituency and are  established  before head attach?
ments, rather than vice versa (as would be the case 
for many probabilistic, chunker based systems, or 
1The only exception is what we consider a problem in the dependency-version 
of the German TIGER treebank, where postnominal attributes of nouns appear 
as dependents of that noun's head if the latter is a preposition, but not otherwise 
(e.g. if the head's head is a preposition). LingPars  failed to learn this somewhat 
idiosyncratic distinction, but performance improved when  the analysis was pre?
processed with an additional np-layer (to be re-flattened after parsing.).
171
the classical PENN treebank descriptive model). In 
our hand-written,  rule based parsers,  dependency 
treebanks are  constructed by using sequential  at?
tachment rules, generally attaching functions (e.g. 
subject, object, postnominal) to forms (finite verb, 
noun) or lexical tags (tense, auxiliary, transitive), 
with  a  direction  condition  and  the  possibility  of 
added target,  context  or  barrier  conditions  (Bick 
2005).
In LingPars, we tried to mimic this methodology 
by trying to learn probabilities for both CG style 
syntactic-function  contexts  and  function-to-form 
attachment rules.  We could not,  however, imple?
ment the straightforward idea of learning probabili?
ties and optimal ordering for an existing body of 
(manual) seeding rules,  because the 13 treebanks 
were not harmonized in their tag sets and descrip?
tive conventions2.
As  an  example,  imagine  a  linguistic  rule  that 
triggers  "subclause-hood"  for  a  verb-headed  de?
pendency-node as soon as a subordinator attaches 
to  it,  and  then,  implementing  "subclause-hood", 
tries to attach the verb not to the root, but to anoth?
er verb left of the subordinator, or right to a root-
attaching verb. For the given set of treebanks prob?
abilities and ordering priorities for this rule cannot 
be learned by one and the same parser, simply be?
cause some treebanks attach the verb to the subor?
dinator rather than vice versa, and for verb chains, 
there is no descriptive consensus as to whether the 
auxiliary/construction  verb  (e.g.  Spanish)  or  the 
main verb (e.g. Swedish) is regarded as head.
3 System architecture
The point of departure for pattern learning in Ling?
Pars  were  the  fine-grained  part  of  speech  (PoS) 
tags (POSTAG) and the LEMMA tag.  For  those 
languages that did not provide a lemma tag, lower-
cased  word  form was  used  instead.  Also,  where 
available from the FEATS field and not already in?
tegrated into the PoS tag, the following informa?
tion was integrated into the PoS tag:
a) case, which was regarded as a good predictor 
for function, as well as a good dependency-indica?
tor for e.g. preposition- and adnominal attachment
b) mood/finiteness, in order to predict subordina?
tion and verb chaining, especially in the absence of 
2 Neither was there time (and for some languages: reading knowledge) to write 
the necessary converters to and from a normalized standard formalism for each 
treebank.
auxiliary class information in the FEATS field
c) pronoun subclass, in order to predict adnomi?
nal vs. independent function as well as subordinat?
ing function (relatives and interrogatives)
A few treebanks did not classify subordinating 
words  as  conjunctions,  relatives,  interrogatives 
etc., but lumped them into the general adverb and 
pronoun classes. Danish is a case in point - here, 
the treebank classified all non-inflecting words as 
PoS 'U'3. Our solution, implemented only for Dan?
ish and Swedish, was to introduce a list of struc?
ture-words, that would get their PoS appended with 
an '-S', enabling the  learner to distinguish between 
e.g. "ordinary" ADV, and "structural" ADV-S.
3.1 The parser
In a first round, our parser calculates a preference 
list of functions and dependencies for each word, 
examining all possible mother-daughter pairs and 
n-grams in the sentence (or paragraph). Next, de?
pendencies  are  adjusted  for  function,  basically 
summing up the  frequency-,  distance- and direc?
tion-calibrated function?PoS attachment probabil?
ities  for  all  contextually  allowed  functions  for  a 
given word. Finally, dependency probabilities are 
weighted  using  linked  probabilities  for  possible 
mother-, daughter- and sister-tags in a second pass.
The result are 2 arrays, one for possible daugh?
ter?mother  pairs,  one  for  word:function  pairs. 
Values in both arrays are normalized to the 0..1 in?
terval, meaning that for instance even an originally 
low probability, long distance attachment will get 
high values after normalization if there are few or 
no competing alternatives for the word in question.
LingPars  then  attempts  to  "effectuate"  the  de?
pendency (daughter?mother) array, starting with 
the - in normalized terms - highest value4.  If  the 
daughter candidate is as yet unattached, and the de?
pendency does not produce circularities or crossing 
branches, the corresponding part of the (ordered) 
word:function array is calibrated for the suggested 
dependency, and the top-ranking function chosen.
In principle,  one pass through the  dependency 
array would suffice to parse a sentence. However, 
3For the treebank as such, no information is lost, since it will be recoverable 
from the function tag. In a training situation, however, there is much less to train 
on than in a treebank with a more syntactic definition of PoS.
4 Though we prefer to think of attachments as bottom-up choices, the value-or?
dered approach is essentially neither bottom-up nor top-down, depending on the 
language and the  salience of relations in a sentence, all runs had a great varia?
tion in the order of attachments. A middle-level attachment like case-based 
preposition-attachment, for instance, can easily outperform (low) article- or 
(high) top-node-attachment.
172
due to linguistic constraints like uniqueness princi?
ple, barrier tags and "full" heads5, some words may 
be  left  unattached  or  create  conflicts  for  their 
heads. In these cases, weights are reduced for the 
conflicting functions, and increased for all daugh?
ter?mother  values  of  the  unattached  word.  The 
value arrays are then recomputed and rerun. In the 
case of unattached words, a complete rerun is per?
formed, allowing problematic words to attach be?
fore  those  words  that  would  otherwise  have 
blocked them. In the case of a function (e.g subject 
uniqueness)  conflict,  only  the  words  involved  in 
the conflict are rerun. If no conflict-free solution is 
found after 19 runs, barrier-, uniqueness- and pro?
jectivity-constraints are relaxed for a last run6.
Finally,  the  daughter-sequence  for  each  head 
(with the head itself  inserted) is  checked against 
the  probability  of  its  function  sequence  (learned 
not  from n-grams  proper,  but  from  daughter-se?
quences in the training corpus). For instance, the 
constituents of a clause would make up such a se?
quence and allow to correct a sequence like SUBJ 
VFIN  ARG2  ARG1  into  SUBJ  VFIN  ARG1 
ARG2, where ARG1 and ARG2 are object func?
tions  with  a  preferred  order  (for  the  language 
learned) of ARG1 ARG2.
3.2 Learning functions (deprels)
LingPars  computes  function  probabilities  (Vf, 
function value) at three levels: First, each lemma 
and PoS is assigned local (context-free) probabili?
ties for all possible functions. Second, the proba?
bility of  a  given function occurring at  a  specific 
place  in  a  function  n-gram (func-gram,  example 
(a))  is  calculated (with n between 2 and 6).  The 
learner only used endocentric func-grams, marking 
which  of  the  function  positions  had  their  head 
within the func-gram. If no funcgram supported a 
given function, its probability for the word in ques?
tion was set to zero. At the third level, for each en?
docentric n-gram of word classes (PoS), the proba?
bility for a given function occurring at a given po?
sition  in  the  n-gram (position  2  in  example  (b)) 
was computed. Here, only the longest possible n-
grams were used by the parser, and first and last 
positions of the n-gram were used only to provide 
context, not to assign function probabilities.
5Head types with a limited maximum number of dependents (usually, one)
6In the rare case of still missing heads or functions, these are computed using 
probabilities for a simplified set of word classes (mostly the CPOSTAG), or - as 
a last resort - set to ROOT-attachment.
(a)>N?2 SUBJ?4 <N?2 AUX MV?4 ACC?5
(b) art?2 n:SUBJ?4 adj?2 v-fin v-inf?4 n?5
3.3 Learning dependencies
In a rule based Constraint Grammar system, depen?
dency would be expressed as attachment of func?
tions to forms (i.e. subject to verb, or modifier to 
adjective).  However,  with  empty  deprel  fields, 
LingPars cannot use functions directly, only their 
probabilities. Therefore, in a first pass, it computes 
the probability for the whole possible attachment 
matrix for a sentence, using learned mother- and 
daughter-normalized  frequencies  for  attachments 
of  type  (a)  PoS?PoS,  (b)  PoS?Lex,  (c) 
Lex?PoS and (d) Lex?Lex, taking into account 
also  the  learned  directional  and  distance  prefer?
ences. Each matrix cell is then filled with a value 
Vfa ("function attachment value") - the sum of the 
individual normalized probabilities of all possible 
functions  for  that  particular  daughter  given  that 
particular  mother  multiplied  with  the  preestab?
lished,  attachment-independent  Vf  value  for  that 
token-function combination.
Inspired by the BARRIER conditions in CG rule 
contexts, our learner also records the frequency of 
those PoS and those functions (deprels) that may 
appear between a dependent of PoS A and a head 
of PoS B. The parser then regards all  other,  non-
registered interfering PoS or functions as blocking 
tokens for a given attachment pair, reducing its at?
tachment value by a factor of 1/100.
In a second pass, the attachment matrix is cali?
brated  using  the  relative  probabilities  for  depen?
dent daughters, dependent sisters and head mother 
given. This way, probabilities of object and object 
complement  sisters  will  enhance  each  other,  and 
given the fact that treebanks differ as to which ele?
ment of a verb chain arguments attach to, a verbal 
head  can  be  treated  differently  depending  on 
whether it has a high probability for another verb 
(with auxiliary,  modal  or  main verb function) as 
mother or daughter or not.
Finally, like for functions, n-grams are used to 
calculate attachment probabilities. For each endo?
centric PoS n-gram (of length 6 or less), the proba?
bilities  of  all  treebank-supported  PoS:function 
chains and their dependency arcs are learned, and 
the value for an attachment word pair occurring in 
the chain will be corrected using both the chain/n-
gram probability and the Vf value for the function 
173
associated  with  the  dependent  in  that  particular 
chain. For contextual reasons, arcs central to the n-
gram are weighted higher than peripheral arcs.7
3.4 Non-projectivity and other language-spe?
cific problems
As a general rule, non-projective arcs were only al?
lowed if no other, projective head could be found 
for a given word. However, linguistic knowledge 
suggests that non-projective arcs should be particu?
larly likely in  connection with verb-chain-depen?
dencies,  where subjects  attach to  the  finite  verb, 
but objects to the non-finite verb, which can create 
crossing arcs in the case of object fronting, chain 
inversion  etc.  Since  we  also  noted  an  error-risk 
from arguments getting attached to the closest verb 
in  a  chain  rather  than  the  linguistically  correct 
one8, we chose to introduce systematic, after-parse 
raising of certain pre-defined arguments from the 
auxiliary to the main verb. This feature needs lan?
guage-dependent parameters, and time constraints 
only allowed the implementation for Danish, Span?
ish, Portuguese and Czech. For Dutch, we also dis?
covered word-class-related projectivity-errors, that 
could be  remedied by exempting certain  FEATS 
classes from the parser's general projectivity con?
straint altogether (prep-voor and V-hulp)9.
In  order  to  improve  root  accuracy,  topnode 
probability was set to zero for verbs with a safe 
subordinator dependent. However, even those tree?
banks descriptively supporting this did not all PoS-
mark  subordinators.  Therefore,  FEATS-informa?
tion was used, or as a last resort - for Danish and 
Swedish  - word forms.
A  third  language-specific  error-source  was 
punctuation, because some treebanks (cz, sl, es) al?
lowed punctuation as heads. Also, experiments for 
the Germanic and Romance languages showed that 
performance decreased when punctuation was al?
lowed as BARRIER, but increased, when a fine-
grained punctuation PoS10 was included in function 
and dependency n-grams.
7Due to BARRIER constraints, or simply because of insufficient training data in 
the face of a very detailed tag set, it may be impossible to assign all words n-
gram supported functions or dependencies. In the former case, local function 
probabilities are used, in the latter attachment is computed as function ? PoS 
probability only, using the most likely function.
8 Single verbs being more frequent than verb chains, the learner tended to gener?
alize close attachment, and even (grand)daughter and (grand)mother conditions 
could not entirely remedy this problem.
9Though desirable, there was no time to implement this for other languages.
10 Only for Spanish and Swedish was there a subdivision of punctuation PoS, so 
we had to supply  this information in all other cases by adding token-informa?
tion to the POSTAG field.
4 Evaluation
Because of LingPars' strong focus on function tags, 
a separate analysis of attachment versus label per?
formance was thought to be of interest. Ill. 1 plots 
the latter (Y-axis) against the former (X-axis), with 
dot size symbolizing treebank size. In this evalua?
tion, a fixed training chunk size of 50,000 tokens11 
was used, and tested on a different sample of 5,000 
tokens (see also 5/50 evaluation in ill. 2). For most 
languages,  function  performance  was  better  than 
attachment performance (3.2 percentage points on 
average,  as opposed to 0.44 for  the CoNLL sys?
tems overall), with dots above the hyphenated "di?
agonal of balance". Interestingly, the graphics also 
makes  it  clear  that  performance  was  lower  for 
small treebanks, despite the fact that training cor?
pus size had been limited in the experiment, possi?
bly indicating correlated differences in the balance 
between tag set size and treebank size.
Illustration 1: Attachment accuracy 
(x-axis) vs. label accuracy (y-axis)
Ill.  2 keeps the information from ill. 1 (5/50-dep 
and 5/50-func), represented in the two lower lines, 
but adds performance for maximal training corpus 
size12 with  (a)  a  randomly  chosen  test  chunk  of 
5,000 tokens  not included in  the  training corpus 
(5/all-5)  and (b)  a  20,000 token chunk  from the 
training corpus (20/all). Languages were sorted ac?
11Smaller for Slovene and Arabic (for these languages: largest possible)
12Due to deadline time constraints, an upper limit of 400,000 lines was forced on 
the biggest treebanks, when training for unknown test data,  meaning that only ? 
of the German data and 1/3 of the Czech data could be used.
174
cording  to  20/all-func  accuracy.  As  can  be  seen 
from  the  dips  in  the  remaining  (lower)  curves, 
small training corpora (asterisk-marked languages) 
made it difficult for the parser (1) to match 20/all 
attachment performance on unknown data, and (2) 
to  learn  labels/functions  in  general  (dips  in  all 
function curves, even 20/all).  For the larger tree?
banks, the parser performed better (1-3 percentage 
points) for the full training set than for the 50,000 
token training set.
Illustration 2: Performance with different training cor?
pus sizes (upper 2 curves: Test data included)
5 Outlook
We have  shown that  a  probabilistic  dependency 
parser can be built on CG-inspired linguistic prin?
ciples with a strong focus on function and tag se?
quences. Given the time constraint and the fact that 
the learner had to be built from scratch, its perfor?
mance would encourage further research. In partic?
ular, a systematic parameter/performance analysis13 
should be performed for the individual languages. 
In the long term, a notational harmonization of the 
treebanks  should  allow  the  learner  to  be  seeded 
with existing hand-written dependency rules.
References 
Afonso, S., E. Bick, R. Haber and D. Santos. Floresta Sint?(c)tica: A treebank of Portuguese. In   Proceed?ings of LREC'02. pp. 1698-1703 . Paris: ELRA
van der Beek, L. G. Bouma, R. Malouf, G. van Noord. 2002. The Alpino Dependency Treebank. In: Compu?tational  Linguistics  in  the  Netherlands  CLIN 2001. 
13Parameters like uniqueness and directedness are already learned by the system 
(through  probability thresholds), while others, like function weights, structural 
word classes and frequency thresholds for barriers and lexeme n-grams are used 
now, but with a fixed value for all languages.
pp. 8-22. Rodopi
Bick, Eckhard. 2005. Turning Constraint Grammar Data into  Running  Dependency  Treebanks.  In:  Civit, Montserrat & K?bler, Sandra & Mart?, Ma. Ant?nia (ed.), Proceedings of TLT 2005, Barcelona. pp.19-2
Brants, S., S. Dipper, S. Hansen, W. Lezius, G. Smith. 2002. The TIGER Treebank. Proc. of TLT1, Sozopol
D?erosky,  S.,  T.  Erjavec,  N.  Ledinek,  P.  Pajas,  Z. ?abokrtsky, A.  ?ele.  2006. Towards a Slovene De?pendency Treebank. In Proc. of LREC'06, Genoa
Haji?, J., B. Hladk?, and P. Pajas. 2001. The Prague De?pendency Treebank:  Annotation Structure  and Sup?port.  In  Proc.  of  the IRCS Workshop on Linguistic  Databases, pp. 105-114. University of Pennsylvania.
Karlsson,  Fred, Atro Vouitilainen, Jukka Heikkil?  and A. Anttila. 1995. Constraint Grammar - A Language-Independent  System  for  Parsing  Unrestricted  Text. Mouton de Gruyter: Berlin.
Kawata,  Y.  and  J.  Bartels.  2000.  Stylebook  for  the Japanese  Treebank  in  VERBMOBIL.  Universit?t T?bingen: Verbmobil-Report 240.
Chen, Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, Chi-Ching Luo, Ming-Chung Chang, Chao-Jan Chen, and Zhao-Ming Gao. 2003. Sinica Treebank: Design Cri?teria, Representational Issues and Implementation. In A.  Abeille  (ed.)  Treebanks  Building  and  Using Parsed Corpora. Dordrecht:Kluwer, pp231-248.
Kromann, M. T. 2003. The Danish Dependency Tree?bank. In J. Nivre and E. Hinrichs (ed.) Proceedings of  TLT2003. V?xj? University Press, Sweden
Nart,  B.  Atalay,  Kemal  Oflazr,  Bilge  Say.  2003.  The Annotation Process in the Turkish Treebank. In Pro?ceedings of the EACL Workshop on Linguistically In?terpreted Corpora - LINC 2003. Budapest
Nilsson, J, J. Hall and J. Nivre. 2005. MAMBA Meets TIGER:  Reconstructing  a  Swedish  Treebank  from Antiquity. In Proceedings NODALIDA 2005. Joenssu
Oflazer,  K.,  B. Say, D.Z. Hakkani-T?r,  G. T?r.  2003. Building  a  Turkish  Treebank.  In  A.  Abeill?  (ed.) Building and Exploiting Syntactically-annotated Cor?pora. Kluwer
Palomar, M. et. al. 2004. Construcci?n de una base de datos de ?rboles sint?ctico-sem?nticos para el catal?n, euskera y castellano. In:  Proceedings of SEPLN XX, pp 81-88. Barcelona: ISSN 1135-5948
Simov, K., P. Osenova, A. Simov, M. Kouylekov. 2004. Design and Implementation of the Bulgarian HPSG-based Treebank. In E. Hinrichs and K. Simov (ed.), Journal of Research on Language and Computation,  Vol. 2, No. 4 , pp. 495-522. Kluwer
Smr?, Otakar,  Jan ?naidauf,  and Petr  Zem?nek.  2002. Prague Dependency Treebank for Arabic: Multi-Lev?el Annotation of Arabic corpus. In Proceedings of the  International  Symposium  on  Processing  of  Arabic, pages 147-155, Manouba, Tunisia, April 2002.
cz de pt bu se nl tu 
*
ar 
*
sl  
*
da ja es 
*
zh
65
67,5
70
72,5
75
77,5
80
82,5
85
87,5
90
92,5
95
97,5
5/50 dep
5/50 func
20/all dep
20/all func
5/all-5 dep
5/all-5 func
175
