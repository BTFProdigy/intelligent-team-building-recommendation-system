Proceedings of the Linguistic Annotation Workshop, pages 125?131,
Prague, June 2007. c?2007 Association for Computational Linguistics
Building Chinese Sense Annotated Corpus  
with the Help of Software Tools 
Yunfang Wu 
School of Electronic Engineering and 
Computer Science, Peking University, 
Beijing 100871 
wuyf@pku.edu.cn 
Peng Jin 
School of Electronic Engineering and 
Computer Science, Peking University, 
Beijing 100871 
jandp@pku.edu.cn 
Tao Guo 
School of Electronic Engineering and 
Computer Science, Peking University, 
Beijing 100871 
gtwcq@pku.edu.cn 
Shiwen Yu 
School of Electronic Engineering and 
Computer Science, Peking University, 
Beijing 100871 
yusw@pku.edu.cn 
 
 
Abstract 
This paper presents the building procedure 
of a Chinese sense annotated corpus. A set 
of software tools is designed to help hu-
man annotator to accelerate the annotation 
speed and keep the consistency. The soft-
ware tools include 1) a tagger for word 
segmentation and POS tagging, 2) an an-
notating interface responsible for the sense 
describing in the lexicon and sense anno-
tating in the corpus, 3) a checker for con-
sistency keeping, 4) a transformer respon-
sible for the transforming from text file to 
XML format, and 5) a counter for sense 
frequency distribution calculating. 
1 Introduction 
There is a strong need for a large-scale Chinese 
corpus annotated with word senses both for word 
sense disambiguation (WSD) and linguistic re-
search. Although much research has been carried 
out, there is still a long way to go for WSD tech-
niques to meet the requirements of practical NLP 
programs such as machine translation and infor-
mation retrieval. It was argued that no fundamen-
tal progress in WSD could be made until large-
scale lexical resources were built (Veronis, 2003). 
In English a word sense annotated corpus SEM-
COR (Semantic Concordances) (Landes et al, 
1999) has been built, which was later trained and 
tested by many WSD systems and stimulated large 
amounts of WSD work. In Japanese the Hinoki 
Sensebank is constructed (Tanaka et al, 2006). In 
the field of Chinese corpus construction, plenty of 
attention has been paid to POS tagging and syn-
tactic structures bracketing, for instance the Penn 
Chinese Treebank (Xue et al, 2002) and Sinica 
Corpus (Huang et al, 1992), but very limited 
work has been done with semantic knowledge 
annotation. Huang et al (2004) introduced the 
Sinica sense-based lexical knowledge base, but as 
is well known, Chinese pervasive in Taiwan is not 
the same as mandarin Chinese. SENSEVAL-3 
provides a Chinese word sense annotated corpus, 
which contains 20 words and 15 sentences per 
meaning for most words, but obviously the data is 
too limited to achieve wide coverage, high accu-
racy WSD systems. 
This paper is devoted to building a large-scale 
Chinese corpus annotated with word senses. A 
small part of the Chinese sense annotated corpus 
has been adopted as one of the SemEval-2007 
tasks namely ?Multilingual Chinese-English Lexi-
cal Sample Task? This paper concentrates on the 
description of the manually annotating schemes 
125
with the help of software tools. The software tools 
will help human annotators mainly in the two as-
pects: 1) Reduce the labor time and accelerate the 
speed; 2) Keep the inter-annotator agreement. The 
overall procedure along with the software tools is 
illustrated in figure 1.
.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
            Preprocessing                                                                                 
Tagger: word segmentation and POS tagging
 
 
Annotating interface: word sense annotating
 
 
 
 
Checker: consistency checking 
 
 
 
 
Word sense annotated corpus
 
 
 
 
 
                                                                                                                                                      Postprocessing
Transformer: XML format transforming Counter: sense frequency distribution calculating
 
This paper is so organized as follows. In section 
2 the preprocessing stage (word segmentation and 
POS tagging) is discussed. Then in section 3 the 
annotating scheme and the annotating interface 
are demonstrated in detail. The strategy to keep 
consistency is addressed in section 4. And then in 
section 5 and 6 the two postprocessing stages are 
respectively presented. Finally in section 7 con-
clusions are drawn and future works are presented. 
2 Word segmentation and POS tagging 
The input data for word sense annotating is firstly 
word segmented and POS tagged using Peking 
University?s POS tagger (Yu et al, 2003). The 
POS tagging precision is up to 97.5%, which lays 
a sound foundation for researches on sense anno-
tating. This is actually to make use of the full-
fledged syntactic processing techniques to deal 
with the semantic annotation problems. Different 
senses of one ambiguous word sometimes behave 
so differently that they bear different POS tags. 
Take ???/hold? in sentence (1) as an example. 
The noun of ???/hold? means ?confidence?, but 
the verb means ?grasp?.  
(1) a ?(have)  ??/n(confidence)  
b ??/v(grasp)  ?(ZHU)  ??(chance) 
Due to the unique characteristic of Chinese lan-
guage that lacks word inflection, the ambiguous 
words with different POSs are very common. Ac-
cording to the research of Li (1999), after POS 
tagging the ratio of ambiguous word occurrences 
in the text of People?s Daily is reduced from 42% 
to 26%. Therefore the emphasis of manually sense 
annotating in this paper falls on the ambiguous 
words with the same part of speech. This will in 
turn save 16% of the annotation effort compared 
with the sense annotating before the preprocessing 
of POS tagging. 
Fig.1.The overall procedure along with the software tools 
3 Word sense annotating 
The resulting lexical knowledge base in this pro-
ject will contain three major components: 1) a 
corpus annotated with Chinese word senses 
namely Chinese Senses Pool (CSP); 2) a lexicon 
containing sense distinction and description 
namely Chinese Semantic Dictionary (CSD); 3) 
the linking between the CSD and the Chinese 
Concept Dictionary (CCD) (Liu et al, 2002). The 
corpus CSP, the lexicon CSD and CCD constitute 
a highly relational and tightly integrated system: 1) 
In CSD the sense distinctions are described rely-
ing on the corpus; 2) In CSP the word occurrences 
are assigned sense tags according to the sense en-
126
try specified in CSD; 3) The linking between the 
sense entry in CSD and CCD synsets are estab-
lished. The dynamic model is shown in figure 2. A 
software tool is developed in Java to be used as 
the word sense annotating interface (figure 3), 
which embodies the spirit of the dynamic model 
properly.
  
. 
 
 
 
 
 
 
 
 
             
 
 
 
 
 
 
 
      
 
 
 
 
 
 
 
3.1 Sense describing in the lexicon and sense 
annotating in the corpus 
In this project the lexicon CSD containing sense 
descriptions and the corpus CSP annotated with 
senses are built interactively, simultaneously and 
dynamically. On one hand, the sense distinctions in 
the lexicon are made relying heavily on the corpus 
usage. On the other hand, using the sense informa-
tion specified in the lexicon the human annotators 
assign semantic tags to all the instances of the 
word in a corpus.  
In the word sense annotating interface, the sen-
tences from CSP containing the target ambiguous 
words are displayed in the upper section, and the 
word senses with feature-based description from 
CSD are displayed in the bottom section. 
Through reading the context in the corpus, the 
human annotator decides to add or delete or edit a 
sense entry in the lexicon. The default value of the 
range of the context is within a sentence, and the 
surrounding characters in the left and right of the 
target word can be specified by the annotator. An-
notators can do four kinds of operations in CSD: 1) 
Add a sense entry and then fill in all the features; 2) 
Delete a sense entry along with all its feature de-
scription; 3) Edit a sense entry and change any of 
the features; 4) Select a sample sentence form the 
CSP and add it to the lexicon in the corresponding 
sense entry. 
        
 
interactive construction 
 
linking 
 
indirect relation  
Corpus 
CSP 
CCD Lexicon 
CSD 
Fig 2. The dynamic model between the CSP, CSD and CCD 
Fig3. The word sense annotating interface 
127
According to the sense specification in CSD the 
human annotator assigns semantic tags to the word 
occurrences in CSP. The operation is quite easy. 
When the annotator double clicks the appropriate 
sense entry in CSD the sense tag is automatically 
added to the target word.  
The notable feature in this word sense annotat-
ing interface is that it provides flexible searching 
schemes. 1) Search sequentially (forward or back-
ward) all the instances of an ambiguous words re-
gardless of the annotating state; 2) Search sequen-
tially (forward or backward) the already annotated 
instances; 3) Search sequentially (forward or back-
ward) the yet un-annotated instances and 4) Search 
the instances of a specific ambiguous word (the 
window named Find/Replace in figure3, and again 
is shown in figure 4 for clearness). 
The tool of Find/Replace is widely used in this 
project and has proven to be effective in annotating 
word senses. It allows the annotator to search for a 
specific word to finish tagging all its occurrences 
in the same period of time rather than move se-
quentially through the text. The consistency is 
more easily kept when the annotator manages 
many different instances of the same word than 
handle a few occurrences of many different words 
in a specific time frame, because the former 
method enables the annotator to establish an inte-
grative knowledge system about a specific word 
and its sense distinction. Also the tool of 
Find/Replace provides flexible searching schemes 
for a specific ambiguous word. For instance, 
search in the corpus with different directions (for-
ward/backward) and search with different annotat-
ing states (annotated/un-annotated/both). Using the 
tool the annotator can also replace some specific 
word occurrences in the corpus (often with special 
POS tags) with a sense tag, thus can finish annotat-
ing the corpus quickly and with a batch method. 
For instance the POS tag of ?vq? (means verb 
complement) often uniquely corresponds to a spe-
cific verb sense such as ??/vq??/vq!8?. 
There is the status bar in the bottom line of the 
word sense annotating interface, and there clearly 
show the annotating status: the total word occur-
rences, the serial number of the current processing 
instance and the number of the already annotated 
instances.  
 
 
Fig.4  The tool of Find/Replace 
 
3.2 Linking between CSD and CCD 
The feature-based description of word meanings in 
CSD describes mainly the syntagmatic information, 
such as the subcategory frames of verbs, the se-
mantic categories of the head noun of adjectives, 
but cannot include the paradigmatic relations. 
WordNet is a popular open resource and has been 
widely experimented in WSD researches. Chinese 
Concept Dictionary (CCD) is a WordNet-like Chi-
nese lexicon (Liu et al, 2002), which carries the 
main relations defined in WordNet and can be seen 
as a bilingual concept lexicon with the parallel 
Chinese-English concepts to be simultaneously 
included. So the linking between the sense entries 
in CSD and the synsets in CCD is tried to establish 
in this project. After the linking has been estab-
lished, the paradigmatic relations (such as hy-
pernym / hyponym, meronym / holonym) ex-
pressed in CCD can map automatically to the sense 
entry in CSD. What?s more, the many existing 
WSD approaches based on WordNet can be trained 
and tested on the Chinese sense tagged corpus. 
In the right section of the word sense annotating 
interface there displays the synset information 
from CCD. When coping with a specific ambigu-
ous word (such as ??/open?) in CSD, the linking 
between CSD and CCD is automatically estab-
lished with the word itself (??/open?) as the pri-
mary key. And then all the synsets of the word 
(??/open?) in CCD, along with the hypernyms of 
each sense (expressed by the first word in a synset), 
are displayed in the right section. A synset selec-
tion window (namely Set synsets) containing the 
offset numbers of the synsets then appears in the 
right section. The annotator clicks on the appropri-
ate box(es) before the corresponding offset number 
and then the offset number is automatically added 
128
to the feature ?CCD? in the currently selected 
sense entry in CSD. 
The linking is now done manually. Unfortu-
nately some of the ambiguous words existing in 
CSD are not included in CCD. This also provides a 
good way to improve the coverage and quality of 
CCD.  
4 Consistency Checking 
Consistency is always an important concern for 
hand-annotated corpus, and is even critical for the 
sense tagged corpus due to the subtle meanings to 
handle. A software tool namely Sense Consistency 
Checker is developed in the checking procedure. 
The checker extracts all the instances of a specific 
ambiguous word into a checking file with the for-
mat of the sense concordances (as shown in figure 
5 ). The checking file enables the checker to have a 
closer examination of how the senses are used and 
distributed, and to form a general view of how the 
sense distinctions are made. The inter-annotator in-
agreement thus can be reached quickly and cor-
rectly. As illustrated in figure 5, it is obviously an 
error to assign the same semantic tag to ??/drive 
??/car? and ???/meeting ?/held?. Simply as 
it is the checker greatly accelerates the checking 
speed and improve the consistency. 
 
 
Fig. 5. Some example sentences in the checking file of ??/open? 
 
Together five researchers took part in the anno-
tation, of which three are majored in linguistics 
and two are majored in computational linguistics. 
In this project the annotators are also checkers, 
who check other annotators? work. A text gener-
ally is first tagged by one annotator and then veri-
fied by two checkers. 
After the preprocessing of word segmentation 
and Pos tagging, the word sense annotating and 
the consistency checking, the Chinese word sense 
annotated corpus is constructed. And then other 
software tools are needed to do further processing 
in the sense annotated corpus. 
5 XML format transforming 
The original format of the Chinese sense anno-
tated corpus is in text file as shown in figure 6. In 
the text file the sign following ?/? denotes the 
POS tag, and the number following ?!? indicates 
the sense ID. The text file complies with the other 
language resources at the Institute of Computa-
tional Linguistics, Peking University, which pro-
vides a quite easy way to make full use of the ex-
isting resources and techniques at ICL/PKU when 
constructing the sense annotated corpus.  
At the same time in order to exchange and 
share information easily with other language re-
sources in the world, a software tool namely Text-
to-XML Transformer is developed to change the 
text to XML format (as shown in figure 7). In the 
XML file, the item ?pos? denotes the POS tag of 
the word, and the item ?senseid? denotes sense ID 
of the ambiguous word. 
Thus there are two kinds of format for the Chi-
nese sense annotated corpus, each of which has its 
advantages and can be adopted to meet different 
requirements in different situations. 
 
129
 
 
 
 
 
Fig. 6. The sense annotated corpus in text file 
??/a  ?/u  ??/vn  ?/vt!2  ??/b  ??/n  ?/p  ?/m  ?/q!1  ?/r2  ??/n  ??/n  ??/vi  ?/u  ??/d  ??/a  
?/u  ??/n  ?/w  ??/vn  ??/n  ??/d  ??/vt  ?/w  ??/t  ???/n  ?/r  ?/q  ??/vn  ??/n  ?/d  ?/vt!3  ?
/v  9000/m  ?/m  ?/q ?/w 
 
<head date="20000201" page="01" articleno="003" passageno="019"> 
<passage> 
????????????????????????????????????????????????? 
??? 9000?? 
</passage> 
<postagging> 
<word id="0" pos="a" senseid=""> 
<token>??</token> 
</word> 
<word id="1" pos="u" senseid=""> 
<token>?</token> 
</word> 
<word id="2" pos="vn" senseid=""> 
<token>??</token> 
</word> 
<word id="3" pos="vt" senseid="2"> 
<token>?</token> 
</word> 
??   ?? 
 
Fig. 7. The sense annotated corpus in XML format 
 
6 Sense frequency calculating 
Word sense frequency distribution in the real texts 
is a vital kind of information both for the algo-
rithms of word sense disambiguation and for the 
research on lexical semantics. In the postprocess-
ing stage a software tool namely Sense Frequency 
Counter is developed to make statistics on the 
sense frequency distribution. Quite valuable in-
formation can be acquired through the counter 
based on the sense annotated corpus: 1) The 
amount of all the instances of an ambiguous word; 
2) The number of the already annotated instances; 
3) The occurrence of each sense of an ambiguous 
word and 4) The sense frequency. Table 1 illus-
trates the sense frequency distribution of ambigu-
ous verb ??/open? in 10 day?s People?s Daily. 
7 Conclusions 
This paper describes the overall building proce-
dure of a Chinese sense annotated corpus. The 
corpus is firstly word segmented and POS tagging 
using Peking University?s tagger in the preproc-
essing stage. Then the lexicon Chinese Semantic 
Dictionary (CSD) containing sense descriptions 
and the corpus Chinese Senses Pool (CSP) anno-
tated with senses are built interactively, simulta-
neously and dynamically using the word sense 
annotating interface. At the same time the linking 
between the sense entries in CSD and the synsets 
in Chinese Concept Dictionary (CCD) are manu-
ally established. And then the Sense Consistency 
Checker is used to keep the inter-annotator 
agreement. Finally two software tools are devel-
oped to do further processing based on the sense 
annotated corpus. A software tool namely Text-to-
XML Transformer is developed to change the text 
to XML format, and the Sense Frequency Counter 
is developed to make statistics on the sense fre-
quency distribution. The annotation schemes and 
all the software tools have been experimented in 
building the SemEval-2007 task 5 ?Multilingual 
Chinese-English Lexical Sample Task?, and have 
proven to be effective. 
 
 
130
Table 1 the sense frequency distribution of ambiguous verb ??/open? 
Ambiguous verbs Sense ID Occurrences Frequency(%) 
? 8 30 32.26
? 4 13 13.98
? 6 12 12.90
? 7 8 8.60
? 0 6 6.45
? 1 6 6.45
? 9 4 4.30
? 12 4 4.30
? 11 3 3.23
? 2 3 3.23
? 10 3 3.23
? 14 1 1.08
? 15 0 0.00
? 3 0 0.00
? 5 0 0.00
? 13 0 0.00
Acknowledgments. This research is supported by 
Humanity and Social Science Research Project of 
China State Education Ministry (No. 06JC740001) 
and National Basic Research Program of China 
(No. 2004CB318102). 
References 
Huang, Ch. R and Chen, K. J. 1992. A Chinese Corpus 
for Linguistics Research. In Proceedings of COL-
ING-1992. 
Huang, Ch. R., Chen, Ch. L., Weng C. X. and Chen. K. 
J. 2004. The Sinica Sense Management System: De-
sign and Implementation. In Recent advancement in 
Chinese lexical semantics. 
Landes, S., Leacock, C. and Tengi, R. 1999. Building 
Semantic Concordances. In Christiane Fellbaum 
(Ed.) WordNet: an Electronic Lexical Database. 
MIT Press, Cambridge. 
Li, J. 1999. The research on Chinese word sense dis-
ambiguation. Doctoral dissertation in computer sci-
ence department of Tsinghua University. 
Liu, Y., Yu, S. W. and Yu, J.S. 2002. Building a Bilin-
gual WordNet-like Lexicon: the New Approach and 
Algorithms. In Proceedings of COLING 2002.  
Tanaka, T., Bond F. and Fujita, S. 2006. The Hinoki 
Sensebank----A large-scale word sense tagged cor-
pus of Japanese. In Proceedings of the Workshop on 
Frontiers in Linguistically Annotated Corpora 2006. 
Veronis, J. 2003. Sense Tagging: Does It Make Sense? 
In Wilson et al (Eds). Corpus Linguistics by the 
Rule: a Festschrift for Geoffrey Leech.  
Xue, N., Chiou, F. D. and Palmer, M. 2002. Building a 
Large-Scale Annotated Chinese Corpus. In Proceed-
ings of COLING 2002. 
Yu, S. W., Duan, H. M., Zhu, X. F., Swen, B. and 
Chang, B. B. 2003. Specification for Corpus Proc-
essing at Peking University: Word Segmentation, 
POS tagging and Phonetic Notation. Journal of Chi-
nese Language and Computing. 
 
131
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 19?23,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 5: Multilingual Chinese-English Lexical Sample 
Peng Jin, Yunfang Wu and Shiwen Yu 
Institute of Computational Linguistics  
 Peking University, Beijing China 
{jandp, wuyf, yusw}@pku.edu.cn 
 
 
Abstract 
The Multilingual Chinese-English lexical 
sample task at SemEval-2007 provides a 
framework to evaluate Chinese word sense 
disambiguation and to promote research. 
This paper reports on the task preparation 
and the results of six participants. 
1 Introduction 
The Multilingual Chinese-English lexical sample 
task is designed following the leading ideas of the 
Senseval-3 Multilingual English-Hindi lexical 
sample task (Chklovski et al, 2004). The ?sense 
tags? for the ambiguous Chinese target words are 
given in the form of their English translations. 
The data preparation is introduced in the second 
section. And then the participating systems are 
briefly described and their scores are listed.  
In the conclusions we bring forward some sug-
gestion for the next campaign. 
2 Chinese Word Sense Annotated Corpus 
All the training and test data come from the 
People?s Daily in January, February and March of 
2000. The People?s Daily is the most popular 
newspaper in China and is open domain. Before 
manually sense annotating, the texts have been 
word-segmented and part of speech (PoS) tagged 
according to the PoS tagging scheme of Institute of 
Computational Linguistics in Peking University 
(ICL/PKU). The corpus had been used as one of 
the gold-standard data set for the second 
international Chinese word segmentation bakeoff 
in 2005.1
2.1 Manual Annotation 
The sense annotated corpus is manually con-
structed with the help of a word sense annotating 
interface developed in Java. Three native annota-
tors, two major in Chinese linguistics and one ma-
jor in computer science took part in the construc-
tion of the sense-annotated corpus. A text generally 
is first annotated by one annotator and then veri-
fied by two checkers. Checking is of course a nec-
essary procedure to keep the consistency. Inspired 
by the observation that checking all the instances 
of a word in a specific time frame will greatly im-
prove the precision and accelerate the speed, a 
software tool is designed in Java to gather all the 
occurrences of a word in the corpus into a check-
ing file with the sense KWIC (Key Word in Con-
text) format in sense tags order. The inter-
annotator agreement gets to 84.8% according to 
Wu. et al (2006). 
The sense entries are specified in the Chinese 
Semantic Dictionary (CSD) developed by 
ICL/PKU. The sense distinctions are made mainly 
according to the Contemporary Chinese Dictionary, 
the most widely used dictionary in mandarin Chi-
nese, with necessary adjustment and improvement 
is implemented according to words usage in real 
texts. Word senses are described using the feature-
based formalism.  The features, which appear in 
the form ?Attribute =Value?, can incorporate ex-
tensive distributional information about a word 
sense. The feature set constitutes the representation 
of a sense, while the verbal definitions of meaning 
                                                 
1 http://sighan.cs.uchicago.edu/bakeoff2005/ 
19
serve only as references for human use. The Eng-
lish translation is assigned to each sense in the at-
tribute ?English translation? in CSD. 
Based on the sense-annotated corpus, a sense is 
replaced by its English translation, which might 
group different senses together under the same 
English word. 
2.2 Instances selection 
In this task together 40 Chinese ambiguous words: 
19 nouns and 21 verbs are selected for the evalua-
tion. Each sense of one word is provided at least 15 
instances and at most 40 instances, in which 
around 2/3 is used as the training data and 1/3 as 
the test data. Table 1 presents the number of words 
under each part of speech, the average number of 
senses for each PoS and the number of instances 
respectively in the training and test set. 
 
 # Average 
senses 
# training 
instances 
# test 
instances
19 
nouns 
2.58 1019 364 
21 
verbs 
3.57 1667 571 
 
Table 1: Summary of the sense inventory and 
number of training data and test set 
 
In order to escape from the sense-skewed distri-
bution that really exists in the corpus of People?s 
Daily, many instances of some senses have been 
removed from the sense annotated corpus. So the 
sense distribution of the ambiguous words in this 
task does not reflect the usages in real texts. 
3 Participating Systems 
In order to facilitate participators to select the fea-
tures, we gave a specification for the PoS-tag set. 
Both word-segmented and un-segmented context 
are provided. 
Two kinds of precisions are evaluated. One is 
micro-average: 
 
??
==
=
N
i
i
N
i
imir nmP
11
/  
 
N  is the number of all target word-types. is 
the number of labeled correctly to one specific tar-
get word-type and  is the number of all test in-
stances for this word-type. 
im
in
The other is macro-average: 
 
?
=
=
N
i
imar NpP
1
/ ,  iii nmp /=
 
All teams attempted all test instances. So the re-
call is the same with the precision. The precision 
baseline is obtained by the most frequent sense. 
Because the corpus is not reflected the real usage, 
the precision is very low. 
Six teams participated in this word sense disam-
biguation task. Four of them used supervised learn-
ing algorithms and two used un-supervised method. 
For each team two kinds of precision are given as 
in table 2.  
 
Team Micro-average Macro-average
SRCB-WSD 0.716578 0.749236 
I2R 0.712299 0.746824 
CITYU-HIF 0.710160 0.748761 
SWAT 0.657754 0.692487 
TorMd 0.375401 0.431243 
HIT 0.336898 0.395993 
baseline 0.4053 0.4618 
 
Table 2: The scores of all participating systems 
 
As follow the participating systems are briefly 
introduced. 
SRCB-WSD system exploited maximum entropy 
model as the classifier from OpenNLP2 The fol-
lowing features are used in this WSD system: 
 
? All the verbs and nouns in the context, that is, 
the words with tags ?n, nr, ns, nt, nz, v, vd, vn?  
? PoS of the left word and the right word 
?noun phrase, verb phrase, adjective phrase, 
time phrase, place phrase and quantity phrase. 
These phrases are considered as constituents of 
context, as well as words and punctuations which 
do not belong to any phrase.  
?the type of these phrases which are around the 
target phrases   
                                                 
2 http:// maxent.sourceforge.net/ 
20
? word category information comes from Chi-
nese thesaurus 
 
I2R system used a semi-supervised classification 
algorithm (label propagation algorithm) (Niu, et al, 
2005). They used three types of features: PoS of 
neighboring words with position information, un-
ordered single words in topical context, and local 
collocations.  
In the label propagation algorithm (LP) (Zhu 
and Ghahramani, 2002), label information of any 
vertex in a graph is propagated to nearby vertices 
through weighted edges until a global stable stage 
is achieved. Larger edge weights allow labels to 
travel through easier. Thus the closer the examples, 
the more likely they have similar labels (the global 
consistency assumption). In label propagation 
process, the soft label of each initial labeled exam-
ple is clamped in each iteration to replenish label 
sources from these labeled data. Thus the labeled 
data act like sources to push out labels through 
unlabeled data. With this push from labeled exam-
ples, the class boundaries will be pushed through 
edges with large weights and settle in gaps along 
edges with small weights. If the data structure fits 
the classification goal, then LP algorithm can use 
these unlabeled data to help learning classification 
plane. 
CITYU-HIF system was a fully supervised one 
based on a Na?ve Bayes classifier with simple fea-
ture selection for each target word.  The features 
used are as follows: 
 
? Local features at specified positions: 
PoS of word at w-2, w-1, w1, w2
Word at w-2, w-1, w1, w2
? Topical features within a given window: 
Content words appearing within w-10 to w10
? Syntactic features: 
PoS bi-gram at w-2w0 , w-1w0 , w0w1 , w0w2
PoS tri-gram at w-2 w-1w0 and w0w1w2
 
One characteristic of this system is the incorpo-
ration of the intrinsic nature of each target word in 
disambiguation. It is assumed that WSD is highly 
lexically sensitive and each word is best character-
ized by different lexical information. Human 
judged to consider for each target word the type of 
disambiguation information if they found useful.  
During disambiguation, they run two Na?ve Bayes 
classifiers, one on all features above, and the other 
only on the type of information deemed useful by 
the human judges. When the probability of the best 
guess from the former is under a certain threshold, 
the best guess from the latter was used instead.  
SWAT system uses a weighted vote from three 
different classifiers to make the prediction. The 
three systems are: a Na?ve Bayes classifier that 
compares similarities based on Bayes' Rule, a clas-
sifier that creates a decision list of context features, 
and a classifier that compares the angles between 
vectors of the features found most commonly with 
each sense. The features include bigrams, and tri-
grams, and unigrams are weighted by distance 
from the ambiguous word. 
TorMd used an unsupervised naive Bayes classi-
fier. They combine Chinese text and an English 
thesaurus to create a `Chinese word'--`English 
category' co-occurrence matrix. This system gener-
ated the prior-probabilities and likelihoods of a 
Na?ve Bayes word sense classifier not from sense-
annotated (in this case English translation anno-
tated) data, but from this word--category co-
occurrence matrix. They used the Macquarie The-
saurus as very coarse sense inventory. 
They asked a native speaker of Chinese to map 
the English translations of the target words to ap-
propriate thesaurus categories. Once the Na?ve 
Bayes classifier identifies a particular category as 
the intended sense, the mapping file is used to label 
the target word with the corresponding English 
translation. They rely simply on the bag of words 
that co-occur with the target word (window size of 
5 words on either side). 
HIT is a fully unsupervised WSD system, which 
puts bag of words of Chinese sentences and the 
English translations of target ambiguous word to 
search engine (Google and Baidu). Then they 
could get al kinds of statistic data. The correct 
translation was found through comparing their 
cross entropy. 
4 Conclusion 
The goal of this task is to create a framework to 
evaluate Chinese word sense disambiguation and 
to promote research. 
21
 
Scores Target 
Word 
Sen
se # 
Train
ing # 
Test 
# 
Base-
line SRCB
-WSD
I2R CITY
U-HIF
SWA
T-MP
TOR
MD 
HIT 
? 3 63 20 .50 .70 .80 .75 .75 .55 .55 
?? 3 73 27 .370 .778 .815 .741 .778 .481 .407 
? 4 69 23 .435 .696 .609 .696 .696 .174 .174 
? 9 222 77 .130 .506 .506 .481 .532 .169 .091 
? 8 197 67 .150 .567 .552 .537 .433 .119 .104 
? 4 58 20 .50 .60 .50 .55 .60 .30 .30 
?? 2 47 16 .625 .875 .875 .875 .563 .50 .438 
? 5 105 36 .278 .694 .667 .611 .889 .25 .139 
? 3 56 18 .50 .667 .722 .667 .667 .389 .333 
? 4 106 39 .256 .718 .615 .641 .538 .256 .256 
? 5 132 44 .227 .659 .75 .727 .568 .25 .114 
?? 2 56 20 .50 .90 .95 .95 .60 .50 .50 
? 4 103 34 .294 .765 .706 .765 .559 .294 .294 
?? 2 20 8 .50 .75 .75 .75 .625 .375 .50 
? 2 46 16 .625 .938 .813 .813 .875 .563 .438 
?? 2 60 18 .556 .667 .722 .778 .722 .444 .556 
? 2 40 14 .429 .571 .643 .571 .571 .143 .286 
?? 2 29 10 .60 .80 .70 .90 .80 .30 .30 
? 2 37 13 .769 .769 .769 .769 .769 .462 .462 
? 4 110 37 .270 .730 .676 .676 .541 .216 .216 
?? 2 38 14 .714 .930 1.0 .929 .786 .714 .571 
Ave. 3.5
7 
1667 571 .342/ 
.44 
.685/   
.728 
.676/   
.721 
.671/   
.723 
.618/   
.66 
.30/     
.355 
.263/   
.335 
 
 Table 3: Performance on verbs. Micro / macro average precisions are spitted by ?/? at the last row. 
 
Together six teams participate in this WSD task, 
four of them adopt supervised learning methods 
and two of them used unsupervised algorithms. All 
of the four supervised learning systems exceed ob-
viously the baseline obtained by the most frequent 
sense. It is noted that the performances of the first 
three systems are very close. Two unsupervised 
methods? scores are below the baseline. More 
unlabeled data maybe improve their performance.  
Although the SRCB-WSD system got the high-
est scores among the six participants, it does not 
perform always better than other system from table 
2 and table 3. But to each word, the four super-
vised systems always predict correctly more in-
stances than the two un-supervised systems.  
Besides the corpus, we provide a specification of 
the PoS tag set. Only SRCB-WSD system utilized 
this knowledge in feature selection. We will pro-
vide more instances in the next campaign. 
22
Scores Target 
Word 
Sen
se # 
Train
ing # 
Test 
# 
Base-
line SRCB
-WSD
I2R CITY
U-HIF
SWA
T-MP
TOR
MD 
HIT 
? 3 68 25 .40 .88 .84 .88 .76 .72 .32 
?? 2 53 18 .611 .611 .722 .722 .833 .556 .333 
? 2 56 19 .526 .842 .842 .684 .789 .474 .632 
?? 3 48 21 .476 .571 .591 .619 .619 .429 .619 
?? 2 50 17 .588 .824 .824 .824 .647 .706 .529 
? 3 53 18 .50 .778 .722 .778 .611 .50 .222 
?? 3 64 22 .455 .591 .591 .636 .545 .318 .364 
?? 2 60 20 .50 1.0 .95 1.0 1.0 .50 .50 
?? 2 38 14 .714 1.0 1.0 1.0 1.0 .643 .571 
?? 2 45 15 .533 .733 .733 .60 .467 .467 .467 
? 3 67 23 .435 .783 .783 .739 .696 .348 .696 
?? 2 44 17 .353 .529 .589 .588 .588 .353 .529 
?? 3 50 18 .556 .611 .611 .722 .722 .50 .111 
?? 2 39 14 .714 .929 .786 .714 .786 .857 .571 
?? 2 47 16 .625 .813 .813 .938 1.0 .438 .563 
?? 3 88 32 .313 .656 .563 .625 .656 .281 .344 
?? 3 65 25 .40 .88 1.0 .92 .60 .56 .44 
?? 2 41 14 .714 .786 .714 .786 .643 .714 .50 
?? 2 43 16 .625 .875 .938 1.0 .875 .438 .50 
Ave. 2.4
5 
1019 364 .506/ 
.528
.766/   
.773 
.761/ 
.769
.772/  
.778 
.72/     
.728 
.50/     
.516 
.456/   
.464 
  
Table 4: Performance on nouns. Micro / macro average precisions are spitted by ?/? at the last row. 
 
5 Acknowledgements 
This research is supported by Humanity and Social 
Science Research Project of China State Education 
Ministry (No. 06JC740001) and National Basic 
Research Program of China (No. 2004CB318102). 
We would like to thank Tao Guo and Yulai Pei 
for their hard work to guarantee the quality of the 
corpus. Huiming Duan provides us the corpus 
which has been word-segmented and PoS-tagged 
and gives some suggestions during the manual an-
notation. 
References 
Rada Mihalcea, Timothy Chklovski and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample 
task. Proceedings of SENSEVAL-3. 25-28. 
Timothy Chklovski, Rada Mihalcea, Ted Pedersen and 
Amruta Purandare. 2004. The Senseval-3 Multilin- 
 
 
gual English-Hindi lexical sample task. Proceedings of 
SENSEVAL-3. 5-8. 
Xiaojin Zhu, Zoubin Ghahramani. 2002. Learning from 
Labeled and Unlabeled Data with Label Propagation. 
CMU CALD tech report CMU-CALD-02-107. 
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen Yu. 
2006. A Chinese Corpus with Word Sense Annota-
tion. Proceedings of ICCPOL, Singapore, 414-421. 
Zhen-Yu Niu, Dong-Hong Ji and Chew-Lim Tan. 2005. 
Word Sense Disambiguation Using Label Propaga-
tion Based Semi Supervised Learning. Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics.395-402 
23
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 261?263,
Prague, June 2007. c?2007 Association for Computational Linguistics
PKU: Combining Supervised Classifiers with Features Selection 
Peng Jin, Danqing Zhu, *Fuxin Li and Yunfang Wu 
Institute of Computational Linguistics   
Peking University, Beijing, China 
*Institute of Automation Chinese Academy of Sciences  
Beijing, China 
{jandp,zhudanqing,wuyf}@pku.edu.cn *Fuxin.li@ia.ac.cn 
 
 
Abstract 
This paper presents the word sense disam-
biguation system of Peking University 
which was designed for the SemEval-2007 
competition. The system participated in the 
Web track of task 11 ?English Lexical 
Sample Task via English-Chinese Parallel 
Text?. The system is a hybrid model by 
combining two supervised learning algo-
rithms SVM and ME. And the method of 
entropy-based feature chosen was experi-
mented. We obtained precision (and recall) 
of 81.5%. 
1 Introduction 
The PKU system participated in the web track of 
task 11. In this task, the organizers propose an 
English lexical sample task for word sense disam-
biguation (WSD), where the sense-annotated ex-
amples are (semi)-automatically gathered from 
word-aligned English-Chinese parallel texts. After 
assigning appropriate Chinese translations to each 
sense of an English word, the English side of the 
parallel texts can then serve as the training data, as 
they are considered to have been disambiguated 
and "sense-annotated" by the appropriate Chinese 
translations. This proposed task is thus similar to 
the multilingual lexical sample task in Senseval3, 
except that the training and test examples are col-
lected without manually annotating each individual 
ambiguous word occurrence. 
The system consists of two supervised learning 
classifiers, support vector machines (SVM) and 
maximum entropy (ME). A method of entropy-
based feature chosen was experimented to reduce 
the feature dimensions. The training data was lim-
ited to the labeled data provided by the task, and a 
PoS-tagger (tree-tagger) was used to get more fea-
tures. 
2 Features Selection 
We used tree-tagger to PoS-tag the texts before the 
feature extractor. No other resource is used in the 
system. The window size of the context is set to 5 
around the ambiguous word. Only the following 
features are used in the system:      
 
      Local words  
Local PoSs 
      Bag-of-words 
Local collocations 
 
Here local collocation means any two words 
which fall into the context window to form collo-
cation pair.  
Two methods are used to reduce the dimensions 
of feature space. One comes from the linguistic 
knowledge, some words whose PoSs are IN, DT, 
SYM, POS, CC or ?``? are not included as the fea-
tures. 
The second method is based on entropy. To each 
word, the training data was split to two parts for 
parameter estimation. One (usually consist of 30 ? 
50 instances) as the simultaneous test and the rest 
instances form the other part. 
First the entropy of each feature was calculated. 
For example, the target word ?work?, it has two 
senses and the dimensions of its feature space is N. 
For feature , if it appears in m instances belong-
ing to sense A and n instances in sense B. So the 
if
261
probability distributions are:
nm
mp +=1   and 
nm
np +=2 . The entropy of  is: if
 
?
=
=
2
1
1log)(
j j
ji p
pfH   
 
We rank all the features according to their en-
tropy from small to big. And then first percent 
lambda features are chosen as the final feature set. 
Using this smaller feature set, we use the classifier 
to make a new prediction. 
The parameter ? is estimated by comparing the 
system performance on the simultaneous test. In 
our system, .68 is chosen. It means that 68% origi-
nal features used to form the new feature space. 
The same classifier was tried on different feature 
sets to get different outputs and then were com-
bined. 
3 Classifiers 
The Support Vector Machines (SVM) are a group 
of supervised learning methods that can be applied 
to classification or regression. It is developed by 
Vapnik and has been applied into WSD (Lee et al, 
2004). Since most of the target words have more 
than two senses, we used the implementation of 
SVM that includes lib-svm (Chang and Lin, 2001) 
and svm-multiclass (Joachims, 2004). To lib-svm, 
the parameter of ?b? which is used to obtain prob-
ability information after training is set 0 or 1 indi-
vidually to form different classifiers. The default 
linear kernel is used.  
Each vector dimension represents a feature. The 
numerical value of a vector entry is the numerical 
value of the corresponding feature. In our system, 
we use binary features. If the context of an instance 
has a particular feature, then the feature value is set 
to 1, otherwise the value is set to 0. 
ME modeling provides a framework for inte-
grating information for classification from many 
heterogeneous information sources. The intuition 
behind the maximum entropy principle is: given a 
set of training data, model what is known and as-
sume no further knowledge about the unknown by 
assigning them equal probability (entropy is 
maximum). There are also some researchers using 
ME to WSD (Chao and Dyer, 2002). Dekang Lin?s 
implementation of ME was used. He used General-
ized Iterative Scaling (GIS) algorithm. 
4 Development 
Because of time constraints, we could not experi-
ment all the training data by cross-validation. To 
each target word, we extract first 50 training in-
stances as the test.  
 
Lib-svm 
Prob. Output
Target 
Word 
Svm- 
Multi-
class 
ME 
Orig. 
F.S. 
Red.
FS 
Non- 
prob. 
Output
Age .68 .70 .70 .70 .66 
Area .80 .70 .80 .74 .82 
Body .84 .84 .90 .92 .16 
Change .48 .42 .66 .42 .58 
Director .96 .94 .96 .96 .96 
Experience .90 .88 .88 .90 .88 
Future .94 .94 .94 .98 .94 
interest .84 .82 .82 .88 .84 
issue .88 .88 .84 .90 .88 
Life .92 .94 .98 1.0 .94 
Material .88 .92 .94 .94 .88 
Need .86 .86 .86 .86 .86 
performance .78 .82 .80 .82 .80 
Program .70 .74 .72 .72 .72 
Report .94 .94 .94 .94 .94 
System .76 .70 .76 .76 .70 
Time .70 .64 .68 .60 .76 
today .72 .70 .74 .68 .76 
Water .90 .92 .88 .82 .90 
Work .90 .86 .90 .92 .90 
 
Table 1: The Performance on Nouns 
 
For some adjectives, we just extract first 30 be-
cause the training data is small. For ten of adjec-
tives, the training data is too small, we directly use 
the lib-svm (with probability output) as the final 
classifier.  
Both SVM and ME could output the probability 
for each instance to each class. So we try to com-
bine them to improve the performance. Several 
methods of combining classifiers have been inves-
tigated (Radu et al, 2002). The enhanced Counted-
based Voting (CBV) and Rank-Based Voting, 
Probability Mixture Model, and best single Classi-
fier are experimented in the training data. Table 1 
and Table 2 indicate the results of nouns and adjec-
tives individually, which were achieved with each 
of the different methods. In these tables, "Orig 
F.S." and "Red. F.S." mean original feature set and 
reduced feature set. "Prob. output" and "Non Prob. 
262
output" are two implementation of lib-svm. The 
former output the probability of each instance be-
longing to each class, otherwise the latter not. Dif-
ferent from the results of Radu, choosing the best 
single classifier get the better performance than 
any kinds of combination. In this paper, we did not 
list the performances of combining.  
According to Table 1 and Table 2, the particular 
classifier chosen for that word was the one with the 
highest score in the training data. 
 
Lib-svm 
Prob. Output 
Target 
Word 
Svm- 
Multi- 
class 
ME 
Orig. 
F.S. 
Red.
F.S. 
Non- 
prob. 
output
Early .77 .80 .77 .80 .77 
Educational .87 .87 .87 .83 .87 
Free .74 .80 .84 .90 82 
Human .96 .92 .96 .90 .96 
Long .70 .70 .73 .87 .70 
Major .78 .78 .78 .80 .78 
Medical .76 .86 .78 .84 .78 
New .73 .77 .63 .43 .63 
Simple .73 .77 .77 .77 .80 
Third .98 .94 .98 1.0 .96 
 
Table 2:  The performance on Adjectives 
 
Two parameters are different from these two 
SVMs. One is the ?-c?, which is the tradeoff be-
tween training error and margin. In lib-svm the 
value of ?-c? is set 1; but in svm-multiclass is 0.01. 
The other is the strategy of how to utility binary-
classification to resolve multi-class. In svm-
multiclass, no strategy is needed since the algo-
rithm in (Crammer and Singer, 2001) solves the 
multi-class problem directly. In lib-svm, we use 
the one-against-all approach which is the default in 
lib-svm. Down-sampling is used if some result is 
trivial classification. The reason is that the unbal-
anced distribution of training data. We compared 
selecting support vectors and down-sampling. The 
latter is better. 
5 Results 
We participated in the subtask of SemEval-2007 
English lexical sample task via English-Chinese 
parallel text. The organizers make use of English-
Chinese documents gathered from the URL pairs 
given by the STRAND Bilingual Databases. They 
used this corpus for the evaluation of 40 English 
words (20 nouns and 20 adjectives). 
Our system gives exactly one sense for each test 
example. So the recall is always the same as preci-
sion. Micro-average precision is 81.5%. According 
to the task organizers, the recall of the best partici-
pating in this subtask is 81.9%. So the performance 
of our system compares favorably with the best 
participating system. 
6 Acknowledgements 
This research is supported by Humanity and Social 
Science Research Project of China State Education 
Ministry (No. 06JC740001) and National Basic 
Research Program of China (No. 2004CB318102). 
We are indebted to Helmut Schmid, IMS, Uni-
versity of Stuttgart, for making Tree-Tagger avail-
able free of charge. 
Finally, the authors thank the organizers Hwee 
Tou Ng and Yee Seng Chan, for their hard work to 
collect the training and test data. 
References 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM : 
a library for support vector machines. 
www.csie.ntu.edu.tw/~cjlin/libsvm
Gerald Chao and Michael G. Dyer. 2002. Maximum 
entropy models for word sense disambiguation. Pro-
ceedings of the 19th international conference on 
Computational linguistics.Vol (1):1-7 
Koby Crammer and Yoram Singer. 2001. On the Algo-
rithmic Implementation of Multiclass Kernel-based 
Vector Machines. Journal of Machine Learning Re-
search, 2, 265-292 
Radu Florian, Silviu Cucerzan, Charles Schafer and 
David Yarowsky. 2002. Combining Classifiers for 
Word Sense Disambiguation. Natural Language En-
gineering, 8(4): 327 ? 341. 
Thorsten Joachims. SVM-Multiclass. 
http://svmlight.joachims.org/svm-
multiclass.html,2004. 
Yoong Keok Lee, Hwee Tou Ng and Tee Kiah Chia, 
Supervised Word Sense Disambiguation with Sup-
port Vector Machines and Multiple Knowledge 
Sources. Proceedings of SENSEVAL-3. 137 - 140 
263
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1191?1199,
Beijing, August 2010
Disambiguating Dynamic Sentiment Ambiguous Adjectives
Yunfang Wu
Key Laboratory of Computational
Linguistics (Peking University),
Ministry of EducationRI China
wuyf@pku.edu.cn
Miaomiao Wen?
Department of Electrical Engineering and
Information Systems,
University of Tokyo
wenmiaomiao98@gmail.com
?Most of the work was performed when the author was a student at Peking University.
Abstract
Dynamic sentiment ambiguous
adjectives (DSAAs) like ?large, small,
high, low? pose a challenging task on
sentiment analysis. This paper proposes a
knowledge-based method to
automatically determine the semantic
orientation of DSAAs within context.
The task is reduced to sentiment
classification of target nouns, which we
refer to sentiment expectation instead of
semantic orientation widely used in
previous researches. We mine the Web
using lexico-syntactic patterns to infer
sentiment expectation of nouns, and then
exploit character-sentiment model to
reduce noises caused by the Web data.
At sentence level, our method achieves
promising result with an f-score of
78.52% that is substantially better than
baselines. At document level, our
method outperforms previous work in
sentiment classification of product
reviews.
1 Introduction
In recent years, sentiment analysis has attracted
considerable attention in the NLP community. It
is the task of mining positive and negative
opinions from natural language, which can be
applied to many research fields. Previous work
on this problem falls into three groups: opinion
mining of documents, sentiment classification of
sentences and polarity prediction of words.
Sentiment analysis both at document and
sentence level rely heavily on word level.
The most frequently explored task at the word
level is to determine the polarity of words, in
which most work centers on assigning a prior
polarity to words or word senses in the lexicon
out of context. However, for some words, the
polarity varies strongly with context, making it
hard to attach each to a fixed sentiment category
in the lexicon. For example, the word ?low?has
a positive orientation in ?low cost? but a
negative orientation in ?low salary?. We call
these words like ?low? dynamic sentiment
ambiguous adjectives (DSAAs). Turney and
Littman (2003) claim that DSAAs cannot be
avoided in a real-world application. But
unfortunately, DSAAs are discarded by most
research concerning sentiment analysis.
In this paper, we are devoted to the
challenging task of disambiguating DSAAs. The
task is to automatically determine the semantic
orientation (SO) of DSAAs within context. We
limit our work to 14 frequently used adjectives
in Chinese, such as ?large, small, many, few,
high, low?, which all have the meaning of
measurement. Although the number of such
ambiguous adjectives is not large, they are
frequently used in real text, especially in the
texts expressing opinions and emotions. As
demonstrated by the experimental results in this
paper, the disambiguation of 14 DSAAs can
obviously improve the performance of sentiment
classification of product reviews.
The task of disambiguating DSAAs is reduced
to sentiment classification of nouns. Previous
studies classify nouns into three categories:
positive, negative and neutral. In contrast, we
propose two categories of sentiment expectation
1191
of nouns: positive expectation and negative
expectation. This paper presents a novel
approach to automatically predict sentiment
expectation of nouns. First, we infer the
sentiment expectation of a noun by mining the
Web with strongly-polar-steering lexico-
syntactic patterns. Secondly, we derive the
sentiment expectation of a noun from its
component characters, which capture the
semantic relationship between Chinese words
and characters. Finally, a better performance is
obtained by combing the two methods. We
conduct two types of experiments: the
experimental results at the sentence level
validate the effectiveness of our approach; the
experimental results at the document level
confirm the significance of the problem we
addressed.
2 Related Work
2.1 Word-level Sentiment Analysis
Recently there has been extensive research in
sentiment analysis, for which Pang and Lee
(2008) give an in-depth survey of literature.
Closer to our study is the large body of work on
automatic SO prediction of words
(Hatzivassiloglou and McKeown, 1997; Turney
and Littman, 2003; Kim and Hovy, 2004;
Andreevskaia and Bergler, 2006), but
unfortunately they all discard DSAAs in their
research. In recent years, some studies go a step
further, attaching SO to senses instead of word
forms (Esuli and Sebastiani, 2006; Wiebe and
Mihalcea, 2006; Su and Markert 2008), but their
work is still limited in lexicon out of context.
The most relevant work is Ding et al (2008),
in which DSAAs are named as context
dependant opinions. They argue that there is no
way to know the SO of DSAAs without prior
knowledge, and asking a domain expert to
provider such knowledge is scalable. They adopt
a holistic lexicon-based approach to solve this
problem, which exploits external information
and evidences in other sentences and other
reviews. On the contrary in this paper, we obtain
the prior knowledge of a product by mining the
web, and then use such knowledge to determine
the SO of DSAAs. The prior knowledge of a
product, which is closer to the sentiment
expectation of nouns described in this paper, is
an important research issue in itself and has
many applications in sentiment analysis, as
discussed in section 3.2.
2.2 Phrase-level Sentiment Analysis
The disambiguation of DSAAs can also be
considered as a problem of phrase-level
sentiment analysis. Wilson et al (2004) present a
two-step process to recognize contextual polarity
that employs machine learning and a variety of
features. Takamura et al (2006, 2007) propose
latent variable model and lexical network to
determine SO of phrases, focusing on
?noun+adjective? pairs. Their experimental
results suggest that the classification of pairs
containing ambiguous adjectives is much harder
than those with unambiguous adjectives. The
above mentioned approaches are all supervised,
and need human labeled data for training. In
contrast, our method is unsupervised and can
overcome the data acquisition bottleneck.
Moreover, we focus on the much harder task of
disambiguating DSAAs in ?noun+adjective?
pairs.
2.3 Pattern-based Method
Previous studies have applied pattern-based
method to sentiment analysis (Riloff and Wiebe,
2003; Wiebe et al, 2004; Riloff et al, 2005;
Wiebe and Mihalcea, 2006; Andreevskaia and
Berger; 2006). The differences with our method
lie in two aspects: the used resources (corpus
versus web) and the research target (subjective
expressions versus sentiment expectation).
2.4 Character-based Method
Chinese characters carry semantic information
that is indicative of semantic properties of words.
Previous studies have exploited the character-
based model to predict the semantic categories
of Chinese unknown words (Chen, 2004; Lu,
2007). Yuen et al (2004) presents a method to
infer the SO of a Chinese word from its
statistical association with strong-polarized
characters rather than with strong-polarized
words. The work by Ku et al (2006) is similar to
ours because they also define the sentiment score
of a word by its composite characters. However,
their algorithm is based only on frequency, while
we exploit point mutual information that can
capture the character-sentiment association.
1192
3 Determining SO of Adjective by
Target Noun
3.1 Classification of DSAAs
The frequently used DSAAs are given below.
We group them into two categories: positive-like
adjectives and negative-like adjectives. These
adjectives are neutral out of context, but positive
or negative emotion will be evoked when they
co-occur with some target nouns, making it hard
to assign each to a fixed sentiment category in
lexicon.
(1) Positive-like adjectives (Pa) = {? da|large,
? duo|many,? gao|high,? hou|thick,?
shen|deep,? zhong|heavy,?? ju-da|huge,?
? zhong-da|great}
(2) Negative-like adjectives (Na) ={?
xiao|small,? shao |few, ? di|low, ? bao|thin,
? qian|shallow, ? qing|light}
3.2 Sentiment Expectation of Noun
The SO of most DSAAs can be determined by
target nouns in noun-adjective phrases, as shown
in Table 1. For example, the word ?high?has a
positive orientation when the target noun is
?salary? but a negative orientation when the
target noun is ?price?. Therefore, the task can be
reduced to sentiment classification of nouns.
Positive
? ? ? |potential is great
???|salary is high
Negative
? ? ? |potential is small
??? |salary is low
Negative
? ? ? |pressure is big
?? ?|price is high
Positive
? ? ? |pressure is small
?? ? |price is low
Table 1: The SO of DSAAs in noun-adjective phrases
In previous research, the SO of nouns is
classified into three categories: positive,
negative and neutral. Accordingly, ??? ya-
li|pressure?will be assigned as negative and ??
? qian-li|potential? as positive, while ???
gong-zi|salary? and ??? jia-ge|price? will be
assigned as neutral, as the two terms are
objective and cannot evoke positive or negative
emotion. Different from the traditional
classification scheme, we propose sentiment
expectation and classify nouns into two
categories: positive expectation and negative
expectation. For a positive expectation noun,
people usually expect the thing referred to by the
noun to be bigger, higher or happen frequently.
On the contrary, for a negative expectation noun,
people usually expect the thing referred to by the
noun to be smaller, lower or don?t happen . For
example, ?? ? jia-ge|price? is a negative
expectation noun, as most people in most cases
expect that the product prices become low,
whereas ??? gong-zi|salary? is a positive
expectation noun, as most people in most cases
expect that their salaries become high. The
relationship between traditional SO and
sentiment expectation can be defined as: positive
(negative) terms correspond to positive (negative)
expectation terms, but some neutral terms may
also carry positive (negative) expectation.
Su and Markert (2008) argue that polarity can
also be attached to objective words. The
difference with our scheme is that, for example,
?? ? jia-ge|price? is attached to negative
expectation in our scheme while is still neutral in
Su and Markert?s method.
The distinction between positive and negative
expectation nouns is vital to determine the SO of
some phrases. Using it to disambiguate DSAAs
is a good example. Another application is the
phrase containing verbs with the meaning of
status change. For example, ??????|salary
has been raised? will evoke positive emotion,
while ?????? jiage-shangzhang-le|prices
have gone up?will evoke negative emotion. As
far as we are aware, this is the first sentiment
analysis scheme that tries to exploit people?s
expectation towards nouns.
3.3 Determination of DSAAs
The SO of DSAAs in a given phrase can be
calculated by Eq. (1).
1 if a is positive-like
C(a) =
-1 if a is negative-like
???
1 if n is positive expectation
C(n) =
-1 if n is negative expectation
???
SO(a)=C(a)*C(n)
If adverb=?? bu|not?, SO(a)= -SO(a)
Where C(a) denotes the category of DSAAs; C(n)
denotes the sentiment expectation of nouns;
SO(a) is the SO of DSAAs in a give noun-
adjective phrase. When the adverb is the
negation term ?? bu|not?, the SO is reversed.
(1)
1193
4 Predicting Sentiment Expectation of
Noun
4.1 Pattern-based Prediction Using a Web
Search Engine
In natural language, there are some lexico-
syntactic patterns that people frequently use
when they express their opinion about something.
For example:
(3) ? ?? ? ? | Salary is a little low.
(4) ? ? ? ? ?| Price is a little high.
The pattern ?<n> ?? <a>? carries a strong
negative association in Chinese language. When
a man is saying ??????| Salary is a little
low?, it indicates that he wishes his ???
|salary? to be raised. On the contrary, when a
man is saying ?????? | price is a little
high?, it indicates that he wishes ??? |price?
to go down. As a result, ??? |salary? has
positive expectation while ??? |price? has
negative expectation.
With the rapid development and expansion of
the internet, Web has become an important
medium for people to post their ideas. The
opinions expressed on the Web reflect the
common cognition shared by collection of
people in a culture. Therefore, using a Web
search engine with the strong-polar-steering
lexico-syntactic patterns as queries, we can infer
the sentiment expectation of a noun, by
calculating its statistical association with
positive and negative hits.
As an example, using the search engine
Baidu 2 with the pattern ?<n> ?? <a>? as
queries, we obtain the following hits:
(5) ? ?? ? ? | Salary is a little low. (2890 hits)
? ?? ? ? | Salary is a little high (67 hits)
(6) ? ? ? ? ? | Price is a little high. (19400 hits)
? ? ? ? ? | Price is a little low. (1080 hits)
The more than 40 times more numerous hits for
?????? |Salary is a little low?indicate that
that ???|salary?is a positive expectation noun.
For the same reason, we can infer that ???
|price?has negative expectation.
DSAAs are classified into two opposite sets
Pa and Na, as listed in (1) and (2) respectively.
2 http://baidu.com.cn.
Here two-character adjectives (??? |huge?and
??? |great?) are discarded. Four types of
lexico-syntactic patterns, which are also
classified into two opposite sets in consistent
with Pa and Na, are used in this paper, as listed
in Table 2. These patterns were manually
designed, inspired by linguistic knowledge and
after a deep investigation on the Web.
Pos. expectation patterns Neg. expectation patterns
1) <n>?? Na
n is a little Na
2) <n>??? Na
n is a little Na
3) <n> Na, ???
n is Na, what should we
do?
4)? <n> Na
n is too Na
1) <n>?? Pa
n is a little Pa
2) <n>??? Pa
n is a little Pa
3) <n> Pa, ???
n is Pa, what should we
do?
4)? <n> Pa
n is too Pa
Table 2: The lexico-syntactic patterns
Here the noun (n) in these patterns was
instantiated by 9,468 nouns in our collected data.
A noun has together 48 patterns, 24 positive and
24 negative ones. For each noun, we obtain the
hits of both positive and negative expectation
patterns, using the search engine Baidu. The
sentiment expectation of a noun is acquired by
Eq. (2) and Eq. (3), where the magnitude of
_ ( )PT SO n can be considered as the strength of
sentiment expectation.
4
1
4
1
_ ( ) ( , )
( , )
i
b Na i
i
a Pa i
PT SO n PositivePatternHit n b
NegativePatternHit n a
? ?
? ?
?
?
??
??
positive expectation if _ ( )>0
n is negative expectation if _ ( )<0
not predicted if _ ( )=0
PT SO n
PT SO n
PT SO n
???
??
(3)
Table 3 gives some nouns with sentiment
expectation predicted by the pattern-based
method, descending (the left column) and
ascending (the right column) by the absolute
value of _ ( )PT SO n . Most words (9 out of 10)
are correctly predicted, demonstrating that the
result of pattern-based method is promising. The
only wrong predicted noun is ??? |feeling?,
due to the fact that most instances of it on the
Web data are used as verb rather than noun, like
??????| I think it is large?.
(2)
1194
Positive expectation Negative expectation
Noun ( _ ( )PT SO n ) Noun ( _ ( )PT SO n )
?|money (31349) ? ? |temperature(-111576)
??|wage (26311 ) ? ?|noise (-45790)
? ?|feeling (20102) ? ? |price (-25653)
? ? |income(19429) ? ? |cost (-22051)
? |officer (10630) ?? |blood pressure (-21788)
Table 3: Examples of nouns with sentiment
expectation predicted by the pattern-based method
4.2 Character-based Derivation Using
Sentiment Lexicons
But the sentiment expectation of some nouns
cannot be predicted with the pattern-based
method, mainly due to the reason that these
nouns don?t occur in the listed patterns in Table
2. An alternate way is to exploit the semantic
knowledge of Chinese characters. It is assumed
that there is a strong association between the
sentiment category of a word and its component
characters. For example, the three words ???
zui?e|evil, ? ? zuixing|crime, ? ?
zuiguo|fault?, which all contain the character ??
zui|sin? that carries negative meaning, are all
negative expectation nouns.
First, we compute the character-word
sentiment association by the following PMI
formula, based on a sentiment lexicon:
( , )
, log
( ) ( )
P c Positive
PMI c Positive
P c P Positive
? ?=
( , )
, log
( ) ( )
P c Negative
PMI c Negative
P c P Negative
? ?=
( ) ( , ) ( , )SO c PMI c Positive PMI c Negative? ?
Where ( , )P c Positive is the probability of a
character c in the positive category; ( )P c is the
probability of a character c in the sentiment
lexicon; ( )P Positive is the probability of the
positive category in the sentiment lexicon.
,PMI c Negative? ? has the similar meaning.
Probabilities are estimated according to the
maximum likelihood principle.
The open language resources for Chinese
sentiment analysis are quite limited. We selected
the following two sentiment lexicons.
Sentiment HowNet. HowNet has published
the Chinese vocabulary for sentiment analysis3,
3 http://www.keenage.com/html/c_index.html.
which was manually constructed. The positive
category contains 4,566 words and the negative
category contains 4,370 words.
Sentiment BaiduHit. In our collected data,
we extracted 9,468 nouns. Using the pattern-
based method we acquired sentiment expectation
of these nouns, where 2,530 ones were assigned
as positive expectation, 1,837 ones as negative
expectation and 5,101 ones were not predicted. It
is assumed that most nouns are correctly
predicted. These nouns with their sentiment
expectation constitute the lexicon of Sentiment
BaiduHit, which is automatically constructed.
Combining HowNet and BaiduHit. Most
sentiment characters derived from HowNet have
adjective property, since most words in
Sentiment HowNet are adjectives. On the
contrary, most sentiment characters derived from
BaiduHit have noun property. Therefore, the
combination of the two lexicons can cover more
characters. As Sentiment HowNet is manually
compiled, the sentiment characters derived from
it should be more reasonable than those from
BaiduHit. When combining the two lexicons in
computing character polarity, we assign a high
priority to HowNet. Only when a character is out
of vocabulary in HowNet, we resort to BaiduHit.
Then, we acquire the sentiment category of a
word by computing the following equation. Let a
word consist of n characters 1 2, nw c c c? ?... ,
the sentiment category of the word is calculated
by the average sentiment value of its component
characters:
1
1_ ( ) ( )
n
i
i
CH SO w SO c
n ?
? ? (5)
positive expectation if _ ( )>0
w is negative expectation if _ ( )<0
neutral if _ ( )=0
CH SO w
CH SO w
CH SO w
???
??
(6)
We acquired sentiment expectation of 9,468
nouns in our collected data, based on Sentiment
HowNet, Sentiment BaiduHit, and the
combination of the two lexicons, respectively.
Table 6 gives examples of nouns with
sentiment expectation acquired by the character-
based method combining the two lexicons of
HowNet and BaiduHit, descending (the left
column) and ascending (the right column) by the
absolute value of _ ( )CH SO w .
(4)
1195
Positive expectation Negative expectation
Noun( _ ( )CH SO w ) Noun( _ ( )CH SO w )
? ? |good name (3.23) ? |ash (-3.22)
? ? |health (3.06) ? |gross (-2.93)
?|fragrance (3.05) ? |tax (-2.89)
? ? |U.S.A (2.98) ? ? |fault (-2.84)
? ? |title (2.64) ? |poison (-2.82)
Table 4: Example of nouns with sentiment
expectation predicted by the character-based method
4.3 Integrating Pattern-based Prediction
and Character-based Derivation
The two methods of pattern-based prediction
and character-based derivation have
complementary properties. The pattern-based
method concentrates on a word?s usage on the
Web, whereas the character-based method
focuses on the internal structure of a word. So
the two methods can be integrated to get better
performance. The results using pattern-based
method are much better than character-based
method, as illustrated in Table 3 and Table 4. So
in the integrated scheme, we give a high priority
to pattern-based method. The pattern-based
approach is mainly used, and only when the
value of | _ ( ) |PT SO n is smaller than a threshold
r, the character-based method is adopted.
Because when the value of | _ ( ) |PT SO n is very
small, it could be caused by random noises on
the Web. We set r to 9 according to empirical
analysis in the development data.
5 Experiments
5.1 Sentiment Analysis at Sentence Level
5.1.1 Data
We collected data from two sources. The main
part was extracted from Xinhua News Agency of
Chinese Gigaword (Second Edition) released by
LDC. The texts were automatically word-
segmented and POS-tagged using the open
software ICTCLAS4. In order to concentrate on
the disambiguation of DSAAs, and reduce the
noise introduced by the parser, we extracted
sentences containing strings in pattern of (7),
where the target noun is modified by the
adjective in most cases.
4 http://www.ictclas.org/.
(7) noun+adverb+adjective (adjective?DSAAs)
e.g. ??/n ?/d ?/a | the cost is low.
Another small part of data was extracted from
the Web. Using the search engine Google5, we
searched the queries as in (8):
(8) ? | very+ adjective (adjective?DSAAs )
From the returned snippets, we manually picked
out some sentences that contain the strings of (7).
Also, the sentences were automatically word-
segmented and POS-tagged using ICTCLAS.
DSAAs in the data were assigned as positive,
negative or neutral, independently by two
annotators. Since we focus on the distinction
between positive and negative categories, the
neutral instances were removed. Table 5 gives
statistics of the data, and the inter-annotator
agreement is in a high level with a kappa of 0.91.
After cases with disagreement were negotiated
between the two annotators, a gold standard
annotation was agreed upon. In this paper, 3066
instances were divided randomly into three parts,
1/3 of which were used as the development data,
and 2/3 were the test data.
Most of the data has been used as the
benchmark dataset of SemEval-2010 task 18
?disambiguating sentiment ambiguous
adjectives?(Wu and Jin, 2010), and so it can be
downloaded freely for research.
Table 5: The statistics of DSAAs data
5.1.2 Baseline
We conducted two types of baseline.
Simple Baseline. Not considering the context,
assign all positive-like adjectives as positive, and
all negative-like adjectives as negative.
HowNet Baseline. Acquiring SO of nouns
from Sentiment HowNet, the polarity of DSAAs
is computed by Eq. (1).
5.1.3 Methods
Pattern-based method. Acquiring sentiment
expectation of nouns using the pattern-based
method, the polarity of DSAAs is computed by
Eq.(1).
5 http://www.google.com/.
Pos# Neg# Total#
Pos# 1280 58 1338
Neg# 72 1666 1738
Total# 1352 1724 3066
1196
Character-based method. Acquiring
sentiment expectation of nouns using the
character-based method, based on Sentiment
HowNet, Sentiment BaiduHit and the
combination of the two lexicons respectively, the
polarity of DSAAs is computed by Eq.(1).
Integrated method. Acquiring sentiment
expectation of nouns by integrating pattern-
based and character-based methods, the polarity
of DSAAs is computed by Eq. (1).
5.1.4 Results
Table 6 gives the experimental results at
sentence level with different methods.
Methods Pre. Rec. F
Simple Baseline 61.20 61.20 61.20
HowNet Baseline 97.58 9.88 17.94
Pattern-based 75.83 71.67 73.69
Character-based (HowNet) 69.89 69.37 69.63
Character-based (BaiduHit) 68.66 68.59 68.62
Character-based (Combined) 71.01 70.94 70.97
Integrated method 78.52 78.52 78.52
Table 6: The experimental results at sentence level
As for the simple baseline, both the precision
and recall are low, suggesting that DSAAs
cannot be neglected for sentiment analysis in a
real-world application.
The HowNet baseline achieves a quite high
precision of 97.58%, but a rather poor recall of
9.88%, suggesting that SO of nouns described in
traditional sentiment lexicon, like HowNet,
cannot effectively disambiguate DSAAs.
The proposed methods in this paper all yield
results that are substantially better than two
types of baseline. The pattern-based method, as
straightforward as it is, achieves promising result
with an f-score of 73.69%, which is 12.49%
higher than the simple baseline. The pattern-
based method outperforms the character-based
method (combined) by 4.82% in precision and
0.73% in recall. The performance of the
character-based method based on Sentiment
BaiduHit is competitive with that based on
Sentiment HowNet, which again proves the
effectiveness of the pattern-based method. The
character-based method combining the two
lexicons outperforms each lexicon with small
improvement. The approach integrating pattern-
based and character-based methods outperforms
each method in isolation, achieving an f-score of
78.52% that is 17.32% higher than the simple
baseline and 60.58% higher than HowNet
baseline.
5.2 Sentiment Analysis at Document Level
5.2.1 Data
We also investigated the impact of
disambiguating DSAAs on the sentiment
classification of product reviews. Following the
work of Wan (2008), we selected the same
dataset. The dataset contains 886 Chinese
product reviews, which are manually annotated
with polarity labels: positive or negative. Also,
the files are automatically word-segmented and
POS-tagged using ICTCLAS. We extracted the
files that contain the following strings, where the
nouns are modified by DSAAs in most cases.
(9) noun+adjective (adjective?DSAAs)
noun+adverb+adjective
noun+adverb+adverb+adjective.
We obtained 212 files, up to 24% of the overall
data, suggesting again that DSAAs are
frequently used in product reviews and cannot be
avoided in a real-world application.
5.2.2 Methods
Our goal is not to propose a new method, but
instead to test the performance gain by adding
the disambiguation of DSAAs. We adopted the
same algorithm with Wan (2008), and also used
Sentiment-HowNet. But in our experiment,
Negation_Dic contains only one term ??
bu|not?, for the sake of repeatable experiments.
The baseline algorithm is illustrated by the
non-italic part in Figure 1, where we set the
same parameters with Wan?s approach:
PosValue=1, NegValue=-2, q=2, ?=2.
We added the disambiguation of DSAAs to
the algorithm, as illustrated by the italic part in
Figure 1. When a word is a DSAA, compute its
SO with the proposed integrated method, rather
than using its prior polarity specified in HowNet.
For Dy_PosValue and Dy_NegValue, we first set
Dy_PosValue=1 and Dy_NegValue=-2, just the
same as PosValue and NegValue. In the second
attempt, in order to further intensify the polarity
of DSAAs, we set Dy_PosValue=1.5 and
Dy_NegValue=-2.5. Other parameters were set
the same as baseline.
1197
Algorithm Compute_SO:
1. Tokenize document d into sentence set S, and each
sentence s?S is tokenized into word set Ws;
2. For any word w in a sentence s?S, compute its
value SO(w) as follows:
1) if w?DSAAs, compute SO(w) with the
integrated method.
If SO(w)=1, SO(w)=Dy_PosValue;
If SO(w)=-1, SO(w)=Dy_NegValue;
2) if w?Positive_Dict, SO(w)=PosValue;
3) If w?Negative_Dict, SO(w)=NegValue;
4) Otherwise, SO(w)=0;
5) Within the window of q words previous to w, if
there is a term w'?Negation_Dict,
SO(w)= ?SO(w);
6) Within the window of q words previous to w, if
there is a term w'?Intensifier_Dict,
SO(w) =??SO(w);
3. ( ) ( )S d SO w
s S w Ws
? ? ?
? ?
Figure 1: Algorithm of computing SO of documents
5.2.3 Results
Adding the disambiguation of DSAAs, the
performance of sentiment classification of 212
product reviews was significantly improved, as
shown in Table 7.
Baseli
ne
DSAAs
(1, -2)
DSAAs
(1.5, -2.5)
Pre. 75.89 77.50 76.61
Rec. 78.70 86.11 87.96Pos.
F 77.27 81.58 81.90
Pre. 87.01 88.46 87.06
Rec. 64.42 66.35 71.15Neg.
F 74.03 75.82 78.31
MacroF 75.62 78.60 80.06Total Accu. 71.70 76.42 79.72
Table 7: The experimental results at document level
As an example, the following review, which
consists of only one sentence, is correctly
classified as positive by DSAAs method, but is
classified as negative by the baseline approach.
(10) ? ? ? , ? ? ? , ? ? ? ? ? ?
| Small size, light weight, and easy to carry.
According to HowNet, as shown in Table 8, the
sentence contains two negative words ??
|small?and ??|light?and one positive word ??
? fangbian|easy?, resulting the overall negative
prediction. In our approach, ??? tiji|size?and
?? ? zhongliang|weight? are assigned as
negative expectation, and consequently both ??
??|small size?and ????|light weight?have
positive meaning, resulting the overall positive
prediction.
Pos. ? |large, ? |high, ? |thick, ? |deep,
?|heavy, ??|great
Neg. ? |small, ? |low, ? |thin, ? |shallow,
?|light
OOV ?|many, ?|few, ??|huge
Table 8: The SO of DSAAs described in HowNet
Adding the disambiguation of DSAAs, our
method obviously outperforms the baseline by
4.44% in f-score and 8.02% in accuracy. The
improvement in recall is especially obvious.
When intensifying the polarity of DSAAs by
setting Dy_PosValue=1.5 and Dy_NegValue=-
2.5, the recall is improved by 9.26% for positive
category and 6.73% for negative category.
6 Conclusion and Future Work
This paper presents a knowledge-based
unsupervised method to automatically
disambiguate dynamic sentiment ambiguous
words, focusing on 14 DSAAs. We exploit
pattern-based and character-based methods to
infer sentiment expectation of nouns, and then
determine the polarity of DSAAs based on the
nouns. For the sentiment analysis at sentence
level, our method achieves promising result that
is significantly better than two types of baseline,
which validates the effectiveness of our
approach. We also apply the disambiguation of
14 DSAAs to the sentiment classification of
product reviews, resulting obvious improvement
in performance, which proves the significance of
the issue.
There leaves room for improvement. Our
future work will explore more contextual
information in disambiguating DSAAs. In
addition, we will find out new methods to reduce
noises when mining the Web to infer sentiment
expectation of nouns. Discovering the lexico-
syntactic patterns for sentiment expectation of
nouns automatically or semi-automatically with
bootstrapping method is also a challenging
direction.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (No. 60703063)
and National Social Science Foundation of
China (No. 08CYY016).
1198
References
Andreevskaia A. and Bergler S. 2006. Sentiment
tagging of adjectives at the meaning level. The
19th Canadian Conference on Artificial
Intelligence.
Andreevskaia, A. and Bergler, S. 2006. Mining
WordNet for fuzzy sentiment: Sentiment tag
extraction from WordNet glosses. Proceedings of
EACL 2006.
Chen, C-J. 2004. Character-sense association and
compounding template similarity: automatic
semantic classification of Chinese compounds.
Proceedings of the 3rd workshop on Chinese
language processing.
Ding X., Liu B. and Yu, P. 2008. A holistic lexicon-
based approach to opinion mining. Proceedings of
WSDM?08.
Esuli, A. and Sebastiani, F. 2006. SentiWordNet: a
publicly available lexical resource for opinion
mining. Proceedings of LREC?06.
Hatzivassiloglou, V. and McKeown, K. 1997
Predicting the semantic orientation of adjectives.
Proceedings of ACL?97.
Kim, S and Hovy, E. 2004. Determining the
sentiment of opinions. Proceedings of COLING?04.
Ku, L, Liang Y. and Chen, H. 2006. Opinion
extraction, summarization and tracking in news
and blog corpora. Proceedings of AAAI-2006
Spring Symposium on Computational Approaches
to Analyzing Weblogs.
Lu X-F, 2007. Hybrid models for semantic
classification of Chinese unknown words.
Proceedings of NAACL HLT?07..
Pang, B. and Lee, L. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in
Information Retrieval.
Riloff, E. and Wiebe, J. 2003. Learning Extraction
Patterns for Subjective Expressions. Proceedings
of EMNLP?03.
Riloff, E., Wiebe, J. and Phillips, W. 2005. Exploiting
Subjectivity Classification to Improve Information
Extraction. Proceedings of AAAI?05.
Su, F. and Markert, K. 2008. From words to senses: a
case study of subjectivity recognition. Proceedings
of COLING?08.
Takamura, H., Inui,T. and Okumura, M. 2006. Latent
Variable Models for Semantic Orientations of
phrases. Proceedings of EACL?06.
Takamura, H., Inui,T. and Okumura, M. 2007.
Extracting Semantic Orientations of Phrases from
Dictionary. Proceedings of NAACL HLT ?07.
Turney, P. and Littman, M. 2003. Measuring praise
and criticism: inference of semantic orientation
from association. ACM transaction on information
systems.
Wan, X. 2008. Using Bilingual Knowledge and
Ensemble Techniques for Unsupervised Chinese
Sentiment Analysis. Proceedings of EMNLP?08.
Wiebe, J. and Mihalcea, R. 2006. Word sense and
subjectivity. Proceedings of ACL?06.
Wiebe, J., Wilson, T., Bruce, R., Bell, M. and Martin,
M. 2004. Learning Subjective Language.
Computational Linguistics.
Wilson, T., Wiebe, J. and Hoffmann, P. 2005.
Recognizing contextual polarity in phrase-level
sentiment analysis. Proceedings of
HLT/EMNLP?05.
Wu, Y. and Jin, P. 2010. SemEval-2010 task 18:
disambiguating sentiment ambiguous adjectives.
Proceedings of SemEval 2010.
Yuen R., Chan T., Lai T., Kwong O., T?sou B. 2004.
Morpheme-based derivation of bipolar semantic
orientation of Chinese words. Proceedings of
COLING?04.
1199
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 81?85,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task 18:
Disambiguating Sentiment Ambiguous Adjectives
Yunfang Wu
Key Laboratory of Computational
Linguistics (Peking University)?
Ministry of Education, China
wuyf@pku.edu.cn
Peng Jin
Laboratory of Intelligent Information
Processing and Application, Leshan
Normal University, China
jinp@lstc.edu.cn
Abstract
Sentiment ambiguous adjectives cause
major difficulties for existing algorithms
of sentiment analysis. We present an
evaluation task designed to provide a
framework for comparing different
approaches in this problem. We define the
task, describe the data creation, list the
participating systems and discuss their
results. There are 8 teams and 16 systems.
1 Introduction
In recent years, sentiment analysis has attracted
considerable attention (Pang and Lee, 2008). It is
the task of mining positive and negative opinions
from natural language, which can be applied to
many natural language processing tasks, such as
document summarization and question answering.
Previous work on this problem falls into three
groups: opinion mining of documents, sentiment
classification of sentences and polarity prediction
of words. Sentiment analysis both at document
and sentence level rely heavily on word level.
The most frequently explored task at word
level is to determine the semantic orientation
(SO) of words, in which most work centers on
assigning a prior polarity to words or word
senses in the lexicon out of context. However,
for some words, the polarity varies strongly with
context, making it hard to attach each to a
specific sentiment category in the lexicon. For
example, consider ?low cost? versus ?low
salary?. The word ?low? has a positive
orientation in the first case but a negative
orientation in the second case.
Turney and Littman (2003) claimed that
sentiment ambiguous words could not be avoided
easily in a real-world application in the future
research. But unfortunately, sentiment
ambiguous words are discarded by most research
concerning sentiment analysis (Hatzivassiloglou
and McKeown, 1997; Turney and Littman, 2003;
Kim and Hovy, 2004). The exception work is
Ding et al (2008). They call these words as
context dependant opinions and propose a
holistic lexicon-based approach to solve this
problem. The language they deal with is English.
The disambiguation of sentiment ambiguous
words can also be considered as a problem of
phrase-level sentiment analysis. Wilson et al
(2005) present a two-step process to recognize
contextual polarity that employs machine
learning and a variety of features. Takamura et al
(2006, 2007) propose latent variable model and
lexical network to determine SO of phrases,
focusing on ?noun+adjective? pairs. Their
experimental results suggest that the
classification of pairs containing ambiguous
adjectives is much harder than those with
unambiguous adjectives.
The task 18 at SemEval 2010 provides a
benchmark data set to encourage studies on this
problem. This paper is organized as follows.
Section 2 defines the task. Section 3 describes
the data annotation. Section 4 gives a brief
summary of 16 participating systems. Finally
Section 5 draws conclusions.
2 Task Set up
2.1 Task description
In this task, we focus on 14 frequently used
sentiment ambiguous adjectives in Chinese,
which all have the meaning of measurement, as
shown below.
81
(1) Sentiment ambiguous adjectives(SAAs)
={? da ?large?, ? duo ?many?, ? gao
?high?, ? hou ?thick?,? shen ?deep?, ?
zhong ?heavy?, ?? ju-da ?huge?, ??
zhong-da ?great?, ? xiao ?small?, ? shao
?few?, ? di ?low?, ? bao ?thin?, ? qian
?shallow?,? qing ?light?}
These adjectives are neutral out of context, but
when they co-occur with some target nouns,
positive or negative emotion will be evoked.
Although the number of such ambiguous
adjectives is not large, they are frequently used
in real text, especially in the texts expressing
opinions and emotions.
The task is designed to automatically
determine the SO of these sentiment ambiguous
adjectives within context: positive or negative.
For example, ? gao ?high?should be assigned
as positive in ??? gong-zi-gao ?salary is
high?but negative in ??? jia-ge-gao ?price is
high?.
This task was carried out in an unsupervised
setting. No training data was provided, but
external resources are encouraged to use.
2.2 Data Creation
We collected data from two sources. The main
part was extracted from Xinhua News Agency of
Chinese Gigaword (Second Edition) released by
LDC. The texts were automatically word-
segmented and POS-tagged using the open
software ICTCLAS1. In order to concentrate on
the disambiguation of sentiment ambiguous
adjectives, and reduce the noise introduced by
the parser, we extracted sentences containing
strings in pattern of (2), where the target nouns
are modified by the adjectives in most cases.
(2) noun+adverb+adjective (adjective?SAAs)
e.g.??/n ?/d ?/a cheng-ben-jiao-di
?the cost is low.?
Another small part of data was extracted from
the Web. Using the search engine Google2, we
searched the queries as in (3):
(3) ? hen ?very?+ adjective (adjective?SAAs )
From the returned snippets, we manually picked
out some sentences that contain the strings of (2).
Also, the sentences were automatically
segmented and POS-tagged using ICTCLAS.
Sentiment ambiguous adjectives in the data
were assigned as positive, negative or neutral,
1 http://www.ictclas.org/.
2 http://www.google.com/.
independently by two annotators. Since we focus
on the distinction between positive and negative
categories, the neutral instances were removed.
The inter-annotator agreement is in a high level
with a kappa of 0.91. After cases with
disagreement were negotiated between the two
annotators, a gold standard annotation was
agreed upon. In total 2917 instances were
provided as the test data in the task, and the
number of sentences of per target adjective is
listed in Table 2.
Evaluation was performed in micro accuracy
and macro accuracy:
1 1
/
N N
mir i i
i i
P m n
? ?
?? ? (1)
1
/
N
mar i
i
P P N
?
?? /i i iP m n? (2)
where N is the number of all target words, in is
the number of all test instances for a specific
word, and im is the number of correctly labeled
instances.
2.3 Baseline
We group 14 sentiment ambiguous adjectives
into two categories: positive-like adjectives and
negative-like adjectives. The former has the
connotation towards large measurement, whereas
the latter towards small measurement.
(4) Positive-like adjectives (Pa) ={? da ?large?,
? duo ?many?, ? gao ?high?, ? hou
?thick?, ? shen ?deep?, ? zhong ?heavy?,
?? ju-da ?huge?,?? zhong-da ?great?}
(5) Negative-like adjectives (Na) ={? xiao
?small?, ? shao ?few?, ? di ?low?,? bao
?thin?, ? qian ?shallow?,? qing ?light?}
We conduct a baseline in the dataset. Not
considering the context, assign all positive-like
adjectives as positive and all negative-like
adjectives as negative. The micro accuracy of the
baseline is 61.20%.
The inter-annotator agreement of 0.91 can be
considered as the upper bound of the dataset.
3 Systems and Results
We published firstly trial data and then test data.
In total 11 different teams downloaded both the
trial and test data. Finally 8 teams submitted their
experimental results, including 16 systems.
82
3.1 Results
Table 1 lists all systems?scores, ranked from
best to worst performance measured by micro
accuracy. To our surprise, the performance of
different systems differs greatly. The micro
accuracy of the best system is 94.20% that is
43.12% higher than the worst system. The
accuracy of the best three systems is even higher
than inter-annotator agreement. The performance
of the worst system is only a little higher than
random baseline, which is 50% when we
randomly assign the SO of sentiment ambiguous
adjectives.
Table 1: The scores of 16 systems
Table 2 shows that the performance of
different systems differs greatly on each of 14
target adjectives. For example, the accuracy of
? da ?large?is 95.53% by one system but only
46.51% by another system.
Table 2: The scores of 14 ambiguous adjectives
3.2 Systems
In this section, we give a brief description of the
systems.
YSC-DSAA This system creates a new word
library named SAAOL (SAA-Oriented Library),
which is built manually with the help of software.
SAAOL consists of positive words, negative
words, NSSA, PSSA, and inverse words. The
system divides the sentences into clauses using
heuristic rules, and disambiguates SAA by
analyzing the relationship between SAA and the
keywords.
HITSZ_CITYU This group submitted three
systems, including one baseline system and two
improved systems.
HITSZ_CITYU_3: The baseline system is
based on collocation of opinion words and their
targets. For the given adjectives, their
collocations are extracted from People?s Daily
Corpus. With human annotation, the system
obtained 412 positive and 191 negative
collocations, which are regarded as seed
collocations. Using the context words of seed
collocations as features, the system trains a one-
class SVM classifier.
HITSZ_CITYU_2 and HITSZ_CITYU_1:
Using HowNet-based word similarity as clue, the
authors expand the seed collocations on both
ambiguous adjectives side and collocated targets
side. The authors then exploit sentence-level
opinion analysis to further improve performance.
The strategy is that if the neighboring sentences
on both sides have the same polarity, the
ambiguous adjective is assigned as the same
polarity; if the neighboring sentences have
conflicted polarity, the SO of ambiguous
adjective is determined by its context words and
the transitive probability of sentence polarity.
The two systems use different parameters and
combination strategy.
OpAL This system combines supervised
methods with unsupervised ones. The authors
employ Google translator to translate the task
dataset from Chinese to English, since their
system is working in English. The system
explores three types of judgments. The first one
trains a SVM classifier based on NTCIR data and
EmotiBlog annotations. The second one uses
search engine, issuing queries of ?noun + SAA +
AND + non-ambiguous adjective?. The non-
ambiguous adjectives include positive set
(?positive, beautiful, good?) and negative set
(?negative, ugly, bad?). An example is ?price
high and good?. The third one uses ?too, very-
System Micro
Acc.(%)
Macro
Acc.(%)
YSC-DSAA 94.20 92.93
HITSZ_CITYU_1 93.62 95.32
HITSZ_CITYU_2 93.32 95.79
Dsaa 88.07 86.20
OpAL 76.04 70.38
CityUHK4 72.47 69.80
CityUHK3 71.55 75.54
HITSZ_CITYU_3 66.58 62.94
QLK_DSAA_R 64.18 69.54
CityUHK2 62.63 60.85
CityUHK1 61.98 67.89
QLK_DSAA_NR 59.72 65.68
Twitter Sentiment 59.00 62.27
Twitter Sentiment_ext 56.77 61.09
Twitter Sentiment_zh 56.46 59.63
Biparty 51.08 51.26
Words Ins# Max% Min% Stdev
? |large 559 95.53 46.51 0.155
? |many 222 95.50 49.10 0.152
? ||high 546 95.60 54.95 0.139
? |thick 20 95.00 35.00 0.160
? |deep 45 100.00 51.11 0.176
? |heavy 259 96.91 34.75 0.184
?? |huge 49 100.00 10.20 0.273
?? |great 28 100.00 7.14 0.243
? |small 290 93.10 49.66 0.167
? few 310 95.81 41.29 0.184
? |low 521 93.67 48.37 0.147
? |thin 33 100.00 18.18 0.248
? |shallow 8 100.00 37.50 0.155
? |light 26 100.00 34.62 0.197
83
rules?. The final result is determined by the
majority vote of the three components.
CityUHK This group submitted four systems.
Both machine learning method and lexicon-
based method are employed in their systems. In
the machine learning method, maximum entropy
model is used to train a classifier based on the
Chinese data from NTCIR opinion task. Clause-
level and sentence-level classifiers are compared.
In the lexicon-based method, the authors classify
SAAs into two clusters: intensifiers (our
positive-like adjectives in (4)) and suppressors
(our negative-like adjectives in (5)), and then use
the polarity of context to determine the SO of
SAAs.
CityUHK4: clause-level machine learning +
lexicon.
CityUHK3: sentence-level machine learning +
lexicon.
CityUHK2: clause-level machine learning.
CityUHK2: sentence-level machine learning.
QLK_DSAA This group submitted two
systems. The authors adopt their SELC model
(Qiu, et al, 2009), which is proposed to exploit
the complementarities between lexicon-based
and corpus-based methods to improve the whole
performance. They determine the sentence
polarity by SELC model, and simply regard the
sentence polarity as the polarity of SAA in the
sentence.
QLK_DSAA_NR: Based on the result of
SELC model, they inverse the SO of SAA when
it is modified by negative terms. Our task
includes only positive and negative categories, so
they replace the neutral value obtained by SELC
model by the predominant polarity of the
adjective.
QLK_DSAA_R: Based on the result of
QLK_DSAA_NR, they add a rule to cope with
two modifiers ? pian ?specially? and ? tai
?too?, which always have the negative meaning.
Twitter sentiment This group submitted three
systems. The authors use a training data collected
from microblogging platform. By exploiting
Twitter, they collected automatically a dataset
consisting of negative and positive expressions.
The sentiment classifier is trained using Naive
Bayes with n-grams of words as features.
Twitter Sentiment: Translating the task dataset
from Chinese to English using Google translator,
and then based on training data in English texts
from Twitter.
Twitter Sentiment_ext: With Twitter
Sentiment as basis, using extended data.
Twitter Sentiment_zh: Based on training data
in Chinese texts from Twitter.
Biparty This system transforms the problem
of disambiguating SAAs to predict the polarity
of target nouns. The system presents a
bootstrapping method to automatically build the
sentiment lexicon, by building a nouns-verbs
biparty graph from a large corpus. Firstly they
select a few nouns as seed words, and then they
use a cross inducing method to expand more
nouns and verbs into the lexicon. The strategy is
based on a random walk model.
4 Discussion
The experimental results of some systems are
promising. The micro accuracy of the best three
systems is over 93%. Therefore, the inter-
annotator agreement (91%) is not an upper
bound on the accuracy that can be achieved. On
the contrary, the experimental results of some
systems are disappointing, which are below our
predefined simple baseline (61.20%), and are
only a little higher than random baseline (50%).
The accuracy variance of different systems
makes this task more interesting.
The participating 8 teams exploit totally
different methods.
Human annotation. In YSC-DSAA system,
the word library of SAAOL is verified by human.
In HITSZ_CITYU systems, the seed collocations
are annotated by human. The three systems rank
top 3. Undoubtedly, human labor can help
improve the performance in this task.
Training data. The OpAL system employs
SVM machine learning based on NTCIR data
and EmotiBlog annotations. The CityUHK
systems trains a maximum entropy classifier
based on the annotated Chinese data from
NTCIR. The Twitter Sentiment systems use a
training data automatically collected from
Twitter. The results show that some of these
supervised methods based on training data
cannot rival unsupervised ones, partly due to the
poor quality of the training data.
English resources. Our task is in Chinese.
Some systems use English resources by
translating Chinese into English, as OpAL and
Twitter Sentiment. The OpAL system achieves a
quite good result, making this method a
promising direction. This also shows that
disambiguating SAAs is a common problem in
natural language.
84
5 Conclusion
This paper describes task 18 at SemEval-2010,
disambiguating sentiment ambiguous adjectives.
The experimental results of the 16 participating
systems are promising, and the used approaches
are quite novel.
We encourage further research into this issue,
and integration of the disambiguation of
sentiment ambiguous adjectives into applications
of sentiment analysis.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (No. 60703063),
National Social Science Foundation of China
(No. 08CYY016), and the Open Projects
Program of Key Laboratory of Computational
Linguistics(Peking University) ? Ministry of
Education. We thank Miaomiao Wen and Tao
Guo for careful annotation of the data.
References
Ding X., Liu B. and Yu, P. 2008. A holistic lexicon-
based approach to opinion mining. Proceedings of
WSDM?08.
Hatzivassiloglou, V. and McKeown, K. 1997
Predicting the semantic orientation of adjectives.
Proceedings of ACL?97.
Kim, S and Hovy, E. 2004. Determining the sentiment
of opinions. Proceedings of COLING?04.
Pang, B. and Lee, L. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in
Information Retrieval.
Qiu L., Zhang W., Hu, C. and Zhao, K. 2009. SELC:
A self-supervised model for sentiment analysis. In
Proceedings of CIKM?09.
Takamura, H., Inui,T. and Okumura, M. 2006. Latent
Variable Models for Semantic Orientations of
phrases. Proceedings of EACL?06.
Takamura, H., Inui,T. and Okumura, M. 2007.
Extracting Semantic Orientations of Phrases from
Dictionary. Proceedings of NAACL HLT ?07.
Turney, P. and Littman, M. 2003. Measuring praise
and criticism: inference of semantic orientation
from association. ACM transaction on information
systems.
Wilson, T., Wiebe, J. and Hoffmann, P. 2005.
Recognizing contextual polarity in phrase-level
sentiment analysis. Proceedings of
HLT/EMNLP?05.
85
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, page 87,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2 Task 15: Infrequent Sense Identification for Mandarin 
Text to Speech Systems   
Peng Jin1 and Yunfang Wu2
1Laboratory of Intelligent Information Processing and Application, Leshan Normal 
University, Leshan China 
2Institute of Computational Linguistics  Peking University, Beijing China 
{jandp, wuyf}@pku.edu.cn 
 
1 Introduction 
There are seven cases of grapheme to phoneme in 
a text to speech  system (Yarowsky, 1997). Among 
them, the most difficult task is disambiguating the 
homograph word, which has the same POS but 
different pronunciation. In this case, different pro-
nunciations of the same word always correspond to 
different word senses. Once the word senses are 
disambiguated, the problem of GTP is resolved. 
There is a little different from traditional WSD, 
in this task two or more senses may correspond to 
one pronunciation. That is, the sense granularity is 
coarser than WSD. For example, the preposition 
???  has three senses: sense1 and sense2 have the 
same pronunciation {wei 4}, while sense3 corre-
sponds to {wei 2}. In this task, to the target word, 
not only the pronunciations but also the sense la-
bels are provided for training; but for test, only the 
pronunciations are evaluated. The challenge of this 
task is the much skewed distribution in real text: 
the most frequent pronunciation occupies usually 
over 80%. 
In this task, we will provide a large volume of 
training data (each homograph word has at least 
300 instances) accordance with the truly distribu-
tion in real text. In the test data, we will provide at 
least 100 instances for each target word. The 
senses distribution in test data is the same as in 
training data.All instances come from People Daily 
newspaper (the most popular newspaper in Manda-
rin). Double blind annotations are executed manu-
ally, and a third annotator checks the annotation. 
2 Participating Systems 
Two kinds of precisions are evaluated. One is 
micro-average: 
??
==
=
N
i
i
N
i
imir nmP
11
/  
N is the number of all target word-types. mi is 
the number of labeled correctly to one specific tar-
get word-type and ni is the number of all test in-
stances for this word-type. The other is macro-
average: 
?
=
=
N
i
imar NpP
1
/ ,  iii nmp /=
 
There are two teams participated in and submit-
ted nine systems. Table 1 shows the results, all sys-
tems are better than baseline (Baseline is using the 
most frequent sense to tag all the tokens). 
 
System Micro-average Macro-average
156-419 0.974432 0.951696 
205-332 0.97028 0.938844 
205-417 0.97028 0.938844 
205-423 0.97028 0.938844 
205-425 0.97028 0.938844 
205-424 0.968531 0.938871 
156-420 0.965472 0.942086 
156-421 0.965472 0.94146 
156-422 0.965472 0.942086 
baseline 0.923514 0.895368 
Table 1: The scores of all participating systems 
References 
Yarowsky, David. 1997. ?Homograph disambiguation 
in text-to-speech synthesis.? In van Santen, Jan T. H.; 
Sproat, Richard; Olive, Joseph P.; and Hirschberg, 
Julia. Progress in Speech Synthesis. Springer-Verlag, 
New York, 157-172. 
87
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 374?377,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 4: Evaluating Chinese Word Similarity 
 
 
Peng Jin Yunfang Wu 
School of Computer Science Institute of Computational Linguistics 
Leshan Normal University Peking University 
Leshan, 614000, China Beijing, 100871, China 
jandp@pku.edu.cn wuyf@pku.edu.cn 
 
 
 
 
 
 
Abstract 
This task focuses on evaluating word similari-
ty computation in Chinese. We follow the way 
of Finkelstein et al (2002) to select word 
pairs. Then we organize twenty under-
graduates who are major in Chinese linguis-
tics to annotate the data. Each pair is assigned 
a similarity score by each annotator. We rank 
the word pairs by the average value of similar 
scores among the twenty annotators. This data 
is used as gold standard. Four systems partici-
pating in this task return their results. We 
evaluate their results on gold standard data in 
term of  Kendall's tau value, and the results 
show three of them have a positive correlation 
with the rank manually created while the taus' 
value is very small. 
1 Introduction 
The goal of word similarity is to compute the simi-
larity degree between words. It is widely used in 
natural language processing to alleviate data 
sparseness which is an open problem in this field. 
Many research have focus on English language 
(Lin, 1998; Curran and Moens, 2003; Dinu and 
Lapata, 2010), some of which rely on the manual 
created thesaurus such as WordNet (Budanitsky 
and Hirst, 2006), some of which obtain the similar-
ity of the words via large scale corpus (Lee, 1999), 
and some research integrate both thesaurus and 
corpus (Fujii et al, 1997). This task tries to evalu-
ate the approach on word similarity for Chinese 
language. To the best of our knowledge, this is first 
release of  benchmark data for this study. 
In English language, there are two data sets: Ru-
benstein and Goodenough (1965) and Finkelstein 
et al (2002) created a ranking of word pairs as the 
benchmark data. Both of them are manually anno-
tated. In this task, we follow the way to create the 
data and annotate the similarity score between 
word pairs by twenty Chinese native speakers. 
Finkelstein et al (2002) carried out a psycholin-
guistic experiment: they selected out 353 word 
pairs, then ask the annotators assign a numerical 
similarity score between 0 and 10 (0 denotes that 
words are totally unrelated, 10 denotes that words 
are VERY closely related) to each pair. By defini-
tion, the similarity of the word to itself should be 
10. A fractional score is allowed.  
It should be noted that besides the rank of word 
pairs, the thesaurus such as Roget's thesaurus are 
often used for word similarity study (Gorman and 
Curran, 2006).  
The paper is organized as follows. In section 2 
we describe in detail the process of the data prepa-
ration. Section 3 introduces the four participating 
systems. Section 4 reports their results and gives a  
brief discussion.. And finally in section 5 we bring 
forward some suggestions for the next campaign 
and conclude the paper. 
374
2 Data Preparation 
2.1 Data Set 
We use wordsim 353 (Finkelstein et al, 2002) as 
the original data set. First, each word pair is trans-
lated into Chinese by two undergraduates who are 
fluent in English. 169 word pairs are the same in 
their translation results. To the rest 184 word pairs, 
the third undergraduate student check them   fol-
lowing the rules: 
(i) Single character vs. two characters. If one 
translator translate one English word into the Chi-
nese word which consists only one Chinese charac-
ter and the other use two characters to convey the 
translation, we will prefer to the later provided that 
these two translations are semantically same. For 
example, "tiger" is translated into "?" and "??", 
we will treat them as same and use "??" as the 
final translation. This was the same case in "drug" 
("?" and "??" are same translations). 
(ii) Alias. The typical instance is "potato", both "
??" and "???" are the correct translations. So 
we will treat them as same and prefer "??" as the 
final translation because it is more general used 
than the latter one.  
(iii) There are five distinct word pairs  in the 
translations and are removed.    
At last, 348 word pairs are used in this task. 
Among these 348 word pairs, 50 ones are used as 
the trial data and the rest ones are used as the test 
data1. 
2.2 Manual Annotation 
Each word pair is assigned the similarity score by 
twenty Chinese native speakers. The score ranges 
from 0 to 5 and 0 means two words have nothing 
to do with each other and 5 means they are identi-
cally in semantic meaning. The higher score means 
the more similar between two words. Not only in-
teger but also real is acceptable as the annotated 
score. We get the average of all the scores given by 
the annotators for each word pair and then sort 
them according to the similarity scores. The distri-
bution of word pairs on the similar score is illus-
trated as table 1.   
                                                          
1 In fact there are 297 word pairs are evaluated because one 
pair is missed during the annotation.  
Score 0.0-1.0 1.0-2.0 2.0-3.0 3.0-4.0 4.0-5.0 
# Word pairs 39 90 132 72 13 
Table1: The distribution of similarity score 
 
Ra-
nk 
Word in Chi-
nese/English 
Word 2 in 
Chinese/ Eng-
lish 
Simi-
larity 
score 
Std. 
dev 
RSD 
(%) 
1 ??/football ??/soccer 4.98 0.1 2.0 
2 ??/tiger ??/tiger 4.89 0.320 6.55 
3 ??/planet ??/star 4.72 0.984 20.8 
4 ???
/admission 
??/ticket 4.60 0.516 11.2 
5 ?/money ??/cash 4.58 0.584 12.7 
6 ??/bank ?/cash 4.29 0.708 16.5 
7 ??/cell ??/phone 4.28 0.751 17.5 
8 ??/gem ??/jewel 4.24 0.767 18.1 
9 ??/type ??/kind 4.24 1.000 23.6 
10 ?? / calcu-
lation 
?? / compu-
tation 
4.14 0.780 19.0 
Avg - - 4.496 0.651 14.80 
Table 2: Top ten similar word pairs 
 
Table 2 and table 3 list top ten similar word 
pairs and top ten un-similar word pairs individual-
ly. Standard deviation (Std. dev) and relative standard 
deviation (RSD) are also computed. Obviously, the rela-
tive standard deviation of top ten similar word pairs is 
far less than the un-similar pairs. 
 
2.3 Annotation Analysis 
Figure 1 illustrates the relationship between the 
similarity score and relative standard deviation. 
The digits in "x" axes are the average similarity 
score of every integer interval, for an instance, 
1.506 is the average of all word pairs' similarity 
score between 1.0 and 2.0. 
3 Participating Systems  
Four systems coming from two teams participated 
in this task. 
 
375
 
Figure 1. The relationship between RSD and simi-
lar score 
 
Ra-
nk 
Word1 in Chi-
nese/in English 
Word2 in Chi-
nese/in English 
Simi-
larity 
score 
Std. 
dev 
RSD(
%) 
1 ??/noon ??/string 0.06 .213 338.7 
2 ??/king ???
/cabbage 
0.16 .382 245.3 
3 ??
/production 
??/hike 0.17 .432 247.5 
4 ??/delay ????
/racism 
0.26 .502 191.1 
5 ??/professor ??/cucumber 0.30 .62 211.1 
6 ??/stock ???/jaguar 0.30 .815 268.2 
7 ??/sign ??/recess 0.30 .655 215.4 
8 ??/stock CD/CD 0.31 .540 173.6 
9 ?/drink ??/ear 0.31 .833 264.8 
10 ??/rooster ??/voyage 0.33 .771 236.7 
Avg - - 0.25 .576 239.2 
Table 3: Top ten un-similar word pairs 
 
 
MIXCC: This system used two machine reada-
ble dictionary (MRD), HIT IR-Lab Tongyici Cilin 
(Extended) (Cilin) and the other is Chinese Con-
cept Dictionary (CCD). The extended CiLin con-
sists of 12 large classes, 97 medium classes, 1,400 
small classes (topics), and 17,817 small synonym 
sets which cover 77,343 head terms. All the items 
are constructed as a tree with five levels. With the 
increasing of levels, word senses are more fine-
grained. The Chinese Concept Dictionary is a Chi-
nese WordNet produced by Peking University. 
Word concepts  are presented as synsets   corre-
sponding to WordNet 1.6. Besides synonym, anto-
nym, hypernym/hyponym, holonym/meronym, 
there is another semantic relation type named as 
attribute which happens between two words with 
different part-of-speeches.  
They first divide all word pairs into five parts 
and rank them according to their levels in Cilin in 
descending order. For each part, they computed 
word similarity by Jiang and Conrath (1997) meth-
od2. 
 
MIXCD: Different form MIXCC, this system 
used the trial data to learn a multiple linear regres-
sion functions. The CCD was considered as a di-
rected graph. The nodes were synsets and edges 
were the semantic relations between two synsets. 
The features for this system were derived from  
CCD and a corpus and listed as follows: 
 
? the shortest path between two synsets 
which contain the words 
? the rates of 5 semantic relation types  
? mutual information of a word pair in the 
corpus 
 
They used the result of multiple linear regres-
sions to forecast the similarity of other word pairs 
and get the rank. 
 
GUO-ngram: This system used the method 
proposed by (Gabrilovich and Markovitch, 2007). 
They downloaded the Wikipedia on 25th Novem-
ber, 2011 as the knowledge source. In order to by-
pass the Chinese segmentation, they extract one 
character (uni-gram) and two sequential characters 
(bi-gram) as the features. 
 
GUO-words: This system is very similar to 
GUO-ngram except that the features consist of 
words rather than n-grams. They implemented a 
simple index method which searches all continuous 
character strings appearing in a dictionary. For ex-
ample, given a text string ABCDEFG in which 
ABC, BC, and EF appear in the dictionary. The 
output of the tokenization algorithm is the three 
words ABC, BC, EF and the two characters E and 
G. 
                                                          
2 Because there is no sense-tagged corpus for CCD, the fre-
quency of each concept was set to 1 in this system. 
376
4 Results  
Each system is required to rank these 500 word 
pairs according to their similarity scores. Table 4 
gives the overall results obtained by each of the 
systems. 
 
Rank Team ID System ID Tau's 
value 
1 
lib 
MIXCC 0.050 
2 MIXCD 0.040 
3 
Gfp1987 
Guo-ngram 0.007 
4 Guo-words -0.011 
Table 4: The results of four systmes 
 
The ranks returned by these four systems will be 
compared with the rank from human annotation by 
the Kendall Rank Correlation Coefficient: 
 
? ?
? ?
2 ,1 1 / 2
S
N N
? ?? ? ? ?
 
Where N  is the number of objects. ? and ? are 
two distinct orderings of a object in two ranks. 
( , )S ? ? is the minimum number of adjacent 
transpositions needing to bring ? and ?  (Lapata, 
2006). In this metric, tau's value ranges from -1 to 
+1 and -1 means that the two ranks are inverse to 
each other and +1 means the identical rank.  
From table 4, we can see that except the final 
system, three of them got the positive tau's value. It 
is regret that the tau's is very small even if the 
MIXCC system  is the best one.   
5 Conclusion  
We organize an evaluation task focuses on word 
similarity in Chinese language. Totally 347 word 
pairs are annotated similarity scores by twenty na-
tive speakers. These word pairs are ordered by the 
similarity scores and this rank is used as bench-
mark data for evaluation.  
Four systems participated  in this task.  Except 
the system MIXCD, three ones got their own rank 
only via the corpus. Kendall's tau is used as the 
evaluation metric. Three of them got the positive 
correlation rank compared with the gold standard 
data 
Generally the tau's value is very small, it indi-
cates that obtaining a good rank is still difficult. 
We will provide more word pairs and distinct them 
relatedness from similar, and attract more teams to 
participate in the interesting task. 
 
Acknowledgments 
This research is supported by National Natural 
Science Foundation of China (NSFC) under Grant 
No. 61003206, 60703063. 
References  
A. Budanitsky and G. Hirst. Evaluating WordNet-based 
Measures of Lexical Semantic Relatedness. Compu-
tational Linguistics, 2006, 32(1):13-47. 
J. Curran and M. Moens. Scaling Context Space. Pro-
ceedings of ACL, 2002, pp. 231-238. 
G. Dinu and M. Lapata. Measuring Distributional Simi-
larity in Context. Proceedings of EMNLP, 2010, pp. 
1162-1172. 
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. 
Solan, G. Wolfman, and E. Ruppin. 2002. Placing 
Search in Context: The Concept Revisited. ACM 
Transactions on Information Systems, 20(1):116-131. 
A. Fujii, T. Hasegawa, T. Tokunaga and H. Tanaka. 
Integration of Hand-Crafted and Statistical Resources 
in Measuring Word Similarity. 1997. Proceedings of 
Workshop of Automatic Information Extraction and 
Building of Lexical Semantic Resources for NLP Ap-
plications. pp. 45-51. 
E. Gabrilovich and S. Markovitch, Computing Semantic 
Relatedness using Wikipedia-based Explicit Seman-
tic Analysis, Proceedings of IJCAI, Hyderabad, 2007, 
pp. 1606?1611. 
J. Gorman and J. Curran. Scaling Distributional Similar-
ity to Large Corpora. Proceedings of ACL, 2006, pp. 
361-368. 
J. Jiang and D. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. Pro-
ceedings of International Conference on Research in 
Computational Linguistics, Taiwan. 
M. Lapata. Automatic Evaluation of Information Order-
ing: Kendall's Tau. Computational Linguistics, 2006, 
32(4):471-484. 
D. Lin. Automatic Retrieval and Clustering of Similar 
Words. Proceedings of ACL / COLING, 1998, pp. 
768-774. 
L. Lee. Measures of Distributional Similarity. Proceed-
ings of ACL, 1999, pp. 25-32. 
H. Rubenstein and J.B. Goodenough. 1965. Contextual 
correlates of synonymy. Communications of the ACM, 
8(10):627-633. 
377
