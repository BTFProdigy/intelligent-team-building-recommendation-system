  
A Unicode based Adaptive Segmentor 
Q. Lu, S. T. Chan, R. F. Xu, T. S. Chiu 
Dept. Of Computing, 
The Hong Kong Polytechnic University, 
Hung Hom, Hong Kong 
{csluqin,csrfxu}@comp.polyu.edu.hk 
B. L. Li, S. W. Yu 
The Institute of Computational Linguistics, 
Peking University, 
Beijing, China 
{libi,yusw}@pku.edu.cn 
 
Abstract 
This paper presents a Unicode based 
Chinese word segmentor. It can handle 
Chinese text in Simplified, Traditional, or 
mixed mode. The system uses the strategy 
of divide-and-conquer to handle the 
recognition of personal names, numbers, 
time and numerical values, etc in the pre-
processing stage. The segmentor further 
uses tagging information to work on 
disambiguation. Adopting a modular 
design approach, different functional parts 
are separately implemented using 
different modules and each module 
tackles one problem at a time providing 
more flexibility and extensibility. Results 
show that with added pre-processing 
modules and accessorial modules, the 
accuracy of the segmentor is increased 
and the system is easily adaptive to 
different applications. 
1 Introduction 
The most difficult problem in Chinese word 
segmentation is due to overlapping ambiguities [1-
2]. The recognition of names, foreign names, and 
organizations are quite unique for Chinese. Some 
systems can already achieve very high accuracy [3], 
but they heavily rely on manual work in getting the 
system to be trained to work certain language 
environment. However, for many applications, we 
need to look at the cost to achieve high accuracy. 
In a competitive environment, we also need to 
have systems that are quickly adaptive to new 
requirements with limited resources available. 
In this paper, we report a Unicode based Chinese 
word segmentor. The segmentor can handle 
Chinese text in Simplified, Traditional, or mixed 
mode where internally only one dictionary is 
needed. The system uses the strategy of divide-
and-conquer to handle the recognition of personal 
names, numbers, time and numerical values. The 
system has a built-in new word extractor that can 
extract new words from running text, thus save 
time on training and getting the system quickly 
adaptive to new language environment. The 
Bakeoff results in the open text for our system in 
all categories have shown that it works reasonably 
good for all different corpora. 
The rest of the paper is organized as follows. 
Section 2 presents our system design objectives 
and components. Section 3 discusses more 
implementation details. Section 4 gives some 
performance evaluations. Section 5 is the 
conclusion. 
2 Design Objectives and Components 
With the wide use of Unicode based operating 
systems such as Window 2000 and Window XP, 
we now see more and more text data written in 
both the Simplified form and the Traditional form 
to co-exist on the same system. It is also likely that 
text written in mixed mode. Because of this reality, 
the first design objective of this system is its ability 
to handle the segmentation of Chinese text written 
in either Simplified Chinese, Traditional Chinese, 
or mixed mode.  As an example, we should be able 
to segment the same sentence in different forms 
such as the example given below:   
 
The second design objective is to adopt the 
modular design approach where different 
functional parts are separately implemented using 
independent modules and each module tackles one 
problem at a time. Using this modular approach, 
we can isolate problems and fine tune each module 
with minimal effect on other modules in the system. 
  
Special features like adding new rules or new 
dictionary can be easily done without affecting 
other modules. Consequently, the system is more 
flexible and can be easily extended.  
The third design objective of the system is to make 
the segmentor adaptive to different application 
domains. We consider it having more practical 
value if the segmentor can be easily trained using 
some semi-automatic process to work in different 
domains and work well for text with different 
regional variations. We consider it essential that 
the segmentor has tools to help it to obtain regional 
related information quickly even if annotated 
corpora are not available. For instance, when it 
runs text from Hong Kong, it must be able to 
recognize the personal names such as  if 
such a name(quadra-gram) appears in the text often. 
 
Figure 1. System components 
Figure 1 shows the two major components, the 
segmentor and data manager. The segmentor is the 
core component of the system. It has a pre-
processor, the kernel, and a post-processor. As the 
system has to maintain a number of tables such as 
the dictionaries, family name list, etc., a separate 
component called data manager is responsible in 
handling the maintenance of these data.  The pre-
processor has separate modules to handle 
paragraphs, ASCII code, numbers, time, and 
proper names including personal names, place and 
organizational names, and foreign names. The 
kernel supports different segmentation algorithms. 
It is the application or user?s choice to invoke the 
preferred segmentation algorithms that at current 
time include the basic maximum matching and 
minimum matching in both forward and backward 
mode. These can also be used to build more 
complicated algorithms later on. In addition, the 
system provides segmentation using part-of-speech 
tagging information to help resolve ambiguity. The 
post-processor applies morphological rules which 
cannot be easily applied using a dictionary.   
The data manager helps to maintain the knowledge 
base in the system. It also has an accessory 
software called the new word extractor which can 
collect statistical information based on character 
bi-grams, tri-grams and quadra-grams to semi-
automatically extract words and names so that they 
can be used by the segmentor to improve 
performance especially when switching to a new 
domain. Another characteristic of this segmentor is 
that it provides tagging information for segmented 
text. The tagging information can be optionally 
omitted if not needed by an application. 
3 Implementation Details 
The basic dictionary of this system was provided 
by Peking University [4] and we also used the 
tagging data from [4]. The data structure for our 
dictionaries are very similar to that discussed in [5]. 
As our program needs to handle both Simplified 
and Traditional Chinese characters, Unicode is the 
only solution for dealing with more than one script 
at the same time. 
Even though it is our design objective to support 
both Simplified and Traditional Chinese, we do not 
want to keep two different sets of dictionaries for 
Simplified and Traditional Chinese. Even if two 
versions are kept, it would not serve well for text 
in mixed mode. For example, Traditional Chinese 
word of ?the day after tomorrow? should be , 
and for Simplified Chinese, it should be . 
However sometimes we can see the word  
appears in a Traditional Chinese text. We cannot 
say that it is wrong because the sentence is still 
semantically correct especially in Unicode 
environment. Therefore the segmentor should be 
able to segment those words correctly such as in 
the examples: ? ?, and in ?  
?. We must also deal with dictionary 
maintenance related to Chinese variants. For 
example, characters  are variants, so are 
. 
Data manager Segmentor 
Pre-
Processor 
Kernel 
Post 
Processor 
New Word
Extractor
Knowledge-
base 
  
In order to keep the dictionary maintenance simple, 
our system uses a single dictionary which only 
keeps the so called canonical form of a word. In 
our system, the canonical form of a word is its 
?simplified form?.  We quoted the word 
?simplified? because only certain characters have 
simplified forms such as  to , but for  
, there is no simplified form. In the case of 
variants, we simply choose one of them as the 
canonical character.  The canonical characters are 
maintained in the traditional-simplified character 
conversion table as well as in a variant table.  
Whenever a new word, item, is added into the 
dictionary, it must be added using a function 
CanonicalConversion(), which takes item as an 
input. During segmentation, the corresponding 
dictionary look up function will first convert the 
token to its canonical form before looking up in the 
dictionary.  
The personal name recognizers (separate for 
Chinese names and foreign names) use the 
maximum-likelihood algorithm with consideration 
of commonly used Chinese family names, given 
names, and foreign name characters. It works for 
Chinese names of length up to 5 characters. In the 
following examples you can see that our system 
successfully recognized the name . This 
is done using our algorithm, not by putting her 
name in our dictionary: 
 
Organization names and place names are 
recognized mainly using special purpose 
dictionaries. The segmentor uses tagging 
information to help resolve ambiguity. The 
disambiguation is mostly based on rules such as  
p + (n + f) -> p + n + f 
which would word to correct 
  
For efficiency reasons, our system uses only about 
20 rules. The system is flexible enough for new 
rules to be added to improve performance.  
The new word extractor is an accessory program to 
extract new words from running text based on 
statistical data which can either be grabbed from 
the internet or collected from other sources. The 
basic statistical data include bi-gram frequency, tri-
gram frequency, and quadra-gram frequencies. In 
order to further example whether a bi-gram, say  
, is indeed a word, we further collect forward 
conditional frequency of  , and 
the back-ward conditional frequency of , 
. For an i-gram token, we also 
use the (i+1)-gram statistics to eliminate those i-
grams that are only a part of (i+1) ? gram word.  
For instance, if the frequency of bi-gram  is 
very close to the frequency of tri-gram , it 
is less likely that  is a word. Of course, 
whether  is a word depends on quadra-gram 
results.  Using the statistical result, a set of rules 
was applied to these i-grams to eliminate entries 
that are not considered new words. Minimal 
manual work is required to identify whether the 
remaining candidates are new words. Before words 
are added into the dictionary, part-of-speech 
information are added manually (although not 
necessary) before using the canonical function. 
The following table shows examples of bi-grams 
which are found by the new word extractor using 
one year Hong Kong Commercial Daily News data. 
 
 
 
4 Performance Evaluation 
The valuation metrics used in [6] were adopted 
here. 
1
3
N
N
recall =     (1) 
 
2
3
N
Npresicion =     (2) 
  
precisionrecall
precisionrecallrecallprecisionF +
??= 2),(1   (3) 
where N  1  denotes the number of words in the 
annotated corpus, N 2 denotes the number of words 
identified by the segmentation algorithm , and N 3 is 
the number of words correctly identified. 
We participated in the open tests for all four 
corpora. The results are shown in the following 
table. 
The worst performance in the 4 tests were for the 
CTB(UPenn) data. From the observation from the 
testing data, we found that the main problem with 
have with CTB data is the difference in word 
granularity. To confirm our observation, we have 
done an analysis of combining errors and 
overlapping errors. The results show that the ratios 
of combining errors in all the error types are 
0.8425(AS), 0.87684(CTB), 0.82085(HK), and 
0.77102(PK). The biggest problem we have with 
AS data, on the other hand is due to out of 
vocabulary mistakes. Even though our new word 
extractor can help us to reduce this problem, but 
we have not trained our system using data from 
Taiwan.  Our best performance was on PK data 
because we used a very similar dictionary. The 
additional training of data for HK was done using 
one year Commercial Daily( ). 
The following table summarizes the execution 
speed of our program for the 4 different 
sources: 
Data No. of 
chars 
Processi
ng Time 
(sec.) 
Processin
g Rate 
(char/sec) 
Segmentat
ion Rate 
(char/sec) 
AS 18,743 4.703 3,985 7,641 
CTB 62,332 10.110 6,165 7,930 
HK 57,432 10.329 5,560 7,109 
PK 28,458 4.829 5,893 10,970 
 
The program initialization needs around 2.25 
seconds mainly to load the dictionaries and other 
data into the memory before the segmentation can 
start. If we only count the segmentation time, the 
rate of segmentation on the average is around 
7,500 characters for the first three corpora. It 
seems that the processing speed for Peking U. data 
is faster. This may be because the dictionaries we 
used are closer to the PK system, thus it would 
take less time to work on disambiguation.  
5 Conclusion 
In this paper, design and algorithms of a general-
purposed Unicode based segmentor is proposed. It 
is able to process Simplified and Traditional 
Chinese appear in the same text. Sophisticated pre-
processing and other auxiliary modules help 
segmenting text more accurately. User interactions 
and modules can be easily added with the help of 
its modular design. A built-in new word extractor 
is also implemented for extracting new words from 
running text. It saves much time on training and 
thus it can be quickly adapted to new environments. 
Acknowledgement 
We thank the PI of ITF Grant by ITC of 
HKSARG (ITS/024/01) entitled: Towards Cost-
Effective E-business in the News Media & 
Publishing Industry for the use of HK Commercial 
Daily. 
References 
[1] Automatic Segmentation and Tagging for Chinese 
Text ( ) , K.Y. Liu, 
Commercial Press, 2000 
[2] Segmentation Issues in Chinese Information 
Processing,  (C.N. Huang Issue No. 
1, 1997) 
[3] The design and Implementation of a Modern General 
Purpose Segmentation System (B. Lou,  R. Song, W.L. 
Li, and Z.Y. Luo, Journal of Chinese Information 
Processing, Issue No. 5, 2001) 
[4] (Institute of 
Computational Linguistics, Peking Univ., 2002) 
[5] 
  Journal of Chinese information 
processing vol. 14, no. 1, 2001) 
[6] Chinese Word Segmentation and Information 
Retrieval, Palmer D., and Burger J., In AAAI 
Symposium Cross-Language Text and Speech 
Retrieval 1997 
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 937?946,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
CoCQA: Co-Training Over Questions and Answers 
with an Application to Predicting Question Subjectivity Orientation 
Baoli Li 
Emory University 
csblli@gmail.com 
Yandong Liu 
Emory University 
yliu49@emory.edu 
Eugene Agichtein 
Emory University 
eugene@mathcs.emory.edu
 
 
Abstract 
An increasingly popular method for 
finding information online is via the 
Community Question Answering 
(CQA) portals such as Yahoo! An-
swers, Naver, and Baidu Knows. 
Searching the CQA archives, and rank-
ing, filtering, and evaluating the sub-
mitted answers requires intelligent 
processing of the questions and an-
swers posed by the users. One impor-
tant task is automatically detecting the 
question?s subjectivity orientation: 
namely, whether a user is searching for 
subjective or objective information. 
Unfortunately, real user questions are 
often vague, ill-posed, poorly stated. 
Furthermore, there has been little la-
beled training data available for real 
user questions. To address these prob-
lems, we present CoCQA, a co-training 
system that exploits the association be-
tween the questions and contributed 
answers for question analysis tasks. 
The co-training approach allows 
CoCQA to use the effectively unlim-
ited amounts of unlabeled data readily 
available in CQA archives. In this pa-
per we study the effectiveness of 
CoCQA for the question subjectivity 
classification task by experimenting 
over thousands of real users? questions.
1 Introduction 
Automatic question answering (QA) has been 
one of the long-standing goals of natural lan-
guage processing, information retrieval, and 
artificial intelligence research. For a natural 
language question we would like to respond 
with a specific, accurate, and complete an-
swer that addresses the question. Although 
much progress has been made, answering 
complex, opinion, and even many factual 
questions automatically is still beyond the 
current state-of-the-art.  At the same time, the 
rise of popularity in social media and collabo-
rative content creation services provides a 
promising alternative to web search or com-
pletely automated QA. The explicit support 
for social interactions between participants, 
such as posting comments, rating content, and 
responding to questions and comments makes 
this medium particularly amenable to Ques-
tion Answering. Some very successful exam-
ples of Community Question Answering 
(CQA) sites are Yahoo! Answers 1  and 
Naver2, and Baidu Knows3. Yahoo! Answers 
alone has already amassed hundreds of mil-
lions of answers posted by millions of par-
ticipants on thousands of topics.  
The questions posted to such CQA portals 
are typically complex, subjective, and rely on 
human interpretation to understand the corre-
sponding information need. At the same time, 
the questions are also usually ill-phrased, 
vague, and often subjective in nature. Hence, 
analysis of the questions (and of the corre-
sponding user intent) in this setting is a par-
ticularly difficult task. At the same time, 
CQA content incorporates the relationships 
between questions and the corresponding an-
swers. Because of the various incentives pro-
vided by the CQA sites, answers posted by 
users tend to be, at least to some degree, re-
sponsive to the question. This observation 
suggests investigating whether the relation-
                                                 
1 http://answers.yahoo.com 
2 http://www.naver.com 
3 http://www.baidu.com 
937
ship between questions and answers can be 
exploited to improve automated analysis of the 
CQA content and the user intent behind the 
questions posted.  
     Figure 1: Example question (Yahoo! Answers) 
To this end, we exploit the ideas of co-
training, a general semi-supervised learning 
approach naturally applicable to cases of com-
plementary views on a domain, for example, 
web page links and content (Blum and 
Mitchell, 1998). In our setting, we focus on the 
complimentary views for a question, namely 
the text of the question and the text of the as-
sociated answers.  
As a concrete case-study of our approach 
we focus on one particularly important aspect 
of intent detection: the subjectivity orientation. 
We attempt to predict whether a question 
posted in a CQA site is subjective or objective. 
Objective questions are expected to be an-
swered with reliable or authoritative informa-
tion, typically published online and possibly 
referenced as part of the answer, whereas sub-
jective questions seek answers containing pri-
vate states, e.g. personal opinions, judgment, 
experiences. If we could automatically predict 
the orientation of a question, we would be able 
to better rank or filter the answers, improve 
search over the archives, and more accurately 
identify similar questions. For example, if a 
question is objective, we could try to find a 
few highly relevant articles as references, 
whereas if a question is subjective, useful an-
swers are not expected to be found in authori-
tative sources and tend to rank low with cur-
rent question answering and CQA search tech-
niques. Finally, learning how to identify ques-
tion orientation is a crucial component of in-
ferring user intent, a long-standing problem in 
web information access settings.  
In particular, we focus on the following re-
search questions: 
? Can we utilize the inherent structure of the 
CQA interactions and use the unlimited 
amounts of unlabeled data to improve classi-
fication performance, and/or reduce the 
amount of manual labeling required?  
? Can we automatically predict question sub-
jectivity in Community Question Answering 
? and which features are useful for this task 
in the real CQA setting? 
The rest of the paper is structured as fol-
lows. We first overview the community ques-
tion answering setting, and state the question 
orientation classification problem, which we 
use as the motivating application for our sys-
tem, more precisely. We then introduce our 
CoCQA system for semi-supervised classifi-
cation of questions and answers in CQA com-
munities (Section 3). We report the results of 
our experiments over thousands of real user 
questions in Section 4, showing the effective-
ness of our approach. Finally, we review re-
lated work in Section 5, and discuss our con-
clusions and future work in Section 6.
2 Question Orientation in CQA 
We first briefly describe the essential features 
of question answering communities such as 
Yahoo! Answers or Naver. Then, we formally 
state the problem addressed in this paper, and 
the features used for this setting. 
938
2.1 Community Question Answering  
Online social media content and associated 
services comprise one of the fastest growing 
segments on the Web. The explicit support for 
social interactions between participants, such 
as posting comments, rating content, and re-
sponding to questions and comments makes 
the social media unique. Question answering 
has been particularly amenable to social media 
by directly connecting information seekers 
with the community members willing to share 
the information. Yahoo! Answers, with mil-
lions of users and hundreds of millions of an-
swers for millions of questions is a very suc-
cessful implementation of CQA. 
For example, consider two example user-
contributed questions, objective and subjective 
respectively:  
Q1: What?s the difference between 
chemotherapy and radiation treat-
ments? 
Q2: Has anyone got one of those 
home blood pressure monitors? and 
if so what make is it and do you 
think they are worth getting? 
Figure 1 shows an example of community 
interactions in Yahoo! Answers around the 
question Q2 above. A user posted the question 
in the Health category of the site, and was able 
to obtain 10 responses from other users. Even-
tually, the asker chooses the best answer. Fail-
ing that, as shown in the example, the best an-
swer can also be chosen according to the votes 
from other users. Many of the interactions de-
pend on the perceived goals of the asker: if the 
participants interpret the question as subjec-
tive, they will tend to share their experiences 
and opinions, and if they interpret the question 
as objective, they may still share their experi-
ences but may also provide more factual in-
formation. 
2.2 Problem Definition 
We now state our problem of question orienta-
tion more precisely. We consider question ori-
entation from the perspective of user goals: 
authors of objective questions request authori-
tative, objective information (e.g., published 
literature or expert opinion), whereas authors 
of subjective questions seek opinions or judg-
ments of other users in the community.  We 
state our problem as follows. 
 
Question Subjectivity Problem: Given a 
question Q in a question answering com-
munity, predict whether Q has objective 
or subjective orientation, based on ques-
tion and answer text as well as the user 
and community feedback. 
3 CoCQA: A Co-Training Frame-
work over Questions and Answers 
In the CQA setting we could easily obtain 
thousands or millions of unlabeled examples 
from the online CQA archives. On the other 
hand, it is difficult to create a labeled dataset 
with a reasonable size, which could be used 
to train a perfect classifier to analyze ques-
tions from different domains and sub-
domains. Therefore, semi-supervised learning 
(Chapelle et al, 2006) is a natural approach 
for this setting. 
Intuitively, we can consider the text of the 
question itself or answers to it. In other 
words, we have multiple (at least two) natural 
views of the data, which satisfies the condi-
tions of the co-training approach (Blum and 
Mitchell, 1998). In co-training, two separate 
classifiers are trained on two sets of features, 
respectively. By automatically labeling the 
unlabeled examples, these two classifiers it-
eratively ?teach? each other by giving their 
partners a newly labeled data that they can 
predict with high confidence. Based on the 
original co-training algorithm in (Blum and 
Mitchell, 1998) and other implementations, 
we develop our algorithm CoCQA shown in 
Figure 2. 
At Steps 1 and 2, the K examples are com-
ing from different feature spaces, and each 
category (for example, Subjective and Objec-
tive) has top Kj most confident examples cho-
sen, where Kj corresponds to the distribution 
of class in the current set of labeled examples 
L. CoCQA will terminate when the incre-
ments of both classifiers are less than a speci-
fied threshold X or the maximum number of 
iterations are exceeded. Following the co-
training approach, we include the most confi-
dently predicted examples as additional ?la-
beled? data. The SVM output margin value 
was used to estimate confidence; alternative 
939
methods (including reliability of this confi-
dence prediction) could further improve per-
formance, and we will explore these issues in 
future work. Finally, the next question is how 
to estimate classification performance with 
training data. For each pass, we randomly split 
the original training data into N folds (N=10 in 
our experiments), and keep one part for valida-
tion and the rest, augmented with the newly 
added examples, as the expanded training set. 
After CoCQA terminates, we obtain two 
classifiers. When a new example arrives, we 
will classify it with these two classifiers based 
on both of the feature sets, and combine the 
predictions of these two classifiers. We ex-
plored two strategies to make the final deci-
sion based on the confidence values given by 
two classifiers: 
? Choose the class with higher confidence 
? Multiply the confidence values, and 
choose the class that has the highest 
product. 
We found the second heuristic to be more 
effective than the first in our experiments. As 
the base classifier we use SVM in the current 
implementation, but other classifiers could be 
incorporated as well. 
4 Experimental Evaluation  
We experiment with supervised and semi-
supervised methods on a relatively large data 
set from Yahoo! Answers. 
4.1 Datasets 
To our knowledge, there is no standard data-
set of real questions and answers posted by 
online users, labeled for subjectivity orienta-
tion. Hence, we had to create a dataset our-
selves. To create our dataset, we downloaded 
more than 30,000 resolved questions from 
each of the following top-level categories of 
Yahoo! Answers: Arts, Education, Health, 
Science, and Sports. We randomly chose 200 
questions from each category to create a raw 
dataset with 1,000 questions total. Then, we 
labeled the dataset with annotators from the 
Amazon?s Mechanical Turk service4.  
For annotation, each question was judged 
by 5 Mechanical Turk workers who passed a 
qualification test of 10 questions (labeled by 
ourselves) with at least 9 of them correctly 
marked. The qualification test was required to 
ensure that the raters were sufficiently com-
petent to make reasonable judgments. We 
grouped the tasks into 25 question batches, 
where the whole batch was submitted as the 
Mechanical Turk?s Human Intelligence Task 
(HIT). The batching of questions was done to 
easily detect the ?random? ratings produced 
by irresponsible workers. That is, each 
worker rated a batch of 25 questions.  
While precise definition of subjectivity is 
elusive, we decided to take the practical per-
spective, namely the "majority" interpreta-
tion. The annotators were instructed to guess 
orientation according to how the question 
would be answered by most people. We did 
not deal with multi-part questions: if any part 
of question was subjective, the whole ques-
tion was labeled as subjective. The gold stan-
dard was thus derived with the majority strat-
egy, followed by manual inspection as a ?san-
ity check?. At this stage we removed 22 ques-
tions with undeterminable meaning, including 
gems such as ?Upward Soccer 
                                                 
4 http://www.mturk.com 
Figure 2: Algorithm CoCQA: A co-training algo-
rithm for exploiting redundant feature sets in 
community question answering. 
Input: 
? FQ and FA are Question and Answer feature views 
? CQ and CA are classifiers trained on FQ and FA  respec-
tively 
? L is a set of labeled training examples 
? U is a set of unlabeled examples 
? K: Number of unlabeled examples to choose on  
each iteration 
? X:  the threshold for  increment 
? R:  the maximal number of iterations 
Algorithm CoCQA 
1. Train CQ ,0 on L: FQ , and record resulting   ACCQ,0 
2. Train CA ,0 on L: FA , and record resulting  ACCA ,0 
3. for j=1 to R do: 
        Use CQ,j-1 to predict labels for U and choose 
               top K items with highest confidence ? EQ, , j-1 
        Use CA,j-1 to predict labels for U and  choose  
                top K items with highest confidence ? EA, , j-1 
        Move examples EQ, , j-1 U EA, , j-1 ? L 
        Train CQ,j on L: FQ and record training  ACCQ,j 
        Train CA,j on L: FA and record training  ACCA,j 
             if Max(?ACCQ,j, ? ACCA,j) < X break 
  
4.     return final classifiers CQ,j ? CQ and CA,j ? CA
940
Shorts??5 and ?1+1=?fdgdgdfg??6. Fi-
nally, we create a labeled dataset consisting of 
978 resolved questions, available online7.  
 
 
Num. of 
SUB. Q 
Num. of 
OBJ. Q 
Total 
Num. 
Annotator
agreement
Arts 137 (70%) 58 (30%) 195 0.841 
Education 127 (64%) 70 (36%) 197 0.716 
Health 125 (64%) 69 (36%) 194 0.833 
Science 103 (52%) 94 (48%) 197 0.618 
Sports 154 (79%) 41 (21%) 195 0.877 
Total 646 (66%) 332 (34%) 978 0.777 
Table 1: Labeled dataset statistics. 
 
Table 1 reports the statistics of the annotated 
dataset. The overall inter-annotator percentage 
agreement between Mechanical Turk workers 
and final annotation is 0.777, indicating that 
the task is difficult, but feasible for humans to 
annotate manually.  
The statistics of our labeled sample show 
that the vast majority of the questions in all 
categories except for Science are subjective in 
nature. The relatively high ratio of subjective 
questions in the Science category is surprising. 
However, we find that users often post polem-
ics and statements instead of questions, using 
CQA as a forum to share their opinions on 
controversial topics. Overall, we were struck 
by the expressed need in Subjective informa-
tion, even for categories such as Health and 
Education, where objective information would 
intuitively seem more desirable.  
4.2 Features Used in Experiments 
For the subjectivied experiments to follow, 
we attempt to capture the linguistic 
characteristics identified in previous work 
(Section 5) in a lightweight and robust manner, 
due to the informal and noisy nature of CQA. 
In particular, we use the following feature 
classes, computed separately over question and 
answer content: 
? Character 3-grams  
? Words 
? Word with Character 3-grams 
? Word n-grams (n<=3, i.e. Wi, WiWi+1,  
WiWi+1Wi+2) 
                                                 
5http://answers.yahoo.com/question/?qid=20060829074901AA
DBRJ4  
6 http://answers.yahoo.com/question/?qid=1006012003651  
7 Available at http://ir.mathcs.emory.edu/datasets/. 
? Word and POS n-gram (n<=3, i.e. Wi, 
WiWi+1, Wi POSi+1, POSiWi+1 , 
POSiPOSi+1, etc.).  
We use the character 3-grams features to 
overcome spelling errors and problems of ill-
formatted or ungrammatical questions, and 
the POS information to capture common pat-
terns across domains, as words, especially the 
content words, are quite diverse in different 
topical domains. For word and character 3-
gram features, we consider two different ver-
sions: case-sensitive and case-insensitive. 
Case-insensitive features are assumed to be 
helpful for mitigating negative effects of ill-
formatted text. 
Moreover, we experimented with three 
term weighting schemes: Binary, TF, and 
TF*IDF. Term frequency (TF) exhibited bet-
ter performance in our development experi-
ments, so we use this weighting scheme for 
all the experiments in Section 4. Regarding 
features: both words and structure of the text 
(e.g., word order) can be used to infer subjec-
tivity. Therefore, the features we employ, 
such as words and word n-grams, are ex-
pected to be useful as a (coarse) proxy to 
grammatical and phrase features. Unlike tra-
ditional work on news-like text, the text of 
CQA and has poor spelling, grammar, and 
heavily uses non-standard abbreviations, 
hence our decision to use character n-grams.  
4.3 Experimental Setting 
Metrics: Since the prediction  on both sub-
jective questions and objective questions is 
equally important, we use the macro-
averaged F1 measure as the evaluation met-
ric. This is computed as the macro average of 
F1 measures computed for the Subjective and 
Objective classes individually. The F1 meas-
ure for either class is computed 
as
RecallPrecision 
Recall Precision 2
 
+
?? . 
 
Methods compared: We compare our ap-
proach with both the base supervised learning, 
as well as GE, a state-of-the-art semi-
supervised method:  
? Supervised: we use the LibSVM im-
plementation (Chang and Lin, 2001) 
with linear kernel.   
941
? GE: This is a state-of-the-art semi-
supervised learning algorithm, General-
ized Expectation (GE), introduced in 
(McCallum et al, 2007) that incorporates 
model expectations into the objective 
functions for parameter estimation. 
? CoCQA: Our method (Section 3).  
 
For semi-supervised learning experiments, 
we selected a random subset of 2,000 unla-
beled questions for each of the topical catego-
ries, for the total of 10,000 unlabeled questions. 
4.4 Experimental Results 
First we report the performance of our Super-
vised baseline system with a variety of fea-
tures, reporting the average results of 5-fold 
cross validation. Then we investigate the per-
formance to our new CoCQA framework under 
a variety of settings. 
4.4.1 Supervised Learning 
Table 2 reports the classification perform-
ance for varying units of representation (e.g., 
question text vs. answer text) and the varying 
feature sets. We used case-insensitive features 
and TF (term frequency within the text unit) as 
feature weights, as these two settings achieved 
the best results in our development experi-
ments. The rows show performance consider-
ing only the question text (question), the best 
answer (best_ans), text of all answers to a 
question (all_ans), the text of the question and 
the best answer (q_bestans), and the text of 
the question with all answers (q_allans), re-
spectively.  In particular, using the words in 
the question alone achieves F1 of 0.717, com-
pared to using words in the question and the 
best answers text (F1 of 0.695). For compari-
son, a na?ve baseline that always guesses the 
majority class (Subjective) obtains F1 of 0.398. 
With character 3-gram, our system achieves 
performance comparable with words as fea-
tures, but combining them together does not 
improve performance. We observe a slight 
gain with more complicated features, e.g. word 
n-gram, or word and POS n-grams, but the 
gain is not significant, and hence not worth the 
increased complexity of the feature generation. 
Finally, combining question text with answer 
text does not improve performance.  
Interestingly, the best answer itself is not as 
effective as the question for subjectivity 
analysis, nor is using all of the answers sub-
mitted. One possible reason is that approxi-
mately 40% of the best answers were chosen 
by the community and not the asker herself, 
are hence not necessarily representative of the 
asker intent.  
 
Feature
set
 
Unit 
Char 
3-
gram 
Word 
Word+ 
Char 
3-gram 
Word 
n-gram 
(n<=3) 
Word 
POS 
n-gram
(n<=3) 
question 0.700 0.717 0.694 0.716 0.720 
best_ans 0.587 0.597 0.578 0.580 0.565 
all_ans 0.603 0.628 0.607 0.648 0.630 
q_bestans 0.681 0.695 0.662 0.687 0.712 
q_allans 0.679 0.677 0.676 0.708 0.689 
Na?ve (majority class) baseline:  0.398 
Table 2. Performance of predicting question 
orientation on the mixed dataset with varying 
feature sets (Supervised). 
 
Table 3 reports the supervised subjectivity 
classification performance for each question 
category with word features. The overall clas-
sification results are significantly lower com-
pared to training and testing on the mixture of 
the questions drawn from all categories, 
likely caused by the small amount of labeled 
training data for each category. Another pos-
sibility is that the subjectivity clues are not 
topical and hence are not category dependent, 
with the possible exception of the questions 
in the Health domain.  
 
Category Arts Edu. Health Science Sports
F1 0.448 0.572 0.711 0.647 0.441 
Table 3. Experiment results on sub-categories 
with supervised SVM (q_bestans features).  
 
As words are simple and effective features 
in this experiment, we will use them in the 
subsequent experiments. Furthermore, the 
feature set using the words in the question 
with best answer together (q_bestans) exhibit 
higher performance than question with all 
answers (q_allans). Thus, we will only con-
sider questions and best answers in the fol-
lowing experiments with GE and CoCQA. 
4.4.2 Semi-Supervised Learning 
We now focus on investigating the effec-
tiveness of CoCQA, our co-training-based 
framework for community question answer-
ing analysis. Table 4 summarizes the main 
942
results of this section. The values for CoCQA 
are derived with the parameter settings: K=100, 
X=0.001. These optimal settings are chosen 
after comprehensive experiments with differ-
ent combinations, described later in this sec-
tion. GE does not exhibit a significant im-
provement over Supervised. In contrast, 
CoCQA performs significantly better than the 
purely supervised method, with F1 of 0.745 
compared to the F1 of 0.717 for Supervised. 
While it may seem surprising that a semi-
supervised method outperforms a supervised 
one, note that we use all of the available la-
beled data as provided to the Supervised 
method, as well as a large amount of unlabeled 
data, that is ultimately responsible for the per-
formance improvement. 
  
Features 
Method 
Question Question+ Best Answer 
Supervised 0.717 0.695 
GE 0.712 (-0.7%) 0.717 (+3.2%) 
CoCQA 0.731 (+1.9%) 0.745 (+7.2%) 
Table 4. Performance of CoCQA, GE, and Su-
pervised with the same feature and data settings.  
 
As an added advantage, CoCQA approach is 
also practical. In a realistic application, we 
have two different situations: offline and 
online. With online processing, we may not 
have best answers available to predict ques-
tion?s orientation, whereas we can employ in-
formation from best answers in offline setting. 
Co-training is a solution that is applicable to 
both situations. With CoCQA, we have two 
classifiers using the question text and the best 
answer text, respectively. We can use both of 
them to obtain better results in the offline set-
ting, while in online setting, we can use the 
text of the question alone. In contrast, GE may 
not have this flexibility.  
We now analyze the performance of 
CoCQA under a variety of settings to derive 
optimal parameters and to better understand 
the performance. Figure 3 reports the perform-
ance of CoCQA with varying the K parameter 
from 20 to 200. In this experiment, we fix X to 
be 0.001. The combination of question and 
best answer is superior to that of question and 
all answers. When K is 100, the system obtains 
the best result, 0.745.  
Figure 4 reports the number of co-training 
iterations needed to converge to optimal per-
formance. After 13 iterations (and 2500 unla-
beled examples added), CoCQA achieves op-
timal performance, and eventually terminates 
after an additional 3 iterations. While a vali-
dation set should have been used for CoCQA 
parameter tuning, Figures 3 and 4 indicate 
that CoCQA is not sensitive to the specific 
parameter settings. In particular, we observe 
that any K is greater than 100, and for any 
number of iterations R is greater than 10, 
CoCQA exhibits in effectively equivalent per-
formance. 
 
0.64
0.65
0.66
0.67
0.68
0.69
0.7
0.71
0.72
0.73
0.74
0.75
0.76
20 40 60 80 100 120 140 160 180 200K: # labeled examples added on each 
co-training iteration
F
1
CoCQA(Question and Best Answer)
Supervised Q_bestans
CoCQA(Question and All Answers)
Supervised Q_allans
 
Figure 3: Performance of CoCQA for varying 
the K (number of examples added on each it-
eration of co-training). 
 
Figure 5 reports the performance of 
CoCQA for varying the number of labeled 
examples from 50 to 400 (that is, up to 50% 
of the available labeled training data). Note 
that for this comparison we use the same fea-
ture sets  (words in question and best answer 
text), but using only the (varying) fractions of 
the manually labeled data. Surprisingly, 
CoCQA exhibits comparable performance of 
F1=0.685 with only 200 labeled examples are 
used, compared to the F1=0.695 for Super-
vised with all 800 labeled training examples 
on this feature set. In other words, CoCQA is 
able to achieve comparable performance to 
supervised SVM with only one quarter of the 
labeled training data. 
 
943
0.71
0.72
0.73
0.74
0.75
161377776666
# co-training iterations
F
1
0
500
1000
1500
2000
2500
3000
3500
T
ot
al
 #
 U
n
la
b
el
ed
 A
d
d
ed
CoCQA (Question + Best Answer)
Supervised
Total # Unlabeled
 
Figure 4: Performance and the total number of 
unlabeled examples added for varying number 
of co-training iterations (K=100, using q_bestans 
features) 
 
 
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
0.7
0.72
50 100 150 200 250 300 350 400
# of labeled data used
F1
CoCQA (Question + Best Answer)
Supervised Q_Best Ans
 
Figure 5: Performance of CoCQA with varying 
number of labeled examples used, compared to 
Supervised method, on same features. 
5 Related Work 
Question analysis, especially question classifi-
cation, has been long studied in the question 
answering research community. However, 
most of the previous research primarily con-
sidered factual questions, with the notable ex-
ception of the most recent TREC opinion QA 
track. Furthermore, the questions were specifi-
cally designed for benchmark evaluation. A 
related thread of research considered deep 
analysis of the questions (and corresponding 
sentences) by manually classifying questions 
along several orientation dimensions, notably 
(Stoyanov et al, 2005).  In contrast, our work 
focuses on analyzing real user questions 
posted in a question answering community. 
These questions are often complex or subjec-
tive, and are typically difficult to answer 
automatically as the question author probably 
was not able to find satisfactory answers with 
quick web search. 
Automatic complex question answering has 
been an active area of research, ranging from 
simple modification to factoid QA techniques 
(e.g., Soricut and Brill, 2003) to knowledge 
intensive approaches for specific domains 
(e.g., Harabagiu et al 2001, Fushman and Lin 
2007). However, the technology does not yet 
exist to automatically answer open-domain 
complex and subjective questions. While 
there has been some recent research (e.g., 
Agichtein et al 2008, Bian et al 2008) on 
retrieving high quality answers from CQA 
archives, the subjectivity orientation of the 
questions has not been considered as a feature 
for ranking.  
A related corresponding problem is com-
plex QA evaluation. Recent efforts at auto-
matic evaluation show that even for well-
defined, objective, complex questions, 
evaluation is extremely labor-intensive and 
introduces many challenges (Lin and 
Fushman 2006, Lin and Zhang 2007). As part 
of our contribution we showed that it is feasi-
ble to use the Amazon Mechanical Turk ser-
vice for evaluation by combining large degree 
of annotator redundancy (5 annotators per 
question) with more sparse but higher-quality 
expert annotation. 
The problem of automatic subjective ques-
tion answering has recently started to be ad-
dressed in the question answering commu-
nity, most recently as the first opinion QA 
track in (Dang et al, 2007). Unlike the con-
trolled TREC opinion track (introduced in 
2007), many of the questions in Yahoo! An-
swers community are inherently subjective, 
complex, ill-formed, or all of the above. To 
our knowledge, this paper is the first large-
scale study of subjective/objective orientation 
of information needs, and certainly the first in 
the CQA environment. 
A closely related research thread is subjec-
tivity analysis at document and sentence 
level. For example, reference (Yu, H., and 
Hatzivassiloglou, V. 2003; Somasundaran et 
944
al. 2007) attempted to classify sentences into 
those reporting facts or opinions. Also related 
is research on sentiment analysis (e.g., Pang et 
al., 2004) where the goal is to classify a sen-
tence or text fragment as being overall positive 
or negative. More generally, (Wiebe et al 
2004) and subsequent work focused on the 
analysis of subjective language in narrative 
text, primarily news. Our problem is quite dif-
ferent in the sense that we are trying to iden-
tify the orientation of a question. Nevertheless, 
our baseline method is similar to the methods 
and features used for sentiment analysis, and 
one of our contributions is evaluating the use-
fulness of the established features and tech-
niques to the new CQA setting. 
In order to predict question orientation, we 
build on co-training, one of the known semi-
supervised learning techniques. Many models 
and techniques have been proposed for classi-
fication, including support vector machines, 
decision tree based techniques, boosting-based 
techniques, and many others. We use LIBSVM 
(Chang and Lin, 2001) as a robust implemen-
tation of SVM algorithms. 
In summary, while we draw on many tech-
niques in question answering, natural language 
processing, and text classification, our work 
differs from previous research in that a) de-
velop a novel co-training based algorithm for 
question and answer classification; b) we ad-
dress a relatively new problem of automatic 
question subjectivity prediction; c) demon-
strate the effectiveness of our techniques in the 
new CQA setting and d) explore the character-
istics unique to CQA ? while showing good 
results for a quite difficult task. 
6 Conclusions 
We presented CoCQA, a co-training frame-
work for modeling the textual interactions in 
question answer communities. Unlike previous 
work, we have focused on real user questions 
(often noisy, ungrammatical, and vague) sub-
mitted in Yahoo! Answers, a popular commu-
nity question answering portal. We demon-
strated CoCQA for one particularly important 
task of automatically identifying question sub-
jectivity orientation, showing that CoCQA is 
able to exploit the structure of questions and 
corresponding answers. Despite the inherent 
difficulties of subjectivity analysis for real user 
questions, we have shown that by applying 
CoCQA to this task we can significantly im-
prove prediction performance, and substan-
tially reduce the size of the required training 
data, while outperforming a general state-of-
the-art semi-supervised algorithm that does 
not take advantage of the CQA characteris-
tics.  
In the future we plan to explore more so-
phisticated features such semantic concepts 
and relationships (e.g., derived from WordNet 
or Wikipedia), and richer syntactic and lin-
guistic information. We also plan to explore 
related variants of semi-supervised learning 
such as co-boosting methods to further im-
prove classification performance. We will 
also investigate other applications of our co-
training framework to tasks such as sentiment 
analysis in community question answering 
and similar social media content. 
Acknowledgments 
This research was partially supported by the 
Emory University Research Committee 
(URC) grant, and by the Emory College Seed 
grant. We thank the Yahoo! Answers team for 
providing access to the Answers API, and 
anonymous reviewers for their excellent sug-
gestions. 
References 
Agichtein, E., Castillo, C., Donato, D., Gionis, A., and 
Mishne, G. 2008. Finding High-Quality Content in 
Social Media with an Application to Community-
Based Question Answering. WSDM2008  
Bian, J., Liu, Y., Agichtein, E., and H. Zha. 2008, to 
appear. Finding the Right Facts in the Crowd: Fac-
toid Question Answering over Social Media, Pro-
ceedings of the Inter-national World Wide Web Con-
ference (WWW), 2008  
Blum, A., and Mitchell, T. 1998. Combining Labeled 
and Unlabeled Data with Co-Training. Proc. of the 
Annual Conference on Computational Learning 
Theory.  
Chang, C. C. and Lin, C. J. 2001. LIBSVM : a library 
for support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm.  
Chapelle, O., Scholkopf, B., and Zien, A. 2006. Semi-
supervised Learning. The MIT Press, Cambridge, 
Mas-sachusetts.  
Dang, H. T., Kelly, D., and Lin, J. 2007. Overview of 
the TREC 2007 Question Answering track. In Pro-
ceedings of TREC-2007.  
945
Demner-Fushman, D. and Lin, J. 2007. Answering clini-
cal questions with knowledge-based and statistical 
techniques. Computational Linguistics, 33(1):63?103.  
Harabagiu, S., Moldovan, D., Pasca, M., Surdeanu, M. , 
Mihalcea, R., Girju, R., Rusa, V., Lacatusu, F., 
Morarescu, P., and Bunescu, R. 2001. Answering 
Complex, List and Context Questions with LCC's 
Question-Answering Server. In Proc. of TREC 2001.  
Lin, J. and Demner-Fushman, D. 2006. Methods for 
automatically evaluating answers to complex ques-
tions. In-formation Retrieval, 9(5):565?587  
Lin, J. and Zhang, P. 2007. Deconstructing nuggets: the 
stability and reliability of complex question answering 
evaluation. In Proceedings of the 30th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 327?334.  
Mann, G., and McCallum, A. 2007. Simple, Robust, 
Scalable Semi-supervised Learning via Expectation 
Regularization. Proceedings of ICML 2007.  
Pang, B., and Lee, L. 2004. A Sentimental Education: 
Sen-timent Analysis Using Subjective Summarization 
Based on Minimum Cuts. In Proc. of ACL.  
Prager, J. 2006. Open-Domain Question-Answering. 
Foundations and Trends in Information Retrieval.  
Sindhwani, V., Keerthi, S. 2006. Large Scale Semi-
supervised Linear SVMs. Proceedings of SIGIR 2006.  
Somasundaran, S., Wilson, T., Wiebe, J. and Stoyanov, 
V. 2007. QA with Attitude: Exploiting Opinion Type 
Analysis for Improving Question Answering in On-
line Discussions and the News. In proceedings of In-
ternational Conference on Weblogs and Social Media 
(ICWSM-2007).  
Soricut, R. and Brill, E. 2004. Automatic question an-
swering: Beyond the factoid. Proceedings of HLT-
NAACL.  
Stoyanov, V., Cardie, C., and Wiebe, J. 2005. Multi-
Perspective question answering using the OpQA cor-
pus. In Proceedings of EMNLP.  
Tri, N. T., Le, N. M., and Shimazu, A. 2006. Using 
Semi-supervised Learning for Question Classification. 
In Proceedings of ICCPOL-2006.  
Wiebe, J., Wilson, T., Bruce R., Bell M., and Martin M. 
2004. Learning subjective language. Computational 
Linguistics, 30 (3).  
Yu, H., and Hatzivassiloglou, V. 2003. Towards Answer-
ing Opinion Questions: Separating Facts from Opin-
ions and Identifying the Polarity of Opinion Sentences. 
In Proceedings of EMNLP-2003.  
Zhang, D., and Lee, W.S. 2003. Question Classification 
Using Support Vector Machines. Proceedings of the 
26th Annual International ACM SIGIR Conference on 
Re-search and Development in Information Retrieval.  
Zhu, X. 2005. Semi-supervised Learning Literature 
Survey. Technical Report 1530, Computer Sciences, 
University of Wisconsin-Madison. 
 
946
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 73?78,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Multilingual Semantic Role Labeling 
 
Baoli Li, Martin Emms, Saturnino Luz, Carl Vogel 
Department of Computer Science 
Trinity College Dublin 
Dublin 2, Ireland 
{baoli.li,mtemms,luzs,vogel}@cs.tcd.ie 
 
 
 
Abstract 
This paper describes the multilingual semantic 
role labeling system of Computational Lin-
guistics Group, Trinity College Dublin, for the 
CoNLL-2009 SRLonly closed shared task. 
The system consists of two cascaded compo-
nents: one for disambiguating predicate word 
sense, and the other for identifying and classi-
fying arguments. Supervised learning tech-
niques are utilized in these two components. 
As each language has its unique characteris-
tics, different parameters and strategies have 
to be taken for different languages, either for 
providing functions required by a language or 
for meeting the tight deadline. The system ob-
tained labeled F1 69.26 averaging over seven 
languages (Catalan, Chinese, Czech, English, 
German, Japanese, and Spanish), which ranks 
the system fourth among the seven systems 
participating the SRLonly closed track. 
1 Introduction 
Semantic role labeling, which aims at computa-
tionally identifying and labeling arguments of 
predicate words, has become a leading research 
problem in computational linguistics with the ad-
vent of various supporting resources (e.g. corpora 
and lexicons) (M?rquez et al, 2008). Word seman-
tic dependencies derived by semantic role labeling 
are assumed to facilitate automated interpretation 
of natural language texts. Moreover, techniques for 
automatic annotation of semantic dependencies can 
also play an important role in adding metadata to 
corpora for the purposes of machine translation 
and speech processing. We are currently investi-
gating such techniques as part of our research into 
integrated language technology in the Center for 
Next Generation Localization (CNGL, 
http://www.cngl.ie). The multilingual nature of the 
CoNLL-2009 shared task on syntactic and seman-
tic dependency analysis, which includes Catalan, 
Chinese, Czech, English, German, Japanese, and 
Spanish (Haji? et al, 2009), makes it a good test-
bed for our research. 
We decided to participate in the CoNLL-2009 
shared task at the beginning of March, signed the 
agreement for getting the training data on March 
2nd, 2009, and obtained all the training data (espe-
cially the part from LDC) on March 4th, 2009. Due 
to the tight time constraints of the task, we chose to 
use existing packages to implement our system. 
These time constraints also meant that we had to 
resort to less computationally intensive methods to 
meet the deadline, especially for some large data-
sets (such as the Czech data). In spite of these dif-
ficulties and resource limitations, we are proud to 
be among the 21 teams who successfully submitted 
the results1. 
As a new participant, our goals in attending the 
CoNLL-2009 SRLonly shared task were to gain 
more thorough knowledge of this line of research 
and its state-of-the-art, and to explore how well a 
system quickly assembled with existing packages 
can fare at this hard semantic analysis problem.  
Following the successful approaches taken by 
the participants of the CoNLL-2008 shared task 
(Surdeanu et al, 2008) on monolingual syntactic 
and semantic dependency analysis, we designed 
and implemented our CoNLL-2009 SRLonly sys-
tem with pipeline architecture. Two main compo-
nents are cascaded in this system: one is for 
disambiguating predicate word sense 2 , and the 
other for identifying and classifying arguments for 
                                                          
1
 According to our correspondence with Dr. Jan Haji?, totally 
31 teams among 60 registered ones signed and got the evalua-
tion data. 
2
 As predicate words are marked in the CoNLL-2009 datasets, 
we don?t need to identify predicate words. 
73
predicate words. Different supervised learning 
techniques are utilized in these two components. 
For predicate word sense disambiguation (WSD), 
we have experimented with three algorithms: SVM, 
kNN, and Na?ve Bayes. Based on experimental 
results on the development datasets, we chose 
SVM and kNN to produce our submitted official 
results. For argument identification and classifica-
tion, we used a maximum entropy classifier for all 
the seven datasets. As each language has its unique 
characteristics and peculiarities within the dataset, 
different parameters and strategies have to be taken 
for different languages (as detailed below), either 
for providing functions required by a language or 
for meeting the tight deadline. Our official submis-
sion obtained 69.26 labeled F1 averaging over the 
seven languages, which ranks our system fourth 
among the seven systems in the SRLonly closed 
track. 
The rest of this paper is organized as follows. 
Section 2 discusses the first component of our sys-
tem for predicate word sense disambiguation. Sec-
tion 3 explains how our system detects and 
classifies arguments with respect to a predicate 
word. We present experiments in Section 4, and 
conclude in Section 5. 
2 Predicate Word Sense Disambiguation 
This component tries to determine the sense of a 
predicate word in a specific context. As a sense of 
a predicate word is often associated with a unique 
set of possible semantic roles, this task is also 
called role set determination. Based on the charac-
teristics of different languages, we take different 
strategies in this step, but the same feature set is 
used for different languages. 
2.1 Methods 
Intuitively, each predicate word should be treated 
individually according to the list of its possible 
senses. We therefore designed an initial solution 
based on the traditional methods in WSD: repre-
sent each sense as a vector from its definition or 
examples; describe the predicate word for disam-
biguation as a vector derived from its context; and 
finally output the sense which has the highest simi-
larity with the current context. We also considered 
using singular value decomposition (SVD) to over-
come the data sparseness problem. Unfortunately, 
we found this solution didn?t work well in our pre-
liminary experiments. The main problem is that the 
definition of each sense of a predicate word is not 
available. What we have is just a few example con-
texts for one sense of a predicate word, and these 
contexts are often not informative enough for 
WSD. On the other hand, our limited computing 
resources could not afford SVD operation on a 
huge matrix. 
We finally decided to take each sense tag as a 
class tag across different words and transform the 
disambiguation problem into a normal multi-class 
categorization problem. For example, in the Eng-
lish datasets, all predicates with ?01? as a sense 
identifier were counted as examples for the class 
?01?. With this setting, a predicate word may be 
assigned an invalid sense tag. It is an indirect solu-
tion, but works well. We think there are at least 
two possible reasons: firstly, most predicate words 
take their popular sense in running text. For exam-
ple, in the English dataset (training and develop-
ment), 160,477 of 185,406 predicate occurrences 
(about 86.55%) take their default sense ?01?. Sec-
ondly, predicates may share some common role 
sets, even though their senses may not be exactly 
the same, e.g. ?tell? and ?inform?. 
Unlike the datasets in other languages, the Japa-
nese dataset doesn?t have specialized sense tags 
annotated for each predicate word, so we simply 
copy the predicted lemma of a predicate word to its 
PRED field. For other datasets, we derived a train-
ing sample for each predicate word, whose class 
tag is its sense tag. Then we trained a model from 
the generated training data with a supervised learn-
ing algorithm, and applied the learned model for 
predicting the sense of a predicate word. This is 
our base solution. 
When transforming the datasets, the Czech data 
needs some special processing because of its 
unique annotation format. The sense annotation for 
a predicate word in the Czech data does not take 
the form ?LEMMA.SENSE?. In most cases, no 
specialized sense tags are annotated. The PRED 
field of these words only contains ?LEMMA?. In 
other cases, the disambiguated senses are anno-
tated with an internal representation, which is 
given in a predicate word lexicon. We decomposed 
the internal representation of each predicate word 
into two parts: word index id and sense tag. For 
example, from ?zv??en? v-w10004f2? we know ?v-
w10004? is the index id of word ?zv??en??, and 
?f2? is its sense tag. We then use these derived 
74
sense tags as class tags and add a class tag ?=? for 
samples without specialized sense tag. 
For each predicate word, we derive a vector de-
scribing its context and attributes, each dimension 
of which corresponds to a feature. We list the fea-
ture types in the next subsection. Features appear-
ing only once are removed. The TF*IDF weighting 
schema is used to calculate the weight of a feature. 
Three different algorithms were tried during the 
development period: support vector machines 
(SVM), distance-weighted k-Nearest Neighbor 
(kNN) (Li et al, 2004), and Na?ve Bayes with mul-
tinomial model (Mccallum and Nigam, 1998). As 
to the SVM algorithm, we used the robust 
LIBSVM package (Chang and Lin, 2001), with a 
linear kernel and default values for other parame-
ters. The algorithms achieving the best results in 
our preliminary experiments are chosen for differ-
ent languages: SVM for Catalan, Chinese, and 
Spanish; kNN for German (k=20). 
We used kNN for English (k=20) and Czech 
(k=10) because we could not finish training with 
SVM on these two datasets in limited time. Even 
with kNN algorithm, we still had trouble with the 
English and Czech datasets, because thousands of 
training samples make the prediction for the 
evaluation data unacceptably slow. We therefore 
had to further constrain the search space for a new 
predicate word to those samples containing the 
same predicate word. If there are not samples con-
taining the same predicate word in the training data, 
we will assign it the most popular sense tag (e.g. 
?01? for English). 
How to use the provided predicate lexicons is a 
challenging issue. Lexicons for different languages 
take different formats and the information included 
in different lexicons is quite different. We derived 
a sense list lexicon from the original predicate 
lexicon for Chinese, Czech, English, and German. 
Each entry in a sense list lexicon contains a predi-
cate word, its internal representation (especially for 
Czech), and a list of sense tags that the predicate 
can have. Then we obtained a variant of our base 
solution, which uses the sense list of a predicate 
word to filter impossible senses. It works as fol-
lows: 
- Disambiguate a new predicate with the base 
solution; 
- Choose the most possible sense from all the 
candidate senses obtained in step 1: if the 
base classifier doesn?t output a vector of 
probabilities for classes, only check 
whether the predicted one is a valid sense 
for the predicate; 
- If there is not a valid sense for a new predi-
cate (including the cases where the predi-
cate does not have an entry in the sense list 
lexicon), output the most popular sense tag; 
Unfortunately, preliminary experiments on the 
German and Chinese datasets didn?t support to in-
clude such a post-processing stage. The perform-
ance with this filtering became a little worse. 
Therefore, we decided not to use it generally, but 
one exception is for the Czech data. 
With kNN algorithm, we can greatly reduce the 
time for training the Czech data, but we do have 
problem with prediction, as there are totally 
469,754 samples in the training dataset. It?s a time-
consuming task to calculate the similarities be-
tween a new sample and all the samples in the 
training dataset to find its k nearest neighbors, thus 
we have to limit the search space to those samples 
that contain the predicate word for disambiguation. 
To process unseen predicate words, we used the 
derived sense list lexicon: if a predicate word for 
disambiguation is out of the sense list lexicon, we 
simply copy its predicted lemma to the PRED field; 
if no sample in the training dataset has the same 
predicate word, we take its first possible sense in 
the sense list lexicon. With this strategy, our sys-
tem can process the huge Czech dataset in short 
time. 
2.2 Features 
The features we used in this step include3: 
 
a. [Lemma | (Lemma with POS)] of all words in the sen-
tence; 
b. Attributes of predicate word, which is obtained from 
PFEAT field by splitting the field at symbol ?|? and 
removing the invalid attribute of ?*?; 
c. [Lemma | POS] bi-grams of predicate word and its 
[previous | following] one word; 
d. [Lemma | POS] tri-grams of predicate word and its 
[previous | following] two words; 
e. [Lemma | (Lemma with POS)] of its most [left | right] 
child; 
f. [(Lemma+Dependency_Relation+Lemma) | (POS 
+Dependency_Relation+POS)] of predicate word and 
its most [left | right] child; 
                                                          
3
 We referred to those CoNLL-2008 participants? reports, e.g. 
(Ciaramita et al, 2008), when we designed the feature sets for 
the two components. 
75
g. [Lemma | (Lemma with POS)] of the head of the pre-
dicate word; 
h. [(Lemma+Dependency_Relation+Lemma) | (POS+D-
ependency_Relation+POS)] of predicate word and its 
head; 
i. [Lemma | (Lemma with POS)] of its [previous | fol-
lowing] two brothers; 
j. [Lemma | POS | (Dependency relation)] bi-gram of 
predicate word and its [previous | following] one 
brother; 
k. [Lemma | POS | (Dependency relation)] tri-gram of 
predicate word and its [previous | following] two 
brothers. 
3 Argument Identification and Classifica-
tion  
The second component of our system is used to 
detect and classify arguments with respect to a 
predicate word. We take a joint solution rather than 
solve the problem in two consecutive steps: argu-
ment identification and argument classification. 
3.1 Methods  
By introducing an additional argument type tag ?_? 
for non-arguments, we transformed the two tasks 
(i.e. argument identification and argument classifi-
cation) into one multi-class classification problem. 
As a word can play different roles with respect to 
different predicate words and a predicate word can 
be an argument of itself, we generate a training set 
by deriving a training example from each word-
predicate pair. For example, if a sentence with two 
predicates has 7 words, we will derive 7*2=14 
training examples. Therefore, the number of train-
ing examples generated in this step will be around 
L times larger than that obtained in the previous 
step, where L is the average length of sentences. 
We chose to use maximum entropy algorithm in 
this step because of its success in the CoNLL-2008 
shared task (Surdeanu et al, 2008). Le Zhang?s 
maximum entropy package (Zhang, 2006) is inte-
grated in our system. 
The Czech data cause much trouble again for us, 
as the training data derived by the above strategy 
became even larger. We had to use a special strat-
egy for the Czech data: we selectively chose word-
predicate pairs for generating the training dataset. 
In other words, not all possible combinations are 
used. We chose the following words with respect 
to each predicate: the first and the last two words 
of a sentence; the words between the predicate and 
any argument of it; two words before the predicate 
or any argument; and two words after the predicate 
or any argument. 
In the Czech and Japanese data, some words 
may play multiple roles with respect to a predicate 
word. We thus have to consider multi-label classi-
fication problem (Tsoumakas and Katakis, 2007) 
for these two languages? data. We tried the follow-
ing two solutions: 
? Take each role type combination as a class 
and transform the multi-label problem to a 
single-label classification problem; 
? Classify a word with a set of binary classi-
fiers: consider each role type individually 
with a binary classifier; any possible role 
type will be output; if no role type is ob-
tained after considering all the role types, 
the role type with the highest confidence 
value will be output; and, if ?_? is output 
with any other role type, remove it. 
We used the second solution in our official 
submission, but we finally found these two solu-
tions perform almost the same. The performance 
difference is very small. We found the cases with 
multi-labels (actually at most two) in the training 
data are very limited: 690 of 414,326 in the Czech 
data and 113 of 46,663 in the Japanese data. 
3.2 Features 
The features we used in this step include: 
 
a. Whether the current word is a predicate; 
b. [Lemma | POS] of current word and its [previous | fol-
lowing] one word; 
c. [Lemma | POS] bi-grams of current word and its [pre-
vious | following] one word; 
d. POS tri-grams of current word, its previous word and 
its following word; 
e. Dependency relation of current word to its head; 
f. [Lemma | POS] of the head of current word; 
g. [Lemma | POS] bi-grams of current word and its head; 
h. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of current word and its 
head; 
i. [Lemma | POS] of its most [left | right] child; 
j. [Lemma | POS] bi-grams of current word and its most 
[left | right] child; 
k. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS) of current word and its 
most [left | right] child; 
l. The number of children of the current word and the 
predicate word; 
m. Attributes of the current word, which is obtained from 
PFEAT field by splitting the field at symbol ?|? and 
removing the invalid attribute of ?*?; 
n. The sense tag of the predicate word; 
76
o. [Lemma | POS] of the predicate word and its head; 
p. Dependency relation of the predicate word to its head; 
q. [Lemma | POS] bi-grams of the predicate word and its 
head; 
r. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of the predicate word and 
its head; 
s. [Lemma | POS] of the most [left | right] child of the 
predicate word; 
t. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of predicate word and its 
head; 
u. [Lemma | POS] bi-gram of the predicate word and its 
most [left | right] child; 
v. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of the predicate word and 
its most [left | right] child; 
w. The relative position of the current word to the predi-
cate one: before, after, or on; 
x. The distance of the current word to the predicate one; 
y. The relative level (up, down, or same) and level dif-
ference on the syntactic dependency tree of the current 
word to the predicate one; 
z. The length of the shortest path between the current 
word and the predicate word. 
4 Experiments 
4.1 Datasets  
The datasets of the CoNLL-2009 shared task con-
tain seven languages: Catalan (CA), Chinese (CN), 
Czech (CZ), English (EG), German (GE), Japanese 
(JP), and Spanish (SP). The training and evaluation 
data of each language (Taul? et al, 2008; Xue et 
al., 2008; Haji? et al, 2006; Palmer et al, 2002; 
Burchardt et al, 2006; Kawahara et al, 2002) have 
been converted to a uniform CoNLL Shared Task 
format. Each participating team is required to 
process all seven language datasets.  
 
Lanuage CA CN CZ EN GE JP SP 
Size (KB) 48974 41340 94284 58155 41091 8948 52430 
# of Sen-
tences 14924 24039 43955 40613 38020 4643 15984 
# of Predi-
cate words 42536 110916 469754 185404 17988 27251 48900 
Avg. # of 
Predicates 
per sentence 
2.85 4.61 10.69 4.57 0.47 5.87 3.06 
popular 
sense tag 
a2 
(37%) 
01 
(90%) 
= 
(81%) 
01 
(87%) 
1 
(75%) 
= 
(100%) 
a2 
(39%) 
Table 1. Statistical information of the seven language 
datasets (training and development). 
 
Table 1 shows some statistical information of 
both training and development data for each lan-
guage. The total size of the uncompressed original 
data without lexicons is about 345MB. The Czech 
dataset is the largest one containing 43,955 sen-
tences and 469,754 predicate words, while the 
Japanese dataset the smallest one. On average, 
10.69 predicate words appear in a Czech sentence, 
while only 0.47 predicate words exist in a German 
sentence. The most popular sense tag in the Czech 
datasets is ?=?, which means the PRED field has 
the same value as the PLEMMA field or the 
FORM field. About 81% of Czech predicate words 
take this value. 
4.2 Experimental Results  
F1 is used as the main evaluation metric in the 
CoNLL-2009 shared task. As to the SRLonly track, 
a joint semantic labeled F1, which considers predi-
cate word sense disambiguation and argument la-
beling equally, is used to rank systems. 
 
Avg. CA CN CZ EG GE JP SP 
69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54 
Table 2. Official results of our system. 
 
Table 2 gives the official results of our system 
on the evaluation data. The system obtained the 
best result (74.06) on the Catalan data, but per-
formed very poor (57.46) on the Czech data. Ex-
cept the Czech data, our system performs quite 
stable on the other six language data with mean of 
71.23 and standard deviation of 2.42. 
 
 Avg. CA CN CZ EG GE JP SP 
Over-
all F1 69.47 74.12 70.52 57.57 70.24 67.97 72.17 73.68 
Pred. 
WSD 
F1 
86.9 84.42 94.54 72.23 92.98 81.09 99.07 83.96 
Arg 
I&C 
F1 
57.24 69.29 57.71 33.19 58.25 60.64 52.72 68.86 
Arg 
I&C 
PR 
69.77 73.43 72.48 62.14 70.14 66.63 69.37 74.23 
Arg 
I&C 
RE 
49.77 65.6 47.94 22.64 49.81 55.64 42.52 64.21 
Table 3. Results of our system after fixing a minor bug. 
 
After submitting the official results, we found 
and fixed a minor bug in the implementation of the 
second component. Table 3 presents the results of 
our system after fixing this bug. The overall per-
formance doesn?t change much. We further ana-
lyzed the bottlenecks by checking the performance 
of different components. 
At the predicate WSD part, our system works 
reasonable with labeled F1 86.9, but the perform-
ance on the Czech data is lower than that of a base-
line system that constantly chooses the most 
popular sense tag. If we use this baseline solution, 
77
we can get predicate WSD F1 78.66, which further 
increases the overall labeled F1 on the Czech data 
to 61.68 from 57.57 and the overall labeled F1 
over the seven languages to 70.05 from 69.47. 
From table 3, we can see our system performs 
relatively poorly for argument identification and 
classification (57.24 vs. 86.9). The system seems 
too conservative for argument identification, which 
makes the recall very lower. We explored some 
strategies for improving the performance of the 
second component, e.g. separating argument iden-
tification and argument classification, and using 
feature selection (with DF threshold) techniques, 
but none of them helps much. We are thinking the 
features currently used may not be effective 
enough, which deserves further study. 
5 Conclusion and Future Work  
In this paper, we describe our system for the 
CoNLL-2009 shared task -- SRLonly closed track. 
Our system was built on existing packages with a 
pipeline architecture, which integrated two cas-
caded components: predicate word sense disam-
biguation and argument identification and 
classification. Our system performs well at disam-
biguating the sense of predicate words, but poorly 
at identifying and classifying arguments. In the 
future, we plan to explore much effective features 
for argument identification and classification. 
Acknowledgments 
This research was funded by Science Foundation 
Ireland under the CNGL grant. We used the IITAC 
Cluster in our initial experiments. We thank IITAC, 
the HEA, the National Development Plan and the 
Trinity Centre for High Performance Computing 
for their support. We are also obliged to John 
Keeney for helping us running our system on the 
CNGL servers. 
References  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2006). Genoa, Italy. 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Massimiliano Ciaramita, Giuseppe Attardi, Felice 
Dell?Orletta, and Mihai Surdeanu. 2008. DeSRL: A 
Linear-Time Semantic Role Labeling System. Pro-
ceedings of the CoNLL-2008. 
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th 
Conference on Computational Natural Language 
Learning (CoNLL-2009). Boulder, Colorado, USA. 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie 
Mikulov? and Zden?k ?abokrtsk?. 2006. The Prague 
Dependency Treebank 2.0. Linguistic Data 
Consortium, USA. ISBN 1-58563-370-4. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. 
Baoli Li, Qin Lu and Shiwen Yu. 2004. An Adaptive k-
Nearest Neighbor Text Categorization Strategy. ACM 
Transactions on Asian Language Information 
Processing, 3(4): 215-226. 
Llu?s M?rquez, Xavier Carreras, Kenneth C. Litkowski 
and Suzanne Stevenson. 2008. Semantic Role Label-
ing: An Introduction to the Special Issue. Computa-
tional Linguistics, 34(2):145-159. 
Andrew Mccallum and Kamal Nigam. 1998. A Com-
parison of Event Models for Naive Bayes Text Clas-
sification. Proceedings of AAAI/ICML-98 Workshop 
on Learning for Text Categorization. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis Marquez and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and 
Semantic Dependencies. Proceedings of the 12th 
Conference on Computational Natural Language 
Learning (CoNLL-2008).  
Mariona Taul?, Maria Ant?nia Mart? and Marta 
Recasens. 2008. AnCora: Multilevel Annotated 
Corpora for Catalan and Spanish. Proceedings of the 
6th International Conference on Language Resources 
and Evaluation (LREC-2008). Marrakech, Morocco. 
Grigorios Tsoumakas and Ioannis Katakis. 2007. Multi-
Label Classification: An Overview. International 
Journal of Data Warehousing and Mining, 3(3):1-13. 
Nianwen Xue and Martha Palmer. 2009. Adding 
semantic roles to the Chinese Treebank.  Natural 
Language Engineering, 15(1):143-172.  
Le Zhang. 2006. Maximum Entropy Modeling Toolkit 
for Python and C++. Software available at 
http://homepages.inf.ed.ac.uk/s0450736/maxent_tool
kit.html. 
78
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 159?164,
Prague, June 2007. c?2007 Association for Computational Linguistics
Machine Learning Based Semantic Inference: Experiments and Ob-
servations at RTE-3 
Baoli Li1, Joseph Irwin1, Ernest V. Garcia2, and Ashwin Ram1 
 
1 College of Computing 
Georgia Institute of Technology 
Atlanta, GA 30332, USA 
baoli@gatech.edu 
gtg519g@mail.gatech.edu 
ashwin@cc.gatech.edu 
 
2 Department of Radiology 
School of Medicine, Emory University 
Atlanta, GA 30322, USA 
Ernest.Garcia@emoryhealthcare.org 
 
Abstract 
Textual Entailment Recognition is a se-
mantic inference task that is required in 
many natural language processing (NLP) 
applications. In this paper, we present our 
system for the third PASCAL recognizing 
textual entailment (RTE-3) challenge. The 
system is built on a machine learning 
framework with the following features de-
rived by state-of-the-art NLP techniques: 
lexical semantic similarity (LSS), named 
entities (NE), dependent content word pairs 
(DEP), average distance (DIST), negation 
(NG), task (TK), and text length (LEN). On 
the RTE-3 test dataset, our system achieves 
the accuracy of 0.64 and 0.6488 for the two 
official submissions, respectively. Experi-
mental results show that LSS and NE are 
the most effective features. Further analy-
ses indicate that a baseline dummy system 
can achieve accuracy 0.545 on the RTE-3 
test dataset, which makes RTE-3 relatively 
easier than RTE-2 and RTE-1. In addition, 
we demonstrate with examples that the cur-
rent Average Precision measure and its 
evaluation process need to be changed. 
1 Introduction 
Textual entailment is a relation between two text 
snippets in which the meaning of one snippet, 
called the hypothesis (H), can be inferred from the 
other snippet, called the text (T). Textual 
entailment recognition is the task of deciding 
whether a given T entails a given H. An example 
pair (pair id 5) from the RTE-3 development 
dataset is as follows: 
 
T: A bus collision with a truck in Uganda has resulted 
in at least 30 fatalities and has left a further 21 injured. 
H: 30 die in a bus collision in Uganda. 
 
Given such a pair, a recognizing textual entail-
ment (RTE) system should output its judgement 
about whether or not an entailment relation holds 
between them. For the above example pair, H is 
entailed by T. 
The PASCAL Recognizing Textual Entailment 
Challenge is an annual challenge on this task 
which has been held since 2005 (Dagan et al, 
2006; Bar-Haim et al 2006). As textual entailment 
recognition is thought to be a common underlying 
semantic inference task for many natural language 
processing applications, such as Information Ex-
traction (IE), Information Retrieval (IR), Question 
Answering (QA), and Document Summarization 
(SUM), the PASCAL RTE Challenge has been 
gaining more and more attention in the NLP com-
munity. In the past challenges, various approaches 
to recognizing textual entailment have been pro-
posed, from syntactic analysis to logical inference 
(Bar-Haim et al 2006). 
As a new participant, we have two goals by at-
tending the RTE-3 Challenge: first, we would like 
to explore how state-of-the-art language techniques 
help to deal with this semantic inference problem; 
second, we try to obtain a more thorough knowl-
edge of this research and its state-of-the-art. 
Inspired by the success of machine learning 
techniques in RTE-2, we employ the same strategy 
in our RTE-3 system. Several lexical, syntactical, 
and semantical language analysis techniques are 
159
explored to derive effective features for determin-
ing textual entailment relation. Then, a general 
machine learning algorithm is applied on the trans-
formed data for training and prediction. Our two 
official submissions achieve accuracy 0.64 and 
0.6488, respectively.  
In the rest of this paper we describe the detail of 
our system and analyze the results. Section 2 gives 
the overview of our system, while Section 3 dis-
cusses the various features in-depth. We present 
our experiments and discussions in Section 4, and 
conclude in Section 5. 
2 System Description 
Figure 1 gives the architecture of our RTE-3 sys-
tem, which finishes the process of both training 
and prediction in two stages. At the first stage, a T-
H pair goes through language processing and fea-
ture extraction modules, and is finally converted to 
a set of feature-values. At the second stage, a ma-
chine learning algorithm is applied to obtain an 
inference/prediction model when training or output 
its decision when predicting. 
In the language processing module, we try to 
analyze T-H pairs with the state-of-the-art NLP 
techniques, including lexical, syntactical, and se-
mantical analyses. We first split text into sentences, 
and tag the Part of Speech (POS) of each word. 
The text with POS information is then fed into 
three separate modules: a named entities recog-
nizer, a word sense disambiguation (WSD) module, 
and a dependency parser. These language analyz-
ers output their own intermediate representations 
for the feature extraction module. 
We produce seven features for each T-H pair: 
lexical semantic similarity (LSS), named entities 
(NE), dependent content word pairs (DEP), aver-
age distance (DIST), negation (NG), task (TK), 
and text length (LEN). The last two features are 
extracted from each pair itself, while others are 
based on the results of language analyzers. 
The resources that we used in our RTE-3 system 
include: 
OAK: a general English analysis tool (Sekine 
2002). It is used for sentence splitting, POS tag-
ging, and named entities recognition. 
WordNet::SenseRelate::Allwords package: a 
word sense disambiguation (WSD) module for as-
signing each content word a sense from WordNet 
(Pedersen et al, 2005). It is used in WSD module. 
 
Figure 1. System Architecture. 
 
WordNet::Similarity package: a Perl module 
that implements a variety of semantic similarity 
and relatedness measures based on WordNet (Pe-
dersen et al, 2005). This package is used for deriv-
ing LSS and DIST features in feature extraction 
module. 
C&C parser: a powerful CCG parser (Clark 
and Curran 2004). We use C&C parser to obtain 
dependent content word pairs in dependency pars-
ing module. 
WEKA: the widely used data mining software 
(Witten&Frank 2005). We have experimented with 
several machine learning algorithms implemented 
in WEKA at the second stage. 
3 Features 
In this section, we explain the seven features that 
we employ in our RTE-3 system. 
3.1 Lexical Semantic Similarity (LSS) 
Let H={HW 1, HW 2, ?, HW m} be the set of words in 
a hypothesis, and T={TW 1, TW 2, ?, TW n} the set of 
words in a text, then the lexical semantic similarity 
feature LSS for a T-H pair is calculated as the fol-
lowing equation: 
?
?
=
i
i
i
i
ii
ji
j
HWIDF
HWIDF
HWHWSSim
TWHWSSim
MAX
THLSS )(
))(*)),(
),(((
),( . (1) 
where IDF(w) return the Inverse Document Fre-
quency (IDF) value of word w, and SSim is any 
function for calculating the semantic relatedness 
between two words. We use WordNet::Similarity 
160
package to calculate the semantic similarity of two 
content words in WordNet (Fellbaum 1998). This 
package provides many different semantic related-
ness measures. In our system, we use the Lesk re-
latedness measure for function SSim, as it can be 
used to make comparisons between concepts of 
different parts of speech (POS) (Baner-
jee&Pedersen, 2002). Because the value of SSim 
may be larger than 1, we normalize the original 
value from the WordNet::Similarity package to 
guarantee it fall between 0 and 1. 
For the words out of WordNet, e.g. new proper 
nouns, we use the following strategy: if two words 
match exactly, the similarity between them is 1; 
otherwise, the similarity is 0. 
It needs to be pointed out that Equation (1) is a 
variant of the text semantic similarity proposed in 
(Mihalcea et al 2006). However, in Equation (1), 
we take into account out of vocabulary words and 
normalization for some word-to-word similarity 
metrics that may be larger than 1. 
In addition, we use an IDF dictionary from 
MEAD (Radev et al 2001; http://www.summari-
zation.com/mead/) for retrieving the IDF value for 
each word. For the words out of the IDF diction-
ary, we assign a default value 3.0. 
3.2 Named Entities (NE) 
Named Entities are important semantic information 
carriers, which convey more specific information 
than individual component words. Intuitively, we 
can assume that all named entities in a hypothesis 
would appear in a textual snippet which entails the 
hypothesis. Otherwise, it is very likely that the en-
tailment relation in a T-H pair doesn?t hold. Based 
on this assumption, we derive a NE feature for 
each T-H pair as follows: 
??
??
?
>
?
=
=
.0|)(_|,|)(_|
|)(_)(_|
,0|)(_|,                       1                 
),( HSNEif
HSNE
TSNEHSNE
HSNEif
THNE
 
Function NE_S derives the set of named entities 
from a textual snippet. When we search in T the 
counterpart of a named entity in H, we use a looser 
matching strategy: if a named entity neA in H is 
consumed by a named entity neB in T, neA and 
neB are thought to be matched. We use the English 
analysis tool OAK (Sekine 2002) to recognize 
named entities in textual snippets. 
3.3 Dependent Content Word Pairs (DEP) 
With the NE feature, we can capture some local 
dependency relations between words, but we may 
miss many dependency relations expressed in a 
long distance. These missed long distance depend-
ency relations may be helpful for determining 
whether entailment holds between H and T. So, we 
design a DEP feature as follows: 
??
??
?
>
?
=
=
.0|)(_|,|)(_|
|)(_)(_|
,0|)(_|,                       1                    
),( HSDEPif
HSDEP
TSDEPHSDEP
HSDEPif
THDEP
 
Function DEP_S derives the set of dependent 
content word pairs from a textual snippet. We re-
quire that the two content words of each pair 
should be dependent directly or linked with at most 
one function word. We use C&C parser (Clark and 
Curran 2004) to parse the dependency structure of 
a textual snippet and then derive the dependent 
content word pairs. We don?t consider the type of 
dependency relation between two linked words. 
3.4 Average Distance (DIST) 
The DIST feature measures the distance between 
unmapped tokens in the text. Adams (2006) uses a 
simple count of the number of unmapped tokens in 
the text that occur between two mapped tokens, 
scaled to the length of the hypothesis. Our system 
uses a different approach, i.e. measuring the aver-
age length of the gaps between mapped tokens. 
The number of tokens in the text between each 
consecutive pair of mapped tokens is summed up, 
and this sum is divided by the number of gaps 
(equivalent to the number of tokens ? 1). In this 
formula, consecutive mapped tokens in the text 
count as gaps of 0, so a prevalence of consecutive 
mapped tokens lowers the value for this feature. 
The purpose of this approach is to reduce the effect 
of long appositives, which may not be mapped to 
the hypothesis but should not rule out entailment. 
3.5 Negation (NG) 
The Negation feature is very simple. We simply 
count the occurrences of negative words from a list 
in both the hypothesis (nh) and the text (nt). The list 
includes some common negating affixes. Then the 
value is: 
??
??
?
=
otherwise 0,
parity  samethe have n and n if 1,T)NEG(H, th  
161
3.6 Task (TK) 
The Task feature is simply the task domain from 
which the text-hypothesis pair was drawn. The 
values are Question Answering (QA), Information 
Retrieval (IR), Information Extraction (IE), and 
Multi-Document Summarization (SUM).  
3.7 Text Length (LEN) 
The Text Length feature is drawn directly from the 
length attribute of each T-H pair. Based on the 
length of T, its value is either ?short? or ?long?. 
4 Experiments and Discussions 
We run several experiments using various datasets 
to train and test models, as well as different com-
binations of features. We also experiment with 
several different machine learning algorithms, in-
cluding support vector machine, decision tree, k-
nearest neighbor, na?ve bayes, and so on. Decision 
tree algorithm achieves the best results in all ex-
periments during development. Therefore, we 
choose to use decision tree algorithm (J48 in 
WEKA) at the machine learning stage. 
4.1 RTE-3 Datasets 
RTE-3 organizers provide two datasets, i.e. a de-
velopment set and a test set, each consisting of 800 
T-H pairs. In both sets pairs are annotated accord-
ing to the task the example was drawn from and its 
length. The length annotation is introduced in this 
year?s competition, and has a value of either 
?long? or ?short.? In addition, the development set 
is annotated as to whether each pair is in an en-
tailment relation or not. 
In order to aid our analysis, we compile some 
statistics on the datasets of RTE-3. Statistics on the 
development dataset are given in Table 1, while 
those on the test dataset appear in Table 2. 
From these two tables, we found the distribution 
of different kinds of pairs is not balanced in both 
the RTE-3 development dataset and the RTE-3 test 
dataset. 412 entailed pairs appear in the develop-
ment dataset, where 410 pairs in the test dataset are 
marked as ?YES?. Thus, the first baseline system 
that outputs all ?YES? achieves accuracy 0.5125. 
If we consider task information (IE, IR, QA, and 
SUM) and assume the two datasets have the same 
?YES? and ?NO? distribution for each task, we 
will derive the second baseline system, which can 
get accuracy 0.5450. Similarly, if we further con-
sider length information (short and long) and as-
sume the two datasets have the same ?YES? and 
?NO? distribution for each task with length infor-
mation, we will derive the third baseline system, 
which can also get accuracy 0.5450. 
Table 1. Statistical Information of the RTE-3 De-
velopment Dataset. 
Table 2. Statistical Information of the RTE-3 Test 
Dataset. 
As different kinds of pairs are evenly distributed 
in RTE-1 and RTE-2 datasets, the baseline system 
for RTE-1 and RTE-2 that assumes all ?YES? or 
all ?NO? can only achieve accuracy 0.5. The rela-
tively higher baseline performance for RTE-3 data-
sets (0.545 vs. 0.5) makes us expect that the aver-
age accuracy may be higher than those in previous 
RTE Challenges. 
Another observation is that the numbers of long 
pairs in both datasets are very limited. Only 
NO 11 1.38% IE 
YES 17 2.13% 
NO 22 2.75% IR 
YES 21 2.63% 
NO 20 2.50% QA 
YES 27 3.38% 
NO 4 0.50% 
Long 
(135) 
SUM 
YES 13 1.63% 
NO 80 10.00% IE 
YES 92 11.50% 
NO 89 11.13% IR 
YES 68 8.50% 
NO 73 9.13% QA 
YES 80 10.00% 
NO 89 11.13% 
Short 
(665) 
SUM 
YES 94 11.75% 
NO 11 1.38% IE 
YES 8 1.00% 
NO 31 3.88% IR 
YES 23 2.88% 
NO 13 1.63% QA 
YES 22 2.75% 
NO 4 0.50% 
Long 
(117) 
SUM 
YES 5 0.63% 
NO 84 10.50% IE 
YES 97 12.13% 
NO 82 10.25% IR 
YES 64 8.00% 
NO 81 10.13% QA 
YES 84 10.50% 
NO 84 10.50% 
Short 
(683) 
SUM 
YES 107 13.38% 
162
16.88% and 14.63% pairs are long in the develop-
ment dataset and the test dataset respectively. 
4.2 Evaluation Measures 
Systems are evaluated by simple accuracy as in 
Equation (2); that is, the number of pairs (C) clas-
sified correctly over the total number of pairs (N). 
This score can be further broken down according 
to task.  
N
CAccuracy = .                                      (2) 
There is another scoring available for ranked re-
sults, Average Precision, which aims to evaluate 
the ability of systems to rank all the T-H pairs in 
the test set according to their entailment confi-
dence (in decreasing order from the most certain 
entailment to the least certain). It is calculated as in 
Equation (3).  
?=
=
N
i i
iNepiE
R
AvgP
1
)(*)(1
.                        (3) 
Where R is the total number of positive pairs in 
the test set, E(i) is 1 if the i-th pair is positive and 0 
otherwise, and Nep(i) returns the number of posi-
tive pairs in the top i pairs. 
Table 3. Our Official RTE-3 Run Results. 
4.3 Official RTE-3 Results 
The official results for our system are shown in 
Table 3. For our first run, the model was trained on 
all the datasets from the two previous challenges as 
well as the RTE-3 development set, using only the 
LSS, NE, and TK features. This feature combina-
tion achieves the best performance on the RTE-3 
development dataset in our experiments. For the 
second run, the model was trained only on the 
RTE-3 development dataset, but adding other two 
features LEN and DIST. We hope these two fea-
tures may be helpful for differentiating pairs with 
different length. 
RUN2 with five features achieves better results 
than RUN1. It performs better on IE, QA and SUM 
tasks than RUN1, but poorer on IR task. Both runs 
obtain the best performance on QA task, and per-
form very poor on IE task. For the IE task itself, a 
baseline system can get accuracy 0.525. RUN1 
cannot beat this baseline system on IE task, while 
RUN2 only has a trivial advantage over it. In fur-
ther analysis on the detailed results, we found that 
our system tends to label all IE pairs as entailed 
ones, because most of the IE pairs exhibit higher 
lexical overlapping between T and H. In our opin-
ion, word order and long syntactic structures may 
be helpful for dealing with IE pairs. We will ex-
plore this idea and other methods to improve RTE 
systems on IE pairs in our future research. 
Table 4. Accuracy by task and selected feature set 
on the RTE-3 Test dataset (Trained on the RTE-3 
development dataset). 
4.4 Discussions 
4.4.1 Feature Analysis 
Table 4 lays out the results of using various feature 
combinations to train the classifier. All of the 
models were trained on the RTE 3 development 
dataset only. 
It is obvious that the LSS and NE features have 
the most utility. The DIST and LEN features seem 
useless for this dataset, as these features them-
selves can not beat the baseline system with accu-
racy 0.545. Systems with individual features per-
form similarly on SUM pairs except NG, and on IE 
pairs except NG and DEP features. However, on 
IR and QA pairs, they behave quite differently. For 
example, system with NE feature achieves accu-
racy 0.78 on QA pairs, while system with DEP 
feature obtains 0.575. NE and LSS features have 
similar effects, but NE is more useful for QA pairs. 
Accuracy by Task 
RUN Overall Accuracy IE IR QA SUM 
1 0.6400 0.5100 0.6600 0.7950 0.5950 
2 0.6488 0.5300 0.6350 0.8050 0.6250 
Accuracy by Task 
Feature Set 
IE IR QA SUM 
Acc. 
LSS 0.530 0.660 0.720 0.595 0.6263 
NE 0.520 0.620 0.780 0.580 0.6250 
         DEP 0.495 0.625 0.575 0.570 0.5663 
          TK 0.525 0.565 0.530 0.560 0.5450 
                    DIST 0.525 0.435 0.530 0.560 0.5125 
          NG 0.555 0.505 0.590 0.535 0.5463 
                    LEN 0.525 0.435 0.530 0.560 0.5125 
LSS+NE 0.525 0.645 0.805 0.585 0.6400 
LSS+NE+DEP 0.520 0.650 0.810 0.580 0.6400 
LSS+NE+TK 0.530 0.625 0.805 0.595 0.6388 
LSS+NE+TK+LEN 0.530 0.630 0.805 0.625 0.6475 
LSS+NE+TK+DEP 0.530 0.625 0.805 0.620 0.6450 
LSS+NE+TK+DEP+NG 0.460 0.625 0.785 0.655 0.6313 
LSS+NE+TK+LEN+DEP 0.525 0.615 0.790  0.600 0.6325 
LSS+NE+TK+LEN+DIST 
(run2) 0.530 0.635 0.805 0.625 0.6488 
All Features 0.500 0.590 0.790 0.630 0.6275 
163
It is interesting to note that some features im-
prove the score in some combinations, but in oth-
ers they decrease it. For instance, although DEP 
scores above the baseline at 0.5663, when added to 
the combination of LSS, NE, TK, and LEN it low-
ers the overall accuracy by 1.5%. 
4.4.2 About Average Precision Measure 
As we mentioned in section 4.2, Average Precision 
(AvgP) is expected to evaluate the ranking ability 
of a system according to confidence values. How-
ever, we found that the current evaluation process 
and the measure itself have some problems and 
need to be modified for RTE evaluation. 
On one hand, the current evaluation process 
doesn?t consider tied cases where many pairs may 
have the same confidence value. It is reasonable to 
assume that the order of tied pairs will be random. 
Accordingly, the derived Average Precision will 
vary. 
Let?s look at a simple example: suppose we 
have two pairs c and d, and c is the only one posi-
tive entailment pair. Here, R=1, N=2 for Equation 
(3). Two systems X and Y output ranked results as 
{c, d} and {d,c} respectively. According to Equa-
tion (3), the AvgP value of system X is 1, where 
that of system Y is 0.5. If these two systems assign 
same confidence value for both pairs, we can not 
conclude that system X is better than system Y. 
To avoid this problem, we suggest requiring that 
each system for ranked submission output its con-
fidence for each pair. Then, when calculating Av-
erage Precision measure, we first re-rank the list 
with these confidence values and true answers for 
each pair. For tied pairs, we rank pairs with true 
answer ?NO? before those with positive entailment 
relation. By this way, we can produce a stable and 
more reasonable Average Precision value. For ex-
ample, in the above example, the modified average 
precisions for both systems will be 0.5. 
On the other hand, from the Equation (3), we 
know that the upper bound of Average Precision is 
1. At the same time, we can also derive a lower 
bound for this measure as in Equation (4). It corre-
sponds to the worst system which places all the 
negative pairs before all the positive pairs. The 
lower bound of Average Precision for RTE-3 test 
dataset is 0.3172. 
?
?
?
=
?
=
1
0
1
_
R
j jN
jR
R
AvgPLB .                       (4) 
As the values of N and R change, the lower 
bound of Average Precision will vary. Therefore, 
the original Average Precision measure as in Equa-
tion (3) is not an ideal one for comparison across 
datasets. 
To solve this problem, we propose a normalized 
Average Precision measure as in Equation (5). 
AvgPLB
AvgPLBAvgPAvgPNorm
_1
_
_
?
?
= .            (5) 
5 Conclusion and Future Work 
In this paper, we report our RTE-3 system. The 
system was built on a machine learning framework 
with features produced by state-of-the-art NLP 
techniques. Lexical semantic similarity and Named 
entities are the two most effective features. Data 
analysis shows a higher baseline performance for 
RTE-3 than RTE-1 and RTE-2, and the current 
Average Precision measure needs to be changed. 
As T-H pairs from IE task are the most difficult 
ones, we will focus on these pairs in our future re-
search. 
References 
Rod Adams. 2006. Textual Entailment Through Extended Lexical 
Overlap. In Proceedings of RTE-2 Workshop. 
Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted Lesk Algo-
rithm for Word Sense Disambiguation Using WordNet. In Pro-
ceedings of CICLING-02. 
Roy Bar-Haim et al 2006. The Second PASCAL Recognising Textual 
Entailment Challenge. In Proceedings of RTE-2 Workshop. 
Stephen Clark and James R. Curran. 2004. Parsing the WSJ using 
CCG and Log-Linear Models. In Proceedings of ACL-04. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PAS-
CAL Recognising Textual Entailment Challenge. In Qui?onero-
Candela et al (editors.), MLCW 2005, LNAI Volume 3944. 
Christiane Fellbaum. 1998. WordNet: an Electronic Lexical Database. 
MIT Press. 
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Cor-
pus-based and Knowledge-based Measures of Text Semantic Simi-
larity. In Proceedings of AAAI-06. 
Ted Pedersen et al 2005. Maximizing Semantic Relatedness to Per-
form Word Sense Disambiguation. Research Report UMSI 
2005/25, Supercomputing Institute, University of Minnesota. 
Dragomir Radev, Sasha Blair-Goldensohn, and ZhuZhang. 2001. 
Experiments in single and multidocument summarization using 
MEAD. In Proceedings of DUC 2001. 
Satoshi Sekine. 2002. Manual of Oak System (version 0.1). Computer 
Science Department, New York University, 
http://nlp.cs.nyu.edu/oak. 
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine 
learning tools and techniques. Morgan Kaufmann, San Francisco. 
 
164
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 126?131,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Exploiting CCG Structures with Tree Kernels for Speculation Detection
Liliana Mamani Sa?nchez, Baoli Li, Carl Vogel
Computational Linguistics Group
Trinity College Dublin
Dublin 2, Ireland
{mamanisl,baoli.li,vogel}@tcd.ie
Abstract
Our CoNLL-2010 speculative sentence
detector disambiguates putative keywords
based on the following considerations: a
speculative keyword may be composed of
one or more word tokens; a speculative
sentence may have one or more specula-
tive keywords; and if a sentence contains
at least one real speculative keyword, it is
deemed speculative. A tree kernel classi-
fier is used to assess whether a potential
speculative keyword conveys speculation.
We exploit information implicit in tree
structures. For prediction efficiency, only
a segment of the whole tree around a spec-
ulation keyword is considered, along with
morphological features inside the segment
and information about the containing doc-
ument. A maximum entropy classifier
is used for sentences not covered by the
tree kernel classifier. Experiments on the
Wikipedia data set show that our system
achieves 0.55 F-measure (in-domain).
1 Introduction
Speculation and its impact on argumentation has
been studied by linguists and logicians since at
least as far back as Aristotle (trans 1991, 1407a,
1407b), and under the category of linguistic
?hedges? since Lakoff (1973). Practical appli-
cation of this research has emerged due to the
efforts to create a biomedical database of sen-
tences tagged with speculation information: Bio-
Scope (Szarvas et al, 2008) and because of the
association of some kinds of Wikipedia data with
the speculation phenomenon (Ganter and Strube,
2009). It is clear that specific words can be con-
sidered as clues that can qualify a sentence as
speculative. However, the presence of a specu-
lative keyword not always conveys a speculation
assertion which makes the speculation detection a
tough problem. For instance, the sentences below
contain the speculative keyword ?may?, but only
the sentence (a) is speculative.
(a) These effects may be reversible.
(b) Members of an alliance may not attack each other.
The CoNLL-2010 Shared Task (Farkas et al,
2010), ?Learning to detect hedges and their scope
in natural language text? proposed two tasks re-
lated to speculation research. Task 1 aims to detect
sentences containing uncertainty and Task 2 aims
to resolve the intra-sentential scope of hedge cues.
We engaged in the first task in the biomedical and
Wikipedia domains as proposed by the organizers,
but eventually we got to submit only Wikipedia
domain results. However, in this paper we include
results in the biomedical domain as well.
The BioScope corpus is a linguistically hand an-
notated corpus of negation and speculation phe-
nomena for medical free texts, biomedical article
abstracts and full biomedical articles. The afore-
said phenomena have been annotated at sentence
level with keyword tags and linguistic scope tags.
Some previous research on speculation detection
and boundary determination over biomedical data
has been done by Medlock & Briscoe (2007) and
O?zgu?r & Radev (2009) from a computational view
using machine learning methods.
The Wikipedia speculation dataset was gener-
ated by exploiting a weasel word marking. As
weasel words convey vagueness and ambiguity by
providing an unsupported opinion, they are dis-
couraged by Wikipedia editors. Ganter & Strube
(2009) proposed a system to detect hedges based
on frequency measures and shallow information,
achieving a F-score of 0.691.
We formulate the speculation detection prob-
lem as a word disambiguation problem and de-
veloped a system as a pipelined set of natural
1They used different Wikipedia data.
126
language processing tools and procedures to pre-
process the datasets. A Combinatory Categorial
Grammar parsing (CCG) (Steedman, 2000) tool
and a Tree Kernel (TK) classifier constitute the
core of the system.
The Section 2 of this paper describes the over-
all architecture of our system. Section 3 depicts
the dataset pre-processing. Section 4 shows how
we built the speculation detection module, outlines
the procedure of examples generation and the use
of the Tree-kernel classifier. Section 5 presents
the experiments and results, we show that sentence
CCG derivation information helps to differentiate
between apparent and real speculative words for
speculation detection. Finally Section 6 gives our
conclusions.
2 Speculation detection system
Our system for speculation detection is a machine
learning (ML) based system (Figure 1). In the pre-
processing module a dataset of speculative/non-
speculative sentences goes through a process of
information extraction of three kinds: specula-
tive word or keyword extraction,2 sentence extrac-
tion and document feature extraction (i.e docu-
ment section). Later the extracted keywords are
used to tag potential speculative sentences in the
training/evaluation datasets and used as features
by the classifiers. The sentences are submitted to
the tokenization and parsing modules in order to
provide a richer set of features necessary for creat-
ing the training/evaluation datasets, including the
document features as well.
In the ML module two types of dataset are built:
one used by a TK classifier and other one by a bag-
of-features based maximum entropy classifier. As
the first one processes only those sentences that
contain speculative words, we use the second clas-
sifier, which is able to process samples of all the
sentences.
The models built by these classifiers are com-
bined in order to provide a better performance and
coverage for the speculation problem in the clas-
sification module which finally outputs sentences
labeled as speculative or non-speculative. Used
tools are the GeniaTagger (Tsuruoka et al, 2005)
for tokenization and lemmatization, and the C&C
Parser (Clark and Curran, 2004). The next sec-
tions explain in detail the main system compo-
nents.
2Extraction of keywords for the training stage.
3 Dataset pre-processing for rich feature
extraction
The pre-processing module extracts keywords,
sentences and document information.
All sentences are processed by the tok-
enizer/lemmatizer and at the same time specific in-
formation about the keywords is extracted.
Speculative keywords
Speculative sentences are evidenced by the pres-
ence of speculation keywords. We have the fol-
lowing observations:
? A hedge cue or speculative keyword 3 may be
composed of one or more word tokens.
? In terms of major linguistic categories, the
word tokens are heterogeneous: they may be
verbs, adjectives, nouns, determiners, etc. A
stop-word removing strategy was dismissed,
since no linguistic category can be elimi-
nated.
? A keyword may be covered by another longer
one. For instance, the keyword most can be
seen in keywords like most of all the heroes
or the most common.
Considering these characteristics for each sen-
tence, in the training stage, the keyword extraction
module retrieves the speculative/non-speculative
property of each sentence, the keyword occur-
rences, number of keywords in a sentence, the ini-
tial word token position and the number of word
tokens in the keyword. We build a keyword lex-
icon with all the extracted keywords and their
frequency in the training dataset, this speculative
keyword lexicon is used to tag keyword occur-
rences in non-speculative training sentences and
in all the evaluation dataset sentences.
The overlapping problem when tagging key-
words is solved by maximal matching strategy. It
is curious that speculation phrases come in de-
grees of specificity; the approach adopted here
favors ?specific? multi-word phrases over single-
word expressions.
Sentence processing
Often, speculation keywords convey certain in-
formation that can not be successfully expressed
by morphology or syntactic relations provided by
phrase structure grammar parsers. On the other
3Or just ?keyword? for sake of simplicity.
127
Figure 1: Block diagram for the speculation detection system.
hand, CCG derivations or dependencies provide
deeper information, in form of predicate-argument
relations. Previous works on semantic role label-
ing (Gildea and Hockenmaier, 2003; Boxwell et
al., 2009) have used features derived from CCG
parsings and obtained better results.
C&C parser provides CCG predicate-argument
dependencies and Briscoe and Carroll (2006) style
grammatical relations. We parsed the tokenized
sentences to obtain CCG derivations which are
binary trees as shown in the Figure 2. The
CCG derivation trees contain function category
and part-of-speech labels; this information is con-
tained in the tree structures to be used in building
a subtree dataset for the TK classifier.
4 Speculative sentence classifier
4.1 Tree Kernel classification
The subtree dataset is processed by a Tree Kernel
classifier (Moschitti, 2006) based on Support Vec-
tor Machines. TK uses a kernel function between
two trees, allowing a comparison between their
substructures, which can be subtrees (ST) or sub-
set trees (SST). We chose the comparison between
subset trees since it expands the kernel calculation
to those substructures with constituents that are
not in the leaves. Our intuition is that real specula-
tive sentences have deep semantic structures that
are particularly different from those ones in ap-
parent speculative sentences, and consequently the
comparison between the structures of well identi-
fied and potential speculative sentences may en-
hance the identification of real speculative key-
words.
4.2 Extracting tree structures
The depth of a CCG derivation tree is propor-
tional to the number of word tokens in the sen-
tence. Therefore, the processing of a whole deriva-
tion tree by the classifier is highly demanding and
many subtrees are not relevant for the classifica-
tion of speculative/non-speculative sentences, in
particular when the scope of the speculation is a
small proportion of a sentence.
In order to tackle this problem, a fragment of
the CCG derivation tree is extracted. This frag-
ment or subtree spans the keyword together with
neighbors terms in a fixed-size window of n word
tokens, (i.e. n word tokens to the left and n word
tokens to the right of the keyword) and has as root
the lower upper bound node of the first and last
tokens of this span. After applying the subtree ex-
traction, the subtree can contain more word tokens
in addition to those contained in the n-span, which
are replaced by a common symbol.
Potential speculative sentences are turned into
training examples. However, as described in Sec-
tion 3, a speculative sentence can contain one or
more speculative keywords. This can produce an
overlapping between their respective n-spans of
individual keywords during the subtree extraction,
producing subtrees with identical roots for both
keywords. For instance, in the following sen-
tence(c), the spans for the keywords suggests and
thought will overlap if n = 3.
(c) This suggests that diverse agents thought to ac-
tivate NF-kappa B ...
The overlapping interacts with the windows size
and potential extraction of dependency relations
128
It was reported to have burned for a day
PRP VBD VBN TO VB VBN IN DT NN
NP (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/(S[to]\NP) (S[to]\NP)/(S[b]\NP) (S[b]\NP)/(S[pt]\NP) S[pt]\NP ((S\NP)\(S\NP))/NP NP[nb]/N N
NP[nb]
(S[X]\NP)\(S[X]\NP)
S[pt]\NP
S[b]\NP
S[to]\NP
S[pss]\NP
S[dcl]\NP
S[dcl]
Figure 2: CCG derivations tree for It was reported to have burned for a day.
shared by terms belonging to the two different
spans. We deal with this issue by extracting one
training example if two spans have a common root
and two different examples otherwise.
4.3 Bag of features model
By default, our system classifies the sentences not
covered by the TK model using a baseline clas-
sifier that labels a sentence as speculative if this
has at least one keyword. Alternatively, a bag of
features classifier is used to complement the tree
kernel, aimed to provide a more precise method
that might detect even speculative sentences with
new keywords in the evaluation dataset. The set of
features used to build this model includes:
a) Word unigrams;
b) Lemma unigrams;
c) Word+POS unigrams;
d) Lemma+POS unigrams;
e) Word+Supertag unigrams;
f) Lemma+Supertag unigrams;
g) POS+Supertag unigrams;
h) Lemma bigrams;
i) POS bigrams;
j) Supertag bigrams;
k) Lemma+POS bigrams;
l) Lemma+Supertag bigrams;
m) POS+Supertag bigrams;
n) Lemma trigrams;
o) POS trigrams;
p) Supertag trigrams;
q) Lemma+POS trigrams;
r) Lemma+Supertag trigrams;
s) POS+Supertag trigrams;
t) Number of tokens;
u) Type of section in the document (Title, Text,
Section);
v) Name of section in the document;
w) Position of the sentence in a section starting
from beginning;
Dataset Dev. Train. Eval.
Biomedical 39 14541 5003
Wikipedia 124 11111 9634
Table 1: Datasets sizes.
x) Position of the sentence in a section starting
from end.
Position of the sentence information, composed by
the last four features, represents the information
about the sentence relative to a whole document.
The bag of features model is generated using a
Maximum Entropy algorithm (Zhang, 2004).
5 Experiments and results
5.1 Datasets
In the CoNLL-2010 Task 1, biomedical and
Wikipedia datasets were provided for develop-
ment, training and evaluation in the BioScope
XML format. Development and training datasets
are tagged with cue labels and a certainty feature.4
The number of sentences for each dataset 5 is de-
tailed in Table 1.
After manual revision of sentences not parsed
by C&C parser, we found that they contain equa-
tions, numbering elements (e.g. (i), (ii).. 1),
2) ), or long n-grams of named-entities, for in-
stance: ...mannose-capped lipoarabinomannan (
ManLAM ) of Mycobacterium tuberculosis ( M.
tuberculosis )... that out of a biomedical domain
appear to be ungrammatical. Similarly, in the
Wikipedia datasets, some sentences have many
named entities. This suggests the need of a spe-
cific pre-processor or a parser for this kind of sen-
tences like a named entity tagger.
In Table 2, we present the number of parsed sen-
tences, processed sentences by the TK model and
examples obtained in the tree structure extraction.
4certainty=?uncertain? and certainty=?certain?.
5The biomedical abstracts and biomedical articles training
datasets are processed as a single dataset.
129
Dataset Parsed Process. Samples
Biomedical train. 14442 10852 23511
Biomedical eval. 4903 3395 7826
Wikipedia train. 10972 7793 13461
Wikipedia eval. 9559 4666 8467
Table 2: Count of processed sentences.
5.2 Experimental results
The CoNLL-2010 organizers proposed in-domain
and cross-domain evaluations. In cross-domain
experiments, test datasets of one domain can be
used with classifiers trained on the other or on the
union of both domains. We report here our results
for the Wikipedia and biomedical datasets.
So far, we mentioned two settings for our clas-
sifier: a TK classifier complemented by a baseline
classifier (BL) and TK classifier complemented
by a bag of features classifier (TK+BF). Table
3 shows the scores of our submitted system (in-
domain Task 1) on the Wikipedia dataset, whereas
Table 4 gives the scores of the baseline system.
TP FP FN Precision Recall F
Our system 1033 480 1201 0.6828 0.4624 0.5514
Max. 1154 448 1080 0.7204 0.5166 0.6017
Min. 147 9 2087 0.9423 0.0658 0.123
Table 3: Comparative scores for our system with
CoNLL official maximum and minimum scores in
Task 1, Wikipedia dataset in-domain.
TP FP FN Precision Recall F
Biomedical 786 2690 4 0.2261 0.9949 0.3685
Wikipedia 1980 2747 254 0.4189 0.8863 0.5689
Table 4: Baseline results.
Additionally, we consider a bag of features clas-
sifier (BF) and a classifier that combines the base-
line applied to the sentences that have at least one
keyword plus the BF classifier for the remaining
sentences (BL+BF). In Tables 5 to 10, results for
the four classifiers (TK, TK+BF, BF, BL+BF) with
evaluations in-domain and cross-domain are pre-
sented6.The baseline scores confirm that relying on just
the keywords is not enough to identify speculative
sentences. In the biomedical domain, the classi-
fiers give high recall but too low precision result-
ing in low F-scores. Still, the TK, TK+BF and BF
(in-domain configurations) gives much better re-
sults than BL and BL+BF which indicates that the
information from CCG improves the performance
6It is worth to note that the keyword lexicons have been
not used in cross-domain way, so the TK and TK+BF models
have not been tested in regards to keywords.
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1033 480 1201 0.6828 0.4624 0.5514
TK+BF 1059 516 1175 0.6729 0.4740 0.5560
BF 772 264 1462 0.7452 0.3456 0.4722
BL+BF 2028 2810 206 0.4192 0.9078 0.5735
Table 5: Results for Wikipedia dataset in-domain.
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1776 2192 458 0.4476 0.7950 0.5727
TK+BF 1763 2194 471 0.4455 0.7892 0.5695
BF 403 323 1831 0.5551 0.1804 0.2723
BL+BF 1988 2772 246 0.4176 0.8899 0.5685
Table 6: Wikipedia data classified with biomedical
model scores (cross-domain).
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1081 624 1153 0.6340 0.4839 0.5489
TK+BF 1099 636 1135 0.6334 0.4919 0.5538
BF 770 271 1464 0.7397 0.3447 0.4702
BL+BF 2017 2786 217 0.4199 0.9029 0.5733
Table 7: Wikipedia data classified with biomedical
+ Wikipedia model scores (cross-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 759 777 31 0.4941 0.9606 0.6526
TK+BF 751 724 39 0.5092 0.9506 0.6631
BF 542 101 248 0.8429 0.6861 0.7565
BL+BF 786 2695 4 0.2258 0.9949 0.3681
Table 8: Biomedical data scores (in-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 786 2690 4 0.2261 0.9949 0.3685
TK+BF 771 2667 19 0.2243 0.9759 0.3647
BF 174 199 616 0.4665 0.2206 0.2992
BL+BF 787 2723 3 0.2242 0.9962 0.3660
Table 9: Biomedical data classified with
Wikipedia model scores (cross-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 697 357 93 0.6613 0.8823 0.7560
TK+BF 685 305 105 0.6919 0.8671 0.7697
BF 494 136 296 0.7841 0.6253 0.6958
BL+BF 786 2696 4 0.2257 0.9949 0.3679
Table 10: Biomedical data classified with biomed-
ical + Wikipedia model scores (cross-domain).
of the classifiers when compared to the baseline
classifier.
Even though in the Wikipedia domain the
TK+BF score is less than the baseline score, still
the performance of the classifiers do not fall much
in any of the in-domain and cross-domain exper-
iments. On the other hand, BF does not have a
good performance in 5 of 6 the experiments. To
make a more precise comparison between TK and
BF, the TK and BL+BF scores show that BL+BF
performs better than TK in only 2 of the 6 ex-
periments but the better performances achieved
by BL+BF are very small. This suggests that
130
the complex processing made by tree kernels is
more useful when disambiguating speculative key-
words than BF. Nonetheless, the bag-of-features
approach is also of importance for the task at hand
when combined with TK. We observe that the TK
classifer and BF classifier perform well making us
believe that the CCG derivations provide relevant
information for speculation detection. The use of
tree kernels needs further investigations in order to
evaluate the suitability of this approach.
6 Concluding remarks
Speculation detection is found to be a tough task
given the high ambiguity of speculative keywords.
We think these results can be improved by study-
ing the influences of context on speculation asser-
tions.
This paper presents a new approach for disam-
biguating apparent speculative keywords by us-
ing CCG information in the form of supertags and
CCG derivations. We introduce the use of the tree
kernel approach for CCG derivations trees. The
inclusion of other features like grammatical rela-
tions provided by the parser needs to be studied
before incorporating this information into the cur-
rent classifier and possibly to resolve the boundary
speculation detection problem.
Acknowledgments
This research is supported by the Trinity College
Research Scholarship Program and the Science
Foundation Ireland (Grant 07/CE/I1142) as part
of the Centre for Next Generation Localisation
(www.cngl.ie) at Trinity College of Dublin.
References
Aristotle. trans. 1991. The Art of Rhetoric. Penguin
Classics, London. Translated with an Introduction
and Notes by H.C. Lawson-Tancred.
Stephen Boxwell, Dennis Mehay, and Chris Brew.
2009. Brutus: A semantic role labeling system in-
corporating CCG, CFG, and dependency features.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 37?45, Suntec, Singapore.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC DepBank. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
41?48, Morristown, NJ, USA.
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In ACL
?04: Proceedings of the 42nd Annual Meeting on As-
sociation for Computational Linguistics, page 103,
Morristown, NJ, USA.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore.
Daniel Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial
grammar. In Proceedings of 2003 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sapporo, Japan.
George Lakoff. 1973. Hedges: A study in meaning
criteria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2(4):458?508.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999, Prague, Czech Republic.
AlessandroMoschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of the 11th Conference of the European Chapter of
the Association for Computational Linguistics.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398?1407, Singapore.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of the Workshop
on Current Trends in Biomedical Natural Language
Processing, pages 38?45, Columbus, Ohio.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics, pages 382?392.
Le Zhang. 2004. Maximum entropy modeling toolkit
for Python and C++ (version 20041229). In Natural
Language Processing Lab, Northeastern.
131
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 129?135,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Learning to Model Multilingual Unrestricted Coreference in OntoNotes 
 
 
Baoli LI 
Department of Computer Science 
Henan University of Technology 
1 Lotus Street, High&New Technology 
Industrial Development Zone, Zhengzhou, 
Henan, China, 450001 
csblli@gmail.com 
 
 
 
 
 
 
Abstract 
Coreference resolution, which aims at 
correctly linking meaningful expressions in 
text, is a much challenging problem in 
Natural Language Processing community. 
This paper describes the multilingual 
coreference modeling system of Web 
Information Processing Group, Henan 
University of Technology, China, for the 
CoNLL-2012 shared task (closed track). 
The system takes a supervised learning 
strategy, and consists of two cascaded 
components: one for detecting mentions, 
and the other for clustering mentions. To 
make the system applicable for multiple 
languages, generic syntactic and semantic 
features are used to model coreference in 
text. The system obtained combined 
official score 41.88 over three languages 
(Arabic, Chinese, and English) and ranked 
7th among the 15 systems in the closed 
track. 
1 Introduction 
Coreference resolution, which aims at correctly 
linking meaningful expressions in text, has become 
a central research problem in natural language 
processing community with the advent of various 
supporting resources (e.g. corpora and different 
kinds of knowledge bases). OntoNotes (Pradhan et 
al. 2007), compared to MUC (Chinchor, 2001; 
Chinchor and Sundheim, 2003) and ACE 
(Doddington et al 2000) corpora, is a large-scale, 
multilingual corpus for general anaphoric 
coreference that covers entities and events not 
limited to noun phrases or a limited set of entity 
types. It greatly stimulates the research on this 
challenging problem ? Coreference Resolution. 
Moreover, resources like WordNet (Miller, 1995) 
and the advancement of different kinds of syntactic 
and semantic analysis technologies, make it 
possible to do in-depth research on this topic, 
which is demanded in most of natural language 
processing applications, such as information 
extraction, machine translation, question answering, 
summarization, and so on. 
Our group is exploring how to extract 
information from grain/cereal related Chinese text 
for business intelligence. This shared task provides 
a good platform for advancing our research on IE 
related topics. We experiment with a machine 
learning strategy to model multilingual coreference 
for the CoNLL-2012 shared task (Pradhan et al 
2012). Two steps are taken to detect coreference in 
text: mention detection and mention clustering. We 
consider mentions that correspond to a word or an 
internal node in a syntactic tree and ignore the rest 
mentions, as we think a mention should be a valid 
meaningful unit of a sentence. Maximal entropy 
algorithm is used to model what a mention is and 
how two mentions link to each other. Generic 
features are designed to facilitate these modeling.  
129
Our official submission obtained combined 
official score 41.88 over three languages (Arabic, 
Chinese, and English), which ranked the system 7th 
among 15 systems participating the closed track. 
Our system performs poor on the Arabic data, and 
has relatively high precision but low recall. 
The rest of this paper is organized as follows. 
Section 2 gives the overview of our system, while 
Section 3 discusses the first component of our 
system for mention detection. Section 4 explains 
how our system links mentions. We present our 
experiments and analyses in Section 5, and 
conclude in Section 6.  
 
Pre-processing
Mention Detection
Mention Clustering
Post-processing
Pipelined Processing Modules
 
Figure 1. System Architecture. 
2 System Description 
Figure 1 gives the architecture of our CoNLL-2012 
system, which consists of four pipelined 
processing modules: pre-processing, mention 
detection, mention clustering, and post-processing. 
Pre-processing: this module reads in the data 
files in CoNLL format and re-builds the syntactic 
and semantic analysis trees in memory. 
Mention Detection: this module chooses 
potential sub-structures on the syntactic parsing 
trees and determines whether they are real 
mentions. 
Mention Clustering: this module compares 
pairs of mentions and links them together. 
Post-processing: this module removes singleton 
mentions and produces the final results. 
To facilitate the processing, the data files of the 
same languages are combined together to form big 
files for training, development, and test 
respectively. 
Compared to the CoNLL-2011 shared task, the 
task of this year focuses on the multilingual 
capacity of a corefernece resolution system. We 
plan to take a generic solution for different 
languages rather than customized approach to 
some languages with special resources. In other 
words, our official system didn?t take any special 
processing for data of different languages but used 
the same strategy and feature sets for all three 
languages. 
Stanford?s Rule-based method succeeded in 
resolving the coreferences in English text last year 
(Pradhan et al 2011; Lee et al 2011). Therefore, 
we planed to incorporate the results of a rule-based 
system (simple or complex as the Stanford?s 
system) if available and derive some relevant 
features for our machine learning engine. However, 
due to limited time and resources, we failed to 
implement in our official system such a solution 
integrating rules within the overall statistical model. 
Intuitively, mentions are meaningful sub-
structures of sentences. We thus assume that a 
mention should be a word or a phrasal sub-
structure of a parsing tree. Mention detection 
modules focus on these mentions and ignore others 
that do not correspond to a valid phrasal sub-
structure. 
A widely used machine learning algorithm in 
solving different NLP problems, Maximal Entropy 
(Berger et al1996), is used to model mentions and 
detect links between them. Compared with Naive 
Bayes algorithm, Maximum entropy does not 
assume statistical independence of the different 
features. In our system, Le Zhang?s maximum 
entropy package (Zhang, 2006) is integrated. 
In the following two sections, we will detail the 
two critical modules: mention detection and 
mention clustering. 
3 Mention Detection 
This module determines all mentions in text. We 
take the assumption that a mention should be a 
valid sub-structure of a sentence. 
3.1 Methods 
130
We first choose potential mentions in text and then 
use statistical machine learning method to make 
final decisions. 
From the train and development datasets, we 
could obtain a list of POS and syntactic structure 
tags that a mention usually has. For example, 
below is given such a list for English data: 
 
 POS_TAG     "NP" /*145765*/  
 POS_TAG     "NML" /*910*/  
 POS_TAG     "S" /*207*/  
 POS_TAG     "VP" /*189*/  
 POS_TAG     "ADVP" /*75*/  
 POS_TAG     "FRAG" /*73*/  
 POS_TAG     "WHNP" /*67*/  
 POS_TAG     "ADJP" /*65*/  
 POS_TAG     "QP" /*62*/  
 POS_TAG     "INTJ" /*40*/  
 POS_TAG     "PP" /*16*/  
 POS_TAG     "SBAR" /*10*/  
 POS_TAG     "WHADVP" /*7*/  
 POS_TAG     "UCP" /*5*/  
 //POS_TAG     "SINV" /*1*/  
 //POS_TAG     "SBARQ" /*1*/  
 //POS_TAG     "RRC" /*1*/  
 //POS_TAG     "SQ" /*1*/  
 //POS_TAG     "LST" /*1*/  
SYN_TAG     "PRP$" /*14734*/  
 SYN_TAG     "NNP" /*3642*/  
 SYN_TAG     "VB" /*733*/  
 SYN_TAG     "VBD" /*669*/  
 SYN_TAG     "VBN" /*384*/  
 SYN_TAG     "VBG" /*371*/  
 SYN_TAG     "NN" /*306*/  
 SYN_TAG     "VBZ" /*254*/  
 SYN_TAG     "VBP" /*235*/  
 SYN_TAG     "PRP" /*137*/  
 SYN_TAG     "CD" /*132*/  
 SYN_TAG     "DT" /*77*/  
 SYN_TAG     "IN" /*64*/  
 SYN_TAG     "NNS" /*57*/  
 SYN_TAG     "JJ" /*52*/  
 SYN_TAG     "RB" /*19*/  
 SYN_TAG     "NNPS" /*17*/  
 SYN_TAG     "UH" /*7*/  
 SYN_TAG     "CC" /*7*/  
 SYN_TAG     "NFP" /*5*/  
 SYN_TAG     "XX" /*4*/  
 SYN_TAG     "MD" /*3*/  
 SYN_TAG     "JJR" /*2*/  
 SYN_TAG     "POS" /*2*/  
 //SYN_TAG     "FW" /*1*/  
 //SYN_TAG     "ADD" /*1*/  
 
We remove tags rarely occurring in the datasets, 
such as FW and ADD for English and consider all 
words and syntactic structures of the rest 
categories as potential mentions. 
To make a decision about whether a potential 
mention is a real one or not, we use a maximal 
entropy classifier with a set of generic features 
concerning the word or sub-structure itself and its 
syntactic and semantic contexts. 
3.2 Features 
The features we used in this step for each potential 
word or sub-structure include: 
 
a. Source and Genre of a document; Speaker of a 
sentence; 
b. Level of the Node in the syntactic parsing tree; 
c. Named entity tag of the word or sub-structure; 
d. Its head predicates and types; 
e. Syntactic tag path to the root; 
f. Whether it?s part of a mention, named entity, 
or an argument; 
g. Features from its parent: syntactic tag, named 
entity tag, how many children it has, whether 
the potential word or sub-structure is the left 
most child of it, the right most child, or middle 
child; binary syntactic tag feature; 
h. Features from its direct left and right siblings: 
their syntactic tags, named entity tags, and 
binary syntactic tag features; 
i. Features from its children: its total  token 
length, words, pos tags, lemma, frameset ID, 
and word sense, tag paths to the left and right 
most child; 
j. Features from its direct neighbor (before and 
after) tokens: words, pos tags, lemma, 
frameset ID, and word sense, and binary 
features of  pos tags; 
4 Mention Clustering 
This component clusters the detected mentions into 
group. 
4.1 Methods 
For each pair of detected mentions, we determine 
whether they could be linked together with a 
maximal entropy classifier. The clustering takes a 
best-of-all strategy and works as the following 
algorithm: 
 
INPUT: a list of mentions; 
OUTPUT: a splitting of the mentions into 
groups; 
ALGORIHTM: 
 
131
1. For each detected mention ANAP from the last to 
the first: 
 1.1 Find its most likely linked antecedant 
ANTE before ANAP 
1.2 if FOUND 
1.2.1 link all anaphors of ANAP to ANTE;  
1.2.2 link ANAP to ANTE 
 
Figure 2. Algorithm for Clustering Detected Mentions 
 
We used the probability value of the maximal 
entropy classifier?s output for weighting the links 
between mentions. 
4.2 Features 
The features we used in this step include: 
 
a. Source and Genre of a document; Speaker of a 
sentence; 
b. Sentence distance between the potential 
antecedent and anaphor; 
c. Syntactic tag of them, whether they are leaf 
node or not in the parsing tree; 
d. Syntactic tag bi-grams of them, and whether 
their syntactic tags are identical; 
e. Named entity tags of them, bi-gram of these 
tags, and whether they are identical; 
f. Syntactic tag path to root of them, bi-gram of 
these paths, and whether they are identical; 
g. Whether they are predicates; 
h. Features of anaphor: Its head predicates and 
types, words, pos tags, the words and pos tags 
of the left/right 3 neighbor tokens, and bi-
grams; 
i. Features of antecedent: Its head predicates and 
types, words, pos tags, the words and pos tags 
of the left/right 3 neighbor tokens, and bi-
grams; 
j. The number of identical words of the 
antecedent and the anaphor; 
k. The number of identical words in the 
neighbors (3 tokens before and after) of the 
antecedent and the anaphor. 
 
The above features include not only those 
suggested by Soon et al (2001), but also some 
context features, such as words within and out of 
the antecedent and the anaphor, and the 
overlapping number of the context words. Features 
about Gender and number agreements are not 
considered in our official system, as we failed to 
work out a generic solution to include them for all 
data of three different languages. 
5 Experiments 
5.1 Datasets  
The datasets of the CoNLL-2012 shared task 
contain three languages: Arabic (ARB), Chinese 
(CHN), and English (ENG). No predicted names 
and propositions are provided in the Arabic data, 
while no predicted names are given in the Chinese 
data. 
Tables 1 and 2 show statistical information of 
both training and development datasets for each 
language. 
 
Language # of Doc. 
# of 
Sent. 
# of 
Ment. 
# of 
mentions 
that do not 
correspond 
to a valid 
phrasal sub-
structure 
Dev 44 950 3,317 262(7.9%) 
ARB 
Train 359 7,422 27,590 2,176(7.9%) 
Dev 252 6,083 14,183 677(4.8%) 
CHN 
Train 1,810 36,487 102,854 6,345(6.2%) 
Dev 343 9,603 19,156 661(3.5%) 
ENG 
Train 2,802 75,187 155,560 4,639(3.0%) 
Table 1. Statistical information of the three language 
datasets (train and development) (part 1). 
 
# of 
sentences per 
document 
# of tokens 
per sentence Language 
Avg. Max Avg. Max 
Dev 21.59 41 29.82 160 
ARB 
Train 20.67 78 32.70 384 
Dev 24.14 144 18.09 190 
CHN 
Train 20.16 283 20.72 242 
Dev 28.00 127 16.98 186 
ENG 
Train 26.83 188 17.28 210 
Table 2. Statistical information of the three language 
datasets (train and development) (part 2). 
 
The total size of the uncompressed original data 
is about 384MB. The English dataset is the largest 
one containing 3,145 documents (343+2802), 
84,790 sentences, and 174,716 mentions. The 
Arabic dataset is the smallest one containing 403 
documents, 8,372 sentences, and 30,907 mentions. 
In the Arabic datasets, about 7.9% mentions do not 
132
correspond to a valid phrasal sub-structure. This 
number of the Chinese dataset is 6%, while that of 
English 3%. These small percentages verify that 
our assumption that a mention is expected to be a 
valid phrasal sub-structure is reasonable. 
The average numbers of sentences in a 
document in the three language datasets are 
roughly 21, 22, and 27 respectively, while the 
longest document that has 283 sentences is found 
in the Chinese train dataset. The average numbers 
of tokens in a sentence in the three language 
datasets are roughly 31, 19, and 17 respectively, 
while the longest sentence with 384 tokens is 
found in the Arabic train dataset. 
5.2 Experimental Results  
For producing the results on the test datasets, we 
combined both train and development datasets for 
training maximal entropy classifiers. 
The official score adopted by CoNLL-2012 is 
the unweighted average of scores on three 
languages, while for each language, the score is 
derived by averaging the three metrics MUC 
(Vilain et al 1995), B-CUBED (Bagga and 
Baldwin, 1998), and CEAF(E) (Constrained Entity 
Aligned F-measure)(Luo, 2005) as follows: 
MUC + B-CUBED + CEAF (E) 
OFFICIAL SCORE =  ---------------------------------------- 
                                  3 
Our system achieved the combined official score 
42.32 over three languages (Arabic, Chinese, and 
English). On each of the three languages, the 
system obtained scores 33.53, 46.27, and 45.85 
respectively. It performs poor on the Arabic dataset, 
but equally well on the Chinese and English 
datasets. 
Tables 3, 4, and 5 give the detailed results on 
three languages respectively. 
 
Metric Recall Precision F1 
MUC 10.77 55.60 18.05 
B-CUBED 36.17 93.34 52.14 
CEAF (M) 37.03 37.03 37.03 
CEAF (E) 55.45 20.95 30.41 
BLANC1 52.91 73.93 54.12 
OFFICIAL 
SCORE NA NA 33.53 
Table 3. Official results of our system on the Arabic test 
dataset. 
                                                          
1
 For this metric, please refer to (Recasens and Hovy, 2011). 
 
Metric Recall Precision F1 
MUC 32.48 71.44 44.65 
B-CUBED 45.51 86.06 59.54 
CEAF (M) 45.70 45.70 45.70 
CEAF (E) 55.11 25.24 34.62 
BLANC 64.99 76.63 68.92 
OFFICIAL 
SCORE NA NA 46.27 
Table 4. Official results of our system on the Chinese 
test dataset. 
 
Metric Recall Precision F1 
MUC 39.12 72.57 50.84 
B-CUBED 43.03 80.06 55.98 
CEAF (M) 41.97 41.97 41.97 
CEAF (E) 49.44 22.30 30.74 
BLANC 64.01 66.86 65.24 
OFFICIAL 
SCORE NA NA 45.85 
Table 5. Official results of our system on the English 
test dataset. 
 
Comparing the detailed scores, we found that 
our submitted system performs much poor on the 
MUC metric on the Arabic data. It can only 
recover 10.77% valid mentions. As a whole, the 
system works well in precision perspective but 
poor in recall perspective. 
 
Language Recall Precision F1 
Arabic 18.17 80.43 29.65 
Chinese 36.60 87.01 51.53 
English 45.78 86.72 59.93 
Table 6. Mention Detection Scores on the test datasets. 
 
Table 6 shows the official mention detection 
scores on the test datasets, which could be 
regarded as the performance upper bounds (MUC 
metric) of the mention clustering component. 
Taking the mention detection results as a basis, the 
mention clustering component could achieve 
roughly 60.88 (18.05/29.65), 86.65 (44.65/51.53), 
and 84.83 (50.84/59.93) for the Arabic, Chinese, 
and English data respectively. It seems that the 
performance of the whole system is highly 
bottlenecked by that of the mention detection 
component. However, it may not be true as the task 
requires removing singleton mentions that do not 
refer to any other mentions. To examine how 
133
singleton mentions affect the final scores, we 
conducted additional experiments on the 
development datasets. Table 7 shows the mention 
detection scores on the dev datasets. When we 
include the singletons, the mention detection 
scores become 59, 63.75, and 71.27 from 31.46, 
53.99, and 59.16 for the three language datasets 
respectively. They are reasonable and close to 
those that we can get at the mention clustering 
component. These analyses tell us that the 
requirement of removing singletons for scoring 
may deserve further study. At the same time, we 
realize that to get better performance we may need 
to re-design the feature sets (e.g. including more 
useful features like gender and number) and try 
some more powerful machine learning algorithms 
such as linear classification or Tree CRF (Bradley 
and Guestrin, 2010). 
 
Recall Precision F1 Language 
-Sing +Sing -Sing +Sing -Sing +Sing 
Arabic 19.42 47.58 82.88 77.61 31.46 59 
Chinese 39.05 53.78 87.43 78.24 53.99 63.75 
English 44.9 65.2 86.67 78.58 59.16 71.27 
Table 7. Mention Detection Scores on the development 
(Dev) datasets. ?-Sing? means without singletons, which 
is required by the task specification, while ?+Sing? 
means including singletons. 
 
Table 8. F1 scores of the two supplementary 
submissions with additional gold mention boundaries 
and gold mentions respectively. 
 
Besides the official submission for the task with 
predicted data, we also provide two supplementary 
submissions with gold mention boundaries and 
gold mentions respectively. Table 8 summarizes 
the scores of these two submissions. 
With gold mentions, our official system does 
achieve better performance with gain of 8.77 
(50.65-41.88). On Chinese data, we get the highest 
score 61.61. However, the system performs worse 
when the gold mention boundaries are available. 
The F1 score drops 2.62 from 41.88 to 39.26. We 
guess that more candidate mentions bring more 
difficulties for the maximal entropy classifier to 
make decisions. The best-of-all strategy may not 
be a good choice when a large number of 
candidates are available. More efforts are required 
to explore the real reason behind the results. 
6 Conclusions  
In this paper, we describe our system for the 
CoNLL-2012 shared task ? Modeling Multilingual 
Unrestricted Coreference in OntoNotes (closed 
track). Our system was built on machine learning 
strategy with a pipeline architecture, which 
integrated two cascaded components: mention 
detection and mention clustering. The system relies 
on successful syntactic analyses, which means that 
only valid sub-structures of sentences are 
considered as potential mentions. 
Due to limited time and resources, we had not 
conducted thorough enough experiments to derive 
optimal solutions, but the system and the 
involvement in this challenge do provide a good 
foundation for further study. It?s a success for us to 
finish all the submissions on time. In the future, we 
plan to focus on those mentions that do not 
correspond to a syntactic structure and consider 
introducing virtual nodes for them. We may also 
explore different strategies when linking an 
anaphor and its antecedent. In addition, maximal 
entropy may not be good enough for this kind of 
task. Therefore, we also plan to explore other 
powerful algorithms like large linear classification 
and tree CRF (Bradley and Guestrin, 2010; Ram 
and Devi, 2012) in the future. 
Acknowledgments 
This research was funded by HeNan Provincial 
Research Project on Fundamental and Cutting-
Edge Technologies (No. 112300410007). We used 
the Amazon Elastic Compute Cloud (Amazon EC2) 
web service in our experiments. We thank 
Amazon.com for providing such great service for 
not only industrial applications, but also academic 
research. 
With gold mention 
boundaries (39.26) 
With gold mentions 
(50.65) 
 
ARB CHN ENG ARB CHN ENG 
MUC 11.30 38.70 38.21 33.31 66.13 60.45 
B-CUBED 54.25 59.27 59.51 53.74 66.84 57.18 
 CEAF (M) 33.68 41.06 39.30 42.25 57.50 47.82 
CEAF (E) 28.84 31.86 31.39 34.81 46.83 36.58 
BLANC 51.46 61.47 61.33 57.96 73.47 67.12 
MD Score 29.78 51.90 51.08 52.58 77.73 72.75 
Official 
Score 31.46 43.28 43.04 40.62 59.93 51.40 
134
References  
Amit Bagga and Breck Baldwin. 1998. Algorithms for 
Scoring Coreference Chains. In Proceedings of the 
First International Conference on Language 
Resources and Evaluation Workshop on Linguistics 
Coreference. 
Adam L. Berger, Stephen A. Della Pietra, and Vincent J. 
Della Pietra. 1996. A Maximal Entropy Approach to 
Natural Language Processing. Computational 
Linguistics, 22(1):39-42. 
Joseph K. Bradley and Carlos Guestrin. 2010. Learning 
Tree Conditional Random Fields. In Proceedings of 
the International Conference on Machine Learning 
(ICML-2010). 
Nancy Chinchor. 2001. Message Understanding 
Coference (MUC) 7. In LDC2001T02. 
Nancy Chinchor and Beth Sundheim. 2003. Message 
Understanding Conference (MUC) 6. In 
LDC2003T13. 
G.G. Doddington, A. Mitchell, M. Przybocki, L. 
Ramshaw, S. Strassell, and R. Weischedel. 2000. The 
Automatic Content Extraction (ACE) Program: Tasks, 
Data, and Evaluation. In Proceedings of LREC-2000. 
Heeyoung Lee, Yves Peirsman, Angei Chang, 
Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky. 
2011. Stanford?s Multi-Pass Sieve Coreference 
Resolution System at the CoNLL-2011 Shared Task. 
In Proceedings of the 15th Conference on 
Computational Natural Language Learning: Shared 
Task. 
Xiaoqiang Luo. 2005. On coreference resolution 
performance metrics. In Proceedings of the Human 
Language Technology Conference and Conference 
on Empirical Methods in Natural Language 
Processing. 
George A. Miller. 1995. WordNet: a Lexical Database 
for English. Communications of the ACM. 38(11): 
39-41. 
Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha 
Palmer, Lance Ramshaw, Ralph Weischedel. 2007. 
OntoNotes: A Unified Relational Semantic 
Representation. International Journal of Semantic 
Computing, 1(4): 405-419. 
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, 
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual 
Unrestricted Coreference in OntoNotes. In 
Proceedings of the Sixteenth Conference on 
Computational Natural Language Learning (CoNLL 
2012): Shared Task. 
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, 
Martha Palmer, Ralph Weischedel and Nianwen Xue. 
2011. CoNLL-2011 Shared Task: Modeling 
Unrestricted Coreference in OntoNotes. In 
Proceedings of the 15th Conference on Computational 
Natural Language Learning: Shared Task, pp. 1-27. 
Vijay Sundar Ram R. and Sobha Lalitha Devi. 2012. 
Coreference Resolution Using Tree CRFs. In 
Proceedings of the 13th Conference on Computational 
Linguistics and Intelligent Text Processing 
(CICLing-2012). 
Marta Recasens and Eduard Hovy. 2011. Blanc: 
Implementing the rand index for coreference 
evaluation. Natural Language Engineering, 17(4): 
485-510. 
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung 
Yong Lim. 2001. A machine learning approach to 
coreference resolution of noun phrase. 
Computational Linguistics, 27(4): 521-544. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. 
Hirschman. 1995. A model theoretic coreference 
scoring scheme. In Proceedings of the Sixth Message 
Understanding Conference (MUC-6). 
Le Zhang. 2006. Maximum Entropy Modeling Toolkit 
for Python and C++. Software available at 
http://homepages.inf.ed.ac.uk/s0450736/maxent_tool
kit.html. 
 
135
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 119?123,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Recognizing English Learners? Native Language from Their Writings 
 
 
Baoli LI 
Department of Computer Science 
Henan University of Technology 
1 Lotus Street, High & New Technology Industrial Development Zone 
Zhengzhou, China, 450001 
csblli@gmail.com 
 
 
 
 
 
 
Abstract 
Native Language Identification (NLI), which 
tries to identify the native language (L1) of a 
second language learner based on their writ-
ings, is helpful for advancing second language 
learning and authorship profiling in forensic 
linguistics. With the availability of relevant 
data resources, much work has been done to 
explore the native language of a foreign lan-
guage learner. In this report, we present our 
system for the first shared task in Native Lan-
guage Identification (NLI). We use a linear 
SVM classifier and explore features of words, 
word and character n-grams, style, and 
metadata. Our official system achieves accu-
racy of 0.773, which ranks it 18th among the 
29 teams in the closed track. 
1 Introduction 
Native Language Identification (NLI) (Ahn, 2011; 
Kochmar, 2011), which tries to identify the native 
language (L1) of a second language learner based 
on their writings, is expected to be helpful for ad-
vancing second language learning and authorship 
profiling in forensic linguistics. With the availabil-
ity of relevant data resources, much work has been 
done to explore the effective way to identify the 
native language of a foreign language learner 
(Koppel et al, 2005; Wong et al, 2011; Brooke 
and Hirst, 2012a, 2012b; Bykh and Meurers, 2012; 
Crossley and McNamara, 2012; Jarvis et al, 2012; 
Jarvis and Paquot, 2012; Tofighi et al, 2012; Tor-
ney et al 2012). 
To evaluate different techniques and approaches 
to Native Language Identification with the same 
setting, the first shared task in Native Language 
Identification (NLI) was organized by researchers 
from Nuance Communications and Educational 
Testing Service (Tetreault et al, 2013). A larger 
and more reliable data set, TOEFL11 (Blanchard et 
al., 2013), was used in this open evaluation. 
This paper reports our NLI2013 shared task sys-
tem that we built at the Department of Computer 
Science, Henan University of Technology, China. 
To be involved in this evaluation, we would like to 
obtain a more thorough knowledge of the research 
on native language identification and its state-of-
the-art, as we may focus on authorship attribution 
(Koppel et al, 2008) problems in the near future. 
The NLI2013 shared task is framed as a super-
vised text classification problem where the set of 
native languages (L1s), i.e. categories, is known, 
which includes Arabic, Chinese, French, German, 
Hindi, Italian, Japanese, Korean, Spanish, Telugu, 
and Turkish. A system is given a large part of the 
TOEFL11 dataset for training a detection model, 
and then makes predictions on the test writing 
samples. 
Inspired by our experience of dealing with dif-
ferent text classification problems, we decide to 
employ a linear support vector machine (SVM) in 
our NLI2013 system. We plan to take this system 
as a starting point, and may explore other complex 
classifiers in the future. Although in-depth syntac-
119
tic features may be helpful for this kind of tasks 
(Bergsma et al, 2012; Wong and Dras, 2011; 
Swanson and Charniak, 2012; Wong et al, 2012), 
we decide to explore the effectiveness of the tradi-
tional word and character features, as well as style 
features, in our system. We would like to verify on 
the first open available large dataset whether these 
traditional features work and how good they are. 
 
 
 
Figure 1. System Architecture. 
 
We submitted four runs with different feature 
sets. The run with all the features achieved the best 
accuracy of 0.773, which ranks our system 18th 
among the 29 systems in the closed track. 
In the rest of this paper we describe the detail of 
our system and analyze the results. Section 2 gives 
the overview of our system, while Section 3 dis-
cusses the various features in-depth. We present 
our experiments and discussions in Section 4, and 
conclude in Section 5. 
2 System Description  
Figure 1 gives the architecture of our NLI2013 
system, which takes machine learning framework. 
At the training stage, annotated data is first pro-
cessed through preprocessing and feature extrac-
tion, then fed to the classifier learning module, and 
we can finally obtain a NLI model. At the testing 
stage, each test sample goes through the same pre-
processing and feature extraction modules, and is 
assigned a category with the learned NLI model. 
Data Preprocessing: this module aims at trans-
forming the original data into a suitable format for 
the system, e.g. inserting the category information 
into the individual writing sample and attaching 
metadata to essays. 
Feature Extraction: this module tries to obtain 
all the useful features from the original data. We 
considered features like: word, word n-gram, char-
acter n-gram, style, and available metadata. 
Linear SVM training and testing: these two 
modules are the key components. The training 
module takes the transformed digitalized vectors as 
input, and train an effective NLI model, where the 
testing module just applies the learned model on 
the testing data. As linear support vector machines 
(SVM) achieves quite good performance on a lot 
of text classification problems, we use this general 
machine learning algorithm in our NLI2013 system. 
The excellent SVM implementation, Libsvm 
(Chang and Lin, 2011), was incorporated in our 
system and TFIDF is used to derive the feature 
values in vectors. Then, we turn to focus on what 
features are effective for native language identifi-
cation. We explore words, word n-grams, character 
n-grams, style, and metadata features in the system. 
3 Features 
In this section, we explain what kind of features we 
used in our NLI2013 system. 
3.1 Word and Word n-gram 
The initial feature set is words or tokens in the da-
taset. As the dataset is tokenized and sen-
tence/paragraph split, we simply use space to 
delimit the text and get individual tokens. We re-
move rare features that appear only once in the 
training dataset. Words or tokens are transformed 
to lowercase. 
Word n-grams are combined by consecutive 
words or tokens. They are expecting to capture 
some syntactic characteristics of writing samples. 
Two special tokens, ?BOS? and ?EOS?, which in-
dicate ?Beginning? and ?Ending?, are attached at 
the two ends of a sentence. We considered word 2-
grams and word 3-grams in our system. 
3.2 Character n-gram 
120
We assume sub-word features like prefix and 
suffix are useful for detecting the learners? native 
languages. To simplify the process rather than 
employing a complex morphological analyzer, we 
consider character n-grams as another important 
feature set. The n-grams are extracted from each 
sentence by regarding the whole sentence as a 
large word / string and replacing the delimited 
symbol (i.e. white space) with a special uppercase 
character ?S?. As what we did in getting word n-
grams, we attached two special character ?B? and 
?E? at the two ends of a sentence. Character 2-
grams, 3-grams, 4-grams, and 5-grams are used in 
our system. 
3.3 Style 
We would like to explore whether the traditional 
style features are helpful for this task as those fea-
tures are widely used in authorship attribution. We 
include the following style features: 
? __PARA__: a paragraph in an essay; 
? __SENT__: a sentence in an essay; 
? PARASENTLEN=NN: a paragraph of NN 
sentences long; 
? SENTWDLEN=NN: a sentence of 4*NN 
words long; 
? WDCL=NN: a word of NN characters long; 
3.4 Other 
As the TOEFL11 dataset includes two metadata for 
each essay, English language proficiency level 
(high, medium, or low) and Prompt ID, we include 
them as additional features in our system. 
4 Experiments and Results 
4.1 Dataset 
The dataset of the NLI2013 shared task contains 
12,100 English essays from the Test of English as 
a Foreign Language (TOEFL). Educational Testing 
Service (ETS) published the dataset through the 
LDC with the motivation to create a larger and 
more reliable data set for researchers to conduct 
Native Language Identification experiments on. 
This dataset, henceforth TOEFL11, comprises 11 
native languages (L1s) with 1,000 essays per lan-
guage. The 11 covered native languages are: Ara-
bic, Chinese, French, German, Hindi, Italian, 
Japanese, Korean, Spanish, Telugu, and Turkish. 
In addition, each essay in the TOEFL11 is marked 
with an English language proficiency level (high, 
medium, or low) based on the judgments of human 
assessment specialists. The essays are usually 300 
to 400 words long. 9,900 essays of this set are cho-
sen as the training data, 1,100 are for development 
and the rest 1,100 as test data.  
 
Runs HAUTCS-1 HAUTCS-2 HAUTCS-3 HAUTCS-4 
Accuracy 0.773 0.758 0.76 0.756 
ARA 0.7311 0.703 0.703 0.71 
CHI 0.82 0.794 0.794 0.782 
FRE 0.806 0.788 0.786 0.783 
GER 0.897 0.899 0.899 0.867 
HIN 0.686 0.688 0.694 0.707 
ITA 0.83 0.84 0.844 0.844 
JPN 0.832 0.792 0.798 0.81 
KOR 0.763 0.764 0.768 0.727 
SPA 0.703 0.651 0.651 0.65 
TEL 0.702 0.702 0.702 0.751 
TUR 0.736 0.715 0.716 0.698 
 
Table 1. Official results of our system. 
 
 
Figure 2. Performance of our official runs. 
 
4.2 Official Results 
Accuracy, which measures the percentage of how 
many essays are correctly detected, is used as the 
main evaluation metric in the NLI2013 shared task.  
Table 1 gives the official results of our system 
on the evaluation data. We submitted four runs 
with different feature sets: 
HAUTCS-1: all the features, which include 
words, word 2-grams, word 3-grams, character 2-
grams, character 3-grams, character 4-grams, 
                                                          
1
 This number, as well as others in the cells from this row to 
the bottom, is value of F-1 measure for each language. 
121
character 5-grams, style, and other metadata fea-
tures; 
HAUTCS-2:  uses words, word 2-grams, word 
3-grams, style, and other metadata features; 
HAUTCS-3: uses words, word 2-grams, word 
3-grams, and other metadata features; 
HAUTCS-4: uses words or tokens and other 
metadata features. 
For the runs HAUTCS-2, HAUTCS-3, and 
HAUTCS-4, we combined the development and 
training data for learning the identification model, 
where for the HAUTCS-1, it?s a pity that we forgot 
to include the development data for training the 
model. 
Our best run (HAUTCS-1) achieved the overall 
accuracy (0.773). The system performs best on the 
German category, but poorest on the Hindi catego-
ry, as can be easily seen on figure 2. 
Analyzing the four runs? performance showing 
on figure 2, we observe: word features are quite 
effective for Telugu and Hindi categories, but not 
powerful enough for others; word n-grams are 
helpful for languages Chinese, French, German, 
Korean, and Turkish, but useless for others; Style 
features only boost a little for French; Character n-
grams work for Arabic, Chinese, French, Japanese, 
Spanish, and Turkish; Spanish category prefers 
character n-grams, where Telugu category likes 
word features. As different features have different 
effects on different languages, a better NLI system 
is expected to use different features for different 
languages. 
After the evaluation, we experimented with the 
same setting as the HAUTCS-1 run, but included 
both training and development data for learning the 
NLI model. We got accuracy 0.781 on the new 
released test data, which has the same format with 
paragraph split as the training and development 
data. 
As we include style features like how many par-
agraphs in an essay, the old test data, which re-
moved the paragraph delimiters (i.e. single blank 
lines), may be not good for our trained model. 
Therefore, we did experiments with the new test 
data. Unfortunately, the accuracy 0.772 is a little 
poorer than that we obtained with the old test data. 
It seems that the simple style features are not effec-
tive in this task. As shown in table 1, HAUTCS-2 
performs poorer than HAUTCS-3, which helps us 
derive the same conclusion. 
4.3 Additional Experiments 
We did 10-fold cross validation on the training and 
development data with the same setting as the 
HAUTCS-1 run. The data splitting is given by the 
organizers. Accuracies of the 10 runs are show in 
table 2. The overall accuracy 0.799 is better than 
that on the test data. 
 
Fold 1 2 3 4 5 
Accuracy 0.802 0.795 0.81 0.791 0.79 
Fold 6 7 8 9 10 
Accuracy 0.805 0.789 0.803 0.798 0.805 
Table 2. Results of 10-fold cross validation on the train-
ing and development data. 
 
To check how metadata features work, we did 
another run HAUTCS-5, which uses only words as 
features. This run got the same overall accuracy 
0.756 on the old test data as HAUTCS-4 did, 
which demonstrates that those metadata features 
may not provide much useful information for na-
tive language identification. 
5 Conclusion and Future Work 
In this paper, we report our system for the 
NLI2013 shared task, which automatically detect-
ing the native language of a foreign English learner 
from her/his writing sample. The system was built 
on a machine learning framework with traditional 
features including words, word n-grams, character 
n-grams, and writing styles. Character n-grams are 
simple but quite effective. 
We plan to explore syntactic features in the fu-
ture, and other machine learning algorithms, e.g. 
ECOC (Li and Vogel, 2010), also deserve further 
experiments. As we discussed in section 4, we are 
also interested in designing a framework to use 
different features for different categories. 
 
Acknowledgments 
This work was supported by the Henan Provincial 
Research Program on Fundamental and Cutting-
Edge Technologies (No. 112300410007), and the 
High-level Talent Foundation of Henan University 
of Technology (No. 2012BS027). Experiments 
were performed on the Amazon Elastic Compute 
Cloud. 
122
References  
Ahn, C. S. 2011. Automatically Detecting Authors' Na-
tive Language. Master's thesis, Naval Postgraduate 
School, Monterey, CA. 
Bergsma, S., Post, M., and Yarowsky, D. 2012. Stylo-
metric analysis of scientific articles. In Proceedings 
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics: 
Human Language Technologies, pages 327?337, 
Montr?al, Canada. Association for Computational 
Linguistics. 
Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., and 
Chodorow, M. 2013. TOEFL11: A Corpus of Non-
Native English. Technical report, Educational Test-
ing Service. 
Brooke, J. and Hirst, G. 2012a. Measuring interlanguage: 
Native language identification with l1-influence met-
rics. In Calzolari, N., Choukri, K., Declerck, T., 
Dogan, M. U., Maegaard, B., Mariani, J., Odijk, J., 
and Piperidis, S., editors, Proceedings of the Eighth 
International Conference on Language Resources and 
Evaluation (LREC-2012), pages 779?784, Istanbul, 
Turkey 
Brooke, J. and Hirst, G. 2012b. Robust, Lexicalized 
Native Language Identification. In Proceedings of 
COLING 2012, pages 391-408, Mumbai, India.  
Bykh, S. and Meurers, D. 2012. Native Language Iden-
tification using Recurring n-grams - Investigating 
Abstraction and Domain Dependence. In Proceedings 
of COLING 2012, pages 425-440, Mumbai, India. 
Chang, C.-C. and Lin C.-J. 2011. LIBSVM : a library 
for support vector machines. ACM Transactions on 
Intelligent Systems and Technology, 2:3:27:1-27. 
Crossley, S. A. and McNamara, D. 2012. Detecting the 
First Language of Second Language Writers Using 
Automated Indices of Cohesion, Lexical Sophistica-
tion, Syntactic Complexity and Conceptual 
Knowledge. In Jarvis, S. and Crossley, S. A., editors, 
Approaching Language Transfer through Text Classi-
fication, pages 106-126. Multilingual Matters. 
Jarvis, S., Casta?eda-Jim?nez, G., and Nielsen, R. 2012. 
Detecting L2 Writers' L1s on the Basis of Their Lex-
ical Styles. In Jarvis, S. and Crossley, S. A., editors, 
Approaching Language Transfer through Text Classi-
fication, pages 34-70. Multilingual Matters. 
Jarvis, S. and Paquot, M. 2012. Exploring the Role of n-
Grams in L1 Identification. In Jarvis, S. and Crossley, 
S. A., editors, Approaching Language Transfer 
through Text Classification, pages 71-105. Multilin-
gual Matters. 
Kochmar, E. 2011. Identification of a writer?s native 
language by error analysis. Master?s thesis, Universi-
ty of Cambridge. 
Koppel, M., Schler, J., and Zigdon, K. 2005. Determin-
ing an author?s native language by mining a text for 
errors. In Proceedings of the eleventh ACM 
SIGKDD international conference on Knowledge 
discovery in data mining, pages 624?628, Chicago, 
IL. ACM. 
Koppel, M., Schler, J., and Argamon, S. 2008. Compu-
tational methods in authorship attribution. Journal of 
the American Society for information Science and 
Technology, 60(1):9?26. 
Li, B., and Vogel, C. 2010. Improving Multiclass Text 
Classification with Error-Correcting Output Coding 
and Sub-class Partitions. In Proceedings of the 23rd 
Canadian Conference on Artificial Intelligence, pag-
es 4-15, Ottawa, Canada. 
Swanson, B. and Charniak, E. 2012. Native language 
detection with tree substitution grammars. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short 
Papers), pages 193?197, Jeju Island, Korea. 
Tetreault, J., Blanchard, D., and Cahill, A. 2013.  A 
report on the first native language identification 
shared task.  In Proceedings of the Eighth Workshop 
on Innovative Use of NLP for Building Educational 
Applications.  Atlanta, GA, USA. 
Tofighi, P.; K?se, C.; and Rouka, L.  2012.  Author?s 
native language identification from web-based 
texts.  International Journal of Computer and Com-
munication Engineering. 1(1):47-50 
Torney, R.; Vamplew, P.; and Yearwood, 
J.  2012.  Using psycholinguistic features for profil-
ing first language of authors.  Journal of the Ameri-
can Society for Information Science and 
Technology.  63(6):1256-1269. 
Wong, S.-M. J. and Dras, M. 2011. Exploiting Parse 
Structures for Native Language Identification. In 
Proceedings of the 2011 Conference on Empirical 
Methods in Natural Language Processing, pages 
1600?1610, Edinburgh, Scotland, UK. 
Wong, S.-M. J., Dras, M., and Johnson, M. 2012. Ex-
ploring Adaptor Grammars for Native Language 
Identification. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, pages 699?709, Jeju Island, Korea. 
123
