Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 180?183,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Preliminary Experience with Amazon?s Mechanical Turk  for Annotating Medical Named Entities  Meliha Yetisgen-Yildiz, Imre Solti  Fei Xia, Scott Russell Halgrim Biomedical & Health Informatics Department of Linguistics University of Washington University of Washington Seattle, WA 98195, USA Seattle, WA 98195, USA {melihay,solti}@uw.edu {fxia,captnpi}@uw.edu  Abstract Amazon?s Mechanical Turk (MTurk) service is becoming increasingly popular in Natural Language Processing (NLP) research. In this paper, we report our findings in using MTurk to annotate medical text extracted from clini-cal trial descriptions with three entity types: medical condition, medication, and laboratory test. We compared MTurk annotations with a gold standard manually created by a domain expert. Based on the good performance re-sults, we conclude that MTurk is a very prom-ising tool for annotating large-scale corpora for biomedical NLP tasks. 1 Introduction The manual construction of annotated corpora is ex-tremely expensive both in terms of time and money. Snow et al (2008) demonstrated the potential power of Amazon?s Mechanical Turk (MTurk) service in annotat-ing large corpora for natural language tasks cheaply and quickly. We are working on a Natural Language Proc-essing (NLP) project to automate the clinical trial eligi-bility screening of patients. This project involves building statistical models for medical named entity recognition which requires a large-scale annotated cor-pus for training. As part of corpus development, we tested the feasibility of using MTurk for the annotation of medical named entities in biomedical text and we report our findings in this paper. In the following sections we describe how we used MTurk to annotate the biomedical corpus created from publicly available clinical trial announcements. The main goal of our study was to understand how well non-experts perform compared to medical expert in annotat-ing the biomedical text.  2 Related Work MTurk1 is an online micro-task market that allows re-questers to distribute work to a large number of workers from all over the world. The inspiration of the system                                                            1 https://www.MTurk.com/MTurk/welcome 
was to have human workers complete simple tasks that would otherwise be extremely difficult for computers to perform (Kittur et al, 2008). A complex task is broken down into simple, one-time tasks called Human Intelli-gence Tasks (HITs). Requesters post their HITs on the MTurk marketplace by specifying the amount paid for the completion of each task, and the workers select from the available HITs the ones that they would like to work on. In 2007, Amazon claimed that the user base of MTurk consisted of over 100,000 users from 100 coun-tries2. MTurk has been adopted for a variety of uses both in industry and academia, ranging from user studies (Kittur et al, 2008) to image labeling (Sorokin and Forsyth, 2008). Snow et al (2008) examined the quality of labels created by MTurk workers for various NLP tasks in-cluding word sense disambiguation, word similarity, text entailment, and temporal ordering. Since the publi-cation of Snow et al?s paper, MTurk has become in-creasingly popular as an annotation tool for NLP research. Nakov (2008) used MTurk to create a manu-ally annotated resource for noun-noun compound inter-pretation based on paraphrasing verbs. In a different NLP task, Callison-Burch (2009) used MTurk to evalu-ate machine translation quality. With a budget of only $10, Callison-Burch demonstrated the feasibility of per-forming manual evaluations of machine translation quality by recreating judgments from a WMT08 transla-tion task.  In our pilot study we used MTurk to annotate entities in the biomedical text. To our knowledge, this is the first study that investigates the feasibility of MTurk for bio-medical named entity annotation. 3 Annotation Task Description In this section we will describe the types of entities in our annotation task and the details of our corpus crea-tion process.  
                                                           2 Source: New York Times article  ?Artificial Intelligence, With Help from the Humans? , Available at: http://www.nytimes.com/2007/03/25/business/yourmoney/25Stream.html 
180
3.1 Entity Types We used MTurk to annotate the biomedical text for the following three entity types:  ? Medical Conditions Example: First-degree relative who developed <Medical_Condition>breast cancer </Medical_Condition> at ? 50 years of age. ? Medications Example: Previous treatment with an <Medica-tion>anthracycline</Medication> in the metas-tatic breast cancer setting. ? Laboratory Test Example: <Laboratory_Test>Platelet count >=100,000 cells/mL</Laboratory_Test>. 3.2 Corpus  Our corpus came from the publicly available clinical trial announcements available at the ClinicalTrials.gov website. This website is a registry of federally and pri-vately supported clinical trials conducted in the United States and around the world. The objectives and proce-dures of each clinical trial are explained in detail along with participant selection criteria and logistical informa-tion such as locations and contact information.  For this task we selected 50,109 announcements from the roughly 85,000 announcements posted on the Clini-calTrials.gov site. For selection criteria we relied on the following keywords: "heart | cancer | tumor | influenza | alzheimer | parkinson | malignant | stroke | respiratory | diabetes | pneumonia | nephritis | nephrotic | nephrosis | septicemia | liver | cirrhosis | hypertension | renal | neo-plasm". We chose these keywords because they were part of the phrases of diagnoses for the top 12 leading causes of death excluding suicide, homicide and acci-dents (Heron et al, 2009). We limited the selection to trials for "Adult" or "Senior" patients.  After downloading the corpus of XML files we con-verted them to ANSI text using ABC Amber XML Converter3. 49,794 files successfully converted to ANSI text format. Using a simple regular expression search we selected documents that had both the "Inclusion Cri-teria" and "Exclusion Criteria" phrases. The final selec-tion process resulted in 35,385 files. From this latest set we randomly selected 100 files to build the corpus for our pilot study. One of the authors, who has medical training, then manually annotated the three entity types in those selected files. We used this annotated set as the gold standard to measure the quality of the MTurk workers? annotations.  4 HIT Design                                                             3 ABC Amber XML Converter. Available at: http://www.processtext.com/abcxml.html. 
Biomedical text is full of jargon, and finding the three entity types in such text can be difficult for non-expert annotators. To make the annotation task more conven-ient for the MTurk workers, we used a customized user interface and provided detailed annotation guidelines. We also tested the bonus system available in the MTurk environment and evaluated the performance of the workers. 4.1 User Interface In order to adapt the task of entity annotation to the MTurk format, we used an in-house web-based graphi-cal user interface that allows the worker to select a span of text with the mouse cursor. The interface also uses simple tokenization heuristics to divide the text into highlightable spans and resolve partial token highlights or double-clicks into the next largest span. For instance, highlighting the word ?cancer? from the second ?c? to ?e? will result in the entire span ?cancer? being high-lighted.  4.2 Annotation Guidelines We created three separate annotation tasks, one for each entity type. For each task, we wrote annotation guide-lines that explained the task and showed examples of entities that should be tagged and the ones that should not.  4.3 Bonus System MTurk provides two methods for paying workers ? fixed rates on each document and bonuses to workers for especially good work. In this study, we experi-mented with the bonus system to see its effect on per-formance and annotation time. Annotating a document would receive a base rate of $0.01-$0.05, but each tagged entity span could elicit a bonus of $0.01. The base rate would cover the case where the document truly contained no entities, but the bonus amount could potentially be much larger than the base rate if the document was entity-rich. Bonuses for each tagged en-tity span were awarded based on an agreement threshold with peer workers. In this study, each document was annotated by four workers and we granted bonuses for entity spans that were agreed upon by at least three workers. 4.4 Performance Monitoring We monitored a worker?s performance by comparing the worker?s annotations with his/her peer workers? annotations. After we posted the HITs, we continuously monitored the workers? performance and rejected the annotations from the ones who tried to cheat the system by either not doing any annotations (e.g., immediately submitting the document after accepting it) or con-
181
stantly doing wrong annotations (e.g., always annotating the first word of the text). Those rejected documents were automatically re-posted on the MTurk so other workers could work on them. In this pilot study, per-formance monitoring was done mainly manually. As future work, we plan to automate the process in order to scale it for larger annotation tasks. 4.5 Communication with Workers The workers could send us their questions and com-ments about the individual documents or the general annotation task through a text box in the interface. Dur-ing this study we received more than 100 messages from the workers. The majority of the messages were positive messages (?thank you?, ?easy hit!?). However, some of the comments included questions such as: ?Is pregnancy a medical condition?? or ?Text doesn?t men-tion the type of insulin but I highlighted it because insu-lin is a medication!?. We responded to the questions in a timely manner to increase the quality of annotations.  5 Annotation Experiments In our annotation experiments, each of 100 documents in our corpus was annotated by four workers, resulting in 100?4=400 files per experiment. We experimented with different pay scales to understand how they affect the quality and speed of the annotations. 5.1 Cost of Annotations We investigated the cost of annotations both in terms of money and time. The summary of the results is in Table 1. We ran five different MTurk annotation experiments for our corpus of 100 documents. A total of 139 workers were involved in our experiments, and we identified eight of those workers as cheaters and rejected their annotation. The remaining workers spent 138.86 hours to complete 1872 files. The slowest experiment was MedicalCondition-I, in which we paid a base document rate of $0.01 without any bonuses. With this pay scale, it took 71.16 hours for workers to annotate 272 out of 400 files. We suspected we could not attract enough 
workers to finish the annotation task on time so we stopped the experiment before all 400 files were com-pleted. When we compiled the results, we noticed that there was a general tendency for the workers to tag the first one or two entities and then ignore the rest of the document. Based on this observation, we decided to add bonuses to motivate the workers to read through the whole document. We ran the same annotation task, MedicalCondition-II, with a higher base document rate of $0.05 and a bonus rate of $0.01. With this new pay-ment scale the annotation task was fully completed in 7.28 hours.  We also compared the effect of base rates when the bo-nus amounts were kept the same. For medication anno-tations, increasing the base document rate from $0.01 to $0.05 decreased the total amount of annotation time from 31.65 hours to 4.36 hours and also decreased the number of workers from 45 to 17. We ordered the workers based on the number files they annotated. The top ranked 5 workers in Medication-I annotated 187 files (46%) and the top ranked 5 workers in Medication-II annotated 313 files (78%). The difference between those two values was interesting since it indicated that by increasing the base rate, we managed to attract work-ers who worked on more documents.  The average amount of time workers spent per docu-ment varied based on entity type. They spent the longest amount of time for medical condition and shortest amount of time for laboratory test. This can be ex-plained by the richness of documents in terms of enti-ties. In the manually created gold standard there were 1159 mentions of medical condition, 518 mentions of medication, and 249 mentions of laboratory tests. An-other observation was that the change in pay scales did not affect the average annotation time per document. 5.2 Quality of Annotations We measured the quality of the MTurk annotations at different inter-annotator agreement levels by comparing the agreed entity spans with the spans in the gold stan-dard.  
Table 1. Cost analysis of annotation experiments (?File? in this table  means the annotation of a document. There are 100 documents, and each document is  annotated by four workers.) MONETARY COST TIME COST File Count Pay Rate ($) Total Cost ($) Completion Time Experiment Label Total Completed Total Worker Count File Bonus File Bonus Per file  (seconds) Total (hours) MedicalCondition-I 400 272 45 0.01 0 2.72 0 156.09 71.16 MedicalCondition-II 400 400 30 0.05 0.01 20 22.61 162.66 7.28 Medication-I 400 400 45 0.01 0.01 4 4.43 87.96 31.65 Medication-II 400 400 17 0.05 0.01 20 6.11 89.06 4.36 Laboratory Test  400 400 26 0.05 0.01 20 1.49 75.61 24.41  
182
Given a document annotated by multiple workers and an agreement level k, there are different ways of creating a new span file that includes only the spans that are agreed by at least k workers. One method is to go over each span in each annotation and output only the spans that are marked by at least k workers. This method does not work well when the spans are long and the workers could disagree on the boundary. We used an alternative method which first goes over each word position in the document and marks the positions that are part of spans in at least k annotations, and then outputs the spans that cover those marked positions. We call the new span file agreement-k file. Once we have created agreement-k file, we compare it with the gold standard to calculate precision, recall, and F-measure. A span in agreement-k file and a span in the gold standard are called an exact match if they are iden-tical and are called an overlap match if they overlap (exact match is a special case of overlap match). Table 2 shows the performance for the MedicalCondition-II, Medication-II, and LaboratoryTest experiments at dif-ferent agreement levels (k). As can be seen from the table, as the value of k increased, the precision values increased and the recall values decreased. For all of the experiments, the best F-Score was achieved at agree-ment-level 2.  Of the three entity types, laboratory test was the hardest partly because laboratory test entities tend to be longer (the average length for entities in gold standard was 5.25 words, compared to 1.84 words for medication and 3.18 words for medical condition), making the exact boundary harder to define. The results for MedicalCon-dition-II and Medication-II were higher than Laborato-ryTest. In addition, accuracy for Medication-I (not shown here due to space limit) and Medication-II were similar, indicating that pay rate did not affect accuracy much in our experiments. In the future, we plan to in-crease the number of annotations for each document, which we believe could further improve the perform-ance.  6   Conclusion Human annotation is crucial for many NLP tasks. In this paper, we demonstrated the potential of using MTurk 
for annotating medical text. By continuously monitoring the workers? performance and using the bonus system, we acquired high quality annotations from non-expert MTurk workers with limited time and budget.  As future work, we plan to analyze the MTurk annota-tions in detail in order to understand the problematic areas. Based on our observations, we will redesign our annotation tasks and continue our experiments with MTurk to create large-scale annotated corpora to be used in biomedical NLP projects.  Acknowledgement This project was supported in part by NIH Grants 1K99LM010227-0110 and 5 U54 LM008748. References  [1] Chris Callison-Burch. 2009. Fast, Cheap, and Crea-tive: Evaluating Translation Quality Using Amazon?s Mechanical Turk. In Proceedings of EMNLP?09. [2] Melonie Heron, Donna L. Hoyert, Sherry L. Mur-phy, Jiaquan Xu, Kenneth D. Kochanek, and Bet-zaida Tejada-Vera. 2009. Deaths: Final data for 2006. National Vital Statistics Reports, 57:14. [3] Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008. Crowdsourcing User Studies with Mechanical Turk. In Proceedings of CHI?08. [4] Preslav Nakov. 2008. Noun compound interpretation using paraphrasing verbs: Feasibility study. In Pro-ceedings of the 13th international conference on Arti-ficial Intelligence: Methodology, Systems and Applications (AIMSA 2008), 103?117. [5] Philip V. Ogren. 2006. Knowtator: a prot?g? plug-in for annotated corpus construction. In Proceedings NAACL HLT?06, 273-275. [6] Rion Snow, Brendan O'Connor, Daniel Jurafsky and Andrew Y. Ng. 2008. Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natu-ral Language Tasks. In Proceedings of EMNLP?08, 254-263.  [7] Alexander Sorokin and David Forsyth. Utility data annotation with Amazon Mechanical Turk. In Pro-ceedings of Computer Vision and Pattern Recogni-tion Workshop at CVPR?08.  
Table 2. Quality measurement of MTurk annotations (k: Agreement level, P: Precision, R: Recall, F: F-measure; the highest value for each column is in boldface) Medical Condition-II Medication-II Laboratory Test Exact Overlap Exact Overlap Exact Overlap k P R F P R F P R F P R F P R F P R F 1 0.51 0.66 0.58 0.70 0.99 0.79 0.43 0.73 0.54 0.50 0.84 0.62 0.30 0.52 0.38 0.42 0.73 0.53 2 0.64 0.66 0.65 0.84 0.87 0.86 0.71 0.66 0.68 0.79 0.73 0.76 0.47 0.43 0.45 0.72 0.65 0.68 3 0.63 0.52 0.57 0.89 0.73 0.80 0.78 0.38 0.51 0.93 0.45 0.61 0.29 0.13 0.18 0.86 0.40 0.54 4 0.60 0.31 0.41 0.93 0.48 0.63 0.76 0.10 0.18 0.89 0.12 0.21 0.05 0.00 0.01 1.00 0.08 0.14  
183
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 61?67,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Medication Information from Discharge Summaries  Scott Halgrim, Fei Xia, Imre Solti, Eithon Cadag ?zlem Uzuner University of Washington University of Albany, SUNY PO Box 543450 135 Western Ave Seattle, WA 98195, USA Albany, NY 12222, USA   {captnpi,fxia,solti,ecadag}@uw.edu ouzuner@uamail.albany.edu     
 
Abstract Extracting medication information from clinical records has many potential appli-cations and was the focus of the i2b2 challenge in 2009. We present a hybrid system, comprised of machine learning and rule-based modules, for medication information extraction. With only a hand-ful of template-filling rules, the system?s core is a cascade of statistical classifiers for field detection. It achieved good per-formance that was comparable to the top systems in the i2b2 challenge, demon-strating that a heavily statistical ap-proach can perform as well or better than systems with many sophisticated rules. The system can easily incorporate addi-tional resources such as medication name lists to further improve performance.   
1 Introduction   Narrative clinical records store patient medical information, and extracting this information from these narratives supports data management and enables many applications (Levin et al, 2007). Informatics for Integrating the Biology and the Bedside (i2b2) is an NIH-funded National Center for Biomedical Computing based at Partners HealthCare System, and it has organized annual NLP shared tasks and challenges since 2006 (https://www.i2b2.org/). The Third i2b2 Workshop on NLP Challenges for Clinical Records in 2009 studied the extraction of medication information from hospital discharge summaries 
(https://www.i2b2.org/NLP/Medication/), a task we refer to as the i2b2 challenge in this paper.      In the past decade, there has been extensive re-search on information extraction in both the gen-eral and biomedical domains (Wellner et al, 2004; Grenager et al, 2005; Poon and Domingos, 2007; Meystre et al 2008; Rozenfeld and Feldman, 2008). Interestingly, despite the recent prevalence of statistical approaches in most NLP tasks (in-cluding information extraction), most of the sys-tems developed for the i2b2 challenge were rule-based. In this paper we present our hybrid system, whose core is a cascade of statistical classifiers that identify medication fields such as medication names and dosages. The fields are then assembled to form medication entries. While our system did not participate in the i2b2 challenge (as we were part of the organizing team), it achieved good re-sults that matched the top i2b2 systems.  
2 The i2b2 Challenge This section provides a brief introduction to the i2b2 challenge. 2.1 The task The i2b2 challenge studied the automatic extrac-tion of information corresponding to the following fields from hospital discharge summaries (Uzuner, et al, 2010a): names of medications (m) taken by the patient, dosages (do), modes (mo), frequencies (f), durations (du), and reasons (r) for taking these medications. We refer to the medication field as the name field and the other five fields as the non-name fields. All non-name fields correspond to some name field mention; if they were specified within a two-line window of that name mention, 
61
the i2b2 challenge required such fields to be linked to the name field to form an entry. For each entry, a system must determine whether the entry ap-peared in a list of medications or in narrative text. Table 1 shows an excerpt from a discharge sum-mary and the corresponding entries in the gold standard. The first entry appears in narrative text, and the second in a list of medication information.  Excerpt  of  Discharge Summary 55   the patient noted that he had a recurrence of this  56   vague chest discomfort as he was sitting and  57   talking to friends. He took a sublingual  58   Nitroglycerin without relief. ... 65  Flomax ( Tamsulosin  )  0.4 mg,  po, qd,... Gold  standard: m=?Nitroglycerin? 58:0 58:0 ||do=?nm?||       mo=?sublingual? 57:6 57:6 ||f=?nm? ||du=?nm? ||      r=?vague chest discomfort? 56:0 56:2 ||     ln=?narrative? ... m="flomax ( tamsulosin )" 65:0 65:3||do="0.4 mg"    65:4 65:5||mo="po" 65:6 65:6||f="qd" 65:7    65:7||du="nm"||r="nm"||ln="list"  Table 1: A sample discharge summary excerpt and the corresponding entries in the gold standard. The fields inside an entry are separated by ?||?. Each field is represented by the string and its position (i.e., ?line number: token number? offsets). ?nm? means the field value is not mentioned for this me-dication name.  2.2 Data Sets The i2b2 challenge used a total of 1243 discharge summaries: ? 696 of these summaries were released to par-ticipants for system development, and the i2b2 organizing team provided the gold standard annotation for 17 of them.   ? Participating teams could choose to annotate more files themselves. The University of Syd-ney team annotated 145 out of the 696 summa-ries (including re-annotating 14 of the 17 files annotated by the i2b2 organizing team) and generously shared their annotations with i2b2 after the challenge for future research. We ob-tained and used 110 of their annotations as our training set and the remaining 35 summaries as our development set. 
 ? The participating teams produced system out-puts for 547 discharge summaries set aside for testing. After the challenge, 251 of these sum-maries were annotated by the challenge par-ticipants, and these 251 summaries formed the final test set (Uzuner et al, 2010b).     The sizes of the data sets used in our experiments are shown in Table 2. The training and develop-ment sets were created by the University of Syd-ney, and the test data is the i2b2 official challenge test set. The average number of entries and fields vary among the three sets because the summaries in the test set were chosen randomly from the 547 summaries, whereas the University of Sydney team annotated the longest summaries. 
2.3 Additional resources Besides the training data, the participating teams were allowed to use any additional tools and re-sources that they had access to, including resources not available to the public. All challenge partici-pants used additional resources such as UMLS (www.nlm.nih.gov/research/umls/), but the exact resources used varied from team to team. There-fore, the challenge was similar to the so-called open-track challenge in the general NLP field, as opposed to a closed-track challenge that could re-quire that all the participants use only the list of resources specified by the challenge organizers  2.4 Evaluation metrics  The i2b2 challenge used two sets of evaluation metrics:  horizontal and vertical metrics. Horizon-tal metrics measured system performance at the entry level, whereas vertical metrics measured sys-tem performance at the field level. Both sets of metrics compared the system output and the gold standard at the span level for exact match and at the token level for inexact match, using precision, recall, and F-score (Uzuner et al, 2010a). The pri-mary metric for the challenge is exact horizontal F-score, which is the metric we use to evaluate our system.      
62
 Table 2: The data sets used in our experiments. The numbers in parentheses are the average numbers of entries or fields per discharge summary.   2.5 Participating systems Twenty teams participated in the challenge. Fifteen teams used rule-based approaches, and the rest used statistical or hybrid approaches. The perform-ances of the top five systems are shown in Table 3. Among them, only the top system, developed by the University of Sydney, used a hybrid approach, whereas the rest were rule-based.  Rank Precision Recall F-score 1 89.6 82.0 85.7 2 84.0 80.3 82.1 3 86.4 76.6 81.2 4 78.4 82.3 80.3 5 84.1 75.8 79.7 Table 3: The performance (exact horizontal preci-sion/recall/F-score) of the top five i2b2 systems on the test set.  3 System description   We developed a hybrid system with three process-ing steps: (1) a preprocessing step that finds sec-tion boundaries, (2) a field detection step that identifies the six fields, and (3) a field linking step that links fields together to form entries. The sec-ond step is a statistical system, whereas the other two steps are rule-based. The second step was the main focus of this study. 3.1 Preprocessing  In addition to common processing steps such as part-of-speech (POS) tagging, our preprocessing 
step includes a section segmenter that breaks dis-charge summaries into sections. Discharge summa-ries tend to consist of sections such as ?ADMIT DIAGNOSIS?, ?PAST MEDICAL HISTORY?, and ?DISCHARGE MEDICATIONS?. Knowing section boundaries is important for the i2b2 chal-lenge because, according to the annotation guide-lines for creating the gold standard, medications occurring under certain sections (e.g., family his-tory and allergic reaction) should be excluded from the system output. Furthermore, knowing the types of sections could be useful for field detection and field linking; for example, entries in the ?DISCHARGE MEDICATIONS? section are more likely to appear in a list of medications than in nar-rative text. The set of sections and the exact spelling of section headings vary across discharge summaries. The section segmenter uses regular expressions (e.g., ?^\s*([A-Z\s]+):? -- a line starting with a se-quence of capitalized words followed by a colon) to collect potential section headings from the train-ing data, and the headings whose frequencies are higher than a threshold are used to identify section boundaries in the discharge summaries.  3.2 Field detection  This step consists of three modules: the first mod-ule, find_name, finds medication names in a dis-charge summary; the second module, context_type, processes each medication name identified by find_name and determines whether the medication appears in narrative text or in a list of medications; the third module, find_others, detects the five non-name field types.      For find_name and find_others, we follow the common practice of treating named-entity (NE) detection as a sequence labeling task with the BIO 
Data  Sets # of  Summaries # of Entries # of Fields # of Names # of Doses # of Freq # of Modes # of Duration # of Reason Training set 110  5970  (54.3) 14886 (135.3) 5684 (51.7) 2929 (26.6) 2740 (24.9) 2146 (19.5) 302  (2.7) 1085 (9.9) Dev set 35  2401  (68.6) 5988 (171.1) 2302 (65.8) 1163 (33.2) 1096 (31.3) 880 (25.1) 111  (3.2) 436 (12.5) Test set 251  8936 (35.6) 22041 (87.8) 8495 (33.8) 4387 (17.5) 3999 (15.9) 3307 (13.2) 511  (2.0) 1342 (5.3) 
63
tagging scheme; that is, each token in the input is tagged with B-x (beginning an NE of type x), I-x (inside an NE of type x) and O (outside any NE).   3.2.1 The find_name module As this module identifies medication names only, the tagset under the BIO scheme has three tags: B-m for beginning of a name, I-m for inside a name, and O for outside. Various features are used for this module, which we group into four types:   ? (F1) includes word n-gram features (n=1,2,3). For instance, the bigram wi-1 wi looks at the current word and the previous word.   ? (F2) contains features that check properties of the current word and its neighbors (e.g., the POS tag, the affixes and the length of a word, the type of section that a word appears in, whether a word is capitalized, whether a word is a number, etc.)   ? (F3) checks the BIO tags of previous words  ? (F4) contains features that check whether n-grams formed by neighboring words appear as part of medication names in given medication name lists. The name lists can come from la-beled training data or additional resources such as UMLS. 3.2.2 The context_type module This module is a binary classifier which deter-mines whether a medication name occurs in a list or narrative context. Features used by this module include the section name as identified by the pre-processing step, the number of commas and words on the current line, the position of the medication name on the current line, and the current and near-by words.   3.2.3 The find_others module This module complements the find_name module and uses eleven BIO tags to identify five non-name fields. The feature set used in this module is very similar to the one used in find_name except that some features in (F2) and (F4) are modified to suit the needs of the non-name fields. For instance, a feature will check whether a word fits a common pattern for dosage. In addition, some features in 
(F2) look at the output of previous modules: e.g., the location of nearby medication names as this information can be provided by the find_name module at test time.  3.3 Field linking  Once medication names and other fields have been found, the final step is to form entries by associat-ing each medication name with its related fields. Our current implementation uses simple heuristics. First, we go over each non-name field and link it with the closest preceding medication name unless the distance between the non-name field and its closest following medication name is much shorter. Second, we assemble the (name, non-name) pairs to form medication entries with a few rules.       More information about the modules discussed in this section and features used by the modules is available in (Halgrim, 2009). 4 Experimental results  In this section, we report the performance of our system on the development set (Section 4.1-4.3) and the test set (Section 4.4). The data sets are de-scribed in Table 2. For all the experiments in this section, unless specified otherwise, we report exact horizontal precision/recall/F-score, the primary metrics for the i2b2 challenge.     For the three modules in the field detection step, we use the Maximum Entropy (MaxEnt) learner in the Mallet package (McCallum, 2002) because, in general, MaxEnt produces good results without much parameter tuning and the training time for MaxEnt is much faster than more sophisticated algorithms such as CRF (Lafferty et al, 2001).     To determine whether the difference between two systems? performances is statistically signifi-cant, we use approximate randomization tests (No-reen, 1989) as follows. Given two systems that we would like to compare, we first calculate the dif-ference between exact horizontal F-scores. Then two pseudo-system outputs are generated by ran-domly swapping (at 0.5 probability) the two sys-tem outputs for each discharge summary. If the difference between F-scores of these pseudo-outputs is no less than the original F-score differ-ence, a counter, cnt, is increased by one. This process was repeated n=10,000 times, and the p-value of the significance is equal to (cnt+1)/(n+1). 
64
If the p-value is smaller than a predefined thresh-old (e.g., 0.05), we conclude that the difference between the two systems is statistically significant.  4.1 Performance of the whole system  4.1.1 Effect of feature sets  To test the effect of feature sets on system per-formance, we trained find_name and find_others with different feature sets and tested the whole sys-tem on the development set. For (F4), we used two medication name lists. The first list consists of medication names gathered from the training data. The second list includes drug names from the FDA database (www.accessdata.fda.gov/scripts/cder/ndc/). We use the second list to test whether adding features that check the information in an additional re-source could improve the system performance.    The results are in Table 4. For the last two rows, F1-F4a uses the first medication name list, and F1-F4b uses both lists. The F-score difference between all adjacent rows is statistically significant at p?0.01, except for the pair F1-F3 vs. F1-F4a. It is not surprising that using the first medication name list on top of F1-F3 does not improve the perform-ance, as the same kind of information has already been captured by F1 features. The improvement of F1-F4b over F1-F4a shows that the system can easily incorporate additional resources and achieve a statistically significant (at p?0.01) gain.   Features Precision Recall F-score F1 72.5 60.3 65.8 F1-F2 82.5 78.2 80.3 F1-F3 88.4 77.9 82.8 F1-F4a 87.4 77.9 82.4 F1-F4b 88.1 79.4 83.5  Table 4: System performance on the development set with different feature sets 4.1.2 Effect of training data size Figure 1 shows the system performance on the de-velopment set when different portions of the train-
ing set are used for training. The curve with ?+? signs represents the results for F1-F4b, and the curve with circles represents the results for F1-F4a. The figure illustrates that, as the training data size increases, the F-score with both feature sets im-proves. In addition, the additional resource is most helpful when the training data size is small, as in-dicated by the decreasing gap between the two sets of F-scores when the size of training data in-creases.   
 Figure 1: System performance on the development set with different training data sizes (Legend: ? represents F-scores with features in F1-F4a; + rep-resents F-scores with features in F1-F4b)  4.1.3 Pipeline vs. find_all   The current field detection step is a pipeline ap-proach with three modules: find_name, con-text_type, and find_others. Having three separate modules allows each module to choose the features that are most appropriate for it. In addition, later modules can use features that check the output of the previous modules. A potential downside of the pipeline system is that the errors in the early mod-ule would propagate to later modules. An alterna-tive is to use a single module to detect all six field types together.  Figure 2 shows the result of find_all in compari-son to the result for the three-module pipeline. Both use the F1-F4b feature sets, except that find_others uses some features that check the out-put of previous modules which are not available to find_all.  
65
 Figure 2: Pipeline vs. find_all for field detection (Legend: ? represents F-scores with find_all; + represents F-scores with the three-module pipeline)      Interestingly, when 10% of the training set is used for training, find_all has a higher F-score than the pipeline approach, although the difference is not statistically significant at p?0.05. As more data is used for training, the pipeline outperforms find_all, and when at least 50% of the training data is used, the difference between the two is statisti-cally significant at p?0.05. One possible explana-tion for this phenomenon is that as more training data becomes available, the early modules in the pipeline make fewer errors; as a result, the disad-vantage of the pipeline approach caused by error propagation is outweighed by the advantage that the later modules in the pipeline can use features that check the output of the earlier modules. 4.2 Performance of the field detection step Table 5 shows the exact precision/recall/F-score on identifying the six field types, using all the training data, F1-F4b features, and the pipeline approach for field detection. A span in the system output exactly matches a span in the gold standard if the two spans are identical and have the same field type. Among the six fields, the results for duration and reason are the lowest. That is because duration and reason are longer phrases than the other four field types and there are fewer strong, reliable cues to signal their presence.  When making the narrative/list distinction, the accuracy of our context_type module is 95.4%. In contrast, the accuracy of the baseline (which treats each medication name as in a list context) is only 55.6%. 
  Precision Recall F-score Name 91.2 88.5 89.9 Dosage 96.6 90.8 93.6 Frequency 93.9 89.0 91.8 Mode 95.7 90.3 92.9 Duration 73.8 43.2 54.5 Reason 72.2 31.0 43.3 All fields 92.6 84.5 88.4 Table 5: The performance (exact preci-sion/recall/F-score) of field detection on the devel-opment set.   4.3 Performance of the field linking step In order to evaluate the field linking step, we gen-erated a list of (name, non-name) pairs from the gold standard, where the name and non-name fields appear in the same entry in the gold stan-dard. We then compared these pairs with the ones produced by the field linking step and calculated precision/recall/F-score. Table 6 shows the result of two experiments: in the cheating experiment, the input to the field linking step is the fields from the gold standard; in the non-cheating experiment, the input is the output of the field detection step. These experiments show that, while the heuristic rules used in this step work reasonably well when the input is accurate, the performance deteriorates con-siderably when the input is noisy, an issue we plan to address in future work.   Precision Recall F-score Non-cheating 87.4 75.1 80.8 Cheating 96.2 94.5 95.3 Table 6: The performance of the field linking step on the development set (cheating: assuming perfect field input; non-cheating: using the output of the field detection step) 4.4 Results on the test data Table 7 shows the system performance on the i2b2 official test data. The system was trained on the union of the training and development data. Com-pared with the top five i2b2 systems (see Table 3), our system was second only to the best i2b2 sys-tem, which used more resources and more sophis-ticated rules for field linking (Patrick and Li, 2009).    
66
 Precision Recall F-score Horizontal 88.6 80.2 84.1 Name 92.6 87.1 89.8 Dosage 96.3 90.2 93.1 Frequency 95.6 90.8 93.2 Mode 96.7 90.2 93.3 Duration 70.6 40.5 51.5 Reason 73.4 34.7 47.1 All fields 91.6 82.7 86.9 Table 7: System performance on the test set when trained on the union of the training and the devel-opment sets with F1-F4b features. 5 Conclusion  We present a hybrid system for medication extrac-tion. The system is built around a pipeline of cas-cading statistical classifiers for field detection. It achieves good performance that is comparable to the top systems in the i2b2 challenge, and incorpo-rating additional resources as features further im-proves the performance. In the future, we plan to replace the current rule-based field linking module with a statistical module to improve accuracy.  Acknowledgments This work was supported in part by US DOD grant N00244-091-0081 and NIH Grants 1K99LM010227-0110, U54LM008748, and T15LM007442-06. We would also like to thank three anonymous reviewers for helpful comments. 
References  Trond Grenager, Dan Klein, and Christopher Manning. 2005. Unsupervised learning of field segmentation models for information extraction. In Proc. of ACL-2005. Scott Halgrim. 2009. A Pipeline Machine Learning Ap-proach to Biomedical Information Extraction. Master Thesis. University of Washington.  J. Lafferty and A. McCallum and F. Pereira. 2001. Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data. In Proc. of the 18th International Conference on Machine Learning (ICML-2001). Matthew A. Levin, Marina Krol, Ankur M. Doshi, and David L. Reich. 2007. Extraction and mapping of drug names from free text to a standardized nomen-clature. AMIA Symposium Proceedings, pp 438-442. 
 S. Meystre, G. Savova, K. Kipper-Schuler, and J. Hur-dle. 2008. Extracting Information from Textual Documents in the Electronic Health Record: A Re-view of Recent Research. IMIA Yearbook of Medical Informatics Methods Inf Med 2008; 47 Suppl 1:128-44.  Andrew McCallum. 2002. Mallet: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu Eric W. Noreen. 1989. Computer intensive methods for testing hypotheses: an introduction. John Wiley & Sons. Jon Patrick and Min Li, 2009. A Cascade Approach to   Extract Medication Event (i2b2 challenge 2009). Presentation at the Third i2b2 Workshop, November 2009, San Francisco, CA.  Hoifung Poon and Pedro Domingos. 2007. Joint infer-ence in information extraction. In Proc. of the Na-tional Conference on Artificial Intelligence (AAAI), pp 913-918. Benjamin Rozenfeld and Ronen Feldman. 2008. Self-supervised relation extraction from the web. Knowl-edge and Information Systems, 17(1):17-33. ?zlem Uzuner, Imre Solti, and Eithon Cadag, 2010a. Extracting Medication Information from Clinical Text. Submitted to JAMIA.  ?zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag, 2010b. Community Annotation Experiment for Ground Truth Generation for the i2b2 Medication Challenge. Submitted to JAMIA.  Ben Wellner, Andrew McCallum, Fuchun Peng, and Michael Hay. 2004. An integrated, conditional model of information extraction and coreference with appli-cation to citation matching. In Proc. of the 20th Con-ference on Uncertainty in AI (UAI-2004).    
67
