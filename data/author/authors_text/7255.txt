Annotation of Multiword Expressions in the Prague Dependency Treebank
Eduard Bejc?ek, Pavel Stran??k and Pavel Schlesinger
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
{bejcek,stranak,schlesinger}@ufal.mff.cuni.cz
Abstract
In this article we want to demonstrate that
annotation of multiword expressions in the
Prague Dependency Treebank is a well de-
fined task, that it is useful as well as feasible,
and that we can achieve good consistency of
such annotations in terms of inter-annotator
agreement. We show a way to measure agree-
ment for this type of annotation. We also ar-
gue that some automatic pre-annotation is
possible and it does not damage the results.
1 Motivation
Various projects involving lexico-semantic annota-
tion have been ongoing for many years. Among those
there are the projects of word sense annotation, usu-
ally for creating training data for word sense disam-
biguation. However majority of these projects have
only annotated very limited number of word senses
(cf. Kilgarriff (1998)). Even among those that aim
towards ?all words? word-sense annotation, multi-
word expressions (MWE) are not annotated adequa-
tely (see (Mihalcea, 1998) or (Hajic? et al, 2004)),
because for their successful annotation a method-
ology allowing identification of new MWEs during
annotation is required. Existing dictionaries that in-
clude MWEs concentrate only on the most frequent
ones, but we argue that there are many more MWEs
that can only be identified (and added to the dictio-
nary) by annotation.
There are various projects for identification of na-
med entities (for an overview see (?evc??kov? et al,
2007)). We explain below (mainly in Section 2) why
we consider named entities to be concerned with lex-
ical meaning. At this place we just wish to recall that
these projects only select some specific parts of text
and provide information only for these. They do not
aim for full lexico-semantic annotation of texts.
There is also another group of projects that have to
tackle the problem of lexical meaning, namely tree-
banking projects that aim to develop a deeper layer
of annotation in adition to a surface syntactic layer.
This deeper layer is generally agreed to concern lex-
ical meaning. Therefore the units of this layer cannot
be words anymore, they should be lexias.
Lexia is defined by Filipec and C?erm?k (1986)
as equivalent to a ?monosemic lexeme? of (Filipec,
1994) or a ?lexical unit? of (Cruse, 1986): ?a pair
of a single sense and a basic form (plus its derived
forms) with relatively stable semantic properties?.
We work with the Prague Dependency Treebank
(PDT, see Hajic? (2005)), which has in addition to
the morphemic and the surface syntactic layers also
the tectogrammatical layer. The latter has been con-
strued as the layer of the (literal) meaning of the sen-
tence and thus should be composed of lexias (lexical
units) and the relations between their occurrences.1
On the tectogrammatical layer only the autose-
mantic words form nodes in a tree (t-nodes). Synse-
mantic (function) words are represented by various
attributes of t-nodes. Each t-node has a lemma: an at-
tribute whose value is the node?s basic lexical form.
Currently t-nodes, and consequently their t-lemmas,
are still visibly derived from the morphological di-
vision of text into tokens. This preliminary handling
1With a few exceptions, such as personal pronouns (that co-
refer to other lexias) or coordination heads.
793
has always been considered unsatisfactory in FGD.2
There is a clear goal to distinguish t-lemmas through
their senses, but this process has not been completed
so far.
Our project aims at improving the current state
of t-lemmas. Our goal is to assign each t-node a
t-lemma that would correspond to a lexia, i.e. that
would really distinguish the t-node?s lexical mean-
ings. To achieve this goal, in the first phase of the
project, which we report on in this paper, we iden-
tify multiword expressions and create a lexicon of
the corresponding lexias.
2 Introduction
We annotate all occurrences of MWEs (including
named entities, see below) in PDT 2.0. When we
speak of multiword expressions we mean ?idiosyn-
cratic interpretations that cross word boundaries?
(Sag et al, 2002). We understand multiword expres-
sions as a type of lexias. We distinguish also a spe-
cial type of MWEs, for which we are mainly inter-
ested in its type, rather than individual lexias, during
the annotation: named entities (NE).3 Treatment of
NEs together with other MWEs is important, be-
cause syntactic functions are more or less arbitrary
inside a NE (consider an address with phone num-
bers, etc.) and so is the assignment of semantic roles.
That is why we need each NE to be combined into a
single node, just like we do it with MWEs in general.
For the purpose of annotation we have built a repos-
itory of lexias corresponding to MWEs, which we
call SemLex. We have built it using entries from
some existing dictionaries and it is being enriched
during the annotation in order to contain every lexia
that was annotated. We explain this in detail in Sec-
tion 4.1.
3 Current state of MWEs in PDT 2.0
During the annotation of valency that is a part of
the tectogrammatical layer of PDT 2.0 the t-lemmas
2Functional Generative Description (FGD, (Sgall et al,
1986; Hajic?ov? et al, 1998)) is a framework for system-
atic description of a language, that the PDT project is based
upon. In FGD units of the t-layer are construed equivalently to
monosemic lexemes (lexias) and are combined into dependency
trees, based on syntactic valency of the lexias.
3NEs can in general be also single-word, but in this phase of
our project we are only interested in multiword expressions, so
when we say NE in this paper, we always mean multiword.
that correspond to lexias have been basically iden-
tified for all the verbs and some nouns and adjec-
tives. The resulting valency lexicon is called PDT-
VALLEX (Hajic? et al, 2003) and we can see it as
a repository of lexias based on verbs, adjectives and
nouns in PDT that have valency. 4
This is a starting point for having t-nodes corre-
sponding to lexias. However in the current state it is
not fully sufficient even for verbs, mainly because
parts of MWEs are not joined into one node. Parts
of frames marked as idiomatic are still represented
by separate t-nodes in a tectogrammatical tree. Ver-
bal phrasemes are also split into 2 nodes, where the
nominal part is governed by the verb. Non-verbal id-
ioms have not been annotated at all.
Below we give an example of the current state:
an idiom meaning ?in a blink (of an eye)? ? literally
?*what not-see? (Figure 1).
Figure 1: ?Co nevide?t? (in a blink)
4 Methodology
4.1 Building SemLex
Each entry we add into SemLex is considered to be
a lexia. We have also added 9 special entries to iden-
tify NE types, so we do not need to add the expres-
sions themselves. These types are derived from NE
classification by (?evc??kov? et al, 2007). Some fre-
quent names of persons, institutions or other objects
(e.g. film titles) are being added into SemLex dur-
ing annotation (while keeping the information about
a NE type), because this allows for their following
occurrences to be pre-annotated automatically (see
Section 5). For others, like addresses or bibliographic
4It is so because in PDT-VALLEX valency is not the only
criterion for distinguishing frames (=meanings). Two words
with the same morphological lemma and valency frame are as-
signed two different frames if their meaning differs. Thus the
PDT-VALLEX frames correspond to lexias.
794
entries, it makes but little sense, because they most
probably will not reappear during the annotation.
Currently (for the first stage of lexico-semantic
annotation of PDT) SemLex contains only lexias cor-
responding to MWEs. Its base has been composed of
MWEs extracted from Czech WordNet (Smr?, 2003),
Eurovoc (Eurovoc, 2007) and SC?FI (C?erm?k et al,
1994).5 Currently there are over 30,000 multi-word
lexias in SemLex and more are being added during
annotations.
The entries added by annotators must be lexias as
defined above. Annotators define their ?sense? infor-
mally (as much as possible) and we extract an exam-
ple of usage and the basic form from the annotation
automatically. The ?sense? information shall be re-
vised by a lexicographer, based on annotated occur-
rences.
4.2 Annotation
PDT 2.0 uses PML (Pajas and ?te?p?nek, 2005),
which is an application of XML that utilises a stand-
off annotation scheme. We have extended the PDT-
PML with a new schema for so-called s-files. We
use these files to store all of our annotation without
altering the PDT itself. These s-files are very sim-
ple: basically each of them consists of a list of s-
nodes. Each s-node corresponds to an occurrence of
a MWE and it is composed of a link to the entry in
SemLex and a list of identifiers of t-nodes that cor-
respond to this s-node.
Our annotation program reads in a tectogrammati-
cal representation (t-file) and calls TrEd (Pajas, 2007)
to generate plain text. This plain text (still linked to
the tectogrammatical representation) is presented to
the annotator. While the annotator marks MWEs al-
ready present in SemLex or adds new MWEs into
SemLex, tree representations of these MWEs extrac-
ted from underlying t-trees are added into their Sem-
Lex entries via TrEd scripts.
5 Pre-annotation
Because MWEs tend to occur repeatedly in a text,
we have decided to test pre-annotation both for the
speed improvement and for improving the consis-
tency of annotations. On the assumption that all oc-
5Slovn?k c?esk? frazeologie a idiomatiky (Dictionary of
Czech Phraseology and Idiomatics)
currences of a MWE share the same tree structure,
while there are no restrictions on the surface word
order other than those imposed by the tree structure
itself we have decided to employ four types of pre-
annotation:
A) External pre-annotation provided by our col-
league (see Hn?tkov? (2002)). With each MWE a
set of rules is associated that limits possible forms
and surface word order of parts of a MWE. This ap-
proach was devised for corpora that are not syntac-
tically annotated.
B) Our one-time pre-annotation with those lexias
from SemLex that were already used in annotation,
and thus have a tree structure as a part of their entry.
C) Dynamic pre-annotation as in B, only with the
SemLex entries that have been recently added by the
annotator.
D) When an annotator tags an occurrence of a
MWE in the text, other occurrences of this MWE
in the article are identified automatically.6
(A) was executed once for all of the PDT. (B) is
performed each time we merge lexias added by an-
notators into the main SemLex. We carry out this
annotation in one batch for all PDT files remaining
to annotate. (C) should be done for each file while
it is being opened in LexemAnn GUI. (D) happens
each time the annotator adds a new lexia into Sem-
Lex and uses it to annotate an occurrence in the text.
In subsequent files instances of this lexia are already
annotated in step (C), and later even in (B).
After the pilot annotation without pre-annotation
(D) we have compared instances of the same tags
and found that 10.5% of repeated lexias happened
to have two different trees. After closer examination
this 10.5% group is negligible because these cases
are caused by ellipses, variations in lexical form such
as diminutives etc., or wrong lemmatisation, rather
than inconsistencies in the tree structure. These cases
show us some issues of PDT 2.0, for instance:
? ji?n? ? Ji?n? Korea [southern ? South Korea] ?
wrong lemmatisation
6This is exactly what happens: 1) Tree structure of the se-
lected MWE is identified via TrEd 2) The tree structure is added
to the lexeme?s entry in SemLex 3) All the sentences in the
given file are searched for the same MWE using its tree structure
(via TrEd) 4) Other occurrences returned by TrEd are tagged
with this MWE?s ID, but these occurrences receive an attribute
?auto?, which identifies them (both in the s-files and visually in
the annotation tool) as annotated automatically.
795
? obchodn? r?editel ? r?editelka [managing direc-
tor ? man ? woman] ? in future these should
have one t-lemma and gender should be speci-
fied by an attribute of a t-node.
We have not found any case that would show that
there is such a MWE that its structure cannot be rep-
resented by a single tectogrammatical tree. 1.1% of
all occurences were not connected graphs, but this
happened due to errors in data and to coordination.
This corroborates our assumption that (disregarding
errors) all occurrences of a MWE share the same
tree structure. As a result, we started storing the tree
structures in the SemLex entries and employ them in
pre-annotation (D). This also allows us to use pre-
annotations (B) and (C), but we have decided not
to use them at the moment, in order to be able to
evaluate each pre-annotation step separately. Thus
the following section reports on the experiments that
employ pre-annotation (A) and (D).
6 Analysis of Annotations
Two annotators already started to use (and test) the
tool we have developed. They both have got the same
texts. The text is generated from the t-trees and pre-
sented as a plain text with pre-annotated words mark-
ed by colour labels. Annotators add their tags in the
form of different colour labels and they can delete
the pre-annotated tags. In this experiment data con-
sists of approx. 120,000 tokens that correspond to
100,000 t-nodes. Both annotators have marked about
15,200 t-nodes (~15%) as parts of MWEs. annotator
A has grouped them into 7,263 MWEs and annota-
tor B into 6,888. So the average length of a MWE is
2.2 t-nodes.
The ratio of general named entities versus Sem-
Lex lexias was 52:48 for annotator A and 49:51 in
case of annotator B. Annotator B used 10% more
lexias than annotatorA (3,279 and 3,677), while they
both used almost the same number of NEs. Some
comparison is in the Table 1.
type of MWE A B
SemLex lexias 3,677 3,279
Named Entities 3,553 3,587
- person/animal 1130 1137
- institution 842 772
Table 1: Annotated instances of significant types of
MWEs
Both annotators also needed to add missing en-
tries to the originally compiled SemLex or to edit
existing entries. annotatorA added 722 entries while
the annotator B added 861. They modified 796 and
809 existing entries, respectively.
6.1 Inter-anntator Agreement
In this section our primary goal is to assess whether
with our current methodology we produce reliable
annotation of MWEs. To that end we measure the
amount of inter-annotator agreement that is above
chance. There are, however, a few sources of com-
plications in measuring this agreement:
? Each tag of a MWE identifies a subtree of a tec-
togrammatical tree (represented on the surface by a
set of marked words). This allows for partial agree-
ment of tags at the beginning, at the end, but also in
the middle of a surface interval (in a sentence).
? A disagreement of the annotators on the tag is
still an agreement on the fact that this t-node is a part
of a MWE and thus should be tagged. This means we
have to allow for partial agreement on a tag.
? There is not any clear upper bound as to how
many (and how long) MWEs are there in texts.
? There is not a clear and simple way to esti-
mate the amount of the agreement by chance, be-
cause it must include the partial agreements men-
tioned above.
Since we want to keep our agreement calculation
as simple as possible but we also need to take into
account the problems above, we have decided to start
from pi as defined in (Artstein and Poesio, 2007) and
to make a few adjustments to allow for types of par-
tial agreement and estimated maximal agreement.
Because we do not know how many MWEs there
are in our texts, we need to calculate the agreement
over all t-nodes, rather than the t-nodes that ?should
be annotated?. This also means, that the theoretical
maximal agreement (upper bound) U , cannot be 1.
If it was 1, it would be saying that all nodes are part
of a MWE.
Since we know that U < 1 but we do not know
it?s exact value, we use the estimated upper bound
U? (see Equation 1). Because we calculate U? over all
t-nodes, we need to account not only for agreement
on tagging a t-node, but also for agreement, that the
t-node is not a part of a MWE, therefore it is not
796
tagged.7
If N is the number of all t-nodes in our data and
nA?B is the number of t-nodes annotated by at least
one annotator, then we estimate U? as follows:
U? =
nA?B
N
+ 0.052 ?
N ? nA?B
N
= 0.215 (1)
The weight 0.052 used for scoring the t-nodes that
were not annotated is explained below. Because U?
includes all the disagreements of the annotators, we
believe that the real upper bound U lies somewhat
below it and the agreement value 0.215 is not some-
thing that should (or could) be achieved. This is how-
ever based on the assumption that the data we have
not yet seen have similar ratio of MWEs as the data
we have used.
To account for partial agreement we divide the t-
nodes into 5 classes c and assign each class a weight
w as follows:
c1 If the annotators agree on the exact tag from Sem-
Lex, we get maximum information: w = 1
c2 If they agree, that the t-node is a part of a NE or
they agree it is a part of some lexia from Sem-
Lex, but they do not agree which NE or which
lexia, we estimate we get about a half of the in-
formation compared to c1: w = 0.5
c3 If they agree that the t-node is a part of a MWE,
but disagree whether a NE or a lexia from Sem-
Lex, it is again half the information compared to
c2, so w = 0.25
c4 If they agree that the t-node is not a part of a
MWE, w = 0.052. This low value of w accounts
for frequency of t-nodes that are not a part of a
MWE, as estimated from data: Agreement on not
annotating provides the same amount of infor-
mation as agreement on annotating, but we have
to take into account higher frequency of t-nodes
that are not annotated:
c4 = c3 ?
P
annotated
P
not annotated
= 0.25 ?
12797
61433
? 0.052
c5 If the annotators do not agree whether to anno-
tate a t-node or not, w = 0.
The number of t-nodes (n) and weights w per class
c are given in Table 2.
7If we did not do this, there would be no difference between
t-nodes, that were not tagged (annotators agreed they are not a
part of a MWE) and the t-nodes that one annotator tagged and
the other did not (i.e. they disagreed).
Agreement Disagreement
Agreement on annotation Not annotation
Agreement on NE / lexia
Full agreement
class c 1 2 3 4 5
t-nodes n 10,527 2,365 389 83,287 3,988
weight w 1 0.5 0.25 0.052 0
Table 2: The agreement per class and the associated
weights
Now that we have estimated the upper bound of
agreement U? and the weights w for all t-nodes we
can calculate our weighted version of pi:
piw =
Ao ?Ae
U? ?Ae
Ao is the observed agreement of annotators and
Ae is the agreement expected by chance (which is
similar to a baseline). piw is thus a simple ratio of our
observed agreement above chance and maximum a-
greement above chance.
Weights w come into account in calculation ofAo
and Ae.
We calculate Ao by multiplying the number of t-
nodes in each category c by that category?s weight
w, summing these 5 weighted sums and dividing this
sum of all the observed agreement in the data by
the total number of t-nodes: Ao = 1N
?5
c=1 ncwc =
0.160.
Ae is the probability of agreement expected by
chance over all t-nodes. This means it is the sum of
the weighted probabilities of all the combinations of
all the tags that can be obtained by a pair of annota-
tors. Every possible combination of tags (including
not tagging a t-node) falls into one of the categories
c and thus gets the appropriate weight w. Calculat-
ing the value of Ae depends not only on values of
w (see Table 2), but also on the fact that SemLex is
composed of 9 entries for NE types and over 30,000
entries for individual lexias. Based on this we have
obtained Ae = 0.047.
The resulting piw is then
piw =
Ao ?Ae
U? ?Ae
=
0.160? 0.047
0.215? 0.047
= 0.6760
When we analyse the cases of disagreement and
partial agreement we find that most of it has to do
with SemLex lexias rather than NEs. This is mostly
due to imperfectness of the dictionary and its size
(annotators could not explore each of almost 30,000
797
of SemLex entries). Our current methodology, which
relies too much on searching the SemLex, is also to
blame. This should, however, improve by employing
pre-annotation (B) and (C).
One more reason for disagreement consists in the
fact that there are cases, for which non-trivial knowl-
edge of the world is needed: ?Jang Di Pertuan Agong
Sultan Azlan ??h, the sultan of the state of Perak,
[ . . . ] flew back to Perak.? Is ?Sultan Azlan ??h? still
a part of the name or is it (or a part of it) a title?
The last important reason of disagreement is sim-
ple: both annotators identify the same part of text
as MWE instances, but while searching the SemLex
they choose different lexias as the tags. This can be
rectified by:
? Removing duplicate entries from SemLex (cur-
rently there are many close identical entries orig-
inating from Eurovoc and Czech WordNet).
? Imploring improved pre-annotation B and C, as
mentioned above.
7 Conclusion
We have annotated multi-word lexias and named en-
tities in a part of PDT 2.0. We use tectogrammati-
cal tree structures of MWEs for the automatic pre-
annotation. In the analysis of inter-annotator agree-
ment we show that a weighted measure that accounts
for partial agreement as well as the estimation of
maximal agreement is needed.
The resulting piw = 0.6760 is statistically sig-
nificant and should gradually improve as we clean
up the annotation lexicon, more entries can be pre-
annotated automatically, and further types of pre-
annotation are employed.
8 Acknowledgement
This work has been supported by grants 1ET2011205-
05 of Grant Agency of the Academy of Science of
the Czech Republic, projects MSM0021620838 and
LC536 of the Ministry of Education and 201/05/H014
of the Czech Science Foundation.
References
Ron Artstein and Massimo Poesio. 2007. Inter-coder agree-
ment for computational linguistics. Submitted to Computa-
tional Linguistics.
F. C?erm?k, V. C?erven?, M. Churav?, and J. Machac?. 1994.
Slovn?k c?esk? frazeologie a idiomatiky. Academia.
D.A. Cruse. 1986. Lexical Semantics. Cambridge University
Press.
Eurovoc. 2007. http://europa.eu/eurovoc/.
Josef Filipec and Franti?ek C?erm?k. 1986. C?esk? lexikologie.
Academia.
Josef Filipec. 1994. Lexicology and lexicography: Develop-
ment and state of the research. In P. A. Luelsdorff, editor,
The Prague School of Structural and Functional Linguistics,
pages 163?183, Amsterdam/Philadelphia. J. Benjamins.
Jan Hajic?, Jarmila Panevov?, Zden?ka Ure?ov?, Alevtina B?-
mov?, Veronika Kol?r?ov?, and Petr Pajas. 2003. PDT-
VALLEX. In Joakim Nivre and Erhard Hinrichs, editors,
Proceedings of The Second Workshop on Treebanks and Lin-
guistic Theories, volume 9 of Mathematical Modeling in
Physics, Engineering and Cognitive Sciences, pages 57?68,
Vaxjo, Sweden. Vaxjo University Press.
Jan Hajic?, Martin Holub, Marie Huc??nov?, Martin Pavl?k, Pavel
Pecina, Pavel Stran??k, and Pavel Martin ?id?k. 2004.
Validating and improving the Czech WordNet via lexico-
semantic annotation of the Prague Dependency Treebank. In
LREC 2004, Lisbon.
Jan Hajic?, 2005. Insight into Slovak and Czech Corpus Lin-
guistics, chapter Complex Corpus Annotation: The Prague
Dependency Treebank, pages 54?73. Veda Bratislava, Slo-
vakia.
Eva Hajic?ov?, Barbara H. Partee, and Petr Sgall. 1998. Topic-
focus articulation, tripartite structures, and semantic con-
tent, volume 71 of Studies in Linguistics and Philosophy.
Kluwer, Dordrecht.
Milena Hn?tkov?. 2002. Znac?kov?n? fraz?mu? a idiomu? v
C?esk?m n?rodn?m korpusu s pomoc? Slovn?ku c?esk? fraze-
ologie a idiomatiky. Slovo a slovesnost.
A. Kilgarriff. 1998. Senseval: An exercise in evaluating word
sense disambiguation programs. In Proc. LREC, pages 581?
588, Granada.
Rada Mihalcea. 1998. Semcor semantically tagged corpus.
Petr Pajas and Jan ?te?p?nek. 2005. A Generic XML-Based For-
mat for Structured Linguistic Annotation and Its Application
to Prague DependencyTreebank 2.0. Technical Report TR-
2005-29, ?FAL MFF UK, Prague, Czech Rep.
Petr Pajas. 2007. TrEd. http://ufal.mff.cuni.cz/?pajas/
tred/index.html.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A pain
in the neck for nlp. In Third International Conference, CI-
CLing.
Magda ?evc??kov?, Zdene?k ?abokrtsk?, and Oldr?ich Kru?za.
2007. Zpracov?n? pojmenovan?ch entit v c?esk?ch textech
(treatment of named entities in czech texts). Technical Re-
port TR-2007-36, ?FAL MFF UK, Prague, Czech Republic.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?. 1986. The
Meaning of the Sentence in Its Semantic and Pragmatic As-
pects. Academia/Reidel Publ. Comp., Praha/Dordrecht.
Pavel Smr?. 2003. Quality control for wordnet development.
In Petr Sojka, Karel Pala, Pavel Smr?, Christiane Fellbaum,
and Piek Vossen, editors, Proceedings of the Second Inter-
national WordNet Conference?GWC 2004, pages 206?212.
Masaryk University Brno, Czech Republic.
798
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 651?658,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Combining Association Measures for Collocation Extraction
Pavel Pecina and Pavel Schlesinger
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
{pecina,schlesinger}@ufal.mff.cuni.cz
Abstract
We introduce the possibility of combining
lexical association measures and present
empirical results of several methods em-
ployed in automatic collocation extrac-
tion. First, we present a comprehensive
summary overview of association mea-
sures and their performance on manu-
ally annotated data evaluated by precision-
-recall graphs and mean average precision.
Second, we describe several classification
methods for combining association mea-
sures, followed by their evaluation and
comparison with individual measures. Fi-
nally, we propose a feature selection algo-
rithm significantly reducing the number of
combined measures with only a small per-
formance degradation.
1 Introduction
Lexical association measures are mathematical
formulas determining the strength of association
between two or more words based on their occur-
rences and cooccurrences in a text corpus. They
have a wide spectrum of applications in the field
of natural language processing and computational
linguistics such as automatic collocation extrac-
tion (Manning and Sch?tze, 1999), bilingual word
alignment (Mihalcea and Pedersen, 2003) or de-
pendency parsing. A number of various associa-
tion measures were introduced in the last decades.
An overview of the most widely used techniques
is given e.g. in Manning and Sch?tze (1999) or
Pearce (2002). Several researchers also attempted
to compare existing methods and suggest differ-
ent evaluation schemes, e.g Kita (1994) and Evert
(2001). A comprehensive study of statistical as-
pects of word cooccurrences can be found in Evert
(2004) or Krenn (2000).
In this paper we present a novel approach to au-
tomatic collocation extraction based on combin-
ing multiple lexical association measures. We also
address the issue of the evaluation of association
measures by precision-recall graphs and mean av-
erage precision scores. Finally, we propose a step-
wise feature selection algorithm that reduces the
number of combined measures needed with re-
spect to performance on held-out data.
The term collocation has both linguistic and
lexicographic character. It has various definitions
but none of them is widely accepted. We adopt
the definition from Choueka (1988) who defines
a collocational expression as ?a syntactic and se-
mantic unit whose exact and unambiguous mean-
ing or connotation cannot be derived directly from
the meaning or connotation of its components?.
This notion of collocation is relatively wide and
covers a broad range of lexical phenomena such as
idioms, phrasal verbs, light verb compounds, tech-
nological expressions, proper names, and stock
phrases. Our motivation originates from machine
translation: we want to capture all phenomena that
may require special treatment in translation.
Experiments presented in this paper were per-
formed on Czech data and our attention was re-
stricted to two-word (bigram) collocations ? pri-
marily for the limited scalability of some meth-
ods to higher-order n-grams and also for the rea-
son that experiments with longer word expressions
would require processing of much larger corpus to
obtain enough evidence of the observed events.
2 Reference data
The first step in our work was to create a refer-
ence data set. Krenn (2000) suggests that col-
location extraction methods should be evaluated
against a reference set of collocations manually
extracted from the full candidate data from a cor-
pus. To avoid the experiments to be biased by
underlying data preprocessing (part-of-speech tag-
ging, lemmatization, and parsing), we extracted
the reference data from morphologically and syn-
tactically annotated Prague Dependency Treebank
2.0 containing about 1.5 million words annotated
on analytical layer (PDT 2.0, 2006). A corpus of
this size is certainly not sufficient for real-world
applications but we found it adequate for our eval-
uation purposes ? a larger corpus would have made
the manual collocation extraction task infeasible.
651
Dependency trees from the corpus were broken
down into dependency bigrams consisting of lem-
mas of the head word and its modifier, their part-
-of-speech pattern, and dependency type. From
87 980 sentences containing 1 504 847 words, we
obtained a total of 635 952 different dependency
bigrams types. Only 26 450 of them occur in the
data more than five times. The less frequent bi-
grams do not meet the requirement of sufficient
evidence of observations needed by some meth-
ods used in this work (they assume normal dis-
tribution of observations and become unreliable
when dealing with rare events) and were not in-
cluded in the evaluation. We, however, must
agree with Moore (2004) arguing that these cases
comprise majority of all the data (the Zipfian
phenomenon) and thus should not be excluded
from real-world applications. Finally, we filtered
out all bigrams having such part-of-speech pat-
terns that never form a collocation (conjunction?
preposition, preposition?pronoun, etc.) and ob-
tained a list consisting of 12 232 dependency bi-
grams, further called collocation candidates.
2.1 Manual annotation
The list of collocation candidates was manually
processed by three trained linguists in parallel and
independently with the aim of identifying colloca-
tions as defined by Choueka. To simplify and clar-
ify the work they were instructed to select those
bigrams that can be assigned to these categories:
? idiomatic expressions
- studen? v?lka (cold war)
- vis? otazn?k (question mark is hanging? open question)
? technical terms
- pr?edseda vl?dy (prime minister)
- oc?it? sve?dek (eye witness)
? support verb constructions
- m?t pravdu (to be right)
- uc?init rozhodnut? (make decision)
? names of persons, locations, and other entities
- Pra?sk? hrad (Prague Castle)
- C?erven? kr??? (Red Cross)
? stock phrases
- z?sadn? probl?m (major problem)
- konec roku (end of the year)
The first (expected) observation was that the in-
terannotator agreement among all the categories
was rather poor: the Cohen?s ? between annota-
tors ranged from 0.29 to 0.49, which demonstrates
that the notion of collocation is very subjective,
domain-specific, and somewhat vague. The reason
that three annotators were used was to get a more
precise and objective idea about what can be con-
sidered a collocation by combining outcomes from
multiple annotators. Only those bigrams that all
three annotators independently recognized as col-
locations (of any type) were considered true collo-
cations. The reference data set contains 2 557 such
bigrams, which is 20.9% of all. ? between these
two categories reanged from 0.52 to 0.58.
The data was split into six stratified samples.
Five folds were used for five-fold cross validation
and average performance estimation. The remain-
ing one fold was put aside and used as held-out
data in experiments described in Section 5.
3 Association measures
In the context of collocation extraction, lexical as-
sociation measures are formulas determining the
degree of association between collocation com-
ponents. They compute an association score for
each collocation candidate extracted from a cor-
pus. The scores indicate the potential for a can-
didate to be a collocation. They can be used for
ranking (candidates with high scores at the top),
or for classification (by setting a threshold and dis-
carding all bigrams below this threshold).
If some words occur together more often than
by chance, then this may be evidence that they
have a special function that is not simply explained
as a result of their combination (Manning and
Sch?tze, 1999). This property is known in linguis-
tics as non-compositionality. We think of a cor-
pus as a randomly generated sequence of words
that is viewed as a sequence of word pairs (de-
pendency bigrams in our case). Occurrence fre-
quencies and marginal frequencies are used in sev-
eral association measures that reflect how much
the word cooccurrence is accidental. Such mea-
sures include: estimation of joint and conditional
bigram probabilities (Table 1, 1?3), mutual infor-
mation and derived measures (4?9), statistical tests
of independence (10?14), likelihood measures (15?
16), and various other heuristic association mea-
sures and coefficients (17?55) originating in differ-
ent research fields.
By determining the entropy of the immediate
context of a word sequence (words immediately
preceding or following the bigram), the associa-
tion measures (56?60) rank collocations according
to the assumption that they occur as (syntactic)
units in a (information-theoretically) noisy envi-
ronment (Shimohata et al, 1997). By comparing
empirical contexts of a word sequence and of its
components (open-class words occurring within
652
# Name Formula
1. Joint probability P (xy)
?2. Conditional probability P (y|x)
3. Reverse conditional prob. P (x|y)
4. Pointwise mutual inform. log P (xy)P (x?)P (?y)
5. Mutual dependency (MD) log P (xy)2P (x?)P (?y)
6. Log frequency biased MD log P (xy)2P (x?)P (?y)+logP (xy)
7. Normalized expectation 2f(xy)f(x?)+f(?y)
8. Mutual expectation 2f(xy)f(x?)+f(?y) ?P (xy)
?9. Salience log P (xy)2P (x?)P (?y) ? logf(xy)
10. Pearson?s ?2 test Pi,j
(fij?f?ij)2
f?ij
11. Fisher?s exact test f(x?)!f(x??)!f(?y)!f(?y?)!N!f(xy)!f(xy?)!f(x?y)!f(x?y?)!
12.t test f(xy)?f?(xy)?f(xy)(1?(f(xy)/N))
13.z score f(xy)?f?(xy)?
f?(xy)(1?(f?(xy)/N))
14. Poison significance measure f?(xy)?f(xy) logf?(xy)+logf(xy)!logN
15. Log likelihood ratio ?2Pi,jfij log
fij
f?ij
16. Squared log likelihood ratio ?2Pi,j
logfij2
f?ij
Association coefficients:
17. Russel-Rao aa+b+c+d
18. Sokal-Michiner a+da+b+c+d
19. Rogers-Tanimoto a+da+2b+2c+d
20. Hamann (a+d)?(b+c)a+b+c+d
21. Third Sokal-Sneath b+ca+d
22. Jaccard aa+b+c
?23. First Kulczynsky ab+c
24. Second Sokal-Sneath aa+2(b+c)
25. Second Kulczynski 12 ( aa+b+ aa+c )
?26. Fourth Sokal-Sneath 14 ( aa+b+ aa+c+ dd+b+ dd+c )
?27. Odds ratio adbc
28. Yulle?s ?
?ad??bc?ad+?bc
29. Yulle?s Q ad?bcad+bc
30. Driver-Kroeber a?(a+b)(a+c)
31. Fifth Sokal-Sneath ad?(a+b)(a+c)(d+b)(d+c)
32. Pearson ad?bc?(a+b)(a+c)(d+b)(d+c)
33. Baroni-Urbani a+
?ad
a+b+c+?ad
?34. Braun-Blanquet amax(a+b,a+c)
?35. Simpson amin(a+b,a+c)
36. Michael 4(ad?bc)(a+d)2+(b+c)2
37. Mountford 2a2bc+ab+ac
38. Fager a?(a+b)(a+c)?
1
2max(b, c)
39. Unigram subtuples log adbc?3.29
q
1
a+ 1b + 1c + 1d
40. U cost log(1+ min(b,c)+amax(b,c)+a )
41. S cost log(1+min(b,c)a+1 )?
1
2
42. R cost log(1+ aa+b )?log(1+ aa+c )
43. T combined cost ?U?S?R
44. Phi P (xy)?P (x?)P (?y)?P (x?)P (?y)(1?P (x?))(1?P (?y))
45. Kappa P (xy)+P (x?y?)?P (x?)P (?y)?P (x??)P (?y?)1?P (x?)P (?y)?P (x??)P (?y?)
46. J measure max[P (xy)logP (y|x)P (?y) +P (xy?)log
P (y?|x)
P (?y?) ,
P (xy)logP (x|y)P (x?) +P (x?y)log
P (x?|y)
P (x??) ]
# Name Formula
47. Gini index max[P (x?)(P (y|x)2+P (y?|x)2)?P (?y)2
+P (x??)(P (y|x?)2+P (y?|x?)2)?P (?y?)2,
P (?y)(P (x|y)2+P (x?|y)2)?P (x?)2
+P (?y?)(P (x|y?)2+P (x?|y?)2)?P (x??)2]
48. Confidence max[P (y|x), P (x|y)]
49. Laplace max[NP (xy)+1NP (x?)+2 ,
NP (xy)+1
NP (?y)+2 ]
50. Conviction max[P (x?)P (?y)P (xy?) ,
P (x??)P (?y)
P (x?y) ]
51. Piatersky-Shapiro P (xy)?P (x?)P (?y)
52. Certainity factor max[P (y|x)?P (?y)1?P (?y) ,
P (x|y)?P (x?)
1?P (x?) ]
53. Added value (AV) max[P (y|x)?P (?y), P (x|y)?P (x?)]
54. Collective strength P (xy)+P (x?y?)P (x?)P (y)+P (x??)P (?y) ?
1?P (x?)P (?y)?P (x??)P (?y)
1?P (xy)?P (x?y?)
?55. Klosgen pP (xy) ?AV
Context measures:
?56. Context entropy ?Pw P (w|Cxy) logP (w|Cxy)
?57. Left context entropy ?Pw P (w|Clxy) logP (w|Clxy)
58. Right context entropy ?Pw P (w|Crxy) logP (w|Crxy)
59. Left context divergence P (x?) logP (x?)
?PwP (w|Clxy) logP (w|Clxy)
60. Right context divergence P (?y) logP (?y)
?PwP (w|Crxy) logP (w|Crxy)
61. Cross entropy ?PwP (w|Cx) logP (w|Cy)
62. Reverse cross entropy ?PwP (w|Cy) logP (w|Cx)
63. Intersection measure 2|Cx?Cy||Cx|+|Cy|
?64. Euclidean norm
qP
w(P (w|Cx)?P (w|Cy))2
65. Cosine norm
P
w P (w|Cx)P (w|Cy)P
w P (w|Cx)2?
P
w P (w|Cy)2
?66. L1 norm Pw |P (w|Cx)?P (w|Cy)|
67. Confusion probability Pw P (x|Cw)P (y|Cw)P (w)P (x?)
?68. Reverse confusion prob. Pw P (y|Cw)P (x|Cw)P (w)P (?y)
?69. Jensen-Shannon diverg. 12 [D(p(w|Cx)|| 12 (p(w|Cx)+p(w|Cy)))
+D(p(w|Cy)|| 12 (p(w|Cx)+p(w|Cy)))]
?70. Cosine of pointwise MI
P
w MI(w,x)MI(w,y)?P
w MI(w,x)2?
?P
w MI(w,y)2
71. KL divergence Pw P (w|Cx) logP (w|Cx)P (w|Cy)
72. Reverse KL divergence Pw P (w|Cy) log
P (w|Cy)
P (w|Cx)
?73. Skew divergence D(p(w|Cx)||?(w|Cy)+(1??)p(w|Cx))
74. Reverse skew divergence D(p(w|Cy)||?p(w|Cx)+(1??)p(w|Cy))
75. Phrase word coocurrence 12 (
f(x|Cxy)
f(xy) +
f(y|Cxy)
f(xy) )
76. Word association 12 (
f(x|Cy)?f(xy)
f(xy) +
f(y|Cx)?f(xy)
f(xy) )
Cosine context similarity: 12 (cos(cx,cxy)+cos(cy,cxy))
cz=(zi); cos(cx,cy)=
P xiyi?P xi2?
?P yi2
?77. in boolean vector space zi=?(f(wi|Cz))
78. in tf vector space zi=f(wi|Cz)
79. in tf?idf vector space zi=f(wi|Cz)? Ndf(wi); df(wi)= |{x :wi?Cx}|
Dice context similarity: 12 (dice(cx,cxy)+dice(cy ,cxy))
cz=(zi); dice(cx,cy)=
2P xiyiP xi2+
P yi2
80. in boolean vector space zi=?(f(wi|Cz))
81. in tf vector space zi=f(wi|Cz)
82. in tf?idf vector space zi=f(wi|Cz)? Ndf(wi); df(wi)= |{x :wi?Cx}|
a=f(xy) b=f(xy?) f(x?)
c=f(x?y) d=f(x?y?) f(x??)
f(?y) f(?y?) N
A contingency table contains observed frequencies and marginal frequencies for a bigram
xy; w? stands for any word except w; ? stands for any word; N is a total number of bi-
grams. The table cells are sometimes referred to as fij . Statistical tests of independence
work with contingency tables of expected frequenciesf?(xy)=f(x?)f(?y)/N .
Cw empirical context of w
Cxy empirical context of xy
Clxy left immediate context of xy
Crxy right immediate context of xy
Table 1: Lexical association measures used for bigram collocation extraction.
?denotes those selected by the model reduction algorithm discussed in Section 5.
653
Recall
Pr
ec
isi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
Unaveraged precision curve
Averaged precison curve
Figure 1: Vertical averaging of precision-recall curves. Thin
curves represent individual non-averaged curves obtained by
Pointwise mutual information (4) on five data folds.
a specified context window), the association mea-
sures rank collocations according to the assump-
tion that semantically non-compositional expres-
sions typically occur as (semantic) units in differ-
ent contexts than their components (Zhai, 1997).
Measures (61?74) have information theory back-
ground and measures (75?82) are adopted from the
field of information retrieval.
3.1 Evaluation
Collocation extraction can be viewed as classifi-
cation into two categories. By setting a threshold,
any association measure becomes a binary clas-
sifier: bigrams with higher association scores fall
into one class (collocations), the rest into the other
class (non-collocations). Performance of such
classifiers can be measured for example by accu-
racy ? fraction of correct predictions. However,
the proportion of the two classes in our case is far
from equal and we want to distinguish classifier
performance between them. In this case, several
authors, e.g. Evert (2001), suggest using precision
? fraction of positive predictions correct and re-
call ? fraction of positives correctly predicted. The
higher the scores the better the classification is.
3.2 Precision-recall curves
Since choosing a classification threshold depends
primarily on the intended application and there is
no principled way of finding it (Inkpen and Hirst,
2002), we can measure performance of associa-
tion measures by precision?recall scores within
the entire interval of possible threshold values. In
this manner, individual association measures can
be thoroughly compared by their two-dimensional
precision-recall curves visualizing the quality of
ranking without committing to a classification
threshold. The closer the curve stays to the top
and right, the better the ranking procedure is.
Recall
Av
er
ag
e 
pr
ec
isi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
Pointwise mutual information (4)
Pearson?s test (10)
z score (13)
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
Figure 2: Crossvalidated and averaged precision-recall
curves of selected association measures (numbers in brack-
ets refer to Table 1).
Precision-recall curves are very sensitive to data
(see Figure 1). In order to obtain a good esti-
mate of their shapes cross validation and averag-
ing are necessary: all cross-validation folds with
scores for each instance are combined and a single
curve is drawn. Averaging can be done in three
ways: vertical ? fixing recall, averaging precision,
horizontal ? fixing precision, averaging recall, and
combined ? fixing threshold, averaging both preci-
sion and recall (Fawcett, 2003). Vertical averag-
ing, as illustrated in Figure 1, worked reasonably
well in our case and was used in all experiments.
3.3 Mean average precision
Visual comparison of precision-recall curves is
a powerfull evaluation tool in many research fields
(e.g. information retrieval). However, it has a seri-
ous weakness. One can easily compare two curves
that never cross one another. The curve that pre-
dominates another one within the entire interval
of recall seems obviously better. When this is not
the case, the judgment is not so obvious. Also
significance tests on the curves are problematic.
Only well-defined one-dimensional quality mea-
sures can rank evaluated methods by their per-
formance. We adopt such a measure from in-
formation retrieval (Hull, 1993). For each cross-
-validation data fold we define average precision
(AP) as the expected value of precision for all pos-
sible values of recall (assuming uniform distribu-
tion) and mean average precision (MAP) as a mean
of this measure computed for each data fold. Sig-
nificance testing in this case can be realized by
paired t-test or by more appropriate nonparametric
paired Wilcoxon test.
Due to the unreliable precision scores for low
recall and their fast changes for high recall, esti-
mation of AP should be limited only to some nar-
rower recall interval, e.g. ?0.1,0.9?
654
M
ea
n 
av
er
ag
e 
pr
ec
isi
on
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
77 80 32 30 10 42 4 28 63 22 23 7 20 19 43 6 9 50 48 8 59 73 61 25 11 74 68 53 52 35 41 55 47 81 46 2 51 78 58 57 1739 38 31 13 5 37 27 29 16 24 45 33 21 18 34 54 76 3 82 44 66 71 26 15 14 72 70 64 49 65 69 40 75 56 12 60 36 79 62 1 67 77 38 30 5 4 29 22 45 20 18 6 76 48 44 73 26 11 72 53 49 41 40 81 12 51 79 57 67
67
57
79
51
12
81
40
41
49
53
72
11
26
73
44
48
76
6
18
20
45
22
29
4
5
30
38
77
Figure 3: a) Mean average precision of all association measures in descending order. Methods are referred by numbers
from Table 1. The solid points correspond to measures selected by the model reduction algorithm from Section 5. b) Visu-
alization of p-values from the significance tests of difference between each method pair (order is the same for both graphs). The
darker points correspond to p-values greater than ?=0.1 and indicate methods with statistically indistinguishable performance
(measured by paired Wilcoxon test on values of average precision obtained from five independent data folds).
3.4 Experiments and results
In the initial experiments, we implemented all 82
association measures from Table 1, processed all
morphologically and syntactically annotated sen-
tences from PDT 2.0, and computed scores of all
the association measures for each dependency bi-
gram in the reference data. For each associa-
tion measure and each of the five evaluation data
folds, we computed precision-recall scores and
drew an averaged precision-recall curve. Curves
of some well-performing methods are depicted in
Figure 2. Next, for each association measure and
each data fold, we estimated scores of average pre-
cision on narrower recall interval ?0.1,0.9?, com-
puted mean average precision, ranked the asso-
ciation measures according to MAP in descend-
ing order, and result depicted in Figure 3 a). Fi-
nally, we applied a paired Wilcoxon test, detected
measures with statistically indistinguishable per-
formance, and visualized this information in Fig-
ure 3 b).
A baseline system ranking bigrams randomly
operates with average precision of 20.9%. The
best performing method for collocation extrac-
tion measured by mean average precision is co-
sine context similarity in boolean vector space (77)
(MAP 66.49%) followed by other 16 associa-
tion measures with nearly identical performance
(Figure 3 a). They include some popular meth-
ods well-known to perform reliably in this task,
such as pointwise mutual information (4), Pear-
son?s ?2 test (10), z score (13), odds ratio (27), or
squared log likelihood ratio (16).
The interesting point to note is that, in terms
of MAP, context similarity measures, e.g. (77),
slightly outperform measures based on simple oc-
curence frequencies, e.g. (39). In a more thorough
comparison by percision-recall curves, we observe
that the former very significantly predominates the
latter in the first half of the recall interval and vice
versa in the second half (Figure 2). This is a case
where the MAP is not a sufficient metric for com-
parison of association measure performance. It is
also worth pointing out that even if two methods
have the same precision-recall curves the actual bi-
gram rank order can be very different. Existence
of such non-correlated (in terms of ranking) mea-
sures will be essential in the following sections.
4 Combining association measures
Each collocation candidate xi can be described by
the feature vector xi = (xi1, . . . , xi82)T consisting
of 82 association scores from Table 1 and assigned
a label yi ? {0, 1} which indicates whether the
bigram is considered to be a collocation (y = 1)
or not (y = 0). We look for a ranker function
f(x)?R that determines the strength of lexical
association between components of bigram x and
hence has the character of an association measure.
This allows us to compare it with other association
measures by the same means of precision-recall
curves and mean average precision. Further, we
present several classification methods and demon-
strate how they can be employed for ranking, i.e.
what function can be used as a ranker. For refer-
ences see Venables and Ripley (2002).
4.1 Linear logistic regression
An additive model for binary response is repre-
sented by a generalized linear model (GLM) in
a form of logistic regression:
logit(pi) = ?0 + ?1x1 + . . .+ ?pxp
655
method AP MAP
R=20 R=50 R=80 R=?0.1,0.9? +
NNet (5 units) 89.56 82.74 70.11 80.81 21.53
NNet (3 units) 89.41 81.99 69.64 79.71 19.88
NNet (2 units) 86.92 81.68 68.33 78.77 18.47
SVM (linear) 85.72 79.49 63.86 75.66 13.79
LDA 84.72 77.18 62.90 75.11 12.96
SVM (quadratic) 84.29 79.54 64.24 74.53 12.09
NNet (1 unit) 77.98 76.83 66.75 73.25 10.17
GLM 82.45 76.26 58.61 71.88 8.11
Cosine similarity (77) 80.94 68.90 50.54 66.49 0.00
Unigram subtuples (39) 74.55 67.49 55.16 65.74 -
Table 2: Performance of methods combining all association
measures: average precision (AP) for fixed recall values and
mean average precision (MAP) on the narrower recall interval
with relative improvement in the last column (values in %).
where logit(pi)= log(pi/(1?pi)) is a canonical link
function for odds-ratio and pi ? (0, 1) is a con-
ditional probability for positive response given
a vector x. The estimation of ?0 and ? is done
by maximum likelihood method which is solved
by the iteratively reweighted least squares algo-
rithm. The ranker function in this case is defined
as the predicted value ?, or equivalently (due to
the monotonicity of logit link function) as the lin-
ear combination ??0 + ??Tx.
4.2 Linear discriminant analysis
The basic idea of Fisher?s linear discriminant anal-
ysis (LDA) is to find a one-dimensional projection
defined by a vector c so that for the projected com-
bination cTx the ratio of the between variance B
to the within variance W is maximized:
maxc
cTBc
cTW c
After projection, cTx can be directly used as ranker.
4.3 Support vector machines
For technical reason, let us now change the labels
yi?{-1,+1}. The goal in support vector machines
(SVM) is to estimate a function f(x)=?0+?Tx and
find a classifier y(x) = sign(f(x)) which can be
solved through the following convex optimization:
min
?0,?
n?
i=1
[1?yi(?0 + ?T xi)
]++ ?2 ||?||
2
with ? as a regularization parameter. The hinge
loss function L(y,f(x)) = [1?yf(x)]+ is active
only for positive values (i.e. bad predictions) and
therefore is very suitable for ranking models with
??0+ ??Tx as a ranker function. Setting the regu-
larization parameter ? is crucial for both the es-
timators ??0, ?? and further classification (or rank-
ing). As an alternative to a often inappropriate grid
Recall
Av
er
ag
e 
pr
ec
isi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
Neural network (5 units)
Support vector machine (linear)
Linear discriminant analysis
Neural network (1 unit)
Linear logistic regression
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
Figure 4: Precision-recall curves of selected methods com-
bining all association measures compared with curves of two
best measures employed individually on the same data sets.
search, Hastie (2004) proposed an effective algo-
rithm which fits the entire SVM regularization path
[?0(?),?(?)] and gave us the option to choose the
optimal value of ?. As an objective function we
used total amount of loss on training data.
4.4 Neural networks
Assuming the most common model of neural net-
works (NNet) with one hidden layer, the aim is to
find inner weights wjh and outer weights whi for
yi=?0
(?0 +
?
whi?h(?h +
?
wjhxj)
)
where h ranges over units in the hidden layer. Ac-
tivation functions ?h and function ?0 are fixed.
Typically, ?h is taken to be the logistic function
?h(z) = exp(z)/(1 + exp(z)) and ?0 to be the
indicator function ?0(z) = I(z > ?) with ? as
a classification threshold. For ranking we simply
set ?0(z) = z. Parameters of neural networks are
estimated by the backpropagation algorithm. The
loss function can be based either on least squares
or maximum likehood. To avoid problems with
convergence of the algorithm we used the former
one. The tuning parameter of a classifier is then
the number of units in the hidden layer.
4.5 Experiments and results
To avoid incommensurability of association mea-
sures in our experiments, we used a common pre-
processing technique for multivariate standardiza-
tion: we centered values of each association mea-
sure towards zero and scaled them to unit variance.
Precision-recall curves of all methods were ob-
tained by vertical averaging in five-fold cross val-
idation on the same reference data as in the ear-
lier experiments. Mean average precision was
computed from average precision values estimated
656
on the recall interval ?0.1,0.9?. In each cross-
-validation step, four folds were used for training
and one fold for testing.
All methods performed very well in compari-
son with individual measures. The best result was
achieved by a neural network with five units in the
hidden layer with 80.81% MAP, which is 21.53%
relative improvement compared to the best indi-
vidual associaton measure. More complex mod-
els, such as neural networks with more than five
units in the hidden layer and support vector ma-
chines with higher order polynomial kernels, were
highly overfitted on the training data folds and bet-
ter results were achieved by simpler models. De-
tailed results of all experiment are given in Ta-
ble 2 and precision-recall curves of selected meth-
ods depicted in Figure 4.
5 Model reduction
Combining association measures by any of the
presented methods is reasonable and helps in the
collocation extraction task. However, the combi-
nation models are too complex in number of pre-
dictors used. Some association measures are very
similar (analytically or empirically) and as predic-
tors perhaps even redundant. Such measures have
no use in the models, make their training harder,
and should be excluded. Principal component
analysis applied to the evaluation data showed that
95% of its total variance is explained by only 17
principal components and 99.9% is explained by
42 of them. This gives us the idea that we should
be able to significantly reduce the number of vari-
ables in our models with no (or relativelly small)
degradation in their performance.
5.1 The algorithm
A straightforward, but in our case hardly feasible,
approach is an exhaustive search through the space
of all possible subsets of all association measures.
Another option is a heuristic step-wise algorithm
iteratively removing one variable at a time until
some stopping criterion is met. Such algorithms
are not very robust, they are sensitive to data and
generally not very recommended. However, we
tried to avoid these problems by initializing our
step-wise algorithm by clustering similar variables
and choosing one predictor from each cluster as
a representative of variables with the same contri-
bution to the model. Thus we remove the highly
corelated predictors and continue with the step-
-wise procedure.
Recall
Av
er
ag
e 
pr
ec
isi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
NNet (5 units) with 82 predictors
NNet (5 units) with 42 predictors
NNet (5 units) with 17 predictors
NNet (5 units) with 7 predictors
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
Figure 5: Precision-recall curves of four NNet models from
the model reduction process with different number of predic-
tors compared with curves of two best individual methods.
The algorithm starts with the hierarchical clus-
tering of variables in order to group those with
a similar contribution to the model, measured by
the absolute value of Pearson?s correlation coeffi-
cient. After 82?d iterations, variables are grouped
into d non-empty clusters and one representative
from each cluster is selected as a predictor into the
initial model. This selection is based on individual
predictor performance on held-out data.
Then, the algorithm continues with d predictors
in the initial model and in each iteration removes
a predictor causing minimal degradation of perfor-
mance measured by MAP on held-out data. The
algorithm stops when the difference becomes sig-
nificant ? either statistically (by paired Wilcoxon
test) or practically (set by a human).
5.2 Experiments and results
We performed the model reduction experiment on
the neural network with five units in the hidden
layer (the best performing combination method).
The similarity matrix for hierarchical clustering
was computed on the held-out data and parame-
ter d (number of initial predictors) was experimen-
tally set to 60. In each iteration of the algorithm,
we used four data folds (out of the five used in pre-
vious experiments) for fitting the models and the
held-out fold to measure the performance of these
models and to select the variable to be removed.
The new model was cross-validated on the same
five data-folds as in the previous experiments.
Precision-recall curves for some intermediate
models are shown in Figure 5. We can conclude
that we were able to reduce the NNet model to
about 17 predictors without statistically signifi-
cant difference in performance. The correspond-
ing association measures are marked in Table 1
and highlighted in Figure 3a). They include mea-
sures from the entire range of individual mean av-
erage precision values.
657
6 Conclusions and discussion
We created and manually annotated a reference
data set consisting of 12 232 Czech dependency
bigrams. 20.9% of them were agreed to be a col-
location by three annotators. We implemented 82
association measures, employed them for collo-
cation extraction and evaluated them against the
reference data set by averaged precision-recall
curves and mean average precision in five-fold
cross validation. The best result was achieved by
a method measuring cosine context similarity in
boolean vector space with mean average precision
of 66.49%.
We exploit the fact that different subgroups of
collocations have different sensitivity to certain
association measures and showed that combining
these measures aids in collocation extraction. All
investigated methods significantly outperformed
individual association measures. The best results
were achieved by a simple neural network with
five units in the hidden layer. Its mean average
precision was 80.81% which is 21.53% relative
improvement with respect to the best individual
measure. Using more complex neural networks or
a quadratic separator in support vector machines
led to overtraining and did not improve the perfor-
mace on test data.
We proposed a stepwise feature selection algo-
rithm reducing the number of predictors in com-
bination models and tested it with the neural net-
work. We were able to reduce the number of its
variables from 82 to 17 without significant degra-
dation of its performance.
No attempt in our work has been made to select
the ?best universal method? for combining associ-
ation measures nor to elicit the ?best association
measures? for collocation extraction. These tasks
depend heavily on data, language, and notion of
collocation itself. We demonstrated that combin-
ing association measures is meaningful and im-
proves precission and recall of the extraction pro-
cedure and full performance improvement can be
achieved by a relatively small number of measures
combined.
Preliminary results of our research were already
published in Pecina (2005). In the current work,
we used a new version of the Prague Dependecy
Treebank (PDT 2.0, 2006) and the reference data
was improved by additional manual anotation by
two linguists.
Acknowledgments
This work has been supported by the Ministry of
Education of the Czech Republic, projects MSM
0021620838 and LC 536. We would like to thank
our advisor Jan Hajic?, our colleagues, and anony-
mous reviewers for their valuable comments.
References
Y. Choueka. 1988. Looking for needles in a haystack or lo-
cating interesting collocational expressions in large textual
databases. In Proceedings of the RIAO.
S. Evert and B. Krenn. 2001. Methods for the qualitative
evaluation of lexical association measures. In Proceedings
of the 39th Annual Meeting of the ACL, Toulouse, France.
S. Evert. 2004. The Statistics of Word Cooccurrences: Word
Pairs and Collocations. Ph.D. thesis, Univ. of Stuttgart.
T. Fawcett. 2003. ROC graphs: Notes and practical con-
siderations for data mining researchers. Technical report,
HPL-2003-4. HP Laboratories, Palo Alto, CA.
T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004. The
entire regularization path for the support vector machine.
Journal of Machine Learning Research, 5.
D. Hull. 1993. Using statistical testing in the evaluation of
retrieval experiments. In Proceedings of the 16th annual
international ACM SIGIR conference on Research and de-
velopment in information retrieval, New York, NY.
D. Inkpen and G. Hirst. 2002. Acquiring collocations for
lexical choice between near synonyms. In SIGLEX Work-
shop on Unsupervised Lexical Acquisition, 40th meeting
of the ACL, Philadelphia.
K. Kita, Y. Kato, T. Omoto, and Y. Yano. 1994. A compar-
ative study of automatic extraction of collocations from
corpora: Mutual information vs. cost criteria. Journal of
Natural Language Processing.
B. Krenn. 2000. The Usual Suspects: Data-Oriented Models
for Identification and Representation of Lexical Colloca-
tions. Ph.D. thesis, Saarland University.
C. D. Manning and H. Sch?tze. 1999. Foundations of Statis-
tical Natural Language Processing. The MIT Press, Cam-
bridge, Massachusetts.
R. Mihalcea and T. Pedersen. 2003. An evaluation exercise
for word alignment. In Proceedings of HLT-NAACL Work-
shop, Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, Edmonton, Alberta.
R. C. Moore. 2004. On log-likelihood-ratios and the signif-
icance of rare events. In Proceedings of the 2004 Confer-
ence on EMNLP, Barcelona, Spain.
D. Pearce. 2002. A comparative evaluation of collocation ex-
traction techniques. In Third International Conference on
language Resources and Evaluation, Las Palmas, Spain.
P. Pecina. 2005. An extensive empirical study of colloca-
tion extraction methods. In Proceedings of the ACL 2005
Student Research Workshop, Ann Arbor, USA.
S. Shimohata, T. Sugio, and J. Nagata. 1997. Retrieving col-
locations by co-occurrences and word order constraints.
In Proc. of the 35th Meeting of ACL/EACL, Madrid, Spain.
W. N. Venables and B. D. Ripley. 2002. Modern Applied
Statistics with S. 4th ed. Springer Verlag, New York.
C. Zhai. 1997. Exploiting context to identify lexical atoms:
A statistical view of linguistic context. In International
and Interdisciplinary Conf. on Modeling and Using Context.
PDT 2.0. 2006. http://ufal.mff.cuni.cz/pdt2.0/.
658
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 209?212,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Play the Language: Play Coreference
Barbora Hladk
?
a and Ji
?
r?? M??rovsk?y and Pavel Schlesinger
Charles University in Prague
Institute of Formal and Applied Linguistics
e-mail: {hladka, mirovsky, schlesinger@ufal.mff.cuni.cz}
Abstract
We propose the PlayCoref game, whose
purpose is to obtain substantial amount of
text data with the coreference annotation.
We provide a description of the game de-
sign that covers the strategy, the instruc-
tions for the players, the input texts selec-
tion and preparation, and the score evalua-
tion.
1 Introduction
A collection of high quality data is resource-
demanding regardless of the area of research and
type of the data. This fact has encouraged a
formulation of an alternative way of data col-
lection, ?Games With a Purpose? methodology
(GWAP), (van Ahn and Dabbish, 2008). The
GWAP methodology exploits the capacity of Inter-
net users who like to play on-line games. The on-
line games are being designed to generate data for
applications that either have not been implemented
yet, or have already been implemented with a per-
formance lower than human. Moreover, the play-
ers work simply by playing the game - the data are
generated as a by-product of the game. If the game
is enjoyable, it brings human resources and saves
financial resources. The game popularity brings
more game sessions and thus more annotated data.
The GWAP methodology was formulated in
parallel with design and implementation of the
on-line games with images (van Ahn and Dab-
bish, 2004) and subsequently with tunes (Law
et al, 2007),
1
in which the players try to agree
on a caption of the image/tune. The popularity of
the games is enormous so the authors have suc-
ceeded in the basic requirement that the annota-
tion is generated in a substantial amount. Then
the Onto games appeared (Siorpaes and Hepp,
1
www.gwap.org
2008), bringing a new type of input data to GWAP,
namely video and text.
2
The situation with text seems to be slightly dif-
ferent. One has to read a text in order to identify
its topics, which takes more time than observing
images, and the longer text, the worse. Since the
game must be of a dynamic character, it is unimag-
inable that the players will spend minutes reading
an input text. Therefore, the text must be opened
to the players ?part? by ?part?.
So far, besides the Onto games, two more games
with texts have been designed: What did Shan-
non say?
3
, the goal of which is to help the speech
recognizer with difficult-to-recognize words, and
Phrase Detectives
4
(Kruschwitz, Chamberlain,
Poesio, 2009), the goal of which is to identify re-
lationships between words and phrases in a text.
Motivated by the GWAP portal, the LGame por-
tal
5
has been established. Seven key properties
that any game on the LGame portal will satisfy
were formulated ? see Table 1.
The LGame portal has been opened with the
Shannon game, a game of intentionally hidden
words in the sentence, where players guess them,
and the Place the Space game, a game of word
segmentation.
Within a systematic framework established at
the LGame portal, the games PlayCoref, PlayNE,
PlayDoc devoted to the linguistic phenomena
dealing with the contents of documents, namely
coreference, named-entitites, and document la-
bels, respectively, are being designed in parallel
but implemented subsequently since the GWAPs
are open-ended stories the success of which is hard
to estimate in advance. These games are designed
for Czech and English by default. However, the
game rules are language independent.
2
www.ontogame.org
3
lingo.clsp.jhu.edushannongame.html
4
www.phrasedetectives.org
5
www.lgame.cz
209
1. During the game, the data are collected for the natural
language processing tasks that computers cannot solve
at all or not well enough.
2. Playing the game only requires a basic knowledge
of the grammar of the language of the game. No extra
linguistic knowledge is required.
3. The game rules are designed independently of the
language of the game.
4. The game is designed for Czech and English by de-
fault.
5. During the game, the players have at least a general
idea of what their opponent(s) do.
6. The game is designed for at least two players (also a
computer can be an opponent).
7. The game offers several levels of difficulty (to fit a
vast range of players).
Table 1: Key properties of the games on the LGame portal.
We have decided to implement the PlayCoref
first. Coreference crosses the sentence boundaries
and playing coreference offers a great opportunity
to test players? willingness to read a text part by
part, e.g. sentence by sentence. In this paper, we
discuss various aspects of the PlayCoref design.
2 Coreference
Coreference occurs when several referring expres-
sions in a text refer to the same entity (e.g. per-
son, thing, reality). A coreferential pair is marked
between subsequent pairs of the referring expres-
sions. A sequence of coreferential pairs referring
to the same entity in a text forms a coreference
chain.
Various projects on the coreference annotation
by linguists are running. We mention two of
them ? the Prague Dependency Treebank 2.0 and
the coreference task for the sixth Message Under-
standing Conference.
Prague Dependency Treebank 2.0 (PDT 2.0)
6
is the only corpus establishing the coreference
annotation on a layer of meaning, so-called tec-
togrammatical layer (t-layer). The annotation in-
cludes grammatical and textual coreference. Ex-
tended textual coreference (covering additional
categories) is being annotated in PDT 2.0 in an on-
going project (Nedoluzhko, 2007).
Sixth Message Understanding Conference ? the
coreference task (MUC-6)
7
operates on a sur-
face layer. The coreferential pairs are marked be-
tween pairs of the categories nouns, noun phrases,
and pronouns.
6
ufal.mff.cuni.cz/pdt2.0
7
cs.nyu.edu/faculty/grishman/muc6.html
3 The PlayCoref Game
Motivation The PDT 2.0 coreference annota-
tion (including the annotation scheme design,
training of the annotators, technical and linguistic
support, and annotation corrections) spanned the
period from summer 2002 till autumn 2004. Each
of two annotators annotated one half out of 3,165
documents. We are aware that coreferential pairs
marked in the PlayCoref sessions may differ from
the PDT 2.0 coreference annotation. However,
the following estimates reinforce our motivation
to use the GWAP technology on texts: assuming
that (1) the PlayCoref is designed as a two-player
game, (2) at least one document is being present
in each session, (3) the session lasts up to 5 min-
utes and (4) the players play half an hour a day,
then at least 6 documents will be processed a day
by two players. This means that 3,165 documents
will be annotated by two players in 528 days, by
eight players in 132 days, by 32 players in 33 days
etc., and by 128 players in 9 days.
Strategy The game is designed for two players.
The game starts with several first sentences of the
document displayed in the players? sentence win-
dow. According to the restrictions put on the mem-
bers of the coreferential pairs, parts of the text are
unlocked while the other parts are locked. Only
unlocked parts of the text are allowed to become
a member of the coreferential pair. In our case,
only nouns and selected pronouns are unlocked.
8
In Table 2, we provide a list of the locked pro-
noun?s sub-part-of-speech classes (as designed in
the Czech positional tag system). Pronouns of
the other sub-part-of-speech classes are unlocked.
The selection of the locked pronoun?s sub-part-of-
speech classes is based on the fact that some types
of pronouns usually corefer with parts of the text
larger than one word. This type of coreference
cannot be annotated without a linguistic knowl-
edge and without training. Therefore it must be
omitted for the purposes of the PlayCoref game.
The players mark coreferential pairs between
the unlocked words in the text (no phrases are al-
lowed). They mark the coreferential pairs as undi-
rected links.
9
After the session, the coreference
8
A tagging procedure is used to get the part-of-speech
classes of the words.
9
This strategy differs from the general conception of
coreference being understood as either the anaphoric or cat-
aphoric relation depending on ?direction? of the link in the
text. We believe that the players will benefit from this sim-
210
Locked pronouns: subPOS and its description
D Demonstrative (?ten?, ?onen?, ..., lit. ?this?, ?that?, ?that?, ...
?over there?, ... )
E Relative ?co?z? (corresponding to English which in subordinate
clauses referring to a part of the preceding text)
L Indefinite ?v?sechen?, ?s?am? (lit. ?all?, ?alone?)
O ?sv?uj?, ?nesv?uj?, ?tentam? alone (lit. ?own self?, ?not-in-mood?,
?gone?)
Q Relative/interrogative ?co?, ?copak?, ?co?zpak? (lit. ?what?, ?isn?t-
it-true-that?)
W Negative (?nic?, ?nikdo?, ?nijak?y?, ??z?adn?y?, ..., lit. ?nothing?,
?nobody?, ?not-worth-mentioning?, ?no?/?none?)
Y Relative/interrogative ?co? as an enclitic (after a preposition)
(?o?c?, ?na?c?, ?za?c?, lit. ?about what?, ?on?/?onto? ?what?, ?af-
ter?/?for what?)
Z Indefinite (?n?ejak?y?, ?n?ekter?y?, ??c??koli?, ?cosi?, ..., lit. ?some?,
?some?, ?anybody?s?, ?something?)
Table 2: List of the pronoun?s sub-part-of-speech classes in
the Czech positional tag system locked for the PlayCoref.
chains are automatically reconstructed from the
coreferential pairs marked.
During the session, the number of words the
opponent has linked into the coreferential pairs is
displayed to the player. The number of sentences
with at least one coreferential pair marked by the
opponent is displayed to the player as well. Re-
vealing more information about the opponent?s ac-
tions would affect the independency of the play-
ers? decisions.
If the player finishes pairing all the related
words in a visible part of the document (visible
to him), he asks for the next sentence of the docu-
ment. It appears at the bottom of his sentence win-
dow. The player can remove pairs created before
at any time and can make new pairs in the sen-
tences read so far. The session goes on this way
until the end of the session time.
Instructions for the Players Instructions for the
players must be as comprehensible and concise as
possible. To mark a coreferential pair, no linguis-
tic knowledge is required. It is all about the text
comprehension ability.
Input Texts In the first stage of the project, doc-
uments from PDT 2.0 and MUC-6 will be used in
the sessions, so that the quality of the game data
can be evaluated against the manual coreference
annotation.
Since the PDT 2.0 coreference annotation oper-
ates on the tectogrammatical layer and PlayCoref
on the surface layer, the coreferential pairs of the t-
layer must be projected to the surface first. The ba-
sic steps of the projection are depicted in Figure 1.
Going from the t-layer, some of the coreferential
plification and that the quality of the game data will not be
decreased.
pairs get lost because their members do not have
their counterparts on surface.
10
From the remain-
ing coreferential pairs, those between nouns and
unlocked pronouns are selected. In the final game
documents, the difference between the grammat-
ical, textual and extended textual coreference is
omitted, because the players will not be asked to
distinguish them. Table 3 shows the number of
coreferential pairs in various stages of the projec-
tion.
DEEPSURFGRAM DEEPSURFTEXT
DEEPSURFGRAM DEEPSURFTEXT DEEPSURF
EX TT EE XN TD
PDT 2.0 PDT 2.0+ ext. textualcoreference surfacesubset
GRAMSURFunlockedTEXTSURFunlockedEXTENDTEXTSURFunlocked
PlayCorefdatalockedunlockedG  SR  UA  RM  F lockedunlockedT  SE  UX  RT  F lockedunlockedE    T  SE  UX  RT  F
Figure 1: Projection of the PDT coreference annotation to
the surface layer. The first step depicts the annotation of the
extended textual coreference. Pairs that have no surface coun-
terparts are marked DEEP, pairs with surface counterparts
are marked SURF. Pairs suitable for the game are marked un-
locked.
Data from the coreference task on the sixth
Message Understanding Conference can be used
in a much more straightforward way. Coreference
is annotated on the surface and no projection is
needed. The links with noun phrases are disre-
garded.
PDT 2.0 PDT 2.0 surface PlayCoref
+ ext. subset
# coref. pairs 45 96 70 33
Table 3: Number of coreferential pairs (in thousands) in
various stages of projection. Counts in the second, third and
fourth columns are extrapolated on the basis of data anno-
tated so far, which is about 200 thousand word tokens in 12
thousand sentences (out of 833 thousand tokens in 49 thou-
sand sentences in PDT 2.0). Type of the coreferential pairs,
either grammatical or textual one, is not distinguished.
Scoring The players get points for their coref-
erential pairs according to the equation pts
A
=
w
1
?ICA(A, acr)+w
2
?ICA(A,B) where A and
B are the players, acr is an automatic coreference
resolution procedure, weights 0 ? w
1
, w
2
? 1,
w
1
, w
2
? R are set empirically, and ICA stands for
the inter-coder agreement that we can simultane-
ously express either by the F-measure or Krippen-
10
Czech is a ?pro-drop? language, in which the subject pro-
noun on ?he? has a zero form (also in feminine, plural, etc.).
211
D E P
Figure 2: Player ?1? pairs (A,C) ? the dotted curve; player
?2? pairs (A,B) and (B,C) ? the solid lines; player ?3? pairs
(A,B) and (A,C) ? the dashed curves. Although players ?1?
and ?2? do not agree on the coreferential pairs at all, ?1? and
?3? agree only on (A,C) and ?2? and ?3? agree only on (A,B),
for the purposes of the coreference chains reconstruction, the
players? agreement is higher: players ?1? and ?2? agree on two
members of the coreferential chain: A and C, players ?1? and
?3? agree on A and C as well, and players ?2? and ?3? achieved
agreement even on all three members: A, B, and C.
dorff?s ? (Artstein and Poesio, 2008). The score
is calculated at the end of the session and no run-
ning score is being presented during the session.
Otherwise, the players might adjust their decisions
according to the changes in the score. Obviously,
it is undesirable.
Assigning a score to the players deals with the
coreferential pairs. However, motivated by (Pas-
sonneau, 2004) and others, the evaluation handles
the coreferential pairs in a way demonstrated in
Figure 2.
PlayCoref vs. PhraseDetectives At least to
our knowledge, there are no other GWAPs deal-
ing with the relationship among words in a text
like PhraseDetectives and PlayCoref. Neverthe-
less, there are many differences between these two
games ? the main ones are enumerated in Table 4.
PlayCoref PhraseDetectives
detection of coreference
chains
anaphora resolution
two-player game one-player game
a document presented sen-
tence by sentence
a paragraph presented at
once
? checking the pairs marked
in the previous sessions
pairing not restricted to the
position in the text
the closest antecedent
simple instructions players training
scoring with respect to the
automatic coreference reso-
lution and to the opponent?s
pairs
scoring with respect to the
players that play with the
same document before
coreferential pairs correc-
tion
no corrections allowed
Table 4: PlayCoref vs. PhraseDetectives.
4 Conclusion
We propose the PlayCoref game, a concept of a
GWAP with texts that aims at getting the docu-
ments with the coreference annotation in substan-
tially larger volume than can be obtained from
experts. In the proposed game, we introduce
coreference to the players in a way that no lin-
guistic knowledge is required from them. We
present the game rules design, the preparation of
the game documents and the evaluation of the
players? score. A short comparison with a simi-
lar project is also provided.
Acknowledgments
We gratefully acknowledge the support of the
Czech Ministry of Education (grants MSM-
0021620838 and LC536), the Czech Grant
Agency (grant 405/09/0729), and the Grant
Agency of Charles University in Prague (project
GAUK 138309).
References
Ron Artstein, Massimo Poesio. 2008. Inter-Coder Agree-
ment for Computational Linguistics. Computational Lin-
guistics, December 2008, vol. 34, no. 4, pp. 555?596.
Udo Kruschwitz, Jon Chamberlain, Massimo Poesio. 2009.
(Linguistic) Science Through Web Collaboration in the
ANAWIKI project. In Proceedings of the WebSci?09: So-
ciety On-Line, Athens, Greece, in press.
Lucie Ku?cov?a, Eva Haji?cov?a. 2005. Coreferential Relations
in the Prague Dependency Treebank. In Proceedings of
the 5th International Conference on Discourse Anaphora
and Anaphor Resolution, San Miguel, Azores, pp. 97?102.
Edith. L. M. Law et al 2007. Tagatune: A game for music
and sound annotation. In Proceedings of the Music In-
formation Retrieval Conference, Austrian Computer Soc.,
pp. 361?364.
Anna Nedoluzhko. 2007. Zpr?ava k anotov?an?? roz?s???ren?e
textov?e koreference a bridging vztah?u v Pra?zsk?em
z?avoslostn??m korpusu (Annotating extended coreference
and bridging relations in PDT). Technical Report, UFAL,
MFF UK, Prague, Czech Republic.
Rebecca J. Passonneau. 2004. Computing Reliability for
Coreference. Proceedings of LREC, vol. 4, pp. 1503?
1506, Lisbon.
Katharina Siorpaes and Martin Hepp. 2008. Games with a
purpose for the Semantic Web. IEEE Intelligent Systems
Vol. 23, number 3, pp. 50?60.
Luis van Ahn and Laura Dabbish. 2004. Labelling images
with a computer game. In Proceedings of the SIGHI Con-
ference on Human Factors in Computing Systems, ACM
Press, New York, pp. 319?326.
Luis van Ahn and Laura Dabbish. 2008. Designing Games
with a Purpose. Communications of the ACM, vol. 51,
No. 8, pp. 58?67.
212
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 52?55,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Designing a Language Game for Collecting Coreference Annotation
Barbora Hladka? and Jir??? M??rovsky? and Pavel Schlesinger
Charles University in Prague
Institute of Formal and Applied Linguistics
e-mail: {hladka, mirovsky, schlesinger@ufal.mff.cuni.cz}
Abstract
PlayCoref is a concept of an on-line lan-
guage game designed to acquire a substan-
tial amount of text data with the corefer-
ence annotation. We describe in detail var-
ious aspects of the game design and dis-
cuss features that affect the quality of the
annotation.
1 Introduction
Creating a collection of high quality data is
resource-demanding regardless of the area of re-
search and type of the data. This fact has encour-
aged a formulation of an alternative way of data
collection, ?Games With a Purpose? methodology
(GWAP), (van Ahn and Dabbish, 2008). The
GWAP methodology exploits the capacity of In-
ternet users who like to play on-line games. The
games are designed to generate data for applica-
tions that either have not been implemented yet,
or have already been implemented with a perfor-
mance lower than human. The players work sim-
ply by playing the game - the data are generated as
a by-product of the game. The more enjoyable the
game is, the more users play it and the more data
is acquired.
The GWAP methodology was first used for on-
line games with images (van Ahn and Dabbish,
2004) and later with tunes (Law et al, 2007),1
in which the players try to agree on a caption of
the image/tune. The popularity of these games is
enormous and generates a huge amount of data.
Onto games (Siorpaes and Hepp, 2008) brought
another type of input data to GWAP ? video and
text.2
The situation with text is slightly different. One
has to read a text in order to identify its topics.
1www.gwap.org
2www.ontogame.org
Reading texts takes more time than observing im-
ages and the longer text, the worse. Since the
game must be of a dynamic character, it is unimag-
inable that the players would spend minutes read-
ing an input text. Therefore, it must be opened to
the players ?part? by ?part?.
So far, besides the Onto games, two more games
with texts have appeared: What did Shannon
say?3, the goal of which is to help the speech
recognizer with difficult-to-recognize words, and
Phrase Detectives4 (Kruschwitz, Chamberlain,
Poesio, 2009), the goal of which is to identify re-
lationships between words and phrases in a text.
No information about their popularity has been
published yet.
Motivated by the GWAP portal, the LGame por-
tal5 dedicated to language games has been estab-
lished. The LGame portal has been opened with
the Shannon game, a game of intentionally hidden
words in the sentence, where players guess them,
and the Place the Space game, a game of word
segmentation.
2 Coreference
Coreference occurs when several referring expres-
sions in a text refer to the same entity (e.g. per-
son, thing, fact). A coreferential pair is marked
between subsequent pairs of the referring expres-
sions. A sequence of coreferential pairs referring
to the same entity in a text forms a coreference
chain. The coreferential pairs and the coreference
chains cover only the identity relation.
Many projects for various languages on the
coreference annotation by linguists are running.
The annotated data serve as a basis for further
linguistic study of coreference, and most impor-
tantly also to train and test procedures for auto-
matic coreference resolution, which is a task that
3lingo.clsp.jhu.edushannongame.html
4www.phrasedetectives.org
5www.lgame.cz
52
many other applications can benefit from, e.g. text
summarization, question answering, and informa-
tion retrieval.
Manual annotation is costly and time consum-
ing. We propose a design of the PlayCoref game
? to appear at the LGame portal ? as an alternative
way of the coreference annotation collection, and
most importantly, of a substantially larger volume
than any expert annotation can ever achieve.
3 The PlayCoref Game
3.1 Game Design
We prepare the game for Czech and English first.
However, PlayCoref can be played in any lan-
guage.
The game is designed for two players. The
game starts with several first sentences of the doc-
ument displayed in the players? sentence window.
According to the restrictions put on the members
of the coreferential pairs, parts of the text are un-
locked (i.e. they are active) while the other parts
are locked (i.e. they are inactive); both of them
are graphically distinguished. In our case, only
nouns and selected pronouns are unlocked. The
players mark coreferential pairs between the in-
dividual unlocked words in the text (no phrases
are allowed). They mark the coreferential pairs as
undirected links.
During the session, the number of words the
opponent has linked into the coreferential pairs is
displayed to the player. The number of sentences
with at least one coreferential pair marked by the
opponent is displayed to the player as well. Re-
vealing more information about the opponent?s ac-
tions would affect the independency of the play-
ers? decisions.
If the player finishes pairing all the related
words in the visible part of the document (visible
to him), he asks for the next sentence of the docu-
ment. It appears at the bottom of the player?s sen-
tence window. The player can remove pairs cre-
ated before at any time and can make new pairs in
the sentences read so far. The session goes on this
way until the end of the session time. More than
one document can be present in the session.
After the session, the players? scores are calcu-
lated and displayed.
Instructions for the Players Instructions for the
players must be as comprehensible and concise as
possible. To mark a coreferential pair, no linguis-
tic knowledge is required, thus no extensive anno-
tation guidelines need to be formulated. It is all
about the text comprehension ability.
3.2 Game Data
Any textual data can be used in the game, but the
following pre-processing steps are necessary.
Tagging Most importantly, the morphological
tagging (usually preceded by tokenization) is
required to recognize part-of-speech categories
(and sub-part-of-speech categories), in order to
lock/unlock individual words for the game. For
most languages, tagging is a well solved problem
(e.g. for Czech: the MORC?E tagger6, for English:
TnT tagger7).
Text Parts Locking In the game, we work with
coreferential links between the individual words
only. The coreferential pairs that link larger text
parts consisting of clauses or even several sen-
tences are disregarded. Their marking requires lin-
guistic knowledge and extensive training.
Our research shows that pronouns that are usu-
ally members of such ?undesirable? links can
be detected automatically in advance (at least in
Czech). They will get locked, so the players will
not consider them at all during the sessions.
Automatic Coreference Resolution According
to the way we calculate the players scores (see be-
low), an automatic procedure for coreference res-
olution is required. If this procedure works on a
different layer than the surface layer, further auto-
matic processing of the data may be needed.
4 Data Quality
4.1 Players? Score
We want to obtain a large volume of data so we
must first attract the players and motivate them
to play the game more and more. As a reward
for their effort we present scoring. We hope that
the players? appetite to win, to confront with their
opponents and to place well in the long-term top
scores tables correlates with our research aims and
objectives.
Our goal is to ensure the highest quality of the
annotation. The scoring function should reflect
the game data quality and thus motivate the play-
ers to produce the right data. An agreement with
6ufal.mff.cuni.cz/morce
7www.coli.uni-saarland.de/?thorsten/
tnt/
53
the manual expert annotation would be a perfect
scoring function. But the manual annotation is not
available for all languages and above all, it is not
our goal to annotate already annotated data.
An automatic coreference resolution procedure
serves as a first approximation for the scoring
function. Since the procedure does not work for
?100%?, we need to add another component. We
suppose that most of the players will mark the
coreferential pairs reliably. Then an agreement
between the players? pairs indicates correctness,
even if the pair differs from the output of auto-
matic coreference resolution procedure. There-
fore, the inter-player agreement will become the
second component of the scoring function. To mo-
tivate the players to ask for more parts of the text
(and not only ?tune? links in the initially displayed
sentences), the third component of the scoring
function will award number of created coreferen-
tial links.
The players get points for their coreferential
pairs according to the equation ptsA = w1 ?
ICA(A, acr) + w2 ? ICA(A,B) + w3 ? N(A)
whereA andB are the players, acr is an automatic
coreference resolution procedure, ICA stands for
the inter-coder agreement that we can simultane-
ously express either by the F-measure or Krippen-
dorff?s ? (Krippendorf, 2004), N is a contribu-
tion of the number of created links, and weights
0 ? w1, w2 ? 1, w1, w2, w3 ? R (summing to 1)
are set empirically.
The score is calculated at the end of the ses-
sion and no running score is being presented dur-
ing the session. From the scientific point of view,
the scores serve for the long term quality control
of the players? annotation.
4.2 Interactivity Issues
The degree of a player-to-player interactivity con-
tributes to the attractiveness of the game. From the
player?s point of view, the more interactivity, the
better. For example, knowing both his and the op-
ponent?s running score would be very stimulating
for the mutual competitiveness. From the linguis-
tics? point of view, once any kind of interaction is
allowed, statistically pure independency between
the players? decisions is lost. A reasonable trade-
off between the interactivity and the independency
must be achieved. Interactivity that would lead to
cheating and decreasing the quality of the game
data must be avoided.
Allowing the players to see their own running
score would lead to cheating. The players might
adjust their decisions according to the changes in
the score. Another possible extension of interac-
tivity that would lead to cheating is highlighting
words that the opponent used in the coreferential
pairs. The players might then wait for the oppo-
nent?s choice and again, adjust their decisions ac-
cordingly. Such game data would be strongly bi-
ased. However, we still believe that a slight idea of
what the opponent is doing can boost inter-coder
agreement and yet avoid cheating. Revealing the
information about the opponent?s number of pairs
and number of sentences with at least one pair of-
fers not zero but low interactivity, yet it will not
harm the quality of the data.
4.3 Post-Processing
The players mark the coreferential links undi-
rected. This strategy differs from the general con-
ception of coreference being understood as either
the anaphoric or cataphoric relation depending on
the ?direction? of the link in the text. We believe
that the players will benefit from this simplifica-
tion and so will the data quality. After the ses-
sion, the coreference chains are automatically re-
constructed from the coreferential pairs.
4.4 Evaluation
Data with manually annotated coreference will be
used to measure the game data quality. We will
also study how much the scoring function suffers
from the difference between the output of the au-
tomatic coreference resolution procedure and the
manual annotation (gold standard). For Czech, we
will use the data from PDT 2.0, for English from
MUC-6.
PDT 2.0 8 contains the annotation of grammat-
ical and pronominal textual coreference. Nomi-
nal textual coreference is being annotated in PDT
2.0 in an ongoing project (Nedoluzhko, 2007).
Since the PDT 2.0 coreference annotation oper-
ates on the so-called tectogrammatical layer (layer
of meaning) and PlayCoref plays on the surface
layer, the coreferential pairs must be projected to
the surface first. The process consists of several
steps and only a part of the coreferential pairs is
actually projectable to the surface (links between
nodes that have no surface counterpart get lost).
8ufal.mff.cuni.cz/pdt2.0
54
MUC-6 9 operates on the surface layer. This
data can be used in a much more straightforward
way. The coreferential pairs are marked between
nouns, noun phrases, and pronouns and no projec-
tion is needed. The links with noun phrases are
disregarded.
Evaluation Methods For the game data evalu-
ation, well established methods for calculating an
inter-annotator agreement in the coreference anno-
tation will be employed. These methods consider
a coreference chain to be a set of words and they
measure the agreement on the membership of the
individual words in the sets (Passonneau, 2004).
Weighted agreement coefficients such as Krippen-
dorf?s ? (Krippendorf, 2004) need to be used -
sets of words can differ only partially, which does
not mean a total disagreement.
5 Further Work
Acquisition Evaluation Process The quality of
the game annotation undergoes standard evalua-
tion. Apart from collecting, assuming the game
reaches sufficient popularity, long-term monitor-
ing of the players? outputs can bring into question
new issues concerning the game data quality: How
much can we benefit from presenting a document
into more sessions? Should we prefer the output of
more reliable and experienced players during the
evaluation? Should we omit the output of ?not-so-
reliable? players?
Named Entity Recognition The step of the
named entity recognition will be applied in the
subsequent stages of the project. Multi-word ex-
pressions that form a named entity (e.g. ?Czech
National Bank?) will be presented to the players
as a single unit of annotation. We also plan to im-
plement a GWAP for named entity recognition.
6 Conclusion
We have presented the concept of the PlayCoref
game, a proposed language game that brings a
novel approach to collecting coreference annota-
tion of texts using the enormous potential of In-
ternet users. We have described the design of the
game and discussed the issues of interactivity of
the players and measuring the player score ? is-
sues that are crucial both for the attractiveness of
the game and for the quality of the game data. The
9cs.nyu.edu/faculty/grishman/muc6.html
game can be applied on any textual data in any lan-
guage, providing certain basic tools also discussed
in the paper exist. The GWAPs are open-ended
stories so until the game is released, it is hard to
say if the players will find it attractive enough. If
so, we hope to collect a large volume of data with
coreference annotation at extremely low costs.
Acknowledgments
We gratefully acknowledge the support of the
Czech Ministry of Education (grants MSM-
0021620838 and LC536), the Czech Grant
Agency (grant 405/09/0729), and the Grant
Agency of Charles University in Prague (project
GAUK 138309).
References
Klaus Krippendorf. 2004. Content Analysis: An Introduc-
tion to Its Methodology, second edition, chapter 11, Sage,
Thousand Oaks, CA.
Udo Kruschwitz, Jon Chamberlain, Massimo Poesio. 2009.
(Linguistic) Science Through Web Collaboration in the
ANAWIKI project. In Proceedings of the WebSci?09: So-
ciety On-Line, Athens, Greece, in press.
Lucie Kuc?ova?, Eva Hajic?ova?. 2005. Coreferential Relations
in the Prague Dependency Treebank. In Proceedings of
the 5th International Conference on Discourse Anaphora
and Anaphor Resolution, San Miguel, Azores, pp. 97?102.
Edith. L. M. Law et al 2007. Tagatune: A game for music
and sound annotation. In Proceedings of the Music In-
formation Retrieval Conference, Austrian Computer Soc.,
pp. 361?364.
Anna Nedoluzhko. 2007. Zpra?va k anotova?n?? rozs???r?ene?
textove? koreference a bridging vztahu? v Praz?ske?m
za?voslostn??m korpusu (Annotating extended coreference
and bridging relations in PDT). Technical Report, UFAL,
MFF UK, Prague, Czech Republic.
Rebecca J. Passonneau. 2004. Computing Reliability for
Coreference. Proceedings of LREC, vol. 4, pp. 1503?
1506, Lisbon.
Katharina Siorpaes and Martin Hepp. 2008. Games with a
purpose for the Semantic Web. IEEE Intelligent Systems
Vol. 23, number 3, pp. 50?60.
Luis van Ahn and Laura Dabbish. 2004. Labelling images
with a computer game. In Proceedings of the SIGHI Con-
ference on Human Factors in Computing Systems, ACM
Press, New York, pp. 319?326.
Luis van Ahn and Laura Dabbish. 2008. Designing Games
with a Purpose. Communications of the ACM, vol. 51, No.
8, pp. 58?67.
Marc Vilain et al 1995. A Model-Theoretic Coreference
Scoring Scheme. Proceedings of the Sixth Message Un-
derstanding Conference, pp. 45?52, Columbia, MD.
55
