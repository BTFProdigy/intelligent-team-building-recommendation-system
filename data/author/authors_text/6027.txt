First Joint Conference on Lexical and Computational Semantics (*SEM), pages 408?412,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
EMNLP@CPH: Is frequency all there is to simplicity?
Anders Johannsen, H?ctor Mart?nez, Sigrid Klerke?, Anders S?gaard
Centre for Language Technology
University of Copenhagen
{ajohannsen|alonso|soegaard}@hum.ku.dk
sigridklerke@gmail.com?
Abstract
Our system breaks down the problem of rank-
ing a list of lexical substitutions according to
how simple they are in a given context into a
series of pairwise comparisons between can-
didates. For this we learn a binary classifier.
As only very little training data is provided,
we describe a procedure for generating artifi-
cial unlabeled data from Wordnet and a corpus
and approach the classification task as a semi-
supervised machine learning problem. We use
a co-training procedure that lets each classi-
fier increase the other classifier?s training set
with selected instances from an unlabeled data
set. Our features include n-gram probabilities
of candidate and context in a web corpus, dis-
tributional differences of candidate in a cor-
pus of ?easy? sentences and a corpus of normal
sentences, syntactic complexity of documents
that are similar to the given context, candidate
length, and letter-wise recognizability of can-
didate as measured by a trigram character lan-
guage model.
1 Introduction
This paper describes a system for the SemEval 2012
English Lexical Simplification shared task. The
task description uses a loose definition of simplic-
ity, defining ?simple words? as ?words that can be
understood by a wide variety of people, including for
example people with low literacy levels or some cog-
nitive disability, children, and non-native speakers of
English? (Specia et al, 2012).
Feature r
N????sf 0.33
N????sf+1 0.27
N????sf?1 0.27
L??sf -0.26
L??max -0.26
RIproto(l) -0.18
S??cn -0.17
S??w -0.17
S??cp -0.17
Feature r
RIproto(f) -0.15
C???max -0.14
RIorig(l) -0.11
L??tokens -0.10
C???min 0.10
SWfreq 0.08
SWLLR 0.07
C???avg -0.04
Table 1: Pearson?s r correlations. The table shows
the three highest correlated features per group, all of
which are significant at the p < 0.01 level
2 Features
We model simplicity with a range of features divided
into six groups. Five of these groups make use of
the distributional hypothesis and rely on external cor-
pora. We measure a candidate?s distribution in terms
of its lexical associations (RI), participation in syn-
tactic structures (S??), or corpus presence in order to
assess its simplicity (N????, SW, C???). A single
group, L??, measures intrinsic aspects of the substi-
tution candidate, such as its length.
The substitution candidate is either an adjective,
an adverb, a noun, or a verb, and all candidates within
a list share the same part of speech. Because word
class might influence simplicity, we allow our model
to fit parameters specific to the candidate?s part of
speech by making a copy of the features for each part
of speech which is active only when the candidate is
in the given part of speech.
408
Simple Wikipedia (SW) These two features con-
tain relative frequency counts of the substitution
form in Simple English Wikipedia (SWfreq), and the
log likelihood ratio of finding the word in the simple
corpus to finding it in regular Wikipedia (SWLLR)1.
Word length (L??) This set of three features de-
scribes the length of the substitution form in char-
acters (L??sf ), the length of the longest token
(L??max), and the length of the substitution form in
tokens (L??tokens). Word length is an integral part
of common measures of text complexity, e.g in the
English Flesch?Kincaid (Kincaid et al, 1975) in the
form of syllable count, and in the Scandinavian LIX
(Bjornsson, 1983).
Character trigram model (C???) These three
features approximate the reading difficulty of a word
in terms of the probabilities of its forming character
trigrams, with special characters to mark word be-
ginning and end. A word with an unusual combi-
nation of characters takes longer to read and is per-
ceived as less simple (Ehri, 2005).
We calculate the minimum, average, and maxi-
mum trigram probability (C???min, C???avg, and
C???max).2
Web corpus N-gram (N????) These 12 features
were obtained from a pre-built web-scale language
model3. Features of the form N????sf?i, where
0 < i < 4, express the probability of seeing the
substitution form together with the following (or pre-
vious) unigram, bigram, or trigram. N????sf is
the probability of substitution form itself, a feature
which also is the backbone of our frequency base-
line.
Random Indexing (RI) These four features are
obtained from measures taken from a word-to-word
distributional semantic model. Random Indexing
(RI) was chosen for efficiency reasons (Sahlgren,
2005). We include features describing the seman-
tic distances between the candidate and the original
1Wikipedia dump obtained March 27, 2012. Date on the
Simple Wikipedia dump is March 22, 2012.
2Trigram probabilities derived from Google T1 unigram
counts.
3The ?jun09/body? trigram model from Microsoft Web N-
gram Services.
form (RIorig), and between the candidate and a proto-
type vector (RIproto). For the distance between can-
didate and original, we hypothesize that annotators
would prefer a synonym closer to the original form.
A prototype distributional vector of a set of words is
built by summing the individual word vectors, thus
obtaining a representation that approximates the be-
havior of that class overall (Turney and Pantel, 2010).
Longer distances indicate that the currently exam-
ined substitution is far from the shared meaning of
all the synonyms, making it a less likely candidate.
The features are included for both lemma and surface
forms of the words.
Syntactic complexity (S??) These 23 features
measure the syntactic complexity of documents
where the substitution candidate occurs. We used
measures from (Lu, 2010) in which they describe 14
automatic measures of syntactic complexity calcu-
lated from frequency counts of 9 types of syntactic
structures. This group of syntax-metric scores builds
on two ideas.
First, syntactic complexity and word difficulty go
together. A sentence with a complicated syntax is
more likely to be made up of difficult words, and
conversely, the probability that a word in a sentence
is simple goes up when we know that the syntax of
the sentence is uncomplicated. To model this we
search for instances of the substitution candidates in
the UKWAC corpus4 and measure the syntactic com-
plexity of the documents where they occur.
Second, the perceived simplicity of a word may
change depending on the context. Consider the ad-
jective ?frigid?, which may be judged to be sim-
pler than ?gelid? if referring to temperature, but per-
haps less simple than ?ice-cold? when characterizing
someone?s personality. These differences in word
sense are taken into account by measuring the sim-
ilarity between corpus documents and substitution
contexts and use these values to provide a weighted
average of the syntactic complexity measures.
3 Unlabeled data
The unlabeled data set was generated by a three-
step procedure involving synonyms extracted from
Wordnet5 and sentences from the UKWAC corpus.
4http://wacky.sslmit.unibo.it/
5http://wordnet.princeton.edu/
409
1) Collection: Find synsets for unambigious lem-
mas in Wordnet. The synsets must have more than
three synonyms. Search for the lemmas in the cor-
pus. Generate unlabeled instances by replacing the
lemma with each of its synonyms. 2) Sampling: In
the unlabeled corpus, reduce the number of ranking
problems per lemma to a maximum of 10. Sample
from this pool while maintaining a distribution of
part of speech similar to that of the trial and test set.
3) Filtering: Remove instances for which there are
missing values in our features.
The unlabeled part of our final data set contains
n = 1783 problems.
4 Ranking
We are given a number of ranking problems (n =
300 in the trial set and n = 1710 for the test data).
Each of these consists of a text extract with a posi-
tion marked for substitution, and a set of candidate
substitutions.
4.1 Linear order
Let X (i) be the substitution set for the i-th problem.
We can then formalize the ranking problem by as-
suming that we have access to a set of (weighted)
preference judgments, w(a ? b) for all a, b ? X (i)
such that w(a ? b) is the value of ranking item a
ahead of b. The values are the confidence-weighted
pair-wise decisions from our binary classifier. Our
goal is then to establish a total order on X (i) that
maximizes the value of the non-violated judgments.
This is an instance of the Linear Ordering Problem
(Mart? and Reinelt, 2011), which is known to be NP-
hard. However, with problems of our size (maximum
ten items in each ranking), we escape these complex-
ity issues by a very narrow margin?10! ? 3.6 mil-
lion means that the number of possible orderings is
small enough to make it feasible to find the optimal
one by exhaustive enumeration of all possibilities.
4.2 Binary classication
In order to turn our ranking problem into binary clas-
sification, we generate a new data set by enumerat-
ing all point-wise comparisons within a problem and
for each apply a transformation function ?(a,b) =
a ? b. Thus each data point in the new set is the
difference between the feature values of two candi-
dates. This enables us to learn a binary classifier for
the relation ?ranks ahead of?.
We use the trial set for labeled training data L and,
in a transductive manner, treat the test set as unla-
beled data Utest. Further, we supplement the pool of
unlabeled data with artificially generated instances
Ugen, such that U = Utest ? Ugen.
Using a co-training setup (Blum and Mitchell,
1998), we divide our features in two independent sets
and train a large margin classifier6 on each split. The
classifiers then provide labels for data in the unla-
beled set, adding the k most confidently labeled in-
stances to the training data for the other classifier, an
iterative process which continues until there is no un-
labeled data left. At the end of the training we have
two classifiers. The classification result is a mixture-
of-experts: the most confident prediction of the two
classifiers. Furthermore, as an upper-bound of the
co-training procedure, we define an oracle that re-
turns the correct answer whenever it is given by at
least one classifier.
4.3 Ties
In many cases we have items a and b that tie?in
which case both a ? b and b ? a are violated. We
deal with these instances by omitting them from the
training set and setting w(a ? b) = 0. For the fi-
nal ranking, our system makes no attempt to produce
ties.
5 Experiments
In our experiments we vary feature-split, size of un-
labeled data, and number of iterations. The first fea-
ture split, S???SW, pooled all syntactic complexity
features and Wikipedia-based features in one view,
with the remaining feature groups in another view.
Our second feature split, S???C????L??, combined
the syntactic complexity features with the character
trigram language model features and the basic word
length features. Both splits produced a pair of classi-
fiers with similar performance?each had an F-score
of around .73 and an oracle score of .87 on the trial
set on the binary decision problem, and both splits
performed equally on the ranking task.
6Liblinear with L1 penalty and L2 loss. Parameter settings
were default. http://www.csie.ntu.edu.tw/?cjlin/liblinear/
410
System All N V R A
M????????F??? 0.449 0.367 0.456 0.487 0.493
S???SWf 0.377 0.283 0.269 0.271 0.421
S???SWl 0.425 0.355 0.497 0.408 0.425
S???C????L??f 0.377 0.284 0.469 0.270 0.421
S???C????L??l 0.435 0.362 0.481 0.465 0.439
Table 2: Performance on part of speech. Unlabeled
set was Utest. Subscripts tell whether the scores are
from the first or last iteration
With a large unlabeled data set available, the clas-
sifiers can avoid picking and labeling data points
with a low certainty, at least initially. The assump-
tion is that this will give us a higher quality training
set. However, as can be seen in Figure 1, none of our
systems are benefitting from the additional data. In
fact, the systems learn more when the pool of unla-
beled data is restricted to the test set.
Our submitted systems, O??1 and O??2 scored
0.405 and 0.393 on the test set, and 0.494 and 0.500
on the trial set. Following submission we adjusted
a parameter7 and re-ran each split with both U and
Utest.
We analyzed the performance by part of speech
and compared them to the frequency baseline as
shown in Table 2. For the frequency baseline, per-
formance is better on adverbs and adjectives alone,
and somewhat worse on nouns. Both our sys-
tems benefit from co-training on all word classes.
S???C????L??, our best performing system, no-
tably has a score reduction (compared to the base-
line) of only 5% on adverbs, eliminates the score re-
duction on nouns, and effectively beats the baseline
score on verbs with a 6% increase.
6 Discussion
The frequency baseline has proven very strong, and,
as witnessed by the correlations in Table 1, frequency
is by far the most powerful signal for ?simplicity?.
But is that all there is to simplicity? Perhaps it is.
For a person with normal reading ability, a sim-
ple word may be just a word with which the per-
son is well-acquainted?one that he has seen be-
fore enough times to have a good idea about what
it means and in which contexts it is typically used.
7In particular, we selected a larger value for the C parameter
in the liblinear classifier.
0 5000 10000 15000 20000 25000
Unlabeled datapoints
0.38
0.40
0.42
0.44
0.46
0.48
Sc
ore
SYN-SW(Utest)
SYN-CHAR-LEN(Utest)
SYN-CHAR-LEN(U)
Figure 1: Test set kappa score vs. number of data
points labeled during co-training
And so an n-gram model might be a fair approxi-
mation. However, lexical simplicity in English may
still be something very different to readers with low
literacy. For instance, the highly complex letter-to-
sound mapping rules are likely to prevent such read-
ers from arriving at the correct pronunciation of un-
seen words and thus frequent words with exceptional
spelling patterns may not seem simple at all.
A source of misclassifications discovered in our
error analysis is the fact that substituting candidates
into the given contexts in a straight-forward manner
can introduce syntactic errors. Fixing these can re-
quire significant revisions of the sentence, and yet
the substitutions resulting in an ungrammatical sen-
tence are sometimes still preferred to grammatical al-
ternatives.8 Here, scoring the substitution and the
immediate context in a language model is of little
use. Moreover, while these odd grammatical errors
may be preferable to many non-native English speak-
ers with adequate reading skills, such errors can be
more obstructing to reading impaired users and be-
ginning language learners.
Acknowledgments
This research is partially funded by the European Commission?s
7th Framework Program under grant agreement n? 238405
(CLARA).
8For example sentence 1528: ?However, it appears they in-
tend to pull out all stops to get what they want.? Gold: {try ev-
erything} {do everything it takes} {pull} {stop at nothing} {go
to any length} {yank}.
411
References
C. H. Bjornsson. 1983. Readability of Newspa-
pers in 11 Languages. Reading Research Quarterly,
18(4):480?497.
A Blum and T Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learning
theory, pages 92?100. ACM.
Linnea C. Ehri. 2005. Learning to read words: The-
ory, findings, and issues. Scientific Studies of Reading,
9(2):167?188.
J P Kincaid, R P Fishburne, R L Rogers, and B S Chissom.
1975. Derivation of New Readability Formulas (Auto-
mated Readability Index, Fog Count and Flesch Read-
ing Ease Formula) for Navy Enlisted Personnel.
Xiaofei Lu. 2010. Automatic analysis of syntactic com-
plexity in second language writing. International Jour-
nal of Corpus Linguistics, 15(4):474?496.
Rafael Mart? and Gerhard Reinelt. 2011. The Lin-
ear Ordering Problem: Exact and Heuristic Methods
in Combinatorial Optimization (Applied Mathematical
Sciences). Springer.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering,
TKE, volume 5.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplifica-
tion. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
P. D Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
412
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 81?83,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Two baselines for unsupervised dependency parsing?
Anders S?gaard
Center for Language Technology
University of Copenhagen
DK-2300 Copenhagen S
soegaard@hum.ku.dk
Abstract
Results in unsupervised dependency parsing
are typically compared to branching baselines
and the DMV-EM parser of Klein and Man-
ning (2004). State-of-the-art results are now
well beyond these baselines. This paper de-
scribes two simple, heuristic baselines that are
much harder to beat: a simple, heuristic al-
gorithm recently presented in S?gaard (2012)
and a heuristic application of the universal
rules presented in Naseem et al (2010). Our
first baseline (RANK) outperforms existing
baselines, including PR-DVM (Gillenwater et
al., 2010), while relying only on raw text, but
all submitted systems in the Pascal Grammar
Induction Challenge score better. Our second
baseline (RULES), however, outperforms sev-
eral submitted systems.
1 RANK: a simple heuristic baseline
Our first baseline RANK is a simple heuristic base-
line that does not rely on part of speech. It only as-
sumes raw text. The intuition behind it is that a de-
pendency structure encodes something related to the
relatively salience of words in a sentence (S?gaard,
2012). It constructs a word graph of the words in
a sentence and applies a random walk algorithm to
rank the words by salience. The word ranking is
then converted into a dependency tree using a simple
heuristic algorithm.
The graph over the words in the input sentence
is constructed by adding directed edges between the
?
word nodes. The edges are not weighted, but mul-
tiple edges between nodes will make transitions be-
tween them more likely.
The edge template was validated on development
data from the English Penn-III treebank (Marcus et
al., 1993) and first presented in S?gaard (2012):
? Short edges. To favor short dependencies, we add
links between all words and their neighbors. This
makes probability mass flow from central words to their
neighboring words.
? Function words. We use a keyword extraction algorithm
without stop word lists to extract function or non-content
words. The algorithm is a crude simplification of
TextRank (Mihalcea and Tarau, 2004) that does not rely
on linguistic resources, so that we can easily apply it
to low-resource languages. Since we do not use stop
word lists, highly ranked words will typically be function
words. For the 50-most highly ranked words, we add
additional links from their neighboring words. This will
add additional probability mass to the function words.
This is relevant to capture structures such as prepositional
phrases where the function words take content words as
complements.
? Morphological inequality. If two words wi, wj have
different prefixes or suffixes, i.e. the first two or last three
letters, we add an edge between them.
Given the constructed graph we rank the nodes
using the algorithm in Page and Brin (1998), also
known as PageRank. The input to the PageRank al-
gorithm is any directed graph G = ?E,V ? and the
output is an assignment PR : V ? R of a score,
also referred to as PageRank, to each node in the
graph, reflecting the probability of ending up in that
node in a random walk.
81
from/to The finger-pointing has already begun .
The 0 3 2 2 3 2
finger-pointing 3 0 5 2 3 2
has 2 4 0 3 3 2
already 2 2 5 0 3 2
begun 2 3 3 3 0 3
. 2 2 3 2 4 0
PR(%) 13.4 17.4 21.2 15.1 19.3 13.6
Figure 1: Graph, pagerank (PR) and predicted depen-
dency structure for sentence 7 in PTB-III Sect. 23.
The words are now ranked by their PageRank
(Figure 1), and from the word ranking we derive
a dependency tree. The derivation is very simple:
We introduce a store of potential heads, initialized
as a singleton containing the word with the high-
est PageRank (which is attached to the artificial root
note). Each word is now assigned a syntactic head
taken from all the words that were already assigned
heads. Of these words, we simply select the clos-
est possible head. In case of ties, we select the head
with the highest PageRank.
2 RULES: a simple rule-based baseline
Our second baseline is even simpler than our first
one, but makes use of input part of speech. In par-
ticular it builds on the idea that unsupervised pars-
ing can be informed by universal dependency rules
(Naseem et al, 2010). We reformulate the univer-
sal dependency rules used in Naseem et al (2010)
in terms of the universal tags provided in the shared
task (Figure 2), but unlike them, we do not engage
in grammar induction. Instead we simply present a
straight-forward heuristic application of the univer-
sal dependency rules:
RULES finds the head of each word w by finding
the nearest word w? such that POS(w?)?POS(w) is
a universal dependency rule. In case of ties, we se-
lect the left-most head in the candidate set. The head
of the sentence is said to be the left-most verb. Note
that we are not guaranteed to find a head satisfying
a universal dependency rule. In fact when the de-
pendent has part of speech AUX or ?.? we will never
find such a head. If no head is found, we attach the
dependent to the artificial root node.
Note that like RANK, RULES would give us
VERB??VERB NOUN??ADJ
VERB??NOUN NOUN??DET
VERB??ADV NOUN??NOUN
VERB??ADP NOUN??NUM
VERB??CONJ
VERB??DET
VERB??NUM
VERB??ADJ
VERB??X
ADP??NOUN ADJ??ADV
ADP??ADV
Figure 2: Universal dependency rules (Naseem et al,
2010) wrt. universal tags.
RANK RULES DMV win best
Arabic 0.340 0.465 0.274 0.541 0.573
Basque 0.255 0.137 0.321 0.440 0.459
Czech 0.329 0.409 0.276 0.488 0.491
Danish 0.424 0.451 0.395 0.502 0.502
Dutch 0.313 0.405 0.284 0.437 0.492
En-Childes 0.481 0.519 0.498 0.538 0.594
En-WSJ 0.328 0.425 0.335 0.555 0.560
Portuguese 0.371 0.546 0.240 0.418 0.652
Slovene 0.284 0.377 0.242 0.580 0.580
Swedish 0.375 0.551 0.290 0.573 0.573
the correct analysis of the sentence in Figure 1
(excl. punctuation). Surprisingly, RULES turns out
to be a very competitive baseline.
3 Results
Shared task results were evaluated by the organiz-
ers in terms of directed accuracy (DA), also known
as unlabeled attachment score, undirected accuracy
(UA) and NED (Schwartz et al, 2011), both for
short and full length sentences. We will focus on
DA for full length sentences here, arguable the most
widely accepted metric. Table 1 presents results for
all 10 datasets, with DMV based on fine-grained na-
tive POS (which performs best on average compared
to DMV-CPOS and DMV-UPOS),1 and Tu, stan-
dard as the winning system (?win?). The ?best? result
cherry-picks the best system for each dataset.
The first thing we note is that our two baselines
1In a way it would be fairer to exclude native POS and CPOS
information, since native tag sets reflect language-specific syn-
tax. Moreover, the validity of relying on manually labeled input
is questionable.
82
are much better than the usual structural baselines.
The macro-averages for the branching baselines are
0.252 (left) and 0.295 (right), but if we allow our-
selves to cherry-pick the best branching baseline for
each language the macro-average of that baseline
is 0.352. This corresponds to the macro-average
of RANK which is 0.350. The macro-average of
RULES is 0.429.
Interestingly, RANK achieves better full length
sentence DA than at least one of the submitted sys-
tems for each language, except English. The same
holds for full length sentence NED. RULES is an
even stronger baseline.
Most interestingly the two baselines are signifi-
cantly better on average than all the baselines pro-
posed by the organizers, including DMV-EM and
DMV-PR. This is surprising in itself, since our two
baselines are completely heuristic and require no
training. It seems none of the baseline systems nec-
essarily learn anything apart from simple, univer-
sal properties of linguistic trees that we could easily
have spelled out in the first place.
More than half of the submitted systems are worse
than RULES in terms of DA, but three systems also
outperform our baselines by some margin (Bisk,
Blunsom and Tu). Since our baselines are better than
harmonic initialization, the obvious next step would
be to try to initialize EM-based unsupervised parsers
by the structures predicted by our baselines.
References
Jennifer Gillenwater, Kuzman Ganchev, Joao Graca, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity in de-
pendency grammar induction. In ACL.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Rada Mihalcea and Paul Tarau. 2004. Textrank: bringing
order into texts. In EMNLP.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In EMNLP.
Larry Page and Sergey Brin. 1998. The anatomy of a
large-scale hypertextual web search engine. In Inter-
national Web Conference.
Roy Schwartz, , Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing eval-
uation. In ACL.
Anders S?gaard. 2012. Unsupervised dependency pars-
ing without training. Natural Language Engineering,
18(1):187?203.
83
Workshop on Computational Linguistics for Literature, pages 54?58,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Mining wisdom
Anders S?gaard
Center for Language Technology
University of Copenhagen
DK-2300 Copenhagen S
soegaard@hum.ku.dk
Abstract
Simple text classification algorithms perform
remarkably well when used for detecting fa-
mous quotes in literary or philosophical text,
with f-scores approaching 95%. We compare
the task to topic classification, polarity classi-
fication and authorship attribution.
1 Introduction
Mark Twain famously said that ?the difference be-
tween the right word and the almost-right word is
the difference between lightning and a lightning
bug.? Twain?s quote is also about the importance
of quotes. A great quote can come in handy when
you are looking to inspire people, make them laugh
or persuade people to believe in a particular point of
view. Quotes are emblems that serve to remind us of
philosophical or political stand-points, world views,
perspectives that comfort or entertain us. Famous
quotes such as ?Cogito ergo sum? (Descartes) and
?God is dead? (Nietzsche) occur millions of times
on the Internet.
The importance of quotes has motivated publish-
ing houses to create and publish large collections of
quotes. In this process, the editor typically spends
years reading philosophy books, literature, and in-
terviews to find good quotes, but this process is both
expensive and cumbersome. In this paper, we con-
sider the possibility of automatically learning what
is a good quote, and what is not.
1.1 Related work
While there seems to have been no previous work
on identifying quotes, the task is very similar to
widely studied tasks such as topic classification, po-
larity classification, (lexical sample) word sense dis-
ambiguation (WSD) and authorship attribution. In
most of these applications, texts are represented as
bags-of-words, i.e. a text is represented as a vector
x = ?x1, . . . , xN ? where each xi encodes the pres-
ence and possibly the frequency of an n-gram. It is
common to exclude stop words or closed class items
such as pronouns and adpositions from the set of n-
grams when constructing the bags-of-words. Some-
times lemmatization or word clustering is also used
to avoid data sparsity.
Topic classification is the classic problem in text
classification of distinguishing articles on a partic-
ular topic from other articles on other topics, say
sports from international politics and letters to the
editor. Several resources exist for evaluating topic
classifiers such as Reuters 20 Newsgroups. Com-
mon baselines are Naive Bayes, logistic regression,
or SVM classifiers trained on bag-of-words repre-
sentations of n-grams with stop words removed.
While newspaper articles typically consist of tens
or hundreds of sentences, famous quotes typically
consist of one or two sentences, and it is interest-
ing to compare quotation mining to work on apply-
ing topic classification techniques to short texts or
sentences (Cohen et al, 2003; Wang et al, 2005;
Khoo et al, 2006). Cohen et al (2003) and Khoo et
al. (2006) classify sentences in email wrt. their role
in discourse. Khoo et al (2006) argue that extend-
ing a bag-of-words representation with frequency
counts is meaningless in small text and restrict them-
selves to binary representations. They show empir-
ically that excluding stop words and lemmatization
54
both lead to impoverished results. We also observe
that stop words are extremely useful for quotation
mining.
Polarity classification is the task of determining
whether an opinionated text about a particular topic,
say a user review of a product, is positive or neg-
ative. Polarity classification is different from quo-
tation mining in that there is a small set of strong
predictors of polarity (pivot features) (Wang et al,
2005; Blitzer et al, 2007), e.g. the polarity words
listed in subjectivity lexica, including opinionated
adjectives such as good or awful. The meaning of
polarity words is context-sensitive, however, so con-
text is extremely important when modeling polarity.
Some quotes are expressions of opinion, and there
has been some previous research on polarity classifi-
cation in direct quotations (not famous quotes). Bal-
ahur et al (2009) present work on polarity classifica-
tion of newspaper quotations, for example. They use
an SVM classifier on a bag-of-words representation
of direct quotes in the news, but using only words
taken from subjectivity lexica as features. Drury et
al. (2011) present a strategy for polarity classifica-
tion of direct quotations from financial news. They
use a Naive Bayes classifier on a bag-of-words mod-
els of unigrams, but learn group-specific models for
analysts and CEOs.
WSD. The lexical sample task in WSD is the task
of determining the meaning of a specific target word
in context. Mooney (1996) argues that Naive Bayes
classification and perceptron classifiers are particu-
larly fit for lexical sample word sense disambigua-
tion problems, because they combine weighted evi-
dence from all features rather than select a subset of
features for early discrimination. This of course also
holds for logistic regression and SVMs. Whether
a sentence is a good quotation or not also depends
on many aspects of the sentence, and experiments
on held-out data comparing Naive Bayes with deci-
sion tree-based learning algorithms, also mentioned
in Sect. 5, clearly demonstrated that early discrimi-
nation based on single features is a bad idea. In this
respect, quotation mining is more similar to lexical
sample WSD than to topic and polarity classification
where there is a small set of pivot features.
Authorship attribution is the task of determin-
ing which of a given set of authors wrote a particular
text. One of the insights from authorship attribution
Positives
Two lives that once part are as ships that divide.
My appointed work is to awaken the divine nature that is within.
Discussion in America means dissent.
Negatives
The business was finished, and Harriet safe.
But how shall I do? What shall I say?
I am quite determined to refuse him.
Figure 1: Examples.
is that stop words are important when you want to
learn stylistic differences. Stylistic differences can
be identified from the distribution of closed class
words (Arun et al, 2009). As already mentioned,
we observe the same holds for quotation mining.
In conclusion, early-discrimination learning algo-
rithms do not seem motivated for applications such
as mining quotes where pivot features are hard to
choose a priori. Furthermore, we hypothesize that
it is better not to exclude stop words. Quotation
mining can thus in our view be thought of as an ap-
plication that is similar to sentence classification in
that famous quotes are relatively small, and similar
to authorship attribution in that style is an important
predictor of whether a sentence is a famous quote.
2 Data
We obtain the database of famous quotes from a
popular on-line collection of quotes1 and use philo-
sophical and literary text sampled from the Guten-
berg corpus as negative data. In particular we use
the portion of Gutenberg documents that is dis-
tributed in the corpora collection at NLTK.2 This
gives us a total of 44,385 positive data points (fa-
mous quotes) and 247,115 negative data points (or-
dinary sentences). In our experiments we use the
top 4,000 data points in each sample, i.e. a total of
8,000 data points, except for when we derive a learn-
ing curve later on, which uses up to 2? 20, 000 data
points. Some sample data points are presented in
Figure 1.
3 Experiment
Each data point is represented as a binary bag-of-
words - or bag-of-n-grams, really. Our initial hy-
pothesis was to include stop words and keep infor-
1http://quotationsbook.com
2http://nltk.org
55
mation about case (capital letters). Stop words are
extremely important to distinguish between literary
styles, and we speculated that quotes can be dis-
tinguished from ordinary text in part by their style.
We also speculated that there would be a tendency
to capitalize some words in quotes, e.g. ?God?, ?the
Other?, or ?the World?. Finally, we hypothesized that
including more context would be beneficial. Our in-
tuition was that sometimes larger chunks such as ?He
who? may indicate that a sentence is a quote without
the component words being indicative of that in any
way.
To evaluate these hypotheses we considered a lo-
gistic regression classifier over bag-of-word repre-
sentations of the quotes and our neutral sentences.
We used a publicly available implementation3 of
limited memory L-BFGS to find the weights that
maximize the log-likelihood of the training data:
w? = argmax
w
?
i
y(i) log
1
1 + e?w?x
+ (1? y(i))
log
e?w?x
1 + e?w?x
where w ? x is the dot product of weights and bi-
nary features in the usual way. We prefer logistic re-
gression over Naive Bayes, since logistic regression
is more resistant to possible dependencies between
variables. The conditional likelihood maximization
in logistic regression will adjust its parameters to
maximize the fit even when the resulting parameters
are inconsistent with the Naive Bayes assumption.
Finally, logistic regression is less sensitive to param-
eter tuning than SVMs, so to avoid expensive param-
eter optimization we settled for logistic regression.
To test the importance of case, we did experi-
ments with and without lowercasing of all words.
To test the importance of stop words, we did experi-
ments where stop words had been removed from the
texts in advance. We also considered models with
bigrams and trigrams to test the impact of bigger
units of text (context). Finally, we varied the size
of the dataset to obtain a learning curve suggesting
how our model would perform in the limit.
3http://mallet.cs.umass.edu/
1.0 1.5 2.0 2.5 3.0
n-grams (n<=x)
84
86
88
90
92
F
1
 
(
p
o
s
i
t
i
v
e
s
)
logregr
logregr(case)
logregr(nostop)
Figure 2: Results with n-grams of different sizes w/o
lower-casing and w/o stop words.
4 Results
We report f-scores obtained by 10-fold cross-
validation over a balanced 8,000 data points in Fig-
ure 2. The green line is our hypothesis model us-
ing n-grams of up to different lengths (1, 2 and 3).
In this model features are not lower-cased (case is
preserved), and stop words are included. This cor-
responds to our hypotheses about what would work
best for quotation mining. The green line tells us
that our unigram model is considerably better than
our bigram and trigram models. This is probably
because the bigrams and trigrams are too sparsely
distributed in our data selection.
The blue line represents results with lowercased
features. This means that features will be less sparse,
and we now see that the bigram model is slightly
better than the unigram model.
The red line represents results where stop words
have been removed. This would be a typical model
for topic classification. We see that this performs
radically worse than the other two models, suggest-
ing that our hypothesis about the usefulness of stop
words for quotation mining was correct. The obser-
vation that the bigram and trigram models without
stop words are much worse than the unigram model
without stop words is most likely due to the extra
sparsity introduced by open class trigrams.
Our main result is that with sufficient training data
the f-score for detecting famous quotes in philosoph-
ical and literary text approaches 95%. The learning
curves in Figure 3 are the results of our hypothesis
56
Source Quote
Bill Clinton?s Inaugural 1992 Powerful people maneuver for position and worry endlessly about who is in and who is out,
who is up and who is down, forgetting those people whose toil and sweat sends us here and
paves our way.
Bill Clinton?s Inaugural 1997 But let us never forget : The greatest progress we have made, and the greatest progress we
have yet to make, is in the human heart.
PTB CoNLL 2007 test When the dollar is in a free-fall , even central banks can?t stop it .
Europarl 01-17-00 Our citizens can not accept that the European Union takes decisions in a way that is, at least
on the face of it, bureaucratic .
Europarl 01-18-00 If competition policy is to be made subordinate to the aims of social and environmental
policy , real efficiency and economic growth will remain just a dream .
Europarl 01-19-00 For Europe to become the symbol of peace and fraternity , we need a bold and generous
policy to come to the aid of the most disadvantaged .
Figure 4: The sentence with highest probability of being a quote in each corpus according to our 20K logistic regression
unigram model).
0 5000 10000 15000 20000 25000 30000 35000 40000
data points
91.0
91.5
92.0
92.5
93.0
93.5
94.0
94.5
95.0
F
1
 
(
p
o
s
i
t
i
v
e
s
)
unigrams
bigrams
Figure 3: Learning curves for unigram and bigram mod-
els without lower-casing and with stop words.
model (green line in Figure 2) obtained with vary-
ing amounts of training data, from 4,000 to 40,000
data points. The learning curves also confirm that
the bigram model was suffering from sparsity with
smaller data selections, and we observe that the bi-
gram model becomes superior to the unigram model
with about 30,000 data points. The learning curves
show that F-scores for positive class approach 95%
as we add more training data.
5 Discussion
To confirm Mooney?s hypothesis that it is better to
combine weighted evidence from all features rather
than select a subset of features for early discrimi-
nation, also in the case of mining quotes, we ran a
decision tree algorithm on the same data sets used
above. The f-score for detecting quotes was consis-
tently below 65%.
The decision tree algorithm tries to find good fea-
tures for early discrimination. Interestingly, one of
the most discriminative features picked up by the
decision tree from trigram data with case preserved
was the bigram ?He who?. This feature was used
to split 500 sentences, leaving only 11 in the minor-
ity class. Other discriminative features include ?Peo-
ple?, ?we are?, ?if you have?, and ?Nothing is more?.
Similarly, we can observe remarkable differences
in marginal distributions by considering the most
frequent words in positive and negative texts. Words
such as ?who?, ?all?, ?word?, and ?things? occur
much more frequently in quotes than in more bal-
anced literary philosophical text. Interestingly ???
is also a very good predictor of a sentence being a
potential quote.
Finally, we ran a model on other corpora to iden-
tify novel candidates of famous quotes (Figure 4).
We ran it on texts where you would expect to find
potential famous quotes (e.g. inaugurals), as well as
on texts where you would not expect that.
6 Conclusion
Simple text classification algorithms perform re-
markably well when used for detecting famous
quotes in literary or philosophical text, with f-scores
approaching 95%. We compare the task to topic
classification, polarity classification and authorship
attribution and observe that unlike in topic classifi-
cation, stop words are extremely useful for quotation
mining.
57
References
R Arun, R Saradha, V Suresh, M Murty, and C Madha-
van. 2009. Stopwords and stylometry: a latent Dirich-
let alocation approach. In NIPS workshop on Appli-
cations for Topic Models.
Alexandra Balahor, Ralf Steinberger, Erik van der
Goot, Bruno Pouliquen, and Mijail Kabadjov. 2009.
Opinion mining on newspaper quotations. In
IEEE/WIC/ACM Web Intelligence.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
ACL.
William Cohen, Vitor Carvalho, and Tom Mitchell. 2003.
Learning to classify email into ?speech acts?. In
EMNLP.
Brett Drury, Gae?l Dias, and Luis Torgo. 2011. A con-
textual classification strategy for polarity analysis of
direct quotations from financial news. In RANLP.
Anthony Khoo, Yuval Marom, and David Albrecht.
2006. Experiments with sentence classification. In
ALTW.
Raymond Mooney. 1996. Comparative experiments on
disambiguating word senses. In EMNLP.
Chao Wang, Jie Lu, and Guangquan Zhang. 2005. A
semantic classification approach for online product re-
views. In IEEE/WIC/ACM Web Intelligence.
58
