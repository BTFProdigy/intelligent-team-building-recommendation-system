Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 65?75,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Multi-class Animacy Classification with Semantic Features
Johannes Bjerva
Center for Language and Cognition Groningen
University of Groningen
The Netherlands
j.bjerva@rug.nl
Abstract
Animacy is the semantic property of nouns
denoting whether an entity can act, or is
perceived as acting, of its own will. This
property is marked grammatically in var-
ious languages, albeit rarely in English.
It has recently been highlighted as a rele-
vant property for NLP applications such as
parsing and anaphora resolution. In order
for animacy to be used in conjunction with
other semantic features for such applica-
tions, appropriate data is necessary. How-
ever, the few corpora which do contain
animacy annotation, rarely contain much
other semantic information. The addition
of such an annotation layer to a corpus al-
ready containing deep semantic annotation
should therefore be of particular interest.
The work presented in this paper contains
three main contributions. Firstly, we im-
prove upon the state of the art in multi-
class animacy classification. Secondly, we
use this classifier to contribute to the anno-
tation of an openly available corpus con-
taining deep semantic annotation. Finally,
we provide source code, as well as trained
models and scripts needed to reproduce
the results presented in this paper, or aid
in annotation of other texts.
1
1 Introduction
Animacy is the semantic property of nouns de-
noting whether, or to what extent, the referent
of that noun is alive, human-like or even cogni-
tively sophisticated. Several ways of characteris-
ing the animacy of such referents have been pro-
posed in the literature, the most basic distinction
being between animate and inanimate entities. In
1
https://github.com/bjerva/animacy
such a binary scheme, examples of animate nouns
might include author and dog, while examples
of inanimate nouns might include table and rock.
More elaborate schemes tend to represent a hier-
archy or continuum typically ranging from HU-
MAN ? NON-HUMAN ? INANIMATE (cf. Com-
rie (1989)), with other categories in between.
In various languages, animacy affects linguis-
tic phenomena such as case marking and argument
realization. Furthermore, hierarchical restrictions
are often imposed by animacy, e.g. with subjects
tending to be higher in an animacy hierarchy than
objects (Dahl and Fraurud, 1996). Even though
animacy is rarely overtly marked in English, it still
influences the choice of certain grammatical struc-
tures, such as the choice of relative pronouns (e.g.
who vs. which).
The aims of this work are as follows: (i) to im-
prove upon the state of the art in multi-class an-
imacy classification by comparing and evaluating
different classifiers and features for this task, (ii) to
investigate whether a corpus of spoken language
containing animacy annotation can be used as a
basis to annotate animacy in a corpus of written
language, (iii) to use the resulting classifier as part
of the toolchain used to annotate a corpus contain-
ing deep semantic annotation.
The remainder of this paper is organized as fol-
lows: In Section 2 we go through the relevance of
animacy for Natural Language Processing (NLP)
and describe some corpora which contain animacy
annotation. Previous attempts and approaches to
animacy classification are portrayed in Section 3.
Section 4 contains an overview of the data used
in this study, as well as details regarding the man-
ual annotation of animacy carried out as part of
this work. The methods employed and the results
obtained are presented in Sections 5 and 6. The
discussion is given in Section 7. Finally, Section 8
contains conclusions and some suggestions for fu-
ture work in multi-class animacy classification.
65
2 Background
2.1 Relevance of animacy for NLP
Although seemingly overlooked in the past, ani-
macy has recently been shown to be an impor-
tant feature for NLP. ?vrelid & Nivre (2007)
found that the accuracy of a dependency parser for
Swedish could be improved by incorporating a bi-
nary animacy distinction. Other work has high-
lighted animacy as relevant for anaphora and co-
reference resolution (Or?asan and Evans, 2007; Lee
et al., 2013) and verb argument disambiguation
(Dell?Orletta et al., 2005).
Furthermore, in English, the choices for dative
alternation (Bresnan et al., 2007), between geni-
tive constructions (Stefanowitsch, 2003), and be-
tween active and passive voice (Rosenbach, 2008)
are also affected by the animacy of their con-
stituent nouns. With this in mind, Zaenen et al.
(2004) suggest that animacy, for languages such
as English, is not a matter of grammatical and un-
grammatical sentences, but rather of sentences be-
ing more and less felicitous. This highlights anno-
tation of animacy as potentially particularly useful
for applications such as Natural Language Gener-
ation.
In spite of this, animacy appears to be rarely an-
notated in corpora, and thus also rather rarely used
in tools and algorithms for NLP (although some
recent efforts do exist, cf. Moore et al. (2013)).
Furthermore, the few corpora that do include ani-
macy in their annotation do not contain much other
semantic annotation, making them less interesting
for computational semanticists.
2.2 Annotation of animacy
Resources annotated with animacy are few and
far between. One such resource is the MC160
dataset which has recently been labelled for bi-
nary animacy (Moore et al., 2013). The distinc-
tion between animate and inanimate was based on
whether or not an entity could ?move under its
own will?. Although interesting, the size of this
data set (approximately 8,000 annotated nouns)
limits its usefulness, particularly with the methods
used in this paper.
Talbanken05 is a corpus of Swedish spoken lan-
guage which includes a type of animacy annota-
tion (Nivre et al., 2006). However, this annotation
is better described as a distinction between human
and non-human, than between animate and inani-
mate (?vrelid, 2009). Although the work in this
paper focusses on English, a potential application
of this corpus is discussed at the end of this paper
(see Section 8).
The NXT Switchboard corpus represents a
larger and more interesting resource for our pur-
poses (Calhoun et al., 2010). This spoken lan-
guage corpus contains high quality manual anno-
tation of animacy for nearly 200,000 noun phrases
(Zaenen et al., 2004). Furthermore, the annota-
tion is fairly fine-grained, as a total of ten animacy
categories are used (see Table 1), with a few addi-
tional tags for mixed animacy and cases in which
annotators were uncertain. This scheme can be
arranged hierarchically, so that the classes Con-
crete, Non-concrete, Place and Time are grouped
as inanimate, while the remaining classes are
grouped as animate. The availability of this data
allows us to easily exploit the annotation for a su-
pervised learning approach (see Section 5).
3 Related work
In this section we will give an overview of previ-
ous work in animacy classification, some of which
has inspired the approach presented in this paper.
3.1 Exploiting corpus frequencies
A binary animacy classifier which uses syntactic
and morphological features has been previously
developed for Norwegian and Swedish (?vrelid,
2005; ?vrelid, 2006; ?vrelid, 2009). The fea-
tures used are based on frequency counts from the
dependency-parsed Talbanken05 corpus. These
frequencies are counted per noun lemma, mean-
ing that this classifier is not context sensitive. In
other words, cases of e.g. polysemy where head is
inanimate in the sense of human head, but animate
in the sense of head of an organization, are likely
to be problematic. Intuitively, by taking context or
semantically motivated features into account, such
cases ought to be resolved quite trivially.
This classifier performs well, as it reaches an
accuracy for 96.8% for nouns, as compared to a
baseline of 90.5% when always picking the most
common class (?vrelid, 2009). Furthermore, it is
shown that including the binary distinction from
this classifier as a feature in dependency parsing
can significantly improve its labelled attachment
score (?vrelid and Nivre, 2007).
A more language-specific system for animacy
classification has also been developed for Japanese
(Baker and Brew, 2010). In this work, vari-
66
Table 1: Overview of the animacy tag set from Zaenen et al. (2004) with examples from the GMB.
Tag Description Examples
HUM Human Mr. Calderon said Mexico has become a worldwide leader ...
ORG Organization Mr. Calderon said Mexico has become a worldwide leader ...
ANI Animal There are only about 1,600 pandas still living in the wild in China.
LOC Place There are only about 1,600 pandas still living in the wild in China.
NCN Non-concrete There are only about 1,600 pandas still living in the wild in China.
CNC Concrete The wind blew so much dust around the field today.
TIM Time The wind blew so much dust around the field today.
MAC Machine The astronauts attached the robot, called Dextre, to the ...
VEH Vehicle Troops fired on the two civilians riding a motorcycle ...
ous language-specific heuristics are used to im-
prove coverage of, e.g., loanwords from English.
The features used are mainly frequency counts of
nouns as subjects or objects of certain verbs. This
is then fed to a Bayesian classifier, which yields
quite good results on both Japanese and English.
Taking these works into account, it is clear that
the use of morphosyntactic features can provide
relevant information for the task of animacy clas-
sification. However, both of these approaches use
binary classification schemes. It is therefore not
clear whether acceptably good results could be ob-
tained for more elaborate schemes.
3.2 Exploiting lexico-semantic resources
Or?asan & Evans (2007) present an animacy classi-
fier which is based on knowledge obtained from
WordNet (Miller, 1995). In one approach, they
base this on the so-called unique beginners at the
top of the WordNet hierarchy. The fact that some
of these are closely related to animacy is then used
to infer the animacy of their hyponyms. The inclu-
sion of the classifications obtained by this system
for the task of anaphora resolution is shown to im-
prove its results.
An animacy classifier based on exploiting syn-
onymy relations in addition to hyponymy and hy-
peronymy has been described for Basque (de Il-
laraza et al., 2002). In this work, a small set con-
sisting of 100 nouns was manually annotated. Us-
ing an electronic dictionary from which semantic
relations could be inferred, they then further auto-
matically annotated all common nouns in a 1 mil-
lion word corpus.
An approach to animacy classification for
Dutch is presented in Bloem & Bouma (to ap-
pear). This approach exploits a lexical semantic
resource, from which word-senses were obtained
and merged per lemma. This is done, as they pos-
tulate that ambiguity in animacy per lemma ought
to be relatively rare. Each lemma was then as-
signed a simplified animacy class depending on
its animacy category ? either human, non-human
or inanimate. Similarly to Baker & Brew (2010),
they also use dependency features obtained from
an automatically parsed corpus for Dutch. This
type-based approach obtains accuracies in the low
90% range, compared to a most frequent class
baseline of about 81%.
Based on the three aforementioned works, it is
clear that the use of semantic relations obtained
from lexico-semantic resources such as WordNet
are particularly informative for the classification
of animacy.
3.3 Multi-class animacy classification
An animacy classifier which distinguishes be-
tween ten different classes of animacy has been
developed by Bowman & Chopra (2012). They
use a simple logistic regression classifier and
quite straight-forward bag-of-words and PoS fea-
tures, as well as subject, object and PP dependen-
cies. These are obtained from the aforementioned
Switchboard corpus, for which they obtain quite
good results.
A quite involved system for animacy classifi-
cation based on using an ensemble of voters is
presented by Moore et al. (2013). This system
draws its strengths from the fact that it, rather
than defining and using a large number of features
and training one complex classifier, uses more in-
terpretable voting models which differ depending
on the class in question. They distinguish be-
tween three categories, namely person, animal and
67
inanimate. The voters comprise a variety of sys-
tems, based on the n-gram list method of Ji and
Lin (2009), a WordNet-based approach similar to
Or?asan & Evans (2007), and several others. Their
results yield animacy detection rates in the mid-
90% range, and can therefore be seen as an im-
provement upon the state of the art. However,
comparison between animacy classification sys-
tems is not all that straight-forward, considering
the disparity between the data sets and classifica-
tion schemes used.
These two works show that multi-class animacy
classification can be successfully done both with
syntactic and semantic features.
4 Data
Two annotated corpora are used in this work. A
further data source is concreteness ratings ob-
tained through manual annotation (Brysbaert et
al., 2013), and is used as a feature in the classifier.
These ratings were obtained for approximately
40,000 English words and two-word expressions,
through the use of internet crowd-sourcing. The
rating was given on a five-point scale, ranging
from abstract, or language based, to concrete, or
experience based (Brysbaert et al., 2013).
4.1 The NXT Switchboard Corpus
Firstly, the classifier is trained and evaluated on the
Switchboard corpus, as this allows for direct com-
parison of results to at least one previous approach
(i.e. Bowman & Chopra (2012)).
4.1.1 Pre-processing of spoken data
The fact that the Switchboard corpus consists of
transcribed spoken data presents challenges for
some of the tools used in the feature extraction
process. The primary concern identified, apart
from the differing form of spoken language as
compared to written language, is the presence of
disfluency markers in the transcribed texts. As a
preprocessing step, all disfluencies were removed
using a simple automated script. Essentially, this
consisted of removing all words tagged as interjec-
tions (labelled with the tag UH), as this is the tag
assigned to disfluencies in the Switchboard cor-
pus. Although interjections generally can be in-
formative, the occurrences of interjections within
NPs was restricted to usage as disfluencies.
4.2 The Groningen Meaning Bank
There are several corpora of reasonable size which
include semantic annotation on some level, such as
PropBank (Palmer et al., 2005), FrameNet (Baker
et al., 1998), and the Penn Discourse TreeBank
(Prasad et al., 2005). The combination of sev-
eral levels of semantic annotation into one formal-
ism are not common, however. Although some ef-
forts exist, they tend to lack a level of formally
grounded ?deep? semantic representation which
combines these layers.
The Groningen Meaning Bank (GMB) contains
a substantial collection of English texts with such
deep semantic annotation (Basile et al., 2012a).
One of its goals is to combine semantic phenom-
ena into a single formalism, as opposed to deal-
ing with single phenomena in isolation. This pro-
vides a better handle on explaining dependencies
between various ambiguous linguistic phenomena.
Manually annotating a comprehensive corpus
with gold-standard semantic representations is ob-
viously a hard and time-consuming task. There-
fore, a sophisticated bootstrapping approach is
used. Existing NLP tools are used to get a rea-
sonable approximation of the target annotations
to start with. Pieces of information coming from
both experts (linguists) and crowd sourcing meth-
ods are then added in to improve the annotation.
The addition of animacy annotation is done in the
same manner. First, the animacy classifier will
be incorporated into this toolchain. We then cor-
rect the tags for a subset of the corpus, which
is also used to evaluate the classifier. Note that
the classifier used in the toolchain uses a different
model from the conditions where we evaluate on
the Switchboard corpus. For the GMB, we include
training data obtained through the crowd-sourcing
game Wordrobe, which uses a subset of the data
from the GMB (Venhuizen et al., 2013).
4.2.1 Annotation
So as to allow for evaluation of the classifier on
a widely used semantically annotated corpus, one
part (p00) of the GMB was semi-manually anno-
tated for animacy, although this might lead to a
bias with potentially overly good results for our
classifier, if annotators are affected by its out-
put. We use the tagset presented by Zaenen et
al. (2004), which is given in Table 1. This tagset
was chosen for the addition of animacy annota-
tion to the GMB. Including this level of annotation
68
Figure 1: A tagged document in the GMB.
in a resource which already contains other seman-
tic annotation should prove particularly useful, as
this allows animacy to be used in conjunction with
other semantically based features in NLP tools and
algorithms. This annotation was done using the
GMB?s interface for expert annotation (Basile et
al., 2012b). A total of 102 documents, contain-
ing approximately 15,000 tokens, were annotated
by an expert annotator, who corrected the tags as-
signed by the classifier. We assign animacy tags
to all nouns and pronouns. Similarly to our tag-
ging convention for named entities, we assign the
same tag to the whole NP, so that wagon driver
is tagged with HUM, although wagon in isolation
would be tagged with CNC. This has the added
advantage that this is the manner in which NPs are
annotated in the Switchboard corpus, making eval-
uation and comparison with Bowman & Chopra
(2012) somewhat more straight-forward. An ex-
ample of a tagged document can be seen in Fig-
ure 1. Table 2 shows the amount of annotated
nouns per class. In order to verify the integrity of
this annotation, two other experts annotated a ran-
dom selection of ten documents. Inter-annotator
agreement was calculated using Fleiss? kappa on
this selection, yielding a score of ? = .596.
Table 2: Annotation statistics for p00 of the GMB
HUM NCN CNC TIM ORG LOC ANI VEH MAC
1436 2077 79 500 887 512 67 28 0
5 Method
5.1 Classifiers
We experiment using four different classifiers (see
Table 3). All classifiers used are obtained from
the implementations provided by SciKit-learn (Pe-
dregosa et al., 2011). For each type of classifier,
we train one classifier for each class in a one-
versus-all fashion. For source code, trained mod-
els and scripts to run the experiments in this paper,
please see https://github.com/bjerva/
animacy.
The classifiers are trained on a combination of
the Switchboard corpus and data gathered from
Wordrobe, depending on the experimental condi-
tion. In addition to the features explained below,
the classifier exploits named entity tags, in that
these override the proposed animacy tag where ap-
plicable. That is to say, if a named entity has al-
ready been identified and tagged as, e.g., a person,
this is reflected in the animacy layer with the HUM
tag.
Considering that the balance between samples
per class is quite skewed, an attempt was made at
placing lower weights on the samples from the ma-
jority classes. Although this did lead to a marginal
increase in accuracy for the minority classes, over-
all accuracy dropped to such an extent that this
weighting was not used for the results presented
in this work.
5.2 Features
In this section, an overview of the features used by
the classifiers is given.
5.2.1 Bag-of-words feature
The simplest feature used consists of looking at
each lemma in the NP to be classified, and their
corresponding PoS tags. We also experimented
with using whole sentences as context for classi-
fication, but as this worsened results on our devel-
opment data, it was not used for the evaluations
later in the paper.
5.2.2 Concreteness ratings
Considering that two of the categories in our
tag set discriminate between concrete and non-
concrete entities, we include concreteness ratings
69
Table 3: Overview of the classifiers used in the experiments.
Classifier Reference Parameter settings
Logistic Regression (MaxEnt) (Berger et al., 1996) `2 regularization
Support Vector Machine (SVM) (Joachims, 1998) linear kernel
Stochastic Gradient Descent (SGD) (Tsuruoka et al., 2009) `2 regularization, hinge loss
Bernoulli Naive Bayes (B-NB) (McCallum et al., 1998) ?
as a feature in the classifier (Brysbaert et al.,
2013). In its original form, these ratings are quite
fine-grained as they are provided with the average
concreteness score given by annotators on a scale.
We experimented with using different granulari-
ties of these scores as a feature. A simple binary
distinction where anything with a score of c > 2.5
being represented as concrete, and c ? 2.5 be-
ing represented as non-concrete yielded the best
results, and is used in the evaluations in this paper.
5.2.3 WordNet distances
We also include a feature based on WordNet dis-
tances. In this work, we use the path distance sim-
ilarity measure provided in NLTK (Bird, 2006).
In essence, this measure provides a score based
on the shortest path that connects the senses in a
hypernym/hyponym taxonomy. First, we calcu-
late the distance to each hypernym of every given
word. These distances are then summed together
for each animacy class. Taking the most fre-
quent hypernym for each animacy class gives us
the following hypernyms: person.n.01, abstrac-
tion.n.06, city.n.01, time period.n.01, car.n.01, or-
ganization.n.01, artifact.n.01, animal.n.01, ma-
chine.n.01, buddy.n.01. The classifier then uses
whichever of these words is closest as its Word-
Net feature.
5.2.4 Thematic roles
The use of thematic roles for animacy annotation
constitutes a novel contribution from this work.
Intuitively this makes sense, as e.g. agents tend to
be animate. Although the GMB contains an anno-
tation layer with thematic roles, the Switchboard
corpus does not. In order to use this feature, we
therefore preprocessed the latter using Boxer (Bos,
2008). We use the protoroles obtained from Boxer,
namely agent, theme and patient. Although auto-
matic annotation does not provide 100% accuracy,
especially on such a particular data set, this feature
proved somewhat useful (see Section 6.1.2).
6 Results
6.1 Evaluation on the Switchboard corpus
We employ 10-fold cross validation for the evalua-
tions on the Switchboard corpus. All NPs were au-
tomatically extracted from the pre-processed cor-
pus, put into random order and divided into ten
equally-sized folds. In each of the ten cross valida-
tion iterations, one of these folds was left out and
used for evaluation. For the sake of conciseness,
averaged results over all classes are given in the
comparisons of Section 6.1.1 and Section 6.1.2,
whereas detailed results are only given for the best
performing classifier. Note that the training data
from Wordrobe is not used for the evaluations on
the Switchboard corpus, as this would prohibit fair
evaluation with previous work.
6.1.1 Classifier evaluation
We first ran experiments to evaluate which of the
classifiers performed the best on this task. Figure 2
shows the average accuracy for each classifier, us-
ing 10-fold cross validation on the Switchboard
corpus. Table 4 contains the per-class results from
the cross validation performed with the best per-
forming classifier, namely the Logistic Regression
classifier. The remaining evaluations in this pa-
per are all carried out with this classifier. Aver-
age accuracy over the 10 folds was 85.8%. This
is well above the baseline of always picking the
most common class (HUM), which results in an ac-
curacy of 45.3%. More interestingly, this is some-
what higher than the best results for this dataset
reported in the literature (84.9% without cross val-
idation (Bowman and Chopra, 2012)).
6.1.2 Feature evaluation
Using the best performing classifier, we ran exper-
iments to evaluate how different features affect the
results. These experiments were also performed
using 10-fold cross validation on the Switchboard
corpus. Table 5 shows scores from using only one
70
MaxEnt
SGD SVM
B-NB
40
50
60
70
80
90
85.8
82
84.2
80.8
Figure 2: Accuracy of the classifiers, using 10-
fold cross validation on the Switchboard corpus.
The dashed line represents the most frequent class
baseline.
feature in addition to the lemma and PoS of the
head of the NP to be classified. Although none of
the features in isolation add much to the perfor-
mance of the classifier, some marginal gains can
be observed.
Table 5: Comparison of the effect of including sin-
gle features, from cross validation on the Switch-
board corpus. All conditions consist of the fea-
ture named in the condition column in addition to
Lemma+PoS.
Condition Precision Recall F-score
Lemma+PoS 0.846 0.850 0.848
Bag of Words 0.851 0.856 0.853
Concreteness 0.847 0.851 0.849
WordNet 0.849 0.855 0.852
Thematic Roles 0.847 0.851 0.849
All features 0.851 0.857 0.854
6.1.3 Performance on unknown words
For a task such as animacy classification, where
many words can be reliably classified based solely
on their lemma and PoS tag, it is particularly in-
teresting to investigate performance on unknown
words. As in all other conditions, this was evalu-
ated using 10-fold cross validation on the Switch-
board corpus. It should come as no surprise that
the results are substantially below those for known
words, for every single class. The average accu-
racy for this condition was 59.2%, which can be
compared to the most frequent class (NCN) base-
line at 43.0%.
6.2 Evaluation on the GMB
Since one of the purposes of the development of
this classifier was to include it in the tools used in
the tagging of the GMB, we also present the first
results in the literature for the animacy annotation
of this corpus. Due to the limited size of the por-
tion of this corpus for which animacy tags have
been manually corrected, no cross-validation was
performed. However, due to the high differences
in the training data from the Switchboard corpus,
and the evaluation data in the GMB, the results
could be seen as a lower bound for this classifier
on this data set. Table 4 contains the results from
this evaluation. The accuracy on this dataset was
79.4%, which can be compared to a most frequent
class baseline of 37.2%.
6.3 Excluding pronouns
The discrepancy between the results obtained
from the Switchboard corpus and the GMB does
call for some investigation. Considering that the
Switchboard corpus consists of spoken language,
it contains a relatively large amount of personal
pronouns compared to, e.g., news text. Taking into
account that these pronouns are rarely ambiguous
as far as animacy is concerned, it seems feasible
that this may be why the results for the Switch-
board corpus are better than those of the GMB.
To evaluate this, a separate experiment was run
in which all pronouns were excluded. As a large
amount of pronouns are tagged as HUM, the F-
scores for this class dropped by 8% and 5% for
the Switchboard corpus and GMB respectively.
For the GMB, results for other classes remained
fairly stable, most likely due to there not being
many pronouns present which affect the remain-
ing classes. For the Switchboard corpus, however,
an increase in F-score was observed for several
classes. This might be explained by that the ex-
clusion of pronouns lowered the classifier?s pre-
existing bias for the HUM class, as the number
of annotated examples was lowered from approxi-
mately 85,000 to 15,000.
Animacy classification of pronouns can be con-
sidered trivial, as there is little or no ambiguity of
that the referent of e.g. he is HUM. Even so, pro-
nouns were included in the main results provided
71
Table 4: Results from 10-fold cross validation on the Switchboard corpus and evaluation on the GMB.
Switchboard GMB
Class Count Precision Recall F-score Count Precision Recall F-score
HUM 82596 0.91 0.97 0.94 1436 0.82 0.79 0.80
NCN 62740 0.82 0.94 0.88 2077 0.76 0.88 0.82
CNC 12425 0.75 0.43 0.55 79 0.48 0.13 0.20
TIM 7179 0.88 0.85 0.87 500 0.77 0.95 0.85
ORG 6847 0.71 0.26 0.38 887 0.85 0.68 0.75
LOC 5592 0.71 0.66 0.69 512 0.89 0.71 0.79
ANI 2362 0.89 0.36 0.51 67 0.63 0.22 0.33
VEH 1840 0.89 0.45 0.59 28 1.00 0.39 0.56
MAC 694 0.80 0.34 0.47 - - - -
MIX 34 0.00 0.00 0.00 - - - -
here, as this is the standard manner of reporting
results in prior work.
6.4 Summary of results
Table 6 contains a brief overview of the most es-
sential results from this work. For the Switchboard
corpus, this constitutes the current best results in
the literature. As for the GMB, this constitutes the
first results in the literature for animacy classifica-
tion.
Table 6: Main results from all conditions. B&C
(2012) refers to Bowman & Chopra (2012).
Corpus Condition Accuracy
Switchboard
B&C (2012) 0.849
Unknown words 0.592
Known words 0.860
All words 0.858
GMB
Unknown words 0.764
Known words 0.831
All words 0.794
7 Discussion
The work presented in this paper constitutes a mi-
nor improvement to the previously best results for
multi-class animacy classification on the Switch-
board corpus (Bowman and Chopra, 2012). Ad-
ditionally, we also present the first results in the
literature for animacy classification on the GMB,
allowing for future research to use this work as a
point of comparison. It is, however, important to
note that the results obtained for the GMB in this
paper are prone to bias, as the annotation proce-
dure was done in a semi-automatic fashion. If an-
notators were affected by the output of the clas-
sifier, this is likely to have improved the results
presented here.
A striking factor when observing the results, is
the high discrepancy in performance between the
GMB and the Switchboard corpus. This is, how-
ever, not all that surprising. Considering that the
Switchboard corpus consists of spoken language,
and the GMB contains written language, one can
easily draw the conclusion that the domain dif-
ferences pose a substantial obstacle. This can,
for instance, be seen in the differing vocabulary.
In the cross-validation conditions for the Switch-
board corpus, approximately 1% of the words to
be classified in each fold are unknown to the clas-
sifier. As for the GMB, approximately 10% of
the words are unknown. As mentioned in Sec-
tion 6.1.2, the lemma of the head noun in an NP
is a very strong feature, which naturally can not be
used in the case of unknown words. As seen in Ta-
ble 6, performance on known words in the GMB
is not far away from that of known words in the
Switchboard corpus.
Although a fairly good selection of classifiers
were tested in this work, there is room for im-
provement in this area. The fact that the Logistic
Regression classifier outperformed all other clas-
sifiers is likely to have been caused by that not
enough effort was put into parameter selection for
the other classifiers. More sophisticated classi-
fiers, such as Artificial Neural Networks, ought to
72
at the very least replicate the results achieved here.
Quite likely, results should even improve, seeing
that the added computational power of ANNs al-
lows us to capture more interesting/deeper statisti-
cal patterns, if they exist in the data.
The features used in this paper mainly revolved
around semantically oriented ones, such as seman-
tic relations from WordNet, thematic roles and, ar-
guably, concreteness ratings. Better results could
most likely be achieved if one also incorporated
more syntactically oriented features, such as fre-
quency counts from a dependency parsed corpus,
as done by e.g. Bowman & Chopra (2012) and
?vrelid (2009). Other options include the use of
more linguistically motivated features, such as ex-
ploiting relative pronouns (i.e. who vs. which).
8 Conclusions and future work
At the beginning of this paper, we set out three
aims. Firstly, we wanted to improve upon the
state of the art in multi-class animacy classifica-
tion. A conclusive statement to that effect is hard
to make, considering that comparison was only
made directly to one previous work. However, as
our performance compared to this work was some-
what higher, this work certainly marks some sort
of improvement. Secondly, we aimed at investi-
gating whether a corpus of spoken language con-
taining animacy annotation could be used to anno-
tate a corpus of written language. As our results
for the GMB are well above the baseline, we con-
clude that this is indeed feasible, in spite of the
disparities between language form and vocabulary.
Lastly, we aimed at using the resulting classifier as
a part of the toolchain used to annotate the GMB.
This goal has also been met.
As for future work, the fact that animacy is
marked explicitly in many languages presents a
golden opportunity to alleviate the annotation of
this semantic property for languages in which it
is not explicitly marked. By identifying these
markers, the annotation of animacy in such a lan-
guage should be relatively trivial through the use
of parallel texts. Alternatively, one could look
at using existing annotated corpora, such as Tal-
banken05 (Nivre et al., 2006), as a source of an-
notation. One could then look at transferring this
annotation to a second language. Although intu-
itively promising, this approach has some poten-
tial issues, as animacy is not represented univer-
sally across languages. For instance, fluid contain-
ers (e.g. cups, spoons) represent a class of nouns
which are considered grammatically animate in
Algonquian (Quinn, 2001). Annotating such items
as animate in English would most likely not be
considered correct, neither by native speakers nor
by most experts. Nevertheless, if a sufficiently
large amount of languages have some manner of
consensus as to where a given entity is in an ani-
macy hierarchy, this problem ought to be solvable
by simply hand-picking such languages.
73
References
Kirk Baker and Chris Brew. 2010. Multilingual an-
imacy classification by sparse logistic regression.
OSUWPL, 59:52?75.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In 36th An-
nual Meeting of the Association for Computational
Linguistics and 17th International Conference on
Computational Linguistics. Proceedings of the Con-
ference, pages 86?90, Universit?e de Montr?eal, Mon-
treal, Quebec, Canada.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012a. Developing a large semantically
annotated corpus. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istan-
bul, Turkey.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012b. A platform for collaborative se-
mantic annotation. In Proceedings of the Demon-
strations at the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 92?96, Avignon, France.
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Steven Bird. 2006. NLTK: the natural language
toolkit. In Proceedings of the COLING/ACL on In-
teractive presentation sessions, pages 69?72. Asso-
ciation for Computational Linguistics.
Jelke Bloem and Gosse Bouma. to appear. Automatic
animacy classification for dutch. Computational
Linguistics in the Netherlands Journal, 3, 12/2013.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Samuel R Bowman and Harshit Chopra. 2012. Au-
tomatic animacy classification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies: Student Research
Workshop, pages 7?10. Association for Computa-
tional Linguistics.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, R Harald
Baayen, et al. 2007. Predicting the dative alterna-
tion. Cognitive foundations of interpretation, pages
69?94.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known english word lemmas. Behav-
ior Research Methods, pages 1?8.
Sasha Calhoun, Jean Carletta, Jason M Brenier, Neil
Mayo, Dan Jurafsky, Mark Steedman, and David
Beaver. 2010. The nxt-format switchboard corpus:
a rich resource for investigating the syntax, seman-
tics, pragmatics and prosody of dialogue. Language
Resources and Evaluation, 44(4):387?419.
Bernard Comrie. 1989. Language universals and lin-
guistic typology: Syntax and morphology. Univer-
sity of Chicago press.
?
Osten Dahl and Kari Fraurud. 1996. Animacy in gram-
mar and discourse. PRAGMATICS AND BEYOND
NEW SERIES, pages 47?64.
Arantza D??az de Illaraza, Aingeru Mayor, and Kepa
Sarasola. 2002. Semiautomatic labelling of seman-
tic features. In Proceedings of the 19th International
Conference on Computational Linguistics.
Felice Dell?Orletta, Alessandro Lenci, Simonetta Mon-
temagni, and Vito Pirrelli. 2005. Climbing the path
to grammar: A maximum entropy model of sub-
ject/object learning. In Proceedings of the Work-
shop on Psychocomputational Models of Human
Language Acquisition, pages 72?81. Association for
Computational Linguistics.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In PACLIC,
pages 220?229.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. Springer.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolution
based on entity-centric, precision-ranked rules.
Andrew McCallum, Kamal Nigam, et al. 1998. A
comparison of event models for naive bayes text
classification. In AAAI-98 workshop on learning for
text categorization, volume 752, pages 41?48. Cite-
seer.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Joshua L. Moore, Christopher J.C. Burges, Erin Ren-
shaw, and Yih Wen-tau. 2013. Animacy detec-
tion with voting models. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 55?60. Association for
Computational Linguistics.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006.
Talbanken05: A swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
the fifth International Conference on Language Re-
sources and Evaluation (LREC), pages 1392?1395.
74
Constantin Or?asan and Richard Evans. 2007. Np ani-
macy identification for anaphora resolution. J. Artif.
Intell. Res.(JAIR), 29:79?103.
Lilja ?vrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough?Swedish
dependency parsing with rich linguistic features.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP), pages 447?451.
Lilja ?vrelid. 2005. Animacy classification based on
morphosyntactic corpus frequencies: some experi-
ments with norwegian nouns. In Proc. of the Work-
shop on Exploring Syntactically Annotated Corpora.
Lilja ?vrelid. 2006. Towards robust animacy clas-
sification using morphosyntactic distributional fea-
tures. In Proceedings of the Eleventh Conference of
the European Chapter of the Association for Com-
putational Linguistics: Student Research Workshop,
pages 47?54. Association for Computational Lin-
guistics.
Lilja ?vrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 630?638. Association
for Computational Linguistics.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan
Lee, Eleni Miltsakaki, and Bonnie Webber. 2005.
The Penn Discourse TreeBank as a resource for nat-
ural language generation. In Proc. of the Corpus
Linguistics Workshop on Using Corpora for Natural
Language Generation, pages 25?32.
Conor Quinn. 2001. A preliminary survey of animacy
categories in penobscot. In Papers of the 32nd. Al-
gonquian Conference, pages 395?426.
Anette Rosenbach. 2008. Animacy and grammatical
variation?findings from English genitive variation.
Lingua, 118(2):151?171.
Anatol Stefanowitsch. 2003. Constructional semantics
as a limit to grammatical alternation: The two gen-
itives of English. TOPICS IN ENGLISH LINGUIS-
TICS, 43:413?444.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, volume 1, pages 477?485.
Association for Computational Linguistics.
Noortje J. Venhuizen, Valerio Basile, Kilian Evang, and
Johan Bos. 2013. Gamification for word sense
labeling. Proc. 10th International Conference on
Computational Semantics (IWCS-2013), pages 397?
403.
Annie Zaenen, Jean Carletta, Gregory Garretson,
Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina, M Catherine O?Connor, and Tom Wasow.
2004. Animacy encoding in english: why and how.
In Proceedings of the 2004 ACL Workshop on Dis-
course Annotation, pages 118?125. Association for
Computational Linguistics.
75
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642?646,
Dublin, Ireland, August 23-24, 2014.
The Meaning Factory: Formal Semantics for Recognizing Textual
Entailment and Determining Semantic Similarity
Johannes Bjerva
Univ. of Groningen
j.bjerva@rug.nl
Johan Bos
Univ. of Groningen
johan.bos@rug.nl
Rob van der Goot
Univ. of Groningen
r.van.der.goot@rug.nl
Malvina Nissim
Univ. of Bologna
malvina.nissim@unibo.it
Abstract
Shared Task 1 of SemEval-2014 com-
prised two subtasks on the same dataset
of sentence pairs: recognizing textual en-
tailment and determining textual similar-
ity. We used an existing system based on
formal semantics and logical inference to
participate in the first subtask, reaching
an accuracy of 82%, ranking in the top
5 of more than twenty participating sys-
tems. For determining semantic similar-
ity we took a supervised approach using a
variety of features, the majority of which
was produced by our system for recogniz-
ing textual entailment. In this subtask our
system achieved a mean squared error of
0.322, the best of all participating systems.
1 Introduction
The recent popularity of employing distributional
approaches to semantic interpretation has also lead
to interesting questions about the relationship be-
tween classic formal semantics (including its com-
putational adaptations) and statistical semantics.
A promising way to provide insight into these
questions was brought forward as Shared Task 1 in
the SemEval-2014 campaign for semantic evalua-
tion (Marelli et al., 2014). In this task, a system is
given a set of sentence pairs, and has to predict for
each pair whether the sentences are somehow re-
lated in meaning. Interestingly, this is done using
two different metrics: the first stemming from the
formal tradition (contradiction, entailed, neutral),
and the second in a distributional fashion (a simi-
larity score between 1 and 5). We participated in
this shared task with a system rooted in formal se-
mantics. In particular, we were interested in find-
ing out whether paraphrasing techniques could in-
crease the accuracy of our system, whether mean-
ing representations used for textual entailment are
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
useful for predicting semantic similarity, and con-
versely, whether similarity features could be used
to boost accuracy of recognizing textual entail-
ment. In this paper we outline our method and
present the results for both the textual entailment
and the semantic similarity task.
1
2 Recognizing Textual Entailment
2.1 Overview
The core of our system for recognizing textual en-
tailment works as follows: (i) produce a formal se-
mantic representation for each sentence for a given
sentence pair; (ii) translate these semantic repre-
sentations into first-order logic; (iii) use off-the-
shelf theorem provers and model builders to check
whether the first sentence entails the second, or
whether the sentences are contradictory. This is
essentially an improved version of the framework
introduced by Bos & Markert (2006).
To generate background knowledge that could
assist in finding a proof we used the lexical
database WordNet (Fellbaum, 1998). We also
used a large database of paraphrases (Ganitkevitch
et al., 2013) to alter the second sentence in case no
proof was found at the first attempt, inspired by
Bosma & Callison-Burch (2006). The core sys-
tem reached high precision on entailment and con-
tradiction. To increase recall, we used a classifier
trained on the output from our similarity task sys-
tem (see Section 3) to reclassify the ?neutrals? into
possible entailments.
2.2 Technicalities
The semantic parser that we used is Boxer (Bos,
2008). It is the last component in the pipeline of
the C&C tools (Curran et al., 2007), comprising
a tokenizer, POS-tagger, lemmatizer (Minnen et
1
To reproduce these results in a linux environment (with
SWI Prolog) one needs to install the C&C tools (this in-
cludes Boxer and the RTE system), the Vampire theorem
prover, the two model builders Paradox and Mace-2, and the
PPDB-1.0 XL database. Detailed instructions can be found in
the src/scripts/boxer/sick/README folder of the
C&C tools.
642
al., 2001), and a robust parser for CCG (Steed-
man, 2001). Boxer produces semantic represen-
tations based on Discourse Representation Theory
(Kamp and Reyle, 1993). We used the standard
translation from Discourse Representation Struc-
tures to first-order logic, rather than the one based
on modal first-order logic (Bos, 2004), since the
shared task data did not contain any sentences with
propositional argument verbs.
After conversion to first-order logic, we
checked with the theorem prover Vampire (Ri-
azanov and Voronkov, 2002) whether a proof
could be found for the first sentence entailing the
second, and whether a contradiction could be de-
tected for the conjunction of both sentences trans-
lated into first-order logic. If neither a proof nor
a contradiction could be found within 30 seconds,
we used the model builder Paradox (Claessen and
S?orensson, 2003) to produce a model of the two
sentences separately, and one of the two sentences
together. However, even though Paradox is an ef-
ficient piece of software, it does not always return
minimal models with respect to the extensions of
the non-logical symbols. Therefore, in a second
step, we asked the model builder Mace-2 (Mc-
Cune, 1998) to construct a minimal model for the
domain size established by Paradox. These mod-
els are used as features in the similarity task (Sec-
tion 3).
Background knowledge is important to increase
recall of the theorem prover, but hard to acquire
automatically (Bos, 2013). Besides translating hy-
pernym relations of WordNet to first-order logic
axioms, we also reasoned that it would be benefi-
cial to have a way of dealing with multi-word ex-
pressions. But instead of translating paraphrases
into axioms, we used them to rephrase the input
sentence in case no proof or contradiction was
found for the original sentence pair. Given a para-
phrase SRC7?TGT, we rephrased the first sen-
tence of a pair only if SRC matches with up to
four words, no words of TGT were already in the
first sentence, and every word of TGT appeared in
the second sentence. The paraphrases themselves
were taken from PPDB-1.0 (Ganitkevitch et al.,
2013). In the training phrase we found that the XL
version (comprising o2m, m2o, phrasal, lexical)
gave the best results (using a larger version caused
a strong decrease in precision, while smaller ver-
sions lead to a decrease in recall).
We trained a separate classifier in order to re-
classify items judged by our RTE system as be-
ing neutral. This classifier uses a single feature,
namely the relatedness score for each sentence
pair. As training material, we used the gold relat-
edness scores from the training and trial sets. For
classification of the test set, we used the related-
ness scores obtained from our Semantic Similarity
system (see Section 3). The classifier is a Support
Vector Machine classifier, in the implementation
provided by Scikit-Learn (Pedregosa et al., 2011),
based on the commonly used implementation LIB-
SVM (Chang and Lin, 2011). We used the imple-
mentation?s standard parameters.
2.3 Results
We submitted two runs. The first (primary) run
was produced by a configuration that included re-
classifying the ?neutrals?. The second run is with-
out the reclassification of the neutrals. After sub-
mission we ran a system that did not use the para-
phrasing technique in order to measure what in-
fluence the PPDB had on our performance. The
results are summarized in Table 1. In the train-
ing phase we got the best results for the configu-
ration using the PPDB and reclassication, which
was submitted as our primary run.
Table 1: Results on the entailment task for various
system configurations.
System Configuration Accuracy
most frequent class baseline 56.7
?PPDB, ?reclassification 77.6
+PPDB, ?reclassification 79.6
+PPDB, +reclassification 81.6
In sum, our system for recognizing entailment
performed well reaching 82% accuracy and by
far outperforming the most-frequent class baseline
(Table 1). We show some selected examples illus-
trating the strengths of our system below.
Example 1627 (ENTAILMENT)
A man is mixing a few ingredients in a bowl
Some ingredients are being mixed in a bowl by a person
Example 2709 (CONTRADICTION)
There is no person boiling noodles
A woman is boiling noodles in water
Example 9051 (ENTAILMENT)
A pair of kids are sticking out blue and green colored tongues
Two kids are sticking out blue and green colored tongues
A proof for entailment is found for Ex. 1627,
because for passive sentences Boxer produces
a meaning representation equivalent to their ac-
tive variants. A contradiction is detected for
Ex. 2709 because of the way negation is han-
dled by Boxer. Both examples trigger background
knowledge from WordNet hyperonyms (man ?
person; woman ? person) that is used in the
643
proofs.
2
Ex. 9051 shows how paraphrasing helps,
here ?a pair of? 7? ?two?.
3 Determining Semantic Similarity
3.1 Overview
The Semantic Similarity system follows a super-
vised approach to solving the regression problem
of determining the similarity between each given
sentence pair. The system uses a variety of fea-
tures, ranging from simpler ones such as word
overlap, to more complex ones in the form of
deep semantic features and features derived from a
compositional distributional semantic model. The
majority of these features are derived from the
models from our RTE system (see Section 2).
3.2 Technicalities
3.2.1 Regressor
The regressor used is a Random Forest Regressor
in the implementation provided by Scikit-Learn
(Pedregosa et al., 2011). Random forests are ro-
bust with respect to noise and do not overfit easily
(Breiman, 2001). These two factors make them a
highly suitable choice for our approach, since we
are dealing with a relatively large number of weak
features, i.e., features which may be seen as indi-
vidually containing a rather small amount of infor-
mation for the problem at hand.
Our parameter settings for the regressor is fol-
lows. We used a total of 1000 trees, with a maxi-
mum tree depth of 20. At each node in a tree the
regressor looked at maximum 3 features in order
to decide on the split. The quality of each such
split is determined using mean squared error as
measure. These parameter values were optimised
when training on the training set, with regards to
performance on the trial set.
3.2.2 Feature overview
We used a total of 32 features for our regres-
sor. Due to space constraints, we have sub-divided
our features into groups by the model/method in-
volved. For all features we compared the outcome
of the original sentence pair with the outcome of
the paraphrased sentence pairs (see Section 2.2)
3
.
If the paraphrased sentence pair yielded a higher
feature overlap score than the original sentence
pair, we utilized the former. In other words, we
2
In the training data around 20% of the proofs for entail-
ment were established with the help of WordNet, but only 4%
for detecting contradictions.
3
In addition to the PPDB we added handling of negations,
by removing some negations {not, n?t} and substituting oth-
ers {no:a, none:some, nobody:somebody}.
assume that the sentence pair generated with para-
phrases is a good representation of the original
pair, and that similarities found here are an im-
provement on the original score.
Logical model We used the logical models cre-
ated by Paradox and Mace for the two sentences
separately, as well as a combined model (see Sec-
tion 2.2). The features extracted from this model
are the proportion of overlap between the in-
stances in the domain, and the proportion of over-
lap between the relations in the model.
Noun/verb overlap We first extracted and lem-
matised all nouns and verbs from the sentence
pairs. With these lemmas we calculated two new
separate features, the overlap of the noun lemmas
and the overlap of the verb lemmas.
Discourse Representation Structure (DRS)
The two most interesting pieces of information
which easily can be extracted from the DRS mod-
els are the agents and patients. We first extracted
the agents for both sentences in a sentence pair,
and then computed the overlap between the two
lists of agents. Secondly, since all sentences in the
corpus have exactly one patient, we extracted the
patient of each sentence and used this overlap as a
binary feature.
Wordnet novelty We build one tree containing
all WordNet concepts included in the first sen-
tence, and one containing all WordNet concepts
of both sentences together. The difference in size
between these two trees is used as a feature.
RTE The result from our RTE system (entail-
ment, neutral or contradiction) is used as a feature.
Compositional Distributional Semantic Model
Our CDSM feature is based on word vectors de-
rived using a Skip-Gram model (Mikolov et al.,
2013a; Mikolov et al., 2013b). We used the pub-
licly available word2vec
4
tool to calculate these
vectors. We trained the tool on a data set con-
sisting of the first billion characters of Wikipedia
5
and the English part of the French-English 10
9
corpus used in the wmt11 translation task
6
. The
Wikipedia section of the data was pre-processed
using a script
7
which made the text lower case, re-
moved tables etc. The second section of the data
was also converted to lower case prior to training.
We trained the vectors using the following pa-
rameter settings. Vector dimensionality was set
4
code.google.com/p/word2vec/
5
mattmahoney.net/dc/enwik9.zip
6
statmt.org/wmt11/translation-task.html#download
7
mattmahoney.net/dc/textdata.html
644
Table 2: Pearson correlation and MSE obtained on the test set for each feature group in isolation.
Feature group p [?PPDB] p [+PPDB] MSE [?PPDB] MSE [+PPDB]
Logical model 0.649 0.737 0.590 0.476
Noun/verb overlap 0.647 0.676 0.592 0.553
DRS 0.634 0.667 0.610 0.569
Wordnet novelty 0.652 0.651 0.590 0.591
RTE 0.621 0.620 0.626 0.627
CDSM 0.608 0.609 0.681 0.679
IDs 0.493 0.493 0.807 0.807
Synset 0.414 0.417 0.891 0.889
Word overlap 0.271 0.340 0.944 0.902
Sentence length 0.227 0.228 0.971 0.971
All with IDs 0.836 0.842 0.308 0.297
All without IDs 0.819 0.827 0.336 0.322
to 1600 with a context window of 10 words. The
skip-gram model with hierarchical softmax, and a
negative sampling of 1e-3 was used.
To arrive at the feature used for our regressor,
we first calculated the element-wise sum of the
vectors of each word in the given sentences. We
then calculated the cosine distance between the
sentences in the sentence pair.
IDs One surprisingly helpful feature was each
sentence pair?s ID in the corpus.
8
Since this
feature clearly is not representative of what one
would have access to in a real-world scenario, it
was not included in the primary run.
Synset Overlap We built one set for each sen-
tence pair consisting of each possible lemma form
of all possible noun synsets for each word. The
proportion of overlap between the two resulting
sets was then used as a feature. Given cases where
relatively synonymous words are used (e.g. kid
and child), these will often belong to the same
synset, thus resulting in a high overlap score.
Synset Distance We first generated each possi-
ble word pair consisting of one word from each
sentence. Using these pairings, we calculated
the maximum path similarity between the noun
synsets available for these words. This calculation
is restricted so that each word in the first sentence
in each pair is only used once.
Word overlap Our word overlap feature was
calculated by first creating one set per sentence,
containing each word occurring in that sentence.
8
We discovered that the ordering of the entire data set was
informative for the prediction of sentence relatedness. We
have illustrated this by using the ordering of the sentences
(i.e. the sentence IDs) as a feature in our model, and thereby
obtaining better results. Relying on such a non-natural order-
ing of the sentences would be methodologically flawed, and
therefore this feature was not used in our primary run.
The four most common words in the corpus were
used as a stop list, and removed from each set. The
proportion of overlap between the two sets was
then used as our word overlap feature.
Sentence Lengths The difference in length be-
tween the sentence pairs proved to be a somewhat
useful feature. Although mildly useful for this par-
ticular data set, we do not expect this to be a par-
ticularly helpful feature in real world applications.
3.3 Results
We trained our system on 5000 sentence pairs, and
evaluated it on 4927 sentence pairs. Table 2 con-
tains our scores for the evaluation, broken up per
feature group. Our relatedness system yielded the
highest scores compared to all other systems in
this shared task, as measured by MSE and Spear-
man correlation scores. Although our system per-
formed slightly worse as measured by Pearson
correlation, there is no significant difference to the
scores obtained by the two higher ranked systems.
4 Conclusion
Our work shows that paraphrasing techniques can
be used to improve the results of a textual entail-
ment system. Additionally, the scores from our
semantic similarity measure could be used to im-
prove the scores of the textual entailment system.
Our work also shows that deep semantic features
can be used to predict semantic relatedness.
Acknowledgements
We thank Chris Callison-Burch, Juri Ganitkevitch and Ellie
Pavlick for getting the most out of PPDB. We also thank our
colleagues Valerio Basile, Harm Brouwer, Kilian Evang and
Noortje Venhuizen for valuable comments and feedback.
645
References
Johan Bos and Katja Markert. 2006. Recognising
textual entailment with robust logical inference. In
Joaquin Quinonero-Candela, Ido Dagan, Bernardo
Magnini, and Florence d?Alch?e Buc, editors, Ma-
chine Learning Challenges, MLCW 2005, volume
3944 of LNAI, pages 404?426.
Johan Bos. 2004. Computational Semantics in Dis-
course: Underspecification, Resolution, and Infer-
ence. Journal of Logic, Language and Information,
13(2):139?157.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Johan Bos. 2013. Is there a place for logic in rec-
ognizing textual entailment? Linguistic Issues in
Language Technology, 9(3):1?18.
Wauter Bosma and Chris Callison-Burch. 2006. Para-
phrase substitution for recognizing textual entail-
ment. In Proceedings of CLEF.
Leo Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
K. Claessen and N. S?orensson. 2003. New techniques
that improve mace-style model finding. In P. Baum-
gartner and C. Ferm?uller, editors, Model Computa-
tion ? Principles, Algorithms, Applications (Cade-
19 Workshop), pages 11?27, Miami, Florida, USA.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically Motivated Large-Scale NLP with
C&C and Boxer. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 33?36, Prague,
Czech Republic.
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
Juri Ganitkevitch, Benjamin VanDurme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL 2013), Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi,
S. Menini, and R. Zamparelli. 2014. Semeval-2014
task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
W. McCune. 1998. Automatic Proofs and Counterex-
amples for Some Ortholattice Identities. Informa-
tion Processing Letters, 65(6):285?291.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of english. Jour-
nal of Natural Language Engineering, 7(3):207?
223.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
A. Riazanov and A. Voronkov. 2002. The Design and
Implementation of Vampire. AI Communications,
15(2?3):91?110.
Mark Steedman. 2001. The Syntactic Process. The
MIT Press.
646
