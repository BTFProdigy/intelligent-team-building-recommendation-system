A Hybr id  Japanese  Parser wi th  Hand-craf ted Grammar  and 
Stat ist ics 
Hiroshi Kanayama 1, Kentaro Torisawa 1:*, 
Yutaka Mitsuishi* a,nd Jun' ichi  Tsujiit* 
\[ Tokyo Research Lal)oratory, IBM Jal)an, Ltd. 
1623-1 d Shimo-tsuruma, Yamato-shi, Kanagawa 242-8502, Jal)an 
:!: Del)artment of Inlbrnmtion Science, Graduate School of Science, University of 2bkyo 
7-a-1 Hongo, Bunkyo-ku, Tokyo 113-0033, .\]al)an 
? hfformation and Human Behavior, PII.I~3S'\].'O, Jal)an Scie, tce and Technology Corporation 
Kawaguehi l ion-the 4-1-8, Kawaguchi-shi, Saitmna a32-0012, Japan 
, CCL, UMIST, U.K. 
{kanayama, torisawa, mitsuisi, tsujii}@is, s.u-tokyo, ac. jp 
Abstract  
This l)al)er (leseril)es a hybrid t)arsing method for 
Jal)anese which uses both a hand-crafted gram- 
mar and a statistical 1;e(:hniqlte. The key featm'e 
of our  syst (nn is that  in o rder  to es t imate  likeli- 
hood  for a parse tree, the sys te ln  Ilses informa- 
tion taken from Mternative 1)artial parse trees gen- 
erated by the grammar. This utilization of alter- 
native trees enables us to construct a new statis- 
tical model called 'l}'it)let/Quadrul)let Model. We 
show that this model can capture a certain ten- 
dency in .\]apalmse yntactic structures and this point 
COlltril)lltes to i lnl)rovetl lel l t  of l)al'sill~ acct l racy oil 
a shallow level. We rel)ort that, with an under- 
Sl)ecified HPSG-1)ased grammar and ~t maximum en- 
tropy estilnation, our parser achieved high a(:curacy: 
88.6% accuracy in del)endency analysis of the EDR 
annotated eorlms, and that it, outI)erformed oth(u' 
lmrely statistical l)arsing methods on the same cor- 
pus. This result suggests that 1)rol)er t reatlnent of 
hand-crafted gra, mnml'S ca,n contribute to l )arsi ltg ac- 
curacy  on a shallow level. 
1 Introduct ion 
There have been many attempts to combine hand- 
crafted high-level gramnmrs, such as FB-UfAG, 
HPSG and LFG, and statistical disambiguation 
techniques to ol)tain precise linguistic struc, tures 
(Schabes, 1992; Almey, 1996; Carroll el, al., 1998). 
One evident advantage of this apl)roaeh over lmrely 
statistical parsing techniques is that grammars can 
provide precise smnantie representations. However, 
considering that remarkable parsing accuracy in a 
shallow level has been achieved by purely statisti- 
cal techniques (e.g. Ratnal)arkhi (1997)), it may be 
thought more reasonable to use high-level gramnmrs 
just tin' 1)osti)rocessing which nmps results of shallow 
syntactical analyses onto dee 1) analyses. 
'l.'his work was conducted while the first ~mthor was a 
graduate student at Univ. of Tokyo. 
Figure 1: A tree M with a non-head aughter NH and 
a }mad aughter H. 
In this work we prol)ose that hand-crafl, ed high- 
level grammars (:all be useful in shallow-level analy- 
ses and statistical models. In our fl'amework, gram- 
mars are used to obtain precise features for probabil- 
ity estimation, which are difficult to obtain without a 
grannnar, and we show that such features contribute 
to high parsing accuracy on a shallow level. 
In this lmper, the most preferable parse trees are 
chosen with a statistical model. In our method, the 
likelihood value L(M) of a (partial) tree M in Fig- 
ure 1 is detined as in (1): 
L(M)  dor L(NH) x L(H)  x P('n ~ h) (1) 
where NH is M's non-head daughter (whose lexical 
head is n), H is the head-daughter (whose lexical 
head is h), and /)(n -~ h) is the probability of n 
t)eing related to h. For a. single lexical iteni W, L(W) 
is defined as 1.0. 
In most models already proposed, the probability 
P (n  ~ h) is calculated with the conditional proba- 
bility (2): 
- ,  h) d?d P(T I ?',,, %, (2) 
where T indicates that the dependency is true; (1)~ 
and q~h are attributes of 'n and h, respectively. And 
An,h, the distance between the two words, is widely 
used, because this attribute is believed to strongly 
affect whether those two words are going to be re- 
late& 
In contrast, in the statistical model proposed in 
this paper, P(n  -~ It) depends not only on the at- 
trilmtes of the tree M, but also on alternative trees 
411 
Input smltence : < . . . n . . . h l  . . . h i  . . . h t  . . . > 
Mi Mz 
M1 
n h ,1  " " " ?z  h i  ~,  h , l  
Figure 2: Pro'tiM trees whose non-head aughter's lexi- 
cal head is n. 
M 
l r 
Figure 3: ~h'anstbrmation fl'om a tree to a dependency. 
l' and r' denote the bunsets,~s l and r belong to, respec- 
tively. 
in the parse forest generated by the grmmnar. More 
precisely, when P(n  --+ h) is calculated, we consider 
partial trees whose non-head aughter's lexical head 
is n, as displayed in Figure 2. Here alternative pos- 
sible hk (k = 1, - . . ,  l) are taken into consideration, 
and ordered according to their distance to n. We 
call such set of hk modification candidates, and all 
modification candidates are placed together in the 
conditional part of the probability as in (3). Now 
assume h = hi. 
P(i I %,, %2, ,  %, , ,  %,) 
(3) 
where "i" indicates the ith candidate mnong the 
modification candidates. Equation (3) shows two 
important properties of our model. One point lies in 
the new distance metric. (3) is the probability that n 
chooses the ith candidate as the modifiee among the 
modification candidates which are ordered according 
to their distance to n. Thus, we no longer require 
the distance metric A~,h, instead we use the relative 
position among the modification candidates, which 
works as an attribute of the modification. The other 
point is the use of the attributes of the alternative 
parse trees, that is, attributes of the modifier and all 
its modification candidates are considered simulta- 
neously. We show that these techniques ophisticate 
our model, by providing linguistic examples in Sec- 
tion 3.2. 
In practice, however, treating all candidates i not 
feasible because of data-sparseness. We therefore 
apply a strategy of restricting the modification can- 
didates to at most three. The strategy and its justi- 
fication are discussed in Section 3.1. 
Applying the strategy to the equation (3), we ob- 
tain equations (4) and (5): 
P(It hi) de=f P(i I (i = 1, 2) (4) 
hi) der P(i I %, ,  %=, %,) (i = *, 2, t)(5) 
When there are only two candidates, equation (4) 
is used; otherwise, equation (5) is used. Our statis- 
tical model is called the ~Hplet/Quadrut)let Modal, 
which was named after the nmnbcr of constituents 
in the conditional parts of the equations. 
We report that our parsing framework achieved 
high accuracy (88.6%) in dependency analysis of 
Japanese with a combination of an underspecified 
HPSG-based Japanese grammar, SLUNG (Mitsu- 
ishi et al, 1998) and the maximum entropy method 
(Berger et al, 1996). Moreover, the resulting parse 
trees generated by our hybrid parser are legitimate 
trees in terms of given hand-crafted grammars, and 
we are expecting that we can enjoy advantages pro- 
vided by high-level gramnmr formalisms, such as 
construction of semantic structures. 
In the above explanation, we used the notion of 
lexical heads for the estimation of probabilities of 
trees for the sake of simplicity. But, in the present 
implementation, we use bunscts,Ls instead of lexical 
heads, and a relation on a tree is converted to a 
bunsetsu-dependency as shown in Figure 3. A bun- 
sctsu is a basic syntactic unit in Japanese. It consists 
of a content word and some flmctional morphemes 
such as a particle. 
In Section 2, we describe some existing statisti- 
cal parsers, and the Japanese grannnar which we 
adopted. Section 3 describes our statistical method 
and its adwmtages in detail. We report ext)erimental 
results in Section 4. 
2 Background 
In this section, we describe several models for 
Japanese dependency analysis and works on statisti- 
cal approaches with gramlnars. Next, we introduce 
SLUNG, the HPSG-based Japanese grammar which 
is used in our hybrid parser. 
2.1 P rev ious  Dependency Analysis Mode ls  
of Japanese 
Several statistical models for Japanese dependency 
analysis which do not utilize a lland-crafted granl- 
mar have been proposed. We evaluate the accuracy 
of bunsetsu-dependencies as they do, thus here we 
introduce thenl for comparison. All models intro- 
duced below are based on the likelihood value of the 
dependency between two bunsetsus. But they differ 
from each other in the attributes or outputs which 
are considered when a likelihood value is calculated. 
There are some models which calculate the likeli- 
hood values of a dependency between bunsetsu i and 
j as in (6), such as a decision tree model (Haruno et 
al., 1998), a maximum entropy model (Uchimoto et 
al., 1999), a model based on distance and lexical in- 
formation (Pujio and Matsumoto, 1998). Attributes 
(I)i and ~I,j consist of a part-of-speech (POS), a lexi- 
cal item, presence of a comma, and so on. And Ai,j 
412 
is the number of intervening bnnscts'us between i and 
j. 
p(i -~ j) d,j ~'Crl ,I,i, %, a~,j) ((0 
However, these lnodels Nil to reftect contextual 
information because attributes of the surrounding 
bunsets,tts are not considered. 
Uchimoto et al (2000) proposed a model us- 
ing posterior context;. The model utilizes not only 
attributes about bunscts~s i, j but also attributes 
about all bunsets~> (including j) wlfich tbllow bun- 
setsu i. That is, instead of learning two output val- 
ues "T(true)" or ':F(false)" for the del)endency be- 
tween two bunsets~zs, three output values are used 
*br leanfing: the b~m.setsu i is "bynd (dependent on 
a bunsctsu beyond j)" ,  "dpnd (del)endent on the 
b~tsets~t 3)" or "btwn (dependent on a b'unscts~t be- 
tween i and j)".  The 1)robability is calculated by 
multiplying probabilities for all bunscts,~ls which tbl- 
low b~trtsctsu i as in (7). 'l'hey report that this kind 
of contextual information improves accuracy. How- 
ever, the model has to assume, the independency of
all the random variables, which may cause some er- 
ro rs .  
P(i --, j) "?Z H ~'(by.d I ?'~, %, &,k) 
i<k<j  
xP(dpnd I (1)i, il)5, Aid) x Hl'(btw,, \[ (I,i, q?k, A<k)(7) 
k>j 
The difference between our model and these pre- 
vious models are discussed in Section 3. 
2.2 Stat is t ica l  Approaches  w i th  a grmnnmr  
There have been nlally l)rOl)osals tbr statistical 
t'rameworks particularly designed tbr 1)arsers with 
hand-crafted grmnmars (Schal)es, 1992; Briscoe and 
Carroll, 1993; Abney, 1996; Inui et al, 1!)97). The 
main issue in tiffs type of research is how to assign 
likelihoods to a single linguistic structure generated 
by a gramlnar. Some of tlmm (Briscoe and Carroll, 
1!)93; hmi et al, 1997) treat information on contexts, 
but the contextual intbrmation is de.rived only fl'om 
a structure to wlfich the parser is trying to assign 
a likelihood value. Then, tim major difference be.- 
tween their method and ours is that we consider the 
attributes of alternative linguistic structures gener- 
ated by the grammar in order to deternfine the like- 
lihood for linguistic structures. 
2.3 SLUNG : J apanese  Grammar  
The Japanese grammar which we adopted, SLUNG 
(Mitsuishi et al, 1998), is an HPSG-based under- 
specified grammar. It consists of 8 rule schemata, 
48 lexical templates for POSs and 105 lexical entries 
for functional words. As can be seen fl'om these fig- 
ures, the granmmr does not contain detailed lexk:al 
information that needs intensive labor for develop- 
ment. However, it is precise in the sense that it 
aclfieves 83.7% dependency accuracy with a silnple 
heuristics 2 for the El)I{ almotated corl)us , and it 
can produce at least one parse tree for 98.4% sen- 
tences in the EDR annotated corpus. We use the 
grammar for generating parse tree forests, and our 
'l~'iplet/Quadruplet Model is used tbr picking Ul) a 
single tree fl'om a forest. 
3 The  Hybr id  Pars ing  Method 
This section describes tim procedure of parsing with 
the ~l"riplet/Quadrul)let Model. Our hybrid 1)arsing 
method proceeds as tbllows: 
? At; the beginning, dependency structures are 
obtained from trees generated by SLUNG. For 
each bunsctsu, modification candidates are enu- 
merated, and if there are four or more candi- 
dates, tlmy are restricted to three. The lmuristic 
used in this process is described in Section 3.1. 
? Then, with the ~'il)let/Quadruplef; Mode.l and 
maxinnnn entropy estimation, prol)abilities of 
the del)endencies are calculated. Secti(m 3.2 
discusses the characteristics and advantages of 
the model. 
? Finally, the most preferable trees for the whole 
sentence are selected. 
3.1 Rest r i c t ion  o f  Modi f i ca t ion  Cand idates  
Kanayama et al (1999) report that when mod- 
ification candidates are emnnerated according to 
SLUNG, 98.6% of the correct modifie.es are in one of 
the following three 1)ositions among the candidates: 
1;11(; nearest one from the modifier, the second nearest 
one, and the. farthest one. 
As a consequence, we can siml)lil\[y I;11(; problem 
by considering only these three candidates and dis- 
carding tim other candidates, with only 1.4% poten- 
tial errors. We therefore assume that the. number of 
modification candidates ix always three or less. 
This idea is sinfilar to that of Sekine (2000)'s 
study, which restricts the candidates to five, i)ut in 
his case, without a granmmr. 
3.2 The  Tr ip le t /Quadrup le t  Mode l  
The 'Diplel,/Quadruplet Model calculates the like- 
lihood of the dependency between bunsetsu i and 
bunsctsu cn; P( i  --, cn) with the formulas (8) and 
(9), where c,~ denotes the nth candidate among b,m- 
sctsu i's candidates; (I,i denotes some attributes of 
i; and ~I~?,~ denotes attributes of c,~ (including at- 
tributes between i and cn). 
P(i -~ c,d dJ P(n I ?,~, %.,, %~) (.n = 1,2)(8)  
P(~ -~ c,~) ,,~r p(,~ I ?'i, %,, ,I,~, ,I%) (,n = 1, 2 , / ) (9 )  
2This heuristics is a Japanese version of a left-association 
rule: see (Mitsuishi et M., 1998) for detail. 
413 
As (8) and (9) suggest, the model considers at- 
tributes of the modifier bunsetsu and attributes of all 
modification candidates imultaneously in the condi- 
tional parts of the probabilities. Moreover, what is 
calculated is not tile probability of "whether the de- 
pendency is correct (T, see Formula(6))", but the 
probability of "which of tile given candidates i cho- 
sen as tile nlodifiee (n =1, 2, or 1)". These charac- 
teristics imply the fbllowing two advantages. 
Advantage  1 A new distance metric. The correct 
modifiee can be chosen by considering relative 
position among grannnatically licensed candi- 
dates, instead of the absolute distance between 
bunsets~as. 
Advantage  2 2)'eating alternative trees. The can- 
didates are taken into consideration simultane- 
ously. But because the nlodifica?ion candidates 
are restricted to at most three, we considerably 
avoid data-sparseness 1)rot)lems. 
Below we discuss these advantages in order. These 
advantages clarify the differences fl'om previous 
models described in Section 2.1, and are empMcally 
confirmed through the experiments in Section 4. 
3.2 .1  Advantage  1 : A new dis tance  metr i c  
As discussed in Section 2.1, the distance metric Ai,j 
used in previous statistical methods was obtained 
simply by counting intervening words or b'unscts,t~ 
l)etween i and j. On the other hand, we use the rel- 
ative position among the modification candidates as 
the distance metric. Tile following examples illus- 
trate a difference between those two types of melric. 
The correct modifiee of kare-ga is hashir'u-no-wo in 
both (10a) ~u~d (lOb). 
(10)a. kare-ga hashiru-no-wo mira koto 
he-SUBJ mm see fact 
(the fact that I saw him run) 
b. kare-ga yukkuri hashiru-no-wo mira koto 
he-SUB.} slowly run see fact 
(the fact that I saw him run slowly) 
In previous models, (10a) and (10b) would yield, 
P. ( kare-o,~--* t~ashir'u-no-wo)=P(T I kar,~-ga, h,~shiru-no-~vo,A1) 
\])b(kare-ga--+ hashi,.tt-?zo-wo)=\])(~l'll~a~ve-ga, hashi,'u-,zo-wo,A2) 
respectively, where A1 = 1 and A 2 = 2. Then, the 
two probabilities above do not have the same value 
in general. 
Our grammar does not allow the dependency 
"kare-.qa --~yukkurY tbr (10b). The modification 
candidates of karc-ga are hashiru-no-wo and mita, 
hence (8) gives the probabilities between kare-ga and 
hashiru-no-wo as follows, in both examples. 
\]~ (kare-ga -~ hashiru- no- wo ) 
= Pb(karc-,qa --~hashiru-no-wo) 
= P(llkare-ga, hashiru-no-wo, mita) 
Thus ,  P(kare-ga --+hashiru-no-wo) has the  same va lue 
for both examples. Our interl)retation of this difl'er- 
enee is sumnlarized as follows. The word yukk'uri s 
an adverb modifying the verb h, ash&'u. Our linguis- 
tic intuition tells us that the presence of such adverb 
should not affect the strength tbr the dependency 
between kare-ga and hashiru-no-wo. According to 
this intuition, the existence of the adverb should be 
considered as a noise. Our model allows us to ignore 
such a noise in learning from annotated corpus, while 
previous nlodels are atfected by such noisy elements. 
3.2 .2  Advantage  2 : Treat ing  a l te rnat ive  
t rees  or contextua l  in fo rmat ion  
Consider the following examples. 
(11) a. Ta~v-no kawaii musume 
NP Adj NP 
Taro-POgS 1)retty daughter 
(~\[h.ro's pretty daughter) 
b. Taro-no yuujin-no musumc 
NP NP NP 
Taro-POSS friend-POSS daughter 
(Taro's fl'iend's daughter) 
Contrary to tim previous examl)les, TaTv-no ill 
(11) ntodifies different nlodification candidates. In 
example (11a), "~hr'o-no --+musume" is the correct 
dependency while "Taro-no -~musume" is not cor- 
rect in (11l)). This difference is caused t)y the b'u'a- 
setsu between Taro-no and musume, kawaii (Adj) 
in ( l la)  and y,u~lfin-no (NP) in ( l lb) .  Actually, the 
grannnar allows Taro-no to depend on either of these 
types of words. Thus, in our model, 
/',('late-no --, musume) 
l't, ( ~1aro-7~o --. m~*sume) 
= P(21 
Then, P(varo-no-+musume) has different values 
for the two examples, hi the annotated corpus, 
l'(21~laro-no, kawaii, musume) tends to have a high 
value since kawaii is an adjective. However, since 
yuujin-no is an NP, P(2\[Taro-no, yuujin-no, musume) 
tends to have a low value. 
Now consider previous models. 
Pb(Taro-,~o--+ m**s~mz~) = P(TI Tin'o-no, musume, 2) 
Then, contrary to our model, P(Taro-no --~musumc) 
lms exactly the same wdue for both examples. The 
outconle is determined by 
= P(T I  Taro-no, kawaii, 1) 
In text corpora, P(TITaro-no , yu~,jin-no, 1) tends 
to be high, and consequently, P(T ITaro-no, musume, 
2) is very small. These values will make the correct 
prediction for (111)) as yuujin-no will be favored over 
musume. However, for (11a), these models are likely 
to incorrectly favor kawaii over musume. This is 
414 
because 1'('.171 Tin'o-no, mus'ume, 2), being very small, is 
likely to be snlaller than P(T\] :late-no, t~,,waii, 1). 
4 Exper iments  and  D iscuss ion  
\].'his section reports a series of parsing experiments 
with our mode, l and gives some discussion. 
4.1 Env i ron lnents  
We used the EDR ,lal)anese Corl)us (El)R, 1996) 
for training and evaluation of 1)arsing accuracy. The 
EI)R Corpus ix a ,Japanese treebank which consists 
of 208,1.57 sentences from newspapers and maga- 
zines. We. used 192,778 sentences for training, (1,744 
for pro-analysis (as reported in Section 3.1), and 
3,372 tbr testing 3. 
With tril)lets constitute(\] of a modifice and two 
modi f i ca t ion  eandida.te.s ext ractc ( l  t i 'onl the learn- 
ing corl)uS l;hc Triplet Model is ('.onstructed. \Vith 
the quadruplets constituted of a moditiee and three 
candidates, the Quadruplet Model is constructed. 
'?hese~ inodels arc estimated by the ChoiccMaker 
Maxinmm Entropy Estimator (Borthwick, 19!)9). 
The features fin' the estimation are listed in Ta- 
/)le 1. The values partially fo lk)w other researches 
e.g. Uchimoto el; al. (\]999), and JUMAN's outputs 
are used for POS classification. Mainly the head of 
the b'unsc.tsu (the rightmost morl)helnc in a b'unscts'u, 
except for whose major POS is "peculiar", "auxiliary 
verb", "particle", "suffix" or "copula") and type  of 
the b'ltnscts'u (the rightmost morphenm in a b'wnsel.s'lt 
except br whose major P()S is "l)eculiar") are used 
as thc at.tributes. \~;e show the meaning of some 
f( 'atures below. 
POS JUMAN's  minor  \]?()S (for both  "head" and 
':type"). 
part ic le,  adverb  Frequent words: 26 lmrticles and 
69 adverbs. 
head lex 2.(14 lexical forms regardless of their POS. 
type  lex 70 suffixes or auxiliary verbs. 
inf lection 6 types of inttcction : "normal", "a(lver- 
l)ial", "adnominal", "tc-fornf', "ta-tbrm", and 
"others". 
The cohmm %aria(ion" in Tal)h; 1 denotes the 
mnnbcr of possible values tbr the feature. "Valid 
features" indicates the nmnber of features which al)- 
peared three times or more in the training corlms. 
4.2 Resu l ts  
Wil;h our model and the features described above, 
the accuracy shown in Tal)lc 2 is achieved. We oval- 
uate the following two tyl)eS of accuracy: 
35,263 SOld,enccs were rOllloved 1)eCmlSe the order of the 
words in the annotal;ion ditl'ered front that in the original 
SOl l tO l lCeS .  
\[ I i \,,re,it, ........ <,~ 
I l l  Pea l ,  u re  type  Va l ' i a t ion  ' l ' r ip .  \] Q ,md,  
I I I oad  I 'OS  ?~f nmdi f ie r  24  ,12 6,t  
2 Type  P ( )S  o f  i l l od i l ' i e r  34  0(l !)D 
3 l ' a r t i ch ,  o f  l / l ed( t ie r  27  , t7  7:{ 
4 Adverb  ,fl' i n (~d i f io r  70  131 103 
5 Ty lm l(}X ( i f  i i l t ld i f io l"  71 110  225 
d In f lec t i (n l  ()|" i . lod i f i (n"  {; 12 18 
7 \Vh( ! th l ! r  lx l ( id i t i ( !r  has  a COlll l l l lt *2 4 ~- 
8 l Iead  I 'OS  , f f  l tH~dil ' ioo 24  7(I 158  
9 Type  I 'OS  o f  lnqMi | ' i ee  34  96  231 
10 lh ,ad  \]?~x ~,1 in i )d i f ieo  2D5 l lG , I  2597 
11 lh t r t ; i c lo  \[d" Inod i l ie (~ 27  92  20.1 
12 ' l ' ypo  Iox  o |  i l l od i | i /~o  71 210 454 
13 \]nlh~l:t . i tHI  o f  In ( .d i f le ( ,  6 2,1 5:1 
1.l ~ .Vhothor  Inodi l ' ioc~ has  it GI)Ill l IIII 2 8 18 
15 \Vhethor  n lod i f ioe  has  %rod' 2 8 18  
IG ~Vho l ,h~r  inod l f ioe  l i l ts " /o"  2 6 17 
17 :\]/ ( if  (~()1\[111~ii~ b(!tlg(~(!ll l;l';?l IHtlt.utlt.uos ,1 16 36  
18 # i) f  "i l ia" l)tl~,~v(!(*lI t~V() bltYl~t:~gllH 3 12 27  
19 2 X 8 816 1187 2727 
20  2 x 7 x 14 13G 38(} 8711 
21 3 X 10 7905 6,t(15 13,10:{ 
22  2 x 9 1156 1213 311)8 
23  3 X 11 729 618 1637 
2.1 2 X 11 i118 1025 2 .194 
'25 2 X 12 241, t  1483 351 '1  
26  2 x 3 x 7 x 8 132192 1331 3O58 
27  1 X 2 X 6 X 8 X 13 7( )5024 ( i( i05 1,t7{10 
I t 'r''':''~ I - I  '224:':~ I """~" J
Table 1: Used f'eaturcs : l,k,,atures from 8 to 27 are re- 
lated to the nm(litiee, thus they are considered for each 
candidate, li'eatures from 19 to 27 are combination fea- 
\ [ ; I l I 'CS .  
I I I -COVCI 'a~O 
S(~.l l tO l  ICCS 
\]htnset.su accuracy 88.ooX,(23078/2(i062) 
Sentence accu,'acy /16.{)0% (1560/3326) 
All lhm.set.su accuracy 88.33% (2335()/26436) 
sentences ,qentcncc~ accuracy 46.35% (1563/3372) 
Tal)h; 2: l{csulls of parsing with the Tril)let/Quadrul)let 
Modcl. 
Bunsetsu  accuracy  The percentage of bu.n,~cts'us 
whose rood(rice is correctly identified. The dc- 
nonfinator includes all b'unsets'us except for the 
last bun,~cts'u of a sentence. 
Sentence accuracy  The percentage of sentences 
whose detmndencies art'. perfectly correct. 
"h>coverage sentences" is the accuracy for the 
sentences flw which SLUNG could generate parse 
trees. We give the accuracy for "All sentences" too, 
by 1)art(ally 1)arsing sentences which SLUNG fail to 
parse. The coverage of SLUNG is al)out 99%, thus 
high accuracy is achieved even for "All sentences". 
Moreover, we conducted a series of experiments 
in order to evaluate the COld;ribution of each charac- 
teristic in our parsing model. The parsing schemes 
used are the four in Figure 3. Major differences 
among them are (I) whether a gralnlnar is used, 
(II) whether modification candidates are restricted 
to three, and (III) whether a previous pair model 
with Formula (6) or the 'lS'iplet/Quadrulflet Model 
with Formula (8),(9) was used. 
W/O Grammar  Mode l  This model does not use 
a grammar. Likelihood values for dcpenden- 
4I 5 
W/O Grammar 
W/O Restriction 
Pair 
~IMplet/Quadruplet 
I G R P 
P 
\]~Un8gts~L accuracy 
86.70%(22594/26062) 
+ - P 87.37%(22770/26062) 
+ + P 87.67%(22849/26062) 
+ + T 88.55%(23078/26062) 
Table 3: Bunsetsu accuracies for four models. Cohmm 
"G" indicates whether the grmmnar is used, "R" indi- 
cates whether the modification candidates are restricted 
to three, and "F" denotes the formula; "P" is the pair 
tbrmula (6), and "T" is the %'iplet/Quadruplet formula 
(s), (9). 
cies are calculated for all bunsctsiLs that follow 
a modifier bunsctsu. Formula (6) is used, and 
as a distance metric Ai,j, the mnnl)er of bun- 
scts~ls between the modifier and tile modifiee 4 
are combined with all features. In general lines, 
this model corresponds to models such as (Fu- 
lie and Matsumoto, 1998; Haruno et al, 1998; 
Uchimoto et al, 1999). 
W/O Rest r i c t ion  Mode l  Modification candi- 
dates are restricted by SLUNG. Tim remaining 
is the same as the W/O Grannnar Model. 
Pa i r  Mode l  Modification candidates are restricted 
to three, in the way described in Section 3.1. 
The remaining is the same as W/O Grannnar 
Model. 
T r ip le t /Quadrup le t  Mode l  Tiffs is the model 
proposed in the paper. Modification candidates 
are restricted to tln'ee, and Fornmla (8) or (9) 
are used. 
From the result shown in Table 3, we can say 
our method contributes to the improvement of our 
parser, because of the following reasons: 
? The %'iplet/Quadruplet Model outperforms the 
Pair Model by 0.9%. Both of them restricts 
modification candidates to three, l)nt tim accu- 
racy got higher when all candidates are consid- 
ered simultaneously. It is because of the two 
adwmtages described in Section a.2. 
? TILe Pair Model outperforms the W/O Restric- 
tion Model by 0.3%. Thus the restriction of 
modification candidates does slot reduce tile ac- 
curacy. 
? TILe W/O Restriction Model outperforms tile 
W/O Grammar Model by 0.7%. This means 
that the use of a grammar as a preprocessor 
works well to pick up possible modifice. 
We found that many structures imilar to the 
ones described iLL Section 3.2 appeared in the EDR 
4Three vahms: "1", "from 2 to 5", "6 or more" are distin- 
guished. 
In-coverage \]3unsct.su accuracy 87.08% (8299/9530) 
sentences Sentence accuracy 44.70% (493/\]103) 
Table 4: Accuracy tbr Kyoto University Corpus 
corpus. Our Tl'iplet/Quadruplet model could treat 
these structures precisely as we intended. Tlfis is the 
main factor that contributed to the improvement of
the overall parsing accuracy. 
Based on tim above experiments, we can say that 
our approach to use the grammar as a preprocessor 
before the calculating of the probability is appropri- 
ate for the improvement of parsing accuracy. 
4.3 Compar i son  to  o ther  mode ls  
4.3.1 Mode ls  us ing the  EDR corpus  
There are several works which use the EDR corpus 
for evaluation. The decision tree model (Haruno et 
al., 1998) achieves around 85%, the integrated model 
of lexical/syntactic information (Slfirai et al, 1998) 
achieves around 86%, and the lexicalized statistical 
model (Ft0io and Matsumoto, 1999) achieves 86.8% 
in bunsets'u accuracy. Our model outperforms all of 
them by 2 or 3%. 
4.3.2 Mode ls  us ing the  Kyoto  corpus  
Slfirai et al (1998) used the Kyoto University text 
corpus (Kurohashi and Nagao, 1997) for evaluation 
and achieved around 86%. Uclfimoto et al (2000) 
also used the Kyoto corlms , and their accuracy was 
87.9%. For comparison, we applied our method to 
the same 1,246 sentences that Uclfimoto et al (2000) 
used. The result is shown in Table 4. 
Our result is worse than theirs. The reason is 
thought o l)e as follows: 
? g~re use tim EDR corpus for training. Although 
we used around 24 times the amount of train- 
ing data that Uchimoto et al used, our training 
data lead to ca'ors in tile analysis of the Kyoto 
Corpus, because of differences in tile mmotation 
schenms adopted. 
? Uchimoto et al used the correct morphological 
analyses, but we used JUMAN. Solnetimes this 
may cause errors. 
? The grammar SLUNG was designed for tile 
EDR corpus, and some types of structures in 
the Kyoto Corpus are not allowed. 
Clearly, our parser should be improved to overcome 
these problems and compared with other works di- 
rectly. 
4.4 D iscuss ion  and I~lture Work  
TILe following are some observations about the speed 
of our parser. Existing statistical parsers are quite 
etficient compared to grammar-based systems. Par- 
ticularly, our system used an HPSG-1)ased grmmnar, 
416 
whose speed is said to be slow. However, recent ad- 
vances in HPSG 1)arsing (~Ibrisawa et al, 2000) en- 
abled us to obtain a unique parse tree with our sys- 
gem in 0.5 sec. in average tbr sentences in the EDR 
corpus. 
Future work shall extend SLUNG so that senmntie 
representatkms are produced. Carroll el; al. (1.998) 
discussed i;he 1)recisiol~ of argument si;ruetures. V~Te 
1)elieve that the focus of ore' study will shift; from a 
shallow level to such a deeper level for ()Ill' tinal aim, 
realization of intelligent natural anguage processing 
systems. 
5 Conclusion 
\?e 1)resenl;ed a hyl)rid 1)arsing scheme l;hat uses a 
hand-crafted grammar and a statist.teal technique. 
As other hybrid pa.rsing ntethods, l;he st.al;isi;ical 
technique is used for 1licking u 1) the most l)re, ferable, 
lmrs(; ire(; fl'om l;he parse fol"(;sI; gent'.rai;e,d I)y t;h(~ 
grammar. The difference fl'om other works is that 
the precise contexi;ual information needed to esti- 
mate |;he likelihood of a parse, 1;ree is obtained fl'ont 
adternative 1)arse trees generated 1)5' the grammar, 
and that such contextual information from alterna- 
tive I;rees enables Its to eonsl;ruel; our new statisti- 
cal model called the ' l?iplet/Quadruplet model. We 
have shown that these poinl;s contributed to sul)sl;an- 
tia l illlprovenlenl; of parsing acellra(:y ill ,lal)ane~se dc- 
1)en(lency analysis, through a serie, s of ext)(~riments 
using an I iPSG-based .lalmnese grammar SLUNC, 
and the, maxinmm entropy method. 
References 
St;even Abney. 1996. Sl;ochasti(: aH;ribut(',-vahm 
grannnars. The Computation and \]\]anguage E- 
Print Archive, October. 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent. J. Della Pietra. \]996. A itiaxilnuln entropy 
approach to natural anguag('~ processing. Compu- 
tatio'n, al Li'n.gui.stics, 22(1.)::/9 71. 
Andrew Borthwiek. 199.(). Choieemaker maximmn 
entropy estimator. ChoieeMaker 'lbch., Inc. Email 
borthwic~cs .nyu. edu for information. 
~\[l'~d Briseoe and John Carroll. 1993. Generalized 
1)robabilistic LR parsing of natural anguage (col 
1)Ol'~t) with unifieation-I)ased gramnmrs. Compu- 
tational Linguistics, 19(1):25-50. 
Jolm Carroll, Guido Minnen, and ~lL'd Briscoe. 1998, 
Can subeategorisation probabilities help a statis- 
tical parser? In Proc. of th, e 5th, ACL/SIGDAT 
Workshop on Very Lawe Corpora, pages 118 126. 
EDR. 1996. EDR (Japan Electronic Dictionary Re- 
search Institute, Ltd.) dictionary version 1.5 tech- 
nical guide,. Second edition is awdlable via 
h t tp  ://www. i i j ne t ,  or.  j p/edr/E_TG .html. 
Masakazu Fujio and Yuji Matsumot;o. 1998. 
,htl)anes<', <lel)endeney structure analysis 1)ased on 
lexicalized statistics. In PTvc. of the 3rd Cm@r- 
ence on Empirical Methods in Natural Language 
Procc.ssin 9, pages 88 96. 
Masakazu FI0io and Ymtji Ma?sumoto. 1999. Sta- 
tistieal syntactic analysis based on co-occurrence 
probability of words. In P'roc. of 5th workshop 
of Nat'u~nl Language Processing, pages 71 78. (in 
Jal)altese ) .
Masahiko Haruno, Satoshi Shirai, and goshiflmfi 
Ooyama. 1998. Using decision trees to construct 
a. practical parser. In Prec. COLING ACL '98, 
pages 505 -511. 
Kentaro hmi, Virach Sornlertbunwmich, Hozumi 
Tanaka, and Takenobu 9bkmmga. 1997. A new 
l>robal)ilistic LR language lnodel t'(31' statistical 
parsing. 'l'echnical I{eport TR974)005, Dept. of 
Coml)uter Science, Tokyo Institute of 'lbehnology. 
l liroshi Kanayanm, Kentaro 'l.brisawa, Yutaka Mit- 
suishi, and Jun'i(:hi Tsujii. 1999. Statistical de- 
1)e, ndency analysis with an HPSG-1)ased Jal)anese 
grannnar. In P'roc. 5th NLPRb', pages 138-143. 
Sadao I(urohashi att(l Makoto Nagao. 1.997. Kyoto 
University text corpus in'ojecK In Prec. of 3rd 
Ann'ual M(~cti~ N of Nat,u, raI Language i)roccssi'ng, 
l)ages 115 118. (in Japanese). 
Yutaka Mii;suishi, Kentaro Torisawa, and Jun'ichi 
Tsujii. 1.(/98. HPSG-stsde undersl)e(:ified .Japanese 
grammar with wide coverage. In P'mc. COLING- 
ACL '98, 1)ages 876 880, Augusl;. 
Adwait llatnalmrkhi. 1997. A linear obse, rved tinl(; 
statistical lm.rser based Oll maximum entropy 
models. In P'mc. th.c Empirical Mt~thods in Nat'u- 
'ral \])a,'n,guag(: \])'roce, ssi'n,9 Co~@rence. 
Yves Sehabes. \]992. Stoclmsti(: lexicalize, d tree- 
adjohfing granmwms. In P'mc. 1dth COLINO, 
pages d26 432. 
S~toshi Sekim,. 2000. Japanese dependency analysis 
using a deternfinistic, tinite state, transdue(;r. In 
Prec. COLING 2000. (this proceedings). 
Kiyoaki Shirai, Kentaro huff, Takenolm Tokunaga, 
and Hozumi Tanaka. 1998. A framework of inl;e- 
grating ,syntactic and lexical statistics in si;atisti- 
cal 1)arsing..lo,urnal ofNat'ural Langua9c l)~vccss - 
int.\], 5(3). (in Japanese). 
Kentaro 'Ibrisawa, Kenji Nishida, Yusuke Miyao, 
and Jun'ichi Tsujii. 2000. An HPSG parser with 
CFG filtering. Jounal of Nat'mal Language E'n, gi- 
nccrin.q. (to al)pear ).
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi 1sa- 
hara. 19!19. Japanese dependency structure anal- 
ysis based on maximum entropy models. In P'roc. 
13th EACL, pages 196 -203. 
Kiyotaka Uchimoto, Masaki Mural;a, Satoshi Sekine, 
and Ititoshi isahara. 2000. \])el)endeney model us- 
ing posterior context. In Prec. of Sixth, intcrna- 
lionel Workshop on Parsing 7'cch, nolo9ics. 
41 7 
Deeper Sentiment Analysis
Using Machine Translation Technology
KANAYAMA Hiroshi NASUKAWA Tetsuya
WATANABE Hideo
Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
{hkana,nasukawa,hiwat}@jp.ibm.com
Abstract
This paper proposes a new paradigm for senti-
ment analysis: translation from text documents
to a set of sentiment units. The techniques of
deep language analysis for machine translation
are applicable also to this kind of text mining
task. We developed a high-precision sentiment
analysis system at a low development cost, by
making use of an existing transfer-based ma-
chine translation engine.
1 Introduction
Sentiment analysis (SA) (Nasukawa and Yi, 2003; Yi
et al, 2003) is a task to obtain writers? feelings as ex-
pressed in positive or negative comments, questions,
and requests, by analyzing large numbers of docu-
ments. SA is becoming a useful tool for the com-
mercial activities of both companies and individual
consumers, because they want to sort out opinions
about products, services, or brands that are scat-
tered in online texts such as product review articles,
replies given to questionnaires, and messages in bul-
letin boards on the WWW.
This paper describes a method to extract a set
of sentiment units from sentences, which is the key
component of SA. A sentiment unit is a tuple of
a sentiment1, a predicate, and its arguments. For
example, these sentences in a customer?s review of
a digital camera (1) contained three sentiment units
(1a), (1b), and (1c). Apparently these units indicate
that the camera has good features in its lens and
recharger, and a bad feature in its price.
It has excellent lens, but the price is too high.
I don?t think the quality of the recharger
has any problem.
?
?
? (1)
[favorable] excellent (lens) (1a)
[unfavorable] high (price) (1b)
[favorable] problematic+neg (recharger) (1c)
The extraction of these sentiment units is not a
trivial task because many syntactic and semantic op-
erations are required. First, the structure of a pred-
icate and its arguments may be changed from the
1Possible values of a sentiment are ?favorable?, ?unfavor-
able?, ?question?, and ?request?. In this paper the discussion
is mostly focused on the first two values.
syntactic form as in (1a) and (1c). Also modal, as-
pectual, and negation information must be handled,
as in (1c). Second, a sentiment unit should be con-
structed as the smallest possible informative unit so
that it is easy to handle for the organizing processes
after extraction. In (1b) the degree adverb ?too? is
omitted to normalize the expression. For (1c), the
predicate ?problematic? has the argument ?recharger?
instead of the head word of the noun phrase ?the
quality of the recharger?, because just using ?qual-
ity? is not informative to describe the sentiment of
the attribute of a real-world object. Moreover, dis-
ambiguation of sentiments is necessary: in (1b) the
adjective ?high? has the ?unfavorable? feature, but
?high? can be treated as ?favorable? in the expression
?resolution is high?.
We regard this task as translation from text to
sentiment units, because we noticed that the deep
language analysis techniques which are required for
the extraction of sentiment units are analogous to
those which have been studied for the purpose of
language translation. We implemented an accurate
sentiment analyzer by making use of an existing
transfer-based machine translation engine (Watan-
abe, 1992), replacing the translation patterns and
bilingual lexicons with sentiment patterns and a sen-
timent polarity lexicon. Although we used many
techniques for deep language analysis, the system
was implemented at a surprisingly low development
cost because the techniques for machine translation
could be reused in the architecture described in this
paper.
We aimed at the high precision extraction of senti-
ment units. In other words, our SA system attaches
importance to each individual sentiment expression,
rather than to the quantitative tendencies of repu-
tation. This is in order to meet the requirement of
the SA users who want to know not only the over-
all goodness of an object, but also the breakdown of
opinions. For example, when there are many posi-
tive opinions and only one negative opinion, the neg-
ative one should not be ignored because of its low
percentage, but should be investigated thoroughly
since valuable knowledge is often found in such a
minority opinion. Figure 1 illustrates an image of
the SA output. The outliner organizes positive and
negative opinions by topic words, and provides ref-
erences to the original text.
Favorable Unfavorable
battery long life - battery (3)
good - battery (2)
:
not good - battery (1)
s
lens
:
nice - lens (2)
:
(original document)
When I bought this camera,
I thought the battery
was not good, but the
problem was solved after
I replaced it with new one.
Figure 1: An image of an outliner which uses SA output.
Users can refer to the original text by clicking on the
document icons.
MT SA
Japanese
sentence
?parser
Japanese
tree structure
? transfer j
English
tree structure
Sentiment
fragments
? generator ?English
sentence
Sentiment
units
Transfer
patterns
Fragment
patterns
Bilingual
lexicon
Polarity
lexicon
Figure 2: The concept of the machine translation en-
gine and the sentiment analyzer. Some components are
shared between them. Also other components are similar
between MT and SA.
This means that the approach for SA should be
switched from the rather shallow analysis techniques
used for text mining (Hearst, 1999; Nasukawa and
Nagano, 2001), where some errors can be treated as
noise, into deep analysis techniques such as those
used for machine translation (MT) where all of the
syntactic and semantic phenomena must be handled.
We implemented a Japanese SA system using a
Japanese to English translation engine. Figure 2 il-
lustrates our SA system, which utilizes a MT engine,
where techniques for parsing and pattern matching
on the tree structures are shared between MT and
SA.
Section 2 reviews previous studies of sentiment
analysis. In Section 3 we define the sentiment unit
to be extracted for sentiment analysis. Section 4
presents the implementation of our system, compar-
ing the operations and resources with those used
for machine translation. Our system is evaluated
in Section 5. In the rest of paper we mainly use
Japanese examples because some of the operations
depend on the Japanese language, but we also use
English examples to express the sentiment units and
some language-independent issues, for understand-
ability.
2 Previous work on Sentiment
Analysis
Some prior studies on sentiment analysis focused on
the document-level classification of sentiment (Tur-
ney, 2002; Pang et al, 2002) where a document
is assumed to have only a single sentiment, thus
these studies are not applicable to our goal. Other
work (Subasic and Huettner, 2001; Morinaga et al,
2002) assigned sentiment to words, but they relied
on quantitative information such as the frequencies
of word associations or statistical predictions of fa-
vorability.
Automatic acquisition of sentiment expressions
have also been studied (Hatzivassiloglou and McKe-
own, 1997), but limited to adjectives, and only one
sentiment could be assigned to each word.
Yi et al (2003) pointed out that the multiple
sentiment aspects in a document should be ex-
tracted. This paper follows that approach, but ex-
ploits deeper analysis in order to avoid the analytic
failures reported by Nasukawa and Yi (2003), which
occurred when they used a shallow parser and only
addressed a limited number of syntactic phenomena.
In our in-depth approach described in the next sec-
tion, two types of errors out of the four reported by
Nasukawa and Yi (2003) were easily removed2.
3 Sentiment Unit
This section describes the sentiment units which are
extracted from text, and their roles in the sentiment
analysis and its applications.
A sentiment unit consists of a sentiment, a predi-
cate, its one or more arguments, and a surface form.
Formally it is expressed as in Figure 3.
The ?sentiment? feature categorizes a sentiment
unit into four types: ?favorable? [fav], ?unfavorable?
[unf], ?question? [qst], and ?request? [req]. A predi-
cate is a word, typically a verb or an adjective, which
conveys the main notion of the sentiment unit. An
argument is also a word, typically a noun, which
modifies the predicate with a case postpositional in
Japanese. They roughly correspond to a subject and
an object of the predicate in English.
For example, from the sentence (2)3, the extracted
sentiment unit is (2a).
ABC123-ha renzu-ga subarashii.
ABC123-TOPIC lens-NOM excellent
?ABC123 has an excellent lens.?
(2)
[fav] excellent ? ABC123, lens ? (2a)
The sentiment unit (2a) stands for the sentiment
is ?favorable?, the predicate is ?excellent? and its ar-
guments are ?ABC123? and ?lens?. In this case, both
?ABC123? and ?lens? are counted as words which are
associated with a favorable sentiment. Arguments
are used as the keywords in the outliner, as in the
leftmost column in Figure 1. Predicates with no ar-
gument are ignored, because they have no effects on
the view and often become noise.
2Though this paper handles Japanese SA, we also imple-
mented an English version of SA using English-French trans-
lation techniques, and that system solved the problems which
were mentioned in Nasukawa and Yi?s paper.
3?ABC123? is a fictitious product name.
<sentiment unit> ::= <sentiment> <predicate> <argument>+ <surface>
<sentiment> ::= favorable | unfavorable | question | request
<predicate> ::= <word> <feature>*
<argument> ::= <word> <feature>*
<surface> ::= <string>
Figure 3: The definition of a sentiment unit.
The predicate and its arguments can be different
from the surface form in the original text. Seman-
tically similar representations should be aggregated
to organize extracted sentiments, so the examples in
this paper use English canonical forms to represent
predicates and arguments, while the actual imple-
mentation uses Japanese expressions.
Predicates may have features, such as negation,
facility, difficulty, etc. For example, ?ABC123
doesn?t have an excellent lens.? brings a sentiment
unit ?[unf] excellent+neg ? ABC123, lens ??. Also
the facility/difficulty feature affects the sentiments
such as ?[unf] break+facil? for ?easy to break? and
?[unf] learn+diff? ?difficult to learn?.
The surface string is the corresponding part in the
original text. It is used for reference in the view of
the output of SA, because the surface string is the
most understandable notation of each sentiment unit
for humans.
We use the term sentiment polarity for the se-
lection of the two sentiments [fav] and [unf]. The
other two sentiments, [qst] and [req] are important
in applications, e.g. the automatic creation of FAQ.
Roughly speaking, [qst] is extracted from an inter-
rogative sentence, and [req] is used for imperative
sentences or expressions such as ?I want ...? and
?I?d like you to ...?. From a pragmatic point of view
it is difficult to distinguish between them4, but we
classify them using simple rules.
4 Implementation
This section describes operations and resources de-
signed for the extraction of sentiment units. There
are many techniques analogous to those for machine
translation, so first we show the architecture of the
transfer-based machine translation engine which is
used as the basis of the extraction of sentiment units.
4.1 Transfer-based Machine Translation
Engine
As illustrated on the left side of Figure 2, the
transfer-based machine translation system consists
of three parts: a source language syntactic parser,
a bilingual transfer which handles the syntactic tree
structures, and a target language generator. Here
the flow of the Japanese to English translation is
shown with the following example sentence (3).
4For example, the interrogative sentence ?Would you read
it?? implies a request.
kare hon ki
iru
watashi
ha wo ni
no
Figure 4: The Japanese syntactic tree for the sen-
tence (3).
Kare-ha watashi-no
He-TOPIC I-GEN
hon-wo ki-ni iru.
book-ACC mind-DAT enter
?He likes my book.?
(3)
First the syntactic parser parses the sentence (3)
to create the tree structure as shown in Figure 4.
Next, the transfer converts this Japanese parse
tree into an English one by applying the translation
patterns as in Figure 5. A translation pattern con-
sists of a tree of the source language, a tree of the
target language, and the word correspondences be-
tween both languages.
The patterns (a) and (b) in Figure 5 match with
the subtrees in Figure 4, as Figure 6 illustrates.
This matching operation is very complicated because
there can be an enormous number of possible combi-
nations of patterns. The fitness of the pattern com-
binations is calculated according to the similarity of
the source tree and the left side of the translation
pattern, the specificity of the translation pattern,
and so on. This example also shows the process
of matching the Japanese case markers (postposi-
tional particles). The source tree and the pattern
(a) match even though the postpositional particles
are different (?ha? and ?ga?). This process may be
much more complicated when a verb is transformed
into special forms e.g. passive or causative. Besides
this there are many operations to handle syntactic
and semantic phenomena, but here we take them for
granted because of space constraints.
Now the target fragments have been created as in
Figure 6, using the right side of the matched trans-
lation patterns as in Figure 5. The two fragments
are attached at the shared node ? noun2 ?, and lexi-
calized by using the bilingual lexicon. Finally the
target sentence ?He likes my book.? is generated by
the target language generator.
iru
noun noun ki
ga wo ni
like
noun noun
SUBJ OBJ
(a)
noun
no
watashi
noun
my
(b)
Figure 5: Two examples of Japanese-English trans-
lation patterns. The left side and the right side are
Japanese and English syntactic trees, respectively.
The ? noun ? works as a wildcard which matches with
any noun. Curves stand for correspondences be-
tween Japanese and English words.
kare hon ki
iru
watashi
ha wo ni
no
(a)
(b)
like
noun1 noun2
SUBJ OBJ
noun2
my
Figure 6: Transferring the Japanese tree in Figure 4
into the English tree. The patterns in Figure 5 create
two English fragments, and they are attached at the
nodes ? noun2 ? which share the same correspondent
node in the source language tree.
4.2 Techniques Required for Sentiment
Analysis
Our aim is to extract sentiment units with high pre-
cision. Moreover, the set of arguments of each pred-
icate should be selected necessarily and sufficiently.
Here we show that the techniques to meet these re-
quirements are analogous to the techniques for ma-
chine translation which have been reviewed in Sec-
tion 4.1.
4.2.1 Full parsing and top-down tree
matching
Full syntactic parsing plays an important role to ex-
tract sentiments correctly, because the local struc-
tures obtained by a shallow parser are not always
reliable. For example, expressions such as ?I don?t
think X is good?, ?I hope that X is good? are not fa-
vorable opinions about X, even though ?X is good?
appears on the surface. Therefore we use top-down
pattern matching on the tree structures from the full
parsing in order to find each sentiment fragment,
that is potentially a part of a sentiment unit.
In our method, initially the top node is examined
to see whether or not the node and its combination
of children nodes match with one of the patterns
in the pattern repository. In this top-down manner,
the nodes ?don?t think? and ?hope? in the above ex-
amples are examined before ?X is good?, and thus
the above expressions won?t be misunderstood to ex-
press favorable sentiments.
There are three types of patterns: principal pat-
terns, auxiliary patterns, and nominal patterns. Fig-
ure 7 illustrates examples of principal patterns: the
noun
warui
ga [unf]
bad ? noun ?
(c)
noun
iru
ki
wo ni [fav]
like ? noun ?
(d)
Figure 7: Examples of principal patterns.
declinable
to
omowa
nai
unit
+neg
(e)
declinable
monono
declinable
unit
unit
(f)
Figure 8: Examples of auxiliary patterns.
? declinable ? denotes a verb or an adjective in
Japanese. Note that the two unit s on the right side
of (f) are not connected. This means two separated
sentiment units can be obtained.
pattern (c) converts a Japanese expression ? noun -
ga warui? to a sentiment unit ?[unf] bad ? noun ??.
The pattern (d) converts an expression ? noun -wo
ki-ni iru? to a sentiment unit ?[fav] like ? noun ??,
where the subject (the noun preceding the postpo-
sitional ga) is excluded from the arguments because
the subject of ?like? is usually the author, who is not
the target of sentiment analysis.
Another type is the auxiliary pattern, which ex-
pands the scope of matching. Figure 8 has two
examples. The pattern (e) matches with phrases
such as ?X-wa yoi-to omowa-nai. ((I) don?t think
X is good.)? and produces a sentiment unit with the
negation feature. When this pattern is attached to
a principal pattern, its favorability is inverted. The
pattern (f) allows us to obtain two separate senti-
ment units from sentences such as ?Dezain-ga warui-
monono, sousasei-ha yoi. (The design is bad, but the
usability is good.)?.
4.2.2 Informative noun phrase
The third type of pattern is a nominal pattern. Fig-
ure 9 shows three examples. The pattern (g) is used
to avoid a formal noun (nominalizer) being an argu-
ment. Using this pattern, from the sentence ?Kawaii
no-ga suki-da. ((I) like pretty things)?, ?[fav] like
? pretty ?? can be extracted instead of ?[fav] like
? thing ??. The pattern (h) is used to convert a
noun phrase ?renzu-no shitsu (quality of the lens)?
into just ?lens?. Due to this operation, from Sen-
tence (4), an informative sentiment unit (4a) can be
obtained instead of a less informative one (4b).
Renzu-no shitsu-ga yoi.
lens-GEN quality-NOM good
?The quality of the lens is good.?
(4)
[fav] good ? lens ? (4a)
? [fav] good ? quality ? (4b)
adj
no
adj
(g)
noun
no
shitsu
noun
(h)
noun
noun
noun
noun
(i)
Figure 9: Examples of nominal patterns.
The pattern (i) is for compound nouns such as
?juuden jikan (recharging time)?. A sentiment
unit ?long ? time ?? is not informative, but ?long
? recharging time ?? can be regarded as a [unf] sen-
timent.
4.2.3 Disambiguation of sentiment polarity
Some adjectives and verbs may be used for both fa-
vorable and unfavorable predicates. This variation
of sentiment polarity can be disambiguated natu-
rally in the same manner as the word sense dis-
ambiguation in machine translation. The adjective
?takai (high)? is a typical example, as in (5a) and
(5b). In this case the sentiment polarity depends on
the noun preceding the postpositional particle ?ga?:
favorable if the noun is ?kaizoudo (resolution)?, unfa-
vorable if the noun is a product name. The semantic
category assigned to a noun holds the information
used for this type of disambiguation.
Kaizoudo-ga takai.
resolution-NOM high
?The resolution is high.?
? [fav] (5a)
ABC123-ga takai.
ABC123-NOM high (price)
?ABC123 is expensive.?
? [unf] (5b)
4.2.4 Aggregation of synonymous
expressions
In contrast to disambiguation, aggregation of syn-
onymous expressions is important to organize ex-
tracted sentiment units. If the different expressions
which convey the same (or similar) meanings are
aggregated into a canonical one, the frequency in-
creases and one can easily find frequently mentioned
opinions.
Using the translation architecture, any forms can
be chosen as the predicates and arguments by ad-
justing the patterns and lexicons. That is, monolin-
gual word translation is done in our method.
4.3 Resources for Sentiment Analysis
We prepared the following resources for sentiment
analysis:
Principal patterns: The verbal and adjectival
patterns for machine translation were converted
to principal patterns for sentiment analysis.
The left sides of the patterns are compatible
with the source language parts of the original
patterns, so we just assigned a sentiment po-
larity to each word. A total of 3752 principal
patterns were created.
Auxiliary/Nominal patterns: A total of 95 aux-
iliary patterns and 36 nominal patterns were
created manually.
Polarity lexicon: Some nouns were assigned sen-
timent polarity, e.g. [unf] for ?noise?. This po-
larity is used in expressions such as ?... ga ooi.
(There are many ...)?. This lexicon is also used
for the aggregation of words.
Some patterns and lexicons are domain-
dependent. The situation is the same as in
machine translation. Fortunately the translation
engine used here has a function to selectively use
domain-dependent dictionaries, and thus we can
prepare patterns which are especially suited for the
messages on bulletin boards, or for the domain of
digital cameras. For example, ?The size is small.?
is a desirable feature of a digital camera. We can
assign the appropriate sentiment (in this case, [fav])
by using a domain-specific principal pattern.
5 Evaluation
We conducted two experiments on the extraction of
sentiment units from bulletin boards on the WWW
that are discussing digital cameras. A total of 200
randomly selected sentences were analyzed by our
system. The resources were created by looking at
other parts of the same domain texts, and therefore
this experiment is an open test.
Experiment 1 measured the precision of the sen-
timent polarity, and Experiment 2 evaluated the in-
formativeness of the sentiment units. In this section
we handled only the sentiments [fav] and [unf] senti-
ments, thus the other two sentiments [qst] and [req]
were not evaluated.
5.1 Experiment 1: Precision and Recall
In order to see the reliability of the extracted sen-
timent polarities, we evaluated the following three
metrics:
Weak precision: The coincidence rate of the senti-
ment polarity between the system?s output and
manual output when both the system and the
human evaluators assigned either a favorable or
unfavorable sentiment.
Strong precision: The coincidence rate of the sen-
timent polarity between the system?s output
and manual output when the system assigned
either a favorable or unfavorable sentiment.
Recall: The detection rate of sentiment units
within the manual output.
These metrics are measured by using two meth-
ods: (A) our proposed method based on the machine
translation engine, and (B) the lexicon-only method,
which emulates the shallow parsing approach. The
latter method used the simple polarity lexicon of ad-
jectives and verbs, where an adjective or a verb had
only one sentiment polarity, then no disambigua-
tion was done. Except for the direct negation of
(A) MT (B) Lexicon only
Weak prec. 100% (31/31) 80% (41/51)
Strong prec. 89% (31/35) 44% (41/93)
Recall 43% (31/72) 57% (41/72)
Table 1: Precision and recall for the extraction of
sentiment units from 200 sentences.
(A) MT
Manual
f n u
f 20 3 0
Sy
ste
m
n 27 - 14
u 0 1 11
(B) Lexicon only
Manual
f n u
f 26 19 6
Sy
ste
m
n 14 - 7
u 4 23 15
Table 2: The breakdown of the results of Experi-
ment 1. The columns and rows show the manual
output and the system output, respectively (f: favor-
able, n: non-sentiment, u: unfavorable). The sum of
the bold numbers equals the numerators of the pre-
cision and recall.
an adjective or a verb5, no translation patterns were
used. Instead of the top-down pattern matching,
sentiment units were extracted from any part of the
tree structures (the results of full-parsing were used
also here).
Table 1 shows the results. With the MT frame-
work, the weak precision was perfect, and also the
strong precision was much higher, while the recall
was lower than for the lexicon-only method. Their
breakdowns in the two parts of Table 2 indicate that
most of errors where the system wrongly assigned
either of sentiments (i.e. human regarded an expres-
sion as non-sentiment) have been reduced with the
MT framework.
All of the above results are consistent with intu-
ition. The MT method outputs a sentiment unit
only when the expression is reachable from the root
node of the syntactic tree through the combina-
tion of sentiment fragments, while the lexicon-only
method picks up sentiment units from any node in
the syntactic tree. The sentence (6) is an exam-
ple where the lexicon-only method output the wrong
sentiment unit (6a). The MT method did not out-
put this sentiment unit, thus the precision values of
the MT method did not suffer from this example.
... gashitsu-ga kirei-da-to iu hyouka-ha
uke-masen-deshi-ta. (6)
?There was no opinion that the picture was sharp.?
? [fav] clear ? picture ? (6a)
In the lexicon-only method, some errors occurred
due to the ambiguity in sentiment polarity of an ad-
jective or a verb, e.g. ?Kanousei-ga takai. (Capa-
bilities are high.)? since ?takai (high/expensive)? is
always assigned the [unf] feature.
5?He doesn?t like it.? is regarded as negation, but ?I don?t
think it is good.? is not.
declinable
noun noun noun
ga wo ni
Figure 10: A na??ve predicate-argument structure
used by the system (C). Nouns preceding three ma-
jor postpositional particles ?ga?, ?wo?, and ?ni? are
supported as the slots of arguments. On the other
hand, in the system (A), there are over 3,000 prin-
cipal patterns that have information on appropriate
combinations for each verb and adjective.
(A) MT (C) Na??ve
Less redundant 2/35 0/35
More informative 13/35 1/35
Both 1/35 0/35
Table 3: Comparison of scope of sentiment units.
The numbers mean the counts of the better output
for each system among 35 sentiment units. The re-
mainder is the outputs that were the same in both
systems.
The recall was not so high, especially in the MT
method, but according to our error analysis the re-
call can be increased by adding auxiliary patterns.
On the other hand, it is almost impossible to increase
the precision without our deep analysis techniques.
Consequently, our proposed method outperforms the
shallow (lexicon-only) approach.
5.2 Experiment 2: Scope of Sentiment Unit
We also compared the appropriateness of the scope
of the extracted sentiment units between (A) the
proposed method with the MT framework and
(C) a method that supports only na??ve predicate-
argument structures as in Figure 10 and doesn?t use
any nominal patterns.
According to the results shown in Table 3, the MT
method produced less redundant or more informa-
tive sentiment units than did relying on the na??ve
predicate-argument structures in about half of the
cases among the 35 extracted sentiment units.
The following example (7) is a case where the sen-
timent unit output by the MT method (7a) was less
redundant than that output by the na??ve method
(7b). The translation engine understood that the
phrase ?kyonen-no 5-gatsu-ni (last May)? held tem-
poral information, therefore it was excluded from
the arguments of the predicate ?enhance?, while both
?function? and ?May? were the arguments of ?enhance?
in (7b). Apparently the argument ?May? is not nec-
essary here.
... kyonen-no 5-gatsu-ni kinou-ga
kairyou-sare-ta you-desu. (7)
?It seems the function was enhanced last May.?
[fav] enhance ? function ? (7a)
? [fav] enhance ? function, May ? (7b)
Example (8) is another case where the sentiment
unit output by the MT method (8a) was more infor-
mative than that output by the na??ve method (8b).
Than the Japanese functional noun ?hou?, its modi-
fier ?zoom? was more informative. The MT method
successfully selected the noun ?zoom? as the argu-
ment of ?desirable?.
... zuum-no hou-ga nozomashii. (8)
?A zoom is more desirable.?
[fav] desirable ? zoom ? (8a)
? [fav] desirable ? hou ? (8b)
The only one case we encountered where the
MT method extracted a less informative sentiment
unit was the sentence ?Botan-ga satsuei-ni pittari-
desu (The shutter is suitable for taking photos)?.
The na??ve method could produce the sentiment unit
?[fav] suitable ? shutter, photo ??, but the MT
method created ?[fav] suitable ? shutter ??. This is
due to the lack of a noun phrase preceding the post-
positional particle ?ni? in the principal pattern. Such
problems can be avoided by modifying the patterns,
and thus the effect of the combination of patterns
for SA has been shown here.
6 Conclusion
This paper has proposed a new approach to senti-
ment analysis: the translation from text to a set of
semantic fragments. We have shown that the deep
syntactic and semantic analysis makes possible the
reliable extraction of sentiment units, and the out-
lining of sentiments became useful because of the
aggregation of the variations in expressions, and the
informative outputs of the arguments. The experi-
mental results have shown that the precision of the
sentiment polarity was much higher than for the con-
ventional methods, and the sentiment units created
by our system were less redundant and more infor-
mative than when using na??ve predicate-argument
structures. Even though we exploited many advan-
tages of deep analysis, we could create a sentiment
analysis system at a very low development cost, be-
cause many of the techniques for machine translation
can be reused naturally when we regard the extrac-
tion of sentiment units as a kind of translation.
Many techniques which have been studied for the
purpose of machine translation, such as word sense
disambiguation (Dagan and Itai, 1994; Yarowsky,
1995), anaphora resolution (Mitamura et al, 2002),
and automatic pattern extraction from corpora
(Watanabe et al, 2003), can accelerate the further
enhancement of sentiment analysis, or other NLP
tasks. Therefore this work is the first step towards
the integration of shallow and wide NLP, with deep
NLP.
References
Ido Dagan and Alon Itai. 1994. Word sense dis-
ambiguation using a second language monolingual
corpus. Computational Linguistics, 20(4):563?
596.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation
of adjectives. In Proceedings of the 35th Annual
Meeting of the ACL and the 8th Conference of the
European Chapter of the ACL, pages 174?181.
Marti A. Hearst. 1999. Untangling text data min-
ing. In Proc. of the 37th Annual Meeting of the
Association for Computational Linguistics.
Teruko Mitamura, Eric Nyberg, Enrique Torrejon,
Dave Svoboda, Annelen Brunner, and Kathryn
Baker. 2002. Pronominal anaphora resolution in
the kantoo multilingual machine translation sys-
tem. In Proc. of TMI 2002, pages 115?124.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining product
reputations on the web. In Proc. of the 8th ACM
SIGKDD Conference.
Tetsuya Nasukawa and Tohru Nagano. 2001. Text
analysis and knowledge mining system. IBM Sys-
tems Journal, 40(4):967?984.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using nat-
ural language processing. In Proc. of the Second
International Conferences on Knowledge Capture,
pages 70?77.
Bo Pang, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Sentiment
classification using machine learning techniques.
In Proceedings of the 2002 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 79?86.
Pero Subasic and Alison Huettner. 2001. Affect
analysis of text using fuzzy semantic typing. IEEE
Trans. on Fussy Systems.
Peter D. Turney. 2002. Thumbs up or thumbs
down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proc. of the 40th
ACL Conf., pages 417?424.
Hideo Watanabe, Sadao Kurohashi, and Eiji Ara-
maki. 2003. Finding translation patterns from de-
pendency structures. In Michael Carl and Andy
Way, editors, Recent Advances in Example-based
Machine Translation, pages 397?420. Kluwer Aca-
demic Publishers.
Hideo Watanabe. 1992. A similarity-driven trans-
fer system. In Proc. of the 14th COLING, Vol. 2,
pages 770?776.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Meeting of the Association for Computational
Linguistics, pages 189?196.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In Pro-
ceedings of the Third IEEE International Confer-
ence on Data Mining, pages 427?434.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 409?416
Manchester, August 2008
Textual Demand Analysis:
Detection of Users? Wants and Needs from Opinions
Hiroshi Kanayama Tetsuya Nasukawa
Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
{hkana,nasukawa}@jp.ibm.com
Abstract
This paper tackles textual demand analy-
sis, the task of capturing what people want
or need, rather than identifying what they
like or dislike, on which much conven-
tional work has focused. It exploits syn-
tactic patterns as clues to detect previously
unknown demands, and requires domain-
dependent knowledge to get high recall. To
build such patterns we created an unsuper-
vised pattern induction method relying on
the hypothesis that there are commonly de-
sired aspects throughout a domain corpus.
Experimental results show that the pro-
posed method detects twice to four times
as many demand expressions in Japanese
discussion forums compared to a baseline
method.
1 Introduction
Increasingly we can access many opinions towards
products, services, or companies through elec-
tronic documents including questionnaires, call
logs, and other consumer-generated media (CGM)
such as Internet discussion forums and blogs. It is
very important for companies to get insights from
their customers? opinions by analyzing such docu-
ments in large numbers.
The most popular way to utilize such data has
involved sentiment analysis (SA) (Nasukawa and
Yi, 2003; Yi et al, 2003), which is the task of rec-
ognizing the writers? feelings as expressed in pos-
itive or negative comments. Typically, a SA sys-
tem focuses on expressions to identify the strong
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
or weak points of the subjects as in (1) or in the
writers? evaluations as in (2).
(1) I think the pictures are beautiful.
(2) I don?t like this camera very much.
Here we call them polar expressions because
they convey positive or negative polarities. By
counting the polar expressions related to products
or services, one can quantitatively compare the
goodness of competing services, find the draw-
backs of specific products, and so on.
In addition to polar expressions, there are other
types of expressions that provide valuable infor-
mation, especially for the supplier side rather than
the consumer side. Examples (3) and (4) express
the demands of the writers.
(3) I?d be happy if it is equipped with
a crisp LCD.
(4) I?m waiting for a single-lens reflex less
than 30,000 yen to come on the market.
We call such expressions ?demand expres-
sions?, and the underlined phrases ?demand tar-
gets.?
While sentiment analysis reveals evaluations of
existing products or services, the task proposed
here, textual demand analysis
1
, gives more direct
suggestions to companies: things they should do
to attract customers. For example, by investigating
demand targets, companies can add new functions
to products on the market or plan new services to
satisfy customers. These activities should lead to
positive evaluations in the future.
Interestingly, demand expressions may be noise
in sentiment analysis, because the demand ex-
pressions do not actually convey positive or neg-
1
Note that textual demand analysis is different from the
demand analysis in the field of marketing or software engi-
neering.
409
Consumers
Company
Opinions
?
Textual
Demand
Analysis
?
......
......
......
......
Demands Outliner
Syntactic
Patterns
Pattern
Extraction
Frequent
Demand
Instances
-
ff
??
?
Pattern Induction
Figure 1: A demand analysis system and the flow
of the pattern induction method.
ative evaluations of existing products or services,
even though these demand expressions often con-
tain positive or negative words, as in Example (3)
which contains the positive expressions ?happy?
and ?crisp LCD?.
The detection of novel demand targets requires
deep syntactic information because such demand
targets themselves can not be predefined. For ex-
ample, to regard the underlined parts of (3) and
(4) as demand targets, the non-underlined parts of
these sentences have to be properly interpreted as
triggers. This is a major difference from sentiment
analysis where the polar expressions can be de-
fined in the lexicon.
The left parts of Figure 1 illustrate the concepts
of a system that visualizes the users? demands de-
scribed in the input opinion data, where the main
analysis component processes the documents and
extracts the demand targets. The output of the sys-
tem is created by a demand outliner, which the
company uses to grasp the trends of consumers?
demands.
The syntactic patterns that can be used as clues
to demand expressions depend on the topic domain
or the writing style. To organize this linguistic
knowledge we propose an unsupervised induction
method for syntactic patterns. The right part of
Figure 1 shows the flow of pattern induction.
In the next section, we review related work, and
Section 3 defines our task more formally. In Sec-
tion 4 we describe a naive approach to the task and
Section 5 shows a form of unsupervised pattern in-
duction used to cover more demand expressions.
Section 6 gives the experimental results and we
conclude in Section 7.
2 Related Work
Sentiment analysis (SA) and related topics have
been extensively studied in recent years. The tex-
tual demand analysis proposed in this paper shares
some properties with phrase-level SA, the detec-
tion of sentiments and evaluations expressed in
phrases, rather than document-level SA, the clas-
sification of documents in terms of goodness of
reputation. Yi et al (2003) pointed out that the
multiple sentiment aspects in a document should
be extracted, and Nasukawa and Yi (2003) clar-
ified the need for deep syntactic analysis for the
phrase-level SA.
The acquisition of clues is a key technology in
these research efforts, as seen in learning meth-
ods for document-level SA (Hatzivassiloglou and
McKeown, 1997; Turney, 2002) and for phrase-
level SA (Wilson et al, 2005; Kanayama and Na-
sukawa, 2006).
As well as the sentiment expressions leading to
evaluations, there are many semantic aspects to be
extracted from documents which contain writers?
opinions, such as subjectivity (Wiebe and Mihal-
cea, 2006), comparative sentences (Jindal and Liu,
2006), or predictive expressions (Kim and Hovy,
2007). However, the extraction of the contents of
writers? demands which this paper handles is less
studied while this type of information is very valu-
able for commercial applications.
For the tasks of information extraction and re-
lation extraction, bootstrapping approaches have
been proven successful (Yangarber, 2003; Pantel
and Pennacchiotti, 2006). The pattern induction
method in this paper exploits their ideas, but their
application to the demand detection is not trivial,
because some instances of demands are previously
unknown and do not appear frequently, so they
have to be abstracted effectively.
The work by Inui et al (2003) handles seman-
tics of a type similar to ours. They aimed to detect
the requests in the responses to open-ended ques-
tionnaires, seeking direct requests such as ?...
? (?[I] would like you to ...?) and other forms
which can be paraphrased as direct requests. They
classified sentences into requests or non-requests,
where their source documents were responses to
questionnaires, and where more than 60% of the
utterances could be regarded as requests of some
sort. In contrast, our method detects the content
of the demands in the form of noun phrases, and
handles more general target documents including
410
CGM documents that contain much more diverse
utterances.
3 Task Definition
As shown in Section 1, our goal is to create a sys-
tem to enumerate in an easily understandable way
the demand targets in the input text. This section
describes the definition of a demand target and its
representation format.
3.1 Demand targets
Demands or requests written in opinion texts can
be represented by verb phrases as in e.g. ?I want
to V.? and ?I want you to V.?, or noun phrases
as in ?I want N.?
2
In this paper we focus on the
last type, i.e. noun phrases which represent desired
objects, because they are easier to aggregate and
grasp than verb phrases. Another reason is that
some demands represented with a verb phrase only
describe the objects that are desired. For example,
?I want to buy N? and ?I want you to provide N? can
be simply interpreted as meaning that N is what the
writer wants. We call such a noun phrase a demand
target, and these are the outputs of our system.
For applications, the demand targets to be de-
tected by the system depend on the type of in-
put documents. For example, from a consumers?
forum on digital cameras, the underlined parts in
Examples (3) and (4) from Section 1 apparently
describe the writer?s demands, so they are valu-
able information for such users of demand anal-
ysis such as the makers of digital camera. How-
ever, the request in Example (5) does not express
the author?s demands about any digital camera, but
rather it is written for other participants in the fo-
rum. This type should be excluded from the out-
put.
(5) Please give me a good advice.
In contrast, when the responses to a question-
naire about the activities of an organization are
processed, statements such as Example (5) should
be regarded as a demand target, since the writer
wrote it as a request to the sponsor of the question-
naire and the ?advice? is indeed a thing that can be
provided by the sponsoring organization.
3.2 Representation of demand targets
A demand target tends to be expressed by a noun
phrase with modifiers, as seen in Examples (3) and
2
?V? and ?N? indicate a verb phrase and a noun phrase,
respectively.
7
?happy?

?if?
!
?equip?


-NOM
2J
?LCD?
?
db
?crisp?

?standpoint?

F
?I?

?want?
H
?sell?
#
-ACC
-)
?reflex?
?
T!
?can buy?

?with?
30,0003
?-yen?
15
?single-lens?
Figure 2: Syntactic trees for the sentences (6) and
(7). ?*? indicates the headword of the demand tar-
gets.
(4), rather than by a single noun. The headwords
of such phrases (e.g. ?LCD? in (3) and ?reflex? in
(4)) represent the main categories of the demanded
objects, but they are not distinctive enough to rec-
ognize as knowledge of the authors? demands.
Therefore the key task of this research was to
find ways to markup the headword of a noun
phrase that represents the content of a demand in
the syntactic parse tree. Examples (6) and (7) are
the original Japanese sentences corresponding to
Examples (3) and (4).
(6) Fdb2J
!7
?I?d be happy if it is equipped with a crisp LCD.?
(7) Z3T!15-)#H
?[I?m] waiting for a single-lens reflex
less than 30,000 yen to come on the market.?
Figure 2 represents the parse trees correspond-
ing to sentences (6) and (7), where the demand tar-
gets are identified by the mark ?*?.
This simple representation is advantageous for
both the collection of and the deeper investigation
of the demand targets. One can easily grasp the
content of a demand if the application shows the
whole surface structure of the subtree under ?*? in
the tree, e.g. the underlined parts of Examples (6)
and (7). At the same time the tree structure sup-
ports the further analysis of the trends of the de-
mands by picking up the headwords or modifiers
prominent in the subtrees that were detected as de-
mand targets.
4 Baseline Method of Textual Demand
Analysis
This section describes an algorithm to extract de-
mand targets with high precision and describes
a preliminary experiment to measure the perfor-
mance.
411
N?


-ACC
]
?want?
(a)
V

?that?
E
?think?
(b)
V

?though?
V
(c)
Figure 3: (a) is a demand pattern which indicates
that the noun in N
?
is detected as a demand target.
(b) and (c) are auxiliary patterns, where V indicates
the node matches any verb.
4.1 Syntactic patterns and top-down
matching
A major purpose of textual demand analysis is
to discover novel demands embedded in the text,
thus the triggers of their detection should not be
a predefined set of demand targets but should be
their surrounding syntactic information. We use
two types of syntactic patterns shown in Figure 3.
Those patterns are compared with the syntactic
tree as the parsing result of the input sentence.
The pattern (a) in Figure 3 is a demand pattern,
which is used to search for demand targets. The
node with the ??? mark indicates the correspond-
ing node will be the headword of a demand tar-
get. Hence we write the pattern (a) as ?N
?
-
-]
? for simplicity. The patterns are applied in
a top-down manner, that is, initially the top node
of the input tree is examined to see whether or
not the node and its combination of children nodes
match with one of the patterns in the pattern repos-
itory. This method supports higher precision in the
detection than the surface pattern matching. For
example, the expression ?WaV
]K
? (?There is no one who wants low quality
goods?) should not be misunderstood to express a
demand.
The patterns (b) and (c) in Figure 3 are auxil-
iary patterns. These are used to apply the demand
patterns to nodes other than the root of the syn-
tactic tree. For example, by applying the patterns
(b) and (c), the pattern (a) can then be applied to
the expressions ?N
]E? (?I
think that I want N?) and ?N
]	
;
O ? (?Though I want N, I don?t have
enough money?), respectively, even though ?N
?
-

-]? doesn?t appear at the top of the trees.
In other words, the auxiliary patterns contribute to
generate variations of the demand patterns.
In addition, simple rules can be applied to fil-
ter out certain meaningless outputs. When a noun
phrase that matched to the ??? part of the demand
Table 1: The result on the small gold standard with
DP
1
. PM signifies surface pattern matching, TM
signifies tree matching. ?+AP? means that auxil-
iary patterns are used.
Method Precision Recall
PM 39% (14/36) 25% (14/56)
TM 92% (11/12) 20% (11/56)
TM+AP 94% (17/18) 30% (17/56)
pattern was a pronoun or very common noun (e.g.
?camera? in the camera domain) without any mod-
ifier, it is not output as a demand target.
4.2 Preliminary experiment
We conducted a preliminary experiment to assess
the feasibility of our approach.
We prepared a small gold-standard dataset
which consists of 1,152 sentences from a discus-
sion forum on digital cameras, for which two hu-
man annotators attached marks to the demand tar-
gets. There were 56 demand targets that at least
one of the annotators detected, and the sentence-
level agreement value
3
was ? = 0.73, which is
regarded as a good level of agreement. There was
no sentence in which the two annotators attached
marks to different nouns.
First, we made a minimum set of demand pat-
terns DP
1
, which contained only one basic pattern
?N
?
-
-]4? (?I want N??).
To see the effect of the top-down matching and
the auxiliary patterns described in Section 4.1,
demand targets in the gold-standard corpus were
automatically detected using three methods: pat-
tern matching with surface strings like ?
]
? (PM), tree matching without the auxiliary pat-
terns (TM), and tree matching with the auxiliary
patterns
5
(TM+AP).
Table 1 shows the results. The top-down match-
ing on the syntactic tree resulted in much higher
precision than the surface pattern matching, and
the auxiliary patterns improved the recall. The
only misdetection in the tree matching method was
due to an error in the sentence segmentation.
However, all of them show low recall values,
3
The agreement on whether or not the sentence has a de-
mand target.
4
Apparent character variations like ?]? and ??,
and alternative forms of particles were aggregated in the pars-
ing process.
5
A total of 95 auxiliary patterns which Kanayama et al
(2004) used for the sentiment analysis.
412
Table 2: The list of augmented demand patterns
DP
q
.
N
?
-
-] (I want N?), N?-#-Y (I hope N?),
N
?
-#-	6-! (Please [give] N?), N?-#-6
(I wish N
?
), N
?
-#--4 (Please do N?),
N
?
-#-^ (I ask [you] N?), N?-
-!-Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 355?363,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Fully Automatic Lexicon Expansion
for Domain-oriented Sentiment Analysis
Hiroshi Kanayama Tetsuya Nasukawa
Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
{hkana,nasukawa}@jp.ibm.com
Abstract
This paper proposes an unsupervised
lexicon building method for the detec-
tion of polar clauses, which convey pos-
itive or negative aspects in a specific
domain. The lexical entries to be ac-
quired are called polar atoms, the min-
imum human-understandable syntactic
structures that specify the polarity of
clauses. As a clue to obtain candidate
polar atoms, we use context coherency,
the tendency for same polarities to ap-
pear successively in contexts. Using
the overall density and precision of co-
herency in the corpus, the statistical
estimation picks up appropriate polar
atoms among candidates, without any
manual tuning of the threshold values.
The experimental results show that the
precision of polarity assignment with
the automatically acquired lexicon was
94% on average, and our method is ro-
bust for corpora in diverse domains and
for the size of the initial lexicon.
1 Introduction
Sentiment Analysis (SA) (Nasukawa and Yi,
2003; Yi et al, 2003) is a task to recognize
writers? feelings as expressed in positive or
negative comments, by analyzing unreadably
large numbers of documents. Extensive syn-
tactic patterns enable us to detect sentiment
expressions and to convert them into seman-
tic structures with high precision, as reported
by Kanayama et al (2004). From the exam-
ple Japanese sentence (1) in the digital cam-
era domain, the SA system extracts a senti-
ment representation as (2), which consists of
a predicate and an argument with positive (+)
polarity.
(1) Kono kamera-ha subarashii-to omou.
?I think this camera is splendid.?
(2) [+] splendid(camera)
SA in general tends to focus on subjec-
tive sentiment expressions, which explicitly de-
scribe an author?s preference as in the above
example (1). Objective (or factual) expres-
sions such as in the following examples (3) and
(4) may be out of scope even though they de-
scribe desirable aspects in a specific domain.
However, when customers or corporate users
use SA system for their commercial activities,
such domain-specific expressions have a more
important role, since they convey strong or
weak points of the product more directly, and
may influence their choice to purchase a spe-
cific product, as an example.
(3) Kontorasuto-ga kukkiri-suru.
?The contrast is sharp.?
(4) Atarashii kishu-ha zuumu-mo tsuite-iru.
?The new model has a zoom lens, too.?
This paper addresses the Japanese ver-
sion of Domain-oriented Sentiment Analysis,
which identifies polar clauses conveying good-
ness and badness in a specific domain, in-
cluding rather objective expressions. Building
domain-dependent lexicons for many domains
is much harder work than preparing domain-
independent lexicons and syntactic patterns,
because the possible lexical entries are too
numerous, and they may differ in each do-
main. To solve this problem, we have devised
an unsupervised method to acquire domain-
dependent lexical knowledge where a user has
only to collect unannotated domain corpora.
The knowledge to be acquired is a domain-
dependent set of polar atoms. A polar atom is
a minimum syntactic structure specifying po-
larity in a predicative expression. For exam-
ple, to detect polar clauses in the sentences (3)
355
and (4)1, the following polar atoms (5) and (6)
should appear in the lexicon:
(5) [+] kukkiri-suru
?to be sharp?
(6) [+] tsuku ? zuumu-ga
?to have ? zoom lens-NOM?
The polar atom (5) specified the positive po-
larity of the verb kukkiri-suru. This atom can
be generally used for this verb regardless of
its arguments. In the polar atom (6), on the
other hand, the nominative case of the verb
tsuku (?have?) is limited to a specific noun zu-
umu (?zoom lens?), since the verb tsuku does
not hold the polarity in itself. The automatic
decision for the scopes of the atoms is one of
the major issues.
For lexical learning from unannotated cor-
pora, our method uses context coherency in
terms of polarity, an assumption that polar
clauses with the same polarity appear suc-
cessively unless the context is changed with
adversative expressions. Exploiting this ten-
dency, we can collect candidate polar atoms
with their tentative polarities as those adja-
cent to the polar clauses which have been
identified by their domain-independent polar
atoms in the initial lexicon. We use both intra-
sentential and inter-sentential contexts to ob-
tain more candidate polar atoms.
Our assumption is intuitively reasonable,
but there are many non-polar (neutral) clauses
adjacent to polar clauses. Errors in sentence
delimitation or syntactic parsing also result in
false candidate atoms. Thus, to adopt a can-
didate polar atom for the new lexicon, some
threshold values for the frequencies or ratios
are required, but they depend on the type of
the corpus, the size of the initial lexicon, etc.
Our algorithm is fully automatic in the
sense that the criteria for the adoption of po-
lar atoms are set automatically by statistical
estimation based on the distributions of co-
herency: coherent precision and coherent den-
sity. No manual tuning process is required,
so the algorithm only needs unannotated do-
main corpora and the initial lexicon. Thus
our learning method can be used not only by
the developers of the system, but also by end-
users. This feature is very helpful for users to
1The English translations are included only for con-
venience.
analyze documents in new domains.
In the next section, we review related work,
and Section 3 describes our runtime SA sys-
tem. In Section 4, our assumption for unsu-
pervised learning, context coherency and its
key metrics, coherent precision and coherent
density are discussed. Section 5 describes our
unsupervised learning method. Experimental
results are shown in Section 6, and we conclude
in Section 7.
2 Related Work
Sentiment analysis has been extensively stud-
ied in recent years. The target of SA in this
paper is wider than in previous work. For ex-
ample, Yu and Hatzivassiloglou (2003) sepa-
rated facts from opinions and assigned polari-
ties only to opinions. In contrast, our system
detects factual polar clauses as well as senti-
ments.
Unsupervised learning for sentiment analy-
sis is also being studied. For example, Hatzi-
vassiloglou and McKeown (1997) labeled ad-
jectives as positive or negative, relying on se-
mantic orientation. Turney (2002) used col-
location with ?excellent? or ?poor? to obtain
positive and negative clues for document clas-
sification. In this paper, we use contextual
information which is wider than for the con-
texts they used, and address the problem of
acquiring lexical entries from the noisy clues.
Inter-sentential contexts as in our approach
were used as a clue also for subjectivity anal-
ysis (Riloff and Wiebe, 2003; Pang and Lee,
2004), which is two-fold classification into sub-
jective and objective sentences. Compared to
it, this paper solves a more difficult problem:
three-fold classification into positive, negative
and non-polar expressions using imperfect co-
herency in terms of sentiment polarity.
Learning methods for phrase-level sentiment
analysis closely share an objective of our ap-
proach. Popescu and Etzioni (2005) achieved
high-precision opinion phrases extraction by
using relaxation labeling. Their method itera-
tively assigns a polarity to a phrase, relying on
semantic orientation of co-occurring words in
specific relations in a sentence, but the scope
of semantic orientation is limited to within a
sentence. Wilson et al (2005) proposed su-
pervised learning, dividing the resources into
356
Document
to analyze -
Sentence
Delimitation ...
?
??
??
Sentences
?Proposition Detection
Propositions
Clauses
?Polarity Assignment
+
?
Polarities
Polar Clauses
Modality
Patterns
Conjunctive
Patterns
*
Polar
Atoms
-
Figure 1: The flow of the clause-level SA.
prior polarity and context polarity, which are
similar to polar atoms and syntactic patterns
in this paper, respectively. Wilson et al pre-
pared prior polarities from existing resources,
and learned the context polarities by using
prior polarities and annotated corpora. There-
fore the prerequisite data and learned data
are opposite from those in our approach. We
took the approach used in this paper because
we want to acquire more domain-dependent
knowledge, and context polarity is easier to
access in Japanese2. Our approach and their
work can complement each other.
3 Methodology of Clause-level SA
As Figure 1 illustrates, the flow of our sen-
timent analysis system involves three steps.
The first step is sentence delimitation: the in-
put document is divided into sentences. The
second step is proposition detection: proposi-
tions which can form polar clauses are identi-
fied in each sentence. The third step is polarity
assignment: the polarity of each proposition
is examined by considering the polar atoms.
This section describes the last two processes,
which are based on a deep sentiment analy-
sis method analogous to machine translation
(Kanayama et al, 2004) (hereafter ?the MT
method?).
3.1 Proposition Detection
Our basic tactic for clause-level SA is the high-
precision detection of polar clauses based on
deep syntactic analysis. ?Clause-level? means
that only predicative verbs and adjectives such
2For example, indirect negation such as caused by
a subject ?nobody? or a modifier ?seldom? is rare in
Japanese.
as in (7) are detected, and adnominal (attribu-
tive) usages of verbs and adjectives as in (8)
are ignored, because utsukushii (?beautiful?) in
(8) does not convey a positive polarity.
(7) E-ga utsukushii.
?The picture is beautiful.?
(8) Utsukushii hito-ni aitai.
?I want to meet a beautiful person.?
Here we use the notion of a proposition as a
clause without modality, led by a predicative
verb or a predicative adjective. The proposi-
tions detected from a sentence are subject to
the assignment of polarities.
Basically, we detect a proposition only at
the head of a syntactic tree3. However, this
limitation reduces the recall of sentiment anal-
ysis to a very low level. In the example (7)
above, utsukushii is the head of the tree, while
those initial clauses in (9) to (11) below are
not. In order to achieve higher recall while
maintaining high precision, we apply two types
of syntactic patterns, modality patterns and
conjunctive patterns4, to the tree structures
from the full-parsing.
(9) Sore-ha utsukushii-to omou.
?I think it is beautiful.?
(10) Sore-ha utsukushiku-nai.
?It is not beautiful.?
(11) Sore-ga utsukushii-to yoi.
?I hope it is beautiful.?
Modality patterns match some auxiliary
verbs or corresponding sentence-final expres-
sions, to allow for specific kinds of modality
and negation. One of the typical patterns is
[ v to omou] (?I think v ?)5, which allows ut-
sukushii in (9) to be a proposition. Also nega-
tion is handled with a modality pattern, such
as [ v nai] (?not v ?). In this case a neg fea-
ture is attached to the proposition to identify
utsukushii in (10) as a negated proposition.
On the other hand, no proposition is identi-
fied in (11) due to the deliberate absence of
a pattern [ v to yoi] (?I hope v ?). We used
a total of 103 domain-independent modality
patterns, most of which are derived from the
3This is same as the rightmost part of the sentence
since all Japanese modification is directed left to right.
4These two types of patterns correspond to auxil-
iary patterns in the MT method, and can be applied
independent of domains.
5 v denotes a verb or an adjective.
357
coordinative (roughly ?and?)
-te, -shi, -ueni, -dakedenaku, -nominarazu
causal (roughly ?because?)
-tame, -kara, -node
adversative (roughly ?but?)
-ga, -kedo, -keredo, - monono, -nodaga
Table 1: Japanese conjunctions used for con-
junctive patterns.
MT method, and some patterns are manually
added for this work to achieve higher recall.
Another type of pattern is conjunctive pat-
terns, which allow multiple propositions in a
sentence. We used a total of 22 conjunctive
patterns also derived from the MT method, as
exemplified in Table 1. In such cases of coordi-
native clauses and causal clauses, both clauses
can be polar clauses. On the other hand, no
proposition is identified in a conditional clause
due to the absence of corresponding conjunc-
tive patterns.
3.2 Polarity Assignment Using Polar
Atoms
To assign a polarity to each proposition, po-
lar atoms in the lexicon are compared to the
proposition. A polar atom consists of po-
larity, verb or adjective, and optionally, its
arguments. Example (12) is a simple polar
atom, where no argument is specified. This
atom matches any proposition whose head is
utsukushii. Example (13) is a complex polar
atom, which assigns a negative polarity to any
proposition whose head is the verb kaku and
where the accusative case is miryoku.
(12) [+] utsukushii
?to be beautiful?
(13) [?] kaku ? miryoku-wo
?to lack ? attraction-ACC?
A polarity is assigned if there exists a polar
atom for which verb/adjective and the argu-
ments coincide with the proposition, and oth-
erwise no polarity is assigned. The opposite
polarity of the polar atom is assigned to a
proposition which has the neg feature.
We used a total of 3,275 polar atoms, most
of which are derived from an English sentiment
lexicon (Yi et al, 2003).
According to the evaluation of the MT
method (Kanayama et al, 2004), high-
precision sentiment analysis had been achieved
using the polar atoms and patterns, where the
splendid
light have-zoom
small-LCD ? satisfied
?
high-price
Iff
?
Inter-sentential
Context
6
6
Intra-sentential
Context
Figure 2: The concept of the intra- and inter-
sentential contexts, where the polarities are
perfectly coherent. The symbol ??? denotes
the existence of an adversative conjunction.
system never took positive sentiment for neg-
ative and vice versa, and judged positive or
negative to neutral expressions in only about
10% cases. However, the recall is too low, and
most of the lexicon is for domain-independent
expressions, and thus we need more lexical en-
tries to grasp the positive and negative aspects
in a specific domain.
4 Context Coherency
This section introduces the intra- and inter-
sentential contexts in which we assume context
coherency for polarity, and describes some pre-
liminary analysis of the assumption.
4.1 Intra-sentential and
Inter-sentential Context
The identification of propositions described
in Section 3.1 clarifies our viewpoint of the
contexts. Here we consider two types of
contexts: intra-sentential context and inter-
sentential context. Figure 2 illustrates the
context coherency in a sample discourse (14),
where the polarities are perfectly coherent.
(14) Kono kamera-ha subarashii-to omou.
?I think this camera is splendid.?
Karui-shi, zuumu-mo tsuite-iru.
?It?s light and has a zoom lens.?
Ekishou-ga chiisai-kedo, manzoku-da.
?Though the LCD is small, I?m satisfied.?
Tada, nedan-ga chotto takai.
?But, the price is a little high.?
The intra-sentential context is the link be-
tween propositions in a sentence, which are
detected as coordinative or causal clauses. If
there is an adversative conjunction such as
-kedo (?but?) in the third sentence in (14), a
flag is attached to the relation, as denoted
with ??? in Figure 2. Though there are dif-
ferences in syntactic phenomena, this is sim-
358
shikashi (?however?), demo (?but?), sorenanoni
(?even though?), tadashi (?on condition that?),
dakedo (?but?), gyakuni (?on the contrary?),
tohaie (?although?), keredomo (?however?),
ippou (?on the other hand?)
Table 2: Inter-sentential adversative expres-
sions.
Domain Post. Sent. Len.
digital cameras 263,934 1,757,917 28.3
movies 163,993 637,054 31.5
mobile phones 155,130 609,072 25.3
cars 159,135 959,831 30.9
Table 3: The corpora from four domains
used in this paper. The ?Post.? and ?Sent.?
columns denote the numbers of postings and
sentences, respectively. ?Len.? is the average
length of sentences (in Japanese characters).
ilar to the semantic orientation proposed by
Hatzivassiloglou and McKeown (1997).
The inter-sentential context is the link be-
tween propositions in the main clauses of pairs
of adjacent sentences in a discourse. The po-
larities are assumed to be the same in the
inter-sentential context, unless there is an ad-
versative expression as those listed in Table 2.
If no proposition is detected as in a nominal
sentence, the context is split. That is, there is
no link between the proposition of the previous
sentence and that of the next sentence.
4.2 Preliminary Study on Context
Coherency
We claim these two types of context can be
used for unsupervised learning as clues to as-
sign a tentative polarity to unknown expres-
sions. To validate our assumption, we con-
ducted preliminary observations using various
corpora.
4.2.1 Corpora
Throughout this paper we used Japanese
corpora from discussion boards in four differ-
ent domains, whose features are shown in Ta-
ble 3. All of the corpora have clues to the
boundaries of postings, so they were suitable
to identify the discourses.
4.2.2 Coherent Precision
How strong is the coherency in the con-
text proposed in Section 4.1? Using the polar
clauses detected by the SA system with the
initial lexicon, we observed the coherent pre-
cision of domain d with lexicon L, defined as:
cp(d, L) = #(Coherent)#(Coherent)+#(Conflict) (15)
where #(Coherent) and #(Conflict) are oc-
currence counts of the same and opposite po-
larities observed between two polar clauses as
observed in the discourse. As the two polar
clauses, we consider the following types:
Window. A polar clause and the nearest po-
lar clause which is found in the preceding
n sentences in the discourse.
Context. Two polar clauses in the intra-
sentential and/or inter-sentential context
described in Section 4.1. This is the view-
point of context in our method.
Table 4 shows the frequencies of coherent
pairs, conflicting pairs, and the coherent pre-
cision for half of the digital camera domain
corpus. ?Baseline? is the percentage of posi-
tive clauses among the polar clauses6.
For the ?Window? method, we tested for
n=0, 1, 2, and ?. ?0? means two propositions
within a sentence. Apparently, the larger the
window size, the smaller the cp value. When
the window size is ???, implying anywhere
within a discourse, the ratio is larger than the
baseline by only 2.7%, and thus these types
of coherency are not reliable even though the
number of clues is relatively large.
?Context? shows the coherency of the two
types of context that we considered. The
cp values are much higher than those in the
?Window? methods, because the relationships
between adjacent pairs of clauses are handled
more appropriately by considering syntactic
trees, adversative conjunctions, etc. The cp
values for inter-sentential and intra-sentential
contexts are almost the same, and thus both
contexts can be used to obtain 2.5 times more
clues for the intra-sentential context. In the
rest of this paper we will use both contexts.
We also observed the coherent precision for
each domain corpus. The results in the cen-
ter column of Table 5 indicate the number
is slightly different among corpora, but all of
them are far from perfect coherency.
6If there is a polar clause whose polarity is unknown,
the polarity is correctly predicted with at least 57.0%
precision by assuming ?positive?.
359
Model Coherent Conflict cp(d, L)
Baseline 57.0%
Window
n = 0 3,428 1,916 64.1%
n = 1 11,448 6,865 62.5%
n = 2 16,231 10,126 61.6%
n = ? 26,365 17,831 59.7%
Context
intra. 2,583 996 72.2%
inter. 3,987 1,533 72.2%
both 6,570 2,529 72.2%
Table 4: Coherent precision with various view-
points of contexts.
Domain cp(d, L) cd(d, L)
digital cameras 72.2% 7.23%
movies 76.7% 18.71%
mobile phones 72.9% 7.31%
cars 73.4% 7.36%
Table 5: Coherent precision and coherent den-
sity for each domain.
4.2.3 Coherent Density
Besides the conflicting cases, there are many
more cases where a polar clause does not ap-
pear in the polar context. We also observed
the coherent density of the domain d with the
lexicon L defined as:
cd(d, L) = #(Coherent)#(Polar) (16)
This indicates the ratio of polar clauses that
appear in the coherent context, among all of
the polar clauses detected by the system.
The right column of Table 5 shows the co-
herent density in each domain. The movie
domain has notably higher coherent density
than the others. This indicates the sentiment
expressions are more frequently used in the
movie domain.
The next section describes the method of
our unsupervised learning using this imperfect
context coherency.
5 Unsupervised Learning for
Acquisition of Polar Atoms
Figure 3 shows the flow of our unsupervised
learning method. First, the runtime SA sys-
tem identifies the polar clauses, and the can-
didate polar atoms are collected. Then, each
candidate atom is validated using the two met-
rics in the previous section, cp and cd, which
are calculated from all of the polar clauses
found in the domain corpus.
Domain
Corpus d
-
Initial
Lexicon L
*
SA
6
Polar
Clauses
context-
?U
Candidate
Polar Atoms
f(a), p(a), n(a)
cd(d, L)
cp(d, L)
?test
6
R?
N ?
? test-
?
- ? - New
Lexicon
Figure 3: The flow of the learning process.
ID Candidate Polar Atom f(a) p(a) n(a)
1* chiisai ?to be small? 3,014 226 227
2 shikkari-suru ?to be firm? 246 54 10
3 chiisai ? bodii-ga 11 4 0?to be small ? body-NOM?
4* todoku ? mokuyou-ni 2 0 2?to be delivered?on Thursday?
Table 6: Examples of candidate polar atoms
and their frequencies. ?*? denotes that it
should not be added to the lexicon. f(a), p(a),
and n(a) denote the frequency of the atom and
in positive and negative contexts, respectively.
5.1 Counts of Candidate Polar Atoms
From each proposition which does not have a
polarity, candidate polar atoms in the form of
simple atoms (just a verb or adjective) or com-
plex atoms (a verb or adjective and its right-
most argument consisting of a pair of a noun
and a postpositional) are extracted. For each
candidate polar atom a, the total appearances
f(a), and the occurrences in positive contexts
p(a) and negative contexts n(a) are counted,
based on the context of the adjacent clauses
(using the method described in Section 4.1).
If the proposition has the neg feature, the po-
larity is inverted. Table 6 shows examples of
candidate polar atoms with their frequencies.
5.2 Determination for Adding to
Lexicon
Among the located candidate polar atoms,
how can we distinguish true polar atoms,
which should be added to the lexicon, from
fake polar atoms, which should be discarded?
As shown in Section 4, both the coherent
precision (72-77%) and the coherent density
(7-19%) are so small that we cannot rely on
each single appearance of the atom in the po-
lar context. One possible approach is to set
the threshold values for frequency in a polar
context, max(p(a), n(a)) and for the ratio of
appearances in polar contexts among the to-
360
tal appearances, max(p(a),n(a))f(a) . However, the
optimum threshold values should depend on
the corpus and the initial lexicon.
In order to set general criteria, here we as-
sume that a true positive polar atom a should
have higher p(a)f(a) than its average i.e. coher-
ent density, cd(d, L+a), and also have higher
p(a)
p(a)+n(a) than its average i.e. coherent preci-
sion, cp(d, L+a) and these criteria should be
met with 90% confidence, where L+a is the
initial lexicon with a added. Assuming the bi-
nomial distribution, a candidate polar atom is
adopted as a positive polar atom7 if both (17)
and (18) are satisfied8.
q > cd(d, L),
where
p(a)?
k=0
f(a)Ckqk(1? q)f(a)?k = 0.9
(17)
r > cp(d, L) or n(a) = 0,
where
p(a)?
k=0
p(a)+n(a)Ckrk(1? r)p(a)+n(a)?k= 0.9
(18)
We can assume cd(d, L+a) ' cd(d, L), and
cp(d, L+a) ' cp(d, L) when L is large. We
compute the confidence interval using approx-
imation with the F-distribution (Blyth, 1986).
These criteria solve the problems in mini-
mum frequency and scope of the polar atoms
simultaneously. In the example of Table 6, the
simple atom chiisai (ID=1) is discarded be-
cause it does not meet (18), while the complex
atom chiisai ? bodii-ga (ID=3) is adopted
as a positive atom. shikkari-suru (ID=2)
is adopted as a positive simple atom, even
though 10 cases out of 64 were observed in the
negative context. On the other hand, todoku
? mokuyou-ni (ID=4) is discarded because it
does not meet (17), even though n(a)f(a) = 1.0,
i.e. always observed in negative contexts.
6 Evaluation
6.1 Evaluation by Polar Atoms
First we propose a method of evaluation of the
lexical learning.
7The criteria for the negative atoms are analogous.
8nCr notation is used here for combination (n
choose k).
Annotator B
Positive Neutral Negative
Anno- Positive 65 11 3
tator Neutral 3 72 0
A Negative 1 4 41
Table 7: Agreement of two annotators? judg-
ments of 200 polar atoms. ?=0.83.
It is costly to make consistent and large
?gold standards? in multiple domains, espe-
cially in identification tasks such as clause-
level SA (cf. classification tasks). Therefore
we evaluated the learning results by asking hu-
man annotators to classify the acquired polar
atoms as positive, negative, and neutral, in-
stead of the instances of polar clauses detected
with the new lexicon. This can be done be-
cause the polar atoms themselves are informa-
tive enough to imply to humans whether the
expressions hold positive or negative meanings
in the domain.
To justify the reliability of this evaluation
method, two annotators9 evaluated 200 ran-
domly selected candidate polar atoms in the
digital camera domain. The agreement results
are shown in Table 7. The manual classifi-
cation was agreed upon in 89% of the cases
and the Kappa value was 0.83, which is high
enough to be considered consistent.
Using manual judgment of the polar atoms,
we evaluated the performance with the follow-
ing three metrics.
Type Precision. The coincidence rate of the
polarity between the acquired polar atom
and the human evaluators? judgments. It
is always false if the evaluators judged it
as ?neutral.?
Token Precision. The coincidence rate of
the polarity, weighted by its frequency in
the corpus. This metric emulates the pre-
cision of the detection of polar clauses
with newly acquired poler atoms, in the
runtime SA system.
Relative Recall. The estimated ratio of the
number of detected polar clauses with the
expanded lexicon to the number of de-
tected polar clauses with the initial lex-
9For each domain, we asked different annotators
who are familiar with the domain. They are not the
authors of this paper.
361
Domain # Type Token RelativePrec. Prec. Recall
digital cameras 708 65% 96.5% 1.28
movies 462 75% 94.4% 1.19
mobile phones 228 54% 92.1% 1.13
cars 487 68% 91.5% 1.18
Table 8: Evaluation results with our method.
The column ?#? denotes the number of polar
atoms acquired in each domain.
icon. Relative recall will be 1 when no
new polar atom is acquired. Since the pre-
cision was high enough, this metric can
be used for approximation of the recall,
which is hard to evaluate in extraction
tasks such as clause-/phrase-level SA.
6.2 Robustness for Different
Conditions
6.2.1 Diversity of Corpora
For each of the four domain corpora, the an-
notators evaluated 100 randomly selected po-
lar atoms which were newly acquired by our
method, to measure the precisions. Relative
recall is estimated by comparing the numbers
of detected polar clauses from randomly se-
lected 2,000 sentences, with and without the
acquired polar atoms. Table 8 shows the re-
sults. The token precision is higher than 90%
in all of the corpora, including the movie do-
main, which is considered to be difficult for SA
(Turney, 2002). This is extremely high preci-
sion for this task, because the correctness of
both the extraction and polarity assignment
was evaluated simultaneously. The relative re-
call 1.28 in the digital camera domain means
the recall is increased from 43%10 to 55%. The
difference was smaller in other domains, but
the domain-dependent polar clauses are much
informative than general ones, thus the high-
precision detection significantly enhances the
system.
To see the effects of our method, we con-
ducted a control experiment which used pre-
set criteria. To adopt the candidate atom a,
the frequency of polarity, max(p(a), n(a)) was
required to be 3 or more, and the ratio of po-
larity, max(p(a),n(a))f(a) was required to be higher
than the threshold ?. Varying ? from 0.05 to
10The human evaluation result for digital camera do-
main (Kanayama et al, 2004).
6
? -
Relative recall
Token
precision
0.5
1
1.0 1.1 1.2
?
? ? = 0.05
? ? = 0.1??
? = 0.3
??
??
?? = 0.8
? ?digital cameras
?
? ? = 0.05?? = 0.1
?
? ? = 0.3?
?
???
? ?movies
(our method)
?Y
Figure 4: Relative recall vs. token precision
with various preset threshold values ? for the
digital camera and movie domains. The right-
most star and circle denote the performance of
our method.
0.8, we evaluated the token precision and the
relative recall in the domains of digital cam-
eras and movies. Figure 4 shows the results.
The results showed both relative recall and
token precision were lower than in our method
for every ?, in both corpora. The optimum ?
was 0.3 in the movie domain and 0.1 in the
digital camera domain. Therefore, in this pre-
set approach, a tuning process is necessary for
each domain. Our method does not require
this tuning, and thus fully automatic learning
was possible.
Unlike the normal precision-recall tradeoff,
the token precision in the movie domain got
lower when the ? is strict. This is due to the
frequent polar atoms which can be acquired
at the low ratios of the polarity. Our method
does not discard these important polar atoms.
6.2.2 Size of the Initial Lexicon
We also tested the performance while vary-
ing the size of the initial lexicon L. We pre-
pared three subsets of the initial lexicon, L0.8,
L0.5, and L0.2, removing polar atoms ran-
domly. These lexicons had 0.8, 0.5, 0.2 times
the polar atoms, respectively, compared to
L. Table 9 shows the precisions and recalls
using these lexicons for the learning process.
Though the cd values vary, the precision was
stable, which means that our method was ro-
bust even for different sizes of the lexicon. The
smaller the initial lexicon, the higher the rela-
tive recall, because the polar atoms which were
removed from L were recovered in the learning
process. This result suggests the possibility of
362
lexicon cd Token Prec. Relative Rec.
L 7.2% 96.5% 1.28
L0.8 6.1% 97.5% 1.41
L0.5 3.9% 94.2% 2.10
L0.2 3.6% 84.8% 3.55
Table 9: Evaluation results for various sizes of
the initial lexicon (the digital camera domain).
the bootstrapping method from a small initial
lexicon.
6.3 Qualitative Evaluation
As seen in the agreement study, the polar
atoms used in our study were intrinsically
meaningful to humans. This is because the
atoms are predicate-argument structures de-
rived from predicative clauses, and thus hu-
mans could imagine the meaning of a polar
atom by generating the corresponding sen-
tence in its predicative form.
In the evaluation process, some interesting
results were observed. For example, a nega-
tive atom nai ? kerare-ga (?to be free from
vignetting?) was acquired in the digital cam-
era domain. Even the evaluator who was fa-
miliar with digital cameras did not know the
term kerare (?vignetting?), but after looking up
the dictionary she labeled it as negative. Our
learning method could pick up such technical
terms and labeled them appropriately.
Also, there were discoveries in the error
analysis. An evaluator assigned positive to aru
? kamera-ga (?to have camera?) in the mobile
phone domain, but the acquired polar atom
had the negative polarity. This was actually
an insight from the recent opinions that many
users want phones without camera functions11.
7 Conclusion
We proposed an unsupervised method to ac-
quire polar atoms for domain-oriented SA, and
demonstrated its high performance. The lex-
icon can be expanded automatically by us-
ing unannotated corpora, and tuning of the
threshold values is not required. Therefore
even end-users can use this approach to im-
prove the sentiment analysis. These features
allow them to do on-demand analysis of more
narrow domains, such as the domain of digital
11Perhaps because cameras tend to consume battery
power and some users don?t need them.
cameras of a specific manufacturer, or the do-
main of mobile phones from the female users?
point of view.
References
C. R. Blyth. 1986. Approximate binomial confi-
dence limits. Journal of the American Statistical
Asscoiation, 81(395):843?855.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation
of adjectives. In Proceedings of the 35th ACL
and the 8th EACL, pages 174?181.
Hiroshi Kanayama, Tetsuya Nasukawa, and Hideo
Watanabe. 2004. Deeper sentiment analysis us-
ing machine translation technology. In Proceed-
ings of the 20th COLING, pages 494?500.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using nat-
ural language processing. In Proceedings of the
Second K-CAP, pages 70?77.
Bo Pang and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using subjectiv-
ity summarization based on minimum cuts. In
Proceedings of the 42nd ACL, pages 271?278.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from
reviews. In Proceedings of HLT/EMNLP-05,
pages 339?346.
Ellen Riloff and Janyee Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In
Proceedings of EMNLP-03, pages 105?112.
Peter D. Turney. 2002. Thumbs up or thumbs
down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proceedings of
the 40th ACL, pages 417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings
of HLT/EMNLP-05, pages 347?354.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In Pro-
ceedings of the Third IEEE International Con-
ference on Data Mining, pages 427?434.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity
of opinion sentences. In Proceedings of EMNLP-
2003, pages 129?136.
363
Paraphrasing Rules for Automatic Evaluation of Translation into Japanese
KANAYAMA Hiroshi
Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimo-tsuruma, Yamato-shi, Kanagawa 242-8502, Japan
kanayama@trl.ibm.com
Abstract
Automatic evaluation of translation qual-
ity has proved to be useful when the target
language is English. In this paper the eval-
uation of translation into Japanese is stud-
ied. An existing method based on n-gram
similarity between translations and refer-
ence sentences is difficult to apply to the
evaluation of Japanese because of the ag-
glutinativeness and variation of semanti-
cally similar expressions in Japanese. The
proposed method applies a set of para-
phrasing rules to the reference sentences
in order to increase the similarity score
for the expressions that differ only in their
writing styles. Experimental results show
the paraphrasing rules improved the cor-
relation between automatic evaluation and
human evaluation from 0.80 to 0.93.
1 Introduction
Evaluating natural language processing applica-
tions? output is important both for users and devel-
opers. Tasks such as sentential parsing, morpho-
logical analysis and named entity recognition are
easy to evaluate automatically because the ?right an-
swer? can be defined deterministically under a spe-
cific grammar or assumed criterion.
The evaluation of machine translation is not so
straightforward since there are infinite ways to out-
put similar meanings and one can not enumerate the
right answers exhaustively. In spite of that, auto-
matic translation evaluation is practically important
because the evaluation is laborious work for humans
and evaluation by humans tends to be arbitrary. Au-
tomatic evaluation is more reliable than human eval-
uation because of its consistency for the same trans-
lations.
BLEU (Papineni et al, 2002b) is one of the meth-
ods for automatic evaluation of translation quality.
It uses the ratio of co-occurring n-grams between
a translation and single or multiple reference sen-
tences. High correlation is reported between the
BLEU score and human evaluations for translations
from Arabic, Chinese, French, and Spanish to En-
glish (Papineni et al, 2002a).
This paper investigates how to apply BLEU to the
evaluation of English-to-Japanese translation. The
main goal of this paper is to design a reliable method
of evaluation for translations from another language
to Japanese (henceforth we call this Japanese trans-
lation evaluation). There are some difficulties in ad-
justing BLEU for Japanese: BLEU uses n-grams of
words, so words in a sentence are assumed to be sep-
arated by spaces, while Japanese does not use spaces
between words. Moreover, Japanese has more vari-
ation in writing styles than English. A major differ-
ence in these languages is that Japanese has polite
forms expressed by inflections or auxiliary verbs. If
the style of the translations is not the same as that
of the reference sentences, the evaluation score be-
comes low even though the translations are accurate
in their meanings and grammar. To solve these prob-
lems, we apply paraphrasing rules to the reference
sentences so that the differences in writing styles do
not affect the evaluation score.
Another goal is derived from this application
of paraphrasing: to define a ?good paraphrase?.
Here paraphrasing means rewriting sentences with-
out changing their semantics. Several methods of
paraphrasing have been studied. Some of them aim
at the preprocessing of machine translation (Mita-
mura and Nyberg, 2001; Takahashi et al, 2001).
They use paraphrasing to transform the input sen-
tences so that the language-transferring routines
can handle them easily. Another application of
paraphrasing is to canonicalize many expressions
that have the same semantics, supporting informa-
tion retrieval or question answering (Zukerman and
Raskutti, 2002; Torisawa, 2002). Paraphrasing tech-
niques in these studies are considered to be useful,
but they are difficult to evaluate.
Machine translation evaluation requires methods
to judge whether two sentences have the same
meaning even when they are syntactically different.
Therefore if a set of paraphrasing rules contributes
to more reliable translation evaluation, it can be said
to be ?good? paraphrasing. Thus the study in this
paper also presents a new paradigm for evaluating
paraphrases.
Section 2 overviews the BLEU metric. Section 3
presents the proposed method of Japanese transla-
tion evaluation, and its performance is evaluated in
Section 4. Based on the experimental results, Sec-
tion 5 discusses qualitative and quantitative features
of paraphrasing.
2 Background: Overview of BLEU
This section briefly describes the original BLEU
(Papineni et al, 2002b)1, which was designed for
English translation evaluation, so English sentences
are used as examples in this section.
2.1 N-gram precision
BLEU evaluation uses a parallel corpus which con-
sists of sentences in the source language and their
translations to the target language by professional
translators. We call the professional translations ref-
erence sentences. It is preferable if the corpus has
multiple reference sentences translated by multiple
translators for each source sentence.
Sentences in the source language are also trans-
lated by the translation systems to be evaluated. The
1See the cited paper for more detailed definitions.
translations are called candidate sentences. Below
is an example.
Example 1
Reference 1
I had my watch repaired by an office worker.
Reference 2
A person in the office repaired my watch.
Candidate 1
I had a man in the office repair a watch.
Candidate 2
I had the person of an office correct a clock.
The BLEU score is based on n-gram precision
shown in Equation (1). It is the ratio of n-grams
which appear both in the candidate sentence and in
at least one of the reference sentences, among all n-
grams in the candidate sentence.
pn =
?
s?cand
?
ngr?s
min(C(ngr), Cr(ngr))
?
s?cand
?
ngr?s
C(ngr)
cand : candidates s : sentence ngr : n-gram
C : count in the candidate sentence
Cr : count in a corresponding reference sentence
(1)
Candidate 1 in Example 1 contains 11 unigrams
including punctuation. 8 unigrams out of these also
appear in Reference 1 or Reference 2: ?I?, ?had?, ?a?,
?in?, ?the?, ?office?, ?watch? and ?.?, therefore, the un-
igram precision of Candidate 1 is 8/11. The bigram
precision is 4/10 since ?I had?, ?in the?, ?the office?
and ?watch .? are found. The only matched trigram
is ?in the office?, so the trigram precision is 1/9.
On the other hand, the unigram, bigram, and tri-
gram precisions of Candidate 2 are 8/11, 2/10, 0/9,
respectively, which are lower than those of Candi-
date 1. Indeed Candidate 1 is a better English trans-
lation than Candidate 2.
In practice the n-gram precision is calculated not
for each sentence but for all of the sentences in the
corpus.
2.2 Brevity Penalty
The n-gram precision is calculated by dividing the
number of matched n-grams by the number of n-
grams in the candidate sentence. Therefore, a short
candidate sentence which consists only of frequently
used words can score a high n-gram precision. For
example, if the candidate sentence is just ?The?, its
unigram precision is 1.0 if one of reference sen-
tences contains at least one ?the?, and that is usually
true.
To penalize such a meaningless translation, the
BLEU score is multiplied by the brevity penalty
shown in (2).
BP =
{
1 if c > r
e(1?r/c) if c ? r (2)
where c and r are the total numbers of words in
the candidate sentences and the reference sentences
which have the closest numbers of words in each
parallel sentence.
2.3 BLEU score
The BLEU score is calculated by Equation (3) be-
low. It is the geometric average of the n-gram pre-
cisions multiplied by the brevity penalty. The geo-
metric average is used because pn decreases expo-
nentially as n increases. The BLEU score ranges
between 0 and 1.
BLEU = BP ?
( N?
n=1
pn
) 1N
(3)
The evaluations use unigrams up to N -grams. If a
large n is used, the fluency of the sentences becomes
a more important factor than the correctness of the
words. Empirically the BLEU score has a high cor-
relation with human evaluation when N = 4 for En-
glish translation evaluations (Papineni et al, 2002b).
3 Japanese Version of BLEU and Its
Extension
This section describes how to adapt BLEU for
Japanese translation evaluation. The adaptation con-
sists of three steps.
3.1 Use of Morphological Analyzer
The first modification is mandatory for using the n-
gram metric as in the original BLEU implementa-
tion. Since Japanese has no spaces between words,
the words have to be separated by morphological
analysis as in Example 2.
Example 2
Kare ga hon wo yo mi mashi ta .
He SUBJ book ACC read INF POLITE PAST .
?He read a book.?
3.2 Distinguish between Different
Parts-of-speech
Many English words can be used as various parts-
of-speech (POSs), but BLEU doesn?t distinguish be-
tween the words with the same surface form in terms
of their POSs, since the sentences are not processed
by a tagger, so the system can?t handle POSs. This
doesn?t cause a problem because most of the multi-
POS words have conceptually similar meanings, as
exemplified by the adverb ?fast? and the adjective
?fast? which have the same basic concept, so match-
ing them between the candidate and references rea-
sonably reflects the quality of the translation.
On the other hand, Japanese homonyms tend to
be completely different if their POSs are different.
For example, the postpositional phrasal particle ?ga?
and the connective particle ?ga? should be distin-
guished from one another since the former acts as
a subject case marker, while the latter connects two
clauses that normally contradict each other. Fortu-
nately the morphological analyzer outputs POS in-
formation when the sentence is separated into words,
and therefore the words are also distinguished by
their POSs in the described method.
3.3 Paraphrasing Rules
Example 3 is another possible translation of the
source sentence of Example 2.
Example 3
Kare ga hon wo yo n da .
He SUBJ book ACC read INF-EUPH PAST .
?He read a book.?
The only difference here is the ending of the sen-
tence has a less polite form. However, when Ex-
ample 2 is the only reference sentence, the BLEU
evaluation of Example 3 does not score high: 6/8 for
unigrams, 4/7 for bigrams, 3/6 for trigrams, and 2/5
1 $1(verb-c) : n : da ? $1 : mi : masi : ta
2 nai(adj) : . ? ari : mase : n : .
3 $1(noun) : da ? $1 : dearu
4 ni : yo : t : te ? ni : yo : ri
Table 1: Examples of paraphrasing rules. $1 denotes
a wild card shared by both sides. ?:? is a boundary of
morphemes. ?(verb-c)? means a consonant verb such
as ?yomu?. Actually these rules have conditions not
described here so that they are not overused.
for 4-grams, while its meaning is same as that of the
reference sentence.
Basically BLEU copes with this problem of varia-
tion in writing styles by relying on the number of ref-
erence sentences available for each source sentence
and by reflecting the total size of corpus. That is,
if the corpus has multiple reference sentences trans-
lated by different translators, multiple writing styles
will tend to be included, and if the corpus is very
large, such inconsistencies of writing style are sta-
tistically not a problem.
In Japanese translation evaluation, however, this
problem can not be resolved using such a quantita-
tive solution because the influence of the differences
in writing styles are too large. For example, whether
or not the translation is given in the polite form de-
pends on the translation system2, so the evaluation
score is strongly affected by the degree of matching
of the writing styles between the translation system
and the reference sentences.
To cancel out the differences in writing styles,
we apply some paraphrasing rules to the reference
sentences to generate new sentences with different
writing styles. The generated sentences are added
to the reference sentences, and therefore, n-grams
in the candidate sentences can match the reference
sentences regardless of their writing styles. Table 1
shows examples of paraphrasing rules.
These rules are applied to the reference sentences.
If a reference sentence matches to a paraphrasing
rule, the sentence is replicated and the replica is
rewritten using the matched rule. For example, the
Japanese sentence in Example 2 matches Rule 1 in
Table 1 so the Japanese sentence in Example 3 is
2Some translation systems allow us to specify such writing
styles but some systems don?t.
produced. In this case, the evaluation is done as if
there are two reference sentences, therefore, a can-
didate sentence gets the same score regardless of its
politeness.
To avoid applying the same rules repeatably, the
rules are applied in a specific order. How to generate
the rules is described in Section 4.1.
4 Experiments
4.1 Environments
To see how much the three extensions above con-
tribute to the evaluation of translation, the correla-
tion between the automatic evaluation and the hu-
man evaluation is calculated. We used a bilingual
corpus which consists of 6,871 English sentences
on a technical domain and their translations into
Japanese.
100 sentences were randomly selected and trans-
lated by 5 machine translation systems S1-S5 and
a human H1 who is a native Japanese speaker but
does not have strong knowledge of the technical do-
main. These 6 translations were evaluated by five
methods: B1 to B4 are Japanese versions of BLEU
with the extension described in Section 3 and M1 is
a manual evaluation.
B1: Morphological analysis is applied to translated
Japanese sentences. Only the technique de-
scribed in Section 3.1 is used.
B2: Functional words are distinguished by their
POSs. This corresponds to the technique in
Section 3.1 and 3.2.
B3: Paraphrasing rules are applied to the reference
sentences as described in Section 3.3. Here
the applied rules are limited to 51 rules which
rewrite polite forms (e.g. 1 and 2 in Table 1).
B4: All 88 paraphrasing rules including other types
(e.g. 3 and 4 in Table 1) are applied.
M1: Average score of the manual evaluation of all
translations in the corpus. The sentences were
scored using a 5-level evaluation: 1 (poor) to
5 (good). The evaluator was different from the
translator of H1.
B1 B2 B3 B4 M1
S1 0.115 0.114 0.132 0.135 2.38
S2 0.130 0.129 0.149 0.151 2.74
S3 0.134 0.132 0.148 0.152 2.77
S4 0.137 0.135 0.148 0.158 3.16
S5 0.183 0.177 0.179 0.180 3.38
H1 0.170 0.166 0.179 0.187 4.40
correl 0.797 0.803 0.865 0.931 (1.0)
Table 2: BLEU scores evaluated by each method.
?correl? means the correlation of each method with
the manual evaluation (M1).
0
0.5
1
S1 S2 S3 S4 S5 H1
?
? ?
?
?
?
?
? ? ?
? ?
?
? ?
?
?
?
?
? ?
?
?
?
? B2
? B3
? B4
? M1
Figure 1: BLEU scores normalized as S1 is 0 and
H1 is 1. B1 is omitted since it is close to B2.
The paraphrasing rules used in B3 and B4 were
prepared manually by comparing the candidate sen-
tences and the reference sentences in the reminder of
the corpus which are not used for the evaluation. The
application of the rules are unlikely to produce in-
correct sentences, because the rules are adjusted by
adding the applicable conditions, and the rules that
may have side effects are not adopted. This was con-
firmed by applying the rules to 200 sentences in an-
other corpus. A total of 189 out of the 200 sentences
were paraphrased in at least a part, and all of the
newly created sentences were grammatically correct
and had the same meaning as the original sentences.
4.2 Experimental Results
Table 2 shows the result of evaluation using the five
methods. Comparing the correlation with M1, B2
slightly outperformed B1, thus the POS information
improves the evaluation. B3 was better than B2 in
correlation by 0.06. This is because the scores by the
B3 evaluation were much higher than the B2 evalu-
ation except for S5, since only S5 tends to output
sentences in polite form while the most of reference
sentences are written in polite form. Further im-
provement was observed in B4, by applying other
types of paraphrasing rules.
Figure 1 graphically illustrates the correlation be-
tween the BLEU evaluations and the human evalua-
tions, by normalizing the results so that S1 is 0, H1
is 1, and the rest of scores are linearly interpolated.
We can see that only B4 ranks all six systems in the
same order as the manual evaluation.
5 Discussion
5.1 Lexical or Structural Paraphrasing Rules
The paraphrasing rules used here have no lexical
rules that rewrite content words into other expres-
sions as in Example 4.
Example 4
dokusho : suru ? hon : wo : yo : mu
?read? ?read a book?
The main reason why we don?t use such rules is
that this type of rules may produce incorrect sen-
tences. For instance, (a) in Example 5 is rewritten
into (b) by the rule in Example 4, but (b) is not cor-
rect.
Example 5
(a) Kare ha watashi no hon wo yo mu.
?He reads my book.?
(b)* Kare ha watashi no dokusho suru.
?He my reads.? (literally)
This error can be decreased if the paraphrasing
rules have more strict conditions about surrounding
words, however, using such lexical rules contradicts
the original BLEU?s strategy that the differences in
expressions should be covered by the number of ref-
erence sentences. This strategy is reasonable be-
cause complicated rules tend to make the evaluation
arbitrary, that is, the evaluation score strongly de-
pends on the lexical rules. To verify that the lex-
ical rules are unnecessary, we added 17,478 word-
replacing rules to B4. The rules mainly replace Chi-
nese characters or Kana characters with canonical
Paraphrasing rule ?correl
da(aux) ? de : a : ru 0.025
$1(verb-v) : ru ? $1(verb-v) : masu 0.022
$1(noun) : (dot) : $2(noun) ? $1 : $2 0.020
Table 3: The three best paraphrasing rules which
contributed to the translation evaluation. The col-
umn ??correl? means the decrease of the correla-
tion in the translation evaluation when the rule is re-
moved. ?(verb-v)? denotes a vowel verb.
ones. With the rules, the correlation with M1 was
0.886, which is much lower than B4.
This result implies the differences in content
words do not affect the evaluations. More specifi-
cally, BLEU?s misjudgments because of differences
in content words occur with almost equal probabil-
ity for each translation system. Thus it is enough
to use the structural (i.e. non-lexical) paraphrasing
rules which rewrite only functional words.
5.2 Evaluation of Each Paraphrasing Rule
The contribution of the paraphrasing was measured
by the increase of reliability of the translation eval-
uation, as described in Section 4.2. In the same
way, the effect of each single paraphrasing rule can
be also evaluated quantitatively. Table 3 shows the
three best paraphrasing rules which contributed to
the translation evaluation. Here the contribution of
a rule to the automatic evaluation is measured by
the increase of correlation with the human evalua-
tion when the rule is used.
6 Conclusion and Future Work
This paper has proposed an automatic translation
evaluation method applicable to Japanese translation
evaluation. The paraphrasing rules that cancel out
the differences in writing styles contributed to im-
prove the reliability of the automatic evaluation. The
proposed evaluation method with paraphrasing rules
achieved a high correlation of 0.93 with the human
evaluation, while the correlation was 0.80 without
the rules.
The experiments clarified how much the para-
phrasing rules improved the evaluation by compar-
ing the correlations. This means our system can
evaluate not only the translation quality but also
the paraphrasing rules under the assumption that the
more properly the semantically similar sentences are
judged as close sentences the more reliable the trans-
lation evaluation is. Therefore the translation evalu-
ation gives us an objective evaluation method of the
paraphrasing quality that has been difficult to evalu-
ate.
This paper focuses on non-lexical paraphrasing
since lexical paraphrasing rules make the translation
evaluation inconsistent, but if an exhaustive and pre-
cise set of paraphrasing rules can be generated, it
will be useful for translation evaluation, and its ap-
propriateness should be shown by the reliability of
the translation evaluation. In order to develop such
desirable paraphrasing rules, the automatic acquisi-
tion of paraphrasing rules will be our next research
direction.
Acknowledgments
I am grateful to Dr. Kishore Papineni for the instruc-
tion of BLEU. I would like to thank people in Yam-
ato Research Laboratory for helping the evaluation.
References
Teruko Mitamura and Eric Nyberg. 2001. Automatic
rewriting for controlled language translation. In Proc.
of NLPRS2001 Workshop on Automatic Paraphrasing,
pages 1?12.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002a. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Proc.
of HLT2002, pages 124?127.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002b. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the 40th
ACL, pages 311?318.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, and
Kentaro Inui. 2001. Kura: A revision-based
lexico-structural paraphrasing engine. In Proc. of
NLPRS2001 Workshop on Automatic Paraphrasing,
pages 37?46.
Kentaro Torisawa. 2002. An unsupervised learn-
ing method for associative relationships between verb
phrases. In Proc. of COLING 2002, pages 1009?1015.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In Proc. of
COLING 2002, pages 1177?1183.
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 2?12,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning from a Neighbor: Adapting a Japanese Parser for Korean
through Feature Transfer Learning
Hiroshi Kanayama
IBM Research - Tokyo
Koto-ku, Tokyo, Japan
hkana@jp.ibm.com
Youngja Park
IBM Research - T.J.Watson Research Center
Yorktown Heights, NY, USA
young park@us.ibm.com
Yuta Tsuboi
IBM Research - Tokyo
Koto-ku, Tokyo, Japan
yutat@jp.ibm.com
Dongmook Yi
Korea Software Solutions Laboratory, IBM Korea
Gangnam-gu, Seoul, Korea
dmyi@kr.ibm.com
Abstract
We present a new dependency parsing
method for Korean applying cross-lingual
transfer learning and domain adaptation
techniques. Unlike existing transfer learn-
ing methods relying on aligned corpora or
bilingual lexicons, we propose a feature
transfer learning method with minimal su-
pervision, which adapts an existing parser
to the target language by transferring the
features for the source language to the tar-
get language. Specifically, we utilize the
Triplet/Quadruplet Model, a hybrid pars-
ing algorithm for Japanese, and apply a
delexicalized feature transfer for Korean.
Experiments with Penn Korean Treebank
show that even using only the transferred
features from Japanese achieves a high
accuracy (81.6%) for Korean dependency
parsing. Further improvements were ob-
tained when a small annotated Korean cor-
pus was combined with the Japanese train-
ing corpus, confirming that efficient cross-
lingual transfer learning can be achieved
without expensive linguistic resources.
1 Introduction
Motivated by increasing demands for advanced
natural language processing (NLP) applications
such as sentiment analysis (Pang et al., 2002;
Nasukawa and Yi, 2003) and question answer-
ing (Kwok et al., 2001; Ferrucci et al., 2010), there
is a growing need for accurate syntactic parsing
and semantic analysis of languages, especially for
non-English languages with limited linguistic re-
sources. In this paper, we propose a new depen-
dency parsing method for Korean which requires
minimal human supervision. Dependency parsing
can handle long-distance relationships and coor-
dination phenomena very well, and has proven to
be very effective for parsing free-order languages
such as Korean and Japanese (K?ubler et al., 2009).
Most statistical parsing methods rely on anno-
tated corpora labeled with phrase structures or
dependency relationships, but it is very expen-
sive to create a large number of consistent anno-
tations. Recently, treebanks have become avail-
able for many languages such as English, Ger-
man, Arabic, and Chinese. However, the pars-
ing results on these treebanks vary a lot depend-
ing on the size of annotated sentences and the type
of annotations (Levy and Manning, 2003; Mc-
Donald et al., 2013). Further, many languages
lack annotated corpus, or the size of the anno-
tated corpus is too small to develop a reliable sta-
tistical method. To address these problems, there
have been several attempts at unsupervised pars-
ing (Seginer, 2007; Spitkovsky et al., 2011), gram-
mar induction (Klein and Manning, 2004; Naseem
et al., 2010), and cross-lingual transfer learning
using annotated corpora of other languages (Mc-
Donald et al., 2011). However, the accuracies of
unsupervised methods are unacceptably low, and
results from cross-lingual transfer learning show
different outcomes for different pairs of languages,
but, in most cases, the parsing accuracy is still low
for practical purposes. A recent study by McDon-
ald et al. (2013) concludes that cross-lingual trans-
fer learning is beneficial when the source and tar-
get languages were similar. In particular, it reports
that Korean is an outlier with the lowest scores
(42% or less in UAS) when a model was trained
from European languages.
In this paper, we present a new cross-lingual
2
transfer learning method that learns a new model
for the target language by transferring the fea-
tures for the source language. Unlike other ap-
proaches which rely on aligned corpora or a
bilingual lexicon, we learn a parsing model for
Korean by reusing the features and annotated
data used in the Japanese dependency parsing,
the Triplet/Quadruplet Model (Kanayama et al.,
2000), which is a hybrid approach utilizing both
grammatical knowledge and statistics.
We exploit many similarities between the two
languages, such as the head-final structure, the
noun to verb modification via case and topic mark-
ers, and the similar word-order constraints. It was
reported that the mapping of the grammar formal-
ism in the language pair was relatively easy (Kim
et al., 2003b; Kim et al., 2003a). However, as the
two languages are classified into independent lan-
guage families (Gordon and Grimes, 2005), there
are many significant differences in their morphol-
ogy and grammar (especially in the writing sys-
tems), so it is not trivial to handle the two lan-
guages in a uniform way.
We show the Triplet/Quadruplet Model is suit-
able for bilingual transfer learning, because the
grammar rules and heuristics reduce the number
of modification candidates and can mitigate the
differences between two languages efficiently. In
addition, this model can handle the relationships
among the candidates as a richer feature space,
making the model less dependent upon the lexi-
cal features of the content words that are difficult
to align between the two languages. Similarly to
the delexicalized parsing model in (McDonald et
al., 2011), we transfer only part-of-speech infor-
mation of the features for the content words. We
create new mapping rules to extract syntactic fea-
tures for Korean parsing from the Japanese anno-
tated corpus and refine the grammar rules to get
closer modification distributions in two languages.
Our experiments with Penn Korean Tree-
bank (Han et al., 2002) confirm that the
Triplet/Quadruplet Model adapted for Korean out-
performs a distance-based dependency parsing
method, achieving 81.6% accuracy when no an-
notated Korean corpus was used. Further perfor-
mance improvements were obtained when a small
size of annotated Korean corpus was added, con-
firming that our algorithm can be applied with-
out more expensive linguistic resources such as an
aligned corpora or bilingual lexicons. Moreover,
the delexicalized feature transfer method enables
the algorithm applicable to any two languages that
have similar syntactic structures.
2 Related Work
2.1 Parsing for Korean
Since Korean is a morphologically-rich language,
many efforts for Korean parsing have focused
on automatically extracting rich lexical informa-
tion such as the use of case frame patterns for
the verbs (Lee et al., 2007), the acquisition of
case frames and nominal phrases from raw cor-
pora (Park et al., 2013), and effective features
from phrases and their neighboring contexts (Choi
and Palmer, 2011). Recently, Choi et al. (2012)
discussed the transformation of eojeol-based Ko-
rean treebank to entity-based treebank to effec-
tively train probabilistic CFG parsers. We apply
similar techniques as in (Choi et al., 2012) to miti-
gate the differences between Korean and Japanese
syntactic structures.
Chung and Rim (2003) applied the
Triplet/Quadruplet Model for Korean parsing
as done in our work. They reported that the model
performed well for long-distance dependencies,
but, in their experiments, the number of modi-
fication candidates was not effectively reduced
(only 91.5% of phrases were in one of the three
positions, while it was 98.6% in Kanayama?s
work for Japanese). In this paper, we introduce
more sophisticated grammatical knowledge and
heuristics to have similar dependency distribu-
tions in the two languages. Smith and Smith
(2004) attempted a bilingual parsing for English
and Korean by combining statistical dependency
parsers, probabilistic context-free grammars, and
word translation models into a unified framework
that jointly searches for the best English parse,
Korean parse and word alignment. However, we
utilize an existing parser and align the features
from the source language to the features for
the target language, and, thus, our method is
applicable to situations where there is no aligned
corpora or word translation models.
2.2 Transfer learning and domain adaptation
Recently, transfer learning has attracted much at-
tention, as it can overcome the lack of training
data for new languages or new domains for both
classification and regression tasks (Pan and Yang,
2010). Transfer learning has also been applied to
3
???
??/NNC?/PCA
?
?/VV?/EAN
????
???/NPR?/PAN
??
??/NNC
???
??/NNC?/PCA
????
??/NNC??/PAD
???
??/VV?/ECS
??
?/VX?/EFN
?
./SFN
?wife-NOM?
?buy-PAST?
?France-GEN? ?travel?
?bag-ACC?
?friend-DAT? ?show? ?want? ?.?
e
1
e
2
e
3
e
4
e
5
e
6
e
7
e
8
e
9
? ??? ?? ? ?
Figure 1: An example of dependency structures of a Korean sentence ?????????????
? ???? ??? ??.? (?(I) want to show the French travel bag which (my) wife bought to (my)
friend?). Each box corresponds to a Korean phrasal unit eojeol.
??
?/n?/pc
???
?/v?/aux
?????
????/np?/pn
??????
??/n???/n?/pc
???
??/n?/pc
?????
??/v??/aux?/
?wife-NOM?
?buy-PAST?
?France-GEN?
?travel bag-ACC?
?friend-DAT? ?want to show?
b
1
b
2
b
3
b
4
b
5
b
6
? ?? ??
Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in
Figure 1, ??????????????????????????. Each box corresponds to a Japanese
phrasal unit bunsetsu.
syntactic parsing, where a parsing model for a tar-
get language is learned from linguistic resources
in one or more different languages (Hwa et al.,
2005; Zeman and Resnik, 2008; McDonald et al.,
2011; Durrett et al., 2012; Georgi et al., 2012;
Naseem et al., 2012). McDonald et al. (2011)
proposed a delexicalized parsing model for cross-
lingual dependency parsing and demonstrated that
a high accuracy parsing was achieved for Indo-
European languages where significant amount of
parallel texts exist. However, in more recent work,
McDonald et al. (2013) showed that, unlike trans-
fer learning within close language families, build-
ing a Korean parser from European languages was
not successful with a very low accuracy. Durrett
et al. (2012) and Georgi et al. (2012) show that
transfer parsing can be improved when additional
bilingual resources are available, such as bilingual
dictionaries and parallel corpora of glossed texts
respectively.
Our method does not require such resources and
does not have restrictions on the sentence type that
can be parsed. Instead, we use a mixture of a
small corpus in a target language (i.e., Korean) and
a larger corpus of a source language (Japanese).
This task is similar to domain adaptation, and our
objective is to outperform the training model built
on each language separately. To avoid the loss of
accuracy due to the differences between two do-
mains, we apply the domain adaptation technique
proposed by Daum?e III (2007) which duplicates
the feature space into three categories with each
of the features trained by source, by target, and by
combined domains.
3 Dependency Structures of Korean and
Japanese
A dependency structure in Korean is typically an-
alyzed in terms of eojeol units, a basic phrase
that consists of a content word agglutinated with
optional functional morphemes such as postposi-
tional particles or endings for verbs. Figure 1
shows an example Korean sentence with the de-
pendencies between the eojeols indicated by ar-
rows. Figure 2 illustrates the dependency struc-
ture between the bunsetsus in Japanese that cor-
responds to the Korean structure in Figure 1. As
these figures show, the syntactic structures are
quite similar in these languages: All of the de-
pendencies are directed from left to right, and the
postpositional particles determine if the content
word modifies a verb (??? in e
1
and ??? in b
1
)
or a noun (??? in e
3
and ??? in b
3
). The eojeols
in Korean roughly correspond to the bunsetsus in
Japanese. In the remainder of this paper, we de-
note both an eojeol or a bunsetsu as a ?PU? (phrasal
unit) when distinction is not needed.
While Korean and Japanese have similar syn-
tactic structures, the two languages have many dif-
ferences. The eojeols in Korean are separated by
white space, while the bunsetsus in Japanese are
not. Further, the statistics show several differences
in the two languages. Table 1 compares a Korean
corpus, Penn Korean Treebank (henceforth KTB)
4
Table 1: Statistics of Korean and Japanese corpora.
KTB (Korean) EDR (Japanese)
Average number of characters (except for whitespace) per sentence 73.7 28.0
Average number of PUs per sentence 25.5 8.53
Average number of morphemes per PU 1.83 2.86
Ratio of modification to the next PU 70.0% 61.8%
Table 2: Simplified examples of Japanese grammar rules.
Rightmost morpheme of the modifier PU Conditions for the modified PUs
postpositional ??? wo (accusative) verb, adjective
postpositional ??? no (genitive, nominative) noun, verb, adjective
postpositional ??? to (conjunctive) noun, verb, adjective, adverb ????? isshoni (?together?)
adverb verb, adjective, adverb, copula
(Han et al., 2002), and a Japanese corpus, EDR
Corpus (EDR, 1996). Both corpora consist of
word-level bracketed constituents, so they are con-
verted into PU-level dependency structures using
the method described in Choi and Palmer (2011).
Though both corpora consist mainly of newspaper
or magazine articles, the sentences are not aligned
with each other, so the statistics show the compar-
isons of the two corpora, rather than the theoret-
ical comparisons of the two languages. However,
we can see that Korean sentences tend to be longer
than Japanese sentences both in terms of the num-
ber of characters and PUs. More eojeols modify an
adjacent eojeol in Korean than in Japanese. For in-
stance, e
1
, e
4
, e
6
, e
7
, and e
8
modify the next eojeol
in Figure 1, but only b
1
, b
3
, and b
5
modify the next
bunsetsu in Figure 2. Those differences suggest
some of the difficulties in applying the Japanese
dependency model to Korean. The Japanese pars-
ing method that will be described in the next sec-
tion exploits these characteristics, which we apply
to Korean parsing.
4 Triplet/Quadruplet Model
This section describes the Triplet/Quadruplet
Model (Kanayama et al., 2000) which was origi-
nally designed for Japanese parsing. First, we re-
view the two main ideas of the model ? restriction
of modification candidates and feature selection
for probability calculation. Then, we describe how
we apply the Triplet/Quadruplet Model to Korean
parsing in Section 4.3.
4.1 Restriction of modification candidates
The Triplet/Quadruplet Model utilizes a small
number (about 50) of hand-crafted grammar rules
that determine whether a PU can modify each PU
to its right in a sentence. The main goal of the
grammar rules is to maximize the coverage, and
the rules are simple describing high-level syntac-
tic dependencies, and, thus, the rules can be eas-
ily created without worrying about the precision
or contradictory rules. The statistical information
is later used to select the right rules for a given
sentence to produce an accurate parsing result. Ta-
ble 2 shows several grammar rules for Japanese, in
which the modified PUs are determined depend-
ing on the conditions of the rightmost morpheme
in the modifier PU.
An analysis of the EDR corpus shows that
98.6% of the correct dependencies are either the
nearest PU, the second nearest PU, or the farthest
PU from the modifier (more details in Table 4(a)).
Therefore, the model can be simplified by restrict-
ing the candidates to these three candidates and
by ignoring the other PUs with a small sacrifice
(1.4%) of parsing accuracy.
4.2 Calculation of modification probabilities
Let u be a modifier PU in question, c
un
the u?s n-
th modification candidate PU, ?
u
and ?
c
un
the at-
tributes of u and c
un
, respectively. Then the prob-
ability that u modifies its n-th candidate is calcu-
lated by the triplet equation (1) or the quadruplet
equation (2) when u has two or three candidates,
respectively
1
.
P (u ? c
un
) = P (n | ?
u
,?
c
u1
,?
c
u2
) (1)
P (u ? c
un
) = P (n | ?
u
,?
c
u1
,?
c
u2
,?
c
u3
) (2)
1
It is trivial to show that P (u ? cu
1
) = 1, when u has
only one candidate.
5
Table 3: Simplified examples of Korean grammar rules.
Rightmost morpheme of the modifier PU Conditions for the modified PUs
PCA,PCJ,PAD,PAU (postpositional particles) V* (verb, adjective or auxiliary), CO (copula)
EAN (nominal verb ending e.g. ??? eun) N* (noun)
ADV (adverb), ADC (conjunction) N* (noun), V* (verb, adjective or auxiliary), ADV (adverb), ADC (conjunction)
postpositional ??? gwa (conjunctive) N* (noun), V* (verb, adjective or aux), adverb ???? hamkke (?together?)
N* (noun) N* (noun), V* (verb, adjective or auxiliary)
Table 4: Distribution (percentage) of the position of the correct modified PU among the candidate PUs
selected by the initial grammar rules. The column ?Sum? shows the coverage of the 1st, 2nd and last
PUs. The EDR corpus was used for Japanese, and the KTB was used for Korean in this analysis.
(a) Japanese
# of Candidates Ratio 1st 2nd Last Sum
1 32.7 100.0 ? ? 100.0
2 28.1 74.3 26.7 ? 100.0
3 17.5 70.6 12.6 16.8 100.0
4 9.9 70.4 11.1 13.8 95.3
?5 11.8 70.2 11.1 10.5 91.9
Total 100 ? ? ? 98.6
(b) Korean (with the initial grammar)
# of Candidates Ratio 1st 2nd Last Sum
1 10.5 100.0 ? ? 100.0
2 11.4 85.9 14.1 ? 100.0
3 10.4 76.2 13.4 10.4 100.0
4 9.3 74.7 11.3 8.0 93.9
?5 58.4 75.5 10.0 4.9 90.5
Total 100 ? ? ? 93.9
These probabilities are estimated by the maxi-
mum entropy method with a feature set to express
? and ?. Assuming the independence of those
modifications, the probability of the dependency
tree for an entire sentence P (T ) is calculated as
the product of the probabilities of all of the depen-
dencies in the sentence using beam search to max-
imize P (T ) under the constraints of the projected
structure.
P (T ) ?
?
u
P (u ? c
un
) (3)
In comparison, a traditional statistical parser
(Collins, 1997) uses Equation (4) to calculate the
probability of u modifying t.
P (u ? t) = P (True | ?
u
,?
t
,?
u,t
) (4)
We call the model based on Equation (4) the Dis-
tance Model, since ?
u,t
(the distance between u
and t) is typically used as the key feature. Though
other contextual information, in addition to the at-
tributes of u and t, can be added, the model calcu-
lates the probabilities of the dependencies between
u and t independently and thus often fails to incor-
porate appropriate contextual information.
Equations (1) and (2) have two major advan-
tages over the Distance Model: First, all the at-
tributes of the modifier and its candidates can be
handled simultaneously. The combination of those
attributes helps the model to express the context of
the modifications. Second, the probability of each
modification is calculated based on the relative po-
sitions of the candidates, instead of the distance
from the modifier PU in the surface sentence, and,
thus, the model is more robust.
4.3 Korean dependency parsing with the
Triplet/Quadruplet Model
We design the Korean parser by adapting the
Triplet/Quadruplet Model based on the analogous
characteristics of Japanese and Korean. First, we
created the Korean grammar rules for generating
candidate modified PUs by modifying the rules
for Japanese shown in Table 2 for Korean. The
rule set, containing fewer than 50 rules, is sim-
ple enough to be created manually, because the
rules simply describe possible dependencies, and
Japanese phenomena are good hints for Korean
phenomena. Table 3 shows some examples of the
rules for Korean based on the POS schema used in
the KTB corpus. We did not automatically extract
the rules from the annotated corpora so that the
rules are general and independent of the training
corpus. Nonetheless, 96.6% of the dependencies
in KTB are covered by the grammar rules. The re-
maining dependencies (3.4%) not covered by the
rule set are mainly due to rare modifications and
may indicate inconsistencies in the annotations,
so we do not seek any grammar rules to achieve
nearly 100%.
6
??? (?wife-NOM?) ? (?buy?) ??? (?show?) ?? (?want?)
e
1
e
2
e
7
e
8
? ? ?
?1? NNC (common noun)
?2? PCA (postpositional)
?3???? (?-NOM?)
?5? VV (verb)
?7? EAN (adnominal ending)
?8???? (past adnominal)
?5? VV (verb)
?7? ECS (ending)
?8???? (conjunctive)
?5? VX (auxiliary)
?7? EFN (final ending)
?8???? (predicative)
Figure 3: The features used to select the modified PU of e
1
among its three candidates. The full sentence
of this example is shown in Figure 1. The numbers in brackets correspond to the feature IDs in Table 5.
Table 5: The features to express attributes of a modifier and modification candidates.
Feature set ID Description
?1? PoS of the head morpheme of the modifier
?2? PoS of the last morpheme of the modifier
?3? Lex of the postpositional or endings of the modifier
?4? Lex of the adverb of the modifier
?5? PoS of the head morpheme of the modification candidate
?6? Lex of the head morpheme of the modification candidate
?7? PoS of the last morpheme of the modification candidate
?8? Lex of the postpositional or endings of the modification candidate
?9? Existence of a quotation expression ???? dago or ???? rago
?10? Number of ??? eun (TOPIC marker) between the modifier and modification candidate
?11? Number of commas between the modifier and modification candidate
combination ?1? ? ?5? / ?2? ? ?5? / ?2? ? ?7? / ?3? ? ?5? / ?3? ? ?8?
Table 4(a) and (b) show the distribution of the
numbers of candidate PUs and the position of the
correct modified PUs obtained from the analysis
of the EDR corpus and the KTB corpus respec-
tively. As we can see, the first candidate is pre-
ferred in both languages, but the preference of the
nearer candidate is stronger in Korean. For in-
stance, when there are more than one candidates,
the probability that the first candidate is the cor-
rect one is 78% for Korean but 71% for Japanese.
Further, when there are more than 2 candidates,
Japanese prefers the last candidate, while Korean
prefers the second candidate. Based on the analy-
sis results, the number of modification candidates
is restricted to at most three (the first, second and
last candidates) for Korean as well.
The next step is to design ?
u
and ?
c
un
, which
are required in Equations (1) and (2) to choose the
correct modified PU. We converted the feature set
from the Japanese study to get the Korean features
as listed in Table 5. For example, to find the mod-
ified PU of e
1
????? anae-ga (?wife-NOM?) in
the sentence shown in Figure 1, the attributes of
e
1
and the attributes of the three candidates, e
2
,
e
7
, and e
8
, are extracted as shown in Figure 3, and
their attributes are used to estimate the probability
of each candidate in Equation (2).
5 Adaptation for Bilingual Transfer
Learning
In Section 4.3, we explained how the
Triplet/Quadruplet Model can be used for
Korean. In this section, we describe the feature
adaption techniques in more detail and investigate
if the new model with transferred features works
well when a small amount of annotated corpus for
the target language is provided. Further, we study
if we can leverage the annotated corpus for the
source language in addition to the parsing model
and train a model for the target language using the
training data for the source language.
5.1 Feature Transfer
With the assumption that Korean and Japanese
have similar syntactic dependencies, we adopt
the delexicalized parsing model presented in Mc-
Donald et al. (2011). We transfer the part-of-
speech (POS) in the Japanese features to the POS
scheme in the KTB corpus, and translate Japanese
functional words to the corresponding functional
words in Korean. This transfer process is manda-
tory because we use the language specific POS
systems to capture language-specific dependency
phenomena, unlike other works using language
universal but coarser POS systems.
We do not transfer lexical knowledge on con-
7
Table 6: Example of mapping rules for parts-of-speech and functional words.
Japanese PoS Korean PoS
common noun NNC
verb VV
adjective VJ
nominal suffix XSF
case particle
???,???,???,???? PAD
others PCA
Japanese particle Korean particle
??? wo (?-ACC?) ??? eul
???? yori (?from?) ???? buteo
??? ha (?-TOPIC?) ??? eun
??? mo (?too?) ??? do
??? ga case particle (?-NOM?) ??? i
conjunctive particle (?but?) ???? jiman
tent words and exceptional cases, so feature sets
?4? and ?6? are not transferred. Table 6 shows
some examples of the feature transfer which han-
dle POS tags and functional words. We note that
the Korean features shown in Figure 3 are directly
extracted from Japanese corpus using those rules.
5.2 Adaptation of parsing rules
While Japanese and Korean are similar in terms of
syntactic dependencies, there are significant dif-
ferences between the two languages in the distri-
bution of modification as shown in the Table 4(a)
and (b): In Korean, more than half of modifiers
have 5 or more candidates, while only 12% of
Japanese modifiers do. In Japanese, 98.6% of cor-
rect modified PUs are located in one of the three
positions (1st, 2nd or last), but, in Korean, the ratio
falls to 93.9% as shown in Table 4. Another ma-
jor difference of the two languages is the different
average numbers of PUs per sentence as shown in
Table 1. Korean has 25.5 PUs per sentence, while
the number is only 8.5 in Japanese. This is mainly
caused by the difference between eojeol in Korean
and bunsetsu in Japanese. In Japanese, compound
nouns and verb phrases with an auxiliary verb are
likely to form a single bunsetsu, while, in Korean,
they are split into multiple eojeols with a whites-
pace in-between.
These differences significantly reduce the ef-
fect of transfer learning. To address these prob-
lems, we further refine the grammar rules as in
the following. We added heuristic rules for the
Korean model to effectively reduce the number of
candidates in compound nouns which consist of a
noun sequence in multiple eojeols, and verbs or
adjectives followed by auxiliary verbs. Figure 4
shows an algorithm to reduce the number of mod-
ified PUs considering the structure of compound
nouns. In this example, both PUs e
4
(?travel?) and
e
5
(?bag-ACC?) can be candidate PUs for eojeol
e
3
. However, based on the rule in Figure 4, e
4
and
e
5
are considered as a compound noun (line 1),
and e
4
is determined to modify e
5
(line 3). Sub-
sequently, e
3
?s modifiability to e
4
is rejected (line
5), and, thus, the correct modified PU of e
3
is de-
termined as e
5
. After refining the rules for com-
pound nouns and auxiliary verbs, the probability
of the correct modified PU being the 1st, 2nd or
last candidate PU increases from 93.9% to 96.3%
as shown in Table 7, and the distribution of the
candidate?s positions for Korean became closer to
the Japanese distribution shown in Table 4(a).
5.3 Learning from heterogeneous bilingual
corpora
The feature transfer and rule adaptation methods
described in previous sections generate a very ac-
curate Korean parser using only a Japanese cor-
pus as shown in the first row in Table 8. The next
question is if we can leverage bilingual corpora to
further improve the accuracy, when annotated cor-
pus for the target language (Korean) is available.
We note that the two corpora do not need to be
aligned and can come from different domains. To
mitigate the side effects of merging heterogeneous
training data in different languages, we apply the
domain adaptation method proposed by Daum?e III
(2007) and augment the feature set to a source
language-specific version, a target-specific version
and a general version. Specifically, a feature set x
in Table 5 is expanded as follows:
x
K
=< x,0,x > (5)
x
J
=< 0,x,x > (6)
where x
K
and x
J
denote the feature sets extracted
from the Korean corpus and the Japanese corpus
respectively. Then, the features specific to Korean
and Japanese get higher weights for the first part
or the second part respectively, and the character-
istics existing in both languages influence the last
part.
8
if PoS of u
i
?s last morpheme is N* and PoS of u
i+1
?s first morpheme is N*
then
u
i
must modify u
i+1
if u
i?1
?s last morpheme is not ??? then u
i?1
cannot modify u
i+1
else u
i?1
cannot modify u
i
u
1
to u
i?2
cannot modify u
i
????
???/NPR?/PAN
??
??/NNC
???
??/NNC?/PCA
?France-GEN? ?travel?
?bag-ACC?
e
3
e
4
e
5
?? ?
Figure 4: Heuristic rules to reduce the number of modification candidates surrounding compound nouns
in Korean. The example in the right figure shows that candidates in the dotted lines are removed by the
heuristics.
Table 7: Distribution of the position of correct modified PU for Korean after the refinement of the Korean
grammar rules.
# of candidates Ratio 1st 2nd Last Sum
1 46.4% 100.0% ? ? 100.0%
2 9.8% 79.0% 21.0% ? 100.0%
3 9.2% 75.5% 12.7% 11.8% 100.0%
4 8.0% 71.0% 11.8% 9.6% 92.4%
? 5 26.6% 70.4% 10.1% 7.8% 88.3%
Total 100% ? ? ? 96.3%
6 Experiments
In this section, we validate the effectiveness of
learning a Korean parser using the feature transfer
learning from the Japanese parser and compare the
Korean model with other baseline cases. We also
compare the parsing results when various sizes of
bilingual corpora were used to train the Korean
model.
6.1 Korean parsing using the
Triplet/Quadruplet Model
First, to validate the effectiveness of the
Triplet/Quadruplet Model for parsing Korean, we
built eight Korean dependency parsing models us-
ing different numbers of training sentences for Ko-
rean. The KTB corpus Version 2.0 (Han et al.,
2002) containing 5,010 annotated sentences was
used in this study. We first divide the corpus into 5
subsets by putting each sentence into its (sentence
ID mod 5)-th group. We use sentences from the
first subgroup for estimating the parameters, sen-
tences from the second subgroup for testing, and
use the remaining three subgroups for training. We
built 8 models in total, using from 0 sentence up
to 3,006 sentences selected from the training set.
The number of training sentences in each model
is shown in the first column in Table 8. The pa-
rameters were estimated by the maximum entropy
method, and the most preferable tree is selected
using each dependency probability and the beam
search. The test data set contains 1,043 sentences.
We compare the Triplet/Quadruplet Model-
based models with the Distance Model. For the
Distance Model, we used the same feature set as
in Table 5, and added the distance feature (?
u,t
)
by grouping the distance between two PUs into 3
categories (1, 2 to 5, and 6 or more). The perfor-
mances are measured by UAS (unlabeled attach-
ment score), and the results of the two methods
are shown in the second column, where Japanese
Corpus Size=0, in Table 8 (a) and (b) respectively.
The top leftmost cells (80.61% and 71.63%) show
the parsing accuracies without any training cor-
pora. In these cases the nearest candidate PU is
selected as the modified PU. The difference be-
tween two models suggests the effect of restriction
of modification candidates by the grammar rules.
We note that the Triplet/Quadruplet Model pro-
duces more accurate results and outperforms the
Distance Model by more than 2 percentage points
in all cases. The results confirm that the method
for Japanese parsing is suitable for Korean pars-
ing.
6.2 Results of bilingual transfer learning
Next, we evaluate the transfer learning when anno-
tated sentences for Japanese were also added. Ta-
ble 8(a) shows the accuracies of our model when
various numbers of training sentences from Ko-
rean and Japanese are used. The first row shows
the accuracies of Korean parsing when the models
were trained only with the Japanese corpus, and
9
Table 8: The accuracy of Korean dependency parsing with various numbers of annotated sentences in the
two languages.
?
denotes that the mixture of bilingual corpora significantly outperformed (p < .05)
the parser trained with only the Korean corpus without Japanese corpus.
(a) Triplet/Quadruplet Model
Japanese Corpus Size
0 2,500 5,000 10,000
K
o
r
e
a
n
C
o
r
p
u
s
S
i
z
e
0 80.61% 80.78% 81.23%
?
81.58%
?
50 82.21% 82.32% 82.40%
?
82.43%
?
98 82.36% 82.66%
?
82.69%
?
82.70%
?
197 83.13% 83.18% 83.30%
?
83.28%
383 83.62% 83.92%
?
83.94%
?
83.91%
?
750 84.03% 84.00% 84.06% 84.06%
1,502 84.41% 84.34% 84.32% 84.28%
3,006 84.77% 84.64% 84.64% 84.65%
(b) Distance Model
Japanese Corpus Size
0 2,500 5,000
K
o
r
e
a
n
C
o
r
p
u
s
S
i
z
e
0 71.63% 62.42% 54.92%
50 79.31% 79.55%
?
79.54%
?
98 80.53% 80.63%
?
80.72%
?
197 80.91% 80.84% 80.85%
383 81.86% 81.75% 81.76%
750 82.10% 81.92% 81.94%
1,502 82.50% 82.48% 82.50%
3,006 82.66% 82.57% 82.54%
other rows show the results when the Korean and
Japanese corpora were mixed using the method
described in Section 5.3.
As we can see from the results, the bene-
fit of transfer learning is larger when the size
of the annotated corpus for Korean (i.e., target
language) is smaller. In our experiments with
Triplet/Quadruplet Model, positive results were
obtained by the mixture of the two languages when
the Korean corpus is less than 500 sentences, that
is, the annotations in the source language success-
fully compensated the small corpus of the target
language. When the size of the Korean corpus is
relatively large (? 1, 500 sentences), adding the
Japanese corpus decreased the accuracy slightly,
due to syntactic differences between the two lan-
guages. Also the effect of the corpus from the
source language tends to saturate as the size of
the source corpus, when the target corpus is larger.
This is mainly because our mapping rules ignore
lexical features, so few new features found in the
larger corpus were incorrectly processed.
When merging the corpus in two languages,
if we simply concatenate the transferred features
from the source language and the features from
the target language (instead of using the dupli-
cated features shown in Equations (5) and (6)), the
accuracy dropped from 82.70% to 82.26% when
the Korean corpus size was 98 and Japanese cor-
pus size was 10,000, and from 83.91% to 83.40%
when Korean=383. These results support that
there are significant differences in the dependen-
cies between two languages even if we have im-
proved the feature mapping, and our approach
with the domain adaptation technique (Daum?e III,
2007) successfully solved the difficulty.
Table 8(b) shows the results of the Distance
Model. As we can see from the first row, using
only the Japanese corpus did not help the Dis-
tance Model at all in this case. The Distance
Model was not able to mitigate the differences be-
tween the two languages, because it does not use
any grammatical rules to control the modifiability.
This demonstrates that the hybrid parsing method
with the grammar rules makes the transfer learn-
ing more effective. On the other hand, the domain
adaptation method described in (5) and (6) suc-
cessfully counteracted the contradictory phenom-
ena in the two languages and increased the accu-
racy when the size of the Korean corpus was small
(size=50 and 98). This is because the interactions
among multiple candidates which cannot be cap-
tured from the small Korean corpus were provided
by the Japanese corpus.
Some of previous work reported the parsing
accuracy with the same KTB corpus; 81% with
trained grammar (Chung et al., 2010) and 83%
with Stanford parser after corpus transformation
(Choi et al., 2012), but as Choi et al. (2012) noted
it is difficult to directly compare the accuracies.
6.3 Discussion
The analysis of e
2
?s dependency in Figure 1
is a good example to illustrate how the
Triplet/Quadruplet Model and the Japanese
corpus help Korean parsing. Eojeol e
2
has three
modification candidates, e
3
, e
5
, and e
6
. In the
10
Distance Model, e
3
is chosen because the distance
between the two eojeols (?
e
2
,e
3
) was 1, which
is a very strong clue for dependency. Also, in
the Triplet/Quadruplet Model trained only with
a small Korean corpus, e
3
received a higher
probability than e
5
and e
6
. However, when a
larger Japanese corpus was combined with the
Korean corpus, e
5
was correctly selected as the
Japanese corpus provided more samples of the
dependency relation of ?verb-PAST? (e
2
) and
?common noun-ACC (?)? (e
5
) than that of
?verb-PAST? and ?proper noun-GEN (?)? (e
3
).
As we can notice, larger contextual information
is required to make the right decision for this case,
which may not exist sufficiently in a small cor-
pus due to data sparseness. The grammar rules in
the Triplet/Quadruplet Model can effectively cap-
ture such contextual knowledge even from a rel-
atively small corpus. Further, since the grammar
rules are based only on part-of-speech tags and a
small number of functional words, they are sim-
ilar to the delexicalized parser (McDonald et al.,
2011). These delexicalized rules are more robust
to linguistic idiosyncrasies, and, thus, are more ef-
fective for transfer learning.
7 Conclusion
We presented a new dependency parsing algo-
rithm for Korean by applying transfer learning
from an existing parser for Japanese. Unlike other
transfer learning methods relying on aligned cor-
pora or bilingual lexical resources, we proposed a
feature transfer method utilizing a small number
of hand-crafted grammar rules that exploit syn-
tactic similarities of the source and target lan-
guages. Experimental results confirm that the fea-
tures learned from the Japanese training corpus
were successfully applied for parsing Korean sen-
tences and mitigated the data sparseness problem.
The grammar rules are mostly delexicalized com-
prising only POS tags and a few functional words
(e.g., case markers), and some techniques to re-
duce the syntactic difference between two lan-
guages makes the transfer learning more effective.
This methodology is expected to be applied to any
two languages that have similar syntactic struc-
tures, and it is especially useful when the target
language is a low-resource language.
References
Jinho D. Choi and Martha Palmer. 2011. Statistical
dependency parsing in Korean: From corpus gener-
ation to automatic parsing. In Proceedings of the
Second Workshop on Statistical Parsing of Morpho-
logically Rich Languages, pages 1?11.
DongHyun Choi, Jungyeul Park, and Key-Sun Choi.
2012. Korean treebank transformation for parser
training. In Proceedings of the ACL 2012 Joint
Workshop on Statistical Parsing and Semantic Pro-
cessing of Morphologically Rich Languages, pages
78?88.
Hoojung Chung and Heechang Rim. 2003. A
new probabilistic dependency parsing model for
head-final, free word order languages. IEICE
TRANSACTIONS on Information and Systems, E86-
1(11):2490?2493.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, pages 49?57.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1?11.
EDR. 1996. EDR (Japan Electronic Dictionary Re-
search Institute, Ltd.) electronic dictionary version
1.5 technical guide.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59?79.
Ryan Georgi, Fei Xia, and William D Lewis. 2012.
Improving dependency parsing with interlinear
glossed text and syntactic projection. In Proceed-
ings of COLING 2012, pages 371?380.
Raymond GGordon and Barbara F Grimes. 2005. Eth-
nologue: Languages of the world, volume 15. SIL
international Dallas, TX.
11
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Heejong
Yi, and Martha Palmer. 2002. Penn Korean tree-
bank: Development and evaluation. In Proc. Pacific
Asian Conf. Language and Comp.
Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2005. Breaking the resource bottleneck for multi-
lingual parsing. Technical report, DTIC Document.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsu-
ishi, and Jun?ichi Tsujii. 2000. A hybrid Japanese
parser with hand-crafted grammar and statistics. In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 411?417.
Roger Kim, Mary Dalrymple, Ronald M Kaplan, and
Tracy Holloway King. 2003a. Porting grammars
between typologically similar languages: Japanese
to korean. In Proceedings of the 17th Pacific Asia
Conference on Language, Information.
Roger Kim, Mary Dalrymple, Ronald M Kaplan,
Tracy Holloway King, Hiroshi Masuichi, and
Tomoko Ohkuma. 2003b. Multilingual grammar
development via grammar porting. In ESSLLI 2003
Workshop on Ideas and Strategies for Multilingual
Grammar Development, pages 49?56.
Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 478?487.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on
Human Language Technologies, 1(1):1?127.
Cody Kwok, Oren Etzioni, and Daniel S Weld. 2001.
Scaling question answering to the web. ACM Trans-
actions on Information Systems (TOIS), 19(3):242?
262.
Hyeon-Yeong Lee, Yi-Gyu Hwang, and Yong-Seok
Lee. 2007. Parsing of Korean based on CFG using
sentence pattern information. International Journal
of Computer Science and Network Security, 7(7).
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, ACL, pages
439?446.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 62?72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
T?ackstr?om, et al. 2013. Universal dependency an-
notation for multilingual parsing. In Proceedings of
ACL 2013.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234?1244.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629?637.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using natural
language processing. In Proceedings of the Second
International Conferences on Knowledge Capture,
pages 70?77.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345?1359.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in nat-
ural language processing (EMNLP), pages 79?86,
Philadelphia, Pennsylvania.
Jungyeul Park, Daisuke Kawahara, Sadao Kurohashi,
and Key-Sun Choi. 2013. Towards fully lexicalized
dependency parsing for Korean. In Proceedings of
The 13th International Conference on Parsing Tech-
nologies.
Yoav Seginer. 2007. Fast unsupervised incremental
parsing. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 384?391.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 49?56.
Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011. Unsupervised depen-
dency parsing without gold part-of-speech tags. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1281?
1290.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP, pages 35?42.
12
