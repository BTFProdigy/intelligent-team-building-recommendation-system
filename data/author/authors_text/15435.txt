Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 800?807,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
On the role of context and prosody in the interpretation of ?okay?
Agust??n Gravano, Stefan Benus, He?ctor Cha?vez, Julia Hirschberg, Lauren Wilcox
Department of Computer Science
Columbia University, New York, NY, USA
{agus,sbenus,hrc2009,julia,lgw23}@cs.columbia.edu
Abstract
We examine the effect of contextual and
acoustic cues in the disambiguation of three
discourse-pragmatic functions of the word
okay. Results of a perception study show
that contextual cues are stronger predictors
of discourse function than acoustic cues.
However, acoustic features capturing the
pitch excursion at the right edge of okay fea-
ture prominently in disambiguation, whether
other contextual cues are present or not.
1 Introduction
CUE PHRASES (also known as DISCOURSE MARK-
ERS) are linguistic expressions that can be used to
convey explicit information about the structure of
a discourse or to convey a semantic contribution
(Grosz and Sidner, 1986; Reichman, 1985; Cohen,
1984). For example, the word okay can be used to
convey a ?satisfactory? evaluation of some entity in
the discourse (the movie was okay); as a backchan-
nel in a dialogue to indicate that one interlocutor
is still attending to another; to convey acknowledg-
ment or agreement; or, in its ?cue? use, to start or fin-
ish a discourse segment (Jefferson, 1972; Schegloff
and Sacks, 1973; Kowtko, 1997; Ward and Tsuka-
hara, 2000). A major question is how speakers indi-
cate and listeners interpret such variation in mean-
ing. From a practical perspective, understanding
how speakers and listeners disambiguate cue phrases
is important to spoken dialogue systems, so that sys-
tems can convey potentially ambiguous terms with
their intended meaning and can interpret user input
correctly.
There is considerable evidence that the different
uses of individual cue phrases can be distinguished
by variation in the prosody with which they are re-
alized. For example, (Hirschberg and Litman, 1993)
found that cue phrases in general could be disam-
biguated between their ?semantic? and their ?dis-
course marker? uses in terms of the type of pitch
accent borne by the cue phrase, the position of the
phrase in the intonational phrase, and the amount
of additional information in the phrase. Despite the
frequence of the word okay in natural dialogues,
relatively little attention has been paid to the rela-
tionship between its use and its prosodic realization.
(Hockey, 1993) did find that okay differs in terms of
the pitch contour speakers use in uttering it, suggest-
ing that a final rising pitch contour ?categorically
marks a turn change,? while a downstepped falling
pitch contour usually indicates a discourse segment
boundary. However, it is not clear which, if any, of
the prosodic differences identified in this study are
actually used by listeners in interpreting these po-
tentially ambiguous items.
In this study, we address the question of how hear-
ers disambiguate the interpretation of okay. Our goal
is to identify the acoustic, prosodic and phonetic fea-
tures of okay tokens for which listeners assign differ-
ent meanings. Additionally, we want to determine
the role that discourse context plays in this classi-
fication: i.e., can subjects classify okay tokens reli-
ably from the word alone or do they require addi-
tional context?
Below we describe a perception study in which
listeners were presented with a number of spoken
productions of okay, taken from a corpus of dia-
logues between subjects playing a computer game.
The tokens were presented both in isolation and in
context. Users were asked to select the meaning
800
of each token from three of the meanings that okay
can take on: ACKNOWLEDGEMENT/AGREEMENT,
BACKCHANNEL, and CUE OF AN INITIAL DIS-
COURSE SEGMENT. Subsequently, we examined the
acoustic, prosodic and phonetic correlates of these
classifications to try to infer what cues listeners used
to interpret the tokens, and how these varied by con-
text condition. Section 2 describes our corpus. Sec-
tion 3 describes the perception experiment. In Sec-
tion 4 we analyze inter-subject agreement, introduce
a novel representation of subject judgments, and ex-
amine the acoustic, prosodic, phonetic and contex-
tual correlates of subject classification of okays. In
Section 5 we discuss our results and future work.
2 Corpus
The materials for our perception study were selected
from a portion of the Columbia Games Corpus, a
collection of 12 spontaneous task-oriented dyadic
conversations elicited from speakers of Standard
American English. The corpus was collected and
annotated jointly by the Spoken Language Group
at Columbia University and the Department of Lin-
guistics at Northwestern University.
Subjects were paid to play two series of com-
puter games (the CARDS GAMES and the OBJECTS
GAMES), requiring collaboration between partners
to achieve a common goal. Participants sat in front
of laptops in a soundproof booth with a curtain be-
tween them, so that all communication would be ver-
bal. Each player played with two different partners
in two different sessions. On average, each session
took 45m 39s, totalling 9h 8m of dialogue for the
whole corpus. All interactions were recorded, digi-
tized, and downsampled to 16K.
The recordings were orthographically transcribed
and words were aligned by hand by trained annota-
tors in a ToBI (Beckman and Hirschberg, 1994) or-
thographic tier using Praat (Boersma and Weenink,
2001) to manipulate waveforms. The corpus con-
tains 2239 unique words, with 73,831 words in total.
Nearly all of the Objects Games part of the corpus
has been intonationally transcribed, using the ToBI
conventions. Pitch, energy and duration information
has been extracted for the entire corpus automati-
cally, using Praat.
In the Objects Games portion of the corpus each
player?s laptop displayed a gameboard containing 5?
7 objects (Figure 1). In each segment of the game,
both players saw the same set of objects at the same
position on each screen, except for one object (the
TARGET). For one player (the DESCRIBER), this tar-
get appeared in a random location among other ob-
jects on the screen. For the other player (the FOL-
LOWER), the target object appeared at the bottom of
the screen. The describer was instructed to describe
the position of the target object on their screen so
that the follower could move their representation of
the target to the same location on their own screen.
After the players had negotiated what they deter-
mined to be the best location, they were awarded
up to 100 points based on the actual match of the
target location on the two screens. The game pro-
ceeded in this way through 14 tasks, with describer
and follower alternating roles. On average, the Ob-
jects Games portion of each session took 21m 36s,
resulting in 4h 19m of dialogue for the twelve ses-
sions in the corpus. There are 1484 unique words in
this portion of the corpus, and 36,503 words in total.
Figure 1: Sample screen of the Objects Games.
Throughout the Objects Games, we noted that
subjects made frequent use of affirmative cue words,
such as okay, yeah, alright, which appeared to vary
in meaning. To investigate the discourse functions
of such words, we first asked three labelers to inde-
pendently classify all occurrences of alright, gotcha,
huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yup
in the entire Games Corpus into one of ten cate-
gories, including acknowledgment/agreement, cue
beginning or ending discourse segment, backchan-
nel, and literal modifier. Labelers were asked to
801
choose the most appropriate category for each to-
ken, or indicate with ??? if they could not make a
decision. They were allowed to read the transcripts
and listen to the speech as they labeled.
For our perception experiment we chose materials
from the tokens of the most frequent of our labeled
affirmative words, okay, from the Objects Games,
which contained most of these tokens. Altogether,
there are 1151 instances of okay in this part of the
corpus; it is the third most frequent word, follow-
ing the, with 4565 instances, and of, with 1534.
At least two labelers agreed on the functional cat-
egory of 902 (78%) okay tokens. Of those tokens,
286 (32%) were classified as BACKCHANNEL, 255
(28%) as ACKNOWLEDGEMENT/AGREEMENT, 141
(16%) as CUE BEGINNING, 116 (13%) as PIVOT
BEGINNING (a function that combines Acknowl-
edgement/agreement and Cue beginning), and 104
(11%) as one of the other functions. We sampled
from tokens the annotators had labeled as Cue be-
ginning discourse segment, Backchannel, and Ac-
knowledgement/agreement, the most frequent cate-
gories in the corpus; we will refer to these below
simply as ?C?, ?B?, and ?A? classes, respectively.
3 Experiment
We next designed a perception experiment to ex-
amine naive subjects? perception of these tokens of
okay. To obtain good coverage both of the (labeled)
A, B, and C classes, as well as the degrees of po-
tential ambiguity among these classes, we identified
9 categories of okay tokens to include in the experi-
ment: 3 classes (A, B, C) ? 3 levels of labeler agree-
ment (UNANIMOUS, MAJORITY, NO-AGREEMENT).
?Unanimous? refers to tokens assigned to a particu-
lar class label by all 3 labelers, ?majority? to tokens
assigned to this class by 2 of the 3 labelers, and ?no-
agreement? to tokens assigned to this class by only
1 labeler. To decrease variability in the stimuli, we
selected tokens only from speakers who produced at
least one token for each of the 9 conditions. There
were 6 such speakers (3 female, 3 male), which gave
us a total of 54 tokens.
To see whether subjects? classifications of okay
were dependent upon contextual information or not,
we prepared two versions of each token. The iso-
lated versions consisted of only the word okay ex-
tracted from the waveform. For the contextualized
versions, we extracted two full speaker turns for
each okay including the full turn1 containing the tar-
get okay plus the full turn of the previous speaker. In
the following three sample contexts, pauses are indi-
cated with ?#?, and the target okays are underlined:
Speaker A: yeah # um there?s like there?s some space there?s
Speaker B: okay # I think I got it
Speaker A: but it?s gonna be below the onion
Speaker B: okay
Speaker A: okay # alright # I?ll try it # okay
Speaker B: okay the owl is blinking
The isolated okay tokens were single channel au-
dio files; the contextualized okay tokens were for-
matted so that each speaker was presented to sub-
jects on a different channel, with the speaker uttering
the target okay consistently on the same channel.
The perception study was divided into two parts.
In the first part, each subject was presented with
the 54 isolated okay tokens, in a different ran-
dom ordering for each subject. They were given
a forced choice task to classify them as A, B, or
C, with the corresponding labels (Acknowledge-
ment/agreement, Backchannel, and Cue beginning)
also presented in a random order for each token. In
the second part, the same subject was given 54 con-
textualized tokens, presented in a different random
order, and asked to make the same choice.
We recruited 20 (paid) subjects for the study, 10
female, and 10 male, all between the ages of 20 and
60. All subjects were native speakers of Standard
American English, except for one subject who was
born in Jamaica but a native speaker of English. All
subjects reported no hearing problems. Subjects per-
formed the study in a quiet lab using headphones to
listen to the tokens and indicating their classification
decisions in a GUI interface on a lab workstation.
They were given instructions on how to use the in-
terface before each of the two sections of the study.
For the study itself, for each token in the isolated
condition, subjects were shown a screen with the
three randomly ordered classes and a link to the to-
ken?s sound file. They could listen to the sound files
as many times as they wished but were instructed
not to be concerned with answering the questions
1We define a TURN as a maximal sequence of words spoken
by the same speaker during which the speaker holds the floor.
802
?correctly?, but to answer with their immediate re-
sponse if possible. However, they were allowed to
change their selection as many times as they liked
before moving to the next screen. In the contex-
tualized condition, they were also shown an ortho-
graphic transcription of part of the contextualized to-
ken, to help them identify the target okay. The mean
duration of the first part of the study was 25 minutes,
and of the second part, 27 minutes.
4 Results
4.1 Subject ratings
The distribution of class labels in each experimental
condition is shown in Table 1. While this distribu-
tion roughly mirrors our selection of equal numbers
of tokens from each previously-labeled class, in both
parts of the study more tokens were labeled as A
(acknowledgment/agreement) than as B (backchan-
nel) or C (cue to topic beginning). This supports
the hypothesis that acknowledgment/agreement may
function as the default interpretation of okay.
Isolated Contextualized
A 426 (39%) 452 (42%)
B 324 (30%) 306 (28%)
C 330 (31%) 322 (30%)
Total 1080 (100%) 1080 (100%)
Table 1: Distribution of label classes in each
study condition.
We examined inter-subject agreement using
Fleiss? ? measure of inter-rater agreement for mul-
tiple raters (Fleiss, 1971).2 Table 2 shows Fleiss? ?
calculated for each individual label vs. the other two
labels and for all three labels, in both study condi-
tions. From this table we see that, while there is very
little overall agreement among subjects about how
to classify tokens in the isolated condition, agree-
ment is higher in the contextualized condition, with
a moderate agreement for class C (? score of .497).
This suggests that context helps distinguish the cue
beginning discourse segment function more than the
other two functions of okay.
2 This measure of agreement above chance is interpreted as
follows: 0 = None, 0 - 0.2 = Small, 0.2 - 0.4 = Fair, 0.4 - 0.6 =
Moderate, 0.6 - 0.8 = Substantial, 0.8 - 1 = Almost perfect.
Isolated Contextualized
A vs. rest .089 .227
B vs. rest .118 .164
C vs. rest .157 .497
all .120 .293
Table 2: Fleiss? ? for each label class
in each study condition.
Recall from Section 3 that the okay tokens were
chosen in equal numbers from three classes accord-
ing to the level of agreement of our three original
labelers (unanimous, majority, and no-agreement),
who had the full dialogue context to use in making
their decisions. Table 3 shows Fleiss? ? measure
now grouped by amount of agreement of the orig-
inal labelers, again presented for each context con-
dition. We see here that the inter-subject agreement
Isolated Context. OL
no-agreement .085 .104 -
majority .092 .299 -
unanimous .158 .452 -
all .120 .293 .312
Table 3: Fleiss? ? in each study condition, grouped
by agreement of the three original labelers (?OL?).
also mirrors the agreement of the three original la-
belers. In both study conditions, tokens which the
original labelers agreed on also had the highest ?
scores, followed by tokens in the majority and no-
agreement classes, in that order. In all cases, tokens
which subjects heard in context showed more agree-
ment than those they heard in isolation.
The overall ? is small at .120 for the isolated con-
dition, and fair at .293 for the contextualized con-
dition. The three original labelers also achieved fair
agreement at .312.3 The similarity between the lat-
ter two ? scores suggests that the full context avail-
able to the original labelers and the limited context
presented to the experiment subjets offer compara-
ble amounts of information to disambiguate between
the three functions, although lack of any context
clearly affected subjects? decisions. We conclude
3 For the calculation of this ?, we considered four label
classes: A, B, C, and a fourth class ?other? that comprises the
remaining 7 word functions mentioned in Section 2. In conse-
quence, these ? scores should be compared with caution.
803
from these results that context is of considerable im-
portance in the interpretation of the word okay, al-
though even a very limited context appears to suf-
fice.
4.2 Representing subject judgments
In this section, we present a graphical representa-
tion of subject decisions, useful for interpreting, vi-
sualizing, and comparing the way our subjects in-
terpreted the different tokens of okay. For each in-
dividual okay in the study, we define an associated
three-dimensional VOTE VECTOR, whose compo-
nents are the proportions of subjects that classified
the token as A, B or C. For example, if a particu-
lar okay was labeled as A by 5 subjects, as B by 3,
and as C by 12, then its associated vote vector is
( 5
20 ,
3
20 ,
12
20
)
= (0.25, 0.15, 0.6). Following this def-
inition, the vectors A = (1, 0, 0), B = (0, 1, 0) and
C = (0, 0, 1) correspond to the ideal situations in
which all 20 subjects agreed on the label. We call
these vectors the UNANIMOUS-VOTE VECTORS.
Figure 2.i shows a two-dimensional representa-
tion that illustrates these definitions. The black dot
Figure 2: 2D representation of a vote vector (i)
and of the cluster centroids (ii).
represents the vote vector for our example okay,
the vertices of the triangle correspond to the three
unanimous-vote vectors (A, B and C), and the cross
in the center of the triangle represents the vote vector
of a three-way tie between the labelers (13 , 13 , 13
).
We are thus able to calculate the Euclidean dis-
tance of a vote vector to each of the unanimous-vote
vectors. The shortest of these distances corresponds
to the label assigned by the plurality4 of subjects.
Also, the smaller that distance, the higher the inter-
subject agreement for that particular token. For our
4Plurality is also known as simple majority: the candidate
who gets more votes than any other candidate is the winner.
example okay, the distances to A, B and C are 0.972,
1.070 and 0.495, respectively; its plurality label is C.
In our experiment, each okay has two associated
vote vectors, one for each context condition. To
illustrate the relationship between decisions in the
isolated and the contextualized conditions, we first
grouped each condition?s 54 vote vectors into three
clusters, according to their plurality label. Figure
2.ii shows the cluster centroids in a two-dimensional
representation of vote vectors. The filled dots corre-
spond to the cluster centroids of the isolated condi-
tion, and the empty dots, to the centroids of the con-
textualized condition. Table 4 shows the distances
in each condition from the cluster centroids (denoted
Ac, Bc, Cc) to the respective unanimous-vote vec-tors (A, B, C), and also the distance between each
pair of cluster centroids.
Isolated Contextualized
d(Ac,A) .54 .44 (?18%)
d(Bc,B) .57 .52 (?10%)
d(Cc, C) .52 .28 (?47%)
d(Ac, Bc) .41 .48 (+17%)
d(Ac, Cc) .49 .86 (+75%)
d(Bc, Cc) .54 .91 (+69%)
Table 4: Distances from the cluster centroids (Ac,
Bc, Cc) to the unanimous-vote vectors (A, B, C)and between cluster centroids, in each condition.
In the isolated condition, the three cluster cen-
troids are approximately equidistant from each other
?that is, the three word functions appear to be
equally confusable. In the contextualized condi-
tion, while Cc is further apart from the other twocentroids, the distance between Ac and Bc remainspractically the same. This suggests that, with some
context available, A and B tokens are still fairly con-
fusable, while both are more easily distinguished
from C tokens. We posit two possible explanations
for this: First, C is the only function for which
the speaker uttering the okay necessarily continues
speaking; thus the role of context in disambiguat-
ing seems quite clear. Second, both A and B have a
common element of ?acknowledgement? that might
affect inter-subject agreement.
804
4.3 Features of the okay tokens
In this section, we describe a set of acoustic,
prosodic, phonetic and contextual features which
may help to explain why subjects interpret okay dif-
ferently. Acoustic features were extracted automat-
ically using Praat. Phonetic and prosodic features
were hand-labeled by expert annotators. Contextual
features were considered only in the analysis of the
contextualized condition, since they were not avail-
able to subjects in the isolated condition.
We examined a number of phonetic features to de-
termine whether these correlated with subject clas-
sifications. We first looked at the production of the
three phonemes in the target okay (/oU/, /k/, /eI/),
noting the following possible variations:
? /oU/: [], [A], [5], [O], [OU], [m], [N], [@], [@U].
? /k/: [G], [k], [kx], [q], [x].
? /eI/: [e], [eI], [E], [e@].
We also calculated the duration of each phone and
of the velar closure. Whether the target okay was at
least partially whispered or not, and whether there
was glottalization in the target okay were also noted.
For each target okay, we also examined its du-
ration and its maximum, mean and minimum pitch
and intensity, as well as the speaker-normalized ver-
sions of these values.5 We considered its pitch slope,
intensity slope, and stylized pitch slope, calculated
over the whole target okay, its last 50, 80 and 100
milliseconds, its second half, its second syllable, and
the second half of its second syllable, as well.
We used the ToBI labeling scheme (Pitrelli et al,
1994) to label the prosody of the target okays and
their surrounding context.
? Pitch accent, if any, of the target okay (e.g., H*,
H+!H*, L*).
? Break index after the target okay (0-4).
? Phrase accent and boundary tone, if any, fol-
lowing the target okay (e.g., L-L%, !H-H%).
For contextualized tokens, we included several fea-
tures related to the exchange between the speaker
uttering the target okay (Speaker B) and the other
speaker (Speaker A).
5Speaker-normalized features were normalized by comput-
ing z-scores (z = (X ?mean)/st.dev) for the feature, where
mean and st.dev were calculated from all okays uttered by the
speaker in the session.
? Number of words uttered by Speaker A in the
context, before and after the target okay. Same
for Speaker B.
? Latency of Speaker A before Speaker B?s turn.
? Duration of silence of Speaker B before and af-
ter the target okay.
? Duration of speech by Speaker B immediately
before and after the target okay and up to a si-
lence.
4.4 Cues to interpretation
We conducted a series of Pearson?s tests to look for
correlations between the proportion of subjects that
chose each label and the numeric features described
in Section 4.3, together with two-sided t-tests to find
whether such correlations differed significantly from
zero. Tables 5 and 6 show the significant results
(two-sided t-tests, p < 0.05) for the isolated and
contextualized conditions, respectively.
Acknowledgement/agreement r
duration of realization of /k/ ?0.299
Backchannel r
stylized pitch slope over 2nd half 2nd syl. 0.752
pitch slope over 2nd half of 2nd syllable 0.409
speaker-normalized maximum intensity ?0.372
pitch slope over last 80 ms 0.349
speaker-normalized mean intensity ?0.327
duration of realization of /eI/ 0.278
word duration 0.277
Cue to discourse segment beginning r
stylized pitch slope over the whole word ?0.380
pitch slope over the whole word ?0.342
pitch slope over 2nd half of 2nd syllable ?0.319
Table 5: Features correlated to the proportion of
votes for each label. Isolated condition.
Table 5 shows that in the isolated condition, sub-
jects tended to classify tokens of okay as Acknowl-
edgment/agreement (A) which had a longer realiza-
tion of the /k/ phoneme. They tended to classify
tokens as Backchannels (B) which had a lower in-
tensity, a longer duration, a longer realization of the
/eI/ phoneme, and a final rising pitch. They tended
to classify tokens as C (cue to topic beginning) that
ended with falling pitch.
805
Acknowledgement/agreement r
latency of Spkr A before Spkr B?s turn ?0.528
duration of silence by Spkr B before okay ?0.404
number of words by Spkr B after okay ?0.277
Backchannel r
pitch slope over 2nd half of 2nd syllable 0.520
pitch slope over last 80 ms 0.455
number of words by Spkr A before okay 0.451
number of words by Spkr B after okay ?0.433
duration of speech by Spkr B after okay ?0.413
latency of Spkr A before Spkr B?s turn ?0.385
duration of silence by Spkr B before okay 0.295
intensity slope over 2nd syllable ?0.279
Cue to discourse segment beginning r
latency of Spkr A before Spkr B?s turn 0.645
number of words by Spkr B after okay 0.481
number of words by Spkr A before okay ?0.426
pitch slope over 2nd half of 2nd syllable ?0.385
pitch slope over last 80 ms ?0.377
duration of speech by Spkr B after okay 0.338
Table 6: Features correlated to the proportion of
votes for each label. Contextualized condition.
In the contextualized condition, we find very dif-
ferent correlations. Table 6 shows that nearly all of
the strong correlations in this condition involve con-
textual features, such as the latency before Speaker
B?s turn, or the number of words by each speaker be-
fore and after the target okay. Notably, only one of
the features that show strong correlations in the iso-
lated condition shows the same strong correlation in
the contextualized condition: the pitch slope at the
end of the word. In both conditions subjects tended
to label tokens with a final rising pitch contour as
B, and tokens with a final falling pitch contour as C.
This supports (Hockey, 1993)?s findings on the role
of pitch contour in disambiguating okay.
We next conducted a series of two-sided Fisher?s
exact tests to find correlations between subjects? la-
belings of okay and the nominal features described
in Section 4.3. We found significant associations be-
tween the realization of the /oU/ phoneme and the
okay function in the isolated condition (p < 0.005).
Table 7 shows that, in particular, [m] seems to be the
preferred realization for B okays, while [@] seems to
be the preferred one for A okays, and [OU] and [O]
for A and C okays.
? [A] [5] [OU] [O] [N] [@U] [@] [] [m]
A 0 0 5 6 4 0 0 8 0 0
B 2 0 4 1 0 1 0 1 1 5
C 1 1 2 3 4 0 1 3 0 0
Table 7: Realization of the /oU/ phoneme, grouped
by subject plurality label. Isolated condition only.
Notably, we did not find such significant asso-
ciations in the contextualized condition. We did
find significant correlations in both conditions, how-
ever, between okay classifications and the type of
phrase accent and boundary tone following the target
(Fisher?s Exact Test, p < 0.05 for the isolated con-
dition, p < 0.005 for the contextualized condition).
Table 8 shows that L-L% tends to be associated with
A and C classes, H-H% with B classes, and L-H%
with A and B classes. In this case, such correlations
are present in the isolated condition, and sustained
or enhanced in the contextualized condition.
H-H% H-L% L-H% L-L% other
Isolated
A 0 2 4 8 9
B 3 3 1 5 3
C 1 1 0 8 5
Context.
A 0 2 3 10 10
B 4 3 2 1 2
C 0 1 0 10 5
Table 8: Phrase accent and boundary tone, grouped
by subject plurality label.
Summing up, when subjects listened to the okay
tokens in isolation, with only their acoustic, prosodic
and phonetic properties available, a few features
seem to strongly correlate with the perception of
word function; for example, maximum intensity,
word duration, and realizing the /oU/ phoneme as
[m] tend to be associated with backchannel, while
the duration of the realization of the /k/ phoneme,
and realizing the /oU/ phoneme as [@] tend to be as-
sociated with acknowledgment/agreement.
In the second part of the study, when subjects
listened to contextualized versions of the same to-
kens of okay, most of the strong correlations of word
function with acoustic, prosodic and phonetic fea-
tures were replaced by correlations with contextual
features, like latency and turn duration. In other
words, these results suggest that contextual features
806
might override the effect of most acoustic, prosodic
and phonetic features of okay. There is nonethe-
less one notable exception: word final intonation ?
captured by the pitch slope and the ToBI labels for
phrase accent and boundary tone ? seems to play a
central role in the interpretation of both isolated and
contextualized okays.
5 Conclusion and future work
In this study, we have presented evidence of differ-
ences in the interpretation of the function of isolated
and contextualized okays. We have shown that word
final intonation strongly correlates with the subjects?
classification of okays in both conditions. Addition-
ally, the higher degree of inter-subject agreement in
the contextualized condition, along with the strong
correlations found for contextualized features, sug-
gests that context, when available, plays a central
role in the disambiguation of okay. (Note, how-
ever, that further research is needed in order to assess
whether these features are indeed, in fact, perceptu-
ally important, both individually and combined.)
We have also presented results suggesting that ac-
knowledgment/agreement acts as a default function
for both isolated an contextualized okays. Further-
more, while that function remains confusable with
backchannel in both conditions, the availability of
some context helps in distinguishing those two func-
tions from cue to topic beginning.
These results are relevant to spoken dialogue sys-
tems in suggesting how systems can convey the cue
word okay with the intended meaning and can inter-
pret users? productions of okay correctly. How these
results extend to other cue words and to other word
functions remains an open question.
As future work, we will extend this study to in-
clude the over 5800 occurrences of alright, gotcha,
huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yup
in the entire Games Corpus, and all 10 discourse
functions mentioned in Section 2, as annotated by
our three original labelers. Since we have observed
considerable differences in conversation style in the
two parts of the corpus (the Objects Games elicited
more ?dynamic? conversations, with more overlaps
and interruptions than the Cards Games), we will
compare cue phrase usage in these two settings. Fi-
nally, we are also interested in examining speaker
entrainment in cue phrase usage, or how subjects
adapt their choice and production of cue phrases to
their conversation partner?s.
Acknowledgments
This work was funded in part by NSF IIS-0307905.
We thank Gregory Ward, Elisa Sneed, and Michael
Mulley for their valuable help in collecting and la-
beling the data, and the anonymous reviewers for
helpful comments and suggestions.
References
Mary E. Beckman and Julia Hirschberg. 1994. The ToBIannotation conventions. Ohio State University.
Paul Boersma and David Weenink. 2001. Praat: Doingphonetics by computer. http://www.praat.org.
Robin Cohen. 1984. A computational theory of the func-
tion of clue words in argument understanding. 22nd
Conference of the ACL, pages 251?258.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-ment among many raters. Psychological Bulletin,76(5):378?382.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
putational Linguistics, 12(3):175?204.
Julia Hirschberg and Diane Litman. 1993. EmpiricalStudies on the Disambiguation of Cue Phrases. Com-
putational Linguistics, 19(3):501?530.
Beth Ann Hockey. 1993. Prosody and the role of okay
and uh-huh in discourse. Proceedings of the Eastern
States Conference on Linguistics, pages 128?136.
Gail Jefferson. 1972. Side sequences. Studies in social
interaction, 294:338.
Jacqueline C. Kowtko. 1997. The function of intonation
in task-oriented dialogue. Ph.D. thesis, University of
Edinburgh.
John Pitrelli, Mary Beckman, and Julia Hirschberg.1994. Evaluation of prosodic transcription labelingreliability in the ToBI framework. In ICSLP94, vol-ume 2, pages 123?126, Yokohama, Japan.
Rachel Reichman. 1985. Getting Computers to Talk Like
You and Me: Discourse Context, Focus, and Seman-
tics: (an ATN Model). MIT Press.
Emanuel A. Schegloff and Harvey Sacks. 1973. Openingup closings. Semiotica, 8(4):289?327.
Nigel Ward and Wataru Tsukahara. 2000. Prosodic fea-tures which cue back-channel responses in English and
Japanese. Journal of Pragmatics, 23:1177?1207.
807
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 60?64,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Adapting Slovak ASR for native Germans speaking Slovak 
 
?tefan Be?u? 1,2, Milo? Cer?ak1, Milan Rusko1, Mari?n Trnka1, Sachia Darjaa1 
1 Institute of Informatics, Slovak Academy of Sciences, Bratislava, Slovakia 
2 Constantine the Philosopher University, Nitra, Slovakia 
 
sbenus@ukf.sk, {Milos.Cernak, Milan.Rusko, Marian.Trnka, 
Sachia.Darzagin}@savba.sk 
 
 
 
Abstract 
We explore variability involved in speech 
with a non-native accent. We first employ a 
combination of knowledge-based and data-
driven approaches for the analysis of 
pronunciation variants between L1 
(German) and target L2 (Slovak). 
Knowledge gained in this two-step process 
is then used in adapting acoustic models 
and the lexicon. We focus on modifications 
in the pronunciation dictionary and speech 
rate. Our results show that the recognition 
of German-accented Slovak is significantly 
improved with techniques modeling slow 
L2 speech, and that the adaptation of the 
pronunciation dictionary yields only 
insignificant gains.  
1 Introduction 
Automatic recognition of non-native accented 
speech represents a complex problem, especially 
since this type of variability becomes more 
common even in languages with a relatively small 
number of speakers due to globalization and 
increased mobility of people. The methods most 
commonly used for dealing with this type of 
speech variability include pronunciation modeling, 
acoustic modeling, or topological modeling (Oh, 
Yoon and Kim, 2007, Tomokiyo, 2000). This 
paper presents an approach that starts with an 
analysis of the pronunciation variability of 
nonnative speech taking into account most salient 
differences between L1 language (in our case 
German) and L2 target language (Slovak). 
Following this knowledge-base step, a semi-
automatic data-driven approach analyzes the 
pronunciation variants on a subset of a training 
corpus is proposed. The knowledge gained in this 
two-step process is then used to adapt our state-of-
the-art ASR system for Slovak in an effort to 
improve the baseline recognition of this system in 
German accented Slovak. We primarily experiment 
with adapting the pronunciation dictionary and 
speech rate. In short, we test the acoustic model 
and lexicon adaptation based on the analysis of 
pronunciation proximity between the German-
accented and standard varieties of Slovak. 
The paper is structured as follows. Section 2 
describes the corpora used for testing and training. 
Section 3 discusses differences between Slovak 
and German pronunciation by analyzing the 
phonological systems of the two languages (3.1) 
and by analyzing the errors Germans make when 
speaking Slovak (3.2).  Section 4 presents the setup 
and results of experiments in adapting our state-of-
the-art ASR system for Slovak to German-accented 
pronunciation of Slovak focusing on speech rate 
manipulation and appending pronunciation 
dictionary. Section 5 discusses the findings and 
concludes the paper. 
2 Description of the databases 
Our testing corpus consists of Slovak sentences 
read by 18 native speakers of German. The 
sentences were selected or created to represent four 
types of variability: dialectological (100), foreign 
accent (100), phonetic richness and balance (300), 
and prosody (90). The first type was based on 
common differences among Slovak dialects, the 
second specially designed for problematic areas of 
native German speakers speaking Slovak. 
60
Depending on the L2 proficiency level of the 
subjects, they were divided into two groups: 
Beginner ? Intermediate (A1-B1), and Upper-
intermediate ? Advanced (B2-C2). The subjects 
were evenly distributed into these two groups with 
9 speakers each. The first group read sentences for 
the dialectological and accent tests accompanied 
by 100 phonetically rich and balance sentences, 
and the second group read all 590 sentences. In 
total, the testing corpus represents 8010 sentences 
(9*300 + 9*590). 
3 Features of Slovak with German accent 
3.1 Knowledge-based approach 
One of the most common ways of predicting 
differences between native (L1) and foreign-
accented (L2) speech is to compare the sound 
systems of L1 and L2. Here we present a brief 
overview of most robust pronunciation differences 
between German and Slovak.  
In terms of segmental inventories, Slovak does 
not have front rounded vowels and has only one 
front mid vowel quality while German has two. 
Also, both languages have phonemically distinct 
short and long vowels, but the length distinction in 
German robustly affects vowel quality (short 
vowels being lax and more centralized), while this 
tendency for Slovak is much less salient and a mid 
central schwa is missing in the Slovak inventory 
(Be?u? and M?dy 2010). Additionally, a major 
difference comes from Slovak palatal consonants 
(stops, nasal, and lateral) that are missing in 
German. Finally, /r/ is an apical trill in Slovak in 
all positions while it commonly has uvular or 
vocalized qualities in German.  
Many allophonic processes are different in the 
two languages. The most perceptually salient 
include the aspiration of voiceless stops and the 
glottalization of initial vowels in German and its 
absence in Slovak. German lacks a so called dark 
/l/ quality in syllable codas while most /l/s in 
Slovak have this quality. In terms of phonotactics, 
Slovak has a richer set of potential onset clusters 
than German. Additionally, Slovak syllabic nuclei 
might be formed by liquids (/l/, /r/) that also 
participate in lengthening alternations, which is not 
the case in German. While both languages have 
pervasive voicing assimilation and neutralization, 
voicing neutralization in obstruent coda consonants 
is slightly more salient in German than in Slovak.  
Finally, most salient prosodic differences 
include a fixed left-most word stress in Slovak (cf. 
variable in German). Slovak in general also 
reduces the length and quality of unstressed vowels 
minimally, while in German, unstressed vowels 
tend to be shortened and centralized.  
3.2 Analysis of accent sentences 
In this section we test the theoretical predictions of 
pronunciation problems in Slovak with German 
accent stemming from interferences between L1 
and L2 described in the previous section. We took 
a subset of our corpus, 100 accent sentences read 
by all 18 speakers and asked trained annotators to 
mark all perceptually salient markers of accented 
speech at the level of segments together with word 
stress differences. Different annotators (N=6) were 
given identical instructions and labeled different 
subsets of the data. A single expert then checked 
all annotations for mistakes and inconsistencies. 
 
 
Figure 1. Error counts for all subjects divided by 
their L2 proficiency level (there were 2540 reference 
phonemes for each speaker) 
 
 
The annotators found 6966 segmental 
differences between ?standard? and German 
accented Slovak, which represents 15.2% of all 
45720 phonemes in the 1800 accent sentences. 
Roughly half of the differences involved syllable 
61
nuclei including liquids (53.1%) and the rest 
involved onset and coda consonants. The 
assignment to proficiency levels showed a fairly 
reasonable correspondence with the number of 
segmental problems in the accent sentences, as can 
be seen in Figure 1 above. 
Given the discussion in Section 3.1, we noticed 
several expected and unexpected patterns in the 
distribution of pronunciation deviations. Table 1 
below lists the most frequent groups of 
pronunciation problems. The expected problems 
involved differences in the palatalization of 
alveolar consonants (15.6%), and the presence of 
aspiration with voiceless plosives (3.3%). Two 
notable unexpected patterns were observed. First, 
despite some differences in the short and long 
syllabic nuclei, described in 3.1, the overall 
frequency of deviations in phonemic length was 
surprising: almost one third (31.6%) of all marked 
differences involved either the shortening of long 
nuclei or lengthening of short ones. Additionally, 
despite the clear and predictable placement of 
Slovak word stress, 13.7% of differences involved 
an incorrect placement of word stress. The 
production of German vowel quality (such as front 
rounded vowels or schwa) was relatively low 
(1.8%). Hence, prosodic and metrical features of 
vowels were perceived as far more problematic 
than the features related to their quality.  
 
Type of error Count % 
Vowel shortening 1164 16.7 
Palatalization 1090 15.6 
Obstruent voicing 1078 15.5 
Vowel lengthening 1038 14.9 
Nucleus stress 954 13.7 
Rhotic 537 7.7 
Aspiration 227 3.3 
German vow. quality 123 1.8 
 
Table 1: Most common errors in accent sentences 
 
The second unexpected pattern was a relatively 
high frequency of differences in the voicing of 
obstruent consonants (15.5%). The majority of 
these cases included the devoicing of consonants 
that, in regular fluent Slovak, would be produced 
as voiced. This pattern is related to pervasive coda 
voicing neutralization in German mentioned in 
section 3.1. Voicing of canonically voiceless 
consonants was observed as well, especially in the 
voicing of /s/ to /z/. 
It is worth noting that both of the unexpected 
patterns relate to speech rate. A generally slower 
rate of L2 speakers results in frequent pauses 
between words thus creating an environment that 
meets the description for obstruent devoicing in 
German and prevents across-the-word voice 
assimilation that is pervasive in Slovak. 
Additionally, the presence of these pauses 
facilitates so called pre-boundary lengthening (e.g. 
Delattre, 1968 for German), in which the rime of 
the pre-pausal syllable is elongated. Finally, a 
generally slower rate may result in vowels 
intended as short to be perceived as long especially 
in the speech that is slowed down locally (for 
example with unknown words for L2 speakers).  
4 ASR experiment  
The analysis of accent sentences in the previous 
section revealed a potential impact of slower 
speaking rate of L2 speakers on the frequency of 
pronunciation deviations. We test the effects of 
speaking rate and variability in the pronunciation 
dictionary on the recognition of German accented 
Slovak in the following experiment. 
4.1 Test setup 
The training audio database contained 130 hours of 
phonetically rich sentences, gender balanced, from 
domains such as news and articles from various 
magazines, recorded from 140 speakers with 
Sennheiser ME3 headset microphone with 
Sennheiser MZA 900 P in-line preamplifier and 
EMU Tracker Pre USB audio interface. Database 
was annotated using the Transcriber annotation 
tool (Barras et al, 2000), twice checked and 
corrected. Recordings were split on segments if 
possible not bigger than 10 sec. 
The training text corpora contained a total of 
about 92 million sentences with 1.25 billion Slovak 
words. A general-domain trigram language model 
(LM) was created with a vocabulary size of 350k 
unique words (400k pronunciation variants) which 
passed the spell-check lexicon and subsequently 
were also checked manually. Similarly to other 
recognizers in Slovak (Sta?, Hl?dek and Juh?r, 
2010) the modified Kneser-Ney algorithm was 
used as a smoothing technique. The general LM 
62
was adapted with all 590 sentences from the target 
domain. 
The Julius decoder (Lee, Kawahara, and 
Shikano, 2001) was used as a reference speech 
recognition engine, and the HTK toolkit was used 
for word-internal acoustic models (AMs) training. 
We trained AMs using the triphone mapping as 
described in (Darjaa et al, 2011), with 32 Gaussian 
densities per each HMM state. 
Experiments have been performed using AMs 
and LM trained from the training databases, and 
the 8010 sentences from the testing corpus as 
described in Section 2. 
4.2 Results 
To estimate the potential effect of slow L2 speech 
on the recognition accuracy, we first performed 
signal level acceleration directly on the recorded 
waveforms. The Praat speech analysis system 
(Boersma and Weenink 2011) was used, 
particularly its functionality of adjusting the time-
domain of a sound file with a fixed conversion 
factor used in subsequent PSOLA resynthesis of 
the resulting file. We resynthesized all test 
sentences using the factors 0.9, 0.7, and 0.5 (the 
last one corresponding to 50% duration of the 
original file) and performed recognition with an 
unadapted LM that had the baseline WER of 55%. 
The results showed that the acceleration factor 
with the highest accuracy gain was 0.7, which 
improved the baseline to 43.4% WER. Factor 0.9 
lowered WER to 49.5% while factor 0.5 showed 
the worst result (54.1% WER).  
Following this encouraging initial result, feature 
level acceleration was performed by simple change 
of frame shift in the ASR front-end. The original 
features were calculated from 25 ms frame 
durations and a 10 ms frame shift. While keeping 
the frame durations constant, we increased the 
frame shift to 14 ms. This corresponds to the 
acceleration factor of 0.714, approximately 
identical to the best performing factor in the signal 
modulation experiments. 
Table 2 shows achieved recognition results 
based on the adapted LM used as the baseline. This 
refers to the performance of the system on German 
accent sentences without any rate modifications. 
Unfortunately, we don?t have a corpus of these 
sentences produced by Slovak speakers to provide 
a system baseline for non-accented speech but in a 
similar, albeit larger, corpus of 18 speakers reading 
380 sentences this system?s WER was 21.3% 
(Be?u? et al, 2011).  
Speaker rate was accelerated at the signal and 
feature levels. We see that both signal and feature 
adaptation of speech rate significantly improved 
the accuracy of recognition with the latter 
outperforming the former. The extent of the 
improvement is rather surprising and suggests that 
speech rate in read sentences is a major factor 
when recognizing German-accented Slovak. 
 
Test WER 
% 
Baseline 40.58 
Alternate dictionary 40.48 
Signal-adapted speech rate 28.67 
Signal-adapted rate+alt. dictionary 28.13 
Feature-adapted speech rate 25.79 
Feature-adapted rate+alt. dictionary 25.33 
 
Table 2: Word error rates (WER) for signal and 
feature adaptations (speech rate accelerations). 
 
The analysis in section 3 also identified two 
common patterns: devoicing of consonants of 
German speakers that, in regular fluent Slovak, 
would be produced as voiced, and vowel 
shortening of German speakers. We tried to use 
this knowledge for improving the speech 
recognition system. In order to better match the 
pronunciation of German speakers in Slovak ASR 
system, we added alternative pronunciations to 
each entry of Slovak dictionary according to Table 
3. For example, the entry ?Aachene? with 
pronunciation /a: x e J e/, was extended with an 
alternative pronunciation /a x e n e/ by the 
application of the rules in the 1st and 4th rows. 
 
 
Original phones Phones used in  
alternative pronunciations 
/J/, /n/ /n/ 
/c/, /t/ /t/ 
/J\/, /d/ /d/ 
/a:/ /e:/ /i:/ /o:/ /u:/ /a/, /e/, /i/, /o/, /u/ 
 
Table 3: Rules for generation of alternative 
pronunciations (/J/, /c/, /J\/ are Slovak SAMPA symbols 
for palatal variants of /n/, /t/, and /d/ respectively).  
 
63
The results in Table 2 show that the changes to 
the dictionary resulted in only insignificant 
improvements on top of the rate adjustment. 
Finally, we compared the average WER for 
individual speakers in the baseline system with the 
adapted systems. For 17 out of 18 speakers the 
improvement was greater than 5% and ranged up 
to 34%; only one speaker?s results showed 
deterioration (2%). Interestingly, despite a 
relatively good correspondence between the 
proficiency level and the number of pronunciation 
errors showed in Figure 1, neither the recognition 
accuracy of the adapted model, nor the extent of 
improvement after feature adaptation, showed a 
consistent relationship to the perceived proficiency 
of our subjects. This may be due to the greater 
number and complexity of test sentences used for 
advanced speakers compared to the beginners. 
5 Discussion and conclusion 
Our results showed that adjusting the rate of non-
native speech to resemble the rate of the native 
training corpus significantly improves the 
recognition of speech with foreign accent. 
Moreover, we showed that feature-based 
acceleration outperforms signal-based acceleration. 
This is important since feature-based acceleration 
is much easier to perform, and an ASR system runs 
faster as it processes less frames. Furthermore, it is 
plausible that speech rate variability will be similar 
in non-native accents of multiple L1 languages, 
which cannot be expected for the pronunciation 
variants. Hence, although the acceleration of the 
signal or features does not account for all of the 
phonetic interference phenomena described in 
Section 3.2, sophisticated speech rate modeling 
that includes the combination of phone rate, 
syllable rate, and word rate promises to provide a 
robust technique for dealing with variability 
stemming from non-native accents.  
6 Acknowledgments 
This work was supported by the European Project 
of Structural Funds, ITMS: 26240220064. 
References  
Barras, C., Geoffrois, E., Wu, Z., and Liberman, M. 
2000. Transcriber: development and use of a tool for 
assisting speech corpora production. Speech 
Communication, 33 (1?2). 
Be?u?, ?., Cer?ak, M., Rusko, M., Trnka, M., Darjaa, 
S., and Sabo, R. 2011. Semi-automatic approach to 
ASR errors categorization in multi-speaker corpora. 
Proceedings of the International Conference Slovko. 
Be?u?, ?, and M?dy, K. 2010. Effects of lexical stress 
and speech rate on the quantity and quality of Slovak 
vowels. Proceedings of the 5th International 
Conference on Speech Prosody. 
Boersma, P., and Weenink, D. 2011. Praat: doing 
phonetics by computer [Computer program, 
http://www.praat.org/].  
Darjaa, S., Cer?ak, M., Trnka, M., Rusko, M., Sabo, R. 
2011. Effective Triphone Mapping for Acoustic 
Modeling in Speech Recognition. Proceedings of 
Interspeech 2011 Conference. 
Delattre, P. 1968. A Comparison of Syllable Length 
Conditioning Among Languages. International 
Review of Applied Linguistics, IV:183-198. 
Gusfield, D.. 1997. Algorithms on Strings, Trees and 
Sequences. Cambridge University Press, Cambridge, 
UK. 
Lee, A., Kawahara, T., and Shikano, K. 2001. Julius ? 
an Open Source Real-Time Large Vocabulary 
Recognition Engine. In Proc. of the European 
Conference on Speech Communications and 
Technology (EUROSPEECH). 
Oh, Y.R., Yoon, J.S., Kim, H.K. 2007. Acoustic model 
adaptation based on pronunciation variability 
analysis for non-native speech recognition. Speech 
Communication 49(1), 59-70.  
Sta?, J., Hl?dek, D., Juh?r, J. 2010. Language Model 
Adaptation for Slovak LVCSR. In Proc. of the Intl. 
Conference on AEI, pp. 101?106. 
Tomokiyo, L.M., 2000. Lexical and acoustic modeling 
of non-native speech in LVCSR. In: Proc. ICSLP, 
Beijing, China, pp. 346?349. 
64
