Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 551?559,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Context-Dependent Term Relations for Information Retrieval 
 
Jing Bai      Jian-Yun Nie     Guihong Cao
DIRO, University of Montreal 
CP. 6128, succ. Centre-ville, Montreal,  
Quebec H3C 3J7, Canada 
{baijing,nie,caogui}@iro.umontreal.ca 
 
 
 
Abstract 
Co-occurrence analysis has been used to 
determine related words or terms in many 
NLP-related applications such as query 
expansion in Information Retrieval (IR). 
However, related words are usually 
determined with respect to a single word, 
without relevant information for its 
application context. For example, the word 
?programming? may be considered to be 
strongly related to ?Java?, and applied 
inappropriately to expand a query on ?Java 
travel?. To solve this problem, we propose 
to add another context word in the relation 
to specify the appropriate context of the 
relation, leading to term relations of the 
form ?(Java, travel) ? Indonesia?. The 
extracted relations are used for query 
expansion in IR. Our experiments on 
several TREC collections show that this 
new type of context-dependent relations 
performs much better than the traditional 
co-occurrence relations.  
1. Introduction 
A query usually is a poor expression of an 
information need. This is not only due to its short 
length (usually a few words), but also due to the 
inability of users to provide the best terms to 
describe their information need. At best, one can 
expect that some, but not all, relevant terms are 
used in the query. Query expansion thus aims to 
improve query expression by adding related 
terms to the query. However, the effect of query 
expansion is strongly determined by the term 
relations used (Peat and Willett, 1991). For 
example, even if ?programming? is strongly 
related to ?Java?, if this relation is used to 
expand a query on ?Java travel?, the retrieval 
result will likely deteriorate because the 
irrelevant term ?programming? is introduced, 
leading to the retrieval of irrelevant documents 
about ?programming?.  
    A number of attempts have been made to deal 
with the problem of selecting appropriate 
expansion terms. For example, Wordnet has been 
used in (Voorhees, 1994) to determine the 
expansion terms. However, the experiments did 
not show improvement on retrieval effectiveness. 
Many experiments have been carried out using 
associative relations extracted from term co-
occurrences; but they showed variable results 
(Peat and Willett, 1991). In (Qiu and Frei, 1993), 
it is observed that one of the reasons is that one 
tried to determine expansion terms according to 
each original query term separately, which may 
introduce much noise. Therefore, they proposed 
to determine the expansion terms by summing up 
the relations of a candidate expansion term to 
each of the query terms. In so doing, a candidate 
expansion term is preferred if it has a strong 
relationship with many of the query terms. 
However, it is still difficult to prevent the 
expansion process from adding ?programming? 
to a query on ?Java travel? because of its very 
strong relation with ?Java?. 
The approach used in (Qiu and Frei, 1993) 
indeed tries to correct a handicap inherent in the 
relations: as term relations are created between 
two single words such as ?Java ? 
programming?, no information is available to 
help determine the appropriate context to apply 
it. The approach used in (Qiu and Frei, 1993) can 
simply alleviate the problem without solving it 
radically. 
    In this paper, we argue that the solution lies in 
the relations themselves. They have to contain 
more information to help determine the 
appropriate context to apply them. We thus 
propose a way to add some context information 
into the relations: we introduce an additional 
word into the condition part of the relation, such 
as ?(Java, computer) ? programming?, which 
551
 means ?programming? is related to ?(Java, 
computer)? together. In so doing, we would be 
able to prevent from extracting and applying a 
relation such as ?(Java, travel) ? 
programming?.  
    In this paper, we will test the extracted 
relations in query expansion for IR. We choose to 
implement query expansion within the language 
modeling (LM) framework because of its 
flexibility and high performance. The 
experiments on several TREC collections will 
show that our query expansion approach can 
bring large improvements in retrieval 
effectiveness. 
    In the following sections, we will first review 
some of the relevant approaches on query 
expansion and term relation extraction. Then we 
will describe our general IR models and the 
extraction of term relations. The experimental 
results will be reported and finally some 
conclusions will be drawn. 
2. Query Expansion and Term Relations 
It has been found that a key factor that 
determines the effect of query expansion is the 
selection of appropriate expansion terms (Peat 
and Willett, 1991). To determine expansion 
terms, one possible resource is thesauri 
constructed manually, such as Wordnet. Thesauri 
contain manually validated relations between 
terms, which can be used to suggest related 
terms. (Voorhees, 1994) carried out a series of 
experiments on selecting related terms (e.g. 
synonyms, hyonyms, etc.) from Wordnet. 
However, the experiments did not show that this 
can improve retrieval effectiveness. Some of the 
reasons are as follows: Although Wordnet 
contains many relations validated by human 
experts, the coverage is far from complete for the 
purposes of IR: not only linguistically motivated 
relations, but also association relations, are useful 
in IR. Another problem is the lack of information 
about the appropriate context to apply relations. 
For example, Wordnet contains two synsets for 
?computer?, one for the sense of ?machine? and 
another for ?human expert?. It is difficult to 
automatically select the correct synset to expand 
the word ?computer? even if we know that the 
query?s area is computer science. 
Another often used resource is associative 
relations extracted from co-occurrences: two 
terms that co-occur frequently are thought to be 
associated to each other (Jing and Croft, 1994). 
However, co-occurrence relations are noisy: 
Frequently co-occurring terms are not necessarily 
related. On the other hand, they can also miss 
true relations. The most important problem is still 
that of ambiguity: when one term is associated 
with another, it may be related for one sense and 
not for other possible senses. It is then difficult to 
determine when the relation applies. 
In most of the previous studies, relations 
extracted are restricted between one word and 
another. This limitation makes the relations 
ambiguous, and their utilization in query 
expansion often introduces undesired terms. We 
believe that the key to make a relation less 
ambiguous is to add some contextual 
information. 
In an attempt to select better expansion terms, 
(Qiu and Frei, 1993) proposed the following 
approach to select expansion terms: terms are 
selected according to their relation to the whole 
query, which is calculated as the sum of their 
relations to each of the query terms. Therefore, a 
term that is related to several query terms will be 
favored. In a similar vein, (Bai et al 2005) also 
try to determine the relationship of a word to a 
group of words by combining its relationships to 
each of the words in the group. This can indeed 
select better expansion terms. The consideration 
of other query terms produces a weak contextual 
effect. However, this effect is limited due to the 
nature of the relations extracted, in which a term 
depends on only one other term. Much of the 
noise in the sets will remain after selection.  
For a query composed of several words, what 
we would really like to have is a set of terms that 
are related to all the words taken together (and 
not separately). By combining words in the 
condition part such as ?(Java, travel)? or ?(base, 
bat)?, each word will serve as a context to the 
other in order to constrain the related terms. In 
these cases, we would expect that ?hotel?, 
?island? or ?Indonesia? would co-occur much 
more often with ?(Java, travel)? than 
?programming?, and ?ball?, ?catcher? etc. co-
occur much more often with ?(base, bat)? than 
?animal? or ?foundation?. 
One naturally would suggest that compound 
terms can be used for this purpose. However, for 
many queries, it is difficult to form a legitimate 
compound term. Even if we can detect one 
occurrence of a compound, we may miss others 
that use its variants. For example, if ?Java travel? 
is used as a query, we will likely be able to 
consider it as a compound term. The same 
compound (or its variant) would be difficult to 
552
 detect in a document talking about traveling to 
Java: the two words may appear at some distance 
or not in some specific syntactic structure as 
required in (Lin, 1997). This will lead to the 
problem of mismatching between document and 
query. 
In fact, compound terms are not the only way 
to add contextual information to a word. By 
putting two words together (without forming a 
compound term), we usually obtain a more 
precise sense for each word. For example, from 
?Java travel?, we can guess that the intended 
meaning is likely related to ?traveling to Java 
Island?. People will not interpret this 
combination in the sense of ?Java 
programming?. In the same way, people would 
not consider ?animal? to be a related term to 
?base, bat?. These examples show that in a 
combination of words, each word indeed serves 
to specify a context to interpret another word. It 
then suggests the following approach: we can 
adjunct some additional word(s) in the condition 
part of a relation, such as ?(Java, travel) ? 
Indonesia?, which means ?Indonesia? is related 
to ?(Java, travel)? together. It is expected that 
one would not obtain ?(Java, travel) ? 
programming?. 
Owing to the context effect explained above, 
we will call the relations with multiple words in 
the condition part context-dependent relations. In 
order to limit the computation complexity, we 
will only consider adding one additional word 
into relations.  
The proposed approach follows the same 
principle as (Yarowsky, 1995), which tried to 
determine the appropriate word sense according 
to one relevant context word. However, the 
requirement for query expansion is less than 
word sense disambiguation: we do not need to 
know the exact word sense to make expansion. 
We only need to determine the relevant 
expansion terms. Therefore, there is no need to 
determine manually a set of seeds before the 
learning process takes place. 
To some extent, the proposed approach is also 
related to (Sch?tze and Pedersen, 1997), which 
calculate term similarity according to the words 
appearing in the same context, or to second-order 
co-occurrences. However, a key difference is that 
(Sch?tze and Pedersen, 1997) consider only 
separate context words, while we consider 
multiple context words together. 
Once term relations are determined, they will 
be used in query expansion. The basic IR process 
will be implemented in a language modeling 
framework. This framework is chosen for its 
flexibility to integrate term relations. Indeed, the 
LM framework has proven to be capable of 
integrating term relations and query expansion 
(Bai et al, 2005; Berger and Lafferty, 1999; Zhai 
and Lafferty, 2001). However, none of the above 
studies has investigated the extraction of strong 
context-dependent relations from text collections. 
In the next section, we will describe the 
general LM framework and our query expansion 
models. Then the extraction of term relation will 
be explained. 
3. Context-Dependent Query Expansion 
in Language Models 
The basic IR approach based on LM (Ponte and 
Croft, 1998) determines the score of relevance of 
a document D by its probability to generate the 
query Q. By assuming independence between 
query terms, we have: 
??
??
?=
Qw
i
Qw
i
ii
DwPDwPDQP )|(log)|()|(  
where )|( DwP i denotes the probability of a word 
in the language model of the document D. As no 
ambiguity will arise, we will use D to mean both 
the language model of the document and the 
document itself (similarly for a query model and 
a query Q). 
Another score function is based on KL-
divergence or cross entropy between the 
document model and the query model: 
?
?
=
Vw
ii
i
DwPQwPQDscore )|(log)|(),(
 
where V is the vocabulary. Although we have 
both document and query models in the above 
formulation, usually only the document model is 
smoothed, while the query model uses Maximum 
Likelihood Estimation (MLE) )|( QwP iML . Then 
we have: 
?
?
=
Qw
iiML
i
DwPQwPQDscore )|(log)|(),(  
However, it is obvious that a distance (KL-
divergence) measured between a short query of a 
few words and a document cannot be precise. A 
better expression would contain all the related 
terms. The construction of a better query 
expression is the very motivation for query 
expansion in traditional IR systems. It is the same 
in LM for IR: to create a better query expression 
(model) to be able to measure the distance to a 
553
 document in a more precise way. The key to 
creating the new model is the integration of term 
relations. 
3.1 LM for Query Expansion 
Term relations have been used in several recent 
language models in IR. (Berger and Lafferty, 
1999) proposed a translation model that expands 
the document model. The same approach can also 
be used to expand the query model. Following 
(Berger and Lafferty, 1999), we arrive at the first 
expansion model as follows, which has also been 
used in (Bai et al, 2005): 
Model 1: Context-independent query 
expansion model (CIQE) 
??
??
==
Qq
jMLjiR
Vq
jiRiR
jj
QqPqwPQqwPQwP )|()|()|,()|(  
In this model, each original query term qj is 
expanded by related terms wi. The relations 
between them are determined by )|( jiR qwP . We 
will explain how this probability is defined in 
Section 3.2. However, we can already see here 
that wi is determined solely by one of the query 
term qj. So, we call this model ?context-
independent query expansion model? (CIQE). 
The above expanded query model enables us 
to obtain new related expansion terms, to which 
we also have to add the original query. This can 
be obtained through the following smoothing: 
?
?
?
+=
Qq
jMLjiR
iMLi
j
QqPqwP
QwPQwP
)|()|()1(                   
)|()|(
1
1
?
?
      (1) 
where 1? is a smoothing parameter. 
However, if the query model is expanded on 
all the vocabulary (V), the query evaluation will 
be very time consuming because the query and 
the document have to be compared on every word 
(dimension). In practice, we observe that only a 
small number of terms have strong relations with 
a given term, and the terms having weak relations 
usually are not truly related. So we can limit the 
expansion terms only to the strongly related ones. 
By doing this, we can also expect to filter out 
some noise and considerably reduce the retrieval 
time. 
Suppose that we have selected a set E of 
strong expansion terms. Then we have: 
?
?
??
?
?
=
QEw
ii
Vw
ii
i
i
DwPQwP
DwPQwPQDscore
)|(log)|(                    
)|(log)|(),(
 
This query expansion method uses the same 
principle as (Qiu and Frei, 1993), but in a LM 
setting: the selected expansion terms are those 
that are strongly related to all the query terms 
(this is what the summation means). The 
approach used in (Bai et al, 2005) is slightly 
different: A context vector is first built for each 
word; then a context vector for a group of words 
(e.g. a multi-word query) is composed from the 
context vectors of the words of the group; finally 
related terms to the group of words are 
determined according to the similarity of their 
context vectors to that of the group. This last step 
uses second-order co-occurrences similarly to 
(Sch?tze and Pedersen, 1997). In both (Qiu and 
Frei, 1993) and (Bai et al, 2005), the terms 
related to a group of words are determined from 
their relations to each of the words in the group, 
while the latter relations are extracted separately. 
Irrelevant expansion terms can be retained. 
As we showed earlier, in many cases, when 
one additional word is used with another word, 
the sense of each of them can usually be better 
determined. This additional word may be 
sufficient to interpret correctly many multi-word 
user queries. Therefore, our goal is to extract 
stronger context-dependent relations of the form 
(qj qk) ? wi, or to build a probability 
function )|( kjiR qqwP . Once this function is 
determined, it can be integrated into a new 
language model as follows. 
Model 2: Context-dependent query expansion 
model (CDQE) 
?
?
?
?
?
=
Qqq
kjkjiR
Vqq
kjkjiRiR
kj
kj
QqqPqqwP
QqqPqqwPQwP
,
,
)|()|(                 
)|()|()|(
 
As )|( kjiR qqwP  is a relation with two terms as 
condition, we will also call it a biterm relation. 
The name ?biterm? is due to (Srikanth and 
Srihari, 2002), which means two terms co-
occurring within some distance. Similarly, 
)|( jiR qwP  will be called unigram relation. The 
corresponding query models will be called biterm 
relation model and unigram relation model.  
As in general LM, the biterm relation model 
can be smoothed with a unigram model. Then we 
have the following score function: 
?
?
?
+=
Qqq
kjkjiR
iMLiR
kj
QqqPqqwP
QwPQwP
,
2
2
)|()|()1(                    
)|()|(
?
?
  (2) 
554
 where 2?  is another smoothing parameter. 
3.2 Extraction of Term Relations 
The key problem now is to obtain the relations 
we need: )|( jiR wwP  and )|( kjiR wwwP . For the first 
probability, as in many previous studies, we 
exploit term co-occurrences. )|( jiR wwP  could be 
built as a traditional bigram model. However, this 
is not a good approach for IR because two related 
terms do not necessarily co-occur side by side. 
They often appear at some distance. Therefore, 
this model is indeed a biterm model (Srikanth 
and Srihari, 2002), i.e., we allow two terms be 
separated within some distance. We use the 
following formula to determine this probability: 
?=
lw
jl
ji
jiR
wwc
wwc
wwP
),(
),()|(  
where ),( ji wwc  is the frequency of co-occurrence 
of the biterm ),( ji ww , i.e. two terms in the same 
window of fixed size across the collection. In our 
case, we set the window size at 10 (because this 
size turned out to be reasonable in our pilot 
experiments). 
For )|( kji wwwP , we further extend the biterm 
to triterm, and we use the frequency of co-
occurrences of three terms ),,( kji wwwc  within the 
same windows in the document collection: 
?=
lw
kjl
kji
kjiR
wwwc
wwwc
wwwP
),,(
),,()|(  
The number of relations determined in this 
way can be very large. The upper bound for 
)|( ji wwP  and )|( kji wwwP  are respectively 
O(|V|2) and O(|V|3). However, many relations 
have very low probabilities and are often noise. 
As we only consider a subset of strong expansion 
terms, the relations with low probability are 
almost never used. Therefore, we set two filtering 
criteria: 
? The biterm in the condition of a relation should 
be higher than a threshold (10 in our case); 
? The probability of a relation should be higher 
than another threshold (0.0001 in our case). 
? One more filtering criterion is mutual 
information (MI), which reflects the 
relatedness of two terms in their combination 
),( kj ww . To keep a relation )|( kji wwwP , we 
require ),( kj ww  be a meaningful combination. 
We use the following pointwise MI (Church 
and Hanks 1989): 
)()(
),(
log),(
kj
kj
kj
wPwP
wwP
wwMI =  
 We only keep meaningful combinations such 
that 0),( >kj wwMI .  
By these filtering criteria, we are able to 
reduce considerably the number of biterms and 
triterms. For example, on a collection of about 
200MB, with a vocabulary size of about 148K, 
we selected only about 2.7M useful biterms and 
about 137M triterms, which remain tractable. 
3.3 Probability of Biterms 
In LM used in IR, each query term is attributed 
the same weight. This is equivalent to a uniform 
probability distribution, i.e.: 
U
i QQqP ||
1)|( =  
where |Q|U is the number of unigrams in the 
query. In CIQE model, we use the same method.  
In CDQE, we also need to attribute a 
probability )|( QqqP kj , to the biterm ),( kj qq . 
Several options are possible. 
Uniform probability 
This simple approach distributes the probability 
uniformly among all biterms in the query, i.e.: 
B
kj QQqqP ||
1)|( =  
where BQ ||  is the number of biterms in Q.  
According to mutual information 
In a query, if two words are strongly associated, 
this also means that their association is more 
meaningful to the query, thus should be weighted 
higher. Therefore, a natural way to assign a 
probability to a biterm in the query is to use 
mutual information, which denotes the strength 
of association between two words. We use again 
the pointwise mutual information MI(qj, qk). If it 
is negative, we consider that the biterm is not 
meaningful, and is ignored. Therefore, we arrive 
at the following probability function: 
?
?
=
Qqq
ml
kj
kj
ml
qqMI
qqMIQqqP
)(
),(
),()|(  
where Qqq ml ?)(  means all the meaningful 
biterms in the query.  
555
 Statistical parsing 
In (Gao et al, 2002), a statistical parsing 
approach is used to determine the best 
combination of translation words for a query. The 
approach is similar to building a minimal 
spanning tree, which is also used in (Smeaton and 
Van Rijsbergen, 1983), to select the strongest 
term relations that cover the whole query. This 
approach can also be used in our model to 
determine the minimal set of the strongest 
biterms that cover the query.  
In our experiments, we tested all the three 
weighting schemas. It turns out that the best 
weighting is the one with MI. Therefore, in the 
next section, we will only report the results with 
the second option. 
4. Experimental Evaluation 
We evaluate query expansion with different 
relations on four TREC collections, which are 
described in Table 1. All documents have been 
processed in a standard manner: terms are 
stemmed using Porter stemmer and stopwords are 
removed. We only use titles of topics as queries, 
which contain 3.58 words per query on average.  
Table 1. TREC collection statistics 
Coll. Description Size (Mb) Vocab. # Doc. Query 
AP Associated Press (1988-89) 491 196,933 164,597 51-100 
SJM 
San Jose 
Mercury News 
(1991) 
286 146,514 90,257 101-150 
WSJ 
Wall Street 
Journal (1990-
92) 
242 121,946 74,520 51-100 
In our experiments, the document model 
remains the same while the query model changes. 
The document model uses the following Dirichlet 
smoothing: 
?
?
+
+
=
U
iMLi
i D
CwPDwtf
DwP ||
)|(),()|(
 
where ),( Dwtf i is the term frequency of wi in D, 
)|( CwP iML  is the collection model and ?  is the 
Dirichlet prior, which is set at 1000 following 
(Zhai and Lafferty, 2001).  
There are two other smoothing parameters 1? , 
and 2?  to be determined. In our experiments, we 
use a simple method to set them: the parameters 
are tuned empirically using a training collection 
containing AP1989 documents and queries 101-
150. These preliminary tests suggest that the best 
value of 1?  and 2?  (in Equations 1-2) are 
relatively stable (we will show this later). In the 
experiments reported below, we will use 4.01 =? ,  
and 3.02 =? . 
4.1 Experimental Results 
The main experimental results are described in 
Table 2, which reports average precision with 
different methods as well as the number of 
relevant documents retrieved. UM is the basic 
unigram model without query expansion (i.e. we 
use MLE for the query model, while the 
document model is smoothed with Dirichlet 
method). CIQE is the context-independent query 
expansion model using unigram relations (Model 
1). CDQE is the context-dependent query 
expansion model using biterm relations (Model 
2). In the table, we also indicate whether the 
improvement in average precision obtained is 
statistically significant (t-test). 
Table 2. Avg. precision and Recall  
Coll. 
#Rel. UM CIQE CDQE 
0.2767 0.2902 (+5%*) 0.3383  (+22%**) 
             [+17%**] AP 6101 3677 3897 4029 
0.2017 0.2225 (+10%**) 0.2448 (+21%**) 
            [+10%*] SJM 2559 1641 1761 1873 
0.2373 0.2393 (+1%) 0.2710 (+14%**) 
            [+13%*] WSJ 2172 1588 1626 1737 
* and ** indicate that the difference is statistically 
significant according to t-test: * indicates p<0.05, ** 
indicates p<0.01; (.) is compared to UM and [.] is 
compared to CIQE. 
CIQE and CDQE vs. UM 
It is interesting to observe that query expansion, 
either by CIQE or CDQE, consistently 
outperforms the basic unigram model on all the 
collections. In all the cases except CIQE for 
WSJ, the improvements in average precision are 
statistically significant. At the same time, the 
increases in the number of relevant documents 
retrieved are also consistent with those in average 
precision. 
The improvement scales obtained with CIQE 
are relatively small: from 1% to 10%. These 
correspond to the typical figure using this 
method.  
Comparing CIQE and CDQE, we can see that 
context-dependent query expansion (CDQE) 
556
 always produces better effectiveness than 
context-independent expansion (CIQE). The 
improvements range between 10% and 17%. All 
the improvements obtained by CDQE are 
statistically significant. This result strongly 
suggests that in general, the context-dependent 
term relations identify better expansion terms 
than context-independent unigram relations. This 
confirms our earlier hypothesis.  
Indeed, when we look at the expansion 
results, we see that the expansion terms 
suggested by biterm relations are usually better. 
For example, the (stemmed) expansion terms for 
the query ?insider trading? suggested 
respectively by CIQE and CDQE are as follows: 
CIQE:  stock:0.0141 market:0.0113 US:0.0112 
year:0.0102 exchang:0.0101 trade:0.0092 
report:0.0082 price:0.0076 dollar:0.0071 
1:0.0069 govern:0.0066 state:0.0065 
futur:0.0061 million:0.0061 dai:0.0060 
offici:0.0059 peopl:0.0059 york:0.0057 
issu:0.0057 ? 
CDQE:  secur:0.0161 charg:0.0158 stock:0.0137 
scandal:0.0128 boeski:0.0125 inform:0.0119 
street:0.0113 wall:0.0112 case:0.0106 
year:0.0090 million:0.0086 investig:0.0082 
exchang:0.0080 govern:0.0077 sec:0.0077 
drexel:0.0075 fraud:0.0071 law:0.0063 
ivan:0.0060 ? 
We can see that in general, the terms suggested 
by CDQE are much more relevant. In particular, 
it has been able to suggest ?boeski? (Boesky) 
who is involved in an insider trading scandal. 
Several other terms are also highly relevant, such 
as scandal, investing, sec, drexel, fraud, etc. 
The addition of these new terms does not only 
improve recall. Precision of top-ranked 
documents is also improved. This can be seen in 
Figure 1 where we compare the full precision-
recall curve for the AP collection for the three 
models. We can see that at all the recall levels, 
the precision values always follow the following 
order: CDQE > UM. The same observation is 
also made on the other collections. This shows 
that the CDQE method does not increase recall to 
the detriment of precision, but both of them. In 
contrast, CIQE increases precision at all but 0.0 
recall points: the precision at the 0.0 recall point 
is 0.6565 for CIQE and 0.6699 for UM. This 
shows that CIQE can slightly deteriorate the top-
ranked few documents. 
Figure 1. Comparison of three models on AP 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Pr
e
c
is
io
n CDQE
CIQE
UM
 
CDQE vs. Pseudo-relevance feedback 
Pseudo-relevance feedback is widely considered 
to be an effective query expansion method. In 
many previous experiments, it produced very 
good results. The mixture model (Zhai and 
Lafferty, 2001) is a representative and effective 
method to implement pseudo-relevance feedback: 
It uses a set of feedback documents to smooth the 
original query model. Compared to the mixture 
model, our CDQE method is also more effective: 
By manually tuning the parameters of the mixture 
model to their best, we obtained the average 
precisions of 0.3171, 0.2393 and 0.2565 
respectively for AP, SJM and WSJ collections. 
These values are lower than those obtained with 
CDQE, which has not been heavily tuned.  
For the same query ?insider trading?, the mixture 
model determines the following expansion terms: 
Mixture: stock:0.0259256 secur:0.0229553 
market:0.0157057 sec:0.013992 
inform:0.011658 firm:0.0110419 
exchang:0.0100346 law:0.00827076 
bill:0.007996 case:0.00764544 
profit:0.00672575 investor:0.00662856 
japan:0.00625859 compani:0.00609675 
commiss:0.0059618 foreign:0.00582441 
bank:0.00572947 investig:0.00572276 
We can see that some of these terms overlap with 
those suggested by biterm relations. However, 
interesting words such as boeski, drexel and 
scandal are not suggested. 
The above comparison shows that our method 
outperforms the state-of-the-art methods of query 
expansion developed so far. 
4.2 Effect of the Smoothing Parameter  
In the previous experiments, we have fixed the 
smoothing parameters. In this series of tests, we 
557
 analyze the effect of this smoothing parameter on 
retrieval effectiveness. The following figure 
shows the change of average precision (AvgP) 
using CDQE (Model 2) along with the change of 
the parameter 2? (UM is equivalent to 12 =? ).  
Figure 2. Effectiveness w.r.t. 2?  
0.15
0.2
0.25
0.3
0.35
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Lambda
A
v
g.
P AP
WSJ
SJM
 
We can see that for all the three collections, 
the effectiveness is good when the parameter is 
set in the range of 0.1-0.5. The best value for 
different collections remains stable: 0.2-0.3.  
The effect of 1?  on Model 1 is slightly 
different, but we observe the same trend. 
4.3 Number of Expansion Terms 
In the previous tests, we limit the number of 
expansion terms to 80. When different numbers 
of expansion terms are used, we obtain different 
effectiveness measures. The following figure 
shows the variation of average precision (AvgP) 
with different numbers of expansion terms, using 
CDQE method.  
Figure 3. Effectiveness w.r.t. #expansion terms 
0.15
0.20
0.25
0.30
0.35
10 20 40 80 150 300
No. expansion terms
Av
g.
P AP
WSJ
SJM
 
We can see that when more expansion terms 
are added, the effectiveness does not always 
increase. In general, a number around 80 will 
produce good results. In some cases, even if 
better effectiveness can be obtained with more 
expansion terms, the retrieval time is also longer. 
The number 80 seems to produce a good 
compromise between effectiveness and retrieval 
speed: the retrieval time remains less than 1 sec. 
per query. 
4.4 Suitability of Relations Across 
Collections 
In many real applications (e.g. Web search), we 
do not have a static document collection from 
which relations can be extracted. The question is 
whether it is possible and beneficial to extract 
relations from one text collection and use them to 
retrieve documents in another text collection. Our 
intuition is that this is possible because the 
relations (especially context-dependent relations) 
encode general knowledge, which can be applied 
to a different collection. In order to show this, we 
extracted term relations from each collection, and 
applied them on other collections. The following 
tables show the effectiveness produced using 
respectively unigram and bi-term relations. 
Table 3. Cross-utilization of relations 
 
Unigram relation Biterm relation 
   Rel. 
Coll. AP SJM WSJ AP SJM WSJ 
AP 0.2902  0.2803  0.2793 0.3383 0.3057 0.2987 
SJM 0.2271 0.2225 0.2267 0.2424 0.2448 0.2453 
WSJ 0.2541  0.2445  0.2393 0.2816 0.2636 0.2710 
 
From this table, we can observe that relations 
extracted from any collection are useful to some 
degree: they all outperform UM (see Table 2). In 
particular, the relations extracted from AP are the 
best for almost all the collections. This can be 
explained by the larger size and wider coverage 
of the AP collection. This suggests that we do not 
necessarily need to extract term relations from 
the same text collection on which retrieval is 
performed. It is possible to extract relations from 
a large text collection, and apply them to other 
collections. This opens the door to the possibility 
of constructing a general relation base for various 
document collections. 
5. Related Work 
Co-occurrence analysis is a common method to 
determine term relations. The previous studies 
have been limited to relations between two 
words, which we called unigram relations. This 
expansion approach has been integrated both in 
traditional retrieval models (Jing and Croft, 
1994) and in LM (Berger and Lafferty 1999). As 
we observed, this type of relation will introduce 
much noise into the query, leading to unstable 
effectiveness. 
Several other studies tried to filter out noise 
expansion (or translation) terms by considering 
the relations between them (Gao et al, 2002; 
558
 Jang et al 1999; Qiu and Frei, 1993; Bai et al 
2005). However, this is insufficient to detect all 
the noise. The key issue is the ambiguity of 
relations due to the lack of context information in 
the relations. In this paper, we proposed a method 
to add some context information into relations.  
 (Lin, 1997) also tries to solve word ambiguity 
by adding syntactic dependency as context. 
However, our approach does not require 
determining syntactic dependency. The principle 
of our approach is more similar to (Yarowsky, 
1995). Compared to this latter, our approach is 
less demanding: we do not need to identify 
manually the exact word senses and seed context 
words. The process is fully automatic. This 
simplification is made possible due to the 
requirement for IR: only in-context related words 
are required, but not the exact senses.  
Our work is also related to (Smadja and 
McKeown, 1996), which tries to determine the 
translation of collocations. Term combinations or 
biterms we used can be viewed as collocations. 
Again, there is much less constraint for our 
related terms than translations in (Smadja and 
McKeown, 1996). 
6. Conclusions 
In many NLP applications such as IR, we need to 
determine relations between terms. In most 
previous studies, one tries to determine the 
related terms to one single term (word). This 
makes the resulting relations ambiguous. 
Although several approaches have been proposed 
to remove afterwards some of the inappropriate 
terms, this only affects part of the noise, and 
much still remains. In this paper, we argue that 
the solution to this problem lies in the addition of 
context information in the relations between 
terms. We proposed to add another word in the 
condition of the relations so as to help constrain 
the context of application. Our experiments 
confirm that this addition of limited context 
information can indeed improve the quality of 
term relations and query expansion in IR. 
In this paper, we only compared biterm 
relations and unigram relations, the general 
method can be extended to triterm relations or 
more complex relations, provided that they can 
be extracted efficiently.  
This paper only investigated the utilization of 
context-dependent relations in IR. These relations 
can be applied in many other tasks, such as 
machine translation, word sense disambiguation / 
discrimination, and so on. These are some 
interesting research work in the future. 
References 
Bai, J., Song, D., Bruza, P., Nie, J. Y. and Cao, G. 
2005. Query expansion using term relationships in 
language models for information retrieval, ACM 
CIKM, pp. 688-695. 
Berger, A. and Lafferty, J. 1999. Information retrieval 
as statistical translation. ACM SIGIR, pp. 222-229. 
Church, K. W. and Hanks, P. 1989. Word association 
norms, mutual information, and lexicography. ACL, 
Vol. 16, pp. 22-29. 
Gao, J., Nie, J.Y., He, H, Chen, W., Zhou, M. 2002. 
Resolving query translation ambiguity using a 
decaying co-occurrence model and syntactic 
dependency relations. ACM SIGIR, pp. 11-15. 
Jang, M. G., Myaeng, S. H., and Park, S. Y. 1999. 
Using mutual information to resolve query 
translation ambiguities and query term weighting. 
ACL, pp. 223-229. 
Jing, Y. and Croft, W.B. 1994. An association 
thesaurus for information retrieval. RIAO, pp. 146-
160. 
Lin, D. 1997. Using syntactic dependency as local 
context to resolve word sense ambiguity, ACL, pp. 
64-71. 
Peat, H.J. and Willett, P. 1991. The limitations of term 
co-occurrence data for query expansion in document 
retrieval systems. JASIS, 42(5): 378-383. 
Ponte, J. and Croft, W.B. 1998. A language modeling 
approach to information retrieval. ACM SIGIR, pp. 
275-281. 
Qiu, Y. and Frei, H.P. 1993. Concept based query 
expansion. ACM SIGIR, pp.160-169. 
Sch?tze, H. and Pedersen J.O. 1997. A cooccurrence-
based thesaurus and two applications to information 
retrieval, Information Processing and Management, 
33(3): 307-318. 
Smeaton, A. F. and Van Rijsbergen, C. J. 1983. The 
retrieval effects of query expansion on a feedback 
document retrieval system. Computer Journal, 26(3): 
239-246. 
Smadja, F., McKeown, K.R., 1996. Translating 
collocations for bilingual lexicons: A statistical 
approach, Computational Linguistics, 22(1): 1-38. 
Srikanth, M. and Srihari, R. 2002. Biterm language 
models for document retrieval. ACM SIGIR, pp. 425-
426  
Voorhees, E. 1994. Query expansion using lexical-
semantic relations. ACM SIGIR, pp. 61-69. 
Yarowsky, D. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. ACL, 
pp. 189-196. 
Zhai, C. and Lafferty, J. 2001. Model-based feedback 
in the language modeling approach to information 
retrieval. ACM SIGIR, pp. 403-410.  
559
Coling 2010: Poster Volume, pages 18?26,
Beijing, August 2010
Cross-Market Model Adaptation with Pairwise Preference Data for
Web Search Ranking
Jing Bai
Microsoft Bing
1065 La Avenida
Mountain View, CA 94043
jbai@microsoft.com
Fernando Diaz, Yi Chang, Zhaohui Zheng
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
{diazf,yichang,zhaohui}@yahoo-inc.com
Keke Chen
Computer Science
Wright State
Dayton, Ohio 45435
keke.chen@wright.edu
Abstract
Machine-learned ranking techniques au-
tomatically learn a complex document
ranking function given training data.
These techniques have demonstrated the
effectiveness and flexibility required of a
commercial web search. However, man-
ually labeled training data (with multiple
absolute grades) has become the bottle-
neck for training a quality ranking func-
tion, particularly for a new domain. In
this paper, we explore the adaptation of
machine-learned ranking models across
a set of geographically diverse markets
with the market-specific pairwise prefer-
ence data, which can be easily obtained
from clickthrough logs. We propose
a novel adaptation algorithm, Pairwise-
Trada, which is able to adapt ranking
models that are trained with multi-grade
labeled training data to the target mar-
ket using the target-market-specific pair-
wise preference data. We present results
demonstrating the efficacy of our tech-
nique on a set of commercial search en-
gine data.
1 Introduction
Web search algorithms provide methods for
ranking web scale collection of documents given
a short query. The success of these algorithms
often relies on the rich set of document prop-
erties or features and the complex relationships
between them. Increasingly, machine learn-
ing techniques are being used to learn these
relationships for an effective ranking function
(Liu, 2009). These techniques use a set of la-
beled training data labeled with multiple rele-
vance grades to automatically estimate parame-
ters of a model which directly optimizes a per-
formance metric. Although training data often
is derived from editorial labels of document rel-
evance, it can also be inferred from a careful
analysis of users? interactions with a working
system (Joachims, 2002). For example, in web
search, given a query, document preference in-
formation can be derived from user clicks. This
data can then be used with an algorithm which
learns from pairwise preference data (Joachims,
2002; Zheng et al, 2007). However, automati-
cally extracted pairwise preference data is sub-
ject to noise due to the specific sampling meth-
ods used (Joachims et al, 2005; Radlinski and
Joachim, 2006; Radlinski and Joachim, 2007).
One of the fundamental problems for a web
search engine with global reach is the develop-
ment of ranking models for different regional
markets. While the approach of training a single
model for all markets is attractive, it fails to fully
exploit of specific properties of the markets. On
the other hand, the approach of training market-
specific models requires the huge overhead of
acquiring a large training set for each market.
As a result, techniques have been developed to
create a model for a small market, say a South-
east Asian country, by combining a strong model
in another market, say the United States, with a
18
small amount of manually labeled training data
in the small market (Chen et al, 2008b). How-
ever, the existing Trada method takes only multi-
grade labeled training data for adaptation, mak-
ing it impossible to take advantage of the easily
harvested pairwise preference data. In fact, to
our knowledge, there is no adaptation algorithm
that is specifically developed for pairwise data.
In this paper, we address the development
market-specific ranking models by leveraging
pairwise preference data. The pairwise prefer-
ence data contains most market-specific train-
ing examples, while a model from a large mar-
ket may capture the common characteristics of
a ranking function. By combining them algo-
rithmically, our approach has two unique advan-
tages. (1) The biases and noises of the pairwise
preference data can be depressed by using the
base model from the large market. (2) The base
model can be tailored to the characteristics of the
new market by incorporating the market specific
pairwise training data. As the pairwise data has
the particular form, the challenge is how to ef-
fectively use pairwise data in adaptation. This
appeals to the following objective of many web
search engines: design algorithms which mini-
mize manually labeled data requirements while
maintaining strong performance.
2 Related Work
In recent years, the ranking problem is fre-
quently formulated as a supervised machine
learning problem, which combines different
kinds of features to train a ranking function.
The ranking problem can be formulated as learn-
ing a function with pair-wise preference data,
which is to minimize the number of contra-
dicting pairs in training data. For example,
RankSVM (Joachims, 2002) uses support vector
machines to learn a ranking function from pref-
erence data; RankNet (Burges et al, 2005a) ap-
plies neural network and gradient descent to ob-
tain a ranking function; RankBoost (Freund et
al., 1998) applies the idea of boosting to con-
struct an efficient ranking function from a set of
weak ranking functions; GBRank (Zheng et al,
2007; Xia et al, 2008) using gradient descent in
function spaces, which is able to learn relative
ranking information in the context of web search.
In addition, Several studies have been focused
on learning ranking functions in semi-supervised
learning framework (Amini et al, 2008; Duh and
Kirchhoff, 2008), where unlabeled data are ex-
ploited to enhance ranking function. Another ap-
proach to learning a ranking function addresses
the problem of optimizing the list-wise perfor-
mance measures of information retrieval, such
as mean average precision or Discount Cumu-
lative Gain (Cao et al, 2007; Xu et al, 2008;
Wu et al, 2009; Chen et al, 2008c). The idea
of these methods is to obtain a ranking function
that is optimal with respect to some information
retrieval performance measure.
Model adaptation has previously been applied
in the area of natural language processing and
speech recognition. This approach has been suc-
cessfully applied to parsing (Hwa, 1999), tag-
ging (Blitzer et al, 2006), and language model-
ing for speech recognition (Bacchiani and Roark,
2003). Until very recently, several works have
been presented on the topic of model adaptation
for ranking (Gao et al, 2009; Chen et al, 2008b;
Chen et al, 2009), however, none of them target
the model adaptation with the pair-wise learn-
ing framework. Finally, multitask learning for
ranking has also been proposed as a means of
addressing problems similar to those we have
encountered in model adaptation (Chen et al,
2008a; Bai et al, 2009; Geng et al, 2009).
3 Background
3.1 Gradient Boosted Decision Trees for
Ranking
Assume we have a training data set, D =
{?(q, d), y?1, . . . , ?(q, d), y?n}, where ?(q, d), t?i
encodes the labeled relevance, y, of a docu-
ment, d, given query, q. Each query-document
pair, (q, d), is represented by a set of features,
(q, d) = {xi1, xi2, xi3, ..., xim}. These features
include, for example, query-document match
features, query-specific features, and document-
specific features. Each relevance judgment, y,
is a relevance grade mapped (e.g. ?relevant?,
?somewhat relevant?, ?non-relevant?) to a real
19
x1 > a1?
x2 > a2? x3 > a3?
YES NO
Figure 1: An example of base tree, where x1, x2
and x3 are features and a1, a2 and a3 are their
splitting values.
number. Given this representation, we can learn
a gradient boosted decision tree (GBDT) which
models the relationship between document fea-
tures, (q, d), and the relevance score, y, as a de-
cision tree (Friedman, 2001). Figure 1 shows a
portion of such a tree. Given a new query docu-
ment pair, the GBDT can be used to predict the
relevance grade of the document. A ranking is
then inferred from these predictions. We refer to
this method as GBDTreg.
In the training phase, GBDTreg iteratively
constructs regression trees. The initial regres-
sion tree minimizes the L2 loss with respect to
the targets, y,
L2(f, y) =
?
?(q,d),y?
(f(q, d)? y)2 (1)
As with other boosting algorithms, the subse-
quent trees minimize the L2 loss with respect to
the residuals of the predicted values and the tar-
gets. The final prediction, then, is the sum of the
predictions of the trees estimated at each step,
f(x) = f1(x) + . . .+ fk(x) (2)
where f i(x) is the prediction of the ith tree.
3.2 Pairwise Training
As alternative to the absolute grades in D,
we can also imagine assembling a data set
of relative judgments. In this case, as-
sume we have a training data set D =
{?(q, d), (q, d?), ??1, . . . , ?(q, d), (q, d?), ??n},
where ?(q, d), (q, d?), ??i encodes the prefer-
ence, of a document, d, to a second document,
d?, given query, q. Again, each query-document
pair is represented by a set of features. Each
preference judgment, ? ? {,?}, indicates
whether document d is preferred to document d?
(d  d?) or not (d ? d?).
Preference data is attractive for several rea-
sons. First, editors can often more easily deter-
mine preference between documents than the ab-
solute grade of single documents. Second, rel-
evance grades can often vary between editors.
Some editors may tend to overestimate relevance
compared to another editor. As a result, judg-
ments need to be rescaled for editor biases. Al-
though preference data is not immune to inter-
editor inconsistency, absolute judgments intro-
duce two potential sources of noise: determin-
ing a relevance ordering and determining a rele-
vance grade. Third, even if grades can be accu-
rately labeled, mapping those grades to real val-
ues is often done in a heuristic or ad hoc manner.
Fourth, GBDTreg potentially wastes modeling
effort on predicting the grade of a document as
opposed to focusing on optimizing the rank order
of documents, the real goal a search engine. Fi-
nally, preference data can often be mined from a
production system using assumptions about user
clicks.
In order to support preference-based
training data, (Zheng et al, 2007) pro-
posed GBRANK based on GBDTreg. The
GBRANK training algorithm begins by con-
structing an initial tree which predicts a constant
score, c, for all instances. A pair is contra-
dicting if the ?(q, d), (q, d?),? and prediction
f(q, d) < f(q, d?). At each boosting stage,
the algorithm constructs a set of contradicting
pairs, Dcontra. The GBRANK algorithm then
adjusts the response variables, f(q, d) and
f(q, d?), so that f(q, d) > f(q, d?). Assume
that (q, d)  (q, d?) and f(q, d) < f(q, d?). To
correct the order, we modify the target values,
f?(q, d) = f(q, d) + ? (3)
f?(q, d?) = f(q, d?)? ? (4)
where ? > 0 is a margin parameter that we
20
need to assign. In our experiments, we set ? to
1. Note that if preferences are inferred from ab-
solute grades, D, minimizing the L2 to 0 also
minimizes the contradictions.
3.3 Tree Adaptation
Recall that we are also interested in using the
information learned from one market, which we
will call the source market, on a second market,
which we will call the target market. To this end,
the Trada algorithm adapts a GBDTreg model
from the source market for the target market by
using a small amount of target market absolute
relevance judgments (Chen et al, 2008b). Let
the Ds be the data in the source domain and
Dt be the data in target domain. Assume we
have trained a model using GBDTreg. Our ap-
proach will be to use the decision tree structure
learned from Ds but to adapt the thresholds in
each node?s feature. We will use Figure 1 to il-
lustrate Trada. The splitting thresholds are a1, a2
and a3 for rank features x1, x2 and x3. Assume
that the data set Dt is being evaluated at the root
node v in Figure 1. We will split the using the
feature vx = x1 but will compute a new thresh-
old v?a using Dt and the GBDTreg algorithm.
Because we are discussing the root node, when
we select a threshold b, Dt will be partitioned
into two sets, D>bt and D<bt representing those
instances whose feature x1 has a value greater
and lower than b. The response value for each
partition will be the uniform average of instances
in that partition,
f =
?
?
?
1
|D>bt |
?
di?D>bt yi if di ? D
>b
t
1
|D<bt |
?
di?D<bt yi if di ? D
<b
t
(5)
We would like to select a value for b which min-
imizes the L2 loss between y and f in Equation
5; equivalently, b can be selected to minimize the
variance of y in each partition. In our imple-
mentation, we compute the L2 loss for all pos-
sible values of the feature v?x and select the value
which minimizes the loss.
Once b is determined, the adaptation consists
of performing a linear interpolation between the
original splitting threshold va and the new split-
ting threshold b as follows:
v?a = pva + (1? p)b (6)
where p is an adaptation parameter which deter-
mines the scale of how we want to adapt the tree
to the new task. If there is no additional informa-
tion, we can select p according to the size of the
data set,
p = |D
<a
s |
|D<as |+ |D<bt |
(7)
In practice, we often want to enhance the adapta-
tion scale since the training data of the extended
task is small. Therefore, we add a parameter ?
to boost the extended task as follows:
p = |D
<a
s |
|D<as |+ ?|D<bt |
(8)
The value of ? can be determined by cross-
validation. In our experiments, we set ? to 1.
The above process can also be applied to ad-
just the response value of nodes as follows:
v?f = pvf + (1? p)f (9)
where v?f is the adapted response at a node, vf is
its original response value of source model, and
f is the response value (Equation 5).
The complete Trada algorithm used in our ex-
periments is presented in Algorithm 1.
Algorithm 1 Tree Adaptation Algorithm
TRADA(v,Dt, p)
1 b? COMPUTE-THRESHOLD(vx,Dt)
2 v?a ? pva + (1? p)b
3 v?f ? pvf + (1? p)MEAN-RESPONSE(Dt)
4 D?t ? {x ? Dt : xi < v?a}
5 v?< ? TRADA(v<,D?t, p)
6 D??t ? {x ? Dt : xi > v?a}
7 v?> ? TRADA(v>,D??t , p)
8 return v?
21
The Trada algorithm can be augmented with a
second phase which directly incorporates the tar-
get training data. Assume that our source model,
Ms, was trained using source data, Ds. Re-
call that Ms can be decomposed as a sum of
regression tree output, fMs(x) = f1Ms(x) +
. . . + fkMs(x). Additive tree adaptation refersaugmenting this summation with a set of regres-
sion trees trained on the residuals between the
model, Ms, and the target training data, Dt.
That is, fMt(x) = f1Ms(x) + . . . + fkMs(x) +
fMt(x)k+1+. . .+fMt(x)k+k
? . In order for us to
perform additive tree adaptation, the source and
target data must use the same absolute relevance
grades.
4 Pairwise Adaptation
Both GBRANK and Trada can be used
to reduce the requirement on editorial data.
GBRANK achieves the goal by leveraging pref-
erence data, while Trada does so by leveraging
data from a different search market. A natural
extension to these methods is to leverage both
sources of data simultaneously. However, no al-
gorithm has been proposed to do this so far in
the literature. We propose an adaptation method
using pairwise preference data.
Our approach shares the same intuition as
Trada: maintain the tree structure but adjust
decision threshold values against some target
value. However, an important difference is
that our adjustment of threshold values does not
regress against some target grade values; rather
its objective is to improve the ordering of doc-
uments. To make use of preference data in
the tree adaptation, we follow the method used
in GBRANK to adjust the target values when-
ever necessary to preserve correct document or-
der. Given a base model, Ms, and preference
data, Dt , we can use Equations 3 and 4 to in-
fer target values. Specifically, we construct a set
Dcontra from Dt and Ms. For each item (q, d)
in Dcontra, we use the value of f?(q, d) as the tar-
get. These tuples, ?(q, d), f?(q, d)? along with
Ms are then are provided as input to Trada. Our
approach is described in Algorithm 2.
Compared to Trada, Pairwise-Trada has two
Algorithm 2 Pairwise Tree Adaptation Algo-
rithm
PAIRWISE-TRADA(Ms,Dt , p)
1 Dcontra ? FIND-CONTRADICTIONS(Ms,Dt )
2 D?t ? {?(q, d), f?(q, d)? : (q, d) ? Dcontra}
3 return TRADA(ROOT(Ms), D?t, p)
important differences. First, Pairwise-Trada can
use a source GBDT model trained either against
absolute or pairwise judgments. When an orga-
nization maintains a set of ranking models for
different markets, although the underlying mod-
eling method may be shared (e.g. GBDT), the
learning algorithm used may be market-specific
(e.g. GBRANK or GBDTreg). Unfortunately,
classic Trada relies on the source model being
trained using GBDTreg. Second, Pairwise-Trada
can be adapted using pairwise judgments. This
means that we can expand our adaptation data to
include click feedback, which is easily obtain-
able in practice.
5 Methods and Materials
The proposed algorithm is a straightforward
modification of previous ones. The question we
want to examine in this section is whether this
simple modification is effective in practice. In
particular, we want to examine whether pairwise
adaptation is better than the original adaptation
Trada using grade data, and whether the pairwise
data from a market can help improve the ranking
function on a different market.
Our experiments evaluate the performance of
Pairwise-Trada for web ranking in ten target
markets. These markets, listed in Table 1, cover
a variety of languages and cultures. Further-
more, resources, in terms of documents, judg-
ments, and click-through data, also varies across
markets. In particular, editorial query-document
judgments range from hundreds of thousands
(e.g. SEA1) to tens of thousands (e.g. SEA5).
Editors graded query-document pairs on a five-
point relevance scale, resulting in our data setD.
Preference labels, D, are inferred from these
judgments.
22
We also include a second set of experiments
which incorporate click data.1 In these experi-
ments, we infer a preference from click data by
assuming the following model. The user is pre-
sented with ten results. An item i  j if i the fol-
lowing conditions hold: i is positioned below j,
i receives a click, and j does not receive a click.
In our experiments, we tested the following
runs,
? GBDTreg trained using only Ds or Dt
? GBRANK trained using only Ds or Dt
? GBRANK trained using only Ds , Dt , and
Ct
? Trada with both GBDTs and GBRANKs,
adapted with Dt.
? Pairwise-Trada with both GBDTs and
GBRANKs, adapted with Dt and Ct at dif-
ferent ratios.
In the all experiments, we use 400 additive trees
when additive adaptation is used.
All models are evaluated using discounted cu-
mulative gain (DCG) at rank cutoff 5 (Ja?rvelin
and Keka?la?inen, 2002).
6 Results
6.1 Adaptation with Manually Labeled
Data
In Table 1, we show the results for all of our ex-
perimental conditions.
We can make a few observations about the
non-adaptation baselines. First, models trained
on the (limited) target editorial data, GBDTt
and GBRANKt, tend to outperform those trained
only on the source editorial data, GBDTs and
GBRANKs. The critical exception is SEA5, the
market with the fewest judgments. We believe
that this behavior is a result of similarity between
the United States source data and the SEA5 tar-
get market; both the source and target query pop-
ulations share the same language, a property not
1For technical reasons, this data set is slightly differ-
ent from the results we show with the purely editorial data.
Therefore the size of the training and testing sets are differ-
ent, but not to a significant degree.
exhibited in other markets. Notice that other
small markets such as LA2 and LA3 see modest
improvements when using target-only runs com-
pared to source-only runs. Second, GBRANK
tends to outperform GBDT when only trained on
the source data. This implies that we should pre-
fer a base model which is based on GBRANK,
something that is difficult to combine with clas-
sic Trada. Third, by comparing GBRANK and
GBDT when only trained on the target data, we
notice that the effectiveness of GBRANK de-
pends on the amount of training data. For mar-
kets where there training data is plentiful (e.g.
SEA1), GBRANK outperforms GBDT. On the
other hand, for smaller markets (e.g. LA3),
GBDT outperforms GBRANK.
In general, the results confirm the hypothe-
sis that adaptation runs outperform all of non-
adaptation baselines. This is the case for both
Trada and Pairwise-Trada. As with the baseline
runs, the Australian market sees different perfor-
mance as a result of the combination of a small
target editorial set and a representative source
domain. This effect has been observed in pre-
vious results (Chen et al, 2009).
We can also make a few observations by com-
paring the adaptation runs. Trada works better
with a GBDT base model than with a GBRANK
base model. We We believe this is the case be-
cause the absolute regression targets are diffi-
cult to compare with the unbounded output of
GBRANK. Pairwise-Trada on the other hand
tends to perform better with a GBRANK base
model than with a GBDT base model. There
are a few exceptions, SEA3 and LA2, where
Pairwise-Trada works better with a GBDT base
model. Comparing Trada to Pairwise-Trada, we
find that using preference targets tends to im-
prove performance for some markets but not all.
The underperformance of Pairwise-Trada tends
to occur in smaller markets such as LA1, LA2,
and LA3. This is similar to the behavior we ob-
served in the non-adaptation runs and suggests
that, in operation, a modeler may have to decide
on the training algorithm based on the amount of
data available.
23
SEA1 SEA2 EU1 SEA3 EU2 SEA4 LA1 LA2 LA3 SEA5
training size 243,790 174,435 137,540 135,066 101,076 100,846 91,638 75,989 66,151 37,445
testing size 18,652 26,752 11,431 13,839 12,118 12,214 11,038 16,339 10,379 21,034
GBDTs 9.4483 8.1271 9.0018 10.0630 8.5339 5.9176 6.1699 11.4167 8.1416 10.5356
GBDTt 9.6011 8.6225 9.3310 10.7591 9.0323 6.4185 6.8441 11.8553 8.5702 10.4561
GBRANKs 9.6059 8.1784 9.0775 10.2486 8.6248 6.1298 6.2614 11.5186 8.2851 10.5915
GBRANKt 9.6952 8.6225 9.3575 10.8595 9.0384 6.4620 6.8543 11.7086 8.4825 10.3469
Trada
GBDTs,Dt 9.6718 8.6120 9.3086 10.8001 9.1024 6.3440 6.9444 11.9513 8.6519 10.6279
GBRANKs,Dt 9.6116 8.5681 9.2125 10.7597 8.9675 6.4110 6.8286 11.7326 8.5498 10.6508
Pairwise-Trada
GBDTs,Dt 9.7364 8.6261 9.3824 10.8549 9.0842 6.4705 6.9438 11.8255 8.5323 10.4655
GBRANKs,Dt 9.7539 8.6538 9.4269 10.8362 9.1044 6.4716 6.9438 11.8034 8.6187 10.6564
Table 1: Adaptation using manually labeled training data Southeast Asia (SEA), Europe (EU), and
Latin America (LA) markets. Markets are sorted by target training set size. Significance tests use
a t-test. Bolded numbers indicate statistically significant improvements over the respective source
model.
SEA1 SEA2 EU1 SEA3 EU2 SEA4 LA1 LA2 LA3 SEA5
training size 194,114 166,396 136,829 161,663 94,875 96,642 73,977 108,350 64,481 71,549
testing size 15,655 11,844 11,028 11,839 11,118 5,092 10,038 12,246 10,201 7,477
GBRANKs 9.0159 8.5763 8.7119 11.4512 9.7641 6.5941 6.894 7.9366 8.058 10.7935
Pairwise-Trada
GBRANKs,Dt, Ct
editorial 9.3577 8.9205 8.901 12.2247 9.9531 6.7421 7.1455 8.2811 8.2503 10.7973
click 9.1149 8.7622 8.8187 11.9361 9.8818 6.7703 7.1812 8.264 8.2485 10.9042
editorial+click 9.4898 9.0177 8.945 12.3172 10.1156 6.8459 7.2414 8.4111 8.292 11.1407
Table 2: Adaptation incorporating click data. Bolded numbers indicate statistically significant im-
provements over the baseline. Markets ordered as in Table 1.
6.2 Incorporating Click Data
One of the advantages of Pairwise-Trada is the
ability to incorporate multiple sources of pair-
wise preference data. In this paper, we use the
heuristic rule approach which is introduced by
(Dong et al, 2009) to extract pairwise preference
data from the click log of the search engine. This
approach yields both skip-next and skip-above
pairs (Joachims et al, 2005), which are sorted
by confidence descending order respectively. In
these experiments, we combine manually gener-
ated preferences with those gathered from click
data. We present these results in Table 2.
We notice that no matter the source of prefer-
ence data, Pairwise-Trada outperforms the base-
line GBRANK model. The magnitude of the
improvement depends on the source data used.
Comparing the editorial-only to the click-only
models, we notice that click-only models outper-
form editorial-only models for smaller markets
(SEA4, LA1, and SEA5). This is likely the case
because the relative quantity of click data with
respect to editorial data is higher in these mar-
kets. This is despite the fact that the click data
may be noisier than the editorial data. The best
performance, though, comes when we combine
both editorial and click data.
6.3 Additive tree adaptation
Recall that Pairwise-Trada consists of two parts:
parameter adaptation and additive tree adapta-
tion. In this section, we examine the contri-
bution to performance each part is responsible
for. Figure 2 illustrates the adaptation results for
the LA1 market. In this experiment, we use a
United States base model and 100K LA1 edito-
rial judgments for adaptation. Pairwise-Trada is
performed on top of differently sized base mod-
els with 600, 900 and 1200 trees. The original
base model has 1200 trees; we selected the first
600, 900 or full 1200 trees for experiments. The
number of trees used in the additive tree adap-
tation step ranges up to 600 trees. From Fig-
ure 2 we can see that the additive adaptation can
24
0 500 1000 1500 2000
6.0
6.2
6.4
6.6
6.8
7.0
number of trees
DC
G5
adaptation
additive (600)
additive (900)
additive (1200)
source model
Figure 2: Illustration of additive tree adaptation
for LA1. The curves are average performance
over a range of parameter settings.
significantly increase DCG over simple parame-
ter adaptation and is therefore a critical step of
Pairwise-Trada. When the number of trees in
the additive tree adaptation step reaches roughly
400, the DCG plateaus.
7 Conclusion
We have proposed a model for adapting retrieval
models using preference data instead of abso-
lute relevance grades. Our experiments demon-
strate that, when much editorial data is present,
our method, Pairwise-Trada, may be preferable
to competing methods based on absolute rele-
vance grades. However, in real world systems,
we often have access to sources of preference
data beyond those resulting from editorial judg-
ments. We demonstrated that Pairwise-Trada can
exploit such data and boost performance signif-
icantly. In fact, if we omit editorial data alto-
gether we see performance improvements over
the baseline model. This suggests that, in prin-
ciple, we can train a single, strong source model
and improve it using target click data alone. De-
spite the fact that the modification we made is
quite simple, we showed that modification is ef-
fective in practice. This tends to validate the
general principle of using pairwise data from a
different market. This principle can be easily
used in other frameworks such as neural net-
works (Burges et al, 2005b). Therefore, the pro-
posed method also points to a new direction for
future improvements of search engines.
There are several areas of future work. First,
we believe that detecting other sources of pref-
erence data from user behavior can further im-
prove the performance of our model. Second,
we only used a single source model in our ex-
periments. We would also like to explore the
effect of learning from an ensemble of source
models. The importance of each may depend on
the similarity to the target domain. Finally, we
would also like to more accurately understand
the queries where click data improves adaptation
and those where editorial judgments is required.
This sort of knowledge will allow us to train sys-
tems which maximally exploit our editorial re-
sources.
References
Amini, M.-R., T.-V. Truong, and C. Goutte. 2008.
A boosting algorithm for learning bipartite rank-
ing functions with partially labeled data. In SIGIR
?08: Proceedings of the 31st annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval.
Bacchiani, M. and B. Roark. 2003. Unsuper-
vised language model adaptation. In ICASSP ?03:
Proceedings of the International Conference on
Acoustics, Speech and Signal Processing.
Bai, J., K. Zhou, H. Zha, B. Tseng, Z. Zheng, and
Y. Chang. 2009. Multi-task learning for learning
to rank in web search. In CIKM ?09: Proceeding
of the 18th ACM conference on Information and
knowledge management.
Blitzer, J., R. McDonald, and F. Pereira. 2006.
Domain adaptation with structural correspondence
learning. In EMNLP ?06: Proceedings of the
2006 Conference on Empirical Methods on Nat-
ural Language Processing.
Burges, C., T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005a.
Learning to rank using gradient descent. In ICML
?05: Proceedings of the 22nd International Con-
ference on Machine learning.
Burges, Chris, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hul-
lender. 2005b. Learning to rank using gradi-
ent descent. In ICML ?05: Proceedings of the
25
22nd international conference on Machine learn-
ing, pages 89?96. ACM.
Cao, Z., T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li.
2007. from pairwise approach to listwise ap-
proach. In ICML ?07: Proceedings of the 24th
international conference on Machine learning.
Chen, D., J. Yan, G. Wang, Y. Xiong, W. Fan, and
Z. Chen. 2008a. Transrank: A novel algorithm for
transfer of rank learning. In ICDM workshop ?08:
Proceeding of IEEE Conference on Data Mining.
Chen, K., R. Lu, C. K. Wong, G. Sun, L. Heck, and
B. Tseng. 2008b. Trada: tree based ranking func-
tion adaptation. In CIKM ?08: Proceeding of the
17th ACM conference on Information and knowl-
edge management, pages 1143?1152, New York,
NY, USA. ACM.
Chen, W., T.-Y. Liu, Y. Lan, Z. Ma, and H. Li. 2008c.
Measures and loss functions in learning to rank. In
NIPS ?08: Proceedings of the Twenty-Second An-
nual Conference on Neural Information Process-
ing Systems.
Chen, K., J. Bai, S. Reddy, and B. Tseng. 2009. On
domain similarity and effectiveness of adapting-
to-rank. In CIKM ?09: Proceeding of the 18th
ACM conference on Information and knowledge
management, pages 1601?1604, New York, NY,
USA. ACM.
Dong, A., Y. Chang, S. Ji, C. Liao, X. Li, and
Z. Zheng. 2009. Empirical exploitation of click
data for query-type-based ranking. In EMNLP
?09: Proceedings of the 2009 Conference on Em-
pirical Methods on Natural Language Processing.
Duh, K. and K. Kirchhoff. 2008. Learning to rank
with partially-labeled data. In SIGIR ?08: Pro-
ceedings of the 31st annual international ACM SI-
GIR conference on Research and development in
information retrieval.
Freund, Y., R. D. Iyer, R. E. Schapire, and Y. Singer.
1998. An efficient boosting algorithm for com-
bining preferences. In ICML ?98: Proceedings of
the Fifteenth International Conference onMachine
Learning.
Friedman, J. H. 2001. Greedy function approxima-
tion: A gradient boosting machine. The Annals of
Statistics, 29(5):1189?1232.
Gao, J., Q. Wu, C. Burges, K. Svore, Y. Su, N. Khan,
Shah S., and H. Zhou. 2009. Model adapta-
tion via model interpolation and boosting for web
search ranking. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods on
Natural Language Processing.
Geng, B., L. Yang, C. Xu, and X.-S. Hua. 2009.
Ranking model adaptation for domain-specific
search. In CIKM ?09: Proceeding of the 18th ACM
conference on Information and knowledge man-
agement, pages 197?206, New York, NY, USA.
ACM.
Hwa, R. 1999. Supervised grammar induction using
training data with limited constituent information.
In ACL ?99: Proceedings of the Conference of the
Association for Computational Linguistics.
Ja?rvelin, Kalervo and Jaana Keka?la?inen. 2002. Cu-
mulated gain-based evaluation of ir techniques.
TOIS, 20(4):422?446.
Joachims, T., L. Granka, B. Pan, and G. Gay. 2005.
Accurately interpreting clickthrough data as im-
plicit feedback.
Joachims, T. 2002. Optimizing search engines using
clickthrough data. In KDD ?02: Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages
133?142. ACM Press.
Liu, T.-Y. 2009. Learning to Rank for Information
Retrieval. Now Publishers.
Radlinski, F. and T. Joachim. 2006. Minimally inva-
sive randomization for collecting unbiased prefer-
ences from clickthrough logs.
Radlinski, F. and T. Joachim. 2007. Active ex-
ploration for learning rankings from clickthrough
data.
Wu, M., Y. Chang, Z. Zheng, and H. Zha. 2009.
Smoothing dcg for learning to rank: A novel ap-
proach using smoothed hinge functions. In CIKM
?09: Proceeding of the 18th ACM conference on
Information and knowledge management.
Xia, F., T.-Y. Liu, J. Wang, W. Zhang, and H. Li.
2008. Listwise approach to learning to rank: The-
orem and algorithm. In ICML ?08: Proceedings
of the 25th international conference on Machine
learning.
Xu, J., T.Y. Liu, M. Lu, H. Li, and W.Y. Ma. 2008.
Directly optimizing evaluation measures in learn-
ing to rank. In SIGIR ?08: Proceedings of the
31st annual international ACM SIGIR conference
on Research and development in information re-
trieval.
Zheng, Z., K. Chen, G. Sun, and H. Zha. 2007. A re-
gression framework for learning ranking functions
using relative relevance judgments. In SIGIR ?07:
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 287?294. ACM.
26
