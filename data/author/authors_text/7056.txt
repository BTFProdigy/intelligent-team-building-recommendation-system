NAACL HLT Demonstration Program, pages 3?4,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
The Automated Text Adaptation Tool 
 
Jill Burstein, Jane Shore, John Sabatini, Yong-Won Lee & Matthew Ventura 
Educational Testing Service 
Rosedale Road MS 12R 
Princeton, New Jersey 08541 
{jburstein, jshore, jsabatini , ylee, mventura}@ets.org 
 
 
1. Introduction 
 
Text adaptation is a teacher practice used to help 
with reading comprehension and English 
language skills development for English language 
learners (ELLs) (Carlo, August, McLaughlin, 
Snow, Dressler, Lippman, Lively, & White, 
2004; Echevarria, Vogt and Short, 2004; Yano, 
Long and Ross, 1994). The practice of text 
adaptation involves a teacher?s modification of 
texts to make them more understandable, given a 
student?s reading level.  Teacher adaptations 
include text summaries, vocabulary support (e.g., 
providing synonyms), and translation. It is a time-
consuming, but critical practice for K-12 teachers 
who teach ELLs, since reading-level appropriate 
texts are often hard to find. To this end, we have 
implemented the Automated Text Adaptation 
Tool v.1.0 (ATA v.1.0): an innovative, 
educational tool that automatically generates text 
adaptations similar to those teachers might create. 
We have also completed a teacher pilot study.  
Schwarm and Ostendorf (2005), and Heilman, 
Collins-Thompson, Callan, and Eskenazi (2006) 
describe related research addressing the 
development of NLP-based reading support tools. 
During our interactive demonstration, 
conference participants can (a) login to the 
Internet-accessible tool, (b) import text files, and 
(c) experiment with adaptation features. We are 
currently interested in feedback from the 
computational linguistics community to inform 
tool development related to (a) feature 
enhancement, and (b) ideas for new NLP-based 
features. Until now, our primary source of 
feedback has been from teachers toward tool 
development from an educational perspective. 
 
2. The Automated Text Adaptation Tool  
 
NLP-based text adaptation capabilities in the tool 
are described in this section (also see Figure 1.) 
These adaptation features were selected for 
implementation since they resemble teacher-
based adaptation methods.  
 
2.1 English and Spanish Marginal Notes 
 
Pedagogically, marginal notes are a kind of text 
summary. The Rhext automatic summarization 
tool (Marcu, 2000) is used to produce marginal 
notes in English. The amount of marginal notes 
generated can be increased or decreased based on 
students? needs. Using Language Weaver?s1 
English-to-Spanish machine translation system, 
English marginal notes can be translated into 
Spanish. 
 
2.2 Vocabulary Support 
 
Synonyms for lower frequency (more difficult) 
words are output using a statistically-generated 
word similarity matrix (Lin, 1998). ATA v.1.0 
generates antonyms for vocabulary in the text 
using WordNet?.2   Cognates are words which 
have the same spelling and meaning in two 
languages (e.g., animal in English and Spanish). 
The tool generates these using an ETS 
English/Spanish cognate lexicon. 
 
2.3 English and Spanish Text-to-Speech  
 
The tool offers English and Spanish text-to-
speech (TTS)3. English TTS may be useful for  
pronunciation support, while Spanish TTS 
provides access to the Spanish texts for Spanish-
speaking ELLs who are not literate in Spanish.   
                                                 
1 See http://www.languageweaver.com 
2 See http://wordnet.princeton.edu/
3 See http://www.cstr.ed.ac.uk/projects/festival/  & 
http://cslu.cse.ogi.edu/tts/download/.  
3
 
Figure 1.  Example Main Interface Screen showing English Marginal Notes  
in the right column and Synonyms for ?enjoyable? (entertaining, enjoyable, pleasant.)  
3. Pilot Study with Teachers 
The survey feedback indicated that the 12 teachers 
were positive about the tool?s potential. Overall, 
the vocabulary and English marginal notes were 
the most favorite features, while the text-to-speech 
was the least favorite.  Teachers commented that 
they would like to see an editing capability added 
that would allow them to make changes to the 
automatically generated outputs (i.e., vocabulary 
support, and English and Spanish marginal notes.) 
Teachers viewed the tool either as lesson planning 
support, or as a student tool for independent work.   
 
4. Future Research 
 
ATA v.1.0 is a young application that uses NLP 
methods to create text adaptations. The teacher pilot 
evaluation suggested that it produces adaptations 
with potentially effective support for ELLs. It could 
also save teachers lesson planning time. We are 
currently implementing teacher-suggested 
modifications, and planning a larger, school-based 
pilot. The pilot will evaluate the tool?s effectiveness 
in terms of measurable learning gains in reading 
comprehension and English language skills.  
 
References  
 
Carlo, M.S., August, D., McLaughlin, B., Snow, C.E., 
Dressler, C., Lippman, D., Lively, T. & White, C.  
 
 
(2004). Closing the gap: Addressing the vocabulary 
needs of English language learners in bilingual and 
mainstream classrooms. Reading Research Quarterly, 
39(2), 188-215. 
 
Echevarria, J., Vogt, M., and Short, D. (2004). Making 
Content Comprehensible for English Language 
Learners: the SIOP model.  New York: Pearson 
Education, Inc. 
 
Heilman, M., Collins-Thompson, K., Callan, J., 
Eskenazi, M. (2006) Classroom Success of an 
Intelligent Tutoring System for Lexical Practice and 
Reading Comprehension. In Proceedings of the Ninth 
International Conference on Spoken Language 
Processing.  Pittsburgh. 
 
Lin, D. (1998). Automatic Retrieval and Clustering of 
Similar Words.  In Proceedings of the 35th Annual 
Meeting of the Association for Computational 
Linguistics, Montreal, 898-904. 
 
Marcu, D. (2000) The Theory and Practice of Discourse 
Parsing and Summarization. The MIT Press, 
Cambridge, Massachusetts. 
 
Schwarm, S. and Ostendorf, M. Reading Level 
Assessment Using Support Vector Machines and 
Statistical Language Models. In Proceedings of the 
Association for Computational Linguistics, Ann 
Arbor, MI, 523-530. 
 
Yano, Y., Long, M. & Ross, S. (1994). The effects of 
simplified and elaborated texts on foreign language 
reading comprehension. Language Learning, 44, 189-
219.  
4
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
        Automatic Scoring of Children's Read-Aloud Text Passages and 
Word Lists 
 
Klaus Zechner and John Sabatini and Lei Chen 
Educational Testing Service 
Rosedale Road 
Princeton, NJ 08541, USA 
{kzechner,jsabatini,lchen}@ets.org 
 
 
 
 
Abstract 
Assessment of reading proficiency is typically 
done by asking subjects to read a text passage 
silently and then answer questions related to 
the text. An alternate approach, measuring 
reading-aloud proficiency, has been shown to 
correlate well with the aforementioned com-
mon method and is used as a paradigm in this 
paper.  
We describe a system that is able to automati-
cally score two types of children?s read speech 
samples (text passages and word lists), using 
automatic speech recognition and the target 
criterion ?correctly read words per minute?. 
Its performance is dependent on the data type 
(passages vs. word lists) as well as on the rela-
tive difficulty of passages or words for indi-
vidual readers. Pearson correlations with 
human assigned scores are around 0.86 for 
passages and around 0.80 for word lists. 
1 Introduction 
It has long been noted that a substantial number of 
U.S. students in the 10-14 years age group have 
deficiencies in their reading competence (National 
Center of Educational Statistics, 2006). With the 
enactment of the No Child Left Behind Act (2002), 
interest and focus on objectively assessing and im-
proving this unsatisfactory situation has come to 
the forefront. 
While assessment of reading is usually done post-
hoc with measures of reading comprehension, di-
rect reading assessment is also often performed 
using a different method, oral (read-aloud) reading. 
In this paradigm, students read texts aloud and 
their proficiency in terms of speed, fluency, pro-
nunciation, intonation etc. can be monitored di-
rectly while reading is in progress. In the reading 
research literature, oral reading has been one of the 
best diagnostic and predictive measures of founda-
tional reading weaknesses and of overall reading 
ability (e.g., Deno et al, 2001; Wayman et al, 
2007).  An association between low reading com-
prehension and slow, inaccurate reading rate has 
been confirmed repeatedly in middle school popu-
lations (e.g., Deno & Marsten, 2006).  Correlations 
consistently fall in the 0.65-0.7 range for predict-
ing untimed passage reading comprehension test 
outcomes (Wayman et al, 2007). 
 
In this paper, we investigate the feasibility of 
large-scale, automatic assessment of read-aloud 
speech of middle school students with a reasonable 
degree of accuracy (these students typically attend 
grades 6-8 and their age is in the 10-14 years 
range).  If possible, this would improve the utility 
of oral reading as a large-scale, school-based as-
sessment technique, making it more efficient by 
saving costs and time of human annotations and 
grading of reading errors. 
The most widely used measure of oral reading pro-
ficiency is ?correctly read words per minute? 
(cwpm) (Wayman et al, 2007). To obtain this 
measure, students? read speech samples are first 
10
recorded, then the reading time is determined, and 
finally a human rater has to listen to the recording 
and note all reading errors and sum them up. Read-
ing errors are categorized into word substitutions, 
deletions etc.  
We have several sets of digitally recorded read-
aloud samples from middle school students avail-
able which were not collected for use with auto-
matic speech recognition (ASR) but which were 
scored by hand. 
Our approach here is to pass the children?s speech 
samples through an automatic speech recognizer 
and then to align its output word hypotheses with 
the original text that was read by the student. From 
this alignment and from the reading time, an esti-
mate for the above mentioned measure of cwpm 
can then be computed. If the automatically com-
puted cwpm measures are close enough to those 
obtained by human hand-scoring, this process may 
be employed in real world settings eventually to 
save much time and money. 
 
Recognizing children?s speech, however, has been 
shown to be substantially harder than adult speech 
(Lee et al, 1999; Li and Russell, 2002), which is 
partly due to children?s higher degree of variability 
in different dimensions of language such as pro-
nunciation or grammar. In our data, there was also 
a substantial number of non-native speakers of 
English, presenting additional challenges. We used 
targeted training and adaptation of our ASR sys-
tems to achieve reasonable word accuracies. While 
for text passages, the word accuracy on unseen 
speakers was about 72%, it was only about 50% 
for word lists, which was due in part to a higher 
percentage of non-native speakers in this data set, 
to the fact that various sources of noise often pre-
vented the recognizer from correctly locating the 
spoken words in the signal, and also due to our 
choice of a uniform language model since conven-
tional n-gram models did not work on this data 
with many silences and noises between words. 
 
The remainder of this paper is organized as fol-
lows: in Section 2 we review related work, fol-
lowed by a description of our data in Section 3. 
Section 4 provides a brief description of our speech 
recognizer as well as the experimental setup. Sec-
tion 5 provides the results of our experiments, fol-
lowed by a discussion in Section 6 and conclusions 
and future work in Section 7. 
 
2 Related work 
Following the seminal paper about the LISTEN 
project (Mostow et al 1994), a number of studies 
have been conducted on using automatic speech 
recognition technology to score children?s read 
speech. 
 
Similar to automated assessment of adults? speech 
(Neumeyer, Franco et al 2000; Witt, 1999), the 
likelihood computed in the Hidden Markov Model 
(HMM) decoding and some measurements of  flu-
ency, e.g., speaking rate, are widely used as fea-
tures for predicting children?s speaking 
proficiency. Children?s speech is different than 
adults?. For example, children?s speech exhibits 
higher fundamental frequencies (F0) than adults on 
average. Also, children?s more limited knowledge 
of vocabulary and grammar results in more errors 
when reading printed text. Therefore, to achieve 
high-quality recognition on children?s speech, 
modifications have to be made on recognizers that 
otherwise work well for adults. 
In the LISTEN project (Mostow et al, 1994), the 
basic technology is to use speech recognition to 
classify each word of text as correctly read or not. 
Such a classification task is hard in that the chil-
dren?s speaking deviations from the text may in-
clude arbitrary words and non-words. In a study, 
they modeled variations by the modification of the 
lexicon and the language model of the Sphinx1 
speech recognizer.  
 
Recently, the Technology Based Assessment  of 
Language and Literacy project (TBALL,  (Alwan, 
2007)) has been attempting to assess and evaluate 
the language and literacy skills of young children 
automatically. In the TBALL project, a variety of 
tests including word verification, syllable blending, 
letter naming, and reading comprehension, are 
jointly used. Word verification is an assessment 
that measures the child?s pronunciation of read-
aloud target words. A traditional pronunciation 
verification method based on log-likelihoods from 
HMM models is used initially (Tepperman et al, 
2006). Then an improvement based on a Bayesian 
network classifier (Tepperman et al, 2007) is em-
                                                          
1 See http://cmusphinx.sourceforge.net/html/cmusphinx.php 
11
ployed to handle complicated errors such as pro-
nunciation variations and other reading mistakes. 
 
Many other approaches have been developed to 
further improve recognition performance on chil-
dren?s speech. For example, one highly accurate 
recognizer of children?s speech has been developed 
by Hagen et al (2007). Vocal tract length normali-
zation (VTLN) has been utilized to cope with the 
children?s different acoustic properties. Some spe-
cial processing techniques, e.g., using a general 
garbage model to model all miscues in speaking, 
have been devised to improve the language model  
used in the recognition of children?s speech (Li et 
al., 2007). 
 
3 Data 
For both system training and evaluation, we use a 
data set containing 3 passages read by the same 
265 speakers (Set1) and a fourth passage (a longer 
version of Passage 1), read by a different set of 55 
speakers (Set2). Further, we have word lists read 
by about 500 different speakers (Set3). All speak-
ers from Set12 and most (84%) from the third set 
were U. S. middle school students in grades 6-8 
(age 10-14). A smaller number of older students in 
grades 10-12 (age 15-18) was also included in the 
third set (16%).3 4  
In terms of native language, about 15% of Set1 and 
about 76% of Set35 are non-native speakers of 
English or list a language different from English as 
their preferred language. 
Table 1 provides the details of these data sets. In 
the word lists data set, there are 178 different word 
lists containing 212 different word types in total 
(some word lists were read by several different 
students). 
 
All data was manually transcribed using a spread-
sheet where each word is presented in one line and 
the annotator, who listens to the audio file, has to 
                                                          
2 For Set1, we have demographics for 254 of 265 speakers 
(both for grade level and native language). 
3 Grade demographics are available for 477 speakers of Set3. 
4 We do not have demographic data for the small Set2 (55 
speakers). 
5 This set (Set 3) has information on native language for 165 
speakers. 
mark-up any insertions, substitutions or deletions 
by the student.  
 
Name Recordings Length in 
words 
Passage 1 
(?Bed?, Set1-A) 
265 158 
Passage 2 
(?Girls?, Set1-B) 
265 74 
Passage 3 
(?Keen?, Set1-C) 
265 100 
Passage 4 
(?Bed*?) (Set2) 
55 197 
Word lists (Set3) 590 62 (average) 
Table 1. Text passages and word lists data sets. 
 
For ASR system training only, we additionally 
used parts of the OGI (Oregon Graduate Institute) 
and CMU (Carnegie Mellon University) Kids data 
sets as well (CSLU, 2008; LDC, 1997). 
 
4 ASR system and experiments 
The ASR system?s acoustic model (AM) was 
trained using portions of the OGI and CMU Kids? 
corpora as well as a randomly selected sub-set of 
our own passage and word list data sets described 
in the previous section. About 90% of each data set 
(Set1, Set2, Set3) was used for that purpose. Since 
the size of our own data set was too small for AM 
training, we had to augment it with the two men-
tioned corpora (OGI, CMU Kids), although they 
were not a perfect match in age range and accent. 
All recordings were first converted and down-
sampled to 11 kHz, mono, 16 bit resolution, PCM 
format. There was no speaker overlap between 
training and test sets. 
 
For the language model (LM), two different mod-
els were created: for passages, we built an interpo-
lated trigram LM where 90% of the weight is 
assigned to a LM trained only on the 4 passages 
from the training set (Set1, Set2) and 10% to a ge-
neric LM using the Linguistic Data Consortium 
(LDC) Broadcast News corpus (LDC, 1997). The 
dictionary contains all words from the transcribed 
passages in the training set, augmented with the 
1,000 most frequent words from the Broadcast 
News corpus. That way, the LM is not too restric-
tive and allows the recognizer to hypothesize some 
12
reading mistakes not already encountered in the 
human transcriptions of the training set. 
 
For the word lists, a trigram LM was found to be 
not working well since the words were spoken in 
isolation with sometimes significant pauses in be-
tween and automatic removal of these silences 
proved too hard given other confounding factors 
such as microphone, speaker, or background noise. 
Therefore it was decided to implement a grammar 
LM for the word list decoder where all possible 
words are present in a network that allows them to 
occur at any time and in any sequence, allowing 
for silence and/or noises in between words. This 
model with uniform priors, however, has the dis-
advantage of not including any words not present 
in the word list training set, such as common mis-
pronunciations and is therefore more restrictive 
than the LM for text passages. 
 
One could make the argument of using forced 
alignment instead of a statistical LM to determine 
reading errors. In fact, this approach is typically 
used when assessing the pronunciation of read 
speech. However, in our case, the interest is more 
in determining how many words were read cor-
rectly in the sequence of the text (and how fast 
they were read) as opposed to details in pronuncia-
tion. Further, even if we had confidence scores 
attached to words in forced alignment, deciding on 
which of the words obtained low confidence due to 
poor pronunciation or due to substitution would 
not be an easy decision. Finally, word deletions 
and insertions, if too frequent, might prevent the 
forced alignment algorithm from terminating. 
 
After training was complete, we tested the recog-
nizer on the held-out passage and word list data. 
After recognizing, we computed our target meas-
ure of ?correct words per minute? (cwpm) accord-
ing to the following formula (W= all words in a 
text, S= substitutions, D= deletions, T= reading 
time in minutes), performing a string alignment 
between the recognizer hypothesis and the passage 
or word list to be read: 
 
(1) 
W S D
cwpm
T
? ?=   
The reason that insertions are not considered here 
is that they contribute to an increase in reading 
time and therefore can be considered to be ac-
counted for already in the formula.  
 
Next, we performed an experiment that looks at 
whether automatic scoring of read-aloud speech 
allows for accurate predictions of student place-
ments in broad cohorts of reading proficiency. 
 
We then also look more closely at typical errors 
made by human readers and the speech recognizer. 
All these experiments are described and discussed 
in the following section. 
 
Table 2 describes the set-up of the experiments. 
Note that Passage4 (Set2) was included only in the 
training but not in the evaluation set since this set 
was very small. As mentioned in the previous sec-
tion, most speakers from the passage sets read 
more than one passage and a few speakers from the 
word lists set read more than one word list. 
 
Data set Recordings Speakers Language 
model 
type 
Passages1-
3 
101 37 Trigram  
Word lists 42 38 Grammar 
Table 2. Experiment set-up (evaluation sets). 
 
5 Results 
5.1 Overall results 
Table 3 depicts the results of our evaluation run 
with the ASR system described above. Word accu-
racy is measured against the transcribed speaker 
reference (not against the true text that was read). 
Word accuracy is computed according to Equation 
(2), giving equal weight to reference and ASR hy-
pothesis (c=correct, s=substitutions, d=deletions, 
i=insertions). This way, the formula is unbiased 
with respect to insertions or deletions: 
 
(2)  
0.5 100.0
c c
wacc
c s d c s i
? ?= ? ? +? ?+ + + +? ?  
 
 
13
 
 
Data set Recordings Speakers Average word 
Accuracy over all 
speech sample 
Minimum word 
accuracy on a 
speech sample 
Maximum word  
accuracy on a speech 
sample 
All Passages  
(1-3) 
101 37 72.2 20.4 93.8 
Passage1 
(?Bed?) 
28 28 70.8 20.4 83.6 
Passage2 
(?Girls?) 
36 36 64.1 25.4 85.7 
Passage3 
(?Keen?) 
37 37 77.7 27.4 93.8 
Word lists 42 38 49.6 10.8 78.9 
Table 3. ASR experiment results (word accuracies in percent) 
 
 
The typical run-time on a 3.2GHz Pentium proces-
sor was less than 30 seconds for a recording (faster 
than real time). 
 
We next compute cwpm measures for both human 
annotations (transcripts, ?gold standard?) and ma-
chine (ASR) hypotheses  
Human annotators went over each read passage 
and word list and marked all reading errors of the 
speakers (here, only deletions and substitutions are 
relevant). The reading time is computed directly 
from the speech sample, so machine and human 
cwpm scores only differ in error counts of dele-
tions and substitutions. Currently we only have one 
human annotation available per speech sample, but 
we aim to obtain a second annotation for the pur-
pose of determining inter-annotator agreement. 
 
Table 4 presents the overall results of comparing 
machine and human cwpm scoring. We performed 
both Pearson correlation as well as Spearman rank 
correlation. While the former provides a more ge-
neric measure of cwpm correlation, the latter fo-
cuses more on the question of the relative 
performance of different speakers compared to 
their peers which is usually the more interesting 
question in practical applications of reading as-
sessment. Note that unlike for Table 3, the ASR 
hypotheses are now aligned with the text to be read 
since in a real-world application, no human tran-
scriptions would be available. 
We can see that despite the less than perfect recog-
nition rate of the ASR system which causes a much 
lower average estimate for cwpm or cw (for word-
lists), both Pearson and Spearman correlation coef-
ficients are quite high, all above 0.7 for Spearman 
rank correlation and equal to 0.8 or higher for the 
Pearson product moment correlation. This is en-
couraging as it indicates that while current ASR 
technology is not yet able to exactly transcribe 
children?s read speech, it is 
 
Data set Gold 
cwpm
ASR-
based 
cwpm 
Pearson 
r corre-
lation 
Spearman 
rank cor-
relation 
All Pas-
sages  
(1-3) 
152.0 109.8 0.86 NA 
Passage1 
(Bed) 
174.3 123.5 0.87 0.72 
Passage2 
(Girls) 
133.1 86.5 0.86 0.73 
Passage3 
(Keen) 
153.4 122.2 0.86 0.77 
Word 
lists* 
 
48.0 29.4 0.80 0.81 
Table 4. CWPM results for passages and word 
lists. All correlations are significant at p<0.01. 
*For word lists, we use ?cw? (correct words, nu-
merator of Equation (1)) as the measure, since stu-
dents were not told to be rewarded for faster 
reading time here. 
 
possible to use its output to compute reasonable 
read-aloud performance measures such as cwpm 
14
which can help to quickly and automatically assess 
reading proficiencies of students. 
5.2 Cohort assignment experiment 
To follow up on the encouraging results with basic 
and rank correlation, we conducted an experiment 
to explore the question of practical importance 
whether the automatic system can assign students 
to reading proficiency cohorts automatically. 
For better comparison, we selected those 27 stu-
dents from 37 total who read all 3 passages (Set 1) 
and grouped them into three cohorts of 9 students 
each, based on their human generated cwpm score 
for all passages combined: (a) proficient 
(cwpm>190), (b) intermediate (135<cwpm<190), 
and (c) low proficient (cwpm<135). 
We then had the automatic system predict each 
student?s cohort based on the cwpm computed 
from ASR. Since ASR-based cwpm values are co-
nsistently lower than human annotator based cwpm 
values, the automatic cohort assignment is not 
based on the cwpm values but rather on their rank-
ing. 
The outcome of this experiment is very encourag-
ing in that there were no cohort prediction errors 
by the automatic system. While the precise ranking 
differs, the system is very well able to predict 
overall cohort placement of students based on 
cwpm. 
5.3 Overall comparison of students? reading er-
rors and ASR recognition errors 
To look into more detail of what types of reading 
errors children make and to what extent they are 
reflected by the ASR system output, we used the 
sclite-tool by the National Institute for Standards 
and Technology (NIST, 2008) and performed two 
alignments on the evaluation set: 
1. TRANS-TRUE: Alignment between human 
transcription and true passage or word list text to 
be read: this alignment informs us about the kinds 
of reading errors made by the students. 
2. HYPO-TRANS: Alignment between the ASR 
hypotheses and the human transcriptions; this 
alignment informs us of ASR errors. (Note that this 
is different from the experiments reported in Table 
4 above where we aligned the ASR hypotheses 
with the true reference texts to compute cwpm.) 
 
Table 5 provides general statistics on these two 
alignments. 
 
Data set Alignment SUB DEL INS 
Passages
1-3 
TRANS-
TRUE 
2.0% 6.1% 1.8% 
Pas-
sages1-3 
HYPO-
TRANS 
18.7% 9.6% 8.1% 
Word 
lists 
TRANS-
TRUE 
5.6% 6.2% 0.6% 
Word 
lists 
HYPO-
TRANS 
42.0%  8.9% 6.4% 
Table 5. Word error statistics on TRANS-TRUE 
and HYPO-TRANS alignments for both evaluation 
data sets. 
 
From Table 5 we can see that while for students, 
deletions occur more frequently than substitutions 
and, in particular, insertions, the ASR system, due 
to its imperfect recognition, generates mostly sub-
stitutions, in particular for the word lists where the 
word accuracy is only around 50%. 
Further, we observe that the students? average 
reading word error rate (only taking into account 
substitutions and deletions as we did above for the 
cwpm and cw measures) lies around 8% for pas-
sages and 12% for wordlists (all measured on the 
held-out evaluation data). 
5.4 Specific examples 
Next, we look at some examples of frequent confu-
sion pairs for those 4 combinations of data sets and 
alignments. Table 6 lists the top 5 most frequent 
confusion pairs (i.e., substitutions).  
 
For passages, all of the most frequent reading er-
rors by students are morphological variants of the 
target words, whereas this is only true for some of 
the ASR errors, while other ASR errors can be far 
off the target words. For word lists, student errors 
are sometimes just orthographically related to the 
target word (e.g., ?liner? instead of ?linear?), and 
sometimes of different part-of-speech (e.g., 
?equally? instead of ?equality?). ASR errors are 
typically related to the target word by some pho-
netic similarity (e.g., ?example? instead of ?sim-
ple?). 
 
15
 
Finally, we look at a comparison between errors 
made by the students and the fraction of those cor-
rectly identified by the ASR system in the recogni-
tion hypotheses. Table 7 provides the statistics on 
these matched errors for text passages and word 
lists.  
 
Data 
set 
Align-
ment 
Refer-
ence 
Spoken/ 
recog-
nized 
Count 
Pas-
sages
1-3 
TRANS
-TRUE 
asks 
savings 
projects 
teacher?s 
time 
ask 
saving 
project 
teacher 
times 
6 
5 
4 
4 
4 
Pas-
sages
1-3 
HYPO-
TRANS 
storm 
lee?s 
lee?s 
observer 
thousand 
storms 
be 
we 
and 
the 
11 
6 
6 
6 
6 
Word 
lists 
TRANS
-TRUE 
nature 
over-
sleep 
equality 
linear 
ware-
housed 
Natural 
overslept 
 
equally 
liner 
ware-
house 
6 
5 
 
4 
4 
3 
Word 
lists 
HYPO-
TRANS 
plan      
see  
simple 
unoffi-
cial   
loud 
planned 
season 
example 
competi-
tion 
through-
out 
8 
6 
6 
5 
 
4 
Table 6. Top 5 most frequent confusion pairs for 
passages and word list evaluation sets in two dif-
ferent alignments. For passages, substitutions 
among closed class words such as determiners or 
prepositions are omitted. 
 
Table 7 shows that while for text passages, almost 
half of the relevant errors (substitutions and dele-
tions) were correctly identified by the recognizer, 
for word lists, this percentage is substantially 
smaller. 
 
 
 
 
6 Discussion 
The goal of this paper is to evaluate the possibility 
of creating a system for automatic oral reading as-
sessment for middle school children, based on text 
passages and word lists. 
We decided to use the common reading profi-
ciency measure of ?correct words per minute? 
which enables us to align ASR word hypotheses 
with the correct texts, estimate cwpm based on this 
alignment and the reading time, and then compare 
the automatically estimated cwpm with human an-
notations of the same texts. 
 
 
Data set / error type Percentage of correctly 
identified errors 
Passages 1-3 ? SUB 20.6 
Passages 1-3 ? DEL 56.4 
Passages 1-3 ? 
SUB+DEL 
47.7 
Word lists ? SUB 2.7 
Word lists ? DEL 29.4 
Word lists ? 
SUB+DEL 
16.8 
Table 7. Statistics on matched errors: percentage of  
students? reading errors (substitutions and dele-
tions) that were also correctly identified by the 
ASR system. 
 
We built a recognizer with an acoustic model 
based on CMU and OGI kids? corpora as well as 
about 90% of our own text passages and word list 
data (Sets 1-3). For the in-context reading (text 
passages) we trained a trigram model focused 
mostly on transcriptions of the passages. For the 
out-of-context isolated word reading, we used a 
grammar language model where every possible 
word of the word lists in the training set can follow 
any other word at any time, with silence and/or 
noise between words. (While this was not our pre-
ferred choice, standard n-gram language models 
performed very poorly given the difficulty of re-
moving inter-word silences or noise automati-
cally.) 
Given how hard ASR for children?s speech is and 
given our small matched data sets, the word accu-
racy of 72% for text passages was not unreason-
able and was acceptable, particularly in a first 
development cycle. The word accuracy of only 
about 50% for word lists, however, is more prob-
16
lematic and we conjecture that the two main rea-
sons for the worse performance were (a) the ab-
sence of time stamps for the location of words 
which made it sometimes hard for the recognizer to 
locate the correct segment in the signal for word 
decoding (given noises in between), and (b) the 
sometimes poor recording conditions where vol-
umes were set too high or too low, too much back-
ground or speaker noise was present etc. Further, 
the high relative number of non-native speakers in 
that data set may also have contributed to the lower 
word accuracy of the word lists. 
While the current data collection had not been 
done with speech recognition in mind, in future 
data collection efforts, we will make sure that the 
sound quality of recordings is better monitored, 
with some initial calibration, and that we store time 
stamps when words are presented on the screen to 
facilitate the recognition task and to allow the rec-
ognizer to expect one particular word at one par-
ticular point in time. 
Despite imperfect word accuracies, however, for 
both passages and word lists we found encourag-
ingly high correlations between human and auto-
matic cwpm measures (cw measures for word 
lists). Obviously, the absolute values of cwpm dif-
fer greatly as the ASR system generates many 
more errors on average than the readers, but both 
Pearson correlation as well as Spearman rank cor-
relation measures are all above 0.7. This means 
that if we would use our automatic scoring results 
to rank students? reading proficiency, the ranking 
order would be overall quite similar to an order 
produced by human annotators. This observation 
about the rank, rather than the absolute value of 
cwpm, is important in so far as it is often the case 
that educators are interested in separating ?co-
horts? of readers with similar proficiency and in 
particular to identify the lowest performing cohort 
for additional reading practice and tutoring. 
An experiment testing the ability of the system to 
place students into three reading proficiency co-
horts based on cwpm was very encouraging in that 
all 27 students of the test set were placed in the 
correct cohort by the system. 
When we compare frequent student errors with 
those made by the machine (Table 6), we see that 
often times, students just substitute slight morpho-
logical variants (e.g., ?ask? for ?asks?), whereas in 
the ASR system, errors are typically more complex 
than just simple substitutions of morphological 
variants. However, in the case of word lists, we do 
find substitutions with related phonological content 
in the ASR output (e.g., ?example? for ?simple?). 
Finally, we observed that, only for the text pas-
sages, the ASR system could correctly identify a 
substantial percentage of readers? substitutions and 
deletions (about 48%, see Table 7). This is also 
encouraging as it is a first step towards meaningful 
feedback in a potential interactive setting. How-
ever, we here only look at recall ? because of the 
much larger number of ASR substitutions, preci-
sion is much lower and therefore the risk of over-
correction (false alarms) is still quite high. 
Despite all of the current shortcomings, we feel 
that we were able to demonstrate a ?proof-of-
concept? with our initial system in that we can use 
our trained ASR system to make reliable estimates 
on students? reading proficiency as measured with 
?correct words per minute?, where correlations 
between human and machine scores are in the 
0.80-0.86 range for text passages and word lists. 
 
7 Conclusions and future work 
This paper demonstrates the feasibility of building 
an automatic scoring system for middle school stu-
dents? reading proficiency, using a targeted trained 
speech recognition system and the widely used 
measure of ?correctly read words per minute? 
(cwpm). 
The speech recognizer was trained both on external 
data (OGI and CMU kids? corpora) and internal 
data (text passages and word lists), yielding two 
different modes for text passages (trigram language 
model) and word lists (grammar language model). 
Automatically estimated cwpm measures agreed 
closely with human cwpm measures, achieving 0.8 
and higher correlation with Pearson and 0.7 and 
higher correlation with Spearman rank correlation 
measures. 
Future work includes an improved set-up for re-
cordings such as initial calibration and on-line 
sound quality monitoring, adding time stamps to 
recordings of word lists, adding more data for 
training/adaptation of the ASR system, and explor-
ing other features (such as fluency features) and 
their potential role in cwpm prediction. 
 
 
17
Acknowledgements 
The authors would like to acknowledge the contri-
butions of Kathy Sheehan, Tenaha O?Reilly and 
Kelly Bruce to this work. We further are grateful 
for the useful feedback and suggestions from our 
colleagues at ETS and the anonymous reviewers 
that greatly helped improve our paper. 
 
References 
Alwan, A. (2007). A System for Technology Based 
Assessment of Language and Literacy in Young 
Children: the Role of Multiple Information 
Sources. Proceedings of MMSP, Greece. 
Center for Spoken Language Understanding 
(CSLU), 2008. Kids? Speech Corpus, 
http://www.cslu.ogi.edu/corpora/kids/.LDC, BN. 
Deno, S. L., Fuchs, L. S., Marston, D., & Shin, J. 
(2001). Using curriculum-based measurements 
to establish growth standards for students with 
learning disabilities. School Psychology Re-
view, 30(4), 507-524. 
Deno, S. L. and D. Marsten (2006). Curriculum-
based measurement of oral reading: An indicator 
of growth in fluency. What Research Has to Say 
about Fluency Instruction. S. J. Samuels and A. 
E. Farstrup. Newark, DE, International Reading 
Association: 179-203. 
Hagen, A., B. Pellom, & R. Cole. (2007). "Highly 
accurate children?s speech recognition for inter-
active reading tutors using subword units." 
Speech Communication 49(6): 861-873. 
Lee, S., A. Potamianos, & S. Narayanan. (1999). 
"Acoustics of children's speech: developmental 
changes of temporal and spectral parameters." 
Journal of Acoustics Society of American 
(JASA) 105: 1455-1468. 
Li, X., Y. C. Ju, L. Deng & A. Acero. (2007). Effi-
cient and Robust Language Modeling in an 
Automatic Children's Reading Tutor System. 
Proc. IEEE International Conference on Acous-
tics, Speech and Signal Processing ICASSP 
2007. 
Li, Q. and M. Russell (2002). An analysis of the 
causes of increased error rates in children's 
speech recognition. ICSLP. Denver, CO. 
Linguistic Data Consortium (LDC), 1997. 1996 
English Broadcast News Speech (HUB4), 
LDC97S44. 
Linguistic Data Consortium (LDC), 1997. The 
CMU Kids Corpus, LDC97S63. 
Mostow, J., S. F. Roth, G. Hauptmann & M. Kane. 
(1994). A prototype reading coach that listens. 
AAAI '94: Proceedings of the twelfth national 
conference on Artificial intelligence, Menlo 
Park, CA, USA, American Association for Arti-
ficial Intelligence. 
National Center of Educational Statistics. (2006). 
National Assessment of Educational Progress. 
Washington DC: U.S. Government Printing Of-
fice. 
National Institute for Standards and Technology 
(NIST), 2008. Sclite software package. 
http://www.nist.gov/speech/tools/ 
Neumeyer, L., H. Franco, V. Digalakis & M. 
Weintraub. (2000). "Automatic Scoring of Pro-
nunciation Quality." Speech Communication 6. 
No Child Left Behind Act of 2001, Pub. L. No. 
107-110, 115 Stat. 1425 (2002). 
Tepperman, J., J. Silva, A. Kazemzadeh, H. You, 
S. Lee, A. Alwan & S. Narayanan. (2006). Pro-
nunciation verification of children's speech for 
automatic literacy assessment. INTERSPEECH-
2006. Pittsburg, PA. 
Tepperman, J., M. Black, P. Price, S. Lee, A. Ka-
zemzadeh, M. Gerosa, M. Heritage, A. Alwan & 
S. Narayanan.(2007). A bayesian network clas-
sifier for word-level reading assessment. Pro-
ceedings of ICSLP, Antwerp, Belgium. 
Wayman, M. M., Wallace, T., Wiley, H. I., Ticha, 
R., & Espin, C. A. (2007). Literature synthesis 
on curriculum-based measurement in reading. 
The Journal of Special Education, 41(2), 85-120. 
Witt, S. M. (1999). Use of Speech Recognition in 
Computer-assisted Language Learning, Univer-
sity of Cambridge. 
 
18
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 1?10,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
 
 
A User Study: Technology to Increase Teachers? Linguistic Awareness  
to Improve Instructional Language Support  
for English Language Learners 
 
 
Jill Burstein, John Sabatini, Jane Shore, Brad Moulder, and Jennifer Lentini 
 
Educational Testing Service 
666 Rosedale Road, Princeton, New Jersey 08541 
{jburstein, jsabatini, jshore, bmoulder, jlentini}@ets.org 
 
 
 
Abstract 
This paper discusses user study outcomes with 
teachers who used Language MuseSM a web-
based teacher professional development (TPD) 
application designed to enhance teachers? lin-
guistic awareness, and support teachers in the 
development of language-based instructional 
scaffolding (support) for their English language 
learners (ELL). System development was 
grounded in literature that supports the notion 
that instruction incorporating language support 
for ELLs can improve their accessibility to 
content-area classroom texts ?in terms of ac-
cess to content, and improvement of language 
skills. Measurement outcomes of user piloting 
with teachers in a TPD setting indicated that 
application use increased teachers' linguistic 
knowledge and awareness, and their ability to 
develop appropriate language-based instruction 
for ELLs. Instruction developed during the pi-
lot was informed by the application?s linguistic 
analysis feedback, provided by natural lan-
guage processing capabilities in Language 
Muse. 
1 Introduction 
Statistics show that between 1997 and 2009 the 
number of ELLs enrolled in U.S. public schools 
has increased by 51% (National Clearinghouse for 
Language Acquisition, 2011). ELLs who have 
lower literacy skills, and who are reading below 
grade level may be mainstreamed into regular con-
tent-area classrooms, and may not receive supple-
mental English language instruction. 
Unfortunately, K-12 content-area teachers1 are less 
likely to be trained to adapt their instructional ap-
proaches to accommodate the diverse cultural and 
linguistic backgrounds of students with varying 
levels of English proficiency (Adger, Snow, & 
Christian, 2002; Calder?n, August, Slavin, Cheun, 
Dur?n, & Madden, 2005; Rivera, Moughamian, 
Lesaux, & Francis, 2008; Walqui & Heritage, 
2012). This situation motivated the development 
of Language MuseSM, a web-based application de-
signed to offer teacher professional development 
(TPD) for content-area teachers to support their 
understanding of potential sources of linguistic 
unfamiliarity that may obscure text content for 
ELLs, and their ability to develop relevant lan-
guage-based instructional scaffolding. We rea-
soned that prerequisite to effectively planning or 
implementing instructional supports for ELLs, 
teachers first needed to be able to recognize poten-
tial sources of linguistic difficulty. Further, teach-
ers might need training about the specific 
linguistic structures that might be unfamiliar to 
learners, and which might lead to learners? inac-
cessibility to core content in text.  
    The motivation for Language Muse, thus, grew 
from the need to provide teachers with training 
about linguistic features in texts that may be un-
familiar to learners. In complement to training 
videos and reading resources, Language Muse 
contains a module that provides automated and 
explicit linguistic feedback for texts, and is intend-
                                                          
1 These are Kindergarten-12th grade teachers of subject areas, 
including math, science, social studies, and English language 
arts. 
1
 
 
ed to support teachers in the development of les-
son plans with language-based instructional activi-
ties and assessments to support reading and 
content comprehension of texts. The linguistic 
feedback module uses various natural language 
processing methods to provide feedback at the vo-
cabulary, phrasal, sentential, and discourse levels. 
Another motivation of application was efficiency. 
Even with a strong linguistic awareness, manual 
identification of linguistic features would be a 
very time-consuming process. 
   Outcomes from pre-post teacher assessments 
delivered through user piloting with teachers indi-
cated that teachers who used Language Muse 
showed gains in linguistic knowledge. Outcomes 
also indicated that Language Muse use supported 
teachers in the ability to develop appropriate lan-
guage-based instruction for ELLs, informed by the 
application?s linguistic analysis feedback.  
2 Related Work 
In a brief literature review, we address the lan-
guage demands for ELLs in reading content-area 
texts, and the need for relevant teacher training for 
content-area teachers (Section 2.1).  We also dis-
cuss NLP-related applications that support the lin-
guistic analysis of texts -- typically in the context 
of developing readability measures -- which con-
tinues to be a prominent area of research; other 
research supports student tools allowing direct 
interaction with language forms (Section 2.2).  
 
2.1 Language Demands on ELLs, and 
Teacher Training 
 
Language Demands on ELLs. The English Lan-
guage Arts Common Core State Standards2 
(Standards) (NGA Center & CCSSO, 2010) has 
now been adopted by 46 states and is a trend-setter 
in U.S. education. The Standards emphasize the 
need for all learners (including ELLs3) to read 
progressively more complex texts across multiple 
genres in the content areas, preparing learners for 
college and careers. To accomplish this, learners 
must have familiarity with numerous linguistic 
features related to vocabulary, English language 
                                                          
2 http://www.corestandards.org/ 
3 For details and about Standards and ELLs, see: 
http://ell.stanford.edu/. 
structures, and a variety of text structures (dis-
course).  
    In terms of vocabulary demands, research re-
ports on investigations of academic vocabulary 
and the Tier word system (Beck, McKeown, & 
Kucan, 2008; Calder?n, 2007). Specifically, Tier 1 
words are those used in everyday conversation; 
Tier 2 words are general academic words; and Tier 
3 words are found in specific domains (Beck et al 
2008; Coleman & Pimental, 2011a).  All three Ti-
ers are necessary to academic content learning.  
Key content-area terms in any text would include 
the vocabulary that students are expected to learn 
regardless of the Tier. However, there are many 
other vocabulary terms in the same text that may 
or may not be key content, but may still pose diffi-
culties for an ELL reader.  For instance, the phrase 
?rock star? is a figurative term whose meaning is 
not obvious from knowing the various meanings 
of ?rock? or ?star?.  A deficit in morphological 
awareness can be a source of reading comprehen-
sion difficulties among native speakers of English, 
(Berninger, Abbott, Nagy, & Carlisle, 2009; Nagy, 
Berninger, & Abbot, 2006), but even more so 
among ELLs (Carlo, August, McLaughlin, Snow, 
Dressler, Lippmann, & White, 2004; Kieffer & 
Lesaux, 2008). Teaching morphological structure 
has been shown to be effective with ELLs 
(Lesaux, Kieffer, Faller, & Kelley, 2010; Proctor, 
Dalton, Uccelli, Biancarosa, Snow, & Neugebauer, 
2011). Native language support can also aid stu-
dents in learning text-based content (Francis, Au-
gust, Goldenberg, & Shanahan, 2004). 
Specifically, lessons that incorporate cognates 
(e.g., individual (English) and individuo (Spanish)) 
have been found to be effective in expanding Eng-
lish vocabulary development and aiding in com-
prehension (August, 2003; Proctor, Dalton, & 
Grisham, 2007).  Polysemous words can contribute 
to overall text difficulty.  Papamihiel, Lake & Rice 
(2005) specifically discuss difficulties of content-
specific, polysemous words, where the more 
common meaning may lead to a misconception 
when using that meaning to infer the more specific 
content meaning (e.g., prime in prime numbers). 
Unfamiliar cultural references (e.g., He?s a mem-
ber of the Senate.), when reading an unfamiliar 
language to learn unfamiliar content, imposes a 
triple cognitive load for ELLs (Goldenberg, 2008). 
    With regard to sentence-level demands, long, 
multi-clause sentences can present frustrating 
2
 
 
complexities. Readers need to analyze sentence 
clauses to understand and encode key information 
in working memory as they build a coherent men-
tal model of the meaning of a text (Kintsch, 1998).  
Different subject areas often have sentential and 
phrasal structures that are unique to that subject, 
resulting in comprehension breakdowns, e.g., the 
noun phrases in math texts ?a number which can 
be divided by itself ?? (Schleppegrell, 2007; 
Schleppegrell & de Oliveira, 2006).   
    Regarding discourse structure demands, con-
tent-area texts may represent varying discourse 
relationships. Discourse relations such as, com-
pare-contrast, cause-effect can all be intermingled 
within a single passage (Goldman & Rakestraw, 
2000; Meyer, 2003). Teachers need to learn how 
to identify discourse-level information and devel-
op scaffolding to support students? ability to navi-
gate discourse elements in texts. Students may also 
be challenged in keeping track of and resolving 
referential (anaphoric) relationships. Pronomial 
reference can be a challenge for ELLs in texts 
with multiple characters or agents (Kral, 2004). 
An equal challenge concerns the resolution of ref-
erential relations among nouns, phrases, or ideas - 
a common occurrence in expository texts- whether 
the category of reference is pronominal, synony-
my, paraphrase, or determiner, e.g., this, that, or 
those (Pretorius, 2005). Also critical to learning 
new content is understanding connector words 
functions (e.g., because, therefore) for building 
text cohesion (Goldman & Murray, 1992; 
Graesser, McNamara, & Louwerse, 2003).   
    Teacher Training. Teachers need to become lin-
guistically aware of aspects of the English lan-
guage that present potential obstacles to content 
access for ELLs. Yet, teachers often lack training 
in the identification of features of English that may 
challenge diverse groups of ELLs (Adger et al, 
2002; Calder?n et al, 2005; Rivera et al, 2008; 
Walqui & Heritage, 2012), and in the implementa-
tion of strategies to help ELLs academic language 
and vocabulary acquisition (Flinspach, Scott, Mil-
ler, Samway, & Vevea, 2008).  Further, the num-
ber of teachers trained in effective instructional 
strategies to meet the range of needs of ELLs has 
not increased consistently with the rate of the ELL 
population (G?ndara, Maxwell-Jolly, & Driscoll, 
2005; Green, Foote, Walker & Shuman, 2010). 
Studies suggest that teachers with specialized 
training have a positive impact on student perfor-
mance (Darling-Hammond, 2000; Peske & Hay-
cock, 2006). 
 
2.2 Text Accessibility and NLP 
 
Considerable research in NLP and text 
accessibility has focussed on linguistic properties 
of text that render a text relatively more or less 
accessible (comprehensible). This research stream 
has often fed into applications offering readability 
measures ? specifically, measures that predict the 
grade level, or grade range of a text (e.g., 
elementary, middle or high-school). Foundational 
research in this area examined the effect of  
morphological and syntactic text properties. Flesch 
(1948) reported that text features such as syllable 
counts of words, and sentence length were 
predictors of text difficulty.  Newer research in 
this area has included increasingly more NLP-
based investigations (Collins-Thompson & Callan, 
2004; Schwarm & Ostendorf, 2005; Miltsakaki, 
2009). Some research examines text quality in 
terms of discourse coherence of  well-formed texts 
(Barzilay & Lapata, 2008; Pitler & Nenkova, 
2008; Graesser, McNamara, & Kulikowich, 
2011).   
    Human evaluation of text complexity in curricu-
lum materials development (i.e., adaptation and 
scaffolding of reading texts, and the creation of 
activities and assessments) is a time-consuming, 
and typically intuitive process. Determining text 
complexity is also not a clear and objective meas-
ure. For example, what is complex for a native 
English speaker reading on grade level may vary 
from what is complex (or unfamiliar) for an ELL 
reading below grade level. This area of research 
continues to grow as is evidenced by NLP shared 
tasks (Mihalcea, Sinha & McCarthy, 2010), in the 
research and educational measurement communi-
ties (Burstein, Sabatini, and Shore, in press; Nel-
son, Perfetti, Liben & Liben, 2012).  
    The REAP system uses statistical language 
modeling to assign readability measures to Web 
documents (Collins-Thompson & Callan, 2004). 
This system is used in college-level ESL class-
rooms for higher level ESL students. It is designed 
to support automatic selection and delivery of ap-
propriate and authentic texts to students in an in-
structional setting (Heilman, Zhao, Pino, & 
Eskenazi, 2008). Teacher users can set a number 
of constraints (e.g., reading level, text length, and 
3
 
 
target vocabulary) to direct the text search.  The 
system then automatically performs the text selec-
tion.  The system also has tools that allow English 
learners to work with the text, including dictionary 
definition access and vocabulary practice exercis-
es. In pilot studies with high-intermediate learners 
in a university setting, a post-test showed promis-
ing learning outcomes (Heilman et al 2008). 
    WERTi (Working with English Real Texts in-
teractively) (Meurers et al, 2010) is an innovative 
Computer-Assisted Language Learning (CALL) 
tool that allows learners to interact directly with 
NLP outputs related to specific linguistic forms. In 
the context of a standard search environment, 
learners can select texts from the web. NLP meth-
ods are applied to identify linguistic forms that are 
often problematic for ELLs, including, use of de-
terminers and prepositions, wh-question formation, 
and phrasal verbs in the texts. Meurers et al point 
out that this CALL method is intended to draw 
learners? attention to specific properties of a lan-
guage (Rutherford and Sharwood Smith , 1985). 
ELLs? direct interaction with different linguistic 
forms could support them in language skills de-
velopment, and content accessibility.  
    To our knowledge, Language Muse is unique 
from other NLP applications in that it is designed 
as a teacher professional development (TPD) ap-
plication intended to enhance teachers? linguistic 
awareness, and as a result, aid teachers in the de-
velopment of language-based scaffolding to sup-
port learners? content accessibility, and language 
skills development. Key text complexity drivers 
cannot be communicated to teachers through nu-
merical aggregate readability measures which ap-
pear to be the predominant approach to analysis of 
text difficulty described in the literature. Lan-
guage Muse fills a critical TPD gap.  The appli-
cation is an innovative resource designed to help 
teachers understand the specific linguistic features 
that may contribute to text difficulty and ELLs? 
inaccessibility to text content; linguistic feedback 
features in SYSTEM are grounded in the literature 
about ELL language demands (Section 2.1). 
3 Language Muse  
Language Muse is a web-based application for 
enhancing teachers? linguistic awareness and sup-
porting the development of language-based in-
struction for ELLs. It uses NLP methods to 
provide explicit linguistic feedback that is ground-
ed in the literature discussing ELL language de-
mands and needs (Section 2.1).      
  We will discuss (a) the system?s specific lesson 
planning components, and (b) a text exploration 
tool that provides automated linguistic feedback. 
    The lesson planning component has three mod-
ules that support the creation of lesson plans, and 
related activities and assessments. To create a les-
son plan, teachers complete a lesson plan template 
(provided by the system) with five sections com-
monly found in lesson plans: (a) standards and 
objectives, (b) formative and summative assess-
ments, (c) engaging student interest/connecting to 
student background  knowledge, (d) modeling and 
guided practice, and (e) independent practice. 
Teachers use system functionality to link specific 
texts to a lesson plan. Texts have typically been 
analyzed, first, using the feedback tool. Feedback 
is then used to inform lesson plan development. 
Activities and assessments may also be created for 
a specific lesson plan and will also be linked to the 
plan.  Teachers are instructed to use linguistic 
feedback from the tool to develop language-
focused activities and assessments that can be used 
to    support the language objectives proposed in 
the lesson plan.      The Text Explorer & Adapter 
(TEA-Tool) feedback module uses NLP methods 
for automatic summarization (Marcu, 1999); Eng-
lish-to-Spanish machine translation (SDL n.d.); 
and, linguistic feedback. A text4, or a webpage 
with the relevant text is uploaded, or accessed, 
respectively, into the TEA-Tool module. The 
summarization capability may be used to reduce 
the amount of text that learners are exposed to re-
duce cognitive load. The machine translation ca-
pability can be used to offer native language 
support to learners with little English proficiency.   
The primary focus in this section, however, will 
center around the linguistic feedback that supports 
the core goal of building teachers? awareness of 
specific linguistic features in texts. The linguistic 
feedback includes specific information about vo-
cabulary, phrasal and sentence complexity, and 
discourse relations.  For vocabulary5, categories of 
feedback include: academic words, cognates, col-
locations and figurative words and terms, cultural 
                                                          
4 Microsoft Word, PDF, and Plain text files may be used. 
5 For academic words, cognates, cultural references, and 
homonyms, customized word lists are used. No NLP is used 
in these cases. 
4
 
 
references, morphological analysis, homonyms 
(e.g., their, there, and they?re), key content words, 
and similes6. For phrasal and sentential complexi-
ty, complex verb and noun phrases, sentences with 
one or more dependent clauses, and passive sen-
tences. For discourse, cause-effect, compare-
contrast, evidence and details, opinion, persuasion, 
and summary relations.  
     The remainder of this section describes features 
in the TEA-Tool module that use NLP to generate 
linguistic feedback. Providing individual evalua-
tion descriptions for each NLP feature is beyond 
the scope of this paper7, intended to focus on user 
study outcomes associated with Language Muse 
use (Section 4).  
    The specific vocabulary (lexical) features that 
use NLP methods or resources include these op-
tions8: basic and challenge synonyms, complex and 
irregular word forms, variant word forms, and 
multiple word expressions.  As discussed earlier, 
unfamiliar vocabulary is recognized as a big con-
tributor to text inaccessibility. The Basic Synonym 
and Challenge Synonym features support the vo-
cabulary comprehension and vocabulary building 
aspects, respectively. To generate the greatest 
breadth of synonyms, the tool uses a distributional 
thesaurus (Lin, 1998), WordNet (Miller, 1995) and 
a paraphrase generation tool (Dorr and Madnani, 
to appear). Previous research has evaluated using 
these combined resources with relevant constraints 
to prevent too many false positives (Burstein and 
Pedersen, 2010).  An additional slider feature al-
lows users to adjust the number of words for 
which the tool will return synonyms for existing 
words in the text. Outputs are based on word fre-
quency. Frequencies are determined using a stand-
ard frequency index (Breland, Jones, and Jenkins, 
1994). If users want synonyms for a larger number 
of words across a broader frequency range that 
includes lower (more rare words) and higher 
(more common words) frequency words, then they 
move the slider further to the right. To retrieve 
synonyms for fewer and rarer words, the slider is 
moved to the left. For all words in the text that are 
within the range of word frequencies at the partic-
ular point on the slider, the tool returns synonyms.  
If users select Basic Synonyms, the tool returns all 
                                                          
6 This new feature was not available during the pilot study. 
7 For details, see Burstein, Sabatini, Shore, Moulder, 
Holtzman & Pedersen (2012). 
8 These reflect the feature names in TEA-Tool. 
words with equivalent or higher frequencies than 
the word in the text. In theory, these words should 
be more common words that support basic com-
prehension. If users select Challenge Synonyms, 
then the tool returns all words with equivalent or 
lower frequencies than the word in the text. In this 
case, the teacher might want to work on vocabu-
lary building skills to help the learner with new 
vocabulary. If the user  selects both the Basic Syn-
onyms and Challenge Synonyms features, then the 
tool will output the  full list of basic (more famil-
iar), and challenge (less familiar) synonyms for 
words in the text.  The teacher can use these syno-
nyms to modify the text directly, or to develop 
instructional activities to support word learning.   
The Complex and Irregular Word Forms and Var-
iant Word Forms feature offers feedback related to 
morphological form. A morphological analyzer 
originally evaluated for an automated short-answer 
scoring system (Leacock & Chodorow, 2003) is 
used. This analyzer handles derivational and in-
flectional morphology. Feedback can be used for 
instructional scaffolding that includes discussion 
and activities related to morphological structure is 
an effective method to build ELLs? vocabulary. 
There are two features that identify words with 
morphological complexity, specifically, words 
with prefixes or suffixes: (1) Complex and Irregu-
lar Word Forms and (2) Variant Word Forms. For 
(1), the morphological analyzer identifies words 
that are morphologically complex. A rollover is 
available for these words. Users can place their 
cursor over the highlighted word, and the word 
stem is shown (e.g., lost ? stem: lose). For (2), the 
system underlines words with the same stem that 
have different parts of speech, such as poles and 
polar. Teachers can build instruction related to this 
kind of morphological variation and teach students 
about variation and relationships to parts of 
speech.   
  Multiple word expressions (MWE) may include 
idioms (e.g., body and soul), phrasal verbs (e.g., 
reach into), and MWEs that are not necessarily 
idiomatic, but typically appear together (colloca-
tions) to express a single meaningful concept (e.g., 
heart disease). All of these MWE types may be 
unfamiliar terms to ELLs, and so they may inter-
fere with content comprehension. Teachers can get 
feedback identifying MWEs to design relevant 
scaffolding for a text. To identify MWEs, two re-
sources are used.  The WordNet 3.0 compounds 
5
 
 
list of approximately 65,000 collocational terms is 
used in combination with a collocation tool that 
was designed to identify collocations in test-taker 
essays (Futagi, Deane, Chodorow, & Tetreault, 
2008). Some terms in the WordNet list are com-
plementary to what is found by the collocation 
tool.  We have found that both outputs are useful. 
Futagi et al?s collocation tool identifies colloca-
tions in a text that occur in seven syntactic struc-
tures that are the most common structures for 
collocations in English based on The BBI Combi-
natory Dictionary of English (Benson, Benson, & 
Ilson, 1997). For instance, these include Noun of 
Noun (e.g., swarm of bees), and Adjective + Noun 
(e.g., strong tea), and Noun + Noun (e.g., house 
arrest). See Futagi et al (2008) for further details.   
    Complex phrasal or sentential features can in-
troduce potential difficulty in a text. A rule-based 
NLP module is used to identify all of these fea-
tures using a shallow parser that had been previ-
ously evaluated for prepositional phrase and noun 
phrase detection (Leacock & Chodorow, 2003). 
The module to identify passive sentence construc-
tion had been previously evaluated for commercial 
use (Burstein, Chodorow, & Leacock, 2004). The 
following feedback features can be selected: Long 
Prepositional Phrases, which identifies sequences 
of two or more consecutive prepositional phrases 
(e.g., He moved the dishes from the table to the 
sink in the kitchen.); Complex Noun Phrases, 
which shows noun compounds composed of two 
or more nouns (e.g., emergency management 
agency) and noun phrases (e.g., shark-infested wa-
ters); Passives, which indicate passive sentence 
constructions (e.g., The book was bought by the 
boy.); 1+Clauses, which highlights sentences with 
at least one dependent clause (e.g., The newspaper 
indicated that there are no weather advisories.); 
and Complex Verbs, which identifies verbs with 
multiple verbal constituents (e.g., would have 
gone, will be leaving, had not eaten). 
       With regard to discourse transition features, 
discourse-relevant cue words and terms are  
highlighted when the following discourse transi-
tions features are identified, including: Evidence 
& Details, Compare-Contrast, Summary, Opinion, 
Persuasion, and Cause-Effect.  A discourse ana-
lyzer previously evaluated for a commercial auto-
mated scoring application is used (Burstein, 
Kukich, Wolff, Lu, Chodorow, Braden-Harder, & 
Harris, 1998). The system identifies cue words and 
phrases in text that are being used as specific dis-
course (or rhetorical) contexts. For instance, ?be-
cause? is typically associated with a cause-effect 
relation. However, some words need to appear in a 
specific syntactic construction to function as a dis-
course term. For instance, the word first functions 
as an adjective modifier and not a discourse term 
in a phrase, e.g., ?the first piece of cake.? When 
first is sentence-initial, as in, ?First, she sliced a 
piece of cake,? then it is more likely to be used as 
a discourse marker, indicating a sequence of 
events.  
4 TPD Pilot 
We report on Language Muse use as it was inte-
grated into a Stanford University TPD program for 
in-service9  teachers.  The site agreed to integrate 
the application into their coursework to support 
coursework instruction, and instructional goals. 
This section describes a pilot study and outcomes 
with in-service teachers enrolled in the program. 
4.1 Study Design 
4.1.1 Site Description 
Stanford University?s courses are offered entirely 
online to teachers as part of a professional devel-
opment program that awards the California State 
Cross-Cultural Language and Academic Devel-
opment (CLAD) certificate through its California 
Teachers of English Learners (CTEL) certification 
process. By state law, all California teachers of 
ELLs must obtain a CLAD/CTEL or equivalent 
certification.  
4.1.2 Teacher Participants 
 
Responses to a background survey administered to 
teachers indicated a range of teaching experience 
from less than a year of teaching experience to as 
much as 37 years of teaching experience.  Teach-
ers taught across a broad range of content areas, 
including Art, Computers, Health, Language Arts, 
Math, Music, Physical Education, Science, and 
Social Studies, and grade levels from Kindergarten 
through 12th grade. 
 
                                                          
9 This refers to teachers who have teaching credentials, and 
can be employed as a classroom teachers. 
6
 
 
4.1.3 Pilot Instructional Activities10, 
 
After responding to the background survey, and 
the two pre-tests (Section 4.1.4), teachers com-
pleted the following TPD activities before moving 
on to post-tests (Section 4.1.4.) First, teachers read 
an article written by a teacher training expert on 
the team. The article describes best practices for 
developing language-based scaffolding for ELLs. 
The article also offers strategy descriptions as to 
how to use Language Muse to complete the lesson 
plan assignment (Section 4.1.4), in particular.  
Teachers then viewed three instructional videos 
that provided instruction about how to use the tool. 
Videos were created by a research team member, 
and included additional instruction about scaffold-
ing strategies. Finally, teachers completed two 
practice activities with Language Muse which 
gave them an opportunity to use the different tool 
modules (TEA-Tool and lesson planning) before 
developing the final lesson plan assignment.  
 
4.1.4 Measurement Instruments11 
 
Teachers completed two surveys, one pre-survey, 
responding to questions about their professional 
background and school context, and a second post-
survey responding to questions related to percep-
tions about Language Muse use.  To evaluate 
teacher knowledge gains, pre- and post-test in-
struments were developed by the project team, and 
included: (a) a multiple-choice (MC) test that 
evaluated teachers? knowledge of linguistic struc-
tures at the Vocabulary, Sentence, and Discourse 
levels, and (b) a constructed- response12 (CR) test t 
measured teachers? ability to identify linguistic 
features in a text13 that were likely to interfere with 
content comprehension,  and to suggest language-
based instructional scaffolding to support compre-
hension. The pretests were administered prior to 
exposure to Language Muse (through the instruc-
tional activities (Section 4.1.3)), and the posttest 
                                                          
10 Instructional activities are available on the Language Muse 
homepage. Teachers save all of their work in Language Muse 
so it can be viewed by course instructors and the research 
team, and accessed by users.  
11 For measurement instruments details, see Burstein et al 
(2012). 
12 Constructed-response tasks require extended written re-
sponses. 
13 An 300-word, 8th grade Social Studies text about U.S. colo-
nization was used. 
after exposure. The same test was administered at 
pre- and post-.14 The CR task was scored by two 
human raters on a 6-point scale (0 to 5, where 
5=highest quality response). Inter-rater reliabili-
ties15 were 0.72 for Vocabulary; 0.75 for Sentenc-
es; and 0.71 for Discourse CR items.  At post-test 
only, teachers developed a lesson plan using the 
lesson planning and TEA-Tool16 modules in Lan-
guage Muse. This occurred after teachers had 
completed the instructional activities included as 
part of Language Muse integration in the Stanford 
program. Lesson plans were evaluated by two hu-
man raters using two distinct rubrics: a) quality of 
Language Skill objectives or b) ELL-specific Skills 
objectives, i.e., unique challenges to ELLs such as, 
idioms or cultural references. Inter-rater reliabili-
ties were 0.61 and 0.71 respectively.   In addition, 
raters reviewed the linguistic feedback features 
that teachers had used to explore the lesson plan 
text, using TEA-Tool. The raters then examined 
the lesson plan and recorded the number of fea-
tures explored that ended up informing the lesson 
plan. Inter-rater reliabilities were 0.69. 
 
4.2 Study Results 
 
    Pre-Posttests, MC and CR. Analyses were con-
ducted for 107 teacher participants for pre- and 
post-MC; 103 pre- and post-CR17.  Paired-samples 
t-test showed statistically significant (p=0.02) in-
crease in the MC Discourse score from pre-test (M 
=13.71, SD =2.22) to post- (M=14.20, SD =2.35; 
(p=0.02) increase in CR Vocabulary pre (M=2.79, 
SD=0.88) to post- (M=2.99, SD=0.86); in the CR 
Sentences score (p=0.02) from pre- (M=1.51, 
SD=1.23) to post- (M=1.91, SD=1.24); in the CR 
Total score (p=0.00) pre- (M=5.96, SD=2.35) to 
post- (M=6.76, SD=2.08).  There were no statisti-
cally significant increases in the MC Vocabulary, 
Sentences, and Total scores, nor CR Discourse.      
    Lesson Plans. Of the 112 teachers who com-
pleted the Lesson Plan assignment, a significant 
                                                          
14 There was a lapse of approximately 8 weeks between the 
pre- and the post-test. 
15 Inter-rater reliabilities in this study reflect Pearson correla-
tions. 
16 The TEA-Tool module is used to explore the linguistic 
features in the text; feedback features are then used to inform 
lesson plan development with regard to the creation of lan-
guage-based scaffolding. 
17 Analyses are reported only for participants who responded 
to the pre- and post-. 
7
 
 
correlation of 0.205 was found between the Lan-
guage Skills Score and the number of feedback 
features used to inform the lesson plan.  
5 Discussion and Conclusions 
This paper discusses how Language Muse, an 
NLP-driven TPD application, supported K-12 
teachers in understanding linguistic features in text 
that may be obstacles to content understanding 
during reading. Through the development of 
teachers? linguistic awareness, our original hy-
pothesis was that teachers would become more 
knowledgeable about linguistic structures, and in 
turn, this would support them in the practice of 
creating lesson plans with greater coverage of text 
language and language objectives that would facil-
itate students? text and content understanding.  
   Study outcomes indicated that the teacher pro-
fessional development package can be successful-
ly implemented in the context of in-service, post-
secondary course work. Through a study with a 
TPD program at Stanford University, results of the 
pre-post assessments administered in the study 
indicated at statistically-significant levels that 
teachers did improve their linguistic knowledge 
about vocabulary, sentences relations, and dis-
course relations, and that they also demonstrated 
and increased ability to offer language-based scaf-
folding strategies as evidenced by an gains pre-
post total score on the CR.  In the context of lesson 
plan development, as a secondary post-test evalua-
tion, teachers who productively used the linguistic 
feedback to inform their lesson plans designed 
higher-quality plans (i.e., addressed language ob-
jectives that target development of new language 
skills), than those who did not.   
   The Language Muse TPD package is now being 
evaluated with nine middle-school teachers with 
high populations of ELLs in California, New Jer-
sey, and Texas. After completion of the TPD, 
teachers will develop lesson units using Language 
Muse, and administer the lessons in their class-
rooms. Pre- and post-tests will be administered to 
students to evaluate the effectiveness of the lesson 
plans vis-?-vis language-based instruction.  
 
Acknowledgments 
 
Research presented in this paper was supported by 
the Institute of Education Science, U.S. Depart-
ment of Education, Award No. R305A100105. 
Any opinions, findings, and conclusions or rec-
ommendations are those of the authors and do not 
necessarily reflect the IES?s views. We are grate-
ful to Steven Holtzman and Jennifer Minsky for 
statistical analysis support. We would like to thank 
Dr. Kenji Hakuta for supporting this work through 
his TPD program at Stanford University. 
 
References 
 
Adger, C. T., Snow, C., & Christian D. (2002). What 
teachers need to know about language. Washington, 
DC: Center for Applied Linguistics. 
August, D. (2003). Supporting the development of Eng-
lish literacy in English language learners: Key issues 
and promising practices (Report No. 61). Baltimore, 
MD: Johns  Hopkins University Center for Re-
search on the Education of Students Placed at Risk.  
Barzilay, Regina and Mirella Lapata (2008). ?Modeling 
Local Coherence: An Entity-Based Approach.? Com-
putational Linguistics, 43(1): 1-34. 
Beck, I. L., McKeown, M. G., & Kucan, L. (2008). 
Creating robust vocabulary: Frequently asked ques-
tions and extended examples. New York, NY: Guil-
ford Press. 
Benson, M., Benson, E., & Ilson, R. (Eds.). (1997).  
The BBI Combinatory Dictionary of English: A 
Guide to Word Combinations. Amsterdam & Phila-
delphia: John Benjamins Publishing Company. 
Berninger, V., Abbot, R., Nagy, W., & Carlisle, J. 
(2009). Growth in phonological, orthographic, and 
morphological awareness in grades 1-6. Journal of 
Psycholinguistic Research, 39, 141-163. 
Breland, H.  Jones, R., and  Jenkins, L (1994). The col-
lege board vocabulary study. Technical Report Col-
lege 
Burstein, J., Sabatini, J., & Shore, J. (in press). In 
Ruslan Mitkov (Ed.), Developing NLP Applications 
for Educational Problem Spaces, Oxford Handbook 
of Computational Linguistics. New York: Oxford 
University Press. 
Burstein, J., Shore, J., Sabatini, J., Moulder, B., 
Holtzman, S., & Pedersen, T. (2012). The Language 
Muse system: Linguistically focused instructional au-
thoring ETS RR-12-21. Princeton, NJ: ETS. 
Burstein, J., and Pedersen, T. (2010). Towards Improv-
ing Synonym Options in a Text Modification Appli-
cation. University of Minnesota Supercomputing 
Institute Research Report Series, UMSI 2010/165, 
November 2010. 
Burstein, J., Chodorow, M., and Leacock, C. (2004). 
Automated Essay Evaluation: The Criterion Online 
Service, AI Magazine, 25(3), 27-36.  
Burstein, J., Kukich, K., Wolff, S., Lu, C.,  Chodorow, 
8
 
 
M., Braden-Harder, L., and Harris, M. D.  (1998). 
Automated Scoring Using A Hybrid Feature Identifi-
cation Technique.  In the Proceedings of the Annual 
Meeting of the Association of Computational Lin-
guistics, August, 1998. Montreal, Canada. 
Calder?n, M. (2007). Teaching reading to English lan-
guage learners, grades 6-12: A framework for im-
proving achievement in the content areas. Thousand 
Oaks, CA: Corwin Press. 
 Calder?n, M., August, D., Slavin, R., Cheung, A., 
Dur?n, D., & Madden, N. (2005). Bringing words to 
life in classrooms with English language learners. In 
A. Hiebert & M. Kamil (Eds.), Research and devel-
opment on vocabulary. Mahwah, NJ: Lawrence Erl-
baum Associates. 
Carlo, M. S., August, D., McLaughlin, B., Snow, C. E., 
Dressler, C., Lippman, D. N., & White, C. E. (2004). 
Closing the gap: Addressing the vocabulary needs of 
English language learners in bilingual and main-
stream classrooms. Reading Research Quarterly, 39, 
188-215. 
Coleman, D., & Pimentel, S. (2011a). Publishers? crite-
ria for the Common Core State Standards in English 
Language Arts and Literacy, grades 3-12. Washing-
ton, DC: National Governors Association Center for 
Best Practices and Council of Chief State School Of-
ficers. 
Collins-Thompson, Kevyn and Jamie Callan (2004). ?A 
Language Modeling Approach to Predicting Reading 
Difficulty.? In Proceedings of the Human Language 
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics. 
Boston, MA: Association for Computational Linguis-
tics, 193-200. 
Darling-Hammond, L. (2000). Teacher quality and stu-
dent achievement: A review of state policy evidence.  
Education Policy Analysis Archives, 8. 
Flesch, R.. (1948). A new readability yardstick. 
Journal of Applied Psychology, 32, 221-233.  
Flinspach, S. L., Scott, J. A., Samway, K. D., & Miller, 
T. (2008, March). Developing cognate awareness to 
enhance literacy: Importante y necesario. Paper pre-
sented at the Annual Meeting of the American Edu-
cational Research Association, New York, NY..  
Francis, D., August, D. Goldenberg, C., & Shanahan, T. 
(2004). Developing literacy skills in English lan-
guage learners: Key issues and promising practices. 
Retrieved June 11, 2007, from:  
www.cal.org/natl-lit-
panel/reports/Executive_Summary.pdf 
Futagi, Y., Deane, P., Chodorow, M., & Tetreault, J.  
(2008). A Computational Approach to Detecting Col-
location Errors in the Writing of Non-native Speakers 
of English, Computer Assisted Language Learning, 
Vol. 21, pp. 353?367. 
G?ndara, P., Maxwell-Jolly, J., & Driscoll, A. (2005). 
Listening to teachers of English language learners: A 
survey of California teachers? challenges, experienc-
es, and professional development needs. Sacramento, 
CA: The Regents of the University of California. Re-
trieved from 
http://www.cftl.org/documents/2005/listeningforweb.
pdf.  
Goldenberg, C. (2008). Teaching English language 
learners: What the research does?and does not?
say. American Educator, 32, 8-21. 
Goldman, S. R., & Rakestraw Jr., J. A. (2000).  Struc-
tural aspects of constructing meaning from text.  In 
M. L. Kamil, P. B. Mosenthal, P. D. Pearson, & R. 
Barr (Eds.), Handbook of reading research (Vol. III, 
pp. 311-335).  Mahwah, NJ: Lawrence Erlbaum As-
sociates. 
Graesser, Arthur C., Danielle S. McNamara, and Jonna 
M. Kulikowich (2011). ?Coh-Metrix: Providing Mul-
tilevel Analyses of Text Characteristics.? Educational 
Researcher, 40(5): 223-234. 
Green, C., Foote, M., Walker, C., & Shuman, C. 
(2010). From questions to answers: Education faculty 
members learn about English learners. In S. Szabo, 
M. B. Sampson, M. M. Foote, & F. Falk-Ross (Eds.), 
Mentoring literacy professionals: Continuing the 
spirit of CRA/ALER after 50 years (pp. 113-125). 
Commerce, TX: Texas A&M University Press. 
Heilman, Michael, Lee Zhao, Juan Pinto, and Maxine 
Eskenazi (2008). ?Retrieval of Reading Materials for 
Vocabulary and Reading Practice.? In Proceedings of 
the Third Workshop on Innovative Use of NLP for 
Building Educational Applications. Columbus, OH: 
Association for Computational Linguistics, 80-88. 
Kieffer, M. J. & Lesaux, N. K. (2008). The role of deri-
vational morphology in the reading comprehension of 
Spanish-speaking English language learners. Reading 
and Writing, 21, 783-804. 
Kintsch, W. (1998). Comprehension: A paradigm for 
comprehension. Cambridge, UK: Cambridge Univer-
sity Press. 
Leacock, C.  & Chodorow, M.  (2003). C-rater: Scoring 
of Short-Answer Questions. Computers and the Hu-
manities, Vol. 37, pp. 389?405. 
Lesaux, N. K., Kieffer, M. J., Faller, S. E., & Kelley, J. 
G. (2010). The effectiveness and ease of implementa-
tion of an academic vocabulary intervention for lin-
guistically diverse students in urban middle schools. 
Reading Research Quarterly, 45, 196-228. 
Lin, Dekang (1998). ?Automatic Retrieval and Cluster-
ing of Similar Words.? In ?Proceedings of the 17th 
International Conference on Computational Linguis-
tics and the 36th Annual Meeting of the Association 
for Computational Linguistics. Montreal, Canada: 
768-774. 
Madnani, Nitin and Bonnie J. Dorr (in press). ?Generat-
ing Targeted Paraphrases for Improved Translation.? 
9
 
 
ACM Transactions on Intelligent Language Muses 
and Technology: Special Issue on Paraphrasing.  
Marcu, Daniel (1999). ?Discourse Trees Are Good In-
dicators of Importance in Text. In Advances in Auto-
matic Text Summarization, eds. Inderjeet Mani and 
Mark T. Maybury. Cambridge, MA: MIT Press, 123-
136. 
Meurers, W. Detmar, Ramon Ziai, Luiz Amaral, Adri-
ane Boyd, Aleksandar Dimitrov, Vanessa Metcalf, 
and Niels Ott (2010). ?Enhancing Authentic Web 
Pages for Language Learners.? In Proceedings of the 
NAACL HLT 2010 Fifth International Workshop on 
Innovative Use of NLP for Building Educational Ap-
plications, eds. Joel Tetreault, Jill Burstein, and 
Claudia Leacock. Los Angeles, CA: Association for 
Computational Linguistics, 10-18. 
Meyer, B. J. F. (2003). Text coherence and readability. 
Topics in Language Disorders, 23, 204-221. 
Mihalcea, Rada, Ravi Sinha, and Diana McCarthy 
(2010). ?SemEval-2010 Task 2: Cross-Lingual Lexi-
cal Substitution.? In Proceedings of SemEval-2010: 
Fifth International Workshop on Semantic Evalua-
tions. Uppsala, Sweden: Association for Computa-
tional Linguistics, 9-14. 
Miller, George A. (1990). ?An On-line Lexical Data-
base.? International Journal of Lexicography 3(4): 
235-312. 
Miltsakaki, Eleni (2009). ?Matching Readers? Prefer-
ences and Reading Skills with Appropriate Web 
Texts.? In Proceedings of the European Association 
for Computational Linguistics. Athens, Greece: As-
sociation for Computational Linguistics, 49-52. 
Nagy, W., Beringer, V., & Abbott, R. (2006). Contribu-
tions of morphology beyond phonology to literacy 
outcomes of upper elementary and middle school 
students. Journal of Educational Psychology, 98, 
134-147. 
National Clearinghouse for English Language Acquisi-
tion (2011). The growing numbers of English learner 
students. Washington, DC: Author. Retrieved from 
http://www.ncela.gwu.edu/files/uploads/9/growingLE
P_0809.pdf.  
National Governors Association Center for Best Prac-
tices and Council of Chief State School Officers 
(2010). Common Core State Standards for English 
language Arts & Literacy in History/Social Studies, 
Science, and Technical Subjects. Appendix A: Re-
search supporting key elements of the Standards. 
Washington, DC: Author. 
Nelson, Jessica, Charles Perfetti, David Liben, and 
Meredith Liben (2012). Measures of Text Difficulty: 
Testing Their Predictive Value for Grade Levels and 
Student Performance. Washington, DC: The Council 
of Chief State School Officers. Retrieved from 
http://www.ccsso.org/Documents/2012/Measures%2
0ofText%20Difficulty_final.2012.pdf.  
Pappamihiel, N. E., Lake, V., & Rice, D. (2005).  
Adapting a Social Studies lesson to include English 
language learners.  Social Studies and the Young 
Learner, 17, 4-7. 
Peske, H. G., & Haycock, K. (2006). Teaching inequal-
ity: How poor and minority students are 
shortchanged on teacher quality. Washington, DC: 
The Education Trust. Retrieved from 
http://www.edtrust.org/sites/edtrust.org/files/publicati
ons/files/TQReportJune2006.pdf.  
Pitler, Emily  and Ani Nenkova (2008). ?Revisiting 
Readability: A Unified Framework for Predicting 
Text Quality.? In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language 
Processing. Honolulu, HI: Association for Computa-
tional Linguistics, 186-195. 
Proctor, C. P., Dalton, D., Uccelli, P., Biancarosa, G., 
Mo, E., Snow, C. E., & Neugebauer, S. (2011).  Im-
proving comprehension online (ICON): Effects of 
deep vocabulary instruction with bilingual and mono-
lingual fifth graders.  Reading and Writing: An Inter-
disciplinary Journal, 24, 517-544. 
Proctor, C. P., Dalton, B., & Grisham, D. (2007).  Scaf-
folding English language learners and struggling 
readers in a multimedia hypertext environment with 
embedded strategy instruction and vocabulary sup-
port.  Journal of Literacy Research, 39, 71-93. 
Rivera, M. O., Moughamian, A. C., Lesaux, N. K., & 
Francis, D. J. (2008). Language and reading inter-
ventions for English language learners and English 
language learners with disabilities. Portsmouth, NJ: 
Research Corporation, Center on Instruction. 
Rutherford William E. and Michael Sharwood Smith 
(1985). ?Consciousness-Raising and Universal 
Grammar.? Applied Linguistics 6(3): 274-282. 
Schwarm, Sarah E.  and Mari Ostendorf (2005). ?Read-
ing Level Assessment Using Support Vector Ma-
chines and Statistical Language Models.? In 
Proceedings of the Annual Meeting of the Association 
for Computational Linguistics. Ann Arbor, MI: As-
sociation for Computational Linguistics, 523-530. 
Schleppegrell, M. J. (2007). The linguistic challenges 
of mathematics teaching and learning: A research re-
view. Reading and Writing Quarterly, 23, 139-159.  
Schleppegrell, M. J., & de Oliveira, L. C. (2006). An 
integrated language and content approach for history 
teachers. Journal of English for Academic Purposes, 
5, 254-268. 
SDL. (n.d.). Automated translation. Retrieved from 
http://www.sdl.com/en/languagetechnology/products/
automated-translation/ 
Walqui, A., & Heritage, M. (2012, January). Instruction 
for diverse groups of ELLs. Paper presented at the 
Understanding Language Conference, Stanford, CA. 
10
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 163?168,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Automated Scoring of a Summary Writing Task
Designed to Measure Reading Comprehension
Nitin Madnani, Jill Burstein, John Sabatini and Tenaha O?Reilly
Educational Testing Service
660 Rosedale Road, Princeton, NJ 08541, USA
{nmadnani,jburstein,jsabatini,toreilly}@ets.org
Abstract
We introduce a cognitive framework for mea-
suring reading comprehension that includes
the use of novel summary writing tasks. We
derive NLP features from the holistic rubric
used to score the summaries written by stu-
dents for such tasks and use them to design a
preliminary, automated scoring system. Our
results show that the automated approach per-
forms well on summaries written by students
for two different passages.
1 Introduction
In this paper, we present our preliminary work on
automatic scoring of a summarization task that is de-
signed to measure the reading comprehension skills
of students from grades 6 through 9. We first intro-
duce our underlying reading comprehension assess-
ment framework (Sabatini and O?Reilly, In Press;
Sabatini et al, In Press) that motivates the task of
writing summaries as a key component of such as-
sessments in ?2. We then describe the summariza-
tion task in more detail in ?3. In ?4, we describe our
approach to automatically scoring summaries writ-
ten by students for this task and compare the results
we obtain using our system to those obtained by hu-
man scoring. Finally, we conclude in ?6 with a brief
discussion and possible future work.
2 Reading for Understanding (RfU)
Framework
We claim that to read for understanding, readers
should acquire the knowledge, skills, strategies, and
dispositions that will enable them to:
? learn and process the visual and typographical
elements and conventions of printed texts and
print world of literacy;
? learn and process the verbal elements of lan-
guage including grammatical structures and
word meanings;
? form coherent mental representations of texts,
consistent with discourse, text structures, and
genres of print;
? model and reason about conceptual content;
? model and reason about social content.
We also claim that the ability to form a coher-
ent mental model of the text that is consistent with
text discourse is a key element of skilled reading.
This mental model should be concise but also reflect
the most likely intended meaning of the source. We
make this claim since acquiring this ability:
1. requires the reader to have knowledge of
rhetorical text structures and genres;
2. requires the reader to model the propositional
content of a text within that rhetorical frame,
both from an author?s or reader?s perspective;
and
3. is dependent on a skilled reader having ac-
quired mental models for a wide variety of
genres, each embodying specific strategies for
modeling the meaning of the text sources to
achieve reading goals.
In support of the framework, research has shown
that the ability to form a coherent mental model
163
is important for reading comprehension. Kintsch
(1998) showed that it is a key aspect in the process of
construction integration and essential to understand-
ing the structure and organization of the text. Sim-
ilarly, Gernsbacher (1997) considers mental models
essential to structure mapping and in bridging and
making knowledge-based inferences.
2.1 Assessing Mental Models
Given the importance of mental models for reading
comprehension, the natural question is how does one
assess whether a student has been able to build such
models after reading a text. We believe that such
an assessment must encompass asking a reader to
(a) sample big ideas by asking them to describe the
main idea or theme of a text, (b) find specific details
in the text using locate/retrieve types of questions,
and (c) bridging gaps between different points in the
text using inference questions. Although these ques-
tions can be multiple-choice, existing research indi-
cates that it is better to ask the reader to write a brief
summary of the text instead. Yu (2003) states that
a good summary can prove useful for assessment of
reading comprehension since it contains the relevant
important ideas, distinguishes accurate information
from opinions, and reflects the structure of the text
itself. More specifically, having readers write sum-
maries is a promising solution since:
? there is considerable empirical support that it
both measures and encourages reading compre-
hension and is an effective instructional strat-
egy to help students improve reading skills
(Armbruster et al, 1989; Bean and Steenwyk,
1984; Duke and Pearson, 2002; Friend, 2001;
Hill, 1991; Theide and Anderson, 2003);
? it is a promising technique for engaging stu-
dents in building mental models of text; and
? it aligns with our framework and cognitive the-
ory described earlier in this section.
However, asking students to write summaries in-
stead of answering multiple choice questions entails
that the summaries must be scored. Asking human
raters to score these summaries, however, can be
time consuming as well as costly. A more cost-
effective and efficient solution would be to use an
automated scoring technique using machine learn-
ing and natural language processing. We describe
such a technique in the subsequent sections.
During the Neolithic Age, humans developed agriculture-what we 
think of as farming.  Agriculture meant that people stayed in one 
place to grow their crops.  They stopped moving from place to 
place to follow herds of animals or to find new wild plants to eat. 
And because they were settling down, people built permanent 
shelters.  The caves they had found and lived in before could be 
replaced by houses they built themselves.
To build their houses, the people of this Age often stacked mud 
bricks together to make rectangular or round buildings.  At first, 
these houses had one big room.  Gradually, they changed to 
include several rooms that could be used for different purposes. 
People dug pits for cooking inside the houses.  They may have 
filled the pits with water and dropped in hot stones to boil it.  You 
can think of these as the first kitchens.
The emergence of permanent shelters had a dramatic effect on 
humans.  They gave people more protection from the weather and 
from wild animals.  Along with the crops that provided more food 
than hunting and gathering, permanent housing allowed people to 
live together in larger communities.
Please write a summary. The first sentence of your summary 
should be about the whole passage.  Then write 3 more 
sentences. Each sentence should be about one of the 
paragraphs.
Passage
Directions
Figure 1: An example passage for which students are
asked to write a summary, and the summary-writing di-
rections shown to the students.
3 Summary Writing Task
Before describing the automated scoring approach,
we describe the details of the summary writing task
itself. The summarization task is embedded within
a larger reading comprehension assessment. As part
of the assessment, students read each passage and
answer a set of multiple choice questions and, in ad-
dition, write a summary for one of the passages. An
example passage and the instructions can be seen in
Figure 1. Note the structured format of summary
that is asked for in the directions: the first sentence
of the summary must be about the whole passage
and the next three should correspond to each of the
paragraphs in the passage. All summary tasks are
structured similarly in that the first sentence should
identify the ?global concept? of the passage and the
164
next three sentences should identify ?local concepts?
corresponding to main points of each subsequent
paragraph.
Each summary written by a student is scored ac-
cording to a holistic rubric, i.e., based on holistic
criteria rather than criteria based on specific dimen-
sions of summary writing. The scores are assigned
on a 5-point scale which are defined as:
Grade 4: summary demonstrates excellent global
understanding and understanding of all 3 lo-
cal concepts from the passage; does not include
verbatim text (3+ words) copied from the pas-
sage; contains no inaccuracies.
Grade 3: summary demonstrates good global un-
derstanding and demonstrates understanding of
at least 2 local concepts; may or may not in-
clude some verbatim text, contains no more
than 1 inaccuracy.
Grade 2: summary demonstrates moderate local
understanding only (2-3 local concepts but no
global); with or without verbatim text, contains
no more than 1 inaccuracy; OR good global un-
derstanding only with no local concepts
Grade 1: summary demonstrates minimal local
understanding (1 local concept only), with or
without verbatim text; OR contains only verba-
tim text
Grade 0: summary is off topic, garbage, or demon-
strates no understanding of the text; OR re-
sponse is ?I don?t know? or ?IDK?.
Note that students had the passage in front of them
when writing the summaries and were not penalized
for poor spelling or grammar in their summaries. In
the next section, we describe a system to automati-
cally score these summaries.
4 Automated Scoring of Student
Summaries
We used a machine learning approach to build an
automated system for scoring summaries of the type
described in ?3. To train and test our system, we
used summaries written by more than 2600 students
from the 6th, 7th and 9th grades about two differ-
ent passages. Specifically, there were a total of 2695
summaries ? 1016 written about a passage describ-
ing the evolution of permanent housing through his-
tory (the passage shown in Figure 1) and 1679 writ-
ten about a passage describing living conditions at
the South Pole. The distribution of the grades for
the students who wrote the summaries for each pas-
sage is shown in Table 1.
Passage Grade Count
South Pole
6 574
7 521
9 584
Perm. Housing
6 387
7 305
9 324
Table 1: The grade distribution of the students who wrote
summaries for each of the two passages.
All summaries were also scored by an experi-
enced human rater in accordance with the 5-point
holistic rubric described previously. Figure 2 shows
the distribution of the human scores for both sets of
summaries.
South Pole (N=1679)
Permanent Housing (N=1016)
0100
200300
400500
600700
800900
Score0 1 2 3 4 Score0 1 2 3 4
Figure 2: A histogram illustrating the human score distri-
bution of the summaries written for the two passages.
Our approach to automatically scoring these sum-
maries is driven by features based on the rubric.
Specifically, we use the following features:
1. BLEU: BLEU (BiLingual Evaluation Under-
study) (Papineni et al, 2002) is an automated
metric used extensively in automatically scor-
ing the output of machine translation systems.
165
It is a precision-based metric that computes n-
gram overlap (n=1 . . . 4) between the summary
(treated as a single sentence) against the pas-
sage (treated as a single sentence). We chose to
use BLEU since it measures how many of the
words and phrases are borrowed directly from
the passage. Note that some amount of borrow-
ing from the passage is essential for writing a
good summary.
2. ROUGE: ROUGE (Recall-Oriented Under-
study for Gisting Evaluation) (Lin and Hovy,
2003) is an automated metric used for scoring
summaries produced by automated document
summarization systems. It is a recall-based
metric that measures the lexical and phrasal
overlap between the summary under consider-
ation and a set of ?model? (or reference) sum-
maries. We used a single model summary for
the two passages by randomly selecting each
from the set of student summaries assigned a
score of 4 by the human rater.
3. CopiedSumm: Ratio of the sum of lengths of
all 3-word (or longer) sequences that are copied
from the passage to the length of the summary.
4. CopiedPassage: Same as CopiedSumm but
with the denominator being the length of the
passage.
5. MaxCopy: Length of the longest word se-
quence in the summary copied from the pas-
sage.
6. FirstSent: Number of passage sentences that
the first sentence of the summary borrows 2-
word (or longer) sequences from.
7. Length: Number of sentences in the summary.
8. Coherence: Token counts of commonly used
discourse connector words in the summary.
ROUGE computes the similarity between the
summary S under consideration and a high-scoring
summary - a high value of this similarity indicates
that S should also receive a high score. Copied-
Summ, CopiedPassage, BLEU, and MaxCopy
capture verbatim copying from the passage. First-
Sent directly captures the ?global understanding?
concept for the first sentence, i.e., a large value for
this feature means that the first sentence captures
more of the passage as expected. Length captures
the correspondence between the number of para-
graphs in the passage and the number of sentences
in the summary. Finally, Coherence captures how
well the student is able to connect the different ?lo-
cal concepts? present in the passage. Note that:
? Although the rubric states that students not be
penalized for spelling errors, we did not spell-
correct the summaries before scoring them. We
plan to do this for future experiments.
? The students were not explicitly told to refrain
from verbatim copying since the summary-
writing instructions indicated this implicitly
(?. . . about the whole passage? and ?. . . about
one of the paragraphs?). However, for future
experiments, we plan to include explicit in-
structions regarding copying.
All features were combined in a logistic regres-
sion classifier that output a prediction on the same
5-point scale as the holistic rubric. We trained a sep-
arate classifier for each of the two passage types.1
The 5-fold cross-validation performance of this clas-
sifier on our data is shown in Table 2. We compute
exact as well as adjacent agreement of our predic-
tions against the human scores using the confusion
matrices from the two classifiers. The exact agree-
ment shows the rate at which the system and the
human rater awarded the same score to a summary.
Adjacent agreement shows the rate at which scores
given by the system and the human rater were no
more than one score point apart (e.g., the system as-
signed a score of 4 and the human rater assigned a
score of 5 or 3). For holistic scoring using 5-point
rubrics, typical exact agreement rates are in the same
range as our scores (Burstein, 2012; Burstein et al,
2013). Therefore, our system performed reasonably
well on the summary scoring task. For comparison,
we also show the exact and adjacent agreement of
the most-frequent-score baseline.
It is important to investigate whether the various
features correlated in an expected manner with the
score in order to ensure that the summary-writing
construct is covered accurately. We examined the
weights assigned to the various features in the clas-
sifier and found that this was indeed the case. As ex-
pected, the CopiedSumm, CopiedPassage, BLEU,
1We used the Weka Toolkit (Hall et al, 2009).
166
Method Passage Exact Adjacent
Baseline
South Pole .51 .90
Perm. Housing .32 .77
Logistic
South Pole .65 .97
Perm. Housing .52 .93
Table 2: Exact and adjacent agreements of the most-
frequent-score baseline and of the 5-fold cross-validation
predictions from the logistic regression classifier, for both
passages.
and MaxCopy features all correlate negatively with
score, and ROUGE, FirstSent and Coherence cor-
relate positively.
In addition to overall performance, we also exam-
ined which features were most useful to the classi-
fier in predicting summary scores. Table 3 shows the
various features ranked using the information-gain
metric for both logistic regression models. These
rankings show that the features performed consis-
tently for both models.
South Pole Perm. Housing
BLEU (.375) BLEU (.450)
CopiedSumm (.290) ROUGE (.400)
ROUGE (.264) CopiedSumm (.347)
Length (.257) Length (.340)
CopiedPassage (.246) MaxCopy(.253)
MaxCopy (.231) CopiedPassage (.206)
FirstSent (.120) Coherence (.155)
Coherence (.103) FirstSent (.058)
Table 3: Classifier features for both passages ranked by
average merit values obtained using information-gain.
5 Related Work
There has been previous work on scoring summaries
as part of the automated document summarization
task (Nenkova and McKeown, 2011). In that task,
automated systems produce summaries of multiple
documents on the same topic and those machine-
generated summaries are then scored by either hu-
man raters or by using automated metrics such as
ROUGE. In our scenario, however, the summaries
are produced by students?not automated systems?
and the goal is to develop an automated system to
assign scores to these human-generated summaries.
Although work on automatically scoring student
essays (Burstein, 2012) and short answers (Lea-
cock and Chodorow, 2003; Mohler et al, 2011) is
marginally relevant to the work done here, we be-
lieve it is different in significant aspects based on
the scoring rubric and on the basis of the underlying
RfU framework. We believe that the work most di-
rectly related to ours is the Summary Street system
(Franzke et al, 2005; Kintsch et al, 2007) which
attempts to score summaries written for tasks not
based on the RfU framework and uses latent seman-
tic analysis (LSA) rather than a feature-based classi-
fication approach.
6 Conclusion & Future Work
We briefly introduced the Reading for Understand-
ing cognitive framework and how it motivates the
use of a summary writing task in a reading compre-
hension assessment. Our motivation is that such a
task is theoretically suitable for capturing the abil-
ity of a reader to form coherent mental representa-
tions of the text being read. We then described a
preliminary, feature-driven approach to scoring such
summaries and showed that it performed quite well
for scoring the summaries about two different pas-
sages. Obvious directions for future work include:
(a) getting summaries double-scored to be able to
compare system-human agreement against human-
human agreement (b) examining whether a single
model trained on all the data can perform as well as
passage-specific models, and (c) using more sophis-
ticated features such as TERp (Snover et al, 2010)
which can capture and reward paraphrasing in ad-
dition to exact matches, and features that can better
model the ?local concepts? part of the scoring rubric.
Acknowledgments
The research reported here was supported by the Institute
of Education Sciences, U.S. Department of Education,
through Grant R305F100005 to the Educational Testing
Service as part of the Reading for Understanding Re-
search Initiative. The opinions expressed are those of the
authors and do not represent views of the Institute or the
U.S. Department of Education. We would also like to
thank Kelly Bruce, Kietha Biggers and the Strategic Ed-
ucational Research Partnership.
167
References
B. B. Armbruster, T. H. Anderson, and J. Ostertag. 1989.
Teaching Text Structure to Improve Reading and Writ-
ing. Educational Leadership, 46:26?28.
T. W. Bean and F. L. Steenwyk. 1984. The Effect of
Three Forms of Summarization Instruction on Sixth-
graders? Summary Writing and Comprehension. Jour-
nal of Reading Behavior, 16(4):297?306.
J. Burstein, J. Tetreault, and N. Madnani. 2013. The E-
rater Automated Essay Scoring System. In M.D. Sher-
mis and J. Burstein, editors, Handbook for Automated
Essay Scoring. Routledge.
J. Burstein. 2012. Automated Essay Scoring and Evalu-
ation. In Carol Chapelle, editor, The Encyclopedia of
Applied Linguistics. Wiley-Blackwell.
N. K. Duke and P. D. Pearson. 2002. Effective Practices
for Developing Reading Comprehension. In A. E.
Farstrup and S. J. Samuels, editors, What Research has
to Say about Reading Instruction, pages 205?242. In-
ternational Reading Association.
M. Franzke, E. Kintsch, D. Caccamise, N. Johnson, and
S. Dooley. 2005. Summary Street: Computer sup-
port for comprehension and writing. Journal of Edu-
cational Computing Research, 33:53?80.
R. Friend. 2001. Effects of Strategy Instruction on Sum-
mary Writing of College Students. Contemporary Ed-
ucational Psychology, 26(1):3?24.
M. A. Gernsbacher. 1997. Two Decades of Structure
Building. Discourse Processes, 23:265?304.
P. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explorations,
11(1).
M. Hill. 1991. Writing Summaries Promotes Think-
ing and Learning Across the Curriculum ? But Why
are They So Difficult to Write? Journal of Reading,
34(7):536?639.
E. Kintsch, D. Caccamise, M. Franzke, N. Johnson, and
S. Dooley. 2007. Summary Street: Computer-guided
summary writing. In T. K. Landauer, D. S. McNa-
mara, S. Dennis, and W. Kintsch, editors, Handbook
of latent semantic analysis. Lawrence Erlbaum Asso-
ciates Publishers.
W. Kintsch. 1998. Comprehension: A Paradigm for
Cognition. Cambridge University Press.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated Scoring of Short-Answer Questions. Comput-
ers and the Humanities, 37(4):389?405.
C.-Y. Lin and E. H. Hovy. 2003. Automatic Evaluation
of Summaries Using N-gram Co-occurrence Statistics.
In Proceedings of HLT-NAACL, pages 71?78.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to Grade Short Answer Questions using Seman-
tic Similarity Measures and Dependency Graph Align-
ments. In Proceedings of ACL, pages 752?762.
A. Nenkova and K. McKeown. 2011. Automatic Sum-
marization. Foundations and Trends in Information
Retrieval, 5(2?3):103?233.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of ACL, pages 311?
318.
J. Sabatini and T. O?Reilly. In Press. Rationale For a
New Generation of Reading Comprehension Assess-
ments. In B. Miller, L. Cutting, and P. McCardle,
editors, Unraveling the Behavioral, Neurobiological,
and Genetic Components of Reading Comprehension.
Brookes Publishing, Inc.
J. Sabatini, T. O?Reilly, and P. Deane. In Press. Prelimi-
nary Reading Literacy Assessment Framework: Foun-
dation and Rationale for Assessment and System De-
sign.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2010.
TER-Plus: Paraphrase, Semantic, and Alignment En-
hancements to Translation Edit Rate. Machine Trans-
lation, 23:117?127.
K. W. Theide and M. C. M. Anderson. 2003. Summariz-
ing Can Improve Metacomprehension Accuracy. Edu-
cational Psychology, 28(2):129?160.
G. Yu. 2003. Reading for Summarization as Reading
Comprehension Test Method: Promises and Problems.
Language Testing Update, 32:44?47.
168
